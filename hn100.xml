<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 30 Sep 2025 06:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Hiring only senior engineers is killing companies (145 pts)]]></title>
            <link>https://workweave.dev/blog/hiring-only-senior-engineers-is-killing-companies</link>
            <guid>45421564</guid>
            <pubDate>Tue, 30 Sep 2025 03:17:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://workweave.dev/blog/hiring-only-senior-engineers-is-killing-companies">https://workweave.dev/blog/hiring-only-senior-engineers-is-killing-companies</a>, See on <a href="https://news.ycombinator.com/item?id=45421564">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-framer-name="Content" data-framer-component-type="RichTextContainer"><p>In the last 3 months, I've interviewed 134 engineers - students, mid-level, seniors and even CTOs.&nbsp;</p><p>My main takeaway: there is a huge pool of exceptional junior engineers<strong> </strong>that most companies won’t even consider.&nbsp;</p><p>While everyone else is fighting over seniors, smart companies can get a significant advantage by going the other direction. If you won’t, your competitors will.</p><p>I’m not alone here. In the beginning of 2025 Shopify recently hired 25 interns (and <!--$--><a href="https://www.firstround.com/ai/shopify?ref=review.firstround.com" rel="noopener">their head of engineering said he</a><!--/$--> aims for 1000(!) more by the end of the year, saying that: “...interns bring energy, drive and intensity that pushes the whole team forward.”)</p><h2>Why do companies avoid juniors?</h2><p>Every company has a different excuse:</p><ul><li data-preset-tag="p"><p>In small startups - “we are a very small team and we don’t have time to mentor juniors, we need engineers who will be very productive from day 1"</p></li><li data-preset-tag="p"><p>In medium-sized companies - “we are going to grow very fast, we need engineers who can handle scale and have faced such challenges before"</p></li><li data-preset-tag="p"><p>In big companies - “our infrastructure is super complex, it’ll take juniors too long to ramp up".</p></li></ul><p>It's safer to hire someone with 4+ years of experience who can contribute from day one, even if their ceiling isn't as high as a motivated junior engineer.</p><p>So colleges continue churning out graduates who can't get a job, while companies complain about how hard it is to find senior engineers (and pay premium rates for them).</p><p>The thing is that many of these "experienced" hires aren't actually that much more productive than a well-mentored junior engineer.</p><p>Software development is not rocket science. There are diminishing returns, you will get better with time, but the pace will become slower. You can tell the difference between a developer with 1 year of experience and one with 5, but between those with 10 vs 15? Probably not.</p><p>The most critical parts (motivation, ambition, character, and brains)<strong> </strong>have little to do with experience.</p><h2>What companies are missing</h2><p>The main mistakes companies make:</p><ol><li data-preset-tag="p"><div><p><strong>Old assumptions about onboarding time.</strong> Companies assume junior engineers need 6-12 months to become productive, but AI-savvy juniors can get up to speed much faster. They can use it to understand codebases (without interrupting team members), generate boilerplate code, and learn new technologies at an accelerated pace.</p></div></li><li data-preset-tag="p"><div><p><strong>Outdated interviews</strong> - most technical interviews still focus on algorithm memorization and whiteboard coding. These skills are less relevant when AI can handle all those known problems. </p></div></li><li data-preset-tag="p"><p><strong>They don't know where to find amazing juniors - </strong>companies recruit from the same places and miss the hidden gems. A perfect example: engineers who applied to Y Combinator but didn't get in. They are very often impressive people with strong motivation and drive.</p></li></ol><p>Before I share my take on junior interviews, let’s talk about what’s so great about them:</p><h2>Why I love working with juniors</h2><ul><li data-preset-tag="p"><p>Juniors are<strong> not restricted by what they know</strong>.&nbsp; They haven't been trained to think "that's just how we do things." They’ll not try to reuse the same technologies from previous companies, or recreate those ‘amazing’ design patterns that were useful only in a specific context. It’s not just being AI-native, it’s about having less resistance to change.&nbsp;</p></li><li data-preset-tag="p"><p>Great juniors<strong> learn fast and search for feedback</strong>. It’s easier to manage them. They <strong>want</strong> to improve and know what you think about their work.</p></li><li data-preset-tag="p"><p><strong>Loyalty.</strong> engineers who you train from the beginning tend to stay longer. They understand your systems deeply and can mentor the next generation of junior engineers.</p></li><li data-preset-tag="p"><p><strong>Higher ceiling.</strong> A motivated junior engineer often has more upside. You're getting someone at the beginning of their growth curve rather than the middle or end.</p></li><li data-preset-tag="p"><p>Juniors bring<strong> fresh energy </strong>to the team - they want to learn, and they have a drive to prove themselves and succeed. Their motivation can be contagious!<br>The existing seniors in your team <strong>will enjoy</strong> working with smart and motivated developers.</p></li></ul><p>Before we continue, a side note:</p><p>The argument here is not to stop hiring seniors. To be effective, juniors need people to mentor them, do high quality code reviews, and so on. And in addition, for some critical challenges, you would still want people who’ve done those before at other places.</p><p>Ok, so you’ve decided it’s worth giving some juniors a chance. Now comes the hard part. As there are so many juniors looking for a job, it can be difficult to find the right ones. Here’s the process that worked for us:</p><h2>How to hire the right junior engineers</h2><p>Most companies are still hiring like it's 2019. AI has fundamentally changed how we write software. If your hiring process hasn't changed, you're probably hiring the wrong people.&nbsp;</p><p>For some reason, companies still ban AI during technical interviews. They're optimizing for skills that matter less while ignoring the skills that actually determine success on the job.</p><p>Here’s our hiring process in 5 steps:</p><h4>1. Filter for the right mindset</h4><ul><li data-preset-tag="p"><p>Ask about <strong>projects they've built, then drill deeper.</strong> Ask "Why did you build this?" and "How does that work?" <br>Keep drilling down until you hit the bottom of their understanding and whether they can think through complex problems.</p></li><li data-preset-tag="p"><p><strong>Look for passion and curiosity.</strong> You want the ones who light up when talking about their projects. They should show real excitement about the problems they've solved. These conversations should be energizing for both of you!</p></li><li data-preset-tag="p"><p><strong>Watch out for red flags.</strong> The ones who get defensive when you ask follow-up questions or can't explain their work past the surface level are probably just chasing a tech salary.&nbsp;</p></li></ul><h4>2. “Use whatever tools you like" home assignment&nbsp;</h4><p>A small and realistic coding challenge with explicit permission to use any tools they want.&nbsp;</p><p>The key is what happens next - we schedule a 30-minute follow-up where they walk through their solution. We ask them to explain their approach, then drill deeper on their reasoning - asking "Why did you make that choice?" or "how does this part work?". We keep going until you reach the limits of their understanding.</p><p>Do they understand the code they submitted? Can they explain design decisions and trade-offs? How deep is their knowledge of the technologies they chose?</p><h4>3. Problem solving without AI</h4><p>A ~40-minute interview to test their ability to think through complex problems and design solutions. We focus on system design, architectural decisions, and reasoning through trade-offs.&nbsp;</p><p>Yes, they will use AI in their job even in those tasks, but we also want to see if they can think for themselves, and what is their fundamental engineering thinking.</p><h4>4. Live implementation with AI</h4><p>We give them 20 minutes for a small live coding task and see how they work with AI tools. Their prompting strategies, how they iterate on AI output, and whether they can effectively guide the AI toward solutions that make sense.</p><h4>5. Evaluate their AI strategy</h4><p>Then, we ask candidates to walk us through their typical development workflow.&nbsp;</p><p>We ask them to explain when they choose AI assistance versus manual coding, ideally with specific examples from projects they've built.</p><p>The engineers who can’t think without AI hit walls when they encounter new problems.</p><p>The ones who resist AI are outpaced by peers who go all-in with the new tools.&nbsp;</p><p>We look for engineers who excel in both phases - the ones who understand the huge benefit AI provides, but also know where it hurts.&nbsp;&nbsp;</p><h2>Making it work after the hire</h2><p>Congrats! You’ve hired a new junior engineer. What’s next?</p><p>Here’s my take:</p><ul><li data-preset-tag="p"><p><strong>Invest in mentoring infrastructure.</strong><br>Don’t just assume they can use AI to learn everything. Make sure your experienced engineers have the time to pair with them, create clear learning paths, and be available for their needs.&nbsp;</p></li><li data-preset-tag="p"><p><strong>Be patient with short-term productivity.</strong> Don’t expect them to produce magic because they are ‘AI-native’. Yes, they should be fast, but give them the proper time to learn the basics of your company.</p></li><li data-preset-tag="p"><p><strong>Measure the right metrics.</strong> Track how quickly junior engineers become productive, their retention rates, and their main blockers. We (very objectively) suggest using Weave to benchmark them to your current engineers and industry standards :)</p></li><li data-preset-tag="p"><p><strong>Talk to them!</strong> Ask them about their experience, and what they think you could improve for the next juniors. Use the fresh perspective they bring, and come with an open mind to their suggestions. Also - make sure to appreciate their work and tell them when you do. Even if they are superstars - they might not know it, and could use some encouragement.&nbsp;</p></li><li data-preset-tag="p"><p><strong>Start small.</strong> Don't hire 10 juniors and let them run amok on your codebases. Start with 1-2 and learn what works (from the previous steps) before scaling up.</p></li></ul><h2>Why this matters more than ever</h2><p>I believe most companies are making a huge mistake by avoiding juniors entirely. They're missing out on AI-native engineers who can learn quickly, adapt to new tools, and grow into really exceptional senior engineers quite fast.</p><p>This market inefficiency won't last forever. As we’ve seen with the Shopify example,&nbsp; smart companies will figure out how to hire and develop junior engineers effectively. They will build incredible teams while their competitors fight over the same pool of expensive senior talent.</p><p>Investing your time in passionate junior developers will pay off in the long run. The question is whether you'll start now or wait until everyone else figures it out.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FAA decides it trusts Boeing enough to certify safety of its own planes again (151 pts)]]></title>
            <link>https://www.theregister.com/2025/09/29/faa_decides_it_trusts_boeing/</link>
            <guid>45420327</guid>
            <pubDate>Mon, 29 Sep 2025 23:56:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/09/29/faa_decides_it_trusts_boeing/">https://www.theregister.com/2025/09/29/faa_decides_it_trusts_boeing/</a>, See on <a href="https://news.ycombinator.com/item?id=45420327">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>After years of relying on the FAA to certify its jets as airworthy, Boeing is finally going to be allowed to do so itself – sometimes.&nbsp;</p>
<p>The US Federal Aviation Administration <a href="https://www.faa.gov/newsroom/faa-statement-boeing-airworthiness-certificates" rel="nofollow">said</a> on Friday that it was granting Boeing "limited delegation" to issue its own airworthiness certificates for the 737 Max and 787 aircraft, which it hasn't been able to do since 2019 and 2022, respectively.&nbsp;</p>
<p>Boeing lost its ability to certify the airworthiness of the 737 Max after a <a href="https://www.theregister.com/2024/05/15/boeing_might_be_criminally_prosecuted/">pair of crashes</a> in 2018 and 2019 that killed 346 people. The agency took 787 certification out of Boeing's hands due to what the FAA said was "production quality issues."&nbsp;</p>

    

<p>As we've previously reported, the 787's problems include things like a need to be <a href="https://www.theregister.com/2024/05/15/boeing_might_be_criminally_prosecuted/">power cycled every 51 days</a> due to faulty software, <a href="https://www.theregister.com/2019/08/08/boeing_787_software_bug_hack/">easy hackability</a> of plane systems, <a href="https://www.theregister.com/2014/01/16/us_safety_authorities_on_boeings_case_787_batteries_fail/">melting batteries</a>, and other issues pointed out before the FAA took control of its airworthiness. Not that things have become much better for the 787 since the FAA assumed responsibility for its airworthiness, mind you: Since then Boeing has <a href="https://www.theregister.com/2023/06/07/boeing_787_production_defect/">delayed 787 deliveries</a> due to faulty horizontal stabilizers, and a whistleblower has argued that <a href="https://www.theregister.com/2024/04/17/boeing_whistleblower_fuselage_gaps/">chronic fuselage gaps</a> have left the entire 787 fleet in a position to fly apart at the seams.&nbsp;</p>

        


        

<p>The 737 Max has continued to have problems since the FAA took over inspection of that Boeing model, too. It was a 737 Max 9 which <a href="https://www.theregister.com/2024/01/08/boeing_737_max_9_airplanes/">lost a door plug in flight</a> last year. The FAA grounded 737 Max aircraft following the door plug incident, after which United Airlines and Alaska Airlines, the only carriers with Max 9s in their fleets, discovered a <a href="https://www.theregister.com/2024/01/09/united_alaska_737_loose_bolts/">chronic problem of loose bolts</a> on the questionably-airworthy aircraft. A Congressional look at internal Boeing documents found emails from engineers <a href="https://www.theregister.com/2020/01/11/boeing_737_max_emails/">saying</a> in 2020 that they wouldn't put their own families on the 737 Max over safety concerns.</p>
<p>The FAA gave Boeing <a href="https://www.theregister.com/2024/02/28/faa_gives_boeing_90_days/">90 days</a> to fix a number of safety shortcomings it flagged in a February 2024 report it published following the door plug blowout. It's well past that 90-day deadline, but the FAA now says that they're at least partially content with improvements the company has made since it started scrutinizing the firm, again, last year.&nbsp;</p>
<h3>
<p>
  <strong>Airworthiness certification custody sharing</strong>
</p>
<p>"The FAA will only allow this step forward because we are confident it can be done safely," the agency said in its Friday press release – but being done safely still means Boeing will be subject to FAA scrutiny.&nbsp;</p>

        

<p>Per the Administration, Boeing will only get to issue airworthiness certificates every other week, with the FAA handling things the other half of the time. Far from being simply an acknowledgement that Boeing is doing better, the FAA is going to use its every-other-week model to spend more time keeping a watchful eye on the assembly process.&nbsp;</p>
<p>"By alternating weeks, we are creating more opportunities to directly observe how Boeing is carrying out this responsibility in practice," an FAA spokesperson told <em>The Register.</em>&nbsp;</p>
<ul>

<li><a href="https://www.theregister.com/2025/01/15/boeing_airbus_commercial_deliveries_2024/">Boeing going backwards as production's slowing and woes keep flowing</a></li>

<li><a href="https://www.theregister.com/2025/04/23/boeing_thoma_bravo_software_sale/">Boeing offloads some software businesses to private equiteer Thoma Bravo</a></li>

<li><a href="https://www.theregister.com/2025/03/28/boeing_starliner_fixes/">Boeing's Starliner may fly again, pending fixes to literally everything</a></li>

<li><a href="https://www.theregister.com/2024/12/31/faa_whistleblower_complaints/">Report claims FAA ignores most whistleblower complaints</a></li>
</ul>
<p>"The FAA's role is to provide oversight of Boeing's performance, ensuring that it issues certificates only when airplanes meet all applicable safety requirements," the spokesperson continued. "Alternating weeks strengthens our ability to identify trends, intervene early if concerns arise, and maintain confidence in the overall safety of Boeing's system."</p>
<p>The FAA didn't tell us whether the agreement was temporary, or how long it might take Boeing to earn its complete trust and confidence in its ability to issue reliable airworthiness certificates. Boeing didn't respond to questions for this story. ®&nbsp;</p>                                
                    </h3></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google to merge Android and ChromeOS in 2026 (121 pts)]]></title>
            <link>https://www.theregister.com/2025/09/25/google_android_chromeos/</link>
            <guid>45418918</guid>
            <pubDate>Mon, 29 Sep 2025 21:21:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/09/25/google_android_chromeos/">https://www.theregister.com/2025/09/25/google_android_chromeos/</a>, See on <a href="https://news.ycombinator.com/item?id=45418918">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p><span>Video</span> Google has confirmed it will merge its ChromeOS and Android operating systems, and that the mobile OS will emerge triumphant.</p>
<p>The ads and search giant has <a target="_blank" href="https://www.theregister.com/2025/07/16/android_replacing_chromeos/">hinted</a> that the two operating systems would merge. On Wednesday at Qualcomm’s Summit event, Google’s president for the Android ecosystem, Sameer Samat, made it official. Android will be the winner and users will see the results in 2026.</p>
<p>"I think the opportunity for us that we see is how do we accelerate all the AI advancement that we're doing on Android and bring that to the laptop form factor as rapidly as possible, and also have the laptop and the rest of the Android ecosystem work seamlessly together?" he said on Wednesday.</p>

    

<p>
  <a href="https://youtu.be/fpHCkdIg9gU?t=4704" data-media="x-videoplayer">Youtube Video</a>
</p>

        


        

<p>"Basically, we're taking the Chrome OS experience, and we're re-baselining the technology underneath it on Android, so that combination is something we're super excited about for next year."</p>
<ul>

<li><a href="https://www.theregister.com/2025/07/16/android_replacing_chromeos/">Google's Android boss suggests ChromeOS could be on borrowed time</a></li>

<li><a href="https://www.theregister.com/2025/05/20/google_high_on_ai_flogs/">Google, high on AI, flogs Gemini for all things</a></li>

<li><a href="https://www.theregister.com/2025/08/01/microsoft_abandons_windows_11_se/">Microsoft gives in to Chromebook bullies and drops Windows 11 SE</a></li>

<li><a href="https://www.theregister.com/2025/09/25/qualcomm_details_x2_elite/">X2 Elite is Qualcomm's latest attempt to bring Apple's M-series magic to the PC</a></li>
</ul>
<p>Chromebooks have helped Google to carve a niche in the laptop market, mostly with low-cost devices <a target="_blank" href="https://www.theregister.com/2025/08/05/canalys_reports_revitalization_of_chromebook/">sold to schools</a> for use by <a target="_blank" href="https://www.theregister.com/2023/08/08/4_in_5_chromebooks_sold_to_us_students/">students</a>. The search behemoth also created <a target="_blank" href="https://www.theregister.com/2013/02/21/google_chromebook_pixel/">expensive and powerful</a> Chromebooks.</p>
<p>But with Google – along with everyone else in tech sphere – <a target="_blank" href="https://www.theregister.com/2025/05/20/google_high_on_ai_flogs/">adding AI</a> to everything, it's Android's time to shine, he said.</p>
<p>Moving to the Android code base will mean Google can deploy its Gemini AI services on more devices, Samat said.</p>

        

<p>To buttress his argument that Android can work on laptops, Samat pointed to the OS being "super successful" on tablet computers.</p>
<p>Qualcomm’s role in Google’s new strategy appears to be adapting its smartphone SoCs for laptops, or ensuring the <a target="_blank" href="https://www.theregister.com/2025/09/25/qualcomm_details_x2_elite/">laptop chips</a> it makes to run Windows can also handle Android.</p>
<p>Samat said that Android also offered opportunities for adding XR (virtual, augmented and other extended-reality systems) to be built into a wide variety of platforms. Android would enable this, he argued.</p>

        

<p>But Samat said merging Google’s two OSes is mostly about AI. Like everything else this year. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: What are you working on? (September 2025) (169 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=45418675</link>
            <guid>45418675</guid>
            <pubDate>Mon, 29 Sep 2025 20:58:11 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=45418675">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><tbody><tr id="45418794"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418794" href="https://news.ycombinator.com/vote?id=45418794&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Currently a one-man side project:</p><p><a href="https://laboratory.love/" rel="nofollow">https://laboratory.love</a></p><p>Last year PlasticList discovered that 86% of food products they tested contain plastic chemicals—including 100% of baby food tested. The EU just lowered their "safe" BPA limit by 20,000x. Meanwhile, the FDA allows levels 100x higher than what Europe considers safe.</p><p>This seemed like a solvable problem.</p><p>Laboratory.love lets you crowdfund independent testing of specific products you actually buy. Think Consumer Reports meets Kickstarter, but focused on detecting endocrine disruptors in your yogurt, your kid's snacks, whatever you're curious about.</p><p>Here's how it works: Find a product (or suggest one), contribute to its testing fund, get detailed lab results when testing completes. If a product doesn't reach its funding goal within 365 days, automatic refund. All results are published openly. Laboratory.love uses the same methodology as PlasticList.org, which found plastic chemicals in everything from prenatal vitamins to ice cream. But instead of researchers choosing what to test, you do.</p><p>The bigger picture: Companies respond to market pressure. Transparency creates that pressure. When consumers have data, supply chains get cleaner.</p><p>Technical details: Laboratory.love works with ISO 17025-accredited labs, test three samples from different production lots, detect chemicals down to parts per billion. The testing protocol is public.</p><p>So far a couple dozen products have received some funding, six products have been fully funded (five product results published, the sixth is at the lab as I write this!)</p><p>You can browse products, add your own, or just follow specific items you're curious about: <a href="https://laboratory.love/" rel="nofollow">https://laboratory.love</a></p></div></td></tr></tbody></table></td></tr><tr id="45420519"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420519" href="https://news.ycombinator.com/vote?id=45420519&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I love this idea. I imagine it could be extended to other types of testing - for example, I've always wished there was a way to more readily verify whether the contents of vitamins were as specified on the label.</p></div></td></tr></tbody></table></td></tr><tr id="45418868"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45418868" href="https://news.ycombinator.com/vote?id=45418868&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Looking at the tofu reports, I really don't know what to make of them. Is there a way to give more meaning to them for the average person? Also, I'd love to see a sort by "almost funded" option.</p></div></td></tr></tbody></table></td></tr><tr id="45420321"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45420321" href="https://news.ycombinator.com/vote?id=45420321&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Second this, it would be useful to have a "EU safe" label or similar to help me understand if 635.8 DEHP is a good thing or bad.</p></div></td></tr></tbody></table></td></tr><tr id="45420780"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420780" href="https://news.ycombinator.com/vote?id=45420780&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>What would be a good strategy to prevent companies from cottoning on to this and gaming the system? They could for example change packaging on production runs for a product that’s undergoing laboratory.love funding campaign.</p></div></td></tr></tbody></table></td></tr><tr id="45420896"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45420896" href="https://news.ycombinator.com/vote?id=45420896&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>My suspicion is if this was gameable, this would be a solved problem by a number of companies. The truth is there is no single simple or even hard step to take, it’s mostly like numerous steps that multiple actors would need to do.</p></div></td></tr></tbody></table></td></tr><tr id="45420301"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420301" href="https://news.ycombinator.com/vote?id=45420301&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>This is great. I thought about a different model even before plasticlist: make a subscription and test various products, but people will have a number of upvotes based on their sub streak. They vote for food to test, and then you show results to everyone subbed. Kind of like what examined does, but they do deep dives into medical topics for subs. I think this model will work better than the one you currently have. Awesome project anyways!</p><p>It is extremely weird to me that countries don't do that on taxpayers money and show the results publicly, this is what they should do.</p></div></td></tr></tbody></table></td></tr><tr id="45420860"><td></td></tr><tr id="45420450"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420450" href="https://news.ycombinator.com/vote?id=45420450&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>A couple of suggestions:</p><p>The completed entry should include the date of the test results, so currency can be judged,</p><p>Ideally the completed entry should contain a scan of the full test report from each of the accredited laboratories.</p></div></td></tr></tbody></table></td></tr><tr id="45419874"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45419874" href="https://news.ycombinator.com/vote?id=45419874&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>This is so incredibly important, well done. The problem of our food being steeped in plastic hits the news here and there, but it should be front and center in my opinion. Testosterone has been plummeting for decades and it scares the heck out of me. The hormone whose job is "form goals, shrug off failure, and try again!" is being destroyed and corporations are given a free pass to pump us full of phthalates and bisphenol. It's infuriating.</p></div></td></tr></tbody></table></td></tr><tr id="45420677"><td></td></tr><tr id="45420763"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45420763" href="https://news.ycombinator.com/vote?id=45420763&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Well that's great for you, but I was making a generalized statement about the role of testosterone, scientific data showing huge decline, and more and more studies linking it to plastics. We can't just alter a key hormone within the span of a few decades and shrug it off. My levels are great for a 40 year old</p><p>And yes there are certainly other factors, but that's not what the original comment was talking about?</p></div></td></tr></tbody></table></td></tr><tr id="45420845"><td></td></tr><tr id="45418816"><td></td></tr><tr id="45420946"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420946" href="https://news.ycombinator.com/vote?id=45420946&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>"Google maps but for old maps": <a href="https://pastmaps.com/" rel="nofollow">https://pastmaps.com</a></p><p>This is a solo startup that I've been working on for 2 years now. It's a labor of love and I'm very lucky and thankful that it's big enough to surprisingly pay all of our bills. Still constantly feeling FOMO over all of my startup buddies working with AI and LLMs while I plug away at old maps and GIS .</p><p>It gets ~80K MAUs and just slowly and consistently is growing organically through word of mouth through history focused communities. I'm currently playing with expanding the coverage internationally as I still only support the US which is a wickedly fun project.</p></div></td></tr></tbody></table></td></tr><tr id="45420968"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420968" href="https://news.ycombinator.com/vote?id=45420968&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I would absolutely love if you brought Canadian maps to this.</p><p>I work for Build Canada and I would love to see some maps from the fur trade and early exploration to tell stories.</p><p>If you want to chat my email is brendan at buildcanada.com</p></div></td></tr></tbody></table></td></tr><tr id="45420965"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420965" href="https://news.ycombinator.com/vote?id=45420965&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm working on a video platform called Nickel. 5-second clips and 5-minute (max) videos. I've been slacking on development but realized recently that I lack focus and am easily distracted by other projects. I wrote about this yesterday.</p><p><a href="https://blog.webb.page/2025-09-28-ikigai.txt" rel="nofollow">https://blog.webb.page/2025-09-28-ikigai.txt</a></p><p>I did figure out something I've long wondered about recently. Y'know how you can see previews of videos in Messages? I got it working! Here's an example video: <a href="https://nickel.video/6NI3n_IlIlII" rel="nofollow">https://nickel.video/6NI3n_IlIlII</a></p><p>My inspiration for Nickel was 1) missing Vine and 2) not wanting to use YouTube to share my gaming clips.</p></div></td></tr></tbody></table></td></tr><tr id="45419210"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419210" href="https://news.ycombinator.com/vote?id=45419210&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm working on a system that helps surgeons make precise bone cuts during knee replacement surgery.  Believe it or not, manual cuts are still the standard in that type of procedure.  Robotic systems exist but they are very costly, big, and actually add time to the surgery (bad news when you are under anesthesia and your leg is in a tourniquet).</p><p>It uses 4k stereoscopic capture and bunch of ML models to match bone position with sub-millimeter precision.  The surgeon screws a metal base piece into the bone, and we detect where that is in space.  Then, a Stewart Platform adjusts another part that is placed onto the base.  The robotic adjustment allows the base to be placed in a ballpark area, with the robotically-adjusted piece oriented in the exact spot where the surgeon needs to cut.</p><p>The net result is a robotic system that is many times cheaper than the least expensive incumbent, decreases surgery time significantly, reduces error, and basically "just works" as opposed to requiring a ton of training.  We are debuting at a tradeshow in October.</p></div></td></tr></tbody></table></td></tr><tr id="45420956"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420956" href="https://news.ycombinator.com/vote?id=45420956&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I've been working on an iOS game called Whiplash over the past ~3 months. It's a pretty simple physics-based top-down action game where you swing a ball attached with a springy cord to your "character" and try to knock enemies off-screen. There are various enemy types and a few have characteristics you can take advantage of using the ball weapon and physics.</p><p>The gameplay itself is roughly along the lines of Asteroids, Geometry Wars, and other games of that genre.</p><p>It's something I've wanted to do for years actually based on this old game I saw a long time ago.</p><p>Fortunately I've also been able to keep it compatible with iPhones, iPads, and Macs, although I think it plays best in the iPhone format.</p><p>App Store: <a href="https://apps.apple.com/us/app/whiplash-sling-smash/id6751450271">https://apps.apple.com/us/app/whiplash-sling-smash/id6751450...</a></p></div></td></tr></tbody></table></td></tr><tr id="45419234"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419234" href="https://news.ycombinator.com/vote?id=45419234&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I’m working on an ISBN database that fetches information from several other services, such as Hardcover.app, Google Books, and ISBNDB, merges that information, and return something more complete than using them alone. It also saves that information in the database for future lookups.</p><p>Mostly because I’m working on a personal library management service called Shelvica to solve my own problems[1], and none of those services provided all the information on a book. One might provide the series, the other might provide genres, and yet another might provide a cover with good dimensions, but none provided everything, so I decided to work on something of my own (called Librario).</p><p>While Shelvica is the focus, Librario could become its own thing in time, so I don’t mind the sidetracking.</p><p>I also plan on having a “ISBN Search” kind of website that feeds from that database as a way to let users search for information about books, which then feeds the service’s database, making it stronger for Shelvica.</p><p>I open source everything I make, but I’m still wondering if these will be open sourced or not. I’ll probably go with the EUPL 1.2 license if I do decide on open sourcing them.</p><p>[1]: My wife and I have a personal library with around 1800 books, but most applications for management are either focused on ebooks or choke with this many books. Libib is the exception, but I wanted a little more.</p></div></td></tr></tbody></table></td></tr><tr id="45420967"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420967" href="https://news.ycombinator.com/vote?id=45420967&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I need the search service so bad.</p><p>I attempted something like this because I wanted a good books search service which provided me at-a-glance information I needed from Storygraph &amp; Goodreads. The main things I look for when I search a book is genres/Storygraph's "moods", number of pages, whether it's part of a series, rating across services &amp; how much does it cost.</p><p>Could never make it work properly.</p></div></td></tr></tbody></table></td></tr><tr id="45420619"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420619" href="https://news.ycombinator.com/vote?id=45420619&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Hey I'd like to learn more about what you're doing. I'm working on a tangentially related service but focusing on audiobooks. One big stumbling block I ran into early on was trying to find something close to a unified ISBN datasource.</p><p>If you're up for it, shoot me an email at charles@geuis.com.</p></div></td></tr></tbody></table></td></tr><tr id="45419369"><td></td></tr><tr id="45419522"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45419522" href="https://news.ycombinator.com/vote?id=45419522&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Didn’t have the time yet, but it’s on my todo list. I have extractors for Google Books, Hardcover.app, and ISBNDB already working, and Amazon, Goodreads, and Anna’s Archive in the todo list.</p><p>I do plan on including a link to the book on Anna’s Archive in the “ISBN Search” website. At least to the search page with the filters already filled.</p></div></td></tr></tbody></table></td></tr><tr id="45420963"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420963" href="https://news.ycombinator.com/vote?id=45420963&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>An “everything” feed reader. Its a plugable framework that allows you to push anything into an RSS feed reader type interface. Email, Slack notifications, RSS, etc.</p><p>I want one place to manage ALL notification settings. So if I want to be notified of Slack messages that contain the word “cat”, I can do that.</p><p>I am also looking to add summarization and tagging using a local SLM. Trying to find a method that can run on older hardware.</p></div></td></tr></tbody></table></td></tr><tr id="45420914"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420914" href="https://news.ycombinator.com/vote?id=45420914&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Have been working on my blog ( <a href="https://bryanhogan.com/" rel="nofollow">https://bryanhogan.com</a> ) and writing more about using Obsidian well. Last two posts were first how to use Obsidian to make a website ( <a href="https://bryanhogan.com/blog/obsidian-website" rel="nofollow">https://bryanhogan.com/blog/obsidian-website</a> ) and the latest a tour of how my current vault works ( <a href="https://bryanhogan.com/blog/obsidian-vault" rel="nofollow">https://bryanhogan.com/blog/obsidian-vault</a> ).</p><p>Also working on DailySelfTrack ( <a href="https://dailyselftrack.com/" rel="nofollow">https://dailyselftrack.com/</a> ), an app to track what matters to you in a way that you find relevant. So it is a mix of habit tracker, health log and journal. Like a spreadhsheet app, but with much better UX. And like a habit/health app, but with much greater customization.</p><p>I want this to be a tool highly useful for people who have complex health issues, are working towards ambitious goals, or just want to regularly reflect on their day.</p><p>I'm building it since I couldn't find a satisfying solution anywhere. It's local first and does not force you into a subscription, or tries to exploit you with any other dark patterns</p></div></td></tr></tbody></table></td></tr><tr id="45419300"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419300" href="https://news.ycombinator.com/vote?id=45419300&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I finally collected the courage to release my operating system into the wild:</p><p><a href="https://news.ycombinator.com/item?id=45400006">https://news.ycombinator.com/item?id=45400006</a></p><p>I'm super curious if anybody will pick it up and do something useful with it. This was a couple of years of my life and I absolutely loved working on it but having a child put a hard stop to such entertainment for many years. Now, a good 30 years later I finally found the time to resurrect it.</p><p>I'm not sure yet if I am going to do more work on it or leave it as it is, it's good enough to give someone new to OS development a running start and a foundation to build on.</p></div></td></tr></tbody></table></td></tr><tr id="45420945"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420945" href="https://news.ycombinator.com/vote?id=45420945&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Trying to document my current hobby project, but stuck in the analysis phase. I dont even know what it is. When I try to describe its purpose I get blank looks. People tend to need physical demonstrations to understand whats going on. Its not entirely new, or novel, its definitely not revolutionary, but it is a hybrid of so many things, in a very indirect sense, that its just beyond my verbiage. Not a humble brag, I dont think its amazing or anything. I have just failed to describe it. Have been trying to get a phd I know to look at it, and describe it for me, but he just straight up isnt interested.</p></div></td></tr></tbody></table></td></tr><tr id="45419137"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419137" href="https://news.ycombinator.com/vote?id=45419137&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I've been working on a 3D voxel-based game engine for like 10 years in my spare time. The most recent big job has been to port the world gen and editor to the GPU, which has had some pretty cute knock-on effects. The most interesting is you can hot-reload the world gen shaders and out pop your changes on the screen, like a voxel version of shadertoy.</p><p><a href="https://github.com/scallyw4g/bonsai" rel="nofollow">https://github.com/scallyw4g/bonsai</a></p><p>I also wrote a metaprogramming language which generates a lot of the editor UI for the engine. It's a bespoke C parser that supports a small subset of C++, which is exposed to the user through a 'scripting-like' language you embed directly in your source files. I wrote it as a replacement for C++ templates and in my completely unbiased opinion it is WAY better.</p><p><a href="https://github.com/scallyw4g/poof" rel="nofollow">https://github.com/scallyw4g/poof</a></p></div></td></tr></tbody></table></td></tr><tr id="45419846"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45419846" href="https://news.ycombinator.com/vote?id=45419846&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>That is so neat. I built something a little bit like this for a simulator of a 3D portal mill. Trying it on real wood got expensive fast so for debugging runs and trials of designs I would run a simulation where the toolbit would hack out the shape out of a three dimensional array of voxels. This was then displayed using a very simple engine built with PyGame. I got a lot of use out of that and it saved days (and a small forest).</p><p>Great to see something along those lines but with much better visuals.</p></div></td></tr></tbody></table></td></tr><tr id="45420437"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45420437" href="https://news.ycombinator.com/vote?id=45420437&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>That's an interesting application!  I'm excited to see where these kinds of projects go now that we have so much computing power.</p></div></td></tr></tbody></table></td></tr><tr id="45420568"><td></td></tr><tr id="45419504"><td></td></tr><tr id="45420440"><td></td></tr><tr id="45420933"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420933" href="https://news.ycombinator.com/vote?id=45420933&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>WithAudio a one-time payment, desktop text-to-speech app that helps users read better by highlighting text as it's spoken.</p><p>Current Challenges:</p><p>Technical: It's difficult to consistently parse text from various document formats. The I also wants to expand to more platforms but I know I need to focus on marketing.</p><p>Non-technical: The product has seen some success with minimal marketing, but I keep getting distracted by spending too much time on technical work. I know I need to do more for marketing but I keep going to my safe space (my IDE).</p><p>I believe in the product but it keeps reminding me how difficult is to get somethig to a polished, finished state for all users. 90% of the project takes 90% of the time and the other 10% takes another 90% of the time.</p><p>Appreciate any feedback.</p><p><a href="https://desktop.with.audio/" rel="nofollow">https://desktop.with.audio</a></p></div></td></tr></tbody></table></td></tr><tr id="45419937"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419937" href="https://news.ycombinator.com/vote?id=45419937&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm currently building Visirya, an app that helps people record their night dreams and transforms them into short videos and written journals. The bigger goal is to use this dream data to create dream cartographies, essentially maps of recurring themes, emotions, and symbols—to uncover patterns and insights across dreams over time.</p><p>So far, we've built the video generation and dream journaling features. The app is live on TestFlight, and we're preparing a major update soon that includes a new better UI, and dream questionnaire to help with pattern recognition and dream mapping.</p><p>Would love to hear thoughts, feedback, or connect with others working on similar intersections of tech and the mind! If you're interested in trying it out, you can find the TestFlight link on our website: <a href="https://visirya.com/" rel="nofollow">https://visirya.com</a></p></div></td></tr></tbody></table></td></tr><tr id="45420700"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420700" href="https://news.ycombinator.com/vote?id=45420700&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>The idea is nice, but I wonder if a generated video can have any resemblance of the actual dream. At least for me, dreams are very tied to emotion, and the visuals are kinda blurry, so i don’t know if that sort of thing can provide any satisfaction. But I know certain aesthetics can feel “dreamlike”.</p></div></td></tr></tbody></table></td></tr><tr id="45420755"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420755" href="https://news.ycombinator.com/vote?id=45420755&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Super cool! I'm building in the same space but for Muslims - Dreamstate: Interpret your dreams Islamically <a href="https://dreamstateai.replit.app/" rel="nofollow">https://dreamstateai.replit.app/</a></p><p>I tried your app - it's quite abrupt to go straight to Access Microphone permissions. The voice recording took a long time to analyse, it timed out for me. It's a great idea but didn't work for me unfortunately.</p></div></td></tr></tbody></table></td></tr><tr id="45420590"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420590" href="https://news.ycombinator.com/vote?id=45420590&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm working on an audiobook service (currently for myself) that will fill in major missing features for platforms like Audible.</p><p>- Ignore AI voiced books</p><p>- Show me unread books in series that I have in my library</p><p>- Experimenting with better search. I have experience with building semantic search systems and have been highly disappointed with Audible's extremely sub-par search capabilities. I want results that are actually based on books, authors, and narrators that I have already purchased, read, or listened to.</p><p>- Get automatic notifications when new books from authors and narrators that I subscribe to become available.</p><p>There's at least a few more gripes I want to address, but these are the high priority ones that come to mind right now.</p></div></td></tr></tbody></table></td></tr><tr id="45420915"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420915" href="https://news.ycombinator.com/vote?id=45420915&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>&gt;- Ignore AI voiced books</p><p>My biggest issue these days, is that after spending 1000 hours messing around in eleven labs, almost all female american audiobook narrators sound AI generated to me. I feel like as a demographic they must have sold a lot of voice recordings to the platform for analysis. I have DNR'ed a few audiobooks recently due to this.</p></div></td></tr></tbody></table></td></tr><tr id="45420303"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420303" href="https://news.ycombinator.com/vote?id=45420303&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Hi HN, I am working on Circuitscript, a language based on python to describe electronic schematics/circuits: <a href="https://circuitscript.net/" rel="nofollow">https://circuitscript.net/</a></p><p>Recently, I have released a simple IDE (called the Bench) to try Circuitscript online: <a href="https://bench.circuitscript.net/" rel="nofollow">https://bench.circuitscript.net/</a></p><p>The next steps are to create more schematics with Circuitscript as examples to test the limitations of the language and to generate PCB designs with KiCAD. The Circuitscript tool (currently only the desktop cli tool) is able to generate KiCAD netlists and this can be imported into PCBnew.</p><p>The motivation for creating Circuitscript is to describe schematics in terms of code rather than graphical UIs after using different CAD packages extensively (Allegro, Altium, KiCAD) in the past. I wanted to spend more time thinking about the schematic design itself rather than fiddling around with GUIs.</p><p>The main language goals are to be easy to write and reason, generated graphical schematics should be displayed according to how the designer wishes so (because this is also part of the design process) and to encourage code reuse.</p><p>Please check it out and I look forward to your feedback, especially from electronics designers/hobbyists. Thanks!</p></div></td></tr></tbody></table></td></tr><tr id="45419703"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419703" href="https://news.ycombinator.com/vote?id=45419703&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Building an iOS app for metronome sequencing to get faster at playing guitar and reaching "shred" speeds at different subdivisions/time signatures in a single sequence. Planning on adding accuracy indicators and scoring so rushing or dragging can be easily identified when finishing a saved routine. I.e., some post-routine metrics.</p><p>I've been playing guitar for a little under 6 years and ran into the common problem among many intermediate guitarists fall into, which is stagnating into a plateau at a certain BPM.</p><p>The most effective solution I've found is to take the top speed hit playing a chunk of a lick and simply increase it 20-50 BPM past that limit, attempting one's best to stay in tempo. Regardless of how sloppy it sounds. Then roughly halve that increased addition of BPM, it will become relatively easier to play. For example, if you are stuck at 120 BPM, upping it to 150 BPM with sloppy attempts, then dropping it back down to 130-140 BPM.</p><p>I've gone cleanly from alternate picked 140 BPM triplets to 220 BPM triplets in two months after being stuck at 140 BPM for over a year with this method. Sometimes even hitting 280 BPM triplets when I have the focus and time for it.</p><p>Even then, I want a more consistent, and variable way of customizing a practice session using a metronome from a hobbyist perspective without using a DAW. With a simpler interface for doing so. As well as encourage with said method above for other guitarists in the pursuit of speed.</p></div></td></tr></tbody></table></td></tr><tr id="45419807"><td></td></tr><tr id="45419852"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45419852" href="https://news.ycombinator.com/vote?id=45419852&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>At least a couple more weeks. Hopefully less than a month out from now.</p><p>I have most of the UI done for sequencing. Workflows for speed building and metronome sequencing will be completely free, which is also a top priority for me to get out the door first.</p></div></td></tr></tbody></table></td></tr><tr id="45420811"><td></td></tr><tr id="45420825"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420825" href="https://news.ycombinator.com/vote?id=45420825&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Obsetico App (named after a friends' comment that "it's great for Obsessed people like my wife"</p><p>A mobile app to track tasks, events and any info about anything you care about: your car, home, tools, workshop, appliances, pets, lab equipment... anything really.</p><p>Lets you organize "resources" in a hierarchy (like "folders"). You can then define  tasks, add pictures, geolocation, contacts, notes, events, etc to them. Recently added the feature to "share" resources with others.</p><p>Google Play: <a href="https://play.google.com/store/apps/details?id=com.code54.quickfix">https://play.google.com/store/apps/details?id=com.code54.qui...</a>
App Store: <a href="https://apps.apple.com/us/app/obsetico/id6749025870">https://apps.apple.com/us/app/obsetico/id6749025870</a></p><p>It's so generic that it's hard to describe :-) I need a better elevator pitch.</p></div></td></tr></tbody></table></td></tr><tr id="45420891"><td></td></tr><tr id="45420912"><td></td></tr><tr id="45420281"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420281" href="https://news.ycombinator.com/vote?id=45420281&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm working on Macscope (<a href="https://macscope.app/" rel="nofollow">https://macscope.app</a>), a better Cmd+Tab for macOS. I built it because macOS window management feels slow compared to the keyboard-driven speed of a terminal or code editor.</p><p>It augments your existing muscle memory: a quick tap of a shortcut switches apps like normal, but holding it opens a powerful interface with features like:</p><p>Unified Search: Instantly find any window, app, or browser tab.</p><p>Scopes: Save and restore entire window layouts for different projects (perfect for after you unplug a monitor).</p><p>Placement Modes: Snap windows to screen halves as you switch to them.</p><p>The goal is to make the OS feel as fast as my other tools. I'm always looking for feedback on how to make window management less frustrating!</p></div></td></tr></tbody></table></td></tr><tr id="45420716"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420716" href="https://news.ycombinator.com/vote?id=45420716&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I have been building <a href="https://github.com/zayr0-9/Yggdrasil" rel="nofollow">https://github.com/zayr0-9/Yggdrasil</a></p><p>It started as a solution to LLM front ends having terrible native branching features. But slowly I realize most of our data will be going through LLM's so Yggdrasil is evolving into a platform which consumes all your LLM queries, while keeping it easy to query and reference.</p><p>And now I have begun to realize how detrimental LLM assisted coding can be to someone who starts depending on it too much, so Yggdrasil is a bet in the other direction as compared to mainstream. Instead of agents/AI doing everything I believe human + ai assistance will win in the end.</p><p>Yggdrasil has a simple agent called Valkyrie, so they have their place, but that I believe should be the last step, after the developer has discussed and planned thoroughly through our tree interface, Heimdall.</p><p>And if someone replaces the dev, they can browse their conversations with the LLM, observe their mind map, what questions they asked, what extra things they considered (branches), the whole thought process easily navigable and visible.</p><p>Personally after using Yggdrasil, I feel quite confident in using the LLM, as I can ask all the silly questions I want, without worrying about context pollution. It aligns really well with the natural exploratory tangential thoughts we have when trying to find solutions or learn something.</p></div></td></tr></tbody></table></td></tr><tr id="45419314"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419314" href="https://news.ycombinator.com/vote?id=45419314&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Working on a webapp for critically think with others about a problem.</p><p>The idea is that you build a diagram that contains <i>all</i> the details about the problem and people's thoughts on it, and it's organized in such a way that it's easy to just <i>keep refining</i>, down to the smallest detail. So you build this concrete, shared <i>understanding</i>, and move it forward and forward, until hopefully y'all can make some best decision to improve the situation.</p><p>There's a lot to do. Currently working on UX to allow hiding intermediate nodes and still have indirect edges drawn. Want to add an LLM integration to generate/update diagrams via natural language, which I think will help a lot with usage barriers to using the app.</p><p>Happy to get any feedback :) <a href="https://ameliorate.app/" rel="nofollow">https://ameliorate.app/</a> <a href="https://github.com/amelioro/ameliorate" rel="nofollow">https://github.com/amelioro/ameliorate</a></p></div></td></tr></tbody></table></td></tr><tr id="45420920"><td></td></tr><tr id="45420381"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420381" href="https://news.ycombinator.com/vote?id=45420381&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Building <a href="https://fallinorg.com/" rel="nofollow">https://fallinorg.com/</a>, a Mac app that organizes your files.</p><p>It looks inside each file to see what it’s about, then moves it to the right folder for you.</p><p>Everything happens on your Mac, so nothing leaves your computer. No clouds, no servers.</p><p>It already works with PDFs, text, Markdown, and many other file types.  
Next I’m adding ePub, and later Microsoft Office and iWork support.</p><p>If you have messy folders anywhere on your Mac, Fallinorg can help.</p></div></td></tr></tbody></table></td></tr><tr id="45420934"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420934" href="https://news.ycombinator.com/vote?id=45420934&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>This is perfect for cleaning out my Downloads folder and adhering to the Johnny Decimal system (as a first pass, anyway). Neat!</p></div></td></tr></tbody></table></td></tr><tr id="45420652"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420652" href="https://news.ycombinator.com/vote?id=45420652&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>This might be what I've been looking for. On the first of every month I have Hazel put everything in ~/Downloads/yyyy-mm (previous month), with the intent to move each file to the correct project/area folder in my actual file structure. But I'm about 1.5 years behind on that...</p><p>Have you looked at competitors? If so, what are they? I haven't found anything that does this as elegantly as Fallinorg.</p></div></td></tr></tbody></table></td></tr><tr id="45418821"><td></td></tr><tr id="45420509"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420509" href="https://news.ycombinator.com/vote?id=45420509&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm working on an AI thumbnail/graphics maker using the various image models.</p><p><a href="https://thumbnail.ai/" rel="nofollow">https://thumbnail.ai/</a></p><p>You just upload a picture and pick a design type and it generates a thumbnail for you. Got good feedback last time I posted, steadily and slowly growing now.</p></div></td></tr></tbody></table></td></tr><tr id="45420429"><td></td></tr><tr id="45419090"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419090" href="https://news.ycombinator.com/vote?id=45419090&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>On the side, custom coloring books for kids using nano banana, started with a project for my son, and its a little janky for some photos but have had some interest already: <a href="https://bespokebooks.io/" rel="nofollow">https://bespokebooks.io</a>. I think it needs to be a phone app to really work for most people though, so that's next on my to do list besides some prompt tweaking.</p><p>Notebook to do it yourself here: <a href="https://github.com/dbish/bespoke-books-ai-example" rel="nofollow">https://github.com/dbish/bespoke-books-ai-example</a></p><p>I think there are a lot of really fun projects possible now in the child book creation space, particularly as you build tools that they can use themselves (like adding voice interfaces to building a book or story).</p><p>This is outside my 996 job of AI Agent/Assistant infra + ops :)</p></div></td></tr></tbody></table></td></tr><tr id="45420921"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420921" href="https://news.ycombinator.com/vote?id=45420921&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Working on AuraJoie, a calm, private space to share meaningful photo albums with family and friends.</p><p>No likes, no feeds, no noise... just beautiful albums and good energy.</p><p>Focus is on memory moments, not social media. Early users are using it for family trips, kids, and quiet reflections.</p><p>Would love feedback: <a href="https://aurajoei.com/" rel="nofollow">https://aurajoei.com</a></p></div></td></tr></tbody></table></td></tr><tr id="45420805"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420805" href="https://news.ycombinator.com/vote?id=45420805&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I've been building a website to find great blog posts related to the programming/tech world called <a href="https://greatreads.dev/" rel="nofollow">https://greatreads.dev/</a></p><p>There are a lot of things that I still want to polish, but it's in a usable state already, and I'm very happy with it.</p><p>If someone takes a look and has any suggestions, feedback, or ideas, they are all welcome.</p><p>Also, any suggestions for blogs that could be added as sources is appreciated.</p></div></td></tr></tbody></table></td></tr><tr id="45419150"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419150" href="https://news.ycombinator.com/vote?id=45419150&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Continuing to build <a href="https://crucialexams.com/" rel="nofollow">https://crucialexams.com/</a>, a platform that helps people prepare for IT certifications like CompTIA, AWS, and Microsoft/Azure.  It offers realistic practice tests and study tools.  I also have partnered with educators and universities who now offer it to their students and get dashboards to review student progress and identify where they are struggling.</p></div></td></tr></tbody></table></td></tr><tr id="45418908"><td></td></tr><tr id="45419023"><td></td></tr><tr id="45419433"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45419433" href="https://news.ycombinator.com/vote?id=45419433&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>It's funny you say that. I already do run a weekly "book club" group, but it's at work at my $dayjob employer. And, for various reasons, we've drifted away from the book focus and turned into a more presentation/discussion oriented group. But I still love to read physical books, and wouldn't be opposed to trying to come up with something to structure some discussion around some of these "outside of work" readings that I do.</p><p>If you want, drop me and email (prhodes@fogbeam.com) and maybe we can set something up.</p></div></td></tr></tbody></table></td></tr><tr id="45420601"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420601" href="https://news.ycombinator.com/vote?id=45420601&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I have been working on my terminal editor, but I parked that for now -- <a href="https://github.com/bloomca/love" rel="nofollow">https://github.com/bloomca/love</a>. It is possible to load a file and edit it, copy/paste works, you can select text, etc. The next step is to integrate with the tree-sitter for syntax highlighting and then with LSP, but it took a bit more time than I wanted.</p><p>Another project of mine is to play music from my audio CDs by myself. I built a simple Rust library to read TOC and raw PCM data from a CD drive -- <a href="https://github.com/Bloomca/rust-cd-da-reader" rel="nofollow">https://github.com/Bloomca/rust-cd-da-reader</a> (works on Windows, macOS and Linux), and a ripper -- <a href="https://github.com/Bloomca/audio-cd-ripper" rel="nofollow">https://github.com/Bloomca/audio-cd-ripper</a>, which rips all tracks and encodes it as FLAC and fetches metadata from MusicBrainz.</p><p>The next step is to play it. I looked into using cpal (<a href="https://github.com/RustAudio/cpal" rel="nofollow">https://github.com/RustAudio/cpal</a>), but I feel like using low-level audio API for each platform is a better approach for learning.</p></div></td></tr></tbody></table></td></tr><tr id="45419699"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419699" href="https://news.ycombinator.com/vote?id=45419699&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I launched Quiet UI this week:</p><p><a href="https://quietui.org/" rel="nofollow">https://quietui.org/</a></p><p>It prioritizes accessibility, longevity, performance, and simplicity.</p><p>With the autoloader, one script tag loads components dynamically without downloading the entire library. (npm also available.)</p><p>Theming uses color-mix() and OKLAB to create uniform color palettes from a single CSS property. Adaptive palettes are used for dark mode.</p><p>All form controls are form-associated via ElementInternals and work with native validation APIs (required, pattern, etc.).</p><p>Dialogs, popovers, tooltips, etc. use Popover API for top-layer access without having to portal or hoist.</p><p>Some of the more fun components include: Joystick, Stamp, Mesh Gradient, Flip Card, Random Content, Intersection Observer, Typewriter, Lorem Ipsum, Slide Activator</p><p>The library is free for personal, educational, non-profit use. Commercial use requires a license.</p></div></td></tr></tbody></table></td></tr><tr id="45419789"><td></td></tr><tr id="45419779"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45419779" href="https://news.ycombinator.com/vote?id=45419779&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>FYI the browse components button clips text from the next section on ios</p><p>From the text ‘What's in the box?’ Only the W is visible</p></div></td></tr></tbody></table></td></tr><tr id="45419928"><td></td></tr><tr id="45419601"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419601" href="https://news.ycombinator.com/vote?id=45419601&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Working on a personal recruiter / talent agent for my smartest dev/product/design friends (and theirs) <a href="https://www.hedgy.works/" rel="nofollow">https://www.hedgy.works</a></p><p>Key problems we're solving:</p><p>- Everyone wants to be doing meaningful, fun work that feels like their "life's work". Few feel like they are.</p><p>- In recruiting, the AI spam problem is real and only getting worse, essentially killing the cold application pipeline. You need a referral.</p><p>- Optimizing your career feels like annoying politicking for a lot of the most talented folks who just want to focus on building cool stuff. But, as an employee, if you don't test the market (e.g. take a recruiter conversation) from time to time, your comp can really stagnate.</p></div></td></tr></tbody></table></td></tr><tr id="45420827"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420827" href="https://news.ycombinator.com/vote?id=45420827&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm making a visual explainer site for PyTorch functions:</p><p><a href="https://whytorch.org/" rel="nofollow">https://whytorch.org/</a></p><p>A great example of how it works is <a href="http://whytorch.org/torch.amax/" rel="nofollow">http://whytorch.org/torch.amax/</a></p><p>Clicking items in the tensors explains where they came from and where they are used in the output. The input tensors can be modified too.</p><p>It's a one-man side project that's been half building the site framework, and half re-implementing pytorch functions in javascript. Plenty more functions to go, but hopefully people can already find it useful. I'm planning on doing a Show HN once I've added ~10 more functions.</p><p>Posting this from a throwaway account because my main account is locked due to `noprocrast`!</p></div></td></tr></tbody></table></td></tr><tr id="45420812"><td></td></tr><tr id="45419808"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419808" href="https://news.ycombinator.com/vote?id=45419808&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I’m building TypeQuicker: <a href="https://www.typequicker.com/" rel="nofollow">https://www.typequicker.com</a></p><p>Typing is an extremely underrated skill and especially in the age of LLMs, it is the bottle neck in a lot of cases.</p><p>I’ve never been fond of existing typing apps; excessive ads, typing random words, etc so I built my own.</p><p>You can practice typing code, use your own text, etc</p><p>We have a paid plan for features where you can type natural text that targets your weak points (via SmartPractice) and many others. Other than that, it’s both free to use (and ad-free)</p></div></td></tr></tbody></table></td></tr><tr id="45419851"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45419851" href="https://news.ycombinator.com/vote?id=45419851&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>nice!</p><p>i open and close parens, brackets and curlies at the same time.</p><p>is there a mode/setting to capture this intent?</p></div></td></tr></tbody></table></td></tr><tr id="45420065"><td></td></tr><tr id="45420671"><td></td></tr><tr id="45420951"><td></td></tr><tr id="45420815"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420815" href="https://news.ycombinator.com/vote?id=45420815&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Trying to re-legalize "neighborhood commercial" by right in the city I live in. Things like corner stores or small barber shops or coffee shops or converted restaurants. ACUs or Accessory Commercial Units, home conversions... different ways of doing it.</p></div></td></tr></tbody></table></td></tr><tr id="45419134"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419134" href="https://news.ycombinator.com/vote?id=45419134&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Release engineering for FreeBSD 15.0-RELEASE.  Major releases are always a lot of work, but this is probably the biggest release in 20 years due to the new base system distribution system landing.  (We're switching from "here's a tarball containing everything" to "here's 500 packages", with resulting changes in the build process, download/update mirrors, installer, etc.)</p></div></td></tr></tbody></table></td></tr><tr id="45419850"><td></td></tr><tr id="45420016"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45420016" href="https://news.ycombinator.com/vote?id=45420016&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Target is 2025-12-02 00:00 UTC.</p><p>Given that this is a major release, there are fairly wide error bars on that; it could be as much as 3 weeks earlier if the first release candidate turns out to be perfect, and of course it could be later if things go badly (but I very much hope to get it out by the end of 2025).</p></div></td></tr></tbody></table></td></tr><tr id="45420355"><td></td></tr><tr id="45420201"><td></td></tr><tr id="45420155"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420155" href="https://news.ycombinator.com/vote?id=45420155&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm working on 2 projects right now:</p><p>1. Fluxmail - <a href="https://fluxmail.ai/" rel="nofollow">https://fluxmail.ai</a></p><p>Fluxmail is an AI-powered email app that helps you get done with email faster. I think there's a significant opportunity for AI to change the way we use email, and I'm experimenting with ways to improve the status quo. I'd love to hear what features you'd like to see in such an app!</p><p>2. ExploreJobs.ai - <a href="https://explorejobs.ai/" rel="nofollow">https://explorejobs.ai</a></p><p>This is a job board for AI jobs and companies. The job market in AI is pretty hot right now, and there are a lot of cool AI companies out there. I'm hoping to connect job seekers with fast-growing AI companies.</p></div></td></tr></tbody></table></td></tr><tr id="45419357"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419357" href="https://news.ycombinator.com/vote?id=45419357&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Still working on cataloging a curated list of craft beer venues across the world at <a href="https://wheretodrink.beer/" rel="nofollow">https://wheretodrink.beer</a></p><p>Unsure what the plan is going forward with it, apart from adding more venues and more countries. As long as it's fun for me I'll just keep adding things.</p><p>Next addition will be to add health inspection data from countries that have that in open datasets or APIs, so if anyone know of that I'd be appreciative of hints (know of UK, Norway and might have found for France).</p></div></td></tr></tbody></table></td></tr><tr id="45419667"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419667" href="https://news.ycombinator.com/vote?id=45419667&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I’ve been working on a few utility libraries to make it easier to develop web services, basically exporting packages that I find myself using or rewriting often and exporting them as their own modules.</p><p>I recently published <a href="https://github.com/hxtk/sqlt" rel="nofollow">https://github.com/hxtk/sqlt</a> for SQL query generation with Go templates.</p><p>I’m working on <a href="https://github.com/hxtk/aip" rel="nofollow">https://github.com/hxtk/aip</a> as a collection of libraries giving safe default choices to implement Google’s API improvement proposals in ConnectRPC services. It borrows (with attribution per the license) an unexported implementation of AUP-160 filters from the LuCI project, and I intend to expand it to support data sources other than SQL databases and page tokens, and it also exports an implementation of AIP-161 field masks (which have different semantics compared to standard field masks) and middleware to help with using them for AIP-157 read filtering. I intend to export more middleware that I use frequently, but I don’t know if it’ll live in this module or its own yet.</p></div></td></tr></tbody></table></td></tr><tr id="45420299"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420299" href="https://news.ycombinator.com/vote?id=45420299&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm pursuing my vision of "music-i18n": Open source music software that works for microtonal music and worldwide musical cultures.</p><p>It's not a from-scratch effort, quite the contrary: I'm trying to tie in existing music standards (MIDI, MusicXML, SMuFL, MEI, etc.) and ensure that FOSS systems (MuseScore, Verovio, smaller components) implement enough of those standards to support music-i18n.</p><p>Sometimes, this also includes extending the standards themselves when they are not fully capable of representing some non-mainstream musical aspect. For example, MusicXML lacks the ability of representing multiple accidentals per note (whereas MEI does), which is a must for microtonality.</p><p>I started down this path around 2018, as a music player who got interested in arranging Arabic songs in a "Real Book" style. It opened a giant rabbit hole that I'm still far from having fully explored.</p><p>Now and then, I collaborate with other devs who are interested in adjacent topics. I would love to hear from some of you here!</p><p>As an entry point, I recommend checking out the "progress report" I wrote last October: <a href="https://blog.karimratib.me/2024/10/01/music-grimoire-progress-report.html" rel="nofollow">https://blog.karimratib.me/2024/10/01/music-grimoire-progres...</a> - I'm currently drafting this year's update. My main demo is at <a href="https://blog.karimratib.me/demos/musicxml/" rel="nofollow">https://blog.karimratib.me/demos/musicxml/</a></p></div></td></tr></tbody></table></td></tr><tr id="45418996"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418996" href="https://news.ycombinator.com/vote?id=45418996&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm rebuilding OnlineOrNot's frontend to be powered by the public REST API. Doing this both as a means of dogfooding, and adding features to the REST API, that I easily dumped into the private GraphQL API without thinking too hard.</p><p>Basically I've realised GraphQL has taken me as far as it can, and I should've gone with REST to start with. That, and after I finish the first milestone (uptime checks + cron job monitors), I'll be able to start building a proper terraform provider, and audit logs.</p><p><a href="https://onlineornot.com/" rel="nofollow">https://onlineornot.com/</a>, since early 2021.</p></div></td></tr></tbody></table></td></tr><tr id="45418903"><td></td></tr><tr id="45419156"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45419156" href="https://news.ycombinator.com/vote?id=45419156&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>ok this is really cool. do you do procedural animations as well or it's still animated library of moves you blend?</p></div></td></tr></tbody></table></td></tr><tr id="45419343"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45419343" href="https://news.ycombinator.com/vote?id=45419343&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>No procedural animations yet, but soon we want to get there. We also want to do procedural VFX. There is a lot of meat in there!</p></div></td></tr></tbody></table></td></tr><tr id="45419376"><td></td></tr><tr id="45419400"><td></td></tr><tr id="45420399"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420399" href="https://news.ycombinator.com/vote?id=45420399&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm trying to incentivize people to build IRL communities instead of AI-related apps because the demand for human interaction FAR outweighs the supply. My platform (<a href="https://onthe.town/" rel="nofollow">https://onthe.town</a>), is basically Shopify for social experience clubs. Anyone can start a club and create events based around bringing random people together IRL based on shared interests. You get your own website and infra that handles signups, payments, and matching.</p><p>It's largely based on platform-izing the extremely popular Timeleft app that simply matches 6 random people for dinner. With onthe.town, anyone can create a Timeleft-like app around any concept they're interested in. Some clubs people have created include a golf club (get matched with 3 other people to play golf with), a  vinyl record sharing club, a lunch club for biotech networking, and a club to meet other parents for dinner.</p></div></td></tr></tbody></table></td></tr><tr id="45420750"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420750" href="https://news.ycombinator.com/vote?id=45420750&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>There was a startup in my region who got popular with the simple idea of having a website/service that manages simple events, like talks, presentations etc.</p><p>I think it started with mostly students using it because there used to be a lot of university-related events like these, and eventually they’ve become the standard platform for that, at least in the State. It was all pretty simple, it managed payment etc. and you’d get a QR code by email or in the app that could be scanned in the entrance.</p></div></td></tr></tbody></table></td></tr><tr id="45420181"><td></td></tr><tr id="45419584"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419584" href="https://news.ycombinator.com/vote?id=45419584&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Enhancing the restorative function of sleep, without altering sleep time.</p><p><a href="https://affectablesleep.com/" rel="nofollow">https://affectablesleep.com</a></p><p>Our patent-pending neurostimulation builds on over a decade of research in slow-wave enhancement, and more than 50+ published peer-reviewed papers.</p><p>Today we're building our last 3D printed unit. In October we start our first tooling run.</p></div></td></tr></tbody></table></td></tr><tr id="45420380"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420380" href="https://news.ycombinator.com/vote?id=45420380&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Emilia, a personal relationship manager. Every once in a while I meet extended family (wives of cousins or their children) or I meet a fellow soccer parent and I forget their names, or who's related to who.</p><p>I've used Monica HQ to keep track of this but thought I could tackle differently using AI. With AI you could ask questions like "who's everybody on my aunt's side? Like cousins and their family" and get a good answer.</p><p>Afaik other "relationship managers" out there are professionally oriented, for sales people. A lot of them talk about LinkedIn integration, for example.</p><p>Take a look at <a href="http://emilia-workers-website.inerte.workers.dev/" rel="nofollow">http://emilia-workers-website.inerte.workers.dev/</a> and if you're interested in Alpha testing, send me an email at inerte@gmail.com - I setup a Discord last week so early adopters can chat with me about.</p></div></td></tr></tbody></table></td></tr><tr id="45419289"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419289" href="https://news.ycombinator.com/vote?id=45419289&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Going solo on</p><p><a href="https://meldsecurity.com/" rel="nofollow">https://meldsecurity.com/</a></p><p>I'm putting a bunch of security tools / data feeds together as a service. The goal is to help teams and individuals run scans/analysis/security project management for "freemium" (certain number of scans/projects for free each month, haven't locked in on how it'll pan out fully $$ wise).</p><p>I want to help lower the technical hurdles to running and maintaining security tools for teams and individuals. There are a ton of great open source tools out there, most people either don't know or don't have the time to do a technical deep dive into each. So I'm adding utilities and tools by the day to the platform.</p><p>Likewise, there's a built in expert platform for you to get help on your security problems built into the system. (Currently an expert team consisting of [me]). Longer term, I'm working on some AI plugins to help alert on CVEs custom to you, generate automated scans, and some other fun stuff.</p><p><a href="https://meldsecurity.com/ycombinator" rel="nofollow">https://meldsecurity.com/ycombinator</a> (if you're interested in free credits)</p></div></td></tr></tbody></table></td></tr><tr id="45418799"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418799" href="https://news.ycombinator.com/vote?id=45418799&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Mostly organizing my dotfiles across Windows, macOS, Linux and BSD, however, I have really fallen for Ansible. I discovered at work awhile back, but was able to grok how to make and run a playbook, and I've been hooked since. It also finally allowed me to click the difference between Imperative and Declarative programming!</p></div></td></tr></tbody></table></td></tr><tr id="45419178"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45419178" href="https://news.ycombinator.com/vote?id=45419178&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Careful, not all ansible is declarative or idempotent. Lots of foot guns exist, still a valuable tool</p></div></td></tr></tbody></table></td></tr><tr id="45420121"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420121" href="https://news.ycombinator.com/vote?id=45420121&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I've been working on raytraced lighting in the Bevy game engine, using wgpu's new support for hardware raytracing in WGSL. The initial prototype is launching with the release of Bevy 0.17 tomorrow, but there's still a ton left to improve. Lots of experimenting with shaders and different optimizations.</p><p>I wrote a blog post about my initial findings recently: <a href="https://jms55.github.io/posts/2025-09-20-solari-bevy-0-17" rel="nofollow">https://jms55.github.io/posts/2025-09-20-solari-bevy-0-17</a></p></div></td></tr></tbody></table></td></tr><tr id="45419585"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419585" href="https://news.ycombinator.com/vote?id=45419585&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I might be taking a contracted job to help provide AI/ML guidance for a friend's company here soon, but all I really do is use ChatGPT/Claude Code a lot and don't really have explicit AI/ML tool building experience. They know this and mostly just want me for competency and comfort going from 0-1 with a new project, but I'm still pretty nervous! So I'm trying to conjure up some simple ideas to inspire me to learn :)</p><p>Currently trying to predict student absenteeism in the future based on historical indicators with synthetic data using basic ML modeling and then using LLMs to generate helpful guidance for relevant parties. Basically letting parents know there's concern and citing leading indicators.</p><p>Not sure what I'll do next, but hoping to come up with a few other ideas to put my mind at ease. It's fun having some actual motivation to keep up with the current hype instead of just being a consumer, though!</p></div></td></tr></tbody></table></td></tr><tr id="45419167"><td></td></tr><tr id="45419578"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419578" href="https://news.ycombinator.com/vote?id=45419578&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm finally organising 20 years of voice notes. Some were quite outdated - I probably no longer need the mozzarella cheese I reminded myself to buy in early 2008.</p><p>To organize them, I'm writing a Python Qt application with Claude Code. It started off as vibe coding, but I'm now developing it using processes very similar to those I would use when managing software teams. I've picked up a lot of good tips about that here on HN. I've got Whisper, and fallback online services, transcribing the audio and summarizing it and adding tags. After much UI experimentation, I've landed on something that looks not unlike an email client, with tags in the left pane, a center pane which lists transcriptions and notes about each audio file, and a right pane with more detailed information about the selected audio file.</p><p>Next step is to serve it all as a model context protocol server - I need to pick an agent.</p></div></td></tr></tbody></table></td></tr><tr id="45419616"><td></td></tr><tr id="45420817"><td></td></tr><tr id="45420620"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420620" href="https://news.ycombinator.com/vote?id=45420620&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Improving my 'Video game generator from photos'. 
The bottleneck of this kind of generator is 'how much time to obtain the video game".
I managed on my last vacation (it's a side project) to reduced it to 2 hours. 
 This is an example of one FPS made by my tool : 
<a href="https://free-visit.net/fr/demo01" rel="nofollow">https://free-visit.net/fr/demo01</a></p></div></td></tr></tbody></table></td></tr><tr id="45419138"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419138" href="https://news.ycombinator.com/vote?id=45419138&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I have made a Bürgeramt appointment finder. It was down for a few weeks after the city of Berlin changed its anti-bot measures. I just released an updated version that works again: <a href="https://allaboutberlin.com/tools/appointment-finder" rel="nofollow">https://allaboutberlin.com/tools/appointment-finder</a></p><p>My citizenship wait times page (<a href="https://allaboutberlin.com/guides/citizenship-wait-times" rel="nofollow">https://allaboutberlin.com/guides/citizenship-wait-times</a>) has also gotten enough feedback to be useful since its release last month. I'd like to make it more useful with better visualisations.</p><p>Now I'm working on another iteration of my health insurance calculator (<a href="https://allaboutberlin.com/tools/health-insurance-calculator" rel="nofollow">https://allaboutberlin.com/tools/health-insurance-calculator</a>). It's kind of a big deal both because it's a huge financial decision for recent immigrants, and because it funds a big chunk of all the free stuff I'm putting out. This is especially important with ChatGPT and AI summaries halving my traffic. This iteration will recommend health insurance <i>combinations</i> that work for a visa application and for a long-term stay in Germany. It will provide far better explanations.</p><p>At the same time, I'm testing a new insurance broker with far shorter response times, so people can directly ask an expert to help them choose. They're reachable via Whatsapp, and that made a huge difference in how people get advice. It worked so well that I want to do the same for other topics. I'm already talking with an immigration lawyer who's interested.</p></div></td></tr></tbody></table></td></tr><tr id="45420603"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420603" href="https://news.ycombinator.com/vote?id=45420603&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I am working on Sweet Shop.</p><p><a href="https://sweetshop.app/" rel="nofollow">https://sweetshop.app</a></p><p>It's a digital comic book store. Letterboxd with a buy button. It's really fun. We've got a lot of great publishers signed, and a great team. It's such a thrill to work in a space where people work their ass off to create art, in spite of the fact that the rewards are minimal. Our job, we feel, is to make them more money to make more art.</p></div></td></tr></tbody></table></td></tr><tr id="45420394"><td></td></tr><tr id="45420678"><td></td></tr><tr id="45419859"><td></td></tr><tr id="45420174"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420174" href="https://news.ycombinator.com/vote?id=45420174&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I am working on Octelium <a href="https://github.com/octelium/octelium" rel="nofollow">https://github.com/octelium/octelium</a>  a FOSS unified zero trust secure access platform that is flexible enough to operate as a modern zero-config remote access VPN, a Zero Trust Network Access (ZTNA)/BeyondCorp platform, an API/AI/LLM gateway, an infrastructure for MCP gateways and agentic AI architectures/meshes, a PaaS-like platform, ngrok alternative, and even as a homelab infrastructure. It is basically a unified, generic, Kubernetes-like, zero trust architecture (ZTA) for secure access and deployment, that can operate in many human-to-workload, workload-to-workload, and hybrid environments.</p><p>I actually did a SHOW HN exactly 3 months ago and received lots of invaluable critique regarding how dense, overwhelming and unreadable the docs and repo README were. I've actually spent a lot of time trying to improve the quality of the docs and README since then. I'd love to receive any feedback, negative included, regarding the current overall quality of the docs and README from whoever is interest in that space.</p></div></td></tr></tbody></table></td></tr><tr id="45419793"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419793" href="https://news.ycombinator.com/vote?id=45419793&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I left my job to work on my side project (MCP-B: <a href="https://news.ycombinator.com/item?id=44515403">https://news.ycombinator.com/item?id=44515403</a>) full time. I set out with the goal of making the ability to vibecode a webMCP server for your website and inject it via userscript.</p><p>While building that, I basically wrote a modern version of Tampermonkey with its own marketplace built in. So you can vibe code any userscript and publish it to the marketplace all within the extension.</p><p>The automation stuff is still the core value-prop, but this is a fun bonus feature while I work on solidifying the automation features.</p><p>I'm writing a HN post for it. Excited to show everyone in a couple weeks here.</p></div></td></tr></tbody></table></td></tr><tr id="45420418"><td></td></tr><tr id="45420506"><td></td></tr><tr id="45419292"><td></td></tr><tr id="45420244"><td></td></tr><tr id="45420115"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420115" href="https://news.ycombinator.com/vote?id=45420115&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Browser automation with a Chrome Extension.</p><p>Cordyceps: A port of Playwight that doesn't use CDP or Chrome DevTools Protocol either over websockets or chrome.debugger. Instead it uses pure DOM and Chrome Extension APIs. It includes a port of both Stagehand and Browser Use that run purely inside the Chrome Extension. [0]</p><p>Doomberg Terminal: A Chrome Extension that performs algorithmic trading using Robinhood's web interface and market data. [1]</p><p>crx-mcp-over-cdp: This is a proof of concept demonstrating how to run a Model Context Protocol (MCP) server inside a Chrome Extension using Chrome DevTools Protocol (CDP) - no external server required. (Sort of, I left out the actual MCP library implementation. Ran out of time.) [2]</p><p>[0] <a href="https://github.com/adam-s/cordyceps" rel="nofollow">https://github.com/adam-s/cordyceps</a></p><p>[1] <a href="https://github.com/adam-s/doomberg-terminal" rel="nofollow">https://github.com/adam-s/doomberg-terminal</a></p><p>[2] <a href="https://github.com/adam-s/crx-mcp-over-cdp" rel="nofollow">https://github.com/adam-s/crx-mcp-over-cdp</a></p></div></td></tr></tbody></table></td></tr><tr id="45419945"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419945" href="https://news.ycombinator.com/vote?id=45419945&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>A new type of development environment for working with agents</p><p><a href="https://github.com/stravu/crystal" rel="nofollow">https://github.com/stravu/crystal</a></p><p>It supports Claude Code and Codex, but has you constantly working on multiple features in Git worktrees. This way you are always able to stay busy while waiting on your agents.</p><p>It has built in tools for review, such as a diff viewer, and a quick button to run your application in different worktrees for testing. It has completely transformed the way I work.</p></div></td></tr></tbody></table></td></tr><tr id="45419242"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419242" href="https://news.ycombinator.com/vote?id=45419242&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I started a newsletter that tries to recreate the original magic of stumble upon. To feature cool random stuff from across the internet.</p><p>I believe the old internet is still alive and well. It's just buried under a mountain of shit.</p><p><a href="https://randomdailyurls.com/" rel="nofollow">https://randomdailyurls.com/</a></p></div></td></tr></tbody></table></td></tr><tr id="45419525"><td></td></tr><tr id="45419598"><td></td></tr><tr id="45419891"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419891" href="https://news.ycombinator.com/vote?id=45419891&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Building an app where 1 pushup = 1 minute of scrolling allowed [1]. We've fiiinally started to grow and reached a whooping $30k in the last month!!</p><p>I was literally thinking about quitting in August. My motivation is now at an all-time high - some users have done &gt;8k pushups :)</p><p>As always, the key has been the marketing (10M views on Instagram). But we have to improve the product to make people love it even more. So the roadmap is more full than ever.</p><p>[1] <a href="https://pushscroll.com/" rel="nofollow">https://pushscroll.com</a></p></div></td></tr></tbody></table></td></tr><tr id="45420138"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420138" href="https://news.ycombinator.com/vote?id=45420138&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm building Mighty, a library that lets you render Astro components everywhere.</p><p><a href="https://go-mighty.vercel.app/" rel="nofollow">https://go-mighty.vercel.app/</a></p><p>(Will probably register a proper domain name close to release)</p><p>Historically, Astro hasn't had an API like renderToString for React/Vue/etc. that takes a component and renders it on the server. That changed with the release of the Container API last year: <a href="https://docs.astro.build/en/reference/container-reference/" rel="nofollow">https://docs.astro.build/en/reference/container-reference/</a></p><p>But there are still a lot of rough edges:</p><p>- Importing components is a hassle (you have to go dig through the Astro manifest or create a TS file that exports all your components)</p><p>- No Vite integration (so no local dev support, or hot reload)</p><p>- No styling support (this is probably the biggest one)</p><p>Mighty will provide dev + styling support and a simple way to import your Astro components, with adapters for Hono and Laravel when first releasing. For Hono, it should be as simple as writing a few lines of code:</p><p><a href="https://go-mighty.vercel.app/guides/backend-adapters/hono/#render-astro-components-from-hono-code" rel="nofollow">https://go-mighty.vercel.app/guides/backend-adapters/hono/#r...</a></p><p>Still WIP, but I hope to have something out by the end of the year! Let me know what you think.</p><p><a href="https://go-mighty.vercel.app/" rel="nofollow">https://go-mighty.vercel.app/</a></p><p>(And yes, I wrote the docs before the code! It helps me structure my API design far better, even if not perfectly)</p></div></td></tr></tbody></table></td></tr><tr id="45418857"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418857" href="https://news.ycombinator.com/vote?id=45418857&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Working on a chess / poker hybrid.</p><p>There was "choker" back in the day, which I actually never heard about since I wasn't into chess back then. But (1) there was no web version, and (2) it had a specific gameplay that seems too slow for my taste. My version is highly customizable on the setup/rounds/rules, too. From my research, the original was also overrun by bots.</p></div></td></tr></tbody></table></td></tr><tr id="45419065"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45419065" href="https://news.ycombinator.com/vote?id=45419065&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Looking up choker online I found this reddit thread:</p><p>&gt; It’s a cool concept, but terrible app design and it’s all just bots you connect with, making it terribly easy to win almost every game</p><p>It sounds like this game needs a better AI opponent then? I don’t know anything about this game but something that learned from your gameplay and figured out how to beat you would be very cool.</p></div></td></tr></tbody></table></td></tr><tr id="45419636"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45419636" href="https://news.ycombinator.com/vote?id=45419636&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I already have a better chess engine at different skill levels for 1-player mode. For two human players, I plan to start with sending a link to a friend given there won't be enough random players on the website to find one in real-time.</p></div></td></tr></tbody></table></td></tr><tr id="45418734"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418734" href="https://news.ycombinator.com/vote?id=45418734&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm still working on Danger World (<a href="https://danger.world/" rel="nofollow">https://danger.world</a>), my casual 2D narrative adventure with turn-based RPG elements. Built in Flame, on top of Flutter for iOS, Android, Windows and MacOS.</p><p>We're getting close! It's just a matter of polishing and polishing and polishing, but I'm really excited about how close we are to launch.</p></div></td></tr></tbody></table></td></tr><tr id="45419766"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419766" href="https://news.ycombinator.com/vote?id=45419766&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p><a href="https://spanara.app/" rel="nofollow">https://spanara.app</a></p><p>Spanara - A word game inspired by the "license plate game" my wife taught me while we lived in Finland. License plates in Finland always start with 3 letters, so out on our walks we'd try to come up with a word quickly, and got more kudos for "good" words. This was a first attempt at a personal project using AI.</p><p>I am currently working on a new mode that is more like what played walking around: a few rounds in rapid fire, very little time to think before the next round.</p></div></td></tr></tbody></table></td></tr><tr id="45420454"><td></td></tr><tr id="45420756"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420756" href="https://news.ycombinator.com/vote?id=45420756&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Trying to use Claude Code to crank out 100 free AI-less tools &amp; apps. Hoping this will give me some decent passive income when I finally retire.</p></div></td></tr></tbody></table></td></tr><tr id="45418988"><td></td></tr><tr id="45420391"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420391" href="https://news.ycombinator.com/vote?id=45420391&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Building <a href="https://multi.dev/" rel="nofollow">https://multi.dev</a>, an AI coding agent with bunch of FOSS contributors</p><p>We took a great amount of learning from tools like Cline, Roo.. After spending some time on their tech as active users/devs, we decided to build multi from scratch with drastically different take on core features, tech stack, ux/devex..</p><p>If you are an active user of similar tools, and/or want to try multi.. We want to hear from you.</p><p>--
edit: I am one of the core contributors to multi. And we are in the process of open sourcing it.</p></div></td></tr></tbody></table></td></tr><tr id="45419041"><td></td></tr><tr id="45419373"><td></td></tr><tr id="45420360"><td></td></tr><tr id="45419383"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419383" href="https://news.ycombinator.com/vote?id=45419383&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm working on a Heroku / Render / Flyio alternative thats free, open source, built on top of Kubernetes for about 2 years now.</p><p>I’ve found these services charge <i>way</i> too much per GB of memory (10x more than IaaS providers), but more importantly, offer terrible flexibility. You can’t schedule multiple apps on the same instance, and there aren’t many instance size options.</p><p>Canine also supports deployments of any helm package (postgres, airbyte, dagster, etc) via helm charts.</p><p><a href="https://github.com/czhu12/canine" rel="nofollow">https://github.com/czhu12/canine</a> 
<a href="https://canine.sh/" rel="nofollow">https://canine.sh</a></p></div></td></tr></tbody></table></td></tr><tr id="45419953"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419953" href="https://news.ycombinator.com/vote?id=45419953&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Earth Meta Insights</p><p>Since a few months back I am working on a side project to give a snapshot of the regional and global species and natural ecosystems.</p><p>I use manual (me) and automated tools (web and literature search tools, llms, visualizers ...) to search, extract, organize and visualize ecosystem literature and data.</p><p>A regional example of mountain gorilla's of Rwanda:
<a href="https://www.earthmetainsights.com/emi-cards-gorillas-of-rwanda" rel="nofollow">https://www.earthmetainsights.com/emi-cards-gorillas-of-rwan...</a></p><p>A global example of Elephants across the world:
<a href="https://www.earthmetainsights.com/emi-cards-state-of-elephants" rel="nofollow">https://www.earthmetainsights.com/emi-cards-state-of-elephan...</a></p><p>If there are some species that are you would like to see a snapshot of, and the region/location let me know and i will try to get a similar visualization. 
DM or as reply to the chat.
Share the species name (common or scientific) and location (can be a city, town, region, province, country).</p><p>It is a work 8n progress, but I would be very happy to recieve feedback.</p></div></td></tr></tbody></table></td></tr><tr id="45420628"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420628" href="https://news.ycombinator.com/vote?id=45420628&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I appreciate what you're doing here. I think it's really important to have this kind of high level overview of these species. I have a little feedback based on clicking around the site.</p><p>When you click on a country in the map view(under Elephants, for example), I think the map still has focus instead of the card. So this means you can't highlight text, click on links, etc within the card. Also if you scroll using the scroll wheel, you end up zooming in and out on the map.</p><p>I wonder if it would be good to have a "see more" link or some such here, so you can view the same information in the card, but on its own discrete page for each country?</p></div></td></tr></tbody></table></td></tr><tr id="45420679"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45420679" href="https://news.ycombinator.com/vote?id=45420679&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Really appreciate that you checked out the website. It is a bit hacky, but for now i am happy with it. Indeed that is correct, the focus is on the map. I am going to fix that. Thank you.</p><p>As for the see more, it is in my planning. I can do it manually, but  I am waiting for some free time to automated that.</p></div></td></tr></tbody></table></td></tr><tr id="45420191"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420191" href="https://news.ycombinator.com/vote?id=45420191&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm working on a notes app that is as simple as Apple notes, but has native markdown support and uses semantic search.</p><p>Uses SwiftUI for the UI, and Zig does most of the heavy lifting on the backend. It's inspired by ghostty which uses a similar setup[1].</p><p>Right now it only works for Mac, but I'll be porting to iOS as soon as I get the markdown renderer polished. It's not available to the public yet, but I'm using it as my daily driver and hope to release it later this year. I've open sourced it so you can see the source code here[2].</p><p>1. <a href="https://ghostty.org/" rel="nofollow">https://ghostty.org/</a></p><p>2. <a href="https://github.com/emmettmcdow/nana" rel="nofollow">https://github.com/emmettmcdow/nana</a></p></div></td></tr></tbody></table></td></tr><tr id="45419393"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419393" href="https://news.ycombinator.com/vote?id=45419393&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Very much a hobby, but I'm working on a Pinterest alternative built on ATProto called Scrapboard[1]</p><p>The Bluesky ecosystem is a really great platform to build social media on and with Pinterest being overtaken by AI content I figured I'd give it a shot. There is definitely not as much content there, but it is of much higher quality and the culture of providing alt text on images really makes search work rather well.</p><p>1 - <a href="https://scrapboard.org/" rel="nofollow">https://scrapboard.org</a></p><p>2 - <a href="https://bsky.app/profile/scrapboard.org" rel="nofollow">https://bsky.app/profile/scrapboard.org</a></p></div></td></tr></tbody></table></td></tr><tr id="45420300"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420300" href="https://news.ycombinator.com/vote?id=45420300&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I’m still working on Simple Observability:</p><p><a href="https://simpleobservability.com/" rel="nofollow">https://simpleobservability.com</a></p><p>I built it because I needed two things:</p><p>- A super easy-to-install monitoring tool that doesn’t require bash scripts or config files</p><p>- A mobile-friendly, UX-first interface where I can check everything from my phone</p><p>It’s now pretty feature complete. I can see a full picture of all the servers and VPS I run straight from my phone.</p><p>Setup is one command, no config files, and everything else happens in the UI. There’s a catalog of predefined alert rules, and creating new ones is easier than anything else I’ve used.</p><p>There’s a free tier if anyone wants to try it!</p></div></td></tr></tbody></table></td></tr><tr id="45420286"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420286" href="https://news.ycombinator.com/vote?id=45420286&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Banker (banker.so): An AI spreadsheet that excels at spreadsheet understanding (pun intended).</p><p>There are some AI spreadsheet products out there mostly as plugins along with MS Copilot. However my experience with them showed that they are bad at understanding spreadsheets.</p><p>The reason is that sheets are 2D data models. Because LLMs are trained on 1D data models (simply text), translation of 2D data models to formats an LLM can consume is a big context engineering task.</p><p>I read and implemented some of the algos mentioned in SpreadsheetLLM paper released by Microsoft. Ironic, isn't it?</p><p>Got it to a nice working state. Give it a go - if you need more tokens, let me know!</p></div></td></tr></tbody></table></td></tr><tr id="45420110"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420110" href="https://news.ycombinator.com/vote?id=45420110&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Just an old hobbyist these days. I'm finishing up the written manual portion of a "breadboard helper" for playing with (learning) electronics. The current "helper" I am finishing up gives you instructions (and an explanation) for wiring up over a dozen transistor logic circuits with the aid of a small PCB + breadboard [1].</p><p>Inspired by Forrest Mims III, Don Lancaster and the "75 in 1" style electronic project kits my mom got for me for Christmas when I was a kid.</p><p>I hope to sell them and then probably never recoup my investment.</p><p>[1] <a href="https://imgur.com/8pkHiSm" rel="nofollow">https://imgur.com/8pkHiSm</a></p><p>(I'll leave it as an exercise for someone clever to figure out what circuit is being depicted in the photo.)</p></div></td></tr></tbody></table></td></tr><tr id="45420330"><td></td></tr><tr id="45420788"><td></td></tr><tr id="45420183"><td></td></tr><tr id="45419946"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419946" href="https://news.ycombinator.com/vote?id=45419946&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I’ve been working on <a href="https://fontofweb.com/" rel="nofollow">https://fontofweb.com</a>, a search engine for real-world web design.</p><p>Most design inspiration sites lean heavily on curated mockups (Dribbble) or award-winning showcases (Awwwards, Mobbin). That makes them polished, but they don’t reflect what most production sites actually look like. Font of Web takes a different approach: it sources directly from live websites, and the community can clip specific elements instead of entire pages. That means you can browse navbars, pricing cards, dashboards, etc., not just full screenshots.</p><p>Each clip is enriched with metadata (fonts, color palettes, original domain). Search works across that metadata, natural language queries (“minimalist fintech dashboard”), and even visual similarity — so you can find results either by text or by image.</p><p>There’s also a Chrome extension to snip and save from any site.</p><p>I’d like to hear from designers and frontend engineers: is this useful in your workflow? Anything obviously missing?</p></div></td></tr></tbody></table></td></tr><tr id="45418941"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418941" href="https://news.ycombinator.com/vote?id=45418941&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I am working on my Go UI library called gooey [1] which aims to be a one stop framework to build webview/webview apps in Go and WebASM.</p><p>It started out with bindings for the DOM, Web, and Browser APIs, but as of today I now have custom Web Components support (which is a big deal considering Go's type system quirks).</p><p>Tomorrow I'm gonna polish some of the UI components and start refactoring my git-evac [2] repo management tool which is the first app using the gooey framework.</p><p>[1] <a href="https://github.com/cookiengineer/gooey" rel="nofollow">https://github.com/cookiengineer/gooey</a></p><p>[2] <a href="https://github.com/cookiengineer/git-evac" rel="nofollow">https://github.com/cookiengineer/git-evac</a></p></div></td></tr></tbody></table></td></tr><tr id="45419377"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45419377" href="https://news.ycombinator.com/vote?id=45419377&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>&gt; Components are bad for web accessibility (aria- property fatigue).</p><p>I've been using web components as a vehicle to automate and auto validate accessibility aspects as much as possible, because I think the only way to truly make things sustainably accessible is to find a way to unburden the developer by either inferring as much as possible or making validation a natural part of development rather than a separate testing cycle that will invariably cause accessibility support to become out of sync.</p><p>It sounds like you might have similar concerns. Do you have any insights to share along these lines for Gooey?</p></div></td></tr></tbody></table></td></tr><tr id="45419199"><td></td></tr><tr id="45419328"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45419328" href="https://news.ycombinator.com/vote?id=45419328&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>The bindings should also work with tinygo's compiler if you're careful with deadlocks (see docs/ERRATA.md).</p><p>Haven't tested the typecasting that's required for the components yet though, they might break because of some generics quirks (e.g. Wrap/Unwrap helper methods).</p></div></td></tr></tbody></table></td></tr><tr id="45418957"><td></td></tr><tr id="45420253"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420253" href="https://news.ycombinator.com/vote?id=45420253&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Codexes Factory: algorithmic tools to create, operate, distribute, and market entire publishing imprints. This week I am launching my first imprint, Xynapse Traces, with 66 books in the Korean pilsa (筆寫) style.  Later in October, Nimble Ultra, devoted to the history and practice of intelligence and espionage.  Last week I built a giant collection of 575 imprints that are a shadow superset of the ~540 imprints operated by the Big Five publishing houses (Penguin Random House, the largest has ~300).  Teeny weeny tip of the iceberg at NimbleBooks.com.</p></div></td></tr></tbody></table></td></tr><tr id="45419095"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419095" href="https://news.ycombinator.com/vote?id=45419095&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Often, when I use generative AI to produce videos, the results are close to what I envision but rarely capture my imagination exactly. Prompting the AI to fix specific details can be a cumbersome and time-consuming process. To address this, I'm developing solutions that make the creative workflow more intuitive. So far, I’ve built an app that allows users to provide visual clues as guides, along with a 3D environment where the camera can be freely manipulated within the generated scene.</p><p>The community is moving fast though. Now higgsfield allows using arrows and pointers to edit the video but so far, no one is doing a good camera control visually.</p></div></td></tr></tbody></table></td></tr><tr id="45420492"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420492" href="https://news.ycombinator.com/vote?id=45420492&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Building Bloomberry - an alternative to Builtwith. While the latter focuses on frontend tech, I cover almost every SaaS product category. Want to know companies that use Microsoft Dynamics or Zoom? You can with Bloomberry, but not with Builtwith.</p><p><a href="https://bloomberry.com/" rel="nofollow">https://bloomberry.com</a></p></div></td></tr></tbody></table></td></tr><tr id="45418961"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418961" href="https://news.ycombinator.com/vote?id=45418961&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I am working on PicPickr.</p><p>It is a desktop app built with Electron and React. I built to help newlywed couples to quickly sort thousands of wedding photos with a Tinder style swipe UI. It is offline first, fully private, and offers one click export of your selected pictures.</p><p>I started building it earlier this year after going through my own wedding photo experience and realizing how overwhelming it can be. I saw my wife dragging and dropping photos from one folder to other and thought there has to be a better way for non-photographer folks.</p><p>Right now, I have a working prototype, a landing page live, and I am testing distribution and feedback from early users.</p><p><a href="https://picpickr.com/" rel="nofollow">https://picpickr.com</a></p></div></td></tr></tbody></table></td></tr><tr id="45419318"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419318" href="https://news.ycombinator.com/vote?id=45419318&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Most of our jobs consist of working with tools. Yet it’s very hard to get insights into which tools are required most, are growing in your area, etc. So I decided to keep track of tools and technologies mentioned in the data space by keeping track of job openings for the last two years. Now I’ve opened up that data set. Here’s an analysis for jobs per data warehouse: <a href="https://selectfrom.work/insights/data_warehouses" rel="nofollow">https://selectfrom.work/insights/data_warehouses</a></p></div></td></tr></tbody></table></td></tr><tr id="45420913"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420913" href="https://news.ycombinator.com/vote?id=45420913&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm writing a programming language for feature-flags/remote-config. I figure a simple DSL has to be an improvement over YAML or a series of forms in a web app.</p><p>I'm also generally disappointed by the lack of testing that's performed on feature-flag definitions. So I'd like to have a test runner capable of asserting your feature flag's rules matches your intent.</p></div></td></tr></tbody></table></td></tr><tr id="45419066"><td></td></tr><tr id="45420563"><td></td></tr><tr id="45419056"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419056" href="https://news.ycombinator.com/vote?id=45419056&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Nothing extraordinary like yall.</p><p>I've been down a prime numbers rabbit hole. Trying to see the largest prime I can generate in a browser.</p></div></td></tr></tbody></table></td></tr><tr id="45419524"><td></td></tr><tr id="45420367"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420367" href="https://news.ycombinator.com/vote?id=45420367&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I am a bit of a checklist nerd, so I wrote a web app do to checklists: <a href="https://checkoff.ai/" rel="nofollow">https://checkoff.ai</a></p><p>As it is fashionable these days, it can create checklists with AI ("Fun things to do in Pittsburg"), you can create checklists from templates (some stuff you do every day), etc.</p><p>I also have an MCP server that allows you to plug it into your favorite LLM.</p></div></td></tr></tbody></table></td></tr><tr id="45420462"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420462" href="https://news.ycombinator.com/vote?id=45420462&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I am creating a webapp to let screenwriters collaborate when writing their scripts.</p><p>I have several friends in this industry and their tooling is either expensive, not localized for their market or straight away bad (I've seen terrible dataloss).</p><p>I got some inspiration from linear and am building it on top of ruby on rails with CRDTs.</p></div></td></tr></tbody></table></td></tr><tr id="45420507"><td></td></tr><tr id="45420594"><td></td></tr><tr id="45419334"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419334" href="https://news.ycombinator.com/vote?id=45419334&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm working on a partition-oriented declarative data build system. The inspiration comes from working with systems like Airflow and AWS step functions, where data orchestration is described explicitly, and the dependency relationships between input and produced data partitions is complex. Put simply, writing orchestration code for this case sucks - the goal of the project is to enable whole data platforms to be made up of jobs that declare their input and output partition deps, so that they can be automatically fulfilled, enabling kubernetes-like continuous reconciliation of desired partitions.</p><p>This means, instead of the answer to "how do we produce this output data" being "trigger and pray everything upstream is still working", we can answer with "the system was asked to produce this output data partition and its dependencies were automatically built for it". My hope is that this allows the interface with the system to instead be continuously telling it what partitions we want to exist, and letting it figure out the rest, instead of the byzantine DAGs that get built in airflow/etc.</p><p>This comes out of a big feeling that even more recent orchestrators like Prefect, Dagster, etc are still solving the wrong problem, and not internalizing the right complexity.</p></div></td></tr></tbody></table></td></tr><tr id="45420086"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420086" href="https://news.ycombinator.com/vote?id=45420086&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Very much agree that to this is the direction data orchestration platforms should go towards - the basic DAG creation can be straightforward, depending on how you do the authoring - (parsing SQL is always the wrong answer, but is tempting) - but backfills, code updates, etc are when it starts to get spicy.</p></div></td></tr></tbody></table></td></tr><tr id="45420403"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420403" href="https://news.ycombinator.com/vote?id=45420403&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I just shipped 3pio, a drop-in test runner that context-optimizes your test output. It uses your existing test runner and tests so zero changes to your codebase or tooling to use it.</p><p>IME it results in much less context clutter from your test output.</p><p><a href="https://github.com/zk/3pio" rel="nofollow">https://github.com/zk/3pio</a></p></div></td></tr></tbody></table></td></tr><tr id="45419096"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419096" href="https://news.ycombinator.com/vote?id=45419096&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>In my free time I’m still working on My Financé (I keep getting feedback this name is confusing), which is a fairly undifferentiated personal finance tool.</p><p>It’s a labor of love, but I love it!</p><p>I’m currently building a simulation engine that lets you forecast your spending, build scenarios (like taking a year off, getting a cat, move to a new city, etc based on your current spending patterns and assets.</p><p><a href="https://myfinancereport.com/" rel="nofollow">https://myfinancereport.com/</a></p><p>It’s great fun to have a project of one’s own to just toil away on.</p></div></td></tr></tbody></table></td></tr><tr id="45419129"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45419129" href="https://news.ycombinator.com/vote?id=45419129&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I don't know what it is about this name, but I read it as "My Fiancé". My brain did not register the first "n" and it wasn't until I read your parenthetical remark that I went back and re-read.</p><p>The name isn't confusing, per se ("get married to/be exclusive with your finances", OK), but it also isn't very strong... "financé" is also very strange and awkward to pronounce as a native English speaker. Probably because it comes across more as Spanish-seeming despite it being a play on a French work.</p></div></td></tr></tbody></table></td></tr><tr id="45419474"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45419474" href="https://news.ycombinator.com/vote?id=45419474&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Yeah it was meant to be along the lines of:</p><p>My Financé, because you should love your finances.</p><p>To your point, I think it’s hard to notice the spelling, and hard to figure out how to pronounce it.</p><p>It also is the same spelling as My Finance, which is tricky to rank for on Google.</p><p>Overall, it seems like it has potential to be a fun brand, but the constant confusion has led me to strongly consider a “rebrand”.</p></div></td></tr></tbody></table></td></tr><tr id="45419232"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45419232" href="https://news.ycombinator.com/vote?id=45419232&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>&gt; I don't know what it is about this name, but I read it as...</p><p>same misreading</p><p>I'm blaming typoglycemia</p></div></td></tr></tbody></table></td></tr><tr id="45419399"><td></td></tr><tr id="45420270"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420270" href="https://news.ycombinator.com/vote?id=45420270&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm vibe coding using GitHub Copilot and JetBrains AI Pro on a Blazor web app that tracks my investment in like index funds, stocks, ETFs, etc. It's a simple CRUD web app.</p><p>The app is nearly completed, and Grok (preview in Copilot, currently free) wrote most of the CRUD pages with Entity Framework. Of course, it does get things wrong, and I use Claude 4 to fix the issues. (i'm a C# dev, I review code generated by Grok sometimes.)</p></div></td></tr></tbody></table></td></tr><tr id="45419173"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419173" href="https://news.ycombinator.com/vote?id=45419173&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Adding a chat feature to my iOS app size analysis tool that runs locally on your Mac. My goal is to make everyone a build engineer, where you can chat with your builds and get insights and improvement areas. Testing out on-device Apple Intelligence models but need to find the time to do more validation testing.</p><p><a href="https://apps.apple.com/us/app/dotipa/id6742254881">https://apps.apple.com/us/app/dotipa/id6742254881</a></p></div></td></tr></tbody></table></td></tr><tr id="45420268"><td></td></tr><tr id="45419125"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419125" href="https://news.ycombinator.com/vote?id=45419125&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Working on Fraim, open-source agents for cloudsec and appsec engineers to complement existing deterministic scanners. Born out of our 3 years of learnings building such scanners for IaC. Turns out in the real world policies are subjective enough to make this hard.</p><p>Examples:</p><p>- Policies are frequently subjective. Hard to codify, but LLMs can evaluate them more like a security engineer would. "IAM policies should use least privilege." What is "least" enough? "Admin ports shouldn't be exposed to the Internet." What's an admin port?</p><p>- Security engineers are stretched thin. LLMs can watch PRs for potentially risky changes that need closer human review. "PR loosens authz/authn." "PR changes network perimeter configuration."</p><p>- Traditional check runs (SAST, IaC, etc.) flood PRs with findings. Security doesn't have time to review them all. Devs tends to ignore them. Frequent false positives. LLMs can draw attention to the important ones. "If the findings are unusual for this repo, require the author to acknowledge the risk before merging."</p><p><a href="https://github.com/fraim-dev/fraim" rel="nofollow">https://github.com/fraim-dev/fraim</a></p><p><a href="https://www.fraim.dev/" rel="nofollow">https://www.fraim.dev</a></p></div></td></tr></tbody></table></td></tr><tr id="45419290"><td></td></tr><tr id="45420718"><td></td></tr><tr id="45419567"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419567" href="https://news.ycombinator.com/vote?id=45419567&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>In my free time I'm starting a new Low Power FM community radio station for the east San Fernando Valley.</p><p>www.kpbj.fm</p></div></td></tr></tbody></table></td></tr><tr id="45419708"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419708" href="https://news.ycombinator.com/vote?id=45419708&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Currently bootstrapping a SaaS side project:
<a href="https://diplomium.com/" rel="nofollow">https://diplomium.com</a></p><p>Diplomium helps educators and event organizers create and deliver authenticated certificates at scale. Instead of manually designing and emailing PDFs, you upload a simple Excel, pick a template, and the system generates + sends personalized certificates automatically—each with a unique QR code for instant validation.</p><p>The bigger picture: Certificates are often the only tangible outcome of a learning experience. By making them verifiable, permanent, and easy to distribute, organizations save admin time while learners get a trustworthy credential.</p><p>Status: Running for 2 years, used by schools and training centers in Latin America. Now building AI-powered features for design editing and data extraction from PDFs.</p></div></td></tr></tbody></table></td></tr><tr id="45420421"><td></td></tr><tr id="45418939"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418939" href="https://news.ycombinator.com/vote?id=45418939&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Working on real-time log visualization platform with wallboard/tv support, initially inspired by Logstalgia:</p><p><a href="https://tailstream.io/" rel="nofollow">https://tailstream.io</a></p><p>Launched the initial version a couple of weeks ago and making good progress, trying to share as much of the process as I can on X.</p><p>Backend API can be used by any client, but I also built an open source agent in Go that makes setup really easy.</p><p>Currently working on a proper log viewer, alerts and visualization improvements.</p></div></td></tr></tbody></table></td></tr><tr id="45419040"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45419040" href="https://news.ycombinator.com/vote?id=45419040&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Please show this to AWS. CloudWatch is such a pain, arcade visuals is what I want, if I have to look at logs.</p></div></td></tr></tbody></table></td></tr><tr id="45420456"><td></td></tr><tr id="45419920"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419920" href="https://news.ycombinator.com/vote?id=45419920&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>i've been incrementally hiking the via francigena (<a href="https://www.viefrancigene.org/en/walking/" rel="nofollow">https://www.viefrancigene.org/en/walking/</a>) and am working through integrating my gpx, geotagged photos, and oura ring data to both illustrate my journey and analyze how different terrains and altitudes affected the collected biometrics.</p><p>ingesting/parsing gpx layers into duckdb using python to extract tags and load api data. using minio right now but ultimately want to push to cloudflare free tools or vercel.</p></div></td></tr></tbody></table></td></tr><tr id="45419088"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419088" href="https://news.ycombinator.com/vote?id=45419088&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I've been making and selling my electronic social battery pin badges for a while now (<a href="https://hortus.dev/products/social-battery" rel="nofollow">https://hortus.dev/products/social-battery</a>) and I'm expanding the range with seasonal versions like a Christmas mood badge, and a halloween themed ghost badge that's coming soon. I'm lucky enough that these projects have gone down well and are making enough money to fund some more complicated (and expensive) projects that I wouldn't have otherwise had the guts to try. Currently I'm working on an RGB digital sand timer with customizable timing sequences so that you can use it for things like the pomodoro technique - I have a working prototype and at the moment I'm experimenting with interfaces for setting the sequences. I wanted to use a combination of buttons and an accelerometer for this but it's not as intuitive as I'd like so I may end up making a small smartphone app to configure it.</p></div></td></tr></tbody></table></td></tr><tr id="45419253"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419253" href="https://news.ycombinator.com/vote?id=45419253&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Two side projects, as if 3 kids are not enough!</p><p>- a booking platform for surfing schools
- a tool for pelvic physiotherapy practitioners handle appointments and exercise prescriptions</p><p>Doing backend and frontend for both, but there is a small team helping with #2. Both come from actual needs of actual businesses.</p><p>Tech is pretty standard typescript, react and node.</p><p>Would love to be working on these full time.</p></div></td></tr></tbody></table></td></tr><tr id="45419646"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419646" href="https://news.ycombinator.com/vote?id=45419646&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>An editor for creating custom accessible color palettes for web/UI design. :)</p><p><a href="https://www.inclusivecolors.com/" rel="nofollow">https://www.inclusivecolors.com/</a></p><p>It gives you precise control over every shade/tint (no AI or auto generation!) so you can incorporate your own brand colors, and helps you build palettes that have simple to follow color contrast guarantees by design e.g. all grade 600 colors have 4.5:1 WCAG contrast (for body text) against all grade 50 colors, such as red-600 vs gray-50, or green-600 vs gray-50. There's export options for plain CSS, Tailwind, Figma, and Adobe.</p><p>I'm really open to feedback on what problems and needs people have for creating accessible designs!</p></div></td></tr></tbody></table></td></tr><tr id="45419356"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419356" href="https://news.ycombinator.com/vote?id=45419356&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Full-time indie dev breathing life into next-gen terminals [0]. This is my lifelong career ambition.</p><p>If you can't afford early access, please email me and I'll grant you a free copy: I need all the feedback I can get!</p><p>[0] <a href="https://terminal.click/" rel="nofollow">https://terminal.click</a></p></div></td></tr></tbody></table></td></tr><tr id="45420271"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420271" href="https://news.ycombinator.com/vote?id=45420271&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>As someone who's curious (I see lots of room for improvement in terminals!), I can't tell what this does from the website, other than the ability to load and view 3d models.</p></div></td></tr></tbody></table></td></tr><tr id="45420882"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45420882" href="https://news.ycombinator.com/vote?id=45420882&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Ah right, that’s just the “wow factor” hook. If you scroll down a smidge you’ll see a 3-minute video trailer.</p><p>The Media page also has a 15-minute demo comparing Terminal Click against suckless.</p><p>Of course I just need to do a better job telling you what we’re all about without the need to watch videos.</p></div></td></tr></tbody></table></td></tr><tr id="45419411"><td></td></tr><tr id="45419684"><td></td></tr><tr id="45420143"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420143" href="https://news.ycombinator.com/vote?id=45420143&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>LLM-Jail is a Simple Docker Container to Contain Your LLM CLI</p><p><a href="https://github.com/codazoda/llm-jail" rel="nofollow">https://github.com/codazoda/llm-jail</a></p><p>I don’t know if this is really necessary, but I created it after doing an in-house CTF challenge, with no LLM rules, and I was giving several LLM CLI’s a lot of leeway and iterating very quickly.</p></div></td></tr></tbody></table></td></tr><tr id="45419069"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419069" href="https://news.ycombinator.com/vote?id=45419069&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>A burnout detector for SREs. The goal is to help teams identify incident responders who may be overworked/getting burned out.</p><p>We are looking at:</p><p>-Objective data: signals from incident management tools (Rootly/PagerDuty), GitHub, and Slack</p><p>-Self-reported data: asking the engineers how they feel via short survey</p><p>From this, we generate a CBI score (Copenhagen Burnout Inventory). We're still in beta, but we've received positive feedback from our beta testers, especially from manager of large and distributed orgs.</p><p>It's fully open-source, you can test it out locally
<a href="https://github.com/Rootly-AI-Labs/rootly-burnout-detector-web" rel="nofollow">https://github.com/Rootly-AI-Labs/rootly-burnout-detector-we...</a></p><p>Alternatively, we offer a hosted version with mock data, allowing you to play with it.
<a href="https://www.oncallburnout.com/" rel="nofollow">https://www.oncallburnout.com/</a></p><p>If you have any feedback or ideas, shoot them my way :)</p></div></td></tr></tbody></table></td></tr><tr id="45419450"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45419450" href="https://news.ycombinator.com/vote?id=45419450&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Very good idea. I could have used this multiple times in my career. I am a go until I drop type of person, and I'd just keep going.</p></div></td></tr></tbody></table></td></tr><tr id="45420277"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420277" href="https://news.ycombinator.com/vote?id=45420277&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>markdown and image cms in the browser. run and store documents totally offline and static. requires no server. not a pwa or electron</p><p><a href="https://opaledx.com/" rel="nofollow">https://opaledx.com</a></p><p>- rich markdown editor (via mdxeditor.dev) and source (codemirror6)</p><p>- uses indexeddb and optionally opfs (select a directory on your local hd)</p><p>- some service worker hacks to do seamless image processing (jpg/png -&gt; webp), storage and retrieval</p><p>- document snapshot history, thumbnail preview with iframe and snapdom: html-&gt;img sorcery</p><p>- live previews and compilations</p><p>- loads very quickly, navigation and cold starts, images make heavy use of the Cache api</p><p>- use in-browser git (thanks isomorphic-git) for version control; optionally sync with github via cors proxy (host your own if you want)</p><p>- best of all completely free to use. 99.5% finished MIT github repo dropping soon ;)</p></div></td></tr></tbody></table></td></tr><tr id="45419477"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419477" href="https://news.ycombinator.com/vote?id=45419477&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm working on a site for filmmakers to help showcase themselves!</p><p>Why? &gt;</p><p>LinkedIn isn't for creatives. 
Actor's Access is dated and charges a ton for basic extras
Squarespace/wix is fine but everyone in 'the biz' has one and nobody wants to maintain it. Plus they're all silo'd.</p><p>Check out my site if you wanna. You get to host your own headshots, resume, and reels. You can upload your screenplay there and hear it read outloud. You can put up your cinematic scores and make a place to send people to hear your music.</p><p><a href="https://cinesignal.com/" rel="nofollow">https://cinesignal.com</a></p><p>Looking for users who wanna test the system out. Give me a shout and I'll throw you some credits if you wanna hear your screenplay read outloud.</p></div></td></tr></tbody></table></td></tr><tr id="45419158"><td></td></tr><tr id="45418856"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418856" href="https://news.ycombinator.com/vote?id=45418856&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>A command-line tool called berk that is a versatile job dispatcher written in c. It is meant to replace big clunky tools like Jenkins, Ansible etc. It has syntax similar to git. It works pretty well, just need to iron out some kinks before final release. <a href="https://github.com/jezze/berk" rel="nofollow">https://github.com/jezze/berk</a></p></div></td></tr></tbody></table></td></tr><tr id="45419030"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419030" href="https://news.ycombinator.com/vote?id=45419030&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Testing jig for a traction control system for a locomotive. Microcontroller connected to a DDS waveform generator simulates the sensor that picks wheel speed, various ADCs and DACs read in analog voltages that are compared to determine loss of traction. 1980s analog computing at its finest. If I had a choice I would be doing anything else ;)</p></div></td></tr></tbody></table></td></tr><tr id="45419562"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419562" href="https://news.ycombinator.com/vote?id=45419562&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm building with python/fastapi, react/tailwind/vite, with Claude Code and using test-driven development.</p><p>Red-green-refactor is tedious for humans but perfect for AI. And the test names &amp; code make great documentation of every micro decision, running in milliseconds to prevent regressions.</p><p>The software itself helps people perform construction approvals.</p><p>Old way: dozens of documents and versions sent back and forth over email. <i>Many</i> fiddly details that must be checked - to streamline the process we'll use AI to provide verdicts that help humans make decisions.</p><p>I plan to create content &amp; teach what I've learned.</p><p><a href="https://approviq.com/" rel="nofollow">https://approviq.com</a></p></div></td></tr></tbody></table></td></tr><tr id="45419516"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419516" href="https://news.ycombinator.com/vote?id=45419516&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I am all-in on a Unity game right now. Working with one other person and hoping to ship to Steam later this year.</p><p>Thinking about play testing at scale is a new thing for me. I've been getting into visualization techniques like using 3d textures to build voxel heat maps in-editor. We've managed to accumulate quite a bit of play testing telemetry already. The power of aggregated statistics in the editor views is absolutely mind-blowing to me. For level designers it's like having proper omniscience. Being able to see things like thousands of samples (manifesting as a bright red voxel) that wound up tripping over the same misplaced geometry is like cheating.</p></div></td></tr></tbody></table></td></tr><tr id="45418806"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418806" href="https://news.ycombinator.com/vote?id=45418806&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Working on the Restful Atmos Sleep Lamp, a smart bedside lamp that automatically shifts throughout the day and night for the circadian rhythm, reducing blue light at night and maximizing blue light during the day. There is a machine learning layer that learns your preferences and automatically adjusts the intensity of the light, similarly to the Nest Thermostat [0].</p><p>Also, shipping Bedtime Bulb v2 next month. This is a hybrid LED-incandescent design meant for the evening that is the best of both worlds: low blue light, high color quality, perfect compatibility with dimmers, 10x less flicker than incandescent, includes near infrared, low energy use, long lifespan [1].</p><p>[0]: <a href="https://restfullighting.com/products/restful-atmos-preorder" rel="nofollow">https://restfullighting.com/products/restful-atmos-preorder</a></p><p>[1]: <a href="https://restfullighting.com/products/bedtime-bulb-v2-preorder" rel="nofollow">https://restfullighting.com/products/bedtime-bulb-v2-preorde...</a></p></div></td></tr></tbody></table></td></tr><tr id="45420333"><td></td></tr><tr id="45418976"><td></td></tr><tr id="45419761"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419761" href="https://news.ycombinator.com/vote?id=45419761&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Everyone’s drowning in long articles, dense PDFs, and hour-long videos. I’m working on <a href="https://unrav.io/" rel="nofollow">https://unrav.io</a> , it lets you flip any article, paper, or YouTube link into the format you actually want (summary, mindmap, podcast, infographic, etc.) in one click.</p><p>Right now I’m experimenting with a simple bookmarklet trigger instead of a browser extension. Curious: how do HN folks feel about bookmarklets in 2025, still viable, or do you prefer extensions?</p></div></td></tr></tbody></table></td></tr><tr id="45419861"><td></td></tr><tr id="45418837"><td></td></tr><tr id="45419887"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419887" href="https://news.ycombinator.com/vote?id=45419887&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm working on Zettelgarden: <a href="https://zettelgarden.com/" rel="nofollow">https://zettelgarden.com</a>.</p><p>It's a personal knowledge system. It's a zettelkasten with an LLM substrate. It uses LLMs to build a model of the theses, arguments and facts used in cards, and uses these to both summarize the information on the card and to automatically link cards together based on shared concepts.</p></div></td></tr></tbody></table></td></tr><tr id="45418965"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418965" href="https://news.ycombinator.com/vote?id=45418965&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm working on a WordPress PaaS with dedicated lanes for bots. The status quo around WordPress is that you block bots using Cloudflare, else your site crashes. Since AI search is here to stay, we need a way to let bots crawl WordPress sites without crashing the server.</p><p>Currently at MVP stage, no domain yet.</p></div></td></tr></tbody></table></td></tr><tr id="45418843"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418843" href="https://news.ycombinator.com/vote?id=45418843&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm trying to get my agentic software specification tool Arbiter to release (UI polish/debugging is so slow :/, browser shenanigans are harder than Rust fr).  It's basically a tool that AI agents can use to construct a project specification. The twist to Arbiter is that the specs are structured and validated, and you can compile them to get:</p><p>Services with stubbed endpoints,
UIs with placeholder components,
Dockerfiles/Terraform/K8s infra,
E2E tests (via declared flows),
Github/Gitlab epics/issues/subissues</p><p>It's also got github/gitlab webhook integration, so you can do stuff like trigger agents reactively when events occur on a repo, it includes cloudflare tunnel support so you can set up webhooks even in a local dev environment, and the project generator is fully customizable.</p></div></td></tr></tbody></table></td></tr><tr id="45419133"><td></td></tr><tr id="45419455"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45419455" href="https://news.ycombinator.com/vote?id=45419455&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Nope, it's a structured spec agents construct using a CLI or MCP (you can also interact with the spec using a web UI). It's CUE, and validated against a schema.  Instead of taking your conversation and generating a markdown document that agents might (but often don't) respect, the agent populates the spec in the service from your conversation, then when you're done you can use the CLI to automatically generate a bunch of code.</p></div></td></tr></tbody></table></td></tr><tr id="45419252"><td></td></tr><tr id="45420785"><td></td></tr><tr id="45419818"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419818" href="https://news.ycombinator.com/vote?id=45419818&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm working on a super-simple budgeting app called <a href="https://4keynumbers.com/" rel="nofollow">https://4keynumbers.com</a>, which is based on Ramit Sethi's Conscious Spending Plan. It currently syncs my expenses from Plaid and cooks it down into a single chart, with only savings, investments, bills/fixed, and "safe to spend" as categories.</p></div></td></tr></tbody></table></td></tr><tr id="45420103"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420103" href="https://news.ycombinator.com/vote?id=45420103&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I've been working on writing two appendix sections on knowledge distillation and reinforcement learning for <i>Machine Learning for Drug Discovery</i> [1], which were initiated as tangents to expand coverage of material from a few earlier chapters. I hope to also write these appendix sections up as freely available articles (at least in a condensed form). Thankfully, I'll be able to finish the knowledge distillation section this month but, unfortunately, I need to pivot to finishing out chapter 11 to stay on schedule for full publication.</p><p>[1] <a href="https://www.manning.com/books/machine-learning-for-drug-discovery" rel="nofollow">https://www.manning.com/books/machine-learning-for-drug-disc...</a></p></div></td></tr></tbody></table></td></tr><tr id="45418913"><td></td></tr><tr id="45418832"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418832" href="https://news.ycombinator.com/vote?id=45418832&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>trying to build a webapp where i apply some recommender systems knowledge to TCG deckbuilding. MtG in particular is suffering from product fatigue and as someone who is both an MLE and a casual MtG player, it has been a fun challenge to apply my skills to a domain of interest</p></div></td></tr></tbody></table></td></tr><tr id="45419283"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419283" href="https://news.ycombinator.com/vote?id=45419283&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I’m building a daily word puzzle game with a twist!</p><p>In Tiled Words you rearrange tiles to solve clues and rebuild a broken crossword.</p><p>You can play a demo at <a href="https://tiledwords.com/" rel="nofollow">https://tiledwords.com</a> - it’s free and web based so it works on whatever device you’ve got.</p><p>I’ll be officially launching on October 19th at the Portland Retro Gaming Expo. You can sign up to be notified on launch. Starting then there will be a new puzzle every day!</p><p>So far I’ve gotten really positive feedback and have around 100 people signed up to get notified. It’s been a lot of fun to build!</p></div></td></tr></tbody></table></td></tr><tr id="45419644"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45419644" href="https://news.ycombinator.com/vote?id=45419644&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>The UX is phenomenal. Easily in the top 1% of daily word puzzles. Love the concept, I'm sure it will do well at your launch!!</p></div></td></tr></tbody></table></td></tr><tr id="45420836"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45420836" href="https://news.ycombinator.com/vote?id=45420836&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>This is such a nice comment, and I only checked out the website because of your comment! I am glad I did. Indeed a spectacular puzzle.</p></div></td></tr></tbody></table></td></tr><tr id="45419803"><td></td></tr><tr id="45419910"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45419910" href="https://news.ycombinator.com/vote?id=45419910&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Thank you! Happy to feature your game, I've played a _lot_ putting together Playlin so I have a pretty good sense of what's out there.</p></div></td></tr></tbody></table></td></tr><tr id="45419663"><td></td></tr><tr id="45420536"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420536" href="https://news.ycombinator.com/vote?id=45420536&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Cool idea.  One suggestion is to allow a selection box to be dragged around a block of letters.  Once selected, all of the tiles in the area could be dragged at once.</p><p>That would reduce the frustration of having to move a large chunk of words around piece by piece.  It would be better than the existing affordance, which moves the whole grid.</p></div></td></tr></tbody></table></td></tr><tr id="45420635"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45420635" href="https://news.ycombinator.com/vote?id=45420635&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Thanks! I experimented with this but struggled making it feel natural on mobile.</p><p>I ended up implementing an alternate solution that I’m hoping solves for the same paint point.</p><p>My current dev build “merges” tiles when you connect them to complete a word. This allows you to move them as</p><p>I’m going to release a demo with that feature soon. My core playtesters seem to like it but I want to get more feedback on it from a larger group!</p></div></td></tr></tbody></table></td></tr><tr id="45420194"><td></td></tr><tr id="45418917"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418917" href="https://news.ycombinator.com/vote?id=45418917&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I’m working on Colanode, which is built to close the gap between the convenience of cloud tools and the ownership of local software. It brings chat, docs, databases, and files into one open-source, self-hostable workspace where data lives on your devices first and syncs in the background. Unlike typical SaaS tools, Colanode is local-first: everything works instantly and offline, infrastructure stays minimal, and you keep full control of your data.</p><p>Website: <a href="https://colanode.com/" rel="nofollow">https://colanode.com</a>
Repo: <a href="https://github.com/colanode/colanode" rel="nofollow">https://github.com/colanode/colanode</a></p></div></td></tr></tbody></table></td></tr><tr id="45418979"><td></td></tr><tr id="45419751"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419751" href="https://news.ycombinator.com/vote?id=45419751&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I’m at a crossroads with my Speed Cubing Competitions listing app (SCComps.com). It’s an iOS app built in Flutter, has around 250 downloads, and currently generates no revenue. I'm spending about $500 a year just to keep it running. There’s little community engagement, and I'm debating whether to double down and rebuild it in Swift—or just shut it down altogether.</p></div></td></tr></tbody></table></td></tr><tr id="45420683"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420683" href="https://news.ycombinator.com/vote?id=45420683&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>A tool for Muslims. Dreamstate: Interpret your dreams Islamically</p><p><a href="https://dreamstateai.replit.app/" rel="nofollow">https://dreamstateai.replit.app/</a></p><p>Traditional Knowledge: Constrained to Ibn Seerin's classical teachings — trusted by Muslims for over 1,000 years
AI-Powered Analysis: Unlock the meaning of your dream with 4,300 dream symbols from the Dictionary of Dreams.</p><p>Share your dream confidentially, answer a few context questions, and receive your authentic Islamic interpretation in under a minute.</p><p>This is an MVP which I started &lt;4 weeks ago. Currently validating Desirability, Feasibility, and Viability.</p></div></td></tr></tbody></table></td></tr><tr id="45420701"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420701" href="https://news.ycombinator.com/vote?id=45420701&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Confidentiality is very difficult to guarantee. You may want to put some brackets on what your users can realistically expect and give them tips on how they can stay anonymous. But lovely and novel idea, really neat to see these kind of cross-overs.</p></div></td></tr></tbody></table></td></tr><tr id="45420783"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_45420783" href="https://news.ycombinator.com/vote?id=45420783&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>On the app's dream input page, it specifies a bit more "Your dreams are private and not stored or collected." - would that cover it? Thanks for your feedback and encouragement!</p></div></td></tr></tbody></table></td></tr><tr id="45420916"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_45420916" href="https://news.ycombinator.com/vote?id=45420916&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>That is a 'pinky promise', it may well be true and let's assume you are well intentioned but it leaves the door open to you not being trustworthy after all or someone intercepting the data while it is being processed (for instance, by compromising your service).</p><p>In order for you to process the dream data you have to at least make a temporary copy. One way to get rid of that is to move the interpretation part to the client side if possible. Another thing you could do is if people are really concerned about the content of a particular dream to suggest they use TOR or some other anonymization (not perfect, I know) service to at least hide their internet location from you, the operator of the service.</p><p>Does the app itself run entirely within your own infrastructure or does it call out for part of the work?</p></div></td></tr></tbody></table></td></tr><tr id="45419675"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419675" href="https://news.ycombinator.com/vote?id=45419675&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Currently I've been working on <a href="https://terragonlabs.com/" rel="nofollow">https://terragonlabs.com</a> which is a way to orchestrate Claude Code and other agents (Amp, Codex) as background agents.</p><p>I feel like I am locally constantly bouncing between different agents for different tasks and really wanted to be able to do the same in a remote environment.</p></div></td></tr></tbody></table></td></tr><tr id="45419754"><td></td></tr><tr id="45420099"><td></td></tr><tr id="45419307"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419307" href="https://news.ycombinator.com/vote?id=45419307&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm building Kavla, its an infinite multiplayer canvas for data analytics.</p><p>I have a video on how it works on <a href="https://kavla.dev/" rel="nofollow">https://kavla.dev/</a></p><p>And a live demo here: <a href="https://demo.kavla.dev/" rel="nofollow">https://demo.kavla.dev/</a></p><p>I've been working in the data space for five years now. Kavla is something that I personally feel would make my job more fun!</p><p>Built with tldraw and duckdb</p></div></td></tr></tbody></table></td></tr><tr id="45419425"><td></td></tr><tr id="45420050"><td></td></tr><tr id="45420200"><td></td></tr><tr id="45419392"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419392" href="https://news.ycombinator.com/vote?id=45419392&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm working on a text-based softball league simulator where you forcibly enlist your friends and family to join your co-ed softball team. You play as their manager/coach/fellow player.</p><p>Every aspect of the games are narrated in real time so you know what's going on. I'm still in the prototype stage and I've seen some pretty hilarious interactions already.</p></div></td></tr></tbody></table></td></tr><tr id="45419209"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419209" href="https://news.ycombinator.com/vote?id=45419209&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>A script which will find random pictures of anyone in the family from the Immich database, resize them and add metadata on them like where they were taken and when and put them on the TV to show as kind of a screen saver when we're at home.</p><p>I like this Facebook feature which shows you "Today 10 years ago", Immich, does have it in it's UI too and perhaps I will mix in those pictures also to show on TV.</p></div></td></tr></tbody></table></td></tr><tr id="45418788"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418788" href="https://news.ycombinator.com/vote?id=45418788&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Working on a "Data Governance in a Box" solution for small businesses that are using out of data routers and security practices. Starting here in Canada, but open to collaboration.</p></div></td></tr></tbody></table></td></tr><tr id="45418966"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418966" href="https://news.ycombinator.com/vote?id=45418966&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Still working on <a href="https://gridwhale.com/" rel="nofollow">https://gridwhale.com</a>.</p><p>This is mostly a nostalgia play--I'm pining for a time when app development was much easier. I'm trying to apply lessons from early Rapid Application Development while still providing a full-featured language.</p><p>I confess that I haven't gotten any traction at all, but I find it incredibly useful for my own consulting business, so I'm going to keep on working on it.</p></div></td></tr></tbody></table></td></tr><tr id="45420051"><td></td></tr><tr id="45420074"><td></td></tr><tr id="45419245"><td></td></tr><tr id="45418873"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418873" href="https://news.ycombinator.com/vote?id=45418873&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Trying to build a secure, configurable and easy to use authentication system (relative to my understanding)</p><p>I have experienced knowledge gaps and blind spots that I am attempting to fix. For example most users worry about security of hashed passwords and yet they do not realize that the TOTP (eg Google Authenticator) use symmetric encryption and quite a lot of the authentication providers store the private key in plain text in their database. List goes on...</p></div></td></tr></tbody></table></td></tr><tr id="45420179"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420179" href="https://news.ycombinator.com/vote?id=45420179&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I’ve created an AI-powered app designed to help candidates prepare for Meta’s product manager interviews, with a focus on product execution questions. The app allows you to practice by speaking or typing your responses, then uses AI to score answers against a rubric and track your progress over time.</p><p>I’m looking for beta testers—happy to share early access if you’re interested! If you are please message me.</p></div></td></tr></tbody></table></td></tr><tr id="45420165"><td></td></tr><tr id="45419115"><td></td></tr><tr id="45418791"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418791" href="https://news.ycombinator.com/vote?id=45418791&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>An XDP/eBPF load balancer with Golang control plane library and an application to replace high capacity legacy appliances with COTS servers.</p></div></td></tr></tbody></table></td></tr><tr id="45418822"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418822" href="https://news.ycombinator.com/vote?id=45418822&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Deterministic guarantees, and corrective behavioral monitoring for ai agents (starting with claude code, and ADK). Think security + performance bumper rails. At the cost of 0 context.</p><p>I was the feature requestor for Claude Code Hooks - and have been involved in ai governance for quite awhile, this is an idea I'm excited about.</p><p>Ping below if you want to early beta test. everything is open source, no signups.</p></div></td></tr></tbody></table></td></tr><tr id="45418724"><td></td></tr><tr id="45419473"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45419473" href="https://news.ycombinator.com/vote?id=45419473&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Nice work! I've recently been modeling sheds in SketchUp both with and without the Framer extension and it can be really tedious.</p><p>Random question as I don't know a ton of framing... is your sample model missing jack studs on the large door opening?</p></div></td></tr></tbody></table></td></tr><tr id="45418863"><td></td></tr><tr id="45419224"><td></td></tr><tr id="45420834"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420834" href="https://news.ycombinator.com/vote?id=45420834&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I've been wondering for years if historical magazines/periodicals could ever be transformed into a modern ebook format. PDF doesn't cut it, but most other efforts are unsatisfactory... part of what makes a magazine a magazine is the rich, mixed content. So, for the past few weeks (months?) I've been taking a stab at it with the science fiction pulps. Started with Analog/Astounding, and I was able to re-typeset the cover (with original art), most of the interior, many of the ads, and so on.</p><p><a href="https://github.com/NoMoreNicksLeft/repulp" rel="nofollow">https://github.com/NoMoreNicksLeft/repulp</a></p><p>I still need to put together a build system to actually zip this up into an epub file...</p></div></td></tr></tbody></table></td></tr><tr id="45418807"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418807" href="https://news.ycombinator.com/vote?id=45418807&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm working on character.ai for learning Chinese, you chat with characters at your level, and get instant feedback on your writing. It's a way to get a wide amount of comprehensible input in an engaging way that also practices output.</p><p><a href="https://koucai.chat/" rel="nofollow">https://koucai.chat</a></p></div></td></tr></tbody></table></td></tr><tr id="45420879"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420879" href="https://news.ycombinator.com/vote?id=45420879&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>This is really cool, I'm interested in this as I'm also a chinese learner and I thought about doing sometihng kinda similar (just locally)</p><p>I like the UI, really cool project.</p><p>I think the prompting might need more work to make it natural though. I just tried a "hungover chat with 996" worker, and the responses seemed to be lacking a little too much context</p></div></td></tr></tbody></table></td></tr><tr id="45419875"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419875" href="https://news.ycombinator.com/vote?id=45419875&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Building an email-to-calendar-feed service for all the mails from the multitude of services and attachments that I get related to my kindergartener.</p></div></td></tr></tbody></table></td></tr><tr id="45420813"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420813" href="https://news.ycombinator.com/vote?id=45420813&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I've been building an innovative guided munition for Ukraine for the last 4 months, we have first prototypes made and have arrangements with the testing range to begin flying them soon.</p></div></td></tr></tbody></table></td></tr><tr id="45420638"><td></td></tr><tr id="45420851"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420851" href="https://news.ycombinator.com/vote?id=45420851&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>For now, I think it would be funny if you put plaintext that says "just bought the milliondollargpt.com and have no idea what to do with it...". Optionally as a hyperlink to your comment I am replying to.</p><p>Edit: Spelling</p></div></td></tr></tbody></table></td></tr><tr id="45420969"><td></td></tr><tr id="45418882"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418882" href="https://news.ycombinator.com/vote?id=45418882&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Writing a specification for a personal library app in the hopes I can get AppSheet + Gemini to make one for me. I'm working on library science in general, so it will hopefully implement ideas I have about book classification and entity catalogs.</p></div></td></tr></tbody></table></td></tr><tr id="45419610"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419610" href="https://news.ycombinator.com/vote?id=45419610&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I'm working on a few things, but the one that's gaining the most traction right now in terms of users is kyoubenkyou</p><p><a href="https://www.kyoubenkyou.com/" rel="nofollow">https://www.kyoubenkyou.com/</a></p><p>In short, it's a few things:</p><p>- JA-&gt;EN dictionary</p><p>- hiragana / katakana / time reading / number reading quizzers</p><p>- learn kanji with FSRS, anki-style</p><p>- vocab quizzer</p><p>- the coolest feature (imo) is a "reader": upload Japanese texts (light novels, children's books, etc), then translate them to your native language to practice your reading comprehension. Select text anywhere on the page (with your cursor) to instantly do a dictionary lookup. A LLM evaluates your translation accuracy (0..100%) and suggests other possible interpretations.</p><p>It's all elixir+liveview+postgres+pgroonga (though there are times when I would like to have SolidJS).</p><p>I've been considering open-sourcing it due to lack of commercial success, but might try an ad-based approach first.</p></div></td></tr></tbody></table></td></tr><tr id="45419469"><td></td></tr><tr id="45419380"><td></td></tr><tr id="45420488"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420488" href="https://news.ycombinator.com/vote?id=45420488&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Idea looks interesting if done well. I believe you should, on your landing page, show an example :
- mini photo of a bill
 - what it generated.</p><p>So people get it right away</p></div></td></tr></tbody></table></td></tr><tr id="45418769"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45418769" href="https://news.ycombinator.com/vote?id=45418769&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I got a dumb phone. Been messing around with setting a phone number to call to get SMS directions and things of that sort.
Then I wanted to build my own phone so I got a LTE module and been messing around with that.</p></div></td></tr></tbody></table></td></tr><tr id="45420889"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45420889" href="https://news.ycombinator.com/vote?id=45420889&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>About the first part, I am working on something similar for myself. If you want an api to get SMS for free, without needing any 10dlc stuff, check out groupme, which supports SMS.</p></div></td></tr></tbody></table></td></tr><tr id="45419696"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419696" href="https://news.ycombinator.com/vote?id=45419696&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I develop simulators for digital twins and games. Currently working on a simulator for LLMs to use as world models.</p></div></td></tr></tbody></table></td></tr><tr id="45419913"><td></td></tr><tr id="45418949"><td></td></tr><tr id="45419186"><td></td></tr><tr id="45420542"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420542" href="https://news.ycombinator.com/vote?id=45420542&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>I have been prototyping a local-only social media manager initially targeting the game development community. I am sick of all the subscription only platforms such as buffer, hootsuite etc.</p><p>Initially I have been looking at Mastodon and Bluesky since they have sane APIs.</p><p>The plan is to make it so that you can sync your data folder either manually (e.g. dropbox, or sneakernet if you want) or a via a basic cheap data plan.</p></div></td></tr></tbody></table></td></tr><tr id="45420482"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420482" href="https://news.ycombinator.com/vote?id=45420482&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>An extension which treats tabs as a stack - so I can go down a rabbit hole opening new tabs and then use a shortcut to close a tab and take me to the parent of that tab</p></div></td></tr></tbody></table></td></tr><tr id="45420874"><td></td></tr><tr id="45419027"><td></td></tr><tr id="45418820"><td></td></tr><tr id="45418852"><td></td></tr><tr id="45419217"><td></td></tr><tr id="45419240"><td></td></tr><tr id="45419782"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_45419782" href="https://news.ycombinator.com/vote?id=45419782&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>That's pretty cool. It's nice to see this exists.</p><p>For me, I'll probably send an email later to support to ask (no rush, since it's out of stock anyway), but I was checking for info on compatibility with Yamaha (e.g. my Cross Connect) ebikes. It's not on the compatibility list. They make their own (mid-drive) motors (PW-SE on mine I think) and proprietary batteries. They pulled out of the United States market altogether so getting more batteries from them again is doubtful. (Mine currently charges to ~85% and then throws an error code, but it still works for now.) It is a Yamaha 500Wh36V battery pack on the down tube with 3 wires (I just unscrewed where the battery plugs in to see).</p></div></td></tr></tbody></table></td></tr><tr id="45418970"><td></td></tr><tr id="45418825"><td></td></tr><tr id="45420494"><td></td></tr><tr id="45420354"><td></td></tr><tr id="45419075"><td></td></tr><tr id="45420160"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420160" href="https://news.ycombinator.com/vote?id=45420160&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>A kanji learning app using free dictionary data and the FSRS spaced repetition system for maximum context per card and optimal memory retention.</p><p><a href="https://shodoku.app/" rel="nofollow">https://shodoku.app/</a></p><p><a href="https://github.com/runarberg/shodoku" rel="nofollow">https://github.com/runarberg/shodoku</a></p><p>My theory of learning is that you learn the characters better if you learn how to read and write them at the same time. And flash cards are better by giving you as much information as possible about the character.</p><p>This is fundamentally different from e.g. WaniKani which only teaches you how to read the character and relies on pre-made mnemonics (plus SRS) for easier retention, and from Anki which (normally) has very minimal flash cards, showing only small bits of information per card. When you have the whole dictionary on each card it gives you the opportunity to create the easiest connection with what you already know. This may be some made up story about the components (radicals) in the kanji (like WaniKani does) a word you already know, other kanji sharing the components, etc. The more connections you make the easier it is to learn them.</p><p>One of the features I personally use extensively is the ability to bookmark words containing the kanji, which will then pop up at the top of the words section in a later review. If I remember the meaning and the reading of the words I have bookmarked for this character during a reading review, I consider mark card as <i>good</i>. If I remember none of them I mark it “again”.</p></div></td></tr></tbody></table></td></tr><tr id="45420150"><td></td></tr><tr id="45419390"><td></td></tr><tr id="45419032"><td></td></tr><tr id="45419607"><td></td></tr><tr id="45419458"><td></td></tr><tr id="45419452"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419452" href="https://news.ycombinator.com/vote?id=45419452&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Still focused on metal 3D printer slicer software with Blender geometry nodes, and a microscopic positioning stage design for hobbyists.</p><p>On the weekend built a lattice-filter test jig with the LiteVNA64, so sorting though the pile of crystals is less time intensive.</p><p>Other hobbies maybe 3 other people would find amusing.  =3</p></div></td></tr></tbody></table></td></tr><tr id="45418766"><td></td></tr><tr id="45419863"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45419863" href="https://news.ycombinator.com/vote?id=45419863&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>For the past ten months I've been working on a way to transmit and receive around 10 kilobytes halfway across town. I've blown through government grants totaling in the hundreds of millions of dollars but it seems this is an unsolvable problem.</p></div></td></tr></tbody></table></td></tr><tr id="45420513"><td></td></tr><tr id="45420207"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420207" href="https://news.ycombinator.com/vote?id=45420207&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Yet another online cycling calculator, this time with an emphasis on power/speed difference between different tires.</p><p>I'm sick and tired of audiophile level bs floating around online forums and I want to create a simple tool for people to fiddle around with different settings to see what really impacts their speed while cycling.</p><p>As usual - no plans for monetization whatsoever. Nothing fancy either, just an elaborated weekend project.</p><p>If you like the idea and want to help with graphic design and or html just let me know. :)</p></div></td></tr></tbody></table></td></tr><tr id="45420031"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_45420031" href="https://news.ycombinator.com/vote?id=45420031&amp;how=up&amp;goto=item%3Fid%3D45418675"></a></center></td><td><br>
<div><p>Yet another browser-based screen/video recorder and editor but with multiple inputs, full privacy and scriptable effects - a slow weekend project</p></div></td></tr></tbody></table></td></tr></tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[California governor signs AI transparency bill into law (270 pts)]]></title>
            <link>https://www.gov.ca.gov/2025/09/29/governor-newsom-signs-sb-53-advancing-californias-world-leading-artificial-intelligence-industry/</link>
            <guid>45418428</guid>
            <pubDate>Mon, 29 Sep 2025 20:33:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gov.ca.gov/2025/09/29/governor-newsom-signs-sb-53-advancing-californias-world-leading-artificial-intelligence-industry/">https://www.gov.ca.gov/2025/09/29/governor-newsom-signs-sb-53-advancing-californias-world-leading-artificial-intelligence-industry/</a>, See on <a href="https://news.ycombinator.com/item?id=45418428">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
				
				
				
				<p><strong>What you need to know:</strong> Governor Newsom today signed legislation further establishing California as a world leader in safe, secure, and trustworthy artificial intelligence, creating a new law that helps the state both boost innovation and protect public safety.</p>
			</div><div>
				
				
				
				
				<p><strong>SACRAMENTO — </strong>Governor Newsom today signed into law Senate Bill 53, the Transparency in Frontier Artificial Intelligence Act (TFAIA), authored by Senator Scott Wiener (D-San Francisco) – legislation carefully designed to enhance online safety by installing commonsense guardrails on the development of frontier artificial intelligence models, helping build public trust while also continuing to spur innovation in these new technologies. The new law builds on recommendations from <a href="https://www.gov.ca.gov/2025/03/18/california-strengthens-its-position-as-the-global-ai-leader-with-new-working-report-issued-by-experts-and-academics/" data-cke-saved-href="https://www.gov.ca.gov/2025/03/18/california-strengthens-its-position-as-the-global-ai-leader-with-new-working-report-issued-by-experts-and-academics/">California’s first-in-the-nation report</a>, called for by Governor Newsom and published earlier this year — and helps advance California’s position as a <a href="https://www.gov.ca.gov/2025/09/18/californication-of-ai-golden-state-is-1-in-ai-and-the-birthplace-of-modern-tech-so-yeah-be-quiet-ted-cruz/" data-cke-saved-href="https://www.gov.ca.gov/2025/09/18/californication-of-ai-golden-state-is-1-in-ai-and-the-birthplace-of-modern-tech-so-yeah-be-quiet-ted-cruz/">national leader in responsible and ethical AI</a>, the world’s fourth-largest economy, the birthplace of new technology, and the top pipeline for tech talent.</p>
			</div><div><h4>“California has proven that we can establish regulations to protect our communities while also ensuring that the growing AI industry continues to thrive. This legislation strikes that balance. AI is the new frontier in innovation, and California is not only here for it – but stands strong as a national leader by enacting the first-in-the-nation frontier AI safety legislation that builds public trust as this emerging technology rapidly evolves.”</h4>
<p><strong>Governor Gavin Newsom</strong></p></div><div>
				
				
				
				
				<p>California works closely to foster tech leadership and create an environment where industry and talent thrive. The state is balancing its work to advance AI with commonsense laws to protect the public, embracing the technology to make our lives easier and make government more efficient, effective, and transparent. California’s leadership in the AI industry is helping to guide the world in the responsible implementation and use of this emerging technology.&nbsp;</p>
			</div><div><h4>“With a technology as transformative as AI, we have a responsibility to support that innovation while putting in place commonsense guardrails to understand and reduce risk. With this law, California is stepping up, once again, as a global leader on both technology innovation and safety. I’m grateful to the Governor for his leadership in convening the Joint California AI Policy Working Group, working with us to refine the legislation, and now signing it into law. His Administration’s partnership helped this groundbreaking legislation promote innovation and establish guardrails for trust, fairness, and accountability in the most remarkable new technology in many years.”</h4>
<p><strong>Senator Scott Wiener</strong></p></div><div>
				
				
				
				
				<p>Earlier this year, a group of world-leading AI academics and experts — <a href="https://www.gov.ca.gov/2024/09/29/governor-newsom-announces-new-initiatives-to-advance-safe-and-responsible-ai-protect-californians/" data-cke-saved-href="https://www.gov.ca.gov/2024/09/29/governor-newsom-announces-new-initiatives-to-advance-safe-and-responsible-ai-protect-californians/">convened at the request of Governor Newsom</a> — released a <a href="https://www.gov.ca.gov/2025/03/18/california-strengthens-its-position-as-the-global-ai-leader-with-new-working-report-issued-by-experts-and-academics/" data-cke-saved-href="https://www.gov.ca.gov/2025/03/18/california-strengthens-its-position-as-the-global-ai-leader-with-new-working-report-issued-by-experts-and-academics/">first-in-the-nation report </a>on sensible AI guardrails, based on an empirical, science-based analysis of the capabilities and attendant risks of frontier models. The report included recommendations on ensuring evidence-based policymaking, balancing the need for transparency with considerations such as security risks, and determining the appropriate level of regulation in this fast-evolving field. SB 53 is responsive to the recommendations in the report — and will help ensure California’s position as an AI leader. This legislation is particularly important given the failure of the federal government to enact comprehensive, sensible AI policy. SB 53 fills this gap and presents a model for the nation to follow.</p>
			</div><div><h4>“Last year Governor Newsom called upon us to study how California should properly approach frontier artificial intelligence development. The Transparency in Frontier Artificial Intelligence Act (TFAIA) moves us towards the transparency and ‘trust but verify’ policy principles outlined in our report.&nbsp; As artificial intelligence continues its long journey of development, more frontier breakthroughs will occur. AI policy should continue emphasizing thoughtful scientific review and keeping America at the forefront of technology.”</h4>
<p><strong>Mariano-Florentino (Tino) Cuéllar</strong><br>Former California Supreme Court Justice and former member of National Academy of Sciences Committee on the Social and Ethical Implications of Computing Research</p>
<p><strong>Dr. Fei-Fei Li</strong><br>Co-Director, Stanford Institute for Human-Centered Artificial Intelligence</p>
<p><strong>Jennifer Tour Chayes</strong><br>Dean of the College of Computing, Data Science, and Society at UC Berkeley</p></div><div><h2><strong>California’s AI dominance&nbsp;</strong></h2>
<p>California continues to dominate the AI sector. In addition to being the birthplace of AI, the state is home to <a href="https://www.forbes.com/lists/ai50/" data-cke-saved-href="https://www.forbes.com/lists/ai50/">32 of the 50 top AI companies</a> worldwide. California leads U.S. demand for AI talent. In 2024, 15.7% of all U.S. AI job postings were in California — #1 by state, well ahead of Texas (8.8% and New York (5.8%), per the 2025 Stanford AI Index. In 2024, <a href="https://pitchbook.com/news/articles/as-sf-bay-area-eats-up-52-of-all-ai-and-ml-vc-dollars-foreign-vcs-warn-of-waste" data-cke-saved-href="https://pitchbook.com/news/articles/as-sf-bay-area-eats-up-52-of-all-ai-and-ml-vc-dollars-foreign-vcs-warn-of-waste">more than half</a> of global VC funding for AI and machine learning startups went to companies in the Bay Area. California is also home to three of the four companies that have passed the $3 trillion valuation mark. Each of these California-based companies — Google, Apple, and Nvidia — are tech companies involved in AI and have created hundreds of thousands of jobs.</p>
<h2>What the law does:</h2>
<p>SB 53 establishes new requirements for frontier AI developers creating stronger:</p>
<p><strong>✅ Transparency: </strong>Requires large frontier developers to publicly publish a framework on its website describing how the company has incorporated national standards, international standards, and industry-consensus best practices into its frontier AI framework.</p>
<p><strong>✅ Innovation: </strong>Establishes a new consortium within the Government Operations Agency to develop a framework for creating a public computing cluster. The consortium, called CalCompute, will advance the development and deployment of artificial intelligence that is safe, ethical, equitable, and sustainable by fostering research and innovation.</p>
<p><strong>✅ Safety: </strong>Creates a new mechanism for frontier AI companies and the public to report potential critical safety incidents to California’s Office of Emergency Services.</p>
<p><strong>✅ Accountability: </strong>Protects whistleblowers who disclose significant health and safety risks posed by frontier models, and creates a civil penalty for noncompliance, enforceable by the Attorney General’s office.</p>
<p><strong>✅ Responsiveness:</strong> Directs the California Department of Technology to annually recommend appropriate updates to the law based on multistakeholder input, technological developments, and international standards.&nbsp;</p>
<p><a href="https://www.gov.ca.gov/wp-content/uploads/2025/09/SB-53-Signing-Message.pdf" data-cke-saved-href="https://www.gov.ca.gov/wp-content/uploads/2025/09/SB-53-Signing-Message.pdf">A signing message can be found here.</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Remember: Kurt Vonnegut Was 47 (138 pts)]]></title>
            <link>https://www.joanwestenberg.com/p/remember-kurt-vonnegut-was-47</link>
            <guid>45418269</guid>
            <pubDate>Mon, 29 Sep 2025 20:19:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.joanwestenberg.com/p/remember-kurt-vonnegut-was-47">https://www.joanwestenberg.com/p/remember-kurt-vonnegut-was-47</a>, See on <a href="https://news.ycombinator.com/item?id=45418269">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-blocks"><p><span><span>At forty-seven, Kurt Vonnegut published&nbsp;</span></span><span><span><em>Slaughterhouse-Five</em></span></span><span><span>. He had been a struggling writer, a car salesman, a PR man at General Electric, and a failed playwright. He had seen war firsthand, lived through firebombs, raised six children (four of them adopted after his sister's death), and produced a shelf of novels that garnered little attention. Then suddenly, almost accidentally, he became one of the most important American voices of the twentieth century. When people recall Vonnegut now, they picture the wry, cigarette-smoking humanist, the man who wrote about time travel and Dresden and the strange species of Tralfamadorians. But in 1969, when&nbsp;</span></span><span><span><em>Slaughterhouse-Five</em></span></span><span><span>&nbsp;came out, he was not young, not new, and certainly not destined to succeed. He was forty-seven.</span></span></p><p><span><span>Why does this matter? Because we live in a culture obsessed with precocity. We valorize the twenty-two-year-old founder, the thirty-year-old Nobel laureate, the poet who dies before publishing her second book. To be forty-seven in America often feels like you are past your prime, coasting toward irrelevance. And yet Vonnegut’s story punctures this narrative. It raises the uncomfortable, thrilling question: how much can be done late, when everyone thinks the window has closed?</span></span></p><p><span><span>American culture has always been suspicious of age. Fitzgerald made it clear in&nbsp;</span></span><span><span><em>This Side of Paradise</em></span></span><span><span> - the whole point was to capture the fleeting brilliance of youth before it calcified into routine. The Beats chased a similar myth, a reckless vitality that had to burn out quickly. Silicon Valley today has its own catechism: Zuckerberg’s infamous line, “Young people are just smarter.” It’s the same fetish, rebranded.</span></span></p><p><span><span>But history doesn’t quite bear this out. Galileo was in his forties when he published his most radical works. Thomas Paine was forty when&nbsp;</span></span><span><span><em>Common Sense</em></span></span><span><span>&nbsp;reshaped political thought. Susan B. Anthony was fifty-two when she cast her first illegal vote. The assumption that genius peaks young has always been a convenient myth. It flatters the ambitious and terrifies the hesitant. It also blinds us to the fact that many of history’s breakthroughs came from people who had been around long enough to see patterns others missed.</span></span></p><p><span><span>Vonnegut is a particularly vivid example because he had already failed. He had written science fiction for pulp magazines, novels like&nbsp;</span></span><span><span><em>Player Piano</em></span></span><span><span>&nbsp;and&nbsp;</span></span><span><span><em>The Sirens of Titan</em></span></span><span><span>&nbsp;that earned him modest attention but little money. He was not a wunderkind. He was a midlist author, typing away between family obligations and day jobs. By the logic of our culture, he should have given up. Instead, he wrote the book that only someone with his scars, his age, and his accumulated oddities could have produced.</span></span></p><p><span><span>There is dignity in failure, especially prolonged failure. Consider Melville: forgotten after&nbsp;</span></span><span><span><em>Moby-Dick</em></span></span><span><span>, reduced to writing insurance reports, rediscovered decades later. Or Emily Dickinson, who failed in the most invisible way: unread in her lifetime, her poems quietly fermenting in a drawer. Vonnegut’s failures were of a different sort - his books did get published, but with disappointing results. He was a writer who could fill a shelf in a used bookstore, gathering dust beside more fashionable authors. But that experience mattered.&nbsp;</span></span><span><span><em>Slaughterhouse-Five</em></span></span><span><span>&nbsp;is not a young man’s book. Its humor is laced with bitterness, its form is fractured by time, and its philosophy is resigned rather than triumphant. Only someone who had seen things fall apart (repeatedly) could have written it.</span></span></p><p><span><span>And maybe this is why age can produce greatness. Youth thrives on conviction; age is forced into complexity. Vonnegut could not tell a clean story about Dresden. He knew memory was fractured, that trauma distorted time, that irony was the only language left. His narrative jumps back and forth through decades, between planets and battlefields; because that was the only way to be honest.</span></span></p><p><span><span>Middle age is rarely glamorous. Dante placed himself “midway in our life’s journey” in the dark wood, lost and confused. The Greeks had their crises too - Solon supposedly argued that you could not call a man happy until he died, because only the full arc could reveal whether fortune had spared him. At forty-seven, Vonnegut was in that territory. He had no assurance his career would matter. His books were not chart-toppers. He was supporting a sprawling family on uneven income. He had lived enough to know the absurdities of both war and corporate America. Out of that stew came&nbsp;</span></span><span><span><em>Slaughterhouse-Five</em></span></span><span><span>.</span></span></p><p><span><span>This matters; because middle age is often treated as decline, the moment when one’s creativity has been used up. Neuroscience papers are circulated to show how fluid intelligence peaks in your twenties, how mathematicians do their best work before thirty-five. But there is another kind of intelligence: crystallized, layered, associative. The ability to see connections across disciplines, to synthesize long experience into something new. Vonnegut’s novel is precisely that kind of synthesis: war memoir, science fiction, satire, elegy.</span></span></p><p><span><span>Vonnegut’s war had always haunted him. As a young soldier, he was captured in the Battle of the Bulge, held in Dresden, and survived the firebombing by hiding in a slaughterhouse basement. It took him decades to turn this into art.</span></span></p><p><span><span>Some traumas resist immediate rendering.</span></span></p><p><span><span>Primo Levi needed years before&nbsp;</span></span><span><span><em>If This Is a Man</em></span></span><span><span>&nbsp;could be written. Pat Barker’s&nbsp;</span></span><span><span><em>Regeneration</em></span></span><span><span>&nbsp;trilogy required the hindsight of the 1990s to reimagine the First World War. Vonnegut’s forty-seven-year-old self could finally write what his twenty-five-year-old self could only (and barely) endure.</span></span></p><p><span><span><em>Slaughterhouse-Five</em></span></span><span><span> is not cathartic. It does not end with redemption. The famous refrain - “So it goes” - is less about acceptance or closure, than about repetition, about the endless cycle of death. The book offers no comfort, but it does offer recognition.</span></span></p><p><span><span>That recognition is the work of age.</span></span></p><p><span><span>Life, all this living, all this striving is a long apprenticeship that cannot be compressed. Some writers learn style and voice quickly; others take decades. Henry James distinguished between the “young genius” and the “late bloomer,” but suggested both paths were legitimate.</span></span></p><p><span><span>Vonnegut’s early books were uneven, witty but scattered. He had not yet found the tone that made him distinctive. By the time he reached&nbsp;</span></span><span><em><span>Slaughterhouse-Five</span></em></span><span><span>, he had rehearsed irony, satire, science fiction tropes, and autobiographical fragments enough times to finally bring them together. Forty-seven was not late; it was right on time.</span></span></p><p><span><span>Chartres was not built in a decade. The Parthenon was rebuilt multiple times. Sometimes greatness takes patience, not precocity.</span></span></p><p><span><span>Why does our culture cling to the idea that if you haven’t made it by thirty, you won’t? Part of it is economic: industries want to exploit youthful energy at low wages. Part of it is romantic: the myth of the prodigy is more cinematic than the tale of the slow grinder. But part of it may also be anxiety about mortality. To celebrate the late bloomer is to admit that our lives can change radically past middle age, which is destabilizing. If anything can happen at forty-seven, then perhaps we cannot measure ourselves against arbitrary deadlines.</span></span></p><p><span><span>Vonnegut mocked all deadlines. He wrote about time as non-linear, about events existing simultaneously. In&nbsp;</span></span><span><span><em>Slaughterhouse-Five</em></span></span><span><span>, Billy Pilgrim comes “unstuck in time.” That phrase could apply to Vonnegut himself: his career looked like a sequence of failures until it suddenly wasn’t.</span></span></p><p><span><span>We are all unstuck in time.</span></span></p><p><span><span>Our successes and failures do not unfold in neat order.</span></span></p><p><span><span>Sometimes they arrive decades late.</span></span></p><p><span><span><strong>Remember: </strong></span></span><span><span>Kurt Vonnegut was forty-seven when he wrote his masterpiece. And this fact should unsettle us! It should challenge the myth that our best years are always early. It should remind us that the middle of life can be fertile, that failure can ripen into art, that age can distill experience into something no youth could mimic. To be forty-seven is not to be finished. It may, for some, be the very beginning.</span></span></p></div><p><span><span>Where </span></span><span><span><strong>Builders</strong></span></span><span><span> Come to </span></span><span><span><strong>Think</strong></span></span><span><span>.</span></span></p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FCC Accidentally Leaked iPhone Schematics (228 pts)]]></title>
            <link>https://www.engadget.com/big-tech/fcc-accidentally-leaked-iphone-schematics-potentially-giving-rivals-a-peek-at-company-secrets-154551807.html</link>
            <guid>45416231</guid>
            <pubDate>Mon, 29 Sep 2025 17:12:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.engadget.com/big-tech/fcc-accidentally-leaked-iphone-schematics-potentially-giving-rivals-a-peek-at-company-secrets-154551807.html">https://www.engadget.com/big-tech/fcc-accidentally-leaked-iphone-schematics-potentially-giving-rivals-a-peek-at-company-secrets-154551807.html</a>, See on <a href="https://news.ycombinator.com/item?id=45416231">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-article-body="true"><p>The Federal Communications Commission (FCC) <a data-i13n="cpos:1;pos:1" href="https://fccid.io/BCG-E8726A/Schematics/A3212-A3408-A3409-A3410-System-Electrical-Schematics-V1-0-8024509" rel="nofollow noopener" target="_blank" data-ylk="slk:recently published a 163-page PDF;cpos:1;pos:1;elm:context_link;itc:0;sec:content-canvas"><ins>recently published a 163-page PDF</ins></a> showing the electrical schematics for the iPhone 16e, despite Apple specifically requesting them to be confidential. This was most likely a mistake on the part of the FCC, <a data-i13n="cpos:2;pos:1" href="https://appleinsider.com/articles/25/09/29/fcc-mistakenly-leaks-confidential-iphone-16e-schematics?utm_source=feedly" rel="nofollow noopener" target="_blank" data-ylk="slk:according to a report by AppleInsider;cpos:2;pos:1;elm:context_link;itc:0;sec:content-canvas"><ins>according to a report by </ins><em><ins>AppleInsider</ins></em></a>.</p><p>The agency also distributed a cover letter from Apple alongside the schematics, which is dated September 16, 2024. This letter verifies the company's request for privacy, indicating that the documents contain "confidential and proprietary trade secrets." The cover letter asks for the documents to be withheld from public view "indefinitely." Apple even suggested that a release of the files could give competitors an "unfair advantage."</p><p>To that end, the documents feature full schematics of the iPhone 16e. These include block diagrams, electrical schematic diagrams, antenna locations and more. Competitors could simply buy a handset and open it up to get to this information, as the <a data-i13n="cpos:3;pos:1" href="https://www.engadget.com/mobile/smartphones/iphone-16e-review-whats-your-acceptable-compromise-020016288.html" data-ylk="slk:iPhone 16e came out back in February;cpos:3;pos:1;elm:context_link;itc:0;sec:content-canvas"><ins>iPhone 16e came out back in February</ins></a>, but this leak would eliminate any guesswork. However, Apple is an extremely litigious company when it comes to <a data-i13n="cpos:4;pos:1" href="https://www.engadget.com/big-tech/apple-wins-250-in-masimo-smartwatch-patent-case-150020340.html" data-ylk="slk:stuff like patent infringement;cpos:4;pos:1;elm:context_link;itc:0;sec:content-canvas"><ins>stuff like patent infringement</ins></a>.</p><p>The FCC hasn't addressed how this leak happened or what it intends to do about it. <em>AppleInsider's</em> reporting suggested that this probably happened due to an incorrect setting in a database. This was likely not an intentional act against Apple, which tracks given that the company has been especially supportive of the Trump administration. CEO Tim Cook even <a data-i13n="cpos:5;pos:1" href="https://www.engadget.com/mobile/engadget-podcast-apple-bows-to-the-trump-regime-again-113025360.html" data-ylk="slk:brought the president a gold trophy;cpos:5;pos:1;elm:context_link;itc:0;sec:content-canvas"><ins>brought the president a gold trophy</ins></a> for being such a good and important boy.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude Code 2.0 (634 pts)]]></title>
            <link>https://www.npmjs.com/package/@anthropic-ai/claude-code</link>
            <guid>45416228</guid>
            <pubDate>Mon, 29 Sep 2025 17:12:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.npmjs.com/package/@anthropic-ai/claude-code">https://www.npmjs.com/package/@anthropic-ai/claude-code</a>, See on <a href="https://news.ycombinator.com/item?id=45416228">Hacker News</a></p>
Couldn't get https://www.npmjs.com/package/@anthropic-ai/claude-code: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Buy It in ChatGPT: Instant Checkout and the Agentic Commerce Protocol (191 pts)]]></title>
            <link>https://openai.com/index/buy-it-in-chatgpt/</link>
            <guid>45416080</guid>
            <pubDate>Mon, 29 Sep 2025 17:00:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/buy-it-in-chatgpt/">https://openai.com/index/buy-it-in-chatgpt/</a>, See on <a href="https://news.ycombinator.com/item?id=45416080">Hacker News</a></p>
Couldn't get https://openai.com/index/buy-it-in-chatgpt/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Claude Sonnet 4.5 (1229 pts)]]></title>
            <link>https://www.anthropic.com/news/claude-sonnet-4-5</link>
            <guid>45415962</guid>
            <pubDate>Mon, 29 Sep 2025 16:52:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/news/claude-sonnet-4-5">https://www.anthropic.com/news/claude-sonnet-4-5</a>, See on <a href="https://news.ycombinator.com/item?id=45415962">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><p>Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It’s the best model at using computers. And it shows substantial gains in reasoning and math.</p><p>Code is everywhere. It runs every application, spreadsheet, and software tool you use. Being able to use those tools and reason through hard problems is how modern work gets done.</p><p>Claude Sonnet 4.5 makes this possible. We're releasing it along with a set of major upgrades to our products. In <a href="https://anthropic.com/news/enabling-claude-code-to-work-more-autonomously" target="_blank" rel="noopener noreferrer">Claude Code</a>, we've added checkpoints—one of our most requested features—that save your progress and allow you to roll back instantly to a previous state. We've refreshed the terminal interface and shipped a <a href="https://marketplace.visualstudio.com/items?itemName=anthropic.claude-code" target="_blank" rel="noopener noreferrer">native VS Code extension</a>. We've added a new <a href="https://anthropic.com/news/context-management" target="_blank" rel="noopener noreferrer">context editing feature and memory tool</a> to the Claude API that lets agents run even longer and handle even greater complexity. In the Claude <a href="https://claude.ai/redirect/website.v1.f79c597c-8da8-4257-a3ef-0233ea5a1dd0/download" target="_blank" rel="noopener noreferrer">apps</a>, we've brought code execution and <a href="https://www.anthropic.com/news/create-files" target="_blank" rel="noopener noreferrer">file creation</a> (spreadsheets, slides, and documents) directly into the conversation. And we've made the <a href="https://www.anthropic.com/news/claude-for-chrome" target="_blank" rel="noopener noreferrer">Claude for Chrome</a> extension available to Max users who joined the waitlist last month.</p><p>We're also giving developers the building blocks we use ourselves to make Claude Code. We're calling this the <a href="https://anthropic.com/engineering/building-agents-with-the-claude-agent-sdk" target="_blank" rel="noopener noreferrer">Claude Agent SDK</a>. The infrastructure that powers our frontier products—and allows them to reach their full potential—is now yours to build with.</p><p>This is the <a href="https://www.anthropic.com/claude-sonnet-4-5-system-card" target="_blank" rel="noopener noreferrer">most aligned frontier model</a> we’ve ever released, showing large improvements across several areas of alignment compared to previous Claude models.</p><p>Claude Sonnet 4.5 is available everywhere today. If you’re a developer, simply use <code>claude-sonnet-4-5</code> via <a href="https://docs.claude.com/en/docs/about-claude/models/overview" target="_blank" rel="noopener noreferrer">the Claude API</a>. Pricing remains the same as Claude Sonnet 4, at $3/$15 per million tokens.</p><h2 id="frontier-intelligence">Frontier intelligence</h2><p>Claude Sonnet 4.5 is state-of-the-art on the SWE-bench Verified evaluation, which measures real-world software coding abilities. Practically speaking, we’ve observed it maintaining focus for more than 30 hours on complex, multi-step tasks.</p><div><figure><img alt="Chart showing frontier model performance on SWE-bench Verified with Claude Sonnet 4.5 leading" loading="lazy" width="3840" height="2160" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6421e7049ff8b2c4591497ec92dc4157b2ac1b30-3840x2160.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6421e7049ff8b2c4591497ec92dc4157b2ac1b30-3840x2160.png&amp;w=3840&amp;q=75"></figure></div><p>Claude Sonnet 4.5 represents a significant leap forward on computer use. On OSWorld, a benchmark that tests AI models on real-world computer tasks, Sonnet 4.5 now leads at 61.4%. Just four months ago, Sonnet 4 held the lead at 42.2%. Our <a href="https://www.anthropic.com/news/claude-for-chrome" target="_blank" rel="noopener noreferrer">Claude for Chrome</a> extension puts these upgraded capabilities to use. In the demo below, we show Claude working directly in a browser, navigating sites, filling spreadsheets, and completing tasks.</p><p>The model also shows improved capabilities on a broad range of evaluations including reasoning and math:</p><div><figure><img alt="Benchmark table comparing frontier models across popular public evals" loading="lazy" width="2600" height="2288" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F2825bb34cb9b9aa1ef347f816c590b5bbdae2b4d-2600x2288.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F2825bb34cb9b9aa1ef347f816c590b5bbdae2b4d-2600x2288.png&amp;w=3840&amp;q=75"><figcaption>Claude Sonnet 4.5 is our most powerful model to date. See footnotes for methodology.</figcaption></figure></div><p>Experts in finance, law, medicine, and STEM found Sonnet 4.5 shows dramatically better domain-specific knowledge and reasoning compared to older models, including Opus 4.1.</p><p>The model’s capabilities are also reflected in the experiences of early customers:</p><div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/464cf83cd04ad624fee1730a71914b18e89cdf9b-150x48.svg"></p><p><span>“</span></p><blockquote><p><strong>We're seeing state-of-the-art coding performance from Claude Sonnet 4.5</strong>, with significant improvements on longer horizon tasks. It reinforces why many developers using Cursor choose Claude for solving their most complex problems.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/7715b118c5eb0ff2a85f1f7914bce8c634ecacbd-150x48.svg"></p><p><span>“</span></p><blockquote><p><strong>Claude Sonnet 4.5 amplifies GitHub Copilot's core strengths</strong>. Our initial evals show significant improvements in multi-step reasoning and code comprehension—enabling Copilot's agentic experiences to handle complex, codebase-spanning tasks better.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/daef759120b29e4db8ba4a5664d7574750964ab9-150x48.svg"></p><p><span>“</span></p><blockquote><p><strong>Claude Sonnet 4.5 is excellent at software development tasks</strong>, learning our codebase patterns to deliver precise implementations. It handles everything from debugging to architecture with deep contextual understanding, transforming our development velocity.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/eb96f772e9ae5e340de41e6b07f3c6d50b3fff22-150x48.svg"></p><p><span>“</span></p><blockquote><p>Claude Sonnet 4.5 <strong>reduced average vulnerability intake time for our Hai security agents by 44% while improving accuracy by 25%</strong>, helping us reduce risk for businesses with confidence.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/8cbf56e184dd5174705a0f55cb91b0af545982ff-150x48.svg"></p><p><span>“</span></p><blockquote><p><strong>Claude Sonnet 4.5 is state of the art on the most complex litigation tasks.</strong> For example, analyzing full briefing cycles and conducting research to synthesize excellent first drafts of an opinion for judges, or interrogating entire litigation records to create detailed summary judgment analysis.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/431e098a503851789fa4508b88a0418853f513eb-150x48.svg"></p><p><span>“</span></p><blockquote><p>Claude Sonnet 4.5's edit capabilities are exceptional —<strong> we went from 9% error rate on Sonnet 4 to 0% on our internal code editing benchmark</strong>. Higher tool success at lower cost is a major leap for agentic coding. Claude Sonnet 4.5 balances creativity and control perfectly.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/66e0000e396aea64ea31ed3fea7b2b20ac329312-150x48.svg"></p><p><span>“</span></p><blockquote><p>Claude Sonnet 4.5 delivers impressive gains on our most complex, long-context tasks—from engineering in our codebase to in-product features and research. <strong>It's noticeably more intelligent and a big leap forward</strong>, helping us push what 240M+ users can design with Canva.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/cdec0ff1244295571db38838e90f61c47681d63d-150x48.svg"></p><p><span>“</span></p><blockquote><p><strong>Claude Sonnet 4.5 has noticeably improved Figma Make in early testing</strong>, making it easier to prompt and iterate. Teams can explore and validate their ideas with more functional prototypes and smoother interactions, while still getting the design quality Figma is known for.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/094b76abf3e64453c224e12ae388b8008b02660e-150x48.svg"></p><p><span>“</span></p><blockquote><p><strong>Sonnet 4.5 represents a new generation of coding models</strong>. It's surprisingly efficient at maximizing actions per context window through parallel tool execution, for example running multiple bash commands at once. </p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/6e418ccebe0a1d6fd13f21094852b080a0c93ae5-150x48.svg"></p><p><span>“</span></p><blockquote><p>For Devin, Claude Sonnet 4.5 increased planning performance by 18% and end-to-end eval scores by 12%—<strong>the biggest jump we've seen since the release of Claude Sonnet 3.6</strong>. It excels at testing its own code, enabling Devin to run longer, handle harder tasks, and deliver production-ready code.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/5a7dfab326b449aedc0d11053f9d42f48951ae7e-150x48.svg"></p><p><span>“</span></p><blockquote><p><strong>Claude Sonnet 4.5 shows strong promise for red teaming</strong>, generating creative attack scenarios that accelerate how we study attacker tradecraft. These insights strengthen our defenses across endpoints, identity, cloud, data, SaaS, and AI workloads.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/b0b6b40b55f3aa73e8a32ce81f9bb927134fd3da-150x48.svg"></p><p><span>“</span></p><blockquote><p>Claude Sonnet 4.5 resets our expectations—<strong>it handles 30+ hours of autonomous coding</strong>, freeing our engineers to tackle months of complex architectural work in dramatically less time while maintaining coherence across massive codebases.</p></blockquote></div><div><p><img alt=" logo" loading="lazy" width="120" height="48" decoding="async" data-nimg="1" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/4fcce1a2389ddafa9f3302c51960e1ff4bfbd3d7-150x48.svg"></p><p><span>“</span></p><blockquote><p>For complex financial analysis—risk, structured products, portfolio screening—Claude Sonnet 4.5 with thinking <strong>delivers investment-grade insights that require less human review</strong>. When depth matters more than speed, it's a meaningful step forward for institutional finance.</p></blockquote></div></div><h2 id="our-most-aligned-model-yet">Our most aligned model yet</h2><p>As well as being our most capable model, Claude Sonnet 4.5 is our most aligned frontier model yet. Claude’s improved capabilities and our extensive safety training have allowed us to substantially improve the model’s behavior, reducing concerning behaviors like sycophancy, deception, power-seeking, and the tendency to encourage delusional thinking. For the model’s agentic and computer use capabilities, we’ve also made considerable progress on defending against prompt injection attacks, one of the most serious risks for users of these capabilities.</p><p>You can read a detailed set of safety and alignment evaluations, which for the first time includes tests using techniques from mechanistic interpretability, in the Claude Sonnet 4.5 <a href="https://www.anthropic.com/claude-sonnet-4-5-system-card" target="_blank" rel="noopener noreferrer">system card</a>.</p><div><figure><img loading="lazy" width="3840" height="2160" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F33efc283321feeff94dd80973dbcd38409806cf5-3840x2160.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F33efc283321feeff94dd80973dbcd38409806cf5-3840x2160.png&amp;w=3840&amp;q=75"><figcaption>Overall misaligned behavior scores from an automated behavioral auditor (lower is better). Misaligned behaviors include (but are not limited to) deception, sycophancy, power-seeking, encouragement of delusions, and compliance with harmful system prompts. More details can be found in the Claude Sonnet 4.5 <a href="https://www.anthropic.com/claude-sonnet-4-5-system-card">system card</a>.</figcaption></figure></div><p>Claude Sonnet 4.5 is being released under our AI Safety Level 3 (ASL-3) protections, as per <a href="https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy" target="_blank" rel="noopener noreferrer">our framework</a> that matches model capabilities with appropriate safeguards. These safeguards include filters called classifiers that aim to detect potentially dangerous inputs and outputs—in particular those related to chemical, biological, radiological, and nuclear (CBRN) weapons.</p><p>These classifiers might sometimes inadvertently flag normal content. We’ve made it easy for users to continue any interrupted conversations with Sonnet 4, a model that poses a lower CBRN risk. We've already made significant progress in reducing these false positives, reducing them by a factor of ten since <a href="https://www.anthropic.com/news/constitutional-classifiers" target="_blank" rel="noopener noreferrer">we originally described them</a>, and a factor of two since Claude Opus 4 was released in May. We’re continuing to make progress in making the classifiers more discerning<sup>1</sup>.</p><h2 id="the-claude-agent-sdk">The Claude Agent SDK</h2><p>We've spent more than six months shipping updates to Claude Code, so we know what it takes to <a href="https://www.youtube.com/watch?v=DAQJvGjlgVM" target="_blank" rel="noopener noreferrer">build</a> and <a href="https://www.youtube.com/watch?v=vLIDHi-1PVU" target="_blank" rel="noopener noreferrer">design</a> AI agents. We've solved hard problems: how agents should manage memory across long-running tasks, how to handle permission systems that balance autonomy with user control, and how to coordinate subagents working toward a shared goal.</p><p>Now we’re making all of this available to you. The <a href="https://anthropic.com/engineering/building-agents-with-the-claude-agent-sdk" target="_blank" rel="noopener noreferrer">Claude Agent SDK</a> is the same infrastructure that powers Claude Code, but it shows impressive benefits for a very wide variety of tasks, not just coding. As of today, you can use it to build your own agents.</p><p>We built Claude Code because the tool we wanted didn’t exist yet. The Agent SDK gives you the same foundation to build something just as capable for whatever problem you're solving.</p><h2 id="bonus-research-preview">Bonus research preview</h2><p>We’re releasing a temporary research preview alongside Claude Sonnet 4.5, called "<a href="https://claude.ai/redirect/website.v1.f79c597c-8da8-4257-a3ef-0233ea5a1dd0/imagine" target="_blank" rel="noopener noreferrer">Imagine with Claude</a>".</p><p>In this experiment, Claude generates software on the fly. No functionality is predetermined; no code is prewritten. What you see is Claude creating in real time, responding and adapting to your requests as you interact.</p><p>It's a fun demonstration showing what Claude Sonnet 4.5 can do—a way to see what's possible when you combine a capable model with the right infrastructure.</p><p>"Imagine with Claude" is available to Max subscribers for the next five days. We encourage you to try it out on <a href="https://claude.ai/redirect/website.v1.f79c597c-8da8-4257-a3ef-0233ea5a1dd0/imagine" target="_blank" rel="noopener noreferrer">claude.ai/imagine</a>.</p><h2 id="further-information">Further information</h2><p>We recommend upgrading to Claude Sonnet 4.5 for all uses. Whether you’re using Claude through our apps, our API, or Claude Code, Sonnet 4.5 is a drop-in replacement that provides much improved performance for the same price. Claude Code updates are available to all users. <a href="https://claude.com/platform/api" target="_blank" rel="noopener noreferrer">Claude Developer Platform</a> updates, including the Claude Agent SDK, are available to all developers. Code execution and file creation are available on all paid plans in the Claude apps.</p><p>For complete technical details and evaluation results, see our <a href="https://www.anthropic.com/claude-sonnet-4-5-system-card" target="_blank" rel="noopener noreferrer">system card</a>, <a href="https://www.anthropic.com/claude/sonnet" target="_blank" rel="noopener noreferrer">model page</a>, and <a href="https://docs.claude.com/en/docs/about-claude/models/overview" target="_blank" rel="noopener noreferrer">documentation</a>. For more information, explore our <a href="https://www.anthropic.com/engineering/building-agents-with-the-claude-agent-sdk" target="_blank" rel="noopener noreferrer">engineering</a> <a href="https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents" target="_blank" rel="noopener noreferrer">posts</a> and research post on <a href="https://red.anthropic.com/2025/ai-for-cyber-defenders" target="_blank" rel="noopener noreferrer">cybersecurity</a>.</p></div></article></div><div><h4>Footnotes</h4><div><p><em>1<strong>: </strong>Customers in the cybersecurity and biological research industries can work with their account teams to join our allowlist in the meantime.</em></p><p><strong>Methodology</strong></p></div><ul><li><strong>SWE-bench Verified</strong>: All Claude results were reported using a simple scaffold with two tools—bash and file editing via string replacements. We report 77.2%, which was averaged over 10 trials, no test-time compute, and 200K thinking budget on the full 500-problem SWE-bench Verified dataset.<ul><li>The score reported uses a minor prompt addition: "You should use tools as much as possible, ideally more than 100 times. You should also implement your own tests first before attempting the problem."</li><li>A 1M context configuration achieves 78.2%, but we report the 200K result as our primary score as the 1M configuration was implicated in our recent <a href="https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues">inference issues</a>.</li><li>For our "high compute" numbers we adopt additional complexity and parallel test-time compute as follows:<ul><li>We sample multiple parallel attempts.</li><li>We discard patches that break the visible regression tests in the repository, similar to the rejection sampling approach adopted by <a href="https://arxiv.org/abs/2407.01489">Agentless</a> (Xia et al. 2024); note no hidden test information is used.</li><li>We then use an internal scoring model to select the best candidate from the remaining attempts.</li><li>This results in a score of 82.0% for Sonnet 4.5.</li></ul></li></ul></li><li><strong>Terminal-Bench</strong>: All scores reported use the default agent framework (Terminus 2), with XML parser, averaging multiple runs during different days to smooth the eval sensitivity to inference infrastructure.</li><li><strong>τ2-bench: </strong>Scores were achieved using extended thinking with tool use and a prompt addendum to the Airline and Telecom Agent Policy instructing Claude to better target its known failure modes when using the vanilla prompt. A prompt addendum was also added to the Telecom User prompt to avoid failure modes from the user ending the interaction incorrectly.</li><li><strong>AIME</strong>: Sonnet 4.5 score reported using sampling at temperature 1.0. The model used 64K reasoning tokens for the Python configuration.</li><li><strong>OSWorld: </strong>All scores reported use the official OSWorld-Verified framework with 100 max steps, averaged across 4 runs.</li><li><strong>MMMLU</strong>: All scores reported are the average of 5 runs over 14 non-English languages with extended thinking (up to 128K).</li><li><strong>Finance Agent</strong>: All scores reported were run and published by <a href="https://vals.ai/">Vals AI</a> on their public leaderboard. All Claude model results reported are with extended thinking (up to 64K) and Sonnet 4.5 is reported with interleaved thinking on.</li><li>All OpenAI scores reported from their <a href="https://openai.com/index/introducing-gpt-5/">GPT-5 post</a>, <a href="https://openai.com/index/introducing-gpt-5-for-developers/">GPT-5 for developers post</a>, <a href="https://cdn.openai.com/gpt-5-system-card.pdf">GPT-5 system card</a> (SWE-bench Verified reported using n=500), <a href="https://www.tbench.ai/">Terminal Bench leaderboard</a> (using Terminus 2), and public <a href="http://vals.ai/">Vals AI</a> leaderboard. All Gemini scores reported from their <a href="https://deepmind.google/models/gemini/pro/">model web page</a>, <a href="https://www.tbench.ai/">Terminal Bench leaderboard</a> (using Terminus 1), and public <a href="https://vals.ai/">Vals AI</a> leaderboard.</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Every single torrent is on this website (110 pts)]]></title>
            <link>https://infohash.lol/</link>
            <guid>45415539</guid>
            <pubDate>Mon, 29 Sep 2025 16:14:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://infohash.lol/">https://infohash.lol/</a>, See on <a href="https://news.ycombinator.com/item?id=45415539">Hacker News</a></p>
Couldn't get https://infohash.lol/: Error: getaddrinfo ENOTFOUND infohash.lol]]></description>
        </item>
        <item>
            <title><![CDATA[ML on Apple ][+ (108 pts)]]></title>
            <link>https://mdcramer.github.io/apple-2-blog/k-means/</link>
            <guid>45415510</guid>
            <pubDate>Mon, 29 Sep 2025 16:12:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mdcramer.github.io/apple-2-blog/k-means/">https://mdcramer.github.io/apple-2-blog/k-means/</a>, See on <a href="https://news.ycombinator.com/item?id=45415510">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
        <header>
          
          
        </header>
      

      <section itemprop="text">
        <p>Wait. Does k-means count as machine learning? Yes. Yes, it does.</p>

<p><a href="https://cs229.stanford.edu/" target="_blank">CS229</a> is the graduate-level machine learning course I took at Stanford as part of the <a href="https://www.linkedin.com/pulse/graduate-certificate-ai-achievement-unlocked-mark-cramer/" target="_blank">Graduate Certificate in AI</a> which I received back in 2021. While k-means is my choice as the easiest to understand algorithm in machine learning, it was taught as the introductory clustering algorithm for unsupervised learning. As a TA for <a href="https://online.stanford.edu/courses/xcs229-machine-learning" target="_blank">XCS229</a>, which I have been doing since 2022 and most recently did this Spring, I know that it is still being taught as part of this seminal course in AI.</p>

<h2 id="we-have-liftoff">We have liftoff!</h2>

<p>Unlike <a href="https://mdcramer.github.io/apple-2-blog/synthesizing-data/">previously</a> where I saved the result for the end, let’s start by taking a look at the algorithm in action!</p>

<p><a href="https://youtube.com/shorts/Cy0wMMLObVA?autoplay=1" title="Video of Apple ]\[+ running k-means" target="_blank"><img src="https://img.youtube.com/vi/Cy0wMMLObVA/0.jpg" alt="Video of Apple 2+ running k-means"></a></p>

<p>Here is a screenshot of the final decision boundary after convergence.</p>

<p><img src="https://mdcramer.github.io/assets/images/apple2/k-means-convergence.jpg" alt="K-means decision boundary after convergence" title="K-means decision boundary after convergence"></p>

<p>The final accuracy is 90% because 1 of the 10 observations is on the incorrect side of the decision boundary.</p>

<p>For debugging purposes, to speed up execution, I reduced the number of samples in each class to 5. (You might note that, on the graph, there are only 4 points in class 1, which are the □s. That’s because one of the points is at <code>(291, 90)</code>, which is off the edge of the screen. Gaussian distributions can generate extreme outliers, so I decided to preserve those points rather than clip them to the edge of the screen.) That’s obviously pretty small but you can see the algorithm iterating.</p>

<p>At the end of each loop I draw a line between the latest estimates of cluster centroids. The perpendicular bisector of these segments are the decision boundaries between the classes, so I draw them, too. Some of the code was written to handle more than two classes but here there are only two which makes this relatively easy.</p>

<h2 id="k-means-explained">K-means explained</h2>

<p><a href="https://en.wikipedia.org/wiki/K-means_clustering" target="_blank">K-means clustering</a> is a recursive algorithm that aims to partition \(n\) observations into \(k\) clusters in which each observation belongs to the cluster with the nearest mean, called the cluster centroid.</p>

<table>
  <thead>
    <tr>
      <th>Step</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Initialize</td>
      <td>Produce and initial set of k cluster centroids. This can be done by randomly choosing k observations from the dataset.</td>
    </tr>
    <tr>
      <td>Step 1 - Assignment</td>
      <td>Using Euclidean distance to the centroids, assign each observation to a cluster.</td>
    </tr>
    <tr>
      <td>Step 2 - Update</td>
      <td>For each cluster, recompute the centroid using the newly assigned observations. If the centroids change (outside of a certain tolerance), go back to step 1 and repeat.</td>
    </tr>
  </tbody>
</table>

<p>Ezpz.</p>

<p>The math is also simple. In step 1, the distance between two points, \(x\) and \(y\), is simply \(\sqrt{(x_0 - y_0)^2 + (x_1 - y_1)^2 + \cdots + (x_{d-1} - y_{d-1})^2}\), where \(d\) is the dimensionality of the observations. In our case \(d=2\) which is why we only have \(x_0\) and \(x_1\). Also, since we’re only using the distances for comparative purposes, it’s not even necessary to take the square root. In step 2, the centroid is simply the sum of all the points divided by the number of points.</p>

<h2 id="implementation">Implementation</h2>

<p>First, a little housekeeping before getting to the implementation of the algorithm.</p>

<div><pre><code><span>10</span>  <span>HOME</span> <span>:</span> <span>VTAB</span> <span>21</span>
<span>20</span>  <span>PI</span> <span>=</span> <span>3.14159265</span>
<span>30</span>  <span>GOSUB</span> <span>1000</span> <span>:</span> <span>REM  DRAW AXIS</span>
<span>40</span>  <span>GOSUB</span> <span>100</span> <span>:</span> <span>REM  GENERATE DATA</span>
<span>50</span>  <span>GOSUB</span> <span>900</span> <span>:</span> <span>REM  WAIT FOR KEY</span>
<span>60</span>  <span>GOSUB</span> <span>2000</span> <span>:</span> <span>REM  RUN K-MEANS</span>
<span>70</span>  <span>END</span>

<span>100</span> <span>REM  == HYPERPARAMETERS ==</span>
<span>...</span>
<span>450</span> <span>DIM</span> <span>P%</span><span>(</span><span>2</span><span>,</span><span>1</span><span>)</span> <span>:</span> <span>REM  RANDOM POINTS</span>
<span>460</span> <span>REM  == K-MEANS DATA TABLES ==</span>
<span>470</span> <span>DIM</span> <span>DI</span><span>(</span><span>NS</span> <span>-</span> <span>1</span><span>,</span><span>KN</span> <span>-</span> <span>1</span><span>)</span>
<span>480</span> <span>REM  -- K - MU-XO, MU-X1, N-K --</span>
<span>490</span> <span>DIM</span> <span>KM</span><span>(</span><span>KN</span> <span>-</span> <span>1</span><span>,</span><span>2</span><span>)</span>
<span>500</span> <span>REM  -- K - OLD MU-X0, OLD MU-X1 --</span>
<span>510</span> <span>DIM</span> <span>KO</span><span>(</span><span>KN</span> <span>-</span> <span>1</span><span>,</span><span>1</span><span>)</span>
<span>...</span>

<span>900</span> <span>REM  == WAIT FOR KEYSTROKE ==</span>
<span>910</span> <span>POKE</span> <span>49168</span><span>,</span><span>0</span> <span>:</span> <span>REM  CLEAR BUFFER</span>
<span>920</span> <span>IF</span> <span>PEEK</span><span>(</span><span>49152</span><span>)</span> <span>&lt;</span> <span>128</span> <span>GOTO</span> <span>920</span>
<span>930</span> <span>POKE</span> <span>49168</span><span>,</span><span>0</span>
<span>940</span> <span>RETURN</span>
</code></pre></div>

<p>At the very top of the program I decided to organize everything into subroutines. The idea here is to enable expansion into other ML algorithms.</p>

<p>The “wait for key” subroutine is the APPLESOFT BASIC method for simply waiting for any keystroke before continuing. (<code>PEEK</code> and <code>POKE</code> are commands for directly accessing addresses in memory. I had those numbers memorized in high school but, naturally, I had to look them up.) I thought it’d be nice to add this pause after generating the data but I might take it out later.</p>

<p>Lastly, at the end of the “hyperparameters” section I declare a convenience array, <code>P%(2,1)</code> to keep track of 3 random points as well as a few arrays I’m going to use in the k-means algorithm. The reason I do this here is because in APPLESOFT BASIC you get an error if you declare an array that already exists. Should at some point I want to call the k-means algorithm multiple times, this won’t be a problem.</p>

<h3 id="initialize">Initialize</h3>

<p>Getting started, the first thing to do is initialize the algorithm by generating \(k\) cluster centroids. (\(k\) is a hyperparameter that specifies the number of clusters to be “found.” I set it previously with <code>KN = 2</code>.)</p>

<div><pre><code><span>2000</span> <span>REM  == K-MEANS ==</span>
<span>2010</span> <span>PRINT</span> <span>"RUN K-MEANS"</span>
<span>2020</span> <span>REM  -- CLEAR PREDICTIONS --</span>
<span>2030</span> <span>FOR</span> <span>I</span> <span>=</span> <span>0</span> <span>TO</span> <span>NS</span> <span>-</span> <span>1</span>
<span>2040</span>   <span>DS%</span><span>(</span><span>I</span><span>,</span><span>3</span><span>)</span> <span>=</span> <span>0</span>
<span>2050</span> <span>NEXT</span> <span>I</span>
<span>2100</span> <span>REM  -- INITIALIZE CENTROIDS --</span>
<span>2110</span> <span>FOR</span> <span>I</span> <span>=</span> <span>0</span> <span>TO</span> <span>KN</span> <span>-</span> <span>1</span>
<span>2120</span>   <span>J</span> <span>=</span> <span>INT</span><span>(</span><span>RND</span><span>(</span><span>1</span><span>)</span> <span>*</span> <span>NS</span><span>)</span>
<span>2130</span>   <span>IF</span> <span>DS%</span><span>(</span><span>J</span><span>,</span><span>3</span><span>)</span> <span>=</span> <span>1</span> <span>GOTO</span> <span>2120</span>
<span>2140</span>   <span>KM</span><span>(</span><span>I</span><span>,</span><span>1</span><span>)</span> <span>=</span> <span>DS%</span><span>(</span><span>J</span><span>,</span><span>1</span><span>)</span>
<span>2150</span>   <span>KM</span><span>(</span><span>I</span><span>,</span><span>2</span><span>)</span> <span>=</span> <span>DS%</span><span>(</span><span>J</span><span>,</span><span>2</span><span>)</span>
<span>2160</span>   <span>DS%</span><span>(</span><span>J</span><span>,</span><span>3</span><span>)</span> <span>=</span> <span>1</span>
<span>2170</span> <span>NEXT</span> <span>I</span>
<span>2200</span> <span>REM  -- DRAW LINES BETWEEN CENTROIDS --</span>
<span>2210</span> <span>FOR</span> <span>I</span> <span>=</span> <span>1</span> <span>TO</span> <span>KN</span> <span>-</span> <span>1</span>
<span>2220</span>   <span>HPLOT</span> <span>KM</span><span>(</span><span>I</span><span>-</span><span>1</span><span>,</span><span>0</span><span>)</span><span>,</span> <span>159</span><span>-</span><span>KM</span><span>(</span><span>I</span><span>-</span><span>1</span><span>,</span><span>1</span><span>)</span> <span>TO</span> <span>KM</span><span>(</span><span>I</span><span>,</span><span>0</span><span>)</span><span>,</span> <span>159</span><span>-</span><span>KM</span><span>(</span><span>I</span><span>,</span><span>1</span><span>)</span>
<span>2230</span> <span>NEXT</span> <span>I</span>
<span>2240</span> <span>GOSUB</span> <span>3000</span><span>:</span> <span>REM  DRAW DECISION BOUNDARY</span>
</code></pre></div>

<p>I start by clearing out the prediction column, \(y\), of the dataset table, <code>DS%(NS - 1,3)</code> because I’m going to use this to make sure I don’t randomly pick the same point twice. Then for each class I randomly pick a point from the dataset. If it’s already been used I randomly pick another. <code>KM(KN - 1, 2)</code> is where I store the means for each cluster along with a count of the number of points in each cluster.</p>

<p>Finally, I draw a line between the cluster centroids. This loop does not take into account all combinations of centroids (it works fine if \(k=2\)) and generates an error if a centroid is off the screen, which is possible, so I might just get rid of this later, since it’s not really necessary, rather than try to fix it.</p>

<h3 id="step-1---assignment">Step 1 - Assignment</h3>

<p>The fist step is to assign every data point to the nearest cluster centroid.</p>

<div><pre><code><span>2300</span> <span>REM  -- COMPUTE ASSIGNMENTS --</span>
<span>2310</span> <span>FOR</span> <span>I</span> <span>=</span> <span>0</span> <span>TO</span> <span>NS</span> <span>-</span> <span>1</span>
<span>2320</span>   <span>PRINT</span> <span>"POINT "</span><span>;</span><span>I</span><span>;</span><span>" AT "</span><span>;</span><span>DS%</span><span>(</span><span>I</span><span>,</span><span>0</span><span>)</span><span>;</span><span>","</span><span>;</span><span>DS%</span><span>(</span><span>I</span><span>,</span><span>1</span><span>)</span><span>;</span>
<span>2330</span>   <span>DS%</span><span>(</span><span>I</span><span>,</span><span>3</span><span>)</span> <span>=</span> <span>0</span>
<span>2340</span>   <span>FOR</span> <span>J</span> <span>=</span> <span>0</span> <span>TO</span> <span>KN</span> <span>-</span> <span>1</span>
<span>2350</span>     <span>DI</span><span>(</span><span>I</span><span>,</span><span>J</span><span>)</span> <span>=</span> <span>(</span><span>DS%</span><span>(</span><span>I</span><span>,</span><span>0</span><span>)-</span><span>KM</span><span>(</span><span>J</span><span>,</span><span>0</span><span>))^</span><span>2</span> <span>+</span> <span>(</span><span>DS%</span><span>(</span><span>I</span><span>,</span><span>1</span><span>)-</span><span>KM</span><span>(</span><span>J</span><span>,</span><span>1</span><span>))^</span><span>2</span>
<span>2360</span>     <span>IF</span> <span>J</span> <span>&gt;</span><span>0</span> <span>AND</span> <span>(</span><span>DI</span><span>(</span><span>I</span><span>,</span><span>J</span><span>)</span> <span>&lt;</span> <span>DI</span><span>(</span><span>I</span><span>,</span><span>DS%</span><span>(</span><span>I</span><span>,</span><span>3</span><span>)))</span> <span>THEN</span> <span>DS%</span><span>(</span><span>I</span><span>,</span><span>3</span><span>)</span> <span>=</span> <span>J</span>
<span>2370</span>   <span>NEXT</span> <span>J</span>
<span>2380</span>   <span>PRINT</span> <span>" -&gt; "</span><span>;</span><span>DS%</span><span>(</span><span>I</span><span>,</span><span>3</span><span>)</span><span>;</span><span>" Y^="</span><span>;</span><span>DS%</span><span>(</span><span>I</span><span>,</span><span>2</span><span>)</span>
<span>2390</span> <span>NEXT</span> <span>I</span>
<span>2500</span> <span>REM  -- COMPUTE ACCURACY --</span>
<span>2510</span> <span>CT</span> <span>=</span> <span>0</span>
<span>2520</span> <span>FOR</span> <span>I</span> <span>=</span> <span>0</span> <span>TO</span> <span>NS</span> <span>-</span> <span>1</span>
<span>2530</span>   <span>IF</span> <span>DS%</span><span>(</span><span>I</span><span>,</span><span>2</span><span>)</span> <span>=</span> <span>DS%</span><span>(</span><span>I</span><span>,</span><span>3</span><span>)</span> <span>THEN</span> <span>CT</span> <span>=</span> <span>CT</span> <span>+</span> <span>1</span>
<span>2540</span> <span>NEXT</span> <span>I</span>
<span>2550</span> <span>A</span> <span>=</span> <span>CT</span> <span>/</span> <span>NS</span>
<span>2560</span> <span>IF</span> <span>A</span> <span>&lt;</span> <span>0.5</span> <span>THEN</span> <span>A</span> <span>=</span> <span>1</span> <span>-</span> <span>A</span>
<span>2570</span> <span>PRINT</span> <span>"ACCURACY = "</span><span>;</span> <span>INT</span><span>(</span><span>A</span><span>*</span><span>10000</span><span>+</span><span>0.5</span><span>)/</span><span>100</span><span>;</span><span>"%"</span>
</code></pre></div>

<p>The assignment step is also quite easy. I loop through all the data points, computing the Euclidean distance to each cluster centroid. (Since <code>SQRT()</code> is expensive, and unnecessary here since we’re just comparing, I actually just compute the square of the Euclidean distance.) If the distance is less than the previous minimum distance, <code>DI(I,DS%(I,3))</code>, I update the assignment, <code>DS%(I,3) = J</code>.</p>

<p>At the end, I compute the accuracy of the computed assignments by simply counting the number of assignments, <code>DS%(I,3)</code>, that match the actual labels, <code>DS%(I,2)</code>. Here, however, there’s an interesting wrinkle: with two classes, half the time the label I choose for the assignment is the opposite of the label from the original dataset. K-means doesn’t require the distinction, so at times I was seeing a perfect classification reporting 0% accuracy. The line <code>IF A &lt; 0.5 THEN A = 1 - A</code> addresses this, however, it only works for 2 classes. I’ll need something more robust should I want this to work for \(k &gt; 2\).</p>

<h3 id="step-2---update">Step 2 - Update</h3>

<p>The second step is to recompute the cluster centroids based on the assigned data points. Convergence occurred if the centroids don’t change (within a tolerance) from the previous iteration.</p>

<div><pre><code><span>2600</span> <span>REM  -- COMPUTE CENTROIDS --</span>
<span>2610</span> <span>FOR</span> <span>J</span> <span>=</span> <span>0</span> <span>TO</span> <span>KN</span> <span>-</span> <span>1</span>
<span>2620</span>   <span>K0</span><span>(</span><span>J</span><span>,</span><span>0</span><span>)</span> <span>=</span> <span>KM</span><span>(</span><span>J</span><span>,</span><span>0</span><span>)</span>
<span>2630</span>   <span>K0</span><span>(</span><span>J</span><span>,</span><span>1</span><span>)</span> <span>=</span> <span>KM</span><span>(</span><span>J</span><span>,</span><span>1</span><span>)</span>
<span>2640</span>   <span>KM</span><span>(</span><span>J</span><span>,</span><span>0</span><span>)</span> <span>=</span> <span>0</span><span>:</span> <span>KM</span><span>(</span><span>J</span><span>,</span><span>1</span><span>)</span> <span>=</span> <span>0</span>
<span>2650</span>   <span>KM</span><span>(</span><span>J</span><span>,</span><span>2</span><span>)</span> <span>=</span> <span>0</span>
<span>2660</span> <span>NEXT</span>
<span>2670</span> <span>FOR</span> <span>I</span> <span>=</span> <span>0</span> <span>TO</span> <span>NS</span> <span>-</span> <span>1</span>
<span>2680</span>   <span>Y</span> <span>=</span> <span>DS%</span><span>(</span><span>I</span><span>,</span><span>3</span><span>)</span>
<span>2690</span>   <span>KM%</span><span>(</span><span>Y</span><span>,</span><span>0</span><span>)</span> <span>=</span> <span>KM%</span><span>(</span><span>Y</span><span>,</span><span>0</span><span>)</span> <span>+</span> <span>DS%</span><span>(</span><span>I</span><span>,</span><span>0</span><span>)</span>
<span>2700</span>   <span>KM%</span><span>(</span><span>Y</span><span>,</span><span>1</span><span>)</span> <span>=</span> <span>KM%</span><span>(</span><span>Y</span><span>,</span><span>1</span><span>)</span> <span>+</span> <span>DS%</span><span>(</span><span>I</span><span>,</span><span>1</span><span>)</span>
<span>2710</span>   <span>KM%</span><span>(</span><span>Y</span><span>,</span><span>2</span><span>)</span> <span>=</span> <span>KM%</span><span>(</span><span>Y</span><span>,</span><span>2</span><span>)</span> <span>+</span> <span>1</span>
<span>2720</span> <span>NEXT</span>
<span>2730</span> <span>FOR</span> <span>I</span> <span>=</span> <span>0</span> <span>TO</span> <span>KN</span> <span>-</span> <span>1</span>
<span>2740</span>   <span>KM%</span><span>(</span><span>I</span><span>,</span><span>0</span><span>)</span> <span>=</span> <span>KM%</span><span>(</span><span>I</span><span>,</span><span>0</span><span>)</span> <span>/</span> <span>KM%</span><span>(</span><span>I</span><span>,</span><span>2</span><span>)</span>
<span>2750</span>   <span>KM%</span><span>(</span><span>I</span><span>,</span><span>1</span><span>)</span> <span>=</span> <span>KM%</span><span>(</span><span>I</span><span>,</span><span>1</span><span>)</span> <span>/</span> <span>KM%</span><span>(</span><span>I</span><span>,</span><span>2</span><span>)</span>
<span>2760</span> <span>NEXT</span>
<span>2800</span> <span>REM  -- DETERMINE CONVERGENCE --</span>
<span>2810</span> <span>DI</span> <span>=</span> <span>0</span>
<span>2820</span> <span>FOR</span> <span>I</span> <span>=</span> <span>0</span> <span>TO</span> <span>KN</span> <span>-</span> <span>1</span>
<span>2830</span>   <span>DI</span> <span>=</span> <span>DI</span> <span>+</span> <span>(</span><span>KM%</span><span>(</span><span>I</span><span>,</span><span>0</span><span>)</span> <span>-</span> <span>KO%</span><span>(</span><span>I</span><span>,</span><span>0</span><span>))</span> <span>^</span> <span>2</span> <span>+</span> <span>(</span><span>KM%</span><span>(</span><span>I</span><span>,</span><span>1</span><span>)</span> <span>-</span> <span>KO%</span><span>(</span><span>I</span><span>,</span><span>1</span><span>))</span> <span>^</span> <span>2</span>
<span>2840</span> <span>NEXT</span>
<span>2850</span> <span>IF</span> <span>DI</span> <span>&gt;</span> <span>0.01</span> <span>THEN</span> <span>GOTO</span> <span>2200</span>
<span>2860</span> <span>PRINT</span> <span>"K-MEANS CONVERGED"</span>
<span>2900</span> <span>REM  -- CLEAR GRAPHICS AND REDRAW WITH DECISION BOUNDARY --</span>
<span>2910</span> <span>GOSUB</span> <span>1000</span>
<span>2920</span> <span>FOR</span> <span>I</span> <span>=</span> <span>0</span> <span>TO</span> <span>NS</span> <span>-</span> <span>1</span>
<span>2930</span>   <span>X0%</span> <span>=</span> <span>DS%</span><span>(</span><span>I</span><span>,</span><span>0</span><span>)</span>
<span>2940</span>   <span>X1%</span> <span>=</span> <span>DS%</span><span>(</span><span>I</span><span>,</span><span>1</span><span>)</span>
<span>2950</span>   <span>K</span> <span>=</span> <span>DS%</span><span>(</span><span>I</span><span>,</span><span>2</span><span>)</span>
<span>2960</span>   <span>ON</span> <span>K</span> <span>+</span> <span>1</span> <span>GOSUB</span> <span>1200</span><span>,</span><span>1300</span>
<span>2970</span> <span>NEXT</span>
<span>2980</span> <span>GOSUB</span> <span>3000</span>
<span>2990</span> <span>RETURN</span>
</code></pre></div>

<p>I start by saving the cluster centroids to <code>KO(KN - 1,1)</code>. This is used later to determine convergence. I then iterate through ever data point, adding it’s values to the cluster to which it belongs while keeping track of the number of data points in each cluster. Next I iterate through each cluster and compute the mean of each dimension by dividing by the number of data point in that cluster.</p>

<p>Lastly, I determine if there’s convergence by measuring how far all the centroid have moved. (Again, I don’t bother with the <code>SQRT()</code>.) If the answer is more than the specified tolerance, \(0.01\), I go back to Step #1. Otherwise, I clear the graphics, redraw the axis and data points and finish by drawing the decision boundary.</p>

<h3 id="drawing-the-decision-boundary">Drawing the decision boundary</h3>

<p>This code is a slog and it’s not really critical to understanding ML but I thought it’d be cool to drawn a decision boundary while k-means is iterating and then again at the end. Given a point (the midpoint on the segment between two cluster centroids) and a slope (which is perpendicular to that segment), the challenge is to drawn a line inside the ‘box’ of the screen, assuming the line intersects that box.</p>

<div><pre><code><span>3000</span> <span>REM  -- DRAW DECISION BOUNDARY --</span>
<span>3010</span> <span>FOR</span> <span>I</span> <span>=</span> <span>1</span> <span>TO</span> <span>KN</span> <span>-</span> <span>1</span>
<span>3020</span>   <span>M</span> <span>=</span> <span>1</span><span>E6</span>
<span>3030</span>   <span>IF</span> <span>KM%</span><span>(</span><span>I</span> <span>-</span> <span>1</span><span>,</span><span>1</span><span>)</span> <span>-</span> <span>KM%</span><span>(</span><span>I</span><span>,</span><span>1</span><span>)</span> <span>&lt;&gt;</span> <span>0</span> <span>THEN</span> <span>M</span> <span>=</span> <span>-</span><span>1</span> <span>*</span> <span>(</span><span>KM%</span><span>(</span><span>I</span> <span>-</span> <span>1</span><span>,</span><span>0</span><span>)</span> <span>-</span> <span>KM%</span><span>(</span><span>I</span><span>,</span><span>0</span><span>))</span> <span>/</span> <span>(</span><span>KM%</span><span>(</span><span>I</span> <span>-</span> <span>1</span><span>,</span><span>1</span><span>)</span> <span>-</span> <span>KM%</span><span>(</span><span>I</span><span>,</span><span>1</span><span>))</span>
<span>3040</span>   <span>P%</span><span>(</span><span>0</span><span>,</span><span>0</span><span>)</span> <span>=</span> <span>(</span><span>KM%</span><span>(</span><span>I</span><span>,</span><span>0</span><span>)</span> <span>-</span> <span>KM%</span><span>(</span><span>I</span> <span>-</span> <span>1</span><span>,</span><span>0</span><span>))</span> <span>/</span> <span>2</span> <span>+</span> <span>KM%</span><span>(</span><span>I</span> <span>-</span> <span>1</span><span>,</span><span>0</span><span>)</span>
<span>3050</span>   <span>P%</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span> <span>=</span> <span>(</span><span>KM%</span><span>(</span><span>I</span><span>,</span><span>1</span><span>)</span> <span>-</span> <span>KM%</span><span>(</span><span>I</span> <span>-</span> <span>1</span><span>,</span><span>1</span><span>))</span> <span>/</span> <span>2</span> <span>+</span> <span>KM%</span><span>(</span><span>I</span> <span>-</span> <span>1</span><span>,</span><span>1</span><span>)</span>
<span>3060</span>   <span>GOSUB</span> <span>3500</span>
<span>3070</span> <span>NEXT</span>
<span>3080</span> <span>REM  -- DRAW LINE FROM SLOPE AND POINT --</span>
<span>3090</span> <span>NX</span> <span>=</span> <span>1</span> <span>:</span> <span>REM  -- REM NUMBER OF INTERSECTIONS --</span>
<span>3100</span> <span>IF</span> <span>ABS</span><span>(</span><span>M</span><span>)</span> <span>&gt;</span> <span>1</span><span>E5</span> <span>THEN</span> <span>GOSUB</span> <span>3240</span> <span>:</span> <span>GOTO</span> <span>3210</span> <span>:</span> <span>REM  VERTICAL LINE</span>
<span>3110</span> <span>P%</span><span>(</span><span>NX</span><span>,</span><span>1</span><span>)</span> <span>=</span> <span>M</span> <span>*</span> <span>(</span><span>10</span> <span>-</span> <span>P%</span><span>(</span><span>0</span><span>,</span><span>0</span><span>))</span> <span>+</span> <span>P%</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span>
<span>3120</span> <span>IF</span> <span>P%</span><span>(</span><span>NX</span><span>,</span><span>1</span><span>)</span> <span>&gt;</span> <span>10</span> <span>AND</span> <span>P%</span><span>(</span><span>NX</span><span>,</span><span>1</span><span>)</span> <span>&lt;</span> <span>149</span> <span>THEN</span> <span>P%</span><span>(</span><span>NX</span><span>,</span><span>0</span><span>)</span> <span>=</span> <span>10</span> <span>:</span> <span>NX</span> <span>=</span> <span>NX</span> <span>+</span> <span>1</span>
<span>3130</span> <span>P%</span><span>(</span><span>NX</span><span>,</span><span>1</span><span>)</span> <span>=</span> <span>M</span> <span>*</span> <span>(</span><span>269</span> <span>-</span> <span>P%</span><span>(</span><span>0</span><span>,</span><span>0</span><span>))</span> <span>+</span> <span>P%</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span>
<span>3140</span> <span>IF</span> <span>P%</span><span>(</span><span>NX</span><span>,</span><span>1</span><span>)</span> <span>&gt;</span> <span>10</span> <span>AND</span> <span>P%</span><span>(</span><span>NX</span><span>,</span><span>1</span><span>)</span> <span>&lt;</span> <span>149</span> <span>THEN</span> <span>P%</span><span>(</span><span>NX</span><span>,</span><span>0</span><span>)</span> <span>=</span> <span>269</span> <span>:</span> <span>NX</span> <span>=</span> <span>NX</span> <span>+</span> <span>1</span>
<span>3150</span> <span>IF</span> <span>NX</span> <span>=</span> <span>3</span> <span>THEN</span> <span>GOTO</span> <span>3210</span>
<span>3160</span> <span>IF</span> <span>M</span> <span>&lt;&gt;</span> <span>0</span> <span>THEN</span> <span>P%</span><span>(</span><span>NX</span><span>,</span><span>0</span><span>)</span> <span>=</span> <span>(</span><span>10</span> <span>-</span> <span>P%</span><span>(</span><span>0</span><span>,</span><span>1</span><span>))</span> <span>/</span> <span>M</span> <span>+</span> <span>P%</span><span>(</span><span>0</span><span>,</span><span>0</span><span>)</span>
<span>3170</span> <span>IF</span> <span>M</span> <span>&lt;&gt;</span> <span>0</span> <span>AND</span> <span>P%</span><span>(</span><span>NX</span><span>,</span><span>0</span><span>)</span> <span>&gt;</span> <span>10</span> <span>AND</span> <span>P%</span><span>(</span><span>NX</span><span>,</span><span>0</span><span>)</span> <span>&lt;</span> <span>269</span> <span>THEN</span> <span>P%</span><span>(</span><span>NX</span><span>,</span><span>1</span><span>)</span> <span>=</span> <span>10</span> <span>:</span> <span>NX</span> <span>=</span> <span>NX</span> <span>+</span> <span>1</span>
<span>3180</span> <span>IF</span> <span>NX</span> <span>=</span> <span>3</span> <span>THEN</span> <span>GOTO</span> <span>3210</span>
<span>3190</span> <span>IF</span> <span>M</span> <span>&lt;&gt;</span> <span>0</span> <span>THEN</span> <span>P%</span><span>(</span><span>NX</span><span>,</span><span>0</span><span>)</span> <span>=</span> <span>(</span><span>149</span> <span>-</span> <span>P%</span><span>(</span><span>0</span><span>,</span><span>1</span><span>))</span> <span>/</span> <span>M</span> <span>+</span> <span>P%</span><span>(</span><span>0</span><span>,</span><span>0</span><span>)</span>
<span>3200</span> <span>IF</span> <span>M</span> <span>&lt;&gt;</span> <span>0</span> <span>AND</span> <span>P%</span><span>(</span><span>NX</span><span>,</span><span>0</span><span>)</span> <span>&gt;</span> <span>10</span> <span>AND</span> <span>P%</span><span>(</span><span>NX</span><span>,</span><span>0</span><span>)</span> <span>&lt;</span> <span>269</span> <span>THEN</span> <span>P%</span><span>(</span><span>NX</span><span>,</span><span>1</span><span>)</span> <span>=</span> <span>149</span> <span>:</span> <span>NX</span> <span>=</span> <span>NX</span> <span>+</span> <span>1</span>
<span>3210</span> <span>REM  -- DRAW LINE --</span>
<span>3220</span> <span>IF</span> <span>NX</span> <span>=</span> <span>3</span> <span>THEN</span> <span>HPLOT</span> <span>P%</span><span>(</span><span>1</span><span>,</span><span>0</span><span>)</span><span>,</span><span>159</span> <span>-</span> <span>P%</span><span>(</span><span>1</span><span>,</span><span>1</span><span>)</span> <span>TO</span> <span>P%</span><span>(</span><span>2</span><span>,</span><span>0</span><span>)</span><span>,</span><span>159</span> <span>-</span> <span>P%</span><span>(</span><span>2</span><span>,</span><span>1</span><span>)</span>
<span>3230</span> <span>RETURN</span>
<span>3240</span> <span>REM  -- VERTICAL LINE --</span>
<span>3250</span> <span>P%</span><span>(</span><span>1</span><span>,</span><span>0</span><span>)</span> <span>=</span> <span>P%</span><span>(</span><span>0</span><span>,</span><span>0</span><span>)</span>
<span>3260</span> <span>P%</span><span>(</span><span>2</span><span>,</span><span>0</span><span>)</span> <span>=</span> <span>P%</span><span>(</span><span>0</span><span>,</span><span>0</span><span>)</span>
<span>3270</span> <span>P%</span><span>(</span><span>1</span><span>,</span><span>1</span><span>)</span> <span>=</span> <span>10</span>
<span>3280</span> <span>P%</span><span>(</span><span>2</span><span>,</span><span>1</span><span>)</span> <span>=</span> <span>269</span>
<span>3290</span> <span>RETURN</span>
</code></pre></div>

<p>Without delving too far into the details, this routine relies heavily on the convenience array, <code>P%(2,1)</code>, that I declared during the “hyperparameters” routine. I start by computing the slope of the perpendicular segment connecting two centroids. I then find the midpoint of that segment. (By the way, this routine also does not account for all combinations of centroids, but it works when \(k=2\).) I accommodate for when the slope is vertical and use <code>P%(0,0)</code> and <code>P%(0,1)</code> to store the midpoint between the two centroids and <code>M</code> for the slope.</p>

<p>I then iterate through the 4 sides of the ‘box’ on the screen, using the corners <code>(10,10)</code> and <code>(269,149)</code> so that the decision boundary isn’t drawn all the way to the edges of the screen. I thought that would look prettier this way. I next determine if the decision boundary intersects, respectively, the left, right, top and bottom edges of the box. I use <code>NX</code> to keep track of the number of sides of the box intersected by the decision boundary and <code>P%(NX,0)</code> and <code>P%(NX,1)</code> to keep track of those intersections. If <code>NX = 3</code>, which means there are two intersections, I draw the line because it’s inside the box.</p>

<h2 id="can-we-do-better">Can we do better?</h2>

<p>Yes! Yes, we can.</p>

<p>While k-means is simple, it does not take advantage of our knowledge of the Gaussian nature of the data. If we know that the distributions are Gaussian, which is very frequently the case in machine learning, we can employ a more powerful algorithm: Expectation Maximization (EM). This post is already long enough, so we’ll deal with that another day. Eventually, perhaps, we’ll also get to deep learning, although developing back propagation for an arbitrary size neural net using APPLESOFT BASIC on an <span>Apple ][+</span> is not going to be easy.</p>

        
      </section>

      

  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Subtleties of SQLite Indexes (102 pts)]]></title>
            <link>https://emschwartz.me/subtleties-of-sqlite-indexes/</link>
            <guid>45415332</guid>
            <pubDate>Mon, 29 Sep 2025 15:54:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://emschwartz.me/subtleties-of-sqlite-indexes/">https://emschwartz.me/subtleties-of-sqlite-indexes/</a>, See on <a href="https://news.ycombinator.com/item?id=45415332">Hacker News</a></p>
Couldn't get https://emschwartz.me/subtleties-of-sqlite-indexes/: Error: getaddrinfo ENOTFOUND emschwartz.me]]></description>
        </item>
        <item>
            <title><![CDATA[Write the Damn Code (200 pts)]]></title>
            <link>https://antonz.org/write-code/</link>
            <guid>45415232</guid>
            <pubDate>Mon, 29 Sep 2025 15:45:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://antonz.org/write-code/">https://antonz.org/write-code/</a>, See on <a href="https://news.ycombinator.com/item?id=45415232">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article data-namespace=""><div><header></header><p>Here's some popular programming advice these days:</p><blockquote><p>Learn to decompose problems into smaller chunks, be specific about what you want, pick the right AI model for the task, and <strong>iterate on your prompts</strong>.</p></blockquote><p>Don't do this.</p><p>I mean, "learn to decompose the problem" — sure. "Iterate on your prompts" — not so much. Write the actual code instead:</p><ul><li>Ask AI for an initial version and then refactor it to match your expectations.</li><li>Write the initial version yourself and ask AI to review and improve it.</li><li>Write the critical parts and ask AI to do the rest.</li><li>Write an outline of the code and ask AI to fill the missing parts.</li></ul><p>You probably see the pattern now. Get involved with the code, don't leave it all to AI.</p><p>If, given the prompt, AI does the job perfectly on first or second iteration — fine. Otherwise, stop refining the prompt. Go write some code, then get back to the AI. You'll get much better results.</p><p>Don't get me wrong: this is not anti-AI advice. Use it, by all means. Use it a lot if you want to. But don't fall into the trap of endless back-and-forth prompt refinement, trying to get the perfect result from AI by "programming in English". It's an imprecise, slow and terribly painful way to get things done.</p><p>Get your hands dirty. Write the code. It's what you are good at.</p><p>You are a software engineer. Don't become a prompt refiner.</p><p><a href="https://antonz.org/subscribe/">★&nbsp;Subscribe</a> to keep up with new posts.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Loadmo.re: design inspiration for unconventional web (313 pts)]]></title>
            <link>https://loadmo.re</link>
            <guid>45415207</guid>
            <pubDate>Mon, 29 Sep 2025 15:42:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://loadmo.re">https://loadmo.re</a>, See on <a href="https://news.ycombinator.com/item?id=45415207">Hacker News</a></p>
Couldn't get https://loadmo.re: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Not all OCuLink eGPU docks are created equal (106 pts)]]></title>
            <link>https://www.jeffgeerling.com/blog/2025/not-all-oculink-egpu-docks-are-created-equal</link>
            <guid>45414479</guid>
            <pubDate>Mon, 29 Sep 2025 14:46:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jeffgeerling.com/blog/2025/not-all-oculink-egpu-docks-are-created-equal">https://www.jeffgeerling.com/blog/2025/not-all-oculink-egpu-docks-are-created-equal</a>, See on <a href="https://news.ycombinator.com/item?id=45414479">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I recently tried using the <a href="https://amzn.to/46Y65Wo">Minisforum DEG1 GPU Dock</a> with a Raspberry Pi 500+, using an M.2 to OCuLink adapter, and <a href="https://amzn.to/48CZlyl">this chenyang SFF-8611 Cable</a>.</p>

<p>After figuring out there's a power button on the DEG1 (which needs to be turned on), and after fiddling around with the switches on the PCB (hidden under the large metal plate on the bottom; TGX to OFF was the most important setting), I was able to get the Raspberry Pi's PCIe bus to at least tell the graphics card installed in the eGPU dock to spin up its fans and initialize.</p>

<p>But I wasn't able to get any output from the card (using <a href="https://github.com/geerlingguy/raspberry-pi-pcie-devices/discussions/756">this Linux kernel patch</a>), and <code>lspci</code> did not show it. (Nor were there any logs showing errors in <code>dmesg</code>).</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/pi-500-plus-egpu-jmt-dock.jpeg" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-3844c36f-6031-4ca4-a3b1-c52098338202" data-insert-attach="{&quot;id&quot;:&quot;3844c36f-6031-4ca4-a3b1-c52098338202&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="Pi 500+ JMT eGPU Dock Setup"></p>

<p>I switched back to my <a href="https://amzn.to/3IBI3XY">JMT eGPU OCuLink dock</a> for the rest of my testing, and uploaded a <a href="https://youtu.be/Dv3RRAx7G6E?t=321">video detailing some of my struggles</a>, and a <a href="https://www.jeffgeerling.com/blog/2025/full-egpu-acceleration-on-pi-500-15-line-patch">blog post detailing the Pi 500+ eGPU testing</a>.</p>

<p>A few commenters mentioned they <em>too</em> had issues with the Minisforum DEG1. But a few of them looked closely at the <em>OCuLink cable</em> Minisforum included, and noted there were a couple extra colored wires going through the cable sleeve that <em>didn't</em> seem to be present on other cables—like the chenyang I was using! They suggested I try swapping cables.</p>

<p>So I did... and testing it with an RX 6500 XT worked!</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/minisforum-dock-working-with-pi-500-plus.jpeg" width="700" height="467" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-eab0557c-ed9e-4acd-86f4-ce6f71c8ada7" data-insert-attach="{&quot;id&quot;:&quot;eab0557c-ed9e-4acd-86f4-ce6f71c8ada7&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="Minisforum dock working with AMD RX 6500 XT eGPU on Raspberry Pi 500+"></p>

<p>Looking closely at the cables side by side, I can confirm what some of the commenters said: the cable that came with the DEG1 looks like it has additional colored wires going between the connectors.</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/oculink-cable-difference-minisforum-colored-wires.jpeg" width="700" height="467" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-c94ceb12-bcc5-4d7e-a735-56c7458e3735" data-insert-attach="{&quot;id&quot;:&quot;c94ceb12-bcc5-4d7e-a735-56c7458e3735&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="OCuLink cable that came with Minisforum DEG1"></p>

<p>Moral of the <em>this</em> portion of the story: not all OCuLink cables are created equal.</p>

<h2>Going Deeper</h2>

<p>But then I swapped back to my RX 7900 XT, the one that was previously unrecognized in the Miniforum dock... and it still wouldn't work.</p>

<pre><code>$ lspci
0002:00:00.0 PCI bridge: Broadcom Inc. and subsidiaries BCM2712 PCIe Bridge (rev 30)
0002:01:00.0 Ethernet controller: Raspberry Pi Ltd RP1 PCIe 2.0 South Bridge
</code></pre>

<p>I tried all three switches in different settings, I tried swapping OCuLink cables back and forth again... nothing. The RX 6500 XT was happy as can be, but the 7900? Nope.</p>

<p>I even popped in an Intel B580 card, and <em>it</em> worked too...</p>

<pre><code>$ lspci
0001:00:00.0 PCI bridge: Broadcom Inc. and subsidiaries BCM2712 PCIe Bridge (rev 30)
0001:01:00.0 PCI bridge: Intel Corporation Device e2ff (rev 01)
0001:02:01.0 PCI bridge: Intel Corporation Device e2f0
0001:02:02.0 PCI bridge: Intel Corporation Device e2f1
0001:03:00.0 VGA compatible controller: Intel Corporation Battlemage G21 [Arc B580]
0001:04:00.0 Audio device: Intel Corporation Device e2f7
0002:00:00.0 PCI bridge: Broadcom Inc. and subsidiaries BCM2712 PCIe Bridge (rev 30)
0002:01:00.0 Ethernet controller: Raspberry Pi Ltd RP1 PCIe 2.0 South Bridge
</code></pre>

<p>So now I'm left scratching my head: <em>what's different about the RX 7900 XT?</em> And <em>why does my cheaper $50 eGPU dock seem to work with everything, but the $99 Minisforum DEG1 doesn't?</em></p>

<p>Searching through forum posts, I even found <a href="https://forums.raspberrypi.com/viewtopic.php?t=387794">someone running a 7900 XT in the DEG1 on a Pi</a>, so maybe it's just a strange fluke with my setup?</p>

<p>Inconsistencies like these really bother me. And they usually eat up an entire afternoon, because I'm always <em>certain</em> it's a PEBKAC, and I usually exhaust every route debugging before I'd waste a vendor or a maintainer's time with a bug report!</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/oculink-amphenol-cable-info.jpg" width="700" height="495" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-3daa4ec4-7400-4afc-8d7c-93639fa17ffa" data-insert-attach="{&quot;id&quot;:&quot;3daa4ec4-7400-4afc-8d7c-93639fa17ffa&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="OCuLink cable dimensions and pinout from Amphenol"></p>

<p>I haven't yet torn down one of these cables to try to figure out which pins are perhaps missing on the chenyang cable (see <a href="https://jpc-pt.com/wp-content/uploads/2017/10/AMPH-OCUL4X-A-RA-XXX-Sent-Out_01.jpg">OCuLink Pinouts here</a>. The bigger issue there is, I can't find a source for the cable Minisforum includes separate from the DEG1 dock, and most online listings don't clearly show which kind of cable you'll get—with or without the extra wires!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vertical Solar Panels Are Out Standing (152 pts)]]></title>
            <link>https://hackaday.com/2025/09/25/vertical-solar-panels-are-out-standing/</link>
            <guid>45414215</guid>
            <pubDate>Mon, 29 Sep 2025 14:22:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hackaday.com/2025/09/25/vertical-solar-panels-are-out-standing/">https://hackaday.com/2025/09/25/vertical-solar-panels-are-out-standing/</a>, See on <a href="https://news.ycombinator.com/item?id=45414215">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
        <main id="main" role="main">

        
            
<article itemscope="" itemtype="http://schema.org/Article" id="post-833943">
    <!-- .entry-header -->

    <div itemprop="articleBody">
        <p>If you’re mounting solar panels, everybody knows the drill, right? Point them south, angled according to latitude. It’s easy. In a video which demonstrates that [Everyday Dave] is truly out standing in his field, we hear a different story. [Dave] has a year’s worth of data in his <a href="https://youtu.be/I-Fz5T5c0OQ" target="_blank">Solar Panel Showdown</a> that suggests there are good reasons to mount your panels vertically.</p>
<p>Specifically, [Dave] is using bifacial solar panels– panels that have cells on both sides. In his preferred orientation, one side faces South, while the other faces North. [Dave] is in the Northern Hemisphere, so those of you Down Under would have to do the opposite, pointing one face North and the other South.</p>
<p>Since [Dave] is far from the equator, the N/S vertical orientation beats the pants off of East-West facing panels, especially in winter. What’s interesting is how much better the bifacial panels do compared to the “standard” tilted orientation. While peak power in the summer is much better with the tilted bifacial panels (indeed, even the tilted single-sided panels), in winter the vertical N/S panels blow them out of the water. (Especially when snow gets involved. Vertical panels don’t need sweeping!)</p>
<p><span id="more-833943"></span>Even in the summer, though, there are advantages: the N/S panels may produce less power overall, but they give a trickle earlier and later in the day than the tilted orientation. Still, that extra peak power really shows, and over a six-month period from solstice-to-solstice, the vertical panels only produced 77% what the tilted bifacial panels did (while tilted single-sided panels produced 90%).</p>
<p>Is it worth it? That depends on your use case. If most of the power is going to A/C, you’ll need the extra in the warmer months. In that case, you want to tilt the panels. If you have a steady, predictable load, though, having even production winter/summer might be more to your liking– in that case you can join [Dave] in sticking solar panels straight up and down.</p>
<p>These results probably apply at latitudes similar to [Dave] who is in cloudy and snowy Ohio, which is perhaps not the ideal place for solar experimentation. If you’re not an Ohio-like distance from the equator, you might find an <a href="https://hackaday.com/2025/05/07/tracking-the-sun-nah/">East-West array is the best bang for the buck</a>. Of course if you really want to max out power from each individual cell, <a href="https://hackaday.com/2024/08/15/the-sunchronizer-keeps-your-solar-panel-aligned/">you can’t beat sun tracking</a> regardless of where you are.</p>

<p><iframe title="Solar Panel Showdown: The Surprising Winner!" width="800" height="450" src="https://www.youtube.com/embed/I-Fz5T5c0OQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
	            </div><!-- .entry-content -->
    
    <!-- .entry-footer -->
</article><!-- #post-## -->

            	<!-- .navigation -->
	
            

            
<!-- #comments -->

        
        

        
        

        
        </main><!-- #main -->
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Electronic Arts Goes Private for $52.5B in Largest LBO (109 pts)]]></title>
            <link>https://www.wsj.com/business/deals/electronic-arts-to-go-private-in-55-billion-deal-a4a4479c</link>
            <guid>45413767</guid>
            <pubDate>Mon, 29 Sep 2025 13:47:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/business/deals/electronic-arts-to-go-private-in-55-billion-deal-a4a4479c">https://www.wsj.com/business/deals/electronic-arts-to-go-private-in-55-billion-deal-a4a4479c</a>, See on <a href="https://news.ycombinator.com/item?id=45413767">Hacker News</a></p>
Couldn't get https://www.wsj.com/business/deals/electronic-arts-to-go-private-in-55-billion-deal-a4a4479c: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Why friction is necessary for growth (152 pts)]]></title>
            <link>https://jameelur.com/blog/overcoming-friction-leads-to-growth</link>
            <guid>45413654</guid>
            <pubDate>Mon, 29 Sep 2025 13:39:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jameelur.com/blog/overcoming-friction-leads-to-growth">https://jameelur.com/blog/overcoming-friction-leads-to-growth</a>, See on <a href="https://news.ycombinator.com/item?id=45413654">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-mdx-content="true"><p><img alt="" loading="lazy" width="1200" height="627" decoding="async" data-nimg="1" srcset="https://jameelur.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ffriction-growth.dcafe3c3.png&amp;w=1200&amp;q=75 1x, https://jameelur.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ffriction-growth.dcafe3c3.png&amp;w=3840&amp;q=75 2x" src="https://jameelur.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ffriction-growth.dcafe3c3.png&amp;w=3840&amp;q=75"></p><p>The title of this article says it all. Overcoming friction leads to growth. Comfort leads to stagnation.</p>
<p>ChatGPT and by extension “AI” is likely the biggest “revolution” of my generation. It is likely also going to be the biggest killer of creativity in my generation. I always thought the creativity killer was going to be access to infinite entertainment. I think I was wrong.</p>
<p>I’ve come to believe that with the rise of convenience and comfort, it becomes harder for us to reach our potential. Technology and Capitalism is taking us towards an extreme.</p>
<p>A certain level of convenience can lead to efficiency gains. Automation is important for a reason. Too much convenience though, that's a killer. When friction was inherent in the system, applying ourselves led to growth as we overcame that friction. We simply didn’t have an alternative that was viable. And this principle applies to everything.</p>
<p>When I was a child in Sri Lanka, I ended up memorizing the landline numbers of all my close relatives. To this day I remember them. The moment I got a phone where my contacts could be saved, I stopped remembering numbers. It may seem like a small thing but it illustrates the principle. The ease of access to information has geared us towards efficiently looking up information instead of remembering it. I won't argue the utility of having hundreds of numbers saved on your phone, I simply want to make a point. Overcoming friction leads to growth.</p>
<p>Let's take another activity where creativity is important, writing. When it's easier to prompt ChatGPT to write your college essay, you'll never apply yourself. Afterall, when everyone is doing it, why not you? As everyone uses ChatGPT, the expectation of high quality writing will increase, making it harder for people to be vulnerable. You can’t become a master without making mistakes and learning from it.</p>
<p>Humans are creatures of comfort. Just like so many things in this world, we follow the path of least resistance. With access to technology being ubiquitous, and ChatGPT being so widely available, to choose not to use it is very hard. You need to deliberately prioritize your growth and choose to go against the current. You need to deliberately introduce friction to the process.</p>
<p>That said, total abstinence is not the solution. ChatGPT is here to stay. Just like most advancements in technology are. As a child of the 21st Century, you’ll need to learn to utilize this new tool in a manner that aids you, not hinders you. More importantly, not hinder the <strong>future</strong> you.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta-analysis of 2.2M people: Loneliness increases mortality risk by 32% (344 pts)]]></title>
            <link>https://lightcapai.medium.com/the-loneliness-epidemic-threatens-physical-health-like-smoking-e063220dde8b</link>
            <guid>45413481</guid>
            <pubDate>Mon, 29 Sep 2025 13:25:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lightcapai.medium.com/the-loneliness-epidemic-threatens-physical-health-like-smoking-e063220dde8b">https://lightcapai.medium.com/the-loneliness-epidemic-threatens-physical-health-like-smoking-e063220dde8b</a>, See on <a href="https://news.ycombinator.com/item?id=45413481">Hacker News</a></p>
Couldn't get https://lightcapai.medium.com/the-loneliness-epidemic-threatens-physical-health-like-smoking-e063220dde8b: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Larry Ellison – 'citizens will be on their best behavior' amid nonstop recording (197 pts)]]></title>
            <link>https://fortune.com/2025/09/28/larry-ellison-ai-surveillance-oracle-tiktok-deal-social-media/</link>
            <guid>45413090</guid>
            <pubDate>Mon, 29 Sep 2025 12:51:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fortune.com/2025/09/28/larry-ellison-ai-surveillance-oracle-tiktok-deal-social-media/">https://fortune.com/2025/09/28/larry-ellison-ai-surveillance-oracle-tiktok-deal-social-media/</a>, See on <a href="https://news.ycombinator.com/item?id=45413090">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-cy="article-content" id="article-content"><p>A year ago, <a href="https://fortune.com/company/oracle/" target="_blank" aria-label="Go to https://fortune.com/company/oracle/">Oracle</a> cofounder and Chairman Larry Ellison described a future where everyone, including law enforcement, <a href="https://fortune.com/2024/09/17/oracle-larry-ellison-surveillance-state-police-ai/" target="_blank" rel="noreferrer noopener" aria-label="Go to https://fortune.com/2024/09/17/oracle-larry-ellison-surveillance-state-police-ai/" data-type="link" data-id="https://fortune.com/2024/09/17/oracle-larry-ellison-surveillance-state-police-ai/">will face regular surveillance</a> as daily life is documented seemingly nonstop.</p><div>



<p>At Oracle’s&nbsp;<a href="https://www.oracle.com/events/financial-analyst-meeting-2024/" target="_blank" rel="noreferrer noopener" aria-label="Go to https://www.oracle.com/events/financial-analyst-meeting-2024/">financial analyst meeting</a>&nbsp;last September, he predicted artificial intelligence will help process the vast amounts of footage recorded by cameras placed on everything from car dashboards and front doors to security systems and cops.



</p><p>“We’re going to have supervision,” Ellison said. “Every police officer is going to be supervised at all times, and if there’s a problem, AI will report that problem and report it to the appropriate person. Citizens will be on their best behavior because we are constantly recording and reporting everything that’s going on.”



</p><p>Those comments have gained fresh relevance now that his company has emerged as a major player in the AI industry and is poised to play a critical role in the deal for TikTok’s U.S. operations. TikTok’s video-sharing platform is among the most popular social media properties in the country.



</p><p>Oracle didn’t immediately respond to a request for comment.</p><p>The company has been an AI infrastructure provider and stunned Wall Street earlier this month by reaching a <a href="https://www.wsj.com/business/openai-oracle-sign-300-billion-computing-deal-among-biggest-in-history-ff27c8fe?st=vUBGuq&amp;reflink=desktopwebshare_permalink" target="_blank" rel="noreferrer noopener" aria-label="Go to https://www.wsj.com/business/openai-oracle-sign-300-billion-computing-deal-among-biggest-in-history-ff27c8fe?st=vUBGuq&amp;reflink=desktopwebshare_permalink" data-type="link" data-id="https://www.wsj.com/business/openai-oracle-sign-300-billion-computing-deal-among-biggest-in-history-ff27c8fe?st=vUBGuq&amp;reflink=desktopwebshare_permalink">$300 billion deal with OpenAI</a>, which will purchase computing power over about five years in one of the largest cloud contracts ever signed. 



</p><p>And earlier this week, OpenAI signed deals with SoftBank and Oracle&nbsp;for new data centers as part of the massive Stargate Project.



</p><p>In its most recent quarterly earnings call with analysts, management also offered&nbsp;revenue projections that cited $455 billion in contracts, up 359% from a year earlier.&nbsp;CEO Safra Catz&nbsp;revealed that Oracle landed deals with three different customers during the quarter.



</p><p>Meanwhile, <a href="https://fortune.com/how-we-test-supplements/" target="_blank" rel="noreferrer noopener" aria-label="Go to https://fortune.com/how-we-test-supplements/" data-type="page" data-id="4319596">Oracle is expected to be among the companies</a> that will buy TikTok’s U.S. business from Chinese parent company ByteDance. In addition, Oracle&nbsp;will spearhead U.S. oversight of the algorithm and security.



</p><p>President Donald Trump on Thursday afternoon signed an executive order clearing the way for a deal to put TikTok in U.S. hands. The <a href="https://fortune.com/2025/09/25/trumps-billionaire-backers-tiktok-deal-murdochs/" target="_blank" rel="noreferrer noopener" aria-label="Go to https://fortune.com/2025/09/25/trumps-billionaire-backers-tiktok-deal-murdochs/" data-type="link" data-id="https://fortune.com/2025/09/25/trumps-billionaire-backers-tiktok-deal-murdochs/">U.S. ownership structure is still being finalized</a>, but Trump said&nbsp;Oracle&nbsp;and Ellison would play a “big” role in managing the app, while conservative media mogul Rupert Murdoch and computer billionaire Michael Dell would sit on the board. Trump hinted that three more “blue-chip” backers are also part of the ownership group.</p><p>Vice President JD Vance said that the algorithm will be “under the control of American investors,” adding that more details would be forthcoming.&nbsp;



</p><p><a href="https://finance.yahoo.com/news/tiktok-algorithm-secured-oracle-trump-100000512.html" target="_blank" rel="noreferrer noopener" aria-label="Go to https://finance.yahoo.com/news/tiktok-algorithm-secured-oracle-trump-100000512.html">Reports earlier this week</a>&nbsp;said&nbsp;Oracle&nbsp;will re-create TikTok’s algorithm and provide a new U.S. version while also ensuring security for users’ data.



</p><p>“This deal will allow for the U.S. to control the app’s algorithm,” Vance said. “It’s actually going to be American-operated all the way.”
</p></div><p><strong>Fortune Global Forum</strong> returns Oct. 26–27, 2025 in Riyadh. CEOs and global leaders will gather for a dynamic, invitation-only event shaping the future of business. <a href="https://conferences.fortune.com/event/global-forum-2025/summary?utm_source=fortunecom&amp;utm_medium=plealink" target="_self" aria-label="Go to https://conferences.fortune.com/event/global-forum-2025/summary?utm_source=fortunecom&amp;utm_medium=plealink">Apply for an invitation.</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EA Announces Agreement to be Acquired by PIF, Silver Lake, and Affinity Partners (249 pts)]]></title>
            <link>https://ir.ea.com/press-releases/press-release-details/2025/EA-Announces-Agreement-to-be-Acquired-by-PIF-Silver-Lake-and-Affinity-Partners-for-55-Billion/default.aspx</link>
            <guid>45413083</guid>
            <pubDate>Mon, 29 Sep 2025 12:50:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ir.ea.com/press-releases/press-release-details/2025/EA-Announces-Agreement-to-be-Acquired-by-PIF-Silver-Lake-and-Affinity-Partners-for-55-Billion/default.aspx">https://ir.ea.com/press-releases/press-release-details/2025/EA-Announces-Agreement-to-be-Acquired-by-PIF-Silver-Lake-and-Affinity-Partners-for-55-Billion/default.aspx</a>, See on <a href="https://news.ycombinator.com/item?id=45413083">Hacker News</a></p>
Couldn't get https://ir.ea.com/press-releases/press-release-details/2025/EA-Announces-Agreement-to-be-Acquired-by-PIF-Silver-Lake-and-Affinity-Partners-for-55-Billion/default.aspx: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[What if I don't want videos of my hobby time available to the world? (621 pts)]]></title>
            <link>https://neilzone.co.uk/2025/09/what-if-i-dont-want-videos-of-my-hobby-time-available-to-the-entire-world/</link>
            <guid>45412419</guid>
            <pubDate>Mon, 29 Sep 2025 11:28:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://neilzone.co.uk/2025/09/what-if-i-dont-want-videos-of-my-hobby-time-available-to-the-entire-world/">https://neilzone.co.uk/2025/09/what-if-i-dont-want-videos-of-my-hobby-time-available-to-the-entire-world/</a>, See on <a href="https://news.ycombinator.com/item?id=45412419">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<article>
    <p>I am very much enjoying my newly-resurrected hobby of Airsoft.</p>
<p>Running around in the woods, firing small plastic pellets at other people, in pursuit of a contrived-to-be-fun mission, turns out to be, well, fun.</p>
<p>I have also had to accept that, for some other players, part of that fun comes from making videos of their game days, and uploading them to YouTube.</p>
<p>They often have quite impressive setups, with multiple cameras - head, rear-facing from barrel of weapon, and scope cam - and clearly put time, money, and effort into doing this.</p>
<p>Great! Just like someone taking photos on their holidays, or when out and about, I can see the fun in it.</p>
<p>It is the “non-consensually publishing it online for the world to see” aspect which bugs me a bit.</p>
<p>In the handful of games that I have played, no-one has ever asked about consent of other participants.</p>
<p>There has been no “put on this purple lanyard if you don’t want to be included in the public version of the video” rule, which I’ve seen work pretty well at conferences I have attended (even if it is opt-out rather than consent).</p>
<p>I could, I suppose, ask each person that I see with a camera “would you mind not including me in anything you upload, please?”. And, since everyone with whom I’ve spoken at games, so far anyway, has been perfectly pleasant and friendly, I’d be hopeful that they would at least consider my request. I have not done this.</p>
<p>The impression I get is that this is just seen as part and parcel of the hobby: by running around in the woods of northern Newbury on a Sunday morning, I need to accept that I may well appear on YouTube, for the world to see.</p>
<p>I don’t love it, but it is not a big enough deal for me to make a fuss.</p>
<h2 id="other-notes">Other notes</h2>
<p>I occasionally see people saying “well, if you don’t want to be in photos published online, don’t be in public spaces”.</p>
<p>This is nonsense, for a number of reasons. Clearly, one should be able to exist in society, including going outside one’s own home, without needing to accept this kind of thing.</p>
<p>In any case, here, the issue is somewhat different, since it is a private site, where people engage in private activity (a hobby).</p>
<p>But then I’ve seen the same at (private) conferences, with people saying “Of course I’m free to take photos of identifiable individuals without their consent and publish them online”.</p>
<p>Publishing someone’s photo online, without their consent, without another strong justification, just because they happen to be in view of one’s camera lens, feels wrong to me.</p>
<p>This isn’t about what is legal (although, in some cases, claims of legality may be poorly conceived), but around my own perceptions of a private life, and a dislike for the fact that, just because one <em>can</em> publish such things, that one <em>should</em>.</p>



  <div>
	<hr>   
	<h2>You may also like:</h2>
	
    
</div>


</article>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek-v3.2-Exp (293 pts)]]></title>
            <link>https://github.com/deepseek-ai/DeepSeek-V3.2-Exp</link>
            <guid>45412098</guid>
            <pubDate>Mon, 29 Sep 2025 10:26:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp">https://github.com/deepseek-ai/DeepSeek-V3.2-Exp</a>, See on <a href="https://news.ycombinator.com/item?id=45412098">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">DeepSeek-V3.2-Exp</h2><a id="user-content-deepseek-v32-exp" aria-label="Permalink: DeepSeek-V3.2-Exp" href="#deepseek-v32-exp"></a></p>



<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true"><img src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3"></a>
</p>
<hr>
<p><a href="https://www.deepseek.com/" rel="nofollow">
    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/badge.svg?raw=true">
  </a>
  <a href="https://chat.deepseek.com/" rel="nofollow">
    <img alt="Chat" src="https://camo.githubusercontent.com/00ba04aacbe45f97b5ebcc3d1b9c0f546e0ce3981265e97a110994184ef67fc8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa496253230436861742d446565705365656b25323056332d3533366166353f636f6c6f723d353336616635266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&amp;logoColor=white">
  </a>
  <a href="https://huggingface.co/deepseek-ai" rel="nofollow">
    <img alt="Hugging Face" src="https://camo.githubusercontent.com/5e3115539d4583e22d65cb89eb1759e767cb9e1d70772923292fcfc80a654be4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d446565705365656b25323041492d6666633130373f636f6c6f723d666663313037266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;logoColor=white">
  </a>
</p>
<p><a href="https://discord.gg/Tc7c45Zzu5" rel="nofollow">
    <img alt="Discord" src="https://camo.githubusercontent.com/e227481a149714ed5187e4fd0b60b9f736099c2dd2083e6c091e29f1446cbb1a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d446565705365656b25323041492d3732383964613f6c6f676f3d646973636f7264266c6f676f436f6c6f723d776869746526636f6c6f723d373238396461" data-canonical-src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;logoColor=white&amp;color=7289da">
  </a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true">
    <img alt="Wechat" src="https://camo.githubusercontent.com/562efc618da65f0a69bc804395005b8124f5c2ed2eb73441c4e359185cc01467/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5765436861742d446565705365656b25323041492d627269676874677265656e3f6c6f676f3d776563686174266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;logoColor=white">
  </a>
  <a href="https://twitter.com/deepseek_ai" rel="nofollow">
    <img alt="Twitter Follow" src="https://camo.githubusercontent.com/8272710ecd020c821b4f62c1c455efb89e0db4eb179c5f5f971c3c1f69452c54/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f547769747465722d646565707365656b5f61692d77686974653f6c6f676f3d78266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;logoColor=white">
  </a>
</p>
<p><a href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/LICENSE">
    <img alt="License" src="https://camo.githubusercontent.com/1af067540f64107f8fe7715d12150fb910e21f0d2c6aa0c319087c7510c8b934/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d6635646535333f26636f6c6f723d663564653533" data-canonical-src="https://img.shields.io/badge/License-MIT-f5de53?&amp;color=f5de53">
  </a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">We are excited to announce the official release of DeepSeek-V3.2-Exp, an experimental version of our model. As an intermediate step toward our next-generation architecture, V3.2-Exp builds upon V3.1-Terminus by introducing DeepSeek Sparse Attention—a sparse attention mechanism designed to explore and validate optimizations for training and inference efficiency in long-context scenarios.</p>
<p dir="auto">This experimental release represents our ongoing research into more efficient transformer architectures, particularly focusing on improving computational efficiency when processing extended text sequences.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/cost.jpg"><img src="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/raw/main/cost.jpg"></a>
</p>
<ul dir="auto">
<li>
<p dir="auto">DeepSeek Sparse Attention (DSA) achieves fine-grained sparse attention for the first time, delivering substantial improvements in long-context training and inference efficiency while maintaining virtually identical model output quality.</p>
</li>
<li>
<p dir="auto">To rigorously evaluate the impact of introducing sparse attention, we deliberately aligned the training configurations of DeepSeek-V3.2-Exp with V3.1-Terminus. Across public benchmarks in various domains, DeepSeek-V3.2-Exp demonstrates performance on par with V3.1-Terminus.</p>
</li>
</ul>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Benchmark</th>
<th>DeepSeek-V3.1-Terminus</th>
<th>DeepSeek-V3.2-Exp</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Reasoning Mode w/o Tool Use</strong></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MMLU-Pro</td>
<td>85.0</td>
<td>85.0</td>
</tr>
<tr>
<td>GPQA-Diamond</td>
<td>80.7</td>
<td>79.9</td>
</tr>
<tr>
<td>Humanity's Last Exam</td>
<td>21.7</td>
<td>19.8</td>
</tr>
<tr>
<td>LiveCodeBench</td>
<td>74.9</td>
<td>74.1</td>
</tr>
<tr>
<td>AIME 2025</td>
<td>88.4</td>
<td>89.3</td>
</tr>
<tr>
<td>HMMT 2025</td>
<td>86.1</td>
<td>83.6</td>
</tr>
<tr>
<td>Codeforces</td>
<td>2046</td>
<td>2121</td>
</tr>
<tr>
<td>Aider-Polyglot</td>
<td>76.1</td>
<td>74.5</td>
</tr>
<tr>
<td><strong>Agentic Tool Use</strong></td>
<td></td>
<td></td>
</tr>
<tr>
<td>BrowseComp</td>
<td>38.5</td>
<td>40.1</td>
</tr>
<tr>
<td>BrowseComp-zh</td>
<td>45.0</td>
<td>47.9</td>
</tr>
<tr>
<td>SimpleQA</td>
<td>96.8</td>
<td>97.1</td>
</tr>
<tr>
<td>SWE Verified</td>
<td>68.4</td>
<td>67.8</td>
</tr>
<tr>
<td>SWE-bench Multilingual</td>
<td>57.8</td>
<td>57.9</td>
</tr>
<tr>
<td>Terminal-bench</td>
<td>36.7</td>
<td>37.7</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Open-Source Kernels</h2><a id="user-content-open-source-kernels" aria-label="Permalink: Open-Source Kernels" href="#open-source-kernels"></a></p>
<p dir="auto">For TileLang kernels with <strong>better readability and research-purpose design</strong>, please refer to <a href="https://github.com/tile-ai/tilelang/tree/main/examples/deepseek-v32">TileLang</a>.</p>
<p dir="auto">For <strong>high-performance CUDA kernels</strong>, indexer logit kernels (including paged versions) are available in <a href="https://github.com/deepseek-ai/DeepGEMM/pull/200" data-hovercard-type="pull_request" data-hovercard-url="/deepseek-ai/DeepGEMM/pull/200/hovercard">DeepGEMM</a>. Sparse attention kernels are released in <a href="https://github.com/deepseek-ai/FlashMLA/pull/98" data-hovercard-type="pull_request" data-hovercard-url="/deepseek-ai/FlashMLA/pull/98/hovercard">FlashMLA</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to Run Locally</h2><a id="user-content-how-to-run-locally" aria-label="Permalink: How to Run Locally" href="#how-to-run-locally"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">HuggingFace</h3><a id="user-content-huggingface" aria-label="Permalink: HuggingFace" href="#huggingface"></a></p>
<p dir="auto">We provide an updated inference demo code in the <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference" rel="nofollow">inference</a> folder to help the community quickly get started with our model and understand its architectural details.</p>
<p dir="auto">First convert huggingface model weights to the the format required by our inference demo. Set <code>MP</code> to match your available GPU count:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd inference
export EXPERTS=256
python convert.py --hf-ckpt-path ${HF_CKPT_PATH} --save-path ${SAVE_PATH} --n-experts ${EXPERTS} --model-parallel ${MP}"><pre><span>cd</span> inference
<span>export</span> EXPERTS=256
python convert.py --hf-ckpt-path <span>${HF_CKPT_PATH}</span> --save-path <span>${SAVE_PATH}</span> --n-experts <span>${EXPERTS}</span> --model-parallel <span>${MP}</span></pre></div>
<p dir="auto">Launch the interactive chat interface and start exploring DeepSeek's capabilities:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export CONFIG=config_671B_v3.2.json
torchrun --nproc-per-node ${MP} generate.py --ckpt-path ${SAVE_PATH} --config ${CONFIG} --interactive"><pre><span>export</span> CONFIG=config_671B_v3.2.json
torchrun --nproc-per-node <span>${MP}</span> generate.py --ckpt-path <span>${SAVE_PATH}</span> --config <span>${CONFIG}</span> --interactive</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">SGLang</h3><a id="user-content-sglang" aria-label="Permalink: SGLang" href="#sglang"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Installation with Docker</h4><a id="user-content-installation-with-docker" aria-label="Permalink: Installation with Docker" href="#installation-with-docker"></a></p>
<div data-snippet-clipboard-copy-content="# H200
docker pull lmsysorg/sglang:dsv32

# MI350
docker pull lmsysorg/sglang:dsv32-rocm

# NPUs
docker pull lmsysorg/sglang:dsv32-a2
docker pull lmsysorg/sglang:dsv32-a3"><pre><code># H200
docker pull lmsysorg/sglang:dsv32

# MI350
docker pull lmsysorg/sglang:dsv32-rocm

# NPUs
docker pull lmsysorg/sglang:dsv32-a2
docker pull lmsysorg/sglang:dsv32-a3
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Launch Command</h4><a id="user-content-launch-command" aria-label="Permalink: Launch Command" href="#launch-command"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m sglang.launch_server --model deepseek-ai/DeepSeek-V3.2-Exp --tp 8 --dp 8 --page-size 64"><pre>python -m sglang.launch_server --model deepseek-ai/DeepSeek-V3.2-Exp --tp 8 --dp 8 --page-size 64</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">vLLM</h3><a id="user-content-vllm" aria-label="Permalink: vLLM" href="#vllm"></a></p>
<p dir="auto">vLLM provides day-0 support of DeepSeek-V3.2-Exp. See the <a href="https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-V3_2-Exp.html" rel="nofollow">recipes</a> for up-to-date details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This repository and the model weights are licensed under the <a href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/LICENSE">MIT License</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<div data-snippet-clipboard-copy-content="@misc{deepseekai2024deepseekv32,
      title={DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention}, 
      author={DeepSeek-AI},
      year={2025},
}"><pre><code>@misc{deepseekai2024deepseekv32,
      title={DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention}, 
      author={DeepSeek-AI},
      year={2025},
}
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contact</h2><a id="user-content-contact" aria-label="Permalink: Contact" href="#contact"></a></p>
<p dir="auto">If you have any questions, please raise an issue or contact us at <a href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/service@deepseek.com">service@deepseek.com</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Optimizing a 6502 image decoder, from 70 minutes to 1 minute (178 pts)]]></title>
            <link>https://www.colino.net/wordpress/en/archives/2025/09/28/optimizing-a-6502-image-decoder-from-70-minutes-to-1-minute/</link>
            <guid>45412022</guid>
            <pubDate>Mon, 29 Sep 2025 10:11:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.colino.net/wordpress/en/archives/2025/09/28/optimizing-a-6502-image-decoder-from-70-minutes-to-1-minute/">https://www.colino.net/wordpress/en/archives/2025/09/28/optimizing-a-6502-image-decoder-from-70-minutes-to-1-minute/</a>, See on <a href="https://news.ycombinator.com/item?id=45412022">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<p><span><i></i>2025/09/28</span>
						<span> - </span>
						<span><i></i>About 8 minutes read</span>
					</p>
					
				</div><div>	
						
<p>When I set out to write a program that would allow me to do basic digital photography on the Apple II, I decided I would do it with the Quicktake cameras. It seemed the obvious choice as they were Apple cameras, and their interface to the computer is via serial port.</p>



<p>The scope creeped a bit after managing to decode Quicktake 100 photos. I wanted it to be able to decode Quicktake 150 and Quicktake 200 pictures too. This threw me into image processing much more than I initially wanted to. This article explains the process of how I got the Quicktake 150 decoder to a reasonabl-ish speed on a 6502 at 1MHz.</p>



<p>The Quicktake 150 format is proprietary and undocumented. Free software decoders exist though, in the dcraw project. This was my source for the initial Apple II decoder. Sadly, it is written in C, is extremely non-documented, and is extremely hard to read and understand (to me). The compression is based on Huffman coding, with variable-length codes (which means bit-shifting), and the image construction involves a lot of 16-bits math. None of this is good on a 6502.</p>



<p>But first I had to rework the original algorithm to work with bands of 20 pixels, for memory reasons. Once I had a functional decoder, it ran perfectly, but it took… seventy minutes to decode a single picture.</p>



<figure><a href="https://www.colino.net/wordpress/wp-content/uploads/QT150_24.png"><img fetchpriority="high" decoding="async" width="640" height="480" src="https://www.colino.net/wordpress/wp-content/uploads/QT150_24.png" alt="" srcset="https://www.colino.net/wordpress/wp-content/uploads/QT150_24.png 640w, https://www.colino.net/wordpress/wp-content/uploads/QT150_24-400x300.png 400w, https://www.colino.net/wordpress/wp-content/uploads/QT150_24-150x113.png 150w" sizes="(max-width: 640px) 100vw, 640px"></a><figcaption>A Quicktake 150 picture, fully decoded by dcraw</figcaption></figure>



<figure><a href="https://www.colino.net/wordpress/wp-content/uploads/TEST150.DGHR_.png"><img decoding="async" width="560" height="384" src="https://www.colino.net/wordpress/wp-content/uploads/TEST150.DGHR_.png" alt="" srcset="https://www.colino.net/wordpress/wp-content/uploads/TEST150.DGHR_.png 560w, https://www.colino.net/wordpress/wp-content/uploads/TEST150.DGHR_-400x274.png 400w, https://www.colino.net/wordpress/wp-content/uploads/TEST150.DGHR_-150x103.png 150w" sizes="(max-width: 560px) 100vw, 560px"></a><figcaption>The same picture, dithered for display on a monochrome Apple II</figcaption></figure>



<p>Of course, I didn’t get there that fast. My first implementation was released two years ago, in November 2023. Getting where I’m now took, I think, five or six deep dives with each time, one or two weeks worth of late evenings and full week-ends dedicated to progressing, wading through hundreds or thousands of debug printf()s, gdb’ing, variables and offsets comparisons, etc.</p>



<p>Follow me through the algorithmic iterations that allowed me to get that decoding time to under one minute. My implementation is now full assembly, but the commits I will link to here are to the general decoding algorithm, that is easier to read in C. </p>



<p>I have noticed that hand-optimizing assembler yields good results, but usually optimizing the algorithm itself leads to much more impressive speed gains. Doing <strong>too many things</strong> faster is not as good as doing <strong>the minimum</strong> faster. And that Quicktake 150 decoder sure did useless things, especially in my case where I don’t care about color and end up with a 256×192 image!</p>



<p>I have made a specific repository to track these algorithmic changes. It started <a href="https://github.com/colinleroy/qtkn_decoder/commit/ceef883b4528a01658f446f9fde233d8846b98e0#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3" data-type="link" data-id="https://github.com/colinleroy/qtkn_decoder/commit/ceef883b4528a01658f446f9fde233d8846b98e0#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3">here</a> (already a little bit deobfuscated <a href="https://github.com/ncruces/dcraw/blob/8fe4d3825595816ea7d6880d9253f9edd143cacb/dcraw.c#L2198">compared to dcraw</a>), at 301 millions x86_64 instructions.</p>



<p><strong>Dropping color</strong></p>



<p>I didn’t have to decode color at all, as I was going to drop it, anyway. <a href="https://github.com/colinleroy/qtkn_decoder/commit/615ecd7406a65dd43da07443cb5c3f93a8d5fa95#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3R217">I added a flag to only decode the green pixels out of the Bayer matrix, and drop the rest</a>. 264M instructions.</p>



<p><strong>Understanding the buffers</strong></p>



<p>I then set out to understand the use of the various temporary buffers: the more buffers, the more intermediary steps, the more copy and looping. I wanted to drop as much of them as possible. <a href="https://github.com/colinleroy/qtkn_decoder/commit/fd6cd2c44d1cc961f8827ec6b5958521543e6d8d#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3L244">The first step towards it was unrolling some little imbricated loops that worked on y [1,2], x [col+1,col].</a> 238M instructions.</p>



<p>I figured I still had extra processing I didn’t need, <a href="https://github.com/colinleroy/qtkn_decoder/commit/73935481468c2a1d20a4640352caed4e47308b54#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3">removed it, dropped a buffer</a> (and dropped the #ifdef COLOR conditional to make things clearer). 193M instructions.</p>



<figure><a href="https://www.colino.net/wordpress/wp-content/uploads/image-green.png"><img decoding="async" width="640" height="480" src="https://www.colino.net/wordpress/wp-content/uploads/image-green.png" alt="" srcset="https://www.colino.net/wordpress/wp-content/uploads/image-green.png 640w, https://www.colino.net/wordpress/wp-content/uploads/image-green-400x300.png 400w, https://www.colino.net/wordpress/wp-content/uploads/image-green-150x113.png 150w" sizes="(max-width: 640px) 100vw, 640px"></a><figcaption>Our image looks like this now</figcaption></figure>



<p>At that point, my implementation still outputted green pixels only in a Bayer matrix to a 640×480 buffer, and then interpolated them. It was useless, so <a href="https://github.com/colinleroy/qtkn_decoder/commit/75a45d8fdf6943cc26a56c3f53c21f57e2877bbf#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3L263" data-type="link" data-id="https://github.com/colinleroy/qtkn_decoder/commit/75a45d8fdf6943cc26a56c3f53c21f57e2877bbf#diff-3564ad20c962b9eec56839b1adac221e589cda13b85547ac3d409396e9b6a28c">I dropped that entirely</a>. 29M instructions.</p>



<figure><a href="https://www.colino.net/wordpress/wp-content/uploads/out-grayscale-large.png"><img loading="lazy" decoding="async" width="640" height="480" src="https://www.colino.net/wordpress/wp-content/uploads/out-grayscale-large.png" alt="" srcset="https://www.colino.net/wordpress/wp-content/uploads/out-grayscale-large.png 640w, https://www.colino.net/wordpress/wp-content/uploads/out-grayscale-large-400x300.png 400w, https://www.colino.net/wordpress/wp-content/uploads/out-grayscale-large-150x113.png 150w" sizes="auto, (max-width: 640px) 100vw, 640px"></a><figcaption>Without interpolating, we can clearly see we only have half the pixels.</figcaption></figure>



<p>I still had half the pixels black in the destination buffer, so I dropped them earlier rather than later, by <a href="https://github.com/colinleroy/qtkn_decoder/commit/fdad1e8efb45e7597659b6c7d08e0a7714094d59#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3L130" data-type="link" data-id="https://github.com/colinleroy/qtkn_decoder/commit/fdad1e8efb45e7597659b6c7d08e0a7714094d59">outputting a 320×240 images with only the relevant pixels</a>. 25M instructions.</p>



<figure><a href="https://www.colino.net/wordpress/wp-content/uploads/image-119.png"><img loading="lazy" decoding="async" width="640" height="480" src="https://www.colino.net/wordpress/wp-content/uploads/image-119.png" alt="" srcset="https://www.colino.net/wordpress/wp-content/uploads/image-119.png 640w, https://www.colino.net/wordpress/wp-content/uploads/image-119-400x300.png 400w, https://www.colino.net/wordpress/wp-content/uploads/image-119-150x113.png 150w" sizes="auto, (max-width: 640px) 100vw, 640px"></a><figcaption>The 8-bits grayscale buffer at that point</figcaption></figure>



<p>At this point I was able to figure out that out of the three buf_m[3], only <a href="https://github.com/colinleroy/qtkn_decoder/commit/b3ac5d70edf0e836aa4b784fc06093d2606f7747">buf_m[1] was used to construct the picture</a>, that <a href="https://github.com/colinleroy/qtkn_decoder/commit/8b6a1c65011645c810f1a9fbb7d9eb56fe3e702e">buf_m[2] was only used to feed back into buf_m[0] at the start of a row</a>, that I could <a href="https://github.com/colinleroy/qtkn_decoder/commit/b44aac77d966c292f0762165cdf5b0818a8a0f91">construct the image from the buf_m[1] values on the fly instead of doing an extra loop on it</a>, and that <a href="https://github.com/colinleroy/qtkn_decoder/commit/4142412bda1388b6fadb5d343de07605c1e93ae8">I could entirely drop it too</a>. This allowed me to <a href="https://github.com/colinleroy/qtkn_decoder/commit/5161e9e33e5c3f064b0a3c0e5d29e6e7ece4280f" data-type="link" data-id="https://github.com/colinleroy/qtkn_decoder/commit/5161e9e33e5c3f064b0a3c0e5d29e6e7ece4280f">rename the last remaining buffer for more clarity</a>. 22M instructions. </p>



<p><strong>Optimizing divisions</strong></p>



<p>That was about it for the buffers. The rework of the code, at that point, made clear that every final pixel value was computed by dividing the 16-bits values computed from the image data by a given factor, and that this factor changes at most once every two rows. The result of that division was then clamped to [0-255]. This allowed me to <a href="https://github.com/colinleroy/qtkn_decoder/commit/865ca287f15bfccbca909b464b5939d8c4368f5a">precompute a division table every two rows, storing the final result, pre-clamped, in a simple array.</a> This also came with a bit of non-visible precision loss. On an x86_64, still 22M instructions, but on 6502, this was a huge gain, transforming 153600 divisions into less than 2000.</p>



<p>I verified the precision loss was acceptable using a small ad-hoc tool displaying my output buffers and comparing the reference decoding to the approximated one. Pixel values differ by at most 1.</p>



<figure><a href="https://www.colino.net/wordpress/wp-content/uploads/image-122.png"><img loading="lazy" decoding="async" width="1541" height="386" src="https://www.colino.net/wordpress/wp-content/uploads/image-122.png" alt="" srcset="https://www.colino.net/wordpress/wp-content/uploads/image-122.png 1541w, https://www.colino.net/wordpress/wp-content/uploads/image-122-400x100.png 400w, https://www.colino.net/wordpress/wp-content/uploads/image-122-150x38.png 150w, https://www.colino.net/wordpress/wp-content/uploads/image-122-768x192.png 768w, https://www.colino.net/wordpress/wp-content/uploads/image-122-1536x385.png 1536w, https://www.colino.net/wordpress/wp-content/uploads/image-122-1280x321.png 1280w" sizes="auto, (max-width: 1541px) 100vw, 1541px"></a><figcaption>Left: the normal output; middle: the comparison; right: the output with approximated divisions</figcaption></figure>



<p><strong>Output index</strong></p>



<p>So far we set the output buffer using the usual <strong>buffer[y*WIDTH+x]</strong> access method, which is really slow on a processor with no multiplication support. <a href="https://github.com/colinleroy/qtkn_decoder/commit/7112b2b96e3d249f4a65532035dff9a8708bc914">I changed that to a much simpler line-by-line indexing</a>. (Even on x86_64, the change is notable: 20M instructions).</p>



<p><strong>Huffman decoding</strong></p>



<p>The algorithm initialized full tables so that it was possible to get a Huffman code by just looking at the bitbuffer: for code <strong>10001</strong>, for example, all codes from <strong>10001000</strong> to <strong>10001111</strong> were matched to the correct value, then the bitbuffer shifted &lt;&lt;5. This seems good at first, but not on 6502, as this requires a 16-bits bitbuffer to make sure we always have a full byte to look at. <a href="https://github.com/colinleroy/qtkn_decoder/commit/b0179e034a4396b8b6d09d489608bf0080e7b8d6">I reworked that to get bits one at a time</a>. This made the x86_64 implementation slower, but the 6502 one 20 seconds faster, spending 9 seconds shifting bits instead of 29. It also allowed me to pack the tables more tight, freeing up some memory for the cache.</p>



<p><strong>Assembly</strong></p>



<p><a href="https://github.com/colinleroy/qtkn_decoder/blob/b0179e034a4396b8b6d09d489608bf0080e7b8d6/qtkn-decoder.c">This algorithm</a> still performs very poorly when compiled by cc65, but is far easier to manually translate into optimized 6502 assembly. There are also a lot of ad-hoc optimisations, for example:</p>



<ul>
<li>The division factor for final pixel values for a pair of rows is 48 more than 50% of the time, on any image I tested. So the 6502 implementation has two divisions lookup tables, one for 48 that is never recomputed, one for another factor, recomputed if needed at the start of a pair of rows.</li>



<li>The row initialization multiplies all 320 next_line values by a factor, which is 255 about 66% of the time. In this case, instead of multiplying a = a*255, the assembly version does (a*256)-a, which is (a&lt;&lt;8)-a, which is much faster.</li>



<li>There is a whole lot of &lt;&lt;4 going on in the main loop, which is lookup-table based in the assembly implementation. &lt;&lt;4 is larger than 8 bits, so there are two tables needed, but it still is worth the memory usage.</li>



<li>Half the Huffman codes read are discarded (they are used for blue and red pixels), so “discarder” functions are used in that case, only shifting the bitbuffer without fetching the value.</li>



<li>Buffers accesses (to next_line and output buffer) are patched in self-modifying code rather than using zero-page pointers, which require to keep track and patch about 54 labels on each page cross. This is ugly as hell, but this requires about 50k cycles per image, but spares 9M cycles overall.</li>
</ul>



<p><strong>The final code</strong></p>



<p>I have pointed to commits to my “test” repository so far, but if you’re interested in the actual 6502 implementation, you can find it in my repository: <a href="https://github.com/colinleroy/a2tools/blob/master/src/quicktake/qtkn_platform.s">the decoder</a>, and <a href="https://github.com/colinleroy/a2tools/blob/master/src/quicktake/qtk_bithuff.s">the bitbuffer</a>.</p>



<p><strong>Questions remain</strong></p>



<p>There still are things I don’t understand in dcraw’s decoder, that my simplifications didn’t uncover. The main thing I wonder is, how did Dave Coffin, dcraw’s author, implement this decoder first? It seems so full of “magic” numbers and arithmetic operations that I have no idea how one would look at pictures at the bit level, and figure out <em>anything</em> about the format. Did he reverse-engineer Apple’s binary? Did he have documentation? Is it in fact a common kind of encoding I have no idea about?</p>



<p>I would love to see documentation of this format, maybe I would understand more and be able to progress further.</p>



<p><strong>Bonus: first and current implementation video</strong></p>



<figure><video controls="" src="https://www.colino.net/wordpress/wp-content/uploads/qt150-decoding-web.mp4"></video><figcaption>The current implementation</figcaption></figure>



<figure><video controls="" src="https://www.colino.net/wordpress/wp-content/uploads/qt150-original.mp4"></video><figcaption>The first implementation (are you patient?)</figcaption></figure>
																
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google appears to have deleted its political ad archive for the EU (248 pts)]]></title>
            <link>https://www.thebriefing.ie/google-just-erased-7-years-of-our-political-history/</link>
            <guid>45411332</guid>
            <pubDate>Mon, 29 Sep 2025 07:57:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thebriefing.ie/google-just-erased-7-years-of-our-political-history/">https://www.thebriefing.ie/google-just-erased-7-years-of-our-political-history/</a>, See on <a href="https://news.ycombinator.com/item?id=45411332">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    <article>

        <header>

            


                <figure>
        <img srcset="https://www.thebriefing.ie/content/images/size/w320/2025/09/Briefing-1-1.png 320w,
                    https://www.thebriefing.ie/content/images/size/w600/2025/09/Briefing-1-1.png 600w,
                    https://www.thebriefing.ie/content/images/size/w960/2025/09/Briefing-1-1.png 960w,
                    https://www.thebriefing.ie/content/images/size/w1200/2025/09/Briefing-1-1.png 1200w,
                    https://www.thebriefing.ie/content/images/size/w2000/2025/09/Briefing-1-1.png 2000w" sizes="(max-width: 1200px) 100vw, 1120px" src="https://www.thebriefing.ie/content/images/size/w1200/2025/09/Briefing-1-1.png" alt="Google just erased 7 years of our political history">
    </figure>

        </header>

        <section>
            <p>Google appears to have deleted its political ad archive for the EU; so the last 7 years of ads, of political spending, of messaging, of targeting - on YouTube, on Search and for display ads - for countless elections across 27 countries - is all gone. </p><p>We had been told that Google would try to stop people placing political ads, a "ban" that was to come into effect this week. I did not read anywhere that this would mean the erasure of this archive of our political history. </p><p>When you go to the Google Ad Archive (which you can here: <a href="https://adstransparency.google.com/?region=IE&amp;ref=thebriefing.ie">https://adstransparency.google.com/</a>) until last week you could search all political ads shown in your country by a date range of your choosing going back to 2018. You could browse all the ads in that range, or search for keywords, candidates, parties. You could view each ad - watch the video, see the images - who had been targeted, how much had been spent etc. </p><p>Now when you try to click on "political ads" you get re-directed to a page asking you to select from a small number of countries - the US, of course, UK, India, Australia, Brazil, Israel - but not one EU country (see below): </p><figure><img src="https://www.thebriefing.ie/content/images/2025/09/Screenshot-2025-09-28-at-10.38.47.png" alt="" loading="lazy" width="585" height="819"><figcaption><span>The page you are diverted too if you try to find political ads on Google. </span></figcaption></figure><p>The political ad archive - now deleted? - allowed people like me (and many others) to understand what happened in elections, like this longer piece I was able to write during the European &amp; Local elections last year on the use of YouTube by a far right party, Sinn Féin's big push on search result ads, and the growth of attacks ads in Ireland: </p><figure><a href="https://www.thebriefing.ie/campaign-round-up-google-youtube-ad-spend/"><div><p>Campaign round up: Irish Freedom Party dominating YouTube ad spend</p><p>The Irish Freedom Party spent €4,100 in the past 3 weeks on YouTube ads, the biggest spenders on YouTube ads this election.</p><p><img src="https://www.thebriefing.ie/content/images/icon/tb-logos-61.png" alt=""><span>Liz Carolan</span></p></div><p><img src="https://www.thebriefing.ie/content/images/thumbnail/Website-1920x1080-7.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Now you need the specific name of an advertiser, and when I looked for, for example, "Sinn Fein", it (a) only gave me the option of searching for their website, and (b) showed zero results. This is despite Sinn Fein spending upwards of <a href="https://www.thebriefing.ie/ad-spend-roundup-and-last-minute-shenanigans/" rel="noreferrer">€10k a day during some of the elections last year</a>. </p><figure><img src="https://www.thebriefing.ie/content/images/2025/09/Screenshot-2025-09-28-at-11.17.51.png" alt="" loading="lazy" width="850" height="496" srcset="https://www.thebriefing.ie/content/images/size/w600/2025/09/Screenshot-2025-09-28-at-11.17.51.png 600w, https://www.thebriefing.ie/content/images/2025/09/Screenshot-2025-09-28-at-11.17.51.png 850w" sizes="(min-width: 720px) 720px"><figcaption><span>Google no longer share any data on Irish political party and candidate spending on political ads on their platform; as shown here, a search for "Sinn Fein" returns "no ads found" </span></figcaption></figure><p>Some good things have been written about the impact that an ad "ban" might have on campaigning, especially when the algorithm still dominates all major social platforms (see this <a href="https://www.liberties.eu/en/stories/meta-google-advertising-bans/45513?ref=thebriefing.ie" rel="noreferrer">great piece</a> by the Civil Liberties Union for Europe), </p><p>But the ad archives were introduced 7 years ago for a reason - in no small part because of the chaos of the Brexit and Trump 2016 votes, and our own advocacy here in Ireland about interference in the 2018 8th amendment referendum. </p><p>They were introduced to allow for scrutiny of campaigns, and also to provide a historical record so we could go back and look at what had been promised, and what had been spent, and to see if this lined up with what happened later. </p><p>This erasure of our political past feels dangerous, for scrutiny, for accountability, for shared memory, for enforcement of our rules - for our democracy. </p><h2 id="icymi">ICYMI </h2><figure><a href="https://www.thebriefing.ie/campaigning-likes-it-1999-when-it-feels-like-its-1938/"><div><p>Campaigning likes it 1999 (when it feels like its 1938)</p><p>How do you run a political campaign without digital ads? I asked two of our best campaign gurus what we might expect.</p><p><img src="https://www.thebriefing.ie/content/images/icon/tb-logos-62.png" alt=""><span>The Briefing</span><span>Liz Carolan</span></p></div><p><img src="https://www.thebriefing.ie/content/images/thumbnail/Briefing-11.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://www.thebriefing.ie/musks-profoundly-ignorant-and-dangerous-intervention-in-aras25/"><div><p>Musk’s profoundly ignorant and dangerous intervention in #Aras25</p><p>I really, really hope this isn’t the start of a whole thing. Elon Musk has tweeted about the race for the Aras in the most profoundly ignorant and stupidly dangerous way possible. The tweet is screenshot below; in it Musk sub-tweets another tweet claiming Simon Harris “ordered his Party to</p><p><img src="https://www.thebriefing.ie/content/images/icon/tb-logos-63.png" alt=""><span>The Briefing</span><span>Liz Carolan</span></p></div><p><img src="https://www.thebriefing.ie/content/images/thumbnail/thebriefing_elon-2.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://www.thebriefing.ie/the-first-aras25-ads-are-out/"><div><p>The first #Aras25 ads are out</p><p>What the first political ads tell us about how the campaign might look</p><p><img src="https://www.thebriefing.ie/content/images/icon/tb-logos-64.png" alt=""><span>The Briefing</span><span>Liz Carolan</span></p></div><p><img src="https://www.thebriefing.ie/content/images/thumbnail/Briefing-1-1.png" alt="" onerror="this.style.display = 'none'"></p></a></figure>
        </section>

    </article>

        

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What is "good taste" in software engineering? (305 pts)]]></title>
            <link>https://www.seangoedecke.com/taste/</link>
            <guid>45410940</guid>
            <pubDate>Mon, 29 Sep 2025 06:41:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.seangoedecke.com/taste/">https://www.seangoedecke.com/taste/</a>, See on <a href="https://news.ycombinator.com/item?id=45410940">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p>Technical taste is different from technical skill. You can be technically strong but have bad taste, or technically weak with good taste. Like taste in general, technical taste sometimes runs ahead of your ability: just like you can tell good food from bad without being able to cook, you can know what kind of software you like before you’ve got the ability to build it. You can develop technical ability by study and repetition, but good taste is developed in a more mysterious way.</p>
<p>Here are some indicators of software taste:</p>
<ul>
<li>What kind of code “looks good” to you? What kind of code “looks ugly”?</li>
<li>Which design decisions you feel really good about, and which ones are just fine?</li>
<li>Which software problems really bother you, to the point where you’re worrying about them outside of work? Which problems can you just brush off?</li>
</ul>
<p>I think taste is <strong>the ability to adopt the set of engineering values that fit your current project</strong>.</p>
<h3>Why taste is different from skill</h3>
<p>Aren’t the indicators above just a part of skill? For instance, doesn’t code look good <em>if it’s good code</em>? I don’t think so.</p>
<p>Let’s take an example. Personally, I feel like code that uses map and filter looks nicer than using a for loop. It’s tempting to think that this is a case of me being straightforwardly correct about a point of engineering. For instance, map and filter typically involve pure functions, which are easier to reason about, and they avoid an entire class of off-by-one iterator bugs. It feels to me like this isn’t a matter of taste, but a case where I’m right and other engineers are wrong.</p>
<p>But of course it’s more complicated than that. Languages like Golang don’t contain map and filter at all, for principled reasons. Iterating with a for loop is easier to reason about from a performance perspective, and is more straightforward to extend to other iteration strategies (like taking two items at a time). I don’t care about these reasons as much as I care about the reasons in favour of map and filter - that’s why I don’t write a lot of for loops - but it would be far too arrogant for me to say that engineers who prefer for loops are simply less skilled. In many cases, they have technical capabilites that I don’t have. They just care about different things.</p>
<p>In other words, our disagreement comes down to a difference in <em>values</em>. I wrote about this point in <a href="https://www.seangoedecke.com/confidence"><em>I don’t know how to build software and you don’t either</em></a>. Even if the big technical debates do have definite answers, no working software engineer is ever in a position to know what those answers are, because you can only fit so much experience into one career. We are all at least partly relying on our own personal experience: on our particular set of engineering values.</p>
<h3>What engineering taste actually is</h3>
<p>Almost every decision in software engineering is a tradeoff. You’re rarely picking between two options where one is strictly better. Instead, each option has its own benefits and downsides. Often you have to make hard tradeoffs between engineering <em>values</em>: past a certain point, you cannot easily increase performance without harming readability, for instance<sup id="fnref-1"><a href="#fn-1">1</a></sup>.</p>
<p>Really understanding this point is (in my view) the biggest indicator of maturity in software engineering. Immature engineers are rigid about their decisions. They think it’s always better to do X or Y. Mature engineers are usually willing to consider both sides of a decision, because they know that both sides come with different benefits. The trick is not deciding if technology X is better than Y, but whether the benefits of X outweigh Y <em>in this particular case</em>.</p>
<p>In other words, <strong>immature engineers are too inflexible about their taste</strong>. They know what they like, but they mistake that liking for a principled engineering position. What defines a particular engineer’s taste?</p>
<p>In my view, <strong>your engineering taste is composed of the set of engineering values you find most important</strong>. For instance:</p>
<p><strong>Resiliency</strong>. If an infrastructure component fails (a service dies, a network connection becomes unavailable), does the system remain functional? Can it recover without human intervention?</p>
<p><strong>Speed</strong>. How fast is the software, compared to the theoretical limit? Is work being done in the hot path that isn’t strictly necessary?</p>
<p><strong>Readability</strong>. Is the software easy to take in at a glance and to onboard new engineers to? Are functions relatively short and named well? Is the system well-documented?</p>
<p><strong>Correctness</strong>. Is it possible to represent an invalid state in the system? How locked-down is the system with tests, types, and asserts? Do the tests use techniques like fuzzing? In the extreme case, has the program been proven correct by formal methods like <a href="https://en.wikipedia.org/wiki/Alloy_(specification_language)">Alloy</a>?</p>
<p><strong>Flexibility</strong>. Can the system be trivially extended? How easy is it to make a change? If I need to change something, how many different parts of the program do I need to touch in order to do so?</p>
<p><strong>Portability</strong>. Is the system tied down to a particular operational environment (say, Microsoft Windows, or AWS)? If the system needs to be redeployed elsewhere, can that happen without a lot of engineering work?</p>
<p><strong>Scalability</strong>. If traffic goes up 10x, will the system fall over? What about 100x? Does the system have to be over-provisioned or can it scale automatically? What bottlenecks will require engineering intervention?</p>
<p><strong>Development speed</strong>. If I need to extend the system, how fast can it be done? Can most engineers work on it, or does it require a domain expert?</p>
<p>There are many other engineering values: elegance, modern-ness, use of open source, monetary cost of keeping the system running, and so on. All of these are important, but <strong>no engineer cares equally about all of these things.</strong> Your taste is determined by which of these values you rank highest. For instance, if you value speed and correctness more than development speed, you are likely to prefer Rust over Python. If you value scalability over portability, you are likely to argue for a heavy investment in your host’s (e.g. AWS) particular quirks and tooling. If you value resiliency over speed, you are likely to want to split your traffic between different regions. And so on<sup id="fnref-2"><a href="#fn-2">2</a></sup>.</p>
<p>It’s possible to break these values down in a more fine-grained way. Two engineers who both deeply care about readability could disagree because one values short functions and the other values short call-stacks. Two engineers who both care about correctness could disagree because one values exhaustive test suites and the other values formal methods. But the principle is the same - there are lots of possible engineering values to care about, and because they are often in tension, each engineer is forced to take some more seriously than others.</p>
<h3>How to identify bad taste</h3>
<p>I’ve said that all of these values are important. Despite that, it’s possible to have <em>bad</em> taste. In the context of software engineering, bad taste means that <strong>your preferred values are not a good fit for the project you’re working on</strong>.</p>
<p>Most of us have worked with engineers like this. They come onto your project evangelizing about something - formal methods, rewriting in Golang, Ruby meta-programming, cross-region deployment, or whatever - because it’s worked well for them in the past. Whether it’s a good fit for your project or not, they’re going to argue for it, because it’s what they like. Before you know it, you’re making sure your internal metrics dashboard has five nines of reliability, at the cost of making it impossible for any junior engineer to understand.</p>
<p>In other words, most bad taste comes from <strong>inflexibility</strong>. I will always distrust engineers who justify decisions by saying “it’s best practice”. No engineering decision is “best practice” in all contexts! You have to make the right decision for the specific problem you’re facing.</p>
<p>One interesting consequence of this is that engineers with bad taste are like broken compasses. If you’re in the right spot, a broken compass will still point north. It’s only when you start moving around that the broken compass will steer you wrong. Likewise, many engineers with bad taste can be quite effective in the particular niche where their preferences line up with what the project needs. But when they’re moved between projects or jobs, or when the nature of the project changes, the wheels immediately come off. No job stays the same for long, particularly <a href="https://www.seangoedecke.com/good-times-are-over">in these troubled post-2021 times</a>.</p>
<h3>How to identify good taste</h3>
<p>Good taste is a lot more elusive than technical ability. That’s because, unlike technical ability, good taste is the ability to select the right set of engineering values <strong>for the particular technical problem you’re facing</strong>. It’s thus much harder to identify if someone has good taste: you can’t test it with toy problems, or by asking about technical facts. You need there to be a real problem, with all of its messy real-world context.</p>
<p>You can tell you have good taste if the projects you’re working on succeed. If you’re not meaningfully contributing to the design of a project (maybe you’re just doing ticket-work), you can tell you have good taste if the projects where you agree with the design decisions succeed, and the projects where you disagree are rocky. Importantly, you need a set of different kinds of projects. If it’s just the one project, or the same kind of project over again, you might just be a good fit for that. Even if you go through many different kinds of projects, that’s no guarantee that you have good taste in domains you’re less familiar with<sup id="fnref-3"><a href="#fn-3">3</a></sup>.</p>
<p>How do you develop good taste? It’s hard to say, but I’d recommend working on a variety of things, paying close attention to which projects (or which parts of the project) are easy and which parts are hard. You should focus on <strong>flexibility</strong>: try not to acquire strong universal opinions about the right way to write software. What good taste I have I acquired pretty slowly. Still, I don’t see why you couldn’t acquire it fast. I’m sure there are prodigies with taste beyond their experience in programming, just as there are prodigies in other domains.</p>
</section></div>]]></description>
        </item>
    </channel>
</rss>