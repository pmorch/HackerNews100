<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 30 Jan 2025 17:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[GitHub Is Down (107 pts)]]></title>
            <link>https://www.githubstatus.com</link>
            <guid>42877995</guid>
            <pubDate>Thu, 30 Jan 2025 14:29:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.githubstatus.com">https://www.githubstatus.com</a>, See on <a href="https://news.ycombinator.com/item?id=42877995">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <!-- postmortem -->

    <!-- incident updates -->
      <p><strong>Resolved</strong> -
      	<span>On January 23, 2025, between 9:49 and 17:00 UTC, the available capacity of large hosted runners was degraded.  On average, 26% of jobs requiring large runners had a &gt;5min delay getting a runner assigned.  This was caused by the rollback of a configuration change and a latent bug in event processing, which was triggered by the mixed data shape that resulted from the rollback.  The processing would reprocess the same events unnecessarily and cause the background job that manages large runner creation and deletion to run out of resources.  It would automatically restart and continue processing, but the jobs were not able to keep up with production traffic.  We mitigated the impact by using a feature flag to bypass the problematic event processing logic.  While these changes had been rolling out in stages over the last few months and had been safely rolled back previously, an unrelated change prevented rollback from causing this problem in earlier stages.<p>We are reviewing and updating the feature flags in this event processing workflow to ensure that we have high confidence in rollback in all rollout stages.  We are also improving observability of the event processing to reduce the time to diagnose and mitigate similar issues going forward.</p></span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">17:27</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>We are seeing recovery with the latest mitigation. Queue time for a very small percentage of larger runner jobs are still longer than expected so we are monitoring those for full recovery before going green.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">17:03</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>We are actively applying mitigations to help improve larger runner start times. We are currently seeing delays starting about 25% of larger runner jobs.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">16:25</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>We are still actively investigating a slowdown in larger runner assignment and are working to apply additional mitigations.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">15:33</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>We're still applying mitigations to unblock queueing Actions in large runners. We are monitoring for full recovery.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">14:53</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>We are applying further mitigations to fix the issues with delayed queuing for Actions jobs in large runners. We continue to monitor for full recovery.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">14:17</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>We are investigating further mitigations for queueing Actions jobs in large runners. We continue to watch telemetry and are monitoring for full recovery.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">13:42</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>We've applied a mitigation to fix the issues with queuing and running Actions jobs. We are seeing improvements in telemetry and are monitoring for full recovery.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">13:09</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>The team continues to apply mitigations for issues with some Actions jobs delayed being enqueued for larger runners seen by a small number of customers. We will continue providing updates on the progress towards full mitigation.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">12:36</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>The team continues to apply mitigations for issues with some Actions jobs delayed being enqueued for larger runners. We will continue providing updates on the progress towards full mitigation.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">12:03</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>The team continues to investigate issues with some Actions jobs delayed being enqueued for larger runners. We will continue providing updates on the progress towards mitigation.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">11:31</var> UTC
        </small>
      </p>
      <p><strong>Update</strong> -
      	<span>The team continues to investigate issues with some Actions jobs having delays in being queued for larger runners. We will continue providing updates on the progress towards mitigation.</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">10:58</var> UTC
        </small>
      </p>
      <p><strong>Investigating</strong> -
      	<span>We are investigating reports of degraded performance for Actions</span>

        <br>

        <small>
            Jan <var data-var="date">23</var>, <var data-var="time">10:25</var> UTC
        </small>
      </p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral Small 3 (307 pts)]]></title>
            <link>https://mistral.ai/news/mistral-small-3/</link>
            <guid>42877860</guid>
            <pubDate>Thu, 30 Jan 2025 14:16:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/news/mistral-small-3/">https://mistral.ai/news/mistral-small-3/</a>, See on <a href="https://news.ycombinator.com/item?id=42877860">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Today we’re introducing Mistral Small 3, a latency-optimized 24B-parameter model released under the Apache 2.0 license.</p><p><img src="https://mistral.ai/images/news/mistral-small-3/up-and-to-the-left.png" alt="Detailed benchmarks" width="77%"></p><p>Mistral Small 3 is competitive with larger models such as Llama 3.3 70B or Qwen 32B, and is an excellent open replacement for opaque proprietary models like GPT4o-mini. Mistral Small 3 is on par with Llama 3.3 70B instruct, while being more than 3x faster on the same hardware.</p><p>Mistral Small 3 is a pre-trained and instructed model catered to the ‘80%’ of generative AI tasks—those that require robust language and instruction following performance, with very low latency.</p><p>We designed this new model to saturate performance at a size suitable for local deployment. Particularly, Mistral Small 3 has far fewer layers than competing models, substantially reducing the time per forward pass. At over 81% accuracy on MMLU and 150 tokens/s latency, Mistral Small is currently the most efficient model of its category.</p><p>We’re releasing both a pretrained and instruction-tuned checkpoint under Apache 2.0. The checkpoints can serve as a powerful base for accelerating progress. Note that Mistral Small 3 is neither trained with RL nor synthetic data, so is earlier in the model production pipeline than models like Deepseek R1 (a great and complementary piece of open-source technology!). It can serve as a great base model for building accrued reasoning capacities. We look forward to seeing how the open-source community adopts and customizes it.</p><h3 id="performance">Performance</h3><h4 id="human-evaluations">Human Evaluations</h4><p><img src="https://mistral.ai/images/news/mistral-small-3/mistral-small-3-human-evals.png" alt="Detailed benchmarks" width="77%"></p><p>We conducted side by side evaluations with an external third-party vendor, on a set of over 1k proprietary coding and generalist prompts. Evaluators were tasked with selecting their preferred model response from anonymized generations produced by Mistral Small 3 vs another model. We are aware that in some cases the benchmarks on human judgement starkly differ from publicly available benchmarks, but have taken extra caution in verifying a fair evaluation. We are confident that the above benchmarks are valid.</p><h4 id="instruct-performance">Instruct performance</h4><p>Our instruction tuned model performs competitively with open weight models three times its size and with proprietary GPT4o-mini model across Code, Math, General knowledge and Instruction following benchmarks.</p><p><img src="https://mistral.ai/images/news/mistral-small-3/mistral-instruct-knowledge.png" alt="Detailed benchmarks" width="77%"></p><p><img src="https://mistral.ai/images/news/mistral-small-3/mistral-instruct-code-math.png" alt="Detailed benchmarks" width="77%"></p><p><img src="https://mistral.ai/images/news/mistral-small-3/mistral-instruct-IF.png" alt="Detailed benchmarks" width="77%"></p><p>Performance accuracy on all benchmarks were obtained through the same internal evaluation pipeline - as such, numbers may vary slightly from previously reported performance (<a href="https://qwenlm.github.io/blog/qwen2.5-llm/">Qwen2.5-32B-Instruct</a>, <a href="https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct">Llama-3.3-70B-Instruct</a>, <a href="https://huggingface.co/google/gemma-2-27b-it">Gemma-2-27B-IT</a>). Judge based evals such as Wildbench, Arena hard and MTBench were based on gpt-4o-2024-05-13.</p><h4 id="pretraining-performance">Pretraining performance</h4><p><img src="https://mistral.ai/images/news/mistral-small-3/mistral-base-benchmarks.png" alt="Detailed benchmarks" width="77%"></p><p><img src="https://mistral.ai/images/news/mistral-small-3/mistral-base-mmlu-int.png" alt="Detailed benchmarks" width="77%"></p><p>Mistral Small 3, a 24B model, offers the best performance for its size class and rivals with models three times larger such as Llama 3.3 70B.</p><h3 id="when-to-use-mistral-small-3">When to use Mistral Small 3</h3><p>Across our customers and community, we are seeing several distinct use cases emerge for pre-trained models of this size:</p><ul><li>Fast-response conversational assistance: Mistral Small 3 excels in scenarios where quick, accurate responses are critical. This includes virtual assistants in many scenarios where users expect immediate feedback and near real-time interactions.</li><li>Low-latency function calling: Mistral Small 3 is able to handle rapid function execution when used as part of automated or agentic workflows.</li><li>Fine-tuning to create subject matter experts: Mistral Small 3 can be fine-tuned to specialize in specific domains, creating highly accurate subject matter experts. This is particularly useful in fields like legal advice, medical diagnostics, and technical support, where domain-specific knowledge is essential.</li><li>Local inference: Particularly beneficial for hobbyists and organizations handling sensitive or proprietary information. When quantized, Mistral Small 3 can be run privately on a single RTX 4090 or a Macbook with 32GB RAM.</li></ul><p>Our customers are evaluating Mistral Small 3 across multiple industries, including:</p><ul><li>Financial services customers for fraud detection</li><li>Healthcare providers for customer triaging</li><li>Robotics, automotive, and manufacturing companies for on-device command and control</li><li>Horizontal use cases across customers include virtual customer service, and sentiment and feedback analysis.</li></ul><h3 id="using-mistral-small-3-on-your-preferred-tech-stack">Using Mistral Small 3 on your preferred tech stack</h3><p>Mistral Small 3 is now available on la Plateforme as <code>mistral-small-latest</code> or <code>mistral-small-2501</code>. Explore our <a href="https://docs.mistral.ai/">docs</a> to learn how to use our models for text generation.</p><p>We are also excited to collaborate with Hugging Face, Ollama, Kaggle, Together AI, and Fireworks AI to make the model available on their platforms starting today:</p><ul><li><a href="https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501">Hugging Face</a> (<a href="https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501">base model</a>)</li><li><a href="https://ollama.com/library/mistral-small">Ollama</a></li><li><a href="https://www.kaggle.com/models/mistral-ai/mistral-small-24b">Kaggle</a></li><li><a href="https://www.together.ai/blog/mistral-small-3-api-now-available-on-together-ai-a-new-category-leader-in-small-models">Together AI</a></li><li><a href="https://fireworks.ai/models/fireworks/mistral-small-24b-instruct-2501">Fireworks AI</a></li><li>Coming soon on NVIDIA NIM, Amazon SageMaker, Groq, Databricks and Snowflake</li></ul><h3 id="the-road-ahead">The road ahead</h3><p>It’s been exciting days for the open-source community! Mistral Small 3 complements large open-source reasoning models like the recent releases of DeepSeek, and can serve as a strong base model for making reasoning capabilities emerge.</p><p>Among many other things, expect small and large Mistral models with boosted reasoning capabilities in the coming weeks. Join the journey if you’re keen (we’re hiring), or beat us to it by hacking Mistral Small 3 today and making it better!</p><h3 id="open-source-models-at-mistral">Open-source models at Mistral</h3><p><strong>We’re renewing our commitment to using Apache 2.0 license for our general purpose models, as we progressively move away from MRL-licensed models</strong>. As with Mistral Small 3, model weights will be available to download and deploy locally, and free to modify and use in any capacity. These models will also be made available through a serverless API on <a href="https://console.mistral.ai/">la Plateforme</a>, through our on-prem and VPC deployments, customisation and orchestration platform, and through our inference and cloud partners. Enterprises and developers that need specialized capabilities (increased speed and context, domain specific knowledge, task-specific models like code completion) can count on additional commercial models complementing what we contribute to the community.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Note on the Relationship Between Artificial Intelligence and Human Intelligence (242 pts)]]></title>
            <link>https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20250128_antiqua-et-nova_en.html</link>
            <guid>42877709</guid>
            <pubDate>Thu, 30 Jan 2025 14:01:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20250128_antiqua-et-nova_en.html">https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20250128_antiqua-et-nova_en.html</a>, See on <a href="https://news.ycombinator.com/item?id=42877709">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
										<td>
										
										
										
										
<p><span color="#663300"> 
          [</span><a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20250128_antiqua-et-nova_ge.html">DE</a><span color="#663300"> 
          - 
          <a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20250128_antiqua-et-nova_en.html">EN</a> - 
          <a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20250128_antiqua-et-nova_sp.html">ES</a> - 
          <a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20250128_antiqua-et-nova_it.html">IT</a>] </span></p> 
										
										
										
										
<p><span color="#663300">DICASTERY FOR THE DOCTRINE OF THE FAITH</span><b><br>
</b><span color="#663300">DICASTERY FOR CULTURE AND EDUCATION</span></p>

<p><span color="#663300" size="4"><b>ANTIQUA ET NOVA</b></span></p>
<p><span color="#663300"><b><i>Note</i> on the Relationship Between <br>
Artificial Intelligence and Human Intelligence</b></span></p>

<p>I. <b> <a name="Introduction">Introduction</a></b></p>
<p>1.&nbsp; With wisdom both ancient and new (cf. Mt. 13:52), we are called to reflect on 
the current challenges and opportunities posed by scientific and technological 
advancements, particularly by the recent development of Artificial Intelligence 
(AI). The Christian tradition regards the gift of intelligence as an essential aspect 
of how humans are created “in the image of God” (Gen. 1:27). Starting from an 
integral vision of the human person and the biblical calling to “till” and 
“keep” the earth (Gen. 2:15), the Church emphasizes that this gift of 
intelligence should be expressed through the responsible use of reason and 
technical abilities in the stewardship of the created world. </p>
<p>2. The Church encourages the advancement of science, technology, the arts, and 
other forms of human endeavor, viewing them as part of the “collaboration of man 
and woman with God in perfecting the visible creation.”<a name="_ftnref1" title="" href="#_ftn1">[1]</a> As Sirach affirms, God “gave skill to human beings, that he might be 
glorified in his marvelous works” (Sir. 38:6). Human abilities and creativity 
come from God and, when used rightly, glorify God by reflecting his wisdom and 
goodness. In light of this, when we ask ourselves what it means to “be human,” 
we cannot exclude a consideration of our scientific and technological abilities.</p>
<p>3. It is within this perspective that the present <i>Note</i> addresses the 
anthropological and ethical challenges raised by AI—issues that are particularly 
significant, as one of the goals of this technology is <i>to imitate the human 
intelligence that designed it. </i>For instance, unlike many other human 
creations, AI can be trained on the results of human creativity and then 
generate new “artifacts” with a level of speed and skill that often rivals or 
surpasses what humans can do, such as producing text or images indistinguishable 
from human compositions. This raises critical concerns about AI’s potential role 
in the growing crisis of truth in the public forum. Moreover, this technology is 
designed to learn and make certain choices autonomously, adapting to new 
situations and providing solutions not foreseen by its programmers, and thus, it 
raises fundamental questions about ethical responsibility and human safety, with 
broader implications for society as a whole. This new situation has prompted 
many people to reflect on what it means to be human and the role of humanity in 
the world.</p>
<p>4. Taking all this into account, there is broad consensus that AI marks a new and 
significant phase in humanity’s engagement with technology, placing it at the 
heart of what Pope Francis has described as an “epochal change.”<a name="_ftnref2" title="" href="#_ftn2">[2]</a> Its impact is felt globally and in a wide range of areas, including 
interpersonal relationships, education, work, art, healthcare, law, warfare, and 
international relations. As AI advances rapidly toward even greater 
achievements, it is critically important to consider its anthropological and 
ethical implications. This involves not only mitigating risks and preventing 
harm but also ensuring that its applications are used to promote human progress 
and the common good.</p>
<p>5. To contribute positively to the discernment regarding AI, and in response to 
Pope Francis’ call for a renewed “wisdom of heart,”<a name="_ftnref3" title="" href="#_ftn3">[3]</a> the Church offers its experience through the anthropological and ethical 
reflections contained in this <i>Note</i>. Committed to its active role in the 
global dialogue on these issues, the Church invites those entrusted with 
transmitting the faith—including parents, teachers, pastors, and bishops—to 
dedicate themselves to this critical subject with care and attention. While this 
document is intended especially for them, it is also meant to be accessible to a 
broader audience, particularly those who share the conviction that scientific 
and technological advances should be directed toward serving the human person 
and the common good.<a name="_ftnref4" title="" href="#_ftn4">[4]</a></p>
<p>6. To this end, the document begins by distinguishing between concepts of 
intelligence in AI and in human intelligence. It then explores the Christian 
understanding of human intelligence, providing a framework rooted in the 
Church’s philosophical and theological tradition. Finally, the document offers 
guidelines to ensure that the development and use of AI uphold human dignity and 
promote the integral development of the human person and society.</p>
<p>II. <b> <a name="What_is_Artificial_Intelligence">What is Artificial Intelligence?</a></b></p>
<p>7. The concept of “intelligence” in AI has evolved over time, drawing on a range of 
ideas from various disciplines. While its origins extend back centuries, a 
significant milestone occurred in 1956 when the American computer scientist John 
McCarthy organized a summer workshop at Dartmouth University to explore the 
problem of “Artificial Intelligence,” which he defined as “that of making a 
machine behave in ways that would be called intelligent if a human were so 
behaving.”<a name="_ftnref5" title="" href="#_ftn5"> [5]</a> This workshop launched a research program focused on designing machines 
capable of performing tasks typically associated with the human intellect and 
intelligent behavior.</p>
<p>8. Since then, AI research has advanced rapidly, leading to the development of 
complex systems capable of performing highly sophisticated tasks.<a name="_ftnref6" title="" href="#_ftn6">[6]</a> These so-called “narrow AI” systems are typically designed to handle 
specific and limited functions, such as translating languages, predicting the 
trajectory of a storm, classifying images, answering questions, or generating 
visual content at the user’s request. While the definition of “intelligence” in 
AI research varies, most contemporary AI systems—particularly those using 
machine learning—rely on statistical inference rather than logical deduction. By 
analyzing large datasets to identify patterns, AI can “predict”<a name="_ftnref7" title="" href="#_ftn7">[7]</a> outcomes and propose new approaches, mimicking some cognitive processes 
typical of human problem-solving. Such achievements have been made possible 
through advances in computing technology (including neural networks, 
unsupervised machine learning, and evolutionary algorithms) as well as hardware 
innovations (such as specialized processors). Together, these technologies 
enable AI systems to respond to various forms of human input, adapt to new 
situations, and even suggest novel solutions not anticipated by their original 
programmers.<a name="_ftnref8" title="" href="#_ftn8">[8]</a></p>
<p>9. Due to these rapid advancements, many tasks once managed exclusively by humans 
are now entrusted to AI. These systems can augment or even supersede what humans 
are able to do in many fields, particularly in specialized areas such as data 
analysis, image recognition, and medical diagnosis. While each “narrow AI” 
application is designed for a specific task, many researchers aspire to develop 
what is known as “Artificial General Intelligence” (AGI)—a single system capable 
of operating across all cognitive domains and performing any task within the 
scope of human intelligence. Some even argue that AGI could one day achieve the 
state of “superintelligence,” surpassing human intellectual capacities, or 
contribute to “super-longevity” through advances in biotechnology. Others, 
however, fear that these possibilities, even if hypothetical, could one day 
eclipse the human person, while still others welcome this potential 
transformation.<a name="_ftnref9" title="" href="#_ftn9">[9]</a></p>
<p>10. Underlying this and 
many other perspectives on the subject is the implicit assumption that the term 
“intelligence” can be used in the same way to refer to both human intelligence 
and AI. Yet, this does not capture the full scope of the concept. In the case of humans, 
intelligence is a faculty that pertains to the person in his or her entirety, 
whereas in the context of AI, “intelligence” is understood functionally, often 
with the presumption that the activities characteristic of the human mind can be 
broken down into digitized steps that machines can replicate.<a name="_ftnref10" title="" href="#_ftn10">[10]</a></p>
<p>11. This functional 
perspective is exemplified by the “Turing Test,” which considers a machine 
“intelligent” if a person cannot distinguish its behavior from that of a human.<a name="_ftnref11" title="" href="#_ftn11">[11]</a> However, in this context, the term “behavior” refers only to the 
performance of specific intellectual tasks; it does not account for the full 
breadth of human experience, which includes abstraction, emotions, creativity, 
and the aesthetic, moral, and religious sensibilities. Nor does it encompass the 
full range of expressions characteristic of the human mind. Instead, in the case 
of AI, the “intelligence” of a system is evaluated methodologically, but also 
reductively, based on its<i> ability to produce appropriate responses—in this 
case, those associated with the human intellect—regardless of how those 
responses are generated</i>.</p>
<p>12. AI’s advanced features 
give it sophisticated abilities to <i>perform tasks</i>, but not the ability to
<i>think</i>.<a name="_ftnref12" title="" href="#_ftn12">[12]</a> This distinction is crucially important, as the way “intelligence” is 
defined inevitably shapes how we understand the relationship between human 
thought and this technology.<a name="_ftnref13" title="" href="#_ftn13">[13]</a> To appreciate this, one must recall the richness of the philosophical 
tradition and Christian theology, which offer a deeper and more comprehensive 
understanding of intelligence—an understanding that is central to the Church’s 
teaching on the nature, dignity, and vocation of the human person.<a name="_ftnref14" title="" href="#_ftn14">[14]</a></p>
<p>III. <b> <a name="Intelligence_in_the_Philosophical">Intelligence in the Philosophical</a> and Theological Tradition</b></p>
<p><i><a name="Rationality">Rationality</a></i></p>
<p>13. From the dawn of human 
self-reflection, the mind has played a central role in understanding what it 
means to be “human.” Aristotle observed that “all people by nature desire to 
know.”<a name="_ftnref15" title="" href="#_ftn15">[15]</a> This knowledge, with its capacity for abstraction that grasps the nature 
and meaning of things, sets humans apart from the animal world.<a name="_ftnref16" title="" href="#_ftn16">[16]</a> As philosophers, theologians, and psychologists have examined the exact 
nature of this intellectual faculty, they have also explored how humans 
understand the world and their unique place within it. Through this exploration, 
the Christian tradition has come to understand the human person as a being 
consisting of both body and soul—deeply connected to this world and yet 
transcending it.<a name="_ftnref17" title="" href="#_ftn17">[17]</a></p>
<p>14. In the classical 
tradition, the concept of intelligence is often understood through the 
complementary concepts of “reason” (<i>ratio</i>) and “intellect” (<i>intellectus</i>). 
These are not separate faculties but, as Saint Thomas Aquinas explains, they are 
two modes in which the same intelligence operates: “The term <i>intellect</i> is 
inferred from the inward grasp of the truth, while the name <i>reason</i> is 
taken from the inquisitive and discursive process.”<a name="_ftnref18" title="" href="#_ftn18">[18]</a> This concise description highlights the two fundamental and complementary 
dimensions of human intelligence. <i>Intellectus</i> refers to the intuitive 
grasp of the truth—that is, apprehending it with the “eyes” of the mind—which 
precedes and grounds argumentation itself. <i>Ratio</i> pertains to reasoning 
proper: the discursive, analytical process that leads to judgment. Together, 
intellect and reason form the two facets of the act of <i>intelligere</i>, “the 
proper operation of the human being as such.”<a name="_ftnref19" title="" href="#_ftn19">[19]</a></p>
<p>15. Describing the human 
person as a “rational” being does not reduce the person to a specific mode of 
thought; rather, it recognizes that the ability for intellectual understanding 
shapes and permeates all aspects of human activity.<a name="_ftnref20" title="" href="#_ftn20"> [20]</a> Whether exercised well or poorly, this capacity is an intrinsic aspect of 
human nature. In this sense, the “term ‘rational’ encompasses all the capacities 
of the human person,” including those related to “knowing and understanding, as 
well as those of willing, loving, choosing, and desiring; it also includes all 
corporeal functions closely related to these abilities.”<a name="_ftnref21" title="" href="#_ftn21"> [21]</a>&nbsp;This comprehensive perspective underscores how, in the human person, 
created in the “image of God,” reason is integrated in a way that elevates, 
shapes, and transforms both the person’s will and actions.<a name="_ftnref22" title="" href="#_ftn22"> [22]</a></p>
<p><i><a name="Embodiment">Embodiment</a></i></p>
<p>16. Christian thought 
considers the intellectual faculties of the human person within the framework of 
an integral anthropology that views the human being as essentially embodied. In 
the human person, spirit and matter “are not two natures united, but rather 
their union forms a single nature.”<a name="_ftnref23" title="" href="#_ftn23">[23]</a>&nbsp;In other words, the soul is not merely the immaterial “part” of the person 
contained within the body, nor is the body an outer shell housing an intangible 
“core.” Rather, the entire human person is simultaneously both material and 
spiritual. This understanding reflects the teaching of Sacred Scripture, which 
views the human person as a being who lives out relationships with God and 
others (and thus, an authentically spiritual dimension) within and through this 
embodied existence.<a name="_ftnref24" title="" href="#_ftn24">[24]</a>&nbsp;The profound meaning of this condition is further illuminated by the 
mystery of the Incarnation, through which God himself took on our flesh and 
“raised it up to a sublime dignity.”<a name="_ftnref25" title="" href="#_ftn25">[25]</a></p>
<p>17. Although deeply rooted 
in bodily existence, the human person transcends the material world through the 
soul, which is “almost on the horizon of eternity and time.”<a name="_ftnref26" title="" href="#_ftn26">[26]</a>&nbsp;The intellect's capacity for transcendence and the self-possessed freedom 
of the will belong to the soul, by which the human person “shares in the light 
of the divine mind.”<a name="_ftnref27" title="" href="#_ftn27">[27]</a>&nbsp;Nevertheless, the human spirit does not exercise its normal mode of 
knowledge without the body.<a name="_ftnref28" title="" href="#_ftn28">[28]</a>&nbsp;In this way, the intellectual faculties of the human person are an 
integral part of an anthropology that recognizes that the human person is a 
“unity of body and soul.”<a name="_ftnref29" title="" href="#_ftn29">[29]</a>&nbsp;Further aspects of this understanding will be developed in what follows.</p>
<p><i><a name="Relationality">Relationality</a></i></p>
<p>18. &nbsp;Human 
beings are “ordered by their very nature to interpersonal communion,”<a name="_ftnref30" title="" href="#_ftn30">[30]</a> 
&nbsp;possessing the capacity to know one another, to give themselves in love, and to 
enter into communion with others. Accordingly, human intelligence is not an 
isolated faculty but is exercised in relationships, finding its fullest 
expression in dialogue, collaboration, and solidarity. We learn with others, and 
we learn through others.</p>
<p>19. The relational orientation of the human person is ultimately grounded in the 
eternal self-giving of the Triune God, whose love is revealed in creation and 
redemption.<a name="_ftnref31" title="" href="#_ftn31">[31]</a>&nbsp;The human person is “called to share, by knowledge and love, in God’s own 
life.”<a name="_ftnref32" title="" href="#_ftn32">[32]</a></p>
<p>20. This vocation to 
communion with God is necessarily tied to the call to communion with others. 
Love of God cannot be separated from love for one’s neighbor (cf. 1 Jn. 4:20; 
Mt. 22:37-39). By the grace of sharing God’s life, Christians are also called to 
imitate Christ’s outpouring gift (cf. 2 Cor. 9:8-11; Eph. 5:1-2) by following 
his command to “love one another, as I have loved you” (Jn.&nbsp;13:34).<a name="_ftnref33" title="" href="#_ftn33">[33]</a>&nbsp;Love and service, echoing the divine life of self-giving, transcend 
self-interest to respond more fully to the human vocation (cf. 1 Jn. 2:9). Even 
more sublime than knowing many things is the commitment to care for one another, 
for if “I understand all mysteries and all knowledge 
[...] but do not have love, I am nothing” (1 Cor. 13:2).</p>
<p><i><a name="Relationship_with_the_Truth">Relationship with the Truth</a></i></p>
<p>21. Human intelligence is ultimately “God’s gift fashioned for the assimilation of 
truth.”<a name="_ftnref34" title="" href="#_ftn34">[34]</a>In the dual sense of <i>intellectus</i>-<i>ratio</i>, it enables the person to 
explore realities that surpass mere sensory experience or utility, since “the 
desire for truth is part of human nature itself. It is an innate property of 
human reason to ask why things are as they are.”<a name="_ftnref35" title="" href="#_ftn35">[35]</a>&nbsp;Moving beyond the limits of empirical data, human intelligence can “with 
genuine certitude attain to reality itself as knowable.”<a name="_ftnref36" title="" href="#_ftn36">[36]</a>&nbsp;While reality remains only partially known, the desire for truth “spurs 
reason always to go further; indeed, it is as if reason were overwhelmed to see 
that it can always go beyond what it has already achieved.”<a name="_ftnref37" title="" href="#_ftn37">[37]</a>&nbsp;Although Truth in itself transcends the boundaries of human intelligence, 
it irresistibly attracts it.<a name="_ftnref38" title="" href="#_ftn38">[38]</a>&nbsp;Drawn by this attraction, the human person is led to seek “truths of a 
higher order.”<a name="_ftnref39" title="" href="#_ftn39">[39]</a></p>
<p>22. &nbsp;This innate drive toward the pursuit of truth is especially evident in the 
distinctly human capacities for semantic understanding and creativity,<a name="_ftnref40" title="" href="#_ftn40">[40]</a>&nbsp;through which this search unfolds in a “manner that is appropriate to the 
social nature and dignity of the human person.”<a name="_ftnref41" title="" href="#_ftn41">[41]</a>&nbsp;Likewise, a steadfast orientation to the truth is essential for charity to 
be both authentic and universal.<a name="_ftnref42" title="" href="#_ftn42">[42]</a></p>
<p>23. The search for truth 
finds its highest expression in openness to realities that transcend the 
physical and created world. In God, all truths attain their ultimate and original meaning.<a name="_ftnref43" title="" href="#_ftn43">[43]</a>&nbsp;Entrusting oneself to God is a “fundamental decision that engages the 
whole person.”<a name="_ftnref44" title="" href="#_ftn44">[44]</a><a name="_Hlk181548087">
</a>In this way, the human person becomes fully what he 
or she is called to be: “the intellect and the will display their spiritual 
nature,” enabling the person “to act in a way that realizes personal freedom to 
the full.”<a name="_ftnref45" title="" href="#_ftn45">[45]</a></p>
<p><i><a name="Stewardship_of_the_World">Stewardship of the World</a></i></p>
<p>24. The Christian faith 
understands creation as the free act of the Triune God, who, as Saint 
Bonaventure of Bagnoregio explains, creates “not to increase his glory, but to 
show it forth and to communicate it.”<a name="_ftnref46" title="" href="#_ftn46">[46]</a>&nbsp;Since God creates according to his Wisdom (cf. Wis. 9:9; Jer. 10:12), 
creation is imbued with an intrinsic order that reflects God’s plan (cf. Gen. 1; 
Dan. 2:21-22; Is. 45:18; Ps. 74:12-17; 104),<a name="_ftnref47" title="" href="#_ftn47">[47]</a>&nbsp;within which God has called human beings to assume a unique role: <i>to</i>
<i>cultivate and care for the world</i>.<a name="_ftnref48" title="" href="#_ftn48">[48]</a></p>
<p>25. Shaped by the Divine 
Craftsman, humans live out their identity as beings made <i>in imago Dei</i> by 
“keeping” and “tilling” (cf. Gen. 2:15) creation—using their intelligence and 
skills to care for and develop creation in accord with God’s plan.<a name="_ftnref49" title="" href="#_ftn49">[49]</a>&nbsp;In this, human intelligence reflects the Divine 
Intelligence that created all things (cf. Gen. 1-2; Jn. 1),<a name="_ftnref50" title="" href="#_ftn50">[50]</a>&nbsp;continuously sustains them, and guides them to their ultimate purpose in 
him.<a name="_ftnref51" title="" href="#_ftn51">[51]</a>&nbsp;Moreover, human beings are called to develop their abilities in science 
and technology, for through them, God is glorified (cf<i>. </i>Sir.<i> </i>
38:6). Thus, in a proper relationship with creation, humans, on the one hand, 
use their intelligence and skill to cooperate with God in guiding creation 
toward the purpose to which he has called it.<a name="_ftnref52" title="" href="#_ftn52">[52]</a> On the other hand, creation itself, as Saint Bonaventure observes, helps 
the human mind to “ascend gradually to the supreme Principle, who is God.”<a name="_ftnref53" title="" href="#_ftn53">[53]</a></p>
<p><i><a name="An_Integral">An Integral</a> Understanding of Human Intelligence</i></p>
<p>26. In this context, human 
intelligence becomes more clearly understood as a faculty that forms an integral 
part of how the whole person engages with reality. Authentic engagement requires 
embracing the full scope of one’s being: spiritual, cognitive, embodied, and 
relational.</p>
<p>27. This engagement with 
reality unfolds in various ways, as each person, in his or her multifaceted 
individuality<a name="_ftnref54" title="" href="#_ftn54">[54]</a>, seeks to understand the world, relate to others, solve problems, express 
creativity, and pursue integral well-being through the harmonious interplay of 
the various dimensions of the person’s intelligence.<a name="_ftnref55" title="" href="#_ftn55">[55]</a>&nbsp;This involves logical and linguistic abilities but can also encompass 
other modes of interacting with reality. Consider the work of an artisan, who 
“must know how to discern, in inert matter, a particular form that others cannot 
recognize”<a name="_ftnref56" title="" href="#_ftn56">[56]</a>&nbsp;and bring it forth through insight and practical skill. Indigenous peoples 
who live close to the earth often possess a profound sense of nature and its 
cycles.<a name="_ftnref57" title="" href="#_ftn57">[57]</a>&nbsp;Similarly, a friend who knows the right word to say or a person adept at 
managing human relationships exemplifies an intelligence that is “the fruit of 
self-examination, dialogue and generous encounter between persons.”<a name="_ftnref58" title="" href="#_ftn58">[58]</a>&nbsp;As Pope Francis observes, “in this age of artificial intelligence, we 
cannot forget that poetry and love are necessary to save our humanity.”<a name="_ftnref59" title="" href="#_ftn59">[59]</a></p>
<p>28. At the heart of the 
Christian understanding of intelligence is the integration of truth into the 
moral and spiritual life of the person, guiding his or her actions in light of 
God’s goodness and truth. According to God’s plan, 
intelligence, in its fullest sense, also includes the ability to savor what is 
true, good, and beautiful. As the twentieth-century 
French poet Paul Claudel expressed, “intelligence is nothing without delight.”<a name="_ftnref60" title="" href="#_ftn60">[60]</a> 
Similarly, Dante, upon reaching the highest heaven in <i>Paradiso</i>, testifies 
that the culmination of this intellectual delight is found in the “light 
intellectual full of love, love of true good filled with joy, joy which 
transcends every sweetness.”<a name="_ftnref61" title="" href="#_ftn61">[61]</a></p>
<p>29. A proper understanding 
of human intelligence, therefore, cannot be reduced to the mere acquisition of 
facts or the ability to perform specific tasks. Instead, it involves the 
person’s openness to the ultimate questions of life and reflects an orientation 
toward the True and the Good.<a name="_ftnref62" title="" href="#_ftn62">[62]</a>&nbsp;As an expression of the divine image within the person, human intelligence 
has the ability to access the totality of being, contemplating existence in its 
fullness, which goes beyond what is measurable, and grasping the meaning of what 
has been understood. For believers, this capacity includes, in a particular way, 
the ability to grow in the knowledge of the mysteries of God by using reason to 
engage ever more profoundly with revealed truths (<i>intellectus fidei</i>).<a name="_ftnref63" title="" href="#_ftn63">[63]</a>&nbsp;True intelligence is shaped by divine love, which “is poured forth in our 
hearts by the Holy Spirit” (Rom. 5:5). From this, it follows that human 
intelligence possesses an essential <i>contemplative </i>dimension, an unselfish 
openness to the True, the Good, and the Beautiful, beyond any utilitarian 
purpose.</p>
<p><i><a name="The_Limits_of_AI">The Limits of AI</a></i></p>
<p>30. In light of the 
foregoing discussion, the differences between human intelligence and current AI 
systems become evident. While AI is an extraordinary technological achievement capable of imitating 
certain outputs associated with human intelligence, it operates by performing 
tasks, achieving goals, or making decisions based on quantitative data and 
computational logic. For example, with its analytical power, AI excels at 
integrating data from a variety of fields, modeling complex systems, and 
fostering interdisciplinary connections. In this way, it can help experts 
collaborate in solving complex problems that “cannot be dealt with from a single 
perspective or from a single set of interests.”<a name="_ftnref64" title="" href="#_ftn64">[64]</a></p>
<p>31. However, even as AI 
processes and simulates certain expressions of intelligence, it remains 
fundamentally confined to a logical-mathematical framework, which imposes 
inherent limitations. Human intelligence, in contrast, develops organically 
throughout the person’s physical and psychological growth, shaped by a myriad of 
lived experiences in the flesh. Although advanced AI systems can “learn” through processes such as machine 
learning, this sort of training is fundamentally different from the 
developmental growth of human intelligence, which is shaped by embodied 
experiences, including sensory input, emotional responses, social interactions, 
and the unique context of each moment. These elements shape and form individuals 
within their personal history.<b></b>In contrast, AI, lacking a physical body, relies on computational reasoning and 
learning based on vast datasets that include recorded human experiences and 
knowledge. </p>
<p>32. Consequently, although 
AI can simulate aspects of human reasoning and perform specific tasks with incredible speed and efficiency, its computational 
abilities represent only a fraction of the broader capacities of the human mind. 
For instance, AI cannot currently replicate moral discernment or the ability to 
establish authentic relationships. Moreover, human intelligence is situated 
within a personally lived history of intellectual and moral formation that 
fundamentally shapes the individual’s perspective, encompassing the physical, emotional, social, moral, 
and spiritual dimensions of life. Since AI cannot offer this fullness of 
understanding, approaches that rely solely on this technology or treat it as the 
primary means of interpreting the world can lead to “a loss of appreciation for 
the whole, for the relationships between things, and for the broader horizon.”<a name="_ftnref65" title="" href="#_ftn65">[65]</a></p>
<p>33. Human intelligence is 
not primarily about completing functional tasks but about understanding and 
actively engaging with reality in all its dimensions; it is also capable of 
surprising insights. Since AI lacks the richness of corporeality, relationality, 
and the openness of the human heart to truth and goodness, its capacities—though 
seemingly limitless—are incomparable with the human ability to grasp reality. So 
much can be learned from an illness, an embrace of reconciliation, and even a 
simple sunset; indeed, many experiences we have as humans open new horizons and 
offer the possibility of attaining new wisdom. No device, working solely with 
data, can measure up to these and countless other experiences present in our 
lives.</p>
<p>34. Drawing an overly 
close equivalence between human intelligence and AI risks succumbing to a 
functionalist perspective, where people are valued based on the work they can 
perform. However, a person’s worth does not depend on possessing specific 
skills, cognitive and technological achievements, or individual success, but on 
the person’s inherent dignity, grounded in being created in the image of God.<a name="_ftnref66" title="" href="#_ftn66">[66]</a>&nbsp;This dignity remains intact in all circumstances, including 
for those unable 
to exercise their abilities, whether it be an unborn child, an unconscious 
person, or an older person who is suffering.<a name="_ftnref67" title="" href="#_ftn67"> [67]</a>&nbsp;It 
also underpins the tradition of human rights (and, in particular, 
what are now called “neuro-rights”), which represent “an important point of 
convergence in the search for common ground”<a name="_ftnref68" title="" href="#_ftn68">[68]</a>&nbsp;and can, thus, serve as a fundamental ethical guide in discussions on the 
responsible development and use of AI.</p>
<p>35. Considering all these 
points, as Pope Francis observes, “the very use of the word ‘intelligence’” in 
connection with AI “can prove misleading”<a name="_ftnref69" title="" href="#_ftn69">[69]</a>&nbsp;and risks overlooking what is most precious in the human person. In light 
of this, AI should not be seen as <i>an artificial form </i>of<i> </i>human 
intelligence but as<i> a product </i>of<i> </i>it.<a name="_ftnref70" title="" href="#_ftn70">[70]</a></p>
<p>IV. <b> <a name="The_Role_of_Ethics">The Role of Ethics</a> in Guiding the Development and Use of AI</b></p>
<p>36. Given these 
considerations, one can ask how AI can be understood within God’s plan. To answer this, it is important to recall that 
techno-scientific activity is not neutral in character but is a <i>human </i>
endeavor that engages the humanistic and cultural dimensions of human 
creativity.<a name="_ftnref71" title="" href="#_ftn71">[71]</a></p>
<p>37. Seen as a fruit of the 
potential inscribed within human intelligence,<a name="_ftnref72" title="" href="#_ftn72">[72]</a>&nbsp;scientific inquiry and the development of technical skills are part of the 
“collaboration of man and woman with God in perfecting the visible creation.”<a name="_ftnref73" title="" href="#_ftn73">[73]</a>&nbsp;At the same time, all scientific and technological achievements are, 
ultimately, gifts from God.<a name="_ftnref74" title="" href="#_ftn74">[74]</a>&nbsp;Therefore, human beings must always use their abilities in view of the 
higher purpose for which God has granted them.<a name="_ftnref75" title="" href="#_ftn75">[75]</a></p>
<p>38. We can gratefully 
acknowledge how technology has “remedied countless evils which used to harm and 
limit human beings,”<a name="_ftnref76" title="" href="#_ftn76">[76]</a>&nbsp;a fact for which we should rejoice. Nevertheless, not all technological 
advancements in themselves represent genuine human progress.<a name="_ftnref77" title="" href="#_ftn77">[77]</a>&nbsp;The Church is particularly opposed to those applications that threaten the 
sanctity of life or the dignity of the human person.<a name="_ftnref78" title="" href="#_ftn78">[78]</a>&nbsp;Like any human endeavor, technological development must be directed to 
serve the human person and contribute to the pursuit of “greater justice, more 
extensive fraternity, and a more humane order of social relations,” which are 
“more valuable than advances in the technical field.”<a name="_ftnref79" title="" href="#_ftn79">[79]</a>&nbsp;Concerns about the ethical implications of technological development are 
shared not only within the Church but also among many scientists, technologists, 
and professional associations, who increasingly call for ethical reflection to 
guide this development in a responsible way. </p>
<p>39. To address these 
challenges, it is essential to emphasize <i>the importance of moral 
responsibility grounded in the dignity and vocation of the human person</i>. 
This guiding principle also applies to questions concerning AI. In this context, 
the ethical dimension takes on primary importance because it is people who 
design systems and determine the purposes for which they are used.<a name="_ftnref80" title="" href="#_ftn80">[80]</a>&nbsp;Between a machine and a human being, only the latter is truly a moral 
agent—a subject of moral responsibility who exercises freedom in his or her 
decisions and accepts their consequences.<a name="_ftnref81" title="" href="#_ftn81">[81]</a>&nbsp;It is not the machine but the human who is in relationship with truth and 
goodness, guided by a moral conscience that calls the person “to love and to do 
what is good and to avoid evil,”<a name="_ftnref82" title="" href="#_ftn82">[82]</a>&nbsp;bearing witness to “the authority of truth in reference to the supreme 
Good to which the human person is drawn.”<a name="_ftnref83" title="" href="#_ftn83">[83]</a>&nbsp;Likewise, between a machine and a human, only the human can be 
sufficiently self-aware to the point of listening and following the voice of 
conscience, discerning with prudence, and seeking the good that is possible in 
every situation.<a name="_ftnref84" title="" href="#_ftn84">[84]</a>&nbsp;In fact, all of this also belongs to the person’s exercise of 
intelligence.</p>
<p>40. Like any product of 
human creativity, AI can be directed toward positive or negative ends.<a name="_ftnref85" title="" href="#_ftn85">[85]</a>&nbsp;When used in ways that respect human dignity and promote the well-being of 
individuals and communities, it can contribute positively to the human vocation.
Yet, as in all areas where humans are called to make 
decisions, the shadow of evil also looms here. Where human freedom allows for 
the possibility of choosing what is wrong, the moral evaluation of this 
technology will need to take into account how it is directed and used.</p>
<p>41. At the same time, it 
is not only the ends that are ethically significant but also the means employed 
to achieve them. Additionally, the overall vision and understanding of the human 
person embedded within these systems are important to consider as well. 
Technological products reflect the worldview of their developers, owners, users, 
and regulators,<a name="_ftnref86" title="" href="#_ftn86">[86]</a>&nbsp;and have the power to “shape the world and engage consciences on the level 
of values.”<a name="_ftnref87" title="" href="#_ftn87">[87]</a>&nbsp;On a societal level, some technological developments could also reinforce 
relationships and power dynamics that are inconsistent with a proper 
understanding of the human person and society. </p>
<p>42. Therefore, the ends 
and the means used in a given application of AI, as well as the overall vision 
it incorporates, must all be evaluated to ensure they respect human dignity and 
promote the common good.<a name="_ftnref88" title="" href="#_ftn88">[88]</a>&nbsp;As Pope Francis has stated, “the intrinsic dignity of every man and every 
woman” must be “the key criterion in evaluating emerging technologies; these 
will prove ethically sound to the extent that they help respect that dignity and 
increase its expression at every level of human life,”<a name="_ftnref89" title="" href="#_ftn89">[89]</a>&nbsp;including in the social and economic spheres. In this sense, human 
intelligence plays a crucial role not only in designing and producing technology 
but also in directing its use in line with the authentic good of the human 
person.<a name="_ftnref90" title="" href="#_ftn90">[90]</a>&nbsp;The responsibility for managing this wisely pertains to every level of 
society, guided by the principle of subsidiarity and other principles of 
Catholic Social Teaching. </p>
<p><i><a name="Helping_Human">Helping Human</a> Freedom and Decision-Making</i></p>
<p>43. The commitment to 
ensuring that <i>AI always supports and promotes the supreme value of the 
dignity of every human being and the fullness of the human vocation</i> serves 
as a criterion of discernment for developers, owners, operators, and regulators 
of AI, as well as to its users. It remains valid for every application of the 
technology at every level of its use.</p>
<p>44. An evaluation of the 
implications of this guiding principle could begin by considering the importance 
of <i>moral responsibility. </i>Since full moral causality belongs only to <i>
personal</i> agents, not artificial ones, it is crucial to be able to identify 
and define who bears responsibility for the processes involved in AI, 
particularly those capable of learning, correction, and reprogramming. While 
bottom-up approaches and very deep neural networks enable AI to solve complex 
problems, they make it difficult to understand the processes that lead to the 
solutions they adopted. This complicates accountability since if an AI 
application produces undesired outcomes, determining who is responsible becomes 
difficult. To address this problem, attention needs to be given to the nature of
<i>accountability </i>processes in complex, highly automated settings, where 
results may only become evident in the medium to long term. For this, it is 
important that ultimate responsibility for decisions made using AI rests with 
the human decision-makers and that there is accountability for the use of AI at 
each stage of the decision-making process.<a name="_ftnref91" title="" href="#_ftn91">[91]</a></p>
<p>45. In addition to 
determining who is responsible, it is essential to identify the objectives 
given to AI systems. Although these systems may use unsupervised autonomous 
learning mechanisms and sometimes follow paths that humans cannot reconstruct, 
they ultimately pursue goals that humans have assigned to them and are governed 
by processes established by their designers and programmers. Yet, this presents 
a challenge because, as AI models become increasingly capable of independent 
learning, the ability to maintain control over them to ensure that such 
applications serve human purposes may effectively diminish. This raises the 
critical question of how to ensure that AI systems are ordered for the good of 
people and not against them. </p>
<p>46. While responsibility 
for the ethical use of AI systems starts with those who develop, produce, 
manage, and oversee such systems, it is also shared by those who use them. As Pope 
Francis noted, the machine “makes a technical choice among several possibilities 
based either on well-defined criteria or on statistical inferences. Human 
beings, however, not only choose, but in their hearts are capable of deciding.”<a name="_ftnref92" title="" href="#_ftn92">[92]</a>&nbsp;Those who use AI to accomplish a task and follow its results create a 
context in which they are ultimately responsible for the power they have 
delegated. Therefore, insofar as AI can assist humans in making decisions, the 
algorithms that govern it should be trustworthy, secure, robust enough to handle 
inconsistencies, and transparent in their operation to mitigate biases and 
unintended side effects.<a name="_ftnref93" title="" href="#_ftn93">[93]</a>&nbsp;Regulatory frameworks should ensure that all legal entities remain 
accountable for the use of AI and all its consequences, with appropriate 
safeguards for transparency, privacy, and accountability.<a name="_ftnref94" title="" href="#_ftn94">[94]</a>&nbsp;Moreover, those using AI should be careful not to become overly dependent 
on it for their decision-making, a trend that increases contemporary society’s 
already high reliance on technology.</p>
<p>47. The Church’s moral and 
social teaching provides resources to help ensure that AI is used in a way that 
preserves human agency. Considerations about justice, for example, should also 
address issues such as fostering just social dynamics, upholding international 
security, and promoting peace. By exercising prudence, individuals and 
communities can discern ways to use AI to benefit humanity while avoiding 
applications that could degrade human dignity or harm the environment. In this 
context, the concept of responsibility should be understood not only in its most 
limited sense but as a “responsibility for the care for others, which is more 
than simply accounting for results achieved.”<a name="_ftnref95" title="" href="#_ftn95">[95]</a></p>
<p>48. Therefore, AI, like 
any technology, can be part of a conscious and responsible answer to humanity’s 
vocation to the good. However, as previously discussed, AI must be directed by 
human intelligence<i> </i>to align with this vocation, ensuring it respects the 
dignity of the human person. Recognizing this “exalted dignity,” the Second 
Vatican Council affirmed that “the social order and its development must 
invariably work to the benefit of the human person.”<a name="_ftnref96" title="" href="#_ftn96">[96]</a>&nbsp;In light of this, the use of AI, as Pope Francis said, must be 
“accompanied by an ethic inspired by a vision of the common good, an ethic of 
freedom, responsibility, and fraternity, capable of fostering the full 
development of people in relation to others and to the whole of creation.”<a name="_ftnref97" title="" href="#_ftn97">[97]</a></p>
<p>V. <b> <a name="Specific_Questions">Specific Questions</a></b></p>
<p>49. Within this general 
perspective, some observations follow below to illustrate how the preceding 
arguments can help provide an ethical orientation in practical situations, in 
line with the “wisdom of heart” that Pope Francis has proposed.<a name="_ftnref98" title="" href="#_ftn98">[98]</a> 
While not exhaustive, this discussion is offered in service of the dialogue that 
considers how AI can be used to uphold the dignity of the human person and 
promote the common good.<a name="_ftnref99" title="" href="#_ftn99">[99]</a></p>
<p><i><a name="AI_and_Society">AI and Society</a></i></p>
<p>50. As Pope Francis 
observed, “the inherent dignity of each human being and the fraternity that 
binds us together as members of the one human family must undergird the 
development of new technologies and serve as indisputable criteria for 
evaluating them before they are employed<a name="_Int_Zt1Jg2IN">.”</a><a name="_ftnref100" title="" href="#_ftn100">[100]</a></p>
<p>51. Viewed through this 
lens, AI could “introduce important innovations in agriculture, education and 
culture, an improved level of life for entire nations and peoples, and the 
growth of human fraternity and social friendship,” and thus be “used to promote 
integral human development.”<a name="_ftnref101" title="" href="#_ftn101">[101]</a>&nbsp;AI could also help organizations identify those in need and counter 
discrimination and marginalization. These and other similar applications of this 
technology could contribute to human development and the common good.<a name="_ftnref102" title="" href="#_ftn102">[102]</a></p>
<p>52. However, while AI 
holds many possibilities for promoting the good, it can also hinder or even 
counter human development and the common good. Pope Francis has noted that 
“evidence to date suggests that digital technologies have increased inequality 
in our world. Not just differences in material wealth, which are also 
significant, but also differences in access to political and social influence.”<a name="_ftnref103" title="" href="#_ftn103">[103]</a>&nbsp;In this sense, AI could be used to perpetuate marginalization and 
discrimination, create new forms of poverty, widen the “digital divide,” and 
worsen existing social inequalities.<a name="_ftnref104" title="" href="#_ftn104">[104]</a></p>
<p>53. Moreover, the 
concentration of the power over mainstream AI applications in the hands of a few 
powerful companies raises significant ethical concerns. Exacerbating this 
problem is the inherent nature of AI systems, where no single individual can 
exercise complete oversight over the vast and complex datasets used for 
computation. This lack of well-defined accountability creates the risk that AI 
could be manipulated for personal or corporate gain or to direct public opinion 
for the benefit of a specific industry. Such entities, motivated by their own 
interests, possess the capacity to exercise “forms of control as subtle as they 
are invasive, creating mechanisms for the manipulation of consciences and of the 
democratic process.”<a name="_ftnref105" title="" href="#_ftn105">[105]</a></p>
<p>54. &nbsp;Furthermore, there is 
the risk of AI being used to promote what Pope Francis has called the 
“technocratic paradigm,” which perceives all the world’s problems as solvable 
through technological means alone.<a name="_ftnref106" title="" href="#_ftn106">[106]</a>&nbsp;In this paradigm, human dignity and fraternity are often set aside in the 
name of efficiency, “as if reality, goodness, and truth automatically flow from 
technological and economic power as such.”<a name="_ftnref107" title="" href="#_ftn107">[107]</a>&nbsp;Yet, human dignity and the common good must never be violated for the sake 
of efficiency,<a name="_ftnref108" title="" href="#_ftn108">[108]</a>&nbsp;for “technological developments that do not lead to an improvement in the 
quality of life of all humanity, but on the contrary, aggravate inequalities and 
conflicts, can never count as true progress.”<a name="_ftnref109" title="" href="#_ftn109">[109]</a>&nbsp;Instead, AI should be put “at the service of another type of progress, one 
which is healthier, more human, more social, more integral.”<a name="_ftnref110" title="" href="#_ftn110">[110]</a></p>
<p>55. Achieving 
this objective requires a deeper reflection on the relationship between autonomy 
and responsibility. Greater autonomy heightens each person’s responsibility 
across various aspects of communal life. For Christians, the foundation of this 
responsibility lies in the recognition that all human capacities, including the 
person’s autonomy, come from God and are meant to be used in the service of 
others<a name="_Int_JUdzKSa4">.</a><a name="_ftnref111" title="" href="#_ftn111">[111]</a>&nbsp;Therefore, rather than merely pursuing economic or technological 
objectives, AI should serve “the common good of the entire human family,” which 
is “the sum total of social conditions that allow people, either as groups or as 
individuals, to reach their fulfillment more fully and more easily.”<a name="_ftnref112" title="" href="#_ftn112">[112]</a></p>
<p><i><a name="AI_and_Human_Relationships">AI and Human Relationships</a></i></p>
<p>56. The Second Vatican 
Council observed that “by his innermost nature man is a social being; and if he 
does not enter into relations with others, he can neither live nor develop his 
gifts.”<a name="_ftnref113" title="" href="#_ftn113">[113]</a>&nbsp;This conviction underscores that living in society is intrinsic to the 
nature and vocation of the human person.<a name="_ftnref114" title="" href="#_ftn114">[114]</a>&nbsp;As social beings, we seek relationships that involve mutual exchange and 
the pursuit of truth, in the course of which, people “share with each other the 
truth they have discovered, or think they have discovered, in such a way that 
they help one another in the search for truth.”<a name="_ftnref115" title="" href="#_ftn115">[115]</a></p>
<p>57. &nbsp;Such a quest, along 
with other aspects of human communication, presupposes encounters and mutual 
exchange between individuals shaped by their unique histories, thoughts, 
convictions, and relationships. Nor can we forget that human intelligence is a 
diverse, multifaceted, and complex reality: individual and social, rational and 
affective, conceptual and symbolic. Pope Francis underscores this dynamic, 
noting that “together, we can seek the truth in dialogue, in relaxed 
conversation or in passionate debate. To do so calls for perseverance; it 
entails moments of silence and suffering, yet it can patiently embrace the 
broader experience of individuals and peoples. […] The process of building 
fraternity, be it local or universal, can only be undertaken by spirits that are 
free and open to authentic encounters.”<a name="_ftnref116" title="" href="#_ftn116">[116]</a></p>
<p>58. It is in this context 
that one can consider the challenges AI poses to human relationships. Like other 
technological tools, AI has the potential to foster connections within the human 
family. However, it could also hinder a true encounter with reality and, 
ultimately, lead people to “a deep and melancholic dissatisfaction with 
interpersonal relations, or a harmful sense of isolation.”<a name="_ftnref117" title="" href="#_ftn117">[117]</a>&nbsp;Authentic human relationships require the richness of being with others in 
their pain, their pleas, and their joy.<a name="_ftnref118" title="" href="#_ftn118">[118]</a>&nbsp;Since human intelligence is expressed and enriched also in interpersonal 
and embodied ways, authentic and spontaneous encounters with others are 
indispensable for engaging with reality in its fullness.</p>
<p>59. Because “true wisdom 
demands an encounter with reality,”<a name="_ftnref119" title="" href="#_ftn119">[119]</a>&nbsp;the rise of AI introduces another challenge. Since AI can effectively 
imitate the products of human intelligence, the ability to know when one is 
interacting with a human or a machine can no longer be taken for granted. 
Generative AI can produce text, speech, images, and other advanced outputs that 
are usually associated with human beings. Yet, it must be understood for what it 
is: a tool, not a person.<a name="_ftnref120" title="" href="#_ftn120">[120]</a>&nbsp;This distinction is often obscured by the language used by practitioners, 
which tends to anthropomorphize AI and thus blurs the line between human and 
machine. </p>
<p>60. Anthropomorphizing AI 
also poses specific challenges for the development of children, potentially 
encouraging them to develop patterns of interaction that treat human 
relationships in a transactional manner, as one would relate to a chatbot. Such 
habits could lead young people to see teachers as mere dispensers of information 
rather than as mentors who guide and nurture their intellectual and moral 
growth. Genuine relationships, rooted in empathy and a steadfast commitment to 
the good of the other, are essential and irreplaceable in fostering the full 
development of the human person. </p>
<p>61. In this context, it is 
important to clarify that, despite the use of anthropomorphic language, no AI 
application can genuinely experience empathy. Emotions cannot be reduced to 
facial expressions or phrases generated in response to prompts; they reflect the 
way a person, as a whole, relates to the world and to his or her own life, with 
the body playing a central role. True empathy requires the ability to listen, 
recognize another’s irreducible uniqueness, welcome their otherness, and grasp 
the meaning behind even their silences.<a name="_ftnref121" title="" href="#_ftn121">[121]</a>&nbsp;Unlike the realm of analytical judgment in which AI excels, true empathy 
belongs to the relational sphere. It involves intuiting and apprehending the 
lived experiences of another while maintaining the distinction between self and 
other.<a name="_ftnref122" title="" href="#_ftn122">[122]</a>&nbsp;While AI can simulate empathetic responses, it cannot replicate the 
eminently personal and relational nature of authentic empathy.<a name="_ftnref123" title="" href="#_ftn123">[123]</a></p>
<p>62. In light of the above, 
it is clear why misrepresenting AI as a person should always be avoided; doing 
so for fraudulent purposes is a grave ethical violation that could erode social 
trust. Similarly, using AI to deceive in other contexts—such as in education or 
in human relationships, including the sphere of sexuality—is also to be 
considered immoral and requires careful oversight to prevent harm, maintain 
transparency, and ensure the dignity of all <a name="_Int_d05vIJmL">people.</a><a name="_ftnref124" title="" href="#_ftn124">[124]</a></p>
<p>63. &nbsp;In an increasingly 
isolated world, some people have turned to AI in search of deep human 
relationships, simple companionship, or even emotional bonds. However, while 
human beings are meant to experience authentic relationships, AI can only 
simulate them. Nevertheless, such relationships with 
others are an integral part of how a person grows to become who he or she is 
meant to be. If AI is used to help people foster genuine connections 
between people, it can contribute positively to the full realization of the 
person. Conversely, if we replace relationships with God and with others with 
interactions with technology, we risk replacing authentic relationality with a 
lifeless image (cf. Ps. 106:20; Rom. 1:22-23). Instead of retreating into 
artificial worlds, we are called to engage in a committed and intentional way 
with reality, especially by identifying with the poor and suffering, consoling 
those in sorrow, and forging bonds of communion with all. </p>
<p><i><a name="AI,_the_Economy,_and_Labor">AI, the Economy, and Labor</a></i></p>
<p>64. Due to its 
interdisciplinary nature, AI is being increasingly integrated into economic and 
financial systems. Significant investments are currently being made not only in 
the technology sector but also in energy, finance, and media, particularly in 
the areas of marketing and sales, logistics, technological innovation, 
compliance, and risk management. At the same time, AI’s applications in these 
areas have also highlighted its ambivalent nature, as a source of tremendous 
opportunities but also profound risks. A first real critical point in this area 
concerns the possibility that—due to the concentration of AI applications in the 
hands of a few corporations—only those large companies would benefit from the 
value created by AI rather than the businesses that use it.</p>
<p>65. Other broader aspects 
of AI’s impact on the economic-financial sphere must also be carefully examined, 
particularly concerning the interaction between concrete reality and the digital 
world. One important consideration in this regard involves the coexistence of 
diverse and alternative forms of economic and financial institutions within a 
given context. This factor should be encouraged, as it can bring benefits in how 
it supports the real economy by fostering its development and stability, 
especially during times of crisis. Nevertheless, it should be stressed that 
digital realities, not restricted by any spatial bonds, tend to be more 
homogeneous and impersonal than communities rooted in a particular place and a 
specific history, with a common journey characterized by shared values and 
hopes, but also by inevitable disagreements and divergences. This diversity is 
an undeniable asset to a community’s economic life. Turning over the economy and 
finance entirely to digital technology would reduce this variety and richness. 
As a result, many solutions to economic problems that can be reached through 
natural dialogue between the involved parties may no longer be attainable in a 
world dominated by procedures and only the appearance of nearness.</p>
<p>66. Another area where AI 
is already having a profound impact is the world of work. As in many other 
fields, AI is driving fundamental transformations across many professions, with 
a range of effects. On the one hand, it has the potential to enhance expertise 
and productivity, create new jobs, enable workers to focus on more innovative 
tasks, and open new horizons for creativity and innovation. </p>
<p>67. However, while AI 
promises to boost productivity by taking over mundane tasks, it frequently 
forces workers to adapt to the speed and demands of machines rather than 
machines being designed to support those who work. As a result, contrary to the 
advertised benefits of AI, current approaches to the technology can 
paradoxically <i>deskill</i> workers, subject them to automated surveillance, 
and relegate them to rigid and repetitive tasks. The need to keep up with the 
pace of technology can erode workers’ sense of agency and stifle
the innovative abilities they are expected to bring to 
their work.<a name="_ftnref125" title="" href="#_ftn125">[125]</a> </p>
<p>68. AI is currently 
eliminating the need for some jobs that were once performed by humans. If AI is 
used to replace human workers rather than complement them, there is a 
“substantial risk of disproportionate benefit for the few at the price of the 
impoverishment of many.”<a name="_ftnref126" title="" href="#_ftn126">[126]</a> 
Additionally, as AI becomes more powerful, there is an associated risk that 
human labor may lose its value in the economic realm. This is the logical 
consequence of the technocratic paradigm: a world of humanity enslaved to 
efficiency, where, ultimately, the cost of humanity 
must be cut. Yet, human lives are intrinsically valuable, independent of their 
economic output. Nevertheless, the “current model,” Pope Francis explains, “does 
not appear to favor an investment in efforts to help the slow, the weak, or the 
less talented to find opportunities in life.”<a name="_ftnref127" title="" href="#_ftn127">[127]</a> 
In light of this, “we cannot allow a tool as powerful and indispensable as 
Artificial Intelligence to reinforce such a paradigm, but rather, we must make 
Artificial Intelligence a bulwark against its expansion.”<a name="_ftnref128" title="" href="#_ftn128"> [128]</a></p>
<p>69. It is important to 
remember that “the order of things must be subordinate to the order of persons, 
and not the other way around.”<a name="_ftnref129" title="" href="#_ftn129">[129]</a> 
Human work must not only be at the service of profit but at “the service of the 
whole human person […] taking into account the person’s material needs and the 
requirements of his or her intellectual, moral, spiritual, and religious life.”<a name="_ftnref130" title="" href="#_ftn130">[130]</a> 
In this context, the Church recognizes that work is “not only a means of earning 
one’s daily bread” but is also “an essential dimension of social life” and “a 
means […] of personal growth, the building of healthy relationships, 
self-expression and the exchange of gifts. Work gives us a sense of shared 
responsibility for the development of the world, and ultimately, for our life as 
a people.”<a name="_ftnref131" title="" href="#_ftn131">[131]</a></p>
<p>70. &nbsp;Since work is a “part 
of the meaning of life on this earth, a path to growth, human development and 
personal fulfillment,” “the goal should not be that technological progress 
increasingly replaces human work, for this would be detrimental to humanity”<a name="_ftnref132" title="" href="#_ftn132">[132]</a>—rather, 
it should promote human labor. Seen in this light, AI should assist, not 
replace, human judgment. Similarly, it must never degrade creativity or reduce 
workers to mere “cogs in a machine.” Therefore, “respect for the dignity of 
laborers and the importance of employment for the economic well-being of 
individuals, families, and societies, for job security and just wages, ought to 
be a high priority for the international community as these forms of technology 
penetrate more deeply into our workplaces.”<a name="_ftnref133" title="" href="#_ftn133">[133]</a></p>
<p><i><a name="AI_and_Healthcare">AI and Healthcare</a></i></p>
<p>71. As participants in 
God’s healing work, healthcare professionals have the vocation and 
responsibility to be “guardians and servants of human life.”<a name="_ftnref134" title="" href="#_ftn134">[134]</a> 
Because of this, the healthcare profession carries an “intrinsic and undeniable 
ethical dimension,” recognized by the Hippocratic Oath, which obliges physicians 
and healthcare professionals to commit themselves to having “absolute respect 
for human life and its sacredness.”<a name="_ftnref135" title="" href="#_ftn135">[135]</a> 
Following the example of the Good Samaritan, this commitment is to be carried 
out by men and women “who reject the creation of a society of exclusion, and act 
instead as neighbors, lifting up and rehabilitating the fallen for the sake of 
the common good.”<a name="_ftnref136" title="" href="#_ftn136">[136]</a></p>
<p>72. Seen in this light, AI 
seems to hold immense potential in a variety of applications in the medical 
field, such as assisting the diagnostic work of healthcare providers, 
facilitating relationships between patients and medical staff, offering new 
treatments, and expanding access to quality care also for those who are isolated 
or marginalized. In these ways, the technology could enhance the “compassionate 
and loving closeness”<a name="_ftnref137" title="" href="#_ftn137">[137]</a> that 
healthcare providers are called to extend to the sick and suffering.</p>
<p>73. However, if AI is used 
not to enhance but to replace the relationship between patients and healthcare 
providers—leaving patients to interact with a machine rather than a human 
being—it would reduce a crucially important human relational structure to a 
centralized, impersonal, and unequal framework. Instead of encouraging 
solidarity with the sick and suffering, such applications of AI would risk 
worsening the loneliness that often accompanies illness, especially in the 
context of a culture where “persons are no longer seen as a paramount value to 
be cared for and respected.”<a name="_ftnref138" title="" href="#_ftn138">[138]</a> 
This misuse of AI would not align with respect for the dignity of the human 
person and solidarity with the suffering.</p>
<p>74. Responsibility for the 
well-being of patients and the decisions that touch upon their lives are at the 
heart of the healthcare profession. This accountability requires medical 
professionals to exercise all their skill and intelligence in making 
well-reasoned and ethically grounded choices regarding those entrusted to their 
care, always respecting the inviolable dignity of the patients and the need for 
informed consent. As a result, decisions regarding patient treatment and the 
weight of responsibility they entail must always remain with the human person 
and should never be delegated to AI.<a name="_ftnref139" title="" href="#_ftn139">[139]</a></p>
<p>75. In addition, using AI 
to determine who should receive treatment based predominantly on economic 
measures or metrics of efficiency represents a particularly problematic instance 
of the “technocratic paradigm” that must be rejected.<a name="_ftnref140" title="" href="#_ftn140">[140]</a> 
For, “optimizing resources means using them in an ethical and fraternal way, and 
not penalizing the most fragile.”<a name="_ftnref141" title="" href="#_ftn141">[141]</a> 
Additionally, AI tools in healthcare are “exposed to forms of bias and 
discrimination,” where “systemic errors can easily multiply, producing not only 
injustices in individual cases but also, due to the domino effect, real forms of 
social inequality.”<a name="_ftnref142" title="" href="#_ftn142">[142]</a></p>
<p>76. The integration of AI 
into healthcare also poses the risk of amplifying other existing disparities in 
access to medical care. As healthcare becomes increasingly oriented toward 
prevention and lifestyle-based approaches, AI-driven solutions may inadvertently 
favor more affluent populations who already enjoy better access to medical 
resources and quality nutrition. This trend risks reinforcing a “medicine for 
the rich” model, where those with financial means benefit from advanced 
preventative tools and personalized health information while others struggle to 
access even basic services. To prevent such inequities, equitable frameworks are 
needed to ensure that the use of AI in healthcare does not worsen existing 
healthcare inequalities but rather serves the common good. </p>
<p><i><a name="AI_and_Education">AI and Education</a></i></p>
<p>77. The words of the 
Second Vatican Council remain fully relevant today: “True education strives to 
form individuals with a view toward their final end and the good of the society 
to which they belong.”<a name="_ftnref143" title="" href="#_ftn143">[143]</a> As 
such, education is “never a mere process of passing on facts and intellectual 
skills: rather, its aim is to contribute to the person’s holistic formation in 
its various aspects (intellectual, cultural, spiritual, etc.), including, for 
example, community life and relations within the academic community,”<a name="_ftnref144" title="" href="#_ftn144">[144]</a> 
in keeping with the nature and dignity of the human person.</p>
<p>78. This approach involves 
a commitment to cultivating the mind, but always as a part of the integral 
development of the person: “We must break that idea of education which holds 
that educating means filling one’s head with ideas. That is the way we educate 
automatons, cerebral minds, not people. Educating is taking a risk in the 
tension between the mind, the heart, and the hands.”<a name="_ftnref145" title="" href="#_ftn145">[145]</a></p>
<p>79. At the center of this 
work of forming the whole human person is the indispensable relationship between 
teacher and student. Teachers do more than convey knowledge; they model 
essential human qualities and inspire the joy of discovery.<a name="_ftnref146" title="" href="#_ftn146">[146]</a> 
Their presence motivates students both through the content they teach and the 
care they demonstrate for their students. This bond fosters trust, mutual 
understanding, and the capacity to address each person’s unique dignity and 
potential. On the part of the student, this can generate a genuine desire to 
grow. The physical presence of a teacher creates a relational dynamic that AI 
cannot replicate, one that deepens engagement and nurtures the student’s 
integral development.</p>
<p>80. In this context, AI 
presents both opportunities and challenges. If used in a prudent manner, within 
the context of an existing teacher-student relationship and ordered to the 
authentic goals of education, AI can become a valuable educational resource by 
enhancing access to education, offering tailored support, and providing 
immediate feedback to students. These benefits could enhance the learning 
experience, especially in cases where individualized attention is needed, or 
educational resources are otherwise scarce. </p>
<p>81. Nevertheless, an 
essential part of education is forming “the intellect to reason well in all 
matters, to reach out towards truth, and to grasp it,”<a name="_ftnref147" title="" href="#_ftn147">[147]</a> 
while helping the “language of the head” to grow harmoniously with the “language 
of the heart” and the “language of the hands.”<a name="_ftnref148" title="" href="#_ftn148">[148]</a> 
This is all the more vital in an age marked by technology, in which “it is no 
longer merely a question of ‘using’ instruments of communication, but of living 
in a highly digitalized culture that has had a profound impact on […] our 
ability to communicate, learn, be informed and enter into relationship with 
others.”<a name="_ftnref149" title="" href="#_ftn149">[149]</a> However, instead of 
fostering “a cultivated intellect,” which “brings with it a power and a grace to 
every work and occupation that it undertakes,”<a name="_ftnref150" title="" href="#_ftn150">[150]</a> 
the extensive use of AI in education could lead to the students’ increased 
reliance on technology, eroding their ability to perform some skills 
independently and worsening their dependence on screens.<a name="_ftnref151" title="" href="#_ftn151">[151]</a> 
</p>
<p>82. Additionally, while 
some AI systems are designed to help people develop their critical thinking 
abilities and problem-solving skills, many others merely provide answers instead 
of prompting students to arrive at answers themselves or write text for 
themselves.<a name="_ftnref152" title="" href="#_ftn152">[152]</a>&nbsp;Instead of training young people how to amass information and generate 
quick responses, education should encourage “the responsible use of freedom to 
face issues with good sense and intelligence.”<a name="_ftnref153" title="" href="#_ftn153">[153]</a>&nbsp;Building on this, “education in the use of forms of artificial 
intelligence should aim above all at promoting critical thinking. Users of all 
ages, but especially the young, need to develop a discerning approach to the use 
of data and content collected on the web or produced by artificial intelligence 
systems. Schools, universities, and scientific societies are challenged to help 
students and professionals to grasp the social and ethical aspects of the 
development and uses of technology.”<a name="_ftnref154" title="" href="#_ftn154">[154]</a></p>
<p>83. As Saint John Paul II 
recalled, “in the world today, characterized by such rapid developments in 
science and technology, the tasks of a Catholic University assume an ever 
greater importance and urgency.”<a name="_ftnref155" title="" href="#_ftn155">[155]</a>&nbsp;In a particular way, Catholic universities are urged to be present as 
great laboratories of hope at this crossroads of history. In an 
inter-disciplinary and cross-disciplinary key, they are urged to engage “with 
wisdom and creativity”<a name="_ftnref156" title="" href="#_ftn156">[156]</a>&nbsp;in careful research on this phenomenon, helping to draw out the salutary 
potential within the various fields of science and reality, and guiding them 
always towards ethically sound applications that clearly serve the cohesion of 
our societies and the common good, reaching new frontiers in the dialogue 
between faith and reason. </p>
<p>84. Moreover, it should be noted that current AI 
programs have been known to provide biased or fabricated information, which can 
lead students to trust inaccurate content. This problem “not only runs the risk 
of legitimizing fake news and strengthening a dominant culture’s advantage, but, 
in short, it also undermines the educational process itself.”<a name="_ftnref157" title="" href="#_ftn157">[157]</a>&nbsp;With time, clearer distinctions may emerge between proper and improper 
uses of AI in education and research. Yet, a decisive guideline is that the use 
of AI should always be transparent and never misrepresented.</p>
<p><i><a name="AI,_Misinformation">AI, Misinformation</a>, Deepfakes, and Abuse</i></p>
<p>85. AI could be used as an 
aid to human dignity if it helps people understand complex concepts or directs 
them to sound resources that support their search for the truth.<a name="_ftnref158" title="" href="#_ftn158">[158]</a></p>
<p>86. However, AI also 
presents a serious risk of generating manipulated content and false information, 
which can easily mislead people due to its resemblance to the truth. Such 
misinformation might occur unintentionally, as in the case of AI 
“hallucination,” where a generative AI system yields results that appear real 
but are not. Since generating content that mimics human artifacts is central to 
AI’s functionality, mitigating these risks proves challenging. Yet, the 
consequences of such aberrations and false information can be quite grave. For 
this reason, all those involved in producing and using AI systems should be 
committed to the truthfulness and accuracy of the information processed by such 
systems and disseminated to the public. </p>
<p>87. While AI has a latent 
potential to generate false information, an even more troubling problem lies in 
the deliberate misuse of AI for manipulation. This can occur when individuals or 
organizations intentionally generate and spread false content with the aim to 
deceive or cause harm, such as “deepfake” images, videos, and audio—referring to 
a false depiction of a person, edited or generated by an AI algorithm. The 
danger of deepfakes is particularly evident when they are used to target or harm 
others. While the images or videos themselves may be artificial, the damage they 
cause is real, leaving “deep scars in the hearts of those who suffer it” and 
“real wounds in their human dignity.”<a name="_ftnref159" title="" href="#_ftn159">[159]</a></p>
<p>88. On a broader scale, by 
distorting “our relationship with others and with reality,”<a name="_ftnref160" title="" href="#_ftn160">[160]</a>&nbsp;AI-generated fake media can gradually undermine the foundations of 
society. This issue requires careful regulation, as misinformation—especially 
through AI-controlled or influenced media—can spread unintentionally, fueling 
political polarization and social unrest. When society becomes indifferent to 
the truth, various groups construct their own versions of “facts,” weakening the 
“reciprocal ties and mutual dependencies”<a name="_ftnref161" title="" href="#_ftn161">[161]</a>&nbsp;that underpin the fabric of social life. As deepfakes cause people to 
question everything and AI-generated false content erodes trust in what they see 
and<b> </b>hear, polarization and conflict will only grow. Such widespread 
deception is no trivial matter; it strikes at the core of humanity, dismantling 
the foundational trust on which societies are built.<a name="_ftnref162" title="" href="#_ftn162">[162]</a></p>
<p>89. Countering AI-driven 
falsehoods is not only the work of industry experts—it requires the efforts of 
all people of goodwill. “If technology is to serve human dignity and not harm 
it, and if it is to promote peace rather than violence, then the human community 
must be proactive in addressing these trends with respect to human dignity and 
the promotion of the good.”<a name="_ftnref163" title="" href="#_ftn163">[163]</a>&nbsp;Those who produce and share AI-generated content should always exercise 
diligence in verifying the truth of what they disseminate and, in all cases, 
should “avoid the sharing of words and images that are degrading of human 
beings, that promote hatred and intolerance, that debase the goodness and 
intimacy of human sexuality or that exploit the weak and vulnerable.”<a name="_ftnref164" title="" href="#_ftn164">[164]</a>&nbsp;This calls for the ongoing prudence and careful discernment of all users 
regarding their activity online.<a name="_ftnref165" title="" href="#_ftn165">[165]</a></p>
<p><i><a name="AI,_Privacy">AI, Privacy</a>, and Surveillance</i></p>
<p>90. &nbsp;Humans are inherently 
relational, and the data each person generates in the digital world can be seen 
as an objectified expression of this relational nature. Data conveys not only 
information but also personal and relational knowledge, which, in an 
increasingly digitized context, can amount to power over the individual. 
Moreover, while some types of data may pertain to public aspects of a person’s 
life, others may touch upon the individual’s interiority, perhaps even their 
conscience. Seen in this way, privacy plays an essential role in protecting the 
boundaries of a person’s inner life, preserving their freedom to relate to 
others, express themselves, and make decisions without undue control. This 
protection is also tied to the defense of religious freedom, as surveillance can 
also be misused to exert control over the lives of believers and how they 
express their faith.</p>
<p>91. It is appropriate, 
therefore, to address the issue of privacy from a concern for the legitimate 
freedom and inalienable dignity of the human person “in all circumstances.”<a name="_ftnref166" title="" href="#_ftn166">[166]</a> 
The Second Vatican Council included the right “to safeguard privacy” among the 
fundamental rights “necessary for living a genuinely human life,” a right that 
should be extended to all people on account of their “sublime dignity.”<a name="_ftnref167" title="" href="#_ftn167">[167]</a> 
Furthermore, the Church has also affirmed the right to the legitimate respect 
for a private life in the context of affirming the person’s right to a good 
reputation, defense of their physical and mental integrity, and freedom from 
harm or undue intrusion<a name="_ftnref168" title="" href="#_ftn168">[168]</a>—essential 
components of the due respect for the intrinsic dignity of the human person.<a name="_ftnref169" title="" href="#_ftn169">[169]</a></p>
<p>92. Advances in AI-powered 
data processing and analysis now make it possible to infer patterns in a 
person’s behavior and thinking from even a small amount of information, making 
the role of data privacy even more imperative as a safeguard for the dignity and 
relational nature of the human person. As Pope Francis observed, “while closed 
and intolerant attitudes towards others are on the rise, distances are otherwise 
shrinking or disappearing to the point that the right to privacy scarcely 
exists. Everything has become a kind of spectacle to be examined and inspected, 
and people’s lives are now under constant surveillance.”<a name="_ftnref170" title="" href="#_ftn170">[170]</a></p>
<p>93. While there can be 
legitimate and proper ways to use AI in keeping with human dignity and the 
common good, using it for surveillance aimed at exploiting, restricting others’ 
freedom, or benefitting a few at the expense of the many is unjustifiable. The 
risk of surveillance overreach must be monitored by appropriate regulators to 
ensure transparency and public accountability. Those responsible for 
surveillance should never exceed their authority, which must always favor the 
dignity and freedom of every person as the essential basis of a just and humane 
society.</p>
<p>94. Furthermore, 
“fundamental respect for human dignity demands that we refuse to allow the 
uniqueness of the person to be identified with a set of data.”<a name="_ftnref171" title="" href="#_ftn171">[171]</a> 
This especially applies when AI is used to evaluate individuals or groups based 
on their behavior, characteristics, or history—a practice known as “social 
scoring”: “In social and economic decision-making, we should be cautious about 
delegating judgments to algorithms that process data, often collected 
surreptitiously, on an individual’s makeup and prior behavior. Such data can be 
contaminated by societal prejudices and preconceptions. A person’s past behavior 
should not be used to deny him or her the opportunity to change, grow, and 
contribute to society. We cannot allow algorithms to limit or condition respect 
for human dignity, or to exclude compassion, mercy, forgiveness, and above all, 
the hope that people are able to change.”<a name="_ftnref172" title="" href="#_ftn172">[172]</a> 
</p>
<p><i><a name="AI_and_the_Protection">AI and the Protection</a> of Our Common Home</i></p>
<p>95. AI has many promising 
applications for improving our relationship with our “common home,” such as 
creating models to forecast extreme climate events, proposing engineering 
solutions to reduce their impact, managing relief operations, and predicting 
population shifts.<a name="_ftnref173" title="" href="#_ftn173">[173]</a> 
Additionally, AI can support sustainable agriculture, optimize energy usage, and 
provide early warning systems for public health emergencies. These advancements 
have the potential to strengthen resilience against climate-related challenges 
and promote more sustainable development.</p>
<p>96. At the same time, 
current AI models and the hardware required to support them consume vast amounts 
of energy and water, significantly contributing to CO<sub>2</sub> emissions and 
straining resources. This reality is often obscured by the way this technology 
is presented in the popular imagination, where words such as “the cloud”<a name="_ftnref174" title="" href="#_ftn174"> [174]</a>&nbsp;can give the impression that data is stored and processed in an intangible 
realm, detached from the physical world. However, “the cloud” is not an ethereal 
domain separate from the physical world; as with all computing technologies, it 
relies on physical machines, cables, and energy. The same is true of the 
technology behind AI. As these systems grow in 
complexity, especially large language models (LLMs), they require ever-larger 
datasets, increased computational power, and greater storage infrastructure. 
Considering the heavy toll these technologies take on the environment, it is 
vital to develop sustainable solutions that reduce their impact on our common 
home.</p>
<p>97. &nbsp;Even then, as Pope 
Francis teaches, it is essential “that we look for solutions not only in 
technology but in a change of humanity.”<a name="_ftnref175" title="" href="#_ftn175">[175]</a> 
A complete and authentic understanding of creation recognizes that the value of 
all created things cannot be reduced to their mere utility. Therefore, a fully 
human approach to the stewardship of the earth rejects the distorted 
anthropocentrism of the technocratic paradigm, which seeks to “extract 
everything possible” from the world,<a name="_ftnref176" title="" href="#_ftn176"> [176]</a> 
and rejects the “myth of progress,” which assumes that “ecological problems will 
solve themselves simply with the application of new technology and without any 
need for ethical considerations or deep change.”<a name="_ftnref177" title="" href="#_ftn177">[177]</a> 
Such a mindset must give way to a more holistic approach that respects the order 
of creation and promotes the integral good of the human person while 
safeguarding our common home.<a name="_ftnref178" title="" href="#_ftn178"> [178]</a> 
</p>
<p><i><a name="AI_and_Warfare">AI and Warfare</a></i></p>
<p>98. The Second Vatican 
Council and the consistent teaching of the Popes since then have insisted that 
peace is not merely the absence of war and is not limited to maintaining a 
balance of powers between adversaries. Instead, in the words of Saint Augustine, 
peace is “the tranquility of order.”<a name="_ftnref179" title="" href="#_ftn179"> [179]</a>&nbsp;Indeed, peace cannot be attained without safeguarding the goods of 
persons, free communication, respect for the dignity of persons and peoples, and 
the assiduous practice of fraternity. Peace is the work of justice and the 
effect of charity and cannot be achieved through force alone; instead, it must 
be principally built through patient diplomacy, the active promotion of justice, 
solidarity, integral human development, and respect for the dignity of all 
people.<a name="_ftnref180" title="" href="#_ftn180">[180]</a>&nbsp;In this way, the tools used to maintain peace should never be allowed to 
justify injustice, violence, or oppression. Instead, they should always be 
governed by a “firm determination to respect other people and nations, along 
with their dignity, as well as the deliberate practice of fraternity.”<a name="_ftnref181" title="" href="#_ftn181">[181]</a></p>
<p>99. While AI’s analytical 
abilities could help nations seek peace and ensure security, the “weaponization 
of Artificial Intelligence” can also be highly problematic. Pope Francis has 
observed that “the ability to conduct military operations through remote control 
systems has led to a lessened perception of the devastation caused by those 
weapon systems and the burden of responsibility for their use, resulting in an 
even more cold and detached approach to the immense tragedy of war.”<a name="_ftnref182" title="" href="#_ftn182">[182]</a>&nbsp;Moreover, the ease with which autonomous weapons make war more viable 
militates against the principle of war as a last resort in legitimate 
self-defense,<a name="_ftnref183" title="" href="#_ftn183">[183]</a>&nbsp;potentially increasing the instruments of war well beyond the scope of 
human oversight and precipitating a destabilizing arms race, with catastrophic 
consequences for human rights.<a name="_ftnref184" title="" href="#_ftn184">[184]</a></p>
<p>100. In particular, Lethal Autonomous Weapon 
Systems, which are capable of identifying and striking targets without direct 
human intervention, are a “cause for grave ethical concern” because they lack 
the “unique human capacity for moral judgment and ethical decision-making.”<a name="_ftnref185" title="" href="#_ftn185">[185]</a>&nbsp;For this reason, Pope Francis has urgently called for a reconsideration of 
the development of these weapons and a prohibition on their use, starting with 
“an effective and concrete commitment to introduce ever greater and proper human 
control. No machine should ever choose to take the life of a human being.”<a name="_ftnref186" title="" href="#_ftn186">[186]</a></p>
<p>101. Since it is a small step from machines that 
can kill autonomously with precision to those capable of large-scale 
destruction, some AI researchers have expressed concerns that such technology 
poses an “existential risk” by having the potential to act in ways that could 
threaten the survival of entire regions or even of humanity itself. This danger 
demands serious attention, reflecting the long-standing concern about 
technologies that grant war “an uncontrollable destructive power over great 
numbers of innocent civilians,”<a name="_ftnref187" title="" href="#_ftn187">[187]</a>&nbsp;without even sparing children. In this context, the call from <i>
<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a></i> to “undertake an evaluation of war with an entirely new attitude”<a name="_ftnref188" title="" href="#_ftn188">[188]</a>&nbsp;is more urgent than ever.</p>
<p>102. At the same time, while the theoretical risks 
of AI deserve attention, the more immediate and pressing concern lies in how 
individuals with malicious intentions might misuse this technology.<a name="_ftnref189" title="" href="#_ftn189">[189]</a>&nbsp;Like any tool, AI is an extension of human power, and while its future 
capabilities are unpredictable, humanity’s past actions provide clear warnings. 
The atrocities committed throughout history are enough to raise deep concerns 
about the potential abuses of AI.</p>
<p>103. Saint John Paul II observed that “humanity 
now has instruments of unprecedented power: we can turn this world into a 
garden, or reduce it to a pile of rubble.”<a name="_ftnref190" title="" href="#_ftn190">[190]</a>&nbsp;Given this fact, the Church reminds us, in the words of Pope Francis, that 
“we are free to apply our intelligence towards things evolving positively,” or 
toward “decadence and mutual destruction.”<a name="_ftnref191" title="" href="#_ftn191">[191]</a>&nbsp;To prevent humanity from spiraling into self-destruction,<a name="_ftnref192" title="" href="#_ftn192">[192]</a>&nbsp;there must be a clear stand against all applications of technology that 
inherently threaten human life and dignity. This commitment requires careful 
discernment about the use of AI, particularly in military defense applications, 
to ensure that it always respects human dignity and serves the common good. The 
development and deployment of AI in armaments should be subject to the highest 
levels of ethical scrutiny, governed by a concern for human dignity and the 
sanctity of life.<a name="_ftnref193" title="" href="#_ftn193">[193]</a></p>
<p><i><a name="AI_and_Our">AI and Our</a> Relationship with God</i></p>
<p>104. Technology offers remarkable tools to oversee 
and develop the world’s resources. However, in some cases, humanity is increasingly ceding control of these 
resources to machines. Within some circles of scientists and futurists, there is 
optimism about the potential of artificial general intelligence (AGI), a 
hypothetical form of AI that would match or surpass human intelligence and bring 
about unimaginable advancements. Some even speculate that AGI could achieve 
superhuman capabilities. At the same time, as society drifts away from a 
connection with the transcendent, some are tempted to turn to AI in search of 
meaning or fulfillment—longings that can only be truly satisfied in communion 
with God.<a name="_ftnref194" title="" href="#_ftn194">[194]</a></p>
<p>105. However, <i>the presumption of substituting 
God for an artifact of human making is idolatry</i>, a practice Scripture 
explicitly warns against (e.g., Ex. 20:4; 32:1-5; 34:17). Moreover, AI may prove 
even more seductive than traditional idols for, unlike idols that “have mouths 
but do not speak; eyes, but do not see; ears, but do not hear” (Ps. 115:5-6), AI 
can “speak,” or at least gives the illusion of doing so (cf. Rev. 13:15). Yet, 
it is vital to remember that AI is but a pale reflection of humanity—it is 
crafted by human minds, trained on human-generated material, responsive to human 
input, and sustained through human labor. AI cannot possess many of the 
capabilities specific to human life, and it is also fallible. By turning to AI 
as a perceived “Other” greater than itself, with which to share existence and 
responsibilities, humanity risks creating a substitute for God. However, it is 
not AI that is ultimately deified and worshipped, but humanity itself—which, in 
this way, becomes enslaved to its own work.<a name="_ftnref195" title="" href="#_ftn195">[195]</a> 
</p>
<p>106. While AI has the potential to serve humanity 
and contribute to the common good, it remains a creation of human hands, bearing 
“the imprint of human art and ingenuity” (Acts 17:29). It must never be ascribed 
undue worth. As the Book of Wisdom affirms: “For a man made them, and one whose 
spirit is borrowed formed them; for no man can form a god which is like himself. 
He is mortal, and what he makes with lawless hands is dead, for he is better 
than the objects he worships since he has life, but they never have” (Wis. 
15:16-17). </p>
<p>107. In contrast, human beings, “by their interior 
life, transcend the entire material universe; they experience this deep 
interiority when they enter into their own heart, where God, who probes the 
heart, awaits them, and where they decide their own destiny in the sight of 
God.”<a name="_ftnref196" title="" href="#_ftn196">[196]</a> It is within the 
heart, as Pope Francis reminds us, that each individual discovers the 
“mysterious connection between self-knowledge and openness to others, between 
the encounter with one’s personal uniqueness and the willingness to give oneself 
to others.”<a name="_ftnref197" title="" href="#_ftn197">[197]</a> Therefore, it is 
the heart alone that is “capable of setting our other powers and passions, and 
our entire person, in a stance of reverence and loving obedience before the 
Lord,”<a name="_ftnref198" title="" href="#_ftn198">[198]</a> who “offers to treat 
each one of us as a ‘Thou,’ always and forever.”<a name="_ftnref199" title="" href="#_ftn199">[199]</a> 
</p>
<p>VI. <b> <a name="Concluding_Reflections">Concluding Reflections</a></b></p>
<p>108. Considering the various challenges posed by 
advances in technology, Pope Francis emphasized the need for growth in “human 
responsibility, values, and conscience,” proportionate to the growth in the 
potential that this technology brings<a name="_ftnref200" title="" href="#_ftn200">[200]</a>—recognizing that “with an increase in human power comes a broadening of 
responsibility on the part of individuals and communities.”<a name="_ftnref201" title="" href="#_ftn201">[201]</a></p>
<p>109. At the same time, the “essential and 
fundamental question” remains “whether in the context of this progress man, as 
man, is becoming truly better, that is to say, more mature spiritually, more 
aware of the dignity of his humanity, more responsible, more open to others, 
especially the neediest and the weakest, and readier to give and to aid all.”<a name="_ftnref202" title="" href="#_ftn202">[202]</a></p>
<p>110. As a result, it is crucial to know how to 
evaluate individual applications of AI in particular contexts to determine 
whether its use promotes human dignity, the vocation of the human person, and 
the common good. As with many technologies, the effects of the various uses of 
AI may not always be predictable from their inception. As these applications and 
their social impacts become clearer, appropriate responses should be made at all 
levels of society, following the principle of subsidiarity. Individual users, 
families, civil society, corporations, institutions, governments, and 
international organizations should work at their proper levels to ensure that AI 
is used for the good of all. </p>
<p>111. A significant challenge and opportunity for 
the common good today lies in considering AI within a framework of relational 
intelligence, which emphasizes the interconnectedness of individuals and 
communities and highlights our shared responsibility for fostering the integral 
well-being of others. The twentieth-century philosopher Nicholas Berdyaev 
observed that people often blame machines for personal and social problems; 
however, “this only humiliates man and does not correspond to his dignity,” for 
“it is unworthy to transfer responsibility from man to a machine.”<a name="_ftnref203" title="" href="#_ftn203">[203]</a> 
Only the human person can be morally responsible, and the challenges of a 
technological society are ultimately <i>spiritual</i> in nature. Therefore, 
facing those challenges “demands an intensification of spirituality.”<a name="_ftnref204" title="" href="#_ftn204">[204]</a></p>
<p>112. A further point to consider is the call, 
prompted by the appearance of AI on the world stage, for a <i>renewed 
appreciation of all that is human</i>. Years ago, the 
French Catholic author Georges Bernanos warned that “the danger is not in the 
multiplication of machines, but in the ever-increasing number of men accustomed 
from their childhood to desire only what machines can give.”<a name="_ftnref205" title="" href="#_ftn205">[205]</a> 
This challenge is as true today as it was then, as the rapid pace of 
digitization risks a “digital reductionism,” where non-quantifiable aspects of 
life are set aside and then forgotten or even deemed irrelevant because they 
cannot be computed in formal terms. AI should be used only as a tool to 
complement human intelligence rather than replace its richness.<a name="_ftnref206" title="" href="#_ftn206">[206]</a> Cultivating those aspects of human life that transcend computation is 
crucial for preserving “an authentic humanity” that “seems to dwell in the midst 
of our technological culture, almost unnoticed, like a mist seeping gently 
beneath a closed door.”<a name="_ftnref207" title="" href="#_ftn207">[207]</a></p>
<p><i><a name="True_Wisdom">True Wisdom</a></i></p>
<p>113. The vast expanse of the world’s knowledge is 
now accessible in ways that would have filled past generations with awe. 
However, to ensure that advancements in knowledge do not become humanly or 
spiritually barren, one must go beyond the mere accumulation of data and strive 
to achieve true wisdom.<a name="_ftnref208" title="" href="#_ftn208">[208]</a></p>
<p>114. This wisdom is the gift that humanity needs 
most to address the profound questions and ethical challenges posed by AI: “Only 
by adopting a spiritual way of viewing reality, only by recovering a wisdom of 
the heart, can we confront and interpret the newness of our time.”<a name="_ftnref209" title="" href="#_ftn209">[209]</a>&nbsp;Such “wisdom of the heart” is “the virtue that enables us to integrate the 
whole and its parts, our decisions and their consequences.” It “cannot be sought 
from machines,” but it “lets itself be found by those who seek it and be seen by 
those who love it; it anticipates those who desire it, and it goes in search of 
those who are worthy of it (cf. Wis 6:12-16).”<a name="_ftnref210" title="" href="#_ftn210">[210]</a></p>
<p>115. In a world marked by AI, we need the grace of 
the Holy Spirit, who “enables us to look at things with God’s eyes, to see 
connections, situations, events and to uncover their real meaning.”<a name="_ftnref211" title="" href="#_ftn211">[211]</a></p>
<p>116. Since a “person’s perfection is measured not 
by the information or knowledge they possess, but by the depth of their 
charity,”<a name="_ftnref212" title="" href="#_ftn212">[212]</a>&nbsp;how we incorporate AI “to include the least of our brothers and sisters, 
the vulnerable, and those most in need, will be the true measure of our 
humanity.”<a name="_ftnref213" title="" href="#_ftn213">[213]</a>&nbsp;The “wisdom of the heart” can illuminate and guide the human-centered use 
of this technology to help promote the common good, care for our “common home,” 
advance the search for the truth, foster integral human development, favor human 
solidarity and fraternity, and lead humanity to its ultimate goal: happiness and 
full communion with God.<a name="_ftnref214" title="" href="#_ftn214">[214]</a></p>
<p>117. From this perspective of wisdom, believers 
will be able to act as moral agents capable of using this technology to promote 
an authentic vision of the human person and society.<a name="_ftnref215" title="" href="#_ftn215">[215]</a>&nbsp;This should be done with the understanding that technological progress is 
part of God’s plan for creation—an activity that we are called to order toward 
the Paschal Mystery of Jesus Christ, in the continual search for the True and 
the Good.</p>
<p><i>The Supreme Pontiff, Francis, at the Audience granted on 14 January 2025 to the 
undersigned Prefects and Secretaries of the Dicastery for the Doctrine of the 
Faith and the Dicastery for Culture and Education, approved this </i>Note<i> and ordered its publication.</i></p>
<p>Given in Rome, at the offices of the Dicastery for the Doctrine of the Faith and 
the Dicastery for Culture and Education, on 28 January 2025, the Liturgical 
Memorial of Saint Thomas Aquinas, Doctor of the Church.</p>

<div>
	<table>
		<tbody><tr>
			<td>Víctor Manuel Card. Fernández <br>
			Prefect</td>
			<td>José Card. Tolentino de Mendonça<br>
			Prefect</td>
		</tr>
		<tr>
			<td>&nbsp;</td>
			<td>&nbsp;</td>
		</tr>
		<tr>
			<td>Msgr. Armando Matteo<br>
			Secretary, Doctrinal Section</td>
			<td>Most Rev. Paul Tighe<br>
			Secretary, Culture Section </td>
		</tr>
	</tbody></table>
</div>

<p><i>Ex audientia die 14 ianuarii 2025<br>
Franciscus</i><br clear="all">
			</p>
			
<p><b>Contents</b></p>
<p><b>I. <a href="#Introduction">Introduction</a></b></p>
<p><b>II. <a href="#What_is_Artificial_Intelligence">What is Artificial Intelligence?</a></b></p>
<p><b>III. <a href="#Intelligence_in_the_Philosophical">Intelligence in the Philosophical and Theological Tradition</a></b></p>
<blockquote>
	<p><a href="#Rationality">Rationality</a></p>
	<p><a href="#Embodiment">Embodiment</a></p>
	<p><a href="#Relationality">Relationality</a></p>
	<p><a href="#Relationship_with_the_Truth">Relationship with the Truth</a></p>
	<p><a href="#Stewardship_of_the_World">Stewardship of the World</a></p>
	<p><a href="#An_Integral">An Integral Understanding of Human Intelligence</a></p>
	<p><a href="#The_Limits_of_AI">The Limits of AI</a></p>
</blockquote>
<p><b>IV. <a href="#The_Role_of_Ethics">The Role of Ethics in Guiding the Development and Use of AI</a></b></p>
<blockquote>
	<p><a href="#Helping_Human">Helping Human Freedom and Decision-Making</a></p>
</blockquote>
<p><b>V. <a href="#Specific_Questions">Specific Questions</a></b></p>
<blockquote>
	<p><a href="#AI_and_Society">AI and Society</a></p>
	<p><a href="#AI_and_Human_Relationships">AI and Human Relationships</a></p>
	<p><a href="#AI,_the_Economy,_and_Labor">AI, the Economy, and Labor</a></p>
	<p><a href="#AI_and_Healthcare">AI and Healthcare</a></p>
	<p><a href="#AI_and_Education">AI and Education</a></p>
	<p><a href="#AI,_Misinformation">AI, Misinformation, Deepfakes, and Abuse</a></p>
	<p><a href="#AI,_Privacy">AI, Privacy, and Surveillance</a></p>
	<p><a href="#AI_and_the_Protection">AI and the Protection of Our Common Home</a></p>
	<p><a href="#AI_and_Warfare">AI and Warfare</a></p>
	<p><a href="#AI_and_Our">AI and Our Relationship with God</a></p>
</blockquote>
<p><b>VI. <a href="#Concluding_Reflections">Concluding Reflections</a></b></p>
<blockquote>
	<p><a href="#True_Wisdom">True Wisdom</a></p>
</blockquote>
<div>
	&nbsp;<hr size="1">
	<p><a name="_ftn1" title="" href="#_ftnref1">[1]</a>&nbsp;<i>Catechism of the Catholic Church</i>, par. 378. See also Second Vatican Ecumenical Council, 
Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 34: <i>AAS</i> 
58 (1966), 1052-1053.</p>
	
	
	<p><a name="_ftn4" title="" href="#_ftnref4">[4]</a>&nbsp;Cf. <i>Catechism of the Catholic Church</i>, par. 2293; Second Vatican Ecumenical Council, 
Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 35: <i>AAS
</i>58 (1966), 1053.</p>
	<p><a name="_ftn5" title="" href="#_ftnref5">[5]</a>&nbsp;J. McCarthy, <i>et al</i>., “A Proposal for the Dartmouth Summer Research 
Project on Artificial Intelligence” (31 August 1955), 
http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html (accessed: 21 October 2024).</p>
	
	<p><a name="_ftn7" title="" href="#_ftnref7">[7]</a>&nbsp;Terms in this document describing the outputs or processes of AI are used 
figuratively to explain its operations and are not intended to anthropomorphize 
the machine.</p>
	
	<p><a name="_ftn9" title="" href="#_ftnref9">[9]</a>&nbsp;Here, one can see the primary positions of the “transhumanists” and the 
“posthumanists.” <i>Transhumanists</i> argue that technological advancements 
will enable humans to overcome their biological limitations and enhance both 
their physical and cognitive abilities. <i>Posthumanists</i>, on the other hand, 
contend that such advances will ultimately alter human identity to the extent 
that humanity itself may no longer be considered truly “human.” Both views rest 
on a fundamentally negative perception of human corporality, which treats the 
body more as an obstacle than as an integral part of the person’s identity and 
call to full realization. Yet, this negative view of the body is inconsistent 
with a proper understanding of human dignity. While the Church supports genuine 
scientific progress, it affirms that human dignity is rooted in “the person as 
an inseparable unity of body and soul.” Thus, “dignity is also inherent in each 
person’s body, which participates in its own way in being <i>in</i> <i>imago Dei</i>” 
(Dicastery for the Doctrine of the Faith, Declaration <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a> </i>[8 April 2024], par. 18).</p>
	<p><a name="_ftn10" title="" href="#_ftnref10">[10]</a>&nbsp;This approach reflects a functionalist perspective, which reduces the 
human mind to its functions and assumes that its functions can be entirely 
quantified in physical or mathematical terms. However, even if a future AGI were 
to appear truly intelligent, it would still remain functional in nature.</p>
	<p><a name="_ftn11" title="" href="#_ftnref11">[11]</a>&nbsp;Cf. A.M. Turing, “Computing Machinery and Intelligence,” <i>Mind </i>59 
(1950) 443-460.</p>
	<p><a name="_ftn12" title="" href="#_ftnref12">[12]</a>&nbsp;If “thinking” is attributed to machines, it must 
be clarified that this refers to calculative thinking rather than critical 
thinking. Similarly, if machines are said to operate using logical thinking, it 
must be specified that this is limited to computational logic. On the other 
hand, by its very nature, human thought is a creative process that eludes 
programming and transcends constraints.</p>
	<p><a name="_ftn13" title="" href="#_ftnref13">[13]</a>&nbsp;On the foundational role of language in shaping understanding, cf. M. Heidegger,
<i>Über den Humanismus</i>, Klostermann, Frankfurt am Main 1949 (en. tr. “Letter 
on Humanism,” in <i>Basic Writings: Martin Heidegger</i>, Routledge, London ‒ 
New York 2010, 141-182). </p>
	<p><a name="_ftn14" title="" href="#_ftnref14">[14]</a>&nbsp;For further discussion of these anthropological and theological 
foundations, see AI Research Group of the Centre for Digital Culture of the Dicastery for Culture 
and Education, <i>Encountering Artificial Intelligence: Ethical and 
Anthropological Investigations</i>&nbsp;(Theological Investigations of Artificial 
Intelligence 1), M.J. Gaudet, N. Herzfeld, P. Scherz, J.J. Wales, eds., <i>
Journal of Moral Theology</i>, Pickwick, Eugene 2024, 43-144.</p>
	<p><a name="_ftn15" title="" href="#_ftnref15">[15]</a>&nbsp;Aristotle, <i>Metaphysics</i>, I.1, 980 a 21.</p>
	<p><a name="_ftn16" title="" href="#_ftnref16">[16]</a> Cf. Augustine, <i>De Genesi ad litteram </i>III, 20, 30: PL 34, 292: “Man is 
made in the image of God in relation to that [faculty] by which he is superior 
to the irrational animals. Now, this [faculty] is reason itself, or the ‘mind,’ 
or ‘intelligence,’ whatever other name it may more suitably be given”;<b> </b>Id.,
<i>Enarrationes in Psalmos </i>54, 3: PL 36, 629: “When considering all that 
they have, humans discover that they are most distinguished from animals 
precisely by the fact they possess intelligence.” This is also reiterated by 
Saint Thomas Aquinas, who states that “man is the most perfect of all earthly 
beings endowed with motion, and his proper and natural operation is 
intellection,” by which man abstracts from things and “receives in his mind 
things actually intelligible” (Thomas Aquinas, <i>Summa Contra Gentiles </i>II, 
76).</p>
	<p><a name="_ftn17" title="" href="#_ftnref17">[17]</a>&nbsp;Cf. Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 15: <i>AAS </i>58 (1966), 1036.</p>
	<p><a name="_ftn18" title="" href="#_ftnref18">[18]</a>&nbsp;Aquinas, <i>Summa Theologiae</i>, II-II, q. 49, a. 5, ad 3. Cf. <i>ibid</i>., 
I, q. 79; II-II, q. 47, a. 3; II-II, q. 49, a. 2. For a contemporary perspective 
that echoes elements of the classical and medieval distinction between these two 
modes of cognition, cf. D. Kahneman, <i>Thinking, Fast and Slow</i>, New York<i>
</i>2011. </p>
	<p><a name="_ftn19" title="" href="#_ftnref19">[19]</a>&nbsp;Aquinas, <i>Summa Theologiae</i>, I, q. 76, a. 1, <i>resp</i>. 
	</p>
	<p><a name="_ftn20" title="" href="#_ftnref20">[20]</a>&nbsp;Cf. Irenaeus of Lyon, <i>Adversus Haereses</i>,<i> </i>V, 6, 1: PG 7(2), 
1136-1138.</p>
	<p><a name="_ftn21" title="" href="#_ftnref21">[21]</a>&nbsp;Dicastery for the Doctrine of the Faith, Declaration <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a> </i>(8 April 2024), par. 9. Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a></i> 
(3 October 2020), par. 213: <i>AAS </i>112 (2020), 1045: “The intellect can 
investigate the reality of things through reflection, experience and dialogue, 
and come to recognize in that reality, which transcends it, the basis of certain 
universal moral demands.”</p>
	
	<p><a name="_ftn23" title="" href="#_ftnref23">[23]</a>&nbsp;<i>Catechism of the Catholic Church</i>, par. 365. Cf. Aquinas, <i>Summa 
Theologiae</i>, I, q. 75, a. 4, resp.</p>
	<p><a name="_ftn24" title="" href="#_ftnref24">[24]</a>&nbsp;Indeed, Sacred Scripture “generally considers the human person as a being 
who exists in the body and is unthinkable outside of it” (Pontifical Biblical Commission,
<i><a href="https://www.vatican.va/roman_curia/congregations/cfaith/pcb_documents/rc_con_cfaith_doc_20190930_cosa-e-luomo_sp.pdf">“Che cosa è l’uomo?” (Sal 8,5): Un itinerario di antropologia biblica</a> </i>[30 
September 2019], par. 19). Cf. <i>ibid</i>., pars. 20-21, 43-44, 48. </p>
	<p><a name="_ftn25" title="" href="#_ftnref25">[25]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 22: <i>AAS </i>58 (1966), 1042: Cf. Congregation for the Doctrine of the Faith, 
Instruction <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_con_cfaith_doc_20081208_dignitas-personae_en.html">Dignitas Personae</a> </i>(8 September 2008), par. 7: <i>AAS </i>100 
(2008), 863: “Christ did not disdain human bodiliness, but instead fully 
disclosed its meaning and value.”</p>
	<p><a name="_ftn26" title="" href="#_ftnref26">[26]</a>&nbsp;Aquinas, <i>Summa Contra Gentiles</i> II, 81.</p>
	<p><a name="_ftn27" title="" href="#_ftnref27">[27]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 15: <i>AAS </i>58 (1966), 1036.</p>
	<p><a name="_ftn28" title="" href="#_ftnref28">[28]</a>&nbsp;Cf. Aquinas<i>, Summa Theologiae </i>I, q. 89, a. 1, <i>resp</i>.: “to be 
separated from the body is not in accordance with [the soul’s] nature […] and 
hence it is united to the body in order that it may have an existence and an 
operation suitable to its nature.”</p>
	<p><a name="_ftn29" title="" href="#_ftnref29">[29]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 14: <i>AAS </i>58 (1966), 1035. Cf. Dicastery for the Doctrine of the Faith, Declaration 
		<i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a> </i>(8 April 2024), par. 18.</p>
	
	<p><a name="_ftn31" title="" href="#_ftnref31">[31]</a>&nbsp;Cf. Congregation for the Doctrine of the Faith, Instruction <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_con_cfaith_doc_20081208_dignitas-personae_en.html">Dignitas Personae</a> </i>(8 September 2008), pars. 5, 8; Dicastery for the Doctrine of the Faith, Declaration 
		<i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a> </i>(8 April 2024), pars. 15, 24, 53-54.</p>
	<p><a name="_ftn32" title="" href="#_ftnref32">[32]</a>&nbsp;<i>Catechism of the Catholic Church</i>, par. 356. Cf. <i>ibid.</i>, par. 
221.</p>
	<p><a name="_ftn33" title="" href="#_ftnref33">[33]</a>&nbsp;Cf. Dicastery for the Doctrine of the Faith, Declaration <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a> </i>(8 April 2024), pars. 13, 26-27.</p>
	<p><a name="_ftn34" title="" href="#_ftnref34">[34]</a>&nbsp;Congregation for the Doctrine of the Faith, Instruction <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_con_cfaith_doc_19900524_theologian-vocation_en.html">Donum Veritatis</a></i> 
(24 May 1990), 6: <i>AAS</i> 82 (1990), 1552. Cf. John Paul II, Encyclical <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_06081993_veritatis-splendor.html">Veritatis Splendor</a> </i>(6 August 1993), par. 109: <i>AAS </i>85 (1993), 1219. 
Cf. Pseudo-Dionysius, <i>De divinis nominibus, </i>VII, 2: PG 3, 868B-C: “Human 
souls also possess reason and with it they circle in discourse around the truth 
of things. […] [O]n account of the manner in which they are capable of 
concentrating the many into the one, they too, in their own fashion and as far 
as they can, are worthy of conceptions like those of the angels” (en. tr. <i>
Pseudo-Dionysius: The Complete Works</i>, Paulist Press, New York – Mahwah 1987, 
106-107).</p>
	<p><a name="_ftn35" title="" href="#_ftnref35">[35]</a>&nbsp;John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_14091998_fides-et-ratio.html">Fides et Ratio</a></i> (14 September 1998), 
par. 3: <i>AAS </i>91 (1999), 7.</p>
	<p><a name="_ftn36" title="" href="#_ftnref36">[36]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 15: <i>AAS </i>58 (1966), 1036.</p>
	<p><a name="_ftn37" title="" href="#_ftnref37">[37]</a>&nbsp;John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_14091998_fides-et-ratio.html">Fides et Ratio</a></i> (14 September 1998), 
par. 42: <i>AAS </i>91 (1999), 38. Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a></i> (3 October 2020), par. 208: <i>AAS </i>112 (2020), 1043: “the human 
mind is capable of transcending immediate concerns and grasping certain truths 
that are unchanging, as true now as in the past. As it peers into human nature, 
reason discovers universal values derived from that same nature”; <i>ibid</i>., 
par. 184: <i>AAS </i>112 (2020), 1034.</p>
	<p><a name="_ftn38" title="" href="#_ftnref38">[38]</a>&nbsp;Cf. B. Pascal, <i>Pensées</i>, no. 267 (ed. Brunschvicg): “The last 
proceeding of reason is to recognize that there is an infinity of things which 
are beyond it” (en. tr. <i>Pascal’s Pensées</i>, E.P. Dutton, New York 1958, 
77).</p>
	<p><a name="_ftn39" title="" href="#_ftnref39">[39]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 15: <i>AAS </i>58 (1966), 1036. Cf. Congregation for the Doctrine of the Faith,
		<i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_con_cfaith_doc_20071203_nota-evangelizzazione_en.html">Doctrinal Note on Some 
Aspects of Evangelization</a></i> (3 December 2007), par. 
4: <i>AAS</i> 100 (2008), 491-492.</p>
	<p><a name="_ftn40" title="" href="#_ftnref40">[40]</a>&nbsp;Our <i>semantic capacity</i> allows us to understand messages in any form 
of communication in a manner that both takes into account and transcends their 
material or empirical structures (such as computer code). Here, intelligence 
becomes a wisdom that “enables us to look at things with God’s eyes, to see 
connections, situations, events and to uncover their real meaning” (Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/messages/communications/documents/20240124-messaggio-comunicazioni-sociali.html">Message for the LVIII World Day of Social Communications</a> </i> 
		[24 January 2024]:
<i>L’Osservatore Romano</i>, 24 January 2024, 8). Our <i>creativity</i> enables 
us to generate new content or ideas, primarily by offering an original viewpoint 
on reality. Both capacities depend on the existence of a personal subjectivity 
for their full realization.</p>
	<p><a name="_ftn41" title="" href="#_ftnref41">[41]</a>&nbsp;Second Vatican Ecumenical Council, Declaration <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_decl_19651207_dignitatis-humanae_en.html">Dignitatis Humanae</a></i>
(7 December 1965), par. 3: <i>AAS </i>58 (1966), 931. </p>
	<p><a name="_ftn42" title="" href="#_ftnref42">[42]</a>&nbsp;Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a></i> (3 October 2020), 
par. 184: <i>AAS</i> 112 (2020), 1034: “Charity, when accompanied by a 
commitment to the truth, is much more than personal feeling […]. Indeed, its 
close relation to truth fosters its universality and preserves it from being 
‘confined to a narrow field devoid of relationships.’ […] Charity’s openness to 
truth thus protects it from ‘a fideism that deprives it of its human and 
universal breadth.’” The internal quotes are from Benedict XVI, Encyclical 
Letter <i>
		<a href="https://www.vatican.va/content/benedict-xvi/en/encyclicals/documents/hf_ben-xvi_enc_20090629_caritas-in-veritate.html">Caritas in Veritate</a></i> (29 June 2009), pars. 2-4: <i>AAS</i> 101 
(2009), 642-643. </p>
	
	
	<p><a name="_ftn45" title="" href="#_ftnref45">[45]</a>&nbsp;John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_14091998_fides-et-ratio.html">Fides et Ratio</a></i> (14 September 1998), 
par. 13: <i>AAS</i> 91 (1999), 15.</p>
	<p><a name="_ftn46" title="" href="#_ftnref46">[46]</a>&nbsp;Bonaventure, <i>In II Librum Sententiarum</i>, d. I, p. 2, a. 2, q. 1; as 
quoted in <i>Catechism of the Catholic Church</i>, par. 293. Cf. <i>ibid.</i>, 
par. 294. </p>
	<p><a name="_ftn47" title="" href="#_ftnref47">[47]</a>&nbsp;Cf. <i>Catechism of the Catholic Church</i>, pars. 295, 299, 302. 
Bonaventure likens the universe to “a book reflecting, representing, and 
describing its Maker,” the Triune God who grants existence to all things (<i>Breviloquium</i> 
2.12.1). Cf. Alain de Lille, <i>De Incarnatione Christi</i>, PL 210, 579a: “<i>Omnis 
mundi creatura quasi liber et pictura nobis est et speculum.</i>”</p>
	<p><a name="_ftn48" title="" href="#_ftnref48">[48]</a>&nbsp;Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 67:
<i>AAS </i>107 (2015), 874; John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_14091981_laborem-exercens.html">Laborem Exercens</a> </i>(14 September 1981), par. 6: <i>AAS </i>73 (1981), 589-592; Second Vatican Ecumenical Council, 
Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), pars. 33-34: <i>
AAS </i>58 (1966), 1052-1053; International Theological Commission, <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/cti_documents/rc_con_cfaith_doc_20040723_communion-stewardship_en.html">Communion and Stewardship: Human 
Persons Created in the Image of God</a> </i>(2004), 
par. 57: “human beings occupy a unique place in the universe according to the 
divine plan: they enjoy the privilege of sharing in the divine governance of 
visible creation. […] Since man’s place as ruler is in fact a participation in 
the divine governance of creation, we speak of it here as a form of 
stewardship.”</p>
	<p><a name="_ftn49" title="" href="#_ftnref49">[49]</a>&nbsp;Cf. John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_06081993_veritatis-splendor.html">Veritatis Splendor</a> </i>(6 August 
1993), pars. 38-39: <i>AAS </i>85 (1993), 1164-1165.</p>
	<p><a name="_ftn50" title="" href="#_ftnref50">[50]</a> Cf.&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), pars. 33-34: <i>AAS </i>58 (1966), 1052-1053. This 
idea is also reflected in the creation account, where God brings creatures to 
Adam “to see what he would call them. And whatever [he] called every living 
creature, that was its name” (Gen. 2:19), an action that demonstrates the active 
engagement of human intelligence in the stewardship of God’s creation. Cf. John Chrysostom,
<i>Homiliae in Genesim</i>, XIV, 17-21: PG 53, 116-117.</p>
	<p><a name="_ftn51" title="" href="#_ftnref51">[51]</a>&nbsp;Cf. <i>Catechism of the Catholic Church</i>, par. 301. 
	</p>
	<p><a name="_ftn52" title="" href="#_ftnref52">[52]</a>&nbsp;Cf. <i>Catechism of the Catholic Church</i>, par. 302.</p>
	<p><a name="_ftn53" title="" href="#_ftnref53">[53]</a>&nbsp;Bonaventure, <i>Breviloquium </i>2.12.1. Cf. <i>ibid</i>., 2.11.2.</p>
	
	
	
	<p><a name="_ftn57" title="" href="#_ftnref57">[57]</a>&nbsp;Cf. Francis, Post-Synodal Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20200202_querida-amazonia.html">Querida Amazonia</a> </i>(2 
February 2020), par. 41: <i>AAS</i> 112 (2020), 246; Id., Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 146: <i>AAS </i>107 (2015), 906.</p>
	<p><a name="_ftn58" title="" href="#_ftnref58">[58]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 47: <i>
AAS </i>107 (2015), 864. Cf. Id., Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/20241024-enciclica-dilexit-nos.html">Dilexit Nos</a> </i>(24 
October 2024), pars. 17-24: <i>L’Osservatore Romano</i>, 24 October 2024, 5; Id., 
Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), par. 47-50: <i>AAS </i>
112 (2020), 985-987.</p>
	<p><a name="_ftn59" title="" href="#_ftnref59">[59]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/20241024-enciclica-dilexit-nos.html">Dilexit Nos</a></i> (24 October 2024), par. 20:
<i>L’Osservatore Romano</i>, 24 October 2024, 5.</p>
	<p><a name="_ftn60" title="" href="#_ftnref60">[60]</a>&nbsp;P. Claudel, <i>Conversation sur Jean Racine</i>, Gallimard, Paris 1956, 
32: “<i>L’intelligence n’est rien sans la délectation</i>.” Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/20241024-enciclica-dilexit-nos.html">Dilexit Nos</a> </i>(24 October 2024), par. 13:
<i>L’Osservatore Romano</i>, 24 October 2024, 5: “The mind and the will are put 
at the service of the greater good by sensing and savoring truths.”</p>
	<p><a name="_ftn61" title="" href="#_ftnref61">[61]</a>&nbsp;Dante, <i>Paradiso</i>, Canto XXX: “<i>luce intellettüal, piena d’amore; / 
amor di vero ben, pien di letizia; / letizia che trascende ogne dolzore</i>” 
(en. tr. <i>The Divine Comedy of Dante Alighieri</i>, C.E. Norton, tr., Houghton 
Mifflin, Boston 1920, 232).</p>
	<p><a name="_ftn62" title="" href="#_ftnref62">[62]</a>&nbsp;Cf. Second Vatican Ecumenical Council, Declaration <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_decl_19651207_dignitatis-humanae_en.html">Dignitatis Humanae</a></i>
(7 December 1965), par. 3: <i>AAS </i>58 (1966), 931: “[T]he highest norm of 
human life is the divine law itself—eternal, objective and universal, by which 
God orders, directs and governs the whole world and the ways of the human 
community according to a plan conceived in his wisdom and love. God has enabled 
man to participate in this law of his so that, under the gentle disposition of 
divine providence, many may be able to arrive at a deeper and deeper knowledge 
of unchangeable truth.” Also cf. Id., Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 16: <i>AAS </i>58 (1966), 1037.</p>
	<p><a name="_ftn63" title="" href="#_ftnref63">[63]</a>&nbsp;Cf. First Vatican Council, Dogmatic Constitution <i>
		<a href="https://www.vatican.va/content/pius-ix/la/documents/constitutio-dogmatica-dei-filius-24-aprilis-1870.html">Dei Filius</a></i> (24 
April 1870), ch. 4, DH 3016. </p>
	<p><a name="_ftn64" title="" href="#_ftnref64">[64]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 110: <i>
AAS </i>107 (2015), 892. </p>
	<p><a name="_ftn65" title="" href="#_ftnref65">[65]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 110: <i>
AAS </i>107 (2015), 891. Cf. Id., Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a></i> (3 
October 2020), par. 204: <i>AAS </i>112 (2020), 1042.</p>
	<p><a name="_ftn66" title="" href="#_ftnref66">[66]</a>&nbsp;Cf. John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_01051991_centesimus-annus.html">Centesimus Annus</a> </i>(1 May 1991), 
par. 11: <i>AAS </i>83 (1991), 807: “God has imprinted his own image and 
likeness on man (cf. Gen 1:26), conferring upon him an incomparable dignity […]. 
In effect, beyond the rights which man acquires by his own work, there exist 
rights which do not correspond to any work he performs, but which flow from his 
essential dignity as a person.” Cf. Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/speeches/2024/june/documents/20240614-g7-intelligenza-artificiale.html">Address at the G7 Session on 
Artificial Intelligence in Borgo Egnazia (Puglia)</a></i> (14 June 2024): <i>L’Osservatore Romano</i>, 14 June 2024, 3-4.</p>
	<p><a name="_ftn67" title="" href="#_ftnref67">[67]</a>&nbsp;Cf. Dicastery for the Doctrine of the Faith, Declaration <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a> </i>(8 April 2024), par. 8. Cf. <i>ibid., </i>par. 9; Congregation for the Doctrine of the Faith, 
Instruction <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_con_cfaith_doc_20081208_dignitas-personae_en.html">Dignitas Personae</a> </i>(8 September 2008), par. 22.</p>
	
	
	<p><a name="_ftn70" title="" href="#_ftnref70">[70]</a>&nbsp;In this sense, “Artificial Intelligence” is understood as a technical term 
to indicate this technology, recalling that the expression is also used to 
designate the field of study and not only its applications.</p>
	<p><a name="_ftn71" title="" href="#_ftnref71">[71]</a>&nbsp;Cf. Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), pars. 34-35: <i>AAS </i>58 (1966), 1052-1053; John Paul II, 
Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_01051991_centesimus-annus.html">Centesimus Annus</a> </i>(1 May 1991), par. 51: <i>AAS </i>83 
(1991), 856-857.</p>
	<p><a name="_ftn72" title="" href="#_ftnref72">[72]</a>&nbsp;For example, see the encouragement of scientific exploration in Albertus Magnus 
(<i>De Mineralibus</i>, II, 2, 1) and the appreciation for the mechanical arts 
in Hugh of St. Victor (<i>Didascalicon</i>, I, 9). These writers, among a long 
list of other Catholics engaged in scientific research and technological 
exploration, illustrate that “faith and science can be united in charity, 
provided that science is put at the service of the men and woman of our time and 
not misused to harm or even destroy them” (Francis, <i>A<a href="https://www.vatican.va/content/francesco/en/speeches/2024/june/documents/20240620-specola-vaticana.html">ddress to Participants 
in the 2024 Lemaître Conference of the Vatican Observatory</a> </i>[20 June 2024]:
<i>L’Osservatore Romano</i>,<i> </i>20 June 2024, 8). Cf. Second Vatican Ecumenical Council, 
Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 36: <i>AAS
</i>58 (1966), 1053-1054; John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_14091998_fides-et-ratio.html">Fides et Ratio</a></i>  
(14 September 1998), pars. 2, 106: <i>AAS </i>91 (1999), 6-7.86-87.</p>
	<p><a name="_ftn73" title="" href="#_ftnref73">[73]</a>&nbsp;<i>Catechism of the Catholic Church</i>, par. 378.</p>
	<p><a name="_ftn74" title="" href="#_ftnref74">[74]</a>&nbsp;Cf. Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 34: <i>AAS </i>58 (1966), 1053.</p>
	<p><a name="_ftn75" title="" href="#_ftnref75">[75]</a>&nbsp;Cf. Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 35: <i>AAS </i>58 (1966), 1053.</p>
	<p><a name="_ftn76" title="" href="#_ftnref76">[76]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 102: <i>
AAS </i>107 (2015), 888.</p>
	<p><a name="_ftn77" title="" href="#_ftnref77">[77]</a>&nbsp;Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a></i> (24 May 2015), par. 105:
<i>AAS </i>107 (2015), 889; Id., Encyclical <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a></i> (3 October 2020), par. 27: <i>AAS </i>112 (2020), 978; Benedict XVI, 
Encyclical <i>
		<a href="https://www.vatican.va/content/benedict-xvi/en/encyclicals/documents/hf_ben-xvi_enc_20090629_caritas-in-veritate.html">Caritas in Veritate</a></i> (29 June 2009), par. 23: <i>AAS </i>101 
(2009), 657-658.</p>
	<p><a name="_ftn78" title="" href="#_ftnref78">[78]</a>&nbsp;Cf. Dicastery for the Doctrine of the Faith, Declaration <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a> </i>(8 April 2024), pars. 38-39, 47; Congregation for the Doctrine of the Faith, 
Instruction <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_con_cfaith_doc_20081208_dignitas-personae_en.html">Dignitas Personae</a> </i>(8 September 2008), <i>passim</i>.</p>
	<p><a name="_ftn79" title="" href="#_ftnref79">[79]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 35: <i>
AAS </i>58 (1966), 1053. Cf. <i>Catechism of the Catholic Church</i>, par 2293.</p>
	
	<p><a name="_ftn81" title="" href="#_ftnref81">[81]</a>&nbsp;Cf. <i>Catechism of the Catholic Church</i>, par. 1749: “Freedom makes man 
a moral subject. When he acts deliberately, man is, so to speak, the father of 
his acts.”</p>
	<p><a name="_ftn82" title="" href="#_ftnref82">[82]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 16: <i>AAS </i>58 (1966), 1037. Cf.<i> 
Catechism of the Catholic Church</i>, par. 1776.</p>
	<p><a name="_ftn83" title="" href="#_ftnref83">[83]</a>&nbsp;<i>Catechism of the Catholic Church</i>, par. 1777.</p>
	<p><a name="_ftn84" title="" href="#_ftnref84">[84]</a>&nbsp;Cf. <i>Catechism of the Catholic Church</i>, pars. 1779-1781; Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/speeches/2023/march/documents/20230327-minerva-dialogues.html">Address to the Participants in the “Minerva Dialogues”</a></i> (27 March 2023): <i>
AAS </i>115 (2023), 463, where the Holy Father encouraged efforts “to ensure 
that technology remains human-centered, ethically grounded and directed toward 
the good.”</p>
	<p><a name="_ftn85" title="" href="#_ftnref85">[85]</a>&nbsp;Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), 
par. 166: <i>AAS </i>112 (2020), 1026-1027; Id., <i>
		<a href="https://www.vatican.va/content/francesco/en/speeches/2024/september/documents/20240923-plenaria-accademia-scienze.html">Address to the Plenary 
Assembly of the Pontifical Academy of Sciences</a> </i>(23 September 2024): <i>L’Osservatore 
</i>Romano, 23 September 2024, 10. On the role of human agency in choosing a 
wider aim (<i>Ziel</i>) that then informs the particular purpose (<i>Zweck</i>) 
for which each technological application is created, cf. F. Dessauer, <i>Streit 
um die Technik</i>, Herder-Bücherei, Freiburg i. Br. 1959, 70-71.</p>
	<p><a name="_ftn86" title="" href="#_ftnref86">[86]</a>&nbsp;Cf. Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/speeches/2024/june/documents/20240614-g7-intelligenza-artificiale.html">Address at the G7 Session on 
Artificial Intelligence in Borgo Egnazia (Puglia)</a></i> (14 June 2024): <i>L’Osservatore Romano</i>, 14 June 2024, 4: 
“Technology is born for a purpose and, in its impact on human society, always 
represents a form of order in social relations and an arrangement of power, thus 
enabling certain people to perform specific actions while preventing others from 
performing different ones. In a more or less explicit way, this constitutive 
power-dimension of technology always includes the worldview of those who 
invented and developed it.”</p>
	
	
	
	
	<p><a name="_ftn91" title="" href="#_ftnref91">[91]</a>&nbsp;Cf. Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/speeches/2024/june/documents/20240614-g7-intelligenza-artificiale.html">Address at the G7 Session on 
Artificial Intelligence in Borgo Egnazia (Puglia)</a></i> (14 June 2024): <i>L’Osservatore Romano</i>, 14 June 2024, 2: 
“Faced with the marvels of machines, which seem to know how to choose 
independently, we should be very clear that decision-making […] must always be 
left to the human person. We would condemn humanity to a future without hope if 
we took away people’s ability to make decisions about themselves and their 
lives, by dooming them to depend on the choices of machines.”</p>
	
	<p><a name="_ftn93" title="" href="#_ftnref93">[93]</a>&nbsp;The term “bias” in this document refers to <i>algorithmic bias</i> 
(systematic and consistent errors in computer systems that may 
disproportionately prejudice certain groups in unintended ways) or <i>learning 
bias</i> (which will result in training on a biased data set) and not the “<i>bias 
vector</i>” in neural networks (which is a parameter used to adjust the output 
of “neurons” to adjust more accurately to the data).</p>
	<p><a name="_ftn94" title="" href="#_ftnref94">[94]</a>&nbsp;Cf. Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/speeches/2023/march/documents/20230327-minerva-dialogues.html">Address to the Participants in the “Minerva Dialogues”</a></i>  
(27 March 2023): <i>AAS </i>115 (2023), 464, where the Holy Father affirmed the 
growth in consensus “on the need for development processes to respect such 
values as inclusion, transparency, security, equity, privacy and reliability,” 
and also welcomed “the efforts of international organizations to regulate these 
technologies so that they promote genuine progress, contributing, that is, to a 
better world and an integrally higher quality of life.”</p>
	
	<p><a name="_ftn96" title="" href="#_ftnref96">[96]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965)<i>, </i>par. 
26: <i>AAS </i>58 (1966), 1046-1047.</p>
	
	<p><a name="_ftn98" title="" href="#_ftnref98">[98]</a>&nbsp;Cf. Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/messages/communications/documents/20240124-messaggio-comunicazioni-sociali.html">Message for the LVIII World Day of Social Communications</a></i> 
(24 January 2024): <i>L’Osservatore Romano</i>, 24 January 2024, 8. For further 
discussion of the ethical questions raised by AI from a Catholic perspective, 
see AI Research Group of the Centre for Digital Culture of the Dicastery for Culture 
and Education, <i>Encountering Artificial Intelligence: Ethical and 
Anthropological Investigations </i>(Theological Investigations of Artificial 
Intelligence 1), M.J. Gaudet, N. Herzfeld, P. Scherz, J.J. Wales, eds., <i>
Journal of Moral Theology</i>, Pickwick, Eugene 2024, 147-253.</p>
	<p><a name="_ftn99" title="" href="#_ftnref99">[99]</a>&nbsp;On the importance of dialogue in a pluralist society oriented toward a 
“robust and solid social ethics,” see Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), pars. 211-214: <i>AAS </i>112 (2020), 1044-1045. 
	</p>
	
	<p><a name="_ftn101" title="" href="#_ftnref101">[101]</a>&nbsp;Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/messages/peace/documents/20231208-messaggio-57giornatamondiale-pace2024.html">Message for the LVII World Day of 
Peace</a> </i>(1 January 2024), par. 6:<i>&nbsp;L’Osservatore Romano</i>,<i>&nbsp;</i>14 December 2023, 3. 
Cf. Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965)<i>, </i>par. 26: <i>AAS </i>58 (1966), 1046-1047.</p>
	<p><a name="_ftn102" title="" href="#_ftnref102">[102]</a> Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 112: <i>
AAS </i>107 (2015), 892-893. </p>
	
	<p><a name="_ftn104" title="" href="#_ftnref104">[104]</a> Cf.&nbsp;Pontifical Council for Social Communications, <i>
		<a href="https://www.vatican.va/roman_curia/pontifical_councils/pccs/documents/rc_pc_pccs_doc_20020228_ethics-internet_en.html">Ethics in Internet</a> </i>
(22 February 2002), par. 10. </p>
	
	<p><a name="_ftn106" title="" href="#_ftnref106">[106]</a> Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), pars. 105-114:
<i>AAS </i>107 (2015), 889-893; Id., Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/20231004-laudate-deum.html">Laudate Deum</a> </i>(4 October 2023), pars. 20-33: <i>AAS</i> 115 (2023), 1047-1050. </p>
	<p><a name="_ftn107" title="" href="#_ftnref107">[107]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 105: <i>AAS</i> 107 
(2015), 889. Cf. Id., Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/20231004-laudate-deum.html">Laudate Deum</a> </i>(4 October 
2023), pars. 20-21: <i>AAS </i>115<i> </i>(2023), 1047. </p>
	
	
	<p><a name="_ftn110" title="" href="#_ftnref110">[110]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 112: <i>AAS </i>107 
(2015), 892.</p>
	<p><a name="_ftn111" title="" href="#_ftnref111">[111]</a>&nbsp;Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), 
pars. 101, 103, 111, 115, 167:<i> AAS </i>112 (2020), 1004-1005, 1007-1009, 
1027.</p>
	<p><a name="_ftn112" title="" href="#_ftnref112">[112]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 26: <i>AAS </i>58 (1966), 1046-1047; cf. Leo XIII, 
Encyclical Letter <i>
		<a href="https://www.vatican.va/content/leo-xiii/en/encyclicals/documents/hf_l-xiii_enc_15051891_rerum-novarum.html">Rerum Novarum</a> </i>(15 May 1891), par. 35: <i>Acta Leonis 
XIII</i>,<i> </i>11 (1892), 123. </p>
	<p><a name="_ftn113" title="" href="#_ftnref113">[113]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 12: <i>AAS </i>58 (1966), 1034. </p>
	<p><a name="_ftn114" title="" href="#_ftnref114">[114]</a>&nbsp;Cf. Pontifical Council for Justice and Peace, <i>Compendium of the Social 
Doctrine of the Church</i> (2004), par. 149.</p>
	<p><a name="_ftn115" title="" href="#_ftnref115">[115]</a>&nbsp;Second Vatican Ecumenical Council, Declaration <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_decl_19651207_dignitatis-humanae_en.html">Dignitatis Humanae</a> </i>
(7 December 1965), par. 3: <i>AAS </i>58 (1966), 931. Cf. Francis, Encyclical 
Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), par. 50: <i>AAS </i>112 (2020), 
986-987.</p>
	<p><a name="_ftn116" title="" href="#_ftnref116">[116]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), par. 
50: <i>AAS </i>112 (2020), 986-987.</p>
	<p><a name="_ftn117" title="" href="#_ftnref117">[117]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 47: <i>AAS </i>107 
(2015), 865. Cf. Id., Post-Synodal Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20190325_christus-vivit.html">Christus Vivit</a></i> (25 March 
2019), pars. 88-89: <i>AAS </i>111 (2019), 413-414.</p>
	<p><a name="_ftn118" title="" href="#_ftnref118">[118]</a>&nbsp;Cf. Francis, Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20131124_evangelii-gaudium.html#Yes_to_the_new_relationships_brought_by_Christ">Evangelii Gaudium</a></i> (24 November 
2013), par. 88: <i>AAS</i> 105 (2013), 1057.</p>
	<p><a name="_ftn119" title="" href="#_ftnref119">[119]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), par. 
47: <i>AAS </i>112 (2020), 985.</p>
	
	<p><a name="_ftn121" title="" href="#_ftnref121">[121]</a>&nbsp;Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), 
par. 50: <i>AAS </i>112 (2020), 986-987.</p>
	<p><a name="_ftn122" title="" href="#_ftnref122">[122]</a>&nbsp;Cf. E. Stein, 
		<i>Zum Problem der Einfühlung</i>, Buchdruckerei des Waisenhauses, Halle 1917 (en. tr. <i>On the Problem of Empathy</i>, ICS Publications, Washington D.C. 
1989).</p>
	<p><a name="_ftn123" title="" href="#_ftnref123">[123]</a>&nbsp;Cf. Francis, Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20131124_evangelii-gaudium.html#Yes_to_the_new_relationships_brought_by_Christ">Evangelii Gaudium</a></i> (24 November 
2013), par. 88: <i>AAS</i> 105 (2013), 1057: “[Many people] want their 
interpersonal relationships provided by sophisticated equipment, by screens and 
systems which can be turned on and off on command. Meanwhile, the Gospel tells 
us constantly to run the risk of a face-to-face encounter with others, with 
their physical presence which challenges us, with their pain and their pleas, 
with their joy which infects us in our close and continuous interaction. True 
faith in the incarnate Son of God is inseparable from self-giving, from 
membership in the community, from service, from reconciliation with others.” 
Also cf. Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 24: <i>AAS </i>58 (1966), 1044-1045.</p>
	<p><a name="_ftn124" title="" href="#_ftnref124">[124]</a>&nbsp;Cf. Dicastery for the Doctrine of the Faith, Declaration <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a></i> (8 April 2024), 
par. 1.</p>
	
	
	<p><a name="_ftn127" title="" href="#_ftnref127">[127]</a>&nbsp;Francis, Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20131124_evangelii-gaudium.html#Concern_for_the_vulnerable">Evangelii Gaudium</a> </i>(24 November 2013), par. 209:
<i>AAS </i>105 (2013), 1107.</p>
	
	<p><a name="_ftn129" title="" href="#_ftnref129">[129]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 26: <i>
AAS </i>58 (1966), 1046-1047.; as quoted in <i>Catechism of the Catholic Church</i>, 
par. 1912. Cf. John XXIII, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-xxiii/en/encyclicals/documents/hf_j-xxiii_enc_15051961_mater.html">Mater et Magistra</a></i> (15 May 
1961), par. 219: <i>AAS </i>53 (1961), 453. </p>
	<p><a name="_ftn130" title="" href="#_ftnref130">[130]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par 64: <i>AAS
</i>58 (1966), 1086.</p>
	<p><a name="_ftn131" title="" href="#_ftnref131">[131]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), par. 162: <i>AAS </i>
112 (2020), 1025. Cf. John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_14091981_laborem-exercens.html">Laborem Exercens</a> </i>(14 September 1981), par. 6: <i>AAS </i>73 (1981), 591: “work is ‘for man’ and 
not man ‘for work.’ Through this conclusion one rightly comes to recognize the 
pre-eminence of the subjective meaning of work over the objective one.”</p>
	<p><a name="_ftn132" title="" href="#_ftnref132">[132]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 128: <i>AAS </i>107 
(2015), 898. Cf. Id., Post-Synodal Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20160319_amoris-laetitia.html">
		Amoris Laetitia</a></i>  
(19 March 2016), par. 24: <i>AAS </i>108 (2016), 319-320.</p>
	
	<p><a name="_ftn134" title="" href="#_ftnref134">[134]</a>&nbsp;John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_25031995_evangelium-vitae.html">Evangelium Vitae</a></i> (25 March 1995), 
par. 89: <i>AAS </i>87 (1995), 502.</p>
	
	
	
	
	
	
	<p><a name="_ftn141" title="" href="#_ftnref141">[141]</a>&nbsp;Francis, <i>Address to the Participants at the Meeting Sponsored by the 
Charity and Health Commission of the Italian Bishops’ Conference </i>(10 
February 2017): <i>AAS </i>109 (2017), 243. Cf. <i>ibid</i>., 242-243: “If there 
is a sector in which the throwaway culture is manifest, with its painful 
consequences, it is that of healthcare. When a sick person is not placed in the 
center or their dignity is not considered, this gives rise to attitudes that can 
lead even to speculation on the misfortune of others. And this is very grave! 
[…] The application of a business approach to the healthcare sector, if 
indiscriminate […] may risk discarding human beings.”</p>
	
	<p><a name="_ftn143" title="" href="#_ftnref143">[143]</a>&nbsp;Second Vatican Ecumenical Council, Declaration <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_decl_19651028_gravissimum-educationis_en.html">Gravissimum Educationis</a> </i>(28 October 1965), par. 1: <i>AAS
</i>58 (1966), 729.</p>
	<p><a name="_ftn144" title="" href="#_ftnref144">[144]</a>&nbsp;Congregation for Catholic Education, <i>Instruction on the Use of Distance Learning in Ecclesiastical Universities 
and Faculties</i>, I. Cf. Second Vatican Ecumenical Council, Declaration <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_decl_19651028_gravissimum-educationis_en.html">Gravissimum Educationis</a> </i>(28 October 1965), par. 1: <i>AAS </i>58 (1966), 
729; Francis, <i><span size="4">
		<a href="https://www.vatican.va/content/francesco/en/messages/peace/documents/papa-francesco_20151208_messaggio-xlix-giornata-mondiale-pace-2016.html">Message for the LXIX World Day of Peace</a></span></i> (1 January 2016), 
6: <i>AAS</i> 108 (2016), 57-58.</p>
	
	<p><a name="_ftn146" title="" href="#_ftnref146">[146]</a>&nbsp;Cf. Paul VI, Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/paul-vi/en/apost_exhortations/documents/hf_p-vi_exh_19751208_evangelii-nuntiandi.html">Evangelii Nuntiandi</a></i> (8 December 1975), 
par. 41: <i>AAS </i>68 (1976), 31, quoting Id., <i>Address to the Members of the 
“Consilium de Laicis”</i> (2 October 1974): <i>AAS</i> 66 (1974), 568: “if [the 
contemporary person] does listen to teachers, it is because they are witnesses.”</p>
	<p><a name="_ftn147" title="" href="#_ftnref147">[147]</a> J.H. Newman, <i>The Idea of a University Defined and Illustrated</i>, Discourse 
6.1, London 1873<sup>3</sup>, 125-126.</p>
	
	<p><a name="_ftn149" title="" href="#_ftnref149">[149]</a>&nbsp;Francis, Post-Synodal Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20190325_christus-vivit.html">Christus Vivit</a></i> (25 March 2019), par. 
86: <i>AAS </i>111 (2019), 413, quoting the XV Ordinary General Assembly of the Synod of Bishops,
		<a href="https://www.vatican.va/roman_curia/synod/documents/rc_synod_doc_20181027_doc-final-instrumentum-xvassemblea-giovani_en.html">
<i>Final Document</i> (27 October 2018)</a>, par. 21: <i>AAS </i>110 (2018), 1592.</p>
	<p><a name="_ftn150" title="" href="#_ftnref150">[150]</a> J.H. Newman, <i>The Idea of a University Defined and Illustrated</i>, Discourse 
7.6, Basil Montagu Pickering, London 1873<sup>3</sup>, 167.</p>
	<p><a name="_ftn151" title="" href="#_ftnref151">[151]</a> Cf. Francis, Post-Synodal Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20190325_christus-vivit.html">Christus Vivit</a></i> (25 March 
2019), par. 88: <i>AAS </i>111 (2019), 413.</p>
	<p><a name="_ftn152" title="" href="#_ftnref152">[152]</a> In a 2023 policy document about the use of generative AI in education and 
research, UNESCO notes: “One of the key questions [of the use of generative AI 
(GenAI) in education and research] is whether humans can possibly cede basic 
levels of thinking and skill-acquisition processes to AI and rather concentrate 
on higher-order thinking skills based on the outputs provided by AI. Writing, 
for example, is often associated with the structuring of thinking. With GenAI 
[…], humans can now start with a well-structured outline provided by GenAI. Some 
experts have characterized the use of GenAI to generate text in this way as 
‘writing without thinking’” (UNESCO, <i>Guidance for Generative AI in Education 
and Research </i>[2023], 37-38). The German-American philosopher Hannah Arendt 
foresaw such a possibility in her 1959 book, <i>The Human Condition</i>, and 
cautioned: “If it should turn out to be true that knowledge (in the sense of 
know-how) and thought have parted company for good, then we would indeed become 
the helpless slaves, not so much of our machines as of our know-how” (Id., <i>
The Human Condition</i>, University of Chicago Press, Chicago 2018<sup>2</sup>, 
3). </p>
	<p><a name="_ftn153" title="" href="#_ftnref153">[153]</a>&nbsp;Francis, Post-Synodal Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20160319_amoris-laetitia.html">
		Amoris Laetitia</a></i> (19 March 2016), 
par. 262: <i>AAS </i>108 (2016), 417. </p>
	<p><a name="_ftn154" title="" href="#_ftnref154">[154]</a>&nbsp;Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/messages/peace/documents/20231208-messaggio-57giornatamondiale-pace2024.html">Message for the LVII World Day of 
Peace</a></i> (1 January 2024), par. 7: <i>L’Osservatore Romano</i>, 14 December 2023, 3; 
cf. Id., Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 167: <i>AAS
</i>107 (2015), 914. </p>
	<p><a name="_ftn155" title="" href="#_ftnref155">[155]</a>&nbsp;John Paul II, Apostolic Constitution <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/apost_constitutions/documents/hf_jp-ii_apc_15081990_ex-corde-ecclesiae.html">Ex Corde Ecclesiae</a></i> (15 August 
1990), 7: <i>AAS</i> 82 (1990), 1479.</p>
	<p><a name="_ftn156" title="" href="#_ftnref156">[156]</a>&nbsp;Francis, Apostolic Constitution <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_constitutions/documents/papa-francesco_costituzione-ap_20171208_veritatis-gaudium.html">Veritatis Gaudium</a></i> (29 January 
2018), 4c: <i>AAS</i> 110 (2018), 9-10.</p>
	
	<p><a name="_ftn158" title="" href="#_ftnref158">[158]</a>&nbsp;For example, it might help people access the “array of resources for 
generating greater knowledge of truth” contained in the works of philosophy (John Paul II, 
Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_14091998_fides-et-ratio.html">Fides et Ratio</a></i> [14 September 1998], par. 3: <i>AAS </i>
91 [1999], 7). Cf. <i>ibid</i>., par. 4: <i>AAS </i>91 (1999), 7-8.</p>
	<p><a name="_ftn159" title="" href="#_ftnref159">[159]</a>&nbsp;Dicastery for the Doctrine of the Faith, Declaration <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a> </i>(8 April 2024), par. 43. Cf. <i>ibid.</i>, pars. 61-62.</p>
	
	<p><a name="_ftn161" title="" href="#_ftnref161">[161]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par 25: <i>AAS
</i>58 (1966), 1053; cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 
October 2020), <i>passim</i>: <i>AAS </i>112 (2020), 969-1074.</p>
	<p><a name="_ftn162" title="" href="#_ftnref162">[162]</a>&nbsp;Cf. Francis., Post-Synodal Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20190325_christus-vivit.html">Christus Vivit</a></i> (25 March 
2019), par. 89: <i>AAS </i>111 (2019), 414; John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_14091998_fides-et-ratio.html">Fides et Ratio</a></i> (14 September 1998), par. 25: <i>AAS </i>91 (1999), 25-26: 
“People cannot be genuinely indifferent to the question of whether what they 
know is true or not. […] It is this that Saint Augustine teaches when he writes: 
‘I have met many who wanted to deceive, but none who wanted to be deceived’”; 
quoting Augustine, <i>Confessiones</i>, X, 23, 33: PL 32, 794.</p>
	<p><a name="_ftn163" title="" href="#_ftnref163">[163]</a>&nbsp;Dicastery for the Doctrine of the Faith, Declaration <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a></i> 
(4 April 2024), par. 62.</p>
	
	
	<p><a name="_ftn166" title="" href="#_ftnref166">[166]</a>&nbsp;Dicastery for the Doctrine of the Faith, Declaration <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a></i> 
(4 April 2024), pars. 1, 6, 16, 24. </p>
	<p><a name="_ftn167" title="" href="#_ftnref167">[167]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a></i>, (7 December 1965), par. 26: <i>AAS </i>58 (1966), 1046. Cf. Leo XIII, 
Encyclical Letter<i>
		<a href="https://www.vatican.va/content/leo-xiii/en/encyclicals/documents/hf_l-xiii_enc_15051891_rerum-novarum.html">Rerum Novarum</a></i> (15 May 1891), par. 40: <i>Acta Leonis 
XIII</i>,<i> </i>11 (1892), 127: “no man may with impunity violate that human 
dignity which God himself treats with great reverence”; as quoted in John Paul II, 
Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_01051991_centesimus-annus.html">Centesimus Annus</a> </i>(1 May 1991), par. 9: <i>AAS </i>83 
(1991), 804.</p>
	
	<p><a name="_ftn169" title="" href="#_ftnref169">[169]</a>&nbsp;Cf. Permanent Observer Mission of the Holy See to the United Nations, <i>
Holy See Statement to the Thematic Discussion on Other Disarmament Measures and 
International Security</i> (24 October 2022): “Upholding human dignity in 
cyberspace obliges States to also respect the right to privacy, by shielding 
citizens from intrusive surveillance and allowing them to safeguard their 
personal information from unauthorized access.”</p>
	<p><a name="_ftn170" title="" href="#_ftnref170">[170]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), par. 
42: <i>AAS </i>112 (2020), 984.</p>
	
	
	<p><a name="_ftn173" title="" href="#_ftnref173">[173]</a>&nbsp;The 2023 <i>Interim Report</i> of the United Nations AI Advisory Body<i>
</i>identified a list of “early promises of AI helping to address climate 
change” (United Nations AI Advisory Body, <i>Interim Report: Governing AI for 
Humanity</i> [December 2023], 3). The document observed that, “taken together 
with predictive systems that can transform data into insights and insights into 
actions, AI-enabled tools may help develop new strategies and investments to 
reduce emissions, influence new private sector investments in net zero, protect 
biodiversity, and build broad-based social resilience” (<i>ibid</i>.).</p>
	<p><a name="_ftn174" title="" href="#_ftnref174">[174]</a>&nbsp;“The cloud” refers to a network of physical servers throughout the world 
that enables users to store, process, and manage their data remotely.</p>
	<p><a name="_ftn175" title="" href="#_ftnref175">[175]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 9: <i>
AAS </i>107 (2015), 850.</p>
	<p><a name="_ftn176" title="" href="#_ftnref176">[176]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 106: <i>
AAS </i>107 (2015), 890. </p>
	<p><a name="_ftn177" title="" href="#_ftnref177">[177]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 60: <i>
AAS </i>107 (2015), 870. </p>
	<p><a name="_ftn178" title="" href="#_ftnref178">[178]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), pars. 3, 13:
<i>AAS </i>107 (2015), 848.852.</p>
	<p><a name="_ftn179" title="" href="#_ftnref179">[179]</a>&nbsp;Augustine, <i>De Civitate Dei</i>, XIX, 13, 1: PL 41, 640.</p>
	<p><a name="_ftn180" title="" href="#_ftnref180">[180]</a>&nbsp;Cf. Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), pars.&nbsp;77-82: <i>AAS </i>58 (1966), 1100-1107; Francis, 
Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), pars.&nbsp;256-262: <i>
AAS </i>112 (2020), 1060-1063; Dicastery for the Doctrine of the Faith, 
Declaration<i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a> </i>(4 April 2024), pars. 38-39; <i>Catechism of 
the Catholic Church</i>, pars. 2302-2317.</p>
	<p><a name="_ftn181" title="" href="#_ftnref181">[181]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par.&nbsp;78: <i>AAS </i>58 (1966), 1101.</p>
	
	<p><a name="_ftn183" title="" href="#_ftnref183">[183]</a>&nbsp;Cf.<b> </b><i>Catechism of the Catholic Church</i>, pars. 2308-2310. 
	</p>
	<p><a name="_ftn184" title="" href="#_ftnref184">[184]</a> Cf. Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), pars.&nbsp;80-81: <i>AAS </i>58 (1966), 1103-1105.</p>
	
	<p><a name="_ftn186" title="" href="#_ftnref186">[186]</a>&nbsp;Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/speeches/2024/june/documents/20240614-g7-intelligenza-artificiale.html">Address at the G7 Session on 
Artificial Intelligence in Borgo Egnazia (Puglia)</a></i> (14 June 2024): <i>L’Osservatore Romano</i>, 14 June 2024, 2. Cf. Permanent Observer Mission of the Holy See to the United Nations,
<i>Holy See Statement to Working Group II on Emerging Technologies at the UN 
Disarmament Commission </i>(3 April 2024): “The development and use of lethal 
autonomous weapons systems (LAWS) that lack the appropriate human control would 
pose fundamental ethical concerns, given that LAWS can never be morally 
responsible subjects capable of complying with international humanitarian law.”</p>
	<p><a name="_ftn187" title="" href="#_ftnref187">[187]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a> </i>(3 October 2020), par.&nbsp;258: 
<i>AAS </i>112 (2020), 1061. Cf. Second Vatican Ecumenical Council, Pastoral 
Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 80: <i>AAS </i>58 
(1966), 1103-1104. </p>
	<p><a name="_ftn188" title="" href="#_ftnref188">[188]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 80: <i>AAS </i>58 (1966), 1103-1104.</p>
	<p><a name="_ftn189" title="" href="#_ftnref189">[189]</a>&nbsp;Cf. Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/messages/peace/documents/20231208-messaggio-57giornatamondiale-pace2024.html">Message for the LVII World Day of 
Peace</a> </i>(1 January 
2024), par. 6: <i>L’Osservatore Romano</i>,<i>&nbsp;</i>14 December 2023, 3: 
“Nor can we ignore the possibility of sophisticated weapons ending up in the 
wrong hands, facilitating, for instance, terrorist attacks or interventions 
aimed at destabilizing the institutions of legitimate systems of government. In 
a word, the world does not need new technologies that contribute to the unjust 
development of commerce and the weapons trade and consequently end up promoting 
the folly of war.”</p>
	
	<p><a name="_ftn191" title="" href="#_ftnref191">[191]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 79: <i>
AAS </i>107 (2015), 878.</p>
	<p><a name="_ftn192" title="" href="#_ftnref192">[192]</a>&nbsp;Cf. Benedict XVI, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/benedict-xvi/en/encyclicals/documents/hf_ben-xvi_enc_20090629_caritas-in-veritate.html">Caritas in Veritate</a> </i> (29 June 
2009), par. 51: <i>AAS </i>101 (2009), 687.</p>
	<p><a name="_ftn193" title="" href="#_ftnref193">[193]</a>&nbsp;Cf. Dicastery for the Doctrine of the Faith, Declaration <i>
		<a href="https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20240402_dignitas-infinita_en.html">Dignitas 
Infinita</a> </i>(8 April 2024), pars. 38-39. </p>
	<p><a name="_ftn194" title="" href="#_ftnref194">[194]</a>&nbsp;Cf.&nbsp;Augustine, <i>Confessiones</i>, I, 1, 1: PL 32, 661.</p>
	<p><a name="_ftn195" title="" href="#_ftnref195">[195]</a>&nbsp;Cf. John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_30121987_sollicitudo-rei-socialis.html">Sollicitudo Rei Socialis</a> </i>(30 
December 1987), par. 28: <i>AAS </i>80 (1988), 548: “[T]here is a better 
understanding today that the mere accumulation of goods and services […] is not 
enough for the realization of human happiness. Nor, in consequence, does the 
availability of the many real benefits provided in recent times by science and 
technology, including the computer sciences, bring freedom from every form of 
slavery. On the contrary, […] unless all the considerable body of resources and 
potential at man’s disposal is guided by a moral understanding and by an 
orientation towards the true good of the human race, it easily turns against man 
to oppress him.” Cf. <i>ibid.</i>, pars. 29, 37: <i>AAS </i>80 (1988), 
550-551.563-564.</p>
	<p><a name="_ftn196" title="" href="#_ftnref196">[196]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 14: <i>AAS </i>58 (1966), 1036.</p>
	<p><a name="_ftn197" title="" href="#_ftnref197">[197]</a>&nbsp;Francis, Encyclical Letter<i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/20241024-enciclica-dilexit-nos.html">Dilexit Nos</a> </i>(24 October 2024), par. 18:
<i>L’Osservatore Romano</i>, 24 October 2024, 5. </p>
	<p><a name="_ftn198" title="" href="#_ftnref198">[198]</a>&nbsp;Francis, Encyclical Letter<i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/20241024-enciclica-dilexit-nos.html">Dilexit Nos</a> </i>(24 October 2024), par. 27:
<i>L’Osservatore Romano</i>, 24 October 2024, 6. </p>
	<p><a name="_ftn199" title="" href="#_ftnref199">[199]</a>&nbsp;Francis, Encyclical Letter<i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/20241024-enciclica-dilexit-nos.html">Dilexit Nos</a> </i>(24 October 2024), par. 25:
<i>L’Osservatore Romano</i>, 24 October 2024, 5-6. </p>
	<p><a name="_ftn200" title="" href="#_ftnref200">[200]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 105: <i>AAS </i>107 
(2015), 889. Cf. R. Guardini, <i>Das Ende der Neuzeit</i>, Würzburg 1965<sup>9</sup>, 
87 ff. (en. tr. <i>The End of the Modern World</i>, Wilmington 1998, 82-83).</p>
	<p><a name="_ftn201" title="" href="#_ftnref201">[201]</a>&nbsp;Second Vatican Ecumenical Council, Pastoral Constitution <i>
		<a href="https://www.vatican.va/archive/hist_councils/ii_vatican_council/documents/vat-ii_const_19651207_gaudium-et-spes_en.html">Gaudium et 
Spes</a> </i>(7 December 1965), par. 34: <i>AAS </i>58 (1966), 1053.</p>
	<p><a name="_ftn202" title="" href="#_ftnref202">[202]</a>&nbsp;John Paul II, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/john-paul-ii/en/encyclicals/documents/hf_jp-ii_enc_04031979_redemptor-hominis.html">Redemptor Hominis</a> </i>(4 March 1979), 
par. 15: <i>AAS </i>71 (1979), 287-288.</p>
	<p><a name="_ftn203" title="" href="#_ftnref203">[203]</a>&nbsp;N. Berdyaev, “Man and Machine,” in C. Mitcham – R. Mackey, eds., <i>
Philosophy and Technology: Readings in the Philosophical Problems of Technology</i>, 
New York 1983<sup>2</sup>, 212-213.</p>
	<p><a name="_ftn204" title="" href="#_ftnref204">[204]</a>&nbsp;N. Berdyaev, “Man and Machine,” 210. 
	</p>
	<p><a name="_ftn205" title="" href="#_ftnref205">[205]</a>&nbsp;G. Bernanos, “La révolution de la liberté” (1944), in Id., <i>Le Chemin de 
la Croix-des-Âmes</i>, Rocher 1987, 829.</p>
	
	<p><a name="_ftn207" title="" href="#_ftnref207">[207]</a>&nbsp;Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 112: <i>
AAS </i>107 (2015), 892-893.</p>
	<p><a name="_ftn208" title="" href="#_ftnref208">[208]</a>&nbsp;Cf. Bonaventure, <i>Hex. </i>XIX, 3; Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20201003_enciclica-fratelli-tutti.html">Fratelli Tutti</a></i> (3 October 2020), par. 50: <i>AAS </i>112 (2020), 986: “The 
flood of information at our fingertips does not make for greater wisdom. Wisdom 
is not born of quick searches on the internet nor is it a mass of unverified 
data. That is not the way to mature in the encounter with truth.”</p>
	
	
	
	<p><a name="_ftn212" title="" href="#_ftnref212">[212]</a>&nbsp;Francis, Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20180319_gaudete-et-exsultate.html#An_intellect_without_God_and_without_flesh">Gaudete et Exsultate</a></i> (19 March 2018), par. 37: <i>
AAS </i>110 (2018), 1121. </p>
	<p><a name="_ftn213" title="" href="#_ftnref213">[213]</a>&nbsp;Francis, <i>
		<a href="https://www.vatican.va/content/francesco/en/messages/peace/documents/20231208-messaggio-57giornatamondiale-pace2024.html">Message for the LVII World Day of 
Peace</a></i> (1 January 2024), par. 6: <i>L’Osservatore Romano</i>, 14 December 2023, 3. 
Cf. Id., Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 112: <i>AAS
</i>107 (2015), 892-893; Id., Apostolic Exhortation <i>
		<a href="https://www.vatican.va/content/francesco/en/apost_exhortations/documents/papa-francesco_esortazione-ap_20180319_gaudete-et-exsultate.html#The_limits_of_reason">Gaudete et Exsultate</a></i> 
(19 March 2018), par. 46:<i> AAS </i>110 (2018), 1123-1124.</p>
	<p><a name="_ftn214" title="" href="#_ftnref214">[214]</a> Cf. Francis, Encyclical Letter <i>
		<a href="https://www.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html">Laudato Si’</a> </i>(24 May 2015), par. 112: <i>
AAS </i>107 (2015), 892-893.</p>
	
</div>
										</td>
												</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LibreOffice 400M Downloads, and Counting (172 pts)]]></title>
            <link>https://blog.documentfoundation.org/blog/2025/01/30/400-million-downloads-and-counting/</link>
            <guid>42876998</guid>
            <pubDate>Thu, 30 Jan 2025 11:59:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.documentfoundation.org/blog/2025/01/30/400-million-downloads-and-counting/">https://blog.documentfoundation.org/blog/2025/01/30/400-million-downloads-and-counting/</a>, See on <a href="https://news.ycombinator.com/item?id=42876998">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
		<article>
			<div id="content_box">
							
														<header>
								
								
							</header><!--.headline_area-->
							<div>
								<p><a href="https://blog.documentfoundation.org/wp-content/uploads/2025/01/libreofficedownloadsperyear.png"><img fetchpriority="high" decoding="async" src="https://blog.documentfoundation.org/wp-content/uploads/2025/01/libreofficedownloadsperyear.png" alt="" width="761" height="410" srcset="https://blog.documentfoundation.org/wp-content/uploads/2025/01/libreofficedownloadsperyear.png 761w, https://blog.documentfoundation.org/wp-content/uploads/2025/01/libreofficedownloadsperyear-300x162.png 300w" sizes="(max-width: 761px) 100vw, 761px"></a></p>
<p>The histogram says it all.</p>
<p>First, rapid growth between 2011 and 2014 to 30 million downloads, despite the fierce hostility of the project created to kill LibreOffice.</p>
<p>Then a few years of stagnation, at a time when it seemed that desktop office suites were destined to die, and fashion was driving users to the cloud.</p>
<p>Then the upswing, when even the most fashionable users realised that desktop office suites would never die and would coexist with the cloud.</p>
<p>In 2019, a series of attacks on the download counter – no data is collected other than the click on the DOWNLOAD button – led to a barely credible increase (the figure you see has already been cleaned up as much as possible).</p>
<p>After 2019, a slow but inexorable growth to over 35 million downloads – and 400 million downloads since 2011, with an average of 28.6 million downloads per year – in 2024.</p>
<p>Thanks to everyone: those who developed LibreOffice, those who helped improve it, and those who downloaded it to use it.</p>
<h3><a href="https://www.libreoffice.org/download/">Click here to download LibreOffice</a></h3>
																							</div>
						</div>
		</article>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[JavaScript Temporal Is Coming (592 pts)]]></title>
            <link>https://developer.mozilla.org/en-US/blog/javascript-temporal-is-coming/</link>
            <guid>42876840</guid>
            <pubDate>Thu, 30 Jan 2025 11:28:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.mozilla.org/en-US/blog/javascript-temporal-is-coming/">https://developer.mozilla.org/en-US/blog/javascript-temporal-is-coming/</a>, See on <a href="https://news.ycombinator.com/item?id=42876840">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main><article lang="en-US"><figure><img alt="JavaScript Temporal is coming title. A JavaScript logo, a clock graphic and a globe with orbiting bodies symbolizing calendars and time." src="https://developer.mozilla.org/en-US/blog/javascript-temporal-is-coming/featured.png" height="420" width="800"></figure><div><p>Implementations of the new JavaScript Temporal object are starting to be shipped in experimental releases of browsers.
This is big news for web developers because working with dates and times in JavaScript will be hugely simplified and modernized.</p>
<p>Applications that rely on scheduling, internationalization, or time-sensitive data will be able to use built-ins for efficient, precise and consistent dates, times, durations, and calendars.
We're a long way away from stable, cross-browser support, and there may be changes as implementations develop, but we can already take a look at Temporal as it stands now, why it's exciting, and what problems it solves.</p>
<p>To help you get up to speed, there are over <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal">270 pages of Temporal docs on MDN</a> added this week, with detailed explanations and examples.</p></div><section aria-labelledby="what_is_javascript_temporal"><h2 id="what_is_javascript_temporal"><a href="#what_is_javascript_temporal">What is JavaScript Temporal?</a></h2><div><p>To understand Temporal, we can look at JavaScript's <code>Date</code> object.
When JavaScript was created in 1995, the <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Date"><code>Date</code></a> object was copied from Java's early, flawed <code>java.util.Date</code> implementation.
Java replaced this implementation in 1997, but JavaScript is stuck with the same API for almost 30 years, despite known problems.</p>
<p>The major issues with JavaScript's <code>Date</code> object are that it only supports the user's local time and UTC, and there's no time zone support.
Additionally, its parsing behavior is very unreliable, and <code>Date</code> itself is mutable, which can introduce hard-to-trace bugs.
There are other problems like calculations across Daylight Saving Time (DST) and historical calendar changes, which are notoriously difficult to work with.</p>
<p>All of these issues make working with dates and times in JavaScript complex and prone to bugs, which can have serious consequences for some systems.
Most developers rely on dedicated libraries like <a href="https://momentjs.com/" target="_blank">Moment.js</a> and <a href="https://date-fns.org/" target="_blank">date-fns</a> for better handling of dates and times in their applications.</p>
<p>Temporal is designed as a full replacement for the <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Date"><code>Date</code></a> object, making date and time management reliable and predictable.
Temporal adds support for time zone and calendar representations, many built-in methods for conversions, comparisons and computations, formatting, and more.
The API surface has over 200 utility methods, and you can find information about all of them in the <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal">Temporal docs on MDN</a>.</p></div></section><section aria-labelledby="core_concepts"><h2 id="core_concepts"><a href="#core_concepts">Core concepts</a></h2><div><p>In Temporal, the key concepts are that it has instants (unique points in history), wall-clock times (regional time), and durations.
The APIs have this overall structure to handle these concepts:</p>
<ul>
<li><strong>Duration</strong>: <code>Temporal.Duration</code> the difference between two points in time</li>
<li><strong>Points in time</strong>:
<ul>
<li><strong>Unique points in time</strong>:
<ul>
<li>As a timestamp: <code>Temporal.Instant</code></li>
<li>A date-time with a time zone: <code>Temporal.ZonedDateTime</code></li>
</ul>
</li>
<li><strong>Time-zone-unaware date/time ("Plain")</strong>:
<ul>
<li>Full date and time: <code>Temporal.PlainDateTime</code>
<ul>
<li>Just the date: <code>Temporal.PlainDate</code>
<ul>
<li>Year and month: <code>Temporal.PlainYearMonth</code></li>
<li>Month and day: <code>Temporal.PlainMonthDay</code></li>
</ul>
</li>
<li>Just the time: <code>Temporal.PlainTime</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>Now:</strong> using <code>Temporal.now</code> to get the current time as various class instances, or in a specific format</li>
</ul></div></section><section aria-labelledby="temporal_examples"><h2 id="temporal_examples"><a href="#temporal_examples">Temporal examples</a></h2><div><p>Some of the most basic usages of Temporal include getting current dates and times as an ISO string, but we can see from the example below, that we can now provide time zones with many methods, which takes care of complex calculations you may be doing yourself:</p>
<div><pre><code>// The current date in the system's time zone
const dateTime = Temporal.Now.plainDateTimeISO();
console.log(dateTime); // e.g.: 2025-01-22T11:46:36.144

// The current date in the "America/New_York" time zone
const dateTimeInNewYork = Temporal.Now.plainDateTimeISO("America/New_York");
console.log(dateTimeInNewYork);
// e.g.: 2025-01-22T05:47:02.555
</code></pre></div>
<p>Working with different calendars is also simplified, as it's possible to create dates in calendar systems other than Gregorian, such as Hebrew, Chinese, and Islamic, for example.
The code below helps you find out when the next Chinese New Year is (which is quite soon!):</p>
<div><pre><code>// Chinese New Years are on 1/1 in the Chinese calendar
const chineseNewYear = Temporal.PlainMonthDay.from({
  monthCode: "M01",
  day: 1,
  calendar: "chinese",
});
const currentYear = Temporal.Now.plainDateISO().withCalendar("chinese").year;
let nextCNY = chineseNewYear.toPlainDate({ year: currentYear });
// If nextCNY is before the current date, move forward by 1 year
if (Temporal.PlainDate.compare(nextCNY, Temporal.Now.plainDateISO()) &lt;= 0) {
  nextCNY = nextCNY.add({ years: 1 });
}
console.log(
  `The next Chinese New Year is on ${nextCNY.withCalendar("iso8601").toLocaleString()}`,
);
// The next Chinese New Year is on 1/29/2025 (at the time of writing)
</code></pre></div>
<p>Working with Unix timestamps is a very common use case as many systems (APIs, databases) use the format to represent times.
The following example shows how to take a Unix Epoch timestamp in milliseconds, create an instant from it, get the current time with <code>Temporal.Now</code>, then calculate how many hours from now until the Unix timestamp:</p>
<div><pre><code>// 1851222399924 is our timestamp
const launch = Temporal.Instant.fromEpochMilliseconds(1851222399924);
const now = Temporal.Now.instant();
const duration = now.until(launch, { smallestUnit: "hour" });
console.log(`It will be ${duration.toLocaleString("en-US")} until the launch`);
// "It will be 31,600 hr until the launch" &lt;- @js-temporal/polyfill
// "It will be PT31600H until the launch" &lt;- Firefox Nightly
</code></pre></div>
<p>Currently, <code>toLocaleString</code> <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1839694" target="_blank">doesn't output a locale-sensitive string</a> in the Firefox implementation, so durations above (<code>PT31600H</code>) are returned as a non-locale-sensitive <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal/Duration#iso_8601_duration_format">duration format</a>.
This may change as it's more of a design decision rather than a technical limitation as <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal/PlainDate/since#using_since">formatting the duration</a> is possible, so the polyfill and Firefox implementations may eventually converge.</p>
<p>There's a lot to highlight, but one pattern that I thought was interesting in the API is the <code>compare()</code> methods, which allow you to <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal/Duration/compare#sorting_an_array_of_durations">sort durations</a> in an elegant and efficient way:</p>
<div><pre><code>const durations = [
  Temporal.Duration.from({ hours: 1 }),
  Temporal.Duration.from({ hours: 2 }),
  Temporal.Duration.from({ hours: 1, minutes: 30 }),
  Temporal.Duration.from({ hours: 1, minutes: 45 }),
];

durations.sort(Temporal.Duration.compare);
console.log(durations.map((d) =&gt; d.toString()));
// [ 'PT1H', 'PT1H30M', 'PT1H45M', 'PT2H' ]
</code></pre></div></div></section><section aria-labelledby="trying_temporal_and_browser_support"><h2 id="trying_temporal_and_browser_support"><a href="#trying_temporal_and_browser_support">Trying Temporal and browser support</a></h2><div><p>Support is slowly starting to be included in experimental browser releases, and Firefox appears to have the most mature implementation at this point.
In Firefox, Temporal is being built into the <a href="https://www.mozilla.org/en-US/firefox/channel/desktop/" target="_blank">Nightly version</a> behind the <code>javascript.options.experimental.temporal</code> preference.
If you want to see the full compatibility story, you can check the (quite epic) <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal#browser_compatibility">Temporal object Browser Compatibility section</a>.</p>
<p>Here are the main browser bugs that track Temporal implementations:</p>
<ul>
<li><strong>Firefox:</strong> <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1912757" target="_blank">Build temporal in Nightly by default</a></li>
<li><strong>Safari:</strong> <a href="https://bugs.webkit.org/show_bug.cgi?id=223166" target="_blank">[JSC] Implement Temporal</a></li>
<li><strong>Chrome:</strong> <a href="https://issues.chromium.org/issues/42201538" target="_blank">Implement the Temporal proposal</a></li>
</ul>
<p>Additionally, you can visit <a href="https://tc39.es/proposal-temporal/docs/" target="_blank">https://tc39.es/proposal-temporal/docs/</a> which has <code>@js-temporal/polyfill</code> available.
That means, you can open the developer tools on the TC39 docs page, and try some of the examples in the console in any browser without changing flags or preferences.</p>
<p>With experimental implementations landing, now is a good time to try out Temporal and become familiar with what will be the modern approach to handling dates and times in JavaScript.</p></div></section><section aria-labelledby="acknowledgements"><h2 id="acknowledgements"><a href="#acknowledgements">Acknowledgements</a></h2><div><ul>
<li>Thanks to <a href="https://meyerweb.com/" target="_blank">Eric Meyer</a> for his work on the topic.
It's been roughly 4 years since Eric's efforts to document <a href="https://github.com/mdn/browser-compat-data/pull/10643" target="_blank">browser compatibility data</a> and scaffold the documentation <a href="https://github.com/meyerweb/content/tree/temporal/" target="_blank">in his fork of mdn/content</a>.</li>
<li><a href="https://developer.mozilla.org/en-US/community/spotlight/joshua-chen">Joshua Chen</a> for picking up the torch from Eric and getting a pull request together for <a href="https://github.com/mdn/content/pull/37344" target="_blank">the MDN documentation</a>.</li>
<li><a href="https://bugzilla.mozilla.org/user_profile?user_id=339940" target="_blank">André Bargull</a> for the work on the Firefox Temporal implementation.</li>
</ul></div></section><section aria-labelledby="see_also"><h2 id="see_also"><a href="#see_also">See also</a></h2></section><section><a href="https://developer.mozilla.org/en-US/blog/fix-image-lcp/"><article><h2><strong>Previous<!-- --> Post</strong> <!-- -->Fix your website's Largest Contentful Paint by optimizing image loading</h2></article></a></section></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The US government's open data on Data.gov is currently being scrubbed (129 pts)]]></title>
            <link>https://old.reddit.com/r/climate/comments/1idiliv/the_us_governments_open_data_on_datagov_is/</link>
            <guid>42876055</guid>
            <pubDate>Thu, 30 Jan 2025 08:36:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/climate/comments/1idiliv/the_us_governments_open_data_on_datagov_is/">https://old.reddit.com/r/climate/comments/1idiliv/the_us_governments_open_data_on_datagov_is/</a>, See on <a href="https://news.ycombinator.com/item?id=42876055">Hacker News</a></p>
Couldn't get https://old.reddit.com/r/climate/comments/1idiliv/the_us_governments_open_data_on_datagov_is/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Investigating the case of human nose shape and climate adaptation (2017) (113 pts)]]></title>
            <link>https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1006616</link>
            <guid>42875888</guid>
            <pubDate>Thu, 30 Jan 2025 08:04:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1006616">https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1006616</a>, See on <a href="https://news.ycombinator.com/item?id=42875888">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content">

<header>





<ul id="almSignposts">
  <li id="loadingMetrics">
    <p>Loading metrics</p>
  </li>
</ul>







    <div>
  <p id="licenseShort">Open Access</p>
  <p id="peerReviewed">Peer-reviewed</p>

<p id="artType">Research Article</p>


</div>
    <div>



<div>
  

<ul data-js-tooltip="tooltip_container" id="author-list">



<li data-js-tooltip="tooltip_trigger">
       
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="1">
Brooke C. Mattern,</a>    
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="2">
Peter Claes,</a>    
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="3">
Brian McEcoy,</a>    
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="4">
Cris Hughes,</a>    
</li>

<li data-js-tooltip="tooltip_trigger">
   <a data-author-id="5">
Mark D. Shriver <span>  </span></a>    
</li>

</ul>


</div>


<div id="floatTitleTop" data-js-floater="title_author" role="presentation">
    <div>
      <h2><!--?xml version="1.0" encoding="UTF-8"?-->Investigating the case of human nose shape and climate adaptation</h2>

<ul id="floatAuthorList" data-js-floater="floated_authors">

  <li data-float-index="1">Arslan A. Zaidi,&nbsp;

  </li>
  <li data-float-index="2">Brooke C. Mattern,&nbsp;

  </li>
  <li data-float-index="3">Peter Claes,&nbsp;

  </li>
  <li data-float-index="4">Brian McEcoy,&nbsp;

  </li>
  <li data-float-index="5">Cris Hughes,&nbsp;

  </li>
  <li data-float-index="6">Mark D. Shriver

  </li>

</ul>



    </div>
    <div id="titleTopCloser">
      <p><img src="https://journals.plos.org/resource/img/logo-plos.png" alt="PLOS"></p><p>x</p>
    </div>
  </div>

      <ul>
        <li id="artPubDate">Published: March 16, 2017</li>
        <li id="artDoi">
<a href="https://doi.org/10.1371/journal.pgen.1006616">https://doi.org/10.1371/journal.pgen.1006616</a>
        </li>
        <li></li>
      </ul>

    </div>
  
</header>
  <div>




  


<div id="figure-carousel-section">
  <h2>Figures</h2>

  
</div>





        <div id="artText">
          



<div xmlns:plos="http://plos.org"><h2>Abstract</h2><div><p>The evolutionary reasons for variation in nose shape across human populations have been subject to continuing debate. An import function of the nose and nasal cavity is to condition inspired air before it reaches the lower respiratory tract. For this reason, it is thought the observed differences in nose shape among populations are not simply the result of genetic drift, but may be adaptations to climate. To address the question of whether local adaptation to climate is responsible for nose shape divergence across populations, we use Qst–Fst comparisons to show that nares width and alar base width are more differentiated across populations than expected under genetic drift alone. To test whether this differentiation is due to climate adaptation, we compared the spatial distribution of these variables with the global distribution of temperature, absolute humidity, and relative humidity. We find that width of the nares is correlated with temperature and absolute humidity, but not with relative humidity. We conclude that some aspects of nose shape may indeed have been driven by local adaptation to climate. However, we think that this is a simplified explanation of a very complex evolutionary history, which possibly also involved other non-neutral forces such as sexual selection.</p>
</div></div><div xmlns:plos="http://plos.org"><h2>Author summary</h2>
<div><p>The study of human adaptation is essential to our understanding of disease etiology. Evolutionary investigations into why certain disease phenotypes such as sickle-cell anemia and lactose intolerance occur at different rates in different populations have led to a better understanding of the genetic and environmental risk factors involved. Similarly, research into the geographical distribution of skin pigmentation continues to yield important clues regarding risk of vitamin D deficiency and skin cancer. Here, we investigate whether variation in the shape of the external nose across populations has been driven by regional differences in climate. We find that variation in both nares width and alar base width appear to have experienced accelerated divergence across human populations. We also find that the geospatial distribution of nares width is correlated with temperature, and absolute humidity, but not with relative humidity. Our results support the claim that local adaptation to climate may have had a role in the evolution of nose shape differences across human populations.</p>
</div></div>


<div xmlns:plos="http://plos.org"><p><strong>Citation: </strong>Zaidi AA, Mattern BC, Claes P, McEcoy B, Hughes C, Shriver MD (2017) Investigating the case of human nose shape and climate adaptation. PLoS Genet 13(3):
           e1006616.
        
        https://doi.org/10.1371/journal.pgen.1006616</p><p><strong>Editor: </strong>Greg Gibson, Georgia Institute of Technology, UNITED STATES</p><p><strong>Received: </strong>December 17, 2015; <strong>Accepted: </strong>February 3, 2017; <strong>Published: </strong> March 16, 2017</p><p><strong>Copyright: </strong> © 2017 Zaidi et al. This is an open access article distributed under the terms of the <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p><p><strong>Data Availability: </strong>We have made almost all data, which were used to generate results in the paper, publicly available in a spreadsheet in the Supporting Information section (<a href="#pgen.1006616.s009">S1 File</a>). The spreadsheet includes de-identified data, split by the analyses for which they were used. More specifically, we have provided several phenotypes (nose shape measures, skin pigmentation, height, weight, and BMI) as well as relevant covariates (ancestry components, sex, and age). Where relevant, we have also provided climate values (mean annual temperature, annual aridity index, and ultraviolet B irradiance) ascribed to each individual. These data can be used to replicate almost all the results presented in the manuscript, without compromising the privacy of the participants. Some raw data, such as the 3D photographs, genotypes, and birthplaces, have not been shared because they are highly identifiable in nature and sharing them would be in ethical and legal violation of the informed consent obtained from participants. This restriction is not because of any personal or commercial interests.</p><p><strong>Funding: </strong>This work was supported in part by the United States National Institute of Justice, the United States Department of Defense, the University of Illinois Interdisciplinary Innovation Initiative Research Grant, the Flemish Institute for the Promotion of Innovation by Science and Technology in Flanders (IWT Vlaanderen), the Research Program of the Fund for Scientific Research - Flanders (Belgium), and the Center for Human Evolution and Development at Penn State. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p><p><strong>Competing interests: </strong> The authors have declared that no competing interests exist.</p></div>





<div xmlns:plos="http://plos.org" id="section1"><h2>Introduction</h2><p>The shape of the nose, like many other parts of the face, varies both within as well as across human populations. For example, the distance between the nasal alare (wings of the nose) are significantly larger in individuals of West African, South Asian, and East Asian ancestry as compared to persons with European ancestry [<a href="#pgen.1006616.ref001">1</a>]. The nasal index (width/height of the nasal aperture of the skull) is also known to vary significantly among populations [<a href="#pgen.1006616.ref001">1</a>,<a href="#pgen.1006616.ref002">2</a>]. Whether these population differences in nose shape are due primarily to genetic drift or natural selection is unclear.</p>
<p>A vital function of the nose is to warm inspired air to core body temperature and saturate it with water vapor before it reaches the lower respiratory tract [<a href="#pgen.1006616.ref003">3</a>]. In fact, inhaled air reaches 90% of the required temperature and humidity levels before even reaching the nasopharynx, implicating the nasal cavity as the major conditioning apparatus in the respiratory tract [<a href="#pgen.1006616.ref004">4</a>,<a href="#pgen.1006616.ref005">5</a>]. This conditioning acts to maintain proper functioning of the mucociliary apparatus, which functions in trapping particles and pathogens and removing them from the airways. Low humidity in the respiratory tract leads to impaired mucociliary function and increases the risk of both upper and lower respiratory tract infections [<a href="#pgen.1006616.ref006">6</a>]. Much of the air conditioning occurs as it passes through the turbinates, the walls of which are lined with blood vessels and mucus producing goblet cells [<a href="#pgen.1006616.ref004">4</a>]. Studies have shown that the efficacy of the conditioning process depends on the flow dynamics of the inspired air, which in turn, depend on the geometry of the nasal cavity and inlets [<a href="#pgen.1006616.ref005">5</a>,<a href="#pgen.1006616.ref007">7</a>]. Because of the function of the nose as an air-conditioning apparatus, it is hypothesized that differences in nose shape across populations may have been driven by local adaptation to climate [<a href="#pgen.1006616.ref002">2</a>,<a href="#pgen.1006616.ref003">3</a>,<a href="#pgen.1006616.ref008">8</a>].</p>
<p>There are several challenges in testing this hypothesis. We know there is substantial nose shape variation among human populations, in both the external morphology of the nose as well as the underlying cranial morphology [<a href="#pgen.1006616.ref001">1</a>,<a href="#pgen.1006616.ref002">2</a>,<a href="#pgen.1006616.ref009">9</a>]. While this can be explained by adaptation to local selection pressures, it could also be explained by the fact that phenotypic differences among geographically distant populations can arise simply due to genetic drift. Thus, in order to invoke divergent selection as an explanation, one must demonstrate that the observed variation in nose shape across human populations is greater than that expected under genetic drift alone. This can be carried out using the Qst statistic, which measures the degree of genetic differentiation underlying a quantitative trait [<a href="#pgen.1006616.ref010">10</a>]. In principle, the Qst of a neutrally evolving trait is expected to follow the Fst distribution of neutrally evolving loci [<a href="#pgen.1006616.ref011">11</a>]. Thus, when Qst is much greater than Fst, trait divergence exceeds neutral expectations and can be attributed to divergent selection [<a href="#pgen.1006616.ref011">11</a>]. The problem with Qst is that its calculation requires knowledge of the additive genetic variances within- and among-populations. These can only be estimated reliably using ‘common-garden’ experiments, in which environmental effects on the phenotype can be effectively controlled. Since such experiments are not possible in humans, Qst-based inference regarding divergent selection on human phenotypes necessitates making realistic assumptions about the heritabilities of the phenotypes in question.</p>
<p>Several studies have used this approach and found that while most aspects of the skull seem to be evolving neutrally, the shape of the nasal aperture appears to be more differentiated across human populations than expected under genetic drift [<a href="#pgen.1006616.ref009">9</a>,<a href="#pgen.1006616.ref012">12</a>,<a href="#pgen.1006616.ref013">13</a>]. More recently, it was also reported that the divergence in the shape of the external nose, at least between Europeans and Han Chinese populations, also exceeds neutral expectations [<a href="#pgen.1006616.ref014">14</a>]. While this might be true, most of these studies employed anticonservative heritability assumptions, which overestimate the genetic differentiation underlying a trait, and thus, lead to incorrect conclusions regarding the relative roles of selection and drift in driving phenotypic differences among populations.</p>
<p>Here, we carried out an exploration of external nose shape variation in light of quantitative genetic theory to investigate whether nose shape variation among populations exceeds neutral expectations. In doing so, we discuss the methodological challenges involved in tackling such questions in humans, and show how the limitations of previous studies can be addressed with recent advances in statistical genetics. As proof of principle, we compare the differentiation of nose shape with two highly heritable morphological traits, which are known to have a polygenic basis; namely height and skin pigmentation [<a href="#pgen.1006616.ref015">15</a>–<a href="#pgen.1006616.ref018">18</a>]. Both height and skin pigmentation exhibit substantial variation within and across human populations, and are thought to be under positive selection in various populations [<a href="#pgen.1006616.ref019">19</a>–<a href="#pgen.1006616.ref022">22</a>]. Skin pigmentation is also a good example of a phenotype which is known to have evolved in response to a geospatially varying selection pressure: exposure to ground level ultraviolet B radiation (UVB) [<a href="#pgen.1006616.ref023">23</a>]. Finally, we test whether clinal variation in aspects of nose shape, which appear to be under accelerated divergence across populations, covaries with geographic variation in temperature and humidity, in order to determine whether this divergence is due to climatic selection pressures.</p>
</div>

<div xmlns:plos="http://plos.org" id="section2"><h2>Results</h2>
<div id="section1"><h3>Describing variation in nose shape</h3>
<p>To quantify variation in nose shape, we first captured high resolution 3D images of participants’ faces using the 3dMD Face system (<em>3dMD Atlanta</em>, <em>GA</em>). Five positioning landmarks (two on the inner corner of the eyes, two on the outer corners of the mouth, and one on the tip of the nose) were placed in order to establish facial orientation. A spatially dense mesh of 7,150 quasi-landmarks (QLs) was mapped onto each image and its reflection. Generalized Procrustes Superimposition [<a href="#pgen.1006616.ref024">24</a>] was carried out on both sets of images (original and reflected) to remove differences in position and orientation. The Procrustes coordinates of the original and reflected image for each participant were then averaged to remove effects of bilateral asymmetry following Claes <em>et al</em>. (2014) [<a href="#pgen.1006616.ref025">25</a>]. The nose region, which is comprised of 709 out of the 7,150 QLs, was selected for downstream analyses.</p>
<p>We used linear distances and areas to characterize the shape of the nose (<a href="#pgen-1006616-g001">Fig 1</a>). The linear distances (measured in mm) were calculated using seven standard anthropometric landmarks: i. nasion (n), ii. pronasale (prn), iii. subnasale (sn), iv. Left alar curvature, (al<sub>l</sub>), v. right alar curvature (al<sub>r</sub>), vi. left alar base (ac<sub>l</sub>), and vii. Right alar base (ac<sub>r</sub>) (<a href="#pgen-1006616-g001">Fig 1</a>) [<a href="#pgen.1006616.ref026">26</a>,<a href="#pgen.1006616.ref027">27</a>]. These seven landmarks were placed on the spatially-dense mesh of 709 QLs, which facilitated automated placement on each face. We used the x, y, and z coordinates of these landmarks to calculate five Euclidean distances: i. nares width (al<sub>l</sub>−al<sub>r</sub>), ii. alar base width (ac<sub>l</sub>−ac<sub>r</sub>), iii. nasal height (n–sn), iv. nasal ridge length (n–prn), and v. nasal tip protrusion (sn–prn) (<a href="#pgen-1006616-g001">Fig 1</a>). We also computed two areas (measured in mm<sup>2</sup>): i. total external area of the nose and ii. mean nostril area (<a href="#pgen-1006616-g001">Fig 1</a>) (Methods).</p>
<div data-doi="10.1371/journal.pgen.1006616.g001"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosgenetics/article/figure/image?size=medium&amp;id=10.1371/journal.pgen.1006616.g001" data-doi="10.1371/journal.pgen.1006616" data-uri="10.1371/journal.pgen.1006616.g001"><img src="https://journals.plos.org/plosgenetics/article/figure/image?size=inline&amp;id=10.1371/journal.pgen.1006616.g001" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 1. </span> Measurements used to describe the shape of the nose.</p><p>A) The locations of landmarks, which were used to calculate linear distances, are shown as red points. B) Five Linear distances (red lines) and two surface areas (red mesh) were used to describe nose shape. Linear distances were calculated from the 3D coordinates of landmarks (red points). Surface areas were computed as the sum of the polygons highlighted in red.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.g001">
              https://doi.org/10.1371/journal.pgen.1006616.g001</a></p></div><p><a href="#pgen-1006616-g002">Fig 2</a> shows the distribution of aspects of nose shape in males and females from four human population groups: West African, East Asian, South Asian, and Northern European. These groups were defined based on genetic and self-reported ancestry (Methods). <a href="#pgen-1006616-g002">Fig 2</a> also shows the distribution of height and melanin index, a measure of skin pigmentation level derived from reflectance spectrophotometry (measured on the upper inner arms; see <a href="#sec009">Methods</a>), for comparison. The sample size, mean, and standard deviation for each phenotype are provided in <a href="#pgen-1006616-t001">Table 1</a>. One clear observation is that all aspects of nose shape, at least those considered here, are highly sexually dimorphic (<a href="#pgen-1006616-g002">Fig 2</a>, <a href="#pgen-1006616-t001">Table 1</a>). Males, on average, tend to have wider nares, longer nasal ridges, more outwardly protruding nasal tips, bigger nostrils, and a larger overall external surface area, compared to females (<a href="#pgen-1006616-g002">Fig 2</a>, <a href="#pgen-1006616-t001">Table 1</a>). All nasal measurements are also significantly different across populations (<a href="#pgen.1006616.s008">S1 Table</a>). Based on <a href="#pgen-1006616-t001">Table 1</a>, we note some general patterns. For example, nares width and alar base width are largest in W. Africans and smallest in N. Europeans. These findings are consistent with previous observations [<a href="#pgen.1006616.ref001">1</a>,<a href="#pgen.1006616.ref002">2</a>]. Nasal tip protrusion is greatest in N. Europeans and smallest in W. Africans and E. Asians. E. Asians also tend to have the smallest noses in terms of external surface area.</p>
<div data-doi="10.1371/journal.pgen.1006616.g002"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosgenetics/article/figure/image?size=medium&amp;id=10.1371/journal.pgen.1006616.g002" data-doi="10.1371/journal.pgen.1006616" data-uri="10.1371/journal.pgen.1006616.g002"><img src="https://journals.plos.org/plosgenetics/article/figure/image?size=inline&amp;id=10.1371/journal.pgen.1006616.g002" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 2. </span> Boxplots of phenotypes by population and sex overlaid with the individual data points.</p><p>Height is measured in centimeters and melanin index is measured in percentage reflectance (Methods). Linear distances are measured in millimeters (mm) and area are measured in mm<sup>2</sup>. Points are individual observations and the color of the boxplots and points represents sex with blue indicating males and red indicating females.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.g002">
              https://doi.org/10.1371/journal.pgen.1006616.g002</a></p></div></div>

<div id="section2"><h3>Testing for accelerated divergence</h3>
<p>Differences in a phenotype can accumulate across populations simply due to genetic drift. In order to invoke positive directional selection, one must demonstrate that the variation across populations is more than that expected under genetic drift. We used Qst (see Leinonen <em>et al</em>. [<a href="#pgen.1006616.ref028">28</a>] for a recent review), the quantitative genetic analog of Fst [<a href="#pgen.1006616.ref010">10</a>,<a href="#pgen.1006616.ref029">29</a>], to test whether certain aspects of nose shape exhibit greater differentiation across populations than expected under genetic drift alone. Qst measures the degree of genetic differentiation in a quantitative trait across populations and is defined as:
<a name="pgen.1006616.e001" id="pgen.1006616.e001"></a><span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e001" loading="lazy"><span>(1)</span></span>
where <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e002" loading="lazy"></span> and <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e003" loading="lazy"></span> are the components of phenotypic variance due to additive genetic effects among and within populations, respectively. It has been shown that, in principle, the distribution of Qst of a quantitative trait that has evolved under genetic drift alone is expected to be equal to Fst of neutral genetic markers [<a href="#pgen.1006616.ref011">11</a>,<a href="#pgen.1006616.ref030">30</a>,<a href="#pgen.1006616.ref031">31</a>]. This expectation allows one to compare Qst to Fst to test whether genetic drift alone is sufficient to explain the divergence of a trait among populations. If the Qst of a trait across a set of populations is much greater than Fst, it means that the phenotypic differentiation exceeds the expectation under neutrality.</p>
<p>The components of additive genetic variance, <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e004" loading="lazy"></span> and <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e005" loading="lazy"></span>, are typically estimated from ‘common-garden’ experiments in which the effects due to the environment can be controlled [<a href="#pgen.1006616.ref028">28</a>]. This is often not possible in non-model organisms, especially humans, because of practical and ethical limitations. In such cases, <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e006" loading="lazy"></span> and <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e007" loading="lazy"></span> can be estimated from the among- and within-population components of <em>phenotypic</em> variance, <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e008" loading="lazy"></span> and <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e009" loading="lazy"></span>, respectively, provided the heritabilities underlying these components are known:
<a name="pgen.1006616.e010" id="pgen.1006616.e010"></a><span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e010" loading="lazy"><span>(2)</span></span></p>
<p>Here <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e011" loading="lazy"></span> and <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e012" loading="lazy"></span> are among- and within-population components of the phenotypic variance and <em>c</em> and <em>h</em><sup><em>2</em></sup> are proportions of <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e013" loading="lazy"></span> and <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e014" loading="lazy"></span>, respectively, that are due to additive genetic effects. Both <em>c</em> and <em>h</em><sup><em>2</em></sup> are can range from 0 (none of the variance is due to additive genetic effects) to 1 (all of the variance is due to additive genetic effects). Eq (<a href="#pgen.1006616.e010">2</a>) shows that Qst calculated from phenotypic variance components depends on the ratio between <em>c</em> and <em>h</em><sup><em>2</em></sup> [<a href="#pgen.1006616.ref032">32</a>]. Without prior information, it is reasonable to assume <em>c/h</em><sup><em>2</em></sup> = 1, i.e., the proportion of phenotypic variance due to additive genetic effects is the same among- and within-populations. Qst calculated this way is sometimes referred to as Pst [<a href="#pgen.1006616.ref033">33</a>]. However, we will continue to use the term Qst to avoid confusion and will evaluate the validity of the assumption that <em>c/h</em><sup><em>2</em></sup> = 1 in the following section.</p>
<p>We calculated Qst for each aspect of nose shape, described in the previous section, across four human population groups: i) West African (N = 40), ii) North European (N = 236), iii) East Asian (N = 127), and iv) South Asian (N = 73) (see <a href="#sec009">Methods</a> for selection criteria). We used a non-parametric bootstrap approach to generate the empirical distributions of Qst and Fst and to test whether the observed value of Qst is greater than Fst (Methods). The statistic we used is Qst–Fst, which, under the null hypothesis of genetic drift, is expected to be equal to zero. The larger the Qst–Fst of a phenotype, the stronger the evidence that the variation in the phenotype across populations is more than that expected under genetic drift alone. We refer to outliers in the neutral distribution as signals of accelerated divergence for brevity. The strength of evidence for accelerated divergence can be measured using an empirical p-value, which is the proportion of bootstrapped values of Qst–Fst that are less than zero. To compare with other quantitative traits with a polygenic basis, we also tested whether height and skin pigmentation exhibit signals of accelerated divergence. The results are illustrated in <a href="#pgen-1006616-g003">Fig 3</a> and the p-values are listed in <a href="#pgen-1006616-t002">Table 2</a>. We treat phenotypes that pass a stringent Bonferonni correction (p-value &lt;0.0071 = 0.05/7 for seven nose shape traits) as exhibiting signals of accelerated divergence across populations.</p>
<div data-doi="10.1371/journal.pgen.1006616.g003"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosgenetics/article/figure/image?size=medium&amp;id=10.1371/journal.pgen.1006616.g003" data-doi="10.1371/journal.pgen.1006616" data-uri="10.1371/journal.pgen.1006616.g003"><img src="https://journals.plos.org/plosgenetics/article/figure/image?size=inline&amp;id=10.1371/journal.pgen.1006616.g003" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 3. </span> Qst–Fst results across all populations.</p><p>The bootstrapped distribution of Qst–Fst for each phenotype (shown by a violin plot) is compared against the expected value of zero under neutrality (horizontal dashed line). Phenotypes, which exhibit accelerated divergence (using a Bonferronni corrected p-value threshold of 0.0071), are shown in red.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.g003">
              https://doi.org/10.1371/journal.pgen.1006616.g003</a></p></div><p>As expected, skin pigmentation shows a strong signal of accelerated divergence across populations (Qst = 0.642, p-value = 1.00E-04). This is in accordance with the idea that differences in skin pigmentation, at least across continental populations, have been driven by positive selection. Height, on the other hand, does not show signals of accelerated divergence (Qst = 0.177, p-value = 0.14). While height seems to be under positive <em>selection within</em> European and African populations separately [<a href="#pgen.1006616.ref019">19</a>,<a href="#pgen.1006616.ref034">34</a>], it does not appear to be driven by divergent selection pressures across the populations considered here. Of all the nose shape measurements, only nares width (Qst = 0.467, p-value = 2.80E-03) and alar base width (Qst = 0.440, p-value = 4.90E-03) exhibit signals of accelerated divergence. However, the degree of divergence in these traits is not as high as that observed for skin pigmentation.</p>
</div>

<div id="section3"><h3>Evaluating validity of assumptions regarding <em>c</em> and <em>h</em><sup><em>2</em></sup></h3>
<p>We estimated Qst under the assumption that <em>c/h</em><sup><em>2</em></sup> = 1. In other words, we assume that the proportion of between-population variance in the trait due to additive genetic effects is the same as the proportion of within-population variance due to additive genetic effects. This can be an incorrect assumption if a large proportion of the phenotypic variance between populations is due to direct environmental effects (i.e., phenotypic plasticity). If the true value of <em>c/h</em><sup><em>2</em></sup> is drastically lower than 1, phenotype-derived estimates of the among-population genetic variance, and hence, Qst will be inflated, resulting in false-positive conclusions regarding the role of selection in driving phenotypic differentiation. This fact has largely been ignored in previous studies, in which Qst-based approaches were used to explore craniofacial divergence [<a href="#pgen.1006616.ref012">12</a>–<a href="#pgen.1006616.ref014">14</a>,<a href="#pgen.1006616.ref035">35</a>]. In these studies, <em>h</em><sup><em>2</em></sup> was assumed to be 0.55 and <em>c</em> was implicitly assumed to be 1, resulting in <em>c/h</em><sup><em>2</em></sup> to be greater than 1. While this might be true for some traits (for example, see skin pigmentation in <a href="#pgen-1006616-t003">Table 3</a>), this is an anticonservative approach in our opinion, because it assumes that the heritability across populations is higher than the heritability within populations. If anything, the opposite case (<em>c/h</em><sup><em>2</em></sup> &lt; 1) is more plausible as we expect environmental variation to be large across geographically distant populations. Keeping this in mind, we evaluated the sensitivity of the Qst–Fst results to the case where <em>c</em>/<em>h</em><sup><em>2</em></sup> &lt; 1 [<a href="#pgen.1006616.ref032">32</a>], by determining the ‘critical value’ of <em>c/h</em><sup><em>2</em></sup> at which the 95% lower bound of Qst meets the 95% upper bound of Fst (<a href="#pgen-1006616-g004">Fig 4</a>). Smaller critical values imply that phenotype-based Qst–Fst comparisons are robust to <em>c/h</em><sup><em>2</em></sup> assumptions. Such ‘sensitivity analyses’, while common in the molecular ecology literature, are underrepresented in human genetic and anthropological studies. <a href="#pgen-1006616-g004">Fig 4</a> shows the sensitivity curves for height, skin pigmentation, and nose shape traits. Skin pigmentation, which shows a very high Qst value, has a critical value of 0.25, which is much lower than 1 (<a href="#pgen-1006616-g004">Fig 4</a> and <a href="#pgen-1006616-t002">Table 2</a>). The critical value of nares width is 0.50 and that of alar base width is 0.55, which are also much lower than 1 (<a href="#pgen-1006616-g004">Fig 4</a> and <a href="#pgen-1006616-t002">Table 2</a>). Thus, we expect that these phenotypes would exhibit signals of accelerated divergence, even if the true values of <em>c/h</em><sup><em>2</em></sup> were much less than 1.</p>
<div data-doi="10.1371/journal.pgen.1006616.g004"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosgenetics/article/figure/image?size=medium&amp;id=10.1371/journal.pgen.1006616.g004" data-doi="10.1371/journal.pgen.1006616" data-uri="10.1371/journal.pgen.1006616.g004"><img src="https://journals.plos.org/plosgenetics/article/figure/image?size=inline&amp;id=10.1371/journal.pgen.1006616.g004" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 4. </span> Sensitivity plots showing critical values of <em>c/h</em><sup><em>2</em></sup>.</p><p>Change in median Qst of height, skin pigmentation, and nose shape measures as a function of <em>c/h</em><sup><em>2</em></sup> is shown as a solid red line. Median Fst is shown as a solid blue line. The lower and upper 95% bounds for Qst and Fst are shown as dashed red and blue lines, respectively. The critical value at which the lower bound of Qst meets the upper bound of Fst is shown on the upper left corner of each plot. Lower critical values indicate the Qst–Fst is more robust to <em>c/h</em><sup><em>2</em></sup> assumptions.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.g004">
              https://doi.org/10.1371/journal.pgen.1006616.g004</a></p></div></div>

<div id="section4"><h3>Investigating heritability of nose shape within- and among-populations</h3>
<p>In addition to the sensitivity analysis, we wanted to demonstrate that differences in nose shape among individuals both within- and between-populations are heritable. Within-population heritability (<em>h</em><sup>2</sup>) for a phenotype is traditionally estimated from data collected on large sets of twins or from pedigrees where the genetic relationships among individuals are known [<a href="#pgen.1006616.ref036">36</a>]. Recently, Yang and colleagues (2010) introduced a linear mixed model approach, which can be used to estimate an alternative statistic in unrelated individuals: the proportion of phenotypic variance explained by genotyped SNPs (<em>h</em><sub><em>g</em></sub><sup><em>2</em></sup>) [<a href="#pgen.1006616.ref018">18</a>]. A major advantage of this approach over traditional twin- and pedigree-based approaches is that it is applicable to data from unrelated individuals, which are more accessible and easier to collect in large numbers. We calculated <em>h</em><sub><em>g</em></sub><sup><em>2</em></sup> as an estimate of <em>h</em><sup><em>2</em></sup> for height, melanin index, and nose shape traits using 118,420 autosomal SNPs in a sample of unrelated persons of European ancestry, implemented in the GCTA software [<a href="#pgen.1006616.ref037">37</a>] (Methods). The values of <em>h</em><sub><em>g</em></sub><sup><em>2</em></sup> are provided in <a href="#pgen-1006616-t003">Table 3</a>. The heritability for height in Europeans (<em>h</em><sub><em>g</em></sub><sup><em>2</em></sup> = 0.394; N = 1,825) is similar to previously reported estimates [<a href="#pgen.1006616.ref018">18</a>]. Interestingly, the heritability of skin pigmentation is very low in Europeans (<em>h</em><sub><em>g</em></sub><sup><em>2</em></sup> = 0.191; N = 1,231), which we think can be explained by reduced genetic variation at skin pigmentation loci due to strong positive selection for lighter skin in Europeans [<a href="#pgen.1006616.ref017">17</a>,<a href="#pgen.1006616.ref021">21</a>,<a href="#pgen.1006616.ref038">38</a>]. Almost all aspects of nose shape seem to be highly heritable in Europeans (<em>h</em><sub><em>g</em></sub><sup><em>2</em></sup> range: 0.401–0.657 N = 1,718) (<a href="#pgen-1006616-t003">Table 3</a>).</p>
<p>Estimation of the among-population heritability (<em>c</em>) is more difficult, since genetic effects between geographically distant populations can be confounded with large environmental effects. This parameter can often only be reliably estimated through ‘common garden’ experiments, in which systematic differences in the environment, which might be confounded with genetic differences, can be taken into account. However, as mentioned earlier, this approach is only possible in model organisms, which are amenable to experimental manipulation. In humans and other non-model organisms, this is a severe limitation, which can be overcome by studying naturally occurring admixed populations.</p>
<p>The process of admixture reunites gene pools from two or more populations, which might have diverged due to genetic drift, mutation, and selection. Admixture, which may occur repeatedly over several generations, followed by recombination, leads to chromosomes that are essentially mosaics of ancestry segments (<a href="#pgen-1006616-g005">Fig 5</a>) [<a href="#pgen.1006616.ref039">39</a>]. In a randomly mating admixed population, ancestry segments segregate randomly with respect to the environment, which decouples the between-population genetic effects on the phenotype from the environmental effects, allowing for the estimation of the genetic variance underlying a phenotype. We propose that a method recently developed to estimate heritability in admixed populations (Zaitlen <em>et al</em>. (2014)) [<a href="#pgen.1006616.ref040">40</a>], might provide a valid estimate of <em>c</em>. Zaitlen <em>et al</em>. (2014) extend the method developed by Yang <em>et al</em>. (2010) [<a href="#pgen.1006616.ref018">18</a>], using local ancestry at SNPs, instead of genotypes, to construct the genetic relationship matrix among admixed individuals. The proportion of phenotypic variance in an admixed population that can be explained by local ancestry (<em>h</em><sub><em>y</em></sub><sup><em>2</em></sup>) is conceptually equivalent to <em>c</em> between the parental populations. Our reasoning is that, on a genotypic level, the genetic variation in an admixed population should be the sum of the genetic variation within the parental populations and the genetic variation between them. Variation at the scale of local ancestry only represents genetic variation between populations (<a href="#pgen-1006616-g005">Fig 5</a>). Thus, the proportion of phenotypic variation that can be explained by local ancestry (<em>h</em><sub><em>y</em></sub><sup><em>2</em></sup>), should be equivalent to <em>c</em>.</p>
<div data-doi="10.1371/journal.pgen.1006616.g005"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosgenetics/article/figure/image?size=medium&amp;id=10.1371/journal.pgen.1006616.g005" data-doi="10.1371/journal.pgen.1006616" data-uri="10.1371/journal.pgen.1006616.g005"><img src="https://journals.plos.org/plosgenetics/article/figure/image?size=inline&amp;id=10.1371/journal.pgen.1006616.g005" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 5. </span> Admixture brings together within- and between-population variation in admixed individuals.</p><p>Genetic variation between the two parental populations is represented by difference in color of the chromosomes, whereas genetic variation within the populations is represented by hue intensity. Admixture brings together genetic variation from both populations. On a genotypic level, genetic variation within admixed populations is composed of both within-population and between-population variation. The variation at the level of ancestry is variation between the two parental populations.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.g005">
              https://doi.org/10.1371/journal.pgen.1006616.g005</a></p></div><p>Following this reasoning, we estimated <em>h</em><sub><em>y</em></sub><sup><em>2</em></sup> from local ancestry at 623,625 autosomal SNPs in a sample of 409 Cape Verdeans, who derive their ancestry primarily from W. Africans and Europeans (see <a href="#sec009">Methods</a> for details and <a href="#pgen-1006616-t003">Table 3</a> for results). <a href="#pgen-1006616-t003">Table 3</a> shows that the phenotypic variation between W. Africans and Europeans in height (<em>h</em><sub><em>y</em></sub><sup><em>2</em></sup> = 0.224, N = 409) is quite heritable. Results for skin pigmentation data are not presented here as they were unavailable. Differences in many aspects of nose shape are also heritable (Nares width: <em>h</em><sub><em>y</em></sub><sup><em>2</em></sup> = 0.226, Alar base width: <em>h</em><sub><em>y</em></sub><sup><em>2</em></sup> = 0.212, Nasal tip protrusion: 0.177, External surface area: <em>h</em><sub><em>y</em></sub><sup><em>2</em></sup> = 0.121, N = 409). Other aspects of nose shape may not be as heritable between these two populations (Nasal height: <em>h</em><sub><em>y</em></sub><sup><em>2</em></sup> = 0.03, <em>h</em><sub><em>y</em></sub><sup><em>2</em></sup> = 0.059, N = 409). Another interesting observation from <a href="#pgen-1006616-t003">Table 3</a> is that estimates of <em>h</em><sub><em>y</em></sub><sup><em>2</em></sup> are generally lower than estimates of <em>h</em><sub><em>g</em></sub><sup><em>2</em></sup> for all traits. This suggests that for most human traits, the additive genetic variance between populations might be less than the additive genetic variance within populations, which agrees with the fact that most of the genetic variation in humans exists within populations. However, we are cautious of over-stating this conclusion since the heritabilities were estimated in individuals with W. African and European ancestry only and do not reflect the variation within and across other populations. Altogether, our results show that genetic differences underlie the variation in many aspects of nose shape, both within- and between-populations.</p>
</div>

<div id="section5"><h3>Testing for adaptation to climate</h3>
<p>Previously, several studies have shown that the shapes of the nasal aperture and nasal cavity are correlated with climate variables related to temperature and humidity such that individuals from cold-dry climates exhibit narrower nasal cavities compared to individuals from warm-humid climates [<a href="#pgen.1006616.ref041">41</a>,<a href="#pgen.1006616.ref042">42</a>]. We were interested in testing whether aspects of external nose shape showing unusually high differentiation across populations based on Qst–Fst analysis, show correlations with climate. For this purpose, we selected, from the subset used in the Qst analysis, females with genetic data whose parents were born in a region that coincided with their continental ancestry (N = 140) (<a href="#pgen-1006616-g006">Fig 6</a>). This was done to assign to each individual, a climate value that was most similar to their ‘ancestral’ climate. Since we did not have genotype data available for males of Northern European ancestry, we only used females for this analysis. The genotype data were necessary to correct for genetic structure (see <a href="#sec009">Methods</a>). We tested the correlation of nares width and alar base width with three climate variables: i) mean annual temperature (hereafter referred to as temperature), ii) relative humidity, and iii) absolute humidity. The choice of these climate variables follows from the functional importance of the nose in warming and humidifying inspired air. We also tested whether skin pigmentation is correlated with UVB levels. This was used as proof of principle, since multiple lines of evidence suggest that differences in skin pigmentation across populations have evolved primarily in response to ultraviolet radiation [<a href="#pgen.1006616.ref017">17</a>,<a href="#pgen.1006616.ref020">20</a>–<a href="#pgen.1006616.ref022">22</a>]. The association between phenotypes and climate variables was tested using linear mixed models, which correct for age, BMI, and genetic similarity (Methods). Sex was not included as a covariate as only females were used. We used a likelihood ratio test (LRT) to evaluate the statistical significance of the slope between phenotype and climate variable. The LRT statistic and its corresponding p-values were generated by comparing full (climate predictor included) and reduced (climate predictor removed) models. Results are presented in <a href="#pgen-1006616-t004">Table 4</a>.</p>
<div data-doi="10.1371/journal.pgen.1006616.g006"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosgenetics/article/figure/image?size=medium&amp;id=10.1371/journal.pgen.1006616.g006" data-doi="10.1371/journal.pgen.1006616" data-uri="10.1371/journal.pgen.1006616.g006"><img src="https://journals.plos.org/plosgenetics/article/figure/image?size=inline&amp;id=10.1371/journal.pgen.1006616.g006" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 6. </span> Geographic distribution of parents’ birthplaces.</p><p>Individual points represent the birth locations of the parents with a line connecting two parents. A single point indicates that the two parents were born in the same location. Climate values at these locations were used to test for signals of climate adaptation.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.g006">
              https://doi.org/10.1371/journal.pgen.1006616.g006</a></p></div><p>As expected, we see a strong relationship between skin pigmentation and UVB (N = 126, T = 6.23, LRT = 55.20, p-value = 1.09E-13). The positive values of the slope and t-statistic indicate that people tend to be darker in regions with higher exposure to UVB and vice versa. This pattern provides further support to the notion that adaptation to ultraviolet radiation has had an important role in the evolution of skin pigmentation in humans. Nares width is significantly correlated with temperature (N = 140, T = 3.09, LRT = 9.24, p-value = 2.37E-03) and absolute humidity (N = 140, T = 2.73, LRT = 7.28, p-value = 6.97E-03). The positive slopes indicate that individuals from warm-humid climates, on average, tend to have wider nares whereas individuals from cool-dry climates tend to have narrower nares. This is consistent with previously reported patterns of ecogeographic variation in the width of the nasal aperture and nasal cavity [<a href="#pgen.1006616.ref041">41</a>,<a href="#pgen.1006616.ref042">42</a>]. Nares width was not significantly correlated with relative humidity (N = 140, T = -1.65, LRT = 2.71, p-value = 0.100). This is not surprising since absolute humidity is more important for the physiological functioning of the nose than relative humidity [<a href="#pgen.1006616.ref043">43</a>]. Alar base width is neither significantly correlated with temperature (N = 140, T = 1.97, LRT = 3.82, p-value = 0.051) nor with relative humidity (N = 140, T = -0.79, LRT = 0.62, p-value = 0.431), and is only mildly correlated with absolute humidity (N = 140, T = 2.09, LRT = 4.30, p-value = 0.038). This might suggest that it is not the width of the nose <em>per se</em> but the width of the nares that is under selection pressure. However, even nares width is not as strongly correlated with temperature and humidity as skin pigmentation is to UVB, suggesting that the strength of selection exerted by temperature on nose width is likely to be much weaker than the selection pressure imposed by UVB exposure on skin pigmentation.</p>
<p>Following a suggestion by one of the reviewers, we investigated whether the observed phenotype-climate correlations were driven by any one population. To do so, we re-estimated the phenotype-climate slopes (skin pigmentation vs UVB and nares width vs temperature) after removing each population in turn. <a href="#pgen-1006616-g007">Fig 7</a> shows that the 95% confidence intervals of the slope overlap with zero (red line in <a href="#pgen-1006616-g007">Fig 7</a>) only when the N. Europeans are removed, suggesting that both phenotype-climate correlations are driven primarily by the inclusion of N. Europeans. This effect might be indicative of positive selection for lighter skin pigmentation and narrower nares in higher latitude populations. However, because our sampling of populations at higher latitude is limited (<a href="#pgen-1006616-g006">Fig 6</a>), this result requires further investigation.</p>
<div data-doi="10.1371/journal.pgen.1006616.g007"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosgenetics/article/figure/image?size=medium&amp;id=10.1371/journal.pgen.1006616.g007" data-doi="10.1371/journal.pgen.1006616" data-uri="10.1371/journal.pgen.1006616.g007"><img src="https://journals.plos.org/plosgenetics/article/figure/image?size=inline&amp;id=10.1371/journal.pgen.1006616.g007" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 7. </span> The effect of removing each population on phenotype-climate correlations.</p><p>The point estimate and the 95% confidence intervals of the phenotype-climate slopes after removing each population (on the y-axis) are shown for A) skin pigmentation and UVB, B) nares width and temperature, and C) nares width and absolute humidity. In all cases, the 95% confidence interval of the slope overlaps with zero (red vertical line), only when the Europeans are removed.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.g007">
              https://doi.org/10.1371/journal.pgen.1006616.g007</a></p></div></div>
</div>

<div xmlns:plos="http://plos.org" id="section3"><h2>Discussion</h2><p>The diversity of facial features across human populations has fascinated scientists for a long time. Even though genetic drift has played a predominant role in human evolution, external physical traits such as facial shape and skin pigmentation, because of their proximity to the environment, have also likely been influenced by natural selection. Substantial evidence has accumulated supporting the hypothesis that differences in skin pigmentation across human populations have evolved largely in response to selection pressures imposed by exposure to ultraviolet radiation [<a href="#pgen.1006616.ref020">20</a>]. How selection may have affected facial shape, a trait that is also quite variable between populations, is unclear, likely because it has received much less attention to date. Given the complexity of the face, we have chosen to study one particularly interesting and variable part of the face; the nose. The broad question driving this study is: Has climate adaptation played an important role in influencing variation in human nose shape? To answer this question, we formulated two hypotheses: i. divergent selection has been involved in the differentiation of certain aspects of nose shape across populations, and ii. climate is the agent of selection in cases where divergent selection can be invoked.</p>
<p>To test the first hypothesis, we used Qst–Fst comparisons to investigate whether the mean difference in nose shape among populations is greater than that expected under genetic drift alone. Estimation of Qst relies on stipulating the within- and among-population components of phenotypic variance that are due to additive genetic effects. These variance components are ideally calculated through experiments in which the effects of environmental variables can be controlled [<a href="#pgen.1006616.ref028">28</a>]. This poses a practical and ethical challenge in non-model organisms, such as humans, who are not amenable to the type of experimentation required. To circumvent these limitations, studies often calculate Qst directly from phenotype data under the assumption that the within-population heritability (<em>h</em><sup><em>2</em></sup>) is equal to the among-population heritability (<em>c</em>). Qst calculated this way is probably best considered to be the ‘minimum’ Qst proposed by Relethford (1994) [<a href="#pgen.1006616.ref035">35</a>]. However, the notion that this takes the minimum value of Qst relies on <em>h</em><sup>2</sup> always being less than <em>c</em>, which is anticonservative as has been noted previously [<a href="#pgen.1006616.ref032">32</a>]. In fact, genetic variation between-populations is likely to be less than the genetic variation within-populations for most phenotypes. Our approach is similar to previous studies in that we also estimated Qst assuming <em>c</em> = <em>h</em><sup><em>2</em></sup>. However, we used sensitivity curves to evaluate the behavior of Qst to cases where <em>c</em> &lt; <em>h</em><sup><em>2</em></sup>. In addition, we demonstrate that both within- and between-population variation in nose shape are heritable.</p>
<p>We carried out Qst–Fst comparisons for seven nose shape traits: nares width, alar base width, nasal height, length of the nasal ridge, nasal tip protrusion, external surface area, and nostril area. We found that the divergence in the width of the nares and alar base deviate from neutral expectations, and that these results are robust to the <em>c</em> = <em>h</em><sup><em>2</em></sup> assumption. This suggests that the width of the nares and alar base may have evolved across populations due to divergent selection.</p>
<p>Next, we hypothesized that the divergence of these two traits is driven by climate adaptation. To investigate this, we tested whether the spatial distribution of these traits is correlated with mean annual temperature and humidity. Our results show that nares width is strongly correlated with temperature and absolutely humidity. The positive direction of the effects indicate that wider noses are more common in warm-humid climates, while narrower noses are more common in cold-dry climates. Nares width is not, however, correlated with relative humidity. This is not surprising since absolute humidity levels are likely more important for respiration than relative humidity [<a href="#pgen.1006616.ref043">43</a>]. Alar base width is only weakly, if at all, correlated with temperature and absolute humidity, suggesting that the signal of climate adaptation might be specific to the width of the nares. Computational fluid dynamics (CFD) studies show that the geometry of the nose is important for its respiratory functions [<a href="#pgen.1006616.ref005">5</a>,<a href="#pgen.1006616.ref044">44</a>]. Inhaled air reaches 90% of the required temperature and humidity levels before even reaching the nasopharynx, implicating the nasal cavity, especially the turbinates, as the major conditioning apparatus in the respiratory tract [<a href="#pgen.1006616.ref004">4</a>,<a href="#pgen.1006616.ref005">5</a>]. We also know that the geometry of the nasal airways influences the velocity of inspired air [<a href="#pgen.1006616.ref004">4</a>,<a href="#pgen.1006616.ref007">7</a>,<a href="#pgen.1006616.ref045">45</a>]. Narrow airways in cold-dry climates might allow better conditioning by increasing the turbulence in inspired air as it reaches the turbinates, thereby facilitating contact with the nasal mucosa [<a href="#pgen.1006616.ref005">5</a>]. However, we note that nostril area does not show unusually high differentiation across populations, which suggests that it is not the size of the nostrils but the shape that might be functionally important.</p>
<p>Another hypothesis regarding the function of the nose that has received less attention compared to the air-conditioning hypothesis is that of selective brain cooling (SBC) [<a href="#pgen.1006616.ref046">46</a>]. Some large mammals are known to regulate brain temperatures in hyperthermic conditions, often caused by sustained exercise [<a href="#pgen.1006616.ref047">47</a>,<a href="#pgen.1006616.ref048">48</a>]. This ability is mostly reported in mammals with a carotid rete, which is a network of capillaries in a cavity under the brain called the cavernous sinus. Arterial blood leaving the rete eventually perfuses the brain through the Circle of Willis. Venous blood traveling back from the nasal mucosa interacts with the carotid rete in the cavernous sinus and is thought to cool the blood entering the brain in arid-zone mammals, such as Oryx [<a href="#pgen.1006616.ref049">49</a>], as well as in winter-acclimatized animals, such as reindeer [<a href="#pgen.1006616.ref050">50</a>]. Although, humans do not possess such a rete and instead have a single artery going through the cavernous sinus, which has led some to question the role of SBC in humans [<a href="#pgen.1006616.ref051">51</a>], proponents argue that the carotid rete is not a pre-requisite as SBC is present in some animals who don’t possess a rete, such as horses [<a href="#pgen.1006616.ref052">52</a>]. Since SBC in humans is highly debated, the role of nose shape differences in contributing to differences to this physiological process, while worth mentioning here, is highly speculative.</p>
<p>Climate may not have been the only factor in contributing to nose shape differences across populations. In fact, we show that temperature is only weakly correlated with nares width, especially when compared with the correlation between skin pigmentation and UVB. What then could be the selective agent driving the divergence of nose shape? We mentioned earlier that all aspects of nose shape studied here are sexually dimorphic, which raises a number of questions. Why does this sexual dimorphism exist? Is it merely a by-product of circulating hormones leading to differences in growth and development in early adulthood, or does it have an adaptive function, such as signaling sex to other males and females? Sexual selection has likely played an important role in human evolution, as evidenced by the presence of sexual dimorphism in many physical traits (e.g., height, waist-hip ratio, facial hair, and breasts to name a few). Could cultural differences in perceptions of dominance and attractiveness have had a role in the divergence of nose shape [<a href="#pgen.1006616.ref053">53</a>]? Could these perceptions have arisen to select mates who signal adaptation to the local environment? Indeed, ecological selection and sexual selection could reinforce each other, accelerating phenotypic divergence across populations in spite of continued gene flow [<a href="#pgen.1006616.ref054">54</a>]. These are interesting avenues of research, which need to be considered in order to sketch out a more complete picture of the evolution of the human nose.</p>
<p>The investigation of nose shape evolution with respect to climate adaptation, while interesting anthropologically, is also relevant medically. As humans are becoming more of a global community, the study of local adaptation is becoming more important to understanding health risks involved in living in ‘foreign’ climates. Obvious examples of such health risks are of increased risk of sunburn, skin cancer, and folate deficiency in light-skinned individuals exposed to high UVB, and of low birth weight and chronic mountain sickness associated with hypoxia at high altitudes [<a href="#pgen.1006616.ref020">20</a>,<a href="#pgen.1006616.ref055">55</a>]. Does the morphology of the external nose, or that of the inner nasal cavity affect risk of respiratory disease in different climates? It’s difficult to say at this point. While our findings provide support for the idea that differences in aspects of nose shape may have evolved across populations as a result of climate-related selection pressures, something that has been demonstrated previously using craniometric data [<a href="#pgen.1006616.ref002">2</a>,<a href="#pgen.1006616.ref012">12</a>,<a href="#pgen.1006616.ref013">13</a>,<a href="#pgen.1006616.ref041">41</a>,<a href="#pgen.1006616.ref042">42</a>], we note that the signal of climate adaptation is not very strong, especially when compared to skin pigmentation. This could be due to weaker selection pressure or selection on standing variation, but also due the sparse sampling of populations shown here, which is a limitation of this study. These results will need to be replicated in a larger set of populations. We expect that studies incorporating diverse populations who have been living long-term in a range of environments, such as the tropics, deserts, and circumpolar regions, will nicely fill in the gaps. Especially useful would be representation of populations from higher altitude regions, such as Andeans, Tibetans, and Ethiopians, who not only have to cope with the stress of a cold and dry climate, but also that of low atmospheric oxygen levels [<a href="#pgen.1006616.ref056">56</a>]. It would also be informative to study non-human primates in this context, who occupy a variety of climes and exhibit extensive variation in nose morphology. Finally, future studies should also focus on genome-wide association studies (GWASs) to identify variants contributing to nose shape. With increasing interest in identifying loci associated with facial shape, some GWASs have recently identified a number of nose shape loci [<a href="#pgen.1006616.ref057">57</a>–<a href="#pgen.1006616.ref060">60</a>]. Genetic variation at these loci will be informative about the nature of selection, as well as for inferring the timing of selection events.</p>
</div>

<div xmlns:plos="http://plos.org" id="section4"><h2>Materials and methods</h2>
<div id="section1"><h3>Ethics statement</h3>
<p>Data collection was carried out with informed consent of participants and with approval from the institutional review boards (IRBs) at The Pennsylvania State University (IRB# 32341, IRB# 45727, IRB# 44929) and at The University of Illinois Urbana-Champaign (IRB# 13103). All participants provided written informed consent.</p>
</div>

<div id="section2"><h3>Participant recruitment and description of the data</h3>
<p>The data used in this paper are part of a larger dataset that were collected in various locations through studies based at the Pennsylvania State University and at the University of Illinois at Urbana-Champaign. The majority of these data were collected in the United States at the Pennsylvania State University, at the World Science Festival in New York, and at the University of Illinois at Urbana-Champaign. Data collection in Europe took place in Dublin (Ireland), in Rome (Italy), in Warsaw (Poland), and in Porto (Portugal). The Cape Verdean data were collected in collaboration with Dr. Sandra Beleza as described in Beleza <em>et al</em>. [<a href="#pgen.1006616.ref061">61</a>]. All data were collected through informed consent from the participants with approval from the institutional review boards at The Pennsylvania State University, at the University of Illinois at Urbana-Champaign, and the National Ethical Committee for Health Research of Cape Verde.</p>
<p>The phenotype data collected include three dimensional (3D) images, skin pigmentation, standing height, and body weight. 3D images of participants were taken using the 3dMD system (<em>3dMD System</em>, <em>Atlanta</em>, <em>GA</em>). Skin pigmentation was measured with melanin index using Derma Spectrometer (<em>Cortex Technology</em>, <em>Hadsund</em>, <em>Denmark</em>). Melanin index is defined as the 100 x log<sub>10</sub> (1/percentage reflectance at 650 nM) [<a href="#pgen.1006616.ref017">17</a>]. We took three readings of melanin index from the inner left and right arm and used the mean of the six measurements for each individual. Statistical testing was performed using the inverse of melanin index since it is more normally distributed (<a href="#pgen.1006616.s007">S7 Fig</a>) but the use of untransformed melanin index produces similar results and does not affect our conclusions. Demographic information was collected through questionnaires and included self-reported ancestry and sex, participant’s birthplace, the birthplaces of the parents and grandparents, and the locations where the participant was raised.</p>
<p>The total number of participants was 4,257. Of these, we selected 2,637 individuals based on genetic ancestry and availability of phenotypic and covariate data, as described in the following sections. A summary of the sample sizes used in different analyses is provided in <a href="#pgen-1006616-t005">Table 5</a>. For a more detailed and accurate representation, refer to the Excel spreadsheet in the supplementary materials (<a href="#pgen.1006616.s009">S1 File</a>), which provides individual-level data for all samples used to generate the results presented here.</p>
</div>

<div id="section3"><h3>Estimation of genetic ancestry</h3>

<div id="section1"><h4>Genotyping and quality control.</h4><p>Out of the 4,257 participants, we had genotype data for 3,746 individuals. Since the data were collected as part of different studies over a long period of time, the samples were genotyped on four different genotyping arrays, namely the Illumina HumanHap300v1 BeadChip, the Illumina Infinium HD Human 1M-Duo Beadarray (<em>Illumina</em>, <em>Inc</em>., <em>San Diego</em>, <em>CA</em>), and the 23andMe v3 and v4 arrays (<em>23andMe Mountainview</em>, <em>CA)</em>. In order to select individuals based on genetic ancestry, we needed to merge genotypes across platforms into a single database. We took several steps to ensure there were no batch effects due to genotyping platform, which could potentially lead to biases in ancestry estimation and Fst calculation. First, we removed SNPs that were not present on all four platforms to ensure there were no systematic differences due to missingness. Second, we performed quality-control separately for each platform by removing individuals and SNPs with a genotyping rate of less than 90%. Finally, we removed palindromic (AT/GC) SNPs and recoded the data in the Illumina A/B format [<a href="#pgen.1006616.ref062">62</a>] to ensure strand differences across platforms were not an issue. This sufficiently removes any potential batch effects due to platform as evidenced by the observation that in a multidimensional scaling plot, individuals cluster by ancestry and not by platform (<a href="#pgen.1006616.s001">S1 Fig</a>). At this stage, the genotypes were merged across platforms into a single dataset. We further removed non-autosomal SNPs from the merged dataset and pruned for linkage disequilibrium (using a pairwise r<sup>2</sup> cutoff of 0.5). This resulted in 118,420 autosomal SNPs. Related individuals were identified using identity-by-state (IBS) analysis (IBS &gt; 0.8). Individuals were removed to minimize relatedness and maximize sample size (e.g. In the case of a mother-father-siblings quartet, the siblings were removed). These QC steps were carried out in Plink 1.9 [<a href="#pgen.1006616.ref063">63</a>,<a href="#pgen.1006616.ref064">64</a>].</p>
</div>

<div id="section2"><h4>Genomic ancestry estimation and principal components analysis.</h4><p>We estimated genetic ancestry for all individuals with available genotype data (N = 3,746) using an unsupervised clustering approach in ADMIXTURE. To assist with the clustering and interpretation of the resulting clusters, we merged this dataset with genotypes from the HapMap dataset for the same SNPs (Populations included: YRI, LWK, CEU, TSI, CHB, CHD, JPT, GIH, MEX; N = 988). We ran ADMIXTURE for three values of k (k = 5, 6, 7), and visually determined the optimum clustering scheme to be k = 6 for our purposes. To further visualize fine-scale structure, we carried out unsupervised principal component analysis (PCA) on the genotype data in Plink 1.9 [<a href="#pgen.1006616.ref064">64</a>]. A combination of filters based on the ADMIXTURE output and the PCA scores allowed us to select individuals from specific populations of interest. These are described below separately for each analysis.</p>
</div>
</div>

<div id="section4"><h3>Qst–Fst analyses</h3>

<div id="section1"><h4>Sub-selection of individuals.</h4><p>For the Qst–Fst analyses, we were interested in selecting individuals with ancestry primarily from one of four populations: i) North European, ii) West African, iii) East Asian, and iv) South Asian. Much of this was based on genetic ancestry calculated from dense genotype data. However, for some individuals (see <em>Northern European</em> below), self-reported ancestry information, corroborated through the birthplace of all four grandparents, was used where genotype data were unavailable.</p>
<p><em>West African (N = 40)</em>: We identified 40 individuals who had close to 90% or greater ancestry from ADMIXTURE cluster 1. ADMIXTURE cluster 1 represents ancestry from West Africa, given that individuals from the HapMap YRI (Yoruba in Ibadan, Nigeria) derive most of their ancestry from ADMIXTURE cluster 1 (<a href="#pgen-1006616-g008">Fig 8</a>). These 40 individuals were included in the ‘West African’ population group.</p>
<div data-doi="10.1371/journal.pgen.1006616.g008"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosgenetics/article/figure/image?size=medium&amp;id=10.1371/journal.pgen.1006616.g008" data-doi="10.1371/journal.pgen.1006616" data-uri="10.1371/journal.pgen.1006616.g008"><img src="https://journals.plos.org/plosgenetics/article/figure/image?size=inline&amp;id=10.1371/journal.pgen.1006616.g008" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 8. </span> ADMIXTURE Ancestry proportions (k = 6) of genotyped individuals used in Qst–Fst analyses.</p><p>Their ADMIXTURE proportions are compared with samples from the HapMap dataset. Each vertical bar in the panels is an individual and the colors represent the proportion of ancestry derived from each of 6 clusters (k = 6). In each panel, the HapMap samples are arranged on the left with a three-letter acronym for the population they are from (e.g. GIH refers to the Gujarati Indians from Houston) listed under them. The samples from our dataset are arranged to the right of the HapMap samples with their population designation (e.g. South Asian) under them.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.g008">
              https://doi.org/10.1371/journal.pgen.1006616.g008</a></p></div><p><em>East Asian (N = 127)</em>: Comparison with the HapMap CHB (Han Chinese in Beijing, China), HapMap CHD (Chinese in Metropolitan Denver, Colorado), and HapMap JPT (Japanese in Tokyo, Japan) samples shows that cluster 4 represents genetic ancestry from East Asia (<a href="#pgen-1006616-g008">Fig 8</a>). We selected 166 individuals who had 90% or more ancestry from ADMIXTURE cluster 4 to be included in the ‘East Asian’ population group (<a href="#pgen-1006616-g008">Fig 8</a>). We further examined the distribution of the PCA plots and removed six individuals who clustered far from the CHB, CHD, and JPT samples. The clustering of the remaining 160 individuals around the CHB, CHD, and JPT samples is shown in <a href="#pgen.1006616.s002">S2 Fig</a>. Of these, we had high quality 3D images and complete covariate data for 127 individuals, who were then used in our analyses.</p>
<p><em>South Asian (N = 73)</em>: 98 individuals in our dataset had more than 60% ancestry from ADMIXTURE cluster 2, which seems to represent the South Asian cluster (<a href="#pgen-1006616-g008">Fig 8</a>). These 98 individuals were added to the ‘South Asian’ population group. While the cutoff here seems liberal compared to that used for the other populations, it is only meant to be a rough guide and was chosen by comparing with the ancestry proportions of the HapMap GIH (Gujarati Indians in Houston, Texas) samples (<a href="#pgen-1006616-g008">Fig 8</a>). However, we examined the distribution of the 98 individuals around the GIH (Gujarati Indians in Houston, Texas) samples using PCA plots (<a href="#pgen.1006616.s002">S2 Fig</a>), and noticed that six individuals appear to fall far from the main cluster, who were subsequently removed. Of the remaining, we had high quality 3D images and complete covariate data for 73 individuals, who were used for further analyses.</p>
<p><em>Northern European (N = 236)</em>: Our full dataset had a large number of individuals with European ancestry from different regions of Europe. In order to minimize structure within Europe, we only retained individuals with Northern European ancestry. Thus, we restricted the analysis to Europeans collected in Ireland (N = 151) and Poland (N = 85) who reported that the ancestry of all four of their grandparents was from the region of sampling. More details for these samples are described in Candille <em>et al</em>. [<a href="#pgen.1006616.ref065">65</a>]. Of this set, we had genotypes for 73 females (Irish = 37, Polish = 36) but none for males. This is the reason why only females were included in the phenotype-climate analyses, where genotypes were used to construct the genetic relationship matrix. The ADMIXTURE results for these women are shown in <a href="#pgen-1006616-g008">Fig 8</a> alongside the results of the CEU (Utah residents with Northern and Western European ancestry from the CEPH collection) and TSI (Tuscans from Italy) samples.</p>
</div>

<div id="section2"><h4>Qst calculation.</h4><p>Estimation of the variance components, <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e015" loading="lazy"></span> and <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e016" loading="lazy"></span>, was carried out using a linear model in which population group was treated as a fixed effect, with sex, age, and BMI as covariates. If <em>MSB</em> is the mean square among populations, <em>MSW</em> is the mean square within populations, <em>n</em><sub><em>i</em></sub> is the number of individuals in the i<sup>th</sup> populations, and <em>a</em> is the number of populations, <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e017" loading="lazy"></span> and <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e018" loading="lazy"></span> can be estimated as described as follows [<a href="#pgen.1006616.ref010">10</a>,<a href="#pgen.1006616.ref066">66</a>,<a href="#pgen.1006616.ref067">67</a>]:
<a name="pgen.1006616.e019" id="pgen.1006616.e019"></a><span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e019" loading="lazy"><span>(3)</span></span>
<a name="pgen.1006616.e020" id="pgen.1006616.e020"></a><span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e020" loading="lazy"><span>(4)</span></span>
<a name="pgen.1006616.e021" id="pgen.1006616.e021"></a><span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e021" loading="lazy"><span>(5)</span></span></p>
</div>

<div id="section3"><h4>Fst calculation.</h4><p>Wright’s Fst measures the genetic divergence between two or more populations. We estimated the Fst across the four populations for the LD-pruned set of 118,420 autosomal SNPs, described above, using Weir and Cockerham’s <em>θ</em> [<a href="#pgen.1006616.ref068">68</a>]. The genomic Fst distribution in shown in <a href="#pgen.1006616.s003">S3 Fig</a>.</p>
</div>

<div id="section4"><h4>Qst–Fst comparison.</h4><p>We used a non-parametric bootstrap approach to test whether the Qst of a phenotype is significantly greater than the Fst. This was done by computing Qst from subsamples generated by randomly selecting individuals with replacement, within population and sex such that the number of males and females in each population remains unchanged. Values of Fst were simulated by randomly sampling from the Fst of 118,420 SNPs (<a href="#pgen.1006616.s003">S3 Fig</a>). We generated 10,000 such pseudo-samples and, for each simulated value of Qst and Fst, computed the difference Qst–Fst. High values of Qst–Fst are indicative of divergent selection, and the empirical p-value for this test was determined by calculating the proportion of Qst–Fst values that are less than zero.</p>
</div>
</div>

<div id="section5"><h3>Estimation of <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e022" loading="lazy"></span> and <em>c</em></h3>

<div id="section1"><h4>Estimation of <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e023" loading="lazy"></span>.</h4><p>For the calculation of <span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e024" loading="lazy"></span>, we identified 1,825 unrelated individuals with primarily European ancestry, who derived more than 80% of their combined genetic ancestry from Cluster 3 and Cluster 6 based on the ADMIXTURE output (<a href="#pgen-1006616-g009">Fig 9</a>). This cutoff was chosen based on comparison with the genetic ancestries of CEU (Utah residents with Northern and Western European ancestry from the CEPH collection) and TSI (Toscans in Italy) individuals from the HapMap dataset (<a href="#pgen-1006616-g009">Fig 9</a>). We used GCTA to calculate the genome-wide kinship matrix for the 1,825 individuals from 118,420 autosomal SNPs, which overlapped across all four platforms. Of the 1,825 individuals with European ancestry, we had measured height for 1,825, measured melanin index for 1,231, and 3D photos for 1,718 individuals. From these data, we used GCTA to estimate the proportion of phenotypic variance explained by genotyped SNPs (<em>h</em><sub><em>g</em></sub><sup><em>2</em></sup>) for height, skin pigmentation, and nose shape aspects. The top four genetic PCs were included as covariates in the model to correct for population structure, which can inflate heritability estimates [<a href="#pgen.1006616.ref018">18</a>,<a href="#pgen.1006616.ref069">69</a>,<a href="#pgen.1006616.ref070">70</a>]. Also included in the model were sex, age, and BMI.</p>
<div data-doi="10.1371/journal.pgen.1006616.g009"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosgenetics/article/figure/image?size=medium&amp;id=10.1371/journal.pgen.1006616.g009" data-doi="10.1371/journal.pgen.1006616" data-uri="10.1371/journal.pgen.1006616.g009"><img src="https://journals.plos.org/plosgenetics/article/figure/image?size=inline&amp;id=10.1371/journal.pgen.1006616.g009" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 9. </span> ADMIXTURE ancestry proportions (k = 6) of 1,825 individuals with primarily European ancestry, who were used to estimate <em>h<sub>g</sub><sup>2</sup></em>.</p><p>Each bar represents an individual and the different colors represent the ADMIXTURE proportions from six clusters (k = 6). The ADMIXTURE proportions for HapMap European populations (CEU and TSI) are shown on the left for comparison.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.g009">
              https://doi.org/10.1371/journal.pgen.1006616.g009</a></p></div></div>

<div id="section2"><h4>Estimation of c.</h4><p>We estimated the proportion of phenotypic variance between populations due to additive genetic effects (<em>c</em>) in an admixed population from Cape Verde, who derive their ancestry primarily from W. Africans and Europeans (<a href="#pgen-1006616-g010">Fig 10</a>), using the approach described in Zaitlen <em>et al</em>. [<a href="#pgen.1006616.ref040">40</a>]. 697 Cape Verdeans were genotyped on the Illumina Infinium HD Human 1M-Duo Beadarray (<em>Illumina</em>, <em>Inc</em>., <em>San Diego</em>, <em>CA</em>) for a total of 1,016,423 SNPs. This was reduced to 623,625 autosomal SNPs after removing palindromic SNPs, SNPs that did not overlap with the HapMap dataset, and other SNPs based on filters described in Beleza <em>et al</em>. [<a href="#pgen.1006616.ref017">17</a>]. We had high quality 3D images and complete covariate data for 409 of the 697 Cape Verdeans who were genotyped. For these individuals, we calculated local ancestry at 623,625 autosomal SNPs with LAMP-LD [<a href="#pgen.1006616.ref071">71</a>] using phased genotypes from the HapMap CEU and YRI samples as reference. Averaging local ancestry across all SNPs yields an estimate of global ancestry that is highly correlated with estimates obtained from ADMIXTURE using k = 2 (r<sup>2</sup> ∼ 1) (<a href="#pgen.1006616.s004">S4 Fig</a>). We used GCTA to calculate a kinship matrix based on local ancestry and to estimate the proportion of phenotypic variance explained by local ancestry (<em>h</em><sub><em>y</em></sub><sup><em>2</em></sup>). Global ancestry, calculated from the mean of local ancestry across all SNPs, was included as a covariate to correct for ancestry stratification and any environmental variables that might covary with ancestry [<a href="#pgen.1006616.ref016">16</a>,<a href="#pgen.1006616.ref072">72</a>]. Sex, age, and BMI were also included as covariates.</p>
<div data-doi="10.1371/journal.pgen.1006616.g010"><div><p><a title="Click for larger image" href="https://journals.plos.org/plosgenetics/article/figure/image?size=medium&amp;id=10.1371/journal.pgen.1006616.g010" data-doi="10.1371/journal.pgen.1006616" data-uri="10.1371/journal.pgen.1006616.g010"><img src="https://journals.plos.org/plosgenetics/article/figure/image?size=inline&amp;id=10.1371/journal.pgen.1006616.g010" alt="thumbnail" loading="lazy"></a></p></div><p><span>Fig 10. </span> ADMIXTURE ancestry proportions (k = 6) of 409 Cape Verdeans, who were used to estimate <em>c</em>.</p><p>Each bar represents an individual and the different colors represent the ADMIXTURE proportions from six clusters (k = 6).</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.g010">
              https://doi.org/10.1371/journal.pgen.1006616.g010</a></p></div></div>
</div>

<div id="section6"><h3>Phenotype-climate associations</h3>

<div id="section1"><h4>Testing for local adaptation.</h4><p>The primary focus here was to answer the question: Does nose shape variation between geographically distant populations show signals of local adaptation? To answer this, we needed to ascribe a climate value to every individual that can serve as a proxy of their ancestral environment. For this reason, we selected individuals used in the Qst–Fst analyses who reported at least one parent’s birthplace. These were further filtered to retain individuals whose parents’ birthplaces coincided with their continental ancestry, for example, East Asian individuals whose parents were born in the United States were removed. Finally, we restricted the analyses to females since genetic data were not available for males sampled in Ireland and Poland. This resulted in 140 females with genetic, phenotypic, and geographic data. We have plotted the geographic distribution of the parents of these individuals in <a href="#pgen-1006616-g006">Fig 6</a> with a line connecting the two parents of each individual. A single dot means that the parents were born in the same location. These locations were geocoded to obtain the latitude and longitude in decimal degrees using the ggmap package in R [<a href="#pgen.1006616.ref073">73</a>,<a href="#pgen.1006616.ref074">74</a>].</p>
<p>Gridded climate data were obtained in latitude/longitude coordinate system. Climate grids with mean monthly values for temperature (measured in degrees Celsius), and vapor pressure (measured in hectopascal or hPa) at 0.5° resolution for the period between 1901 and 2015 were obtained from The Climate Research Unit TS v3.24 Dataset (<a href="https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_3.24/">https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_3.24/</a>) [<a href="#pgen.1006616.ref075">75</a>]. Ground-level Ultraviolet-B (UVB) irradiance data (expressed in J/m<sup>2</sup>), for the period between 1996 and 2005, generated by the Earth Probe Total Ozone Mapping Spectrometer (EP/TOMS) Team at 0.5° resolution, were downloaded from the Goddard Earth Sciences Data and Information Services Center (GES DISC) website (<a href="https://disc.gsfc.nasa.gov/">https://disc.gsfc.nasa.gov/</a>) [<a href="#pgen.1006616.ref076">76</a>]. Relative humidity and absolute humidity grids were derived from temperature and vapor pressure as described in Maddux <em>et al</em>. (2016) [<a href="#pgen.1006616.ref043">43</a>]. For each of the four climate variables (UVB, temperature, absolute humidity, and relative humidity), a single grid was constructed by averaging across all months within the time period to represent long-term climate. R packages RnetCDF [<a href="#pgen.1006616.ref077">77</a>] and Raster [<a href="#pgen.1006616.ref078">78</a>] were used to read gridded data and to extract climate values at each location. We used climate values at the parents’ birthplace to represent the ancestral climate for each individual. Where the birthplace of both parents was known, the mean climate values extracted at the two locations was used. Both parents usually come from nearby regions, at least on a continental scale (<a href="#pgen-1006616-g006">Fig 6</a>), and the climate values for the parents are quite similar (<a href="#pgen.1006616.s005">S5 Fig</a>).</p>
<p>We used a linear mixed model to test the relationship between phenotype and climate variables, while correcting for age and BMI. We took non-independence among observations due to genetic similarity into account by allowing the observations to be correlated according to the SNP-based genome-wide kinship matrix. In matrix notation, the linear model takes the following form:
<a name="pgen.1006616.e025" id="pgen.1006616.e025"></a><span><img src="https://journals.plos.org/plosgenetics/article/file?type=thumbnail&amp;id=10.1371/journal.pgen.1006616.e025" loading="lazy"><span>(6)</span></span>
Where <em>β</em> are fixed effects and <em>ε</em> is the random error term. Thus, <em>ε</em> ∼ <em>N</em>(0,<em>Gσ</em><sub><em>e</em></sub>), where <em>G</em> is the genome-wide kinship matrix, which was constructed from the genotypes at 118,420 SNPs in GCTA [<a href="#pgen.1006616.ref037">37</a>]. The slopes were estimated using maximum likelihood, implemented in the spaMM package in R [<a href="#pgen.1006616.ref074">74</a>,<a href="#pgen.1006616.ref079">79</a>]. P-values for each slope were generated from the likelihood ratio between full and reduced models (fit by removing each climate predictor).</p>
</div>
</div>

<div id="section7"><h3>Morphometric measurements</h3>

<div id="section1"><h4>Imaging processing.</h4><p>We captured high resolution 3D images of participants’ faces using the 3dMD Face system (<em>3dMD Atlanta</em>, <em>GA</em>). Five positioning landmarks (two on the inner corner of the eyes, two on the outer corners of the mouth, and one on the tip of the nose) were placed on each face in order to establish facial orientation. A spatially dense mesh of 7,150 quasi-landmarks (QLs) was mapped onto each image and its reflection. As a result of this mapping, the vertices and triangles are homologous across individuals [<a href="#pgen.1006616.ref080">80</a>]. Generalized Procrustes Superimposition [<a href="#pgen.1006616.ref024">24</a>] was carried out on both sets of images (original and reflected) to remove differences in position and orientation. The Procrustes coordinates of the original and reflected image for each participant were then averaged to remove effects of bilateral asymmetry. We selected the nose region (see outline in <a href="#pgen-1006616-g001">Fig 1</a>), which is comprised of 709 out of the 7,150 QLs making up the entire face, for further downstream analyses.</p>
</div>

<div id="section2"><h4>Linear distances.</h4><p>Linear distances were calculated using three mid-line landmarks; i. nasion (n), ii. pronasale (prn), iii. subnasale (sn), and two sets of paired landmarks; iv. left and right alar curvature (al<sub>l</sub> and al<sub>r</sub>), and v. left and right alar base (ac<sub>l</sub> and ac<sub>r</sub>). These landmarks have been defined previously [<a href="#pgen.1006616.ref001">1</a>,<a href="#pgen.1006616.ref026">26</a>] and their approximate positions are shown in <a href="#pgen-1006616-g001">Fig 1</a>. Two different observers placed each of these seven landmarks three different times on the spatially-dense QL template mesh that was mapped on all the faces. Because the QLs are homologous across individuals, we can automatically select corresponding landmarks for all individuals by indicating these on the QL template. This is possible because any point on the template surface can be expressed as a function of QLs using barycentric coordinates on triangles. Using this approach, we obtained the x, y, and z coordinates of the above-mentioned landmarks for each individual. The Euclidean distances (in mm) were then calculated separately for each of the six replicate placements, which were averaged across replicates and then averaged between the two observers, to obtain mean distances for each individual.</p>
</div>

<div id="section3"><h4>Nostril area.</h4><p>To measure area of the nostrils, the observers defined regions around the nostrils (approximate location shown in <a href="#pgen-1006616-g001">Fig 1</a>) three different times. The area of a region was calculated as the total area of the polygons within the region. Using this approach, we calculated the mean area of the right and left nostrils (in mm<sup>2</sup>) for each replicate. This was then averaged across replicates for the same observer, then across observers to obtain the mean nostril area for each individual.</p>
</div>

<div id="section4"><h4>External surface area.</h4><p>The external surface area of the nose (also in mm<sup>2</sup>) was calculated by summing the areas of polygons bounded by the 709 QLs, defining the entire nose region (outlined in <a href="#pgen-1006616-g001">Fig 1</a>).</p>
</div>

<div id="section5"><h4>Intra-observer error and inter-observer reliability.</h4><p>We calculated the standard deviation in the measurement of linear distances (in mm) and nostril area (in mm<sup>2</sup>) made by the two observers to estimate the degree of intra-observer error. The quantiles for these standard deviations are shown in <a href="#pgen-1006616-t006">Table 6</a>. The measurements from Observer2 are slightly more internally consistent than Observer1. For Observer1, nasal tip protrusion was the least precise distance measure with 95% of the standard deviations falling between 0.791mm and 1.165mm, with error for the remaining distances well under 1mm. The error distribution for nostril area was similar for both Observer1 (95% CI: 0.914–1.930) and Observer2 (95% CI: 0.888–2.243).</p>
<p>Inter-observer reliability was evaluated using intraclass correlation coefficient (ICC). ICC for each measure was calculated using linear mixed models where the observer was treated as fixed effect and subject was treated as random effect. This was carried out using the nlme package in R [<a href="#pgen.1006616.ref074">74</a>,<a href="#pgen.1006616.ref081">81</a>]. With the exception of nasal tip protrusion (ICC = 0.819), the ICCs for all other measures were quite high (ICC &gt; 0.95). This concordance is evident by comparing the mean measurements between Observer1 and Observer2 (<a href="#pgen.1006616.s006">S6 Fig</a>)</p>
</div>
</div>
</div>

<div xmlns:plos="http://plos.org" id="section5"><h2>Supporting information</h2><div><h3><a href="https://journals.plos.org/plosgenetics/article/file?type=supplementary&amp;id=10.1371/journal.pgen.1006616.s001">S1 Fig. </a></h3><p><strong>MDS plots (k = 2) showing clustering of genotyped individuals by A) genotyping platform and B) by ancestry.</strong> In B) the genotyped individuals from the study set are shown in black, while the HapMap samples are shown in color for reference. The plot shows that individuals cluster by ancestry and not by genotyping platform. Thus, batch effects due to genotyping platform are unlikely to lead to artifacts in ancestry inference.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.s001">https://doi.org/10.1371/journal.pgen.1006616.s001</a></p><p>(TIF)</p>
</div><div><h3><a href="https://journals.plos.org/plosgenetics/article/file?type=supplementary&amp;id=10.1371/journal.pgen.1006616.s002">S2 Fig. </a></h3><p><strong>PCA plots showing clustering of genotyped individuals by A) South Asian and B) East Asian ancestry around HapMap reference samples.</strong> The colored points are reference samples from the HapMap dataset and the grey points are individuals from our dataset. A) South Asian individuals are shown with the HapMap GIH samples and B) East Asian individuals are shown with the combined East Asian sample (CHB+JPT+CHD) from the HapMap dataset. The red lines indicate the cutoff for removing individuals who appear to cluster far away from the main cluster. The individuals who were removed, based on clustering, are shown as triangles, whereas all other individuals are shown as circles.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.s002">https://doi.org/10.1371/journal.pgen.1006616.s002</a></p><p>(TIF)</p>
</div><div><h3><a href="https://journals.plos.org/plosgenetics/article/file?type=supplementary&amp;id=10.1371/journal.pgen.1006616.s004">S4 Fig. </a>Global ancestry calculated using LAMP-LD is highly correlated with global ancestry calculated using ADMIXTURE.</h3><p>ADMIXTURE-based ancestry estimates were calculated using a k = 2 assumption. LAMP-LD-based ancestry estimates were calculating the mean local ancestry across 623,625 autosomal SNPs.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.s004">https://doi.org/10.1371/journal.pgen.1006616.s004</a></p><p>(TIF)</p>
</div><div><h3><a href="https://journals.plos.org/plosgenetics/article/file?type=supplementary&amp;id=10.1371/journal.pgen.1006616.s007">S7 Fig. </a></h3><p><strong>QQplots showing that the residuals for A) the inverse of melanin index are more normally distributed than B) untransformed melanin index.</strong> The points are residuals from linear models where population, sex, age, and BMI were used as predictors.</p>
<p><a href="https://doi.org/10.1371/journal.pgen.1006616.s007">https://doi.org/10.1371/journal.pgen.1006616.s007</a></p><p>(TIF)</p>
</div></div>





<div xmlns:plos="http://plos.org"><h2>Acknowledgments</h2>
<p>The authors would first and foremost like to thank the study participants, without whom none of this would have been possible. We are grateful to the Shriver Lab team, Ripan Malhi, and the members of his lab for their efforts in collection and organization of the data. We are also grateful to Sandra Beleza and Isabel Inês Araújo for their efforts in collecting and sharing the Cape Verdean data. We acknowledge Ine Saye from Peter Claes’ group for helping with morphometric analyses. We also thank Nina Jablonski, George Chaplin, George Perry, Laurel Pearson, Julie White, and Matthew Reimherr for helpful discussions on the analytical aspects of the paper. Finally, we would like to thank two anonymous reviewers for providing helpful critique and comments on the first submission of this manuscript.</p>
</div><div xmlns:plos="http://plos.org"><h2>Author Contributions</h2><ol>

<li><strong>Conceptualization:</strong> AAZ MDS.</li>

<li><strong>Data curation:</strong> AAZ BCM MDS BM.</li>

<li><strong>Formal analysis:</strong> AAZ.</li>

<li><strong>Funding acquisition:</strong> MDS PC CH.</li>

<li><strong>Investigation:</strong> AAZ BCM CH MDS.</li>

<li><strong>Methodology:</strong> AAZ MDS.</li>

<li><strong>Project administration:</strong> MDS CH BCM BM.</li>

<li><strong>Resources:</strong> MDS CH PC BM.</li>

<li><strong>Software:</strong> PC AAZ.</li>

<li><strong>Supervision:</strong> MDS.</li>

<li><strong>Visualization:</strong> AAZ PC.</li>

<li><strong>Writing – original draft:</strong> AAZ.</li>

<li><strong>Writing – review &amp; editing:</strong> AAZ MDS PC CH BCM BM.</li>

</ol>
</div><div xmlns:plos="http://plos.org"><h2>References</h2><ol><li id="ref1"><span>1.
            </span><a name="pgen.1006616.ref001" id="pgen.1006616.ref001"></a>Farkas LG, Kolar JC, Munro IR. Geography of the nose: a morphometric study. Aesthetic Plast Surg. 1986;10: 191–223. pmid:3812136 <ul><li><a href="#" data-author="Farkas" data-cit="FarkasLG%2C%20KolarJC%2C%20MunroIR.%20Geography%20of%20the%20nose%3A%20a%20morphometric%20study.%20Aesthetic%20Plast%20Surg.%201986%3B10%3A%20191%E2%80%93223.%203812136" data-title="Geography%20of%20the%20nose%3A%20a%20morphometric%20study" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/3812136" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Geography+of+the+nose%3A+a+morphometric+study+Farkas+1986" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref2"><span>2.
            </span><a name="pgen.1006616.ref002" id="pgen.1006616.ref002"></a>Franciscus RG, Long JC. Variation in human nasal height and breadth. Am J Phys Anthropol. 1991;85: 419–427.  pmid:1928315 <ul data-doi="10.1002/ajpa.1330850406"><li><a href="https://doi.org/10.1002/ajpa.1330850406" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/1928315" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Variation+in+human+nasal+height+and+breadth+Franciscus+1991" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref3"><span>3.
            </span><a name="pgen.1006616.ref003" id="pgen.1006616.ref003"></a>Negus VE. Introduction to the comparative anatomy of the nose and paranasal sinuses. Ann R Coll Surg Engl. 1954;15: 141–171. pmid:13198060 <ul><li><a href="#" data-author="Negus" data-cit="NegusVE.%20Introduction%20to%20the%20comparative%20anatomy%20of%20the%20nose%20and%20paranasal%20sinuses.%20Ann%20R%20Coll%20Surg%20Engl.%201954%3B15%3A%20141%E2%80%93171.%2013198060" data-title="Introduction%20to%20the%20comparative%20anatomy%20of%20the%20nose%20and%20paranasal%20sinuses" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/13198060" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Introduction+to+the+comparative+anatomy+of+the+nose+and+paranasal+sinuses+Negus+1954" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref4"><span>4.
            </span><a name="pgen.1006616.ref004" id="pgen.1006616.ref004"></a>Hanna LM, Scherer PW. A Theoretical Model of Localized Heat and Water Vapor Transport in the Human Respiratory Tract. J Biomech Eng. American Society of Mechanical Engineers; 1986;108: 19. pmid:3959548 <ul><li><a href="#" data-author="Hanna" data-cit="HannaLM%2C%20SchererPW.%20A%20Theoretical%20Model%20of%20Localized%20Heat%20and%20Water%20Vapor%20Transport%20in%20the%20Human%20Respiratory%20Tract.%20J%20Biomech%20Eng.%20American%20Society%20of%20Mechanical%20Engineers%3B%201986%3B108%3A%2019.%203959548" data-title="A%20Theoretical%20Model%20of%20Localized%20Heat%20and%20Water%20Vapor%20Transport%20in%20the%20Human%20Respiratory%20Tract" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/3959548" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=A+Theoretical+Model+of+Localized+Heat+and+Water+Vapor+Transport+in+the+Human+Respiratory+Tract+Hanna+1986" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref5"><span>5.
            </span><a name="pgen.1006616.ref005" id="pgen.1006616.ref005"></a>Naftali S, Rosenfeld M, Wolf M, Elad D. The Air-Conditioning Capacity of the Human Nose. Ann Biomed Eng. 2005;33: 545–553. pmid:15909660 <ul><li><a href="#" data-author="Naftali" data-cit="NaftaliS%2C%20RosenfeldM%2C%20WolfM%2C%20EladD.%20The%20Air-Conditioning%20Capacity%20of%20the%20Human%20Nose.%20Ann%20Biomed%20Eng.%202005%3B33%3A%20545%E2%80%93553.%2015909660" data-title="The%20Air-Conditioning%20Capacity%20of%20the%20Human%20Nose" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/15909660" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=The+Air-Conditioning+Capacity+of+the+Human+Nose+Naftali+2005" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref6"><span>6.
            </span><a name="pgen.1006616.ref006" id="pgen.1006616.ref006"></a>Randell SH, Boucher RC. Effective mucus clearance is essential for respiratory health. Am J Respir Cell Mol Biol. 2006;35: 20–8.  pmid:16528010 <ul data-doi="10.1165/rcmb.2006-0082SF"><li><a href="https://doi.org/10.1165/rcmb.2006-0082SF" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/16528010" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Effective+mucus+clearance+is+essential+for+respiratory+health+Randell+2006" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref7"><span>7.
            </span><a name="pgen.1006616.ref007" id="pgen.1006616.ref007"></a>Churchill SE, Shackelford LL, Georgi JN, Black MT. Morphological variation and airflow dynamics in the human nose. Am J Hum Biol. 2004;16: 625–638.  pmid:15495233 <ul data-doi="10.1002/ajhb.20074"><li><a href="https://doi.org/10.1002/ajhb.20074" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/15495233" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Morphological+variation+and+airflow+dynamics+in+the+human+nose+Churchill+2004" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref8"><span>8.
            </span><a name="pgen.1006616.ref008" id="pgen.1006616.ref008"></a>Weiner JS. Nose Shape and Climate. Am J Phys Anthropol. 1954;12: 615–618. pmid:14350081 <ul><li><a href="#" data-author="Weiner" data-cit="WeinerJS.%20Nose%20Shape%20and%20Climate.%20Am%20J%20Phys%20Anthropol.%201954%3B12%3A%20615%E2%80%93618.%2014350081" data-title="Nose%20Shape%20and%20Climate" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/14350081" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Nose+Shape+and+Climate+Weiner+1954" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref9"><span>9.
            </span><a name="pgen.1006616.ref009" id="pgen.1006616.ref009"></a>Roseman CC, Weaver TD. Multivariate apportionment of global human craniometric diversity. Am J Phys Anthropol. 2004;125: 257–263.  pmid:15386236 <ul data-doi="10.1002/ajpa.10424"><li><a href="https://doi.org/10.1002/ajpa.10424" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/15386236" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Multivariate+apportionment+of+global+human+craniometric+diversity+Roseman+2004" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref10"><span>10.
            </span><a name="pgen.1006616.ref010" id="pgen.1006616.ref010"></a>Spitze K. Population structure in Daphnia obtusa: quantitative genetic and allozymic variation. Genetics. 1993;135: 367–374. Available: <a href="http://www.genetics.org/content/135/2/367">http://www.genetics.org/content/135/2/367</a> pmid:8244001 <ul><li><a href="#" data-author="Spitze" data-cit="SpitzeK.%20Population%20structure%20in%20Daphnia%20obtusa%3A%20quantitative%20genetic%20and%20allozymic%20variation.%20Genetics.%201993%3B135%3A%20367%E2%80%93374.%20Available%3A%20http%3A%2F%2Fwww.genetics.org%2Fcontent%2F135%2F2%2F367%208244001" data-title="Population%20structure%20in%20Daphnia%20obtusa%3A%20quantitative%20genetic%20and%20allozymic%20variation" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/8244001" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Population+structure+in+Daphnia+obtusa%3A+quantitative+genetic+and+allozymic+variation+Spitze+1993" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref11"><span>11.
            </span><a name="pgen.1006616.ref011" id="pgen.1006616.ref011"></a>Whitlock MC, Guillaume F. Testing for spatially divergent selection: comparing QST to FST. Genetics. 2009;183: 1055–63.  pmid:19687138 <ul data-doi="10.1534/genetics.108.099812"><li><a href="https://doi.org/10.1534/genetics.108.099812" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/19687138" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Testing+for+spatially+divergent+selection%3A+comparing+QST+to+FST+Whitlock+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref12"><span>12.
            </span><a name="pgen.1006616.ref012" id="pgen.1006616.ref012"></a>Roseman CC. Detecting interregionally diversifying natural selection on modern human cranial form by using matched molecular and morphometric data. Proc Natl Acad Sci U S A. 2004;101: 12824–9.  pmid:15326305 <ul data-doi="10.1073/pnas.0402637101"><li><a href="https://doi.org/10.1073/pnas.0402637101" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/15326305" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Detecting+interregionally+diversifying+natural+selection+on+modern+human+cranial+form+by+using+matched+molecular+and+morphometric+data+Roseman+2004" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref13"><span>13.
            </span><a name="pgen.1006616.ref013" id="pgen.1006616.ref013"></a>Hubbe M, Hanihara T, Harvati K. Climate signatures in the morphological differentiation of worldwide modern human populations. Anat Rec. 2009;292: 1720–1733. <ul><li><a href="#" data-author="Hubbe" data-cit="HubbeM%2C%20HaniharaT%2C%20HarvatiK.%20Climate%20signatures%20in%20the%20morphological%20differentiation%20of%20worldwide%20modern%20human%20populations.%20Anat%20Rec.%202009%3B292%3A%201720%E2%80%931733." data-title="Climate%20signatures%20in%20the%20morphological%20differentiation%20of%20worldwide%20modern%20human%20populations" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Climate+signatures+in+the+morphological+differentiation+of+worldwide+modern+human+populations+Hubbe+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref14"><span>14.
            </span><a name="pgen.1006616.ref014" id="pgen.1006616.ref014"></a>Guo J, Tan J, Yang Y, Zhou H, Hu S, Hashan A, et al. Variation and signatures of selection on the human face. J Hum Evol. Elsevier Ltd; 2014;75: 143–152.  pmid:25186351 <ul data-doi="10.1016/j.jhevol.2014.08.001"><li><a href="https://doi.org/10.1016/j.jhevol.2014.08.001" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/25186351" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Variation+and+signatures+of+selection+on+the+human+face+Guo+2014" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref15"><span>15.
            </span><a name="pgen.1006616.ref015" id="pgen.1006616.ref015"></a>Shriver MD, Parra EJ, Dios S, Bonilla C, Norton H, Jovel C, et al. Skin pigmentation, biogeographical ancestry and admixture mapping. Hum Genet. 2003;112: 387–99.  pmid:12579416 <ul data-doi="10.1007/s00439-002-0896-y"><li><a href="https://doi.org/10.1007/s00439-002-0896-y" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/12579416" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Skin+pigmentation%2C+biogeographical+ancestry+and+admixture+mapping+Shriver+2003" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref16"><span>16.
            </span><a name="pgen.1006616.ref016" id="pgen.1006616.ref016"></a>Parra EJ, Kittles R a, Shriver MD. Implications of correlations between skin color and genetic ancestry for biomedical research. Nat Genet. 2004;36: S54–S60.  pmid:15508005 <ul data-doi="10.1038/ng1440"><li><a href="https://doi.org/10.1038/ng1440" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/15508005" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Implications+of+correlations+between+skin+color+and+genetic+ancestry+for+biomedical+research+Parra+2004" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref17"><span>17.
            </span><a name="pgen.1006616.ref017" id="pgen.1006616.ref017"></a>Beleza S, Johnson NA, Candille SI, Absher DM, Coram MA, Lopes J, et al. Genetic architecture of skin and eye color in an African-European admixed population. Spritz RA, editor. PLoS Genet. Public Library of Science; 2013;9: e1003372.  pmid:23555287 <ul data-doi="10.1371/journal.pgen.1003372"><li><a href="https://doi.org/10.1371/journal.pgen.1003372" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/23555287" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Genetic+architecture+of+skin+and+eye+color+in+an+African-European+admixed+population.+Spritz+RA%2C+editor+Beleza+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref18"><span>18.
            </span><a name="pgen.1006616.ref018" id="pgen.1006616.ref018"></a>Yang J, Benyamin B, McEvoy BP, Gordon S, Henders AK, Nyholt DR, et al. Common SNPs explain a large proportion of the heritability for human height. Nat Genet. Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.; 2010;42: 565–9.  pmid:20562875 <ul data-doi="10.1038/ng.608"><li><a href="https://doi.org/10.1038/ng.608" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/20562875" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Common+SNPs+explain+a+large+proportion+of+the+heritability+for+human+height+Yang+2010" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref19"><span>19.
            </span><a name="pgen.1006616.ref019" id="pgen.1006616.ref019"></a>Turchin MC, Chiang CWK, Palmer CD, Sankararaman S, Reich D, Hirschhorn JN. Evidence of widespread selection on standing variation in Europe at height-associated SNPs. Nat Genet. Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.; 2012;44: 1015–9.  pmid:22902787 <ul data-doi="10.1038/ng.2368"><li><a href="https://doi.org/10.1038/ng.2368" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/22902787" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Evidence+of+widespread+selection+on+standing+variation+in+Europe+at+height-associated+SNPs+Turchin+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref20"><span>20.
            </span><a name="pgen.1006616.ref020" id="pgen.1006616.ref020"></a>Jablonski NG. the Evolution of Human Skin and Skin Color. Annu Rev Anthropol. 2004;33: 585–623. <ul><li><a href="#" data-author="Jablonski" data-cit="JablonskiNG.%20the%20Evolution%20of%20Human%20Skin%20and%20Skin%20Color.%20Annu%20Rev%20Anthropol.%202004%3B33%3A%20585%E2%80%93623." data-title="the%20Evolution%20of%20Human%20Skin%20and%20Skin%20Color" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=the+Evolution+of+Human+Skin+and+Skin+Color+Jablonski+2004" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref21"><span>21.
            </span><a name="pgen.1006616.ref021" id="pgen.1006616.ref021"></a>Lamason RL, Mohideen M-APK, Mest JR, Wong AC, Norton HL, Aros MC, et al. SLC24A5, a putative cation exchanger, affects pigmentation in zebrafish and humans. Science. 2005;310: 1782–6.  pmid:16357253 <ul data-doi="10.1126/science.1116238"><li><a href="https://doi.org/10.1126/science.1116238" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/16357253" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=SLC24A5%2C+a+putative+cation+exchanger%2C+affects+pigmentation+in+zebrafish+and+humans+Lamason+2005" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref22"><span>22.
            </span><a name="pgen.1006616.ref022" id="pgen.1006616.ref022"></a>Norton HL, Kittles RA, Parra E, McKeigue P, Mao X, Cheng K, et al. Genetic evidence for the convergent evolution of light skin in Europeans and East Asians. Mol Biol Evol. 2007;24: 710–22.  pmid:17182896 <ul data-doi="10.1093/molbev/msl203"><li><a href="https://doi.org/10.1093/molbev/msl203" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/17182896" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Genetic+evidence+for+the+convergent+evolution+of+light+skin+in+Europeans+and+East+Asians+Norton+2007" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref23"><span>23.
            </span><a name="pgen.1006616.ref023" id="pgen.1006616.ref023"></a>Jablonski NG, Chaplin G. The evolution of human skin coloration. J Hum Evol. 2000;39: 57–106.  pmid:10896812 <ul data-doi="10.1006/jhev.2000.0403"><li><a href="https://doi.org/10.1006/jhev.2000.0403" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/10896812" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=The+evolution+of+human+skin+coloration+Jablonski+2000" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref24"><span>24.
            </span><a name="pgen.1006616.ref024" id="pgen.1006616.ref024"></a>Gower JC. Generalized procrustes analysis. Psychometrika. 1975;40: 33–51. <ul><li><a href="#" data-author="Gower" data-cit="GowerJC.%20Generalized%20procrustes%20analysis.%20Psychometrika.%201975%3B40%3A%2033%E2%80%9351." data-title="Generalized%20procrustes%20analysis" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Generalized+procrustes+analysis+Gower+1975" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref25"><span>25.
            </span><a name="pgen.1006616.ref025" id="pgen.1006616.ref025"></a>Claes P, Liberton DK, Daniels K, Rosana KM, Quillen EE, Pearson LN, et al. Modeling 3D Facial Shape from DNA. Luquetti D, editor. PLoS Genet. Public Library of Science; 2014;10: e1004224.  pmid:24651127 <ul data-doi="10.1371/journal.pgen.1004224"><li><a href="https://doi.org/10.1371/journal.pgen.1004224" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/24651127" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Modeling+3D+Facial+Shape+from+DNA.+Luquetti+D%2C+editor+Claes+2014" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref26"><span>26.
            </span><a name="pgen.1006616.ref026" id="pgen.1006616.ref026"></a>Ferrario VF, Mian F, Peretta R, Rosati R, Sforza C. Three-dimensional computerized anthropometry of the nose: Landmark representation compared to surface analysis. Cleft Palate-Craniofacial J. 2007;44: 278–285. <ul><li><a href="#" data-author="Ferrario" data-cit="FerrarioVF%2C%20MianF%2C%20PerettaR%2C%20RosatiR%2C%20SforzaC.%20Three-dimensional%20computerized%20anthropometry%20of%20the%20nose%3A%20Landmark%20representation%20compared%20to%20surface%20analysis.%20Cleft%20Palate-Craniofacial%20J.%202007%3B44%3A%20278%E2%80%93285." data-title="Three-dimensional%20computerized%20anthropometry%20of%20the%20nose%3A%20Landmark%20representation%20compared%20to%20surface%20analysis" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Three-dimensional+computerized+anthropometry+of+the+nose%3A+Landmark+representation+compared+to+surface+analysis+Ferrario+2007" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref27"><span>27.
            </span><a name="pgen.1006616.ref027" id="pgen.1006616.ref027"></a>Nucara A, Pietrafesa M, Rizzo G, Scaccianoce G. Handbook of Anthropometry. Handb Anthr. 2012; 91–114. <ul><li><a href="#" data-author="Nucara" data-cit="NucaraA%2C%20PietrafesaM%2C%20RizzoG%2C%20ScaccianoceG.%20Handbook%20of%20Anthropometry.%20Handb%20Anthr.%202012%3B%2091%E2%80%93114." data-title="Handbook%20of%20Anthropometry" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Handbook+of+Anthropometry+Nucara+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref28"><span>28.
            </span><a name="pgen.1006616.ref028" id="pgen.1006616.ref028"></a>Leinonen T, McCairns RJS, O’Hara RB, Merilä J. Q(ST)-F(ST) comparisons: evolutionary and ecological insights from genomic heterogeneity. Nat Rev Genet. Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.; 2013;14: 179–90.  pmid:23381120 <ul data-doi="10.1038/nrg3395"><li><a href="https://doi.org/10.1038/nrg3395" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/23381120" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Q%28ST%29-F%28ST%29+comparisons%3A+evolutionary+and+ecological+insights+from+genomic+heterogeneity+Leinonen+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref29"><span>29.
            </span><a name="pgen.1006616.ref029" id="pgen.1006616.ref029"></a>Wright S. The Genetical Structure Of Populations. Ann Eugen. 1949;15: 323–354. <ul><li><a href="#" data-author="Wright" data-cit="WrightS.%20The%20Genetical%20Structure%20Of%20Populations.%20Ann%20Eugen.%201949%3B15%3A%20323%E2%80%93354." data-title="The%20Genetical%20Structure%20Of%20Populations" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=The+Genetical+Structure+Of+Populations+Wright+1949" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref30"><span>30.
            </span><a name="pgen.1006616.ref030" id="pgen.1006616.ref030"></a>Lande R. Neutral Theory of Quantitative Genetic Variance in an Island Model with Local Extinction and Colonization. Evolution (N Y). 1992;46: 381–389. <ul><li><a href="#" data-author="Lande" data-cit="LandeR.%20Neutral%20Theory%20of%20Quantitative%20Genetic%20Variance%20in%20an%20Island%20Model%20with%20Local%20Extinction%20and%20Colonization.%20Evolution%20%28N%20Y%29.%201992%3B46%3A%20381%E2%80%93389." data-title="Neutral%20Theory%20of%20Quantitative%20Genetic%20Variance%20in%20an%20Island%20Model%20with%20Local%20Extinction%20and%20Colonization" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Neutral+Theory+of+Quantitative+Genetic+Variance+in+an+Island+Model+with+Local+Extinction+and+Colonization+Lande+1992" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref31"><span>31.
            </span><a name="pgen.1006616.ref031" id="pgen.1006616.ref031"></a>Whitlock MC. Evolutionary inference from QST. Molecular Ecology. 2008. pp. 1885–1896.  pmid:18363667 <ul data-doi="10.1111/j.1365-294X.2008.03712.x"><li><a href="https://doi.org/10.1111/j.1365-294X.2008.03712.x" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/18363667" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Evolutionary+inference+from+QST+Whitlock+2008" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref32"><span>32.
            </span><a name="pgen.1006616.ref032" id="pgen.1006616.ref032"></a>Brommer JE. Whither Pst? The approximation of Qst by Pst in evolutionary and conservation biology. J Evol Biol. 2011;24: 1160–8.  pmid:21457173 <ul data-doi="10.1111/j.1420-9101.2011.02268.x"><li><a href="https://doi.org/10.1111/j.1420-9101.2011.02268.x" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/21457173" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Whither+Pst%3F+The+approximation+of+Qst+by+Pst+in+evolutionary+and+conservation+biology+Brommer+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref33"><span>33.
            </span><a name="pgen.1006616.ref033" id="pgen.1006616.ref033"></a>Leinonen T, Cano JM, Mäkinen H, Merilä J. Contrasting patterns of body shape and neutral genetic divergence in marine and lake populations of threespine sticklebacks. J Evol Biol. 2006;19: 1803–12.  pmid:17040377 <ul data-doi="10.1111/j.1420-9101.2006.01182.x"><li><a href="https://doi.org/10.1111/j.1420-9101.2006.01182.x" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/17040377" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Contrasting+patterns+of+body+shape+and+neutral+genetic+divergence+in+marine+and+lake+populations+of+threespine+sticklebacks+Leinonen+2006" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref34"><span>34.
            </span><a name="pgen.1006616.ref034" id="pgen.1006616.ref034"></a>Perry GH, Foll M, Grenier J-C, Patin E, Nédélec Y, Pacis A, et al. Adaptive, convergent origins of the pygmy phenotype in African rainforest hunter-gatherers. Proc Natl Acad Sci U S A. 2014;111: E3596–603.  pmid:25136101 <ul data-doi="10.1073/pnas.1402875111"><li><a href="https://doi.org/10.1073/pnas.1402875111" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/25136101" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Adaptive%2C+convergent+origins+of+the+pygmy+phenotype+in+African+rainforest+hunter-gatherers+Perry+2014" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref35"><span>35.
            </span><a name="pgen.1006616.ref035" id="pgen.1006616.ref035"></a>Relethford JH. Craniometric variation among modern human populations. Am J Phys Anthropol. 1994;95: 53–62.  pmid:7527996 <ul data-doi="10.1002/ajpa.1330950105"><li><a href="https://doi.org/10.1002/ajpa.1330950105" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/7527996" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Craniometric+variation+among+modern+human+populations+Relethford+1994" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref36"><span>36.
            </span><a name="pgen.1006616.ref036" id="pgen.1006616.ref036"></a>Lynch M, Walsh B. Genetics and analysis of quantitative traits. Sunderland, MA: Sinauer Associates, Inc; 1998. <ul></ul></li><li id="ref37"><span>37.
            </span><a name="pgen.1006616.ref037" id="pgen.1006616.ref037"></a>Yang J, Lee SH, Goddard ME, Visscher PM. GCTA: a tool for genome-wide complex trait analysis. Am J Hum Genet. 2011;88: 76–82.  pmid:21167468 <ul data-doi="10.1016/j.ajhg.2010.11.011"><li><a href="https://doi.org/10.1016/j.ajhg.2010.11.011" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/21167468" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=GCTA%3A+a+tool+for+genome-wide+complex+trait+analysis+Yang+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref38"><span>38.
            </span><a name="pgen.1006616.ref038" id="pgen.1006616.ref038"></a>Sabeti PC, Varilly P, Fry B, Lohmueller J, Hostetter E, Cotsapas C, et al. Genome-wide detection and characterization of positive selection in human populations. Nature. 2007;449: 913–918.  pmid:17943131 <ul data-doi="10.1038/nature06250"><li><a href="https://doi.org/10.1038/nature06250" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/17943131" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Genome-wide+detection+and+characterization+of+positive+selection+in+human+populations+Sabeti+2007" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref39"><span>39.
            </span><a name="pgen.1006616.ref039" id="pgen.1006616.ref039"></a>Darvasi A, Shifman S. The beauty of admixture. Nat Genet. 2005;37: 118–119.  pmid:15678141 <ul data-doi="10.1038/ng0205-118"><li><a href="https://doi.org/10.1038/ng0205-118" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/15678141" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=The+beauty+of+admixture+Darvasi+2005" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref40"><span>40.
            </span><a name="pgen.1006616.ref040" id="pgen.1006616.ref040"></a>Zaitlen N, Pasaniuc B, Sankararaman S, Bhatia G, Zhang J, Gusev A, et al. Leveraging population admixture to characterize the heritability of complex traits. Nat Genet. Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.; 2014;46: 1356–1362.  pmid:25383972 <ul data-doi="10.1038/ng.3139"><li><a href="https://doi.org/10.1038/ng.3139" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/25383972" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Leveraging+population+admixture+to+characterize+the+heritability+of+complex+traits+Zaitlen+2014" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref41"><span>41.
            </span><a name="pgen.1006616.ref041" id="pgen.1006616.ref041"></a>Yokley TR. Ecogeographic variation in human nasal passages. Am J Phys Anthropol. 2009;138: 11–22.  pmid:18623075 <ul data-doi="10.1002/ajpa.20893"><li><a href="https://doi.org/10.1002/ajpa.20893" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/18623075" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Ecogeographic+variation+in+human+nasal+passages+Yokley+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref42"><span>42.
            </span><a name="pgen.1006616.ref042" id="pgen.1006616.ref042"></a>Noback ML, Harvati K, Spoor F. Climate-related variation of the human nasal cavity. Am J Phys Anthropol. 2011;145: 599–614.  pmid:21660932 <ul data-doi="10.1002/ajpa.21523"><li><a href="https://doi.org/10.1002/ajpa.21523" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/21660932" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Climate-related+variation+of+the+human+nasal+cavity+Noback+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref43"><span>43.
            </span><a name="pgen.1006616.ref043" id="pgen.1006616.ref043"></a>Maddux SD, Yokley TR, Svoma BM, Franciscus RG. Absolute humidity and the human nose: A reanalysis of climate zones and their influence on nasal form and function. Am J Phys Anthropol. 2016;161: 309–320.  pmid:27374937 <ul data-doi="10.1002/ajpa.23032"><li><a href="https://doi.org/10.1002/ajpa.23032" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/27374937" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Absolute+humidity+and+the+human+nose%3A+A+reanalysis+of+climate+zones+and+their+influence+on+nasal+form+and+function+Maddux+2016" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref44"><span>44.
            </span><a name="pgen.1006616.ref044" id="pgen.1006616.ref044"></a>Keck T, Lindemann J. Numerical simulation and nasal air-conditioning. GMS Curr Top Otorhinolaryngol Head Neck Surg. 2010;9: Doc08.  pmid:22073112 <ul data-doi="10.3205/cto000072"><li><a href="https://doi.org/10.3205/cto000072" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/22073112" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Numerical+simulation+and+nasal+air-conditioning+Keck+2010" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref45"><span>45.
            </span><a name="pgen.1006616.ref045" id="pgen.1006616.ref045"></a>Inthavong K, Tian ZF, Tu JY. CFD Simulations on the Heating Capability in a Human Nasal Cavity. Rhinology. 2007; 842–847. <ul><li><a href="#" data-author="Inthavong" data-cit="InthavongK%2C%20TianZF%2C%20TuJY.%20CFD%20Simulations%20on%20the%20Heating%20Capability%20in%20a%20Human%20Nasal%20Cavity.%20Rhinology.%202007%3B%20842%E2%80%93847." data-title="CFD%20Simulations%20on%20the%20Heating%20Capability%20in%20a%20Human%20Nasal%20Cavity" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=CFD+Simulations+on+the+Heating+Capability+in+a+Human+Nasal+Cavity+Inthavong+2007" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref46"><span>46.
            </span><a name="pgen.1006616.ref046" id="pgen.1006616.ref046"></a>Irmak MK, Korkmaz A, Erogul O. Selective brain cooling seems to be a mechanism leading to human craniofacial diversity observed in different geographical regions. Med Hypotheses. 2004;63: 974–979.  pmid:15504564 <ul data-doi="10.1016/j.mehy.2004.05.003"><li><a href="https://doi.org/10.1016/j.mehy.2004.05.003" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/15504564" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Selective+brain+cooling+seems+to+be+a+mechanism+leading+to+human+craniofacial+diversity+observed+in+different+geographical+regions+Irmak+2004" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref47"><span>47.
            </span><a name="pgen.1006616.ref047" id="pgen.1006616.ref047"></a>Maloney SK, Mitchell D, Blache D. The contribution of carotid rete variability to brain temperature variability in sheep in a thermoneutral environment. Am J Physiol Regul Integr Comp Physiol. 2007;292: R1298–305.  pmid:17082355 <ul data-doi="10.1152/ajpregu.00275.2006"><li><a href="https://doi.org/10.1152/ajpregu.00275.2006" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/17082355" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=The+contribution+of+carotid+rete+variability+to+brain+temperature+variability+in+sheep+in+a+thermoneutral+environment+Maloney+2007" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref48"><span>48.
            </span><a name="pgen.1006616.ref048" id="pgen.1006616.ref048"></a>Mitchell D, Maloney SK, Jessen C, Laburn HP, Kamerman PR, Mitchell G, et al. Adaptive heterothermy and selective brain cooling in arid-zone mammals. Comp Biochem Physiol Part B Biochem Mol Biol. 2002;131: 571–585. <ul><li><a href="#" data-author="Mitchell" data-cit="MitchellD%2C%20MaloneySK%2C%20JessenC%2C%20LaburnHP%2C%20KamermanPR%2C%20MitchellG%2C%20et%20al.%20Adaptive%20heterothermy%20and%20selective%20brain%20cooling%20in%20arid-zone%20mammals.%20Comp%20Biochem%20Physiol%20Part%20B%20Biochem%20Mol%20Biol.%202002%3B131%3A%20571%E2%80%93585." data-title="Adaptive%20heterothermy%20and%20selective%20brain%20cooling%20in%20arid-zone%20mammals" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Adaptive+heterothermy+and+selective+brain+cooling+in+arid-zone+mammals+Mitchell+2002" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref49"><span>49.
            </span><a name="pgen.1006616.ref049" id="pgen.1006616.ref049"></a>Hetem RS, Strauss WM, Fick LG, Maloney SK, Meyer LCR, Fuller A, et al. Selective brain cooling in Arabian oryx (Oryx leucoryx): a physiological mechanism for coping with aridity? J Exp Biol. 2012;215: 3917–24.  pmid:22899527 <ul data-doi="10.1242/jeb.074666"><li><a href="https://doi.org/10.1242/jeb.074666" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/22899527" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Selective+brain+cooling+in+Arabian+oryx+%28Oryx+leucoryx%29%3A+a+physiological+mechanism+for+coping+with+aridity%3F+Hetem+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref50"><span>50.
            </span><a name="pgen.1006616.ref050" id="pgen.1006616.ref050"></a>Blix AS, Walløe L, Folkow LP. Regulation of brain temperature in winter-acclimatized reindeer under heat stress. J Exp Biol. 2011;214: 3850–6.  pmid:22031750 <ul data-doi="10.1242/jeb.057455"><li><a href="https://doi.org/10.1242/jeb.057455" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/22031750" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Regulation+of+brain+temperature+in+winter-acclimatized+reindeer+under+heat+stress+Blix+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref51"><span>51.
            </span><a name="pgen.1006616.ref051" id="pgen.1006616.ref051"></a>Crandall CG, Brothers RM, Zhang R, Brengelmann GL, Covaciu L, Jay O, et al. Comments on point:counterpoint: humans do/do not demonstrate selective brain cooling during hyperthermia. J Appl Physiol. American Physiological Society; 2011;110: 575–80.  pmid:21304015 <ul data-doi="10.1152/japplphysiol.01375.2010"><li><a href="https://doi.org/10.1152/japplphysiol.01375.2010" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/21304015" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Comments+on+point%3Acounterpoint%3A+humans+do%2Fdo+not+demonstrate+selective+brain+cooling+during+hyperthermia+Crandall+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref52"><span>52.
            </span><a name="pgen.1006616.ref052" id="pgen.1006616.ref052"></a>Cabanac M. Selective brain cooling in humans: “fancy” or fact? FASEB J. 1993;7: 1143-1146-1147. <ul><li><a href="#" data-author="Cabanac" data-cit="CabanacM.%20Selective%20brain%20cooling%20in%20humans%3A%20%E2%80%9Cfancy%E2%80%9D%20or%20fact%3F%20FASEB%20J.%201993%3B7%3A%201143-1146-1147." data-title="Selective%20brain%20cooling%20in%20humans%3A%20%E2%80%9Cfancy%E2%80%9D%20or%20fact%3F" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Selective+brain+cooling+in+humans%3A+%E2%80%9Cfancy%E2%80%9D+or+fact%3F+Cabanac+1993" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref53"><span>53.
            </span><a name="pgen.1006616.ref053" id="pgen.1006616.ref053"></a>Puts DA. Beauty and the beast: mechanisms of sexual selection in humans. Evol Hum Behav. Elsevier; 2010;31: 157–175. <ul><li><a href="#" data-author="Puts" data-cit="PutsDA.%20Beauty%20and%20the%20beast%3A%20mechanisms%20of%20sexual%20selection%20in%20humans.%20Evol%20Hum%20Behav.%20Elsevier%3B%202010%3B31%3A%20157%E2%80%93175." data-title="Beauty%20and%20the%20beast%3A%20mechanisms%20of%20sexual%20selection%20in%20humans" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Beauty+and+the+beast%3A+mechanisms+of+sexual+selection+in+humans+Puts+2010" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref54"><span>54.
            </span><a name="pgen.1006616.ref054" id="pgen.1006616.ref054"></a>van Doorn GS, Edelaar P, Weissing FJ. On the origin of species by natural and sexual selection. Science. 2009;326: 1704–1707.  pmid:19965377 <ul data-doi="10.1126/science.1181661"><li><a href="https://doi.org/10.1126/science.1181661" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/19965377" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=On+the+origin+of+species+by+natural+and+sexual+selection+van+Doorn+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref55"><span>55.
            </span><a name="pgen.1006616.ref055" id="pgen.1006616.ref055"></a>Bigham AW, Mao X, Mei R, Brutsaert T, Wilson MJ, Julian CG, et al. Identifying positive selection candidate loci for high-altitude adaptation in Andean populations. Hum Genomics. 2009;4: 79–90.  pmid:20038496 <ul data-doi="10.1186/1479-7364-4-2-79"><li><a href="https://doi.org/10.1186/1479-7364-4-2-79" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/20038496" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Identifying+positive+selection+candidate+loci+for+high-altitude+adaptation+in+Andean+populations+Bigham+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref56"><span>56.
            </span><a name="pgen.1006616.ref056" id="pgen.1006616.ref056"></a>Wu T, Li S, Ward MP. Tibetans at extreme altitude. Wilderness Environ Med. 2005;16: 47–54. pmid:15813148 <ul><li><a href="#" data-author="Wu" data-cit="WuT%2C%20LiS%2C%20WardMP.%20Tibetans%20at%20extreme%20altitude.%20Wilderness%20Environ%20Med.%202005%3B16%3A%2047%E2%80%9354.%2015813148" data-title="Tibetans%20at%20extreme%20altitude" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/15813148" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Tibetans+at+extreme+altitude+Wu+2005" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref57"><span>57.
            </span><a name="pgen.1006616.ref057" id="pgen.1006616.ref057"></a>Paternoster L, Zhurov AI, Toma AM, Kemp JP, St Pourcain B, Timpson NJ, et al. Genome-wide association study of three-dimensional facial morphology identifies a variant in PAX3 associated with nasion position. Am J Hum Genet. Elsevier; 2012;90: 478–85.  pmid:22341974 <ul data-doi="10.1016/j.ajhg.2011.12.021"><li><a href="https://doi.org/10.1016/j.ajhg.2011.12.021" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/22341974" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Genome-wide+association+study+of+three-dimensional+facial+morphology+identifies+a+variant+in+PAX3+associated+with+nasion+position+Paternoster+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref58"><span>58.
            </span><a name="pgen.1006616.ref058" id="pgen.1006616.ref058"></a>Liu F, van der Lijn F, Schurmann C, Zhu G, Chakravarty MM, Hysi PG, et al. A genome-wide association study identifies five loci influencing facial morphology in Europeans. Gibson G, editor. PLoS Genet. Public Library of Science; 2012;8: e1002932.  pmid:23028347 <ul data-doi="10.1371/journal.pgen.1002932"><li><a href="https://doi.org/10.1371/journal.pgen.1002932" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/23028347" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=A+genome-wide+association+study+identifies+five+loci+influencing+facial+morphology+in+Europeans+Liu+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref59"><span>59.
            </span><a name="pgen.1006616.ref059" id="pgen.1006616.ref059"></a>Shaffer JR, Orlova E, Lee MK, Leslie EJ, Raffensperger ZD, Heike CL, et al. Genome-Wide Association Study Reveals Multiple Loci Influencing Normal Human Facial Morphology. Barsh GS, editor. PLOS Genet. Public Library of Science; 2016;12: e1006149.  pmid:27560520 <ul data-doi="10.1371/journal.pgen.1006149"><li><a href="https://doi.org/10.1371/journal.pgen.1006149" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/27560520" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Genome-Wide+Association+Study+Reveals+Multiple+Loci+Influencing+Normal+Human+Facial+Morphology+Shaffer+2016" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref60"><span>60.
            </span><a name="pgen.1006616.ref060" id="pgen.1006616.ref060"></a>Adhikari K, Fuentes-Guajardo M, Quinto-Sánchez M, Mendoza-Revilla J, Camilo Chacón-Duque J, Acuña-Alonzo V, et al. A genome-wide association scan implicates DCHS2, RUNX2, GLI3, PAX1 and EDAR in human facial variation. Nat Commun. Nature Research; 2016;7: 11616. <ul><li><a href="#" data-author="Adhikari" data-cit="AdhikariK%2C%20Fuentes-GuajardoM%2C%20Quinto-S%C3%A1nchezM%2C%20Mendoza-RevillaJ%2C%20Camilo%20Chac%C3%B3n-DuqueJ%2C%20Acu%C3%B1a-AlonzoV%2C%20et%20al.%20A%20genome-wide%20association%20scan%20implicates%20DCHS2%2C%20RUNX2%2C%20GLI3%2C%20PAX1%20and%20EDAR%20in%20human%20facial%20variation.%20Nat%20Commun.%20Nature%20Research%3B%202016%3B7%3A%2011616." data-title="A%20genome-wide%20association%20scan%20implicates%20DCHS2%2C%20RUNX2%2C%20GLI3%2C%20PAX1%20and%20EDAR%20in%20human%20facial%20variation" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=A+genome-wide+association+scan+implicates+DCHS2%2C+RUNX2%2C+GLI3%2C+PAX1+and+EDAR+in+human+facial+variation+Adhikari+2016" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref61"><span>61.
            </span><a name="pgen.1006616.ref061" id="pgen.1006616.ref061"></a>Beleza S, Campos J, Lopes J, Araújo II, Hoppfer Almada A, Correia e Silva A, et al. The Admixture Structure and Genetic Variation of the Archipelago of Cape Verde and Its Implications for Admixture Mapping Studies. PLoS One. 2012;7. <ul><li><a href="#" data-author="Beleza" data-cit="BelezaS%2C%20CamposJ%2C%20LopesJ%2C%20Ara%C3%BAjoII%2C%20Hoppfer%20AlmadaA%2C%20Correia%20e%20SilvaA%2C%20et%20al.%20The%20Admixture%20Structure%20and%20Genetic%20Variation%20of%20the%20Archipelago%20of%20Cape%20Verde%20and%20Its%20Implications%20for%20Admixture%20Mapping%20Studies.%20PLoS%20One.%202012%3B7." data-title="The%20Admixture%20Structure%20and%20Genetic%20Variation%20of%20the%20Archipelago%20of%20Cape%20Verde%20and%20Its%20Implications%20for%20Admixture%20Mapping%20Studies" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=The+Admixture+Structure+and+Genetic+Variation+of+the+Archipelago+of+Cape+Verde+and+Its+Implications+for+Admixture+Mapping+Studies+Beleza+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref62"><span>62.
            </span><a name="pgen.1006616.ref062" id="pgen.1006616.ref062"></a>Illumina Inc. “TOP/BOT” strand and “A/B” allele—A guide to Illumina’s method for determining Strand and Allele for the GoldenGate and Infinium Assays. (Technical Note) [Internet]. 2006. Available: <a href="http://www.illumina.com/documents/products/technotes/technote_topbot.pdf">http://www.illumina.com/documents/products/technotes/technote_topbot.pdf</a> <ul></ul></li><li id="ref63"><span>63.
            </span><a name="pgen.1006616.ref063" id="pgen.1006616.ref063"></a>Purcell S, Neale B, Todd-Brown K, Thomas L, Ferreira MAR, Bender D, et al. PLINK: a tool set for whole-genome association and population-based linkage analyses. Am J Hum Genet. 2007;81: 559–75.  pmid:17701901 <ul data-doi="10.1086/519795"><li><a href="https://doi.org/10.1086/519795" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/17701901" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=PLINK%3A+a+tool+set+for+whole-genome+association+and+population-based+linkage+analyses+Purcell+2007" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref64"><span>64.
            </span><a name="pgen.1006616.ref064" id="pgen.1006616.ref064"></a>Chang CC, Chow CC, Tellier LC, Vattikuti S, Purcell SM, Lee JJ. Second-generation PLINK: rising to the challenge of larger and richer datasets. Gigascience. BioMed Central Ltd; 2015;4: 7.  pmid:25722852 <ul data-doi="10.1186/s13742-015-0047-8"><li><a href="https://doi.org/10.1186/s13742-015-0047-8" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/25722852" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Second-generation+PLINK%3A+rising+to+the+challenge+of+larger+and+richer+datasets+Chang+2015" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref65"><span>65.
            </span><a name="pgen.1006616.ref065" id="pgen.1006616.ref065"></a>Candille SI, Absher DM, Beleza S, Bauchet M, McEvoy B, Garrison NA, et al. Genome-wide association studies of quantitatively measured skin, hair, and eye pigmentation in four European populations. PLoS One. Public Library of Science; 2012;7: e48294.  pmid:23118974 <ul data-doi="10.1371/journal.pone.0048294"><li><a href="https://doi.org/10.1371/journal.pone.0048294" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/23118974" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Genome-wide+association+studies+of+quantitatively+measured+skin%2C+hair%2C+and+eye+pigmentation+in+four+European+populations+Candille+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref66"><span>66.
            </span><a name="pgen.1006616.ref066" id="pgen.1006616.ref066"></a>Storz JF. Contrasting patterns of divergence in quantitative traits and neutral DNA markers: analysis of clinal variation. Mol Ecol. 2002;11: 2537–2551. pmid:12453238 <ul><li><a href="#" data-author="Storz" data-cit="StorzJF.%20Contrasting%20patterns%20of%20divergence%20in%20quantitative%20traits%20and%20neutral%20DNA%20markers%3A%20analysis%20of%20clinal%20variation.%20Mol%20Ecol.%202002%3B11%3A%202537%E2%80%932551.%2012453238" data-title="Contrasting%20patterns%20of%20divergence%20in%20quantitative%20traits%20and%20neutral%20DNA%20markers%3A%20analysis%20of%20clinal%20variation" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/12453238" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Contrasting+patterns+of+divergence+in+quantitative+traits+and+neutral+DNA+markers%3A+analysis+of+clinal+variation+Storz+2002" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref67"><span>67.
            </span><a name="pgen.1006616.ref067" id="pgen.1006616.ref067"></a>He Y, Li R, Wang J, Blanchet S, Lek S. Morphological Variation Among Wild Populations of Chinese Rare Minnow (Gobiocypris rarus): Deciphering the Role of Evolutionary Processes. Zoolog Sci. 2013;30: 475–83.  pmid:23721472 <ul data-doi="10.2108/zsj.30.475"><li><a href="https://doi.org/10.2108/zsj.30.475" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/23721472" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Morphological+Variation+Among+Wild+Populations+of+Chinese+Rare+Minnow+%28Gobiocypris+rarus%29%3A+Deciphering+the+Role+of+Evolutionary+Processes+He+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref68"><span>68.
            </span><a name="pgen.1006616.ref068" id="pgen.1006616.ref068"></a>Weir BS, Cockerham CC. Estimating F-statistics for the analysis of population structure. Evolution (N Y). 1984;38: 1358–1370. <ul><li><a href="#" data-author="Weir" data-cit="WeirBS%2C%20CockerhamCC.%20Estimating%20F-statistics%20for%20the%20analysis%20of%20population%20structure.%20Evolution%20%28N%20Y%29.%201984%3B38%3A%201358%E2%80%931370." data-title="Estimating%20F-statistics%20for%20the%20analysis%20of%20population%20structure" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Estimating+F-statistics+for+the+analysis+of+population+structure+Weir+1984" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref69"><span>69.
            </span><a name="pgen.1006616.ref069" id="pgen.1006616.ref069"></a>Browning SR, Browning BL. Population structure can inflate SNP-based heritability estimates. Am J Hum Genet. 2011;89: 191–193.  pmid:21763486 <ul data-doi="10.1016/j.ajhg.2011.05.025"><li><a href="https://doi.org/10.1016/j.ajhg.2011.05.025" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/21763486" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Population+structure+can+inflate+SNP-based+heritability+estimates+Browning+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref70"><span>70.
            </span><a name="pgen.1006616.ref070" id="pgen.1006616.ref070"></a>Goddard ME, Lee SH, Yang J, Wray NR, Visscher PM. Response to browning and browning. Am J Hum Genet. 2011;89: 193–195. <ul><li><a href="#" data-author="Goddard" data-cit="GoddardME%2C%20LeeSH%2C%20YangJ%2C%20WrayNR%2C%20VisscherPM.%20Response%20to%20browning%20and%20browning.%20Am%20J%20Hum%20Genet.%202011%3B89%3A%20193%E2%80%93195." data-title="Response%20to%20browning%20and%20browning" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Response+to+browning+and+browning+Goddard+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref71"><span>71.
            </span><a name="pgen.1006616.ref071" id="pgen.1006616.ref071"></a>Baran Y, Pasaniuc B, Sankararaman S, Torgerson DG, Gignoux C, Eng C, et al. Fast and accurate inference of local ancestry in Latino populations. Bioinformatics. 2012;28: 1359–1367.  pmid:22495753 <ul data-doi="10.1093/bioinformatics/bts144"><li><a href="https://doi.org/10.1093/bioinformatics/bts144" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/22495753" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Fast+and+accurate+inference+of+local+ancestry+in+Latino+populations+Baran+2012" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref72"><span>72.
            </span><a name="pgen.1006616.ref072" id="pgen.1006616.ref072"></a>Halder I, Shriver MD. Measuring and using admixture to study the genetics of complex diseases. Hum Genomics. BioMed Central Ltd; 2003;1: 52.  pmid:15601533 <ul data-doi="10.1186/1479-7364-1-1-52"><li><a href="https://doi.org/10.1186/1479-7364-1-1-52" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/15601533" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Measuring+and+using+admixture+to+study+the+genetics+of+complex+diseases+Halder+2003" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref73"><span>73.
            </span><a name="pgen.1006616.ref073" id="pgen.1006616.ref073"></a>Kahle D, Wickham H. ggmap: Spatial Visualization with. R J. 2013;5: 144–161. Available: <a href="http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf">http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf</a> <ul><li><a href="#" data-author="Kahle" data-cit="KahleD%2C%20WickhamH.%20ggmap%3A%20Spatial%20Visualization%20with.%20R%20J.%202013%3B5%3A%20144%E2%80%93161.%20Available%3A%20http%3A%2F%2Fjournal.r-project.org%2Farchive%2F2013-1%2Fkahle-wickham.pdf" data-title="ggmap%3A%20Spatial%20Visualization%20with" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=ggmap%3A+Spatial+Visualization+with+Kahle+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref74"><span>74.
            </span><a name="pgen.1006616.ref074" id="pgen.1006616.ref074"></a>R Core Team. R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. 2013. <ul></ul></li><li id="ref75"><span>75.
            </span><a name="pgen.1006616.ref075" id="pgen.1006616.ref075"></a>Harris I, Jones PD, Osborn TJ, Lister DH. Updated high-resolution grids of monthly climatic observations—the CRU TS3.10 Dataset. Int J Climatol. John Wiley &amp; Sons, Ltd; 2014;34: 623–642. <ul><li><a href="#" data-author="Harris" data-cit="HarrisI%2C%20JonesPD%2C%20OsbornTJ%2C%20ListerDH.%20Updated%20high-resolution%20grids%20of%20monthly%20climatic%20observations%E2%80%94the%20CRU%20TS3.10%20Dataset.%20Int%20J%20Climatol.%20John%20Wiley%20%26%20Sons%2C%20Ltd%3B%202014%3B34%3A%20623%E2%80%93642." data-title="Updated%20high-resolution%20grids%20of%20monthly%20climatic%20observations%E2%80%94the%20CRU%20TS3.10%20Dataset" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Updated+high-resolution+grids+of+monthly+climatic+observations%E2%80%94the+CRU+TS3.10+Dataset+Harris+2014" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref76"><span>76.
            </span><a name="pgen.1006616.ref076" id="pgen.1006616.ref076"></a>TOMS Science Team (1996). TOMS Earth Probe UV Reflectivity Monthly L3 Global 1 deg x 1.25 deg Lat/Lon Grid V008, version 008, Greenbelt, MD, Goddard Earth Sciences Data and Information Services Center (GES DISC) [Internet]. Available: <a href="http://disc.sci.gsfc.nasa.gov/datacollection/TOMSEPL3mref_008.html">http://disc.sci.gsfc.nasa.gov/datacollection/TOMSEPL3mref_008.html</a> <ul></ul></li><li id="ref77"><span>77.
            </span><a name="pgen.1006616.ref077" id="pgen.1006616.ref077"></a>Michna P, Woods M. RNetCDF—A Package for Reading and Writing NetCDF Datasets. R J. 2013;5: 29–35. <ul><li><a href="#" data-author="Michna" data-cit="MichnaP%2C%20WoodsM.%20RNetCDF%E2%80%94A%20Package%20for%20Reading%20and%20Writing%20NetCDF%20Datasets.%20R%20J.%202013%3B5%3A%2029%E2%80%9335." data-title="RNetCDF%E2%80%94A%20Package%20for%20Reading%20and%20Writing%20NetCDF%20Datasets" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=RNetCDF%E2%80%94A+Package+for+Reading+and+Writing+NetCDF+Datasets+Michna+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref78"><span>78.
            </span><a name="pgen.1006616.ref078" id="pgen.1006616.ref078"></a>Hijmans RJ, Etten J Van. raster: Geographic analysis and modeling with raster data. R package version 2.0–12. [Internet]. 2012. Available: <a href="http://cran.r-project.org/package=raster">http://cran.r-project.org/package=raster</a> <ul></ul></li><li id="ref79"><span>79.
            </span><a name="pgen.1006616.ref079" id="pgen.1006616.ref079"></a>Rousset F, Ferdy J-B. Testing environmental and genetic effects in the presence of spatial autocorrelation. Ecography (Cop). 2014;37: 781–790. <ul><li><a href="#" data-author="Rousset" data-cit="RoussetF%2C%20FerdyJ-B.%20Testing%20environmental%20and%20genetic%20effects%20in%20the%20presence%20of%20spatial%20autocorrelation.%20Ecography%20%28Cop%29.%202014%3B37%3A%20781%E2%80%93790." data-title="Testing%20environmental%20and%20genetic%20effects%20in%20the%20presence%20of%20spatial%20autocorrelation" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Testing+environmental+and+genetic+effects+in+the+presence+of+spatial+autocorrelation+Rousset+2014" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref80"><span>80.
            </span><a name="pgen.1006616.ref080" id="pgen.1006616.ref080"></a>Claes P, Reijniers J, Shriver MD, Snyders J, Suetens P, Nielandt J, et al. An investigation of matching symmetry in the human pinnae with possible implications for 3D ear recognition and sound localization. J Anat. 2015;226: 60–72.  pmid:25382291 <ul data-doi="10.1111/joa.12252"><li><a href="https://doi.org/10.1111/joa.12252" data-author="doi-provided" data-cit="doi-provided" data-title="doi-provided" target="_new" title="Go to article">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/25382291" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=An+investigation+of+matching+symmetry+in+the+human+pinnae+with+possible+implications+for+3D+ear+recognition+and+sound+localization+Claes+2015" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref81"><span>81.
            </span><a name="pgen.1006616.ref081" id="pgen.1006616.ref081"></a>Pinheiro J, Bates D, DebRoy S, Sarkar D, Team TRC. nlme: Linear and Nonlinear Mixed Effects Models. October. 2013. pp. 1–3. <ul><li><a href="#" data-author="Pinheiro" data-cit="PinheiroJ%2C%20BatesD%2C%20DebRoyS%2C%20SarkarD%2C%20TeamTRC.%20nlme%3A%20Linear%20and%20Nonlinear%20Mixed%20Effects%20Models.%20October.%202013.%20pp.%201%E2%80%933." data-title="nlme%3A%20Linear%20and%20Nonlinear%20Mixed%20Effects%20Models" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=nlme%3A+Linear+and+Nonlinear+Mixed+Effects+Models+Pinheiro+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li></ol></div>



          

        </div>
      </div>
  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Majority of US teens have lost trust in Big Tech (153 pts)]]></title>
            <link>https://techcrunch.com/2025/01/29/report-majority-of-u-s-teens-have-lost-trust-in-big-tech/</link>
            <guid>42875399</guid>
            <pubDate>Thu, 30 Jan 2025 06:17:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/01/29/report-majority-of-u-s-teens-have-lost-trust-in-big-tech/">https://techcrunch.com/2025/01/29/report-majority-of-u-s-teens-have-lost-trust-in-big-tech/</a>, See on <a href="https://news.ycombinator.com/item?id=42875399">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">American teens have lost their faith in Big Tech, according to a new report from <a href="https://www.commonsensemedia.org/" target="_blank" rel="noreferrer noopener nofollow">Common Sense Media</a>, a nonprofit offering reviews and ratings for media and technology, which <a href="https://techcrunch.com/2024/01/29/openai-partners-with-common-sense-media-to-collaborate-on-ai-guidelines/">more recently includes AI products</a>.</p>

<p>In the <a href="https://www.commonsensemedia.org/research/research-brief-teens-trust-and-technology-in-the-age-of-ai" target="_blank" rel="noreferrer noopener nofollow">study released</a> Wednesday, the organization surveyed over 1,000 teens on whether major technology companies like Google, Apple, Meta, TikTok, and Microsoft cared about their well-being and safety, made ethical decisions, protected their private data, and more. In all cases, a majority of teens reported low levels of trust in these tech companies. Nearly half of teens said they had little or no trust that the companies would make responsible decisions about how they use AI.</p>







<p>Distrust in Big Tech has been building in the U.S. for years, from the 2013 <a href="https://techcrunch.com/2013/06/06/report-nsa-collects-data-directly-from-servers-of-google-apple-microsoft-facebook-and-more/">revelation</a> of the government’s <a href="https://en.wikipedia.org/wiki/PRISM" target="_blank" rel="noreferrer noopener nofollow">mass data collection</a>, to the data scandal involving consulting firm <a href="https://techcrunch.com/tag/cambridge-analytica/">Cambridge Analytica</a>, to the 2021 Facebook <a href="https://techcrunch.com/2021/10/05/facebook-whistleblower-frances-haugen-testifies-before-the-senate/">whistleblower Frances Haugen’s</a> leaks <a href="https://en.wikipedia.org/wiki/Frances_Haugen" target="_blank" rel="noreferrer noopener nofollow">indicating</a> Meta was aware of its harms on society, to the <a href="https://techcrunch.com/2024/01/31/senate-hearing-with-five-social-media-ceos-was-a-missed-opportunity/">multiple</a> <a href="https://techcrunch.com/2021/03/25/these-house-hearings-on-tech-are-a-waste-of-time-and-everyone-knows-it/">Congressional</a> <a href="https://techcrunch.com/tag/tech-antitrust-hearing/">hearings</a> where lawmakers grilled Big Tech CEOs over app safety, antitrust issues, and harmful algorithms.</p>

<p>This year, tech CEOs lined up to pledge allegiance to the Trump administration in the form of <a href="https://www.yahoo.com/news/tech-billionaires-zuckerberg-bezos-altman-002804667.html">$1 million donations</a> to the president’s inaugural fund, hoping to buy favor and avoid scrutiny and regulation of their businesses — no matter the cost to their users. (Even for those aligned with Trump, the tech leaders’ actions are seen as disingenuous, given how they’ve flip-flopped after previously criticizing Trump in his earlier term.)</p>

<p>While teens may or may not track these tech news headlines as closely as their adult counterparts, this overall shift in sentiment is affecting them, too.</p>

<p>Common Sense says that 64% of surveyed U.S. teens don’t trust Big Tech companies to care about their mental health and well-being and 62% don’t think the companies will protect their safety if it hurts profits.</p>

<p>Over half of surveyed U.S. teens (53%) also don’t think major tech companies make ethical and responsible design decisions (think: the growing use of dark patterns in user interface design meant to trick, confuse, and deceive.</p>


<p>A further 52% don’t think that Big Tech will keep their personal information safe and 51% don’t think the companies are fair and inclusive when considering the needs of different users. </p>

<p>Not surprisingly, the mistrust in tech is influencing teens’ opinions around AI, too, as 47% of those surveyed don’t believe these companies will make responsible decisions over their use of AI.</p>

<p>The new study builds on Common Sense’s <a href="https://www.commonsensemedia.org/research/the-dawn-of-the-ai-era-teens-parents-and-the-adoption-of-generative-ai-at-home-and-school" target="_blank" rel="noreferrer noopener nofollow">prior research</a> about the adoption of generative AI among teens and also focuses on how GenAI is impacting the larger media landscape.</p>







<p>For instance, it found that 41% of surveyed teens reported being misled by fake images online, 35% were misled by fake online content in general, and over a quarter (28%) wondered if they were talking to a bot or a human. A third of teens also said that GenAI would make it even harder to trust the accuracy of online information. That figure rises to 40% if the teens had previously been duped by fake or misleading content.</p>

<p>Overall, the report points to a lack of uncertainty over online content, though that’s hardly a new problem for the web. </p>

<p>Still, it seems that AI isn’t helping the matter despite AI chatbots’ authoritative answers. Some 39% of surveyed teens noticed problems with AI’s output when using it for schoolwork help. Plus, a majority of surveyed U.S. teens (74%) said privacy safeguards and transparency are needed to manage AI, 74% said AI companies should discourage people from sharing personal information on their platform, and 73% of teens said AI images and other content should be labeled and watermarked.</p>

<p>When weighing in on the business models of AI, 61% of teens felt that content creators should be compensated when their data is used by AI systems. </p>

<p>As a result of teens’ lack of trust and the fast pace of AI development, 35% of teens think GenAI will make it harder to trust online information — though that number could change in time.</p>
</div><div>
	
	
	
	

	
<div>
	<p>Sarah has worked as a reporter for TechCrunch since August 2011. She joined the company after having previously spent over three years at ReadWriteWeb. Prior to her work as a reporter, Sarah worked in I.T. across a number of industries, including banking, retail and software.</p>
</div>


	
	<p>
		<a data-ctatext="View Bio" data-destinationlink="https://techcrunch.com/author/sarah-perez/" data-event="button" href="https://techcrunch.com/author/sarah-perez/">View Bio <svg style="width: 1em;" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><path fill="var(--c-svg, currentColor)" d="M16.5 12 9 19.5l-1.05-1.05L14.4 12 7.95 5.55 9 4.5z"></path></svg></a>
	</p>
	
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PCBs, copper pours, ground planes, and you (175 pts)]]></title>
            <link>https://lcamtuf.substack.com/p/pcbs-ground-planes-and-you</link>
            <guid>42874885</guid>
            <pubDate>Thu, 30 Jan 2025 04:33:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lcamtuf.substack.com/p/pcbs-ground-planes-and-you">https://lcamtuf.substack.com/p/pcbs-ground-planes-and-you</a>, See on <a href="https://news.ycombinator.com/item?id=42874885">Hacker News</a></p>
Couldn't get https://lcamtuf.substack.com/p/pcbs-ground-planes-and-you: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Hard Mode Rust (135 pts)]]></title>
            <link>https://matklad.github.io/2022/10/06/hard-mode-rust.html</link>
            <guid>42874605</guid>
            <pubDate>Thu, 30 Jan 2025 03:46:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matklad.github.io/2022/10/06/hard-mode-rust.html">https://matklad.github.io/2022/10/06/hard-mode-rust.html</a>, See on <a href="https://news.ycombinator.com/item?id=42874605">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <article>


<p><span>This post is a case study of writing a Rust application using only minimal, artificially constrained API (eg, no dynamic memory allocation).</span>
<span>It assumes a fair bit of familiarity with the language.</span></p>
<section id="Hard-Mode-Rust-1">

    <h2>
    <a href="#Hard-Mode-Rust-1"><span>Hard Mode Rust</span> </a>
    </h2>
<p><span>The back story here is a particular criticism of Rust and C++ from hard-core C programmers.</span>
<span>This criticism is aimed at </span><a href="https://en.cppreference.com/w/cpp/language/raii"><span>RAII</span></a><span> </span>—<span> the language-defining feature of C++, which was wholesale imported to Rust as well.</span>
<span>RAII makes using various resources requiring cleanups (file descriptors, memory, locks) easy </span>—<span> any place in the program can create a resource, and the cleanup code will be invoked automatically when needed.</span>
<span>And herein lies the problem </span>—<span> because allocating resources becomes easy, RAII encourages a sloppy attitude to resources, where they are allocated and destroyed all over the place.</span>
<span>In particular, this leads to:</span></p>
<ul>
<li>
<span>Decrease in reliability. Resources are usually limited in principle, but actual resource exhaustion happens rarely.</span>
<span>If resources are allocated throughout the program, there are many virtually untested codepaths.</span>
</li>
<li>
<span>Lack of predictability. It usually is impossible to predict up-front how much resources will the program consume.</span>
<span>Instead, resource-consumption is observed empirically.</span>
</li>
<li>
<span>Poor performance. Usually, it is significantly more efficient to allocate and free resources in batches.</span>
<span>Cleanup code for individual resources is scattered throughout codebase, increasing code bloat</span>
</li>
<li>
<span>Spaghetti architecture. Resource allocation is an architecturally salient thing.</span>
<span>If all resource management is centralized to a single place, it becomes significantly easier to understand lifecycle of resources.</span>
</li>
</ul>
<p><span>I think this is a fair criticism.</span>
<span>In fact, I think this is the same criticism that C++ and Rust programmers aim at garbage collected languages.</span>
<span>This is a spectrum:</span></p>

<figure>


<pre><code><span>           GC object graph</span>
<span>                 v v</span>
<span>                  v</span>
<span>        Tree of values with RAII</span>
<span>                 v v</span>
<span>                  v</span>
<span>Static allocation of resources at startup</span></code></pre>

</figure>
<p><span>Rust programmers typically are not exposed to the lowest level of this pyramid.</span>
<span>But there</span>’<span>s a relatively compact exercise to gain the relevant experience: try re-implementing your favorite Rust programs on hard mode.</span></p>
<p><strong><strong><span>Hard Mode</span></strong></strong><span> means that you split your program into </span><code>std</code><span> binary and </span><code>#![no_std]</code><span> no-alloc library.</span>
<span>Only the small binary is allowed to directly ask OS for resources.</span>
<span>For the library, all resources must be injected.</span>
<span>In particular, to do memory allocation, the library receives a slice of bytes of a fixed size, and should use that for all storage.</span>
<span>Something like this:</span></p>

<figure>


<pre><code><span><span>// app/src/main.rs</span></span>
<span><span>fn</span> <span>main</span>() {</span>
<span>  <span>let</span> <span>mem_limit</span> = <span>64</span> * <span>1024</span>;</span>
<span>  <span>let</span> <span>memory</span> = <span>vec!</span>[<span>0u8</span>; mem_limit];</span>
<span>  app::<span>run</span>(&amp;<span>mut</span> memory)</span>
<span>}</span>
<span></span>
<span><span>// app/src/lib.rs</span></span>
<span><span>#![no_std]</span> <span>// &lt;- the point of the exercise</span></span>
<span></span>
<span><span>pub</span> <span>fn</span> <span>run</span>(memory: &amp;<span>mut</span> [<span>u8</span>]) {</span>
<span>  ...</span>
<span>}</span></code></pre>

</figure>
</section>
<section id="Ray-Tracing">

    <h2>
    <a href="#Ray-Tracing"><span>Ray Tracing</span> </a>
    </h2>
<p><span>So, this is what the post is about: my experience implementing a toy hard mode ray tracer.</span>
<span>You can find the code on GitHub: </span><a href="http://github.com/matklad/crt">http://github.com/matklad/crt</a><span>.</span></p>
<p><span>The task of a ray tracer is to convert a description of a 3D scene like the following one:</span></p>

<figure>


<pre><code><span>background #000000</span>
<span></span>
<span>camera {</span>
<span>    pos 0,10,-50</span>
<span>    look_at 0,0,0</span>
<span>    up 0,-1,0</span>
<span>    focus 50</span>
<span>    dim 80x60</span>
<span>}</span>
<span></span>
<span>light {</span>
<span>    pos -20,10,0</span>
<span>    color #aa1111</span>
<span>}</span>
<span></span>
<span>plane {</span>
<span>    pos 0,-10,0</span>
<span>    normal 0,1,0</span>
<span>    material {</span>
<span>        color #5566FF</span>
<span>        diffuse 3</span>
<span>    }</span>
<span>}</span>
<span></span>
<span>mesh {</span>
<span>    material {</span>
<span>        color #BB5566</span>
<span>        diffuse 3</span>
<span>    }</span>
<span></span>
<span>    data {</span>
<span>        v 5.92,4.12,0.00</span>
<span>        v 5.83,4.49,0.00</span>
<span>        v 5.94,4.61,0.00</span>
<span>        v 6.17,4.49,0.00</span>
<span>        v 6.42,4.12,0.00</span>
<span>        v 5.38,4.12,2.74</span>
<span>        ...</span>
<span></span>
<span>        vn -0.96,-0.25,0.00</span>
<span>        vn -0.96,0.25,0.00</span>
<span>        vn -0.09,0.99,0.00</span>
<span>        vn 0.68,0.73,0.00</span>
<span>        vn 0.87,0.49,0.00</span>
<span>        vn -0.89,-0.25,-0.36</span>
<span>        ...</span>
<span></span>
<span>        f 1/1 2/2 3/3</span>
<span>        f 4/4 5/5 6/6</span>
<span>        ...</span>
<span>    }</span>
<span></span>
<span>}</span></code></pre>

</figure>
<p><span>Into a rendered image like this:</span></p>

<figure>

<img alt="" src="https://user-images.githubusercontent.com/1711539/194287665-05583649-dcb0-4014-82b9-424f945e19a4.png">
</figure>
<p><span>This works rather intuitive conceptually.</span>
<span>First, imagine the above scene, with an infinite fuchsia colored plane and a red Utah teapot hovering above that.</span>
<span>Then, imagine a camera standing at </span><code>0,10,-50</code><span> (in cartesian coordinates) and aiming at the origin.</span>
<span>Now, draw an imaginary rectangular 80x60 screen at a focus distance of 50 from the camera along its line of sight.</span>
<span>To get a 2D picture, we shoot a ray from the camera through each </span>“<span>pixel</span>”<span> on the screen, note which object on the scene is hit (plan, teapot, background), and color the pixel accordingly.</span>
<span>See </span><a href="https://pbrt.org/"><span>PBRT Book</span></a><span> if you feel like falling further into this particular rabbit hole (warning: it is very deep) (I apologize for </span>“<span>little square pixels</span>”<span> simplification I use throughout the post :-) ).</span></p>
<p><span>I won</span>’<span>t focus on specific algorithms to implement that (indeed, crt is a very naive tracer), but rather highlight Hard Mode Rust specific concerns.</span></p>
</section>
<section id="Pixel-Buffer">

    <h2>
    <a href="#Pixel-Buffer"><span>Pixel Buffer</span> </a>
    </h2>
<p><span>Ultimately, the out of a ray tracer is a 2D buffer with 8bit RGB pixels.</span>
<span>One would typically represent it as follows:</span></p>

<figure>


<pre><code><span><span>pub</span> <span>struct</span> <span>Color</span> { r: <span>u8</span>, g: <span>u8</span>, b: <span>u8</span> }</span>
<span></span>
<span><span>pub</span> <span>struct</span> <span>Buf</span> {</span>
<span>  dim: [<span>u32</span>; <span>2</span>]</span>
<span>  <span>// invariant: data.len() == dim.0 * dim.1</span></span>
<span>  data: <span>Box</span>&lt;[Color]&gt;,</span>
<span>}</span></code></pre>

</figure>
<p><span>For us, we want someone else (main) to allocate that box of colors for us, so instead we do the following:</span></p>

<figure>


<pre><code><span><span>pub</span> <span>struct</span> <span>Buf</span>&lt;<span>'m</span>&gt; {</span>
<span>  dim: [<span>u32</span>; <span>2</span>],</span>
<span>  buf: &amp;<span>'m</span> <span>mut</span> [Color],</span>
<span>}</span>
<span></span>
<span><span>impl</span>&lt;<span>'m</span>&gt; Buf&lt;<span>'m</span>&gt; {</span>
<span>  <span>pub</span> <span>fn</span> <span>new</span>(dim: Idx, buf: &amp;<span>'m</span> <span>mut</span> [Color]) <span>-&gt;</span> Buf&lt;<span>'m</span>&gt; {</span>
<span>    <span>assert!</span>(dim.<span>0</span> * dim.<span>1</span> == buf.<span>len</span>() <span>as</span> <span>u32</span>);</span>
<span>    Buf { dim, buf }</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>The </span><code>'m</code><span> lifetime we use for abstract memory managed elsewhere.</span>
<span>Note how the struct grew an extra lifetime!</span>
<span>This is extra price we have to pay for not relying on RAII to cleanup resources for us:</span></p>

<figure>


<pre><code><span><span>// Easy Mode</span></span>
<span><span>fn</span> <span>paint</span>(buf: &amp;<span>mut</span> Buf) { ... }</span>
<span></span>
<span><span>struct</span> <span>PaintCtx</span>&lt;<span>'a</span>&gt; {</span>
<span>  buf: &amp;<span>'a</span> <span>mut</span> Buf</span>
<span>}</span>
<span></span>
<span><span>// Hard Mode</span></span>
<span><span>fn</span> <span>paint</span>(buf: &amp;<span>mut</span> Buf&lt;<span>'_</span>&gt;) { ... }</span>
<span></span>
<span><span>struct</span> <span>PaintCtx</span>&lt;<span>'a</span>, <span>'m</span>&gt; {</span>
<span>  buf: &amp;<span>'a</span> <span>mut</span> Buf&lt;<span>'m</span>&gt;</span>
<span>}</span></code></pre>

</figure>
<p><span>Note in particular how the </span><code>Ctx</code><span> struct now has to include two lifetimes.</span>
<span>This feels unnecessary: </span><code>'a</code><span> is shorter than </span><code>'m</code><span>.</span>
<span>I wish it was possible to somehow abstract that away:</span></p>

<figure>


<pre><code><span><span>struct</span> <span>PaintCtx</span>&lt;<span>'a</span>&gt; {</span>
<span>  buf: &amp;<span>'a</span> <span>mut</span> Buf&lt;<span>'_</span>&gt; <span>// &amp;'a mut exists&lt;'m&gt;: Buf&lt;'m&gt;</span></span>
<span>}</span></code></pre>

</figure>
<p><span>I don</span>’<span>t think that</span>’<span>s really possible (</span><a href="https://matklad.github.io/2018/05/04/encapsulating-lifetime-of-the-field.html"><span>earlier post about this</span></a><span>).</span>
<span>In particular, the following would run into variance issues:</span></p>

<figure>


<pre><code><span><span>struct</span> <span>PaintCtx</span>&lt;<span>'a</span>&gt; {</span>
<span>  buf: &amp;<span>'a</span> <span>mut</span> Buf&lt;<span>'a</span>&gt;</span>
<span>}</span></code></pre>

</figure>
<p><span>Ultimately, this is annoying, but not a deal breaker.</span></p>
<p><span>With this </span><code>rgb::Buf&lt;'_&gt;</code><span>, we can sketch the program:</span></p>

<figure>


<pre><code><span><span>// hard mode library</span></span>
<span><span>#![no_std]</span></span>
<span><span>pub</span> <span>fn</span> <span>render</span>&lt;<span>'a</span>&gt;(</span>
<span>  crt: &amp;<span>'a</span> <span>str</span>,   <span>// textual description of the scene</span></span>
<span>  mem: &amp;<span>mut</span> [<span>u8</span>], <span>// all the memory we can use</span></span>
<span>  buf: &amp;<span>mut</span> rgb::Buf, <span>// write image here</span></span>
<span>) <span>-&gt;</span> <span>Result</span>&lt;(), Error&lt;<span>'a</span>&gt;&gt; {</span>
<span>  ...</span>
<span>}</span>
<span></span>
<span><span>// main</span></span>
<span><span>#[derive(argh::FromArgs)]</span></span>
<span><span>struct</span> <span>Args</span> {</span>
<span>  <span>#[argh(option, default = <span>"64"</span>)]</span>  mem: <span>usize</span>,</span>
<span>  <span>#[argh(option, default = <span>"800"</span>)]</span> width: <span>u32</span>,</span>
<span>  <span>#[argh(option, default = <span>"600"</span>)]</span> height: <span>u32</span>,</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>main</span>() <span>-&gt;</span> anyhow::<span>Result</span>&lt;()&gt; {</span>
<span>  <span>let</span> <span>args</span>: Args = argh::<span>from_env</span>();</span>
<span></span>
<span>  <span>let</span> <span>mut </span><span>crt</span> = <span>String</span>::<span>new</span>();</span>
<span>  io::<span>stdin</span>()</span>
<span>    .<span>read_to_string</span>(&amp;<span>mut</span> crt)</span>
<span>    .<span>context</span>(<span>"reading input"</span>)?;</span>
<span></span>
<span>  <span>// Allocate all the memory.</span></span>
<span>  <span>let</span> <span>mut </span><span>mem</span> = <span>vec!</span>[<span>0</span>; args.mem * <span>1024</span>];</span>
<span></span>
<span>  <span>// Allocate the image</span></span>
<span>  <span>let</span> <span>mut </span><span>buf</span> = <span>vec!</span>[</span>
<span>    rgb::Color::<span>default</span>();</span>
<span>    (args.width * args.height) <span>as</span> <span>usize</span></span>
<span>  ];</span>
<span>  <span>let</span> <span>mut </span><span>buf</span> =</span>
<span>    rgb::Buf::<span>new</span>([args.width, args.height], &amp;<span>mut</span> buf);</span>
<span></span>
<span>  render::<span>render</span>(</span>
<span>    &amp;crt,</span>
<span>    &amp;<span>mut</span> mem,</span>
<span>    &amp;<span>mut</span> buf,</span>
<span>  )</span>
<span>  .<span>map_err</span>(|err| anyhow::format_err!(<span>"{err}"</span>))?;</span>
<span></span>
<span>  <span>// Write result as a PPM image format.</span></span>
<span>  <span>write_ppm</span>(&amp;buf, &amp;<span>mut</span> io::<span>stdout</span>().<span>lock</span>())</span>
<span>    .<span>context</span>(<span>"writing output"</span>)?;</span>
<span>  <span>Ok</span>(())</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>write_ppm</span>(</span>
<span>  buf: &amp;rgb::Buf,</span>
<span>  w: &amp;<span>mut</span> <span>dyn</span> io::Write,</span>
<span>) <span>-&gt;</span> io::<span>Result</span>&lt;()&gt; {</span>
<span>  ...</span>
<span>}</span></code></pre>

</figure>
</section>
<section id="Hard-Mode-Rayon">

    <h2>
    <a href="#Hard-Mode-Rayon"><span>Hard Mode Rayon</span> </a>
    </h2>
<p><span>Ray tracing is an embarrassingly parallel task </span>—<span> the color of each output pixel can be computed independently.</span>
<span>Usually, the excellent </span><a href="https://lib.rs/crates/rayon"><span>rayon</span></a><span> library is used to take advantage of parallelism, but for our raytracer I want to show a significantly simpler API design for taking advantage of many cores.</span>
<span>I</span>’<span>ve seen this design in </span><a href="https://github.com/sorbet/sorbet/blob/master/common/concurrency/WorkerPool.h"><span>Sorbet</span></a><span>, a type checker for Ruby.</span></p>
<p><span>Here</span>’<span>s how a </span><code>render</code><span> function with support for parallelism looks:</span></p>

<figure>


<pre><code><span><span>type</span> <span>ThreadPool</span>&lt;<span>'t</span>&gt; = <span>dyn</span> <span>Fn</span>(&amp;(<span>dyn</span> <span>Fn</span>() + <span>Sync</span>)) + <span>'t</span>;</span>
<span></span>
<span><span>pub</span> <span>fn</span> <span>render</span>&lt;<span>'a</span>&gt;(</span>
<span>  crt: &amp;<span>'a</span> <span>str</span>,</span>
<span>  mem: &amp;<span>mut</span> [<span>u8</span>],</span>
<span>  in_parallel: &amp;ThreadPool&lt;<span>'_</span>&gt;,</span>
<span>  buf: &amp;<span>mut</span> rgb::Buf&lt;<span>'_</span>&gt;,</span>
<span>) <span>-&gt;</span> <span>Result</span>&lt;(), Error&lt;<span>'a</span>&gt;&gt; {</span></code></pre>

</figure>
<p><span>The interface here is the </span><code>in_parallel</code><span> function, which takes another function as an argument and runs it, in parallel, on all available threads.</span>
<span>You typically use it like this:</span></p>

<figure>


<pre><code><span><span>let</span> <span>work</span>: ConcurrentQueue&lt;Work&gt; = ConcurrentQueue::<span>new</span>();</span>
<span>work.<span>extend</span>(available_work);</span>
<span><span>in_parallel</span>(&amp;|| {</span>
<span>  <span>while</span> <span>let</span> <span>Some</span>(item) = work.<span>pop</span>() {</span>
<span>    <span>process</span>(item);</span>
<span>  }</span>
<span>})</span></code></pre>

</figure>
<p><span>This is </span><em><span>similar</span></em><span> to a typical threadpool, but different.</span>
<span>Similar to a threadpool, there</span>’<span>s a number of threads (typically one per core) which execute arbitrary jobs.</span>
<span>The first difference is that a typical threadpool sends a job to to a single thread, while in this design the same job is broadcasted to all threads.</span>
<span>The job is </span><code>Fn + Sync</code><span> rather than </span><code>FnOnce + Send</code><span>.</span>
<span>The second difference is that we </span><em><span>block</span></em><span> until the job is done on all threads, so we can borrow data from the stack.</span></p>
<p><span>It</span>’<span>s on the caller to explicitly implement a concurrent queue to distributed specific work items.</span>
<span>In my implementation, I slice the image in rows</span></p>

<figure>


<pre><code><span><span>type</span> <span>ThreadPool</span>&lt;<span>'t</span>&gt; = <span>dyn</span> <span>Fn</span>(&amp;(<span>dyn</span> <span>Fn</span>() + <span>Sync</span>)) + <span>'t</span>;</span>
<span></span>
<span><span>pub</span> <span>fn</span> <span>render</span>&lt;<span>'a</span>&gt;(</span>
<span>  crt: &amp;<span>'a</span> <span>str</span>,</span>
<span>  mem: &amp;<span>mut</span> [<span>u8</span>],</span>
<span>  in_parallel: &amp;ThreadPool&lt;<span>'_</span>&gt;,</span>
<span>  buf: &amp;<span>mut</span> rgb::Buf&lt;<span>'_</span>&gt;,</span>
<span>) <span>-&gt;</span> <span>Result</span>&lt;(), Error&lt;<span>'a</span>&gt;&gt; {</span>
<span>  ...</span>
<span>  <span>// Note: this is not mut, because this is</span></span>
<span>  <span>// a concurrent iterator.</span></span>
<span>  <span>let</span> <span>rows</span> = buf.<span>partition</span>();</span>
<span>  <span>in_parallel</span>(&amp;|| {</span>
<span>    <span>// next_row increments an atomic and</span></span>
<span>    <span>// uses the row index to give an `&amp;mut`</span></span>
<span>    <span>// into the row's pixels.</span></span>
<span>    <span>while</span> <span>let</span> <span>Some</span>(row) = rows.<span>next_row</span>() {</span>
<span>      <span>let</span> <span>y</span>: <span>u32</span> = row.y;</span>
<span>      <span>let</span> <span>buf</span>: &amp;<span>mut</span> [rgb::Color] = row.buf;</span>
<span>      <span>for</span> <span>x</span> <span>in</span> <span>0</span>..dim[<span>0</span>] {</span>
<span>        <span>let</span> <span>color</span> = render::<span>render_pixel</span>(&amp;scene, [x, y]);</span>
<span>        buf[x <span>as</span> <span>usize</span>] = <span>to_rgb</span>(&amp;color);</span>
<span>      }</span>
<span>    }</span>
<span>  });</span>
<span>  ...</span>
<span>}</span></code></pre>

</figure>
<p><span>In </span><code>main</code><span>, we implement a concrete </span><code>ThreadPool</code><span> by spawning a thread per core:</span></p>

<figure>


<pre><code><span><span>fn</span> <span>main</span>() <span>-&gt;</span> anyhow::<span>Result</span>&lt;()&gt; {</span>
<span>  ...</span>
<span>  <span>let</span> <span>threads</span> = <span>match</span> args.jobs {</span>
<span>    <span>Some</span>(it) =&gt; Threads::<span>new</span>(it),</span>
<span>    <span>None</span> =&gt; Threads::<span>with_max_threads</span>()?,</span>
<span>  };</span>
<span>  render::<span>render</span>(</span>
<span>    &amp;crt,</span>
<span>    &amp;<span>mut</span> mem,</span>
<span>    &amp;|f| threads.<span>in_parallel</span>(f),</span>
<span>    &amp;<span>mut</span> buf,</span>
<span>  )</span>
<span>  .<span>map_err</span>(|err| anyhow::format_err!(<span>"{err}"</span>))?;</span>
<span>}</span></code></pre>

</figure>
</section>
<section id="Allocator">

    <h2>
    <a href="#Allocator"><span>Allocator</span> </a>
    </h2>
<p><span>The scenes we are going to render are fundamentally dynamically sized.</span>
<span>They can contain arbitrary number of objects.</span>
<span>So we can</span>’<span>t just statically allocate all the memory up-front.</span>
<span>Instead, there</span>’<span>s a CLI argument which sets the amount of memory a ray tracer can use, and we should either manage with that, or return an error.</span>
<span>So we do need to write our own allocator.</span>
<span>But we</span>’<span>ll try very hard to only allocate the memory we actually need, so we won</span>’<span>t have to implement memory deallocation at all.</span>
<span>So a simple bump allocator would do:</span></p>

<figure>


<pre><code><span><span>pub</span> <span>struct</span> <span>Mem</span>&lt;<span>'m</span>&gt; {</span>
<span>  raw: &amp;<span>'m</span> <span>mut</span> [<span>u8</span>],</span>
<span>}</span>
<span></span>
<span><span>#[derive(Debug)]</span></span>
<span><span>pub</span> <span>struct</span> <span>Oom</span>;</span>
<span></span>
<span><span>impl</span>&lt;<span>'m</span>&gt; Mem&lt;<span>'m</span>&gt; {</span>
<span>  <span>pub</span> <span>fn</span> <span>new</span>(raw: &amp;<span>'m</span> <span>mut</span> [<span>u8</span>]) <span>-&gt;</span> Mem&lt;<span>'m</span>&gt; {</span>
<span>    Mem { raw }</span>
<span>  }</span>
<span></span>
<span>  <span>pub</span> <span>fn</span> <span>alloc</span>&lt;T&gt;(&amp;<span>mut</span> <span>self</span>, t: T) <span>-&gt;</span> <span>Result</span>&lt;&amp;<span>'m</span> <span>mut</span> T, Oom&gt; { ... }</span>
<span></span>
<span>  <span>pub</span> <span>fn</span> <span>alloc_array</span>&lt;T&gt;(</span>
<span>    &amp;<span>mut</span> <span>self</span>,</span>
<span>    n: <span>usize</span>,</span>
<span>    <span>mut</span> element: <span>impl</span> <span>FnMut</span>(<span>usize</span>) <span>-&gt;</span> T,</span>
<span>  ) <span>-&gt;</span> <span>Result</span>&lt;&amp;<span>'m</span> <span>mut</span> [T], Oom&gt; { ... }</span>
<span></span>
<span>  <span>pub</span> <span>fn</span> <span>alloc_array_default</span>&lt;T: <span>Default</span>&gt;(</span>
<span>    &amp;<span>mut</span> <span>self</span>,</span>
<span>    n: <span>usize</span>,</span>
<span>  ) <span>-&gt;</span> <span>Result</span>&lt;&amp;<span>'m</span> <span>mut</span> [T], Oom&gt; {</span>
<span>    <span>self</span>.<span>alloc_array</span>(n, |_| T::<span>default</span>())</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>We can create an allocator from a slice of bytes, and then ask it to allocate values and arrays.</span>
<span>Schematically, </span><code>alloc</code><span> looks like this:</span></p>

<figure>


<pre><code><span><span>// PSEUDOCODE, doesn't handle alignment and is broken.</span></span>
<span><span>pub</span> <span>fn</span> <span>alloc</span>&lt;<span>'a</span>, T&gt;(</span>
<span>  &amp;<span>'a</span> <span>mut</span> <span>self</span>,</span>
<span>  val: T,</span>
<span>) <span>-&gt;</span> <span>Result</span>&lt;&amp;<span>'m</span> <span>mut</span> T, Oom&gt; {</span>
<span>  <span>let</span> <span>size</span> = mem::size_of::&lt;T&gt;();</span>
<span>  <span>if</span> <span>self</span>.raw.<span>len</span>() &lt; size {</span>
<span>    <span>// Return error if there isn't enough of memory.</span></span>
<span>    <span>return</span> <span>Err</span>(Oom);</span>
<span>  }</span>
<span></span>
<span>  <span>// Split off size_of::&lt;T&gt; bytes from the start,</span></span>
<span>  <span>// doing a little `mem::take` dance to placate</span></span>
<span>  <span>// the borrowchecker.</span></span>
<span>  <span>let</span> <span>res</span>: &amp;<span>'m</span> <span>mut</span> [<span>u8</span>] = {</span>
<span>    <span>let</span> <span>raw</span> = mem::<span>take</span>(&amp;<span>mut</span> <span>self</span>.raw);</span>
<span>    <span>let</span> (res, raw) = raw.<span>split_at_mut</span>(size);</span>
<span>    <span>self</span>.raw = raw;</span>
<span>    res</span>
<span>  }</span>
<span></span>
<span>  <span>// Initialize the value</span></span>
<span>  <span>let</span> <span>res</span> = res <span>as</span> *<span>mut</span> [<span>u8</span>] <span>as</span> *<span>mut</span> <span>u8</span> <span>as</span> *<span>mut</span> T;</span>
<span>  <span>unsafe</span> {</span>
<span>    ptr::<span>write</span>(res, val);</span>
<span>    <span>Ok</span>(&amp;<span>mut</span> *res)</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>To make this fully kosher we need to handle alignment as well, but I cut that bit out for brevity.</span></p>
<p><span>For allocating arrays, it</span>’<span>s useful if all-zeros bitpattern is a valid default instance of type, as that allows to skip element-wise initialization.</span>
<span>This condition isn</span>’<span>t easily expressible in today</span>’<span>s Rust though, so we require initializing every array member.</span></p>
<p><span>The result of an allocation is </span><code>&amp;'m T</code><span> </span>—<span> this is how we spell </span><code>Box&lt;T&gt;</code><span> on hard mode.</span></p>
</section>
<section id="Parsing">

    <h2>
    <a href="#Parsing"><span>Parsing</span> </a>
    </h2>
<p><span>The scene contains various objects, like spheres and planes:</span></p>

<figure>


<pre><code><span><span>pub</span> <span>struct</span> <span>Sphere</span> {</span>
<span>  <span>pub</span> center: v64, <span>// v64 is [f64; 3]</span></span>
<span>  <span>pub</span> radius: <span>f64</span>,</span>
<span>}</span>
<span></span>
<span><span>pub</span> <span>struct</span> <span>Plane</span> {</span>
<span>  <span>pub</span> origin: v64,</span>
<span>  <span>pub</span> normal: v64,</span>
<span>}</span></code></pre>

</figure>
<p><span>Usually, we</span>’<span>d represent a scene as</span></p>

<figure>


<pre><code><span><span>pub</span> <span>struct</span> <span>Scene</span> {</span>
<span>  <span>pub</span> camera: Camera,</span>
<span>  <span>pub</span> spheres: <span>Vec</span>&lt;Sphere&gt;,</span>
<span>  <span>pub</span> planes: <span>Vec</span>&lt;Plane&gt;,</span>
<span>}</span></code></pre>

</figure>
<p><span>We </span><em><span>could</span></em><span> implement a resizable array (</span><code>Vec</code><span>), but doing that would require us to either leak memory, or to implement proper deallocation logic in our allocator, and add destructors to reliably trigger that.</span>
<span>But destructors is exactly something we are trying to avoid in this exercise.</span>
<span>So our scene will have to look like this instead:</span></p>

<figure>


<pre><code><span><span>pub</span> <span>struct</span> <span>Scene</span>&lt;<span>'m</span>&gt; {</span>
<span>  <span>pub</span> camera: Camera,</span>
<span>  <span>pub</span> spheres: &amp;<span>'m</span> <span>mut</span> [Sphere],</span>
<span>  <span>pub</span> planes: &amp;<span>'m</span> <span>mut</span> [Plane],</span>
<span>}</span></code></pre>

</figure>
<p><span>And that means we want to know the number of objects we</span>’<span>ll need upfront.</span>
<span>The way we solve this problem is by doing two-pass parsing.</span>
<span>In the first pass, we just count things, then we allocate them, then we actually parse them into allocated space.</span></p>

<figure>


<pre><code><span><span>pub</span>(<span>crate</span>) <span>fn</span> <span>parse</span>&lt;<span>'m</span>, <span>'i</span>&gt;(</span>
<span>  mem: &amp;<span>mut</span> Mem&lt;<span>'m</span>&gt;,</span>
<span>  input: &amp;<span>'i</span> <span>str</span>,</span>
<span>) <span>-&gt;</span> <span>Result</span>&lt;Scene&lt;<span>'m</span>&gt;, Error&lt;<span>'i</span>&gt;&gt; {</span>
<span>  <span>// Size the allocations.</span></span>
<span>  <span>let</span> <span>mut </span><span>n_spheres</span> = <span>0</span>;</span>
<span>  <span>let</span> <span>mut </span><span>n_planes</span> = <span>0</span>;</span>
<span>  <span>for</span> <span>word</span> <span>in</span> input.<span>split_ascii_whitespace</span>() {</span>
<span>    <span>match</span> word {</span>
<span>      <span>"sphere"</span> =&gt; n_spheres += <span>1</span>,</span>
<span>      <span>"plane"</span> =&gt; n_planes += <span>1</span>,</span>
<span>      _ =&gt; (),</span>
<span>    }</span>
<span>  }</span>
<span></span>
<span>  <span>// Allocate.</span></span>
<span>  <span>let</span> <span>mut </span><span>res</span> = Scene {</span>
<span>    camera: <span>Default</span>::<span>default</span>(),</span>
<span>    spheres: mem.<span>alloc_array_default</span>(n_spheres)?</span>
<span>    planes: mem.<span>alloc_array_default</span>(n_planes)?,</span>
<span>  };</span>
<span></span>
<span>  <span>// Parse _into_ the allocated scene.</span></span>
<span>  <span>let</span> <span>mut </span><span>p</span> = Parser::<span>new</span>(mem, input);</span>
<span>  <span>scene</span>(&amp;<span>mut</span> p, &amp;<span>mut</span> res)?;</span>
<span>  <span>Ok</span>(res)</span>
<span>}</span></code></pre>

</figure>
<p><span>If an error is encountered during parsing, we want to create a helpful error message.</span>
<span>If the message is fully dynamic, we</span>’<span>d have to allocate it </span><em><span>into</span></em><span> </span><code>'m</code><span>, but it seems simpler to just re-use bits of input for error message.</span>
<span>Hence, </span><code>Error&lt;'i&gt;</code><span> is tied to the input lifetime </span><code>'i</code><span>, rather memory lifetime </span><code>'m</code><span>.</span></p>
</section>
<section id="Nested-Objects">

    <h2>
    <a href="#Nested-Objects"><span>Nested Objects</span> </a>
    </h2>
<p><span>One interesting type of object on the scene is a mesh of triangles (for example, the teapot is just a bunch of triangles).</span>
<span>A naive way to represent a bunch of triangles is to use a vector:</span></p>

<figure>


<pre><code><span><span>pub</span> <span>struct</span> <span>Triangle</span> {</span>
<span>  <span>pub</span> a: v64,</span>
<span>  <span>pub</span> b: v64,</span>
<span>  <span>pub</span> c: v64,</span>
<span>}</span>
<span></span>
<span><span>type</span> <span>Mesh</span> = <span>Vec</span>&lt;Triangle&gt;;</span></code></pre>

</figure>
<p><span>This is wasteful: in a mesh, each edge is shared by two triangles.</span>
<span>So a single vertex belongs to a bunch of triangles.</span>
<span>If we store a vector of triangles, we are needlessly duplicating vertex data.</span>
<span>A more compact representation is to store unique vertexes once, and to use indexes for sharing:</span></p>

<figure>


<pre><code><span><span>pub</span> <span>struct</span> <span>Mesh</span> {</span>
<span>  <span>pub</span> vertexes: <span>Vec</span>&lt;v64&gt;,</span>
<span>  <span>pub</span> faces: <span>Vec</span>&lt;MeshFace&gt;,</span>
<span>}</span>
<span><span>// Indexes point into vertexes vector.</span></span>
<span><span>pub</span> <span>struct</span> <span>MeshFace</span> { a: <span>u32</span>, b: <span>u32</span>, c: <span>u32</span> }</span></code></pre>

</figure>
<p><span>Again, on hard mode that would be</span></p>

<figure>


<pre><code><span><span>pub</span> <span>struct</span> <span>Mesh</span>&lt;<span>'m</span>&gt; {</span>
<span>  <span>pub</span> vertexes: &amp;<span>'m</span> <span>mut</span> [v64],</span>
<span>  <span>pub</span> faces: &amp;<span>'m</span> <span>mut</span> [MeshFace],</span>
<span>}</span></code></pre>

</figure>
<p><span>And a scene contains a bunch of meshes :</span></p>

<figure>


<pre><code><span><span>pub</span> <span>struct</span> <span>Scene</span>&lt;<span>'m</span>&gt; {</span>
<span>  <span>pub</span> camera: Camera,</span>
<span>  <span>pub</span> spheres: &amp;<span>'m</span> <span>mut</span> [Sphere],</span>
<span>  <span>pub</span> planes: &amp;<span>'m</span> <span>mut</span> [Plane],</span>
<span>  <span>pub</span> meshes: &amp;<span>'m</span> <span>mut</span> [Mesh&lt;<span>'m</span>&gt;],</span>
<span>}</span></code></pre>

</figure>
<p><span>Note how, if the structure is recursive, we have </span>“<span>owned pointers</span>”<span> of </span><code>&amp;'m mut T&lt;'m&gt;</code><span> shape.</span>
<span>Originally I worried that that would cause problem with variance, but it seems to work fine for ownership specifically.</span>
<span>During processing, you still need </span><code>&amp;'a mut T&lt;'m&gt;</code><span> though.</span></p>
<p><span>And that</span>’<span>s why parsing functions hold an uncomfortable bunch of lifetimes:</span></p>

<figure>


<pre><code><span><span>fn</span> <span>mesh</span>&lt;<span>'m</span>, <span>'i</span>&gt;(</span>
<span>  p: &amp;<span>mut</span> Parser&lt;<span>'m</span>, <span>'i</span>, <span>'_</span>&gt;,</span>
<span>  res: &amp;<span>mut</span> Mesh&lt;<span>'m</span>&gt;,</span>
<span>) <span>-&gt;</span> <span>Result</span>&lt;(), Error&lt;<span>'i</span>&gt;&gt; { ... }</span></code></pre>

</figure>
<p><span>The parser </span><code>p</code><span> holds </span><code>&amp;'i str</code><span> input and a </span><code>&amp;'a mut Mem&lt;'m&gt;</code><span> memory.</span>
<span>It parses input </span><em><span>into</span></em><span> a </span><code>&amp;'b mut Mesh&lt;'m&gt;</code><span>.</span></p>
</section>
<section id="Bounding-Volume-Hierarchy">

    <h2>
    <a href="#Bounding-Volume-Hierarchy"><span>Bounding Volume Hierarchy</span> </a>
    </h2>
<p><span>With </span><code>Scene&lt;'m&gt;</code><span> fully parsed, we can finally get to rendering the picture.</span>
<span>A naive way to do this would be to iterate through each pixel, shooting a ray through it, and then do a nested iterations over every shape, looking for the closest intersection.</span>
<span>That</span>’<span>s going to be slow!</span>
<span>The teapot model contains about 1k triangles, and we have 640*480 pixels, which gives us 307</span><span>_200</span><span>_000 ray-triangle intersection tests, which is quite slow even with multithreading.</span></p>
<p><span>So we are going to speed this up.</span>
<span>The idea is simple </span>—<span> just don</span>’<span>t intersect a ray with each triangle.</span>
<span>It is possible to quickly discard batches of triangles.</span>
<span>If we have a  batch of triangles, we can draw a 3D box around them as a pre-processing step.</span>
<span>Now if the ray doesn</span>’<span>t intersect the bounding box, we know that it can</span>’<span>t intersect any of the triangles.</span>
<span>So we can use one test with a bounding box instead of many tests for each triangle.</span></p>
<p><span>This is of course one-sided </span>—<span> if the ray intersects the box, it might still miss all of the triangles.</span>
<span>But, if we place bounding boxes smartly (small boxes which cover many adjacent triangles), we can hope to skip a lot of work.</span></p>
<p><span>We won</span>’<span>t go for really smart ways of doing that, and instead will use a simple divide-and-conquer scheme.</span>
<span>Specifically, we</span>’<span>ll draw a large box around all triangles we have.</span>
<span>Then, we</span>’<span>ll note which dimension of the resulting box is the longest.</span>
<span>If, for example, the box is very tall, we</span>’<span>ll cut it in half horizontally, such that each half contains half of the triangles.</span>
<span>Then, we</span>’<span>ll recursively subdivide the two halves.</span></p>
<p><span>In the end, we get a binary tree, where each node contains a bounding box and two children, whose bounding boxes are contained in the parent</span>’<span>s bounding box.</span>
<span>Leaves contains triangles.</span>
<span>This construction is called a bounding volume hierarchy, bvh.</span></p>
<p><span>To intersect the ray with bvh, we use a recursive procedure.</span>
<span>Starting at the root node, we descend into children whose bounding boxes are intersected by the ray.</span>
<span>Sometimes we</span>’<span>ll have to descend into both children, but often enough at least one child</span>’<span>s bounding box won</span>’<span>t touch the ray, allowing us to completely skip the subtree.</span></p>
<p><span>On easy mode Rust, we can code it like this:</span></p>

<figure>


<pre><code><span><span>struct</span> <span>BoundingBox</span> {</span>
<span>  <span>// Opposite corners of the box.</span></span>
<span>  lo: v64, hi: v64,</span>
<span>}</span>
<span></span>
<span><span>struct</span> <span>Bvh</span> {</span>
<span>  root: BvhNode</span>
<span>}</span>
<span></span>
<span><span>enum</span> <span>BvhNode</span> {</span>
<span>  Split {</span>
<span>    bb: BoundingBox,</span>
<span>    children: [<span>Box</span>&lt;BvhNode&gt;; <span>2</span>],</span>
<span>    <span>/// Which of X,Y,Z dimensions was used</span></span>
<span>    <span>// to cut the bb in two.</span></span>
<span>    axis: <span>u8</span>,</span>
<span>  }</span>
<span>  Leaf {</span>
<span>    bb: BoundingBox,</span>
<span>    <span>/// Index of the triangle in a mesh.</span></span>
<span>    triangle: <span>u32</span>,</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>On hard mode, we don</span>’<span>t really love all those separate boxes, we love arrays!</span>
<span>So what we</span>’<span>d rather have is</span></p>

<figure>


<pre><code><span><span>pub</span> <span>struct</span> <span>Bvh</span>&lt;<span>'m</span>&gt; {</span>
<span>  splits: &amp;<span>'m</span> <span>mut</span> [BvhSplit],</span>
<span>  leaves: &amp;<span>'m</span> <span>mut</span> [BvhLeaf],</span>
<span>}</span>
<span></span>
<span><span>struct</span> <span>BvhSplit</span> {</span>
<span>  <span>/// Index into either splits or leaves.</span></span>
<span>  <span>/// The `tag` is in the highest bit.</span></span>
<span>  children: [<span>u32</span>; <span>2</span>],</span>
<span>  bb: BoundingBox,</span>
<span>  axis: <span>u8</span>,</span>
<span>}</span>
<span></span>
<span><span>struct</span> <span>BvhLeaf</span> {</span>
<span>  face: <span>u32</span>,</span>
<span>  bb: BoundingBox,</span>
<span>}</span></code></pre>

</figure>
<p><span>So we want to write the following function which recursively constructs a bvh for a mesh:</span></p>

<figure>


<pre><code><span><span>pub</span> <span>fn</span> <span>build</span>(</span>
<span>  mem: &amp;<span>mut</span> Mem&lt;<span>'m</span>&gt;,</span>
<span>  mesh: &amp;Mesh&lt;<span>'m</span>&gt;,</span>
<span>) <span>-&gt;</span> <span>Result</span>&lt;Bvh&lt;<span>'m</span>&gt;, Oom&gt; { ... }</span></code></pre>

</figure>
<p><span>The problem is, unlike the parser, we can</span>’<span>t cheaply determine the number of leaves and splits without actually building the whole tree.</span></p>
</section>
<section id="Scratch-Space">

    <h2>
    <a href="#Scratch-Space"><span>Scratch Space</span> </a>
    </h2>
<p><span>So what we are going to do here is to allocate a pointer-tree structure into some scratch space, and then copy that into an </span><code>&amp;'m mut</code><span> array.</span>
<span>How do we find the scratch space?</span>
<span>Our memory is </span><code>&amp;'m [u8]</code><span>.</span>
<span>We allocate stuff from the start of the region.</span>
<span>So we can split of some amount of scratch space from the end:</span></p>

<figure>


<pre><code><span>&amp;<span>'m</span> <span>mut</span> [<span>u8</span>] <span>-&gt;</span> (&amp;<span>'m</span> <span>mut</span> [<span>u8</span>], &amp;<span>'s</span> <span>mut</span> [<span>u8</span>])</span></code></pre>

</figure>
<p><span>Stuff we allocate into the first half is allocated </span>“<span>permanently</span>”<span>.</span>
<span>Stuff we allocate into the second half is allocated temporarily.</span>
<span>When we drop temp buffer, we can reclaim all that space.</span></p>
<p><span>This</span>…<span> probably is the most sketchy part of the whole endeavor.</span>
<span>It is </span><code>unsafe</code><span>, requires lifetimes casing, and I actually can</span>’<span>t get it past miri.</span>
<span>But it should be fine, right?</span></p>
<p><span>So, I have the following thing API:</span></p>

<figure>


<pre><code><span><span>impl</span> <span>Mem</span>&lt;<span>'m</span>&gt; {</span>
<span>  <span>pub</span> <span>fn</span> <span>with_scratch</span>&lt;T&gt;(</span>
<span>    &amp;<span>mut</span> <span>self</span>,</span>
<span>    size: <span>usize</span>,</span>
<span>    f: <span>impl</span> <span>FnOnce</span>(&amp;<span>mut</span> Mem&lt;<span>'m</span>&gt;, &amp;<span>mut</span> Mem&lt;<span>'_</span>&gt;) <span>-&gt;</span> T,</span>
<span>  ) <span>-&gt;</span> T { ... }</span>
<span>}</span></code></pre>

</figure>
<p><span>It can be used like this:</span></p>

<figure>


<pre><code><span><span>#[test]</span></span>
<span><span>fn</span> <span>test_scratch</span>() {</span>
<span>  <span>let</span> <span>mut </span><span>buf</span> = [<span>0u8</span>; <span>4</span>];</span>
<span>  <span>let</span> <span>mut </span><span>mem</span> = Mem::<span>new</span>(&amp;<span>mut</span> buf);</span>
<span></span>
<span>  <span>let</span> <span>x</span> = mem.<span>alloc</span>(<span>0u8</span>).<span>unwrap</span>();</span>
<span>  <span>let</span> <span>y</span> = mem.<span>with_scratch</span>(<span>2</span>, |mem, scratch| {</span>
<span>    <span>// Here, we can allocate _permanent_ stuff from `mem`,</span></span>
<span>    <span>// and temporary stuff from `scratch`.</span></span>
<span>    <span>// Only permanent stuff can escape.</span></span>
<span></span>
<span>    <span>let</span> <span>y</span> = mem.<span>alloc</span>(<span>1u8</span>).<span>unwrap</span>();</span>
<span>    <span>let</span> <span>z</span> = scratch.<span>alloc</span>(<span>2u8</span>).<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>((*x, *y, *z), (<span>0</span>, <span>1</span>, <span>2</span>));</span>
<span></span>
<span>    <span>// The rest of memory is occupied by scratch.</span></span>
<span>    <span>assert!</span>(mem.<span>alloc</span>(<span>0u8</span>).<span>is_err</span>());</span>
<span></span>
<span>    y <span>// Returning z here fails.</span></span>
<span>  });</span>
<span></span>
<span>  <span>// The scratch memory is now reclaimed.</span></span>
<span>  <span>let</span> <span>z</span> = mem.<span>alloc</span>(<span>3u8</span>).<span>unwrap</span>();</span>
<span>  <span>assert_eq!</span>((*x, *y, *z), (<span>0</span>, <span>1</span>, <span>3</span>));</span>
<span>  <span>assert_eq!</span>(buf, [<span>0</span>, <span>1</span>, <span>3</span>, <span>0</span>]);</span>
<span>  <span>// Will fail to compile.</span></span>
<span>  <span>// assert_eq!(*x, 0);</span></span>
<span>}</span></code></pre>

</figure>
<p><span>And here</span>’<span>s how </span><code>with_scratch</code><span> implemented:</span></p>

<figure>


<pre><code><span><span>pub</span> <span>fn</span> <span>with_scratch</span>&lt;T&gt;(</span>
<span>  &amp;<span>mut</span> <span>self</span>,</span>
<span>  size: <span>usize</span>,</span>
<span>  f: <span>impl</span> <span>FnOnce</span>(&amp;<span>mut</span> Mem&lt;<span>'m</span>&gt;, &amp;<span>mut</span> Mem&lt;<span>'_</span>&gt;) <span>-&gt;</span> T,</span>
<span>) <span>-&gt;</span> T {</span>
<span>  <span>let</span> <span>raw</span> = mem::<span>take</span>(&amp;<span>mut</span> <span>self</span>.raw);</span>
<span></span>
<span>  <span>// Split off scratch space.</span></span>
<span>  <span>let</span> <span>mid</span> = raw.<span>len</span>() - size;</span>
<span>  <span>let</span> (mem, scratch) = raw.<span>split_at_mut</span>(mid);</span>
<span></span>
<span>  <span>self</span>.raw = mem;</span>
<span>  <span>let</span> <span>res</span> = <span>f</span>(<span>self</span>, &amp;<span>mut</span> Mem::<span>new</span>(scratch));</span>
<span></span>
<span>  <span>let</span> <span>data</span> = <span>self</span>.raw.<span>as_mut_ptr</span>();</span>
<span>  <span>// Glue the scratch space back in.</span></span>
<span>  <span>let</span> <span>len</span> = <span>self</span>.raw.<span>len</span>() + size;</span>
<span>  <span>// This makes miri unhappy, any suggestions? :(</span></span>
<span>  <span>self</span>.raw = <span>unsafe</span> { slice::<span>from_raw_parts_mut</span>(data, len) };</span>
<span>  res</span>
<span>}</span></code></pre>

</figure>
<p><span>With this infrastructure in place, we can finally implement bvh construction!</span>
<span>We</span>’<span>ll do it in three steps:</span></p>
<ol>
<li>
<span>Split of half the memory into a scratch space.</span>
</li>
<li>
<span>Build a dynamically-sized tree in that space, counting leaves and interior nodes.</span>
</li>
<li>
<span>Allocate arrays of the right size in the permanent space, and copy data over once.</span>
</li>
</ol>

<figure>


<pre><code><span><span>pub</span> <span>struct</span> <span>Bvh</span>&lt;<span>'m</span>&gt; {</span>
<span>  splits: &amp;<span>'m</span> <span>mut</span> [BvhSplit],</span>
<span>  leaves: &amp;<span>'m</span> <span>mut</span> [BvhLeaf],</span>
<span>}</span>
<span></span>
<span><span>struct</span> <span>BvhSplit</span> {</span>
<span>  children: [<span>u32</span>; <span>2</span>],</span>
<span>  bb: BoundingBox,</span>
<span>  axis: <span>u8</span>,</span>
<span>}</span>
<span></span>
<span><span>struct</span> <span>BvhLeaf</span> {</span>
<span>  face: <span>u32</span>,</span>
<span>  bb: BoundingBox,</span>
<span>}</span>
<span></span>
<span><span>// Temporary tree we store in the scratch space.</span></span>
<span><span>enum</span> <span>Node</span>&lt;<span>'s</span>&gt; {</span>
<span>  Split {</span>
<span>    children: [&amp;<span>'s</span> <span>mut</span> Node&lt;<span>'s</span>&gt;; <span>2</span>],</span>
<span>    bb: BoundingBox,</span>
<span>    axis: <span>u8</span></span>
<span>  },</span>
<span>  Leaf { face: <span>u32</span>, bb: BoundingBox },</span>
<span>}</span>
<span></span>
<span><span>pub</span> <span>fn</span> <span>build</span>(</span>
<span>  mem: &amp;<span>mut</span> Mem&lt;<span>'m</span>&gt;,</span>
<span>  mesh: &amp;Mesh&lt;<span>'m</span>&gt;,</span>
<span>) <span>-&gt;</span> <span>Result</span>&lt;Bvh&lt;<span>'m</span>&gt;, Oom&gt; {</span>
<span>  <span>let</span> <span>free_mem</span> = mem.<span>free</span>();</span>
<span>  mem.<span>with_scratch</span>(free_mem / <span>2</span>, |mem, scratch| {</span>
<span>    <span>let</span> (node, n_splits, n_leaves) =</span>
<span>      <span>build_scratch</span>(scratch, mesh);</span>
<span></span>
<span>    <span>let</span> <span>mut </span><span>res</span> = Bvh {</span>
<span>      splits: mem.<span>alloc_array_default</span>(n_splits <span>as</span> <span>usize</span>)?,</span>
<span>      leaves: mem.<span>alloc_array_default</span>(n_leaves <span>as</span> <span>usize</span>)?,</span>
<span>    };</span>
<span>    <span>copy</span>(&amp;<span>mut</span> res, &amp;node);</span>
<span></span>
<span>    <span>Ok</span>(res)</span>
<span>  })</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>build_scratch</span>&lt;<span>'s</span>&gt;(</span>
<span>  mem: &amp;<span>mut</span> Mem&lt;<span>'s</span>&gt;,</span>
<span>  mesh: &amp;Mesh&lt;<span>'_</span>&gt;,</span>
<span>) <span>-&gt;</span> <span>Result</span>&lt;(&amp;<span>'s</span> <span>mut</span> Node&lt;<span>'s</span>&gt;, <span>usize</span>, <span>usize</span>), Oom&gt; {</span>
<span>  ...</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>copy</span>&lt;<span>'m</span>, <span>'s</span>&gt;(res: &amp;<span>mut</span> Bvh&lt;<span>'m</span>&gt;, node: &amp;Node&lt;<span>'s</span>&gt;) {</span>
<span>  ...</span>
<span>}</span></code></pre>

</figure>
<p><span>And that</span>’<span>s it!</span>
<span>The thing actually works, miri complaints notwithstanding!</span></p>
</section>
<section id="Conclusions">

    <h2>
    <a href="#Conclusions"><span>Conclusions</span> </a>
    </h2>
<p><span>Actually, I am impressed.</span>
<span>I was certain that this won</span>’<span>t actually work out, and that I</span>’<span>d have to write copious amount of unsafe to get the runtime behavior I want.</span>
<span>Specifically, I believed that </span><code>&amp;'m mut T&lt;'m&gt;</code><span> variance issue would force my hand to add </span><code>'m</code><span>, </span><code>'mm</code><span>, </span><code>'mmm</code><span> and further lifetimes, but that didn</span>’<span>t happen.</span>
<span>For </span>“<span>owning</span>”<span> pointers, </span><code>&amp;'m mut T&lt;'m&gt;</code><span> turned out to work fine!</span>
<span>It</span>’<span>s only when processing you might need extra lifetimes.</span>
<code>Parser&lt;'m, 'i, 'a&gt;</code><span> is at least two lifetimes more than I am completely comfortable with, but I guess I can live with that.</span></p>
<p><span>I wonder how far this style of programming can be pushed.</span>
<span>Aesthetically, I quite like that I can tell precisely how much memory the program would use!</span></p>
<p><span>Code for the post: </span><a href="http://github.com/matklad/crt">http://github.com/matklad/crt</a><span>.</span></p>
<p><span>Discussion on </span><a href="https://old.reddit.com/r/rust/comments/xx7xci/blog_post_hard_mode_rust/"><span>/r/rust</span></a><span>.</span></p>
</section>
</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Commercial jet collides with Black Hawk helicopter near Reagan airport (529 pts)]]></title>
            <link>https://www.mediaite.com/news/breaking-commercial-jet-collides-with-police-chopper-near-reagan-airport/</link>
            <guid>42874301</guid>
            <pubDate>Thu, 30 Jan 2025 02:56:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mediaite.com/news/breaking-commercial-jet-collides-with-police-chopper-near-reagan-airport/">https://www.mediaite.com/news/breaking-commercial-jet-collides-with-police-chopper-near-reagan-airport/</a>, See on <a href="https://news.ycombinator.com/item?id=42874301">Hacker News</a></p>
Couldn't get https://www.mediaite.com/news/breaking-commercial-jet-collides-with-police-chopper-near-reagan-airport/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Decompiling 2024: A Year of Resurgance in Decompilation Research (104 pts)]]></title>
            <link>https://mahaloz.re/dec-progress-2024</link>
            <guid>42873825</guid>
            <pubDate>Thu, 30 Jan 2025 01:48:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mahaloz.re/dec-progress-2024">https://mahaloz.re/dec-progress-2024</a>, See on <a href="https://news.ycombinator.com/item?id=42873825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      

<h3>TL;DR</h3>
The year 2024 was a resurgent year for decompilation. Academic publications from that year made up nearly 30% of all top publications ever made in decompilation. In this post, I do a summarization and retrospective of both the academic and ideological progress of decompilation in 2024. Hint: decompilation research is back. 
<hr>

<br>


<h2 id="looking-back-on-2024">Looking Back on 2024</h2>
<p>Last year, I kicked off 2024 by paying tribute to the <a href="https://mahaloz.re/dec-history-pt1">last 30 years of decompilation research</a>, where I summarized work in <a href="https://mahaloz.re/dec-history-pt2">control flow structuring</a>.
As 2025 has come along, it only feels right to do a similar post, but instead on a tighter scope of the field’s growth in 2024 alone.
Though this field is still small, it feels like it is growing rapidly.
2024 featured a talk tour by the decompilation pioneer <a href="https://x.com/criscifuentes?lang=en">Dr. Cristina Cifuentes</a>, a <a href="https://cfp.recon.cx/recon2024/talk/NAPZWM/">world-renowned decompilation panel</a> at Recon 2024, and the highest top-tier publication record in history for decompilation.</p>

<p>For me, 2024 was filled with lots of travel and new colleagues.
Many of these new colleagues are decompilation practitioners, rising experts, and even bonafide field pioneers.
They reign from academia and industry alike, and many have their own vision for where they believe this field is going.
However, there is a unifying trend in <em>vibes</em>: decompilation research is back.</p>

<h2 id="a-surge-in-academic-work">A Surge in Academic Work</h2>
<p>Academia, publications, and <em>graphs</em> don’t always tell the whole truth about a field, but it’s a great place to start.
The graph below shows the number of decompilation works published in academic venues since 1993 (<a href="https://clei.org/proceedings_data/CLEI1993/TOMO%201/Tomo%201%20por%20articulo_OCR/CLEI%201993-tomo%201_257-266_OCR.pdf">the earliest academic work</a>).</p>

<p><img src="https://mahaloz.re/assets/images/dec-recap-24/pubs_per_year.png" alt=""></p>

<p>The graph data is sourced from my <a href="https://docs.google.com/spreadsheets/d/13QUqON6cwNADk-2E1hwiKxXeCd0ESXUmkA9_dweCESM/edit?usp=sharing">list of decompilation works</a> made for the <a href="https://decompilation.wiki/">Decompilation Wiki</a>.
It includes nearly every paper from software engineering, programming languages, and security conferences published from 1970 onwards.
The papers graphed above are exclusively works in decompilation, as opposed to works in compilation that contributed to decompilation (see sheet for more info).</p>

<p>Starting in 2011, the total number of top-tier published papers (as defined by <a href="https://csrankings.org/">csrankings</a>) was <strong>25</strong>.
Amazingly, <strong>30%</strong> of all top-tier work was published in 2024, 8 papers in total.
It’s hard to claim line trends with such a small dataset (usually one paper a year), but 2024 is clearly out of place.
2024 more than doubled the total top publications in 2022, which had also doubled 2020.</p>

<p>Interestingly, we also see a trend of the top vs low-tier publications for academics.
Decompilation has traditionally been saturated in non-top-tier conferences.
It’s hard to know <em>why</em>, but I speculate it’s due to decompilations’ difficulty to publish in top conferences.
However, we’ve recently seen top publications close to or exceed non-top publications in quantity.</p>

<p>This data indicates two things to me:</p>
<ol>
  <li><strong>Decompilation is now mature enough to publish in top-tier</strong></li>
  <li><strong>Decompilation is growing rapidly</strong></li>
</ol>

<p>This is great news, but is field health just about quantity? What about quality?
Well, let’s take a closer look at three emerging trends from the <strong>eight works</strong> published in 2024.</p>

<h2 id="trend-1-defining-good-decompilation">Trend 1: Defining “Good” Decompilation</h2>
<p>Often, decompiler enthusiasts argue about what the “best” decompilation looks like, primairly with no good answer.
Luckily, <strong>four of the eight</strong> new works measured and defined what good decompilation should look like.
These studies were both automated and human-driven and often defined new ways to improve decompilation for the future.
Take, for instance, the work from <a href="https://se-phd.s3d.cmu.edu/People/students/student-bios/dramko-luke.html">Dramko et al.</a> on studying <a href="https://www.usenix.org/conference/usenixsecurity24/presentation/dramko">“Decompiler Fidelity Issues”</a>, pictured below.</p>

<p><img src="https://mahaloz.re/assets/images/dec-recap-24/dramko_paper.png" alt=""></p>

<p>His work took the gracious time to define what exactly is wrong with decompilation manually.
To the savvy decompiler user, this may seem like a grouping of well-known problems.
However, this work establishes a precedent: <strong>good decompilation looks like its source</strong>.
Interestingly, this work’s claim coincides with work published simultaneously, <a href="https://www.usenix.org/system/files/sec23winter-prepub-301-basque.pdf">SAILR</a> (my work).
SAILR, extended this manual effort by introducing an automated algorithm for <a href="https://github.com/mahaloz/sailr-eval">finding mismatches in source and decompilation</a> and improving those locations, which may find itself <a href="https://github.com/NationalSecurityAgency/ghidra/issues/6133">in Ghidra soon</a>.
Both work towards making decompilation like source.</p>

<p>On the other end of the spectrum, one work, <a href="https://dl.acm.org/doi/10.1145/3643744">R2I</a>, defined good decompilation as being <em>simple</em>, much like previous work.
They focus on finding the best decompilation by <a href="https://dogbolt.org/">comparing decompilers</a> against each other.
Finally, the last work defined good decompilation as being semantically accurate.
The work <a href="https://www.usenix.org/system/files/usenixsecurity24-zou.pdf">D-Helix</a> used <a href="https://en.wikipedia.org/wiki/Symbolic_execution">symbolic execution</a> to verify if decompilation was like its source, finding actual bugs in decompilers.</p>

<p>To keep count, <strong>2</strong> vote source code recovery, <strong>1</strong> vote simple code recovery, and 1 vote <em>semantically</em> correct.
These works open the door for continued development and publication in decompilation research.</p>

<h2 id="trend-2-aixdecompilation-is-here-to-stay">Trend 2: AIxDecompilation is Here to Stay</h2>
<p>If you’ve ever talked to me in person, you’d know that I’m a disbeliever of AI replacing decompilers any time soon.
However, 2024 research has made it clear that AI has a real place in improving <em>specific</em> tasks in decompilation.
There were <strong>four works utilizing AI in 2024</strong>, working on symbol prediction, type prediction, and overall code simplification.</p>

<p>LLMs made their debut in decompilation in two works: <a href="https://www.ndss-symposium.org/wp-content/uploads/2024-401-paper.pdf">DeGPT</a> and <a href="https://www.cs.purdue.edu/homes/lintan/publications/resym-ccs24.pdf">ReSym</a>.
Both utilized LLMs to recover variable names, structures, types, and, in the case of DeGPT, code simplification (though very limited).
Other works used more traditional models to do singular tasks.
<a href="https://adamdoupe.com/publications/varbert-oakland2024.pdf">VarBERT</a> utilized a <a href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT</a> model to predict variable names, while <a href="https://www.usenix.org/system/files/usenixsecurity24-zhu-chang.pdf">TyGr</a> utilized a <a href="https://en.wikipedia.org/wiki/Graph_neural_network">GNN</a> to predict structures and variable types.</p>

<p>Here are some of the inspiring results of structure recovery from AI works (ReSym shown below):</p>

<p><img src="https://mahaloz.re/assets/images/dec-recap-24/resym_example.png" alt=""></p>

<p>These works resulted in a strict improvement over traditional decompilation, which almost always lacks types and symbols.
It’ll be interesting to see if these approaches can improve with bigger models (<em>coughs in o1</em>) and how much of the <a href="https://decompilation.wiki/fundamentals/overview/">fundamental</a> components in decompilers can be replaced.
I think it will only get better from here.</p>

<h2 id="trend-3-theory-comes-with-application">Trend 3: Theory comes with Application</h2>
<p>I often feel that industry looks down on academia because we frequently have unusable inventions and theories.
Being a mainly applied science, decompilation <em>must</em> be the exception to these expectations, and luckily, it was in 2024.</p>

<p>In all but 1, each 2024 paper came with an open-source implementation.
Many of those implementations had <a href="https://github.com/mahaloz/angr-sailr">working</a> <a href="https://github.com/binsync/varbert_api">tools</a> <a href="https://github.com/lt-asset/resym">that</a> <a href="https://github.com/purseclab/D-helix">could</a> <a href="https://github.com/PeiweiHu/DeGPT">be</a> <a href="https://github.com/sefcom/TYGR">integrated</a> <a href="https://github.com/e0mh4/R2I">into</a> real-world decompilers and techniques.
Considering that back in 2011, none of the decompilation works were open-source, this is a massive step in the right direction.</p>

<p>This trend allows the industry to gain usefulness from work in decompilation research more rapidly, making it more valuable.
Additionally, it makes it easier for new researchers to quickly understand flaws in previous work and look for new directions.
I encourage others to make usable and open-source implementations of their epic works so we can all nerd out together about them.</p>

<h2 id="other-researchers-and-conferences">Other Researchers and Conferences</h2>

<p>Aside from direct academic publishing, many other things flourished this year for decompilation.
There were <a href="https://hex-rays.com/blog/discover-ida-9.0-exciting-new-features-and-improvements">big updates</a> for big decompilers, <a href="https://github.com/angr/angr/pull/4695">fundamental changes</a> for smaller ones, new industry <a href="https://blog.reveng.ai/training-an-llm-to-decompile-assembly-code/">approaches in full-AI decompilers</a>, and some great talks across the cons.
I’d also like to acknowledge all the unpublished researchers who showed me really cool things in 2024, like a DSL for decompilation.</p>

<p>For most of 2024, Dr. Cristina Cifuentes (the academic pioneer of decompilers) toured the world, <a href="https://www.youtube.com/watch?app=desktop&amp;v=wo3xEa2elp4&amp;list=PLUzWZANghr3bfJ3teu-bvdAZJzmU0g4iY&amp;index=32">giving talks</a> on her work in shaping the last 30 years of research.
From my perspective, this peaked when she gave <a href="https://cfp.recon.cx/recon2024/talk/GYG8FH/">the keynote</a> at <a href="https://cfp.recon.cx/recon2024/schedule/">Recon 2024</a>.
The announcement of her talk set off a flurry of talk proposals on decompilation at Recon, totaling five talks involving decompilation and including a <a href="https://cfp.recon.cx/recon2024/talk/NAPZWM/">decompilation expert panel</a>.</p>

<p>The panel, consisting of industry and academic experts, was epic, to say the least.
To name a few, it included:</p>

<ul>
  <li><a href="https://web.cs.dartmouth.edu/people/sergey-bratus">Dr. Sergey Bratus</a>, a previous DARPA PM and kickstarter of significant DoD investment in decompilation</li>
  <li><a href="https://en.wikipedia.org/wiki/Ilfak_Guilfanov">Ilfak Guilfanov</a>, the creator of IDA Pro</li>
  <li><a href="https://github.com/D0ntPanic">Rusty Wagner</a>, the man behind some of <a href="https://binary.ninja/2024/06/19/restructuring-the-decompiler.html">the biggest changes</a> in Binary Ninja</li>
</ul>

<p>Dr. Cifuentes grilled each panel member to give their honest opinions on the future of decompilation.
Many said, “There is more to do and more breakthroughs to be had.”
One such panel member, Ilfak, the current king of decompilers, said otherwise but admitted that breaking a decompiler was as easy as “breaking a baby.”
To summarize, it was a meeting of legends that inspired the community to keep pushing.</p>

<p>Note: <em>no babies were harmed in the making of these decompilers</em>.</p>

<h2 id="concluding">Concluding</h2>
<p>With the trend of publications, new findings, and general expert vibes, I can only conclude that 2024 was a special year for decompilation.
This is the field’s most active year since its inception in the 90s.
I remain optimistic that it is only the beginning of the research that will come in decompilation.
That also makes 2025 a spectacular year to ride the new excitement in the field and do great research, both from new and old researchers.</p>

<p>If you are a rising expert (or a zany practitioner), I encourage you to contribute your knowledge to the <a href="https://decompilation.wiki/">Decompilation Wiki</a> so it may continue to organize and guide the field.
Additionally, if you have any smaller research in decompilers you think is neat, consider following me on some social media, where announcements for a new decompilation workshop may come.
The conference to be co-located at is still being discussed, but I’m actively looking to gauge interest from the community.</p>

<p>Finally, I’d like to acknowledge <a href="https://www.linkedin.com/in/dustin-fraze-642682107">Dustin Fraze</a> for his work on DARPA’s <a href="https://www.darpa.mil/research/programs/computers-and-humans-exploring-software-security">CHESS</a> program, which funded and made many of this research possible.
He also reports bugs in decompilers a lot… ha! 
Thank you for reading and joining me on this journey through decompilation.
Here’s to 2024 and the exciting research to come in 2025!</p>



    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Younger cannabis users have reduced brain function, finds largest study yet (263 pts)]]></title>
            <link>https://newatlas.com/brain/young-adult-cannabis-brain-function/</link>
            <guid>42873697</guid>
            <pubDate>Thu, 30 Jan 2025 01:23:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newatlas.com/brain/young-adult-cannabis-brain-function/">https://newatlas.com/brain/young-adult-cannabis-brain-function/</a>, See on <a href="https://news.ycombinator.com/item?id=42873697">Hacker News</a></p>
Couldn't get https://newatlas.com/brain/young-adult-cannabis-brain-function/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Mathesar – an intutive spreadsheet-like interface to Postgres data (170 pts)]]></title>
            <link>https://github.com/mathesar-foundation/mathesar</link>
            <guid>42873312</guid>
            <pubDate>Thu, 30 Jan 2025 00:31:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mathesar-foundation/mathesar">https://github.com/mathesar-foundation/mathesar</a>, See on <a href="https://news.ycombinator.com/item?id=42873312">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/845767/218793207-a84a8c9e-d147-40a8-839b-f2b5d8b1ccba.png"><img src="https://user-images.githubusercontent.com/845767/218793207-a84a8c9e-d147-40a8-839b-f2b5d8b1ccba.png" width="450px" alt="Mathesar logo"></a>
</p>
<p dir="auto"><b>Intuitive spreadsheet-like interface that lets users of all technical skill levels view, edit, query, and collaborate on Postgres data directly—self hosted, with native Postgres access control.</b></p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/bc1d312ad5945d2c19bd914cdd148f9b07d545486f4446367b1504f855eff5db/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d617468657361722d666f756e646174696f6e2f6d61746865736172"><img alt="License" src="https://camo.githubusercontent.com/bc1d312ad5945d2c19bd914cdd148f9b07d545486f4446367b1504f855eff5db/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d617468657361722d666f756e646174696f6e2f6d61746865736172" data-canonical-src="https://img.shields.io/github/license/mathesar-foundation/mathesar"></a>
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a87d3d598533cf64ef058d4402b3afd727a5582bd47ce21b12e36d8a346dde4b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d636c6f7365642f6d617468657361722d666f756e646174696f6e2f6d61746865736172"><img alt="GitHub closed issues" src="https://camo.githubusercontent.com/a87d3d598533cf64ef058d4402b3afd727a5582bd47ce21b12e36d8a346dde4b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d636c6f7365642f6d617468657361722d666f756e646174696f6e2f6d61746865736172" data-canonical-src="https://img.shields.io/github/issues-closed/mathesar-foundation/mathesar"></a>
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/87260150c4020666540dcaf8ae2dd62cb683f6fe8bc3d4e290d5dbfdee05e9c7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f772f6d617468657361722d666f756e646174696f6e2f6d61746865736172"><img alt="GitHub commit activity" src="https://camo.githubusercontent.com/87260150c4020666540dcaf8ae2dd62cb683f6fe8bc3d4e290d5dbfdee05e9c7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f772f6d617468657361722d666f756e646174696f6e2f6d61746865736172" data-canonical-src="https://img.shields.io/github/commit-activity/w/mathesar-foundation/mathesar"></a>
</p>
<p dir="auto">
  <a href="https://mathesar.org/?ref=github-readme" rel="nofollow">Website</a> • <a href="https://docs.mathesar.org/?ref=github-readme-top" rel="nofollow">Docs</a> • <a href="https://wiki.mathesar.org/en/community/matrix" rel="nofollow">Matrix (chat)</a> • <a href="https://discord.gg/enaKqGn5xx" rel="nofollow">Discord</a> • <a href="https://wiki.mathesar.org/" rel="nofollow">Contributor Wiki</a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Mathesar</h2><a id="user-content-mathesar" aria-label="Permalink: Mathesar" href="#mathesar"></a></p>
<p dir="auto">Mathesar is a web application that makes working with PostgreSQL databases both simple and powerful. It empowers users of all technical skill levels to view, edit, query, and collaborate on data with a familiar spreadsheet-like interface—no code needed. It’s self hosted, can be deployed in minutes, and works directly with PostgreSQL databases, schemas, and tables without extra abstractions. The project is 100% open source and maintained by Mathesar Foundation, a 501(c)(3) nonprofit.</p>
<p dir="auto">Mathesar is as scalable as Postgres and supports any size or complexity of data, making it ideal for workflows involving production databases. It requires minimal setup, and integrates into your existing infrastructure. Because Mathesar is self-hosted, your data never leaves your servers, and access control based on Postgres roles and privileges keeps your database secure without adding unnecessary risk.</p>


<p dir="auto"><strong>Table of Contents</strong></p>
<ul dir="auto">
<li><a href="#status">Status</a></li>
<li><a href="#install-mathesar">Install Mathesar</a></li>
<li><a href="#join-our-community">Join our community</a>
<ul dir="auto">
<li><a href="#contribute-to-mathesar">Contribute to Mathesar</a></li>
</ul>
</li>
<li><a href="#features">Features</a></li>
<li><a href="#screeenshots">Screeenshots</a>
<ul dir="auto">
<li><a href="#connecting-a-database">Connecting a database</a></li>
<li><a href="#adding-collaborators">Adding collaborators</a></li>
<li><a href="#viewing-a-postgres-schema">Viewing a Postgres schema</a></li>
<li><a href="#working-with-tables">Working with tables</a></li>
<li><a href="#finding-a-nested-record">Finding a nested record</a></li>
<li><a href="#managing-table-permissions">Managing table permissions</a></li>
<li><a href="#viewing-a-single-record-with-related-records">Viewing a single record with related records</a></li>
<li><a href="#disconnecting-a-database">Disconnecting a database</a></li>
</ul>
</li>
<li><a href="#mathesar-in-action">Mathesar in action</a></li>
<li><a href="#our-motivation">Our motivation</a></li>
<li><a href="#bugs-and-troubleshooting">Bugs and troubleshooting</a></li>
<li><a href="#license">License</a></li>
</ul>

<p dir="auto"><h2 tabindex="-1" dir="auto">Status</h2><a id="user-content-status" aria-label="Permalink: Status" href="#status"></a></p>
<ul>
<li> <strong>Public Alpha</strong>: You can install and deploy Mathesar on your server. Go easy on us!</li>
<li> <strong>Public Beta</strong>: Stable and feature-rich enough to implement in production</li>
<li> <strong>Public</strong>: Widely used in production environments</li>
</ul>
<p dir="auto">We are currently in the <strong>public beta</strong> stage.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install Mathesar</h2><a id="user-content-install-mathesar" aria-label="Permalink: Install Mathesar" href="#install-mathesar"></a></p>
<p dir="auto">Please see <a href="https://docs.mathesar.org/?ref=github-readme-installing" rel="nofollow">our documentation</a> for instructions on installing Mathesar on your own server.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Join our community</h2><a id="user-content-join-our-community" aria-label="Permalink: Join our community" href="#join-our-community"></a></p>
<p dir="auto">The Mathesar team is on <a href="https://wiki.mathesar.org/en/community/matrix" rel="nofollow">Matrix</a> (chat service). We also have <a href="https://wiki.mathesar.org/en/community/mailing-lists" rel="nofollow">mailing lists</a> and the core team discusses day-to-day work on our developer mailing list.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Contribute to Mathesar</h3><a id="user-content-contribute-to-mathesar" aria-label="Permalink: Contribute to Mathesar" href="#contribute-to-mathesar"></a></p>
<p dir="auto">We actively encourage contribution! Get started by reading our <a href="https://github.com/mathesar-foundation/mathesar/blob/develop/CONTRIBUTING.md">Contributor Guide</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><strong>Built on Postgres</strong>: Connect to an existing Postgres database or set one up from scratch.</li>
<li><strong>Install in minutes</strong>: Install using Docker in minutes, integrate into any existing infrastructure.</li>
<li><strong>Postgres-based access control</strong>: Use existing Postgres roles within Mathesar's UI, or set up your own.</li>
<li><strong>Interoperable with other tools</strong>: Mathesar works harmoniously alongside your database and thousands of other tools in the Postgres ecosystem.</li>
<li><strong>Set up your data models</strong>: Easily create and update Postgres schemas and tables.</li>
<li><strong>Data entry</strong>: Use our spreadsheet-like interface to view, create, update, and delete table records.</li>
<li><strong>Filter, sort, and group</strong>: Quickly slice your data in different ways.</li>
<li><strong>Query builder</strong>: Use our Data Explorer to build queries without knowing anything about SQL or joins.</li>
<li><strong>Import and export data</strong>: Import and export data into Mathesar easily to work with your data elsewhere.</li>
<li><strong>Schema migrations</strong>: Transfer columns between tables in two clicks.</li>
<li><strong>Uses Postgres features</strong>: Mathesar uses and manipulates Postgres schemas, primary keys, foreign keys, constraints and data types. e.g. "Relationships" in the UI are foreign keys in the database.</li>
<li><strong>Custom data types</strong>: Custom data types for emails and URLs, validated at the database level.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Screeenshots</h2><a id="user-content-screeenshots" aria-label="Permalink: Screeenshots" href="#screeenshots"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Connecting a database</h3><a id="user-content-connecting-a-database" aria-label="Permalink: Connecting a database" href="#connecting-a-database"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/287034/407736062-c115ad21-e501-4992-bf84-54758a321f41.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMjk3MDMsIm5iZiI6MTczODIyOTQwMywicGF0aCI6Ii8yODcwMzQvNDA3NzM2MDYyLWMxMTVhZDIxLWU1MDEtNDk5Mi1iZjg0LTU0NzU4YTMyMWY0MS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTMwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEzMFQwOTMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1hN2ZhODU0ODU0NjE4MWFmNWZjNmJlZWYwMzU2ODZhNGQ4NWZmNzJjZGMxN2I3ZjNjMDY2ZGM3NGVmMzYyMGYyJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.MWN2IvNFN3tHuWoqWKvMz8uTFlt0VM6ohVhJvIrMooY"><img src="https://private-user-images.githubusercontent.com/287034/407736062-c115ad21-e501-4992-bf84-54758a321f41.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMjk3MDMsIm5iZiI6MTczODIyOTQwMywicGF0aCI6Ii8yODcwMzQvNDA3NzM2MDYyLWMxMTVhZDIxLWU1MDEtNDk5Mi1iZjg0LTU0NzU4YTMyMWY0MS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTMwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEzMFQwOTMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1hN2ZhODU0ODU0NjE4MWFmNWZjNmJlZWYwMzU2ODZhNGQ4NWZmNzJjZGMxN2I3ZjNjMDY2ZGM3NGVmMzYyMGYyJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.MWN2IvNFN3tHuWoqWKvMz8uTFlt0VM6ohVhJvIrMooY" alt="connect-db"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adding collaborators</h3><a id="user-content-adding-collaborators" aria-label="Permalink: Adding collaborators" href="#adding-collaborators"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/287034/407736166-90c65f3f-3edb-4bf3-b00c-586019c78765.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMjk3MDMsIm5iZiI6MTczODIyOTQwMywicGF0aCI6Ii8yODcwMzQvNDA3NzM2MTY2LTkwYzY1ZjNmLTNlZGItNGJmMy1iMDBjLTU4NjAxOWM3ODc2NS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTMwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEzMFQwOTMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0xMmFjOTc0MTc2MDg1MzQzMzI1Yjk2NTc4MGY4OTAyYjgzNTZkZDM0NTRiMDNkODFlNjQ2MDY4NjIxZTIwNjU3JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.1CdS1CbceqJpfAPZxM2lGxwQ4li3MpB9gxirSjNt4e8"><img src="https://private-user-images.githubusercontent.com/287034/407736166-90c65f3f-3edb-4bf3-b00c-586019c78765.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMjk3MDMsIm5iZiI6MTczODIyOTQwMywicGF0aCI6Ii8yODcwMzQvNDA3NzM2MTY2LTkwYzY1ZjNmLTNlZGItNGJmMy1iMDBjLTU4NjAxOWM3ODc2NS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTMwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEzMFQwOTMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0xMmFjOTc0MTc2MDg1MzQzMzI1Yjk2NTc4MGY4OTAyYjgzNTZkZDM0NTRiMDNkODFlNjQ2MDY4NjIxZTIwNjU3JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.1CdS1CbceqJpfAPZxM2lGxwQ4li3MpB9gxirSjNt4e8" alt="add-collaborator"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Viewing a Postgres schema</h3><a id="user-content-viewing-a-postgres-schema" aria-label="Permalink: Viewing a Postgres schema" href="#viewing-a-postgres-schema"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/287034/407736637-305d0d79-fba8-4044-954d-bc511c935321.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMjk3MDMsIm5iZiI6MTczODIyOTQwMywicGF0aCI6Ii8yODcwMzQvNDA3NzM2NjM3LTMwNWQwZDc5LWZiYTgtNDA0NC05NTRkLWJjNTExYzkzNTMyMS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTMwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEzMFQwOTMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT05NzlkODNlNmYwMGRlODZhMTJjNjY0MDk2Y2VkYjc0OWNlYzIzMWU1NjAyYzllMjAzZWRhM2JmYjkyYTk3ZWVhJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.g5ei2KijyVHmtnRsurK18-CruY_zV81qLJqGGswtIPA"><img src="https://private-user-images.githubusercontent.com/287034/407736637-305d0d79-fba8-4044-954d-bc511c935321.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMjk3MDMsIm5iZiI6MTczODIyOTQwMywicGF0aCI6Ii8yODcwMzQvNDA3NzM2NjM3LTMwNWQwZDc5LWZiYTgtNDA0NC05NTRkLWJjNTExYzkzNTMyMS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTMwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEzMFQwOTMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT05NzlkODNlNmYwMGRlODZhMTJjNjY0MDk2Y2VkYjc0OWNlYzIzMWU1NjAyYzllMjAzZWRhM2JmYjkyYTk3ZWVhJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.g5ei2KijyVHmtnRsurK18-CruY_zV81qLJqGGswtIPA" alt="schema-page"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Working with tables</h3><a id="user-content-working-with-tables" aria-label="Permalink: Working with tables" href="#working-with-tables"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/287034/407736536-bb3bbcd7-ef12-4304-9d5c-8220b188d2f5.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMjk3MDMsIm5iZiI6MTczODIyOTQwMywicGF0aCI6Ii8yODcwMzQvNDA3NzM2NTM2LWJiM2JiY2Q3LWVmMTItNDMwNC05ZDVjLTgyMjBiMTg4ZDJmNS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTMwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEzMFQwOTMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wNzZlNWQ5YzEyYjFlMmFlYzJmYzdkNDc1ZDdmMTE0MTc3N2I3NjJjYzcyOTViMWRkODVkYmU3OGQ0NTY3MjNkJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.gEgWziQVSgFSaRvvTaXbSnyTlTzgzpCpvj5vCt1bXm4"><img src="https://private-user-images.githubusercontent.com/287034/407736536-bb3bbcd7-ef12-4304-9d5c-8220b188d2f5.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMjk3MDMsIm5iZiI6MTczODIyOTQwMywicGF0aCI6Ii8yODcwMzQvNDA3NzM2NTM2LWJiM2JiY2Q3LWVmMTItNDMwNC05ZDVjLTgyMjBiMTg4ZDJmNS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTMwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEzMFQwOTMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wNzZlNWQ5YzEyYjFlMmFlYzJmYzdkNDc1ZDdmMTE0MTc3N2I3NjJjYzcyOTViMWRkODVkYmU3OGQ0NTY3MjNkJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.gEgWziQVSgFSaRvvTaXbSnyTlTzgzpCpvj5vCt1bXm4" alt="table-inspector"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Finding a nested record</h3><a id="user-content-finding-a-nested-record" aria-label="Permalink: Finding a nested record" href="#finding-a-nested-record"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/287034/407736784-b3d12cb4-e90b-458b-8e19-8371c1332557.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMjk3MDMsIm5iZiI6MTczODIyOTQwMywicGF0aCI6Ii8yODcwMzQvNDA3NzM2Nzg0LWIzZDEyY2I0LWU5MGItNDU4Yi04ZTE5LTgzNzFjMTMzMjU1Ny5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTMwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEzMFQwOTMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0zM2U4ZTVkZWViM2Q4YWRhYWRhZjEzMWNhMTM3MDFlMmE5MDUwNDJiY2NmNGI5ZmVkNWVmOTQyZmU4ZjBjMzFmJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.v2JQTBTIBBZD2JvIsHvv_7pAaVzzbSMi1sgLZSCHBnA"><img src="https://private-user-images.githubusercontent.com/287034/407736784-b3d12cb4-e90b-458b-8e19-8371c1332557.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMjk3MDMsIm5iZiI6MTczODIyOTQwMywicGF0aCI6Ii8yODcwMzQvNDA3NzM2Nzg0LWIzZDEyY2I0LWU5MGItNDU4Yi04ZTE5LTgzNzFjMTMzMjU1Ny5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTMwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEzMFQwOTMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0zM2U4ZTVkZWViM2Q4YWRhYWRhZjEzMWNhMTM3MDFlMmE5MDUwNDJiY2NmNGI5ZmVkNWVmOTQyZmU4ZjBjMzFmJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.v2JQTBTIBBZD2JvIsHvv_7pAaVzzbSMi1sgLZSCHBnA" alt="record-selector"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Managing table permissions</h3><a id="user-content-managing-table-permissions" aria-label="Permalink: Managing table permissions" href="#managing-table-permissions"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/287034/407736997-094d3c98-8c4c-4cfa-aad5-6fd3faa30473.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMjk3MDMsIm5iZiI6MTczODIyOTQwMywicGF0aCI6Ii8yODcwMzQvNDA3NzM2OTk3LTA5NGQzYzk4LThjNGMtNGNmYS1hYWQ1LTZmZDNmYWEzMDQ3My5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTMwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEzMFQwOTMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT03NmU5YmI3ZDU1NjA2ODcwYzM4NDk5M2QwMjZiMDA4OTY3MWQ0MGNhOTM0ZGRkYjIyODgxZmY1NjE0MWI2ZTVkJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.htTk4KwZguno-a8lg4zeUAHUPPUeL67OhKCpCZo-R2g"><img src="https://private-user-images.githubusercontent.com/287034/407736997-094d3c98-8c4c-4cfa-aad5-6fd3faa30473.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMjk3MDMsIm5iZiI6MTczODIyOTQwMywicGF0aCI6Ii8yODcwMzQvNDA3NzM2OTk3LTA5NGQzYzk4LThjNGMtNGNmYS1hYWQ1LTZmZDNmYWEzMDQ3My5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTMwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEzMFQwOTMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT03NmU5YmI3ZDU1NjA2ODcwYzM4NDk5M2QwMjZiMDA4OTY3MWQ0MGNhOTM0ZGRkYjIyODgxZmY1NjE0MWI2ZTVkJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.htTk4KwZguno-a8lg4zeUAHUPPUeL67OhKCpCZo-R2g" alt="table-permissions"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Viewing a single record with related records</h3><a id="user-content-viewing-a-single-record-with-related-records" aria-label="Permalink: Viewing a single record with related records" href="#viewing-a-single-record-with-related-records"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/287034/407737134-00774552-2acb-4a01-87d6-1813f579e757.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMjk3MDMsIm5iZiI6MTczODIyOTQwMywicGF0aCI6Ii8yODcwMzQvNDA3NzM3MTM0LTAwNzc0NTUyLTJhY2ItNGEwMS04N2Q2LTE4MTNmNTc5ZTc1Ny5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTMwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEzMFQwOTMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT02YjQxZjA3YzI4YmY0NDhlNWIyNDE4ZGMyYTAzOTNhMjhiMzUxMmY2MmIyZThjZGZjMzI0MjU1OWNhMDUwOTg1JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.V2N8dxvBykf28fCLBPzsoBt4znMR1G0i-m-RUScI9bM"><img src="https://private-user-images.githubusercontent.com/287034/407737134-00774552-2acb-4a01-87d6-1813f579e757.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMjk3MDMsIm5iZiI6MTczODIyOTQwMywicGF0aCI6Ii8yODcwMzQvNDA3NzM3MTM0LTAwNzc0NTUyLTJhY2ItNGEwMS04N2Q2LTE4MTNmNTc5ZTc1Ny5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTMwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEzMFQwOTMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT02YjQxZjA3YzI4YmY0NDhlNWIyNDE4ZGMyYTAzOTNhMjhiMzUxMmY2MmIyZThjZGZjMzI0MjU1OWNhMDUwOTg1JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.V2N8dxvBykf28fCLBPzsoBt4znMR1G0i-m-RUScI9bM" alt="record-page"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Disconnecting a database</h3><a id="user-content-disconnecting-a-database" aria-label="Permalink: Disconnecting a database" href="#disconnecting-a-database"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/287034/407737237-48905983-a632-4632-b2c1-b0f25c3b6e75.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMjk3MDMsIm5iZiI6MTczODIyOTQwMywicGF0aCI6Ii8yODcwMzQvNDA3NzM3MjM3LTQ4OTA1OTgzLWE2MzItNDYzMi1iMmMxLWIwZjI1YzNiNmU3NS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTMwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEzMFQwOTMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0yOWQ2OWU0NjgwNzMxY2Y0MTk3ZGUyYjUwYzY1YzQwM2EwZjU0N2U0OGI1MWNmM2MwNjQxNTM2NmM5MjdkMzlmJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.1ITp70EuM5UKpzaxEQTHxSRnhlmdslGRaIn_90E1K8k"><img src="https://private-user-images.githubusercontent.com/287034/407737237-48905983-a632-4632-b2c1-b0f25c3b6e75.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMjk3MDMsIm5iZiI6MTczODIyOTQwMywicGF0aCI6Ii8yODcwMzQvNDA3NzM3MjM3LTQ4OTA1OTgzLWE2MzItNDYzMi1iMmMxLWIwZjI1YzNiNmU3NS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTMwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEzMFQwOTMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0yOWQ2OWU0NjgwNzMxY2Y0MTk3ZGUyYjUwYzY1YzQwM2EwZjU0N2U0OGI1MWNmM2MwNjQxNTM2NmM5MjdkMzlmJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.1ITp70EuM5UKpzaxEQTHxSRnhlmdslGRaIn_90E1K8k" alt="disconnect-db"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Mathesar in action</h2><a id="user-content-mathesar-in-action" aria-label="Permalink: Mathesar in action" href="#mathesar-in-action"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description mathesar-demo-video.mp4">mathesar-demo-video.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/287034/407732446-6bdfb178-17b4-4abf-aac4-9781e1d841ab.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMjk3MDMsIm5iZiI6MTczODIyOTQwMywicGF0aCI6Ii8yODcwMzQvNDA3NzMyNDQ2LTZiZGZiMTc4LTE3YjQtNGFiZi1hYWM0LTk3ODFlMWQ4NDFhYi5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTMwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEzMFQwOTMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04MTllZTEyNTE3MmU4YmY1YzAxYzM0ZTAzMjkxMWI0Mzk1NjY5OWEzNmRlYmM5MDkwMGE5NTA5ODliNGU5YmQ5JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.4z1Y3vALSrh5uUQuFDYj66Q-af7qPkKAZ6TgB6E5Lvk" data-canonical-src="https://private-user-images.githubusercontent.com/287034/407732446-6bdfb178-17b4-4abf-aac4-9781e1d841ab.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMjk3MDMsIm5iZiI6MTczODIyOTQwMywicGF0aCI6Ii8yODcwMzQvNDA3NzMyNDQ2LTZiZGZiMTc4LTE3YjQtNGFiZi1hYWM0LTk3ODFlMWQ4NDFhYi5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMTMwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDEzMFQwOTMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04MTllZTEyNTE3MmU4YmY1YzAxYzM0ZTAzMjkxMWI0Mzk1NjY5OWEzNmRlYmM5MDkwMGE5NTA5ODliNGU5YmQ5JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.4z1Y3vALSrh5uUQuFDYj66Q-af7qPkKAZ6TgB6E5Lvk" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Our motivation</h2><a id="user-content-our-motivation" aria-label="Permalink: Our motivation" href="#our-motivation"></a></p>
<p dir="auto">Using databases shouldn't require technical expertise or expensive, closed-off tools. Databases are incredibly powerful, but they're often trapped behind complex interfaces that are hard to use or limit how people can access and share their data. We want to change that by building user-friendly tools that unlock the power of existing databases without sacrificing accessibility, portability, or extensibility.</p>
<p dir="auto">Mathesar is our answer: an open-source platform designed to unlock the full potential of PostgreSQL, one of the most powerful and trusted open-source databases. Mathesar is easy to use, interoperable, and extensible, while also giving you complete control over your data. As a nonprofit, we're committed to keeping Mathesar 100% open source and available to everyone—because better ways to work with data mean better decisions, and better decisions lead to a better world.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Bugs and troubleshooting</h2><a id="user-content-bugs-and-troubleshooting" aria-label="Permalink: Bugs and troubleshooting" href="#bugs-and-troubleshooting"></a></p>
<p dir="auto">If you run into problems, refer to our <a href="https://github.com/mathesar-foundation/mathesar/blob/develop/TROUBLESHOOTING.md">troubleshooting guide</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Mathesar is open source under the GPLv3 license - see <a href="https://github.com/mathesar-foundation/mathesar/blob/develop/LICENSE">LICENSE</a>. It also contains derivatives of third-party open source modules licensed under the MIT license. See the list and respective licenses in <a href="https://github.com/mathesar-foundation/mathesar/blob/develop/THIRDPARTY">THIRDPARTY</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Blueskyfeedbot: Post RSS Feeds to Bluesky via GitHub Actions (149 pts)]]></title>
            <link>https://github.com/marketplace/actions/feed-to-bluesky</link>
            <guid>42873153</guid>
            <pubDate>Thu, 30 Jan 2025 00:12:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/marketplace/actions/feed-to-bluesky">https://github.com/marketplace/actions/feed-to-bluesky</a>, See on <a href="https://news.ycombinator.com/item?id=42873153">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="markdown-body"><p dir="auto"><h2 dir="auto">Blueskyfeedbot</h2><a id="user-content-blueskyfeedbot" aria-label="Permalink: Blueskyfeedbot" href="#blueskyfeedbot"></a></p>
<p dir="auto">Blueskyfeedbot is a bot that posts RSS feeds to Bluesky via GitHub Actions.</p>
<p dir="auto"><h2 dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Go to <a href="https://bsky.app/settings/app-passwords" rel="nofollow">https://bsky.app/settings/app-passwords</a> and add a new app password.</p>
<ul dir="auto">
<li>Name it whatever you want.</li>
<li>Save it and grab the app password.
For security reasons, you won't be able to view this again.
If you lose this app password, you'll need to generate a new one.</li>
</ul>
</li>
<li>
<p dir="auto">Create a new GitHub repository.</p>
</li>
<li>
<p dir="auto">Go to your repository settings at <code>https://github.com/${YOUR_REPO}/settings/secrets/actions/new</code>, and add a new
secret with the value of the access token.</p>
</li>
<li>
<p dir="auto">Add a file named <code>.github/workflows/blueskyfeedbot.yml</code> with the following content:</p>
<div dir="auto" data-snippet-clipboard-copy-content="name: FeedBot
on:
  schedule:
    # This will run every five minutes. Alter it using https://crontab.guru/.
    - cron: '*/5 * * * *'  
jobs:
  rss-to-bluesky:
    runs-on: ubuntu-latest
    steps:
      - name: Generate cache key
        uses: actions/github-script@v6
        id: generate-key
        with:
          script: |
            core.setOutput('cache-key', new Date().valueOf())
      - name: Retrieve cache
        uses: actions/cache@v3
        with:
          path: ${{ github.workspace }}/blueskyfeedbot
          key: feed-cache-${{ steps.generate-key.outputs.cache-key }}
          restore-keys: feed-cache-
      - name: GitHub
        uses: 'joschi/blueskyfeedbot@v1'
        with:
          # This is the RSS feed you want to publish
          rss-feed: https://www.githubstatus.com/history.rss
          # Template of status posted to Bluesky (Handlebars)
          template: '{{item.title}} {{item.link}}'
          # This is your service URL (optional)
          service-url: https://bsky.social
          # This is the Bluesky username (example: username.bsky.social)
          username: ${{ secrets.BLUESKY_USERNAME }}
          # This is the app password you created earlier
          password: ${{ secrets.BLUESKY_PASSWORD }}
          # This is a path to the cache file, using the above cache path
          cache-file: ${{ github.workspace }}/blueskyfeedbot/cache.json"><pre><span>name</span>: <span>FeedBot</span>
<span>on</span>:
  <span>schedule</span>:
    <span><span>#</span> This will run every five minutes. Alter it using https://crontab.guru/.</span>
    - <span>cron</span>: <span><span>'</span>*/5 * * * *<span>'</span></span>  
<span>jobs</span>:
  <span>rss-to-bluesky</span>:
    <span>runs-on</span>: <span>ubuntu-latest</span>
    <span>steps</span>:
      - <span>name</span>: <span>Generate cache key</span>
        <span>uses</span>: <span>actions/github-script@v6</span>
        <span>id</span>: <span>generate-key</span>
        <span>with</span>:
          <span>script</span>: <span>|</span>
<span>            core.setOutput('cache-key', new Date().valueOf())</span>
<span></span>      - <span>name</span>: <span>Retrieve cache</span>
        <span>uses</span>: <span>actions/cache@v3</span>
        <span>with</span>:
          <span>path</span>: <span>${{ github.workspace }}/blueskyfeedbot</span>
          <span>key</span>: <span>feed-cache-${{ steps.generate-key.outputs.cache-key }}</span>
          <span>restore-keys</span>: <span>feed-cache-</span>
      - <span>name</span>: <span>GitHub</span>
        <span>uses</span>: <span><span>'</span>joschi/blueskyfeedbot@v1<span>'</span></span>
        <span>with</span>:
          <span><span>#</span> This is the RSS feed you want to publish</span>
          <span>rss-feed</span>: <span>https://www.githubstatus.com/history.rss</span>
          <span><span>#</span> Template of status posted to Bluesky (Handlebars)</span>
          <span>template</span>: <span><span>'</span>{{item.title}} {{item.link}}<span>'</span></span>
          <span><span>#</span> This is your service URL (optional)</span>
          <span>service-url</span>: <span>https://bsky.social</span>
          <span><span>#</span> This is the Bluesky username (example: username.bsky.social)</span>
          <span>username</span>: <span>${{ secrets.BLUESKY_USERNAME }}</span>
          <span><span>#</span> This is the app password you created earlier</span>
          <span>password</span>: <span>${{ secrets.BLUESKY_PASSWORD }}</span>
          <span><span>#</span> This is a path to the cache file, using the above cache path</span>
          <span>cache-file</span>: <span>${{ github.workspace }}/blueskyfeedbot/cache.json</span></pre></div>
</li>
<li>
<p dir="auto">Commit and publish your changes.</p>
</li>
</ol>
<p dir="auto"><h2 dir="auto">Status template</h2><a id="user-content-status-template" aria-label="Permalink: Status template" href="#status-template"></a></p>
<p dir="auto">The status template (<code>status-template</code>) is using <a href="https://handlebarsjs.com/" rel="nofollow">Handlebars</a> as template engine.</p>
<p dir="auto">The action is passing in an instance of <code>FeedData</code> (field <code>feedData</code>) and the current <code>FeedEntry</code> (field <code>item</code>) into the template:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export interface FeedEntry {
  link?: string;
  title?: string;
  description?: string;
  published?: Date;
}

export interface FeedData {
  link?: string;
  title?: string;
  description?: string;
  generator?: string;
  language?: string;
  published?: Date;
  entries?: Array<FeedEntry>;
}"><pre><span>export</span> <span>interface</span> <span>FeedEntry</span> <span>{</span>
  <span>link</span>?: <span>string</span><span>;</span>
  <span>title</span>?: <span>string</span><span>;</span>
  <span>description</span>?: <span>string</span><span>;</span>
  <span>published</span>?: <span>Date</span><span>;</span>
<span>}</span>

<span>export</span> <span>interface</span> <span>FeedData</span> <span>{</span>
  <span>link</span>?: <span>string</span><span>;</span>
  <span>title</span>?: <span>string</span><span>;</span>
  <span>description</span>?: <span>string</span><span>;</span>
  <span>generator</span>?: <span>string</span><span>;</span>
  <span>language</span>?: <span>string</span><span>;</span>
  <span>published</span>?: <span>Date</span><span>;</span>
  <span>entries</span>?: <span>Array</span><span>&lt;</span><span>FeedEntry</span><span>&gt;</span><span>;</span>
<span>}</span></pre></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Advice for a friend who wants to start a blog (267 pts)]]></title>
            <link>https://www.henrikkarlsson.xyz/p/start-a-blog</link>
            <guid>42872276</guid>
            <pubDate>Wed, 29 Jan 2025 22:45:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.henrikkarlsson.xyz/p/start-a-blog">https://www.henrikkarlsson.xyz/p/start-a-blog</a>, See on <a href="https://news.ycombinator.com/item?id=42872276">Hacker News</a></p>
Couldn't get https://www.henrikkarlsson.xyz/p/start-a-blog: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[An update on Dart macros and data serialization (138 pts)]]></title>
            <link>https://medium.com/dartlang/an-update-on-dart-macros-data-serialization-06d3037d4f12</link>
            <guid>42871867</guid>
            <pubDate>Wed, 29 Jan 2025 22:08:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/dartlang/an-update-on-dart-macros-data-serialization-06d3037d4f12">https://medium.com/dartlang/an-update-on-dart-macros-data-serialization-06d3037d4f12</a>, See on <a href="https://news.ycombinator.com/item?id=42871867">Hacker News</a></p>
Couldn't get https://medium.com/dartlang/an-update-on-dart-macros-data-serialization-06d3037d4f12: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[From C++ to Clojure: Jank language promises best of both (223 pts)]]></title>
            <link>https://thenewstack.io/from-c-to-clojure-new-language-promises-best-of-both/</link>
            <guid>42871743</guid>
            <pubDate>Wed, 29 Jan 2025 21:56:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thenewstack.io/from-c-to-clojure-new-language-promises-best-of-both/">https://thenewstack.io/from-c-to-clojure-new-language-promises-best-of-both/</a>, See on <a href="https://news.ycombinator.com/item?id=42871743">Hacker News</a></p>
Couldn't get https://thenewstack.io/from-c-to-clojure-new-language-promises-best-of-both/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Exposed DeepSeek database leaking sensitive information, including chat history (630 pts)]]></title>
            <link>https://www.wiz.io/blog/wiz-research-uncovers-exposed-deepseek-database-leak</link>
            <guid>42871371</guid>
            <pubDate>Wed, 29 Jan 2025 21:25:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wiz.io/blog/wiz-research-uncovers-exposed-deepseek-database-leak">https://www.wiz.io/blog/wiz-research-uncovers-exposed-deepseek-database-leak</a>, See on <a href="https://news.ycombinator.com/item?id=42871371">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Wiz Research has identified a publicly accessible ClickHouse database belonging to DeepSeek, which allows full control over database operations, including the ability to access internal data. The exposure includes over a million lines of log streams containing chat history, secret keys, backend details, and other highly sensitive information. The Wiz Research team immediately and responsibly disclosed the issue to DeepSeek, which promptly secured the exposure.&nbsp;</p><p>In this blog post, we will detail our discovery and also consider the broader implications for the industry at large.&nbsp;&nbsp;&nbsp;</p><h2><a id="executive-summary-2"></a><strong>Executive Summary</strong>&nbsp;</h2><p>DeepSeek, a Chinese AI startup, has recently garnered significant media attention due to its groundbreaking AI models, particularly the DeepSeek-R1 reasoning model. This model rivals leading AI systems like OpenAI’s o1 in performance and stands out for its cost-effectiveness and efficiency.&nbsp;</p><p>As DeepSeek made waves in the AI space, the Wiz Research team set out to assess its external security posture and identify any potential vulnerabilities.&nbsp;</p><p>Within minutes, we found a publicly accessible ClickHouse database linked to DeepSeek, completely open and unauthenticated, exposing sensitive data. It was hosted at oauth2callback.deepseek.com:9000 and dev.deepseek.com:9000.&nbsp;</p><p>This database contained a significant volume of chat history, backend data and sensitive information, including log streams, API Secrets, and operational details. &nbsp;</p><p>More critically, the exposure allowed for full database control and potential <a href="https://www.wiz.io/academy/privilege-escalation">privilege escalation</a> within the DeepSeek environment, without any authentication or defense mechanism to the outside world.&nbsp;</p><figure></figure><figure></figure><h2><a id="exposure-walkthrough-10"></a><strong>Exposure Walkthrough</strong>&nbsp;&nbsp;</h2><p>Our reconnaissance began with assessing DeepSeek’s publicly accessible domains. By mapping the external attack surface with straightforward reconnaissance techniques (passive and active discovery of subdomains), we identified around 30 internet-facing subdomains. Most appeared benign, hosting elements like the chatbot interface, status page, and API documentation—none of which initially suggested a high-risk exposure.&nbsp;</p><p>However, as we expanded our search beyond standard HTTP ports (80/443), we detected two <strong>unusual, open ports (8123 &amp; 9000)</strong> associated with the following hosts:&nbsp;</p><ul><li><p><a rel="noreferrer noopener" target="_blank" href="http://oauth2callback.deepseek.com:8123/"><u>http://oauth2callback.deepseek.com:8123</u></a>&nbsp;&nbsp;</p></li><li><p><a rel="noreferrer noopener" target="_blank" href="http://dev.deepseek.com:8123/"><u>http://dev.deepseek.com:8123</u></a>&nbsp;&nbsp;</p></li><li><p><a rel="noreferrer noopener" target="_blank" href="http://oauth2callback.deepseek.com:9000/"><u>http://oauth2callback.deepseek.com:9000</u></a>&nbsp;&nbsp;</p></li><li><p><a rel="noreferrer noopener" target="_blank" href="http://dev.deepseek.com:9000/"><u>http://dev.deepseek.com:9000</u></a>&nbsp;</p></li></ul><p>Upon further investigation, these ports led to a <strong>publicly exposed ClickHouse database</strong>, accessible without any authentication at all – immediately raising red flags. &nbsp;</p><p>ClickHouse is an open-source, columnar database management system designed for fast analytical queries on large datasets. It was developed by Yandex and is widely used for real-time data processing, log storage, and big data analytics, which indicates such exposure as a very valuable and sensitive discovery.&nbsp;</p><figure></figure><p>By leveraging ClickHouse’s HTTP interface, we accessed the /play path, which <strong>allowed direct execution of arbitrary SQL queries</strong> via the browser. Running a simple SHOW TABLES; query returned a full list of accessible datasets.&nbsp;</p><figure><figcaption>Tables output from ClickHouse Web UI</figcaption></figure><p>Among them, one table stood out: log_stream, which contained extensive logs with <strong>highly sensitive data</strong>.&nbsp;</p><p>The log_stream table contained <strong>over 1 million log entries</strong>, with particularly revealing columns:&nbsp;</p><figure></figure><ul><li><p>timestamp – Logs dating from <strong>January 6, 2025</strong>&nbsp;</p></li><li><p>span_name – References to various internal <strong>DeepSeek API endpoints</strong>&nbsp;</p></li><li><p>string.values – <strong>Plaintext logs</strong>, including <strong>Chat History</strong>, <strong>API Keys, backend details, and operational metadata</strong>&nbsp;</p></li><li><p>_service – Indicating which <strong>DeepSeek service</strong> generated the logs&nbsp;</p></li><li><p>_source – Exposing the <strong>origin of log requests</strong>, containing <strong>Chat History, API Keys, directory structures, and chatbot metadata logs</strong>&nbsp;</p></li></ul><figure></figure><p>This level of access posed a critical risk to DeepSeek’s own security and for its end-users. Not only an attacker could retrieve sensitive logs and actual plain-text chat msgs, but they could also potentially <strong>exfiltrate plaintext passwords and local files</strong> along <strong>propriety information</strong> directly from the server using queries like: SELECT LOAD_FILE(‘{FileName}‘);&nbsp;&nbsp;</p><p><em>(Note: We did not execute intrusive queries beyond enumeration to preserve ethical research practices.)</em>&nbsp;</p><h2><a id="key-takeaways-26"></a><strong>Key Takeaways&nbsp;&nbsp;</strong>&nbsp;</h2><p>The rapid adoption of AI services without corresponding security is inherently risky. This exposure underscores the fact that the immediate <a href="https://www.wiz.io/academy/ai-security-risks">security risks for AI applications</a> stem from the infrastructure and tools supporting them.&nbsp;</p><p>While much of the attention around AI security is focused on futuristic threats, the real dangers often come from basic risks—like accidental external exposure of databases. These risks, which are fundamental to security, should remain a top priority for security teams.&nbsp;</p><p>As organizations rush to adopt AI tools and services from a growing number of startups and providers, it’s essential to remember that by doing so, we’re entrusting these companies with sensitive data. The rapid pace of adoption often leads to overlooking security, but protecting customer data must remain the top priority. It’s crucial that security teams work closely with AI engineers to ensure visibility into the architecture, tooling, and models being used, so we can safeguard data and prevent exposure.&nbsp;</p><h2><a id="conclusion-30"></a>Conclusion&nbsp;&nbsp;</h2><p>The world has never seen a piece of technology adopted at the pace of AI. Many AI companies have rapidly grown into critical infrastructure providers without the security frameworks that typically accompany such widespread adoptions. As AI becomes deeply integrated into businesses worldwide, the industry must recognize the risks of handling sensitive data and enforce security practices on par with those required for public cloud providers and major infrastructure providers.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Parsing PDFs (and more) in Elixir using Rust (165 pts)]]></title>
            <link>https://www.chriis.dev/opinion/parsing-pdfs-in-elixir-using-rust</link>
            <guid>42871143</guid>
            <pubDate>Wed, 29 Jan 2025 21:05:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chriis.dev/opinion/parsing-pdfs-in-elixir-using-rust">https://www.chriis.dev/opinion/parsing-pdfs-in-elixir-using-rust</a>, See on <a href="https://news.ycombinator.com/item?id=42871143">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Here's the thing about PDFs - they're complex beasts that require quite a bit of thinking to properly parse - they come in all shapes and sizes, and they can contain a lot of different types of data and formatting. 90% of the time, we just want to extract the text from the file, but that's not always easy - for the remaining 10%, well we won't be covering that in this blog post.</p>
<p>If you've been in the Elixir world for long enough, you'll probably have tried to parse a PDF file and realised that it's not as easy as it seems. A quick look on the <a href="https://elixirforum.com/t/parsing-pdf-file/23287">Elixir Forum</a> will quickly show you that there is no simple way to do it.</p>
<p>Most people will tell you to upload the file to S3 and use a Lambda to handle the contents. Offloading to AWS Lambda might seem elegant at first ("Look, Ma, no dependencies!"), but it comes with its own baggage:</p>
<ul>
<li><span></span>You're adding network latency to what should be a simple operation</li>
<li><span></span>AWS costs can spiral if you're processing lots of PDFs</li>
<li><span></span>You're now dependent on external services for core functionality</li>
<li><span></span>Debugging becomes a distributed systems problem</li>
</ul>
<p>These aren't ideal solutions - and software engineering is already made more complicated than it needs to be at times - we don't need to add more complexity to the mix.</p>
<p>We need a robust, native solution that plays nicely with the BEAM. So how do we do that?</p>
<h2>Enter the crabs!</h2>
<p>Elixir is my favourite language, but it can't do everything - web services, background jobs, and more are easy but sometimes we need a little help from our friends closer to the hardware for some of the tasks Elixir doesn't have a native solution for. That's where Rust and NIFs come in!</p>
<p>Rust is a systems programming language that is fast, safe, and easy to use. It's a great language for writing code that needs to be performant and reliable.</p>
<p>But Rust isn't just fast - it's "zero-cost abstractions" fast. What does that mean? You get high-level, ergonomic code that compiles down to something as efficient as hand-written C. For PDF parsing, where you're dealing with complex file formats and potentially large documents, this performance is a game-changer.</p>
<blockquote>
<p>What is a NIF?
A NIF (Native Implemented Function) is a way to call Rust code from Elixir - it's the BEAMs method of allowing processes to directly call native functions. It allows you to write code in Rust that can be called directly from Elixir, giving you the performance benefits of Rust without sacrificing the ease of use of Elixir.</p>
</blockquote>
<p>For this blog post, we're going to be using the <a href="https://github.com/yobix-ai/extractous">Extractous library</a> which provides fast and efficient unstructured data extraction in Rust. This combined with the NIFs in Elixir gives us a powerful combination for parsing PDFs.</p>
<h2>The Setup</h2>
<p>First things first, ensure you have Elixir and Rust installed on your machine.</p>
<p>Let's begin by creating a new LiveView Elixir application that will allow users to upload a PDF file and see a breakdown of the contents. We won't be needing any database functionality for this so we can use the <code>--no-ecto</code> flag to skip the database setup.</p>
<pre><code>mix phx<span>.</span>new elixir_pdf <span>--</span>no<span>-</span>ecto
</code></pre>
<p>We'll also need to add the <code>rustler</code> dependency to our <code>mix.exs</code> file so we can call Rust code from Elixir.</p>
<pre><code><span>defp</span> deps <span>do</span>
  <span>[</span>
    <span>{</span><span>:rustler</span><span>,</span> <span>"~&gt; 0.27.0"</span><span>}</span>
  <span>]</span>
<span>end</span>
</code></pre>
<p>Once we pull down our dependencies using <code>mix deps.get</code>, we can use <code>mix rustler.new</code> to generate our new Rust project in our code.</p>
<p>If you head to <code>lib/elixir_pdf/&lt;name_of_your_rust_project&gt;.ex</code>, you'll see that it's already generated a basic NIF for us. A default NIF implementation is provided for us, but we'll be implementing our own in the next step. I've named my Rust project <code>rustreader</code> for this example.</p>
<pre><code><span>defmodule</span> <span>RustReader</span> <span>do</span>
  <span>use</span> <span>Rustler</span><span>,</span> <span>otp_app:</span> <span>:elixir_pdf</span><span>,</span> <span>crate:</span> <span>"rustreader"</span>

  <span># Define the function that will be implemented in Rust</span>
  <span>def</span> <span>extract_pdf</span><span>(</span>_path<span>)</span><span>,</span> <span>do:</span> <span>:erlang</span><span>.</span><span>nif_error</span><span>(</span><span>:nif_not_loaded</span><span>)</span>
<span>end</span>

</code></pre>
<p>Now, let's grab the <code>extractous</code> library and add it to our <code>native/rustreader/Cargo.toml</code> file - this will allow us to use the <code>extractous</code> library in our Rust code.</p>
<pre><code><span>[</span>dependencies<span>]</span>
rustler <span>=</span> <span>"0.36.0"</span>
extractous <span>=</span> <span>"0.2.0"</span>
</code></pre>
<p>With this in place, we can run <code>cargo build</code> to build our Rust code - this will also pull down the <code>extractous</code> library and any other dependencies.</p>
<h2>The fun part - writing some code</h2>
<p>Next we need to actually write some Rust code to implement the <code>extract_pdf</code> function in our <code>native/rustreader/src/lib.rs</code> file.</p>
<pre><code><span>use</span> <span>extractous<span>::</span></span><span>Extractor</span><span>;</span>
<span>use</span> <span>rustler<span>::</span></span><span>{</span><span>Encoder</span><span>,</span> <span>Env</span><span>,</span> <span>NifResult</span><span>,</span> <span>Term</span><span>}</span><span>;</span>

<span>#[rustler::nif(schedule = <span>"DirtyCpu"</span>)]</span>
<span>fn</span> <span>extract_pdf</span><span>(</span>path<span>:</span> <span>String</span><span>)</span> <span>-&gt;</span> <span>NifResult</span><span>&lt;</span><span>(</span><span>String</span><span>,</span> <span>String</span><span>)</span><span>&gt;</span> <span>{</span>
    <span>let</span> extractor <span>=</span> <span>Extractor</span><span>::</span><span>new</span><span>(</span><span>)</span><span>;</span>

    <span>match</span> extractor<span>.</span><span>extract_file_to_string</span><span>(</span><span>&amp;</span>path<span>)</span> <span>{</span>
        <span>Ok</span><span>(</span><span>(</span>content<span>,</span> metadata<span>)</span><span>)</span> <span>=&gt;</span> <span>Ok</span><span>(</span><span>(</span>content<span>,</span> <span>format!</span><span>(</span><span>"{:?}"</span><span>,</span> metadata<span>)</span><span>)</span><span>)</span><span>,</span>
        <span>Err</span><span>(</span>e<span>)</span> <span>=&gt;</span> <span>Err</span><span>(</span><span>rustler<span>::</span></span><span>Error</span><span>::</span><span>Term</span><span>(</span><span>Box</span><span>::</span><span>new</span><span>(</span><span>format!</span><span>(</span><span>"Extraction failed: {}"</span><span>,</span> e<span>)</span><span>)</span><span>)</span><span>)</span>
    <span>}</span>
<span>}</span>

<span>rustler<span>::</span></span><span>init!</span><span>(</span><span>"Elixir.RustReader"</span><span>,</span> <span>[</span>extract_pdf<span>]</span><span>)</span><span>;</span>
</code></pre>
<p>This code will define a new instance of the <code>Extractor</code> struct and use it to extract the contents of the PDF file. We'll then return the contents and the metadata as a tuple.</p>
<p>The magic of the <code>rustler::init!</code> macro is that it will automatically generate the necessary code to call the Rust function from Elixir.</p>
<p>Astute observers will note our use of the <code>DirtyCpu</code> schedule. This ingenious feature instructs Rustler and the BEAM to automatically schedule our task in a manner that prevents global blocking during execution. This functionality, known as a DirtyNif, significantly simplifies our work compared to the complexities of manual implementation in C.</p>

<p>Now we need to write a some simple LiveView Elixir code to allow users to upload a PDF file and then call our Rust function from the server.</p>
<pre><code><span>defmodule</span> <span>ElixirPdfWeb</span><span>.</span><span>HomeLive</span> <span>do</span>
  <span>use</span> <span>ElixirPdfWeb</span><span>,</span> <span>:live_view</span>

  <span>@impl</span> <span>true</span>
  <span>def</span> <span>mount</span><span>(</span>_params<span>,</span> _session<span>,</span> socket<span>)</span> <span>do</span>
    <span>{</span><span>:ok</span><span>,</span>
     socket
     <span>|&gt;</span> <span>assign</span><span>(</span><span>:uploaded_files</span><span>,</span> <span>[</span><span>]</span><span>)</span>
     <span>|&gt;</span> <span>allow_upload</span><span>(</span><span>:pdf</span><span>,</span>
       <span>accept:</span> <span>~w(.pdf)</span><span>,</span>
       <span>max_entries:</span> <span>1</span><span>,</span>
       <span># 10MB limit</span>
       <span>max_file_size:</span> <span>10_000_000</span><span>,</span>
       <span>chunk_size:</span> <span>64_000</span>
     <span>)</span><span>}</span>
  <span>end</span>

  <span>@impl</span> <span>true</span>
  <span>def</span> <span>handle_event</span><span>(</span><span>"validate"</span><span>,</span> _params<span>,</span> socket<span>)</span> <span>do</span>
    <span>{</span><span>:noreply</span><span>,</span> socket<span>}</span>
  <span>end</span>

  <span>@impl</span> <span>true</span>
  <span>def</span> <span>handle_event</span><span>(</span><span>"save"</span><span>,</span> _params<span>,</span> socket<span>)</span> <span>do</span>
    uploaded_files <span>=</span>
      <span>consume_uploaded_entries</span><span>(</span>socket<span>,</span> <span>:pdf</span><span>,</span> <span>fn</span> <span>%</span><span>{</span><span>path:</span> path<span>}</span><span>,</span> _entry <span>-&gt;</span>
        dest <span>=</span> <span>Path</span><span>.</span><span>join</span><span>(</span><span>[</span><span>"priv"</span><span>,</span> <span>"static"</span><span>,</span> <span>"uploads"</span><span>,</span> <span>Path</span><span>.</span><span>basename</span><span>(</span>path<span>)</span><span>]</span><span>)</span>
        <span>File</span><span>.</span><span>cp!</span><span>(</span>path<span>,</span> dest<span>)</span>
        <span>{</span><span>:ok</span><span>,</span> dest<span>}</span>
      <span>end</span><span>)</span>

    pdf_document <span>=</span>
      uploaded_files
      <span>|&gt;</span> <span>hd</span><span>(</span><span>)</span>

    <span>{</span><span>:noreply</span><span>,</span>
     socket
     <span>|&gt;</span> <span>assign</span><span>(</span><span>:pdf_document</span><span>,</span> pdf_document<span>)</span>
     <span>|&gt;</span> <span>update</span><span>(</span><span>:uploaded_files</span><span>,</span> <span>&amp;</span><span>(</span><span>&amp;1</span> <span>++</span> uploaded_files<span>)</span><span>)</span><span>}</span>
  <span>end</span>
<span>end</span>
</code></pre>
<p>Alongside this we need to add a little bit of code to our <code>router.ex</code> file to allow us to upload files.</p>
<pre><code>scope <span>"/"</span><span>,</span> <span>ElixirPdfWeb</span> <span>do</span>
  pipe_through <span>:browser</span>

  live <span>"/"</span><span>,</span> <span>HomeLive</span>
<span>end</span>
</code></pre>
<p>We also need a simple LiveView template to allow users to upload a PDF file and see the results.</p>
<pre><code>&lt;div class<span>=</span><span>"mx-auto max-w-2xl py-8"</span>&gt;
  <span>&lt;</span>div class<span>=</span><span>"flex flex-col items-center justify-center"</span>&gt;
    <span>&lt;</span>h1 class<span>=</span><span>"text-2xl font-bold mb-8"</span>&gt;<span>Upload</span> <span>PDF</span>&lt;<span>/</span>h1<span>&gt;</span>

    <span>&lt;</span>form phx<span>-</span>submit<span>=</span><span>"save"</span> phx<span>-</span>change<span>=</span><span>"validate"</span> class<span>=</span><span>"w-full"</span>&gt;
      <span>&lt;</span>div class<span>=</span><span>"flex flex-col items-center space-y-4 w-full"</span> phx<span>-</span>drop<span>-</span>target<span>=</span><span>{</span><span>@uploads</span><span>.</span>pdf<span>.</span>ref<span>}</span><span>&gt;</span>
        <span>&lt;</span>div class<span>=</span><span>"w-full border-2 border-dashed border-gray-300 rounded-lg p-12 text-center hover:border-gray-400 transition-colors"</span>&gt;
          <span>&lt;</span>div class<span>=</span><span>"space-y-2"</span>&gt;
            <span>&lt;</span>div class<span>=</span><span>"text-gray-600"</span>&gt;
              <span>Drag</span> <span>and</span> drop your <span>PDF</span> here <span>or</span>
              <span>&lt;</span>label class<span>=</span><span>"cursor-pointer text-blue-500 hover:text-blue-600"</span>&gt;
                browse <span>&lt;</span><span>.</span>live_file_input upload<span>=</span><span>{</span><span>@uploads</span><span>.</span>pdf<span>}</span> class<span>=</span><span>"hidden"</span> <span>/</span>&gt;
              <span>&lt;</span><span>/</span>label<span>&gt;</span>
            <span>&lt;</span><span>/</span>div<span>&gt;</span>
            <span>&lt;</span>p class<span>=</span><span>"text-xs text-gray-500"</span>&gt;<span>PDF</span> files only<span>,</span> up to 10MB<span>&lt;</span><span>/</span>p<span>&gt;</span>
          <span>&lt;</span><span>/</span>div<span>&gt;</span>
        <span>&lt;</span><span>/</span>div<span>&gt;</span>

        <span>&lt;</span><span>%</span><span>=</span> <span>for</span> entry <span>&lt;-</span> <span>@uploads</span><span>.</span>pdf<span>.</span>entries <span>do</span> <span>%</span><span>&gt;</span>
          <span>&lt;</span>div class<span>=</span><span>"w-full"</span>&gt;
            <span>&lt;</span>div class<span>=</span><span>"flex items-center justify-between p-4 bg-gray-50 rounded"</span>&gt;
              <span>&lt;</span>div class<span>=</span><span>"flex items-center space-x-2"</span>&gt;
                <span>&lt;</span>span class<span>=</span><span>"font-medium"</span>&gt;<span>{</span>entry<span>.</span>client_name<span>}</span><span>&lt;</span><span>/</span>span<span>&gt;</span>
                <span>&lt;</span>span class<span>=</span><span>"text-sm text-gray-500"</span>&gt;
                  <span>(</span><span>{</span>entry<span>.</span>client_size<span>}</span><span>B</span><span>)</span>
                <span>&lt;</span><span>/</span>span<span>&gt;</span>
              <span>&lt;</span><span>/</span>div<span>&gt;</span>

              <span>&lt;</span>button
                type<span>=</span><span>"button"</span>
                class<span>=</span><span>"text-red-500 hover:text-red-700"</span>
                phx<span>-</span>click<span>=</span><span>"cancel-upload"</span>
                phx<span>-</span>value<span>-</span>ref<span>=</span><span>{</span>entry<span>.</span>ref<span>}</span>
              <span>&gt;</span>
                <span>&amp;</span>times;
              <span>&lt;</span><span>/</span>button<span>&gt;</span>
            <span>&lt;</span><span>/</span>div<span>&gt;</span>

            <span>&lt;</span><span>%</span><span>=</span> <span>for</span> err <span>&lt;-</span> <span>upload_errors</span><span>(</span><span>@uploads</span><span>.</span>pdf<span>,</span> entry<span>)</span> <span>do</span> <span>%</span><span>&gt;</span>
              <span>&lt;</span>div class<span>=</span><span>"text-red-500 text-sm"</span>&gt;
                <span>{</span>err<span>}</span>
              <span>&lt;</span><span>/</span>div<span>&gt;</span>
            <span>&lt;</span><span>%</span> <span>end</span> <span>%</span><span>&gt;</span>
          <span>&lt;</span><span>/</span>div<span>&gt;</span>
        <span>&lt;</span><span>%</span> <span>end</span> <span>%</span><span>&gt;</span>

        <span>&lt;</span><span>%</span><span>=</span> <span>if</span> <span>length</span><span>(</span><span>@uploads</span><span>.</span>pdf<span>.</span>entries<span>)</span> <span>&gt;</span> <span>0</span> <span>do</span> <span>%</span><span>&gt;</span>
          <span>&lt;</span>button
            type<span>=</span><span>"submit"</span>
            class<span>=</span><span>"px-4 py-2 bg-blue-500 text-white rounded hover:bg-blue-600 transition-colors"</span>
          <span>&gt;</span>
            <span>Upload</span>
          <span>&lt;</span><span>/</span>button<span>&gt;</span>
        <span>&lt;</span><span>%</span> <span>end</span> <span>%</span><span>&gt;</span>
      <span>&lt;</span><span>/</span>div<span>&gt;</span>
    <span>&lt;</span><span>/</span>form<span>&gt;</span>
  <span>&lt;</span><span>/</span>div<span>&gt;</span>
<span>&lt;</span><span>/</span>div<span>&gt;</span>
</code></pre>
<h2>Putting it all together</h2>
<p>So we can upload a PDF file - but let's call our Rust function and see what it returns.</p>
<p>In our handle_event function, we can call our Rust function as simply as this:</p>
<pre><code>  <span>@impl</span> <span>true</span>
  <span>def</span> <span>handle_event</span><span>(</span><span>"save"</span><span>,</span> _params<span>,</span> socket<span>)</span> <span>do</span>
    uploaded_files <span>=</span>
      <span>consume_uploaded_entries</span><span>(</span>socket<span>,</span> <span>:pdf</span><span>,</span> <span>fn</span> <span>%</span><span>{</span><span>path:</span> path<span>}</span><span>,</span> _entry <span>-&gt;</span>
        dest <span>=</span> <span>Path</span><span>.</span><span>join</span><span>(</span><span>[</span><span>"priv"</span><span>,</span> <span>"static"</span><span>,</span> <span>"uploads"</span><span>,</span> <span>Path</span><span>.</span><span>basename</span><span>(</span>path<span>)</span><span>]</span><span>)</span>
        <span>File</span><span>.</span><span>cp!</span><span>(</span>path<span>,</span> dest<span>)</span>
        <span>{</span><span>:ok</span><span>,</span> dest<span>}</span>
      <span>end</span><span>)</span>

    pdf_document <span>=</span>
      uploaded_files
      <span>|&gt;</span> <span>hd</span><span>(</span><span>)</span>
      <span>|&gt;</span> <span>RustReader</span><span>.</span><span>extract_pdf</span><span>(</span><span>)</span> <span>## This is where the magic happens!</span>

    <span>{</span><span>:noreply</span><span>,</span>
     socket
     <span>|&gt;</span> <span>assign</span><span>(</span><span>:pdf_document</span><span>,</span> pdf_document<span>)</span>
     <span>|&gt;</span> <span>update</span><span>(</span><span>:uploaded_files</span><span>,</span> <span>&amp;</span><span>(</span><span>&amp;1</span> <span>++</span> uploaded_files<span>)</span><span>)</span><span>}</span>
  <span>end</span>
</code></pre>
<p>We're grabbing the first uploaded file and calling our Rust function. The result is a tuple containing the contents of the PDF and the metadata.</p>
<p>Let's try it out with the <a href="https://liveviewcookbook.com/">LiveView Cookbook PDF</a>.</p>
<p><img src="https://i.imgur.com/dpyzduq.png" alt="Unstructured"></p>
<p>Success! We've now got a PDF parser that's fast, efficient, and written in Rust.</p>
<p>But that's quite hard to read so we're not done yet, let's make this a little nicer to work with.</p>
<p>Let's create a new module to handle the Jason encoding of the metadata.</p>
<pre><code><span>defmodule</span> <span>ElixirPdf</span><span>.</span><span>PdfDocument</span> <span>do</span>
  <span>@derive</span> <span>{</span><span>Jason</span><span>.</span><span>Encoder</span><span>,</span> <span>only:</span> <span>[</span><span>:content</span><span>,</span> <span>:metadata</span><span>]</span><span>}</span>
  <span>defstruct</span> <span>[</span><span>:content</span><span>,</span> <span>:metadata</span><span>]</span>

  <span>def</span> <span>from_rustler</span><span>(</span><span>{</span>content<span>,</span> metadata_json<span>}</span><span>)</span> <span>do</span>
    with <span>{</span><span>:ok</span><span>,</span> metadata<span>}</span> <span>&lt;-</span> <span>Jason</span><span>.</span><span>decode</span><span>(</span>metadata_json<span>)</span> <span>do</span>
      <span>%</span>__MODULE__<span>{</span>
        <span>content:</span> <span>String</span><span>.</span><span>trim</span><span>(</span>content<span>)</span><span>,</span>
        <span>metadata:</span> metadata
      <span>}</span>
    <span>end</span>
  <span>end</span>
<span>end</span>
</code></pre>
<p>This will allow us to encode the metadata to JSON and decode it back to a struct in Elixir to make it easier to work with.</p>
<p>All we have to do is pipe the result of our Rust function through this module and we're done!</p>
<pre><code><span>...</span>
<span>RustReader</span><span>.</span><span>extract_pdf</span><span>(</span><span>@pdf_document</span><span>)</span>
<span>|&gt;</span> <span>ElixirPdf</span><span>.</span><span>PdfDocument</span><span>.</span><span>from_rustler</span><span>(</span><span>)</span>
<span>...</span>
</code></pre>
<p>Now when we upload a PDF file, we'll see the metadata results in a much more readable format.</p>
<p><img src="https://i.imgur.com/QvG2QUV.png" alt="Structured"></p>
<p>Much better!</p>
<p>This approach is simple and effective - it's fast, efficient, and leverages the strengths of both Elixir and Rust to provide a robust solution for PDF parsing.</p>
<p>We're only talking about PDF files here but extractous supports a <a href="https://github.com/yobix-ai/extractous?tab=readme-ov-file#-supported-file-formats">wide range of file types</a> - so keep that in mind if you need to extract data from other file types.</p>
<h2>What about deployment?</h2>
<p>Keep in mind that this is a native extension and so you'll need to build the Rust code before deploying your application. This can be done in a CI/CD pipeline or manually.</p>
<p>If you're using Docker, you can update the <code>Dockerfile</code> to build the Rust code as part of the build process and update <code>config/prod.exs</code> to tell Rustler to skip compilation and load the compiled NIF from where it was built in the Docker image.</p>
<p>Check out the <a href="https://fly.io/phoenix-files/elixir-and-rust-is-a-good-mix/">Fly.io blog post</a> for more information on how to deploy an Elixir application with Rust NIFs.</p>
<h2>Shoutouts</h2>
<p>Some shoutouts are in order - firstly this blog post from <a href="https://fly.io/phoenix-files/elixir-and-rust-is-a-good-mix/">Fly.io's Phoenix Files</a> outlining how to use Rust with NIFs in Elixir. It was a key inspiration for this approach and gave me the idea to use Rust in the first place. Also check out Fly in general for some great Elixir hosting options - I use them for all my Elixir applications.</p>
<p>Also a shoutout for the excellent <a href="https://github.com/yobix-ai/extractous">Extractous library</a> which provides fast and efficient unstructured data extraction in Rust - it's also 25x faster than the very popular unstructured-io library.</p>
<p>Finally, a shoutout to the <a href="https://github.com/rusterlium/rustler">Rustler</a> library for providing a simple way to call Rust code from Elixir!</p>

<p>All the code for this blog post can be <a href="https://github.com/chrisgreg/elixir_pdf_tutorial">found here on my Github</a> if anyone wants to clone it and run it yourselves!</p>
<p>I hope you found this post useful, subscribe to my Substack below for similar content and  <a href="https://www.twitter.com/codestirring">follow me on Twitter</a> and <a href="https://bsky.app/profile/codestirring.bsky.social">Bluesky</a> for more Elixir (and general programming) tips.</p>
<p>If you're building a Phoenix project, I'd also encourage you to take a look at my open-source component library <a href="https://bloom-ui.fly.dev/">Bloom</a> to help you out even further or check out my <a href="https://mmbl.io/">new voice to notes application to automatically tag and sync your voice to your calendar</a>.</p><div><p><img src="https://www.liveviewcookbook.com/liveview-cook-book2.png"></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making the video that made Gorillaz (200 pts)]]></title>
            <link>https://animationobsessive.substack.com/p/making-the-video-that-made-gorillaz</link>
            <guid>42870990</guid>
            <pubDate>Wed, 29 Jan 2025 20:50:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://animationobsessive.substack.com/p/making-the-video-that-made-gorillaz">https://animationobsessive.substack.com/p/making-the-video-that-made-gorillaz</a>, See on <a href="https://news.ycombinator.com/item?id=42870990">Hacker News</a></p>
Couldn't get https://animationobsessive.substack.com/p/making-the-video-that-made-gorillaz: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Soviet Shoe Factory Principle (142 pts)]]></title>
            <link>https://wiki.c2.com/?SovietShoeFactoryPrinciple</link>
            <guid>42870690</guid>
            <pubDate>Wed, 29 Jan 2025 20:26:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wiki.c2.com/?SovietShoeFactoryPrinciple">https://wiki.c2.com/?SovietShoeFactoryPrinciple</a>, See on <a href="https://news.ycombinator.com/item?id=42870690">Hacker News</a></p>
Couldn't get https://wiki.c2.com/?SovietShoeFactoryPrinciple: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Dead Games (159 pts)]]></title>
            <link>https://garry.net/posts/dead-games</link>
            <guid>42870230</guid>
            <pubDate>Wed, 29 Jan 2025 19:52:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://garry.net/posts/dead-games">https://garry.net/posts/dead-games</a>, See on <a href="https://news.ycombinator.com/item?id=42870230">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="site-body" b-14kpe6t92i=""><article b-14kpe6t92i="">






<div b-yrzs0ob67i=""><p>You know when people spend 2 years making games and when they release, they don't sell anything, so they stop selling it and throw it all in then bin..</p><p>What happens to all those sounds and models and animations they made? Do they just go in the bin?</p><p>I guess there's reasons they don't just slap an open license on them and release them for free. Like time, storage, bandwidth, and permission. Maybe they use textures or sounds from a library and can't redistribute?</p><p>Well, I was just thinking, what a shame it all gets chucked in the bin, when it'd be nice to be able to use those assets in something like gmod/sbox.&nbsp;</p><p>If your game died and you have some cool assets, email me and tell me what you've got and what you want for them.. garrynewman@gmail.com</p></div></article></div><p>
        
            An error has occurred. This application may no longer respond until reloaded.
        
        
        <a href="">Reload</a>
        <a>🗙</a>
    </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Airflow – Stream media files directly from macOS to AirPlay devices (161 pts)]]></title>
            <link>https://airflow.app/</link>
            <guid>42870171</guid>
            <pubDate>Wed, 29 Jan 2025 19:47:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://airflow.app/">https://airflow.app/</a>, See on <a href="https://news.ycombinator.com/item?id=42870171">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>
<h2>What if we say it's not like the others?</h2>
<p><img src="https://airflow.app/images/pipeline.svg"></p><p>Airflow is <strong>different</strong> We're not cutting any corners. This is <em>not yet another FFmpeg wrapper</em> like you might have seen elsewhere. Don't get us wrong, we love FFmpeg and use many of its parts under the hood, but our custom built video processing pipeline goes way beyond wrapping FFmpeg and calling it a day. We've been working on it for years it and <strong>it lets us do things that other similar software <span id="what_container">simply can't</span></strong>.</p>


<h2>...with a very particular set of skills...</h2>
<p><img src="https://airflow.app/images/device.png" srcset="https://airflow.app/images/device@2x.png 2x"></p><p>Airflow is a razor sharp focused software. It supports specific set of devices and it will pull every trick in the book to get the best possible results on these devices. It may not stream video to your smart fridge, but it will gladly <em>push your Chromecast, Apple TV and AirPlay 2 TVs to their limits</em>.</p>
<p>And yes, Airflow can handle pretty much <strong>any video format and codec you throw at it</strong>.</p>
</div>
<div>
<h2>Pixels, pixels everywhere!</h2>
<p>Airflow can stream full <strong>4K HDR</strong> HEVC files to Chromecast Ultra, Built-in, Apple TV 4K and AirPlay 2 enabled TVs. It will go out of its way not to touch the original video stream unless absolutely needed for compatibility reasons, ensuring <strong>best possible video quality</strong> with <strong>lowest CPU load</strong> (your computer fans will thank you). As far as we can tell, Airflow is still <em>the only desktop software that can natively stream HEVC videos to Apple TV and AirPlay 2 TVs</em>.</p>
<p>And for those pesky videos that are incompatible with your device - Airflow will handle that tranparently, with <em>hardware accelerated transcoding</em> if your computer supports it.</p>
<p><img src="https://airflow.app/images/settings.png" srcset="https://airflow.app/images/settings@2x.png 2x">
</p></div>
<div>
<h2>Audio pipeline that goes to eleven</h2>
<p><img src="https://airflow.app/images/audio-delay.png" srcset="https://airflow.app/images/audio-delay@2x.png 2x"></p><p>Full <strong>multichannel support</strong> including DD+ passthrough with Dolby Atmos? Of course.</p>
<p><strong>Advanced adaptive volume booster + limiter</strong> for late night watching when you don't want to disturb your neighbours with loud scenes but still want to hear the dialogue clearly? Check.</p>
<p><em>Spatial headphone downmix</em> for surround sound videos? Also check.</p>
<p>Detailed <em>A/V sync adjustment</em> where you can compensate for the delay of individual devices like bluetooth headphones? Airflow has it.</p>
</div>
<div>
<h2>And subtitle support to match it</h2>
<p><img src="https://airflow.app/images/subtitle-menu.png" srcset="https://airflow.app/images/subtitle-menu@2x.png 2x"></p><p>For both <strong>embedded and external subtitles</strong>. It's a bit of a secret that pretty much every other streaming software needs to extract embedded subtitle tracks before playing the video. That involves reading the entire file upfront! Crazy, right? <em>Airflow needs no such crude tricks</em>. Embedded or external, for our playback pipeline it's all the same. All widely used subtitle formats are supported, now including vobsub. Integrated <strong>opensubtitles.org</strong> search is a cherry on top.</p>
<p><img src="https://airflow.app/images/sub-search.png" srcset="https://airflow.app/images/sub-search@2x.png 2x"></p><h2>...with real time text recognition</h2>
<p>Some subtitles (DVD, Vobsub, Bluray) are stored as pictures. This means that the only way to render them when streaming is to burn them in the video. That's inconvenient to say the least. It massively increases CPU load (think fan noise and heat) and it's completely infeasible to do for 4K videos.</p>
<p>Enter our new <em>realtime subtitle text recognition (OCR)</em>. During playback Airflow will transparently extract the text from picture subtitles and render it on target device just like it would with regular text subtitles.</p>
<p><img src="https://airflow.app/images/vobsub.png">
<img src="https://airflow.app/images/down-arrow.svg"></p><p>
    We'll be in <i>bigger trouble</i><br>
    if the thief has it.
</p>
</div>
<div>
<h2>But wait, there's more!</h2>
<p><img src="https://airflow.app/images/speed-test.png" srcset="https://airflow.app/images/speed-test@2x.png 2x"></p><p>The "small" things, like the <strong>scrubbing preview</strong>, <em>beautiful polished user interface</em>, multiple playlists support, meticulous last position tracking, or the integrated <strong>Speed Test for Chromecast</strong>, which is invaluable when dealing with network connection issues. The list goes on.</p>

<p><img src="https://airflow.app/images/screenshot-remote-app.png" srcset="https://airflow.app/images/screenshot-remote-app@2x.png 2x"></p><p>Did we mention the <strong>remote control companion app</strong> for Android and iPhone? No? Well, it's pretty cool. It lets you control all Airflow features from the comfort of your couch. And it's completely free!</p>
<p><a href="https://apps.apple.com/us/app/airflow-remote/id1459536549?ls=1"><img src="https://airflow.app/images/appstore.svg"></a>
     <a href="https://play.google.com/store/apps/details?id=xyz.bitcave.airflow.remote"><img src="https://airflow.app/images/googleplay_alt.svg"></a>
</p>
</div>
</div><div>
<h2>F.A.Q.</h2>
<h3>Is Airflow a subscription of any kind?</h3>
<p>Nope. One time payment only. Airflow license does not expire.</p>
<h3>What if I have multiple computers? Do you I need multiple licenses?</h3>
<p>You only need one license. You can activate Airflow on all computers that you own using single license.</p>
<h3>I lost my license key</h3>
<p>No worries. You can always retreive your license <a href="https://license.airflow.app/my">here</a>.</p>
<h3>I'm having issue with Airflow</h3>
<p>Please see our <a href="https://airflow.app/troubleshooting">troubleshooting page</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Waymo to test its autonomous driving technology in over 10 new cities (135 pts)]]></title>
            <link>https://www.reuters.com/business/autos-transportation/alphabets-waymo-test-its-autonomous-driving-technology-over-10-new-cities-2025-01-29/</link>
            <guid>42870056</guid>
            <pubDate>Wed, 29 Jan 2025 19:38:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/business/autos-transportation/alphabets-waymo-test-its-autonomous-driving-technology-over-10-new-cities-2025-01-29/">https://www.reuters.com/business/autos-transportation/alphabets-waymo-test-its-autonomous-driving-technology-over-10-new-cities-2025-01-29/</a>, See on <a href="https://news.ycombinator.com/item?id=42870056">Hacker News</a></p>
Couldn't get https://www.reuters.com/business/autos-transportation/alphabets-waymo-test-its-autonomous-driving-technology-over-10-new-cities-2025-01-29/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[SmolGPT: A minimal PyTorch implementation for training a small LLM from scratch (327 pts)]]></title>
            <link>https://github.com/Om-Alve/smolGPT</link>
            <guid>42868770</guid>
            <pubDate>Wed, 29 Jan 2025 18:09:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Om-Alve/smolGPT">https://github.com/Om-Alve/smolGPT</a>, See on <a href="https://news.ycombinator.com/item?id=42868770">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">SMOL-GPT 🦾</h2><a id="user-content-smol-gpt-" aria-label="Permalink: SMOL-GPT 🦾" href="#smol-gpt-"></a></p>
<p dir="auto">A minimal PyTorch implementation for training your own small LLM from scratch. Designed for educational purposes and simplicity, featuring efficient training, flash attention, and modern sampling techniques.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features ✨</h2><a id="user-content-features-" aria-label="Permalink: Features ✨" href="#features-"></a></p>
<ul dir="auto">
<li><strong>Minimal Codebase</strong>: Pure PyTorch implementation with no abstraction overhead</li>
<li><strong>Modern Architecture</strong>: GPT model with:
<ul dir="auto">
<li>Flash Attention (when available)</li>
<li>RMSNorm and SwiGLU</li>
<li>Efficient top-k/p/min-p sampling</li>
</ul>
</li>
<li><strong>Training Features</strong>:
<ul dir="auto">
<li>Mixed precision (bfloat16/float16)</li>
<li>Gradient accumulation</li>
<li>Learning rate decay with warmup</li>
<li>Weight decay &amp; gradient clipping</li>
</ul>
</li>
<li><strong>Dataset Support</strong>: Built-in TinyStories dataset processing</li>
<li><strong>Custom Tokenizer</strong>: SentencePiece tokenizer training integration</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation 🛠️</h2><a id="user-content-installation-️" aria-label="Permalink: Installation 🛠️" href="#installation-️"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install torch sentencepiece tqdm requests numpy"><pre>pip install torch sentencepiece tqdm requests numpy</pre></div>
<p dir="auto"><strong>Requirements</strong>:</p>
<ul dir="auto">
<li>Python 3.8+</li>
<li>PyTorch 2.0+ with CUDA</li>
<li>Modern GPU (recommended)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start 🚀</h2><a id="user-content-quick-start-" aria-label="Permalink: Quick Start 🚀" href="#quick-start-"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Option 1: Full Training Cycle</h3><a id="user-content-option-1-full-training-cycle" aria-label="Permalink: Option 1: Full Training Cycle" href="#option-1-full-training-cycle"></a></p>
<ol dir="auto">
<li><strong>Prepare Dataset</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python preprocess.py prepare-dataset --vocab-size 4096"><pre>python preprocess.py prepare-dataset --vocab-size 4096</pre></div>
<ol start="2" dir="auto">
<li><strong>Start Training</strong></li>
</ol>

<ol start="3" dir="auto">
<li><strong>Generate Text</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python sample.py \
    --prompt &quot;Once upon a time&quot; \
    --num_samples 3 \
    --temperature 0.7 \
    --max_new_tokens 500"><pre>python sample.py \
    --prompt <span><span>"</span>Once upon a time<span>"</span></span> \
    --num_samples 3 \
    --temperature 0.7 \
    --max_new_tokens 500</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Option 2: Use Pre-trained Model</h3><a id="user-content-option-2-use-pre-trained-model" aria-label="Permalink: Option 2: Use Pre-trained Model" href="#option-2-use-pre-trained-model"></a></p>
<ol dir="auto">
<li><strong>Download Assets</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Download tokenizer
wget https://huggingface.co/OmAlve/TinyStories-SmolGPT/resolve/main/tok4096.model -P data/

# Download pre-trained checkpoint
wget https://huggingface.co/OmAlve/TinyStories-SmolGPT/resolve/main/ckpt-v1.pt -P out/"><pre><span><span>#</span> Download tokenizer</span>
wget https://huggingface.co/OmAlve/TinyStories-SmolGPT/resolve/main/tok4096.model -P data/

<span><span>#</span> Download pre-trained checkpoint</span>
wget https://huggingface.co/OmAlve/TinyStories-SmolGPT/resolve/main/ckpt-v1.pt -P out/</pre></div>
<ol start="2" dir="auto">
<li><strong>Run Inference</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python sample.py \
    --prompt &quot;Once upon a time&quot; \
    --tokenizer_path data/tok4096.model \
    --ckpt_dir out/ \
    --num_samples 3 \
    --max_new_tokens 200 \
    --temperature 0.7"><pre>python sample.py \
    --prompt <span><span>"</span>Once upon a time<span>"</span></span> \
    --tokenizer_path data/tok4096.model \
    --ckpt_dir out/ \
    --num_samples 3 \
    --max_new_tokens 200 \
    --temperature 0.7</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Pre-trained Model Details 🔍</h2><a id="user-content-pre-trained-model-details-" aria-label="Permalink: Pre-trained Model Details 🔍" href="#pre-trained-model-details-"></a></p>
<p dir="auto">The provided checkpoint was trained on the TinyStories dataset.</p>
<p dir="auto">Architecture:</p>
<ul dir="auto">
<li>4096-token vocabulary</li>
<li>8 heads</li>
<li>8-layer transformer</li>
<li>512 embedding dimension</li>
<li>Trained on <code>~4 Billion Tokens</code> for around <code>18.5</code> hours</li>
</ul>
<p dir="auto">Validation Loss - <code>1.0491</code></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Om-Alve/smolGPT/blob/main/assets/loss.png"><img src="https://github.com/Om-Alve/smolGPT/raw/main/assets/loss.png" alt="Loss Curve"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Sample Outputs 📝</h2><a id="user-content-sample-outputs-" aria-label="Permalink: Sample Outputs 📝" href="#sample-outputs-"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example 1</h3><a id="user-content-example-1" aria-label="Permalink: Example 1" href="#example-1"></a></p>
<div data-snippet-clipboard-copy-content="Prompt: One day, Lily met a unicorn

Output:
One day, Lily met a unicorn in the park. The unicorn had shiny fur and a pretty dress. Lily asked the unicorn, &quot;Where did you come from?&quot;
The unicorn replied, &quot;I came from the forest and wanted to meet you, Lily. I am here to make sure you are safe.&quot;
Lily and the unicorn played together and had lots of fun. But then, the unicorn started to act funny. Lily didn't know what was happening. Suddenly, the unicorn turned into a little girl and said, &quot;I was under a spell, and your kindness broke it. Thank you for breaking it.&quot;
Lily was surprised and happy. She gave the unicorn a big hug and they played together all day. From that day on, the unicorn would always come to play with Lily - her new friend."><pre lang="text"><code>Prompt: One day, Lily met a unicorn

Output:
One day, Lily met a unicorn in the park. The unicorn had shiny fur and a pretty dress. Lily asked the unicorn, "Where did you come from?"
The unicorn replied, "I came from the forest and wanted to meet you, Lily. I am here to make sure you are safe."
Lily and the unicorn played together and had lots of fun. But then, the unicorn started to act funny. Lily didn't know what was happening. Suddenly, the unicorn turned into a little girl and said, "I was under a spell, and your kindness broke it. Thank you for breaking it."
Lily was surprised and happy. She gave the unicorn a big hug and they played together all day. From that day on, the unicorn would always come to play with Lily - her new friend.
</code></pre></div>
<div data-snippet-clipboard-copy-content="Prompt: The dragon flew over the mountains

Output:
The dragon flew over the mountains, over the rivers and over the rivers. He was very brave and strong.
One day, the dragon saw something very strange. It was a big, shiny rock. He wanted to know what it was, so he flew down and touched it with his nose. Suddenly, the rock began to move!
The dragon was so surprised! He had never seen anything like it before. He looked around and saw that it was a little mouse! The mouse was very scared and started to run away.
The dragon was very sad. He wanted to help the mouse, so he decided to try and make friends. He flew around and around until he found the mouse. He said hello to the mouse and asked if he wanted to be friends.
The mouse was so happy! He said yes, and they played together all day long. From then on, the dragon and the mouse were the best of friends. They had lots of fun together and the dragon was never lonely again."><pre><code>Prompt: The dragon flew over the mountains

Output:
The dragon flew over the mountains, over the rivers and over the rivers. He was very brave and strong.
One day, the dragon saw something very strange. It was a big, shiny rock. He wanted to know what it was, so he flew down and touched it with his nose. Suddenly, the rock began to move!
The dragon was so surprised! He had never seen anything like it before. He looked around and saw that it was a little mouse! The mouse was very scared and started to run away.
The dragon was very sad. He wanted to help the mouse, so he decided to try and make friends. He flew around and around until he found the mouse. He said hello to the mouse and asked if he wanted to be friends.
The mouse was so happy! He said yes, and they played together all day long. From then on, the dragon and the mouse were the best of friends. They had lots of fun together and the dragon was never lonely again.
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration ⚙️</h2><a id="user-content-configuration-️" aria-label="Permalink: Configuration ⚙️" href="#configuration-️"></a></p>
<p dir="auto">Key parameters (modify in <code>config.py</code>):</p>
<p dir="auto"><strong>Model Architecture</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="GPTConfig(
    block_size=512,    # Context length
    n_layer=8,         # Number of transformer layers
    n_head=8,          # Number of attention heads
    n_embed=512,       # Embedding dimension
    dropout=0.2,       # Dropout rate
    bias=False         # Use bias in layers
)"><pre><span>GPTConfig</span>(
    <span>block_size</span><span>=</span><span>512</span>,    <span># Context length</span>
    <span>n_layer</span><span>=</span><span>8</span>,         <span># Number of transformer layers</span>
    <span>n_head</span><span>=</span><span>8</span>,          <span># Number of attention heads</span>
    <span>n_embed</span><span>=</span><span>512</span>,       <span># Embedding dimension</span>
    <span>dropout</span><span>=</span><span>0.2</span>,       <span># Dropout rate</span>
    <span>bias</span><span>=</span><span>False</span>         <span># Use bias in layers</span>
)</pre></div>
<p dir="auto"><strong>Training</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="TrainingConfig(
    batch_size=64,
    max_iters=30000,
    learning_rate=6e-4,
    weight_decay=0.1,
    grad_clip=1.0,
    warmup_iters=1000
)"><pre><span>TrainingConfig</span>(
    <span>batch_size</span><span>=</span><span>64</span>,
    <span>max_iters</span><span>=</span><span>30000</span>,
    <span>learning_rate</span><span>=</span><span>6e-4</span>,
    <span>weight_decay</span><span>=</span><span>0.1</span>,
    <span>grad_clip</span><span>=</span><span>1.0</span>,
    <span>warmup_iters</span><span>=</span><span>1000</span>
)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">File Structure 📁</h2><a id="user-content-file-structure-" aria-label="Permalink: File Structure 📁" href="#file-structure-"></a></p>
<div data-snippet-clipboard-copy-content="om-alve-smolgpt/
├── config.py       - Model &amp; training configuration
├── dataset.py      - Data loading &amp; preprocessing
├── model.py        - GPT model implementation
├── preprocess.py   - Dataset preparation scripts
├── sample.py       - Text generation script
├── tokenizer.py    - Tokenizer wrapper
└── train.py        - Main training loop"><pre><code>om-alve-smolgpt/
├── config.py       - Model &amp; training configuration
├── dataset.py      - Data loading &amp; preprocessing
├── model.py        - GPT model implementation
├── preprocess.py   - Dataset preparation scripts
├── sample.py       - Text generation script
├── tokenizer.py    - Tokenizer wrapper
└── train.py        - Main training loop
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing 🤝</h2><a id="user-content-contributing-" aria-label="Permalink: Contributing 🤝" href="#contributing-"></a></p>
<p dir="auto">Contributions welcome! Please open an issue or PR for:</p>
<ul dir="auto">
<li>Bug fixes</li>
<li>Performance improvements</li>
<li>New features</li>
</ul>
<hr>
<p dir="auto"><strong>Note</strong>: This implementation is inspired by modern LLM training practices and adapted for educational purposes. For production use, consider scaling up model size and dataset.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Adding iodine to salt played a role in cognitive improvements: research (2013) (212 pts)]]></title>
            <link>https://www.discovermagazine.com/health/how-adding-iodine-to-salt-boosted-americans-iq</link>
            <guid>42868718</guid>
            <pubDate>Wed, 29 Jan 2025 18:06:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.discovermagazine.com/health/how-adding-iodine-to-salt-boosted-americans-iq">https://www.discovermagazine.com/health/how-adding-iodine-to-salt-boosted-americans-iq</a>, See on <a href="https://news.ycombinator.com/item?id=42868718">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p><span>Iodized salt is so commonplace in the U.S. today that you may never have given the additive a second thought. But new research finds that humble iodine has played a substantial role in cognitive improvements seen across the American population in the 20th century.</span></p><p><span>Iodine is a critical micronutrient in the human diet — that is, something our bodies can’t synthesize that we have to rely on food to obtain — and it’s been added to salt (in the form of potassium iodide) since 1924. Originally, iodization was adopted to reduce the incidence of goiter, an enlargement of the thyroid gland. But research since then has found that iodine also plays a crucial role in brain development, especially during gestation.</span></p><p><span>Iodine deficiency&nbsp;today is the leading cause of preventable mental retardation in the world. It’s estimated that nearly one-third of the world’s population has a diet with too little iodine in it, and the problem isn’t limited to developing countries — perhaps&nbsp;</span><a href="http://www.who.int/nutrition/publications/VMNIS_Iodine_deficiency_in_Europe.pdf" color="accent" target="_blank" rel="noopener"><span>one-fifth of those cases are in Europe</span></a><span>&nbsp;(pdf), where iodized salt is still not the norm.</span></p><p id="iodines-natural-experiment"><h2><span>Iodine’s Natural Experiment</span></h2></p><p><span>With this background, then, a group of economists saw a natural experiment: comparing the intelligence of children born just before 1924 — the year iodization began — and those born just after. James Freyer, David Weil and Dimitra Politi used military data from the early 1900s&nbsp;1920s, when World War II drove millions of men and women to enlist.</span></p><p><span>Recruits all took a standardized intelligence test as part of their enlistment. Researchers didn’t have access to the test scores themselves, but they had a clever substitute: smarter recruits were assigned to the Air Forces while the less bright ones went to the Ground Forces. This allowed the researchers to infer test scores depending on which branch a recruit was selected for.</span></p><p><span>Intelligence data were paired with birthdate and hometown, since iodine levels in the soil and water vary significantly from place to place. To estimate which regions were naturally high-iodine and which were low, the researchers referred to nationwide statistics&nbsp;collected after World War I on the prevalence of goiter.</span></p><p><span>In all, researchers had sufficient data on about 2 million male recruits born between 1921 and 1927.</span></p><p id="stark-improvements"><h2><span>Stark Improvements</span></h2></p><p><span>The economists found that in the lowest-iodine areas — the bottom quarter of the study population — the introduction of iodized salt had stark effects. Men from these regions born in 1924 or later were significantly more likely to get into the Air Force and had an average IQ that was 15 points higher than their predecessors.</span></p><p><span>Nationwide, that averages out to a 3.5-point rise in IQ because of iodization, the researchers </span><a href="http://www.nber.org/papers/w19233" color="accent" target="_blank" rel="noopener"><span>report</span></a><span> in a paper for the National Bureau of Economic Research.</span></p><p><span>The initiative wasn’t without its drawbacks — sudden iodine supplementation among people who are deficient can cause thyroid-related deaths. The researchers estimate that 10,000 deaths in the decades after 1924 were caused by salt iodization.</span></p><p><span>But on the positive side, iodine deficiency and its symptoms were vanquished almost overnight. And iodine’s mental benefits may even help explain the Flynn Effect, which observes that IQ rose about 3 points per decade in developed countries throughout the 20th century. It’s been thought that improved health and nutrition were the driving forces of the Flynn Effect. Now, it appears that iodine alone was responsible for roughly one decade of that remarkable climb. All the more reason, then, for the rest of the world to follow suit and relegate iodine deficiency to the history books.</span></p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[No Man's Sky's update introduces billions of new stars, planets, and more (134 pts)]]></title>
            <link>https://blog.playstation.com/2025/01/29/no-mans-skys-latest-update-introduces-billions-of-new-stars-planets-and-more-today/</link>
            <guid>42868618</guid>
            <pubDate>Wed, 29 Jan 2025 17:59:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.playstation.com/2025/01/29/no-mans-skys-latest-update-introduces-billions-of-new-stars-planets-and-more-today/">https://blog.playstation.com/2025/01/29/no-mans-skys-latest-update-introduces-billions-of-new-stars-planets-and-more-today/</a>, See on <a href="https://news.ycombinator.com/item?id=42868618">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-400697">

				

				<div>

						<p><img fetchpriority="high" width="1088" height="612" src="https://blog.playstation.com/tachyon/2028/01/cd90ee7b3768d9e9329202d99226833849c79be2.png?resize=1088%2C612&amp;crop_strategy=smart" alt="No Man’s Sky’s latest update introduces billions of new stars, planets, and more today" sizes="(min-width: 1170px) 936px, (min-width: 960px) 80vw, 100vw" decoding="async" srcset="https://blog.playstation.com/tachyon/2028/01/cd90ee7b3768d9e9329202d99226833849c79be2.png?resize=1088%2C612&amp;crop_strategy=smart&amp;zoom=1 1088w, https://blog.playstation.com/tachyon/2028/01/cd90ee7b3768d9e9329202d99226833849c79be2.png?resize=1088%2C612&amp;crop_strategy=smart&amp;zoom=0.5 544w, https://blog.playstation.com/tachyon/2028/01/cd90ee7b3768d9e9329202d99226833849c79be2.png?resize=1088%2C612&amp;crop_strategy=smart&amp;zoom=0.38 413w, https://blog.playstation.com/tachyon/2028/01/cd90ee7b3768d9e9329202d99226833849c79be2.png?resize=1088%2C612&amp;crop_strategy=smart&amp;zoom=0.31 337w, https://blog.playstation.com/tachyon/2028/01/cd90ee7b3768d9e9329202d99226833849c79be2.png?resize=1088%2C612&amp;crop_strategy=smart&amp;zoom=0.25 272w, https://blog.playstation.com/tachyon/2028/01/cd90ee7b3768d9e9329202d99226833849c79be2.png?resize=1088%2C612&amp;crop_strategy=smart&amp;zoom=0.21 228w, https://blog.playstation.com/tachyon/2028/01/cd90ee7b3768d9e9329202d99226833849c79be2.png?resize=1088%2C612&amp;crop_strategy=smart&amp;zoom=0.16 174w, https://blog.playstation.com/tachyon/2028/01/cd90ee7b3768d9e9329202d99226833849c79be2.png?resize=1088%2C612&amp;crop_strategy=smart&amp;zoom=0.13 141w"></p>

						
						
						<div>
							


<p>Hello! Today we are releasing one of our biggest updates for <a href="https://www.playstation.com/games/no-mans-sky/">No Man’s Sky</a>. We can’t wait for you to see what we’ve been working on.</p>



<p>Last year, we released No Man’s Sky Worlds Part I (5.0) for PS4, PS5, and PS VR2 players, and it was one of our most successful to date. We’ve been planning the changes in Worlds Part II for a long time, and the wait is finally over.&nbsp;</p>


	<div data-youtube-id="BGfnL4s38sc" data-title="">
	<p>
		<img src="https://img.youtube.com/vi/BGfnL4s38sc/maxresdefault.jpg" data-srcset="https://img.youtube.com/vi/BGfnL4s38sc/default.jpg 120w, https://img.youtube.com/vi/BGfnL4s38sc/mqdefault.jpg 320w, https://img.youtube.com/vi/BGfnL4s38sc/hqdefault.jpg 480w, https://img.youtube.com/vi/BGfnL4s38sc/sddefault.jpg 640w, https://img.youtube.com/vi/BGfnL4s38sc/maxresdefault.jpg 1280w" data-sizes="(min-width: 1170px) 936px, (min-width: 960px) 80vw, 100vw" data-src="https://img.youtube.com/vi/BGfnL4s38sc/maxresdefault.jpg" alt="No Man’s Sky’s latest update introduces billions of new stars, planets, and more today" srcset="https://img.youtube.com/vi/BGfnL4s38sc/default.jpg 120w, https://img.youtube.com/vi/BGfnL4s38sc/mqdefault.jpg 320w, https://img.youtube.com/vi/BGfnL4s38sc/hqdefault.jpg 480w, https://img.youtube.com/vi/BGfnL4s38sc/sddefault.jpg 640w, https://img.youtube.com/vi/BGfnL4s38sc/maxresdefault.jpg 1280w"></p>
</div>
	



<p>One of the biggest reasons people play No Man’s Sky is for that sense of adventure and discovery, that feeling of flying to a planet, and landing to explore, knowing no one has ever been there before. With Worlds Part II, we added billions of new star systems and trillions of new planets to the universe. This allows us to push the boundaries of our engine and technology without changing the things people love about the game already. If you settled on your home planet with a beautiful base that you lovingly crafted, that is safe – but now there are new worlds to explore with a level of variety no one has seen before.</p>



<p><a href="https://live.staticflickr.com/65535/54291870269_4e4f33a4a1_h.jpg" target="_blank" rel="noopener"><img decoding="async" src="https://live.staticflickr.com/65535/54291870269_4e4f33a4a1_h.jpg" alt=""></a></p><p>Going to these new systems travelers will notice new terrain, new biomes, new flora, and new fauna. There’s a new terrain system that I’ve been working on for a while now. There are huge mountains to climb, oceans that are kilometers deep to discover, and caverns and canyons at a scale that wasn’t possible before.</p>



<p>In these systems you may even find enormous Gas Giants. These huge worlds can be ten times bigger than any other planet you have explored previously. Gas Giants and deep oceans both require high-level technology to explore. Gas storms, deep sea pressure, and anomalies bring new hazards and challenges to No Man’s Sky.</p>



<p><a href="https://live.staticflickr.com/65535/54291632791_a3101e4947_h.jpg" target="_blank" rel="noopener"><img decoding="async" src="https://live.staticflickr.com/65535/54291632791_a3101e4947_h.jpg" alt=""></a></p><p>Water gameplay has much added depth too, including improvements to our submarine, The Nautilus, as well as improvements to fishing and deep sea diving systems. New ocean tech allows water to react physically to the world around it. Dimples appear when it rains and wake appears as ships fly over the surface, and creatures and players can wade through creating waves.&nbsp;</p>



<p>Our lighting system has been completely rewritten. Shadows show more details, sunlight and ambient occlusion are sharper, and sunlight sparkling through leaves and metals looks crisp and beautiful. Starry night skies and wispy clouds reflect in the water of lakes and oceans.</p>



<p><a href="https://live.staticflickr.com/65535/54292060960_3f430425a6_h.jpg" target="_blank" rel="noopener"><img decoding="async" src="https://live.staticflickr.com/65535/54292060960_3f430425a6_h.jpg" alt=""></a></p><p>A lot of this new technology comes from learnings and hard work on our next big fantasy game, Light No Fire, which is keeping our small team incredibly busy.</p>



<p>As well as new solar systems and new technology Worlds Part II brings a lot of new adventures. This update introduces a large strand of new quests and lore connecting up some of the storylines and mysteries we have been building to for a long time. Those who relish unearthing the collective knowledge of the No Man’s Sky universe have a lot to dig into.</p>



<p><a href="https://live.staticflickr.com/65535/54291632746_6d99f45fd4_h.jpg" target="_blank" rel="noopener"><img decoding="async" src="https://live.staticflickr.com/65535/54291632746_6d99f45fd4_h.jpg" alt=""></a></p><p>There is also a whole new expedition to accompany this major update which deliberately serves as a guided tour to some of the best new additions to Worlds Part II. The rewards for completing this awe-inspiring journey are really special too. Not least is a brand new spacecraft which is a cross between a living ship and a jet fighter. It’s pretty wild!&nbsp;</p>



<p>The Worlds Part II update is a signal to the PlayStation community that 2025 is going to be a very exciting year for No Man’s Sky. Whether you play on PS5, PS4, or PS VR2, there’s a lot to look forward to from this tiny but still energised team. We’re so thrilled to be able to keep working on this game we all love so much.</p>



<p>Our journey continues.</p>
						</div>
							
							
					</div>
			</article><section>
	<h2>Trending Stories</h2>

	
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Astral – "We're building a new static type checker for Python" (287 pts)]]></title>
            <link>https://twitter.com/charliermarsh/status/1884651482009477368</link>
            <guid>42868576</guid>
            <pubDate>Wed, 29 Jan 2025 17:56:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/charliermarsh/status/1884651482009477368">https://twitter.com/charliermarsh/status/1884651482009477368</a>, See on <a href="https://news.ycombinator.com/item?id=42868576">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Intel doesn't know how to be a foundry," Tim Cook reportedly told TSMC's CEO (154 pts)]]></title>
            <link>https://www.tomshardware.com/tech-industry/tsmc-founder-says-tim-cook-told-him-intel-did-not-know-how-to-be-a-foundry</link>
            <guid>42868531</guid>
            <pubDate>Wed, 29 Jan 2025 17:53:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/tech-industry/tsmc-founder-says-tim-cook-told-him-intel-did-not-know-how-to-be-a-foundry">https://www.tomshardware.com/tech-industry/tsmc-founder-says-tim-cook-told-him-intel-did-not-know-how-to-be-a-foundry</a>, See on <a href="https://news.ycombinator.com/item?id=42868531">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-1024-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-1024-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-1024-80.jpg" alt="Morris Chang" srcset="https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-1024-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-1024-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/Si6f9pAhMHpYzqgctsFdfY.jpg" data-pin-nopin="true" fetchpriority="high" crossorigin="anonymous">
</picture>
</div>
<figcaption>
<span>(Image credit: Walid Berrazeg/SOPA Images/LightRocket via Getty Images)</span>
</figcaption>
</div>

<div id="article-body">
<p>When Apple began to build its own processors for iPhones and iPads in 2009 – 2010, it initially used Samsung Foundry, but after custom silicon became a key advantage of iPhones over rivals in the early 2010s, the company began to explore other makers as Samsung was Apple's primary rival at the time. The company considered using Intel Custom Foundry (ICF) and Texas Instruments but quickly realized the ICF was not tailored for external customers at all, while TI did not have advanced process technologies. As a result, it chose TSMC as its exclusive supplier, according to Morris Chang, the founder of TSMC, who spoke to <a data-analytics-id="inline-link" href="https://www.youtube.com/watch?v=FZItbr4ZJnc" data-url="https://www.youtube.com/watch?v=FZItbr4ZJnc" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">Acquired</a>.&nbsp;</p><p>"The [CEO] of Intel has approached Tim Cook and has asked Tim Cook to consider Intel, and at this time, Intel was the major supplier for Apple's Mac line," Chang reminisced. "I knew a lot of Intel's customer customers in Taiwan […] none of them liked Intel [as it] always acted like they were the the only guy [with] microprocessors. […] The Foundry business where TSMC […] does not compete with customers and even if Intel is trying to do business in good faith they do have the conflict [of interests."&nbsp;</p><p>When Intel's CEO Paul Otellini approached Tim Cook in early 2011, offering to manufacture Apple's chips, Apple paused discussions with TSMC for two months to evaluate the proposal.</p><p>Morris Chang, concerned about this pause, traveled to Apple's headquarters to check on the situation. In a private meeting, Tim Cook reassured Chang that Apple would not choose Intel.&nbsp;</p><p>"Intel just does not know how to be a foundry," Tom Cook reportedly told Chang.&nbsp;</p><p>The implication was that Intel lacked the customer-centric mindset required for a foundry business. Unlike TSMC, which tailors its process technologies to meet customer needs, Intel was used to designing and producing its own chips and struggled to adapt to servicing external clients. By contrast, Apple valued TSMC's ability to listen and respond to specific demands, something Intel historically did not do.&nbsp;</p><p>"When the customer asks a lot of things, we have learned to respond to every request," Chang said. "Some of them were crazy, some of them were irrational, [but] we respond to each request courteously. […] Intel has never done that, I knew a lot of customers of Intel's here in Taiwan and all [of them] wished that there were another supplier."&nbsp;</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-6S8ZwAVeqjYYXmY7mXf9rB"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div><p>However, it is notable that Intel has worked to defray those concerns with its now-revamped Intel Foundry, which also now offers support for industry-standard design tools, a notable area it lacked with its first Intel Custom Foundry foray in the past.</p><p>Indeed, the very first encounter with Apple disrupted TSMC's roadmap. TSMC planned to move from 28nm planar to 16nm FinFET, but Apple wanted a custom 20nm-class planar node instead. At the time, TSMC did not have enough R&amp;D teams to develop two process technologies at once, so the company had to divert people working on CLN16FF to CLN20SOC to meet Apple's needs in 2014.&nbsp;</p><p>Although Apple dual-sourced its A8 and A9 processors at 20nm and 16nm-class process technologies from Samsung and TSMC, Apple eventually committed to TSMC for all future processors. The Apple Silicon strategy cemented TSMC's position as the exclusive supplier, as the company's system-on-chips for different applications share quite a lot of IP.</p><p>The decision to meet Apple's demands was critical in TSMC surpassing Intel as the world's most advanced semiconductor manufacturer. Apple's business gave TSMC predictable high-volume orders, helping justify massive CapEx and R&amp;D investments. As a result, TSMC has consistently outpaced Intel by introducing leading-edge nodes.</p>
</div>



<!-- Drop in a standard article here maybe? -->



<div id="slice-container-authorBio-6S8ZwAVeqjYYXmY7mXf9rB"><p>Anton Shilov is a contributing writer at Tom’s Hardware. Over the past couple of decades, he has covered everything from CPUs and GPUs to supercomputers and from modern process technologies and latest fab tools to high-tech industry trends.</p></div>
</section>





<div id="slice-container-relatedArticles"><p><h3>Most Popular</h3></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An analysis of DeepSeek's R1-Zero and R1 (529 pts)]]></title>
            <link>https://arcprize.org/blog/r1-zero-r1-results-analysis</link>
            <guid>42868390</guid>
            <pubDate>Wed, 29 Jan 2025 17:44:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arcprize.org/blog/r1-zero-r1-results-analysis">https://arcprize.org/blog/r1-zero-r1-results-analysis</a>, See on <a href="https://news.ycombinator.com/item?id=42868390">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
            <h2>ARC Prize remains undefeated.<br>New ideas still needed<span>.</span></h2>
        </p><div>
<h2 id="an-analysis-of-deepseeks-r1-zero-and-r1">An Analysis of DeepSeek's R1-Zero and R1</h2>
<h2 id="r1-zero-is-more-important-than-r1">R1-Zero is more important than R1</h2>

<blockquote>
  <p>Special thanks to <a href="https://x.com/tuhinone">Tuhin</a> and <a href="https://www.linkedin.com/in/abuqader/">Abu</a> from <a href="https://www.baseten.co/">Baseten</a> and <a href="https://x.com/yuchenj_uw">Yuchen</a> from <a href="https://hyperbolic.xyz/">Hyperbolic Labs</a> for hosting r1-zero for us. Hardly any providers are hosting this model variant, and its availability is important for research purposes.</p>
</blockquote>

<p>ARC Prize Foundation’s goal is to define, measure, and inspire new ideas towards AGI. To this end, we strive to create the strongest global innovation environment possible.</p>

<p>We do not have AGI yet and are still innovation constrained – scaling up pure LLM pretraining is not the path, despite this being the dominant AI industry narrative and mainstream public view as of last summer.</p>

<p>The reason narratives are important is they end up driving economic activity, like investment, research focus, funding, geopolitics, trade, etc. For example, in 2023-24 there was ~$20B invested into new LLM startups compared to only ~$200M into new AGI startups.</p>

<p>We <a href="https://arcprize.org/blog/launch">launched ARC Prize 2024 last June</a> to grow awareness of limits of scaling LLMs and promote a useful benchmark, ARC-AGI-1, towards a new direction that requires AI systems to adapt to novel, unseen problems instead of being able to rely strictly on memorization.</p>

<figure>
  <img src="https://arcprize.org/media/images/blog/r1-arch.jpg" alt="R1 Training Architecture">
  <figcaption>DeepSeek R1 architecture by <a href="https://x.com/SirrahChan/status/1881488738473357753" target="_blank">@SirrahChan</a>.</figcaption>
</figure>

<p>Last week, DeepSeek <a href="https://arxiv.org/abs/2501.12948">published</a> their new R1-Zero and R1 “reasoner” systems that is <a href="https://x.com/arcprize/status/1881761987090325517">competitive with OpenAI’s o1 system</a> on ARC-AGI-1. R1-Zero, R1, and o1 (low compute) all score around 15-20% – in contrast to <code>GPT-4o</code>’s 5%, the pinnacle of years of pure LLM scaling. Based on this week’s <a href="https://www.reuters.com/technology/chinas-deepseek-sets-off-ai-market-rout-2025-01-27/">US market reaction</a>, the public is starting to understand the limits of scaling pure LLMs too. However, there is still broad public ignorance about impending inference demand.</p>

<p>In December 2024, OpenAI announced a <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">new breakthrough o3 system that we verified</a>. It scored 76% in a low compute mode and 88% in a high compute mode. The o3 system demonstrates the first practical, general implementation of a computer adapting to novel unseen problems.</p>

<p>Despite being <a href="https://www.techmeme.com/241220/h2200">huge tech news</a>, o3 beating ARC-AGI-1 <a href="https://x.com/benspringwater/status/1881507009184530449">went largely unnoticed and unreported</a> by mainstream press.</p>

<p>This is an incredibly important moment for the field of AI and for computer science and these systems demand study. But due to the closed nature of o1/o3, we’re forced to rely on speculation. Thanks to ARC-AGI-1 and now (nearly) open source R1-Zero and R1, we can add to our understanding. In particular, R1-Zero is significantly more important than R1.</p>

<blockquote>
  <p>“Nearly” because DeepSeek did not publish a reproducible way to generate their model weights from scratch</p>
</blockquote>

<h2 id="r1-zero-removes-the-human-bottleneck">R1-Zero removes the human bottleneck</h2>

<p>In our <a href="https://arcprize.org/blog/openai-o1-results-arc-prize">o1</a> and <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">o3 analysis</a>, we speculated how these reasoning systems work. The key ideas:</p>

<ol>
  <li>Generate chains-of-thought (CoT) for a problem domain.</li>
  <li>Label the intermediary CoT steps using a combination of human experts (“supervised fine tuning” or SFT) and automated machines (“reinforcement learning” or RL).</li>
  <li>Train base model using (2).</li>
  <li>At test time, iteratively inference from the process model.</li>
</ol>

<p>Techniques used to iterative sample, along with ARC-AGI-1 scores, are reviewed below:</p>

<div>
  <table>
    <thead>
      <tr>
        <th>System</th>
        <th>ARC-AGI-1</th>
        <th>Method</th>
        <th>Avg Tokens</th>
        <th>Avg Cost</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>r1-zero</td>
        <td>14%</td>
        <td>No SFT / no search</td>
        <td>11K</td>
        <td>$.11</td>
      </tr>
      <tr>
        <td>r1</td>
        <td>15.8%</td>
        <td>SFT / no search</td>
        <td>6K</td>
        <td>$.06</td>
      </tr>
      <tr>
        <td>o1 (low)</td>
        <td>20.5%</td>
        <td>SFT / no search</td>
        <td>7K</td>
        <td>$.43</td>
      </tr>
      <tr>
        <td>o1 (med)</td>
        <td>31%</td>
        <td>SFT / no search</td>
        <td>13K</td>
        <td>$.79</td>
      </tr>
      <tr>
        <td>o1 (high)</td>
        <td>35%</td>
        <td>SFT / no search</td>
        <td>22K</td>
        <td>$1.31</td>
      </tr>
      <tr>
        <td>o3 (low)</td>
        <td>75.7%</td>
        <td>SFT / search + sampling</td>
        <td>335K</td>
        <td>$20</td>
      </tr>
      <tr>
        <td>o3 (high)</td>
        <td>87.5%</td>
        <td>SFT / search + sampling</td>
        <td>57M</td>
        <td>$3.4K</td>
      </tr>
    </tbody>
  </table>
</div>

<p><em>Note: ARC-AGI-1 semi-private score shown.</em></p>

<p>With DeepSeek’s new published research, we can better inform our speculation. The key insight is that higher degrees of novelty adaptation (and reliability) for LLM reasoning systems are achieved along three dimensions:</p>

<ol>
  <li>Adding human labels aka SFT to CoT process model training</li>
  <li>CoT search instead of linear inference (parallel per-step CoT inference)</li>
  <li>Whole CoT sampling (parallel trajectory inference)</li>
</ol>

<p>Item (1) is bottlenecked by human data generation and constrains which domains these reasoning systems benefit most. For example, the <a href="https://openai.com/index/learning-to-reason-with-llms/">MMLU professional law category</a> is surprisingly much lower than the math and logic on o1.</p>

<p>Items (2) and (3) are bottlenecked by efficiency. o1 and o3 both <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">show logarithmic improvement</a> in benchmark accuracy on ARC-AGI-1 as they spend more inference compute at test time, while the different ways to spend that compute adjust the x-axis of the curve.</p>

<p>In my opinion, the most interesting thing DeepSeek has done is to publish R1-Zero separately. R1-Zero is a model which does not use SFT, the (1) item. Instead it relies purely on reinforcement learning.</p>

<p>R1-Zero and R1 show strong score agreement on  ARC-AGI-1, scoring 14% and 15% respectively. DeepSeeks’s own reported benchmark scores also show strong agreement between R1-Zero and R1, eg. on MATH AIME 2024 scores are 71% and 76% respectively (up from ~40% on the base DeepSeek V3).</p>

<p>In the paper, R1-Zero authors say “DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing” and <a href="https://www.reddit.com/r/LocalLLaMA/comments/1i765q0/r1zero_pure_rl_creates_a_mind_we_cant_decodeis/">has been corroborated online</a>. However in our testing, we found little to no evidence of incoherence when testing R1-Zero on ARC-AGI-1 which is similar to the math and coding domains the system was RL’d on.</p>

<p>Taken together, these findings suggest:</p>

<ol>
  <li>SFT (eg. human expert labeling) is not necessary for accurate and legible CoT reasoning in domains with strong verification.</li>
  <li>The R1-Zero training process is capable of creating its own internal domain specific language (“DSL”) in token space via RL optimization.</li>
  <li>SFT is necessary for increasing CoT reasoning domain generality.</li>
</ol>

<p>This makes intuitive sense, as language itself is effectively a reasoning DSL. The exact same “words” can be learned in one domain and applied in another, like a program. The pure RL approach can not yet discover a broad shared vocabulary and I expect this will be a strong focus for future research.</p>

<p>Ultimately, R1-Zero demonstrates the prototype of a potential scaling regime with zero human bottlenecks – even in the training data acquisition itself.</p>

<p>Almost certainly DeepSeek has set its sights on OpenAI’s o3 system. It is important to watch whether SFT ends up being a requirement to add CoT search and sampling, or whether a hypothetical “R2-Zero” could exist along the same logarithmic accuracy vs inference scaling curve. Based on R1-Zero results, I believe SFT will not be required to beat ARC-AGI-1 in this hypothetical scaled up version.</p>

<h2 id="dollars-for-reliability">Dollars for reliability</h2>

<p>There are two major shifts happening in AI, economically speaking:</p>

<ol>
  <li>You can now spend more $ to get higher accuracy and reliability</li>
  <li>Training $ is moving to inference $</li>
</ol>

<p>Both are going to drive a massive amount of demand for inference and neither will curtail the demand for more compute. In fact, they will increase the demand for compute.</p>

<p>AI reasoning systems promise much greater returns than simply higher accuracy on benchmarks. The number one issue preventing more AI automation use (e.g. inference demand) is reliability. I’ve spoken with hundreds of Zapier’s customers trying to deploy AI agents in their businesses and the feedback is strongly consistent: “I don’t trust them yet because they don’t work reliably”.</p>

<p><a href="https://www.cognitiverevolution.ai/the-arc-prize-efficiency-intuition-and-agi-with-mike-knoop-co-founder-of-zapier/">Previously</a> I’ve argued that progress towards ARC-AGI would result in higher reliability. The challenge with LLM agents is they need strong local domain steering to work reliability. Stronger generalization capability requires the ability to adapt to unseen situations. We’re now starting to <a href="https://x.com/woj_zaremba/status/1882290021778313272">see evidence</a> this view is correct. And so it’s no surprise several companies are now introducing agents (Anthropic, OpenAI, Apple, …)</p>

<p>Agents will drive significant near-term demand inference due the reliability needs. More broadly, developers can choose to spend more compute to increase user trust in the system. More reliability does not mean 100% accuracy though – but you’d expect to be more <a href="https://commons.wikimedia.org/wiki/File:Statistical_bias_and_statistical_noise_illustration.png">consistently inaccurate</a>. This is okay because users and developers can now more confidently steer behavior via prompting when accuracy is low.</p>

<p>Problems that were impossible for computers previously now have dollar amounts attached to them. And as efficiency climbs, those dollar amounts will go down.</p>

<h2 id="inference-as-training">Inference as training</h2>

<p>The other major shift occurring is in the provenance of data going into LLM systems for pretraining. Previously, most data was either purchased, scraped, or synthetically generated from an existing LLM (eg. distilling or augmenting).</p>

<p>These reasoning systems offer a new option which is to generate “real” data as opposed to “synthetic”. The AI industry uses the term synthetic to identify low quality data that is typically recycled through an LLM to boost the overall amount of training data – with diminishing returns.</p>

<p>But now with reasoning systems and verifiers, we can create brand new legitimate data to train on. This can either be done offline where the developer pays to create the data or at inference time where the end user pays!</p>

<p>This is a fascinating shift in economics and suggests there could be a runaway power concentrating moment for AI system developers who have the largest number of paying customers. Those customers are footing the bill to create new high quality data … which improves the model … which becomes better and more preferred by users … you get the idea.</p>

<p>If we can break through the human expert CoT barrier and create an extremely efficient system to create new data via search/synthesis and verification, then we should expect a massive influx of compute to go into these inference systems as they quite literally get better just by inputting dollars and raw data. Eventually this type of AI training will eclipse pretraining on human generated data altogether.</p>

<h2 id="conclusion">Conclusion</h2>

<p>We will continue to see market corrections as increased inference demand becomes clear. AI system efficiency is only going to drive more usage, not just due to <a href="https://en.wikipedia.org/wiki/Jevons_paradox">Jevons Paradox</a> but because new regimes of training are unlocked as efficiency increases.</p>

<p>With R1 being open and reproducible, more people and teams will be pushing CoT and search to the limits. This will more quickly tell us where the frontier actually lies and will fuel a wave of innovation that increases the chance of reaching AGI quickly.</p>

<p>Several people have already told me they plan to use R1-style systems for <a href="https://arcprize.org/blog/arc-prize-2025">ARC Prize 2025</a> and I’m excited to see the results.</p>

<p>The fact that R1 is open is a great thing for the world. DeepSeek has pushed the frontier of science forward.</p>

		</div></div>]]></description>
        </item>
    </channel>
</rss>