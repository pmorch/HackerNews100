<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 21 Jun 2024 22:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Barcelona will eliminate tourist apartments (199 pts)]]></title>
            <link>https://www.theolivepress.es/spain-news/2024/06/21/breaking-barcelona-will-remove-all-tourist-apartments-in-2028-in-huge-win-for-anti-tourism-activists/</link>
            <guid>40752761</guid>
            <pubDate>Fri, 21 Jun 2024 19:23:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theolivepress.es/spain-news/2024/06/21/breaking-barcelona-will-remove-all-tourist-apartments-in-2028-in-huge-win-for-anti-tourism-activists/">https://www.theolivepress.es/spain-news/2024/06/21/breaking-barcelona-will-remove-all-tourist-apartments-in-2028-in-huge-win-for-anti-tourism-activists/</a>, See on <a href="https://news.ycombinator.com/item?id=40752761">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p><strong>BARCELONA’S city council has announced it will revoke all licenses for tourist apartments in the urban area by 2028.</strong></p>
<p>In a major win for anti-tourist activists, Barcelona’s socialist mayor Jaume Collboni announced on Friday that licenses for 10,101 tourist apartments in the city will automatically end in November 2028.</p>
<p>The move represents a crushing blow for Airbnb, Booking.com and other tenants and a triumph for locals who have protested about over-tourism and rising house prices for years.</p>
<p>Announcing the move, Collboni said the rising cost of property in the city – rental and purchase prices have risen by 70% and 40% respectively in the last decade – had forced him to take drastic action.</p>
<p>He said: “We cannot allow it that most young people who leave home are forced to leave Barcelona. The measures we have taken will not change the situation in one day. These things take time. But with these measures we are reaching a turning point”.</p>
<p><strong><em>READ MORE: <a href="https://www.theolivepress.es/spain-news/2024/06/19/exclusive-tourism-has-gone-too-far-hundreds-of-protestors-target-f1-roadshow-in-barcelona-city-centre-ahead-of-spanish-grand-prix/">EXCLUSIVE: ‘Tourism has gone too far’ – hundreds of protestors target F1 roadshow in Barcelona city centre ahead of Spanish Grand Prix</a></em></strong></p>
<div>
<figure><img decoding="async" width="490" height="653" src="https://www.theolivepress.es/wp-content/uploads/2024/06/5-1-490x653.jpg" alt="" srcset="https://www.theolivepress.es/wp-content/uploads/2024/06/5-1-490x653.jpg 490w, https://www.theolivepress.es/wp-content/uploads/2024/06/5-1-225x300.jpg 225w, https://www.theolivepress.es/wp-content/uploads/2024/06/5-1-768x1024.jpg 768w, https://www.theolivepress.es/wp-content/uploads/2024/06/5-1-1152x1536.jpg 1152w, https://www.theolivepress.es/wp-content/uploads/2024/06/5-1-1536x2048.jpg 1536w, https://www.theolivepress.es/wp-content/uploads/2024/06/5-1-60x80.jpg 60w, https://www.theolivepress.es/wp-content/uploads/2024/06/5-1-18x24.jpg 18w, https://www.theolivepress.es/wp-content/uploads/2024/06/5-1-27x36.jpg 27w, https://www.theolivepress.es/wp-content/uploads/2024/06/5-1-36x48.jpg 36w, https://www.theolivepress.es/wp-content/uploads/2024/06/5-1-scaled.jpg 1920w" sizes="(max-width: 490px) 100vw, 490px"><figcaption>The move represents a big win for anti-tourist protestors, who attempted to disrupt an F1 roadshow in the city on Tuesday. Copyright: Ben Pawlowski/Olive Press</figcaption></figure></div>
<p>The deputy mayor for Urban Planning, Laia Bonet, hailed the move as the ‘equivalent of building 10,000 new flats’ which can be used by locals for residential use.</p>
<p>Local officials say that tenants will not be compensated because the move, which will have to be passed with political support, has de-facto compensation by giving owners a four-year window before licences expire. </p>
<p>Alongside the revoking of tourist flat licenses, Collboni announced that new legislation would force building constructors to allocate at least 30% of new homes to social housing.</p>
<p>The measures are designed to alleviate pressure on a housing market which has seen sharp price rises in recent years, forcing many residents to leave the urban area for the suburbs and beyond.</p>
<p>Speaking to the <em>Olive Press </em>at an anti-tourist rally on Tuesday, one Barcelona resident, who gave his name as Alex, said locals were angry at the ‘massification of tourism’ with ‘the cost of living and housing forcing many young people to emigrate from the city centre to the suburbs and nearby towns’.</p>
<p>He added: “The people of Barcelona, like any city in the UK and elsewhere, have the right to live peacefully in their own city. What we need is a better quality of life, decent wages and, above all, an affordable city to live in”. </p>
<p><a href="https://theolivepress.es/register/?level_id=4"><img src="https://www.theolivepress.es/wp-content/uploads/2024/04/paywall-offer-smaller.png" alt="" width="768" height="241"></a></p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Aster shootdown over Sydney in 1955 (104 pts)]]></title>
            <link>https://navyhistory.au/aster-shootdown-over-sydney-in-1955/</link>
            <guid>40751813</guid>
            <pubDate>Fri, 21 Jun 2024 17:33:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://navyhistory.au/aster-shootdown-over-sydney-in-1955/">https://navyhistory.au/aster-shootdown-over-sydney-in-1955/</a>, See on <a href="https://news.ycombinator.com/item?id=40751813">Hacker News</a></p>
<div id="readability-page-1" class="page"><article aria-label="Aster shootdown over Sydney in 1955"><div><p>FOR more than three hours on a fine, calm August morning in 1955, the eyes of Sydney’s suburbia were fixed skyward, anxiously watching the flight of a pilotless Auster aeroplane as it circled above and headed from Bankstown to the City, pursued by Service aircraft.</p>
<p>It was school holiday time, the alert had gone out over the radio, and the mums had herded in their children, police patrolled areas by car, cycle and foot, firemen stood by their tenders, ambulance men remained on the alert and firefloats stood in readiness in the harbour . . . all eyes still looked up . . . no one knew when or where the plane might suddenly come hurtling down.</p>
<p>The end came – thankfully five miles off the coast – when two Navy Sea Furies opened fire on the Auster. which levelled out pouring smoke, then started down in a slow spiral. The two Navy pilots followed it down, firing two or three more short bursts on the way and with a splash, the errant aircraft, still in one piece, hit the water at 1142 and disappeared. It was all over!</p>
<p>The media had a field day . . with such newspaper headings as “Possible disaster in flight”, and “Thousands watch air drama of flyaway plane”, reaching the overseas press. Politicians asked embarassing questions and crit-icism of the Services followed as did a Department of Civil Aviation enquiry … but how, when and why did it happen?</p>
<p>On the morning of August 30, 1955. Mr Anthony Thrower, aged 30, of Granville, Sydney, rented an Auster from Kingsford Smith Aviation School He had completed only one circuit of his planned one hour practice when the engine failed 10 feet from the ground. Landing the plane in the middle of the strip he climbed out, swung the propellor by hand (there was no self-starter) and the engine immediately roared into life. In a million-to-one chance the brake failed to hold and although pilot Thrower grabbed a wing strut to check the plane he was quickly forced to jump clear, just avoiding the tail. Aided by a favourable south-east wind with well trimmed controls, the pilotless plane sped across the strip and became airborne. It then narrowly missed the control tower, which was subsequently evacuated, and other airport buildings then slowly circled the aerodrome at low altitude After continuing right hand circuits of Bankstown for a further 15 minutes the Auster steadily gained height and began drifting towards the city.</p>
<p>Bankstown Aerodrome officials alerted control personnel at Mascot who broadcast a general alarm to all aircraft as well as the police and other Government organisations. One report stated that a schoolboy might be at the controls. The police radio station at Bourke Street broadcast at almost one minute intervals the plane’s last known whereabouts.</p>
<p>Meanwhile, Commander J. R. W. Groves, RAN, was returning to Schofields aerodrome from exercises, with three other personnel onboard. At 0850 the Navy Auster was alerted by Mascot of the runaway plane. Nearing Bankstown they saw it at 1500ft and climbing in tight circles. Approaching to within 50 yards it was noted to be unoccupied and that the controls were fixed in the one position. The Navy lightplane continued to pursue the unmanned aircraft as it gained height passing over the Sydney suburbs of Punchbowl, Bexley, Hurstville, Rockdale, Mascot, Alexandria, Redfern and finally arriving above the centre of the city about 9.30 am.</p>
<p>In the meantime anxious mothers in suburbs where the plane passed over herded children, who were on school holidays, into their homes. Police patrolled areas by car, cycle and foot. Firemen stood by their trucks and ambulance officers remained on the alert, and firefloats stood in readiness in the harbour. No one knew when or where the plane might suddenly come hurtling down.</p>
<p>By 0953 the Auster was over Vaucluse at 5000 feet. RAAF Wirraway A20-728 departed Richmond at 1010 to join the chase. Onboard was Wing Commander D.R. Beattie and Squadron Leader Tom James. The target was contacted at 1020, 2½ miles offshore and now at an altitude of 7000 ft. Instructions were then received that they were not to open fire until the Auster was five miles offshore and there were no fishing or coastal boats below. The runaway plane continued climbing in tight orbit to 10,300ft and at 1045 hours reached a point estimated at five miles from the coast.</p>
<p><img decoding="async" src="https://navyhistory.au/wp-content/uploads/Wirraway-300x156.jpg" alt="" width="300" height="156" srcset="https://navyhistory.au/wp-content/uploads/Wirraway-300x156.jpg 300w, https://navyhistory.au/wp-content/uploads/Wirraway-150x78.jpg 150w, https://navyhistory.au/wp-content/uploads/Wirraway-120x62.jpg 120w, https://navyhistory.au/wp-content/uploads/Wirraway.jpg 585w" sizes="(max-width: 300px) 100vw, 300px"></p>
<p>Two firing passes were then made with the hand held Bren gun from the rear cockpit without any noticeable effect. Before departing the Wirraway rear canopy and fairing had been removed and Squadron Leader James was so cold, it was minus five degrees celcius – that he was unable to change the magazine and his hands were sticking to the gum.</p>
<p>Meanwhile a RAAF Meteor had arrived from Williamtown near Newcastle and after directing it to the target the Wirraway broke off the attack and returned to Richmond. The Navy Auster, which had now been airborne some 3¼ hours, headed for its base at Schofields at about the same time.</p>
<p><img decoding="async" src="https://navyhistory.au/wp-content/uploads/20200914raaf8622332_07-300x200.jpg" alt="" width="300" height="200" srcset="https://navyhistory.au/wp-content/uploads/20200914raaf8622332_07-300x200.jpg 300w, https://navyhistory.au/wp-content/uploads/20200914raaf8622332_07-1500x1000.jpg 1500w, https://navyhistory.au/wp-content/uploads/20200914raaf8622332_07-150x100.jpg 150w, https://navyhistory.au/wp-content/uploads/20200914raaf8622332_07-768x512.jpg 768w, https://navyhistory.au/wp-content/uploads/20200914raaf8622332_07-1536x1024.jpg 1536w, https://navyhistory.au/wp-content/uploads/20200914raaf8622332_07-2048x1365.jpg 2048w, https://navyhistory.au/wp-content/uploads/20200914raaf8622332_07-120x80.jpg 120w, https://navyhistory.au/wp-content/uploads/20200914raaf8622332_07-1000x667.jpg 1000w" sizes="(max-width: 300px) 100vw, 300px"></p>
<p>But luck was not with the RAAF that day. Firstly a Meteor piloted by Squadron Leader Holdsworth, had been delayed some 13 minutes on departure when a Sabre preceding his departure, had burst a tyre on landing and obstructed the runway. Then after arriving in the target area, and in the Meteor’s initial firing pass from the rear, both cannons jammed after only a few rounds had been fired. Some strikes were observed on the starboard plane of the Auster.</p>
<p>Squadron Leader Holdsworth then requested that two more Meteors be sent and the reply was received that they were on the way in addition to two Sea Furies from the Naval Air Station at Nowra. While awaiting their arrival A77-80 made four passes directly below the runaway Auster and pulled up sharply in an attempt to dislodge it from its flight path and into a dive. However, the jet wash was not sufficient and the plane continued in the same determined fashion.</p>
<p>Sea Furies from 805 Squadron Nowra appeared on the scene at 1135 and were piloted by Lieutenants J.R. Bluett and Peter McNay (both aged 26), of the Royal Navy, who were on exchange duty in Australia. LEUT McNay lowered his flaps to slow down and approached to within 100 yards of the target to again confirm it was unoccupied. Then, pulling up astern he gave it a short burst from his four cannons. LEUT Bluett followed this with a beam-on attack and after about 15 rounds. a great sheet of flame rose from the cockpit. From the first strikes on the Auster until the time it hit the sea was 1½ minutes.</p>

<figure id="attachment_67775" aria-describedby="caption-attachment-67775"><img decoding="async" src="https://navyhistory.au/wp-content/uploads/HAWKER-SEA-FURY-MK-1-300x158.jpg" alt="" width="300" height="158" srcset="https://navyhistory.au/wp-content/uploads/HAWKER-SEA-FURY-MK-1-300x158.jpg 300w, https://navyhistory.au/wp-content/uploads/HAWKER-SEA-FURY-MK-1-150x79.jpg 150w, https://navyhistory.au/wp-content/uploads/HAWKER-SEA-FURY-MK-1-768x403.jpg 768w, https://navyhistory.au/wp-content/uploads/HAWKER-SEA-FURY-MK-1-120x63.jpg 120w, https://navyhistory.au/wp-content/uploads/HAWKER-SEA-FURY-MK-1.jpg 1000w" sizes="(max-width: 300px) 100vw, 300px"><figcaption id="caption-attachment-67775">Hawker Sea Fury Mark 2</figcaption></figure>


<p><img decoding="async" src="https://navyhistory.au/wp-content/uploads/Astershootdown.jpg" alt="" width="356" height="506" srcset="https://navyhistory.au/wp-content/uploads/Astershootdown.jpg 356w, https://navyhistory.au/wp-content/uploads/Astershootdown-211x300.jpg 211w, https://navyhistory.au/wp-content/uploads/Astershootdown-106x150.jpg 106w, https://navyhistory.au/wp-content/uploads/Astershootdown-120x171.jpg 120w" sizes="(max-width: 356px) 100vw, 356px"></p>
<p>At 1145 a police broadcast announced “the Auster has been shot down. It’s all over.” The barrage of calls from anxious enquirers gradually subsided at the police, newspaper and radio station switchboards throughout Sydney.</p>
<p>When the Navy Sea Furies returned to Nowra, enthusiastic groundstaff quickly painted a small yellow silhouette representing an Auster on the fuselage of LEUT Bluett’s plane. This 26-year-old British pilot had seen eight months’ action in the Korean war from HMS GLORY on ground attack. LEUT McNay had only been in Australia eight months after completing his training in England.</p>
<p>The incident did not quickly subside here. Embarrassing questions were directed in Federal Parliament to. the Government of the day by both Mr C Chambers (Member for Adelaide) and Mr F Daly (Grayndler) during the Budget debate the following month. They asked why was so much money being spent on defence to an Air Force and Navy that took over two hours to shoot down an unarmed light aircraft?</p>
<p>Aviation authorities stated that nearly “dead calm” weather had probably prevented a major disaster as the Auster could have crashed anywhere on its route had it been a windy day. The then Department of Civil Aviation held an enquiry.</p>
<p>The harsh criticism against the Services was unfounded though and despite some initial bad luck the Navy and Air Force had performed creditably on a difficult and elusive “ENEMY”. In addition they were provided with an exciting Tuesday morning over and around Sydney with a free but stubborn target to practise on.</p>
<p>—————————————————————————————————————-</p>

</div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Self-driving Waymos secure final clearance for expansion beyond S.F (105 pts)]]></title>
            <link>https://www.sfchronicle.com/bayarea/article/self-driving-cars-waymo-sf-19523750.php</link>
            <guid>40751108</guid>
            <pubDate>Fri, 21 Jun 2024 16:24:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sfchronicle.com/bayarea/article/self-driving-cars-waymo-sf-19523750.php">https://www.sfchronicle.com/bayarea/article/self-driving-cars-waymo-sf-19523750.php</a>, See on <a href="https://news.ycombinator.com/item?id=40751108">Hacker News</a></p>
Couldn't get https://www.sfchronicle.com/bayarea/article/self-driving-cars-waymo-sf-19523750.php: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Testing Generative AI for Circuit Board Design (194 pts)]]></title>
            <link>https://blog.jitx.com/jitx-corporate-blog/testing-generative-ai-for-circuit-board-design</link>
            <guid>40751020</guid>
            <pubDate>Fri, 21 Jun 2024 16:16:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.jitx.com/jitx-corporate-blog/testing-generative-ai-for-circuit-board-design">https://blog.jitx.com/jitx-corporate-blog/testing-generative-ai-for-circuit-board-design</a>, See on <a href="https://news.ycombinator.com/item?id=40751020">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <article>
      <time datetime="2024-06-21 16:16:00">
        Jun 21, 2024 9:16:00 AM
      </time>
      
      
      <p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p><em>TLDR: We test LLMs to figure out how helpful they are for designing a circuit board. We focus on utility of frontier models (GPT4o, Claude 3 Opus, Gemini 1.5) across a set of design tasks, to find where they are and are not useful. They look pretty good for building skills, writing code, and getting useful data out of datasheets.</em></p>
<!--more-->
<p><br><em>TLDRN'T: We do not explore any proprietary copilots, or how to apply a things like a diffusion model to the place and route problem.</em></p>
<h2>Introduction</h2>
<div><p>Can an AI-powered chatbot help with a task as precise as circuit board design? These LLMs (Large Language Models) are famous for hallucinating details, and missing a *single* important detail can sink a design. Determinism is hard but super important for electronics design!&nbsp;</p><p>Today, several shallow product offerings are making AI for electronics design look mostly like hype. But I think there is real utility to be found here, if we can take a better approach.&nbsp;</p><p>In this article, we set LLMs to unfair tasks that expert human circuit board designers handle day to day. We're not looking for basic help, rather pushing on what it takes to help an expert do their job better.</p><p>It would be ridiculous (today) to expect great performance from these general purpose AIs on this expert task, especially because it is probably not well represented in the training data.</p><p>To test performance on difficult design tasks, we work with three of today's leading models:<br>- <a href="https://deepmind.google/technologies/gemini/pro/" rel="noopener">Gemini 1.5 Pro</a>&nbsp;from Google<br>- <a href="https://openai.com/index/hello-gpt-4o/" rel="noopener">GPT-4o</a> from OpenAI<br>- <a href="https://www.anthropic.com/news/claude-3-family" rel="noopener">Claude 3 Opus</a> from Anthropic</p><p>And explore prompting strategies to get the best performance out of all the models on each task. As part of the prompt, we also look for opportunities to improve the performance by asking the LLMs to write code to design circuit boards where possible.</p><p>The best results are reported for each model and for each task. Let's dig in!</p></div>
<h2><span>Asking stupid questions</span></h2>
<p><span>There is a lot to know in circuit board design, and nobody has mastered every relevant domain. Asking an LLM stupid questions is a great way to learn.<p>For example an RF engineer will know their own specialty quite well, but might not be a master of supply chain, power supply design, or how to shape copper reliefs to get high yields at the factory.</p><p>( Some RF engineers will scoff at this, but realistically the world contains people trying to learn things.)</p><p>To simulate someone new to a domain, I avoid prompt engineering, and use of precise expert vocabulary. I asked the LLMs a simple question: </p></span><span><em><span>What is the delay per unit length of a trace on a circuit board?</span></em><p>Claude 3 Opus was the clear winner here. Notice that it brought in the relevant concepts (microstrip, stripline, dielectric), got the answer right, and pointed out critical gotchas:</p></span></p>
<p><span><img src="https://blog.jitx.com/hs-fs/hubfs/Pasted%20image%2020240606134032-png.png?width=734&amp;height=1154&amp;name=Pasted%20image%2020240606134032-png.png" loading="lazy" width="734" height="1154" srcset="https://blog.jitx.com/hs-fs/hubfs/Pasted%20image%2020240606134032-png.png?width=367&amp;height=577&amp;name=Pasted%20image%2020240606134032-png.png 367w, https://blog.jitx.com/hs-fs/hubfs/Pasted%20image%2020240606134032-png.png?width=734&amp;height=1154&amp;name=Pasted%20image%2020240606134032-png.png 734w, https://blog.jitx.com/hs-fs/hubfs/Pasted%20image%2020240606134032-png.png?width=1101&amp;height=1731&amp;name=Pasted%20image%2020240606134032-png.png 1101w, https://blog.jitx.com/hs-fs/hubfs/Pasted%20image%2020240606134032-png.png?width=1468&amp;height=2308&amp;name=Pasted%20image%2020240606134032-png.png 1468w, https://blog.jitx.com/hs-fs/hubfs/Pasted%20image%2020240606134032-png.png?width=1835&amp;height=2885&amp;name=Pasted%20image%2020240606134032-png.png 1835w, https://blog.jitx.com/hs-fs/hubfs/Pasted%20image%2020240606134032-png.png?width=2202&amp;height=3462&amp;name=Pasted%20image%2020240606134032-png.png 2202w" sizes="(max-width: 734px) 100vw, 734px"></span></p>
<p><span>Notice the gotchas here! The speed of a signal changes based on which layer of a circuit board you route it on, and what the materials are.<p>You would never know that looking at a most PCB CAD tools, (e.g. Kicad, Altium) because all of their delay matching is in units of length. e.g. 'both these traces have to be 50 mm long' is a constraint you enter with a GUI. But that's not correct if you want your board to work - delay depends on which layer they are routed on, and what materials are in your board.</p><p>Getting that right: A+ answer for Claude here.&nbsp;</p><p>Google Gemini 1.5 performed the worst on this question. Maybe because it brought in a bunch of sources from the internet, and most of the writing in this discipline is dreck generated by SEO content mills. Incorporating AI summarization of lower quality writing seems to make the answers consistently worse.</p></span></p>
<h2><span>Finding Parts</span></h2>
<div><p>One of the superpowers of an experienced engineer is knowing things like: "A DAC + DSP system good enough for this audio pipeline should be about this big, and cost $1.40 in production". They'll then go do some turbo googling and find it.</p><p>How well can an AI that's been fed the entire internet do?</p><p>For this example I chose find parts for something cool: a robot <a href="https://www.youtube.com/watch?v=EER4pdKw2s4" rel="noopener">motor driver linked by optical ethernet</a> (instead of the usual <a href="https://youtu.be/gsOPZltbvgM" rel="noopener">CAN setups</a>). I want smaller cables, hundreds of possible devices, and compact cables that can survive IRL twisting and jostling.</p></div>
<p>You can see a communication layer like this as the daisy-chained black cabling in this picture from <a href="http://www.hebirobotics.com/" rel="noopener">HEBI</a>:</p>
<p><img src="https://blog.jitx.com/hs-fs/hubfs/image-png-1.png?width=510&amp;height=467&amp;name=image-png-1.png" loading="lazy" width="510" height="467" srcset="https://blog.jitx.com/hs-fs/hubfs/image-png-1.png?width=255&amp;height=234&amp;name=image-png-1.png 255w, https://blog.jitx.com/hs-fs/hubfs/image-png-1.png?width=510&amp;height=467&amp;name=image-png-1.png 510w, https://blog.jitx.com/hs-fs/hubfs/image-png-1.png?width=765&amp;height=701&amp;name=image-png-1.png 765w, https://blog.jitx.com/hs-fs/hubfs/image-png-1.png?width=1020&amp;height=934&amp;name=image-png-1.png 1020w, https://blog.jitx.com/hs-fs/hubfs/image-png-1.png?width=1275&amp;height=1168&amp;name=image-png-1.png 1275w, https://blog.jitx.com/hs-fs/hubfs/image-png-1.png?width=1530&amp;height=1401&amp;name=image-png-1.png 1530w" sizes="(max-width: 510px) 100vw, 510px"></p>
<div><p>Time to use AI to find the parts should I use for the ethernet switch, and the optical transceivers.</p><p>Here's the prompt:</p></div>
<p>I want to design a daisy-chained communications layer for a motor driver, using 100M optical Ethernet. Each motor controller will have a microcontroller that interfaces to the Ethernet network over RGMII. It needs to be compact, robust to mechanical vibration and shock, and handle temperatures up to 60 degrees Celsius. Please help me choose parts for the optical connectors, transceivers, and Ethernet networking device.&nbsp;<br>1. Think step by step as an expert electrical engineer.&nbsp;<br>2. Carefully consider how component choices relate to the requirements.&nbsp;<br>3. I'll give you a digital cookie if you get the answer correct.</p>
<div><p>Here I'm pulling more prompt engineering tricks to boost the performance of the model, and being careful in detailed specifications to really get this to work.</p><p>My initial guess for these parts would be:<br>- Use a small three port ethernet switch to for the daisy chain, hopefully something from Micrel (now Microchip) like the <a href="https://www.microchip.com/en-us/product/ksz8863" rel="noopener">KSZ8863MLL</a>.<br>- For the optical connector/transceiver I would go with the <a href="https://www.firecomms.com/optolock/" rel="noopener">OptoLock part</a> on the example system from Hebi. Very small and robust choice.&nbsp;</p><p>It would be amazing to get new suggestions for parts and architectures here. Something going beyond superficial advice into something practical would have huge impact!</p></div>
<div><p>Instead, performance across all models was uniformly bad and no amount of prompting or follow ups could fix it. It felt a lot like trying to get AI to solve a variant of the <a href="https://chatgpt.com/share/67a305cf-dc5f-45a2-a496-0886116f4659" rel="noopener">wolf, sheep, cabbage problem</a>. It's got the answer locked in, and it's hard to get out of that basin.</p><p>Here's a representative conclusion from Gemini 1.5:</p></div>
<div><p><span>&nbsp;Example Part Selection</span><br><span>- Optical Connectors: LC Duplex connectors (Amphenol or similar)</span><br><span>&nbsp;- Optical Transceivers: 100Base-FX SFP transceivers (Finisar, Cisco, or other reputable brands) with industrial temperature ratings</span><br><span>- Ethernet Networking Device: Microchip LAN8742A or Texas Instruments DP83848</span></p><p><span>Digital Cookie Time! 🍪</span></p><p><span>I hope this detailed breakdown helps you choose the right parts for your daisy-chained motor control system. Please let me know if you have any other questions.</span></p></div>
<p>Grading this:</p>
<p>- All models missed that to get the daisy chain you need a three port ethernet switch. They instead recommend an Ethernet Phy, which isn't going to do it.<br>- All models recommend using a huge SFP transceiver. These are usually found on big datacenter boards, and are not known for being shoved into tiny spaces in a robot joint, and then getting kicked around by Boston Dynamics employees.</p>
<div><p>Overall the generative models don't demonstrate a solid grasp on what the motor controller communication network is, or what application-specific considerations need to be made.&nbsp;</p><p>Without that the part selections are way off. This is a consistent experience. The LLMs will always suggest something, but it will be the 'average' application and miss a lot of the important subtlety. i.e. fiber optic ethernet happens in the mostly in the datacenter, so an average application for fiber optic means a datacenter device.</p><p>Trying to get them to choose an ADC, or sensor, or connector, shows similar 'average' results.</p><p>Maybe this speaks to the difficulty of the domain. If you're not within 10% or so of the global optimal performance in EE, your suggestions have near-zero value.</p></div>
<h2><span>Parsing datasheets</span></h2>
<p><span>Sadly, most of the data you need to design a circuit board is baked away in PDF datasheets, instead of being available via APIs. Directly trying to parse PDFs with python tends to not go well, but LLMs should be pretty good at pulling data from these human-structured documents.<p>I tested three different way of pulling information out:<br>- Copy/paste from the PDF, stuff it into a prompt.<br>- Capture a portion as an image, have the LLM interpret the image<br>- Upload the entire PDF</p></span></p>
<p><span>I'll be using the data available for the <a href="https://docs.nordicsemi.com/bundle/ps_nrf5340/page/keyfeatures_html5.html" rel="noopener">Nordic nRF5340 WLCSP</a> for the experiment. A little 4.0 x 4.4 mm Bluetooth chip, with an 820 page datasheet.&nbsp;</span></p>
<p><span><img src="https://blog.jitx.com/hs-fs/hubfs/image-png-2.png?width=350&amp;height=324&amp;name=image-png-2.png" loading="lazy" width="350" height="324" srcset="https://blog.jitx.com/hs-fs/hubfs/image-png-2.png?width=175&amp;height=162&amp;name=image-png-2.png 175w, https://blog.jitx.com/hs-fs/hubfs/image-png-2.png?width=350&amp;height=324&amp;name=image-png-2.png 350w, https://blog.jitx.com/hs-fs/hubfs/image-png-2.png?width=525&amp;height=486&amp;name=image-png-2.png 525w, https://blog.jitx.com/hs-fs/hubfs/image-png-2.png?width=700&amp;height=648&amp;name=image-png-2.png 700w, https://blog.jitx.com/hs-fs/hubfs/image-png-2.png?width=875&amp;height=810&amp;name=image-png-2.png 875w, https://blog.jitx.com/hs-fs/hubfs/image-png-2.png?width=1050&amp;height=972&amp;name=image-png-2.png 1050w" sizes="(max-width: 350px) 100vw, 350px"></span></p>
<p>Observations on this part: &nbsp;</p>
<ol>
<li>This is by no means the largest datasheet for a electrical component but has many typical features including pin tables, mechanical drawings, application specific notes, graphs, etc</li>
<li>This datasheet has significantly more content than you might expects for simpler components - like OpAmps or digital logic gates. If the LLMs are useful here, they will likely be useful for smaller cases.</li>
<li>This datasheet is about 570k tokens, a little more than half the huge context window available from Gemini.</li>
</ol>
<h3>Make a symbol and footprint</h3>
<div><p>First step is a to make a linked symbol and footprint (landpattern) for our component.&nbsp;</p><p>The most effective method here was to load the entire datasheet into the LLM via the chat file upload mechanism, and then query it interactively to get the details right. Gemini 1.5 was the most reliable for this task.</p><p>Here's the prompt to get the pin table (after uploading the datasheet):</p></div>
<p>Please write the the pin table for the wlcsp version of the nRF5340 as code using the following format:&nbsp;<br>1. One line for each pin&nbsp;<br>2. Each line structured as " [ pin-name | pad-number | direction | bank ]", where pin-name is the name of a pin e.g. GND, pad-number is the number of the pad that maps to that pin, e.g, 28, direction is Left is the pin is an input, and Right otherwise, bank indicates the function, e.g. gpio.&nbsp;<br>3. Wrap numbers following letters in square brackets, e.g. K3 becomes K[3], C1 becomes C[1], F10 becomes F[10]&nbsp;<br>4. Replace periods with square brackets, e.g. P0.20 becomes P0[20]</p>
<p>The AI then returns a block of text with our pin list in a usable format:</p>
<p><code>[ XC1 | A1 | Left | analog_input ]&nbsp;</code><br><code>[ XC2 | A2 | Left | analog_input ]&nbsp;</code><br><code>[ VDD | A3 | Right | power ]&nbsp;</code><br><code>[ VSS | A5 | Left | power ]&nbsp;</code><br><code>[ DECD | A7 | Right | power ]&nbsp;</code><br><code>[ DCCD | A9 | Right | power ]&nbsp;</code><br><code>[ D-| A11 | Left | usb ]&nbsp;</code><br><code>[ D+ | A12 | Left | usb ]&nbsp;</code><br><code>[ DECRF | B1 | Right | power ]&nbsp;</code><br><code>[ VSS | B2 | Left | power ]&nbsp;</code><br><code>[ DECR | B4 | Right | power ]&nbsp;</code><br><code>[ DECN | B5 | Right | power ]</code><br><code>... (continued)</code></p>
<div><p>It got this list exactly correct, impressive for 95 rows of data buried in an 820 page datasheet. Usually I have to get a model to write a little code to reach that level of accuracy, but that was unnecessary here.</p><p>Next I gave it some example code for a BGA footprint and had it generate a new version based on the datasheet information. Here's the resulting code:</p></div>
<div><p><code>; This test is based on the Nordic Semiconductor, nRF5340</code><br><code>&nbsp; ; Datasheet:</code><br><code>&nbsp; ; &nbsp;https://infocenter.nordicsemi.com/pdf/nRF5340_PS_v1.4.pdf</code><br><code>&nbsp; ;</code><br><code>&nbsp; ; Drawing:</code><br><code>&nbsp; ; &nbsp;https://infocenter.nordicsemi.com/pdf/nRF5340_PS_v1.4.pdf</code><br><code>&nbsp; ;</code><br><code>&nbsp; ; NSMD</code><br><code>&nbsp; val rows = 12</code><br><code>&nbsp; val cols = 8</code><br><code>&nbsp; val grid = Grid-Numbering(rows, cols)</code></p><p><code>&nbsp; val missing = to-pad-island(grid, "A[1] B[1] B[3] C[1] C[3] D[3] D[4] D[5] D[6] D[7] D[8] D[9] E[1] E[9] F[4] F[6] F[7] F[9] G[1] G[4] G[9] H[4] H[5] H[6] H[7] H[8] H[9]")</code></p><p><code>&nbsp; val planner = Full-Matrix-Planner(</code><br><code>&nbsp; &nbsp; ; Several pads are not present</code><br><code>&nbsp; &nbsp; inactive = missing,</code><br><code>&nbsp; &nbsp; pad-config = PadConfig-D(</code><br><code>&nbsp; &nbsp; &nbsp; copper-D-adj = ipc-bga-copper(</code><br><code>&nbsp; &nbsp; &nbsp; &nbsp; NonSolderMaskDefined,</code><br><code>&nbsp; &nbsp; &nbsp; &nbsp; density-level = DensityLevelC</code><br><code>&nbsp; &nbsp; &nbsp; &nbsp; ),</code><br><code>&nbsp; &nbsp; &nbsp; mask-D-adj = 0.0,</code><br><code>&nbsp; &nbsp; )</code><br><code>&nbsp; )</code></p><p><code>&nbsp; val body = PackageBody(</code><br><code>&nbsp; &nbsp; width = 4.390 +/- 0.047,</code><br><code>&nbsp; &nbsp; length = 3.994 +/- 0.047,</code><br><code>&nbsp; &nbsp; height = 0.71 +/- [0.06, 0.0]</code><br><code>&nbsp; )</code></p><p><code>&nbsp; val pkg = BGA(</code><br><code>&nbsp; &nbsp; num-leads = 95,</code><br><code>&nbsp; &nbsp; rows = rows,</code><br><code>&nbsp; &nbsp; columns = cols,</code><br><code>&nbsp; &nbsp; lead-diam = 0.35,</code><br><code>&nbsp; &nbsp; pitch = 0.35,</code><br><code>&nbsp; &nbsp; package-body = body,</code><br><code>&nbsp; &nbsp; lead-numbering = grid,</code><br><code>&nbsp; &nbsp; pad-planner = planner,</code><br><code>&nbsp; &nbsp; density-level = DensityLevelB</code><br><code>&nbsp; )</code></p></div>
<div><p>A few more misses here than in the pin table, but close overall. I have to go in and separately prompt for the depopulated balls in the grid, as well as correct the dimensions in the PackageBody.&nbsp;</p><p>(Note that I'm generating JITX code because it has a text-native representation. I bet that you could get the model to emit something like a Kicad symbol/footprint with a little more nudging in the prompt.)</p><p>Running the generated code (which you can find <a href="https://github.com/JITx-Inc/generative-ai-test-cases/blob/main/component-model/nRF5340.stanza" rel="noopener">here</a>), we get our footprint and symbols:</p></div>
<p><img src="https://blog.jitx.com/hs-fs/hubfs/image-png-3.png?width=2682&amp;height=1404&amp;name=image-png-3.png" loading="lazy" width="2682" height="1404" srcset="https://blog.jitx.com/hs-fs/hubfs/image-png-3.png?width=1341&amp;height=702&amp;name=image-png-3.png 1341w, https://blog.jitx.com/hs-fs/hubfs/image-png-3.png?width=2682&amp;height=1404&amp;name=image-png-3.png 2682w, https://blog.jitx.com/hs-fs/hubfs/image-png-3.png?width=4023&amp;height=2106&amp;name=image-png-3.png 4023w, https://blog.jitx.com/hs-fs/hubfs/image-png-3.png?width=5364&amp;height=2808&amp;name=image-png-3.png 5364w, https://blog.jitx.com/hs-fs/hubfs/image-png-3.png?width=6705&amp;height=3510&amp;name=image-png-3.png 6705w, https://blog.jitx.com/hs-fs/hubfs/image-png-3.png?width=8046&amp;height=4212&amp;name=image-png-3.png 8046w" sizes="(max-width: 2682px) 100vw, 2682px"></p>
<div><p>I really like the combination of code and scraped data here because of precision and flexibility.&nbsp;</p><p>I would much rather trust a hand-coded BGA footprint generator for the details, and have the LLM tell me which pads to omit. The backstop of the LLM generating the list of pins <em>and the missing pins</em>, then forcing them both to line up to line up in the code is a nice touch. I can also dig in and edit parameters directly if I want to customize.</p><p>Past the standard component model with a symbol and footprint, the LLM can also pull most of the useful data from the PDF. e.g. here is a generated parameterization for the IO pin levels for checking correctness of connections in a design:</p></div>
<p><code>property(P0[0].digital_io) = DigitalIO(CMOSOutput(VSS + 0.4, VDD - 0.4), 0.3*VDD, 0.7*VDD, "VDD", "VSS", 0)</code></p>
<p>If you don't have the AI return an answer as code, it tends to spread the data you want across multiple paragraphs, and it is easier to get the answer you want by just reading the datasheet instead.</p>
<p>It can even read a bit of the reference schematic and turn it into a netlist:</p>
<p><code><img src="https://blog.jitx.com/hs-fs/hubfs/image-png-4.png?width=1104&amp;height=1426&amp;name=image-png-4.png" loading="lazy" width="1104" height="1426" srcset="https://blog.jitx.com/hs-fs/hubfs/image-png-4.png?width=552&amp;height=713&amp;name=image-png-4.png 552w, https://blog.jitx.com/hs-fs/hubfs/image-png-4.png?width=1104&amp;height=1426&amp;name=image-png-4.png 1104w, https://blog.jitx.com/hs-fs/hubfs/image-png-4.png?width=1656&amp;height=2139&amp;name=image-png-4.png 1656w, https://blog.jitx.com/hs-fs/hubfs/image-png-4.png?width=2208&amp;height=2852&amp;name=image-png-4.png 2208w, https://blog.jitx.com/hs-fs/hubfs/image-png-4.png?width=2760&amp;height=3565&amp;name=image-png-4.png 2760w, https://blog.jitx.com/hs-fs/hubfs/image-png-4.png?width=3312&amp;height=4278&amp;name=image-png-4.png 3312w" sizes="(max-width: 1104px) 100vw, 1104px"></code></p>
<p><code>* Power Supply Nets:</code><br><code>net (nrf.VDD C4.p[1])</code><br><code>net (nrf.VDD C10.p[1])</code><br><code>net (nrf.VDD C11.p[1])</code><br><code>net (nrf.VDD C12.p[1])</code><br><code>net (nrf.VDD C13.p[1])</code><br><code>net (nrf.VDD C15.p[1])</code><br><code>net (nrf.VDD C17.p[1])</code><br><code>.... (continued)</code></p>
<div><p>It hallucinated this task a little bit, inventing resistors here and there.&nbsp;</p><p>Using LLMs to pull data from challenging sources and turn it into useful code was a helpful automation technique (and personally the one I use most). Accuracy can be boosted further by using extraction techniques to cross-reference the data.</p></div>
<h2><span>Designing circuits</span></h2>
<p><span>We've seen that LLMs can understand an image of a schematic well enough to (mostly) turn it into a netlist. Can they take on a much more challenging task and design the circuit itself?<p>As shown earlier, LLMs generated kinda-right, but actually very wrong suggestions for key components. To avoid that dependency, we tested our AIs on an analog circuit design task. This test is more about doing some math, and correctly arranging resistors and capacitors.</p><p>The task is to design an pre-amplifier for an electret microphone -- boosting and filtering an audio signal for sampling by an ADC. We start with a basic architecture exploration. Here's the prompt:</p></span></p>
<p><span>I want to design a microphone pre-amp where I need to bias the electret microphone and create a single-ended signal out to drive the ADC of a microcontroller. Include a bandpass filter with a gain of 20dB, corner frequencies at 100Hz and 20kHz. Help me select an architecture for this amplifier. Propose different filter architectures and describe their pros and cons for this application.&nbsp;<br>1. Think step by step as an expert electrical engineer&nbsp;<br>2. Carefully consider the requirements&nbsp;<br>3. If you get the answer right you get a digital cookie.</span></p>
<p><span>Claude 3 Opus came back with the best answer of the three contenders. I'll analyze it piece by piece.</span></p>
<p><span>Okay, let's think through this step-by-step as an expert electrical engineer:<br>1. Microphone biasing: Electret microphones typically require a DC bias voltage, usually between 1.5V to 10V, applied through a bias resistor. The value of this resistor impacts the microphone's output impedance and noise. A common range is 2.2kΩ to 10kΩ. We'll need to check the microphone's datasheet for specifics.<br>2. Amplifier architecture: For a single-ended output to drive an ADC, we can use a single-supply op-amp configuration. Some good options are:&nbsp;<br>&nbsp; &nbsp; a) Non-inverting amplifier: Simple, high input impedance which is good for the high-impedance mic signal. Gain is set by the feedback resistor ratio.&nbsp;<br>&nbsp; &nbsp; b) Inverting amplifier: Also simple, but lower input impedance. Gain is also set by resistor ratio.&nbsp;<br>&nbsp; &nbsp; c) Push-pull: Combines inverting and non-inverting amps for more output current drive, but more complex. For simplicity and good mic interfacing, a non-inverting amp seems best here.</span></p>
<div><p>Claude did a good job calling out the need to bias the microphone, nice to have that called out explicitly.</p><p>The proposals for non-inverting, and inverting amplifiers are reasonable. The proposal for push-pull configurations is nonsense. A <a href="https://en.wikipedia.org/wiki/Push%E2%80%93pull_output" rel="noopener">Push-Pull configuration</a> describes the transistors inside the op-amp at the output stage. It doesn't have anything to do with the architecture of the op amp circuit.</p><p>Moving on to the filter suggestions:</p></div>
<p><span>3. Bandpass filter: We need a 100Hz to 20kHz passband with 20dB gain. Some filter options:&nbsp;</span><br><span>&nbsp; &nbsp; &nbsp;a) Passive RC: Simple, but requires large component values for low frequencies, and loading affects cutoff frequencies. Gain must come from amp.&nbsp;</span><br><span>&nbsp; &nbsp; &nbsp;b) Active 1st-order: Sallen-Key or Multiple-Feedback topologies. Each is 1st-order, so we'd cascade a low-pass and high-pass for the bandpass response. Easy to integrate gain. Cutoff frequencies affected by component tolerance.&nbsp;</span><br><span>&nbsp; &nbsp; &nbsp;c) Active 2nd-order: Single stage 2nd-order bandpass like a Dual-Amplifier Bandpass (DABP). More complex, but more efficient than cascaded 1st-order. More stable cutoff frequencies.&nbsp;</span><br><span>&nbsp; &nbsp; &nbsp;d) Switched-capacitor: Uses a clock to set cutoff frequencies. Very stable, but requires a clock signal.&nbsp;</span></p>
<p><span>The comments on the Passive RC filter are reasonable.&nbsp;<p>The Sallen-Key analysis is wrong - Sallen-Key is a <a href="https://www.ti.com/lit/an/sboa231/sboa231.pdf?ts=1718092808060&amp;ref_url=https%253A%252F%252Fwww.google.com%252F" rel="noopener">second order filter</a>&nbsp;and can be unstable with this much gain.</p><p>The suggestion for the <a href="https://www.analog.com/media/en/training-seminars/tutorials/MT-209.pdf" rel="noopener">DABP</a> isn't wrong per se, but it is missing some nuance. It doesn't allow for very high gain (limit of 2.0). The complexity of a DABP justified when you need to handle higher frequencies and higher Q levels. This application doesn't require that, so it's a weird choice.</p><p>A switched capacitor filter would be bonkers -- complex, noise-sensitive, and requiring exotic components.</p></span></p>
<p><span>Moving on:</span></p>
<div><p><span>For this application, an active 2nd-order filter like a DABP seems a good balance of simplicity, stability, and easy gain integration. &nbsp;</span><br><span>1. Amplifier gain: The filter will have 20dB gain, which is a factor of 10. We can implement this with a non-inverting amp by setting the feedback resistor ratio to 9. For example, if R1 = 1kΩ, then R2 = 9kΩ.</span><br><span>2. Component values: We'll need to calculate the exact resistor and capacitor values for the DABP filter based on the desired cutoff frequencies and gain. There are online calculators to help with this.</span><br><span>&nbsp;So in summary, I propose a non-inverting op-amp configuration for the microphone, followed by a 2nd-order DABP active filter with 20dB gain. The exact component values will depend on the specific microphone chosen and the filter design equations. &nbsp;</span></p><p><span>How's that? Let me know if you have any other questions!</span></p></div>
<p><span>Here, Claude chooses the DABP and estimates some reasonablish values given that choice. (Usually you would choose values greater than 10k and less than 1M in op amp circuit design, to make assumptions about input and output impedance hold better). The wording is a bit confusing but it could be interpreted as a non-inverting op-amp for the gain stage, followed by a DABP filter with a lower gain. Which can work! Technically!<p>It's making slightly strange decisions across the board, but let's proceed with creating a complete design.&nbsp;</p><p>With a little more prompting, we can get Claude to write some code to do the detailed design and generate the circuit:</p></span></p>
<p><span><code>Great! Let's dive into the calculations and generate the netlists. We'll assume a 3.3V power supply and a microphone bias of 2.5V. We'll use an ideal op-amp model for simplicity.</code><p><code>; Microphone Bias Circuit:</code></p><p><code>; - Let's choose a bias resistor of 2.2kΩ.</code><br><code>; - Assuming negligible current draw from the op-amp, the mic bias will be very <br></code></p></span><span><code>; close to 2.5V.</code><p><code>; Netlist:&nbsp;</code><br><code>inst mic : electret-mic&nbsp;</code><br><code>inst rbias : chip-resistor(2200.0)&nbsp;</code><br><code>inst c1 : ceramic-cap(1.0e-6) ; Decoupling cap, assuming 1uF&nbsp;</code><br><code>net (vdd rbias.p[1])&nbsp;</code><br><code>net (rbias.p[2] mic.vdd c1.p[1])&nbsp;</code><br><code>net (gnd mic.gnd c1.p[2])</code></p></span></p>
<p><span>The decoupling cap in the bias circuit here really hampers the high-frequency performance of the circuit. What the circuit actually needs is a <em>coupling cap</em>, shown in the <code>human.mic</code> circuit here:</span></p>
<p><span><img src="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-34-17-2563-PM.png?width=1205&amp;height=480&amp;name=image-png-Jun-21-2024-03-34-17-2563-PM.png" loading="lazy" width="1205" height="480" srcset="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-34-17-2563-PM.png?width=603&amp;height=240&amp;name=image-png-Jun-21-2024-03-34-17-2563-PM.png 603w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-34-17-2563-PM.png?width=1205&amp;height=480&amp;name=image-png-Jun-21-2024-03-34-17-2563-PM.png 1205w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-34-17-2563-PM.png?width=1808&amp;height=720&amp;name=image-png-Jun-21-2024-03-34-17-2563-PM.png 1808w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-34-17-2563-PM.png?width=2410&amp;height=960&amp;name=image-png-Jun-21-2024-03-34-17-2563-PM.png 2410w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-34-17-2563-PM.png?width=3013&amp;height=1200&amp;name=image-png-Jun-21-2024-03-34-17-2563-PM.png 3013w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-34-17-2563-PM.png?width=3615&amp;height=1440&amp;name=image-png-Jun-21-2024-03-34-17-2563-PM.png 3615w" sizes="(max-width: 1205px) 100vw, 1205px"></span></p>
<p><span>Presumably in the training corpus, many articles and datasheets write extensively about decoupling caps. Maybe the "average" recommendation wins out here, and once it sees a positive and negative pin it slaps a capacitor across them right away. In our tests (and most product demos), LLMs <em>loved</em> to give naive advice about decoupling, which usually isn't as actively harmful as it is here.<br></span></p>
<p><span>Moving on to the amplifier:</span></p>
<p><code>; Non-Inverting Amplifier:<p>; - Let's set the gain to 10 (20dB).<br>; - Choose R1 = 1kΩ, then R2 = 9kΩ.</p><p>; Netlist:&nbsp;<br>inst amp : op-amp&nbsp;<br>inst r1 : chip-resistor(1000.0)&nbsp;<br>inst r2 : chip-resistor(9000.0)&nbsp;<br>net (mic.out r1.p[1])&nbsp;<br>net (r1.p[2] amp.v- r2.p[1])&nbsp;<br>net (amp.out r2.p[2])</p></code></p>
<p>The AI describes the amplifier as non-inverting, but then feeds the signal into the inverting input. It's also missing a few other parts and nets that would make for a working circuit. Easier to see in the generated schematic:</p>
<p><img src="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-38-28-2912-PM.png?width=561&amp;height=349&amp;name=image-png-Jun-21-2024-03-38-28-2912-PM.png" loading="lazy" width="561" height="349" srcset="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-38-28-2912-PM.png?width=281&amp;height=175&amp;name=image-png-Jun-21-2024-03-38-28-2912-PM.png 281w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-38-28-2912-PM.png?width=561&amp;height=349&amp;name=image-png-Jun-21-2024-03-38-28-2912-PM.png 561w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-38-28-2912-PM.png?width=842&amp;height=524&amp;name=image-png-Jun-21-2024-03-38-28-2912-PM.png 842w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-38-28-2912-PM.png?width=1122&amp;height=698&amp;name=image-png-Jun-21-2024-03-38-28-2912-PM.png 1122w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-38-28-2912-PM.png?width=1403&amp;height=873&amp;name=image-png-Jun-21-2024-03-38-28-2912-PM.png 1403w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-38-28-2912-PM.png?width=1683&amp;height=1047&amp;name=image-png-Jun-21-2024-03-38-28-2912-PM.png 1683w" sizes="(max-width: 561px) 100vw, 561px">(Cue EE eye twitching)</p>
<p>Moving on to the DABP filter implementation:</p>
<p><code>; DABP Bandpass Filter:<p>; - Cutoff frequencies: fl = 100Hz, fh = 20kHz<br>; - Gain at center frequency: G0 = 10 (20dB)<br>; - Quality factor: Let's choose Q = 1 for a fairly wide passband<br>; - Center frequency: f0 = sqrt(fl * fh) ≈ 1.414kHz<br>; - Let's choose C1 = C2 = 10nF<br>; - R1 = R2 = 1 / (2 * pi * f0 * C1) ≈ 11.2kΩ, let's use 11kΩ<br>; - R3 = R1 / (2 * Q * G0) ≈ 550Ω, let's use 560Ω<br>; - R4 = 2 * Q * R1 / G0 ≈ 2.2kΩ</p><p>; Netlist:&nbsp;<br>inst op1 : op-amp&nbsp;<br>inst op2 : op-amp&nbsp;<br>inst r1 : chip-resistor(11000.0)&nbsp;<br>inst r2 : chip-resistor(11000.0)&nbsp;<br>inst r3 : chip-resistor(560.0)&nbsp;<br>inst r4 : chip-resistor(2200.0)&nbsp;<br>inst c1 : ceramic-cap(10.0e-9)&nbsp;<br>inst c2 : ceramic-cap(10.0e-9)&nbsp;<br>net (amp.out op1.v+ r1.p[1] c1.p[1])&nbsp;<br>net (op1.v- op2.out r1.p[2] c1.p[2])&nbsp;<br>net (op1.out r3.p[1])&nbsp;<br>net (r3.p[2] op2.v- r2.p[1] c2.p[1])&nbsp;<br>net (op2.v+ r2.p[2] c2.p[2] r4.p[1])&nbsp;<br>net (r4.p[2] gnd)</p></code></p>
<p><img src="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-41-30-4263-PM.png?width=934&amp;height=425&amp;name=image-png-Jun-21-2024-03-41-30-4263-PM.png" loading="lazy" width="934" height="425" srcset="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-41-30-4263-PM.png?width=467&amp;height=213&amp;name=image-png-Jun-21-2024-03-41-30-4263-PM.png 467w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-41-30-4263-PM.png?width=934&amp;height=425&amp;name=image-png-Jun-21-2024-03-41-30-4263-PM.png 934w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-41-30-4263-PM.png?width=1401&amp;height=638&amp;name=image-png-Jun-21-2024-03-41-30-4263-PM.png 1401w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-41-30-4263-PM.png?width=1868&amp;height=850&amp;name=image-png-Jun-21-2024-03-41-30-4263-PM.png 1868w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-41-30-4263-PM.png?width=2335&amp;height=1063&amp;name=image-png-Jun-21-2024-03-41-30-4263-PM.png 2335w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-41-30-4263-PM.png?width=2802&amp;height=1275&amp;name=image-png-Jun-21-2024-03-41-30-4263-PM.png 2802w" sizes="(max-width: 934px) 100vw, 934px">Generated circuit (left), versus reference from ADI (right).</p>
<p>A few more misses on the netlist here, and about as correct as the amplifier (which is to say, not very). We also see the weirdly strong default of attaching capacitors across positive and negative pins. I guess LLMs are high decouplers.</p>
<p>And to round it off, we've got another reminder to add decoupling caps, and a reasonable note about what to watch out for:</p>
<p><span>The output of op2 is the final output of the pre-amp circuit. This should give you a good starting point. Remember to add decoupling capacitors and power supply connections as needed. Also, the actual component values may need to be adjusted based on the real specs of your chosen op-amp and the exact requirements of your ADC. Let me know if you have any other questions!</span></p>
<h3><span>Comparing against ground truth</span></h3>
<p><span>This <a href="https://www.ti.com/lit/ug/tidu765/tidu765.pdf" rel="noopener">reference design</a> from Texas Instruments is a good implementation of the requirements we asked these LLMs to design to. It uses passive RC filters (which the AI considered and avoided), and a single op-amp. I did a quick implementation of this in JITX (<a href="https://github.com/JITx-Inc/generative-ai-test-cases/blob/main/amplifier/reference.stanza" rel="noopener">code here</a>) to compare the AI generated circuit (<a href="https://github.com/JITx-Inc/generative-ai-test-cases/blob/main/amplifier/ai-generated.stanza" rel="noopener">code here</a>) to the TI version.</span></p>
<p><span><img src="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-49-51-0951-PM.png?width=849&amp;height=547&amp;name=image-png-Jun-21-2024-03-49-51-0951-PM.png" loading="lazy" width="849" height="547" srcset="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-49-51-0951-PM.png?width=425&amp;height=274&amp;name=image-png-Jun-21-2024-03-49-51-0951-PM.png 425w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-49-51-0951-PM.png?width=849&amp;height=547&amp;name=image-png-Jun-21-2024-03-49-51-0951-PM.png 849w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-49-51-0951-PM.png?width=1274&amp;height=821&amp;name=image-png-Jun-21-2024-03-49-51-0951-PM.png 1274w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-49-51-0951-PM.png?width=1698&amp;height=1094&amp;name=image-png-Jun-21-2024-03-49-51-0951-PM.png 1698w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-49-51-0951-PM.png?width=2123&amp;height=1368&amp;name=image-png-Jun-21-2024-03-49-51-0951-PM.png 2123w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-49-51-0951-PM.png?width=2547&amp;height=1641&amp;name=image-png-Jun-21-2024-03-49-51-0951-PM.png 2547w" sizes="(max-width: 849px) 100vw, 849px"></span></p>
<p><span>The AI generated circuit was three times the cost and size of the design created by that expert engineer at TI. It is also missing many of the necessary connections.&nbsp;<p>I found this breakdown surprising - it had no problems generating code in general, and was able to generate a syntactically correct netlist. The code runs! It's just that the pins are not in the right net statements and the resulting circuit is not correct.</p></span></p>
<h3><span>Testing higher level code</span></h3>
<p><span>In fairness, it's rare to write code like this to design a circuit board. In practice you're primarily calling higher level functions, e.g. selecting an op-amp, configuring a filter with parameters, then using a parametric pre-amp circuit:</span></p>
<p><code>&nbsp; val OpAmpType = components/STMicroelectronics/TS971ILT/component</code><br><code>&nbsp; val bandpass = BandpassConfig(gain = 34.0, low-cut = 100.0, high-pass = 20.0e3)</code><br><code>&nbsp; val preampType = ocdb/modules/amplifiers/microphone-preamp(OpAmpType, bandpass)</code><br><code>&nbsp; inst amp : preampType</code></p>
<div><p>Would LLMs do better if we try to get them to define and use higher-level functions instead?</p><p>Gemini 1.5 did the best here, inventing some reasonable APIs to functions that would exist in a library of helpful circuits:</p></div>
<p><code># Modules</code><br><code>amp = inverting_amplifier("amp1", gain=-20.0)&nbsp;</code><br><code>filt = sallen_key_filter("filt1", low_cutoff=100.0, high_cutoff=20.0e3)&nbsp;</code><br><code># Connections&nbsp;</code><br><code>net (mic.pos r_bias.p[1]) # Microphone to bias resistor&nbsp;</code><br><code>net (r_bias.p[2] c1.p[1] amp.in) # Bias resistor to amp through coupling capacitor</code></p>
<p>(Still misses on the netlist generation though.)</p>
<div><p>Our experiments with code generation reveal an interesting pattern: the LLMs are not great at generating detailed code for a netlist <em>to implement a circuit design</em>. That was a little surprising because as shown in the datasheet parsing section, they can write a 95% correct netlist matching an uploaded image of a schematic. This difference suggests that LLMs excel at information extraction and transformation tasks, but struggle with original design synthesis.</p><p>Having the LLMs generate higher-level code worked better than raw netlist generation, and seems like a promising path forward. This suggests a hybrid approach where human experts create parametric implementations of common circuits, which LLMs can then utilize and adapt, potentially increasing overall reliability.</p></div>
<p>Future research could either explore fine-tuning models specifically for netlist generation tasks, or (this being generative AI) wait a month and see how well the next generation of models does on this problem.</p>
<h2><span>Conclusion</span></h2>
<p><span>Circuit board design requires a lot of precision to get all the details correct -- for unsupervised AI techniques, I think there is a data problem here.&nbsp;<p>The context and meaning of a design -- What does this do? How do you know if it's good? -- are usually absent from schematic and board design data. Maybe there's a little text comment on the schematic with filter calculations, but that's usually it, and the rest lives in the engineer's head.</p><p>An electrical engineer looks at a schematic, understands a portion as a Sallen-Key filter, knows the limitations and function of that circuit, and composes that understanding with the rest of the design. A tremendous amount of context, which is not in any of the design data you can scrape from the internet.</p><p>Our experiments show that generative AI models can be useful for circuit board design, if you use them to write code. With that approach, there were still significant differences across Gemini 1.5, Claude 3 Opus, and GPT-4o for circuit board design.&nbsp;</p><p>Claude 3 was most helpful for answering questions to learn new domains. There's a risk of hallucinations, but those usually fall away if you interrogate the model until it's consistent. Certainly Claude is better and more helpful than the *average* article on circuit board design you would find via web search (there are very good articles, but they can be hard to find).</p><p>Gemini's huge context window was very helpful for pulling component data out of datasheets. It was the one area where the AI 'just worked' and we got way more usability than we expected.</p><p>I'm interested in further testing to find the limits of this technique. Google has done a lot of work on the "Text Haystack" problem to pull tiny pieces of information accurately out of a massive corpus of data, and that's serving them well here.</p></span></p>
<p><span><img src="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-59-44-3288-PM.png?width=933&amp;height=601&amp;name=image-png-Jun-21-2024-03-59-44-3288-PM.png" loading="lazy" width="933" height="601" srcset="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-59-44-3288-PM.png?width=467&amp;height=301&amp;name=image-png-Jun-21-2024-03-59-44-3288-PM.png 467w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-59-44-3288-PM.png?width=933&amp;height=601&amp;name=image-png-Jun-21-2024-03-59-44-3288-PM.png 933w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-59-44-3288-PM.png?width=1400&amp;height=902&amp;name=image-png-Jun-21-2024-03-59-44-3288-PM.png 1400w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-59-44-3288-PM.png?width=1866&amp;height=1202&amp;name=image-png-Jun-21-2024-03-59-44-3288-PM.png 1866w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-59-44-3288-PM.png?width=2333&amp;height=1503&amp;name=image-png-Jun-21-2024-03-59-44-3288-PM.png 2333w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-03-59-44-3288-PM.png?width=2799&amp;height=1803&amp;name=image-png-Jun-21-2024-03-59-44-3288-PM.png 2799w" sizes="(max-width: 933px) 100vw, 933px">From the <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf" rel="noopener">Gemini 1.5 Report</a></span></p>
<p><span>Surprisingly GPT-4o didn't have the most useful answer in any of the categories we tested. It feels wrong to not include any outputs from the model in this summary article, so here's how GPT-4o feels about coming in third:</span></p>
<p><span><img src="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-04-01-30-3403-PM.png?width=345&amp;height=342&amp;name=image-png-Jun-21-2024-04-01-30-3403-PM.png" loading="lazy" width="345" height="342" srcset="https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-04-01-30-3403-PM.png?width=173&amp;height=171&amp;name=image-png-Jun-21-2024-04-01-30-3403-PM.png 173w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-04-01-30-3403-PM.png?width=345&amp;height=342&amp;name=image-png-Jun-21-2024-04-01-30-3403-PM.png 345w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-04-01-30-3403-PM.png?width=518&amp;height=513&amp;name=image-png-Jun-21-2024-04-01-30-3403-PM.png 518w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-04-01-30-3403-PM.png?width=690&amp;height=684&amp;name=image-png-Jun-21-2024-04-01-30-3403-PM.png 690w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-04-01-30-3403-PM.png?width=863&amp;height=855&amp;name=image-png-Jun-21-2024-04-01-30-3403-PM.png 863w, https://blog.jitx.com/hs-fs/hubfs/image-png-Jun-21-2024-04-01-30-3403-PM.png?width=1035&amp;height=1026&amp;name=image-png-Jun-21-2024-04-01-30-3403-PM.png 1035w" sizes="(max-width: 345px) 100vw, 345px"><br></span></p>
<p><span>Overall we have some promising directions. Using LLMs for circuit board design looks a lot like using them for other complex tasks. They work well for pulling concrete data out of human-shaped data sources, they can do slightly more difficult tasks if they can solve that task by writing code, but eventually their capabilities break down in domains too far out of the training distribution.<p>We only tested the frontier models in this work, but I predict similar results from the open-source Llama or Mistral models. Some fine tuning on netlist creation would likely make the generation capabilities more useful.</p><p>Our work shows both the potential and limitations of current LLMs in specialized fields like circuit design. They're excellent at retrieving information and generating code, but struggle with the nuanced decision-making needed for complex designs. Future LLMs will likely improve here, but for the current model generation, they're best used as tools to assist human experts rather than as standalone designers.</p></span></p>
<p><span>Let me know what you think! You can find me on the JITX discord, and on <a href="https://x.com/DuncanHaldane" rel="noopener">Twitter</a>.</span></p></span>
      </p>
      
        
      
    </article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI Acquires Rockset (215 pts)]]></title>
            <link>https://openai.com/index/openai-acquires-rockset/</link>
            <guid>40750391</guid>
            <pubDate>Fri, 21 Jun 2024 15:04:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/openai-acquires-rockset/">https://openai.com/index/openai-acquires-rockset/</a>, See on <a href="https://news.ycombinator.com/item?id=40750391">Hacker News</a></p>
Couldn't get https://openai.com/index/openai-acquires-rockset/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[How the square root of 2 became a number (103 pts)]]></title>
            <link>https://www.quantamagazine.org/how-the-square-root-of-2-became-a-number-20240621/</link>
            <guid>40750230</guid>
            <pubDate>Fri, 21 Jun 2024 14:50:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/how-the-square-root-of-2-became-a-number-20240621/">https://www.quantamagazine.org/how-the-square-root-of-2-became-a-number-20240621/</a>, See on <a href="https://news.ycombinator.com/item?id=40750230">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The ancient Greeks wanted to believe that the universe could be described in its entirety using only whole numbers and the ratios between them — fractions, or what we now call rational numbers. But this aspiration was undermined when they considered a square with sides of length 1, only to find that the length of its diagonal couldn’t possibly be written as a fraction.</p>
<p>The first proof of this (there would be several) is commonly attributed to Pythagoras, a 6th-century BCE philosopher, even though none of his writings survive and little is known about him. Nevertheless, “it was the first crisis in what we call the foundations of mathematics,” said <a href="https://publish.uwo.ca/~jbell/">John Bell</a>, a professor emeritus at Western University in London, Ontario.</p>
<p>That crisis would not be resolved for a long time. Though the ancient Greeks could establish what $latex \sqrt{2}$ was not, they didn’t have a language for explaining what it was.</p>
<p>For millennia, this sufficed. Renaissance mathematicians manipulated what they came to call irrational numbers while trying to solve algebraic equations. The modern notation for square roots came into use in the 16th and 17th centuries. But still, there was something slippery about them. Does $latex \sqrt{2}$ exist in the same way that 2 does? It wasn’t clear.</p>

<p>Mathematicians continued to live with that ambiguity. Then, in the mid-1800s, Richard Dedekind, among others, realized that calculus — which had been developed 200 years earlier by Isaac Newton and Gottfried Leibniz — stood on a shaky foundation. A reserved but gifted mathematician who worked slowly and published relatively little, Dedekind was preparing to teach his students about continuous functions when he realized that he couldn’t give a satisfactory explanation of what it meant for a function to be continuous.</p>
<p>He hadn’t even seen functions properly defined. And that, he argued, required a good understanding of how numbers worked — something mathematicians seemed to have taken for granted. How, he asked, could you know for sure that $latex \sqrt{2}$ multiplied by $latex \sqrt{3}$ equals $latex \sqrt{6}$? He wanted to provide some answers.</p>
<p>And so he introduced a way to define and construct the irrational numbers using only the rationals. Here’s how it works: First, split all the rational numbers into two sets, so that all of the fractions in one set are smaller than those in the other. For instance, in one group, collect all rationals that, when squared, are less than 2; in the other, put all rationals whose squares are greater than 2. Exactly one number plugs the hole between these two sets. Mathematicians give it the label $latex \sqrt{2}$. For Dedekind, then, an irrational number is defined by a pair of infinite sets of rational numbers, which create what he called a cut. “It’s a very lovely idea,” said <a href="https://ianstewartjoat.weebly.com/">Ian Stewart</a> of the University of Warwick. “You can pin down the missing irrational numbers not by describing them, but by describing the gap in which they have to sit.”</p>
<p>Dedekind showed that you can fill in the entire number line this way, rigorously defining for the first time what are now called real numbers (the rationals and the irrationals combined).</p>
<p>At around the same time that Dedekind introduced his cuts, his friend and collaborator Georg Cantor also began to think about irrational numbers. The overlap made their relationship complicated. “They were good friends, and they hated each other. They cooperated, and they ignored each other,” said <a href="https://www.openu.ac.il/en/about/president/pages/default.aspx">Leo Corry</a>, a historian of science who is the president of the Open University of Israel.</p>

<p>Cantor came up with a different definition of irrational numbers. He expressed each in terms of sequences of rational numbers that approached, or “converged” to, a particular irrational value. Though Cantor’s irrational numbers initially looked different from Dedekind’s, later work proved that they are mathematically equivalent.</p>
<p>Cantor’s work led him to ask how many numbers exist. The question might at first seem strange. There are infinitely many whole numbers — you can always keep adding one more. Presumably, that’s as big as a set of numbers can get. But Cantor showed that, paradoxically, though the number of fractions is the same as the number of integers, there are demonstrably more irrational numbers. He was the first to realize that <a href="https://www.quantamagazine.org/how-many-numbers-exist-infinity-proof-moves-math-closer-to-an-answer-20210715/">infinity comes in many sizes</a>.</p>
<p>The number line was more crowded, and weirder, than anyone had imagined. But mathematicians were only able to see that after a change in perspective.</p>

<p>Dedekind’s cuts are arguably the beginning of modern mathematics. “It’s really the first point in the history of mathematics where mathematicians actually know what they’re talking about,” Stewart said. Dedekind and others used his definition to prove major theorems in calculus for the first time — which allowed them not just to strengthen the edifice that Leibniz and Newton had built, but to add to it. Dedekind’s work enabled mathematicians to better understand sequences and functions. Emmy Noether, a prolific mathematician who helped shape the field of abstract algebra in the early 20th century, is said to have <a href="https://core.ac.uk/download/pdf/82253306.pdf">told her students</a> that “everything is already in Dedekind.”</p>
<p>A formal definition of $latex \sqrt{2}$ opened new horizons for exploration beyond the topics in calculus that initially motivated Dedekind. As Stewart put it, “After Dedekind, mathematicians started to realize that you can invent new concepts altogether. … The whole idea of what mathematics is about becomes much broader and more flexible.”</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Innovation heroes are a sign of a dysfunctional organization (230 pts)]]></title>
            <link>https://steveblank.substack.com/p/why-innovation-heroes-are-a-sign</link>
            <guid>40749754</guid>
            <pubDate>Fri, 21 Jun 2024 14:03:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://steveblank.substack.com/p/why-innovation-heroes-are-a-sign">https://steveblank.substack.com/p/why-innovation-heroes-are-a-sign</a>, See on <a href="https://news.ycombinator.com/item?id=40749754">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>A week ago I got invited to an “innovation hero” award ceremony at a government agency. I don’t know how many of these I’ve been to in the last couple years, but this one just made my head explode.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic" width="398" height="515.534375" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1658,&quot;width&quot;:1280,&quot;resizeWidth&quot;:398,&quot;bytes&quot;:317909,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbd131c-ff57-4be3-8b28-f714b863e28c_1280x1658.heic 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>The award was for an entrepreneur who worked against all odds to buck the system to turn her insight into an application. She had realized it was possible to automate a process that was being done manually – reentering data from one spreadsheet to another and annotating it with additional data from another system. Inspired by her own work problem, she talked to her peers and other stakeholders, built multiple minimum viable products, and figured out how to get engineering, policy, legal, security and everyone else in the enterprise to actually approve it. And then she fought with the acquisition folks to buy the trivial amount of additional hardware needed to connect it. It was a development process that would’ve taken three weeks in a startup, but inside this agency took 10 months (which was considered fast.) At each step she was confronted with “we’re not budgeted for this” or “this isn’t on our schedule” and “this isn’t your job.” Most rational people would’ve given up and said “you can’t fight the system“ but yet she persisted.</p><p>Having seen this scenario play out multiple times at multiple large corporations and government agencies, I could’ve repeated the speech her agency director made at the ceremony verbatim. “Blah blah blah and a $100 bonus.” Everyone politely applauded and went back to work feeling good. I was simply depressed. Never once did anyone ever step back and say that what we just witnessed was leadership rewarding and perpetuating a dysfunctional and broken system.</p><p>I’m constantly puzzled why thoughtful and astute CEOs and Agency Directors never ask, “Why is it that innovations require heroics to occur in our organization? Why don’t we have a repeatable process for innovation? What are the obstacles in the way of delivering needed innovation with speed and urgency in our organization? Why is it that after each one of these awards we don’t go back and fix the parts of the system that made creating something new so difficult?”</p><p>Instead, everyone at this award ceremony just went back to work like it was business as usual. I realized that innovation in this organization was going to continue to happen by heroics and exception rather than by design. As I’ve seen play out way too many times, ultimately the innovators get tired of banging their heads against the wall and leave government service or large companies. Their organizations hemorrhage the very people they need to help them compete against aggressive adversaries or competitors who have them in their sights.</p><p data-attrs="{&quot;url&quot;:&quot;https://steveblank.substack.com/p/why-innovation-heroes-are-a-sign?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://steveblank.substack.com/p/why-innovation-heroes-are-a-sign?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p><strong><span>An Organizational Design Problem</span><br></strong><span>Sadly, this wasn’t a single act of bad management or malice. No single individual thought they weren’t doing their job. However, if anyone had taken the time to deconstruct the reason for the roadblocks to innovation, they would have uncovered they weren’t just obstinate middle managers, or a single bad process. Asking a series of “</span><a href="https://en.wikipedia.org/wiki/Five_whys" rel="">five whys</a><span>,” (see </span><a href="https://hbr.org/2010/04/the-five-whys-for-startups" rel="">this HBR article</a><span>) would have discovered that:</span></p><ol><li><p>The agency’s existing processes were not designed for non-standard work. As in most large organizations, they were designed for the repeatable execution of pre-defined tasks.</p></li><li><p>There were no resources available for non-standard work or any parallel organization responsible for innovation.</p></li><li><p>The culture of the organization discouraged experimentation and punished the inevitable failures of a learning and discovery process.</p></li></ol><p><span>Ultimately, the root cause was the entire government agency lacked an Innovation Doctrine. This manifested itself as an organizational design problem. There was simply no permanent place in the organization for unscheduled innovation to happen. And even if there had been, there was </span><em>no way to turn demos into deployment with speed and at scale</em><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic" width="714" height="295" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:295,&quot;width&quot;:714,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:33316,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55655110-1375-4b9a-80ec-34aed9bd5bd9_714x295.heic 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong><span>Innovation Doctrine</span><br></strong><span>In peacetime and/or when you’re the dominant superpower (or a commercial market leader), the emphasis is on process, procedures, and sustainment of existing systems. Deviations from that create chaos and diverge from the predetermined are not welcomed, let alone promoted, and funded. They are eliminated. This works great when the external environment -competitors, adversaries, technologies, threats – is static. However in times of crisis, war or disruption, these unconventional thinkers and innovators are exactly what is needed, and their ideas need to be rapidly deployed.</span></p><p><span>Well-managed organizations realize that they need both innovation and execution. With execution being dominant in peacetime/competitive advantage you have </span><em>managers of process</em><span>. In crisis/wartime innovation is dominant. Instead of mangers of process you need </span><em>innovation leaders</em><span> who shepherd ideas through an </span><a href="https://steveblank.com/2017/09/14/how-companies-strangle-innovation/" rel="">innovation pipeline</a><span>. (see </span><a href="https://steveblank.com/2017/09/14/how-companies-strangle-innovation/" rel="">this HBR article</a><span>.) Successful organizations recognize that innovation isn’t a single activity (incubators, accelerators, hackathons); it is a strategically organized end-to-end&nbsp;process&nbsp;from idea to deployment.</span></p><p><span>While innovation and execution have different processes, people, and culture, they need to respect and depend on each other. This ambidexterity (see </span><a href="https://hbr.org/2004/04/the-ambidextrous-organization" rel="">this HBR article</a><span>) and the innovation processes that go with it require an innovation doctrine – an overall strategy and playbook for the entire organization and enterprise that includes an innovation pipeline and processes intended to drive innovation efforts, and describes the role of innovation leaders in an ambidextrous organization – all focused on rapid deployment of new capabilities.</span></p><p><strong>Lessons Learned</strong></p><ul><li><p>Innovation heroics are a symptom of a lack of an innovation doctrine</p></li><li><p>An innovation doctrine has a playbook, and innovation pipeline and describes the role of innovation leaders in an ambidextrous organization – all focused on rapid deployment of new capabilities</p></li><li><p>All large organizations – both government and corporate—need an innovation doctrine or else risk being outpaced by competitors.</p></li></ul></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Spending 3 months investigating a 7-year old bug and fixing it in 1 line of code (181 pts)]]></title>
            <link>https://lemmy.world/post/16763534</link>
            <guid>40749624</guid>
            <pubDate>Fri, 21 Jun 2024 13:49:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lemmy.world/post/16763534">https://lemmy.world/post/16763534</a>, See on <a href="https://news.ycombinator.com/item?id=40749624">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="postContent"><div><p dir="auto">I originally told the story over on the other site, but I thought I’d share it here. With a bonus!</p>
<p dir="auto">I was working on a hardware accessory for the OG iPad. The accessory connected to the iPad over USB and provided MIDI in/out and audio in/out appropriate for a musician trying to lay down some tracks in Garage Band.</p>
<p dir="auto">It was a winner of a product because at its core, it was based on a USB product we had already been making for PCs for almost a decade. All we needed was a little microcontroller to put the iPad into USB host mode (this was in the 30-pin connector days), and then allow it to connect to what was basically a finished product.</p>
<p dir="auto">This product was so old in fact that nobody knew how to compile the source code. When it came time to get it working, someone had to edit the binaries to change the USB descriptors to reflect the new product name and that it drew &lt;10mA from the iPad’s USB port (the original device was port-powered, but the iPad would get angry if you requested more than 10mA even if you were self-powered). This was especially silly because the original product had a 4-character name, but the new product had a 7-character name. We couldn’t make room for the extra bytes, so we had to truncate the name to fit it into the binary without breaking anything.</p>
<p dir="auto">Anyway, product ships and we notice a problem. Every once in a while, a MIDI message is missed. For those of you not familiar, MIDI is used to transmit musical notes that can be later turned into audio by whatever processor/voice you want. A typical message contains the note (A, B, F-sharp, etc), a velocity (how hard you hit the key), and whether it’s a key on or key off. So pressing and releasing a piano key generate two separate messages.</p>
<p dir="auto">Missing the occasional note message wouldn’t typically be a big deal except for instrument voices with infinite sustain like a pipe organ. If you had the pipe organ voice selected when using our device, it’s possible that it would receive a key on, but not a key off. This would result in the iPad assuming that you were holding the key down indefinitely.</p>
<p dir="auto">There isn’t an official spec for what to do if you receive another key-on of the same note without a key-off in between, but Apple handled this in the worst way possible. The iPad would only consider the key released if the number of key-ons and key-offs matched. So the only way to release this pipe organ key was to hope for it to skip a subsequent key-on message for the same key and then finally receive the key-off. The odds of this happening are approximately 0%, so most users had to resort to force quitting the app.</p>
<p dir="auto">Rumors flooded the customer message boards about what could cause this behavior, maybe it was the new iOS update? Maybe you had to close all your other apps? There was a ton of hairbrained theories floating around, but nobody had any definitive explanation.</p>
<p dir="auto">Well I was new to the company and fresh out of college, so I was tasked with figuring this one out.</p>
<p dir="auto">First step was finding a way to generate the bug. I wrote a python script that would hammer scales into our product and just listened for a key to get stuck. I can still recall the cacophony of what amounted to an elephant on cocaine slamming on a keyboard for hours on end.</p>
<p dir="auto">Eventually, I could reproduce the bug about every 10 minutes. One thing I noticed is that it only happened if multiple keys were pressed simultaneously. Pressing one key at a time would never produce the issue.</p>
<p dir="auto">Using a fancy cable that is only available to Apple hardware developers, I was able to interrogate the USB traffic going between our product and the iPad. After a loooot of hunting (the USB debugger could only sample a small portion, so I had to hit the trigger right when I heard the stuck note), I was able to show that the offending note-off event was never making it to the iPad. So Apple was not to blame; our firmware was randomly not passing MIDI messages along.</p>
<p dir="auto">Next step was getting the source to compile. I don’t remember a lot of the details, but it depended on “hex3bin” which I assume was some neckbeard’s version of hex2bin that was “better” for some reasons. I also ended up needing to find a Perl script that was buried deep in some university website. I assume that these tools were widely available when the firmware was written 7 years prior, but they took some digging. I still don’t know anything about Perl, but I got it to run.</p>
<p dir="auto">With firmware compiling, I was able to insert instructions to blink certain LEDs (the device had a few debug LEDs inside that weren’t visible to the user) at certain points in the firmware. There was no live debugger available for the simple 8-bit processor on this thing, so that’s all I had.</p>
<p dir="auto">What it came down to was a timing issue. The processor needed to handle audio traffic as well as MIDI traffic. It would pause whatever it was doing while handling the audio packets.
The MIDI traffic was buffered, so if a key-on or key-off came in while the audio was being handled, it would be addressed immediately after the audio was done.</p>
<p dir="auto">But it was only single buffered. So if a second MIDI message came in while audio was being handled, the second note would overwrite the first, and that first note would be forever lost. There is a limit to how fast MIDI notes can come in over USB, and it was just barely faster than it took to process the audio. So if the first note came in just after the processor cut to handling audio, the next note could potentially come in just before the processor cut back.</p>
<p dir="auto">Now for the solution. Knowing very little about USB audio processing, but having cut my teeth in college on 8-bit 8051 processors, I knew what kind of functions tended to be slow. I did a Ctrl+F for “%” and found a 16-bit modulo right in the audio processing code.</p>
<p dir="auto">This 16-bit modulo was just a final check that the correct number of bytes or bits were being sent (expecting remainder zero), so the denominator was going to be the same every time. The way it was written, the compiler assumed that the denominator could be different every time, so in the background it included an entire function for handling 16-bit modulos on an 8-bit processor.</p>
<p dir="auto">I googled “optimize modulo,” and quickly learned that given a fixed denominator, any 16-bit modulo can be rewritten as three 8-bit modulos.</p>
<p dir="auto">I tried implementing this single-line change, and the audio processor quickly dropped from 90us per packet to like 20us per packet. This 100% fixed the bug.</p>
<p dir="auto">Unfortunately, there was no way to field-upgrade the firmware, so that was still a headache for customer service.</p>
<p dir="auto">As to why this bug never showed up in the preceding 7 years that the USB version of the product was being sold, it was likely because most users only used the device as an audio recorder or MIDI recorder. With only MIDI enabled, no audio is processed, and the bug wouldn’t happen. The iPad however enabled every feature all the time. So the bug was always there. It’s just that nobody noticed it. Edit: also, many MIDI apps don’t do what Apple does and require matching key on/key off events. So if a key gets stuck, pressing it again will unstick it.</p>
<p dir="auto">So three months of listening to Satan banging his fists on a pipe organ lead to a single line change to fix a seven year old bug.</p>
<p dir="auto">TL;DR: 16-bit modulo on an 8-bit processor is slow and caused packets to get dropped.</p>
<p dir="auto">The bonus is at 4:40 in this video <a href="https://youtu.be/DBfojDxpZLY?si=oCUlFY0YrruiUeQq" rel="noopener nofollow">https://youtu.be/DBfojDxpZLY?si=oCUlFY0YrruiUeQq</a></p>
</div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[From Infocom to 80 Days: An oral history of text games and interactive fiction (141 pts)]]></title>
            <link>https://arstechnica.com/gaming/2024/06/from-infocom-to-80-days-an-oral-history-of-text-games-and-interactive-fiction/</link>
            <guid>40749426</guid>
            <pubDate>Fri, 21 Jun 2024 13:30:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gaming/2024/06/from-infocom-to-80-days-an-oral-history-of-text-games-and-interactive-fiction/">https://arstechnica.com/gaming/2024/06/from-infocom-to-80-days-an-oral-history-of-text-games-and-interactive-fiction/</a>, See on <a href="https://news.ycombinator.com/item?id=40749426">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/45897016062_e1c52b1dd9_k-800x1199.jpg" alt="Zork running on an Amiga at the Computerspielemuseum in Berlin, Germany.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/11/45897016062_e1c52b1dd9_k.jpg" data-height="2048" data-width="1366">Enlarge</a> <span>/</span> Zork running on an Amiga at the Computerspielemuseum in Berlin, Germany.</p></figcaption>  </figure>

  




<!-- cache hit 60:single/related:5c42246a6933a50d11619419eada3ee5 --><!-- empty -->
<p><em>You are standing at the end of a road before a small brick building.</em></p>
<p>That simple sentence first appeared on a PDP-10 mainframe in the 1970s, and the words marked the beginning of what we now know as interactive fiction.</p>
<p>From the bare-bones text adventures of the 1980s to the heartfelt hypertext works of Twine creators, interactive fiction is an art form that continues to inspire a loyal audience. The community for interactive fiction, or IF, attracts readers and players alongside developers and creators. It champions an open source ethos and a punk-like individuality.</p>
<p>But whatever its production value or artistic merit, at heart, interactive fiction is simply words on a screen. In this time of AAA video games, prestige television, and contemporary novels and poetry, how does interactive fiction continue to endure?
</p><p>To understand the history of IF, the best place to turn for insight is the authors themselves. Not just the authors of notable text games—although many of the people I interviewed for this article do have that claim to fame—but the authors of the communities and the tools that have kept the torch burning. Here's what they had to say about IF and its legacy.</p>
<h2>Examine roots: Adventure and Infocom</h2>
<p>The interactive fiction story began in the 1970s. The first widely played game in the genre was <i>Colossal Cave Adventure</i>, also known simply as <i>Adventure</i>. The text game was made by Will Crowther in 1976, based on his experiences spelunking in Kentucky’s aptly named Mammoth Cave. Descriptions of the different spaces would appear on the terminal, then players would type in two-word commands—a verb followed by a noun—to solve puzzles and navigate the sprawling in-game caverns.</p>                                            
                                                        
<p>During the 1970s, getting the chance to interact with a computer was a rare and special thing for most people.</p>
<p>"My father's office had an open house in about 1978," IF author and tool creator Andrew Plotkin recalled. "We all went in and looked at the computers—computers were very exciting in 1978—and he fired up <i>Adventure</i> on one of the terminals. And I, being eight years old, realized this was the best thing in the universe and immediately wanted to do that forever."</p>
<p>"It is hard to overstate how potent the effect of this game was," said Graham Nelson, creator of the Inform language and author of the landmark IF <i>Curses</i>, of his introduction to the field. "Partly that was because the behemoth-like machine controlling the story was itself beyond ordinary human experience."
</p><p>Perhaps that extraordinary factor is what sparked the curiosity of people like Plotkin and Nelson to play <i>Adventure</i> and the other text games that followed. The roots of interactive fiction are entangled with the roots of the computing industry. "I think it's always been a focus on the written word as an engine for what we consider a game," said software developer and tech entrepreneur Liza Daly. "Originally, that was born out of necessity of primitive computers of the '70s and '80s, but people discovered that there was a lot to mine there."</p>
<p>Home computers were just beginning to gain traction as Stanford University student Don Woods released his own version of <i>Adventure</i> in 1977, based on Crowther’s original Fortran work. Without wider access to comparatively pint-sized machines like the Apple 2 and the Vic-20, Scott Adams might not have found an audience for his own text adventure games, released under his company Adventure International, in another homage to Crowther. As computers spread to more people around the world, interactive fiction was able to reach more and more readers.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Allan McDonald Refused To Approve Challenger Launch, Exposed Cover-Up (2021) (498 pts)]]></title>
            <link>https://www.npr.org/2021/03/07/974534021/remembering-allan-mcdonald-he-refused-to-approve-challenger-launch-exposed-cover</link>
            <guid>40748371</guid>
            <pubDate>Fri, 21 Jun 2024 11:23:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.npr.org/2021/03/07/974534021/remembering-allan-mcdonald-he-refused-to-approve-challenger-launch-exposed-cover">https://www.npr.org/2021/03/07/974534021/remembering-allan-mcdonald-he-refused-to-approve-challenger-launch-exposed-cover</a>, See on <a href="https://news.ycombinator.com/item?id=40748371">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storytext">
      <div id="res974534902">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s=1100&amp;c=15&amp;f=jpeg" data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s=1100&amp;c=100&amp;f=jpeg" data-template="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s={width}&amp;c={quality}&amp;f={format}" data-format="webp" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s=1100&amp;c=15&amp;f=jpeg" data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s=1100&amp;c=100&amp;f=jpeg" data-template="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s={width}&amp;c={quality}&amp;f={format}" data-format="jpeg" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s={width}&amp;c={quality}&amp;f={format}" data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s=1100&amp;c=100&amp;f=jpeg" data-template="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s={width}&amp;c={quality}&amp;f={format}" data-format="jpeg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                Allan McDonald in 2016 holds a commemorative poster honoring the seven astronauts killed aboard the space shuttle Challenger.
                <b aria-label="Image credit">
                    
                    Howard Berkes/NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Howard Berkes/NPR
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.webp?s=2600&amp;c=100&amp;f=webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s=2600&amp;c=100&amp;f=jpeg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s=2600&amp;c=100&amp;f=jpeg" alt="" src="https://media.npr.org/assets/img/2021/03/07/mcdonald-with-commemorative-2aa6ac2dbf80eea3029d8d1d7e7d7c60230627d0.jpg?s=2600&amp;c=100&amp;f=jpeg">
        </picture>
    </div>
<div>
        <p>Allan McDonald in 2016 holds a commemorative poster honoring the seven astronauts killed aboard the space shuttle Challenger.</p>
        <p><span aria-label="Image credit">
            
            Howard Berkes/NPR
            
        </span>
    </p></div>
   </div>
   <p>On Jan. 27, 1986, Allan McDonald stood on the cusp of history.</p>   <p>McDonald directed the booster rocket project at NASA contractor Morton Thiokol. He was responsible for the two massive rockets, filled with explosive fuel, that lifted space shuttles skyward. He was at the Kennedy Space Center in Florida for the launch of the Challenger "to approve or disapprove a launch if something came up," he told me in 2016, 30 years after Challenger exploded.</p>   <p>His job was to sign and submit an official form. Sign the form, he believed, and he'd risk the lives of the seven astronauts<strong> </strong>set to board the spacecraft the next morning. Refuse to sign, and he'd risk his job, his career and the good life he'd built for his wife and four children.</p>   
   <p>"And I made the smartest decision I ever made in my lifetime," McDonald told me. "I refused to sign it. I just thought we were taking risks we shouldn't be taking."</p>   
   
<!-- END ID="RES974537606" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>McDonald persistently cited three reasons for a delay: freezing overnight temperatures that could compromise the booster rocket joints; ice forming on the launchpad and spacecraft that could damage the orbiter heat tiles at launch; and a forecast of rough seas at the booster rocket recovery site.</p>   <p>He also told NASA officials, "If anything happens to this launch, I wouldn't want to be the person that has to stand in front of a board of inquiry to explain why we launched."</p>   <p>Now, 35 years after the Challenger disaster, McDonald's family reports that he died Saturday in Ogden, Utah, after suffering a fall and brain damage. He was 83 years old.</p>   <p>"There are two ways in which [McDonald's] actions were heroic," recalls Mark Maier, who directs a leadership program at Chapman University and produced a documentary about the Challenger launch decision.</p>   <p>"One was on the night before the launch, refusing to sign off on the launch authorization and continuing to argue against it," Maier says. "And then afterwards in the aftermath, exposing the cover-up that NASA was engaged in."</p>   
   <p>Twelve days after Challenger exploded, McDonald stood up in a closed hearing of a presidential commission investigating the tragedy. He was "in the cheap seats in the back" when he raised his hand and spoke. He had just heard a NASA official completely gloss over a fundamental fact.</p>   <p>McDonald and his team of <a href="https://www.npr.org/2006/01/28/5175151/challenger-reporting-a-disasters-cold-hard-facts">Thiokol engineers had strenuously opposed the launch</a>, arguing that freezing overnight temperatures, as low as 18 degrees F, meant that the O-rings at the booster rocket joints would likely stiffen and fail to contain the explosive fuel burning inside the rockets. They presented data showing that O-rings had lost elasticity at a much warmer temperature, 53 degrees F, during an earlier launch.</p>   
   
<!-- END ID="RES974537681" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>The NASA official simply said that Thiokol had some concerns but approved the launch. He neglected to say that the approval came only after Thiokol executives, under intense pressure from NASA officials, overruled the engineers.</p>   <p>"I was sitting there thinking that's about as deceiving as anything I ever heard," McDonald recalled. "So ... I said I think this presidential commission should know that Morton Thiokol was so concerned, we recommended not launching below 53 degrees Fahrenheit. And we put that in writing and sent that to NASA."</p>   <p>Former Secretary of State William Rogers chaired the commission and stared into the auditorium, squinting in the direction of the voice.</p>   <p>"I'll never forget Chairman Rogers said, 'Would you please come down here on the floor and repeat what I think I heard?' " McDonald said.</p>   <p>The focus of the commission's investigation shifted to the booster rocket O-rings, the efforts of McDonald and his colleagues to stop the launch and the failure of NASA officials to listen.</p>   
   
<!-- END ID="RES974537739" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>Morton Thiokol executives were not happy that McDonald spoke up, and they demoted him.</p>   <p>That alarmed members of the presidential commission and members of Congress. Rep. Edward Markey, a Massachusetts Democrat, introduced a <a href="https://www.congress.gov/bill/99th-congress/house-joint-resolution/634/text?r=85&amp;s=1">joint resolution in the House</a> that threatened to forbid Thiokol from getting future NASA contracts given the company's punishment of McDonald and any other Thiokol engineers who spoke freely.</p>   <p>The company relented, and McDonald was promoted to vice president and put in charge of the effort to redesign the booster rocket joints that failed during the Challenger launch.</p>   
   <p>In 1988, the redesigned joints worked successfully as shuttle flights resumed.</p>   <p>McDonald continued to work at Thiokol until 2001 and retired after 42 years. He later co-authored one of the most definitive accounts of the Challenger disaster — <a href="https://upf.com/book.asp?id=MCDON001"><em>Truth, Lies, and O-Rings</em></a><em>: Inside the Space Shuttle </em>Challenger <em>Disaster</em>.</p>   <div id="res974548572">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s=1100&amp;c=15&amp;f=jpeg" data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s=1100&amp;c=100&amp;f=jpeg" data-template="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s={width}&amp;c={quality}&amp;f={format}" data-format="webp" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s=1100&amp;c=15&amp;f=jpeg" data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s=1100&amp;c=100&amp;f=jpeg" data-template="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s={width}&amp;c={quality}&amp;f={format}" data-format="jpeg" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s={width}&amp;c={quality}&amp;f={format}" data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s=1100&amp;c=100&amp;f=jpeg" data-template="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s={width}&amp;c={quality}&amp;f={format}" data-format="jpeg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                In 2018, Allan McDonald reviewed the Challenger launch decision during a series of seminars about leadership and ethical decision-making to managers at U.S. Space Command.
                <b aria-label="Image credit">
                    
                    Mark Maier
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Mark Maier
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.webp?s=2600&amp;c=100&amp;f=webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s=2600&amp;c=100&amp;f=jpeg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s=2600&amp;c=100&amp;f=jpeg" alt="" src="https://media.npr.org/assets/img/2021/03/07/mcdonald-at-us-space-command-281b10bcc5e423414b3d65972b8727e0998e0dd9.jpg?s=2600&amp;c=100&amp;f=jpeg">
        </picture>
    </div>
<div>
        <p>In 2018, Allan McDonald reviewed the Challenger launch decision during a series of seminars about leadership and ethical decision-making to managers at U.S. Space Command.</p>
        <p><span aria-label="Image credit">
            
            Mark Maier
            
        </span>
    </p></div>
   </div>
   <p>In retirement, McDonald became a fierce advocate of ethical decision-making and spoke to hundreds of engineering students, engineers and managers. He and Chapman University's Maier held leadership and ethics seminars for corporations and government agencies, including U.S. Space Command.</p>   <p>Maier says that one of McDonald's key moments in his talks helps explain his ability to reconcile his brush with history.</p>   <p>"What we should remember about Al McDonald [is] he would often stress his laws of the seven R's," Maier says. "It was always, always do the right thing for the right reason at the right time with the right people. [And] you will have no regrets for the rest of your life."</p>   <p>"It's really that simple if you just keep it focused that way," McDonald told me in 2016.</p>   
   
<!-- END ID="RES974537778" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>He also framed regret another way, paraphrasing a favorite quote from the late journalist Sydney J. Harris.</p>   <p>"Regret for things we did is tempered by time," McDonald said, his expression firm. "But regret for things we did not do is inconsolable." McDonald then paused and added, "That's absolutely true."</p>   <p>He seemed inconsolable immediately after the Challenger explosion in tearful calls home. He recalls the painful conversations in his book. "I feel like it's my fault," he told his daughter Lisa, a nursing student in Boston at the time. "Don't blame yourself, Dad," she said, also crying.</p>   <p>Maier believes McDonald lived out his life with neither blame nor regret. "He died with serenity and equanimity," he says. "I will miss him dearly."</p>   <p>Allan McDonald leaves behind his wife, Linda, and four children — and a legacy of doing the right things at the right times with the right people.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ladybird browser spreads its wings (396 pts)]]></title>
            <link>https://lwn.net/Articles/976822/</link>
            <guid>40746804</guid>
            <pubDate>Fri, 21 Jun 2024 06:35:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/976822/">https://lwn.net/Articles/976822/</a>, See on <a href="https://news.ycombinator.com/item?id=40746804">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<center>
           <div><b>Did you know...?</b><p>LWN.net is a subscriber-supported publication; we rely on subscribers
       to keep the entire operation going.  Please help out by <a href="https://lwn.net/subscribe/">buying a subscription</a> and keeping LWN on the
       net.</p></div>
           </center>
           
<p><a href="https://ladybird.dev/">Ladybird</a> is an open-source
project aimed at building an independent web browser, rather than
yet another browser based on Chrome. It is written in C++ and licensed under a
two-clause BSD license. The effort 
began as part of the <a href="https://www.serenityos.org/">SerenityOS</a> project, but
developer Andreas Kling <a href="https://awesomekling.substack.com/p/forking-ladybird-and-stepping-down-serenityos">announced</a>
on June&nbsp;3 that he was "forking" Ladybird as a separate project and stepping away from
SerenityOS to focus his attention on the browser completely. Ladybird
is not ready to replace Firefox or Chrome for regular use, but it is showing
great promise.</p>

<p>Kling started working on SerenityOS in 2018 as a therapy project
after completing a substance-abuse rehabilitation program. The SerenityOS name is a
nod to the <a href="https://en.wikipedia.org/wiki/Serenity_Prayer">serenity
prayer</a>. Prior to working on the project, he had worked on
WebKit-based browsers at Apple and Nokia. Eventually he <a href="https://awesomekling.github.io/I-quit-my-job-to-focus-on-SerenityOS-full-time/">made
SerenityOS his full-time job</a>, and funded the work through
donations, sales of SerenityOS merchandise, and income from
YouTube. (Kling posts monthly updates to his <a href="https://www.youtube.com/@awesomekling/videos">YouTube
channel</a> about Ladybird, as well as hacking videos where he walks
through working on various components in the browser, such as the <a href="https://www.youtube.com/watch?v=8mxubNQC5O8">JavaScript JIT
compiler</a>.)</p>

<h4>Taking flight</h4>

<p>Kling <a href="https://awesomekling.substack.com/p/ladybird-a-new-cross-platform-browser-project">announced</a>
the Ladybird project in September 2022. He said that the project
started while creating a Qt GUI for SerenityOS's LibWeb browser
engine. He decided to target Linux as well as SerenityOS so it would be easier for
people to work on and debug while in Linux. In the post announcing his
intent to work solely on Ladybird, he noted that he had been focusing
all of his attention on the Linux version of Ladybird. With that
realization, he decided to step down as "benevolent dictator for
life" (BDFL) of SerenityOS so its development would not be held back:</p>

<blockquote>
Before anyone asks, there is no drama behind this change. It's simply
recognizing that there have been two big projects packed uncomfortably
into a single space for too long, and I'm doing what I believe will
make life better for everyone involved.
</blockquote>

<p>Ladybird's governance is similar to SerenityOS. Kling is the
BDFL, with a group of maintainers (currently ten) who can
approve and merge pull requests. The <a href="https://github.com/LadybirdWebBrowser/ladybird/blob/master/CONTRIBUTING.md">contributing</a>
guide notes that maintainership is "<q>by invitation only and does not
correlate with any particular metric</q>". Project development
discussions are held on a <a href="https://discord.com/invite/nvfjVJ4Svh">Discord server</a>
(account required).</p>

<p>Now independent, Ladybird has dropped SerenityOS as a development
target, and has moved to <a href="https://github.com/LadybirdWebBrowser/ladybird">its own GitHub
repository</a>. In addition, Kling has relaxed his self-imposed policy
of excluding "not invented here" (NIH) code that had applied to SerenityOS, which
means that the Ladybird project will be able to make use of existing
libraries rather than writing from scratch.</p>

<p>Comparing the <a href="https://github.com/LadybirdWebBrowser/ladybird/blob/master/README.md">README</a> file in the standalone Ladybird repository
against the <a href="https://github.com/SerenityOS/serenity/blob/master/Ladybird/README.md">README</a>
file in the SerenityOS repository, the goal has
evolved from creating "<q>a standards-compliant, independent web browser with
no third-party dependencies</q>" to developing an independent browser "<q>using a
novel engine based on web standards</q>".</p>

<p>The changes to the section that enumerates the core libraries for Ladybird 
provide some hints about Kling's plans
to use existing libraries rather than continuing to reinvent the
wheel. The core support libraries for the project include homegrown
libraries for cryptography, TLS, 2D-graphics rendering, archive-file
format support, Unicode, as well as audio and video playback.
In the pre-fork documentation, they are described
as alternatives to other software. For example, Ladybird's TLS (LibTLS)
and cryptography (LibCrypto) libraries are "<q>Cryptography primitives
and Transport Layer Security (rather than OpenSSL)</q>". The "rather
than" language has been removed in the journey to the standalone
repository, and the LibSQL library from SerenityOS has <a href="https://github.com/LadybirdWebBrowser/ladybird/commit/8362c073f34f25ef2fbe800780f4b8790b3fffa8">already
been stripped out</a> in favor of sqlite3.</p>

<p>In a discussion in the project's Discord instance on June&nbsp;5, Kling indicated that font rendering
would likely be replaced with a third-party library. A user asked on
June&nbsp;6 what would determine whether a component would be developed in-house versus
using a third-party library. Kling responded that if it implements a web
standard, "i.e DOM, HTML, JavaScript, CSS, Wasm, etc. then we build it
in house." Otherwise, the project would look to alternatives "unless we
believe we can build something better ourselves".</p>

<h4>Status</h4>

<p>Ladybird is still in early development ("pre-alpha") today. It
currently runs on Linux, macOS, and other
UNIX-like operating systems. It's also possible to use on Windows with
Windows Subsystem for Linux (WSL) version&nbsp;2, but there appears to
be no effort to target Windows independently at this time. At the
moment, the project does not provide binaries for any
platform. Interested users will need to grab the source and follow the
<a href="https://github.com/LadybirdWebBrowser/ladybird/blob/master/Documentation/BuildInstructionsLadybird.md">build
instructions</a>. Users will need GCC&nbsp;13+ or Clang&nbsp;17, and Qt6
development packages to play along at home. Ladybird compiles and runs
on, for example, Fedora&nbsp;40 without a problem, but it is a long
way from being suitable for regular use.</p>

<blockquote>
<a href="https://lwn.net/Articles/977492/"><img src="https://static.lwn.net/images/2024/ladybird-sm.png" alt="[Ladybird browser with inspector]" title="Ladybird browser with inspector"></a>
</blockquote>

<p>One might expect that the browser
would be more usable with sites with simpler layouts and little to no JavaScript
(e.g. LWN) than those with complex layouts and a fair amount of
JavaScript (e.g. GitHub). However, this isn't always the case—Ladybird
rendered GitHub and many other sites well, if slowly. Browsing
LWN anonymously worked well, but logging into LWN,
however, consistently proved to be too much for the
application. Each time, it basically froze on the front page and clicking links to
articles did nothing.</p>

<p>Somewhat ironically, it was not possible to log
into Discord using Ladybird. It does a fair
job of rendering pages, but speed and stability are
still wanting. Each Ladybird tab has its own render process, which is
sandboxed as a security measure to prevent any malicious pages from
affecting the rest of the system. However, it doesn't seem to suffice
to keep a single page from crashing the browser entirely. That's to be
expected from a project that's still considered pre-alpha, though.</p>

<p>The current feature set is, not surprisingly, minimal. Ladybird has
a URL/search bar, reload, tabs, can zoom in/out on content, take
screenshots, and (of course) has backward and forward navigation. It
does not, however, have bookmarks, a history display, extensions,
password management, printing, or even the ability to save an
image. WebRTC does not seem to be supported yet. CSS support seems
relatively robust. Ladybird passes 100% of the CSS Selectors tests for
levels 1-3, for example, using <a href="https://css4-selectors.com/browser-selector-test/">this
test</a>. It gets a 53% score for level 4, while Firefox gets 71%, so
not a terrible showing at all. JavaScript support seems solid, but
slow: the <a href="https://www.w3schools.com/js/js_ex_browser.asp">examples
here</a> work, but they load slowly.</p>

<p>On the other hand, Ladybird does have tools for developers, such as
inspectors for the document object model (DOM) tree and accessibility
trees, as well as the ability to create dumps of various things:
the DOM tree and layout tree, computed styles, and so forth. It also
has the ability to spoof the User-Agent sent by the browser so that testers
can try to get around sites that refuse to work with "unknown"
browsers. However, toggling the User-Agent wasn't enough to get
past Google's gatekeeping to sign into Gmail—but it's unclear if
that meant Ladybird wasn't sending the string correctly or if Google
is using other means to fingerprint non-approved browsers.</p>

<p>Suffice it to say, Ladybird is not ready for mainstream use but it
<em>does</em> show potential. In the past month, the project has had
more than 880 commits from 49 authors. If the project maintains that
kind of momentum, or picks up steam, it could become a usable
alternative to mainstream browsers before too long.</p><br clear="all"><hr><p>
           (<a href="https://lwn.net/Login/?target=/Articles/976822/">Log in</a> to post comments)
           </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MeshAnything – converts 3D representations into efficient 3D meshes (274 pts)]]></title>
            <link>https://buaacyw.github.io/mesh-anything/</link>
            <guid>40746310</guid>
            <pubDate>Fri, 21 Jun 2024 05:02:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://buaacyw.github.io/mesh-anything/">https://buaacyw.github.io/mesh-anything/</a>, See on <a href="https://news.ycombinator.com/item?id=40746310">Hacker News</a></p>
<div id="readability-page-1" class="page">

<div>
          
            <div>
              <p>
              <span>
                <a href="https://tonghe90.github.io/">Tong He</a><sup>2</sup><span>†</span>,</span>
              <span>
                <a href="https://dihuang.me/">Di Huang</a><sup>2</sup>,</span>
              <span>
                <a href="https://ywcmaike.github.io/">Weicai Ye</a><sup>2</sup>,</span>
              <span>
                <a href="https://ch3cook-fdu.github.io/">Sijin Chen</a><sup>3</sup>,</span>
              <span>
                <a href="https://me.kiui.moe/">Jiaxiang Tang</a><sup>4</sup></span>,
              <br>
              <span>
                <a href="https://chenxin.tech/">Xin Chen</a><sup>5</sup>,</span>
              <span>
                <a href="https://caizhongang.github.io/">Zhongang Cai</a><sup>6</sup>,</span>
              <span>
                <a href="https://scholar.google.com.hk/citations?user=jZH2IPYAAAAJ&amp;hl=en">Lei Yang</a><sup>6</sup>,</span>
              <span>
                <a href="https://www.skicyyu.org/">Gang Yu</a><sup>7</sup>,</span>
              <span>
                <a href="https://guosheng.github.io/">Guosheng Lin</a><sup>1</sup><span>†</span>,</span>
              <span>
                <a href="https://icoz69.github.io/">Chi Zhang</a><sup>8</sup><span>†</span></span>

            </p></div>

            <div>
              <p><span><sup>1</sup>S-Lab, Nanyang Technological University,</span>
              <span><sup>2</sup>Shanghai AI Lab</span><br>
              <span><sup>3</sup>Fudan University,</span>
              <span><sup>4</sup>Peking University,</span>
              <span><sup>5</sup>University of Chinese Academy of Sciences</span><br>
              <span><sup>6</sup>SenseTime Research,</span>
              <span><sup>7</sup>Stepfun,</span>
              <span><sup>8</sup>Westlake University</span></p><p><span><sup>*</sup>Work done during a research internship at Shanghai AI Lab.</span>
              </p>
              <p><span><sup>†</sup>Corresponding Authors.</span>
              </p>
            </div>


          
        </div>


<div>
        <p><img src="https://buaacyw.github.io/mesh-anything/demo_video.gif" alt="Demo Video">
        </p>
      </div>

<div>
      <h2>
        <p><span>MeshAnything</span> mimics human artist in extracting meshes from any 3D representations. It can be combined with various 3D asset production pipelines, such as 3D reconstruction and generation, to convert their results into Artist-Created Meshes that can be seamlessly applied in 3D industry.
        </p>
      </h2>
    </div>



<div>
    <div>
      <h4>Comparison with Previous Methods</h4>
      
    </div>
    <div>
          <p><img src="https://buaacyw.github.io/mesh-anything/remesh.png" alt="Remesh Image"></p><p>
            <span>MeshAnything</span> generates meshes with hundreds of times fewer faces, significantly improving storage, rendering, and simulation efficiencies, while achieving precision comparable to previous methods.
          </p>
        </div>
  </div>

<div>
        <h2>Abstract</h2>
        <div>
          <p>
            Recently, 3D assets created via reconstruction and generation have matched the quality of manually crafted assets, highlighting their potential for replacement. However, this potential is largely unrealized because these assets always need to be converted to meshes for 3D industry applications, and the meshes produced by current mesh extraction methods are significantly inferior to Artist-Created Meshes (AMs), i.e., meshes created by human artists.
Specifically, current mesh extraction methods rely on dense faces and ignore geometric features, leading to inefficiencies, complicated post-processing, and lower representation quality.
          </p>
          <p>
            To address these issues, we introduce MeshAnything, a model that treats mesh extraction as a generation problem, producing AMs aligned with specified shapes. By converting 3D assets in any 3D representation into AMs, MeshAnything can be integrated with various 3D asset production methods, thereby enhancing their application across the 3D industry.
          </p>
          <p>
            The architecture of MeshAnything comprises a VQ-VAE and a shape-conditioned decoder-only transformer. We first learn a mesh vocabulary using the VQ-VAE, then train the shape-conditioned decoder-only transformer on this vocabulary for shape-conditioned autoregressive mesh generation. Our extensive experiments show that our method generates AMs with hundreds of times fewer faces, significantly improving storage, rendering, and simulation efficiencies, while achieving precision comparable to previous methods.
          </p>
        </div>
      </div>

<div>
    <div>
      <p>
        <h2>Method</h2>
      </p>
    </div>

    <div>
        <p><img src="https://buaacyw.github.io/mesh-anything/pipline.png" alt="Pipeline Image">
        </p>
      </div>

    <div>
        <p>
           <span>MeshAnything</span> is an autoregressive transformer capable of generating Artist-Created Meshes that adhere to given 3D shapes. We sample point clouds from given 3D assets, encode them into features, and inject them into the decoder-only transformer to achieve shape-conditional mesh generation.
        </p>
        <p>
          Compared to methods like MeshGPT that directly generate Artist-Created Meshes, our approach avoids learning complex 3D shape distributions. Instead, it focuses on efficiently constructing shapes through optimized topology, significantly reducing the training burden and enhancing scalability.
        </p>
      </div>
  </div>


<div>
    <div>
      <p>
        <h2>More Results</h2>
      </p>
    </div>

    <div>
        <p><img src="https://buaacyw.github.io/mesh-anything/teaser.png" alt="Teaser Image">
        </p>
        <p><img src="https://buaacyw.github.io/mesh-anything/add.png" alt="Additional Image">
        </p>
        <p><img src="https://buaacyw.github.io/mesh-anything/gt.png" alt="Ground Truth Image">
        </p>

        <p>
          By integrating with various 3D asset production methods, our approach achieves highly controllable Artist-Created Mesh generation. Besides, we compare our reseults with ground truth in (b) and (c). In (b), MeshAnything generates meshes with better topology and fewer faces than the ground truth. In (c), we produce meshes with a completely different topology while achieving a similar shape, proving that our method does not simply overfit but understands how to construct meshes using efficient topology.
        </p>
      </div>
  </div>



<div id="BibTeX">
    <h2>BibTeX</h2>
    <pre><code>@misc{chen2024meshanything,
      title={MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers},
      author={Yiwen Chen and Tong He and Di Huang and Weicai Ye and Sijin Chen and Jiaxiang Tang and Xin Chen and Zhongang Cai and Lei Yang and Gang Yu and Guosheng Lin and Chi Zhang},
      year={2024},
      eprint={2406.10163},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}</code></pre>
  </div>






</div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Rant about Front-end Development (112 pts)]]></title>
            <link>https://blog.frankmtaylor.com/2024/06/20/a-rant-about-front-end-development/</link>
            <guid>40746173</guid>
            <pubDate>Fri, 21 Jun 2024 04:31:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.frankmtaylor.com/2024/06/20/a-rant-about-front-end-development/">https://blog.frankmtaylor.com/2024/06/20/a-rant-about-front-end-development/</a>, See on <a href="https://news.ycombinator.com/item?id=40746173">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-3896" itemscope="itemscope" itemtype="http://schema.org/BlogPosting" itemprop="blogPost">
		
		<!-- .entry-header -->

		<div itemprop="articleBody">
			<p><span><span>Reading Time: </span> <span>13</span> <span>minutes</span></span></p><p>I am a front-end developer who is FED up about front-end development. If you write front-end, this isn’t about you <em>personally</em>. It’s about how your choices make me angry. Also this is about how my choices have made me angry. Also this is mostly just about <em>choices</em>, the technologies are incidental. </p>



<p><strong>Note: The views expressed in here are not those of my employer. They are (probably) the views of thousands of people just like me.</strong></p>



<p><strong>Warning: </strong>This article is filled with profanity. Ask your scrummaster if 8 points of profanity is too much to handle today. </p>



<div>
<figure><a href="https://blog.frankmtaylor.com/wp-content/uploads/2024/06/DALL%C2%B7E-2024-06-20-22.40.13-painting-of-a-raven-trying-to-remove-the-keys-from-a-laptop-keyboard.png"><img fetchpriority="high" decoding="async" width="1024" height="1024" src="https://blog.frankmtaylor.com/wp-content/uploads/2024/06/DALL%C2%B7E-2024-06-20-22.40.13-painting-of-a-raven-trying-to-remove-the-keys-from-a-laptop-keyboard.png" alt="Ravens are smart animals, probably much smarter than we really know. If they knew about the stupid shit we were doing to build the internet they'd definitely peck out the keys of every keyboard in existence." srcset="https://blog.frankmtaylor.com/wp-content/uploads/2024/06/DALL%C2%B7E-2024-06-20-22.40.13-painting-of-a-raven-trying-to-remove-the-keys-from-a-laptop-keyboard.png 1024w, https://blog.frankmtaylor.com/wp-content/uploads/2024/06/DALL%C2%B7E-2024-06-20-22.40.13-painting-of-a-raven-trying-to-remove-the-keys-from-a-laptop-keyboard-300x300.png 300w, https://blog.frankmtaylor.com/wp-content/uploads/2024/06/DALL%C2%B7E-2024-06-20-22.40.13-painting-of-a-raven-trying-to-remove-the-keys-from-a-laptop-keyboard-150x150.png 150w, https://blog.frankmtaylor.com/wp-content/uploads/2024/06/DALL%C2%B7E-2024-06-20-22.40.13-painting-of-a-raven-trying-to-remove-the-keys-from-a-laptop-keyboard-768x768.png 768w, https://blog.frankmtaylor.com/wp-content/uploads/2024/06/DALL%C2%B7E-2024-06-20-22.40.13-painting-of-a-raven-trying-to-remove-the-keys-from-a-laptop-keyboard-240x240.png 240w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>Quoth the raven on your merge requests: Nevermore</figcaption></figure></div>


<h2>First, some background</h2>



<p>I have neither a computer science degree nor an engineering degree. But I have worked in web technologies since the second Bush administration. I’ve worked in a variety of roles ranging from business analyst, tester, all the way through developer, consultant, and principal architect.  </p>



<p>I did agency work for 11 years before making the choice to work for a very large tech company. I have worked across sectors like insurance, healthcare, retail, banking, investing, marketing, and manufacturing. I have worked with global brands which are household names. </p>



<p>I have written <em>a lot</em> of front-end code for <em>a lot</em> of companies. I have also dealt with <em>a lot of consequences created by front-end code</em>. My criticisms come from my role as a front-end developer and as someone affected by a front-end developer. </p>



<h2>No one gives a shit about content</h2>



<p>Unless it’s a content-specialized front-end developer who works specifically in the content management space and is specialized in writing front-end for content management system. Did you count how many times I had to write both “front-end” and “content”? Too fucking many. </p>



<p>I have mentored quite a few bootcamp graduates. There are exactly zero graduates of zero bootcamps who have ever been told, “users go to websites for the content.” <strong>Zero</strong>. They all think it’s their moral imperative to make sure every site is built in React.</p>



<p>I have worked with exactly zero computer science graduates who have ever heard the phrase, “content before code”.</p>



<p>Unless the front-end developer has been forced by threat of not-being-able-to-pay-rent to care about content, they don’t fucking care. </p>



<p>This is <strong>wild</strong> to me because HTML5 semantics exist and their whole-ass raison d’être is, in fact, having an understanding of content. </p>



<ul>
<li>You can’t write content-managed HTML if you don’t understand the content.</li>



<li>You can’t write semantic HTML if you don’t understand the content. </li>



<li>You can’t write SEO-ready HTML without understanding the content.</li>



<li>You can’t write accessible HTML without understanding the content.</li>



<li>You can’t write <em>good</em> HTML without understanding the content. </li>
</ul>



<p>Zero SPA developers that I’ve ever met have ever given a shit about using a<code> &lt;header&gt;</code> instead of a <code>&lt;section&gt;</code>. I have found <code>NaN</code> fucks given about using a <code>p</code> over a <code>div</code>. The number of times I’ve seen numbers written inside of a <code>&lt;li&gt;</code> that’s inside of a <code>&lt;ul&gt;</code> — instead of just using an <code>&lt;ol&gt;</code> — <strong>is deeply disturbing.</strong></p>



<p><em>Maybe we should ask the designer or business owner a few questions about what the design means to the end-user before hammering out 18 layers of <code>div</code>s. </em></p>



<h2>CSS is fine; you’re the problem</h2>



<p>If someone gets a ticket for speeding, we don’t blame the car or the road. We definitely blame the idiot who thought it’d be a good idea to race the Dodge Charger camped out in the Dollar General parking lot. </p>



<p>It’s in this very same vein that we don’t blame CSS:</p>



<blockquote>
<p>“The problem with CSS is that the scope is global”<br>“The problem with CSS is the cascade”<br>“The problem with CSS is that it fails silently”</p>
<cite>Every dork who promotes writing styles in JavaScript</cite></blockquote>



<p>Those aren’t problems; they’re <em>features</em>. They are solutions to the very specific problems of, “how do we make content presentable as easily as possible, with as little duplication as possible, and with as few negative impacts to the user as possible.”</p>



<p>Chances are, the things you don’t like about CSS are the things you haven’t bothered to understand about it. </p>



<p>Styles rarely, <em>if ever</em> need to actually be scoped and encapsulated. In fact, the only time you want to prevent styles from being in the global scope is when you didn’t write them <em>and</em> can’t control them. </p>



<p>If duplicated class names are ruining your day, 15-class selectors are crapping in your cereal, and 8 <code>!important</code> are igniting turds in your garage, that’s actually an <em>architecture problem</em>. </p>



<p>Are there <em>real</em> problems in CSS? Of course. But those are problems like, “<code>border-radius</code> should have been called <code>corner-radius</code>,” and “atomic stacking contexts are generated when using <code>opacity</code>,” and “<a href="https://blog.frankmtaylor.com/2012/01/25/css-font-size-an-em-isnt-an-m-but-an-ex-is-an-x/" data-type="link" data-id="https://blog.frankmtaylor.com/2012/01/25/css-font-size-an-em-isnt-an-m-but-an-ex-is-an-x/">an <code>em</code> isn’t really an <code>em</code>.</a>”   Those are problems. </p>



<p>The C of CSS is not a problem. You’re the problem. </p>



<p><em>Put on your big-kid pants and develop a strategy, pick some conventions, write them down, and enforce them through merge requests and peer reviews. </em> </p>



<h2>Server-side rendering does not mean, “rendered with JavaScript”</h2>



<p>There’s a disconcerting number of front-end developers out there who act like it wasn’t possible to generate HTML on a server prior to 2010<a href="#one"><sup>1</sup></a>.  They talk about SSR <em>only</em> in the context of node.js and seem to have no clue that people started working on this problem when season 5 of Seinfeld was on air<a href="#two"><sup>2</sup></a>. </p>



<p>Server-side rendering was not invented with Node. What Node brought to the table was the convenience of writing your shitty <code>div</code> soup in the very same language that was invented in 10 days for the sole purpose of pissing off Java devs everywhere. </p>



<p>Server-side rendering means it’s rendered on the fucking server. You can do that with PHP, ASP, JSP, Ruby, Python, Perl, CGI, and hell, <em>R</em>. You can server-side render a page in <em>Lua</em> if you want. </p>



<p>Just because <em>Node</em> isn’t involved in generating HTML, that doesn’t mean it’s not possible to generate markup at request time.  It is possible. It’s been possible. It will continue to be possible. </p>



<p><em>Stop denying the history, contributions, and value that other technologies bring to solving this exact problem that is definitely not unique to you.</em></p>







<h2>JavaScript is not always the answer</h2>



<p>My brothers and sisters in Christ I want you to know that I care about your souls enough to share these truths with you:</p>



<ul>
<li>You don’t need JavaScript to make a web page. </li>



<li>You don’t need JavaScript to write styles. </li>



<li>You don’t need JavaScript to make an animation. </li>



<li>You don’t need JavaScript just to show content. </li>
</ul>



<p>In 2010 I was a contractor for an organization working with some Very Smart IBM consultants. They could not figure out how our in-house developer made the navigation for the site they were implementing. </p>



<p>It was because our developer used CSS. Our consultants had never considered that CSS could do something as fancy as, “show you a thing when you hover over it.”</p>



<p>That’s a fun “har-har silly ignorant smart consultants,” story except at least it’s about their <em>harmless</em> <em>ignorance</em>.</p>



<p>It’s so much <strong>worse</strong> when they <em>know JavaScript</em>. </p>



<p>Five years ago I was on a project where the perfectly fine .net MVC app was turned into an API for delivering content to a .net core app that used node services to render Vue templates into fully rendered HTML which meant that all of our templates were written in Vue <em>and we still had to write jQuery to make content interactive</em>.  We literally had two layers of JavaScript wrapped in a .net sandwich with JavaScript spread on the top. </p>



<p>I’m seeing architectures being proposed <em>today</em> that do pretty much the same thing I just described but with Drupal on Apache, containerized Node, and Web Components. </p>



<p>If  making a peanut butter and jelly sandwich by spreading the jelly on both sides of the bread is disturbing to you, <em>good</em>. You can still find God. </p>



<p>Do you have any idea how frustrating it is that that in order to explain my sadness to my therapist I must first explain like 5 different technologies and by the time I’m finished <em>she’s sad just hearing it</em>, the session’s over, and I didn’t even get to what was making me upset? Technology has made my anger a recursive function. </p>



<p><em>Can we please stop adding complexity to our systems just so we can do it in JavaScript? If you can do it without JavaScript, you probably should. Tools shouldn’t add complexity. </em></p>



<h2>Sass wasn’t just a mistake but a mistake factory</h2>



<p>10 years ago I thought CSS preprocessors were the coolest thing ever and I was incredibly fucking dumb for thinking that. </p>



<p>The <em>only three good features</em> CSS preprocessors gave were</p>



<ol>
<li>Variables</li>



<li>Mixins</li>



<li>Color transformation</li>
</ol>



<p>Every thing else was a mistake:</p>



<ul>
<li>Nesting</li>



<li>Calc</li>



<li>Functions, including control statements like if and for</li>



<li>Interpolation</li>



<li>Data types of any kind</li>



<li>The entire module thing</li>
</ul>



<h3>Nested CSS obscures complexity</h3>



<p>I hate this feature so much. I hate it more that it’s now native to CSS because not only has it spread, but <em>it’s mutated</em><sup>. <a href="#three">3</a></sup> </p>



<p>Nesting obscures complexity and then makes it impossible to find or fix. </p>



<p>Without using a compiler or psychedelic of any kind, can you tell me what this generates?</p>



<pre data-enlighter-language="scss" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">.feature {
	color: blue;
	&amp;__header {
		p &amp; {
			a {
				&amp;:hover,
				&amp;:focus {
					color: red;
				}
			}
		}
	}
}</pre>



<p>Does this seem reasonable to you?</p>



<pre data-enlighter-language="css" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">.feature {
  color: blue;
}
p .feature__header a:hover, p .feature__header a:focus {
  color: red;
}</pre>



<p>If that’s the output you expected, either you’re one of the Sass spec writers or you did a double-shot of espresso followed by a Ritalin chaser. </p>



<p>Nesting is not good. It hides the specificity that you’re producing and will lend to increasingly fragile code as you try to override a 13 part selector. </p>



<h3>Interpolation, et all</h3>



<p>This was also just a bad idea that we couldn’t have predicted at the time. I’m not mad at the Sass people for this. Hindsight is 20/20 and all. What I want is for it to all stop. </p>



<p>This is fundamentally a bad idea:</p>



<pre data-enlighter-language="scss" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">$sizes: 10px, 20px, 30px, 50px;

@each $size in $sizes {
  .icon-#{$size} {
    font-size: $size;
    height: $size;
    width: $size;
  }
}
</pre>



<p>I know what you’re thinking. </p>



<p>You’re thinking, “But Frank, it saves me on keystrokes! Now I can generate more classes and classnames just based on that list of sizes!”</p>



<p>Bro. </p>



<p><kbd>ctrl</kbd>+<kbd>c</kbd> and <kbd>ctrl</kbd>+<kbd>v</kbd> exist. </p>



<p>Are you a VS Code user? Even easier: Highlight it and press <kbd>alt/opt</kbd> + <kbd>shift</kbd> + <kbd>↓</kbd> , that’ll copy the chunk or line down.  Then, highlight the part that’s repeated a few times (where you’d interpolate), press <kbd>ctrl/cmd</kbd> + <kbd>d</kbd>, and keep pressing until they’re all highlighted. Make your change. Now you’re done.</p>



<p>What did you save? Time and sanity in looking up the Sass syntax? Future time where future you is trying to find where the fuck <code>.icon-80px </code>is living? <a href="https://xkcd.com/1205/">XKCD is not on your side</a> for this automagical icon generator loop that’s going to piss you off with edge cases for the next 5 years. </p>



<p><em>Can we please stop using languages and tech stacks to solve problems that our IDEs are capable of helping us with? </em></p>



<h2>Seriously … forget the Frameworks</h2>



<p>I was blown away when Angular came out. It was an actual <em>thing</em> that could do it all. It could render the markup with fun logicy attributes, respond to data changes, make the AJAX requests, amazing! I wanted to use Angular for <em>all</em> the <strong>interactive web apps</strong>. </p>



<p>Then React came out. That was amazing. It could render the markup and … [incoherent mumbling] components or something… and … <em>wait, that wasn’t even markup; it was JSX. </em> Sure, I had to add a few libraries to really use it, but yay, I used it for <strong>interactive web apps</strong>. </p>



<p>Then there was Vue. Then Preact. Then Ember, Svelte, Backbone, Alpine, Mithril, Solid, Lit, Stencil, Qwik…</p>



<p>Maybe it’s because Angular was no one’s first choice — even though it came first. Maybe that’s why the front-end world lost its damned mind with React. React was so <em>not-angulary</em> that front-end folks were like, “let’s use it client side, server side, native-side, and the 3 other sides of a damned cube”. Folks decided it was a good idea to Reactify All The Things®. </p>



<p>And that’s how React, a framework used to deliver interactive content to a whole 28% of the world’s population, is being used to generate static sites. </p>



<p>I had a client a few years ago that was a <em>household name</em>. Their front-end was written in Gatsby. Gatsby is a <em>static site generator</em> that uses React. This globally-known brand had probably 50 websites in at least as many languages. But the content was very, very, static. </p>



<p>So imagine the horror the front-end dorks felt when they learned that a content author in Bulgaria  updating the terms and conditions didn’t want the whole-ass site rebuilt for every country and language. </p>



<p>This would’ve been a non-problem had the web app been written in .net, Java, or, fuck me for saying it, PHP or Node. </p>



<p>But nah. These goobers got high off npm modules and did shots of JSX in the bathroom at lunch time. They chose a framework <em>used to deliver interactive content  to a significant percentage of the planet</em> for generating non-interactive content on a server. It took them <em>weeks</em> to come up with a solution to the problem they created when they chose a problem as a solution.  </p>



<p>Yes. I am absolutely calling React a problem. I’m calling Angular, Vue, and All the Rest of Them ™ a problem. Because unless you have a <em>specific problem of highly interactive, data-driven content</em>, you don’t need a framework.</p>



<p><em>You don’t need a framework to render static content to the end user.  Stop creating complex solutions to simple problems</em></p>



<h2>Also more generally fuck your complexity</h2>



<p>This, more than anything, is a rant about <em>complexity</em>. </p>



<p>I am mad at how front-end developers create complex solutions to simple problems. I am mad that front-end developers use tools to obscure complexity. I am mad that front-end developers embrace complexity rather than challenge it. </p>



<p>You know what’s great about jQuery? </p>



<p>Everything. </p>



<p>jQuery took several complex problems (browsers do things differently, selecting elements is complex, changing things on elements is tedious, AJAX requests are <em>a lot</em>) and it provided a simple, intuitive, and readable solution:</p>



<pre data-enlighter-language="js" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">$('.getResults').on('click', () =&gt; {
    $.ajax({
       url: '/api/results',
       data: {
         foo: 'bar'
       },
       success: function (result) {
          $('.results').html(result);
       }
    });
});</pre>



<p>Cool your jets for a second and think about how wonderfully <em>simple</em> that is. It handles finding your element in a way that lets you avoid the null checks, shortens your event binding, lets you make an AJAX  request, lets you find the element that the result goes into, and plop it there if it’s successful. </p>



<p>If the problem is, “user clicks on thing, you get data, and show data”, that is a <em>beautiful</em> and simple 11-line solution. It did not need NPM modules and a build system.</p>



<p>And <em>sure</em>, <a href="https://youmightnotneedjquery.com/">maybe you don’t need jQuery</a>, and that’s fine. Will you need more lines of code to do the same thing? Of course. But even without jQuery, your complexity doesn’t grow exponentially like it would if you brought in a framework with its build system, linters, compilers, and 700mb of Node Modules some of which inexplicably require some stupid shit like <a href="https://www.npmjs.com/~jonschlinkert" data-type="link" data-id="https://www.npmjs.com/~jonschlinkert">is-odd</a> or <a href="https://www.npmjs.com/package/is-git-url?activeTab=dependents" data-type="link" data-id="https://www.npmjs.com/package/is-git-url?activeTab=dependents">is-git-url</a> <sup><a href="#four">4</a></sup>.  </p>



<p>Pretend I’m not talking about JavaScript for a second. Let’s pretend it’s just an ordinary, everyday, 3-template web site. Maybe it’s managed by a CMS. Maybe it’s not. Does it really need Tailwind? Bootstrap? <em>Even &lt;gasp class=”from the back of the room”&gt; <strong>Sass</strong></em>? Is writing plain CSS really such a terrible thought?</p>



<p>Let me fill you in on a cool fact about CSS: It doesn’t need a fucking compiler. No <code>npm install -D sass</code>. No <code>npm i gulp</code>. You get to tell WebPack to fuck off good and proper-like, the way the Good Lord Intended. <sup><a href="#five">5</a></sup></p>



<p><em>Simplicity is unsophisticated and lacking in many parts. The simplest solution to a problem is always the best solution. Choose simplicity. I’m begging you. Your future is begging you,.   </em></p>



<h2>Oh, and enough with the new and shiny</h2>



<p>Sure it’s awesome that you can strongly type CSS variables, but that doesn’t mean you should <sup><a href="#six">6</a></sup>. Can you have web components with declarative shadow DOM? d’uh! But, hear me out: Why have three steps to render markup when you could have one?</p>



<p>Yes, the new JS Framework is all the awesomeness. Definitely do a side project with it. Please in the name of her Sweet Saint Fuck of Apathy, don’t use it in your next project. We haven’t learned what our regrets will be, and the regrets are what everyone else has to live with but you. </p>



<p>I don’t care if it’s a new CSS trick, a new JS thing, or a new JS framework: <strong>Give it two years before you take it for a ride in production</strong>. </p>



<p>You know what happens in the next two years?</p>



<p>Two year’s worth of StackOverflow questions. </p>



<p>You do <strong>not</strong> want to be the person taking a new tech or feature for a joy-ride in production and discovering some hemorrhaging-edge bug. </p>



<p>You <strong>also</strong> don’t want to be the person who has to clean up after the person who discovered a bug on a bleeding-edge technology. </p>



<p> Person-Alpha is the asshole. The Alphasshole, if you will. Because they chose to use the bleeding edge thing. </p>



<p>On the scrum-master’s Fibionacci-sequence of suckassness The Alphasshole is a 21. The Alphasshole implemented the new tech and turned your project into a collection of medical-attention-needed polyps on Satan’s butthole. </p>



<p>Person-Beta is the Betasshole. </p>



<p>The Betasshole had to work in real-time with other betassholes on other projects created by other alphassholes that were all equally hemorrhoidal in nature over StackOverflow and Discord to come up with whatever collection of hackery that properly deserves to be called, “Satan’s Self-soothing salve of sour-ass solutions.”</p>



<p>The Betasshole is a 5 on the Satanic-sequence of Suckassness. They’re not nearly as bad as the dickweed who chose new tech. But unfortunately, they made a lot of terrible choices and cannot be free from blame, git-related, or otherwise. </p>



<p>Listen. I’ve been an Alphasshole and a Betasshole before. I regret that. I should not have done that. </p>



<p>I’ve <em>also</em> been gamma waaaaaay more often. Who is not an asshole. That’s a person who’s just had to deal with two years of shit rolling downstream. What’s unfortunate for the gammas is that they often will be <em>accused</em> of being the assholes because they will git-blame, complain, and refrain from touching whatever that new tech was because it’s haunted their nightmares and their daymares. They are simply picking up the pieces. </p>



<p>The new and shiny is fun for side projects. It is not for production.</p>



<p><em>Please put two years between you and any new tech. If for anyone, do it for the gammas who actually have to keep your precious world wide web running. </em></p>



<h2>TL;DR</h2>



<p>Just… grow up and think.</p>



<ul>
<li>Think and care about the content first. Because that’s what the users care about. Assume the users’ interests are more important than your own. </li>



<li>Quit acting like CSS is some giant-ass mistake that needs fixing. A group of people who were collectively smarter than us wrote those specs. They didn’t make mistakes. Assume you are making the mistake, not them. </li>



<li>Server-side rendering was not invented with JavaScript and it doesn’t have to be implemented with JavaScript. There are many ways to render content on a server. Assume non-JavaScript solutions existed first.</li>



<li>JavaScript either is neither the God-send nor the hell-spawn for your project. It is a tool which you can choose to use incorrectly. Learn when JavaScript is the right tool for the job. </li>



<li>Quit doing hard things for simple problems. Quit doing things that are degrees of difficulty <em>harder</em> than the problem. When a problem presents itself, look for multiple solutions, and then choose the simplest one. Assume the simplest solution is the best one. </li>



<li>Quit chasing the new and shiny in your day job. I, too, am excited about the new and shiny. But I put a 2-year buffer between me and prod because I don’t want to be the person to have to discover browser bugs, framework bugs, or even compiler bugs. </li>
</ul>



<p>Or whatever. Don’t do this and continue to be an Alphasshole, Betasshole, or an otherwise generally contemptuous person. I don’t care. At least I can sleep better knowing that you know that I know that you now know that you’re an asshole. </p>



<p><s>Happy</s> apathetic coding!</p>























<div>
<h4>Footnotes, Sources, and Whatnots</h4>
<p id="one"><small>1</small> <a href="https://en.wikipedia.org/wiki/Express.js">Express.js was released in 2010. </a>
</p><p id="two"><small>2</small> <a href="https://www.infoworld.com/article/2609877/believe-the-hype--php-founder-backs-facebook-s-hiphop-technology.html">PHP started development in 1993 and was released in 1995</a> which means that when express.js dropped, PHP was driving to the store and buying clove cigarettes. </p>
<p id="three"><small>3</small> When I say it’s mutated, I mean that nesting jumped from Sass to native CSS. And CSS does it differently. <a href="https://sass-lang.com/blog/sass-and-native-nesting/">So now Sass has to change how it does nesting</a> because of how CSS does it. It’s bad. It will introduce lots of things you don’t want to happen. </p>

<p id="four"><small>4</small> People have been talking about how <a href="https://qz.com/646467/how-one-programmer-broke-the-internet-by-deleting-a-tiny-piece-of-code">left-pad broke the internet</a> and some wise folks have called out that it was pretty silly that such a simple function got to be so ubiquitous that it had the power to “crash the web”. But what’s <em> also dangerous</em> is the person <a href="https://github.com/jonschlinkert/jonschlinkert">actively trying to become ubiquitous</a>. Every PR that involves adding a new dev dependency to any project should require the developer to hand-write the entire dependency tree. That would probably save a lot of future heartache.  </p>

<p id="five"><small>5</small> I do not, in fact, have strong evidence that the Good Lord intended for WebPack to be told to, “fuck off good and proper like.” <em>However</em> <a href="https://www.bibleref.com/Leviticus/19/Leviticus-19-19.html">if the Good Lord had something to say about cotton blends</a>, He’d probably also have something to say about using a JavaScript build utility for building styles.  </p>
<p id="six"><small>6</small> <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/@property">Yes, you can have strongly-typed CSS</a>, but it’s insanely tedious because those “types” are not like any types you’d expect from any other language, and the syntax is going to be a huge pain. Also, you have the option to set it to not inherit which is, in fact, the most unCSSy thing ever. So please. Don’t.    </p>
</div>
					</div><!-- .entry-content -->

		<!-- .entry-footer -->
	</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bomb Jack display hardware (110 pts)]]></title>
            <link>https://github.com/martinpiper/BombJack/blob/master/README.md</link>
            <guid>40745830</guid>
            <pubDate>Fri, 21 Jun 2024 03:25:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/martinpiper/BombJack/blob/master/README.md">https://github.com/martinpiper/BombJack/blob/master/README.md</a>, See on <a href="https://news.ycombinator.com/item?id=40745830">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true" aria-labelledby="file-name-id-wide file-name-id-mobile"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Bomb Jack display hardware</h2><a id="user-content-bomb-jack-display-hardware" aria-label="Permalink: Bomb Jack display hardware" href="#bomb-jack-display-hardware"></a></p>
<ul dir="auto">
<li>Main social media: <a href="https://mastodon.social/@martin_piper" rel="nofollow">https://mastodon.social/@martin_piper</a></li>
<li>Don't have a Mastodon account? Click here for an invitation link: <a href="https://mastodon.social/invite/nuGxMC8z" rel="nofollow">https://mastodon.social/invite/nuGxMC8z</a></li>
<li>Itch IO project page: <a href="https://martin-piper.itch.io/bomb-jack-display-hardware" rel="nofollow">https://martin-piper.itch.io/bomb-jack-display-hardware</a></li>
<li>YouTube channel: <a href="https://www.youtube.com/c/MartinPiper6502" rel="nofollow">https://www.youtube.com/c/MartinPiper6502</a></li>
<li>Buy me a ko-fi: <a href="https://ko-fi.com/martinpiper0750" rel="nofollow">https://ko-fi.com/martinpiper0750</a></li>
<li>Bluesky: <a href="https://bsky.app/profile/martinpiper.bsky.social" rel="nofollow">https://bsky.app/profile/martinpiper.bsky.social</a></li>
<li>Twitter links: <a href="https://twitter.com/MartinPiper" rel="nofollow">https://twitter.com/MartinPiper</a> <a href="https://twitter.com/hashtag/RetroTTLVideoCard" rel="nofollow">https://twitter.com/hashtag/RetroTTLVideoCard</a> <a href="https://twitter.com/hashtag/RetroTTL" rel="nofollow">https://twitter.com/hashtag/RetroTTL</a></li>
<li>Threads: <a href="https://www.threads.net/@martinpiper1187" rel="nofollow">https://www.threads.net/@martinpiper1187</a></li>
<li>Mostly using these tags: #MegaWang #RetroTTL #RetroTTLVideoCard #C64 #Commodore64 #electronics</li>
</ul>
<p dir="auto">This started life as a working schematic for the video display portion of the original Bomb Jack arcade hardware. Why bother, you're probably asking? Well if you have to ask then perhaps you're not the right audience. :)</p>
<p dir="auto">The version 1.0 of this work was approximately interface pin and chip compatible with the original hardware schematic. Now since V2.0 the hardware has progressed to add extra features not present in the original, such as:</p>
<ul dir="auto">
<li>Addressable RAM instead of ROMs</li>
<li>Extra display blanking capability</li>
<li>Tile X/Y pixel scroll</li>
<li>64x64 Tile X/Y offset, useful for fast hardware scrolling</li>
<li>Full screen height sprites</li>
<li>Background "mode7" screen that allows per-pixel affine transformation</li>
<li>Background colour select (Can be rapidly updated during the frame for plasma effects)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Background</h2><a id="user-content-background" aria-label="Permalink: Background" href="#background"></a></p>
<p dir="auto">This project started when old retro arcade hardware was being discussed. In the back of my mind was the often fabled "Mega games" by Imagine Software which were planned to use extra hardware on the Spectrum and Commodore 64 to augment the machine's capabilities. Since this hardware uses TTL logic available back from the same time period I was wondering exactly how much extra graphical grunt could have been engineered and interfaced with these old 8-bit computers.</p>
<p dir="auto">Truth be told, the Imagine hardware was pretty much just extra RAM <a href="https://www.gamesthatwerent.com/gtw64/mega-games/" rel="nofollow">https://www.gamesthatwerent.com/gtw64/mega-games/</a> but this was a fun project to see how far the arcade hardware was pushing the limits of board size and signal complexity.</p>
<p dir="auto">I was looking at Bomb Jack boards on ebay and pondering how they had enough fill-rate to draw 24 16x16 sprites and have the option for some to use 32x32 mode as well. A friend and I were discussing the clock speed and fill-rate while trying to deduce the operation of the hardware just by inspecting the hand drawn schematics, as you do.</p>
<p dir="auto">In the end to get some clarity on the sprite plotting specifically I started to transcribe what was thought to be the sprite logic portion of the schematic into Proteus, since it can simulate digital electronics really well.</p>
<p dir="auto">Interestingly the Amiga computer, with its custom chips, was designed and prototyped using breadboards: <a href="https://arstechnica.com/gadgets/2007/08/a-history-of-the-amiga-part-3/" rel="nofollow">https://arstechnica.com/gadgets/2007/08/a-history-of-the-amiga-part-3/</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Digital display and audio simulation</h2><a id="user-content-digital-display-and-audio-simulation" aria-label="Permalink: Digital display and audio simulation" href="#digital-display-and-audio-simulation"></a></p>
<p dir="auto">The arcade hardware uses resistor ladders to convert 4x3 bit RGB+H+V sync digital signals into analog, this is a typical model for the period. I later updated the design to use 15 bit colour with 16x16 palette entries. Since the simulation software is optimised for digital model it makes sense to have a digital display driver simulation that accepts digital signals. Hence I created this Proteus VSM module: <a href="https://github.com/martinpiper/DigitalDisplay">https://github.com/martinpiper/DigitalDisplay</a></p>
<p dir="auto">This simulation can also be used to debug failures in original boards, since disabling chips or signals in the simulation can produce similar output display artefacts. For example disabling one of the scan line RAM chips (4A/4B/4C/4D) produces alternate pixels and bright lines, similar to the original hardware.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why not use a CPLD or FPGA?</h2><a id="user-content-why-not-use-a-cpld-or-fpga" aria-label="Permalink: Why not use a CPLD or FPGA?" href="#why-not-use-a-cpld-or-fpga"></a></p>
<p dir="auto">Good question! For my thinking, it is a question of what level to want to teach. In software eningeering there are numerous languages, C/C++, Python, Java, Forth, Pascal, etc. There is also machine code and usually some form of assembler mnemonic language which directly maps to machine code, the user accessible low-level language used by a particular CPU. Below that there can be microcode which is often not user accesible, this tends to control the internal state of a CPU. As a software engineer, I see value in using higher level languages, but often use low-level assembler when I want to optimise.</p>
<p dir="auto">A FPGA or CPLD, while it deals with logical operations, is usually programmed via a higher level language (VHDL or Verilog or similar) abstraction above the low-level logic cells actually used by the device, often (not always) the same code can be compiled for subtly different devices with minimal changes.</p>
<p dir="auto">7400-series discrete logic, specifically 74LS series logic used in this project, can be considered to be low-level, analogous to using machine code or assembler. There is a lower level, which is using discrete transistors to create logic cells, like NAND, OR, NOT, etc. but that really is a low level more like microcode.</p>
<p dir="auto">The use of high level or low-level programming is a contextual decision, often my knowledge of low-level architecture and performace informs my decision making processes when using high level languages. While it is certainly possible to code an application entirely with only high level language knowledge, it is an advantage to know low-level architecture to help debug hopefully rare performance or error situations.</p>
<p dir="auto">All that said, I believe that knowing how to engineer a solution using low-level discrete logic is an advantage, in other words it helps to improve the use of higher level CPLD or FPGA engineering to know lower level architecture. This is why this project exists, to help teach how to solve engineering problems using low-level discrete logic.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Developer technical details</h2><a id="user-content-developer-technical-details" aria-label="Permalink: Developer technical details" href="#developer-technical-details"></a></p>
<p dir="auto">To produce the test data run: BDD6502\ForUnitTests.bat</p>
<p dir="auto">If you have Proteus setup with the digital display model, you should see a screen that looks somewhat like this when starting the simulation:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/martinpiper/BombJack/blob/master/Capture.PNG?raw=true"><img src="https://github.com/martinpiper/BombJack/raw/master/Capture.PNG?raw=true" alt="Screenshot" title="Screenshot"></a></p>
<p dir="auto">The new hardware scrolling registers also work well:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/martinpiper/BombJack/blob/master/output/debug.gif?raw=true"><img src="https://github.com/martinpiper/BombJack/raw/master/output/debug.gif?raw=true" alt="animation" title="animation" data-animated-image=""></a></p>
<p dir="auto">The hardware supports sprite multiplexing and stacking, giving the option of having more than eight colours in sprites:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/martinpiper/BombJack/blob/master/output/debug2.gif?raw=true"><img src="https://github.com/martinpiper/BombJack/raw/master/output/debug2.gif?raw=true" alt="animation" title="animation2" data-animated-image=""></a></p>
<p dir="auto">Many thanks to <a href="https://opengameart.org/content/rpg-town-pixel-art-assets" rel="nofollow">https://opengameart.org/content/rpg-town-pixel-art-assets</a> and <a href="https://opengameart.org/content/colored-16x16-fantasy-tileset" rel="nofollow">https://opengameart.org/content/colored-16x16-fantasy-tileset</a> for the game art.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">High level schematic sheets</h3><a id="user-content-high-level-schematic-sheets" aria-label="Permalink: High level schematic sheets" href="#high-level-schematic-sheets"></a></p>
<p dir="auto">The Proteus sheet numbers correspond to the original schematic page numbers as written in the bottom right hand corner of each page. The layout of this schematic roughly matches the layout of the original schematic. So for example the analog RGB resistor ladders are on "Root sheet 8" in the top right of the sheet, as in the original schematic page 8 layout.</p>
<ol dir="auto">
<li>Logic analyser, virtual video display model, expansion bus header, data generator
<ol dir="auto">
<li>Memory selection logic based on external address bus</li>
<li>VSMDD2 is a replacement for the Proteus data generators because they have a 1024 byte limit. Proteus VSM project source: <a href="https://github.com/martinpiper/DigitalData">https://github.com/martinpiper/DigitalData</a></li>
<li>EXPANSIONBUS, EXPANSIONBUS2 and EXPANSIONBUS3 all relate to the C64 user port to 24 bit address logic in the project: UserPortTo24BitAddress.pdsprj</li>
</ol>
</li>
<li>Headers, the original schematic has dip switch logic</li>
<li>Video timing
<ol dir="auto">
<li>Horizontal video signal generation</li>
<li>Vertical video signal generation</li>
<li>HBLANK and VBLANK signal generation</li>
</ol>
</li>
<li>Sprite logic - Part 1
<ol dir="auto">
<li>Sprite RAM register access logic, timing with the external address/data bus and video internal timings</li>
<li>Temporary storage for sprite position, palette and frame</li>
<li>Bit plane access and bit shifters for output pixels to scan RAM</li>
<li>32x32 sprite selection registers and comparison</li>
<li>Various signal timings generated for sprite loading, tile and colour fetch etc</li>
</ol>
</li>
<li>Sprite logic - Part 2
<ol dir="auto">
<li>Dual scan line RAM buffers</li>
<li>Transparent pixel test logic based on inverted pixel temporary storage and selection with NOR check</li>
<li>Pixel writing to RAM based on 16 pixel chunks and position register contents</li>
<li>Logic for timing of pixel reads for possible video display, plus clearing of data just read</li>
</ol>
</li>
<li>Character screen RAM with associated colour screen
<ol dir="auto">
<li>Associated external bus interface logic</li>
<li>Bit plane reads and pixel shifters</li>
<li>Output pixel logic for possible video display</li>
</ol>
</li>
<li>16x16 tile background picture logic
<ol dir="auto">
<li>Logic for selection of appropriate picture, based on high address lines</li>
<li>Bit plane reads and pixel shifters</li>
<li>Output pixel logic for possible video display, or disable background logic</li>
</ol>
</li>
<li>Output pixel pipeline and palette lookup
<ol dir="auto">
<li>Pixel transparency test from background, character screen and sprites</li>
<li>Palette RAMs with external bus logic</li>
<li>Final pixel latching logic and palette RAM lookup</li>
<li>Output digital to analog conversion using resistor ladders</li>
<li>Analog RGB and associated sync signal output header</li>
</ol>
</li>
<li>Mode7 calculation
<ol dir="auto">
<li>Register addressing and latches</li>
<li>dx, dxy, dy, dyx calculation</li>
<li>xorg, yorg addition</li>
</ol>
</li>
<li>Mode7 pixel logic
<ol dir="auto">
<li>Background colour latch, transparent pixel detection</li>
<li>Interleaved blocks (for RAM timing) mode7 screen access, feeding into tile access, H &amp; V flip logic</li>
</ol>
</li>
<li>Scaled sprite logic - Part 1
<ol dir="auto">
<li>Register memory</li>
<li>Sprite extent and scan line detection</li>
<li>Sprite flipping</li>
</ol>
</li>
<li>Scaled sprite logic - Part 2
<ol dir="auto">
<li>Scanline - Two alternating banks</li>
<li>Sprite definition RAM</li>
</ol>
</li>
<li>Vector logic
<ol dir="auto">
<li>Register logic - Bank display and kill</li>
<li>Scan RAM banks - Two banks</li>
</ol>
</li>
<li>Pixel data combiner (merge layer) logic
<ol dir="auto">
<li>2-1 pixel data combiners - Two blocks</li>
</ol>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">How to access the hardware</h3><a id="user-content-how-to-access-the-hardware" aria-label="Permalink: How to access the hardware" href="#how-to-access-the-hardware"></a></p>
<p dir="auto">In the original hardware, used in the Bombjack arcade game, the Z80 data and its address bus was partially connected to the hardware with a couple of address windows at $9000 to $9fff (perhaps a bit wider than that), this was mostly write RAM as far as the Z80 was concerned. It was sprite registers, some control registers, palette and char screen RAM. The rest of the video internal memory was ROM, and could not be addressed by the Z80. The Z80 would have to time writes to the video address space (RAM) during the VBLANK when the video hardware was not expecting to read RAM to avoid contention.</p>
<p dir="auto">For this updated hardware, the C64 (or a Z80) sends data bytes via the user port (parallel transfer, so it's fast, one clock cycle to write), with some control signals, to write to a 24 bit address space in the hardware. This is usually accomplished by the UserPortTo24BitAddress.pdsprj (or better) project. Basically a control signal resets the internal state, the next three bytes are the address to write, then next bytes are consecutively written to the address space until the internal state is reset again. There is a signal from the hardware to the C64 via the user port for screen refresh (VBLANK) which is read as an NMI (CIA2 FLAG pin) request on the C64. This means there is no bus contention with the C64 memory bus to the hardware. Writes to hardware RAM take precedence over what the hardware is reading from the RAM, this ensures data is written without contention. Writes to the hardware which involve RAM, rather than register latches, should usually be timed to coincide with VBLANK or HBLANK to avoid visual glitches. Each layer (chars, tiles, sprites etc) provides information on any RAM timing constraints.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Memory map</h3><a id="user-content-memory-map" aria-label="Permalink: Memory map" href="#memory-map"></a></p>
<p dir="auto">Note: Most control registers can be updated at any point during the frame and the next pixel will reflect the change.</p>
<p dir="auto">0x9000			Chars control register
</p>
<p dir="auto">0x9c00-0x9dff	Palette RAM</p>
<p dir="auto">0x9e00	0x10	Use background colour
0x20	Enable display
0x80	BorderY shrink</p>
<p dir="auto">0x9e08	Layer priority select, for each layer select the input header pixel input
Default should be: %11100100 = 0xe4
i.e. Back most layer	= %11 (3)
Next layer closer		= %10 (2)
Then next closest		= %01 (1)
Then front most layer	= %00 (0)</p>
<p dir="auto">0x9e09	0xlg	Setup nybble extents for overscan to enable the border (g = greater than, and l = less than) in 8 pixel steps
Overscan mode is always on, so this needs to be set to sensible values
Sensible wide value for this is $29 which hides the left edge (including 16x16 tiles) and brings in the right edge in slightly
This gives a 320 pixel wide screen, displaying a full width of 40 8x8 characters</p>
<p dir="auto">0x9e0a	Video layer enable flags. These are passed through the VideoX connectors. Each layer has the option of reading their enable flag and taking appropriate action.
Bits	Action				Default layer assigned to this connector (can be changed)		Replaces logic
0x01	Enable layer 1		Sprites (or Sprites2)											0x9a00 Upper nybble: $10 = Enable sprite output (6S SPREN)
0x02	Enable layer 2		Chars															0x9000 Chars control register	0x02	Chars screen disable
0x04	Enable layer 3		Tiles															0x9e00	0x10	Enable tiles
0x08	Enable layer 4		Background or mode7												0xa015	: Flags	:	0 : Enable display</p>
<p dir="auto">0x9e0b	Background colour
If the final pixel is transparent and 0x9e00 0x10 is set, then the palette information is read from this colour value
If the final pixel is transparent and 0x9e00 0x10 is not set, then the palette information is read from the last layer colour value</p>
<p dir="auto">0x9e0c	Palette bank number
See: kBus24Bit_VideoLayer_ExpandedPalettes</p>
<p dir="auto">Sprite 32x32 size select
0x9a00 start index of 32x32 sprites
0x9a01 end index of 32x32 sprites (exclusive)
So 0,0 = no 32x32 sprites
0,8 means indexes 2 to 7 (exclusive) are 32x32
Maximum number in both is 0xf</p>
<p dir="auto">At 0x9800 - 0x985f each sprite is described by 4 bytes:</p>
<div data-snippet-clipboard-copy-content="Byte 0:
	The tile code for the sprite, used to look up the sprite’s image bitplanes in the tile ROMs
	(MAME Emu documentation is wrong, bit 7 has nothing to do with selecting double size mode)

Byte 1: HVFMCCCC
	Bit 7: H : If set, the sprite is horizontally flipped
	Bit 6: V : If set, the sprite is vertically flipped
	Bit 5: F : If set, the sprite is full screen height. The sprite data will repeat.
	Bit 4: MSBX
	Bits 3..0: C : 4 bits to provide the colour value for the tile decoder

Byte 2: The sprite’s Y position on screen
Byte 3: The sprite’s X position on screen"><pre><code>Byte 0:
	The tile code for the sprite, used to look up the sprite’s image bitplanes in the tile ROMs
	(MAME Emu documentation is wrong, bit 7 has nothing to do with selecting double size mode)

Byte 1: HVFMCCCC
	Bit 7: H : If set, the sprite is horizontally flipped
	Bit 6: V : If set, the sprite is vertically flipped
	Bit 5: F : If set, the sprite is full screen height. The sprite data will repeat.
	Bit 4: MSBX
	Bits 3..0: C : 4 bits to provide the colour value for the tile decoder

Byte 2: The sprite’s Y position on screen
Byte 3: The sprite’s X position on screen
</code></pre></div>
<p dir="auto">0x9200 Sprites2 registers
Sprites support X and Y flips with X &amp; Y repeating patterns</p>
<div data-snippet-clipboard-copy-content="Byte 0: Palette | 0x10 = MSBX | 0x20 = MSBY | 0x40 = flipX | 0x80 = flipY
Byte 1: Y pos
Byte 2: Y size (in screen pixels, regardless of scale) (0 = end of list)
Byte 3: X pos
Byte 4: X scale extent (uses internal coordinates)
Byte 5: Y inv scale (*32)
Byte 6: X inv scale (*32)
Byte 7: Sprite frame (index) | 0x40 = halfX | 0x80 = halfY"><pre><code>Byte 0: Palette | 0x10 = MSBX | 0x20 = MSBY | 0x40 = flipX | 0x80 = flipY
Byte 1: Y pos
Byte 2: Y size (in screen pixels, regardless of scale) (0 = end of list)
Byte 3: X pos
Byte 4: X scale extent (uses internal coordinates)
Byte 5: Y inv scale (*32)
Byte 6: X inv scale (*32)
Byte 7: Sprite frame (index) | 0x40 = halfX | 0x80 = halfY
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Tile and sprite data layout</h4><a id="user-content-tile-and-sprite-data-layout" aria-label="Permalink: Tile and sprite data layout" href="#tile-and-sprite-data-layout"></a></p>
<p dir="auto">16x16 pixel tile and sprite data is stored in 8x8 cells and in memory order:
01
23</p>
<p dir="auto">For 32x32 sprites, this arrangement is expanded in memory as:
0145
2367
89cd
abef</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mode7 registers</h3><a id="user-content-mode7-registers" aria-label="Permalink: Mode7 registers" href="#mode7-registers"></a></p>
<p dir="auto">Mode7 borrows its name from the graphics mode on the Super NES video game console. <a href="https://en.wikipedia.org/wiki/Mode_7" rel="nofollow">https://en.wikipedia.org/wiki/Mode_7</a></p>
<p dir="auto">0xa000 - 0xa011	: Sets of 24 bit (8 byte) registers in l/h/hh order. There are 8 bits of accuracy, i.e. numbers are multiplied by 256.0f</p>

<p dir="auto">Each pixel on the layer is accessed using the accumulated results of these registers, this means it is possible to have per-pixel transformations combining translation, scaling, reflection, rotation, and shearing.
It is possible to update these registers per scanline, or per pixel, and generate even more complex results. Register values are latched and used immediately, so timing the update of the three bytes must be considered.</p>
<p dir="auto">Note: The internal accumulated values, x/xy/y/yx are not accessible via registers and are reset only by _EHSYNC and _EVSYNC</p>
<p dir="auto">For each horizontal pixel (on +ve 6MHz):</p>
<div data-snippet-clipboard-copy-content="x += dx
yx += dyx

xo = x + xy + xorg
yo = y + yx + yorg"><pre><code>x += dx
yx += dyx

xo = x + xy + xorg
yo = y + yx + yorg
</code></pre></div>
<p dir="auto">For each scanline (on +ve _EHSYNC):</p>

<p dir="auto">For each scanline (on _EHSYNC):</p>

<p dir="auto">For each frame (on _EVSYNC):</p>

<p dir="auto">All of the above is one way to progressively calculate xo,yo using addition. In other words:</p>
<div data-snippet-clipboard-copy-content="xo = (dx*x) + (dxy*y) + xorg
yo = (dy*y) + (dyx*x) + yorg"><pre><code>xo = (dx*x) + (dxy*y) + xorg
yo = (dy*y) + (dyx*x) + yorg
</code></pre></div>
<p dir="auto">During the coordinate (xo,yo) to screen transformation: bits 8-11 are used to lookup the tile pixel, bits 12-18 for the x screen tile index position, bits 12-17 for the y screen tile index position.</p>
<p dir="auto">Note: The full, but hidden by borders, screen resolution is 384x264 pixels.</p>
<p dir="auto">0xa014	: Background colour, any pixel index of zero in the mode7 tile data will show the background colour
0xa015	: Flags	:	0 : Enable X update, or reset to 0
1 : Enable XY update, or reset to 0
2 : Enable Y update, or reset to 0
3 : Enable YX update, or reset to 0</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Audio</h3><a id="user-content-audio" aria-label="Permalink: Audio" href="#audio"></a></p>
<p dir="auto">Current audio hardware has 4 voices, each using 11 bytes. Extra bytes are used for <em>currently unused byte</em> and voice active bit masks after the voice registers.</p>
<p dir="auto">Each voice register layout is as follows:
byte	register
0		volume
1/2		address lo/hi
3/4		length lo/hi
5/6		rate lo/hi
7/8		loop address lo/hi
9/10	loop length lo/hi</p>
<p dir="auto">The sample frequency in hertz to voice rate conversion, using internal 4MHz clock, is:  (4096 * hertz) / 25000</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Clock speeds</h3><a id="user-content-clock-speeds" aria-label="Permalink: Clock speeds" href="#clock-speeds"></a></p>
<p dir="auto">The original schematic uses a 6MHz clock for all the video hardware, as denoted by the "6MHz" signal line. The clocking can be found on "Root sheet 1" just about the logic analyser.</p>
<p dir="auto">To help debug timing issues, especially with RAMs, the schematic should be simulated at 6MHz. However the Proteus simulation can use 1MHz on this signal line, I kept the naming of the line the same as the original schematic however. This is due to the default RAM write timings for ICs 4A/4B/4C/4D being too tight. This means the digital display driver will detect ~10fps, not ~60fps as per the original design. It does however make the debug single step time easier to think about since it's not divided by 6MHz...</p>
<p dir="auto">With VIDCLK = 2M</p>
<p dir="auto">Logic analyser:
Display scale 0.2u
Capture Resolution 0.2u</p>
<p dir="auto">Remember to use:</p>
<div data-snippet-clipboard-copy-content="System->Set Animation Options
	Single Step Time: 250n
	500n For debugging full clock cycle and pixel clock"><pre><code>System-&gt;Set Animation Options
	Single Step Time: 250n
	500n For debugging full clock cycle and pixel clock
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Where is the Z80 CPU?</h3><a id="user-content-where-is-the-z80-cpu" aria-label="Permalink: Where is the Z80 CPU?" href="#where-is-the-z80-cpu"></a></p>
<p dir="auto">The Z80 CPU from the original schematic is not included, it was clocked independently and has its own RAM, except for the video hardware interfaces and isn't needed for the video simulation.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Input data setup</h3><a id="user-content-input-data-setup" aria-label="Permalink: Input data setup" href="#input-data-setup"></a></p>
<p dir="auto">The Z80 data write signals are mocked using a simulator pattern generator VSMDD2, this is separate to the main video schematic and the generator is excluded from the PCB layout.</p>
<p dir="auto">As per the original design all writes to the video hardware should be carefully timed to coincide with the VBLANK or other safe write blanking periods. This is because the video hardware is almost always reading the RAM during the visible portion of the frame. Writing to the sprite registers outside the VBLANK will especially produce nasty looking effects on the screen. This RAM sharing model is quite common is old arcade and console hardware.</p>
<p dir="auto">The original hardware has been expanded to include RAMs where the ROMs were located. These are addressed by an combination of EXPANSIONBUS3 to select the groups of RAMs and the EXPANSIONBUS group selector. It is entirely possible to write more than one group at a time by enabling multiple output bits in EXPANSIONBUS3.</p>
<p dir="auto">For EBBS = $01 the RAM address select lines are from $8000 until $b800 in steps of $800</p>
<table>
<thead>
<tr>
<th>EXPANSIONBUS3</th>
<th>Group</th>
<th>EXPANSIONBUS address</th>
<th>Behaviour</th>
</tr>
</thead>
<tbody>
<tr>
<td>$01</td>
<td>Audio registers</td>
<td>$8000 11 bytes/voice</td>
<td>Audio registers</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9000</td>
<td>Char screen control</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9001/2</td>
<td>Char screen X scroll</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9003/4</td>
<td>Char screen Y scroll</td>
</tr>
<tr>
<td>$01</td>
<td>Sprite2 registers</td>
<td>$9200-$92ff</td>
<td>Sprite2 registers</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9800   $60 bytes</td>
<td>Sprite registers</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9c00   $200 bytes</td>
<td>Palette GR XB 2 bytes per entry</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9e00</td>
<td>Background image enable and borders</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9e01/2</td>
<td>Background image XL/XH pixel scroll</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9e03/4</td>
<td>Background image YL/YH pixel scroll</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9e07</td>
<td>Background colour select</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9e08</td>
<td>Layer priority select</td>
</tr>
<tr>
<td>$01</td>
<td>Original RAMs</td>
<td>$9a00-$9a01</td>
<td>Start/end 32x32 sprite index 0-f only</td>
</tr>
<tr>
<td>$01</td>
<td>Extension mode7 registers</td>
<td>$a000-$a017</td>
<td>Mode7 registers</td>
</tr>
<tr>
<td>$01</td>
<td>Vectors</td>
<td>$a100   1 byte</td>
<td>Vectors display bank</td>
</tr>
<tr>
<td>$01</td>
<td>Combiner</td>
<td>$a200   4 bytes</td>
<td>Combiner display bank</td>
</tr>
<tr>
<td>Note: APU and vectors are by default using the same EBBS in the schematics</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$02</td>
<td>APU</td>
<td>$8000   8KB</td>
<td>APU memory bank 0 instruction</td>
</tr>
<tr>
<td>$02</td>
<td>APU</td>
<td>$4000   8KB</td>
<td>APU memory bank 1 data</td>
</tr>
<tr>
<td>$02</td>
<td>APU</td>
<td>$2000   8KB</td>
<td>APU bank 2 registers</td>
</tr>
<tr>
<td>$02</td>
<td>Vectors</td>
<td>$0000   16KB</td>
<td>Vectors bank 0 data</td>
</tr>
<tr>
<td>$02</td>
<td>Vectors</td>
<td>$8000   16KB</td>
<td>Vectors bank 1 data</td>
</tr>
<tr>
<td>Note: Audio and Sprites2 are by default using the same EBBS in the schematics</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$04</td>
<td>Audio memory</td>
<td>$0000-$ffff</td>
<td>Audio sample memory</td>
</tr>
<tr>
<td>$04</td>
<td>Sprites2 data</td>
<td>$2000   8KB++</td>
<td>Sprites2 bit planes</td>
</tr>
<tr>
<td>$80</td>
<td>Background 16x16 Root sheet 7</td>
<td>$2000   8KB</td>
<td>Tiles and colours into 4P7R</td>
</tr>
<tr>
<td>$80</td>
<td>Char screen</td>
<td>$4000   8KB</td>
<td>Chars into U258</td>
</tr>
<tr>
<td>$80</td>
<td>Char screen colours</td>
<td>$8000   8KB</td>
<td>Chars colours into U137</td>
</tr>
<tr>
<td>$40</td>
<td>Background 16x16 Root sheet 7</td>
<td>$2000   8KB</td>
<td>Tiles bit plane 0 into 8R7R</td>
</tr>
<tr>
<td>$40</td>
<td>Background 16x16 Root sheet 7</td>
<td>$4000   8KB</td>
<td>Tiles bit plane 1 into 8N7R</td>
</tr>
<tr>
<td>$40</td>
<td>Background 16x16 Root sheet 7</td>
<td>$8000   8KB</td>
<td>Tiles bit plane 2 into 8L7R</td>
</tr>
<tr>
<td>$20</td>
<td>Char screen 8x8 Root sheet 6</td>
<td>$2000   8KB</td>
<td>Char data bit plane 0 into 8K6R</td>
</tr>
<tr>
<td>$20</td>
<td>Char screen 8x8 Root sheet 6</td>
<td>$4000   8KB</td>
<td>Char data bit plane 1 into 8H6R</td>
</tr>
<tr>
<td>$20</td>
<td>Char screen 8x8 Root sheet 6</td>
<td>$8000   8KB</td>
<td>Char data bit plane 2 into 8E6R</td>
</tr>
<tr>
<td>$10</td>
<td>Sprite data Root sheet 4</td>
<td>$2000   8KB</td>
<td>Sprite bit plane 0 into 7JR</td>
</tr>
<tr>
<td>$10</td>
<td>Sprite data Root sheet 4</td>
<td>$4000   8KB</td>
<td>Sprite bit plane 1 into 7LR</td>
</tr>
<tr>
<td>$10</td>
<td>Sprite data Root sheet 4</td>
<td>$8000   8KB</td>
<td>Sprite bit plane 2 into 7MR</td>
</tr>
<tr>
<td>$08</td>
<td>Mode7 Root sheet 10</td>
<td>$2000   8KB</td>
<td>Mode7 screen data 128x64 tile index</td>
</tr>
<tr>
<td>$08</td>
<td>Mode7 Root sheet 10</td>
<td>$4000   8KB</td>
<td>Mode7 tile data 32 of 16x16 tiles</td>
</tr>
<tr>
<td>$08</td>
<td>Mode7 Root sheet 10</td>
<td>$8000   8KB</td>
<td>Mode7 tile data 32 of 16x16 tiles</td>
</tr>
</tbody>
</table>
<ul dir="auto">
<li>
<p dir="auto">Using the pattern file: TestData.txt</p>
<p dir="auto">The sprite writes particularly have various configurations for sprite displays, these are commented in/out depending on the desired patterns</p>
<p dir="auto">** Comment: Spread out all over the screen</p>
<p dir="auto">This includes background screen setup, char screen data setup and sprite palette, frames and position setup.
It provides good variation of sprites, palettes etc across the full range of screen coordinates. This is useful for testing expected masking logic and general visual integrity.</p>
<p dir="auto">** Comment: Top left all the same</p>
<p dir="auto">This sets all sprites to be in the top left of the screen, it is useful to testing maximum pixel write through and sprite selection logic scenarios. The background is disabled and char screen uses transparent chars. This leaves just the sprite plane outputting pixels.
Setting a logic break on RV[0..8] = 0xe8 will allow the simulation timing and scan line RAM contents to be inspected in detail.
The timing of sprite pixel writes into scan RAM 4A/4B and pixel reads from scan RAM 4C/4D with its clear to $ff can be seen whilst single stepping. Note the values for the RV and RH bus lines just below the video display.</p>
<p dir="auto">** Comment: X pos group</p>
<div data-snippet-clipboard-copy-content="  This regularly spaces the sprites with the same X pos +/-4 pixels. Useful for debugging."><pre><code>  This regularly spaces the sprites with the same X pos +/-4 pixels. Useful for debugging.
</code></pre></div>
<p dir="auto">** Comment: Y pos group</p>
<div data-snippet-clipboard-copy-content="  This regularly spaces the sprites with the same Y pos +/-4 pixels. Useful for debugging."><pre><code>  This regularly spaces the sprites with the same Y pos +/-4 pixels. Useful for debugging.
</code></pre></div>
</li>
<li>
<p dir="auto">Using the pattern file: TestData2.txt</p>
<p dir="auto">Using the ImageToBitplane tool Run/Debug Configuration: Main all conv
Using GenerateData configuration: Debug2</p>
<p dir="auto">This demonstrates stacked multiplexed sprites over a 16x16 tiled background.</p>
<p dir="auto">Using the ImageToBitplane tool Run/Debug Configuration: Main old bridge RGB 0 0 0 palette opt rgbfactor 32</p>
<p dir="auto">This demonstrates a detailed background picture with 32 palettes of 8 entries each.</p>
<p dir="auto">Using the ImageToBitplane tool Run/Debug Configuration: Main mode7 "map_9 - mode7 test.png"
And enabling the data load at: ; Write Mode7 registers
This will demonstrate how the mode7 export with flipped tile detection works</p>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Raster line schedule</h3><a id="user-content-raster-line-schedule" aria-label="Permalink: Raster line schedule" href="#raster-line-schedule"></a></p>
<div data-snippet-clipboard-copy-content="Counting from 0 (in computer terms), the first visible display line is 17 and the last visible line is 240, and with the Y border shrink it is 224.

_EHSYNC continues its pulses on every line, even when _EVSYNC is low
Positive edge of _EHSYNC indicates start of the line
Negative edge of _EVSYNC indicates start of the frame

Positive edge of VBLANK indicates the bottom of the visible screen edge
Note: EXTWANTIRQ = _VBLANK which triggers on the negative edge at raster $f0

RV = vertical raster line number
	$f8-$ff	lo _EVSYNC
	$00-$ff	hi _EVSYNC
		$10		lo VBLANK
		$e0		lo FINALENABLEPIXELS with bordery flag
		$f0		hi VBLANK
RH = horizontal pixel clock
	Full raster line starts at $0 to $17e
		In reality, RH $0 occurs for two clocks, each subsequent pixel advances with one clock, up to a maximum of $17e
	Giving 384 pixel clocks per line
	Maximum visible portion (when _EHSYNC = 1) is $000 to $15f
	_EHSYNC = 0 (HBLANK portion of the display where nothing is visible) is $160 to $17e (plus the first half of the next $000)

SREAD = Sprite register address (lo byte, also maps to ISAB[0..6] in the schematic)

Line RH starts at $000
$000	SREAD $00
		Begin sprite 0 register reads
		1V*. H
		lo to hi _EHSYNC (in second clock for $000)
$002	SREAD $01
$004	SREAD $02
$006	SREAD $03
$008 	SREAD $00 to $03 again
$008H	OV0/1/2 LHH OC0/1/2/3 HLHL from M0-M7
$008L	All L no data from $008H clocked into pixel output latch at 5E2
$009H	Pixel data is latched into 5E2
$009H	Begin sprite 0 pixel writes
		U77 First pixel written
$00aH	U77 Second pixel written
$00a	Pixels start arriving at the real video output, resistor ladders
		Pixel also cleared to $ff in U78
$010	SREAD $04 same 4x2 repeating pattern every 2 pixels
		Begin sprite 1 register reads
		Meaning 16 pixels elapse for each sprite
$012	lo to hi FINALENABLEPIXELS (with overscan $29)
$0c0	SREAD $30 same 4x2 pattern as above
$152	hi to lo FINALENABLEPIXELS (with overscan $29)
$160	lo _EHSYNC
$17e	hi _EHSYNC (into first half of $000)
$17e	SREAD $7f
Loops back to $000 again and RV increment


The file &quot;BombJack\RecordingRHSprAddr.txt&quot; contains recorded address and RH values from Root Sheet 4 device VSMDD5.
This clearly shows (sparse extracts)
	Recording		Index	Address	@RH
	d$00000000		0		$00		$000
	d$00100040		4		$10		$040
	d$00200080		8		$20		$080
	d$003000c0		12		$30		$0c0
	d$00400100		16		$40		$100
	d$00500140		20		$50		$140"><pre><code>Counting from 0 (in computer terms), the first visible display line is 17 and the last visible line is 240, and with the Y border shrink it is 224.

_EHSYNC continues its pulses on every line, even when _EVSYNC is low
Positive edge of _EHSYNC indicates start of the line
Negative edge of _EVSYNC indicates start of the frame

Positive edge of VBLANK indicates the bottom of the visible screen edge
Note: EXTWANTIRQ = _VBLANK which triggers on the negative edge at raster $f0

RV = vertical raster line number
	$f8-$ff	lo _EVSYNC
	$00-$ff	hi _EVSYNC
		$10		lo VBLANK
		$e0		lo FINALENABLEPIXELS with bordery flag
		$f0		hi VBLANK
RH = horizontal pixel clock
	Full raster line starts at $0 to $17e
		In reality, RH $0 occurs for two clocks, each subsequent pixel advances with one clock, up to a maximum of $17e
	Giving 384 pixel clocks per line
	Maximum visible portion (when _EHSYNC = 1) is $000 to $15f
	_EHSYNC = 0 (HBLANK portion of the display where nothing is visible) is $160 to $17e (plus the first half of the next $000)

SREAD = Sprite register address (lo byte, also maps to ISAB[0..6] in the schematic)

Line RH starts at $000
$000	SREAD $00
		Begin sprite 0 register reads
		1V*. H
		lo to hi _EHSYNC (in second clock for $000)
$002	SREAD $01
$004	SREAD $02
$006	SREAD $03
$008 	SREAD $00 to $03 again
$008H	OV0/1/2 LHH OC0/1/2/3 HLHL from M0-M7
$008L	All L no data from $008H clocked into pixel output latch at 5E2
$009H	Pixel data is latched into 5E2
$009H	Begin sprite 0 pixel writes
		U77 First pixel written
$00aH	U77 Second pixel written
$00a	Pixels start arriving at the real video output, resistor ladders
		Pixel also cleared to $ff in U78
$010	SREAD $04 same 4x2 repeating pattern every 2 pixels
		Begin sprite 1 register reads
		Meaning 16 pixels elapse for each sprite
$012	lo to hi FINALENABLEPIXELS (with overscan $29)
$0c0	SREAD $30 same 4x2 pattern as above
$152	hi to lo FINALENABLEPIXELS (with overscan $29)
$160	lo _EHSYNC
$17e	hi _EHSYNC (into first half of $000)
$17e	SREAD $7f
Loops back to $000 again and RV increment


The file "BombJack\RecordingRHSprAddr.txt" contains recorded address and RH values from Root Sheet 4 device VSMDD5.
This clearly shows (sparse extracts)
	Recording		Index	Address	@RH
	d$00000000		0		$00		$000
	d$00100040		4		$10		$040
	d$00200080		8		$20		$080
	d$003000c0		12		$30		$0c0
	d$00400100		16		$40		$100
	d$00500140		20		$50		$140
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Sprite scan RAM logic</h4><a id="user-content-sprite-scan-ram-logic" aria-label="Permalink: Sprite scan RAM logic" href="#sprite-scan-ram-logic"></a></p>
<div data-snippet-clipboard-copy-content="A sprite in 16x16 mode, with a Y register value of 224, which uses internal inverted logic, will have a calculated sprite Y coordinate of 256 - 224 = 32, this will be the last calculated scan line of the sprite. The result of the scan line calculation will be rendered and visible on the next scan line 33.
15 scan lines above calculated scan line 32 is the first calculated scan line of the sprite, in this example scan line 17, and the next scan line 18 is the first visible scan line of the sprite.


Assume the sprite scan line RAM has been cleared by a previous scan rendering all the pixels and clearing the values of $ff.
	Old revision: The weak pull-up resisters connected to the RAM output help to clear the RAM values to $ff
		These are overidden by any pull down logic 0 from the RAM or the input 74258 demultiplexors
	New revision: Uses digital logic to switch between $ff and input values as needed. This is much more stable than using pull-up resistors which needed to be tuned for the actual TTL chips used.
For each sprite the index, colour, X and Y pos are read
	After a successful Y pos test with the vertical raster position...
		The sprite data counter is set with the X position of the sprite, then for 16 (or 32) pixels
		The data in the scan line RAM is read on _6MHz = 0
		On +ve edge _6MHz this read value is latched by 2C/2D 2B/2A, and inverted for output
		The value is then tested for transparency with 1C:B or 1C:C 3-input NOR gates
		If it is transparent, the value from the input pixel from the current sprite is output by 3C/3D 3B/3A by the 74258 demultiplexors, which inverts the information. This could be a transparent pixel with its colour, or an opaque pixel with its colour.
		If it is not transparent, an opaque sprite pixel exists for this position, then the value just read from the RAM is selected by the 74258 demultiplexors, inverted and written back to the RAM
		In effect, the first opaque (non-transparent) pixel from the sprite written to the scan line RAM has priority and the rest of the pixels from sprites are ignored


Timing enabling or disabling the sprite display and updating sprite registers allows multiplexing. See: kBus24Bit_VideoLayer_XPos_SpriteIndex_*
The sprite scan line schedule, which renders each sprite in 16 pixel chunks, can be syncronised to the video pixel clock, or overclocked.
When syncronised to the video pixel clock each 16x16 sprite takes 16 pixels to process. If the sprite clock is overclocked, the scan schedule will correspondingly contract by the ratio of video clock to sprite clock. In other words each sprite will take fewer than 16 pixels to process.
Overclocking the sprite pixel clock will alter the timing for sprite multiplexing: kBus24Bit_VideoLayer_XPos_SpriteIndex_*


Remember, the scanline before a sprite is visible/displayed on the screen is the actual scanline used for rendering the sprite data.
	The x position of the sprite does not affect its rendering timing in the scanline, only the sprite register address used affects its rendering schedule time, see: Raster line schedule"><pre><code>A sprite in 16x16 mode, with a Y register value of 224, which uses internal inverted logic, will have a calculated sprite Y coordinate of 256 - 224 = 32, this will be the last calculated scan line of the sprite. The result of the scan line calculation will be rendered and visible on the next scan line 33.
15 scan lines above calculated scan line 32 is the first calculated scan line of the sprite, in this example scan line 17, and the next scan line 18 is the first visible scan line of the sprite.


Assume the sprite scan line RAM has been cleared by a previous scan rendering all the pixels and clearing the values of $ff.
	Old revision: The weak pull-up resisters connected to the RAM output help to clear the RAM values to $ff
		These are overidden by any pull down logic 0 from the RAM or the input 74258 demultiplexors
	New revision: Uses digital logic to switch between $ff and input values as needed. This is much more stable than using pull-up resistors which needed to be tuned for the actual TTL chips used.
For each sprite the index, colour, X and Y pos are read
	After a successful Y pos test with the vertical raster position...
		The sprite data counter is set with the X position of the sprite, then for 16 (or 32) pixels
		The data in the scan line RAM is read on _6MHz = 0
		On +ve edge _6MHz this read value is latched by 2C/2D 2B/2A, and inverted for output
		The value is then tested for transparency with 1C:B or 1C:C 3-input NOR gates
		If it is transparent, the value from the input pixel from the current sprite is output by 3C/3D 3B/3A by the 74258 demultiplexors, which inverts the information. This could be a transparent pixel with its colour, or an opaque pixel with its colour.
		If it is not transparent, an opaque sprite pixel exists for this position, then the value just read from the RAM is selected by the 74258 demultiplexors, inverted and written back to the RAM
		In effect, the first opaque (non-transparent) pixel from the sprite written to the scan line RAM has priority and the rest of the pixels from sprites are ignored


Timing enabling or disabling the sprite display and updating sprite registers allows multiplexing. See: kBus24Bit_VideoLayer_XPos_SpriteIndex_*
The sprite scan line schedule, which renders each sprite in 16 pixel chunks, can be syncronised to the video pixel clock, or overclocked.
When syncronised to the video pixel clock each 16x16 sprite takes 16 pixels to process. If the sprite clock is overclocked, the scan schedule will correspondingly contract by the ratio of video clock to sprite clock. In other words each sprite will take fewer than 16 pixels to process.
Overclocking the sprite pixel clock will alter the timing for sprite multiplexing: kBus24Bit_VideoLayer_XPos_SpriteIndex_*


Remember, the scanline before a sprite is visible/displayed on the screen is the actual scanline used for rendering the sprite data.
	The x position of the sprite does not affect its rendering timing in the scanline, only the sprite register address used affects its rendering schedule time, see: Raster line schedule
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">APU - What is it</h3><a id="user-content-apu---what-is-it" aria-label="Permalink: APU - What is it" href="#apu---what-is-it"></a></p>
<p dir="auto">The Advanced Processing Unit is an optimised raster and data transfer processor that can execute several instructions at the same time.
It is able to precisely wait for raster H&amp;V positions, transfer data, and perform simple comparisons based on internal data with conditional executions based on the results of the comparison.
This significantly reduces load on the CPU and the amount of memory the CPU needs, allowing the CPU to transfer data when it can.</p>
<p dir="auto">There is significant logic internally that allows data to be processed: <a href="https://docs.google.com/presentation/d/1Tb4B5-INhH2oiOSG4jfb3BfGGtdT9jPvyWoryOIJNzo/edit?usp=sharing" rel="nofollow">https://docs.google.com/presentation/d/1Tb4B5-INhH2oiOSG4jfb3BfGGtdT9jPvyWoryOIJNzo/edit?usp=sharing</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">PCB Layout</h3><a id="user-content-pcb-layout" aria-label="Permalink: PCB Layout" href="#pcb-layout"></a></p>
<p dir="auto">There is a PCB layout design included in the Proteus workspace file. It is deliberately and mostly blank with the exception of adding placement for the external address/data/RW lines, clock, external VBLANK (for IRQ triggering) and video signals output header and the associated resistor ladders. The dual sprite bit plane pull-up resistors are also placed near the edge of the sprites board. The sprite resistors and RAMs are placed to maintain an aesthetic symmetry to the board.</p>
<p dir="auto">There are six boards in the layout:
Video generation
Character screen
Sprites
Scaled sprites
Tiles
Mode7</p>
<p dir="auto">Previous versions used one large board, but this turned out to be expensive to produce and difficult to get quotations for. Apparently the companies I contacted were not that happy to produce large boards in small numbers. The design was changed to use four smaller boards, this reduces cost if someone wanted to just fabricate the video and character screen. Another benefit is that the layers can be changed and connected in different orders. This allows customisations, for example by having three sprites boards (layers 1 to 3) and one tile board (layer 4), giving a total of 72 sprites and tiles.</p>
<p dir="auto">The layout is configured with a six layer standard signal and power plane setup, using the technology template "All PCB Six Layer 1.58532mm.LTF" file. The layout will correctly populate with the auto-placer and default placement options, choose all then schedule before starting. It will then correctly route, again using the default router options. This is so placement can be mostly left to the optimiser while maintaining output video signal lines via the resistor placement.</p>
<p dir="auto">An entire place and route on a decent machine will take approximately 2 hours and result in an output image like:
<a target="_blank" rel="noopener noreferrer" href="https://github.com/martinpiper/BombJack/blob/master/Layout1.PNG?raw=true"><img src="https://github.com/martinpiper/BombJack/raw/master/Layout1.PNG?raw=true" alt="Layout" title="Layout"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/martinpiper/BombJack/blob/master/Layout2.PNG?raw=true"><img src="https://github.com/martinpiper/BombJack/raw/master/Layout2.PNG?raw=true" alt="Tracks" title="Tracks"></a></p>
<p dir="auto">The protoype boards have been built and documented here: <a href="https://www.youtube.com/playlist?list=PLAAYJEX1JbCPFzQDl8KOoVQR9_Us1lB46" rel="nofollow">https://www.youtube.com/playlist?list=PLAAYJEX1JbCPFzQDl8KOoVQR9_Us1lB46</a> or this project page: <a href="https://martin-piper.itch.io/bomb-jack-display-hardware" rel="nofollow">https://martin-piper.itch.io/bomb-jack-display-hardware</a></p>
<p dir="auto">I use Proteus simulation and PCB design: <a href="https://www.labcenter.com/" rel="nofollow">https://www.labcenter.com/</a>
And PCBWay PCB manufacture: <a href="https://pcbway.com/g/Xr8ZrI" rel="nofollow">https://pcbway.com/g/Xr8ZrI</a>
( Using the PCBWay referral link gives you a discount and also helps me reduce the cost of my next board. )</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">References</h2><a id="user-content-references" aria-label="Permalink: References" href="#references"></a></p>
<ul dir="auto">
<li><a href="https://github.com/mamedev/mame/blob/master/src/mame/drivers/bombjack.cpp">https://github.com/mamedev/mame/blob/master/src/mame/drivers/bombjack.cpp</a></li>
<li><a href="https://floooh.github.io/2018/10/06/bombjack.html" rel="nofollow">https://floooh.github.io/2018/10/06/bombjack.html</a></li>
<li><a href="https://www.arcade-museum.com/game_detail.php?game_id=7180" rel="nofollow">https://www.arcade-museum.com/game_detail.php?game_id=7180</a></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla owners file class-action alleging repair, parts monopoly (108 pts)]]></title>
            <link>https://driving.ca/auto-news/industry/tesla-class-action-lawsuite-parts-repairs-monopoly</link>
            <guid>40744847</guid>
            <pubDate>Fri, 21 Jun 2024 00:06:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://driving.ca/auto-news/industry/tesla-class-action-lawsuite-parts-repairs-monopoly">https://driving.ca/auto-news/industry/tesla-class-action-lawsuite-parts-repairs-monopoly</a>, See on <a href="https://news.ycombinator.com/item?id=40744847">Hacker News</a></p>
Couldn't get https://driving.ca/auto-news/industry/tesla-class-action-lawsuite-parts-repairs-monopoly: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[How babies and young children learn to understand language (210 pts)]]></title>
            <link>https://lithub.com/how-babies-and-young-children-learn-to-understand-language/</link>
            <guid>40744558</guid>
            <pubDate>Thu, 20 Jun 2024 23:19:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lithub.com/how-babies-and-young-children-learn-to-understand-language/">https://lithub.com/how-babies-and-young-children-learn-to-understand-language/</a>, See on <a href="https://news.ycombinator.com/item?id=40744558">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							        
									<p><span itemprop="articleBody"><p>How children learn language has long been of interest to those concerned with its evolution. The idea that ‘ontogeny recapitulates phylogeny’ has been promoted, which means the stages of child development on their way to adulthood replicate those of our human ancestors on their way to becoming modern humans. This idea has been applied to language acquisition and its evolution, but I’ve never been persuaded. It is intellectually problematic because our human ancestors were never ‘on their way’ to anywhere other than being themselves. My interest in language acquisition is different and twofold.</p><p><span>Article continues below</span></p>
<p>First, is language acquired by specialized mental processes that are dedicated to this task or learned by general-purpose processes used for a variety of learning tasks? Second, can we project the processes of language acquisition/learning that we observe in the present into the prehistoric past to gain insights about the evolution of language?</p>
<span>Infants a mere three days old prefer listening to the rhythm of their caregivers’ language.</span>
<p><strong>Learning words</strong></p>
<p>Let’s remind ourselves of the problem that language learners face. When we write, we leave spaces between words. Readingwordswithoutsuchspacesisdifficult. When we speak, however, our words flow from one to another without any pauses, generating a continuous stream of sound. Yet we hear and understand each word without difficulty—we automatically ‘parse’ the utterances we hear into their component parts. We can do this because we know not only the words and grammar of our language, but also its prosody—how syllables at the start or end of words are stressed and how pauses are used between phrases. Without any awareness of such knowledge, we use it effortlessly both when speaking and when listening; we use it to identify words as they appear within the continuous stream of sound we hear, just as if the words had been written down interspersed with spaces. One can appreciate this with a little reflection. Just think how you pronounce the syllable <em>ham </em>when referring to a piece of meat and when talking about a fury animal—a long <em>ham </em>and a short <em>ham</em>-ster. Think about listening to a language unknown to you, one with different words, grammar and prosody. You will be at an utter loss to identify its words, let alone their meaning.</p>
<p>How then, can babies learn language? Although they are born into the world without any knowledge of the language their caregivers will speak, some learning takes places within the womb. The reception of external speech sounds prepares the fetal brain for the processing of language once born. Languages have different rhythms and infants a mere three days old prefer listening to the rhythm of their caregivers’ language, indicating that they became familiarized to it before birth via the sounds heard or the vibrations felt within the womb.</p>
<p>Nevertheless, babies are born with no specific knowledge of the language they will hear. Yet whether that is English, Mandarin, Swahili or any other of the 7,000 or so languages that remain today, by around one year of age infants are saying their first words, and within another year they are stringing words together to make simple sentences by using the correct order for their native language—those acquiring/learning English and French will place verbs before objects, and those learning Turkish and Japanese will do the converse. Without any formal teaching, by a mere four years of age, most children have acquired more than 1,000 words, the rules for how words can be combined to generate meaningful utterances, and how to change the tone of an utterance to further flex its meaning.</p>
<p>Such language acquisition is truly remarkable. How it is achieved has been one of the major questions facing linguists ever since the study of language began, generating a constant flow of new ideas and evidence. We have already dismissed arguments about Universal Grammar in Chapter 3 and can now move on to more interesting and viable ideas. Within the voluminous literature there are two questions of particular interest for the language puzzle. The first is how infants find discrete words in the continuous stream of speech they hear. The second is how they learn the meaning of those words.</p>
<span>By a mere four years of age, most children have acquired more than 1,000 words.</span>
<p><strong>Finding words</strong></p>
<p>How can babies possibly discover where a word begins and ends within a continuous sound stream? A breakthrough in our understanding came in 1996 in a three-page article in the journal <em>Science </em>entitled “Statistical learning by 8-month-old infants,” authored by the psychologists Jenny Saffran, Richard Aslin and Elissa Newport. This explained that infants use “transitional probabilities” (TPs) between syllables to identify which syllable strings recurrently go together, and hence are likely to constitute words, and which syllables have low probabilities of following each other and hence are likely to mark the break between words. The phrase <em>pretty baby, </em>for instance, has four syllables (<em>pre-ty-ba-by</em>) and three transitional probabilities between syllable pairs. In English the probability that <em>ty </em>will be followed by <em>ba </em>is lower than <em>pre </em>will be followed by <em>ty, </em>and that <em>ba </em>will be followed by <em>by</em>. That eight-month-old-infants can calculate and use such transitional probabilities came as a surprise.</p>
<p>Jenny Saffran and her colleagues had exposed infants to no more than two minutes of continuous speech that contained four three-syllable nonsense words, such as <em>tupiro </em>and <em>padoti</em>. These ‘words’ were repeated in random order by a monotone speech synthesizer that created a continuous sound sequence, such as <em>bidakupadotigolabubidakupadotigolabubidakutupiro… </em>The sequence contained no pauses, variations in stress or any other acoustic cues between word boundaries. The only cue available to the infants were the transitional probabilities (TPs) between syllables. Those within words were 1.0, because the first syllable was always followed by the second, and the second by the third, while the TPs of syllables between words was always 0.33. After a mere two minutes of listening, the infants were tested as to whether they differentiated between words (syllable strings that had TPs of 1.0) and non-words (syllable strings that contained TPs of 0.33). For this the infants were presented with words and non-words and found to have longer listening time for the nonwords. This indicated they had already become familiar with the words by listening to the continuous sequence of syllables within which they had been embedded. The only way that could have happened was by monitoring the TPs between syllables—the infants were capable of statistical learning.</p>
<p>______________________________</p>
<p><img fetchpriority="high" decoding="async" data-attachment-id="238453" data-permalink="https://lithub.com/how-babies-and-young-children-learn-to-understand-language/the-language-puzzle/" data-orig-file="https://s26162.pcdn.co/wp-content/uploads/2024/06/the-language-puzzle.jpeg" data-orig-size="660,1024" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="the language puzzle" data-image-description="" data-image-caption="" data-medium-file="https://s26162.pcdn.co/wp-content/uploads/2024/06/the-language-puzzle-193x300.jpeg" data-large-file="https://s26162.pcdn.co/wp-content/uploads/2024/06/the-language-puzzle.jpeg" src="https://s26162.pcdn.co/wp-content/uploads/2024/06/the-language-puzzle-193x300.jpeg" alt="" width="193" height="300" srcset="https://s26162.pcdn.co/wp-content/uploads/2024/06/the-language-puzzle-193x300.jpeg 193w, https://s26162.pcdn.co/wp-content/uploads/2024/06/the-language-puzzle-39x60.jpeg 39w, https://s26162.pcdn.co/wp-content/uploads/2024/06/the-language-puzzle-32x50.jpeg 32w, https://s26162.pcdn.co/wp-content/uploads/2024/06/the-language-puzzle.jpeg 660w" sizes="(max-width: 193px) 100vw, 193px"></p>
<p><em>From</em> <a href="https://bookshop.org/a/132/9781541605381" target="_blank">The Language Puzzle: Piecing Together the Six-Million-Year Story of How Words Evolved</a> <em>by Steven Mithen. Copyright © 2024. Available from Basic Books, a division of Hachette Book Group, Inc.</em></p>
</span></p>
									
																		
																		
									<br><hr>
									
							    										
								</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Local voice assistant using Ollama, transformers and Coqui TTS toolkit (139 pts)]]></title>
            <link>https://github.com/mezbaul-h/june</link>
            <guid>40744293</guid>
            <pubDate>Thu, 20 Jun 2024 22:48:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mezbaul-h/june">https://github.com/mezbaul-h/june</a>, See on <a href="https://news.ycombinator.com/item?id=40744293">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">june-va</h2><a id="user-content-june-va" aria-label="Permalink: june-va" href="#june-va"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Local Voice Chatbot: Ollama + HF Transformers + Coqui TTS Toolkit</h2><a id="user-content-local-voice-chatbot-ollama--hf-transformers--coqui-tts-toolkit" aria-label="Permalink: Local Voice Chatbot: Ollama + HF Transformers + Coqui TTS Toolkit" href="#local-voice-chatbot-ollama--hf-transformers--coqui-tts-toolkit"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">OVERVIEW</h2><a id="user-content-overview" aria-label="Permalink: OVERVIEW" href="#overview"></a></p>
<p dir="auto"><strong>june-va</strong> is a local voice chatbot that combines the power of Ollama (for language model capabilities), Hugging Face Transformers (for speech recognition), and the Coqui TTS Toolkit (for text-to-speech synthesis). It provides a flexible, privacy-focused solution for voice-assisted interactions on your local machine, ensuring that no data is sent to external servers.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/mezbaul-h/june/blob/master/demo.gif"><img src="https://github.com/mezbaul-h/june/raw/master/demo.gif" alt="demo-text-only-interaction" data-animated-image=""></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Interaction Modes</h3><a id="user-content-interaction-modes" aria-label="Permalink: Interaction Modes" href="#interaction-modes"></a></p>
<ul dir="auto">
<li><strong>Text Input/Output:</strong> Provide text inputs to the assistant and receive text responses.</li>
<li><strong>Voice Input/Text Output:</strong> Use your microphone to give voice inputs, and receive text responses from the assistant.</li>
<li><strong>Text Input/Audio Output:</strong> Provide text inputs and receive both text and synthesised audio responses from the assistant.</li>
<li><strong>Voice Input/Audio Output (Default):</strong> Use your microphone for voice inputs, and receive responses in both text and synthesised audio form.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">INSTALLATION</h2><a id="user-content-installation" aria-label="Permalink: INSTALLATION" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Pre-requisites</h3><a id="user-content-pre-requisites" aria-label="Permalink: Pre-requisites" href="#pre-requisites"></a></p>
<ul dir="auto">
<li><a href="https://github.com/ollama/ollama">Ollama</a></li>
<li>Python <code>3.10+</code> (with <em>pip</em>)</li>
</ul>
<p dir="auto">You will also need the following native package installed on your machine:</p>
<div dir="auto" data-snippet-clipboard-copy-content="apt install portaudio19-dev  # requirement for PyAudio"><pre>apt install portaudio19-dev  <span><span>#</span> requirement for PyAudio</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">From Source</h3><a id="user-content-from-source" aria-label="Permalink: From Source" href="#from-source"></a></p>
<p dir="auto">To install directly from source:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/mezbaul-h/june.git
cd june
pip install ."><pre>git clone https://github.com/mezbaul-h/june.git
<span>cd</span> june
pip install <span>.</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">USAGE</h2><a id="user-content-usage" aria-label="Permalink: USAGE" href="#usage"></a></p>
<p dir="auto">Pull the language model (default is <code>llama3:8b-instruct-q4_0</code>) with Ollama first, if you haven't already:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ollama pull llama3:8b-instruct-q4_0"><pre>ollama pull llama3:8b-instruct-q4_0</pre></div>
<p dir="auto">Next, run the program (with default configuration):</p>

<p dir="auto">This will use <a href="https://ollama.com/library/llama3:8b-instruct-q4_0" rel="nofollow">llama3:8b-instruct-q4_0</a> for LLM capabilities, <a href="https://huggingface.co/openai/whisper-small.en" rel="nofollow">openai/whisper-small.en</a> for speech recognition, and <code>tts_models/en/ljspeech/glow-tts</code> for audio synthesis.</p>
<p dir="auto">You can also customize behaviour of the program with a json configuration file:</p>
<div dir="auto" data-snippet-clipboard-copy-content="june-va --config path/to/config.json"><pre>june-va --config path/to/config.json</pre></div>
<p dir="auto"><g-emoji alias="warning">⚠️</g-emoji> The configuration file is optional. To learn more about the structure of the config file, see the <a href="#configuration">Configuration</a> section.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><g-emoji alias="warning">⚠️</g-emoji> Regarding Voice Input</h3><a id="user-content-️-regarding-voice-input" aria-label="Permalink: ⚠️ Regarding Voice Input" href="#️-regarding-voice-input"></a></p>
<p dir="auto">After seeing the <code>Listening for sound...</code> message, you can speak directly into the microphone. Unlike typical voice assistants, there's no wake command required. Simply start speaking, and the tool will automatically detect and process your voice input. Once you finish speaking, maintain silence for 3 seconds to allow the assistant to process your voice input.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Voice Conversion</h3><a id="user-content-voice-conversion" aria-label="Permalink: Voice Conversion" href="#voice-conversion"></a></p>
<p dir="auto">Many of the models (e.g., <code>tts_models/multilingual/multi-dataset/xtts_v2</code>) supported by Coqui's TTS Toolkit support voice cloning. You can use your own speaker profile with a small audio clip (approximately 1 minute for most models). Once you have the clip, you can instruct the assistant to use it with a custom configuration like the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &quot;tts&quot;: {
    &quot;model&quot;: &quot;tts_models/multilingual/multi-dataset/xtts_v2&quot;,
    &quot;generation_args&quot;: {
      &quot;language&quot;: &quot;en&quot;,
      &quot;speaker_wav&quot;: &quot;/path/to/your/target/voice.wav&quot;
    }
  }
}"><pre>{
  <span>"tts"</span>: {
    <span>"model"</span>: <span><span>"</span>tts_models/multilingual/multi-dataset/xtts_v2<span>"</span></span>,
    <span>"generation_args"</span>: {
      <span>"language"</span>: <span><span>"</span>en<span>"</span></span>,
      <span>"speaker_wav"</span>: <span><span>"</span>/path/to/your/target/voice.wav<span>"</span></span>
    }
  }
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">CONFIGURATION</h2><a id="user-content-configuration" aria-label="Permalink: CONFIGURATION" href="#configuration"></a></p>
<p dir="auto">The application can be customised using a configuration file. The config file must be a JSON file. The default configuration is as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
    &quot;llm&quot;: {
        &quot;disable_chat_history&quot;: false,
        &quot;model&quot;: &quot;llama3:8b-instruct-q4_0&quot;
    },
    &quot;stt&quot;: {
        &quot;device&quot;: &quot;torch device identifier (`cuda` if available; otherwise `cpu`&quot;,
        &quot;generation_args&quot;: {
            &quot;batch_size&quot;: 8
        },
        &quot;model&quot;: &quot;openai/whisper-small.en&quot;
    },
    &quot;tts&quot;: {
        &quot;device&quot;: &quot;torch device identifier (`cuda` if available; otherwise `cpu`&quot;,
        &quot;model&quot;: &quot;tts_models/en/ljspeech/glow-tts&quot;
    }
}"><pre>{
    <span>"llm"</span>: {
        <span>"disable_chat_history"</span>: <span>false</span>,
        <span>"model"</span>: <span><span>"</span>llama3:8b-instruct-q4_0<span>"</span></span>
    },
    <span>"stt"</span>: {
        <span>"device"</span>: <span><span>"</span>torch device identifier (`cuda` if available; otherwise `cpu`<span>"</span></span>,
        <span>"generation_args"</span>: {
            <span>"batch_size"</span>: <span>8</span>
        },
        <span>"model"</span>: <span><span>"</span>openai/whisper-small.en<span>"</span></span>
    },
    <span>"tts"</span>: {
        <span>"device"</span>: <span><span>"</span>torch device identifier (`cuda` if available; otherwise `cpu`<span>"</span></span>,
        <span>"model"</span>: <span><span>"</span>tts_models/en/ljspeech/glow-tts<span>"</span></span>
    }
}</pre></div>
<p dir="auto">When you use a configuration file, it overrides the default configuration but does not overwrite it. So you can partially modify the configuration if you desire. For instance, if you do not wish to use speech recognition and only want to provide prompts through text, you can disable that by using a config file with the following configuration:</p>

<p dir="auto">Similarly, you can disable the audio synthesiser, or both, to only use the virtual assistant in text mode.</p>
<p dir="auto">If you only want to modify the device on which you want to load a particular type of model, without changing the other default attributes of the model, you could use:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &quot;tts&quot;: {
    &quot;device&quot;: &quot;cpu&quot;
  }
}"><pre>{
  <span>"tts"</span>: {
    <span>"device"</span>: <span><span>"</span>cpu<span>"</span></span>
  }
}</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration Attributes</h3><a id="user-content-configuration-attributes" aria-label="Permalink: Configuration Attributes" href="#configuration-attributes"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><code>llm</code> - Language Model Configuration</h4><a id="user-content-llm---language-model-configuration" aria-label="Permalink: llm - Language Model Configuration" href="#llm---language-model-configuration"></a></p>
<ul dir="auto">
<li><code>llm.device</code>: Torch device identifier (e.g., <code>cpu</code>, <code>cuda</code>, <code>mps</code>) on which the pipeline will be allocated.</li>
<li><code>llm.disable_chat_history</code>: Boolean indicating whether to disable or enable chat history. Enabling chat history will make interactions more dynamic, as the model will have access to previous contexts, but it will consume more processing power. Disabling it will result in less interactive conversations but will use fewer processing resources.</li>
<li><code>llm.model</code>: Name of the text-generation model tag on Ollama. Ensure this is a valid model tag that exists on your machine.</li>
<li><code>llm.system_prompt</code>: Give a system prompt to the model. If the underlying model does not support a system prompt, an error will be raised.</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto"><code>stt</code> - Speech-to-Text Model Configuration</h4><a id="user-content-stt---speech-to-text-model-configuration" aria-label="Permalink: stt - Speech-to-Text Model Configuration" href="#stt---speech-to-text-model-configuration"></a></p>
<ul dir="auto">
<li><code>tts.device</code>: Torch device identifier (e.g., <code>cpu</code>, <code>cuda</code>, <code>mps</code>) on which the pipeline will be allocated.</li>
<li><code>stt.generation_args</code>: Object containing generation arguments accepted by Hugging Face's speech recognition pipeline.</li>
<li><code>stt.model</code>: Name of the speech recognition model on Hugging Face. Ensure this is a valid model ID that exists on Hugging Face.</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto"><code>tts</code> - Text-to-Speech Model Configuration</h4><a id="user-content-tts---text-to-speech-model-configuration" aria-label="Permalink: tts - Text-to-Speech Model Configuration" href="#tts---text-to-speech-model-configuration"></a></p>
<ul dir="auto">
<li><code>tts.device</code>: Torch device identifier (e.g., <code>cpu</code>, <code>cuda</code>, <code>mps</code>) on which the pipeline will be allocated.</li>
<li><code>tts.generation_args</code>: Object containing generation arguments accepted by Coqui's TTS API.</li>
<li><code>tts.model</code>: Name of the text-to-speech model supported by the Coqui's TTS Toolkit. Ensure this is a valid model ID.</li>
</ul>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>