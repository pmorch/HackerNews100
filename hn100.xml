<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 15 Jan 2024 07:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[FedEx launches new e-commerce platform to compete with Amazon (138 pts)]]></title>
            <link>https://www.theverge.com/2024/1/14/24038042/fedex-fdx-e-commerce-platform-amazon-rival-shoprunner</link>
            <guid>38996120</guid>
            <pubDate>Mon, 15 Jan 2024 00:55:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/1/14/24038042/fedex-fdx-e-commerce-platform-amazon-rival-shoprunner">https://www.theverge.com/2024/1/14/24038042/fedex-fdx-e-commerce-platform-amazon-rival-shoprunner</a>, See on <a href="https://news.ycombinator.com/item?id=38996120">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>FedEx <a href="https://newsroom.fedex.com/newsroom/global-english/fedex-announces-first-of-its-kind-data-driven-commerce-platform">announced</a> today that it will launch a new “data-driven commerce platform” this fall <a href="https://go.redirectingat.com/?xs=1&amp;id=1025X1701640&amp;url=https%3A%2F%2Fwww.fedex.com%2Fen-us%2Fdigital-transformation.html">called fdx</a> that it says will give online merchants “end-to-end e-commerce solutions,” including data for supply chain management. The platform will build on what it offers with ShopRunner, an <a href="https://www.shoprunner.com/">online e-commerce store</a> the company <a href="https://www.forbes.com/sites/retailwire/2020/12/14/why-fedexs-shoprunner-deal-wont-help-much-with-its-amazon-problem/?sh=4b07f83b3911">bought in 2020</a>.</p><p>The company writes that fdx will combine new features with those already in ShopRunner, like enabling merchants to give estimated delivery dates to customers during shopping and ordering, or data on supply chain resources’ carbon impact. FedEx says when fdx launches in the fall, it will add “more efficient, cost-effective deliveries” using its data. FedEx’s system would also enable a “custom post-purchase experience” so brands can give customers more accurate shipment information.</p><div><p>The move appears aimed at competing with Amazon, a company FedEx has seen as <a href="https://www.cnbc.com/2019/09/18/fedex-now-calls-amazon-a-competitor.html">a threat to its business</a> for years. In 2019, FedEx <a href="https://www.theverge.com/2019/6/7/18656813/amazon-prime-fedex-express-delivery-logistics-network-contract-termination-usps-ups">declined to renew</a> a contract to fly Amazon cargo through FedEx Express. Later that year, Amazon <a href="https://www.forbes.com/sites/richardkestenbaum/2019/12/18/amazon-blocked-sellers-from-using-fedex-and-now-we-know-why/?sh=c009ae32da13">forbade its sellers from using FedEx</a> for Prime deliveries during the holidays, blaming declining performance — a ban it <a href="https://www.theverge.com/2020/1/14/21065900/amazon-ban-fedex-ground-shipping-lifted-holiday-season">lifted the next year</a>. </p></div><p>FedEx has been losing ground to Amazon, as has UPS, so much so that Amazon <a href="https://www.wsj.com/business/amazon-vans-outnumber-ups-fedex-750f3c04">made more home package deliveries</a> in the US in 2022 than either of them. That’s just a few years after the online retail giant built up a logistics operation that largely uses tightly controlled third-party contractors that Amazon <a href="https://www.theverge.com/2023/6/18/23765330/amazon-delivery-drivers-union-teamsters">insists aren’t its employees</a>.</p><div><p>At the same time, Amazon was continuing to build up its own logistics operation that uses a fleet of mostly <a href="https://www.theverge.com/2023/6/18/23765330/amazon-delivery-drivers-union-teamsters">tightly controlled third-party contractors</a> that it insists aren’t employees. That move has paid off, as the company now delivers <a href="https://www.wsj.com/business/amazon-vans-outnumber-ups-fedex-750f3c04">more packages than UPS or FedEx</a>.</p></div><p><em>The Verge</em> contacted FedEx for more information, but it did not immediately respond.</p><p><em><strong>Update January 14th, 2024, 3:43PM ET: </strong>Updated to add clarity.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When Random Isn't (234 pts)]]></title>
            <link>https://orlp.net/blog/when-random-isnt/</link>
            <guid>38994817</guid>
            <pubDate>Sun, 14 Jan 2024 21:56:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://orlp.net/blog/when-random-isnt/">https://orlp.net/blog/when-random-isnt/</a>, See on <a href="https://news.ycombinator.com/item?id=38994817">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

<time datetime="2024-01-10">2024-01-10</time>
<p>This post is an anecdote from over a decade ago, of which I lost the actual
code. So please forgive me if I do not accurately remember all the details. Some
details are also simplified so that anyone that likes computer security can
enjoy this article, not just those who have played World of Warcraft (although
the <a href="https://en.wikipedia.org/wiki/Venn_diagram">Venn diagram</a> of those two
groups likely has a solid overlap).</p>
<p>When I was around 14 years old I discovered <a href="https://en.wikipedia.org/wiki/World_of_Warcraft">World of
Warcraft</a> developed by Blizzard
Games and was immediately hooked. Not long after I discovered add-ons which allow
you to modify how your game’s user interface looks and works. However, not all
add-ons I downloaded did exactly what I wanted to do. I wanted more. So I went to
find out how they were made.</p>
<p>In a weird twist of fate, I blame World of Warcraft for me seriously picking up
programming. It turned out that they were made in the
<a href="https://www.lua.org/">Lua</a> programming language. Add-ons were nothing more than
a couple <code>.lua</code> source files in a folder directly loaded into the game. The
barrier of entry was incredibly low: just edit a file, press save and reload the
interface. The fact that the game loaded <em>your</em> source code and you could see it
running was magical!</p>
<p>I enjoyed it immensely and in no time I was only writing add-ons and was barely playing
the game itself anymore. I published <a href="https://www.wowinterface.com/downloads/author-207710.html">quite a few
add-ons</a> in the next
two years, which mostly involved copying other people’s code with some
refactoring / recombining / tweaking to my wishes.</p>
<h2 id="add-on-security"><a href="#add-on-security" aria-label="Anchor link for: add-on-security">Add-on security</a></h2>
<p>A thought you might have is that it’s a really bad idea to let users have fully
programmable add-ons in your game, lest you get bots. However, the system
Blizzard made to prevent arbitrary programmable actions was quite clever.
Naturally, it did nothing to prevent actual botting, but at least
regular rule-abiding players were fundamentally restricted to the automation
Blizzard allowed.</p>
<p>Most UI elements that you could create were strictly decorative or
informational. These were completely unrestricted, as were most APIs that
strictly gather information. For example you can make a health bar display
using two frames, a background and a foreground, sizing the foreground
frame using an API call to get the health of your character.</p>
<p>Not all API calls were available to you however. Some were protected so they
could only be called from official Blizzard code. These typically involved
the API calls that would move your character, cast spells, use items, etc.
Generally speaking anything that actually makes you perform an in-game action
was protected.</p>

<p>However, some UI elements needed to actually interact with the game itself, e.g.
if I want to make a button that casts a certain spell. For this you could
construct a special kind of button that executes code in a secure environment
when clicked. You were only allowed to create/destroy/move such buttons when not
in combat, so you couldn’t simply conditionally place such buttons underneath
your cursor to automate actions during combat.</p>
<p>The catch was that this <a href="https://wowwiki-archive.fandom.com/wiki/RestrictedEnvironment">secure
environment</a>
<em>did</em> allow you to programmatically set which spell to cast, but doesn’t
let you gather the information you would need to do arbitrary automation. All
access to state from outside the secure environment was blocked. There were
some information gathering API calls available to match the more accessible
in-game macro system, but nothing as fancy as getting skill cooldowns or
unit health which would enable automatic optimal spellcasting. </p>
<p>So there were two environments: an insecure one where you can get all
information but can’t act on it, and a secure one where you can act but can’t
get the information needed for automation.</p>
<h2 id="a-backdoor-channel"><a href="#a-backdoor-channel" aria-label="Anchor link for: a-backdoor-channel">A backdoor channel</a></h2>
<p>Fast forward a couple years and I had mostly stopped playing. My interests had
mainly moved on to more “serious” programming, and I was only occasionally
playing, mostly messing around with add-on ideas. But this secure environment kept
on nagging in my brain; I wanted to break it.</p>
<p>Of course there was third-party
software that completely disables the security restrictions from Blizzard, but
what’s the fun in that? I wanted to do it “legitimately”, using the technically
allowed tools, as a challenge.</p>

<p>So I scanned the secure environment allowed function list to see if I could smuggle any
information from the outside into the secure environment. It all seemed pretty
hopeless until I saw one tiny, innocent little function: <code>random</code>.</p>
<p>An evil idea came in my head: random number generators (RNGs) used in computers are almost
always <a href="https://en.wikipedia.org/wiki/Pseudorandom_number_generator">pseudorandom number generators</a>
with (hidden) internal state. If I can manipulate this state, perhaps I can use
that to pass information into the secure environment.</p>
<h2 id="random-number-generator-woes"><a href="#random-number-generator-woes" aria-label="Anchor link for: random-number-generator-woes">Random number generator woes</a></h2>
<p>It turned out that <code>random</code> was just a small shim around C’s
<a href="https://en.cppreference.com/w/c/numeric/random/rand"><code>rand</code></a>. I was excited! 
This meant that there was a single global random state that was shared in the
process. It also helps that <code>rand</code> implementations tended to be on the weak side.
Since World
of Warcraft was compiled with MSVC, the actual implementation of <code>rand</code> was as follows:</p>
<pre data-lang="c"><code data-lang="c"><span>uint32_t state;
</span><span>
</span><span>int </span><span>rand() {
</span><span>    state = state * </span><span>214013 </span><span>+ </span><span>2531011</span><span>;
</span><span>    </span><span>return </span><span>(state &gt;&gt; </span><span>16</span><span>) &amp; </span><span>0x7fff</span><span>;
</span><span>}
</span></code></pre>
<p>This RNG is, for the lack of a better word, shite. It is
a naked <a href="https://en.wikipedia.org/wiki/Linear_congruential_generator">linear congruential generator</a>,
and a weak one at that. Which in my case, was a good thing.</p>

<p>So let’s get to breaking this thing. Since the state is so laughably small
and you can see 15 bits of the state directly you can keep a full list of
all possible states consistent with a single output of the RNG and use
further calls to the RNG to eliminate possibilities until a
single one remains. But we can be significantly more clever.</p>
<p>First we note that the top bit of <code>state</code> never affects anything in this RNG.
<code>(state &gt;&gt; 16) &amp; 0x7fff</code> masks out 15 bits, after shifting away the bottom 16
bits, and thus effectively works mod $2^{31}$. Since on any update the new state
is a linear function of the previous state, we can propagate this modular form
all the way down to the initial state as $$f(x) \equiv f(x \bmod m) \mod m$$ for
any linear $f$.</p>
<p>Let $a = 214013$ and $b = 2531011$. We observe the 15-bit output $r_0, r_1$ of
two RNG calls. We’ll call the 16-bit portion of the RNG state that is hidden by
the shift $h_0, h_1$ respectively, for the states after the first and second
call. This means the state of the RNG after the first call is $2^{16} r_0 + h_0$
and similarly for $2^{16} r_1 + h_1$ after the second call. Then we have the following identity:</p>
<p>$$a\cdot (2^{16}r_0 + h_0) + b \equiv 2^{16}r_1 + h_1 \mod 2^{31},$$</p>
<p>$$ah_0 \equiv h_1 + 2^{16}(r_1 - ar_0) - b \mod 2^{31}.$$</p>
<p>Now let $c \geq 0$ be the known constant $(2^{16}(r_1 - ar_0) - b) \bmod 2^{31}$, then
for some integer $k$ we have</p>
<p>$$ah_0 = h_1 + c + 2^{31} k.$$</p>
<p>Note that the left hand side ranges from $0$ to $a (2^{16} - 1) \approx 2^{33.71}$.
Thus we must have $-1 \leq k \leq 2^{2.71} &lt; 7$. Reordering we get the following
expression for $h_0$:
$$h_0 = \frac{c + 2^{31} k}{a} + h_1/a.$$
Since $a &gt; 2^{16}$ while $0 \leq h_1 &lt; 2^{16}$ we note that the term $0 \leq h_1/a &lt; 1$.
Thus, assuming a solution exists, we must have
$$h_0 = \left\lceil\frac{c + 2^{31} k}{a}\right\rceil.$$</p>
<p>So for $-1 \leq k &lt; 7$ we compute the above guess for the hidden portion of
the RNG state after the first call. This gives us 8 guesses, after which we can
reject bad guesses using follow-up calls to the RNG until a single unique answer remains.</p>

<p>An example implementation of this process in Python:</p>
<pre data-lang="python"><code data-lang="python"><span>import </span><span>random
</span><span>
</span><span>A = </span><span>214013
</span><span>B = </span><span>2531011
</span><span>
</span><span>class </span><span>MsvcRng:
</span><span>    </span><span>def </span><span>__init__(self, state):
</span><span>        self.state = state
</span><span>        
</span><span>    </span><span>def </span><span>__call__(self):
</span><span>        self.state = (self.state * A + B) % </span><span>2</span><span>**</span><span>32
</span><span>        </span><span>return </span><span>(self.state &gt;&gt; </span><span>16</span><span>) &amp; </span><span>0x7fff
</span><span>
</span><span># Create a random RNG state we'll reverse engineer.
</span><span>hidden_rng = MsvcRng(random.randint(</span><span>0</span><span>, </span><span>2</span><span>**</span><span>32</span><span>))
</span><span>
</span><span># Compute guesses for hidden state from 2 observations.
</span><span>r0 = hidden_rng()
</span><span>r1 = hidden_rng()
</span><span>c = (</span><span>2</span><span>**</span><span>16 </span><span>* (r1 - A * r0) - B) % </span><span>2</span><span>**</span><span>31
</span><span>ceil_div = </span><span>lambda </span><span>a, b: (a + b - </span><span>1</span><span>) // b
</span><span>h_guesses = [ceil_div(c + </span><span>2</span><span>**</span><span>31 </span><span>* k, A) </span><span>for </span><span>k </span><span>in </span><span>range(-</span><span>1</span><span>, </span><span>7</span><span>)]
</span><span>
</span><span># Validate guesses until a single guess remains.
</span><span>guess_rngs = [MsvcRng(</span><span>2</span><span>**</span><span>16 </span><span>* r0 + h0) </span><span>for </span><span>h0 </span><span>in </span><span>h_guesses]
</span><span>guess_rngs = [g </span><span>for </span><span>g </span><span>in </span><span>guess_rngs </span><span>if </span><span>g() == r1]
</span><span>while </span><span>len(guess_rngs) &gt; </span><span>1</span><span>:
</span><span>    r = hidden_rng()
</span><span>    guess_rngs = [g </span><span>for </span><span>g </span><span>in </span><span>guess_rngs </span><span>if </span><span>g() == r]
</span><span>    
</span><span># The top bit can not be recovered as it never affects the output,
</span><span># but we should have recovered the effective hidden state.
</span><span>assert </span><span>guess_rngs[</span><span>0</span><span>].state % </span><span>2</span><span>**</span><span>31 </span><span>== hidden_rng.state % </span><span>2</span><span>**</span><span>31
</span></code></pre>
<p>While I did write the above process with a <code>while</code> loop, it appears to only ever
need a third output at most to narrow it down to a single guess.</p>
<h2 id="putting-it-together"><a href="#putting-it-together" aria-label="Anchor link for: putting-it-together">Putting it together</a></h2>
<p>Once we could reverse-engineer the internal state of the random number
generator we could make arbitrary automated decisions in the supposedly secure
environment. How it worked was as follows:</p>
<ol>
<li>
<p>An insecure hook was registered that would execute right before the secure
environment code would run.</p>
</li>
<li>
<p>In this hook we have full access to information, and make a decision as to
which action should be taken (e.g. casting a particular spell). This action
is looked up in a hardcoded list to get an index.</p>
</li>
<li>
<p>The current state of the RNG is reverse-engineered using the above process.</p>
</li>
<li>
<p>We predict the outcome of the next RNG call. If this (modulo the length
of our action list) does not give our desired outcome, we advance the RNG and
try again. This repeats until the next random number would correspond to our
desired action.</p>
</li>
<li>
<p>The hook returns, and the secure environment starts. It generates a “random”
number, indexes our hardcoded list of actions, and performs the “random” action.</p>
</li>
</ol>
<p>That’s all! By being able to simulate the RNG and looking one step ahead we could
use it as our information channel by choosing exactly the right moment to call
<code>random</code> in the secure environment. Now if you wanted to support a list of $n$
actions it would on average take $n$ steps of the RNG before the correct
number came up to pass along, but that wasn’t a problem in practice.</p>
<h2 id="conclusion"><a href="#conclusion" aria-label="Anchor link for: conclusion">Conclusion</a></h2>
<p>I don’t know when Blizzard fixed the issue where the RNG state is so weak and
shared, or whether they were aware of it being an issue at all. A few years
after I had written the code I tried it again out of curiosity, and it had
stopped working. Maybe they switched to a different algorithm,
or had a properly separated RNG state for the secure environment.</p>
<p>All-in-all it was a lot of effort for a niche exploit in a video game that I
didn’t even want to use. But there certainly was a magic to manipulating
something supposedly random into doing exactly what you want, like a magician
pulling four aces from a shuffled deck.</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fast RISC-V-based scripting back end for game engines (117 pts)]]></title>
            <link>https://github.com/fwsGonzo/rvscript</link>
            <guid>38993552</guid>
            <pubDate>Sun, 14 Jan 2024 19:24:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/fwsGonzo/rvscript">https://github.com/fwsGonzo/rvscript</a>, See on <a href="https://news.ycombinator.com/item?id=38993552">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:fwsGonzo/rvscript" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="Q9j7jAyJFs8qe_D6_Jea4Qq0WigkqYTe6-lBnw9MGQREA0BPkAaIkrUmLoPAV1NFOOWBoEsXwhe6oIsBv6BWEw" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="fwsGonzo/rvscript" data-current-org="" data-current-owner="fwsGonzo" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=jhNmprIm1F7mONV3aa7pH0NqNDz8bUROjb6r1eBa3evOR0AcQMroNlt%2FmtDSdEX9tzGJEzAWmIvx%2F2kd91CO2prkNWqBbvoeu4miHxlhSAEvula0CfwixE1Rzo6L5wP9fG13xfaem9BvT1Z3uo1WLMOQp7bp0SQnqG256%2Fv0dbtS%2Fnkma42sw4wVu0TT7rJ%2FCXN953%2FyWwsWUiMwmMrW4rzach1oGZqCquFoSlZvjgERMkn7BvtvKBEs3iGKwsIznNzqL3mp6rsFyeyo9FEMS1TJjOTbN43rdVJ1nfqLeJXGmYC9VN5cfRKXI3QLhtSvCduR4RH%2BKov9yOemBFZKUDX60X3Um7We4ENbWkmUT4vYY7ut3Rhw%2FT2eZzJQAi3dkbSiWj6lpKAbAUfNuCFnl6ZFwytsZE5qFD%2FtFSs2a4Tm5uxj%2FbZOQOgcFd%2BlPtJm%2BaCqrJkNlfA1MrIrgz9lNCA8NrJJgpZKTEMhFA%2BxeSNsmgvSB4xHlEVsAFDGMEB2YbDYnxb40H7%2FE%2BWiCg4%3D--WBArAvnqU0wgzsv9--g9OKnzWnXvI64mAfCxqrkw%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=fwsGonzo%2Frvscript" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/fwsGonzo/rvscript&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="234b447dc3538c88f048c947bd295d03174c89fbe28720dafb2c1d5d8df64778" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Material Files – Open Source Material Design File Manager for Android (121 pts)]]></title>
            <link>https://github.com/zhanghai/MaterialFiles</link>
            <guid>38992689</guid>
            <pubDate>Sun, 14 Jan 2024 18:06:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/zhanghai/MaterialFiles">https://github.com/zhanghai/MaterialFiles</a>, See on <a href="https://news.ycombinator.com/item?id=38992689">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:zhanghai/MaterialFiles" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="a2W7ThVu9kY_-O8YfaSICbws2p8O_-a48SPPa5LcGw7SSZ4Nyi_44R76d2yVYw-U1CSG2Uj9zcKaPcrRQVXLTQ" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="zhanghai/MaterialFiles" data-current-org="" data-current-owner="zhanghai" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=uY0LL5IP1vOgeZIGiB0FUxYLjWxnhrla%2BlMFGwof%2FxziquWCb6oTqKeMJPtpc3maw6Swz8bXL7rKcbDlnrbOKDOL%2FOAH20xiVbDuZFrhYo%2FQqz3VCewxLkL%2BNw7ShwPt8BCsW9Om%2F9wg%2B4Y6KxaQe4uL6Pr4rD8fOe12VdfXmGImR17QRItgjypOGoWLj9k%2FGjTEbug0f7Fc%2FhpWpRExmEvjfaB8uxqFzd%2BGKnYnyjncFunD5Bb%2BLzfnWdE2AXQfNUVroP0oNAq%2Bnh84sKtao3mSK0w6PeZkcDgdR4p9ox%2F97uNB%2FaB9GGrQbHrrTB0Ptk%2BlQ0iCyhM0RL45diuNj%2B9qua8ekvSNFMfd1jpppaXtk52Go2gSw7AR1LMNTFk7p7yVKOqHbt6sbV3pqDqeumLGAUWxF2UgoYQ7DKeh5K1Pm2cXs%2B0Tv4w1AZygXN60PHKguDIFPpYPiu6yHAE%2BVJy2LEXt0JNifBWTyAqw6cvXBTarTCyDPudZeqYQa3hGwoTE2hMyK50oAU4FMrfyYGq1IMbBYw%3D%3D--RHse27a57aj83CwT--xQUykQGvs0WiFNZXMp9vFQ%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=zhanghai%2FMaterialFiles" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/zhanghai/MaterialFiles&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="d2f7786186db5279b58eb7cd189ad8ea1082cf4201ed607df0bd8d62506a6d48" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vanna.ai: Chat with your SQL database (392 pts)]]></title>
            <link>https://github.com/vanna-ai/vanna</link>
            <guid>38992601</guid>
            <pubDate>Sun, 14 Jan 2024 17:58:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/vanna-ai/vanna">https://github.com/vanna-ai/vanna</a>, See on <a href="https://news.ycombinator.com/item?id=38992601">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:vanna-ai/vanna" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="sy62AniNONQrjyQVd791PLl3mu3WH4wPxwcABJpLiSnvmoEAVTiem7156VG_exr3OKfeTflOHmvhA-mrIQb5nw" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="vanna-ai/vanna" data-current-org="vanna-ai" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=15%2B3PTD%2BSJCtIe5UHotNVErUjE3XWPrbHARXwU%2FSXt2qHGtwf71%2BF955wSfg5Gar5QDJifIT2kG55JfQcFcKVg%2Fx7POVW5QFudPjpnGbgZR7P1Ncy2tsMOmtWAB%2B5UXM8qrHZ4hou4v6zb1nGcdaPPqoaXNgLN9F9EsDy9hOdwEb4aLKyARQy5USWO7dbLlFJOPGHMo%2BNrJMQbiNKjuK521efDevj5PYcx77zvK9b%2FyCfRmK8dGXXWssBVYSusC9Ko%2F4tQ8mCqJEdEZBgkEEjrf2wcSktD1iMH2XUxe%2B7CcOwamClZBwmgLVz%2FItFTLF96aTo7AI%2BQKxL2sYycO%2BewHczoetJDcOTjiEg1%2FlQZiByFzqe%2Fc3hZI3Tj5fSKeDjleKP7kPZEo%2FWmxZLMV4Qi%2F%2BWY11gNq38V9oIPh%2Bh429etQhARFmlm8%2FpvqSTcNt0oExCEpszTI0yiRr7Q5ImhrrvAWvBQdRogk5xSvNTTWFNTIdEvfwZXvNSLm5FPwxSouVRhXCap1LPw%3D%3D--iOw7gg8jCFnNEDRZ--5xXUnQh7xRuEpWQNz%2B2V5g%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=vanna-ai%2Fvanna" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/vanna-ai/vanna&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="c214f09fb68fb4fbb987278020c7e49ca34bf8d7b10fb350d2c39c9d973f369b" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Project Bluefin: an immutable, developer-focused, Cloud-native Linux (158 pts)]]></title>
            <link>https://www.ypsidanger.com/announcing-project-bluefin/</link>
            <guid>38992292</guid>
            <pubDate>Sun, 14 Jan 2024 17:23:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ypsidanger.com/announcing-project-bluefin/">https://www.ypsidanger.com/announcing-project-bluefin/</a>, See on <a href="https://news.ycombinator.com/item?id=38992292">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
				<p>Today we "relaunched" <a href="https://github.com/ublue-os/bluefin?ref=ypsidanger.com">ublue-os/bluefin</a> as <a href="https://projectbluefin.io/?ref=ypsidanger.com">projectbluefin.io</a>. This is a beta until Spring 2024.  </p><p>Bluefin is a custom image of Fedora Silverblue by a bunch of cloud-native nerds. We want a reliable desktop experience that runs everything but we're too lazy to maintain anything. So we automated the entire delivery pipeline in GitHub. </p><figure><img src="https://www.ypsidanger.com/content/images/2023/09/BLUEFIN_21_9.png" alt="" loading="lazy" width="2000" height="844" srcset="https://www.ypsidanger.com/content/images/size/w600/2023/09/BLUEFIN_21_9.png 600w, https://www.ypsidanger.com/content/images/size/w1000/2023/09/BLUEFIN_21_9.png 1000w, https://www.ypsidanger.com/content/images/size/w1600/2023/09/BLUEFIN_21_9.png 1600w, https://www.ypsidanger.com/content/images/size/w2400/2023/09/BLUEFIN_21_9.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><span>Say hello to Bluefin. She is a Deinonychus antirrhopus, "terrible claw". Artwork by Andy Frazer.</span></figcaption></figure><p>Originally Bluefin was "<a href="https://github.com/castrojo/ublue?ref=ypsidanger.com">Fedora Silverblue for Ubuntu Expatriates</a>". I was migrating to Fedora at the time and wanted Silverblue but with a more Ubuntu-like desktop: a dock and appindicators. </p><p>Over time it became apparent that it could stand on its own as an alternative for people who are tired of Linux desktops that aren't as reliable as cheap Chromebooks. We were tired of this shitty situation so decided to add a bit o' shine to Fedora Silverblue and maintain it as a clean, atomic layer on top of the default image. It's <a href="https://www.youtube.com/live/cHYyGVOae84?si=-TdsDiY0xtqgbeuu&amp;t=2370&amp;ref=ypsidanger.com" rel="noreferrer">not a distribution</a>, since you can always revert back to a stock image. </p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/Nz-yyDwTfRM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" title="Bluefin Linux introduction"></iframe><figcaption><p><span>Justin Garrison gives us a quick tour!</span></p></figcaption></figure><p>So she became Bluefin, which coincidentally was the name of Canonical's office building on the Thames in London. Since it's basically an Ubuntu-style workflow on top of Fedora this seemed to make sense and scratch the easter egg itch. </p><p>So after about two years of prototyping and real world usage, we think she's ready for more people to try. We're calling this a Beta, with the hope of going GA in the spring. </p><p>Since she's built on <a href="https://universal-blue.org/?ref=ypsidanger.com">Universal Blue</a> we get all the <a href="https://universal-blue.org/?ref=ypsidanger.com#advantages-over-traditional-linux-desktops" rel="noreferrer">benefits</a> of the main images. While it can run on any PC, we also have specific images for Framework, Asus, and Microsoft Surface devices. Work is underway to generate images based on the work of the <a href="https://fedoraproject.org/wiki/SIGs/Asahi?ref=ypsidanger.com" rel="noreferrer">Fedora Asahi SIG</a>, so hopefully soon we'll have something for Mac users. </p><p>Updates are automatic and transparent, with built in drivers. It can do anything a Chromebook can, but can <a href="https://flathub.org/?ref=ypsidanger.com">use flathub packages</a> for applications, and crucially, a choice of browsers. And since a container runtime is included, it also means you can run just about any Linux workload. </p><figure><img src="https://www.ypsidanger.com/content/images/2023/09/bluefin-defaultish.png" alt="" loading="lazy" width="2000" height="1333" srcset="https://www.ypsidanger.com/content/images/size/w600/2023/09/bluefin-defaultish.png 600w, https://www.ypsidanger.com/content/images/size/w1000/2023/09/bluefin-defaultish.png 1000w, https://www.ypsidanger.com/content/images/size/w1600/2023/09/bluefin-defaultish.png 1600w, https://www.ypsidanger.com/content/images/2023/09/bluefin-defaultish.png 2256w" sizes="(min-width: 720px) 720px"><figcaption><span>Wallpapers by Jacob Schnurr, every season represents the ecology of our open source ecosystems</span></figcaption></figure><p>Major upgrades are handled at the CI level by our <a href="https://devboard.gitsense.com/ublue-os/bluefin?ref=ypsidanger.com">contributors</a>, and everything is set up to run automatically. We ingest Fedora, apply our changes, and then ship the OCI image to you every day. An image-based desktop with the least amount of end-user maintenance as we can automate. Since it's just a container you can <a href="https://universal-blue.discourse.group/docs?topic=43&amp;ref=ypsidanger.com" rel="noreferrer">fork to your heart's content</a> to make your own adjustments, or start from scratch. </p><p>We intend to use <a href="https://cockpit-project.org/?ref=ypsidanger.com">Cockpit</a> to act as fleet management. It is a powerful tool, however it's not set up out of the box, we hope to work on this over time as it's a complex issue, but it's also a problem to look forward to!</p><h2 id="taking-care-of-developers">Taking Care of Developers</h2><p>With the normal use case taken care of let's look at ourselves. Fedora Silverblue variants come with a container runtime, <a href="https://podman.io/?ref=ypsidanger.com">Podman</a>. This means that we can basically run anything we want. We include <a href="https://distrobox.privatedns.org/?ref=ypsidanger.com">distrobox</a> for an interactive experience so that users can use whatever distribution image they want as their day-to-day.  </p><p>But we needed a bit more oomph, so doing a <code>just devmode-on</code> in a terminal will switch you to <code>bluefin-dx</code>, our <a href="https://universal-blue.discourse.group/docs?topic=39&amp;ref=ypsidanger.com" rel="noreferrer">developer image</a>. In here you'll find Visual Studio Code with <a href="https://containers.dev/?ref=ypsidanger.com">devcontainers</a>, <a href="https://www.jetpack.io/devbox?ref=ypsidanger.com">devbox</a> with Fleek for nix, <a href="https://devpod.sh/?ref=ypsidanger.com">devpod</a>, and homebrew. And we toss in some shortcuts to install <a href="https://www.jetbrains.com/toolbox-app/?ref=ypsidanger.com">Jetbrains Toolbox</a>. </p><p>We also take care of the underlying stuff that you need, like LXD/LXC, KVM, and virt-manager. <a href="https://linuxcontainers.org/incus/introduction/?ref=ypsidanger.com">Incus</a> is available as a tech preview. And lastly we include Docker on the image so you can <code>just docker</code> yourself to a more familiar place if you prefer to use it. </p><figure><img src="https://www.ypsidanger.com/content/images/2023/10/image.png" alt="" loading="lazy" width="903" height="695" srcset="https://www.ypsidanger.com/content/images/size/w600/2023/10/image.png 600w, https://www.ypsidanger.com/content/images/2023/10/image.png 903w" sizes="(min-width: 720px) 720px"><figcaption><span>VSCode comes ready with devcontainers out of the box</span></figcaption></figure><p>You can swap your userspace as you see fit. By default it's Bluefin/Ubuntu, but I've been enjoying Bluefin/Alpine for it's speed and small size. At some point I'll migrate to using <a href="https://github.com/wolfi-dev?ref=ypsidanger.com">Wolfi images</a> too. </p><p>Use whatever you want, or stick to the stock Fedora images. We purposely don't dictate a developer workflow, whatever works for you.  Homebrew is my preferred route and is what I recommend to people who are migrating to Bluefin DX. </p><p>We use the <a href="https://universal-blue.org/guide/just?ref=ypsidanger.com">just task runner</a> to ship a bunch of convenience features (and workarounds too). This has started to become a great method for our community to ship shortcuts to each other. For example one contributor submitted <code>just ml-box</code> - a one command shortcut to get pytorch up and running. When you look at what it can do vs. setting up Nvidia drivers and container support by hand it becomes clear that enabling contributors to "ship" directly to developers is a powerful pattern. </p><h2 id="sustainability-of-our-open-source-contributors">Sustainability of our Open Source Contributors</h2><p>And lastly, we wanted to make something lightweight from a contribution perspective. Universal Blue is Containerfiles with Python/Shell. It's a common toolset that tons of people know, and with our community's cloud-native knowledge we were able to fully automate much of the toil in maintaining your computer. I have not seen an "upgrade screen" on any of my computers for over two years, Bluefin quietly hums in CI/CD on GitHub. You don't even know she's there. (Probably because we are being hunted). </p><p>We take our <a href="https://universal-blue.org/mission/?ref=ypsidanger.com">governance and procedures</a> seriously, and welcome contributions. I try to take the same amount of care and dedication that I do when contributing to CNCF projects. </p><p>After spending over five years in cloud-native, I feel like we finally have an alternative operating system using the methods we're all familiar with. It just wasn't available on the desktop, until Fedora made it happen! Now it's up to us to consume it, and improve it! </p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/YFXufAVdrw4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" title="2023 10 25 21 54 49"></iframe><figcaption><p><span>Here's the companion video to this blog post.</span></p></figcaption></figure><h2 id="a-fresh-start-for-the-next-generation">A fresh start for the next generation</h2><p>I hope you'll join us. We've already got lots of tech in here including kind, flux, helm, and kubectl. We intend to accelerate the consumption of cloud-native tech by acting as your SREs. </p><p>I'll be traveling to conferences over the next 18 months talking about sustainability of our open source ecosystems and developers. Those talks will be delivered via <a href="https://community.frame.work/t/custom-fedora-oci-images-for-framework-laptops/34253?u=jorge_castro&amp;ref=ypsidanger.com">Bluefin on a Framework 13</a>. After using Linux since 1998 I believe this to be the state of the art of the Linux desktop. You won't find a more powerful system with the least amount of work - so if you see me ask for the demo. </p><p>We've been dogfooding Bluefin in some form or another for about two years, and Universal Blue's images have been pulled over 2 million times, it feels mature enough to move the Beta. I don't think <code>bluefin-dx</code> will ever be finished because we'll be revving at the same pace as the rest of cloud-native development, so that should be fun!</p><p>She might nip at you on occasion with a paper cut, but she evolves quickly. Clever girl.  </p><figure><img src="https://www.ypsidanger.com/content/images/2023/10/image-3.png" alt="" loading="lazy" width="949" height="716" srcset="https://www.ypsidanger.com/content/images/size/w600/2023/10/image-3.png 600w, https://www.ypsidanger.com/content/images/2023/10/image-3.png 949w" sizes="(min-width: 720px) 720px"><figcaption><span>Find me at KubeCon North America for your very own bundle of joy!</span></figcaption></figure><h2 id="full-disclosure">Full Disclosure </h2><p>I started this project when I was on sabbatical. One of those "find yourself" ones where you're thinking about stuff like ... "Where does my career go from here?" The whole open source sustainability thing is bugging me, and I started working at the <a href="https://www.cncf.io/?ref=ypsidanger.com" rel="noreferrer">CNCF</a> to help fix this problem. </p><p>And my passion projected kinda ended up being a "hello world". What if we could use the Linux desktop to onboard people right into cloud native? There's way dumber ideas out there lol. Here's more color on how I'm thinking about sustainability in open source:</p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/fdieyfrJRVk?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" title="Looking at the Past to Understand our Future — Jorge Castro"></iframe><figcaption><p><span>Here's me trying to explain all this in 15 minutes! It also explains why Bluefin is a dinosaur, she's trying to find her place in the ecosystem. </span></p></figcaption></figure><h2 id="finishing-the-beta">Finishing the Beta</h2><p>There's a few things that need to be sorted over the next six months. Mostly it involves the installation experience. It's hard to integrate code that doesn't exist so we're still waiting for Fedora's installer to become more robust at handling OCI installation. I'd like to personally thank the developers at Fedora and Red Hat for their support in working on these features, we're pretty confident that the installation process will get much better in the next six months. Luckily installation is the roughest part, once you're past that, it should be smooth sailing!</p><p>There are also lots of little paper cuts here and there. For example we don't automatically "detect and install Nvidia drivers" since that's not how these systems work, so you have to manually rebase to that image. That sort of thing.</p><p>Reliability is pretty great, the GitHub Package Registry keeps <a href="https://github.com/ublue-os/bluefin/pkgs/container/bluefin?ref=ypsidanger.com" rel="noreferrer">90 days of images</a> available so we can always boot off of any of those images if required, and due to the nature of the system it removes a bunch of the packaging complexity from the client entirely. </p><h2 id="appendix-their-stories">Appendix: Their Stories</h2><p><a href="https://www.etsy.com/listing/1425657775/cretaceous-chonkers-chonky-dinosaur?ref=ypsidanger.com">Jacob Schnurr</a> and <a href="https://www.etsy.com/uk/shop/dragonsofwales?ref=ypsidanger.com">Andy Frazer</a> brought Bluefin to life. If there's one lesson I've learned from this project is the technology is only one part of the equation. I purposely chose the dichotomy of paleo artists and operating systems at the leading edge of machine learning to be a statement on where we stand. A tool designed to make automation the most efficient it can possibly be, and yet unable to exist without the imagination of the human. </p><p>Here's some more shots of the gang in all the prehistoric glory ...</p><figure><img src="https://www.ypsidanger.com/content/images/2023/09/CustomChonk_Jorge_Deinonychus_Color3-1.png" alt="" loading="lazy" width="864" height="864" srcset="https://www.ypsidanger.com/content/images/size/w600/2023/09/CustomChonk_Jorge_Deinonychus_Color3-1.png 600w, https://www.ypsidanger.com/content/images/2023/09/CustomChonk_Jorge_Deinonychus_Color3-1.png 864w" sizes="(min-width: 720px) 720px"><figcaption><span>Deinonychus antirrhopus, "Bluefin" - This was the species that changed our view of dinosaurs</span></figcaption></figure><figure><img src="https://www.ypsidanger.com/content/images/2023/09/Jorge_CustomChonks_DimetrodonA.png" alt="" loading="lazy" width="1296" height="1296" srcset="https://www.ypsidanger.com/content/images/size/w600/2023/09/Jorge_CustomChonks_DimetrodonA.png 600w, https://www.ypsidanger.com/content/images/size/w1000/2023/09/Jorge_CustomChonks_DimetrodonA.png 1000w, https://www.ypsidanger.com/content/images/2023/09/Jorge_CustomChonks_DimetrodonA.png 1296w" sizes="(min-width: 720px) 720px"><figcaption><span>Dolly the Dimetrodon. They are actually a synapsid, not a dinosaur. You have more in common with Dolly than Dolly does to any of these dinosaurs. An incredible notion once you think about it!</span></figcaption></figure><figure><img src="https://www.ypsidanger.com/content/images/2023/09/Jorge_CustomChonks_Armagasaurusus_Post1.png" alt="" loading="lazy" width="1296" height="1296" srcset="https://www.ypsidanger.com/content/images/size/w600/2023/09/Jorge_CustomChonks_Armagasaurusus_Post1.png 600w, https://www.ypsidanger.com/content/images/size/w1000/2023/09/Jorge_CustomChonks_Armagasaurusus_Post1.png 1000w, https://www.ypsidanger.com/content/images/2023/09/Jorge_CustomChonks_Armagasaurusus_Post1.png 1296w" sizes="(min-width: 720px) 720px"><figcaption><span>Karl, represents the Developer Experience. When I'm in my Linux terminal I want to feel powerful and strong. With his help I shred through my backlog – issues, eliminated. Builds – green. Tests – passing. Karl also represents the raw power of Kubernetes, which is why we ship so many cloud-native tools by default.</span></figcaption></figure><figure><img src="https://www.ypsidanger.com/content/images/2023/09/Jorge_CustomChonks_NestingRaptor_Post1.png" alt="" loading="lazy" width="1296" height="1296" srcset="https://www.ypsidanger.com/content/images/size/w600/2023/09/Jorge_CustomChonks_NestingRaptor_Post1.png 600w, https://www.ypsidanger.com/content/images/size/w1000/2023/09/Jorge_CustomChonks_NestingRaptor_Post1.png 1000w, https://www.ypsidanger.com/content/images/2023/09/Jorge_CustomChonks_NestingRaptor_Post1.png 1296w" sizes="(min-width: 720px) 720px"><figcaption><span>Bluefin is a mother and takes care of her family, like we must take care of each other. Things that threaten her chicks end up meeting the other side of her personality ...</span></figcaption></figure><figure><img src="https://www.ypsidanger.com/content/images/2023/09/Jorge_CustomChonks_PivotRaptor_Post1.png" alt="" loading="lazy" width="1296" height="1296" srcset="https://www.ypsidanger.com/content/images/size/w600/2023/09/Jorge_CustomChonks_PivotRaptor_Post1.png 600w, https://www.ypsidanger.com/content/images/size/w1000/2023/09/Jorge_CustomChonks_PivotRaptor_Post1.png 1000w, https://www.ypsidanger.com/content/images/2023/09/Jorge_CustomChonks_PivotRaptor_Post1.png 1296w" sizes="(min-width: 720px) 720px"><figcaption><span>We call this one "murder chicken". Inspired by the raptor chase in S1E4 of Prehistoric Planet</span></figcaption></figure><figure><img src="https://www.ypsidanger.com/content/images/2023/10/Jorge_CustomChonks_PivotRaptor_BLMBlackGold_Post.png" alt="" loading="lazy" width="1296" height="1296" srcset="https://www.ypsidanger.com/content/images/size/w600/2023/10/Jorge_CustomChonks_PivotRaptor_BLMBlackGold_Post.png 600w, https://www.ypsidanger.com/content/images/size/w1000/2023/10/Jorge_CustomChonks_PivotRaptor_BLMBlackGold_Post.png 1000w, https://www.ypsidanger.com/content/images/2023/10/Jorge_CustomChonks_PivotRaptor_BLMBlackGold_Post.png 1296w" sizes="(min-width: 720px) 720px"><figcaption><span>And just like real life, she comes in all sorts of varieties. This one is "Murder Crow"</span></figcaption></figure><figure><img src="https://www.ypsidanger.com/content/images/2023/10/Jorge_CustomChonks_PivotRaptor_Pride_Post.png" alt="" loading="lazy" width="1296" height="1296" srcset="https://www.ypsidanger.com/content/images/size/w600/2023/10/Jorge_CustomChonks_PivotRaptor_Pride_Post.png 600w, https://www.ypsidanger.com/content/images/size/w1000/2023/10/Jorge_CustomChonks_PivotRaptor_Pride_Post.png 1000w, https://www.ypsidanger.com/content/images/2023/10/Jorge_CustomChonks_PivotRaptor_Pride_Post.png 1296w" sizes="(min-width: 720px) 720px"><figcaption><span>The team overwhelmingly fell in love with "Glitter Chicken". I'll be making a special Pride run of these at some point!</span></figcaption></figure><figure><img src="https://www.ypsidanger.com/content/images/2023/10/Jorge_CustomChonks_Kentrosaurus_Post1.png" alt="" loading="lazy" width="1296" height="1296" srcset="https://www.ypsidanger.com/content/images/size/w600/2023/10/Jorge_CustomChonks_Kentrosaurus_Post1.png 600w, https://www.ypsidanger.com/content/images/size/w1000/2023/10/Jorge_CustomChonks_Kentrosaurus_Post1.png 1000w, https://www.ypsidanger.com/content/images/2023/10/Jorge_CustomChonks_Kentrosaurus_Post1.png 1296w" sizes="(min-width: 720px) 720px"><figcaption><span>.. and lastly this Kentrasaurus - you don't want to mess with them.</span></figcaption></figure>
			</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ruffle: 2023 in Review (115 pts)]]></title>
            <link>https://ruffle.rs/blog/2024/01/14/2023-in-review</link>
            <guid>38991953</guid>
            <pubDate>Sun, 14 Jan 2024 16:44:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ruffle.rs/blog/2024/01/14/2023-in-review">https://ruffle.rs/blog/2024/01/14/2023-in-review</a>, See on <a href="https://news.ycombinator.com/item?id=38991953">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>It's been a very busy 2023 for Ruffle, so much so that we didn't find the time to write a new progress report with everything going on! Let's fix that!</p>
<p>Let's summarize with some numbers first.</p>
<p>Since the last blog post...</p>
<ul>
<li>ActionScript 3 Language has gone up from 60% to <strong>75%</strong>!</li>
<li>ActionScript 3 API has gone up from 60% to <strong>68%</strong>! (And it was only 25% at the start of the year!)</li>
<li>We've merged <strong>852</strong> pull requests from <strong>43</strong> people! (And 3 bots.)</li>
<li>We've closed <strong>1,288</strong> issues (and some of them weren't even duplicates or anything!)</li>
</ul>
<hr>
<h3 id="display-filters">Display Filters</h3>
<p>We've now implemented 7 out of 10 of Flash's filter effects, making content look much more accurate (and lots of text actually legible!)</p>
<p>Along with filter support, we've also implemented <code>cacheAsBitmap</code> support - a huge optimisation for games that use it, saving us the need to render the same thing every single frame!</p>
<p><a target="_blank" href="https://ruffle.rs/2024-01-14-2023-in-review/ninja-painter.png"><img src="https://ruffle.rs/2024-01-14-2023-in-review/ninja-painter.png" title="Ninja Painter, before and after filters" alt="Ninja Painter, before and after filters"></a></p>
<p><a target="_blank" href="https://github.com/ruffle-rs/ruffle/issues/15">The remaining 3 filters</a> don't seem to be used often, but we welcome anyone who wishes to come help implement them! Support for the existing filters was done by <a target="_blank" href="https://github.com/Dinnerbone">@Dinnerbone</a> and <a target="_blank" href="https://github.com/torokati44">@torokati44</a>.</p>
<h3 id="text-improvements">Text improvements</h3>
<p>On the topic of rendering, we've been picking and poking at fonts for a while now - and through our combined efforts we're finally starting to see the light at the end of this pixelated and wrongly shaped tunnel.</p>
<p><a target="_blank" href="https://github.com/Lord-McSweeney">@Lord-McSweeney</a> has implemented basic Text Layout Framework (TLF) support, which has started to get some more advanced text rendering working.</p>
<p><a target="_blank" href="https://github.com/kjarosh">@kjarosh</a> has also recently been working on making text inputs more functional, implementing text restrictions and better caret/selection rendering.</p>
<p><a target="_blank" href="https://github.com/Dinnerbone">@Dinnerbone</a> has created the framework for loading device fonts, instead of using the default Noto Sans we use everywhere. It's available on web (<a target="_blank" href="https://github.com/ruffle-rs/ruffle/wiki/Frequently-Asked-Questions-For-Users#how-do-i-change-the-fonts-used-by-flash-on-my-website">with some configuration</a>) but works out of the box on desktop thanks to <a target="_blank" href="https://github.com/evilpie">@evilpie</a>!</p>
<p>With device font support landing, content that didn't embed their own font will now start looking much better. Notably, lots of Japanese content often relied on this, and didn't render text at all previously.</p>
<p><a target="_blank" href="https://ruffle.rs/2024-01-14-2023-in-review/scratch.png"><img src="https://ruffle.rs/2024-01-14-2023-in-review/scratch.png" title="Scratch, before and after" alt="Scratch, before and after"></a>
<a target="_blank" href="https://ruffle.rs/2024-01-14-2023-in-review/material_sniper.png"><img src="https://ruffle.rs/2024-01-14-2023-in-review/material_sniper.png" title="Material Sniper" alt="Material Sniper"></a>
<a target="_blank" href="https://ruffle.rs/2024-01-14-2023-in-review/aq.png"><img src="https://ruffle.rs/2024-01-14-2023-in-review/aq.png" title="Adventure Quest, before and after" alt="Adventure Quest, before and after"></a>
<a target="_blank" href="https://ruffle.rs/2024-01-14-2023-in-review/spongebob.png"><img src="https://ruffle.rs/2024-01-14-2023-in-review/spongebob.png" title="SpongeBob SquarePants: Plunder Blunder, before and after" alt="SpongeBob SquarePants: Plunder Blunder, before and after device texts"></a></p>
<h3 id="sockets-even-on-the-web">Sockets... even on the web!</h3>
<p>This has definitely been one of the biggest hurdles to emulating Flash, but the incredible <a target="_blank" href="https://github.com/sleepycatcoding">@sleepycatcoding</a> came in and made it happen.</p>
<p>Any kind of multiplayer game such as Gaia, Habbo or Club Penguin relied on sockets (TCP sockets, or XML sockets) to function - and nobody was sure if we'd manage to get these working on the web. Modern browsers just don't like that sort of thing!</p>
<p>On desktop it's now supported out of the box (but will prompt you for permission by default, just to be sure), but web needs <a target="_blank" href="https://github.com/ruffle-rs/ruffle/wiki/Frequently-Asked-Questions-For-Users#how-can-i-connect-to-a-tcpsocket-or-xmlsocket-from-the-web">a little configuration</a> to make it work.</p>
<p><a target="_blank" href="https://ruffle.rs/2024-01-14-2023-in-review/gaia.png"><img src="https://ruffle.rs/2024-01-14-2023-in-review/gaia.png" title="Gaia's Towns 2" alt="Gaia's Towns 2"></a>
<video muted="" autoplay="" controls="">
<source src="https://ruffle.rs/2024-01-14-2023-in-review/transformice.mp4" type="video/mp4">
</video></p>
<h3 id="flash-remoting">Flash remoting</h3>
<p>In addition to sockets, another common way for online games to work was a protocol called "Flash Remoting", using <code>NetConnection</code>.
This was most notably seen in any game that uses the Armor Games API, and we're happy to say that that now works more or less as expected!</p>
<h3 id="flv-and-video-playback">FLV and video playback</h3>
<p>This has been a tricky one, but <a target="_blank" href="https://github.com/kmeisthax">@kmeisthax</a> has been tackling this all year and making amazing progress. Ruffle now supports FLV playback, depending on which codec is required, and <a target="_blank" href="https://github.com/danielhjacobs">@danielhjacobs</a> has added a workaround for patented codecs on the web by playing them using the browser over the top of the content (or on desktop, just opening the browser).</p>
<p>There's still a lot of progress to be made here, and <a target="_blank" href="https://github.com/torokati44">@torokati44</a> has a working prototype of decoding H.264 using Cisco's OpenH264 which is very exciting!</p>
<h3 id="initial-air-support">Initial AIR support</h3>
<p>This was a long time stretch goal that's starting to see reality thanks to <a target="_blank" href="https://github.com/Aaron1011">@Aaron1011</a>! We now properly version the ActionScript 3 API, and this has been extended to allow AIR-only classes and methods.</p>
<p>We don't have much to show for AIR stuff yet, but the framework to get us here has already helped close off many longstanding compatibility bugs when movies don't expect certain methods or properties to exist for the version of Flash they target.</p>
<h3 id="mixed-avm">Mixed AVM</h3>
<p>On the topic of longstanding bugs, <a target="_blank" href="https://github.com/Lord-McSweeney">@Lord-McSweeney</a> has been working on allowing mixed-avm movies to run correctly - these are usually ActionScript 3 games running inside ActionScript 2 containers, or vice versa.</p>
<p>There's still more to do here, but it's already helped unblock lots of content from just mysteriously failing to start!</p>
<h3 id="extension-improvements">Extension improvements</h3>
<p>The extension now uses Manifest V3 (somewhat reluctantly), which enables us to... keep existing. Unfortunately this came with needing to remove the "go to a swf url and play it in the browser" feature, as that's no longer possible with MV3.</p>
<p>However, the upside to this is that we're now <a target="_blank" href="https://microsoftedge.microsoft.com/addons/detail/ruffle/pipjjbgofgieknlpefmcckdmgaaegban">in the Edge store</a>! Also, thanks to work from <a target="_blank" href="https://github.com/kmeisthax">@kmeisthax</a> and <a target="_blank" href="https://github.com/danielhjacobs">@danielhjacobs</a>, we're also now available on Firefox for Android! <a target="_blank" href="https://addons.mozilla.org/android/addon/ruffle_rs">Check it out!</a></p>
<p><a target="_blank" href="https://github.com/WumboSpasm">@WumboSpasm</a> also redesigned the demo player in the extension, which <a target="_blank" href="https://github.com/danielhjacobs">@danielhjacobs</a> made serve as the replacement for the aforementioned swf-url feature. It looks pretty neat!</p>
<h3 id="desktop-ui">Desktop UI</h3>
<p>Whilst we had a super basic UI introduced in our last blog post, now it's even better! It's still as simple to use as before, but the new Advanced Open menu has many new options you can toggle to change how content plays - plus we've added a host of brand new debug tools that even Flash Player didn't have!</p>
<p><a target="_blank" href="https://ruffle.rs/2024-01-14-2023-in-review/ui-open-advanced.png"><img src="https://ruffle.rs/2024-01-14-2023-in-review/ui-open-advanced.png" title="Ruffle's Open Advanced menu" alt="Ruffle's Open Advanced menu"></a>
<a target="_blank" href="https://ruffle.rs/2024-01-14-2023-in-review/ui-debug.png"><img src="https://ruffle.rs/2024-01-14-2023-in-review/ui-debug.png" title="Ruffle's debug tools" alt="Ruffle's debug tools"></a></p>
<h3 id="new-website">New website</h3>
<p>Whilst not Ruffle itself, the website got some much-needed love from <a target="_blank" href="https://github.com/Dinnerbone">@Dinnerbone</a> with a total redesign. There's a lot more we'd like to do in the future with it, but web development isn't the specialty of any of our regular contributors... help is very much welcome!</p>
<p>We've also pulled in all the improvements to the Extension's demo player, and brought those over to the website too.</p>
<h3 id="too-many-contributions-to-call-out">Too many contributions to call out!</h3>
<p>There's been just so many PRs landing that not everything can take the spotlight here. We'd like to thank every single person who helped shape Ruffle in 2023, and hope that 2024 brings more great progress.</p>
<p>In addition, a big thank you to our sponsors who help keep the project alive. We appreciate you all!</p>
<p>&lt;3</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rethinking Diabetes – interview with Gary Taubes (102 pts)]]></title>
            <link>https://www.theguardian.com/society/2024/jan/14/unlocking-the-truth-about-diabetes-is-it-time-for-a-diet-based-treatment</link>
            <guid>38991521</guid>
            <pubDate>Sun, 14 Jan 2024 15:53:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/society/2024/jan/14/unlocking-the-truth-about-diabetes-is-it-time-for-a-diet-based-treatment">https://www.theguardian.com/society/2024/jan/14/unlocking-the-truth-about-diabetes-is-it-time-for-a-diet-based-treatment</a>, See on <a href="https://news.ycombinator.com/item?id=38991521">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p><span>G</span>ary Taubes is probably the most single-minded person I have ever met. In 2002, when he was a little-known science journalist and author of two books on scientific controversies, an article of his was published in the <em>New York Times</em>, headlined: <a href="https://www.nytimes.com/2002/07/07/magazine/what-if-it-s-all-been-a-big-fat-lie.html" data-link-name="in body link">What If It’s All Been a Big Fat Lie?</a> In it, he argued that the low-fat dietary advice of the previous couple of decades wasn’t only incorrect, but actively dangerous and the reason for, as he put it, the “rampaging epidemic of obesity in America”. For Taubes, dietary fat wasn’t a problem at all. Instead, the real danger was carbohydrate, he asserted, sparking a backlash, and fuelling the ongoing conversation about what constitutes a “healthy diet”. He wasn’t the first to assert that carbs were bad (<a href="https://www.theguardian.com/news/2003/apr/19/guardianobituaries.williamleith" data-link-name="in body link">Robert Atkins</a> got there before him), but perhaps because of his serious and scientific background – he has a physics degree from Harvard and studied aerospace engineering at Stanford – he has been a polarising figure, with as many ardent followers as detractors.</p><p>Since 2007, Taubes has published five books on sugar, fat and carbohydrate, including his latest, <em>Rethinking Diabetes</em>, in which he posits that low-carb diets have been under-used as a way to manage blood glucose in <a href="https://www.nhs.uk/conditions/type-1-diabetes/" data-link-name="in body link">type 1</a> and <a href="https://www.nhs.uk/conditions/type-2-diabetes/" data-link-name="in body link">type 2</a> diabetes, in favour of drug-heavy treatment regimes which, he suspects, may do more harm than good.</p><p>His writing on nutrition has won several awards, notably from the US National Association of Science Writers, but it has also been sharply criticised, mainly for his almost evangelical attachment to the keto diet, in which you eat so little carbohydrate (50g or less per day) that the body goes into a state called ketosis, meaning you stop burning stored glucose and start burning fat instead. In 2021, he published <em><a href="https://www.theguardian.com/books/2021/jan/03/the-case-for-keto-review-why-a-full-fat-diet-should-be-on-the-menu" data-link-name="in body link">The Case for Keto</a></em>, a self-help book, after which, as he says, he went “from being a respected source of information to somebody who may indeed be a crank after all”. Taubes, who follows the diet himself, is now proposing it should be offered to people living with diabetes. (Keto diets were originally developed as a way to treat certain types of childhood epilepsy.) For Taubes, keto means he doesn’t “eat starches, grains or sweets, and I don’t eat breakfast because I think better in the morning without it. When I snack, it’s nuts or good cheese. If we were to go to dinner together, I’d order a piece of fish or half a roast chicken and ask the waiter to hold the rice or potatoes and give me a green salad or green vegetables instead.”</p><p>When we talk, it’s a bright morning in Berkeley, California, where Taubes, 67, lives with his wife and sons. With his open-neck shirt, tan, salt-and-pepper short hair and slightly drawling delivery, he seems more like a professorial Owen Wilson than someone seeking to radically alter how diabetes is understood and treated.</p><p>The majority of his latest book is an exhaustive retelling of the history of diabetes research and how, in the first half of the 20th century, it went – as Taubes sees it – wrong, with the emergence of a treatment doctrine that mistakenly allowed people living with diabetes to eat whatever they wanted, all the while using insulin and drugs, such as <a href="https://www.nhs.uk/medicines/metformin/" data-link-name="in body link">metformin</a>, to manage the blood-glucose consequences. Taubes has always been fascinated by bad science and for him this was bad science of the highest order, because the regime was based on dietary hypotheses which he says had not – and still have not – been rigorously tested.</p><figure id="a272902e-81f8-4645-87d9-aa67f78243bb" data-spacefinder-role="showcase" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-2"><picture><source srcset="https://i.guim.co.uk/img/media/7746067c9b174a8fc4caf496a3af592bf49182d1/0_552_6496_3898/master/6496.jpg?width=880&amp;dpr=2&amp;s=none" media="(min-width: 1300px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 1300px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/7746067c9b174a8fc4caf496a3af592bf49182d1/0_552_6496_3898/master/6496.jpg?width=880&amp;dpr=1&amp;s=none" media="(min-width: 1300px)"><source srcset="https://i.guim.co.uk/img/media/7746067c9b174a8fc4caf496a3af592bf49182d1/0_552_6496_3898/master/6496.jpg?width=800&amp;dpr=2&amp;s=none" media="(min-width: 1140px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 1140px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/7746067c9b174a8fc4caf496a3af592bf49182d1/0_552_6496_3898/master/6496.jpg?width=800&amp;dpr=1&amp;s=none" media="(min-width: 1140px)"><source srcset="https://i.guim.co.uk/img/media/7746067c9b174a8fc4caf496a3af592bf49182d1/0_552_6496_3898/master/6496.jpg?width=640&amp;dpr=2&amp;s=none" media="(min-width: 980px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 980px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/7746067c9b174a8fc4caf496a3af592bf49182d1/0_552_6496_3898/master/6496.jpg?width=640&amp;dpr=1&amp;s=none" media="(min-width: 980px)"><source srcset="https://i.guim.co.uk/img/media/7746067c9b174a8fc4caf496a3af592bf49182d1/0_552_6496_3898/master/6496.jpg?width=620&amp;dpr=2&amp;s=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/7746067c9b174a8fc4caf496a3af592bf49182d1/0_552_6496_3898/master/6496.jpg?width=620&amp;dpr=1&amp;s=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/7746067c9b174a8fc4caf496a3af592bf49182d1/0_552_6496_3898/master/6496.jpg?width=605&amp;dpr=2&amp;s=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/7746067c9b174a8fc4caf496a3af592bf49182d1/0_552_6496_3898/master/6496.jpg?width=605&amp;dpr=1&amp;s=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/7746067c9b174a8fc4caf496a3af592bf49182d1/0_552_6496_3898/master/6496.jpg?width=445&amp;dpr=2&amp;s=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/7746067c9b174a8fc4caf496a3af592bf49182d1/0_552_6496_3898/master/6496.jpg?width=445&amp;dpr=1&amp;s=none" media="(min-width: 320px)"><img alt="Portrait by Cody Pickens ( His copyright, not commissioned) Gary Taubes is an investigative science and health journalist and co-founder of the non-profit Nutrition Science Initiative (NuSI.org)." src="https://i.guim.co.uk/img/media/7746067c9b174a8fc4caf496a3af592bf49182d1/0_552_6496_3898/master/6496.jpg?width=445&amp;dpr=1&amp;s=none" width="445" height="267.02740147783254" loading="lazy"></picture></div><figcaption><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>‘The science has been pretty awful. So many of the conceptions that evolved around nutrition are based on assumptions that may be wrong’: Gary Taubes</span> Photograph: Cody Pickens</figcaption></figure><p>Before the discovery of insulin in the 1920s, diet was the only way to manage diabetes and although various options were tried by early practitioners, low-carb was, says Taubes, among the most popular (with medics, at least). Insulin was a gamechanger. Not only did it almost magically save the lives of children with type 1 diabetes, who would often arrive at hospital comatose and die swiftly afterwards, but it also meant that people with diabetes of both types could eat a more or less normal diet.</p><p>Another example, for Taubes, of how early researchers were mistaken, concerns the differences between type 1 and type 2 diabetes, which are so different they almost shouldn’t share a name. Type 1 diabetes is an autoimmune disease in which the body’s immune system attacks and destroys the cells in the pancreas that produce insulin, the hormone which regulates the level of glucose in our blood; people living with type 1 need insulin injections or an insulin pump, to survive. Like type 2, type 1 can cause complications such as heart, kidney or eye disease, and nerve damage.</p><p>Type 2 diabetes accounts for about 90% of cases, and is a metabolic disorder in which the body either can’t make or can’t use insulin (AKA insulin resistance) to metabolise glucose, leading to persistently high levels in the blood. Ultimately, people living with type 2 diabetes may need insulin and other diabetes medications, too, but for a lot of people, diet and lifestyle modifications can defer that need. Many, but not all, specialists think there is often a causal relationship between weight and type 2 diabetes, which has led to a high level of stigma around the diagnosis. Diagnoses of both are rising globally – <a href="https://www.diabetes.org.uk/about-us/about-the-charity/our-strategy/statistics#:~:text=We%20estimate%20that%20more%20than,are%20yet%20to%20be%20diagnosed." data-link-name="in body link">five million people</a> live with diabetes in the UK.</p><p>What Taubes would like to see is low-carb diets being offered alongside or instead of diabetes medications. “When insulin therapy started in the 1920s, they had no idea what the long-term side-effects were or what the long-term consequences of living with diabetes were [because most people with type 1 died],” he says. “Then doctors find out that it’s just easier to let patients eat whatever they want and give them drugs to cover them. Then it’s another five, 10 or 20 years before they start seeing the long-term complications, which they think of as long-term complications of the disease.” What he wishes scientists at the time had concluded was: “The reason we’re keeping them alive is insulin therapy. So what we’re seeing is the long-term complications of the disease as controlled by insulin therapy, and the insulin therapy might be causing the complications as much as the disease is.</p><p>“By the late 1930s, you have this tidal wave of diabetic complications: the heart disease, the <a href="https://www.nhs.uk/conditions/atherosclerosis/" data-link-name="in body link">atherosclerosis</a>, the <a href="https://www.nhs.uk/conditions/peripheral-neuropathy/" data-link-name="in body link">neuropathy</a>, the kidney failure, the blindness, amputations. And nobody ties it back.” By then, the low-carb diet had fallen far from favour. “Nobody wants to eat a diet. So nobody’s being told: ‘Look, if I give you insulin, I’m going to keep you alive until you’re 30, especially if I give you a lot of insulin and you do eat your carbs. But if I tell you not to eat the carbs and we minimise the insulin use – which for type 2 could be no insulin – I might keep you alive as long as anyone else in your family.’”</p><p>In the book, which is laden with references, studies and dense historical detail, Taubes mentions case records from the 1700s in which patients on low-sugar diets beg for a medical solution, suggesting that the preference for medication over a highly prescriptive diet has been with us for a long time. “If you’re told, a pill or a diet, we all want the pill. But if you’re told a pill or a diet and the diet will keep you healthy and the pill will give you a chronic degenerative disorder where you’re still going to have these horrible complications, they are just going to be 20 to 30 years later… the pill is going to be easier, because it always is. But if you change the diet, it’s not a hypothetical change: you can put your diabetes into remission, you can stop taking these medications.”</p><p>Convincing as this sounds, there are some apparent flaws in Taubes’s arguments, which are by no means widely accepted in the academic or medical communities. <a href="https://www.ncl.ac.uk/magres/staff/profile/roytaylor.html#background" data-link-name="in body link">Professor Roy Taylor</a> is a leading British diabetes researcher. “When a subgroup of the UK <a href="https://www.gov.uk/government/groups/scientific-advisory-committee-on-nutrition#:~:text=SACN%20advises%20on%20nutrition%20and,and%20other%20UK%20government%20organisations." data-link-name="in body link">Government Scientific Advisory Committee on Nutrition</a> was convened in 2021 to look at low-carbohydrate as an approach to diabetes in general, the literature was very thoroughly assessed and I was part of that panel. Very low-carbohydrate diets had no better results than the modest reduction of carbohydrates,” he says. Other studies, such as one in 2022 at <a href="https://stanmed.stanford.edu/keto-diets-fall-short/#:~:text=In%20a%20head%2Dto%2Dhead,diet%20drastically%20restricts%20carbohydrate%20intake." data-link-name="in body link">Stanford</a> that compared low-carb diets and the Mediterranean diet, have shown that while they both work when it comes to controlling blood glucose, the Mediterranean diet is easier to stick to.</p><p>Second, low-carb diets are now offered as one way of managing diabetes of both types, the pendulum swinging back in their direction after almost a century, possibly more so here in the UK than in the US (Taubes doesn’t include contemporary case histories in the book). Two members of my own family have been put on a very low-carbohydrate diet in recent years when they were deemed at risk of developing type 2 diabetes in their 70s (a risk both of them reversed).</p><p>Jack Leeson, 55, was diagnosed with type 2 diabetes six years ago and on the advice of his NHS doctor, radically altered his diet. “She put me on the diabetes drug metformin and told me about people who lose limbs with it. So I was very motivated.” She didn’t suggest keto, “but she made clear the volume of sugars in bread and pasta, and supposedly healthy things like fruit juice, which is just sugar. I gave them up. I didn’t replace them with fat, just more protein, lots of vegetables, berries, soya milk and yoghurt, beans and lentils and pasta made from Japanese konjac root.” Leeson also does an hour of aerobic exercise every day. “I binned off the diabetes, cholesterol and blood pressure issues in 18 months and lost about 5st.”</p><p>Both <a href="https://www.diabetes.org.uk/" data-link-name="in body link">Diabetes UK</a> and <a href="https://www.diabetes.co.uk/" data-link-name="in body link">diabetes.co.uk</a>, the two biggest diabetes charities in the UK, have information on their sites about low-carb diets, particularly for people with type 2 diabetes. Diabetes UK, the larger charity, <a href="https://www.diabetes.org.uk/for-professionals/supporting-your-patients/clinical-recommendations-for-professionals/low-carb-diets-for-people-with-diabetes#:~:text=These%20diets%20can%20help%20with,approaches%20in%20the%20long%20term." data-link-name="in body link">states</a> that “there is no consistent evidence that a low-carb diet is any more effective than other approaches in the long term. So it shouldn’t be seen as the diet for everyone… At the moment, there is no strong evidence to say that a low-carb diet is safe or effective for people with type 1 diabetes. Because of this, we do not recommend low-carb diets to people with type 1 diabetes.” What Taubes would probably add is that there isn’t much evidence that they’re unsafe either, because low-carb diets haven’t been studied intensively either within or beyond the diabetes research community. But diet is incredibly difficult to study – especially in a context like diabetes where subjects are also often medicated. One of the tropes of nutritional science is that drawing long-term health conclusions from what people eat is nigh on impossible, because diet interacts with lifestyle, because people lie, intentionally or not, about what they eat, and because longitudinal studies of diet are so expensive.</p><p><a href="https://www.instagram.com/the_diabetes_monster/?hl=en" data-link-name="in body link">Munjeeta Sohal</a>, 39, was diagnosed with type 1 diabetes as a teenager and has mixed feelings about the idea it could or should be managed through carb restriction. “A low-carb diet might be a good way to control blood sugars,” she says, “but I am now on an insulin pump system that allows me complete freedom over what I eat. If I eat less carbs, my insulin stays more in range. I see the impact it has on my blood sugars, but that isn’t enough to make me want to do it full-time. My blood sugars are finally, thanks to the technology that’s available, in range between 70% and 90% of the time on an average day, and that is brilliant. I don’t need to eat low-carb for this to be the case.” Having diabetes is also a risk factor for eating disorders and Sohal’s relationship with food veered towards unhealthy when she was first diagnosed as a teen. Like many others she worries that “asking people to restrict may, for some, lead to secret bingeing, or guilt and shame around enjoying food.”</p><p>“They’re right, of course, to worry,” says Taubes. “But if diabetes, like obesity, is triggered in susceptible individuals by the carbohydrate content of the diet and can be put into remission by avoiding carbohydrate-rich foods, what would you tell patients?”</p><p>Another complication is that where Taubes is able to look at diabetes – indeed at diet as a whole – through a single, high-fat-low-carb lens, few others can. “I eat the same thing every day,” he says. “As long as I like it, I will continue to like it and be happy to eat it.” I suspect this makes it hard for him to understand why many of us have such a complicated relationship with cake.</p><p>While Taubes himself has stuck to keto for the last two decades, the rest of us might find it tough to follow. As Professor Taylor says: “The fall-off, in keto, is quite high. People have families and friends, and eating is part of social interaction.” Even in a highly motivated group, like people trying to control diabetes, adherence to low-carb is pretty patchy: a 2022 paper tracked this low adherence and pointed to cultural, religious and – perhaps most important – economic barriers. Keto can be expensive and labour-intensive, as well as socially awkward, at least in the beginning. (There are also some rare but potentially very serious health risks, says Taylor.)</p><p><a href="https://www.instagram.com/everything_endocrine/?hl=en-gb" data-link-name="in body link">Gregory Dodell</a>, a New York-based endocrinologist who takes a weight-inclusive approach to managing diabetes, says: “You have to look at the social determinants of health. We’re not treating a population as a research experiment, we’re treating a population with a lot of different complicated variables and issues and a very complex, multifactorial chronic condition. One size does not fit all.”</p><p>In conversation, Taubes isn’t quite as dogmatic about diet as his writing makes him seem. He regularly says things like, “assuming what I’m arguing is correct”, and at one point casually notes that he could “have a heart attack tomorrow, which is possible the way I eat, and which, God knows, I keep expecting”. He does use a lot of caveats in his books (which apparently drives his publisher slightly crazy), but on the page, he nonetheless comes across as unwaveringly committed to the high-fat, low-carb way of life, so I find his concern surprising. “Well, my world is full of people pointing out the age other people died who believe what I believe. Which, of course, is selection bias, because you don’t see the people who are still alive, you only see the people who die. If I die tomorrow, maybe I would have died 10 years ago, had I not eaten the way I did. It’s always an experiment.”</p><p>Given that there is no control version of any of us against which to measure success, none of us will ever know if we chose the “right” diet. The paradox, of course, is that this kind of diet-by-hypothesis is exactly what <em>Rethinking Diabetes</em> rails against. But Taubes sees no alternative: “The science has been pretty awful. So many of the conceptions that have evolved around eating behaviour and nutrition are based on assumptions that may be wrong. The problem is that people don’t change their [dietary or health] advice because the longer they give it, the more invested they are that it had better have been right. I write from this perspective – of the history – so folks can see the damage that is done by allowing assumptions to be embraced as facts without definitive evidence.”</p><p><em>Rethinking Diabetes – What Science Reveals About Diet, Insulin and Successful Treatments by Gary Taubes is published by Granta at £16.99 on 18 January. Buy a copy for £14.44 at <a href="https://guardianbookshop.com/rethinking-diabetes-9781803510699" data-link-name="in body link">guardianbookshop.com</a></em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Crystal 1.11.0 Is Released (105 pts)]]></title>
            <link>https://crystal-lang.org/2024/01/08/1.11.0-released/</link>
            <guid>38991392</guid>
            <pubDate>Sun, 14 Jan 2024 15:41:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://crystal-lang.org/2024/01/08/1.11.0-released/">https://crystal-lang.org/2024/01/08/1.11.0-released/</a>, See on <a href="https://news.ycombinator.com/item?id=38991392">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>We are announcing a new Crystal release with several new features and bug fixes.</p>

<p>Pre-built packages are available on <a href="https://github.com/crystal-lang/crystal/releases/tag/1.11.0">GitHub Releases</a>
and our official distribution channels.
See <a href="https://crystal-lang.org/install/">crystal-lang.org/install</a> for
installation instructions.</p>

<h2 id="stats">Stats</h2>

<p>This release includes <a href="https://github.com/crystal-lang/crystal/pulls?q=is%3Apr+milestone%3A1.11.0">178 changes since 1.10.1</a>
by 28 contributors. We thank all the contributors for all the effort put into
improving the language! ❤️</p>

<h2 id="changes">Changes</h2>

<p>Below we list the most remarkable changes in the language, compiler and stdlib.
This is a pretty big release with lots of things going on, so hold tight 🚀</p>

<p>For details, visit the <a href="https://github.com/crystal-lang/crystal/releases/tag/1.11.0">full changelog</a>.</p>

<h3 id="llvm-18">LLVM 18</h3>

<p>One of the biggest steps forward is support for upcoming LLVM 18 which allows linking LLVM dynamically on Windows (<a href="https://github.com/crystal-lang/crystal/pull/14101">#14101</a>).
Additionally, LLVM 18 now provides everything we need in the upstream C API, removing the need for our wrapper extension <code><span>llvm_ext</span></code>. It’s still necessary for older LLVM versions, so we’ll keep it around for a while. But the future tool chain is getting simplified. Read more in <a href="https://github.com/crystal-lang/crystal/issues/13946">#13946</a>.</p>

<p><em>Thanks <a href="https://github.com/HertzDevil">@HertzDevil</a></em></p>

<h3 id="compiler-optimization-levels">Compiler Optimization Levels</h3>

<p>The compiler gains four distinct optimization levels:</p>

<ul>
  <li><code><span>-</span><span>O0</span></code>: No optimization</li>
  <li><code><span>-</span><span>O1</span></code>: Low optimization</li>
  <li><code><span>-</span><span>O2</span></code>: Middle optimization</li>
  <li><code><span>-</span><span>O3</span></code>: High optimization</li>
</ul>

<p>Each level activates the respective LLVM <code><span>RunPasses</span></code> and <code><span>CodeGenOptLevel</span></code> optimizations.</p>

<p><code><span>-</span><span>O3</span></code> corresponds to the existing release mode and <code><span>-</span><span>O0</span></code> corresponds to the default non-release mode. <code><span>-</span><span>O0</span></code> remains the default and <code><span>--</span><span>release</span></code> is equivalent to <code><span>-</span><span>O3</span> <span>--</span><span>single</span><span>-</span><span>module</span></code>.</p>

<p>Effectively, this introduces two optimization choices between the previous full or nothing. And it’s now possible to use high optimization without <code><span>--</span><span>single</span><span>-</span><span>module</span></code>.</p>

<p>Read more in <a href="https://github.com/crystal-lang/crystal/pull/13464">#13464</a>.</p>

<p><em>Thanks <a href="https://github.com/kostya">@kostya</a></em></p>

<h3 id="alignment-primitives">Alignment primitives</h3>

<p>The language has two new reflection primitives: <a href="https://crystal-lang.org/reference/1.11/syntax_and_semantics/alignof.html"><code><span>alignof</span></code></a> and <a href="https://crystal-lang.org/reference/1.11/syntax_and_semantics/instance_alignof.html"><code><span>instance_alignof</span></code></a> return a type’s memory alignment (<a href="https://github.com/crystal-lang/crystal/pull/14087">#14087</a>). This allows implementing type-aware allocators in native Crystal with properly aligned pointers.
They are siblings of <code><span>sizeof</span></code> and <code><span>instance_sizeof</span></code> and can be used in the same way.</p>

<div><pre><code><span>class</span> <span>Foo</span>
  <span>def</span> <span>initialize</span><span>(</span><span>@x</span> <span>:</span> <span>Int8</span><span>,</span> <span>@y</span> <span>:</span> <span>Int64</span><span>,</span> <span>@z</span> <span>:</span> <span>Int16</span><span>)</span>
  <span>end</span>
<span>end</span>

<span>Foo</span><span>.</span><span>new</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>)</span>

<span>instance_alignof</span><span>(</span><span>Foo</span><span>)</span> <span># =&gt; 8</span>
</code></pre></div>

<div>
  
    <h4>Effect on existing code</h4>
  
  <p>The introduction of these primitives makes it impossible to define methods of the same names. So <code><span>def</span> <span>alignof</span></code> or <code><span>def</span> <span>instance_alignof</span></code> are now invalid syntax.
 We don’t expect there to be a big impact in practice.</p>
</div>

<p><em>Thanks <a href="https://github.com/HertzDevil">@HertzDevil</a></em></p>

<h3 id="dll-parameter-in-link-annotation"><code><span>dll</span></code> parameter in <code><span>Link</span></code> annotation</h3>

<p>The <a href="https://crystal-lang.org/api/1.11.0/Link.html"><code><span>Link</span></code></a> annotation has a new parameter <code><span>dll</span></code> for specifying dynamic link libraries on Windows (<a href="https://github.com/crystal-lang/crystal/pull/14131">#14131</a>).</p>

<div><pre><code><span>@[Link(dll: "foo.dll")]</span>
<span>lib</span> <span>LibFoo</span>
<span>end</span>
</code></pre></div>

<p><em>Thanks <a href="https://github.com/HertzDevil">@HertzDevil</a></em></p>

<h3 id="macro-caller-context">Macro <code><span>@caller</span></code> context</h3>

<p>Macros now have a reference to their calling context via the special instance variable <a href="https://crystal-lang.org/reference/1.11/syntax_and_semantics/macros/index.html#call-information"><code><span>@caller</span></code></a> (<a href="https://github.com/crystal-lang/crystal/issues/14055">#14055</a>).</p>

<div><pre><code><span>macro</span> <span>foo</span>
  <span>{{</span> <span>@caller</span><span>.</span><span>line_number</span> <span>}}</span>
<span>end</span>

<span>foo</span> <span># =&gt; 5</span>
</code></pre></div>

<p><em>Thanks <a href="https://github.com/Blacksmoke16">@Blacksmoke16</a></em></p>

<h3 id="new-collection-methods">New collection methods</h3>

<p><a href="https://crystal-lang.org/api/1.11.0/Enumerable.html#present?:Bool-instance-method"><code><span>Enumerable</span><span>#present?</span></code></a> is a direct inversion of <code><span>#empty?</span></code> avoiding some quirks with the similar, but not-quite, <code><span>#any?</span></code> (<a href="https://github.com/crystal-lang/crystal/issues/13847">#13847</a>).</p>

<p><em>Thanks <a href="https://github.com/straight-shoota">@straight-shoota</a></em></p>

<p><a href="https://crystal-lang.org/api/1.11.0/Enumerable.html#each_step(n:Int,*,offset:Int=0,&amp;:T-%3E):Nil-instance-method"><code><span>Enumerable</span><span>#each_step</span></code></a> and <a href="https://crystal-lang.org/api/1.11.0/Iterable.html#each_step(n:Int)-instance-method"><code><span>Iterable</span><span>#each_step</span></code></a> are direct methods for creating step iterators (<a href="https://github.com/crystal-lang/crystal/pull/13610">#13610</a>).</p>

<p><em>Thanks <a href="https://github.com/baseballlover723">@baseballlover723</a></em></p>

<p><a href="https://crystal-lang.org/api/1.11.0/Enumerable.html#to_set(&amp;block:T-%3EU):Set(U)forallU-instance-method"><code><span>Enumerable</span><span>(</span><span>T</span><span>)</span><span>#to_set(&amp; : T -&gt; U) : Set(U) forall U</span></code></a> and <a href="https://crystal-lang.org/api/1.11.0/Enumerable.html#to_a%28%26%3AT-%3EU%29%3AArray%28U%29forallU-instance-method"><code><span>#to_a(&amp; : T -&gt; U) forall U</span></code></a> allow materialising an <code><span>Enumerable</span></code> into a pre-defined collection, which gives more flexibility than the standard <code><span>#to_set</span></code> and <code><span>#to_a</span></code> methods (<a href="https://github.com/crystal-lang/crystal/pull/12654">#12654</a>, <a href="https://github.com/crystal-lang/crystal/pull/12653">#12653</a>).</p>

<p><em>Thanks <a href="https://github.com/caspiano">@caspiano</a></em></p>

<h3 id="numeric-enhancements">Numeric enhancements</h3>

<p><a href="https://crystal-lang.org/api/1.11.0/BigFloat.html#%2A%2A%28other%3ABigInt%29%3ABigFloat-instance-method"><code><span>BigFloat</span><span>#**</span></code></a> now works for all <code><span>Int</span><span>::</span><span>Primitive</span></code> arguments and supports the full exponent range for <code><span>BitInt</span></code> arguments (<a href="https://github.com/crystal-lang/crystal/pull/13971">#13971</a>, <a href="https://github.com/crystal-lang/crystal/pull/13881">#13881</a>)</p>

<p>Floating point to string conversion in <code><span>printf</span></code> uses the Ryu algorithm (<a href="https://github.com/crystal-lang/crystal/issues/8441">#8441</a>).</p>

<p>New methods <a href="https://crystal-lang.org/api/1.11.0/Float64.html#to_hexfloat:String-instance-method"><code><span>Float</span><span>::</span><span>Primitive</span><span>.</span><span>to_hexfloat</span></code></a>, <a href="https://crystal-lang.org/api/1.11.0/Float64.html#parse_hexfloat(str:String):self-class-method"><code><span>.</span><span>parse_hexfloat</span></code></a>, and <a href="https://crystal-lang.org/api/1.11.0/Float64.html#parse_hexfloat(str:String):self-class-method"><code><span>.</span><span>parse_hexfloat?</span></code></a> allow conversion to and from the hexfloat format (<a href="https://github.com/crystal-lang/crystal/pull/14027">#14027</a>).</p>

<p>More math features:</p>

<ul>
  <li><a href="https://crystal-lang.org/api/1.11.0/Math.html#fma(value1,value2,value3)-instance-method"><code><span>Math</span><span>.</span><span>fma</span></code></a> (<a href="https://github.com/crystal-lang/crystal/pull/13934">#13934</a>)</li>
  <li><a href="https://crystal-lang.org/api/1.11.0/Number.html#integer?:Bool-instance-method"><code><span>Number</span><span>#integer?</span></code></a> (<a href="https://github.com/crystal-lang/crystal/pull/13936">#13936</a>)</li>
  <li><a href="https://crystal-lang.org/api/1.11.0/Int32.html#abs_unsigned:UInt32-instance-method"><code><span>Int32</span><span>#abs_unsigned</span></code></a>, <a href="https://crystal-lang.org/api/1.11.0/Int32.html#neg_signed:self-instance-method"><code><span>#neg_signed</span></code></a> (<a href="https://github.com/crystal-lang/crystal/pull/13938">#13938</a>)</li>
  <li><a href="https://crystal-lang.org/api/1.11.0/Int32.html#to_signed:Int32-instance-method"><code><span>Int</span><span>::</span><span>Primitive</span><span>#to_signed</span></code></a>, <a href="https://crystal-lang.org/api/1.11.0/Int32.html#to_signed%21%3AInt32-instance-method"><code><span>#to_signed!</span></code></a>, <a href="https://crystal-lang.org/api/1.11.0/Int32.html#to_unsigned:UInt32-instance-method"><code><span>#to_unsigned</span></code></a>, <a href="https://crystal-lang.org/api/1.11.0/Int32.html#to_unsigned%21%3AUInt32-instance-method"><code><span>#to_unsigned!</span></code></a> (<a href="https://github.com/crystal-lang/crystal/pull/13960">#13960</a>)</li>
</ul>

<p><em>Thanks <a href="https://github.com/HertzDevil">@HertzDevil</a></em></p>

<h3 id="enhancements-for-crystal-spec">Enhancements for <code><span>crystal</span> <span>spec</span></code></h3>

<p><code><span>crystal</span> <span>spec</span></code> gets two new commands for introspection:</p>

<p><code><span>crystal</span> <span>spec</span> <span>--</span><span>dry</span><span>-</span><span>run</span></code> prints all active specs without actually executing any spec code (<a href="https://github.com/crystal-lang/crystal/pull/13804">#13804</a>).</p>

<p><em>Thanks <a href="https://github.com/nobodywasishere">@nobodywasishere</a></em></p>

<p><code><span>crystal</span> <span>spec</span> <span>--</span><span>list</span><span>-</span><span>tags</span></code> lists all tags defined in the spec suite (<a href="https://github.com/crystal-lang/crystal/pull/13616">#13616</a>).</p>

<p><em>Thanks <a href="https://github.com/baseballlover723">@baseballlover723</a></em></p>

<h3 id="enhancements-for-crystal-tool-unreachable">Enhancements for <code><span>crystal</span> <span>tool</span> <span>unreachable</span></code></h3>

<p>The basic implementation of <code><span>crystal</span> <span>tool</span> <span>unreachable</span></code> from Crystal 1.10 gets some useful enhancements.</p>

<ul>
  <li>The <code><span>--</span><span>tallies</span></code> option prints all methods and the total number of calls. Those with a zero tally are unreachable (<a href="https://github.com/crystal-lang/crystal/pull/13969">#13969</a>).</li>
  <li>The <code><span>--</span><span>check</span></code> flag exits with a failure status if there is any unreachable code (<a href="https://github.com/crystal-lang/crystal/pull/13930">#13930</a>).</li>
  <li>Annotations show up in the output (<a href="https://github.com/crystal-lang/crystal/pull/13927">#13927</a>).</li>
  <li>New output format: CSV (<a href="https://github.com/crystal-lang/crystal/pull/13926">#13926</a>).</li>
  <li>Paths in the output are relativized, making it more succinct (<a href="https://github.com/crystal-lang/crystal/pull/13929">#13929</a>).</li>
</ul>

<p><em>Thanks <a href="https://github.com/straight-shoota">@straight-shoota</a></em></p>

<h3 id="inherited-macros-in-api-docs">Inherited macros in API docs</h3>

<p>Inherited macros are now exposed in the API docs. They had previously been hidden, in contrast to inherited defs (<a href="https://github.com/crystal-lang/crystal/pull/13810">#13810</a>).</p>

<p><em>Thanks <a href="https://github.com/Blacksmoke16">@Blacksmoke16</a></em></p>

<h3 id="text">Text</h3>

<ul>
  <li><a href="https://crystal-lang.org/api/1.11.0/Regex/MatchData.html#to_s:String-instance-method"><code><span>Regex</span><span>::</span><span>MatchData</span><span>#to_s</span></code></a> returns the matched substring (<a href="https://github.com/crystal-lang/crystal/pull/14115">#14115</a>).
  <em>Thanks <a href="https://github.com/Vendicated">@Vendicated</a></em></li>
  <li>The new <a href="https://crystal-lang.org/api/1.11.0/toplevel.html#EOL"><code><span>EOL</span></code></a>constant (End-Of-Line) is a portable reference to the system-specific new line character sequence (<a href="https://github.com/crystal-lang/crystal/pull/11303">#11303</a>). <em>Thanks <a href="https://github.com/postmodern">@postmodern</a></em></li>
  <li>We got new version-specific constructors for <a href="https://crystal-lang.org/api/1.11.0/UUID.html"><code><span>UUID</span></code></a>: <a href="https://crystal-lang.org/api/1.11.0/UUID.html#v1%28%2A%2Cclock_seq%3AUInt16%7CNil%3Dnil%2Cnode_id%3AMAC%7CNil%3Dnil%29%3Aself-class-method"><code><span>.</span><span>v1</span></code></a>, <a href="https://crystal-lang.org/api/1.11.0/UUID.html#v2%28domain%3ADomain%2Cid%3AUInt32%2Cnode_id%3AMAC%7CNil%3Dnil%29%3Aself-class-method"><code><span>.</span><span>v2</span></code></a>, <a href="https://crystal-lang.org/api/1.11.0/UUID.html#v3%28name%3AString%2Cnamespace%3AUUID%29%3Aself-class-method"><code><span>.</span><span>v3</span></code></a>, <a href="https://crystal-lang.org/api/1.11.0/UUID.html#v4%28randomr%3ARandom%3DRandom%3A%3ASecure%29%3Aself-class-method"><code><span>.</span><span>v4</span></code></a>, and <a href="https://crystal-lang.org/api/1.11.0/UUID.html#v5%28name%3AString%2Cnamespace%3AUUID%29%3Aself-class-method"><code><span>.</span><span>v5</span></code></a> (<a href="https://github.com/crystal-lang/crystal/pull/13693">#13693</a>).
  <em>Thanks <a href="https://github.com/threez">@threez</a></em></li>
  <li><a href="https://crystal-lang.org/api/1.11.0/StringScanner.html"><code><span>StringScanner</span></code></a> now supports <code><span>String</span></code> and <code><span>Char</span></code> patterns (<a href="https://github.com/crystal-lang/crystal/pull/13806">#13806</a>).
  <em>Thanks <a href="https://github.com/funny-falcon">@funny-falcon</a></em></li>
  <li><code><span>Char</span><span>::</span><span>Reader</span></code> got some nilable character accessors: <a href="https://crystal-lang.org/api/1.11.0/Char/Reader.html#current_char%3F%3AChar%7CNil-instance-method"><code><span>#current_char?</span></code></a>, <a href="https://crystal-lang.org/api/1.11.0/Char/Reader.html#next_char%3F%3AChar%7CNil-instance-method"><code><span>#next_char?</span></code></a>, <a href="https://crystal-lang.org/api/1.11.0/Char/Reader.html#previous_char%3F%3AChar%7CNil-instance-method"><code><span>#previous_char?</span></code></a> (<a href="https://github.com/crystal-lang/crystal/pull/14012">#14012</a>).
  <em>Thanks <a href="https://github.com/HertzDevil">@HertzDevil</a></em></li>
  <li><a href="https://crystal-lang.org/api/1.11.0/String.html#matches_full?(regex:Regex):Bool-instance-method"><code><span>String</span><span>#matches_full?</span></code></a> is a simple API when you need a regular expression to match the entire string (<a href="https://github.com/crystal-lang/crystal/pull/13968">#13968</a>).
  <em>Thanks <a href="https://github.com/straight-shoota">@straight-shoota</a></em></li>
</ul>

<h3 id="misc">Misc</h3>

<ul>
  <li>The capacity of <code><span>String</span><span>::</span><span>Buffer</span></code> and <code><span>IO</span><span>::</span><span>Memory</span></code> was unintentionally limited to 1GB. They now support the full range up to <code><span>Int32</span><span>::</span><span>MAX</span></code>, i.e. 2GB (<a href="https://github.com/crystal-lang/crystal/pull/13989">#13989</a>).
  <em>Thanks <a href="https://github.com/straight-shoota">@straight-shoota</a></em></li>
  <li>There was a nasty bug in <code><span>Number</span><span>#format</span></code> which could mess with the integral part. It is now fixed in <a href="https://github.com/crystal-lang/crystal/pull/14061">#14061</a>.
  <em>Thanks <a href="https://github.com/HertzDevil">@HertzDevil</a></em></li>
  <li>Vendored shards <code><span>markd</span></code> and <code><span>reply</span></code> are no longer referenced by paths relative to the compiler source tree. This means they can be local dependencies (i.e. in <code><span>lib</span></code>) when using the compiler as a library (<a href="https://github.com/crystal-lang/crystal/pull/13992">#13992</a>).
  <em>Thanks <a href="https://github.com/nobodywasishere">@nobodywasishere</a></em></li>
  <li>There are two new constants which provide information on the compiler host and target: <code><span>Crystal</span><span>::</span><span>HOST_TRIPLE</span></code> and <code><span>TARGET_TRIPLE</span></code> (<a href="https://github.com/crystal-lang/crystal/pull/13823">#13823</a>).
  <em>Thanks <a href="https://github.com/HertzDevil">@HertzDevil</a></em></li>
</ul>

<h3 id="shards-0174">Shards 0.17.4</h3>

<p>The bundled shards release was updated to <a href="https://github.com/crystal-lang/shards/releases/tag/v0.17.4">0.17.4</a> which brings a couple minor bugfixes. (<a href="https://github.com/crystal-lang/crystal/pull/14133">#14133</a>).</p>

<p><em>Thanks <a href="https://github.com/straight-shoota">@straight-shoota</a></em></p>

<h3 id="experimental-referencestorage-and-pre_initialize">Experimental: <code><span>ReferenceStorage</span></code> and <code><span>.</span><span>pre_initialize</span></code></h3>

<p>We’ve started an effort to make it easier to use custom allocation mechanisms in Crystal and decouple allocation from initialization.
The main tool is <a href="https://crystal-lang.org/api/1.11.0/Reference.html#pre_initialize(address:Pointer)-class-method"><code><span>Reference</span><span>.</span><span>pre_initialize</span></code></a> which performs the rudimentary object initialization, before actually calling <code><span>#initialize</span></code>.</p>

<p><a href="https://crystal-lang.org/api/1.11.0/Reference.html#unsafe_construct%28address%3APointer%2C%2Aargs%2C%2A%2Aopts%29%3Aself-class-method"><code><span>Reference</span><span>.</span><span>unsafe_construct</span></code></a> is a higher level API on top of that.
<a href="https://crystal-lang.org/api/1.11.0/ReferenceStorage.html"><code><span>ReferenceStorage</span></code></a> represents a static buffer for a reference allocation.</p>

<p>These APIs are experimental and might be subject to change. We expect more features in this direction in future releases. Join the discussion about custom reference allocation at <a href="https://github.com/crystal-lang/crystal/issues/13481">#13481</a>.</p>

<blockquote>
  <p><em>NOTE:</em> <code><span>ReferenceStorage</span></code> was removed again in 1.11.1 due to compatibility issues with older versions of the standard library (<a href="https://github.com/crystal-lang/crystal/pull/14207">#14207</a>). It will come back with an improved implementation.</p>
</blockquote>

<p><em>Thanks <a href="https://github.com/HertzDevil">@HertzDevil</a></em></p>

<h2 id="deprecations">Deprecations</h2>

<ul>
  <li>Splat operators in macro expressions are deprecated. Use <code><span>.</span><span>splat</span></code> instead (<a href="https://github.com/crystal-lang/crystal/pull/13939">#13939</a>)</li>
  <li><code><span>LLVM</span><span>.</span><span>start_multithreaded</span></code> and <code><span>.</span><span>stop_multithreaded</span></code>. They have no effect (<a href="https://github.com/crystal-lang/crystal/pull/13949">#13949</a>)</li>
  <li><code><span>LLVMExtSetCurrentDebugLocation</span></code> from <code><span>llvm_ext</span><span>.</span><span>cc</span></code> for LLVM 9+ (<a href="https://github.com/crystal-lang/crystal/pull/13965">#13965</a>)</li>
  <li><code><span>Char</span><span>::</span><span>Reader</span><span>#@end</span></code> (<a href="https://github.com/crystal-lang/crystal/pull/13920">#13920</a>)</li>
</ul>

<hr>

<p>We have been able to do all of this thanks to the continued support of <a href="https://www.84codes.com/">84codes</a> and every other <a href="https://crystal-lang.org/sponsors">sponsor</a>.
To maintain and increase the development pace, donations and sponsorships are
essential. <a href="https://opencollective.com/crystal-lang">OpenCollective</a> is
available for that. Reach out to <a href="mailto:crystal@manas.tech">crystal@manas.tech</a>
if you’d like to become a direct sponsor or find other ways to support Crystal.
We thank you in advance!</p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building a Faraday cage with data passthrough for ESP32 reverse engineering (116 pts)]]></title>
            <link>https://esp32-open-mac.be/posts/0003-faraday-cage/</link>
            <guid>38991248</guid>
            <pubDate>Sun, 14 Jan 2024 15:29:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://esp32-open-mac.be/posts/0003-faraday-cage/">https://esp32-open-mac.be/posts/0003-faraday-cage/</a>, See on <a href="https://news.ycombinator.com/item?id=38991248">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>First, a short recap of what this project is doing: the Wi-Fi stack for the ESP32 (a popular, cheap microcontroller) is provided through a binary blob. We’re trying to reverse engineer the software and hardware to build our own open-source Wi-Fi stack. If this sounds interesting, I recommend you read the <a href="https://esp32-open-mac.be/posts/0001-introduction/">first</a> and <a href="https://esp32-open-mac.be/posts/0002-first-connection/">second</a> blog posts. However, this is not necessary to read the rest of this blog post.</p><p>One problem we encountered while reverse engineering the Wi-Fi hardware, is that there are a lot of Wi-Fi packets flying through the air: even when idle, most access points broadcast a beacon packet about ten times per second. Combined with the data packets that are also sent between access points and clients in the neighbourhood, this adds up to a lot of packets per second. One tool we use in reverse engineering, is a Wi-Fi dongle in monitor mode: in that mode, the dongle captures all packets it sees, even packets not addressed to the dongle’s MAC address. This deluge of packets sometimes makes it hard to find the single packet the ESP32 did or did not send; in most scenarios, you could filter on MAC address, but since we’re still reverse engineering the hardware, we sometimes don’t know the MAC address a packet will be sent with.</p><h2 id="failed-attempts">Failed attempts
<a href="#failed-attempts">
<span>Link to heading</span></a></h2><p>A simple (albeit not very practical) way to fix this, is to go out into the woods where there are no Wi-Fi devices. A better approach is to ‘insulate’ the ESP32 and Wi-Fi dongle from outside Wi-Fi transmitters. My first try at this was to directly connect the antenna connector of the ESP32 with the antenna connector of the dongle (with an attenuator in between, to make the signal weaker not to overpower the receiver).</p><p><img src="https://esp32-open-mac.be/images/dongle-directly-connected.jpg" alt="Wi-Fi dongle connected to the ESP32, with two 30&amp;nbsp;dB attenuators in between"></p><p>However, this did not work: outside packets still leaked in. My second try involved a very basic Faraday cage: a paint tin with a cut-out for the USB leads of the dongle and ESP32. To try to reduce RF leaking in via the USB leads, I added several ferrite chokes and closed the hole with copper tape. This unfortunately did not work as well as people online told it would; it only further attenuated outside packets with 10&nbsp;dB. A 10× decrease in power might sound impressive, but it’s really not: my laptop can receive packets as quiet as -90&nbsp;dBm; right next to an access point the packets are about -35&nbsp;dBm, so attenuating with only 10&nbsp;dB is not enough by a long shot.</p><p><img src="https://esp32-open-mac.be/images/faraday-paint-tin.jpg" alt="Paint tin with USB leads coming out of it"></p><p>I also tried putting my phone in a (turned off!) microwave, but this did not work either, it was still connected to the Wi-Fi access point.</p><h2 id="proper-faraday-cage">Proper Faraday cage
<a href="#proper-faraday-cage">
<span>Link to heading</span></a></h2><p>While researching on how to build a proper Faraday cage that is also affordable, I came across the paper ‘Building and Testing an Economic Faraday Cage for Wireless, IoT
Computing Education and Research’ that seemed pretty good: for 793&nbsp;USD, they built a Faraday cage with data and power passthrough: they accomplished the data passthrough by using an Ethernet-to-fiber converter; they accomplished the power passthrough by using a second-hand power line filter from a MRI chamber. Simply explained, the paper proposes having two cabinets: an inner cabinet, covered in conductive fabric except where the door is, and an outer one, where the door is covered in fabric, sealing against the inner cabinet when the door is closed. If this is not entirely clear, don’t worry, I’ll have pictures later.</p><p>There are, in my opinion, some flaws in that paper:</p><ul><li>they mention the bill of materials, but don’t give important specifications about what exactly it is they used. For example, their bill of materials states ‘Used 20A Powerline Filter’, but this is the most specific description of the power line filter in the paper. By emailing the original authors, I got to know that they used a Lindgren EMI/RFI filter, ELUL-2020, obtained on eBay.</li><li>which brings me to the second point: they use second-hand material for both the power line filter (120&nbsp;USD in the paper) and the cabinet (5&nbsp;USD in the paper). Now, I don’t have anything against re-using components; I sometimes go to the scrapyard to scavenge useful electronic components (motors, displays, …), but you cannot rely on the availability of those components. While it is true that in the US, similar filters can be bought for ~200&nbsp;USD on eBay, you cannot rely on those filters staying available in the second-hand market. Furthermore, in Belgium, the country where I live, there don’t appear to be any EMI power filters available on the online second hand markets. The import costs and shipping charges for buying a used power line filter from the US would be prohibitive.</li><li>using second-hand materials in something where the goal is to make it low-cost feels a bit like cheating: the materials are low-cost because you did a good job in obtaining something expensive for a low price, not because the materials are inherently low-cost. A new EMI power line filter is so expensive that places selling them don’t even display the price (“if you have to ask the price, you can’t afford it”).</li></ul><p>I will try to only use materials that are commonly available, and link to what exactly I bought. The main differences between the approach in the paper and my approach are:</p><ul><li>I built the inner and outer cabinets myself, from wood. No special tools are needed, only a drill.</li><li>Instead of trying to pass through power, I’ll put a lead-acid battery inside the Faraday cage and use buck-boost converters to convert to the necessary voltages for the fiber-optics converter and devices under test. This will lower the costs dramatically, to the point where even when we’re using all-new components, the price will still be lower than the price described in the paper.</li></ul><h2 id="pictures">Pictures!
<a href="#pictures">
<span>Link to heading</span></a></h2><p>(Please excuse me for the bad image quality)</p><p>First of all, a picture of the completed Faraday cage where I’d like to call attention to:</p><ul><li>this consists of two parts that slide into each other: the outer cabinet, made from medium-density fiber board (MDF board), and the inner cabinet, made from a wooden skeleton covered with conductive fabric on five sides.</li><li>there is conductive fabric on the inner side of the door. There is foam tape between the fabric and the wood on the front side of the inner cube; when you close the door, the foam tape is compressed, pressing the fabric of the inner cube tightly against the fabric of the front door, creating an RF tight seal.</li><li>there is a black latch on the door that can keep the door shut (not shown very well here, but I’ll show a better picture later).</li><li>in the top right corner, you can see a yellow fiber entering the cage from behind.</li></ul><p><img src="https://esp32-open-mac.be/images/faraday_global_overview.jpg" alt="Picture of the fully completed Faraday cage"></p><p>This is the inner cube, constructed from wood, before it was covered with conductive fabric on five sides.</p><p><img src="https://esp32-open-mac.be/images/faraday_inner_cabinet.jpg" alt="Inner cube"></p><p>Here, you can see the two fiber-to-Ethernet converters. I used bidirectional converters: this means that only one fiber is used for both transmitting and receiving data; as opposed to unidirectional fiber that has a TX/RX pair. This was done to minimize the size of the connector, which in turn would make it possible to fit through a smaller diameter copper tube.</p><p><img src="https://esp32-open-mac.be/images/faraday_fiber.jpg" alt="Fiber setup"></p><p>This is a copper pipe that has the fiber going through it. On both sides of the pipe, a small 3D printed cone was added to gently convert the diameter of the outer pipe to the diameter of the fiber. Conductive tape was used to seal the hole in the copper pipe: only a very small hole where the fiber exits, remains. Since this hole is sufficiently small compared to the wavelength of the 2.4&nbsp;GHz radio waves, the radio waves cannot enter via there.</p><p>This copper tube was then inserted through a hole in the fabric on the back of the inner cube. It was again taped with conductive tape to both the inside and outside to form a good seal.</p><p><img src="https://esp32-open-mac.be/images/faraday_pipe_end.jpg" alt="End cap">
<img src="https://esp32-open-mac.be/images/faraday_pipe_full.jpg" alt="Full pipe">
<img src="https://esp32-open-mac.be/images/endcap.png" alt="End cap 3D printed part"></p><p>This is the test setup that is placed inside the Faraday cage. It consists of:</p><ul><li>A Raspberry Pi. This runs usbip, so that the USB devices connected to the Pi can be used from other computers on the network.</li><li>A 5V USB buck converter (green case) converting the 12V from the battery to 5V for the Pi</li><li>A TP-Link TL-WN722N v1 Wi-Fi dongle, used to capture packets (it can be put in monitor mode)</li><li>An ESP32, connected to a JTAG debugger (not connected via USB at the moment)</li></ul><p><img src="https://esp32-open-mac.be/images/faraday_esp32_testsetup.jpg" alt="Faraday ESP32 test setup"></p><h2 id="testing">Testing
<a href="#testing">
<span>Link to heading</span></a></h2><p>The Faraday cage was then tested by sniffing packets using the Wi-Fi dongle inside. I first captured packets for 10&nbsp;minutes while the door was open (so RF could enter), then captured packets for 10&nbsp;minutes after the door was closed.</p><p>When the door was closed, no packets were captured at all. So, as to still give an approximate lower bound by how much the Faraday cage attenuates signals at 2.4&nbsp;GHz, I used both the strongest and weakest signal strength when the door was open:</p><pre tabindex="0"><code>&gt;&gt;&gt; from scapy.all import *
&gt;&gt;&gt; scapy_cap = rdpcap('faraday_captured_packets_door_open.pcapng')
&gt;&gt;&gt; max(p.dBm_AntSignal for p in scapy_cap)
-12
&gt;&gt;&gt; min(p.dBm_AntSignal for p in scapy_cap)
-81
&gt;&gt;&gt; (-12) - (-81)
69
</code></pre><p>The weakest signal my wireless dongle could still receive was -81&nbsp;dBm. The strongest signal that arrived was -12&nbsp;dBm. Since the Faraday cage blocked all packets, the power of even the strongest signal would have to have been attenuated to below -81&nbsp;dBm for it to not be able to be received anymore. So, a lower bound of 69&nbsp;dB attenuation is established. I know that this might not be entirely correct (the wireless dongle might not be calibrated, the geometry of the Faraday cage could have increased signal strength when the door was open, etc.), but I think it gives a good enough indication.</p><h2 id="bill-of-materials">Bill of materials
<a href="#bill-of-materials">
<span>Link to heading</span></a></h2><table><thead><tr><th>Item</th><th>Price in EUR (1 EUR = 1.1&nbsp;USD)</th><th>Link, if necessary</th></tr></thead><tbody><tr><td>MDF board 244×122 cm</td><td>29.99</td><td></td></tr><tr><td>3× wooden beam 44×44mm, 210cm</td><td>9.27</td><td></td></tr><tr><td>copper pipe, 15mm outer diameter, 1m (although you only need about 0.5m)</td><td>10.99</td><td></td></tr><tr><td>hinges (could likely also be 3D printed)</td><td>7.78</td><td></td></tr><tr><td>(optional) handles for outer cube</td><td>10.69</td><td></td></tr><tr><td>wood screws; I used 4mm × 40mm for outer MDF cube; 4mm × 60mm screws for inner cube. I didn’t have any wood screws before this project, so I bought in bulk for future projects. The 10 EUR here is an upper bound estimate</td><td>10</td><td></td></tr><tr><td>foam tape: I used 20 mm width × 3 mm height × 10 m length tape</td><td>7.45</td><td><a href="https://web.archive.org/web/20240113175203/https://www.amazon.com.be/dp/B09KBH7WT1" target="_blank" rel="noopener">https://web.archive.org/web/20240113175203/https://www.amazon.com.be/dp/B09KBH7WT1</a></td></tr><tr><td>conductive fabric, 3× 110 cm by 91 cm. I used TitanRF fabric, this was the only fabric that came with a test report</td><td>84</td><td><a href="https://web.archive.org/web/20240113174610/https://www.amazon.com.be/-/nl/Darkness-Military-geleidende-RF-signalen-Bluetooth/dp/B01M294MGK/" target="_blank" rel="noopener">https://web.archive.org/web/20240113174610/https://www.amazon.com.be/-/nl/Darkness-Military-geleidende-RF-signalen-Bluetooth/dp/B01M294MGK/</a></td></tr><tr><td>conductive tape; 2.54 cm × 3.05 m</td><td>12</td><td><a href="https://web.archive.org/web/20240113174817/https://www.amazon.com.be/-/nl/TitanRF-Faraday-Tape-High-Shielding-RF-sluitingen/dp/B07CRLCGCH/" target="_blank" rel="noopener">https://web.archive.org/web/20240113174817/https://www.amazon.com.be/-/nl/TitanRF-Faraday-Tape-High-Shielding-RF-sluitingen/dp/B07CRLCGCH/</a></td></tr><tr><td>1 pair gigabit Ethernet to BiDi fiber converter</td><td>65.23</td><td><a href="https://web.archive.org/web/20240113172956/https://www.amazon.com.be/-/nl/multimedia-singlemode-LC-transceiver-inbegrepen/dp/B09MD4BBFD/" target="_blank" rel="noopener">https://web.archive.org/web/20240113172956/https://www.amazon.com.be/-/nl/multimedia-singlemode-LC-transceiver-inbegrepen/dp/B09MD4BBFD/</a></td></tr><tr><td>single OS2 LC/LC fiber optic cable, 2m</td><td>7.99</td><td><a href="https://web.archive.org/web/20240113174655/https://www.amazon.com.be/-/nl/nexinex-Glasvezel-patchkabel-Simplex-single-mode/dp/B0CDQ6YXPS/" target="_blank" rel="noopener">https://web.archive.org/web/20240113174655/https://www.amazon.com.be/-/nl/nexinex-Glasvezel-patchkabel-Simplex-single-mode/dp/B0CDQ6YXPS/</a></td></tr><tr><td>buck converters (13.99 for 12; I only used two here)</td><td>2.33</td><td></td></tr><tr><td>3d printing filament (way less than 200g, but let’s take that)</td><td>4</td><td></td></tr><tr><td>Ultracell UL18-12 12V 18Ah lead-acid battery</td><td>29.66</td><td></td></tr></tbody></table><p>For a grand total of <strong>291.38&nbsp;EUR</strong>, or about <strong>318&nbsp;USD</strong>.</p><p>You’ll need to 3D print various items, those are detailed at <a href="https://github.com/esp32-open-mac/faraday_cage" target="_blank" rel="noopener">https://github.com/esp32-open-mac/faraday_cage</a></p><h2 id="building-tips">Building tips
<a href="#building-tips">
<span>Link to heading</span></a></h2><h2 id="outer-shell">Outer shell
<a href="#outer-shell">
<span>Link to heading</span></a></h2><ol><li>Assemble outer shell. This is made from MDF, so be careful and definitely pre-drill before inserting screws, otherwise, you would split the MDF. Do this even for screws that advertise you don’t need to pre-drill.</li><li>Attach hardware<ol><li>Hinges</li><li>Latch to keep the door shut</li><li>Optional: handles to lift the box</li></ol></li></ol><h2 id="framed-cube">Framed cube
<a href="#framed-cube">
<span>Link to heading</span></a></h2><ol><li>Assemble cube</li><li>Attach all hardware<ol><li>Holders for copper tube</li><li>Stand-offs for bottom plate</li><li>Foam tape</li></ol></li><li>(Last!) Attach Faraday fabric, with thumbtacks. Don’t use a staple gun, you would tear the fabric.</li></ol><p>If you have any questions, open a GitHub issue at <a href="https://github.com/esp32-open-mac/faraday_cage" target="_blank" rel="noopener">https://github.com/esp32-open-mac/faraday_cage</a> or send me an email via <a href="mailto:esp32-open-mac@devreker.be">esp32-open-mac@devreker.be</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to delete your data from data brokers (179 pts)]]></title>
            <link>https://www.cybercollective.org/blog/how-to-delete-your-data-from-data-brokers</link>
            <guid>38990755</guid>
            <pubDate>Sun, 14 Jan 2024 14:34:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cybercollective.org/blog/how-to-delete-your-data-from-data-brokers">https://www.cybercollective.org/blog/how-to-delete-your-data-from-data-brokers</a>, See on <a href="https://news.ycombinator.com/item?id=38990755">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>You probably don't think much about data brokers, but these behind-the-scenes companies collect, trade, and profit from our personal information every day. Data brokers like Acxiom, Experian, and Equifax gather data about us from public records, online activities, store loyalty programs, and more. This data gets packaged into profiles and sold to other businesses for targeted advertising, credit checks, background screening, and who knows what else.</p><p>All this happens without our knowledge or consent. Most people have no idea the extent of their digital footprint or how to even access the data collected about them. While we willingly give away some personal information to use online services, plenty is taken without our permission.<br></p><p>This lack of transparency and control is cause for concern. We have a right to know what personal data exists about us and where it is going. The good news is that there are steps you can take to uncover and delete your data from broker sites:</p><h3>Request Your Data &amp; Send Removal Requests</h3><p>Many data brokers allow you to access your personal information in their systems. Some even offer opt-out tools to stop data collection. Search for "data broker opt-out" to find relevant sites and submit access/deletion requests.&nbsp;<br></p><p>Be sure to keep records of all your requests, including copies of any emails or letters sent. Maintaining this documentation can be useful if you need to follow up or dispute an unfulfilled request.<br></p><p>For brokers without online tools, you'll need to directly request data removal via email or letter. Clearly state your name, address, relevant account numbers, and that you want your data deleted. Keep records of all requests.</p><p>Here is a list of the most prevalent data brokers:</p><ul role="list"><li>Acxiom</li><li>Experian</li><li>Equifax</li><li>TransUnion&nbsp;</li><li>PeopleSmart</li><li>BeenVerified</li><li>WhitePages</li><li>InstantCheckmate</li><li>Pipl</li><li>Intelius</li></ul><h3>Leverage Consumer Protection Laws</h3><p>Depending on your location, laws like the <a href="https://oag.ca.gov/privacy/ccpa">CCPA</a> and <a href="https://gdpr-info.eu/">GDPR</a> give consumers the right to request that companies delete their data.&nbsp;<br></p><p>The California Consumer Privacy Act (CCPA) and the European Union's General Data Protection Regulation (GDPR) are two pivotal privacy laws that give consumers greater control over their personal data.<br></p><p>The CCPA went into effect in 2020 and grants new rights to California residents, like:<br></p><ul role="list"><li>The right to know what personal information companies collect about them and how it is used.</li><li>The right to opt-out of the sale of their data.</li><li>The right to request that their personal information be deleted.</li></ul><p>The GDPR took effect in 2018 and imposes strict guidelines around collecting and handling EU citizens' personal data, such as:<br></p><ul role="list"><li>Requiring affirmative consent before collecting an individual's data.</li><li>Obligating companies to honor requests for data erasure within one month.</li><li>Establishing protections for data breach notification and cross-border data transfers.</li></ul><p>Leveraging these landmark laws can strengthen your ability to take control of your data from brokers. Understand your rights and don't hesitate to cite CCPA or GDPR to compel compliance.<br></p><h3>Use a Deletion Service</h3><p>Sites like <a href="https://joindeleteme.com/">DeleteMe</a>, <a href="https://get.incogni.io/aff_c?offer_id=1017&amp;aff_id=6796">Incogni</a> and <a href="https://abine.com/">Abine</a> will submit opt-out requests on your behalf and monitor to ensure your data stays removed. This simplifies the process, for a fee.<br></p><p>Regaining control of your personal information takes persistence and vigilance, but is worth it. Data brokers will continue collecting our data until laws and consumer pressure demand change. But we can fight back by actively managing our digital footprints and being conscious about what we are consenting to online.</p><p>‍</p><p>*&nbsp;Links may contain affiliate links. </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Coding Self-Attention, Multi-Head Attention, Cross-Attention, Causal-Attention (127 pts)]]></title>
            <link>https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention</link>
            <guid>38990709</guid>
            <pubDate>Sun, 14 Jan 2024 14:29:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention">https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention</a>, See on <a href="https://news.ycombinator.com/item?id=38990709">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>This article will teach you about self-attention mechanisms used in transformer architectures and large language models (LLMs) such as GPT-4 and Llama. Self-attention and related mechanisms are core components of LLMs, making them a useful topic to understand when working with these models.</p><p>However, rather than just discussing the self-attention mechanism, we will code it in Python and PyTorch from the ground up. In my opinion, coding algorithms, models, and techniques from scratch is an excellent way to learn!</p><p><span>As a side note, this article is a modernized and extended version of "</span><a href="https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html" rel="">Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch</a><span>," which I published on my old blog almost exactly a year ago. Since I really enjoy writing (and reading) 'from scratch' articles, I wanted to modernize this article for </span><em>Ahead of AI</em><span>.</span></p><p><span>Additionally, this article motivated me to write the book </span><em><a href="http://mng.bz/amjo" rel="">Build a Large Language Model (from Scratch)</a></em><span>, which is currently in progress. Below is a mental model that summarizes the book and illustrates how the self-attention mechanism fits into the bigger picture.</span></p><p>To keep the length of this article somewhat reasonable, I'll assume you already know about LLMs and you also know about attention mechanisms on a basic level. The goal and focus of this article is to understand how attention mechanisms work via a Python &amp; PyTorch code walkthrough.</p><p><span>Since its introduction via the original transformer paper (</span><a href="https://arxiv.org/abs/1706.03762" rel="">Attention Is All You Need</a><span>), self-attention has become a cornerstone of many state-of-the-art deep learning models, particularly in the field of Natural Language Processing (NLP). Since self-attention is now everywhere, it's important to understand how it works.</span></p><p><span>The concept of "attention" in deep learning </span><a href="https://arxiv.org/abs/1409.0473" rel="">has its roots in the effort to improve Recurrent Neural Networks (RNNs)</a><span> for handling longer sequences or sentences. For instance, consider translating a sentence from one language to another. Translating a sentence word-by-word is usually not an option because it ignores the complex grammatical structures and idiomatic expressions unique to each language, leading to inaccurate or nonsensical translations.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png" width="432" height="224.7864406779661" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:614,&quot;width&quot;:1180,&quot;resizeWidth&quot;:432,&quot;bytes&quot;:114671,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>An incorrect word-by-word translation (top) compared to a correct translation (bottom)</figcaption></figure></div><p><span>To overcome this issue, attention mechanisms were introduced to give access to all sequence elements at each time step. The key is to be selective and determine which words are most important in a specific context. </span><a href="https://arxiv.org/abs/1706.03762" rel="">In 2017, the transformer architecture</a><span> introduced a standalone self-attention mechanism, eliminating the need for RNNs altogether.</span></p><p>(For brevity, and to keep the article focused on the technical self-attention details, I am keeping this background motivation section brief so that we can focus on the code implementation.)</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png" width="530" height="717.4656593406594" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:530,&quot;bytes&quot;:2180804,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>A visualization from the “Attention is All You Need” paper (</span><a href="https://arxiv.org/abs/1706.03762" rel="">https://arxiv.org/abs/1706.03762</a><span>) showing how much the word “making” depends or focuses on other words in the input via attention weights (the color intensity is proportional the attention weight value).</span></figcaption></figure></div><p>We can think of self-attention as a mechanism that enhances the information content of an input embedding by including information about the input's context. In other words, the self-attention mechanism enables the model to weigh the importance of different elements in an input sequence and dynamically adjust their influence on the output. This is especially important for language processing tasks, where the meaning of a word can change based on its context within a sentence or document.</p><p><span>Note that there are many variants of self-attention. A particular focus has been on making self-attention more efficient. However, most papers still implement the original scaled-dot product attention mechanism introduced in the </span><a href="https://arxiv.org/abs/1706.03762" rel="">Attention Is All You Need paper</a><span> since self-attention is rarely a computational bottleneck for most companies training large-scale transformers.</span></p><p><span>So, in this article, we focus on the original scaled-dot product attention mechanism (referred to as self-attention), which remains the most popular and most widely used attention mechanism in practice. However, if you are interested in other types of attention mechanisms, check out the </span><a href="https://arxiv.org/abs/2009.06732" rel="">2020 </a><em><a href="https://arxiv.org/abs/2009.06732" rel="">Efficient Transformers: A Survey</a></em><span>, the </span><a href="https://arxiv.org/abs/2302.01107" rel="">2023 </a><em><a href="https://arxiv.org/abs/2302.01107" rel="">A Survey on Efficient Training of Transformers</a></em><span> review, and the recent </span><a href="https://arxiv.org/abs/2205.14135" rel="">FlashAttention</a><span> and </span><a href="https://arxiv.org/abs/2307.08691" rel="">FlashAttention-v2</a><span> papers.</span></p><p><span>Before we begin, let's consider an input sentence </span><em>"Life is short, eat dessert first"</em><span> that we want to put through the self-attention mechanism. Similar to other types of modeling approaches for processing text (e.g., using recurrent neural networks or convolutional neural networks), we create a sentence embedding first.</span></p><p><span>For simplicity, here our dictionary </span><code>dc</code><span> is restricted to the words that occur in the input sentence. In a real-world application, we would consider all words in the training dataset (typical vocabulary sizes range between 30k to 50k entries).</span></p><p><strong>In:</strong></p><pre><code>sentence = 'Life is short, eat dessert first'
​
dc = {s:i for i,s 
      in enumerate(sorted(sentence.replace(',', '').split()))}

print(dc)</code></pre><p><strong>Out:</strong></p><pre><code>{'Life': 0, 'dessert': 1, 'eat': 2, 'first': 3, 'is': 4, 'short': 5}</code></pre><p>Next, we use this dictionary to assign an integer index to each word:</p><p><strong>In:</strong></p><pre><code>import torch
​
sentence_int = torch.tensor(
    [dc[s] for s in sentence.replace(',', '').split()]
)
print(sentence_int)</code></pre><p><strong>Out:</strong></p><pre><code>tensor([0, 4, 5, 2, 1, 3])</code></pre><p>Now, using the integer-vector representation of the input sentence, we can use an embedding layer to encode the inputs into a real-vector embedding. Here, we will use a tiny 3-dimensional embedding such that each input word is represented by a 3-dimensional vector. </p><p>Note that embedding sizes typically range from hundreds to thousands of dimensions. For instance, Llama 2 utilizes embedding sizes of 4,096. The reason we use 3-dimensional embeddings here is purely for illustration purposes. This allows us to examine the individual vectors without filling the entire page with numbers.</p><p>Since the sentence consists of 6 words, this will result in a 6×3-dimensional embedding:</p><p><strong>In:</strong></p><pre><code>vocab_size = 50_000
​
torch.manual_seed(123)
embed = torch.nn.Embedding(vocab_size, 3)
embedded_sentence = embed(sentence_int).detach()
​
print(embedded_sentence)
print(embedded_sentence.shape)</code></pre><p><strong>Out:</strong></p><pre><code>tensor([[ 0.3374, -0.1778, -0.3035],
 &nbsp; &nbsp; &nbsp;  [ 0.1794, &nbsp;1.8951, &nbsp;0.4954],
 &nbsp; &nbsp; &nbsp;  [ 0.2692, -0.0770, -1.0205],
 &nbsp; &nbsp; &nbsp;  [-0.2196, -0.3792, &nbsp;0.7671],
 &nbsp; &nbsp; &nbsp;  [-0.5880, &nbsp;0.3486, &nbsp;0.6603],
 &nbsp; &nbsp; &nbsp;  [-1.1925, &nbsp;0.6984, -1.4097]])
torch.Size([6, 3])</code></pre><p>Now, let's discuss the widely utilized self-attention mechanism known as the scaled dot-product attention, which is an integral part of the transformer architecture.</p><p><span>Self-attention utilizes three weight matrices, referred to as </span><em><strong><span>W</span><sub>q</sub></strong></em><span>, </span><em><strong><span>W</span><sub>k</sub></strong></em><span>, and </span><em><strong><span>W</span><sub>v</sub></strong></em><span>, which are adjusted as model parameters during training. These matrices serve to project the inputs into </span><em>query</em><span>, </span><em>key</em><span>, and </span><em>value</em><span> components of the sequence, respectively.</span></p><p><span>The respective query, key and value sequences are obtained via matrix multiplication between the weight matrices </span><em><strong>W</strong></em><span> and the embedded inputs </span><em><strong>x</strong></em><span>:</span></p><ul><li><p><span>Query sequence: </span><em><strong><span>q</span><sup>(i)</sup></strong><span> </span></em><span>= </span><strong>W</strong><em><strong><sub>q</sub></strong></em><span> </span><em><strong><span>x</span><sup>(i)</sup></strong></em><span> for </span><em>i</em><span> in sequence </span><em>1 … T</em></p></li><li><p><span>Key sequence: </span><em><strong><span>k</span><sup>(i)</sup></strong><span> </span></em><span>= </span><strong>W</strong><em><strong><sub>k</sub></strong></em><span> </span><em><strong><span>x</span><sup>(i)</sup></strong></em><span> for </span><em>i</em><span> in sequence </span><em>1 … T</em></p></li><li><p><span>Value sequence: </span><em><strong><span>v</span><sup>(i)</sup></strong><span> </span></em><span>= </span><strong>W</strong><em><strong><sub>v</sub></strong></em><span> </span><em><strong><span>x</span><sup>(i)</sup></strong></em><span> for </span><em>i</em><span> in sequence </span><em>1 … T</em></p></li></ul><p><span>The index </span><em>i</em><span> refers to the token index position in the input sequence, which has length </span><em>T</em><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png" width="174" height="373.672131147541" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:786,&quot;width&quot;:366,&quot;resizeWidth&quot;:174,&quot;bytes&quot;:58530,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Computing the query, key, and value vectors via the input x and weights W.</figcaption></figure></div><p><span>Here, both </span><em><strong><span>q</span><sup>(i)</sup></strong></em><span> and </span><em><strong><span>k</span><sup>(i)</sup></strong></em><span> are vectors of dimension </span><em><strong><span>d</span><sub>k</sub></strong></em><span>. The projection matrices </span><em><strong><span>W</span><sub>q</sub></strong></em><span> and </span><em><strong><span>W</span><sub>k</sub></strong></em><span> have a shape of </span><em><strong><span>d</span><sub>k</sub></strong></em><span> × </span><em><strong>d</strong></em><span>, while </span><em><strong><span>W</span><sub>v</sub></strong></em><span> has the shape </span><em><strong><span>d</span><sub>v</sub></strong></em><span> × </span><em><strong>d</strong></em><span>.</span></p><p><span>(It's important to note that </span><em><strong>d</strong></em><span> represents the size of each word vector, </span><em><strong>x</strong></em><span>.)</span></p><p><span>Since we are computing the dot-product between the query and key vectors, these two vectors have to contain the same number of elements (</span><em><strong><span>d</span><sub>q</sub><span> = d</span><sub>k</sub></strong></em><span>). In many LLMs, we use the same size for the value vectors such that </span><em><strong><span>d</span><sub>q</sub><span> = d</span><sub>k</sub><span> = d</span><sub>v</sub></strong></em><span>. However, the number of elements in the value vector </span><em><strong><span>v</span><sup>(i)</sup></strong></em><span>, which determines the size of the resulting context vector, can be arbitrary.</span></p><p><span>So, for the following code walkthrough, we will set </span><em><strong><span>d</span><sub>q</sub><span> = d</span><sub>k</sub><span> = 2</span></strong></em><span> and use </span><em><strong><span>d</span><sub>v</sub><span> = 4</span></strong></em><span>, initializing the projection matrices as follows:</span></p><p><strong>In:</strong></p><pre><code>torch.manual_seed(123)
​
d = embedded_sentence.shape[1]
​
d_q, d_k, d_v = 2, 2, 4
​
W_query = torch.nn.Parameter(torch.rand(d_q, d))
W_key = torch.nn.Parameter(torch.rand(d_k, d))
W_value = torch.nn.Parameter(torch.rand(d_v, d))</code></pre><p><span>(Similar to the word embedding vectors earlier, the dimensions </span><em><strong><span>d</span><sub>q</sub><span>, d</span><sub>k</sub><span>, d</span><sub>v</sub></strong></em><span> are usually much larger, but we use small numbers here for illustration purposes.)</span></p><p>Now, let's suppose we are interested in computing the attention vector for the second input element -- the second input element acts as the query here:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png" width="192" height="314.1224489795918" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:962,&quot;width&quot;:588,&quot;resizeWidth&quot;:192,&quot;bytes&quot;:124665,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>For the following sections below, we focus on the second input, </span><em><strong>x</strong></em><span>(2)</span></figcaption></figure></div><p>In code, this looks like as follows:</p><p><strong>In:</strong></p><pre><code>x_2 = embedded_sentence[1]
query_2 = W_query @ x_2
key_2 = W_key @ x_2
value_2 = W_value @ x_2
​
print(query_2.shape)
print(key_2.shape)
print(value_2.shape)</code></pre><p><strong>Out:</strong></p><pre><code>torch.Size([2])
torch.Size([2])
torch.Size([4])</code></pre><p>We can then generalize this to compute the remaining key, and value elements for all inputs as well, since we will need them in the next step when we compute the unnormalized attention weights later:</p><p><strong>In:</strong></p><pre><code>keys = embedded_sentence @ W_keys
values = embedded_sentence @ W_value
​
print("keys.shape:", keys.shape)
print("values.shape:", values.shape)</code></pre><p><strong>Out:</strong></p><pre><code>keys.shape: torch.Size([6, 2])
values.shape: torch.Size([6, 4])</code></pre><p><span>Now that we have all the required keys and values, we can proceed to the next step and compute the unnormalized attention weights </span><em><strong>ω</strong></em><span> (omega), which are illustrated in the figure below:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png" width="296" height="331.6363636363636" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/baf9e308-223b-429e-8527-a7b868003e8c_814x912.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:912,&quot;width&quot;:814,&quot;resizeWidth&quot;:296,&quot;bytes&quot;:156710,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Computing the unnormalized attention weights </span><em><strong>ω</strong></em><span> (omega)</span></figcaption></figure></div><p><span>As illustrated in the figure above, we compute </span><em><strong><span>ω</span><sub>i,j</sub></strong></em><span> as the dot product between the query and key sequences, </span><em><strong><span>ω</span><sub>i,j</sub><span> </span></strong><span>=</span></em><span> </span><em><strong><span>q</span><sup>(i)</sup></strong></em><span> </span><em><strong><span>k</span><sup>(j)</sup></strong></em><span>.</span></p><p>For example, we can compute the unnormalized attention weight for the query and 5th input element (corresponding to index position 4) as follows:</p><p><strong>In:</strong></p><pre><code>omega_24 = query_2.dot(keys[4])
print(omega_24)</code></pre><p><span>(Note that </span><em><strong>ω</strong></em><span> is the symbol for the Greek letter "omega", hence the code variable with the same name above.)</span></p><p><strong>Out:</strong></p><pre><code>tensor(1.2903)</code></pre><p><span>Since we will need those unnormalized attention weights </span><em><strong>ω</strong></em><span> to compute the actual attention weights later, let's compute the </span><em><strong>ω</strong></em><span> values for all input tokens as illustrated in the previous figure:</span></p><p><strong>In:</strong></p><pre><code>omega_2 = query_2 @ keys.T
print(omega_2)</code></pre><p><strong>Out:</strong></p><pre><code>tensor([-0.6004, &nbsp;3.4707, -1.5023, &nbsp;0.4991, &nbsp;1.2903, -1.3374])</code></pre><p><span>The subsequent step in self-attention is to normalize the unnormalized attention weights, </span><em><strong>ω</strong></em><span>, to obtain the normalized attention weights, </span><em><strong>α</strong></em><span> (alpha), by applying the softmax function. Additionally, 1/√{</span><em><strong><span>d</span><sub>k</sub></strong></em><span>} is used to scale </span><em><strong>ω</strong></em><span> before normalizing it through the softmax function, as shown below:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F292da0d0-8138-4bef-8265-939827f55caa_1774x1008.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F292da0d0-8138-4bef-8265-939827f55caa_1774x1008.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F292da0d0-8138-4bef-8265-939827f55caa_1774x1008.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F292da0d0-8138-4bef-8265-939827f55caa_1774x1008.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F292da0d0-8138-4bef-8265-939827f55caa_1774x1008.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F292da0d0-8138-4bef-8265-939827f55caa_1774x1008.png" width="618" height="351.0206043956044" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/292da0d0-8138-4bef-8265-939827f55caa_1774x1008.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:827,&quot;width&quot;:1456,&quot;resizeWidth&quot;:618,&quot;bytes&quot;:214098,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F292da0d0-8138-4bef-8265-939827f55caa_1774x1008.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F292da0d0-8138-4bef-8265-939827f55caa_1774x1008.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F292da0d0-8138-4bef-8265-939827f55caa_1774x1008.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F292da0d0-8138-4bef-8265-939827f55caa_1774x1008.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Computing the normalized attention weights </span><em><strong>α</strong></em></figcaption></figure></div><p><span>The scaling by </span><em><strong><span>d</span><sub>k</sub></strong></em><span> ensures that the Euclidean length of the weight vectors will be approximately in the same magnitude. This helps prevent the attention weights from becoming too small or too large, which could lead to numerical instability or affect the model's ability to converge during training.</span></p><p>In code, we can implement the computation of the attention weights as follows:</p><p><strong>In:</strong></p><pre><code>import torch.nn.functional as F
​
attention_weights_2 = F.softmax(omega_2 / d_k**0.5, dim=0)
print(attention_weights_2)</code></pre><p><strong>Out:</strong></p><pre><code>tensor([0.0386, 0.6870, 0.0204, 0.0840, 0.1470, 0.0229])</code></pre><p><span>Finally, the last step is to compute the context vector </span><em><strong><span>z</span><sup>(2)</sup></strong></em><span>, which is an attention-weighted version of our original query input </span><em><strong><span>x</span><sup>(2)</sup></strong></em><span>, including all the other input elements as its context via the attention weights:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png" width="664" height="363.010989010989" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:796,&quot;width&quot;:1456,&quot;resizeWidth&quot;:664,&quot;bytes&quot;:290098,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>The attention weights are specific to a certain input element. Here, we chose input element </span><em><strong>x</strong><span>(2).</span></em></figcaption></figure></div><p>In code, this looks like as follows:</p><p><strong>In:</strong></p><pre><code>context_vector_2 = attention_weights_2 @ values
​
print(context_vector_2.shape)
print(context_vector_2)</code></pre><p><strong>Out:</strong></p><pre><code>torch.Size([4])
tensor([0.5313, 1.3607, 0.7891, 1.3110])</code></pre><p><span>Note that this output vector has more dimensions (</span><em><strong><span>d</span><sub>v</sub><span> = 4</span></strong></em><span>) than the original input vector (</span><em><strong>d</strong><span> </span><strong>= 3</strong></em><span>) since we specified </span><em><strong><span>d</span><sub>v</sub></strong><span> </span><strong>&gt; d</strong></em><span> earlier; however, the embedding size choice </span><em><strong><span>d</span><sub>v</sub></strong></em><span> is arbitrary.</span></p><p><span>Now, to wrap up the code implementation of the self-attention mechanism in the previous sections above, we can summarize the previous code in a compact </span><code>SelfAttention</code><span> class:</span></p><p><strong>In:</strong></p><pre><code>import torch.nn as nn
​
class SelfAttention(nn.Module):
​
 &nbsp; &nbsp;def __init__(self, d_in, d_out_kq, d_out_v):
 &nbsp; &nbsp; &nbsp; &nbsp;super().__init__()
 &nbsp; &nbsp; &nbsp; &nbsp;self.d_out_kq = d_out_kq
 &nbsp; &nbsp; &nbsp; &nbsp;self.W_query = nn.Parameter(torch.rand(d_in, d_out_kq))
 &nbsp; &nbsp; &nbsp; &nbsp;self.W_key &nbsp; = nn.Parameter(torch.rand(d_in, d_out_kq))
 &nbsp; &nbsp; &nbsp; &nbsp;self.W_value = nn.Parameter(torch.rand(d_in, d_out_v))
​
 &nbsp; &nbsp;def forward(self, x):
 &nbsp; &nbsp; &nbsp; &nbsp;keys = x @ self.W_key
 &nbsp; &nbsp; &nbsp; &nbsp;queries = x @ self.W_query
 &nbsp; &nbsp; &nbsp; &nbsp;values = x @ self.W_value
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;attn_scores = queries @ keys.T &nbsp;# unnormalized attention weights &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;attn_weights = torch.softmax(
            attn_scores / self.d_out_kq**0.5, dim=-1
        )
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;context_vec = attn_weights @ values
 &nbsp; &nbsp; &nbsp; &nbsp;return context_vec</code></pre><p><span>Following PyTorch conventions, the </span><code>SelfAttention</code><span> class above initializes the self-attention parameters in the </span><code>__init__</code><span> method and computes attention weights and context vectors for all inputs via the </span><code>forward</code><span> method. We can use this class as follows:</span></p><p><strong>In:</strong></p><pre><code>torch.manual_seed(123)
​
# reduce d_out_v from 4 to 1, because we have 4 heads
d_in, d_out_kq, d_out_v = 3, 2, 4
​
sa = SelfAttention(d_in, d_out_kq, d_out_v)
print(sa(embedded_sentence))</code></pre><p><strong>Out:</strong></p><pre><code>tensor([[-0.1564, &nbsp;0.1028, -0.0763, -0.0764],
 &nbsp; &nbsp; &nbsp;  [ 0.5313, &nbsp;1.3607, &nbsp;0.7891, &nbsp;1.3110],
 &nbsp; &nbsp; &nbsp;  [-0.3542, -0.1234, -0.2627, -0.3706],
 &nbsp; &nbsp; &nbsp;  [ 0.0071, &nbsp;0.3345, &nbsp;0.0969, &nbsp;0.1998],
 &nbsp; &nbsp; &nbsp;  [ 0.1008, &nbsp;0.4780, &nbsp;0.2021, &nbsp;0.3674],
 &nbsp; &nbsp; &nbsp;  [-0.5296, -0.2799, -0.4107, -0.6006]], grad_fn=&lt;MmBackward0&gt;)</code></pre><p><span>If you look at the second row, you can see that it matches the values in </span><code>context_vector_2</code><span> from the previous section exactly: </span><code>tensor([0.5313, 1.3607, 0.7891, 1.3110])</code><span>.</span></p><p><span>In the very first figure, at the top of this article (also shown again for convenience below), we saw that transformers use a module called </span><em>multi-head attention</em><span>.</span></p><p>How does this "multi-head" attention module relate to the self-attention mechanism (scaled-dot product attention) we walked through above?</p><p>In scaled dot-product attention, the input sequence was transformed using three matrices representing the query, key, and value. These three matrices can be considered as a single attention head in the context of multi-head attention. The figure below summarizes this single attention head we covered and implemented previously:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png" width="500" height="303.91483516483515" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:885,&quot;width&quot;:1456,&quot;resizeWidth&quot;:500,&quot;bytes&quot;:178737,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Summarizing the self-attention mechanism implemented previously</figcaption></figure></div><p>As its name implies, multi-head attention involves multiple such heads, each consisting of query, key, and value matrices. This concept is similar to the use of multiple kernels in convolutional neural networks, producing feature maps with multiple output channels.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png" width="526" height="343.5618131868132" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:951,&quot;width&quot;:1456,&quot;resizeWidth&quot;:526,&quot;bytes&quot;:199096,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Multi-head attention: self-attention with multiple heads</figcaption></figure></div><p><span>To illustrate this in code, we can write a </span><code>MultiHeadAttentionWrapper</code><span> class for our previous </span><code>SelfAttention</code><span> class:</span></p><pre><code>class MultiHeadAttentionWrapper(nn.Module):
​
 &nbsp; &nbsp;def __init__(self, d_in, d_out_kq, d_out_v, num_heads):
 &nbsp; &nbsp; &nbsp; &nbsp;super().__init__()
 &nbsp; &nbsp; &nbsp; &nbsp;self.heads = nn.ModuleList(
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  [SelfAttention(d_in, d_out_kq, d_out_v) 
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for _ in range(num_heads)]
 &nbsp; &nbsp; &nbsp;  )
​
 &nbsp; &nbsp;def forward(self, x):
 &nbsp; &nbsp; &nbsp; &nbsp;return torch.cat([head(x) for head in self.heads], dim=-1)</code></pre><p><span>The </span><code>d_*</code><span> parameters are the same as before in the </span><code>SelfAttention</code><span> class -- the only new input parameter here is the number of attention heads:</span></p><ul><li><p><code>d_in</code><span>: Dimension of the input feature vector.</span></p></li><li><p><code>d_out_kq</code><span>: Dimension for both query and key outputs.</span></p></li><li><p><code>d_out_v</code><span>: Dimension for value outputs.</span></p></li><li><p><code>num_heads</code><span>: Number of attention heads.</span></p></li></ul><p><span>We initialize the </span><code>SelfAttention</code><span> class </span><code>num_heads</code><span> times using these input parameters. And we use a PyTorch </span><code>nn.ModuleList</code><span> to store these multiple </span><code>SelfAttention</code><span> instances.</span></p><p><span>Then, the </span><code>forward</code><span> pass involves applying each </span><code>SelfAttention</code><span> head (stored in </span><code>self.heads</code><span>) to the input </span><code>x</code><span> independently. The results from each head are then concatenated along the last dimension (</span><code>dim=-1</code><span>). Let's see it in action below!</span></p><p>First, let's suppose we have a single Self-Attention head with output dimension 1 to keep it simple for illustration purposes:</p><p><strong>In:</strong></p><pre><code>torch.manual_seed(123)
​
d_in, d_out_kq, d_out_v = 3, 2, 1
​
sa = SelfAttention(d_in, d_out_kq, d_out_v)
print(sa(embedded_sentence))</code></pre><p><strong>Out:</strong></p><pre><code>tensor([[-0.0185],
 &nbsp; &nbsp; &nbsp;  [ 0.4003],
 &nbsp; &nbsp; &nbsp;  [-0.1103],
 &nbsp; &nbsp; &nbsp;  [ 0.0668],
 &nbsp; &nbsp; &nbsp;  [ 0.1180],
 &nbsp; &nbsp; &nbsp;  [-0.1827]], grad_fn=&lt;MmBackward0&gt;)</code></pre><p>Now, let's extend this to 4 attention heads:</p><p><strong>In:</strong></p><pre><code>torch.manual_seed(123)
​
block_size = embedded_sentence.shape[1]
mha = MultiHeadAttentionWrapper(
    d_in, d_out_kq, d_out_v, num_heads=4
)
​
context_vecs = mha(embedded_sentence)
​
print(context_vecs)
print("context_vecs.shape:", context_vecs.shape)</code></pre><p><strong>Out:</strong></p><pre><code>tensor([[-0.0185, &nbsp;0.0170, &nbsp;0.1999, -0.0860],
 &nbsp; &nbsp; &nbsp;  [ 0.4003, &nbsp;1.7137, &nbsp;1.3981, &nbsp;1.0497],
 &nbsp; &nbsp; &nbsp;  [-0.1103, -0.1609, &nbsp;0.0079, -0.2416],
 &nbsp; &nbsp; &nbsp;  [ 0.0668, &nbsp;0.3534, &nbsp;0.2322, &nbsp;0.1008],
 &nbsp; &nbsp; &nbsp;  [ 0.1180, &nbsp;0.6949, &nbsp;0.3157, &nbsp;0.2807],
 &nbsp; &nbsp; &nbsp;  [-0.1827, -0.2060, -0.2393, -0.3167]], grad_fn=&lt;CatBackward0&gt;)
context_vecs.shape: torch.Size([6, 4])</code></pre><p>Based on the output above, you can see that the single self-attention head created earlier now represents the first column in the output tensor above.</p><p><span>Notice that the multi-head attention result is a 6×4-dimensional tensor: We have 6 input tokens and 4 self-attention heads, where each self-attention head returns a 1-dimensional output. Previously, in the Self-Attention section, we also produced a 6×4-dimensional tensor. That's because we set the output dimension to 4 instead of 1. In practice, why do we even need multiple attention heads if we can regulate the output embedding size in the </span><code>SelfAttention</code><span> class itself?</span></p><p>The distinction between increasing the output dimension of a single self-attention head and using multiple attention heads lies in how the model processes and learns from the data. While both approaches increase the capacity of the model to represent different features or aspects of the data, they do so in fundamentally different ways.</p><p>For instance, each attention head in multi-head attention can potentially learn to focus on different parts of the input sequence, capturing various aspects or relationships within the data. This diversity in representation is key to the success of multi-head attention.</p><p>Multi-head attention can also be more efficient, especially in terms of parallel computation. Each head can be processed independently, making it well-suited for modern hardware accelerators like GPUs or TPUs that excel at parallel processing.</p><p>In short, the use of multiple attention heads is not just about increasing the model's capacity but about enhancing its ability to learn a diverse set of features and relationships within the data. For example, the 7B Llama 2 model uses 32 attention heads.</p><p><span>In the code walkthrough above, we set </span><em><strong>d_q = d_k = 2</strong></em><span> and </span><em><strong>d_v = 4</strong></em><span>. In other words, we used the same dimensions for query and key sequences. While the value matrix </span><em><strong>W_v</strong></em><span> is often chosen to have the same dimension as the query and key matrices (such as in PyTorch's </span><a href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html" rel="">MultiHeadAttention</a><span> class), we can select an arbitrary number size for the value dimensions.</span></p><p>Since the dimensions are sometimes a bit tricky to keep track of, let's summarize everything we have covered so far in the figure below, which depicts the various tensor sizes for a single attention head.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb75a8df1-0a82-4f79-8e68-4fe16587063d_1474x1108.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb75a8df1-0a82-4f79-8e68-4fe16587063d_1474x1108.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb75a8df1-0a82-4f79-8e68-4fe16587063d_1474x1108.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb75a8df1-0a82-4f79-8e68-4fe16587063d_1474x1108.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb75a8df1-0a82-4f79-8e68-4fe16587063d_1474x1108.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb75a8df1-0a82-4f79-8e68-4fe16587063d_1474x1108.png" width="636" height="477.8736263736264" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b75a8df1-0a82-4f79-8e68-4fe16587063d_1474x1108.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1094,&quot;width&quot;:1456,&quot;resizeWidth&quot;:636,&quot;bytes&quot;:138883,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb75a8df1-0a82-4f79-8e68-4fe16587063d_1474x1108.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb75a8df1-0a82-4f79-8e68-4fe16587063d_1474x1108.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb75a8df1-0a82-4f79-8e68-4fe16587063d_1474x1108.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb75a8df1-0a82-4f79-8e68-4fe16587063d_1474x1108.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Another view of the self-attention mechanism implemented previously, with a focus on the matrix dimensions</figcaption></figure></div><p><span>Now, the illustration above corresponds to the </span><em>self</em><span>-attention mechanism used in transformers. One particular flavor of this attention mechanism we have yet to discuss is </span><em>cross</em><span>-attention.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9057444e-4934-4253-bc91-9e768d23b0c2_846x972.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9057444e-4934-4253-bc91-9e768d23b0c2_846x972.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9057444e-4934-4253-bc91-9e768d23b0c2_846x972.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9057444e-4934-4253-bc91-9e768d23b0c2_846x972.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9057444e-4934-4253-bc91-9e768d23b0c2_846x972.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9057444e-4934-4253-bc91-9e768d23b0c2_846x972.png" width="354" height="406.72340425531917" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9057444e-4934-4253-bc91-9e768d23b0c2_846x972.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:972,&quot;width&quot;:846,&quot;resizeWidth&quot;:354,&quot;bytes&quot;:252431,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9057444e-4934-4253-bc91-9e768d23b0c2_846x972.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9057444e-4934-4253-bc91-9e768d23b0c2_846x972.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9057444e-4934-4253-bc91-9e768d23b0c2_846x972.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9057444e-4934-4253-bc91-9e768d23b0c2_846x972.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>What is cross-attention, and how does it differ from self-attention?</p><p><span>In self-attention, we work with the same input sequence. In cross-attention, we mix or combine two </span><em>different</em><span> input sequences. In the case of the original transformer architecture above, that's the sequence returned by the encoder module on the left and the input sequence being processed by the decoder part on the right.</span></p><p><span>Note that in cross-attention, the two input sequences </span><code>x_1</code><span> and </span><code>x_2</code><span> can have different numbers of elements. However, their embedding dimensions must match.</span></p><p><span>The figure below illustrates the concept of cross-attention. If we set </span><code>x_1</code><em> = </em><code>x_2</code><span>, this is equivalent to self-attention.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa16d2bd0-984d-4224-9c4c-2f0cc144a599_1520x1108.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa16d2bd0-984d-4224-9c4c-2f0cc144a599_1520x1108.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa16d2bd0-984d-4224-9c4c-2f0cc144a599_1520x1108.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa16d2bd0-984d-4224-9c4c-2f0cc144a599_1520x1108.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa16d2bd0-984d-4224-9c4c-2f0cc144a599_1520x1108.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa16d2bd0-984d-4224-9c4c-2f0cc144a599_1520x1108.png" width="624" height="454.7142857142857" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a16d2bd0-984d-4224-9c4c-2f0cc144a599_1520x1108.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1061,&quot;width&quot;:1456,&quot;resizeWidth&quot;:624,&quot;bytes&quot;:166111,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa16d2bd0-984d-4224-9c4c-2f0cc144a599_1520x1108.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa16d2bd0-984d-4224-9c4c-2f0cc144a599_1520x1108.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa16d2bd0-984d-4224-9c4c-2f0cc144a599_1520x1108.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa16d2bd0-984d-4224-9c4c-2f0cc144a599_1520x1108.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>(Note that the queries usually come from the decoder, and the keys and values typically come from the encoder.)</p><p><span>How does that work in code? We will adopt and modify the </span><code>SelfAttention</code><span> class that we previously implemented in the Self-Attention section and only make some minor modifications:</span></p><p><strong>In:</strong></p><pre><code>class CrossAttention(nn.Module):
​
 &nbsp; &nbsp;def __init__(self, d_in, d_out_kq, d_out_v):
 &nbsp; &nbsp; &nbsp; &nbsp;super().__init__()
 &nbsp; &nbsp; &nbsp; &nbsp;self.d_out_kq = d_out_kq
 &nbsp; &nbsp; &nbsp; &nbsp;self.W_query = nn.Parameter(torch.rand(d_in, d_out_kq))
 &nbsp; &nbsp; &nbsp; &nbsp;self.W_key &nbsp; = nn.Parameter(torch.rand(d_in, d_out_kq))
 &nbsp; &nbsp; &nbsp; &nbsp;self.W_value = nn.Parameter(torch.rand(d_in, d_out_v))
​
 &nbsp; &nbsp;def forward(self, x_1, x_2): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # x_2 is new
 &nbsp; &nbsp; &nbsp; &nbsp;queries_1 = x_1 @ self.W_query
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;keys_2 = x_2 @ self.W_key &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# new
 &nbsp; &nbsp; &nbsp; &nbsp;values_2 = x_2 @ self.W_value &nbsp; &nbsp; &nbsp;# new
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;attn_scores = queries_1 @ keys_2.T # new 
 &nbsp; &nbsp; &nbsp; &nbsp;attn_weights = torch.softmax(attn_scores / self.d_out_kq**0.5, dim=-1)
 &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;context_vec = attn_weights @ values_2
 &nbsp; &nbsp; &nbsp; &nbsp;return context_vec</code></pre><p><span>The differences between the </span><code>CrossAttention</code><span> class and the previous </span><code>SelfAttention</code><span> class are as follows:</span></p><ul><li><p><span>The </span><code>forward</code><span> method takes two distinct inputs, </span><code>x_1</code><span> and </span><code>x_2</code><span>. The queries are derived from </span><code>x_1</code><span>, while the keys and values are derived from </span><code>x_2</code><span>. This means that the attention mechanism is evaluating the interaction between two different inputs.</span></p></li><li><p><span>The attention scores are calculated by taking the dot product of the queries (from </span><code>x_1</code><span>) and keys (from </span><code>x_2</code><span>).</span></p></li><li><p><span>Similar to </span><code>SelfAttention</code><span>, each context vector is a weighted sum of the values. However, in </span><code>CrossAttention</code><span>, these values are derived from the second input (</span><code>x_2</code><span>), and the weights are based on the interaction between </span><code>x_1</code><span> and </span><code>x_2</code><span>.</span></p></li></ul><p>Let's see it in action:</p><p><strong>In:</strong></p><pre><code>torch.manual_seed(123)
​
d_in, d_out_kq, d_out_v = 3, 2, 4
​
crossattn = CrossAttention(d_in, d_out_kq, d_out_v)
​
first_input = embedded_sentence
second_input = torch.rand(8, d_in)
​
print("First input shape:", first_input.shape)
print("Second input shape:", second_input.shape)</code></pre><p><strong>In:</strong></p><pre><code>First input shape: torch.Size([6, 3])
Second input shape: torch.Size([8, 3])</code></pre><p>Notice that the first and second inputs don't have to have the same number of tokens (here: rows) when computing cross-attention:</p><p><strong>In:</strong></p><pre><code>context_vectors = crossattn(first_input, second_input)
​
print(context_vectors)
print("Output shape:", context_vectors.shape)</code></pre><p><strong>Out:</strong></p><pre><code>tensor([[0.4231, 0.8665, 0.6503, 1.0042],
 &nbsp; &nbsp; &nbsp;  [0.4874, 0.9718, 0.7359, 1.1353],
 &nbsp; &nbsp; &nbsp;  [0.4054, 0.8359, 0.6258, 0.9667],
 &nbsp; &nbsp; &nbsp;  [0.4357, 0.8886, 0.6678, 1.0311],
 &nbsp; &nbsp; &nbsp;  [0.4429, 0.9006, 0.6775, 1.0460],
 &nbsp; &nbsp; &nbsp;  [0.3860, 0.8021, 0.5985, 0.9250]], grad_fn=&lt;MmBackward0&gt;)
Output shape: torch.Size([6, 4])</code></pre><p>We talked a lot about language transformers above. In the original transformer architecture, cross-attention is useful when we go from an input sentence to an output sentence in the context of language translation. The input sentence represents one input sequence, and the translation represent the second input sequence (the two sentences can different numbers of words).</p><p><span>Another popular model where cross-attention is used is Stable Diffusion. Stable Diffusion uses cross-attention between the generated image in the U-Net model and the text prompts used for conditioning as described in </span><a href="https://arxiv.org/abs/2112.10752" rel="">High-Resolution Image Synthesis with Latent Diffusion Models</a><span> -- the original paper that describes the Stable Diffusion model that was later adopted by Stability AI to implement the popular Stable Diffusion model.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4dfd356-1cfa-4601-a05d-57bc15024970_1204x590.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4dfd356-1cfa-4601-a05d-57bc15024970_1204x590.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4dfd356-1cfa-4601-a05d-57bc15024970_1204x590.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4dfd356-1cfa-4601-a05d-57bc15024970_1204x590.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4dfd356-1cfa-4601-a05d-57bc15024970_1204x590.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4dfd356-1cfa-4601-a05d-57bc15024970_1204x590.png" width="620" height="303.8205980066445" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b4dfd356-1cfa-4601-a05d-57bc15024970_1204x590.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:590,&quot;width&quot;:1204,&quot;resizeWidth&quot;:620,&quot;bytes&quot;:239081,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4dfd356-1cfa-4601-a05d-57bc15024970_1204x590.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4dfd356-1cfa-4601-a05d-57bc15024970_1204x590.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4dfd356-1cfa-4601-a05d-57bc15024970_1204x590.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4dfd356-1cfa-4601-a05d-57bc15024970_1204x590.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>In this section, we are adapting the previously discussed self-attention mechanism into a causal self-attention mechanism, specifically for GPT-like (decoder-style) LLMs that are used to generate text. This causal self-attention mechanism  is also often referred to as “masked self-attention”. In the original transformer architecture, it corresponds to the “masked multi-head attention” module — for simplicity, we will look at a single attention head in this section, but the same concept generalizes to multiple heads.</p><p>Causal self-attention ensures that the  outputs for a certain position in a sequence is based only on the known outputs at previous positions and not on future positions. In simpler terms, it ensures that the prediction for each next word should only depend on the preceding words. To achieve this in GPT-like LLMs, for each token processed, we mask out the future tokens, which come after the current token in the input text.</p><p>The application of a causal mask to the attention weights for hiding future input tokens in the inputs is illustrated in the figure below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png" width="518" height="331.5769230769231" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:932,&quot;width&quot;:1456,&quot;resizeWidth&quot;:518,&quot;bytes&quot;:611309,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>To illustrate and implement causal self-attention, let's work with the unweighted attention scores and attention weights from the previous section. First, we quickly recap the computation of the attention scores from the previous </span><em>Self-Attention</em><span> section:</span></p><p><strong>In:</strong></p><pre><code>torch.manual_seed(123)
​
d_in, d_out_kq, d_out_v = 3, 2, 4
​
W_query = nn.Parameter(torch.rand(d_in, d_out_kq))
W_key &nbsp; = nn.Parameter(torch.rand(d_in, d_out_kq))
W_value = nn.Parameter(torch.rand(d_in, d_out_v))
​
x = embedded_sentence
​
keys = x @ W_key
queries = x @ W_query
values = x @ W_value
​
# attn_scores are the "omegas", 
# the unnormalized attention weights
attn_scores = queries @ keys.T 
​
print(attn_scores)
print(attn_scores.shape)</code></pre><p><strong>Out:</strong></p><pre><code>tensor([[ 0.0613, -0.3491, &nbsp;0.1443, -0.0437, -0.1303, &nbsp;0.1076],
 &nbsp; &nbsp; &nbsp;  [-0.6004, &nbsp;3.4707, -1.5023, &nbsp;0.4991, &nbsp;1.2903, -1.3374],
 &nbsp; &nbsp; &nbsp;  [ 0.2432, -1.3934, &nbsp;0.5869, -0.1851, -0.5191, &nbsp;0.4730],
 &nbsp; &nbsp; &nbsp;  [-0.0794, &nbsp;0.4487, -0.1807, &nbsp;0.0518, &nbsp;0.1677, -0.1197],
 &nbsp; &nbsp; &nbsp;  [-0.1510, &nbsp;0.8626, -0.3597, &nbsp;0.1112, &nbsp;0.3216, -0.2787],
 &nbsp; &nbsp; &nbsp;  [ 0.4344, -2.5037, &nbsp;1.0740, -0.3509, -0.9315, &nbsp;0.9265]],
 &nbsp; &nbsp; &nbsp; grad_fn=&lt;MmBackward0&gt;)
torch.Size([6, 6])</code></pre><p><span>Similar to the </span><em>Self-Attention</em><span> section before, the output above is a 6×6 tensor containing these pairwise unnormalized attention weights (also called attention scores) for the 6 input tokens.</span></p><p>Previously, we then computed the scaled dot-product attention via the softmax function as follows:</p><p><strong>In:</strong></p><pre><code>attn_weights = torch.softmax(attn_scores / d_out_kq**0.5, dim=1)
print(attn_weights)</code></pre><p><strong>Out:</strong></p><pre><code>tensor([[0.1772, 0.1326, 0.1879, 0.1645, 0.1547, 0.1831],
 &nbsp; &nbsp; &nbsp;  [0.0386, 0.6870, 0.0204, 0.0840, 0.1470, 0.0229],
 &nbsp; &nbsp; &nbsp;  [0.1965, 0.0618, 0.2506, 0.1452, 0.1146, 0.2312],
 &nbsp; &nbsp; &nbsp;  [0.1505, 0.2187, 0.1401, 0.1651, 0.1793, 0.1463],
 &nbsp; &nbsp; &nbsp;  [0.1347, 0.2758, 0.1162, 0.1621, 0.1881, 0.1231],
 &nbsp; &nbsp; &nbsp;  [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],
 &nbsp; &nbsp; &nbsp; grad_fn=&lt;SoftmaxBackward0&gt;)</code></pre><p><span>The 6×6 output above represents the attention weights, which we also computed in the </span><em>Self-Attention</em><span> section before.</span></p><p>Now, in GPT-like LLMs, we train the model to read and generate one token (or word) at a time, from left to right. If we have a training text sample like "Life is short eat desert first" we have the following setup, where the context vectors for the word to the right side of the arrow should only incorporate itself and the previous words:</p><ul><li><p>"Life" → "is"</p></li><li><p>"Life is" → "short"</p></li><li><p>"Life is short" → "eat"</p></li><li><p>"Life is short eat" → "desert"</p></li><li><p>"Life is short eat desert" → "first"</p></li></ul><p>The simplest way to achieve this setup above is to mask out all future tokens by applying a mask to the attention weight matrix above the diagonal, as illustrated in the figure below. This way, “future” words will not be included when creating the context vectors, which are created as a attention-weighted sum over the inputs.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png" width="516" height="296.91147540983604" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e1317a05-3542-4158-94bf-085109a5793a_1220x702.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:702,&quot;width&quot;:1220,&quot;resizeWidth&quot;:516,&quot;bytes&quot;:379180,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Attention weights above the diagonal should be masked out</figcaption></figure></div><p><span>In code, we can achieve this via PyTorch's </span><a href="https://pytorch.org/docs/stable/generated/torch.tril.html#" rel="">tril</a><span> function, which we first use to create a mask of 1's and 0's:</span></p><p><strong>In:</strong></p><pre><code>block_size = attn_scores.shape[0]
mask_simple = torch.tril(torch.ones(block_size, block_size))
print(mask_simple)</code></pre><p><strong>Out:</strong></p><pre><code>tensor([[1., 0., 0., 0., 0., 0.],
 &nbsp; &nbsp; &nbsp;  [1., 1., 0., 0., 0., 0.],
 &nbsp; &nbsp; &nbsp;  [1., 1., 1., 0., 0., 0.],
 &nbsp; &nbsp; &nbsp;  [1., 1., 1., 1., 0., 0.],
 &nbsp; &nbsp; &nbsp;  [1., 1., 1., 1., 1., 0.],
 &nbsp; &nbsp; &nbsp;  [1., 1., 1., 1., 1., 1.]])</code></pre><p>Next, we multiply the attention weights with this mask to zero out all the attention weights above the diagonal:</p><p><strong>In:</strong></p><pre><code>masked_simple = attn_weights*mask_simple
print(masked_simple)</code></pre><p><strong>Out:</strong></p><pre><code>tensor([[0.1772, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
 &nbsp; &nbsp; &nbsp;  [0.0386, 0.6870, 0.0000, 0.0000, 0.0000, 0.0000],
 &nbsp; &nbsp; &nbsp;  [0.1965, 0.0618, 0.2506, 0.0000, 0.0000, 0.0000],
 &nbsp; &nbsp; &nbsp;  [0.1505, 0.2187, 0.1401, 0.1651, 0.0000, 0.0000],
 &nbsp; &nbsp; &nbsp;  [0.1347, 0.2758, 0.1162, 0.1621, 0.1881, 0.0000],
 &nbsp; &nbsp; &nbsp;  [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],
 &nbsp; &nbsp; &nbsp; grad_fn=&lt;MulBackward0&gt;)</code></pre><p>While the above is one way to mask out future words, notice that the attention weights in each row don't sum to one anymore. To mitigate that, we can normalize the rows such that they sum up to 1 again, which is a standard convention for attention weights:</p><p><strong>In:</strong></p><pre><code>row_sums = masked_simple.sum(dim=1, keepdim=True)
masked_simple_norm = masked_simple / row_sums
print(masked_simple_norm)</code></pre><p><strong>Out:</strong></p><pre><code>tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
 &nbsp; &nbsp; &nbsp;  [0.0532, 0.9468, 0.0000, 0.0000, 0.0000, 0.0000],
 &nbsp; &nbsp; &nbsp;  [0.3862, 0.1214, 0.4924, 0.0000, 0.0000, 0.0000],
 &nbsp; &nbsp; &nbsp;  [0.2232, 0.3242, 0.2078, 0.2449, 0.0000, 0.0000],
 &nbsp; &nbsp; &nbsp;  [0.1536, 0.3145, 0.1325, 0.1849, 0.2145, 0.0000],
 &nbsp; &nbsp; &nbsp;  [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],
 &nbsp; &nbsp; &nbsp; grad_fn=&lt;DivBackward0&gt;)</code></pre><p>As we can see, the attention weights in each row now sum up to 1.</p><p>Normalizing attention weights in neural networks, such as in transformer models, is advantageous over unnormalized weights for two main reasons. First, normalized attention weights that sum to 1 resemble a probability distribution. This makes it easier to interpret the model's attention to various parts of the input in terms of proportions. Second, by constraining the attention weights to sum to 1, this normalization helps control the scale of the weights and gradients to improve the training dynamics.</p><p><strong>More efficient masking without renormalization</strong></p><p>In the causal self-attention procedure we coded above, we first compute the attention scores, then compute the attention weights, mask out attention weights above the diagonal, and lastly renormalize the attention weights. This is summarized in the figure below:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png" width="1456" height="272" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:272,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:74590,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>The previously implemented causal self-attention procedure</figcaption></figure></div><p>Alternatively, there is a more efficient way to achieve the same results. In this approach, we take the attention scores and replace the values above the diagonal with negative infinity before the values are input into the softmax function to compute the attention weights. This is summarized in the figure below:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png" width="538" height="137.8178368121442" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:270,&quot;width&quot;:1054,&quot;resizeWidth&quot;:538,&quot;bytes&quot;:57470,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>An alternative, more efficient approach to implementing causal self-attention</figcaption></figure></div><p>We can code up this procedure in PyTorch as follows, starting with masking the attention scores above the diagonal:</p><p><strong>In:</strong></p><pre><code>mask = torch.triu(torch.ones(block_size, block_size))
masked = attn_scores.masked_fill(mask.bool(), -torch.inf)
print(masked)</code></pre><p><span>The code above first creates a </span><code>mask</code><span> with 0s below the diagonal, and 1s above the diagonal. Here, </span><code>torch.triu</code><span> (*(</span><strong>u</strong><span>pper </span><strong>tri</strong><span>angle) retains the elements on and above the main diagonal of a matrix, zeroing out the elements below it, thus preserving the upper triangular portion. In contrast, </span><code>torch.tril</code><span> (</span><strong>l</strong><span>ower </span><strong>t</strong><span>riangle) keeps the elements on and below the main diagonal.</span></p><p><span>The </span><code>masked_fill</code><span> method then replaces all the elements on and above the diagonal via positive mask values (1s) with </span><code>-torch.inf</code><span>, with the results being shown below.</span></p><p><strong>Out:</strong></p><pre><code>tensor([[ 0.0613, &nbsp;  -inf, &nbsp;  -inf, &nbsp;  -inf, &nbsp;  -inf, &nbsp;  -inf],
 &nbsp; &nbsp; &nbsp;  [-0.6004,  3.4707, &nbsp;  -inf, &nbsp;  -inf, &nbsp;  -inf, &nbsp;  -inf],
 &nbsp; &nbsp; &nbsp;  [ 0.2432, -1.3934,  0.5869, &nbsp;  -inf, &nbsp;  -inf, &nbsp;  -inf],
 &nbsp; &nbsp; &nbsp;  [-0.0794,  0.4487, -0.1807,  0.0518, &nbsp;  -inf, &nbsp;  -inf],
 &nbsp; &nbsp; &nbsp;  [-0.1510,  0.8626, -0.3597,  0.1112,  0.3216, &nbsp;  -inf],
 &nbsp; &nbsp; &nbsp;  [ 0.4344, -2.5037,  1.0740, -0.3509, -0.9315,  0.9265]],
 &nbsp; &nbsp; &nbsp; grad_fn=&lt;MaskedFillBackward0&gt;)</code></pre><p>Then, all we have to do is to apply the softmax function as usual to obtain the normalized and masked attention weights:</p><p><strong>In:</strong></p><pre><code>attn_weights = torch.softmax(masked / d_out_kq**0.5, dim=1)
print(attn_weights)</code></pre><p><strong>Out:</strong></p><pre><code>tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
 &nbsp; &nbsp; &nbsp;  [0.0532, 0.9468, 0.0000, 0.0000, 0.0000, 0.0000],
 &nbsp; &nbsp; &nbsp;  [0.3862, 0.1214, 0.4924, 0.0000, 0.0000, 0.0000],
 &nbsp; &nbsp; &nbsp;  [0.2232, 0.3242, 0.2078, 0.2449, 0.0000, 0.0000],
 &nbsp; &nbsp; &nbsp;  [0.1536, 0.3145, 0.1325, 0.1849, 0.2145, 0.0000],
 &nbsp; &nbsp; &nbsp;  [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],
 &nbsp; &nbsp; &nbsp; grad_fn=&lt;SoftmaxBackward0&gt;)  </code></pre><p><span>Why does this work? The softmax function, applied in the last step, converts the input values into a probability distribution. When </span><code>-inf</code><span> is present in the inputs, softmax effectively treats them as zero probability. This is because </span><code>e^(-inf)</code><span> approaches 0, and thus these positions contribute nothing to the output probabilities.</span></p><p>IIn this article, we explored the inner workings of self-attention through a step-by-step coding approach. Using this as a foundation, we then looked into multi-head attention, a fundamental component of large language transformers.</p><p>We then also coded cross-attention, a variant of self-attention that is particularly effective when applied between two distinct sequences. And lastly, we coded causal self-attention, a concept crucial for generating coherent and contextually appropriate sequences in decoder-style LLMs such as GPT and Llama.</p><p>By coding these complex mechanisms from scratch, you hopefully gained a good understanding of the inner workings of the self-attention mechanism used in transformers and LLMs.</p><p><span>(Note that the code presented in this article is intended for illustrative purposes. If you plan to implement self-attention for training LLMs, I recommend considering optimized implementations like </span><a href="https://arxiv.org/abs/2307.08691" rel="">Flash Attention</a><span>, which reduce memory footprint and computational load.)</span></p><p><span>If you liked this article, my </span><a href="http://mng.bz/amjo" rel="">Build a Large Language Model from Scratch</a><span> book explains how LLMs work using a similar (but more detailed) from-scratch approach. This includes coding the data processing steps, LLM architecture, pretraining, finetuning, and alignment stages.</span></p><p><span>The book is currently part of Manning's early access program, where new chapters will be released regularly. (Purchasers of the currently discounted early access version through Manning will also receive the final book upon its release.) The corresponding code is </span><a href="https://github.com/rasbt/LLMs-from-scratch" rel="">available on GitHub</a><span>.</span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cars suck, man (183 pts)]]></title>
            <link>https://petargyurov.com/2024-01-14/cars-suck-man</link>
            <guid>38990080</guid>
            <pubDate>Sun, 14 Jan 2024 13:08:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://petargyurov.com/2024-01-14/cars-suck-man">https://petargyurov.com/2024-01-14/cars-suck-man</a>, See on <a href="https://news.ycombinator.com/item?id=38990080">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p><span>Published on</span>
    <span datetime="2024-01-14 00:00:00 +0000">January 14, 2024</span>
     
  </p>

  
  
  

  <p>What better way to start off the new year than with a little rant.</p>

<p>I am not car aficionado, in fact, I am probably less interested in cars than the average person. As the tech bros have slowly crept into the automotive industry, I can’t help but feel like I’m witnessing a <em>car crash</em> in slow motion.</p>

<h3 id="cars-are-the-smart-tvs-of-today">Cars are the Smart TVs of today</h3>

<p>Have you tried shopping for a TV in the past 5 years? Actually, scratch that – have you <em>used</em> a “smart” TV in the past 5 years? – it’s awful. Sure, displays are getting better, but manufacturers insist on cramming bloatware and spyware, embellished in the worst UI imaginable, running on a processor designed for a smartwatch.</p>

<p><img src="https://petargyurov.com/assets/images/cars-suck-man/smart-tv-pooramid.png" width="200"> <em>The Essential Smart TV Pooramid</em></p>

<p>Picture quality is at phenomenal levels, even on low-cost offerings, but those inky blanks come at the cost of compromised privacy and ad-infested UI.</p>

<p>So what do cars have to do with any of this? Every product that takes a concrete position in human dynamics, be it a TV, a fridge or a car, follows a relatively predictable evolution: it starts of limited but promising, gets meaningful upgrades, and eventually slides into into the valley of “enshittification”. Few are those who climb their way out of the shit-pit, fewer still are those who avoid it entirely.</p>

<p><img src="https://petargyurov.com/assets/images/cars-suck-man/valley-of-enshittification.png"> <em>100% science based chart</em></p>

<h3 id="whats-so-wrong-with-cars-then">What’s so wrong with cars then?</h3>

<p>We live in an era of peak automotive power, utility and luxury. Much like with TVs, we probably reached the first plateau of quality: image clarity, super thin displays, etc. Then we started getting things nobody asked for: frame rate interpolation (why God, why!?), insane image presets, automatic audio adjustments. I won’t talk about 3D TVs. Let’s just pretend that never happened and that drawer in your cabinet isn’t full of dusty 3D glasses that haven’t seen the light of day for the last decade.</p>

<p>Cars are entering the “frame-interpolation” era, the slippery slope into the loathed, anti-consumer, anti-logic Shit Pit.</p>

<p>Let’s just fire through this:</p>

<ul>
  <li>Hiding critical/often-used buttons behind touchscreens for the sake of minimalism (and cost?)</li>
  <li>On that note, poor physical layout in general (genuinely thinking that you can rate cars’ on a “hazard-light-button-placement index”)</li>
  <li>YOUR LED LIGHTS ARE TOO BRIGHT GODDAMIT</li>
  <li>Cars are getting too big; why are manufacturers pushing “crossovers” and SUVs on consumers and why are consumers buying them? I could write pages on this point alone.</li>
  <li>Combine the two previous points for peak blindness</li>
  <li>“Low profile” rims/tyres/wheels/whatever: cost a fortune and more prone to getting scratched</li>
  <li>Subscription style plans trying to creep their way in</li>
  <li>ALL the privacy violations you could think of</li>
  <li>Rubbish software and UI all round: it’s 2024 and I dread it every time I need to pair my phone to a car</li>
  <li>That weird glossy pastel paintwork that’s so popular all of a sudden (I’m allowed one subjective point on here, OK?)</li>
</ul>

<h3 id="the-car-of-the-future">The car of the future</h3>

<p>I don’t think we’ve reached the depths of the brown ocean… yet. Who’s to blame here? Consumers? Manufacturers?</p>

<p>I know I sound like a luddite here but the same way I just want a good display from my TV, I want less from my car, not more.</p>

<p>I miss my 2005 Toyota Corolla 😢</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Free unexpected MIT courses to kick start the new year (114 pts)]]></title>
            <link>https://medium.com/open-learning/free-unexpected-mit-courses-to-kick-start-the-new-year-c226e444e61a</link>
            <guid>38988987</guid>
            <pubDate>Sun, 14 Jan 2024 09:49:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/open-learning/free-unexpected-mit-courses-to-kick-start-the-new-year-c226e444e61a">https://medium.com/open-learning/free-unexpected-mit-courses-to-kick-start-the-new-year-c226e444e61a</a>, See on <a href="https://news.ycombinator.com/item?id=38988987">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><h2 id="43a1">From heavy metal to poker to the history of Samurai, make 2024 your year with these online course materials and educational resources.</h2><div><a rel="noopener follow" href="https://medium.com/@mitopenlearning?source=post_page-----c226e444e61a--------------------------------"><div aria-hidden="false"><p><img alt="MIT Open Learning" src="https://miro.medium.com/v2/resize:fill:88:88/1*WGiUuRRlXnFLwkvtM9c7KA.png" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a><a href="https://medium.com/open-learning?source=post_page-----c226e444e61a--------------------------------" rel="noopener follow"><div aria-hidden="false"><p><img alt="MIT Open Learning" src="https://miro.medium.com/v2/resize:fill:48:48/1*RJQZG2O7JCd0njpHuP5r4Q.jpeg" width="24" height="24" loading="lazy" data-testid="publicationPhoto"></p></div></a></div></div><figure><figcaption>Image: Jacek Kita via iStock</figcaption></figure><p id="3504">Are you looking to learn a new language, explore musical genres, or improve your poker game in 2024? We’ve compiled 13 free online courses from MIT OpenCourseWare to kick start the new year on a positive note — from Italian, The Beatles, and heavy metal to poker theory, the history of Samurai, and different ways to understand climate change.</p><h2 id="e177">Learn (or create) a new language</h2><h2 id="2925"><a href="https://ocw.mit.edu/courses/24-917-conlangs-how-to-construct-a-language-fall-2018/" rel="noopener ugc nofollow" target="_blank">How to Construct a Language</a></h2><p id="9bbe">Create your own language while considering the basic linguistic characteristics of various languages of the world. This course material explores languages that have been deliberately constructed, including Esperanto, Klingon, and Tolkien’s Elvish.</p><h2 id="3ceb"><a href="https://ocw.mit.edu/courses/es-s41-speak-italian-with-your-mouth-full-spring-2012/" rel="noopener ugc nofollow" target="_blank">Speak Italian with your Mouth Full</a></h2><p id="69a3">Learn basic conversational Italian, and discover Italian culture and the Mediterranean diet. Each topic in this course is based on bite-sized acquisition of Italian language and culture, as well as the preparation of a delicious dish.</p><h2 id="b464">Get creative with a music course</h2><h2 id="dde8"><a href="https://ocw.mit.edu/courses/res-21m-001-heavy-metal-101-january-iap-2023/" rel="noopener ugc nofollow" target="_blank">Heavy Metal 101</a></h2><p id="14fd">Learn everything you ever wanted to know about heavy metal music, including musicology, songwriting tropes, and how the genre tackles some of today’s biggest socio-political challenges — among other topics.</p><h2 id="f015"><a href="https://ocw.mit.edu/courses/21m-299-the-beatles-fall-2017/" rel="noopener ugc nofollow" target="_blank">The Beatles</a></h2><p id="109d">Delve into the music of The Beatles, from the band’s early years to their break-up. Learn how The Beatles’ musical style changed from skiffle and rock to studio-based experimentation. This course explores the cultural influences that helped to shape The Beatles and the band’s influence worldwide.</p><h2 id="2a6c"><a href="https://ocw.mit.edu/courses/21m-250-beethoven-to-mahler-spring-2014/pages/beethoven-concert/" rel="noopener ugc nofollow" target="_blank">Beethoven to Mahler</a></h2><p id="9ce2">Watch a recording of a concert of Beethoven violin sonatas on period instruments. This course also includes a discussion about Susanna Ogata and Ian Watson’s performance.</p><h2 id="efb0">Take a playful management course</h2><h2 id="dd3b"><a href="https://ocw.mit.edu/courses/15-s50-poker-theory-and-analytics-january-iap-2015/" rel="noopener ugc nofollow" target="_blank">Poker Theory and Analytics</a></h2><p id="9fc1">Improve your poker game by exploring theory and the applications of poker analytics to investment management and trading.</p><h2 id="8e4e"><a href="https://ocw.mit.edu/courses/15-s50-how-to-win-at-texas-holdem-poker-january-iap-2016/" rel="noopener ugc nofollow" target="_blank">How to Win at Texas Hold’em Poker</a></h2><p id="f209">Become a seasoned player at Texas Hold’em, one of the most popular variants of poker. This course covers general concepts, as well as the poker and math concepts needed to play the card game on a professional level.</p><h2 id="d1be">Travel to the past with a history course</h2><h2 id="123e"><a href="https://ocw.mit.edu/courses/21h-154-inventing-the-samurai-fall-2022/" rel="noopener ugc nofollow" target="_blank">Inventing the Samurai</a></h2><p id="64d8">Discover the historical origins of the Japanese warrior class and its reinvention. This course covers the rise of the imperial court, interactions with the broader world, and the establishment of a warrior-dominated state.</p><h2 id="65d0">Expand your mind with a science course</h2><h2 id="5e77"><a href="https://ocw.mit.edu/courses/9-401-tools-for-robust-science-fall-2022" rel="noopener ugc nofollow" target="_blank">Tools for Robust Science</a></h2><p id="b85e">Explore cutting-edge tools and techniques designed to revitalize and enhance scientific practices in the cognitive and neuro sciences. This course illustrates how to identify obstacles to conducting robust scientific research, as well as tools to overcome these obstacles.</p><h2 id="c66c"><a href="https://ocw.mit.edu/courses/res-2-008-thermodynamics-and-climate-change-summer-2020/" rel="noopener ugc nofollow" target="_blank">Thermodynamics and Climate Change</a></h2><p id="313a">Learn the three laws of thermodynamics through their connection with climate change. Explore concepts such as entropy and enthalpy, applications like energy conversion and energy storage, and investigate the causes and effects of global warming from a thermodynamics perspective.</p><h2 id="e95f"><a href="https://ocw.mit.edu/courses/3-091-introduction-to-solid-state-chemistry-fall-2018/pages/why-this-matters-videos/" rel="noopener ugc nofollow" target="_blank">Introduction to Solid-State Chemistry</a></h2><p id="2b1a">This course focuses on how chemistry is connected to important innovations — and sometimes unexpected consequences — in science and in life. You might start with the “Why This Matters” series of short excerpts from the course’s lecture videos to learn about real-world applications and creative directions for research.</p><h2 id="c0f0">Educational resources beyond the virtual classroom</h2><h2 id="eb61"><a href="https://ocw.mit.edu/courses/res-11-003-climate-justice-instructional-toolkit-fall-2023/" rel="noopener ugc nofollow" target="_blank">Climate Justice Instructional Toolkit</a></h2><p id="103a">These resources are made especially for educators who want to integrate climate justice content into courses across many disciplines, and they’re great for anyone exploring the many facets of climate justice.</p><h2 id="8a40"><a href="https://ocw.mit.edu/courses/res-2-006-girls-who-build-cameras-summer-2016/" rel="noopener ugc nofollow" target="_blank">Girls Who Build Cameras</a></h2><p id="7b25">This one-day, hands-on workshop for high school girls provides an introduction to camera physics and technology. Learn how to tear down old dSLR cameras, build a Raspberry Pi camera, design Instagram filters and Photoshop tools, and run your own version of this workshop.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Ultimate Docker Cheat Sheet (143 pts)]]></title>
            <link>https://devopscycle.com/blog/the-ultimate-docker-cheat-sheet/</link>
            <guid>38988960</guid>
            <pubDate>Sun, 14 Jan 2024 09:45:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devopscycle.com/blog/the-ultimate-docker-cheat-sheet/">https://devopscycle.com/blog/the-ultimate-docker-cheat-sheet/</a>, See on <a href="https://news.ycombinator.com/item?id=38988960">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-animation="" data-delay="0">
				
<div>
		<p>Get your Docker Cheat Sheet as&nbsp;<a href="http://devopscycle.com/wp-content/uploads/sites/4/2023/12/the-ultimate-docker-cheat-sheet-1.pdf" target="_blank" rel="noreferrer noopener" data-type="attachment" data-id="902">PDF</a>&nbsp;or as an&nbsp;<a href="http://devopscycle.com/wp-content/uploads/sites/4/2023/11/the-ultimate-docker-cheat-sheet-4.png" target="_blank" rel="noreferrer noopener" data-type="attachment" data-id="900">image</a>. To follow this article, make sure your development machine has&nbsp;<a href="https://docs.docker.com/get-docker/" target="_blank" rel="noreferrer noopener">Docker installed</a>. In this blog post, we write our own Dockerfiles, learn how to create images, and finally run them as container. The complete source code is available on&nbsp;<a href="https://github.com/aichbauer/the-ultimate-docker-cheat-sheet" target="_blank" rel="noreferrer noopener">GitHub</a>.</p>

	</div>



<h2>Table of Contents</h2>
<div>
		<ul>
<li><a href="#download-the-ultimate-docker-cheat-sheet">Download the ultimate Docker Cheat Sheet</a></li>
<li><a href="#what-is-the-difference-between-a-dockerfile-an-image-and-a-container">What is the difference between a Dockerfile, an image and a container?</a></li>
<li><a href="#how-do-you-create-a-dockerfile">How do you create a Dockerfile?</a>
<ul>
<li><a href="#what-is-a-multistage-dockerfile">What is a multistage Dockerfile?</a></li>
</ul>
</li>
<li><a href="#how-do-you-create-a-docker-image">How do you create a Docker image?</a>
<ul>
<li><a href="#how-do-you-list-all-images">How do you list all images?</a></li>
<li><a href="#how-do-you-name-your-images">How do you name your images?</a></li>
</ul>
</li>
<li><a href="#how-do-you-create-a-container">How do you create a container?</a>
<ul>
<li><a href="#how-do-you-run-containers-in-the-background">How do you run containers in the background?</a></li>
<li><a href="#how-do-you-list-all-containers">How do you list all containers?</a></li>
<li><a href="#how-do-you-stop-and-remove-containers">How do you stop and remove containers?</a></li>
<li><a href="#what-if-i-killed-the-terminal-but-the-container-is-still-running">What if I killed the terminal, but the container is still running?</a></li>
<li><a href="#how-do-you-access-containers-from-the-host-system">How do you access containers from the host system?</a></li>
<li><a href="#how-do-you-publish-ports">How do you publish ports?</a></li>
<li><a href="#how-do-you-access-a-running-container">How do you access a running container?</a></li>
</ul>
</li>
<li><a href="#how-do-you-persist-data-with-docker-volumes">How do you persist data with Docker volumes?</a>
<ul>
<li><a href="#how-do-you-list-all-volumes">How do you list all volumes?</a></li>
</ul>
</li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
	</div>



<h2 id="download-the-ultimate-docker-cheat-sheet">Download The Ultimate Docker Cheat Sheet</h2>
<div>
	<p>Please&nbsp;<a href="http://devopscycle.com/wp-content/uploads/sites/4/2023/11/the-ultimate-docker-cheat-sheet-1.pdf" target="_blank" rel="noreferrer noopener" data-type="attachment" data-id="902">download</a>&nbsp;your Docker Cheat Sheet to follow along with this article. You are also welcome to share it with your colleagues and friends.</p>
</div>



<div data-max-width="100%" data-max-width-mobile="default" data-shadow="none" data-animation="none"> 
          <p><a href="http://devopscycle.com/wp-content/uploads/sites/4/2023/11/the-ultimate-docker-cheat-sheet-4.png" target="_blank">
              <img data-delay="0" height="1087" width="768" data-animation="none" src="https://devopscycle.com/wp-content/uploads/sites/4/2023/11/the-ultimate-docker-cheat-sheet-copy-768x1087.webp" alt="The Ultimate Docker Cheat Sheet" srcset="https://devopscycle.com/wp-content/uploads/sites/4/2023/11/the-ultimate-docker-cheat-sheet-copy-768x1087.webp 768w, https://devopscycle.com/wp-content/uploads/sites/4/2023/11/the-ultimate-docker-cheat-sheet-copy-212x300.webp 212w, https://devopscycle.com/wp-content/uploads/sites/4/2023/11/the-ultimate-docker-cheat-sheet-copy-724x1024.webp 724w, https://devopscycle.com/wp-content/uploads/sites/4/2023/11/the-ultimate-docker-cheat-sheet-copy.webp 800w" sizes="(max-width: 768px) 100vw, 768px">
            </a>
          </p>
        </div><h2 id="what-is-the-difference-between-a-dockerfile-an-image-and-a-container">Do You Want More Resources Like This?</h2>
<div>
	<p>Be the first to join our new <a href="https://discord.gg/7xpRbG2gY9" target="_blank" rel="noopener">community</a> and stay up-to-date on the latest DevOps topics (free cookies for the first 42 arrivals!). Don’t miss out on any new resources by signing up for our <a href="#newsletter">newsletter</a>. Get access to our latest resources and insights directly in your inbox!</p>
</div>



<h2>What Is The Difference Between A Dockerfile, An Image And A Container?</h2>
<div>
	<p>All these things build on top of each other. You need a Dockerfile to create an image, and you need an Image to create a Container.</p>
</div>




<div>
		<ul>
<li><strong>Dockerfile:</strong>&nbsp;The first step in using Docker is writing a Dockerfile. It is an essential blueprint for constructing Docker images. It is a text file, is usually named “Dockerfile” without any extension, and contains a series of instructions. Each line in this file represents a new instruction, forming a stack of layers. Each layer is cache-able. When you build an image twice, it will use the cache. When you change a line in the file, it rebuilds all instructions after and including the change.</li>
<li><strong>Image:</strong>&nbsp;Building a Dockerfile outputs a Docker image. You can think of an image like an executable. Just like clicking an icon (executable) on your desktop to launch an application. You can start an image to launch a container. The Docker image encapsulates your application code and all its dependencies. This includes the runtime and system libraries. It is a self-contained unit that ensures consistency and portability across various environments. For example, your development machine and your production server.</li>
<li><strong>Container:</strong>&nbsp;This is a dynamic, running instance of a Docker image. An executed image spawns a container with the command in the Dockerfile. Important to note: one image can give life to many containers. If Linux is your operating system, the Docker container will run as a process on the host machine. If you have a Windows or macOS machine, docker will run in a virtual machine (VM). The container will use the same kernel, either the kernel of Linux or the VM on Windows or macOS. The container itself is not a virtual machine. The container cannot see other processes of the host and has its own file system. This is why it seems as it is a virtual machine. But in reality, it shares the kernel of the host machine (or the kernel of the VM).</li>
</ul>
	</div>




<div>
	<p>In conclusion: the Dockerfile is the base for an image, and an image is used to create a container. A container is running as a process on the host machine. Yet, it has its own file system and is separated from the other processes.</p>
</div>



<h2 id="how-do-you-create-a-dockerfile">How Do You Create A Dockerfile?</h2>
<div>
	<p>To generate a Dockerfile, you can create a plain text file. This article will use the command line for it:</p>
</div>




<div>
		<pre aria-describedby="shcb-language-1" data-shcb-language-name="Dockerfile" data-shcb-language-slug="dockerfile"><code><span># create a new file in your current working directory called Dockerfile</span>
$ touch Dockerfile
<span># open the file in your favorite editor (we are using Visual Studio Code)</span>
<span># if you do not have the code command installed, you will need to open it manually</span>
$ code Dockerfile</code><small id="shcb-language-1"><span>Code language:</span> <span>Bash</span> <span>(</span><span>bash</span><span>)</span></small></pre>
	</div>




<div>
	<p>A Dockerfile holds all the instructions to build, start and run your application. Every command that you otherwise need to execute manually is written in a single file. It starts by using a base image. This is usually a small Linux distribution like alpine. If you have to execute a binary, you should use&nbsp;<code>FROM <a href="https://hub.docker.com/_/scratch/" target="_blank" rel="noreferrer noopener">scratch</a></code>. This article uses a&nbsp;<a href="https://fastify.dev/" target="_blank" rel="noreferrer noopener">Fastify</a>&nbsp;server, so it uses an&nbsp;<a href="https://www.alpinelinux.org/" target="_blank" rel="noreferrer noopener">alpine</a>&nbsp;image configured for&nbsp;<a href="https://nodejs.org/en" target="_blank" rel="noreferrer noopener">Node.js</a>. You can view the complete code on&nbsp;<a href="https://github.com/aichbauer/the-ultimate-docker-cheat-sheet" target="_blank" rel="noreferrer noopener">GitHub</a>.</p>
</div>




<div>
	<p>Now we will edit the Dockerfile in Visual Studio Code:</p>
</div>




<div>
		<pre aria-describedby="shcb-language-2" data-shcb-language-name="Dockerfile" data-shcb-language-slug="dockerfile"><code><span><span># the next line sets the base image for this image</span>
</span><span><span># the base image is also based on a Dockerfile</span>
</span><span><span># see: https://hub.docker.com/layers/library/node/18-alpine/images/sha256-a0b787b0d53feacfa6d606fb555e0dbfebab30573277f1fe25148b05b66fa097</span>
</span><span><span># node provides official images for Node.js and</span>
</span><span><span># alpine: a lightweight Linux distribution to reduce image size</span>
</span><span><span>FROM</span> node:<span>18</span>-alpine
</span>
<span><span># sets the working directory inside the image</span>
</span><span><span># all commands after this instruction will be</span>
</span><span><span># executed inside this directory</span>
</span><span><span>WORKDIR</span><span> /app</span>
</span>
<span><span># copies the package.json and package-lock.json</span>
</span><span><span># from the client (e.g., your server or your development machine)</span>
</span><span><span># into the /app directory inside the image</span>
</span><span><span># before running npm ci to </span>
</span><span><span># get the advantage of layer caching</span>
</span><span><span>COPY</span><span> ./package* .</span>
</span>
<span><span># installs all node.js dependencies</span>
</span><span><span># npm ci is similar to npm install but intended to be</span>
</span><span><span># used in continuous integration (CI) environments</span>
</span><span><span># it will do a clean installion based on the package-lock.json</span>
</span><span><span>RUN</span><span> npm ci</span>
</span>
<span><span># copies the source code into the image</span>
</span><span><span>COPY</span><span> . .</span>
</span>
<span><span># this runs the build command specified in the package.json</span>
</span><span><span>RUN</span><span> npm run build:server</span>
</span>
<span><span># the EXPOSE instruction does not actually expose the port 300 of this image</span>
</span><span><span># this is documentation so that we know which port we need to expose</span>
</span><span><span># we do this when starting the container with the --publish flag</span>
</span><span><span>EXPOSE</span> <span>3000</span>
</span>
<span><span># executes the server.js file that is located in the build directory</span>
</span><span><span>CMD</span><span> [<span>"node"</span>, <span>"./build/index.js"</span>]</span>
</span>
</code><small id="shcb-language-2"><span>Code language:</span> <span>Dockerfile</span> <span>(</span><span>dockerfile</span><span>)</span></small></pre>
	</div>



<h3 id="what-is-a-multistage-dockerfile">What Is A Multistage Dockerfile?</h3>
<div>
	<p>You can compose a Dockerfile with many stages. You can think of one stage, as one image. Furthermore, you can then use materials like files from one stage in another stage. A new stage always starts with the line&nbsp;<code>FROM &lt;base-image&gt;</code>. You name a stage using the&nbsp;<code>as</code>&nbsp;keyword. This article uses a simple HTML and JavaScript file as our client application. As a web server, we will use&nbsp;<a href="https://www.nginx.com/" target="_blank" rel="noreferrer noopener">NGINX</a>. You can view the complete code on&nbsp;<a href="https://github.com/aichbauer/the-ultimate-docker-cheat-sheet" target="_blank" rel="noreferrer noopener" data-type="link" data-id="https://github.com/aichbauer/the-ultimate-docker-cheat-sheet">GitHub</a>.</p>
</div>




<div>
	<p>A typical approach is to have a&nbsp;<code>builder stage</code>, with a larger base image. This image holds all executables and libraries needed to build your source code.</p>
</div>




<div>
	<p>The second stage is the&nbsp;<code>serve stage</code>&nbsp;and has a small base image. The benefit is fewer dependencies and libraries. Just enough to execute and serve your application.</p>
</div>




<div>
	<p>With this approach, you make your final image both smaller and more secure. As the image size decreases and the image consists of fewer libraries. Fewer system libraries means it has a lower attack surface. Multistage Dockerfiles are not limited to this use case and can have more than two stages as well.</p>
</div>




<div>
		<pre aria-describedby="shcb-language-3" data-shcb-language-name="Bash" data-shcb-language-slug="bash"><code><span># lets create a new Dockerfile for our frontend application</span>
<span># as we can specify a file in the build command, we name this Dockerfile.client</span>
$ touch Dockerfile.client
<span># open the file in your favorite editor (we are using Visual Studio Code)</span>
<span># if you do not have the code command installed, you will need to open it</span>
$ code Dockerfile.client</code><small id="shcb-language-3"><span>Code language:</span> <span>Bash</span> <span>(</span><span>bash</span><span>)</span></small></pre>
	</div>




<div>
	<p>Here is an example of a multistage app:</p>
</div>




<div>
		<pre aria-describedby="shcb-language-4" data-shcb-language-name="Dockerfile" data-shcb-language-slug="dockerfile"><code><span><span># the base image</span>
</span><span><span># name it builder</span>
</span><span><span># you can reference this stage</span>
</span><span><span># in other stages by this name</span>
</span><span><span>FROM</span> node:<span>18</span>-alpine as builder
</span>
<span><span># working directory inside the image</span>
</span><span><span>WORKDIR</span><span> /app</span>
</span>
<span><span># copies files from the client to the image</span>
</span><span><span>COPY</span><span> ./package* .</span>
</span>
<span><span># run a command inside the image</span>
</span><span><span>RUN</span><span> npm ci</span>
</span>
<span><span># copies files from the client to the image</span>
</span><span><span>COPY</span><span> . .</span>
</span>
<span><span># run a command inside the container</span>
</span><span><span># this will create a new folder in called dist in our app directory</span>
</span><span><span># inside the dist directory, you will find the</span>
</span><span><span># final HTML and JavaScript file</span>
</span><span><span>RUN</span><span> npm run build:client</span>
</span>
<span><span># serve stage</span>
</span><span><span># slim nginx base image named as serve</span>
</span><span><span># will start nginx as non root user</span>
</span><span><span>FROM</span> nginxinc/nginx-unprivileged:<span>1.24</span> as serve
</span>
<span><span># we can now copy things from the first stage to the second</span>
</span><span><span># we copy the build output to the directory where nginx serves files</span>
</span><span><span>COPY</span><span> --from=builder /app/dist /var/www</span>
</span>
<span><span># we overwrite the default config with our own</span>
</span><span><span># if you take a look at the GitHub repository, you</span>
</span><span><span># see the .nginx directory with the nginx.conf</span>
</span><span><span># here we only use the port 80</span>
</span><span><span># in production, you would also want to make sure</span>
</span><span><span># all requests, even in your internal network or Kubernetes cluster</span>
</span><span><span># is served via HTTPS when dealing with sensible data</span>
</span><span><span>COPY</span><span> --from=builder /app/.nginx/nginx.conf /etc/nginx/conf.d/default.conf</span>
</span>
<span><span># the EXPOSE instruction does not actually expose the port 80 of this image</span>
</span><span><span># this is documentation so that we know which port we need to expose</span>
</span><span><span># we do this when starting the container with the --publish flag</span>
</span><span><span>EXPOSE</span> <span>80</span>
</span>
<span><span># The command used when the image is started as a container</span>
</span><span><span># <span>Note:</span> for Docker containers (or for debugging),</span>
</span><span><span># the "daemon off;" directive which is used in this example</span>
</span><span><span># tells nginx to stay in the foreground.</span>
</span><span><span># for containers, this is useful.</span>
</span><span><span># best practice: one container = one process.</span>
</span><span><span># one server (container) has only one service.</span>
</span><span><span>CMD</span><span> [<span>"nginx"</span>, <span>"-g"</span>, <span>"daemon off;"</span>]</span>
</span>
</code><small id="shcb-language-4"><span>Code language:</span> <span>Dockerfile</span> <span>(</span><span>dockerfile</span><span>)</span></small></pre>
	</div>



<h2 id="how-do-you-create-a-docker-image">How Do You Create A Docker Image?</h2>
<div>
	<p>We use the Docker CLI to build images out of our Dockerfiles.</p>
</div>




<div>
		<pre aria-describedby="shcb-language-5" data-shcb-language-name="Bash" data-shcb-language-slug="bash"><code><span># list the directory to make sure you are in the directory with the Dockerfile</span>
$ ls
<span># if not, change the directory with "cd ./path/to/directory-with-Dockerfile"</span>
<span># build an Image out of the Dockerfile in the current working directory</span>
$ docker build .
<span># if you want to build another Dockerfile in this directory, use the --file flag</span>
<span># e.g., --file &lt;filename&gt;</span>
$ docker build --file Dockerfile.client .</code><small id="shcb-language-5"><span>Code language:</span> <span>Bash</span> <span>(</span><span>bash</span><span>)</span></small></pre>
	</div>




<div>
	<p>Creating an image from a Dockerfile only requires the&nbsp;<code>docker build</code>&nbsp;command. Without specifying a name and tag, you can reference the image only by its image ID.</p>
</div>



<h3 id="how-do-you-list-all-images">How Do You List All Images?</h3>
<div>
		<pre aria-describedby="shcb-language-6" data-shcb-language-name="Bash" data-shcb-language-slug="bash"><code><span># list all local images on the client (your server or your development machine)</span>
$ docker image ls
<span># find your image ID</span></code><small id="shcb-language-6"><span>Code language:</span> <span>Bash</span> <span>(</span><span>bash</span><span>)</span></small></pre>
	</div>




<div>
	<p>If you want to give your image a name, you need to use the&nbsp;<code>--tag</code>&nbsp;(shorthand syntax:&nbsp;<code>-t</code>) flag while building the image. You will need this if you are working with a registry like&nbsp;<a href="https://hub.docker.com/" target="_blank" rel="noreferrer noopener">Docker Hub</a>.</p>
</div>



<h3 id="how-do-you-name-your-images">How Do You Name Your Images?</h3>
<div>
	<p>To name and tag your image, use the following pattern:&nbsp;<code>&lt;name&gt;:&lt;tag&gt;</code>. This is usually translated into&nbsp;<code>&lt;username&gt;/&lt;repository&gt;:&lt;version&gt;</code>. The username corresponds to the username of the registry.</p>
</div>




<div>
		<pre aria-describedby="shcb-language-7" data-shcb-language-name="PHP" data-shcb-language-slug="php"><code><span># build and tag your image</span>
<span># a tag consists of a name and a tag, which is separated by a colon (:)</span>
$ docker build --tag examplename/examplerepository-server:<span>0.1</span><span>.0</span> .
<span># or</span>
$ docker build --file Dockerfile.client --tag examplename/examplerepository-client:<span>0.1</span><span>.0</span> .
<span># list all local images</span>
$ docker image ls
<span># you will see your image with a proper repository name and a tag</span></code><small id="shcb-language-7"><span>Code language:</span> <span>PHP</span> <span>(</span><span>php</span><span>)</span></small></pre>
	</div>



<h2 id="how-do-you-create-a-container">How Do You Create A Container?</h2>
<div>
	<p>A container is a running image. You can run images with the CLI command&nbsp;<code>docker run &lt;image-name&gt;</code>:</p>
</div>




<div>
		<pre aria-describedby="shcb-language-8" data-shcb-language-name="Bash" data-shcb-language-slug="bash"><code><span># start our image</span>
$ docker run examplename/examplerepository-server:0.1.0
<span># or</span>
$ docker run examplename/examplerepository-client:0.1.0</code><small id="shcb-language-8"><span>Code language:</span> <span>Bash</span> <span>(</span><span>bash</span><span>)</span></small></pre>
	</div>




<div>
	<p>Every Docker installation comes with a local registry. Docker stores your images here. First, the&nbsp;<code>docker run</code>&nbsp;command will look at the local registry and try to find the image. This will happen with our image, since we built it on the same machine we are executing the&nbsp;<code>docker run</code>&nbsp;command. If it does not find the image locally, it will take a look at the Docker Hub registry. You can also get images from other registries (e.g., your self-hosted registry). For that, you can use the URL of the self-hosted registry.</p>
</div>




<div>
		<pre aria-describedby="shcb-language-9" data-shcb-language-name="Bash" data-shcb-language-slug="bash"><code><span># try to find an image on another registry</span>
$ docker run https://registrydomain.com/examplename/examplerepository-server:0.1.0</code><small id="shcb-language-9"><span>Code language:</span> <span>Bash</span> <span>(</span><span>bash</span><span>)</span></small></pre>
	</div>




<div>
	<p>If you run an image, it starts in a foreground process. The container is running as a process in the terminal where you executed the&nbsp;<code>docker run</code>&nbsp;command. If you kill the terminal, it will stop the container immediately. To let your container run on your machine or server, you can run the container as a background process. In that way, you can close your terminal with no worries.</p>
</div>



<h3 id="how-do-you-run-containers-in-the-background">How Do You Run Containers In The Background?</h3>
<div>
	<p>The&nbsp;<code>--detached</code>&nbsp;(shorthand syntax:&nbsp;<code>-d</code>) flag will start containers in a background process:</p>
</div>




<div>
		<pre aria-describedby="shcb-language-10" data-shcb-language-name="Bash" data-shcb-language-slug="bash"><code><span># run container in the background</span>
$ docker run --detached examplename/examplerepository-server:0.1.0
<span># or</span>
$ docker run --detached examplename/examplerepository-client:0.1.0</code><small id="shcb-language-10"><span>Code language:</span> <span>Bash</span> <span>(</span><span>bash</span><span>)</span></small></pre>
	</div>




<div>
	<p>You will see that the command will exit, and you can use the terminal again.</p>
</div>



<h3 id="how-do-you-list-all-containers">How Do You List All Containers?</h3>
<div>
	<p>Docker will only show the running containers.</p>
</div>




<div>
		<pre aria-describedby="shcb-language-11" data-shcb-language-name="Bash" data-shcb-language-slug="bash"><code><span># list all running containers</span>
$ docker container ls
<span># short</span>
$ docker ps</code><small id="shcb-language-11"><span>Code language:</span> <span>Bash</span> <span>(</span><span>bash</span><span>)</span></small></pre>
	</div>




<div>
	<p>If you want to see all containers, even the stopped containers, you need to pass the flag&nbsp;<code>--all</code>&nbsp;(shorthand syntax:&nbsp;<code>-a</code>).</p>
</div>




<div>
		<pre aria-describedby="shcb-language-12" data-shcb-language-name="Bash" data-shcb-language-slug="bash"><code><span># list all stopped and running containers</span>
$ docker container ls --all</code><small id="shcb-language-12"><span>Code language:</span> <span>Bash</span> <span>(</span><span>bash</span><span>)</span></small></pre>
	</div>



<h3 id="how-do-you-stop-and-remove-containers">How Do You Stop And Remove Containers?</h3>
<div>
	<p>Occasionally, you would like to stop containers. When you stop containers, they are still on the system, you can start them again. If you wish to clean the container from the system, you will need to remove it. You can only remove a stopped container.</p>
</div>




<div>
		<pre aria-describedby="shcb-language-13" data-shcb-language-name="Bash" data-shcb-language-slug="bash"><code><span># stop a container</span>
$ container stop &lt;container-id&gt;
<span># start a container</span>
$ container start &lt;container-id&gt;
<span># restart container</span>
$ container restart &lt;container-id&gt;
<span># remove a stopped container</span>
$ container rm &lt;container-id&gt;</code><small id="shcb-language-13"><span>Code language:</span> <span>Bash</span> <span>(</span><span>bash</span><span>)</span></small></pre>
	</div>




<div>
	<p>If you wish to remove a container as soon as you stop it, you can pass the&nbsp;<code>--rm</code>&nbsp;flag when starting a container. Only use this if your container is stateless. If your container has its own state, make sure to use volumes to preserve it. Otherwise, all your data inside the container is lost when your container stops.</p>
</div>




<div>
		<pre aria-describedby="shcb-language-14" data-shcb-language-name="Bash" data-shcb-language-slug="bash"><code><span># automatically remove a container after it stops</span>
$ docker run --rm examplename/examplerepository-server:0.1.0
<span># or</span>
$ docker run --rm examplename/examplerepository-client:0.1.0</code><small id="shcb-language-14"><span>Code language:</span> <span>Bash</span> <span>(</span><span>bash</span><span>)</span></small></pre>
	</div>



<h3 id="what-if-i-killed-the-terminal-but-the-container-is-still-running">What If I Killed The Terminal, But The Container Is Still Running?</h3>
<div>
	<p>Sometimes signals are not passed to the container properly. Imagine you have killed your terminal because you could not stop the container with CTRL+C. But If you try to restart the container, it tells you that the port is already allocated. This means your old container is still running. To kill a container, run the following command:</p>
</div>




<div>
		<pre aria-describedby="shcb-language-15" data-shcb-language-name="Bash" data-shcb-language-slug="bash"><code><span># kill a container</span>
$ docker <span>kill</span> &lt;conatiner-id&gt;</code><small id="shcb-language-15"><span>Code language:</span> <span>Bash</span> <span>(</span><span>bash</span><span>)</span></small></pre>
	</div>



<h3 id="how-do-you-access-containers-from-the-host-system">How Do You Access Containers From The Host System?</h3>
<div>
	<p>Usually, a Docker container exposes one or many ports. You can access the application which is running inside the container via those ports. To have access to these ports, you need to publish those port during the container creation. Another way to access containers from the host system is by executing commands inside them. This is often used for debugging or single use container application.</p>
</div>



<h3 id="how-do-you-publish-ports">How Do You Publish Ports?</h3>
<div>
	<p>To expose the ports to the host system, you need to add the&nbsp;<code>--publish</code>&nbsp;(shorthand syntax:&nbsp;<code>-p</code>) flag.</p>
</div>




<div>
		<pre aria-describedby="shcb-language-16" data-shcb-language-name="Bash" data-shcb-language-slug="bash"><code><span># publish ports, e.g., forward container port to a port on the host system</span>
$ docker run --publish 3000:3000 examplename/examplerepository-server:0.1.0
<span># or</span>
$ docker run --publish 80:80 examplename/examplerepository-server:0.1.0
<span># if you run both containers, the server and the client</span>
<span># and you visit the localhost:80 in your browser</span>
<span># you should see the message Hello World</span></code><small id="shcb-language-16"><span>Code language:</span> <span>Bash</span> <span>(</span><span>bash</span><span>)</span></small></pre>
	</div>




<div>
	<p>In the first example above, we bind the port 3000 from the container to the port 3000 on the host system. The host system is your development machine or your server. The format is the following&nbsp;<code>--publish &lt;hostport&gt;:&lt;containerport&gt;</code>.</p>
</div>



<h3 id="how-do-you-access-a-running-container">How Do You Access A Running Container?</h3>
<div>
	<p>Sometimes, you want access to a container. This can be beneficial for debugging.</p>
</div>




<div>
		<pre aria-describedby="shcb-language-17" data-shcb-language-name="Bash" data-shcb-language-slug="bash"><code><span># access the container</span>
$ docker <span>exec</span> --interactive --tty &lt;container-id&gt; &lt;shell-command&gt;</code><small id="shcb-language-17"><span>Code language:</span> <span>Bash</span> <span>(</span><span>bash</span><span>)</span></small></pre>
	</div>




<div>
	<p><code>--interactive --tty</code>&nbsp;(shorthand syntax&nbsp;<code>-it</code>) instructs Docker to allocate a pseudo-TTY connection. In this way, the containers’ stdin (standard input) creates an interactive shell in the container.</p>
</div>




<div>
	<p>You can execute any command that would be possible within the container. If you have a Debian container running, you would be able to list the directory:</p>
</div>




<div>
		<pre aria-describedby="shcb-language-18" data-shcb-language-name="PHP" data-shcb-language-slug="php"><code><span># list the directory inside the container</span>
$ docker exec --interactive --tty &lt;container-id&gt; ls</code><small id="shcb-language-18"><span>Code language:</span> <span>PHP</span> <span>(</span><span>php</span><span>)</span></small></pre>
	</div>




<div>
	<p>You can even create a secure shell like connection with the following command:</p>
</div>




<div>
		<pre aria-describedby="shcb-language-19" data-shcb-language-name="Bash" data-shcb-language-slug="bash"><code><span># SSH into the container (if the `sh` command exists in the container)</span>
$ docker <span>exec</span> --interactive --tty &lt;container-id&gt; sh
<span># this will keep the connection to the container open</span>
<span># and you can execute multiple commands within the container</span>
<span># to exit the container, run the following</span>
$ <span>exit</span></code><small id="shcb-language-19"><span>Code language:</span> <span>Bash</span> <span>(</span><span>bash</span><span>)</span></small></pre>
	</div>



<h2 id="how-do-you-persist-data-with-docker-volumes">How Do You Persist Data With Docker Volumes?</h2>
<div>
	<p>Data that is stored inside a container is not persisted by default. When you stop and remove a container, all data from this container is lost. In our application, if you click the button on the website (<a href="http://localhost/" target="_blank" rel="noopener">http://localhost:80</a>) we will write “New message” into a JSON file inside the container. Now if we stop and remove the container, all those messages are deleted as well.</p>
</div>




<div>
	<p>If you want to persist data between container starts, you need to use volumes. There are two different volume types: named volumes and mounted volumes. Named volumes are completely handled by Docker, a mounted volume is managed by you. For a mounted volume, you need to specify the location on the host system where this data will be stored. We use the&nbsp;<code>--volume</code>&nbsp;(shorthand&nbsp;<code>-v</code>) in our run command.</p>
</div>




<div>
		<pre aria-describedby="shcb-language-20" data-shcb-language-name="Bash" data-shcb-language-slug="bash"><code><span># using a named volume</span>
<span># everything within this path of the container will be stored</span>
<span># in a volume named &lt;volume-name&gt;</span>
$ docker run --volume &lt;volume-name&gt;:/path/<span>in</span>/container &lt;image-name&gt;

<span># using a mounted volume</span>
<span># everything inside the path of the container will be stored </span>
<span># in the path of the host</span>
$ docker run --volume /path/on/host:/path/<span>in</span>/container &lt;image-name&gt;</code><small id="shcb-language-20"><span>Code language:</span> <span>Bash</span> <span>(</span><span>bash</span><span>)</span></small></pre>
	</div>




<div>
	<p>For our application, we need to use the following command for the server container:</p>
</div>




<div>
		<pre aria-describedby="shcb-language-21" data-shcb-language-name="Bash" data-shcb-language-slug="bash"><code><span># create a volume called server-volume</span>
<span># we store the content of /app/build/data within our container</span>
<span># on our host machine (your dev machine or your server)</span>
docker run --volume server-volume:/app/build/data --publish 3000:3000 examplename/examplerepository-server:0.1.0</code><small id="shcb-language-21"><span>Code language:</span> <span>Bash</span> <span>(</span><span>bash</span><span>)</span></small></pre>
	</div>



<h3 id="how-do-you-list-all-volumes">How Do You List All Volumes?</h3>
<div>
	<p>You can get an overview of all volumes and their metadata by listing them.</p>
</div>




<div>
		<pre aria-describedby="shcb-language-22" data-shcb-language-name="Bash" data-shcb-language-slug="bash"><code><span># list all volumes</span>
$ docker volume ls

<span># here you will see the location where Docker will store the named volumes</span>
<span># on the host machine</span></code><small id="shcb-language-22"><span>Code language:</span> <span>Bash</span> <span>(</span><span>bash</span><span>)</span></small></pre>
	</div>



<h2 id="conclusion">Conclusion</h2>
<div>
	<p>In this article we learned the difference between a Dockerfile, an image and a container. You can now write your Dockerfiles to create images. Furthermore, you can start containers and access them from the host system. Additionally, you also learned how to persist data between container starts.</p>
</div>




<div>
	<p>If you need help with your containerization, <a href="https://calendly.com/devopsberatung/meet" target="_blank" rel="noreferrer noopener nofollow">feel free to contact us</a>, or join our new community for further questions and discussions (free cookies for the first 42 arrivals)!</p>
</div>



<p><a role="button" target="_blank" href="https://discord.gg/7xpRbG2gY9" data-color-override="#6bf4b2" data-hover-color-override="false" data-hover-text-color-override="#fff"><span>Join Our Community</span><i></i></a></p><div>
	<p>You liked this article? Share it with your colleagues and friends.</p>
</div>



<h2 id="newsletter">Sign up for our newsletter!</h2>
<div>
	<p>Do not miss out on our latest tips, guides, and updates – sign up for our newsletter now! We promise to only send you the most relevant and useful information. Be part of our journey in exploring the world of Docker and beyond.</p>
</div>








<h2>Author</h2>





			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dynamic programming is not black magic (317 pts)]]></title>
            <link>https://qsantos.fr/2024/01/04/dynamic-programming-is-not-black-magic/</link>
            <guid>38988948</guid>
            <pubDate>Sun, 14 Jan 2024 09:42:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qsantos.fr/2024/01/04/dynamic-programming-is-not-black-magic/">https://qsantos.fr/2024/01/04/dynamic-programming-is-not-black-magic/</a>, See on <a href="https://news.ycombinator.com/item?id=38988948">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="site-content">

	
<article id="post-3499">

	
<!-- .entry-header -->

	<div>

			
<p>This year’s <a href="https://en.wikipedia.org/wiki/Advent_of_Code">Advent of Code</a> has been brutal (compare the <a href="https://adventofcode.com/2023/stats">stats of 2023</a> with <a href="https://adventofcode.com/2022/stats">that of 2022</a>, especially day 1 part 1 vs. day 1 part 2). It included a problem to solve with dynamic programming as soon as day 12, which discouraged some people I know. This specific problem was particularly gnarly for Advent of Code, with multiple special cases to take into account, making it basically intractable if you are not already familiar with dynamic programming.</p>



<p>However, dynamic programming itself is mostly natural when you understand what it does. And <a href="https://en.wikipedia.org/wiki/Dynamic_programming#Algorithms_that_use_dynamic_programming">many common algorithms are actually just the application of dynamic programming to specific problems</a>, including omnipresent path-finding algorithms such as Dijkstra’s algorithm.</p>



<p>This motivated me to write a gentler introduction and a detailed explanation of solving Day 12.</p>



<p>Let me start with a rant. You can <a href="#basic-caching">skip to the following section</a> if you want to get to the meat of the article.</p>



<h2>The Rant</h2>



<p>Software engineering is terrible at naming things.</p>



<ul>
<li>“<strong>Bootstrap</strong>” is an <a href="https://en.wikipedia.org/wiki/Bootstrapping">imaged expression to point to the absurdity and impossibility of a task</a>, but it has become synonymous with “start” without providing any additional information, <a href="https://en.wiktionary.org/wiki/bootstrap#Verb">such as “boot a computer”</a>. The illusion that it is actually meaningful had lead to an <a href="https://en.wikipedia.org/wiki/Bootstrapping_(disambiguation)">absurd level of polysemy</a>.</li>



<li>“<strong>Daemon</strong>”, for a process that is detached from your terminal</li>



<li>“<strong>Cascading Style Sheets</strong>”, just to mean that properties can be overridden</li>



<li>“<strong>Cookie</strong>”, for a piece of data stored on the Web browser, which is automatically sent to the server</li>



<li>“<strong>Artificial Intelligence</strong>” which is so vague it refers just as well to <a href="https://en.wikipedia.org/wiki/Expert_system">if-conditions</a>, or to <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence">AGI</a></li>
</ul>



<p>Now, let’s take a look at “<strong>dynamic programming</strong>”. What can we learn from the name? “<strong>Programming</strong>” must refer to a style of programming, such as “functional <em>programming</em>”, or maybe “test-driven <em>development</em>”. Then, “<strong>dynamic</strong>” could mean:</p>



<ul>
<li>like in <strong>dynamic typing</strong>, maybe it could refer to the more general idea of handling objects of arbitrary types, with techniques such as <a href="https://en.wikipedia.org/wiki/Virtual_function">virtual classes</a>, <a href="https://doc.rust-lang.org/book/ch17-02-trait-objects.html">trait objects</a>, or a <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html">base Object type</a>.</li>



<li>maybe it could be the opposite of preferring <a href="https://en.wikipedia.org/wiki/Immutable_object">immutable state</a></li>



<li>if it’s not about data, maybe it could be about dynamic <em>code</em>, such as <a href="https://en.wikipedia.org/wiki/Self-modifying_code">self-modifying code</a>, or <a href="https://en.wikipedia.org/wiki/Just-in-time_compilation">JIT</a></li>



<li>maybe it could be yet another framework such as <a href="https://en.wikipedia.org/wiki/Agile_software_development">Agile</a>, <a href="https://en.wikipedia.org/wiki/Scrum_(software_development)">SCRUM</a>, <a href="https://en.wikipedia.org/wiki/Extreme_programming">XP</a>, <a href="https://en.wikipedia.org/wiki/V-model_(software_development)">V-model</a>, <a href="https://en.wikipedia.org/wiki/Round-trip_engineering">RTE</a>, <a href="https://en.wikipedia.org/wiki/Rapid_application_development">RAD</a></li>



<li>maybe it could be referring to using practices from <a href="https://en.wikipedia.org/wiki/Competitive_programming">competitive programming</a>? Yes, it’s a stretch, but that might make sense</li>
</ul>



<p>Guess what. It means nothing of that, and it has nothing to do with being “dynamic”. It <em>is</em> an idea that you can use to design an algorithm, so there is a link to “programming”; I will grant it that.</p>



<p>So, what <em>is</em> it?</p>



<h2 id="basic-caching">Basic Caching</h2>



<p>Let’s say we want to solve a problem by splitting it in smaller similar problems. Basically, a recursive function. Often, we end-up having to solve the same smaller problems many times.</p>



<p>The typical example is Fibonacci, where you want to evaluate <code>f(n)</code>, which is defined as <code>f(n - 1) + f(n - 2)</code>. If we implement it naively, we will end up evaluating f(1) many times:</p>



<figure><img fetchpriority="high" decoding="async" width="925" height="404" src="https://qsantos.fr/wp-content/uploads/2024/01/image.png" alt=""><figcaption>Call tree for evaluating f(6), the 6-th Fibonacci number. To evaluate, f(6), we need to evaluate both f(5) and f(4). To evaluate f(5), we will need f(4) and f(3). Already, we see that we are going to need f(4) in two places. If we go further, we see that we will need f(1) 8 eight times, which happens to be f(6).</figcaption></figure>



<p>In fact, in this case, since only <code>f(1)</code> adds anything to the overall result, the number of times we will need <code>f(1)</code> is equal to <code>f(n)</code>. And <code>f(n)</code> grows very fast as <code>n</code> grows.</p>



<p>Of course, we can avoid doing this. We can just cache the results (or <a href="https://en.wikipedia.org/wiki/Memoization">memoize</a> <code>f</code>, in terrible academic vernacular).</p>



<p>In the example, once we have evaluated <code>f(4)</code> once, there is no need to evaluate it again, saving 3 evaluations of <code>f(1)</code>. By doing the same for <code>f(3)</code> and <code>f(2)</code>, we get down to 2 evaluations of <code>f(1)</code>. In total, <code>f(…)</code> is evaluated 7 times (<code>f(0)</code>, <code>f(1)</code>, <code>f(2)</code>, <code>f(3)</code>, <code>f(4)</code>, <code>f(5)</code>, <code>f(6)</code>), which is just f(n) + 1.</p>



<p>This is theoretically (<em>asymptotically</em>) optimal. But we can look at this in a different way.</p>



<h2>Optimized Caching</h2>



<p>With memoization, we keep the recursion: “to solve <code>f(6)</code>, I need <code>f(5)</code>, which will itself need <code>f(4)</code> […] and <code>f(3)</code> […], and <code>f(4)</code>, which will itself need <code>f(3)</code> […] and <code>f(2)</code> […].”. Basically, we figure out what we need just when we need them.</p>



<p>Instead, we can make the simple observation that we will need <em>f(0)</em> and <em>f(1)</em> for all other evaluations of <em>f(…)</em>. Once we have them, we can evaluate <em>f(2)</em>, which will need for all <em>other</em> evaluations of <em>f(…)</em>. </p>



<p>You can think of it as plucking the leaves (the nodes without descendants) from the call tree we saw before, and repeat until there are no more nodes. In other words, perform a <a href="https://en.wikipedia.org/wiki/Topological_sorting">topological sort</a>.</p>



<p>With the example, if we have some array F where we can store our partial results:</p>



<ul>
<li>F[0] = f(0) = 0</li>



<li>F[1] = f(1) = 1</li>



<li>F[2] = f(2) = f(1) + f(0) = F[1] + F[0] = 1 + 0 = 1</li>



<li>F[3] = f(3) = f(2) + f(1) = F[2] + F[1] = 1 + 1 = 2</li>



<li>F[4] = f(4) = f(3) + f(2) = F[3] + F[2] = 2 + 1 = 3</li>



<li>F[5] = f(5) = f(4) + f(3) = F[4] + F[3] = 3 + 2 = 5</li>



<li>F[6] = f(6) = f(5) + f(4) = F[5] + F[4] = 5 + 3 = 8</li>
</ul>



<p>With this approach, we do not have any recursive call anymore. And <em>that</em> is <strong>dynamic programming</strong>.</p>



<p>It also forces us to think clearly about what information we will be storing. In fact, in the case of Fibonacci we can notice that we only need the two last previous values. In other words:</p>



<ul>
<li>F[0] = f(0) = 0</li>



<li>F[1] = f(1) = 1</li>



<li>F[2] = f(2) = previous + previous_previous = 1 + 0 = 1</li>



<li>F[3] = f(3) = previous + previous_previous = 1 + 1 = 2</li>



<li>F[4] = f(4) = previous + previous_previous = 2 + 1 = 3</li>



<li>F[5] = f(5) = previous + previous_previous = 3 + 2 = 5</li>



<li>F[6] = f(6) = previous + previous_previous = 5 + 3 = 8</li>
</ul>



<p>So, we can discard other values and just keep two of them. Doing this in Python, we get:</p>



<pre><code lang="python">def fibo(n):
    if n == 0:
        return 0
    previous_previous = 0
    previous = 1
    for _ in range(n - 1):
        current = previous_previous + previous
        (previous, previous_previous) = (current, previous)
    return previous
</code></pre>



<p>I like that this gives us a natural and systematic progression from the mathematical definition of the Fibonacci function, to the iterative implementation (not the <a href="https://en.wikipedia.org/wiki/Fibonacci_sequence#Matrix_form">optimal one</a>, though).</p>



<p>Now, Fibonacci is more of a toy example. Let’s have a look at </p>



<h2>Edit Distance</h2>



<p>The edit distance between two strings is the smallest number of edits needed to transform one string into the other one.</p>



<p>There are actually several versions, depending on what you count as an “edit”. For instance, if you only allow replacing a character by another, you get <a href="https://en.wikipedia.org/wiki/Hamming_distance">Hamming distance</a>; evaluating the Hamming distance between two strings is algorithmically very simple.</p>



<p>Things become more interesting if you allow insertion and deletion of characters as well. This is the <a href="https://en.wikipedia.org/wiki/Levenshtein_distance">Levenstein distance</a>. Considering this title of the present article, this is of course something that can be solved efficiently using ✨&nbsp;dynamic programming&nbsp;✨.</p>



<p>To do that, we’ll need to find how we can derive a full solution from solutions to smaller-problems. Let’s say we have two strings: <code>A</code> and <code>B</code>. We’ll note <code>d(X, Y)</code> the edit distance between strings <code>X</code> and <code>Y</code>, and we’ll note <code>x</code> the length of string <code>X</code>. We need to formulate <code>d(A, B)</code> from any combination of d(X, Y) where <code>X</code> is a substring of <code>A</code> and <code>Y</code> a substring of <code>B</code><sup data-fn="4598dbdf-9a33-4bd6-976e-a06758eb4f07"><a href="#4598dbdf-9a33-4bd6-976e-a06758eb4f07" id="4598dbdf-9a33-4bd6-976e-a06758eb4f07-link">1</a></sup>.</p>



<p>We’re going to look at a single character. We’ll use the last one. The first one would work just as well but using a middle one would not be as convenient. So, let’s look at A[a – 1] and B[b – 1] (using zero-indexing). We have four cases:</p>



<ul>
<li><code>A[a - 1] == B[b - 1]</code>, then we can ignore that character and look at the rest, so <code>d(A, B) = d(A[0..a&nbsp;-&nbsp;1],&nbsp;B[0..b&nbsp;-&nbsp;1])</code></li>



<li><code>A[a - 1] != B[b - 1]</code>, then we could apply any of the three rules.  Since we want the smallest number of edits, we’ll need to select the smallest value given by applying each rule:
<ul>
<li><strong>substitute</strong> the last character of A by that of B, in which case <code>d(A,&nbsp;B) = d(A[0..a&nbsp;-&nbsp;1],&nbsp;B[0..b&nbsp;-&nbsp;1])&nbsp;+&nbsp;1</code></li>



<li><strong>delete</strong> the last character of <code>A</code>, in which case <code>d(A,&nbsp;B) = d(A[0..a&nbsp;-&nbsp;1],&nbsp;B)&nbsp;+&nbsp;1</code></li>



<li><strong>insert</strong> the last character of <code>B</code>, in which case <code>d(A,&nbsp;B) = d(A,&nbsp;B[0..b&nbsp;-&nbsp;1])&nbsp;+&nbsp;1</code></li>
</ul>
</li>



<li>A is actually empty (<code>a = 0</code>), then we need to <strong>insert</strong> all characters from B<sup data-fn="fb0dcaf1-f2c3-4035-aa3e-9d14652cb2fa"><a href="#fb0dcaf1-f2c3-4035-aa3e-9d14652cb2fa" id="fb0dcaf1-f2c3-4035-aa3e-9d14652cb2fa-link">2</a></sup>, so <code>d(A, B) = b</code></li>



<li>B is actually empty (b = 0), then we need to <strong>delete</strong> all characters from A, so <code>d(A, B) = a</code></li>
</ul>



<p>By translating this directly to Python, we get:</p>



<pre><code lang="python">def levenstein(A: str, B: str) -&gt; int:
    a = len(A)
    b = len(B)
    if a == 0:
        return b
    elif b == 0:
        return a
    elif A[a - 1] == B[b - 1]:
        return levenstein(A[:a - 1], B[:b - 1])
    else:
        return min([
            levenstein(A[:a - 1], B[:b - 1]) + 1,
            levenstein(A[:a - 1], B) + 1,
            levenstein(A, B[:b - 1]) + 1,
        ])


assert levenstein("", "puppy") == 5
assert levenstein("kitten", "sitting") == 3
assert levenstein("uninformed", "uniformed") == 1
# way too slow!
# assert levenstein("pneumonoultramicroscopicsilicovolcanoconiosis", "sisoinoconaclovociliscipocsorcimartluonomuenp") == 36</code></pre>



<p>As hinted by the last test, this version becomes very slow when comparing long strings with lots of differences. In Fibonnacci, we were doubling the number of instances for each level in the call tree; here, we are tripling it!</p>



<p>In Python, we can easily apply memoization:</p>



<pre><code lang="python">from functools import cache

@cache
def levenstein(A: str, B: str) -&gt; int:
    a = len(A)
    b = len(B)
    if a == 0:
        return b
    elif b == 0:
        return a
    elif A[a - 1] == B[b - 1]:
        return levenstein(A[:a - 1], B[:b - 1])
    else:
        return min([
            levenstein(A[:a - 1], B[:b - 1]) + 1,
            levenstein(A[:a - 1], B) + 1,
            levenstein(A, B[:b - 1]) + 1,
        ])


assert levenstein("", "puppy") == 5
assert levenstein("kitten", "sitting") == 3
assert levenstein("uninformed", "uniformed") == 1
# instantaneous!
assert levenstein("pneumonoultramicroscopicsilicovolcanoconiosis", "sisoinoconaclovociliscipocsorcimartluonomuenp") == 36</code></pre>



<p>Now, there is something that makes the code nicer, and more performant, but it is not technically necessary. The trick is that we do not actually need to create new strings in our recursive functions. We can just pass arounds the lengths of the substrings, and always refer to the original strings <code>A</code> and <code>B</code>. Then, our code becomes:</p>



<pre><code lang="python">from functools import cache

def levenstein(A: str, B: str) -&gt; int:
    @cache
    def aux(a: int, b: int) -&gt; int:
        if a == 0:
            return b
        elif b == 0:
            return a
        elif A[a - 1] == B[b - 1]:
            return aux(a - 1, b - 1)
        else:
            return min([
                aux(a - 1, b - 1) + 1,
                aux(a - 1, b) + 1,
                aux(a, b - 1) + 1,
            ])
    return aux(len(A), len(B))


assert levenstein("", "puppy") == 5
assert levenstein("kitten", "sitting") == 3
assert levenstein("uninformed", "uniformed") == 1
# instantaneous!
assert levenstein("pneumonoultramicroscopicsilicovolcanoconiosis", "sisoinoconaclovociliscipocsorcimartluonomuenp") == 36</code></pre>



<p>The next step is to build the cache ourselves:</p>



<pre><code lang="python">def levenstein(A: str, B: str) -&gt; int:
    # cache[a][b] = levenstein(A[:a], B[:b])
    # note the + 1 so that we can actually do cache[len(A)][len(B)]
    # the list comprehension ensures we create independent rows, not references to the same one
    cache = [[None] * (len(B) + 1) for _ in range(len(A) + 1)]
    def aux(a: int, b: int) -&gt; int:
        if cache[a][b] == None:
            if a == 0:
                cache[a][b] = b
            elif b == 0:
                cache[a][b] = a
            elif A[a - 1] == B[b - 1]:
                cache[a][b] = aux(a - 1, b - 1)
            else:
                cache[a][b] = min([
                    aux(a - 1, b - 1) + 1,
                    aux(a - 1, b) + 1,
                    aux(a, b - 1) + 1,
                ])
        return cache[a][b]
    return aux(len(A), len(B))


assert levenstein("", "puppy") == 5
assert levenstein("kitten", "sitting") == 3
assert levenstein("uninformed", "uniformed") == 1
# instantaneous!
assert levenstein("pneumonoultramicroscopicsilicovolcanoconiosis", "sisoinoconaclovociliscipocsorcimartluonomuenp") == 36</code></pre>



<p>The last thing we need to do is to replace the recursion with iterations. The important thing is to make sure we do that in the right order<sup data-fn="1d71982a-5567-4481-b278-15d8251bcee6"><a href="#1d71982a-5567-4481-b278-15d8251bcee6" id="1d71982a-5567-4481-b278-15d8251bcee6-link">3</a></sup>:</p>



<pre><code lang="python">def levenstein(A: str, B: str) -&gt; int:
    # cache[a][b] = levenstein(A[:a], B[:b])
    # note the + 1 so that we can actually do cache[len(A)][len(B)]
    # the list comprehension ensures we create independent rows, not references to the same one
    cache = [[None] * (len(B) + 1) for _ in range(len(A) + 1)]
    for a in range(0, len(A) + 1):
        for b in range(0, len(B) + 1):
            if a == 0:
                cache[a][b] = b
            elif b == 0:
                cache[a][b] = a
            elif A[a - 1] == B[b - 1]:
                # since we are at row a, we have already filled in row a - 1
                cache[a][b] = cache[a - 1][b - 1]
            else:
                cache[a][b] = min([
                    # since we are at row a, we have already filled in row a - 1
                    cache[a - 1][b - 1] + 1,
                    # since we are at row a, we have already filled in row a - 1
                    cache[a - 1][b] + 1,
                    # since we are at column b, we have already filled column b - 1
                    cache[a][b - 1] + 1,
                ])
    return cache[len(A)][len(B)]


assert levenstein("", "puppy") == 5
assert levenstein("kitten", "sitting") == 3
assert levenstein("uninformed", "uniformed") == 1
# instantaneous!
assert levenstein("pneumonoultramicroscopicsilicovolcanoconiosis", "sisoinoconaclovociliscipocsorcimartluonomuenp") == 36
</code></pre>



<p>Now, if you really want to grok dynamic programming, I invite you to try it yourself on the following problems, preferrably in this order:</p>



<ol>
<li><a href="https://en.wikipedia.org/wiki/Longest_common_subsequence">longest common subsequence</a> (not to be confused with <a href="https://en.wikipedia.org/wiki/Longest_common_substring">longest common <em>substring</em></a>, but you can do that one too with dynamic programming)</li>



<li><a href="https://en.wikipedia.org/wiki/Line_wrap_and_word_wrap">line warp</a></li>



<li><a href="https://en.wikipedia.org/wiki/Subset_sum_problem">subset sum</a></li>



<li><a href="https://en.wikipedia.org/wiki/Partition_problem">partition</a></li>



<li><a href="https://en.wikipedia.org/wiki/Knapsack_problem">knapsack</a></li>
</ol>



<p>Once you are comfortable with dynamic programming, Day 12 should become much less daunting!</p>



<h2>Advent of Code, Day 12</h2>



<p>In the <a href="https://adventofcode.com/2023/day/12">Advent of Code of December 12th, 2023</a>, you have to solve 1D <a href="https://en.wikipedia.org/wiki/Nonogram">nonograms</a>. Rather than rephrasing the problem, I will let you read the official description.</p>



<pre><code>.??..??...?##. 1,1,3</code></pre>



<p>This can be solved by brute-force. The proper technique for that is <a href="https://en.wikipedia.org/wiki/Backtracking">backtracking</a>, another terrible name. But the asymptotic complexity is exponential (for n question marks, we have to evaluate 2<sup>n</sup> potential solutions). Let’s see how it goes with this example:</p>



<ul>
<li><code>.??..??...?##. 1,1,3</code> the first question mark could be either a <code>.</code> or a <code>#</code>; in the second case, we “consume” the first group of size 1, and the second question mark has to be a <code>.</code>
<ol>
<li><code>..?..??...?##. 1,1,3</code> the next question mark could be either a . or a #; in the second case, we “consume” the first group of size 1, and the next character has to be a ., which is the case
<ol>
<li><code>.....??...?##. 1,1,3</code> the backtracking algorithm will continue to explore the 8 cases, but none of them is a valid solution</li>



<li><code>..#..??...?##. (1),1,3</code>
<ul>
<li>and so on…</li>
</ul>
</li>
</ol>
</li>



<li><code>.#...??...?##. (1),1,3</code>
<ul>
<li>and so on…</li>
</ul>
</li>
</ol>
</li>
</ul>



<p>There are 32 candidates, which would make 63 list items. I’ll spare you that. Instead, I want to draw your attention to the items 2.2 and 2:</p>



<ul>
<li>2.2. <code>..#..??...?##. <code>(1),</code>1,3</code></li>



<li>2. <code>.#...??...?##. <code>(1),</code>1,3</code></li>
</ul>



<p>They are extremely similar. In fact, if we discard the part that has already been accounted for, they are more like:</p>



<ul>
<li>2.2. <code>.??...?##. 1,3</code></li>



<li>2. <code>..??...?##. 1,3</code></li>
</ul>



<p>There is an extra <code>.</code> on the second one, but we can clearly see that it is actually the same problem, and has the same solutions.</p>



<p>In other words, just like with Fibonacci, the total number of cases is huge, but many of them will just be repeats of other ones. So we are going to apply memoization. And then, dynamic programming.</p>



<p>When we implement the “backtracking” algorithm we’ve overviewed above, we get something like this (<a href="https://github.com/Domyy95/Challenges/blob/02e9e5535b7ba027c11bb0e27fe3f2ce5dbd7d38/2023-12-Advent-of-code/12.py#L61-L79">not my code</a>):</p>



<pre><code lang="python">def count_arrangements(conditions, rules):
    if not rules:
        return 0 if "#" in conditions else 1
    if not conditions:
        return 1 if not rules else 0

    result = 0

    if conditions[0] in ".?":
        result += count_arrangements(conditions[1:], rules)
    if conditions[0] in "#?":
        if (
            rules[0] &lt;= len(conditions)
            and "." not in conditions[: rules[0]]
            and (rules[0] == len(conditions) or conditions[rules[0]] != "#")
        ):
            result += count_arrangements(conditions[rules[0] + 1 :], rules[1:])

    return result</code></pre>



<p>Note the program above handles <code>?</code> by treating it as both <code>.</code> and <code>#</code>. The first case is easy, but the second case need to check that it matches the next rules; and for that, it needs to check that there is a separator afterwards, or the end of the string.</p>



<p>Since it’s Python, to memoize, we just need to add <code>@cache</code>.</p>



<p>To make it dynamic programing, we use the same trick as in the example of the edit distance: we pass the offset in the string, and the offset in the rules as parameters in the recursion. This becomes:</p>



<pre><code lang="python">def count_arrangements(conditions, rules):
    @cache
    def aux(i, j):
        if not rules[j:]:
            return 0 if "#" in conditions[i:] else 1
        if not conditions[i:]:
            return 1 if not rules[j:] else 0

        result = 0

        if conditions[i] in ".?":
            result += aux(i + 1, j)
        if conditions[i] in "#?":
            if (
                rules[j] &lt;= len(conditions[i:])
                and "." not in conditions[i:i + rules[j]]
                and (rules[j] == len(conditions[i:]) or conditions[i + rules[j]] != "#")
            ):
                result += aux(i + rules[j] + 1, j + 1)

        return result
    return aux(0, 0)</code></pre>



<p>Then, we implement our own cache and fill it in the right order:</p>



<pre><code lang="python">def count_arrangements(conditions, rules):
    cache = [[0] * (len(rules) + 1) for _ in range(len(conditions) + 1)]
    # note that we are in the indices in reverse order here
    for i in reversed(range(0, len(conditions) + 1)):
        for j in reversed(range(0, len(rules) + 1)):
            if not rules[j:]:
                result = 0 if "#" in conditions[i:] else 1
            elif not conditions[i:]:
                result = 1 if not rules[j:] else 0
            else:
                result = 0
                if conditions[i] in ".?":
                    # since we are at row i, we already filled in row i + 1
                    result += cache[i + 1][j]
                if conditions[i] in "#?":
                    if (
                        rules[j] &lt;= len(conditions[i:])
                        and "." not in conditions[i:i + rules[j]]
                    ):
                        if rules[j] == len(conditions[i:]):
                            # since we are at row i, we already filled in row i + rules[j] &gt; i
                            result += cache[i + rules[j]][j + 1]
                        elif conditions[i + rules[j]] != "#":
                            # since we are at row i, we already filled in row i + rules[j] + 1 &gt; i
                            result += cache[i + rules[j] + 1][j + 1]
            cache[i][j] = result
    return cache[0][0]</code></pre>



<p>And, voilà! You can also have a look at a <a href="https://github.com/qsantos/advent-of-code/blob/master/2023/day12/src/main.rs#L6-L46">Rust implementation (my code, this time)</a>.</p>



<p><strong>Note:</strong> In this case, it looks like the dynamic programming version is slower than the memoized one. But that’s probably due to it being written in unoptimized Python.</p>



<p><strong>Note:</strong> Independently from using a faster language and micro-optimizations, the dynamic programming version allows us to see that we only need the previous column. Thus, we could replace the 2D array by two 1D arrays (one for the previous column, and one for the column being filled).</p>



<h2>Conclusion</h2>



<p>I’ll concede that dynamic programming is not trivial. But it is far from being unreachable for most programmers. Being able to understand how to split a problem in smaller problems will enable you to use memoization in various contexts, which is already a huge improvement above a naive implementation.</p>



<p>However, mastering dynamic programming will let us understand a whole class of algorithms, better understand trade-offs, and make other optimizations possible. So, if you have not already done them, I strongly encourage you to practice on these problems:</p>



<ol>
<li><a href="https://en.wikipedia.org/wiki/Longest_common_subsequence">longest common subsequence</a> (not to be confused with <a href="https://en.wikipedia.org/wiki/Longest_common_substring">longest common <em>substring</em></a>, but you can do that one too with dynamic programming)</li>



<li><a href="https://en.wikipedia.org/wiki/Line_wrap_and_word_wrap">line warp</a></li>



<li><a href="https://en.wikipedia.org/wiki/Subset_sum_problem">subset sum</a></li>



<li><a href="https://en.wikipedia.org/wiki/Partition_problem">partition</a></li>



<li><a href="https://en.wikipedia.org/wiki/Knapsack_problem">knapsack</a></li>
</ol>



<p>And don’t forget to benchmark and profile your code!</p>



<hr>


<ol><li id="4598dbdf-9a33-4bd6-976e-a06758eb4f07">Excluding, of course, <code>d(A, B)</code> itself <a href="#4598dbdf-9a33-4bd6-976e-a06758eb4f07-link" aria-label="Jump to footnote reference 1">↩︎</a></li><li id="fb0dcaf1-f2c3-4035-aa3e-9d14652cb2fa"><code>B</code> could be empty as well, in which case we need to insert 0 characters <a href="#fb0dcaf1-f2c3-4035-aa3e-9d14652cb2fa-link" aria-label="Jump to footnote reference 2">↩︎</a></li><li id="1d71982a-5567-4481-b278-15d8251bcee6">Note that we could permute the inner and outer loops as shown below. In this case, it works just as well:<br><code>for b in range(0, len(B) + 1):</code><br>    <code>for a in range(0, len(A) + 1):</code><br> <a href="#1d71982a-5567-4481-b278-15d8251bcee6-link" aria-label="Jump to footnote reference 3">↩︎</a></li></ol>
		</div><!-- .post-inner -->

	<!-- .section-inner -->

	
	<!-- .pagination-single -->

	
		<!-- .comments-wrapper -->

		
</article><!-- .post -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ICO fines HelloFresh £140k for spam texts and emails (217 pts)]]></title>
            <link>https://ico.org.uk/about-the-ico/media-centre/news-and-blogs/2024/01/ico-fines-hellofresh-140-000-for-spam-texts-and-emails/</link>
            <guid>38988944</guid>
            <pubDate>Sun, 14 Jan 2024 09:40:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ico.org.uk/about-the-ico/media-centre/news-and-blogs/2024/01/ico-fines-hellofresh-140-000-for-spam-texts-and-emails/">https://ico.org.uk/about-the-ico/media-centre/news-and-blogs/2024/01/ico-fines-hellofresh-140-000-for-spam-texts-and-emails/</a>, See on <a href="https://news.ycombinator.com/item?id=38988944">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

        
    <ul>
<li>Company sent 79 million spam emails and 1 million spam texts in seven months</li>
<li>Customers were not fully aware of what they were opting into, marking a “clear breach of trust”</li>
<li>“We will take clear and decisive action where we find the law has not been followed.” - ICO</li>
</ul>
<p>The Information Commissioner’s Office (ICO) has <a data-udi="umb://document/43b8cac81c294f9aa3ac7c9835b11752" href="https://ico.org.uk/action-weve-taken/enforcement/grocery-delivery-e-services-uk-ltd-ta-hellofresh/" title="Grocery Delivery E-Services UK Ltd t/a HelloFresh" data-anchor="#">fined food delivery company HelloFresh £140,000 for a campaign of 79 million spam emails and 1 million spam texts over a seven-month period</a>.</p>
<p>The marketing messages were sent based on an opt-in statement which did not make any reference to the sending of marketing via text. Whilst there was a reference to marketing via email, this was included in an age confirmation statement which was likely to unfairly incentivise customers to agree.</p>
<p>Customers were also not given sufficient information that their data would continue to be used for marketing purposes for up to 24 months after cancelling their subscriptions.</p>
<p>An investigation by the ICO began in March 2022 following complaints made directly to the regulator, as well as to the 7726 spam message reporting service. As part of this investigation, it was also discovered that the company continued to contact some individuals even after they had requested this to stop.</p>
<p>Following the investigation, we found that the company (Grocery Delivery E-Services UK Limited) contravened regulation 22 of the Privacy and Electronic Communications Regulations 2003 and it has now been served with a fine of £140,000.</p>
<p>Andy Curry, Head of Investigations at the Information Commissioner's Office, said:</p>
<blockquote><span><span></span></span>
<p>“This marked a clear breach of trust of the public by HelloFresh. Customers weren’t told exactly what they’d be opting into, nor was it clear how to opt out. From there, they were hit with a barrage of marketing texts they didn't want or expect, and in some cases, even when they told HelloFresh to stop, the deluge continued.</p>
<p>“In issuing this fine, we are showing that we will take clear and decisive action where we find the law has not been followed. We will always protect the right of customers to choose how their data is used.</p>
<p>“The investigation that led to this fine began following complaints filed by the public, both to the ICO and to the 7726 service. This shows just how important it is that if you are being contacted with nuisance calls, texts or emails, that you report it straight away.”</p>
</blockquote>
<h2>Further details of contraventions</h2>
<ul>
<li>Between 23 August 2021 and 23 February 2022 there were 80,893,013 direct marketing messages comprised of 79,779,279 emails and 1,113,734 SMS messages received by subscribers. The Commissioner found that HelloFresh transmitted those messages contrary to Regulation 22 of PECR.</li>
<li>The consent statement for these messages did not meet the requirement that it be “specific” and “informed”, as it did not mention SMS, was unclear and bundled with other aspects, and did not highlight that customers would receive messages for 24 months after they cancelled their HelloFresh subscription.</li>
<li>This therefore meant that 80,893,013 direct marketing messages were sent by HelloFresh that lacked proper consent.</li>
<li>8,729 complaints were logged with the 7726 service in relation to messages from HelloFresh. Additionally, the ICO received 14 complaints about unauthorised SMS messages from HelloFresh and three complaints about marketing emails.</li>
</ul>
<h2>ICO’s work to tackle nuisance communications</h2>
<p>The ICO enforces the Privacy and Electronic Communications Regulations 2003 (PECR), which cover the rules for organisations wishing to make direct marketing calls, texts or emails.</p>
<p>We have issued more than £2,440,000 million in fines against companies responsible for nuisance calls, texts and emails since April 2023. Some of these investigations began with a single complaint from a member of the public.</p>
<p>For more information about the ICO’s work to tackle nuisance calls, emails and texts visit <a data-udi="umb://document/fe5f20bb70f6448e86dfca76c69c261c" href="https://ico.org.uk/for-the-public/nuisance-calls/" title="Nuisance calls">ico.org.uk/nuisancecalls</a>.</p>
<h2>Advice for the public</h2>
<p>To help you, your friends and relatives stop unlawful marketing calls, texts or emails you can:</p>
<ul>
<li>Register landlines and mobile numbers with the <a rel="noopener" href="https://www.tpsonline.org.uk/" target="_blank" title="Telephone Preference Service">Telephone Preference Service<span></span></a> (TPS) and the <a rel="noopener" href="https://www.tpsonline.org.uk/register/corporate_tps" target="_blank" title="Corporate Telephone Preference Service">Corporate Telephone Preference Service<span></span></a> (CTPS) free of charge. The TPS and CTPS is a register used by legitimate marketing companies to identify people and businesses that have said they don’t want to receive marketing calls. Alternatively, you can tell the company directly that you do not wish to be contacted.</li>
<li>Mobile phone users can report the receipt of unsolicited marketing text messages to the Mobile UK's Spam Reporting Service by forwarding the message to 7726.</li>
<li>Refer concerns that you or someone you know has been the victim of fraud to <a rel="noopener" href="https://www.actionfraud.police.uk/" target="_blank" title="Action Fraud">Action Fraud<span></span></a> (in England, Northern Ireland and Wales) and <a rel="noopener" href="https://www.scotland.police.uk/advice-and-information/scams-and-frauds/" target="_blank" title="Police Scotland">Police Scotland<span></span></a> (in Scotland); wider concerns about a business’ practices can be referred to Trading Standards; any abandoned calls that you receive to <a rel="noopener" href="https://www.ofcom.org.uk/phones-telecoms-and-internet/advice-for-consumers/problems/tackling-nuisance-calls-and-messages/abandoned-and-silent-calls" target="_blank" title="Abandoned and silent calls">Ofcom<span></span></a>.</li>
<li><a data-udi="umb://document/74853109d122412992f3c77e24a232c0" href="https://ico.org.uk/make-a-complaint/nuisance-calls-and-messages/" title="Nuisance calls and messages">Complaints about nuisance calls, texts or emails</a> can be made to the ICO via our website.</li>
</ul>
<details>
    <summary aria-describedby="Details_18a79e96-bc20-449e-bc10-a64bbf4df6b5">
        <span>Notes for editors</span>
    </summary>
    <div>
        <ol>
<li>The Information Commissioner’s Office (ICO) is the UK’s independent regulator for data protection and information rights law, upholding information rights in the public interest, promoting openness by public bodies and data privacy for individuals.</li>
<li>The <a data-udi="umb://document/088cd96f62df4fb6bc35b33832b45cce" href="https://ico.org.uk/about-the-ico/what-we-do/legislation-we-cover/" title="Legislation we cover">ICO has specific responsibilities</a> set out in the Data Protection Act 2018 (DPA2018), the United Kingdom General Data Protection Regulation (UK GDPR), the Freedom of Information Act 2000 (FOIA), Environmental Information Regulations 2004 (EIR), Privacy and Electronic Communications Regulations 2003 (PECR) and a further five acts and regulations. </li>
<li>The ICO can take action to address and change the behaviour of organisations and individuals that collect, use and keep personal information. This includes criminal prosecution, non-criminal enforcement and audit. </li>
<li>To report a concern to the ICO telephone our helpline 0303 123 1113 or go to <a data-udi="umb://document/ab9e48b8fcd3472bb798c3c4cfcb43d3" href="https://ico.org.uk/make-a-complaint/" title="Make a complaint">ico.org.uk/concerns</a>.</li>
</ol>
    </div>
</details>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Towards Modern Development of Cloud Applications (2023) (143 pts)]]></title>
            <link>https://dl.acm.org/doi/10.1145/3593856.3595909</link>
            <guid>38988238</guid>
            <pubDate>Sun, 14 Jan 2024 07:17:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dl.acm.org/doi/10.1145/3593856.3595909">https://dl.acm.org/doi/10.1145/3593856.3595909</a>, See on <a href="https://news.ycombinator.com/item?id=38988238">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><h2 id="d1711058e1">ABSTRACT</h2></p><p>When writing a distributed application, conventional wisdom says to split your application into separate services that can be rolled out independently. This approach is well-intentioned, but a microservices-based architecture like this often backfires, introducing challenges that counteract the benefits the architecture tries to achieve. Fundamentally, this is because microservices conflate logical boundaries (how code is written) with physical boundaries (how code is deployed). In this paper, we propose a different programming methodology that decouples the two in order to solve these challenges. With our approach, developers write their applications as logical monoliths, offload the decisions of how to distribute and run applications to an automated runtime, and deploy applications atomically. Our prototype implementation reduces application latency by up to 15× and reduces cost by up to 9× compared to the status quo.</p></div><div data-widget-def="UX3TagWidget" data-widget-id="2a75f978-170a-4843-8722-e878a1b77fbc">
        



        
        









    
    
        <p data-widget-def="graphQueryWidget" data-widget-id="4ca2ed65-717e-4e67-bbb5-6578b8c6acac">
        



        
        <h2 id="sec-terms">Index Terms</h2>

        </p>
    


<ol><li><h6>Towards Modern Development of Cloud Applications</h6><ol><li><ol><li><ol><li><ol><li><ol></ol></li><li><ol></ol></li></ol></li></ol></li></ol></li></ol></li></ol>

        </div><div data-exp-type="" data-query-id="10.1145/3593856.3595909"><ul><li><div><a href="https://dl.acm.org/doi/10.1109/CISIS.2012.159" title="Towards a Semantic Engine for Cloud Applications Development"><h5 data-lines="2">Towards a Semantic Engine for Cloud Applications Development</h5></a><p>CISIS '12: Proceedings of the 2012 Sixth International Conference on Complex, Intelligent, and Software Intensive Systems (CISIS)  </p><div><p>In this paper we propose a Semantic Engine for Cloud applications development: a semantically enabled Search Engine that can be used by cloud applications developers as development support. The Semantic Engine will be a component of the EU project ...</p></div></div></li><li></li><li></li></ul></div></div>]]></description>
        </item>
    </channel>
</rss>