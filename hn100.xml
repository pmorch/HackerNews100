<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 26 Mar 2025 09:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Coordinating the Superbowl's visual fidelity with Elixir (189 pts)]]></title>
            <link>https://elixir-lang.org/blog/2025/03/25/cyanview-elixir-case/</link>
            <guid>43479094</guid>
            <pubDate>Wed, 26 Mar 2025 05:19:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://elixir-lang.org/blog/2025/03/25/cyanview-elixir-case/">https://elixir-lang.org/blog/2025/03/25/cyanview-elixir-case/</a>, See on <a href="https://news.ycombinator.com/item?id=43479094">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
        <p>How do you coordinate visual fidelity across two hundred cameras for a live event like the Super Bowl?</p>

<p>The answer is: by using the craft of camera shading, which involves adjusting each camera to ensure they match up in color, exposure and various other visual aspects. The goal is to turn the live event broadcast into a cohesive and consistent experience. For every angle used, you want the same green grass and the same skin tones. Everything needs to be very closely tuned across a diverse set of products and brands. From large broadcast cameras, drone cameras, and PTZ cameras to gimbal-held mirrorless cameras and more. This is what Cyanview does. Cyanview is a small Belgian company that sells products for the live video broadcast industry, and its primary focus is shading.</p>

<p>Broadcast is a business where you only get one chance to prove that your tool is up to the task. Reliability is king. There can be no hard failures.</p>

<p>A small team of three built a product so powerful and effective that it spread across the industry purely on the strength of its functionality. Without any marketing, it earned a reputation among seasoned professionals and became a staple at the world’s top live events. Cyanview’s Remote Control Panel (RCP) is now used by specialist video operators on the Olympics, Super Bowl, NFL, NBA, ESPN, Amazon and many more. Even most fashion shows in Paris use Cyanview’s devices.</p>

<p>These devices put Elixir right in the critical path for serious broadcast operations. By choosing Elixir, Cyanview gained best-in-class networking features, state-of-the-art resilience and an ecosystem that allowed fast iteration on product features.</p>

<p><img src="https://elixir-lang.org/images/cases/bg/cyanview-4.jpg" alt="Operating many displays with Cyanview products." title="Operating many displays with Cyanview products."></p>

<h2 id="why-elixir">Why Elixir?</h2>

<p>The founding team of Cyanview primarily had experience with embedded development, and the devices they produce involve a lot of low-level C code and plenty of FPGA. This is due to the low-level details of color science and the really tight timing requirements.</p>

<p>If you’ve ever worked with camera software, you know it can be a mixed bag. Even after going fully digital, much of it remained tied to analog systems or relied on proprietary connectivity solutions. Cyanview has been targeting IP (as in Internet Protocol) from early on. This means Cyanview’s software can operate on commodity networks that work in well-known and well-understood ways. This has aligned well with an increase in remote production, partially due to the pandemic, where production crews operate from a central location with minimal crew on location. Custom radio frequency or serial wire protocols have a hard time scaling to cross-continent distances.</p>

<p>This also paved the way for Elixir, as the Erlang VM was designed to communicate and coordinate millions of devices, reliably, over the network.</p>

<p>Elixir was brought in by the developer Ghislain, who needed to build integrations with cameras and interact with the other bits of required video gear, with many different protocols over the network. The language comes with a lot of practical features for encoding and decoding binary data down to the individual bits. Elixir gave them a strong foundation and the tools to iterate fast.</p>

<p>Ghislain has been building the core intellectual property of Cyanview ever since. While the physical device naturally has to be solid, reliable, and of high quality, a lot of the secret sauce ultimately lies in the massive number of integrations and huge amounts of reverse engineering. Thus, the product is able to work with as many professional camera systems and related equipment as possible. It is designed to be compatible with everything and anything a customer is using. Plus, it offers an API to ensure smooth integration with other devices.</p>

<p>David Bourgeois, the founder of Cyanview, told us a story how these technical decisions alongside Elixir helped them tackle real-world challenges:</p>

<p>“During the Olympics in China, a studio in Beijing relied on a large number of Panasonic PTZ cameras. Most of their team, however, was based in Paris and needed to control the cameras remotely to run various shows throughout the day. The problem? Panasonic’s camera protocols were never designed for internet use — they require precise timing and multiple messages for every adjustment. With network latency, that leads to timeouts, disconnects, and system failures… So they ended up placing our devices next to the cameras in Beijing and controlled them over IP from Paris — just as designed.”</p>

<p><img src="https://elixir-lang.org/images/cases/bg/cyanview-2.jpg" alt="Cyanview RIO device mounted on a camera at a sports field." title="Cyanview RIO device mounted on a camera at a sports field."></p>

<p>The devices in a given location communicate and coordinate on the network over a custom MQTT protocol. Over a hundred cameras without issue on a single Remote Control Panel (RCP), implemented on top of Elixir’s network stack.</p>

<h2 id="technical-composition">Technical composition</h2>

<p>The system as a whole consists of RCP devices running a Yocto Linux system, with most of the logic built in Elixir and C. While Python is still used for scripting and tooling, its role has gradually diminished. The setup also includes multiple microcontrollers and the on-camera device, all communicating over MQTT. Additionally, cloud relays facilitate connectivity, while dashboards and controller UIs provide oversight and control. The two critical devices are the RCP offering control on the production end and the RIO handling low-latency manipulation of the camera. Both run Elixir.</p>

<p>The configuration UI is currently built in Elm, but - depending on priorities - it might be converted to <a href="https://phoenixframework.org/">Phoenix LiveView</a> over time to reduce the number of languages in use. The controller web UI is already in LiveView, and it is performing quite well on a very low-spec embedded Linux machine.</p>

<p>The cloud part of the system is very limited today, which is unusual in a world of SaaS. There are cloud relays for distributing and sharing camera control as well as forwarding network ports between locations and some related features, also built in Elixir, but cloud is not at the core of the business. The devices running Elixir on location form a cluster over IP using a custom MQTT-based protocol suited to the task and are talking to hundreds of cameras and other video devices.</p>

<p>It goes without saying that integration with so much proprietary equipment comes with challenges. Some integrations are more reliable than others. Some devices are more common, and their quirks are well-known through hard-won experience. A few even have good documentation that you can reference while others offer mystery and constant surprises. In this context, David emphasizes the importance of Elixir’s mechanisms for recovering from failures:</p>

<p>“If one camera connection has a blip, a buggy protocol or the physical connection to a device breaks it is incredibly important that everything else keeps working. And this is where Elixir’s supervision trees provide a critical advantage.”</p>

<h2 id="growth--team-composition">Growth &amp; team composition</h2>

<p>The team has grown over the 9 years that the company has been operating, but it did so at a slow and steady pace. On average, the company has added just one person per year. With nine employees at the time of writing, Cyanview supports some of the biggest broadcast events in the world.</p>

<p>There are two Elixir developers on board: Daniil who is focusing on revising some of the UI as well as charting a course into more cloud functionality, and Ghislain, who works on cameras and integration. Both LiveView and Elm are used to power device UIs and dashboards.</p>

<p>What’s interesting is that, overall, the other embedded developers say that they don’t know much about Elixir and they don’t use it in their day-to-day work. Nonetheless, they are very comfortable implementing protocols and encodings in Elixir. The main reason they haven’t fully learned the language is simply time — they have plenty of other work to focus on, and deep Elixir expertise hasn’t been necessary. After all, there’s much more to their work beyond Elixir: designing PCBs, selecting electronic components, reverse engineering protocols, interfacing with displays, implementing FPGAs, managing production tests, real productions and releasing firmware updates.</p>

<h2 id="innovation-and-customer-focus">Innovation and customer focus</h2>

<p><img src="https://elixir-lang.org/images/cases/bg/cyanview-3.jpg" alt="Operator using Cyanview RCP for a massive crowd in an arena." title="Operator using Cyanview RCP for a massive crowd in an arena."></p>

<p>Whether it’s providing onboard cameras in 40+ cars during the 24 hours of Le Mans, covering Ninja Warrior, the Australian Open, and the US Open, operating a studio in the Louvre, being installed in NFL pylons, or connecting over 200 cameras simultaneously – the product speaks for itself. Cyanview built a device for a world that runs on top of IP, using Elixir, a language with networking and protocols deep in its bones. This choice enabled them to do both: implement support for all the equipment and provide features no one else had.</p>

<p>By shifting from conventional local-area radio frequency, serial connections, and inflexible proprietary protocols to IP networking, Cyanview’s devices redefined how camera systems operate. Their feature set is unheard of in the industry: Unlimited multicam. Tally lights. Pan &amp; Tilt control. Integration with color correctors. World-spanning remote production.</p>

<p>The ease and safety of shipping new functionality have allowed the company to support new features very quickly. One example is the increasing use of mirrorless cameras on gimbals to capture crowd shots. Cyanview were able to prototype gimbal control, test it with a customer and validate that it worked in a very short amount of time. This quick prototyping and validation of features is made possible by a flexible architecture that ensures that critical fundamentals don’t break.</p>

<p>Camera companies that don’t produce broadcast shading remotes, such as Canon or RED, recommend Cyanview to their customers. Rather than competing with most broadcast hardware companies, Cyanview considers itself a partner. The power of a small team, a quality product and powerful tools can be surprising. Rather than focusing on marketing, Cyanview works very closely with its customers by supporting the success of their events and providing in-depth customer service.</p>

<h2 id="looking-back-and-forward">Looking back and forward</h2>

<p>When asked if he would choose Elixir again, David responded:</p>

<p>“Yes. We’ve seen what the Erlang VM can do, and it has been very well-suited to our needs. You don’t appreciate all the things Elixir offers out of the box until you have to try to implement them yourself. It was not pure luck that we picked it, but we were still lucky. Elixir turned out to bring a lot that we did not know would be valuable to us. And we see those parts clearly now.”</p>

<p>Cyanview hopes to grow the team more, but plans to do so responsibly over time. Currently there is a lot more to do than the small team can manage.</p>

<p>Development is highly active, with complementary products already in place alongside the main RCP device, and the future holds even more in that regard. Cloud offerings are on the horizon, along with exciting hardware projects that build on the lessons learned so far. As these developments unfold, we’ll see Elixir play an increasingly critical role in some of the world’s largest live broadcasts.</p>

<p><img src="https://elixir-lang.org/images/cases/bg/cyanview-1.jpg" alt="Cyanview Remote Control Panels in a control room." title="Cyanview Remote Control Panels in a control room."></p>

<h2 id="in-summary">In summary</h2>

<p>A high-quality product delivering the right innovation at the right time in an industry that’s been underserved in terms of good integration. Elixir provided serious leverage for developing a lot of integrations with high confidence and consistent reliability. In an era where productivity and lean, efficient teams are everything, Cyanview is a prime example of how Elixir empowers small teams to achieve an outsized impact.</p>


      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CIA Director Reveals Signal Comes Installed on Agency Computers (103 pts)]]></title>
            <link>https://theintercept.com/2025/03/25/signal-chat-encryption-hegseth-cia/</link>
            <guid>43478091</guid>
            <pubDate>Wed, 26 Mar 2025 02:02:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theintercept.com/2025/03/25/signal-chat-encryption-hegseth-cia/">https://theintercept.com/2025/03/25/signal-chat-encryption-hegseth-cia/</a>, See on <a href="https://news.ycombinator.com/item?id=43478091">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    
<p><span>For years, U.S.</span> officials villainized end-to-end encrypted messaging apps like Signal as the domain of criminals and terrorists and a threat to national security.</p>



<p>As fallout over a Signal group chat about Yemen war plans ricocheted through Washington, however, CIA Director John Ratcliffe revealed at a Senate Intelligence Committee hearing on Tuesday that the app is approved for official communication and even comes installed on agency computers.</p>



<p>One longtime critic of government attacks on secure messaging said it was a sign that everybody else should follow suit.</p>



<p>“For everyday Americans, this seems like an inadvertent but strong endorsement of the cybersecurity and privacy value that Signal represents — assuming you actually know who you’re adding to the given chats,” said Sean Vitka, executive director of the progressive group Demand Progress.</p>



<h2 id="h-going-dark">“Going Dark”</h2>



<p>The highly sensitive discussion over whether and when to attack Houthis in Yemen included two members of the panel at Tuesday’s hearing, according to a blockbuster <a href="https://www.theatlantic.com/politics/archive/2025/03/trump-administration-accidentally-texted-me-its-war-plans/682151/">report </a>Monday from The Atlantic’s Jeffrey Goldberg.</p>



<p>There was particular irony to an FBI director’s presence on the panel next to members of the thread. For years, successive FBI chiefs Chris Wray and James Comey had <a href="https://theintercept.com/2015/12/09/comey-calls-on-tech-companies-offering-end-to-end-encryption-to-reconsider-their-business-model/">lambasted</a> end-to-end <a href="https://theintercept.com/2015/07/08/fbi-director-comey-proposes-imaginary-solution-encryption/">encryption</a>. The FBI <a href="https://cdt.org/insights/going-dark-versus-a-golden-age-for-surveillance/">popularized</a> the idea that terrorists and drug cartels were “<a href="https://theintercept.com/2015/07/07/fbi-finds-new-bogeyman-anti-encryption-arguments-isis/">going dark</a>” on law enforcement, and that the government needed to step in to do something about it.</p>



<p>The FBI’s <a href="https://www.huffpost.com/entry/james-comey-phone-encryption_n_5996808">favored solution</a> was to create a back door in the apps that would allow the government to snoop on conversations — but only with proper authority, the FBI said.</p>



<p>In a 2014 speech, then-FBI Director Comey <a href="http://fbi.gov/news/speeches/going-dark-are-technology-privacy-and-public-safety-on-a-collision-course">said</a> that the “post-Snowden pendulum has swung too far” in favor of privacy. Without creating a back door, he added, “homicide cases could be stalled, suspects could walk free, and child exploitation victims might not be identified or recovered.”</p>



<p>The FBI never made much progress in Congress toward securing a back door. Across the pond, attacks on end-to-end encryption are ongoing, with the United Kingdom reportedly ordering Apple in secret to create one. France’s National Assembly last week <a href="https://www.eff.org/deeplinks/2025/03/win-encryption-france-rejects-backdoor-mandate">voted down a backdoor mandate</a> sought by the country’s Interior Ministry.</p>



<!-- BLOCK(cta)[0](%7B%22componentName%22%3A%22CTA%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%7D) -->

<!-- END-BLOCK(cta)[0] -->



<h2 id="h-the-cia-seal-of-approval">The CIA Seal of Approval</h2>



<p>The FBI’s official position became increasingly tenuous last year when revelations about “Salt Typhoon” hackers made clear that unencrypted communications were highly vulnerable to foreign adversaries.</p>



  



<p>The hackers, who were allegedly affiliated with the Chinese government, targeted phones used by Donald Trump, JD Vance, and the Kamala Harris campaign, according to reports, and in some cases were able to scoop up the content of text conversations.</p>



<p>By December, the FBI was still promoting back doors under the banner of what it calls “responsibly managed” encryption. At the same time, however, the Cybersecurity and Infrastructure Security Agency was advising end-to-end encrypted messaging apps such as Signal as a defense against Chinese hackers.</p>



<p><a href="https://theintercept.com/2017/05/01/cybersecurity-for-the-people-how-to-keep-your-chats-truly-private-with-signal/">Signal</a>, which is based on an open-source protocol and operated by a nonprofit foundation, is <a href="https://theintercept.com/2024/03/04/signal-app-username-phone-number-privacy/">designed to reduce to a minimum</a> the amount of information that the app can access. Only the users involved in a conversation have decryption keys, making it impossible for the Signal Foundation to view unencrypted conversations. The foundation also cannot see metadata such as a user’s contacts.</p>



<p>On Tuesday, Ratcliffe revealed that the government has adopted Signal at the highest echelons.</p>



<figure><blockquote><p>“One of the first things that happened when I was confirmed as CIA director was Signal was loaded onto my computer at the CIA.”</p></blockquote></figure>



<p>“One of the first things that happened when I was confirmed as CIA director was Signal was loaded onto my computer at the CIA, as it is for most CIA officers,” Ratcliffe said.</p>



<p>The practice began during President Joe Biden’s administration and had the official approval of CIA records management officials, Ratcliffe said, as long as “any decisions that are made are also recorded through formal channels.”</p>



<p>Critics of government secrecy were immediately alarmed that government officials might be trying to evade leaving records subject to the Freedom of Information Act or the <a href="https://theintercept.com/2023/01/20/biden-trump-classified-documents/">Presidential Records Act</a> by using private devices with disappearing messages.</p>



<p>Despite the high level of protection that end-to-end encryption provides in transit, however, the group chat also raised serious security issues. Even secure messaging apps cannot solve the problem of hackers who have compromised the device running them. Nor can they keep information secret in the event of human error — say, <a href="https://www.nytimes.com/2025/03/24/us/politics/hegseth-classified-war-plans-group-chat.html">inadvertently adding a journalist</a> to a sensitive discussion of military strikes.</p>



<p>Under questioning from Sen. Jack Reed, D-R.I., Director of National Intelligence Tulsi Gabbard refused to say whether she used a personal or government-issued phone for her part of the conversation.</p>



<p>Sen. Michael Bennet, D-Colo., also asked Ratcliffe whether he was aware that Trump’s special envoy Steve Witkoff, another member of the group chat, was on a <a href="https://www.cbsnews.com/news/trump-envoy-steve-witkoff-signal-text-group-chat-russia-putin/">trip in Moscow during the conversation,</a> raising more concerns.</p>



<!-- BLOCK(newsletter)[1](%7B%22componentName%22%3A%22NEWSLETTER%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%7D) -->

<!-- END-BLOCK(newsletter)[1] -->



<p>Signal offers users the ability to sync messages across multiple devices. Vitka, the advocate with Demand Progress, said that if government officials were syncing messages to vulnerable private devices, that would raise a host of questions.</p>



<p>“That personal device could be the liability. And as soon as any of these devices are compromised, then the entire chat, the entire thread — then all of the information in it is compromised,” he said.</p>



<p>Senate Republicans largely attempted to sidestep questions about the Yemen group chat during the committee hearing, but Democrats were united in their criticism.</p>



<p>“This is an embarrassment. This is utterly unprofessional. There has been no apology. There has been no recognition of the gravity of this error,” <a href="https://x.com/cspan/status/1904568753917943950">said</a> Sen. Jon Ossoff, D-Ga.</p>



<p><strong>Correction: March 2, 2025, 10:55 p.m. ET</strong><br><em>This story has been corrected to remove an errant reference to FBI Director Kash Patel’s presence in the national security officials’ Signal group.</em></p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You might want to stop running atop (245 pts)]]></title>
            <link>https://rachelbythebay.com/w/2025/03/25/atop/</link>
            <guid>43477057</guid>
            <pubDate>Tue, 25 Mar 2025 23:09:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rachelbythebay.com/w/2025/03/25/atop/">https://rachelbythebay.com/w/2025/03/25/atop/</a>, See on <a href="https://news.ycombinator.com/item?id=43477057">Hacker News</a></p>
Couldn't get https://rachelbythebay.com/w/2025/03/25/atop/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Better Shell History Search (108 pts)]]></title>
            <link>https://tratt.net/laurie/blog/2025/better_shell_history_search.html</link>
            <guid>43476793</guid>
            <pubDate>Tue, 25 Mar 2025 22:35:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tratt.net/laurie/blog/2025/better_shell_history_search.html">https://tratt.net/laurie/blog/2025/better_shell_history_search.html</a>, See on <a href="https://news.ycombinator.com/item?id=43476793">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-body">





<p>I spend an awful lot of my day in Unix terminals running shell commands. For
some reason, the variance in efficiency between different people when using the shell
is huge: I know people who can run rings around me, and I’ve come across
more than one paid professional who doesn’t use the “up” key to retrieve the
previous command.</p>
<p>I chose that last example very deliberately: most of the commands most of us
run in the shell are highly repetitive. I typically run around 50-100 unique
(i.e. syntactically distinct) shell commands per working day <span>[1]</span> — but
I’ll often run a tiny subset of those commands (e.g. <code>cargo test</code>) hundreds of
time in a single day.</p>
<p>Since many command-line tools have hard-to-remember options, we can save huge
chunks of time – not to mention make fewer errors – if we can search our
shell history to find a previous incantation of a command we want to run. In
this post I’m going to show how, with little effort, searching shell
history can look like this:</p>
<p><video controls=""><source src="https://tratt.net/laurie/blog/extra/2025/better_shell_history_search/skim.mp4" type="video/mp4"><a href="https://tratt.net/laurie/blog/extra/2025/better_shell_history_search/skim.mp4">[Video]</a></video></p>
<h2><a name="searching_shell_history">Searching shell history</a></h2>
<p>Larger Unix shells such as Bash have long allowed users to search through their
shell history by pressing <code>Ctrl-r</code> and entering a substring. If I (in order)
executed the commands <code>cat /etc/motd</code> then <code>cat /etc/rc.conf</code>, then <code>Ctrl-r</code>
followed by “cat” will first match <code>cat /etc/rc.conf</code>; pressing <code>Ctrl-r</code> again
will cycle backwards for the next match which is <code>cat /etc/motd</code>. I almost
never used this feature, because substring matching is too crude.
For example, I may know the command I want is <code>cat</code>, the leaf
name I’m looking for is <code>motd</code> but I don’t remember the directory: substring
matching can’t help me find what I’m looking for. Instead, I regularly used
<code>grep</code> (with wildcards) to search through my shell’s history file instead.</p>
<p>For me, the game changer was pairing <code>Ctrl-r</code> with
<a href="https://github.com/junegunn/fzf">fzf</a>, which brought two changes. First,
matching is “fuzzy”, so I can type “c mo” and <code>cat /etc/motd</code> will be
matched. Second, multiple matches are shown at once. Typing “cat” will show
me several <code>cat</code> commands, allowing me to quickly select the right
incantation (which may not have been the most recent).</p>
<p>It’s difficult for me to overstate how powerful a feature this is. Few things
in life make me as happy as pressing <code>ctrl-R</code> then typing “<code>l1</code>” and having a 100
character command-line execution that runs a complicated debugging tool, with
multiple environment variables set, whose output gets put in <code>/tmp/l1</code> appear
in my terminal.</p>
<p>Using <code>Ctrl-r</code> and fzf roughly doubled my efficiency in the shell
overnight. Interestingly, it had an even greater long term effect: I became a
more ambitious user of shell commands because I knew I could outsource my
memory to fzf. For example, since it’s now very easy to recall past commands, I
no longer set global environment variables, which had previously caused me
grief when I forgot about them <span>[2]</span>. Now I set environment variables on a
per-command basis, knowing that I can recall them with <code>Ctrl-r</code> and fzf.</p>
<p>For many years my favoured shell was zsh. When I later moved from zsh to fish,
<code>Ctrl-r</code> and fzf was the first thing I configured; and when I moved back to zsh
<span>[3]</span>, and redid my configuration from scratch, <code>Ctrl-r</code> and fzf was again the first
thing I got working (shortly followed by
<a href="https://github.com/zsh-users/zsh-autosuggestions">autosuggestions</a>). If you
take nothing else from this
post than “<code>Ctrl-r</code> and fzf are a significant productivity boon for Unix
users”, then I will have done something useful.</p>
<p>No tool, of course, is perfect. A
couple of months back I somehow stumbled across
<a href="https://github.com/skim-rs/skim">skim</a>, an fzf-alike that out-of-the-box
happens to suit me just a little bit better than fzf. The differences
are mostly minor, and you won’t go far wrong with either tool. That said,
I find that skim’s matching more often finds the commands I want quickly,
I prefer skim’s UI, and I find it easier to install skim on
random boxes — small advantages, perhaps, but enough for switching to be
worth it for me.</p>
<h2><a name="doing_even_better">Doing even better</a></h2>
<p>Finding Skim encouraged me to quickly look around to see what else in this sphere might
improve my productivity. I quickly came across <a href="https://atuin.sh/">Atuin</a>, which is a much
more sophisticated shell history recording mechanism: the video on its front
page showed a much nicer matching UI than I had previously considered possible.</p>
<p>However, I quickly realised Atuin wasn’t for me or, at least, wasn’t easily for
me. These days I regularly <code>ssh</code> into many different servers: over time I’ve
streamlined my shell configuration to a single <code>.zshrc</code> file that I can <code>scp</code>
over to a new machine and which instantly makes me productive. Atuin – and
this isn’t a criticism, because it’s a more powerful tool – is more difficult
to install <span>[4]</span> and setup <span>[5]</span> (I’m also not sure the ‘fuzzy’
aspects of Atuin quite match the heights of fzf/skim). That said, some readers may find
it a useful tool to investigate.</p>
<p>However, what I immediately realised from the Atuin video is that I would like
my fuzzy matcher to show me more useful information about the commands it’s
matching.</p>
<p>In particular, fzf and skim both default to showing me a (to me!) meaningless
integer before my matched command: this had always slightly bothered me, but
I’d never thought to work out what it meant. For example, if I use
zsh + fzf + <code>Ctrl-r</code> I see:</p>
<p><img src="https://tratt.net/laurie/blog/extra/2025/better_shell_history_search/fzf.png" alt=""></p>
<p>What does 5408 mean and why is it taking up valuable screen space? Skim
tries to be a bit nicer: it will show <code>5408 today'21:26</code> <span>[6]</span>, but that
takes up even more screen space!</p>
<h2><a name="adapting_zsh_and_fzfskim">Adapting zsh and fzf/skim</a></h2>
<p>Fortunately, it turns out that improving the <code>Ctrl-r</code> and the fzf/skim UI is
easy. Instead of wasting space on a meaningless-to-me integer, what I
now see is the following (where <code>11d</code> means “11 days in the past” and so on):</p>
<p><img src="https://tratt.net/laurie/blog/extra/2025/better_shell_history_search/skim.png" alt=""></p>
<p>I’m going to show how I adapted zsh and skim to do this. My guess is that it
will take very little ingenuity to adapt this to other shells (and adapting
this to fzf mostly involves swapping the <code>sk</code> command for <code>fzf</code>).</p>
<p>The first thing I needed to do is make zsh record <em>when</em> commands
were executed. I added this to my <code>~/.zshrc</code>:</p>
<pre><span>setopt</span><span> EXTENDED_HISTORY
</span><span>setopt</span><span> inc_append_history_time
</span></pre>
<p>The <code>EXTENDED_HISTORY</code> changes the format of <code>.zsh_history</code> to record when (in
seconds from the Unix epoch) a command was executed and (with
<code>inc_append_history_time</code>) how long it ran for. The good news is that these
options migrate “traditionally formatted” history files naturally: any
non-extended-history commands will be given the current date so that
all of <code>.zsh_history</code> is in the same format.</p>
<p>I then needed to understand how zsh’s history ended up being interrogated and
displayed when I pressed <code>Ctrl-r</code>. fzf and skim share almost exactly the same
code here: I’ll use skim’s <a href="https://github.com/skim-rs/skim/blob/master/shell/key-bindings.zsh">zsh key
bindings</a>
as my example. In essence, both tools define a function <code>history-widget</code>
which they then bind to <code>Ctrl-r</code>:</p>
<pre><span>history-widget</span><span>() { </span><span>... </span><span>}
</span><span>zle</span><span>     -N</span><span>   history-widget
</span><span>bindkey </span><span>'</span><span>^R</span><span>' history-widget
</span></pre>
<p>One can override the version fzf and skim provide by putting the code above
into your <code>~/.zshrc</code> after the point you import their normal key bindings.</p>
<p>Let’s look at skim’s <code>history-widget</code>:</p>
<pre><span>skim-history-widget</span><span>() {
</span><span>  </span><span>local </span><span>selected num
</span><span>  </span><span>setopt</span><span> localoptions noglobsubst noposixbuiltins pipefail no_aliases </span><span>2</span><span>&gt; /dev/null
</span><span>  </span><span>local </span><span>awk_filter</span><span>='</span><span>{ cmd=$0; sub(/^\s*[0-9]+\**\s+/, "", cmd); if (!seen[cmd]++) print $0 }</span><span>'  </span><span># filter out duplicates
</span><span>  </span><span>local </span><span>n</span><span>=</span><span>2 </span><span>fc_opts</span><span>=''
</span><span>  </span><span>if </span><span>[[ </span><span>-o</span><span> extended_history </span><span>]]</span><span>; </span><span>then
</span><span>    </span><span>local </span><span>today</span><span>=$</span><span>(</span><span>date</span><span> +</span><span>%</span><span>Y</span><span>-</span><span>%</span><span>m</span><span>-</span><span>%</span><span>d</span><span>)
</span><span>    </span><span># For today's commands, replace date ($2) with "today", otherwise remove time ($3).
</span><span>    </span><span># And filter out duplicates.
</span><span>    </span><span>awk_filter</span><span>='</span><span>{
</span><span>      if ($2 == "</span><span>'$</span><span>today</span><span>'</span><span>") sub($2 " ", "today</span><span>'</span><span>\'</span><span>'</span><span>")
</span><span>      else sub($3, "")
</span><span>      line=$0; $1=""; $2=""; $3=""
</span><span>      if (!seen[$0]++) print line
</span><span>    }</span><span>'
</span><span>    </span><span>fc_opts</span><span>='</span><span>-i</span><span>'
</span><span>    </span><span>n</span><span>=</span><span>3
</span><span>  </span><span>fi
</span><span>  </span><span>selected</span><span>=( $(</span><span>fc</span><span> -rl </span><span>$</span><span>fc_opts</span><span> 1 | </span><span>awk </span><span>"$</span><span>awk_filter</span><span>" |
</span><span>    </span><span>SKIM_DEFAULT_OPTIONS</span><span>="</span><span>--height </span><span>$</span><span>{</span><span>SKIM_TMUX_HEIGHT</span><span>:-</span><span>40%} </span><span>$</span><span>SKIM_DEFAULT_OPTIONS</span><span> -n</span><span>$</span><span>n</span><span>..,.. --bind=ctrl-r:toggle-sort </span><span>$</span><span>SKIM_CTRL_R_OPTS</span><span> --query=</span><span>$</span><span>{</span><span>(qqq)LBUFFER</span><span>} --no-multi</span><span>" $(</span><span>__skimcmd</span><span>)) )
</span><span>  </span><span>...
</span></pre>
<p>The first thing to note is that – thanks to <code>EXTENDED_HISTORY</code> – in
my context the <code>-o extended_history</code> check always returns <code>true</code>, so the body of the <code>if</code> is
always executed.</p>
<p>We can then skip ahead: <code>fc -rli 1</code> gets zsh to output its history in a more
easily digestible form than going through <code>.zsh_history</code> directly:</p>
<pre><span>$ fc -rli 1
</span><span>    4  2025-02-07 15:05  pizauth status
</span><span>    3  2025-02-07 15:03  cargo run --release server
</span><span>    2  2025-02-07 15:03  email quick
</span><span>    1  2025-02-07 14:59  rsync_cmd bencher16 ./build.sh cargo test nested_tracing
</span></pre>
<p>We can also now see what the magical integers from earlier are: they’re the row
numbers from <code>fc</code>, where <code>1</code> is the oldest command in my <code>~/.zsh_history</code>! These
are, in some situations, used as identifiers because one can ask zsh to “return
me command 5408”.</p>
<p>The awk code streams over this output, replacing today’s date with the literal
string <code>today</code>, removes the hours/minutes output from previous days, and
removes duplicates.</p>
<p>Although it’s easily missed, in the final line of the code snippet is
<code>-n$n..,..</code> which tells skim which whitespace-separated columns to fuzzy
match and print out.</p>
<p>At this point we now need to decide how to adapt things to our purposes.
The first thing we need to do with <code>fc</code>’s output is convert the time to seconds
since the Unix epoch. We can get <code>fc</code> to do that for us with <code>-t '%s'</code>. Instead
of outputting <code>2025-03-21 22:10</code> we now get <code>1742595052</code>. Notice that two
fields have now become one! Because <code>fc</code> adds leading space to the row
numbers, we’ll strip that off by piping <code>fc</code>’s output through <code>sed -E "s/^ *//"</code> <span>[7]</span>.</p>
<p>I then needed to decide how to format “how far in the past was the command
run”. After a few tries, I decided that a good approach is to give absolute
<code>hour:minute</code> times for commands in the last 20 hours, and <code>1d</code>, <code>2d</code> (etc.)
for commands 1 or more days in the past. Why 20 hours? Well, it turns out that
if I start work at 08:00, press <code>Ctrl-r</code> and see an entry at <code>08:01</code> I won’t
realise that was <em>yesterday’s</em> 08:01 (today’s 08:01 is only 60 seconds in the
future!). 20 hours solves this ambiguity: it means that, at 08:00,
yesterday afternoon’s commands show as <code>16:33</code> but yesterday
morning’s commands as <code>1d</code>.</p>
<p>We now need to switch to awk. I will admit that I initially balked at the use
of awk, a language I have never used before. I quickly explored alternatives
before realising why the code uses awk: every Unix machine has awk installed.
For those unfamiliar with awk, the program that we’re writing iterates over
each line in the input, splits that line up by whitespace, and puts the split
fields into the variables <code>$1</code>, <code>$2</code> (etc.). We’ll keep the duplicate detection
from the awk code above, but change most of the rest.</p>
<p>The first thing we need to do in awk is to convert the Unix epoch time for a
command (in field/variable <code>$2</code>) to an integer, and calculate how many seconds
it is in the past using <code>systime</code> (which returns the current time relative to
the Unix epoch):</p>
<pre><span>ts = int($2)
</span><span>delta = systime() - ts
</span></pre>
<p>We can then convert <code>delta</code> seconds to days by dividing by 86,400 (24h * 60m *
60s == 86,400s). It’s then a simple series of <code>if</code>/<code>else</code> to format this nicely
bearing in mind that:</p>
<ol>
<li>20h == 72,000s</li>
<li>string concatenation and int-to-string conversion in awk is implicit</li>
</ol>
<p>The conversion code looks as follows:</p>
<pre><span>delta_days = int(delta / 86400)
</span><span>if (delta_days &lt; 1 &amp;&amp; delta &lt; 72000) { $2=strftime("%H:%M", ts) }
</span><span>else if (delta_days == 0) { $2="1d" }
</span><span>else { $2=delta_days "d" }
</span></pre>
<p>One could choose to divvy things up further, perhaps showing commands older
than a week with “1w” and so on: I haven’t found that worth worrying about yet.</p>
<p>There is, however, one minor fly in the ointment: clock skew. This could cause
commands to appear to be executing in the future. I’ve not seen seen this
happen in practice yet, but bitter experience with computers and clocks tells
me it will at some point. I’ve defensively catered for the inevitable confusion
that will cause me by using a <code>+</code> prefix for such cases:</p>
<pre><span>delta_days = int(delta / 86400)
</span><span>if (delta &lt; 0) { $2="+" (-delta_days) "d" }
</span><span>else ...
</span></pre>
<p>Notice that I had to put <code>(-delta_days)</code> in brackets as otherwise – for reasons I’m
too lazy to investigate – awk doesn’t concatenate the integer and
string in the way I want.</p>
<p>Since we have one fewer field than before we can slightly simplify our output:</p>
<pre><span>line=$0; $1=""; $2=""
</span><span>if (!seen[$0]++) print line
</span></pre>
<p>That’s the awk code done. We then need to make one change to the <code>selected=...</code>
line changing <code>-n$n..,..</code> to <code>--with-nth $n..</code>. This tells fzf and skim to
suppress the output of the row number and not to make it part
of the fuzzy matching either.</p>
<p>Putting all that together, the updated chunk of the <code>history-widget</code> now
looks as follows (you can find the <a href="https://tratt.net/laurie/blog/extra/2025/better_shell_history_search/zsh_history_widget.sh">whole code chunk
here</a>):</p>
<pre><span>    </span><span>local </span><span>n</span><span>=</span><span>1 </span><span>fc_opts</span><span>=''
</span><span>    </span><span>if </span><span>[[ </span><span>-o</span><span> extended_history </span><span>]]</span><span>; </span><span>then
</span><span>      </span><span>awk_filter</span><span>='
</span><span>{
</span><span>  ts = int($2)
</span><span>  delta = systime() - ts
</span><span>  delta_days = int(delta / 86400)
</span><span>  if (delta &lt; 0) { $2="+" (-delta_days) "d" }
</span><span>  else if (delta_days &lt; 1 &amp;&amp; delta &lt; 72000) { $2=strftime("%H:%M", ts) }
</span><span>  else if (delta_days == 0) { $2="1d" }
</span><span>  else { $2=delta_days "d" }
</span><span>  line=$0; $1=""; $2=""
</span><span>  if (!seen[$0]++) print line
</span><span>}</span><span>'
</span><span>      </span><span>fc_opts</span><span>='</span><span>-i</span><span>'
</span><span>      </span><span>n</span><span>=</span><span>2
</span><span>    </span><span>fi
</span><span>    </span><span>selected</span><span>=( $(</span><span>fc</span><span> -rl </span><span>$</span><span>fc_opts -t </span><span>'</span><span>%s</span><span>' 1 | </span><span>sed</span><span> -E </span><span>"</span><span>s/^ *//</span><span>" | </span><span>awk </span><span>"$</span><span>awk_filter</span><span>" |
</span><span>      </span><span>SKIM_DEFAULT_OPTIONS</span><span>="</span><span>--height </span><span>$</span><span>{</span><span>SKIM_TMUX_HEIGHT</span><span>:-</span><span>40%} </span><span>$</span><span>SKIM_DEFAULT_OPTIONS</span><span> --with-nth </span><span>$</span><span>n</span><span>.. --bind=ctrl-r:toggle-sort </span><span>$</span><span>SKIM_CTRL_R_OPTS</span><span> --query=</span><span>$</span><span>{</span><span>(qqq)LBUFFER</span><span>} --no-multi</span><span>" $(</span><span>__skimcmd</span><span>)) )
</span></pre>
<p>That simple change is enough to give me this output when I press <code>Ctrl-r</code> and
start typing:</p>
<p><video controls=""><source src="https://tratt.net/laurie/blog/extra/2025/better_shell_history_search/skim.mp4" type="video/mp4"><a href="https://tratt.net/laurie/blog/extra/2025/better_shell_history_search/skim.mp4">[Video]</a></video></p>
<h2><a name="summary">Summary</a></h2>
<p>I’ve been using the changes above for about 6 weeks, and I’ve found it a
meaningful productivity enhancement. It turns out that I often remember enough
about a command I want to recall that seeing if a match is “1d” or “7d” in the
past is enough to immediately rule it in or out without scanning rightwards.
Occasionally I even search on the time delta itself: if I start a match with
“2d” fzf or skim will naturally search commands from 2 days ago.</p>
<p>But, perhaps, there is a larger point to take from this post. If, like me, you
spend a lot of your life in a Unix terminal, it can be easy to fall into
patterns of usage that would be recognisable to shell users from the 1970s.
Not only can we do better, it’s easy to do so, and the productivity
gains can be substantial!</p>
<p><strong>Acknowledgements</strong>: thanks to <a href="http://www.eddbarrett.co.uk/">Edd Barrett</a> for comments.</p>


<p>

2025-03-25 11:50

<a href="https://tratt.net/laurie/blog/2024/can_we_retain_the_benefits_of_transitive_dependencies_without_undermining_security.html">Older</a>

</p>

<div id="article-updates"><p>
If you’d like updates on new blog posts: follow me on
<a href="https://mastodon.social/@ltratt">Mastodon</a>
or <a href="https://twitter.com/laurencetratt">Twitter</a>;
or <a href="https://tratt.net/laurie/blog/blog.rss">subscribe to the RSS feed</a>;
or <a href="https://tratt.net/laurie/newsletter/">subscribe to email updates</a>:

</p>

</div>


<h3>Footnotes</h3>
















<h3>Comments</h3>







</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What's Happening to Students? (117 pts)]]></title>
            <link>https://www.honest-broker.com/p/whats-happening-to-students</link>
            <guid>43476365</guid>
            <pubDate>Tue, 25 Mar 2025 21:47:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.honest-broker.com/p/whats-happening-to-students">https://www.honest-broker.com/p/whats-happening-to-students</a>, See on <a href="https://news.ycombinator.com/item?id=43476365">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F751986e4-8b13-47e2-8f4a-4bd35ef8d176_2400x800.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F751986e4-8b13-47e2-8f4a-4bd35ef8d176_2400x800.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F751986e4-8b13-47e2-8f4a-4bd35ef8d176_2400x800.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F751986e4-8b13-47e2-8f4a-4bd35ef8d176_2400x800.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F751986e4-8b13-47e2-8f4a-4bd35ef8d176_2400x800.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F751986e4-8b13-47e2-8f4a-4bd35ef8d176_2400x800.jpeg" width="1456" height="485" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/751986e4-8b13-47e2-8f4a-4bd35ef8d176_2400x800.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:485,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:130208,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://www.honest-broker.com/i/159140978?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F751986e4-8b13-47e2-8f4a-4bd35ef8d176_2400x800.jpeg&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F751986e4-8b13-47e2-8f4a-4bd35ef8d176_2400x800.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F751986e4-8b13-47e2-8f4a-4bd35ef8d176_2400x800.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F751986e4-8b13-47e2-8f4a-4bd35ef8d176_2400x800.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F751986e4-8b13-47e2-8f4a-4bd35ef8d176_2400x800.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>A frustrated teacher recently took to social media with a </span><a href="https://x.com/j00ny369T/status/1900357484545442119" rel="">desperate warning:</a></p><blockquote><p>You guys don’t know what’s going on in education right now. That’s fine—how could you know unless you were working in it? But I think that you need to know….</p><p>First of all the kids have no ability to be bored whatsoever. They live on their phones. And they’re just fed a constant stream of dopamine from the minute their eyes wake up in the morning until they go to sleep at night.</p><p>Because they are in a constant state of dopamine withdrawal at school, they behave like addicts. They’re super emotional. The smallest things set them off.</p><p>When you are standing in front of them trying to teach, they’re vacant. They have no ability to tune in….</p><p>They’re not there. </p><p>And they have a level of apathy that I’ve never seen before in my whole career. Punishments don’t work because they don’t care about them. They don’t care about grades. They don’t care about college.</p></blockquote><p>They just care about the next fix—because that’s how addicts operate. They have no long term plan, just short term needs.</p><p>They can’t get back to their phones fast enough. </p><p>How bad is it for educators right now? </p><p>Check out this commentary from one experienced teacher, who finds more engaged students in prison than a college classroom.</p><p><span>This comes from Corey McCall, a member of </span><em>The Honest Broker</em><span> community who recently </span><a href="https://www.honest-broker.com/p/the-decline-of-the-novel/comment/71518718" rel="">posted this comment</a><span>:</span></p><blockquote><p>I saw this decline in both reading ability and interest occur firsthand between 2006 and 2021….I had experience teaching undergrads who hadn't comprehended the material before, but hadn't faced the challenge of students who could read it but who simply didn't care….</p><p><span>Since 2021 I've been teaching part-time in prison, and incarcerated students really want to learn. They love to read and think along with authors such as Plato, Descartes, and Simone de Beauvoir. I am teaching </span><em>Intro to Theater</em><span> this semester (the story of how this happened is interesting, but is irrelevant here) and students have been poring over </span><em>Oedipus the King</em><span> and asking why this amazing play isn't performed more regularly alongside plays like </span><em>Hamilton </em><span>and </span><em>The Lion King</em><span>.</span></p><p>I believe that there is hope for the humanities and perhaps for culture more generally, but it will be found in unusual places. </p></blockquote><p><span>I’ve made a similar claim </span><a href="https://www.honest-broker.com/p/the-real-crisis-in-humanities-isnt" rel="">in this article</a><span>—where I look outside of college for a rebirth of the humanities. It would be great if it happened in classrooms, too, but I fear that they are now the epicenter of the zombie wars. </span></p><p>Alas, I fear the number of zombie students is still growing—and at an accelerated pace.</p><p><span>Jonathan Haidt, who has taken the lead in exposing this crisis—and thus gets attacked fiercely by zombie apologists—shares horrifying trendlines from </span><a href="https://monitoringthefuture.org/" rel="">Monitoring the Future</a><span>. </span></p><p>This group at the University of Michigan has studied student behavior since 1975. But what’s happening now is unprecedented.</p><p><span>Students are literally finding it </span><em>too hard to think</em><span>. So they can’t learn new things. </span></p><p><span>Below are more ugly numbers from another in-depth study—which looks at how children spend their day.  It reveals that children </span><em>under the age of two</em><span> are already spending more than an hour per day on screens.</span></p><p>YouTube usage for this group has more than doubled in just four years. </p><p>Poor and marginalized communities are hurt the most. As your income drops, your children’s screen time more than doubles. </p><p>In other words, these children are getting turned into screen addicts long before they enter the school system. </p><p>This is why teachers are speaking out. They see the fallout every day in their classrooms. </p><p>I’m dumbfounded when I hear ‘experts’ claim that phones are not the problem. Like  tobacco companies—whose hired experts long denied the connection between smoking and cancer—they say that “correlation does not prove causation.” </p><p>But that’s just sophistry and spin. </p><p>Parents, for example, have no doubts about the danger—because they see it happening right before their eyes.</p><p>But let’s give tech companies some credit. They have improved one skill among current students—cheating, which has now reached epic proportions. </p><p><span>The situation is so extreme that more than 40% of students were caught cheating recently—</span><em><a href="https://dailynous.com/2023/05/25/am-i-the-unethical-one-a-philosophy-professor-his-cheating-students/" rel="">and it happened in an ethics class!</a></em></p><p>The professor caught them in a simple way. He simply uploaded a copy of his final exam on to the web, but with wrong answers. </p><p>“Most of these answers were not just wrong, but obviously​ wrong to anyone who had paid attention in class,” he adds. But “40 out of 96 students looked at and used the planted final for at least a critical mass of questions.”</p><p><span>Another teacher </span><a href="https://thewalrus.ca/i-used-to-teach-students-now-i-catch-chatgpt-cheats" rel="">shares a similar lament</a><span>: “I used to teach students. Now I catch ChatGPT cheats.” </span></p><blockquote><p>I once believed my students and I were in this together, engaged in a shared intellectual pursuit. That faith has been obliterated over the past few semesters.</p></blockquote><p>Tech companies know exactly what they’re doing. </p><p><span>Microsoft researchers recently published a study showing that excessive use of new tech leads to a </span><a href="https://www.404media.co/microsoft-study-finds-ai-makes-human-cognition-atrophied-and-unprepared-3/" rel="">“deterioration of cognitive faculties that ought to be preserved.”</a></p><p><span>Theses </span><em>innovations</em><span> “deprive the user of the routine opportunities to practice their judgement and strengthen their cognitive musculature, leaving them atrophied and unprepared.” </span></p><p><span>Facebook did similar research, with similarly frightening results—and they </span><a href="https://www.washingtonpost.com/technology/2021/09/16/facebook-files-internal-research-harms/" rel="">worked hard to bury the results</a><span>.  We weren’t supposed to see the charts below. But a brave whistleblower took photos and </span><a href="https://www.wsj.com/articles/facebook-knows-instagram-is-toxic-for-teen-girls-company-documents-show-11631620739" rel="">leaked them to the </a><em><a href="https://www.wsj.com/articles/facebook-knows-instagram-is-toxic-for-teen-girls-company-documents-show-11631620739" rel="">Wall Street Journal</a></em><span>. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a50e4e-d280-4674-b551-e2797e32230d_1786x504.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a50e4e-d280-4674-b551-e2797e32230d_1786x504.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a50e4e-d280-4674-b551-e2797e32230d_1786x504.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a50e4e-d280-4674-b551-e2797e32230d_1786x504.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a50e4e-d280-4674-b551-e2797e32230d_1786x504.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a50e4e-d280-4674-b551-e2797e32230d_1786x504.png" width="1456" height="411" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d7a50e4e-d280-4674-b551-e2797e32230d_1786x504.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:411,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1211715,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.honest-broker.com/i/159140978?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a50e4e-d280-4674-b551-e2797e32230d_1786x504.png&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a50e4e-d280-4674-b551-e2797e32230d_1786x504.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a50e4e-d280-4674-b551-e2797e32230d_1786x504.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a50e4e-d280-4674-b551-e2797e32230d_1786x504.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a50e4e-d280-4674-b551-e2797e32230d_1786x504.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Meanwhile, an </span><a href="https://cdn.openai.com/global-affairs/openai-edu-ai-ready-workforce.pdf" rel="">internal study at OpenAI</a><span> shows that the two most commons uses of AI by students are (1) writing papers and (2) avoiding reading assignments. Other popular uses are answering exam questions and solving math problems. </span></p><p>None of this is a secret—the dysfunctional impact of new tech on students has been documented by study after study. Youngsters are under assault by tech leaders. </p><p>That’s how they meet their profit targets. </p><p>I only have one positive angle on this.</p><p>People are now aware. The blinders have been lifted from the public’s eyes. </p><p>Big tech has destroyed its credibility—and all the billionaires in Silicon Valley can’t restore it. They can buy lobbyists and co-opt “experts” with their cash. But the evil they are doing is now apparent to all unbiased observers.</p><p>Maybe they can stall change in Washington, D.C. by controlling politicians—at least for the time being. But they can’t stop the backlash that’s rising at a grass roots level.</p><p>That’s why the response to zombie culture is happening away from the limelight—in homes, schools, city council meetings, town hall gatherings, and other places where parents, teachers, and concerned individuals gather.</p><p>But it would be wise for our political leaders to take notice, and give their support. Even better, I’d like to see the leading tech companies admit that there’s a huge problem here, and they must fix it—because they caused it in the first place.</p><p>Do you think that’s too much to hope for? Do you doubt that the CEOs of Apple, Meta, Alphabet, X, and other tech empires will help us avert the coming crisis?</p><p>They might not have any other option. Their own management teams and employees are also parents, and just might rebel. </p><p>There must be thousands of people working at these tech behemoths—many in positions of great responsibility—who are horrified by what their own companies are doing. They need to speak up, and lead by example. </p><p> And I’m convinced many of them will.  </p><p>Yes, the palace guards are more powerful than the emperor. So we have legitimate reasons to hope that Silicon Valley itself might someday heal itself—and thus help heal the victims of their overreaching.</p><p>In the meantime, we should continue to push at the local levels. We need to find programs and initiatives that work, and share them. We need to raise awareness. And we need to do what we can to protect those most at risk. </p><p>Others will join us in time. </p><p>And let me make one final plea to those working inside these tech empires. Raise your voices, start a petition, sign an open letter, speak out in private and public. Push back!</p><p>There are others like you who are just waiting from someone brave enough to take the lead. That person could be you. </p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Devs say AI crawlers dominate traffic, forcing blocks on entire countries (272 pts)]]></title>
            <link>https://arstechnica.com/ai/2025/03/devs-say-ai-crawlers-dominate-traffic-forcing-blocks-on-entire-countries/</link>
            <guid>43476337</guid>
            <pubDate>Tue, 25 Mar 2025 21:42:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/ai/2025/03/devs-say-ai-crawlers-dominate-traffic-forcing-blocks-on-entire-countries/">https://arstechnica.com/ai/2025/03/devs-say-ai-crawlers-dominate-traffic-forcing-blocks-on-entire-countries/</a>, See on <a href="https://news.ycombinator.com/item?id=43476337">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
            <article data-id="2084217">
  
  <header>
  <div>
      

      

      <p>
        AI bots hungry for data are taking down FOSS sites by accident, but humans are fighting back.
      </p>

      
    </div>
</header>


  

  
      
    
    <div>
                      
                      
          
<p>Software developer Xe Iaso <a href="https://xeiaso.net/notes/2025/amazon-crawler/">reached a breaking point</a> earlier this year when aggressive AI crawler traffic from Amazon overwhelmed their Git repository service, repeatedly causing instability and downtime. Despite configuring standard defensive measures—adjusting robots.txt, blocking known crawler user-agents, and filtering suspicious traffic—Iaso found that AI crawlers continued evading all attempts to stop them, spoofing user-agents and cycling through residential IP addresses as proxies.</p>
<p>Desperate for a solution, Iaso eventually resorted to moving their server behind a VPN and creating "Anubis," a custom-built proof-of-work challenge system that forces web browsers to solve computational puzzles before accessing the site. "It's futile to block AI crawler bots because they lie, change their user agent, use residential IP addresses as proxies, and more," Iaso wrote in a <a href="https://xeiaso.net/notes/2025/amazon-crawler/">blog post</a> titled "a desperate cry for help." "I don't want to have to close off my Gitea server to the public, but I will if I have to."</p>
<p>Iaso's story highlights a broader crisis rapidly spreading across the open source community, as what appear to be aggressive AI crawlers increasingly overload community-maintained infrastructure, causing what amounts to persistent distributed denial-of-service (DDoS) attacks on vital public resources. According to a <a href="https://thelibre.news/foss-infrastructure-is-under-attack-by-ai-companies/">comprehensive recent report</a> from LibreNews, some open source projects now see as much as 97 percent of their traffic originating from AI companies' bots, dramatically increasing bandwidth costs, service instability, and burdening already stretched-thin maintainers.</p>
<p>Kevin Fenzi, a member of the Fedora Pagure project's sysadmin team, <a href="https://www.scrye.com/blogs/nirik/posts/2025/03/15/mid-march-infra-bits-2025/">reported on his blog</a> that the project had to block all traffic from Brazil after repeated attempts to mitigate bot traffic failed. GNOME GitLab implemented Iaso's "Anubis" system, requiring browsers to solve computational puzzles before accessing content. GNOME sysadmin Bart Piotrowski <a href="https://social.treehouse.systems/@barthalion/114190930216801561">shared</a> on Mastodon that only about 3.2 percent of requests (2,690 out of 84,056) passed their challenge system, suggesting the vast majority of traffic was automated. KDE's GitLab infrastructure was temporarily knocked offline by crawler traffic originating from Alibaba IP ranges, according to LibreNews, citing a KDE Development chat.</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>While Anubis has proven effective at filtering out bot traffic, it comes with drawbacks for legitimate users. When many people access the same link simultaneously—such as when a GitLab link is shared in a chat room—site visitors can face significant delays. Some mobile users have reported waiting up to two minutes for the proof-of-work challenge to complete, according to the news outlet.</p>
<p>The situation isn't exactly new. In December, Dennis Schubert, who maintains infrastructure for the Diaspora social network, <a href="https://pod.geraspora.de/posts/17342163">described</a> the situation as "literally a DDoS on the entire internet" after discovering that AI companies accounted for 70 percent of all web requests to their services.</p>
<p>The costs are both technical and financial. The Read the Docs project reported that blocking AI crawlers immediately decreased their traffic by 75 percent, going from 800GB per day to 200GB per day. This change saved the project approximately $1,500 per month in bandwidth costs, according to their blog post "AI crawlers need to be more respectful."</p>
<h2>A disproportionate burden on open source</h2>
<p>The situation has created a tough challenge for open source projects, which rely on public collaboration and typically operate with limited resources compared to commercial entities. Many maintainers have reported that AI crawlers deliberately circumvent standard blocking measures, ignoring robots.txt directives, spoofing user agents, and rotating IP addresses to avoid detection.</p>
<p>As LibreNews reported, Martin Owens from the Inkscape project <a href="https://floss.social/@doctormo/114191332274003577">noted</a> on Mastodon that their problems weren't just from "the usual Chinese DDoS from last year, but from a pile of companies that started ignoring our spider conf and started spoofing their browser info." Owens added, "I now have a prodigious block list. If you happen to work for a big company doing AI, you may not get our website anymore."</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>On Hacker News, commenters in threads <a href="https://news.ycombinator.com/item?id=43422413">about the LibreNews</a> post last week and a <a href="https://news.ycombinator.com/item?id=42750420">post on Iaso's battles</a> in January expressed deep frustration with what they view as AI companies' predatory behavior toward open source infrastructure. While these comments come from forum posts rather than official statements, they represent a common sentiment among developers.</p>
<p>As one Hacker News user <a href="https://news.ycombinator.com/item?id=43422792">put it</a>, AI firms are operating from a position that "goodwill is irrelevant" with their "$100bn pile of capital." The discussions depict a battle between smaller AI startups that have worked collaboratively with affected projects and larger corporations that have been unresponsive despite allegedly forcing thousands of dollars in bandwidth costs on open source project maintainers.</p>
<p>Beyond consuming bandwidth, the crawlers often hit expensive endpoints, like git blame and log pages, placing additional strain on already limited resources. Drew DeVault, founder of SourceHut, <a href="https://drewdevault.com/2025/03/17/2025-03-17-Stop-externalizing-your-costs-on-me.html">reported on his blog</a> that the crawlers access "every page of every git log, and every commit in your repository," making the attacks particularly burdensome for code repositories.</p>
<p>The problem extends beyond infrastructure strain. As LibreNews points out, some open source projects began receiving AI-generated bug reports as early as December 2023, first <a href="https://daniel.haxx.se/blog/2024/01/02/the-i-in-llm-stands-for-intelligence/">reported</a> by Daniel Stenberg of the Curl project on his blog in a post from January 2024. These reports appear legitimate at first glance but contain fabricated vulnerabilities, wasting valuable developer time.</p>

<h2>Who is responsible, and why are they doing this?</h2>
<p>AI companies have a history of taking without asking. Before the mainstream breakout of AI image generators and ChatGPT attracted attention to the practice in 2022, the machine learning field regularly compiled datasets with little regard to ownership.</p>
<p>While many AI companies engage in web crawling, the sources suggest varying levels of responsibility and impact. Dennis Schubert's <a href="https://pod.geraspora.de/posts/17342163">analysis</a> of Diaspora's traffic logs showed that approximately one-fourth of its web traffic came from bots with an OpenAI user agent, while Amazon accounted for 15 percent and Anthropic for 4.3 percent.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>The crawlers' behavior suggests different possible motivations. Some may be collecting training data to build or refine large language models, while others could be executing real-time searches when users ask AI assistants for information.</p>
<p>The frequency of these crawls is particularly telling. Schubert observed that AI crawlers "don't just crawl a page once and then move on. Oh, no, they come back every 6 hours because lol why not." This pattern suggests ongoing data collection rather than one-time training exercises, potentially indicating that companies are using these crawls to keep their models' knowledge current.</p>
<p>Some companies appear more aggressive than others. KDE's sysadmin team reported that crawlers from Alibaba IP ranges were responsible for temporarily knocking their GitLab offline. Meanwhile, Iaso's troubles came from Amazon's crawler. A member of KDE's sysadmin team told LibreNews that Western LLM operators like OpenAI and Anthropic were at least setting proper user agent strings (which theoretically allows websites to <a href="https://arstechnica.com/information-technology/2023/08/openai-details-how-to-keep-chatgpt-from-gobbling-up-website-data/">block them</a>), while some Chinese AI companies were reportedly more deceptive in their approaches.</p>
<p>It remains unclear why these companies don't adopt more collaborative approaches and, at a minimum, rate-limit their data harvesting runs so they don't overwhelm source websites. Amazon, OpenAI, Anthropic, and Meta did not immediately respond to requests for comment, but we will update this piece if they reply.</p>
<h2>Tarpits and labyrinths: The growing resistance</h2>
<p>In response to these attacks, new defensive tools have emerged to protect websites from unwanted AI crawlers. As Ars <a href="https://arstechnica.com/tech-policy/2025/01/ai-haters-build-tarpits-to-trap-and-trick-ai-scrapers-that-ignore-robots-txt/">reported in January</a>, an anonymous creator identified only as "Aaron" designed a tool called "Nepenthes" to trap crawlers in endless mazes of fake content. Aaron explicitly describes it as "aggressive malware" intended to waste AI companies' resources and potentially poison their training data.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<p>"Any time one of these crawlers pulls from my tarpit, it's resources they've consumed and will have to pay hard cash for," Aaron explained to Ars. "It effectively raises their costs. And seeing how none of them have turned a profit yet, that's a big problem for them."</p>
<p>On Friday, Cloudflare <a href="https://arstechnica.com/ai/2025/03/cloudflare-turns-ai-against-itself-with-endless-maze-of-irrelevant-facts/">announced</a> "AI Labyrinth," a similar but more commercially polished approach. Unlike Nepenthes, which is designed as an offensive weapon against AI companies, Cloudflare positions its tool as a legitimate security feature to protect website owners from unauthorized scraping, as we reported at the time.</p>
<p>"When we detect unauthorized crawling, rather than blocking the request, we will link to a series of AI-generated pages that are convincing enough to entice a crawler to traverse them," Cloudflare explained in its announcement. The company reported that AI crawlers generate over 50 billion requests to their network daily, accounting for nearly 1 percent of all web traffic they process.</p>
<p>The community is also developing collaborative tools to help protect against these crawlers. The "<a href="https://github.com/ai-robots-txt/ai.robots.txt">ai.robots.txt</a>" project offers an open list of web crawlers associated with AI companies and provides premade robots.txt files that implement the Robots Exclusion Protocol, as well as .htaccess files that return error pages when detecting AI crawler requests.</p>
<p>As it currently stands, both the rapid growth of AI-generated content <a href="https://www.404media.co/ai-slop-is-a-brute-force-attack-on-the-algorithms-that-control-reality/">overwhelming</a> online spaces and aggressive web-crawling practices by AI firms threaten the sustainability of essential online resources. The current approach taken by some large AI companies—<a href="https://www.vintagecomputing.com/index.php/archives/3292/the-pc-is-dead-its-time-to-make-computing-personal-again">extracting</a> vast amounts of data from open-source projects without clear consent or compensation—risks severely damaging the very digital ecosystem on which these AI models depend.</p>
<p>Responsible data collection may be achievable if AI firms collaborate directly with the affected communities. However, prominent industry players have shown little incentive to adopt more cooperative practices. Without meaningful regulation or self-restraint by AI firms, the arms race between data-hungry bots and those attempting to defend open source infrastructure seems likely to escalate further, potentially deepening the crisis for the digital ecosystem that underpins the modern Internet.</p>


          
                  </div>

                  
          






  <div>
  <div>
          <p><a href="https://arstechnica.com/author/benjedwards/"><img src="https://cdn.arstechnica.net/wp-content/uploads/2022/08/benj_ega.png" alt="Photo of Benj Edwards"></a></p>
  </div>

  <div>
    

    <p>
      Benj Edwards is Ars Technica's Senior AI Reporter and founder of the site's dedicated AI beat in 2022. He's also a tech historian with almost two decades of experience. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC.
    </p>
  </div>
</div>


  <p>
    <a href="https://arstechnica.com/ai/2025/03/devs-say-ai-crawlers-dominate-traffic-forcing-blocks-on-entire-countries/#comments" title="50 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    50 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  <div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <a href="https://arstechnica.com/space/2025/03/as-preps-continue-its-looking-more-likely-nasa-will-fly-the-artemis-ii-mission/">
              <img src="https://cdn.arstechnica.net/wp-content/uploads/2025/03/KSC-20250323-PH-FMX01_0159orig-copy-768x432.jpg" alt="Listing image for first story in Most Read: As preps continue, it’s looking more likely that NASA will fly the Artemis II mission" decoding="async" loading="lazy">
            </a>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>


  

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sell yourself, sell your work (238 pts)]]></title>
            <link>https://www.solipsys.co.uk/new/SellYourselfSellYourWork.html?yc25hn</link>
            <guid>43476249</guid>
            <pubDate>Tue, 25 Mar 2025 21:35:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.solipsys.co.uk/new/SellYourselfSellYourWork.html?yc25hn">https://www.solipsys.co.uk/new/SellYourselfSellYourWork.html?yc25hn</a>, See on <a href="https://news.ycombinator.com/item?id=43476249">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h2>Sell yourself, Sell Your Work ...</h2>
<a name="toc_name000"></a>

Doing technically brilliant work may be enough for your
personal
gratification, but you should never think it's enough. If you lock
yourself in a room and do the most marvellous work but don't tell
anyone, then no one will know, no one will benefit, and the work
will be lost. You may have derived
personal satisfaction from it,
but as far as the wider world is concerned, you may as well not have
bothered. For the world to benefit from your work, and therefore for
you to benefit <b>fully</b> from your work, you have to make it known.
<p>
In short, you have to "advertise".
</p><p>
I've dealt with any number of technically brilliant people who produce
outstanding work. The majority of them never bothered to write down
their work and communicate it to others - report writing is too boring,
uninteresting, and hard. And irrelevant. Or so they thought.
</p><p>
But if you've done great work, if you've produced superb
software or
fixed a fault with an aeroplane or investigated a problem ... without
telling anyone, your work is wasted. You have to write, you have to
tell people, and you have to do so in a way that they will take notice.
</p><p>
You don't necessarily need to make it flashy, whizzy, colourful and
animated, but you do have to present it well. Spelling errors may
not obscure the meaning, but you will lose some of your audience.
Poor punctuation may not matter to you, but your intended audience
may be put off by it. Muddied writing with no clear purpose makes it
hard for the reader to understand your point.
</p><p>
Write clearly and concisely, however, and your work may well save
others' time and effort. And gain you reputation.
</p><p>
<div>

Richard W Hamming wrote about one's work:
<p>
...
you can either do it in such a fashion that people
can indeed build on what you've done, or you can do
it in such a fashion that the next person has to
essentially duplicate again what you've done ...
</p><p>
...
it is not sufficient to do a job, you have to sell
it. "Selling" to a scientist is an awkward thing to do.
It's very ugly; you shouldn't have to do it. The world
is supposed to be waiting, and when you do something
great, they should rush out and welcome it. But the
fact is everyone is busy with their own work. You must
present it so well that they will set aside what they
are doing, look at what you've done, read it, and come
back and say, "Yes, that was good."
</p><p>
...  ask why you read some articles and not others.
You had better write your report so when it is published
... as the readers are turning the pages they won't
just turn your pages but they will stop and read yours.
If they don't stop and read it, you won't get credit.
</p><p>
You have to learn to write clearly and well so that
people will read it, you must learn to give reasonably
formal talks, and you also must learn to give informal
talks.
</p><p>
From "You and Your Research".
</p><hr>

For reference, some of Hamming's work includes Hamming codes,
the Hamming matrix, the Hamming window, Hamming numbers,
the Hamming bound, and the Hamming distance.
<p>
<a href="https://en.wikipedia.org/wiki/Richard_Hamming">https://en.wikipedia.org/wiki/Richard_Hamming</a>
</p></div> 
It seems crazy to require that technically talented people should
be forced to spend time doing something - report writing - at which
they're not gifted, but how else can the world benefit from their
brilliance? Without communicating their ideas, their work is lost
and might never have been.
</p><p>
Of course, this is one of the benefits of being an entrepreneur or
the founder of a business. The benefits of your work are there to
be seen - you don't have to play the office politics of constantly
justifying your existence. If you don't have customers, or if you
don't have advertisers, then you won't survive. If you do have
customers, or you do have advertisers, then your existence is
justified by the work you've done.
</p><p>
But you still have to sell! You now have to sell your company's
product or service, you now have to get known so that people will
start to use your product or service, or people will constantly
visit your website, which then attracts advertising. Whatever,
you need to sell! A company lives and dies by what it sells.
</p><p>
Some people say that the sole purpose of a company is to make money.
Others are more idealistic and say that it's to make the world better,
or to make their employees' lives better, or some other goal. But
without making money, everything else is moot.
</p><p>
But the word "sell" doesn't necessarily mean what you think it means.
Richard W. Hamming wrote about this in his talk entitled "You and Your Research",
given as a Bell Communications Research Colloquium Seminar on 1986/03/07.
The side-box is a small excerpt from this talk, transcriptions of which
can easily be found on-line.
</p><p>
So let the world benefit from your work.
</p><hr>

<center>

<hr>

<center></center>
<hr>

<h2>Send us a comment ...</h2>
<a name="toc_name001"></a>


</center></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In Jail Without a Lawyer: How a Texas Town Fails Poor Defendants (147 pts)]]></title>
            <link>https://www.nytimes.com/2025/03/25/us/maverick-county-texas-court-system.html</link>
            <guid>43474593</guid>
            <pubDate>Tue, 25 Mar 2025 18:59:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/03/25/us/maverick-county-texas-court-system.html">https://www.nytimes.com/2025/03/25/us/maverick-county-texas-court-system.html</a>, See on <a href="https://news.ycombinator.com/item?id=43474593">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/03/25/us/maverick-county-texas-court-system.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[The highest-ranking personal blogs of Hacker News (282 pts)]]></title>
            <link>https://refactoringenglish.com/tools/hn-popularity/</link>
            <guid>43474505</guid>
            <pubDate>Tue, 25 Mar 2025 18:48:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://refactoringenglish.com/tools/hn-popularity/">https://refactoringenglish.com/tools/hn-popularity/</a>, See on <a href="https://news.ycombinator.com/item?id=43474505">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><div><h4>The highest-ranking personal blogs of Hacker News
<a href="https://refactoringenglish.com/tools/hn-popularity/methodology">[<i></i> Methodology]</a></h4><div><p><label for="time-choose">Dates:</label>
</p><p><label for="start-date">Start:</label>
</p><p><label for="end-date">End:</label>
</p></div></div><main><div><table id="resultsTable"><thead><tr><th>Rank</th><th>Domain</th><th>Total Score</th><th>Author</th><th>Bio</th><th>Topics</th><th>Submissions</th></tr></thead><tbody></tbody></table></div><p><label for="result-limit">Limit:</label>
</p></main><div><p><a href="https://github.com/mtlynch/hn-popularity-contest-data"><i></i> Corrections</a></p></div></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[4o Image Generation (750 pts)]]></title>
            <link>https://openai.com/index/introducing-4o-image-generation/</link>
            <guid>43474112</guid>
            <pubDate>Tue, 25 Mar 2025 18:06:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/introducing-4o-image-generation/">https://openai.com/index/introducing-4o-image-generation/</a>, See on <a href="https://news.ycombinator.com/item?id=43474112">Hacker News</a></p>
Couldn't get https://openai.com/index/introducing-4o-image-generation/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Whose code am I running in GitHub Actions? (164 pts)]]></title>
            <link>https://alexwlchan.net/2025/github-actions-audit/</link>
            <guid>43473623</guid>
            <pubDate>Tue, 25 Mar 2025 17:17:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alexwlchan.net/2025/github-actions-audit/">https://alexwlchan.net/2025/github-actions-audit/</a>, See on <a href="https://news.ycombinator.com/item?id=43473623">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main" tabindex="-1"> <article>   <p>A week ago, somebody added malicious code <a href="https://www.stepsecurity.io/blog/harden-runner-detection-tj-actions-changed-files-action-is-compromised">to the tj-actions/changed-files GitHub Action</a>. If you used the compromised action, it would leak secrets to your build log. Those build logs are public for public repositories, so anybody could see your secrets. Scary!</p> <h2 id="mutable-vs-immutable-references">Mutable vs immutable references</h2> <p>This attack was possible because it’s common practice to refer to tags in a GitHub Actions workflow, for example:</p><pre><code>jobs:
  changed_files:
    ...
    steps:
      - name: Get changed files
        id: changed-files
        uses: <mark>tj-actions/changed-files@v2</mark>
      ...</code></pre><p>At a glance, this looks like an immutable reference to an already-released “version 2” of this action, but actually this is a mutable Git tag. If somebody changes the <code>v2</code> tag in the <code>tj-actions/changed-files</code> repo to point to a different commit, this action will run different code the next time it runs.</p> <p>If you specify a Git commit ID instead (e.g. <code>a5b3abf</code>), that’s an immutable reference that will run the same code every time.</p> <p>Tags vs commit IDs is a tradeoff between convenience and security. Specifying an exact commit ID means the code won’t change unexpectedly, but tags are easier to read and compare.</p> <h2 id="do-i-have-any-mutable-references">Do I have any mutable references?</h2> <p>I wasn’t worried about this particular attack because I don’t use <code>tj-actions</code>, but I was curious about what other GitHub Actions I’m using. I ran a short shell script in the folder where I have local clones of all my repos:</p> <div><pre><code>find <span>.</span> <span>-path</span> <span>'*/.github/workflows/*'</span> <span>-type</span> f <span>-name</span> <span>'*.yml'</span> <span>-print0</span> <span>\</span>
  | xargs <span>-0</span> <span>grep</span> <span>--no-filename</span> <span>"uses:"</span> <span>\</span>
  | <span>sed</span> <span>'s/\- uses:/uses:/g'</span> <span>\</span>
  | <span>tr</span> <span>'"'</span> <span>' '</span> <span>\</span>
  | <span>awk</span> <span>'{print $2}'</span> <span>\</span>
  | <span>sed</span> <span>'s/\r//g'</span> <span>\</span>
  | <span>sort</span> <span>\</span>
  | <span>uniq</span> <span>--count</span> <span>\</span>
  | <span>sort</span> <span>--numeric-sort</span>
</code></pre></div> <p>This prints a tally of all the actions I’m using. Here’s a snippet of the output:</p> <div><pre><code> 1 hashicorp/setup-terraform@v3
 2 dtolnay/rust-toolchain@v1
 2 taiki-e/create-gh-release-action@v1
 2 taiki-e/upload-rust-binary-action@v1
 4 actions/setup-python@v4
 6 actions/cache@v4
 9 ruby/setup-ruby@v1
31 actions/setup-python@v5
58 actions/checkout@v4
</code></pre></div> <p>I went through the entire list and thought about how much I trust each action and its author.</p> <ul> <li> <p>Is it from a large organisation like <code>actions</code> or <code>ruby</code>? They’re not perfect, but they’re likely to have good security procedures in place to protect against malicious changes.</p> </li> <li> <p>Is it from an individual developer or small organisation? Here I tend to be more wary, especially if I don’t know the author personally. That’s not to say that individuals can’t have good security, but there’s more variance in the security setup of random developers on the Internet than among big organisations.</p> </li> <li> <p>Do I need to use somebody else’s action, or could I write my own script to replace it? This is what I generally prefer, especially if I’m only using a small subset of the functionality offered by the action. It’s a bit more work upfront, but then I know exactly what it’s doing and there’s less churn and risk from upstream changes.</p> </li> </ul> <p>I feel pretty good about my list. Most of my actions are from large organisations, and the rest are a few actions specific to my Rust command-line tools which are non-critical toys, where the impact of a compromised GitHub repo would be relatively slight.</p> <h2 id="how-this-script-works">How this script works</h2> <p>This is a classic use of Unix pipelines, where I’m chaining together a bunch of built-in text processing tools. Let’s step through how it works.</p> <dl> <dt> <figure><pre><code data-lang="shell">find <span>.</span> <span>-path</span> <span>'*/.github/workflows/*'</span> <span>-type</span> f <span>-name</span> <span>'*.yml'</span> <span>-print0</span></code></pre></figure> </dt> <dd> <p> This looks for any GitHub Actions workflow file – any file whose name ends with <code>.yml</code> in a folder like <code>.github/workflows/</code>. It prints a list of filenames, like: </p> <p><code>./alexwlchan.net/.github/workflows/build_site.yml<br> ./books.alexwlchan.net/.github/workflows/build_site.yml<br> ./concurrently/.github/workflows/main.yml </code></p> <p> It prints them with a null byte (<code>\0</code>) between them, which makes it possible to split the filenames in the next step. By default it uses a newline, but a null byte is a bit safer, in case you have filenames which include newline characters. </p> <p> I know that I always use <code>.yml</code> as a file extension, but if you sometimes use <code>.yaml</code>, you can replace <code>-name '*.yml'</code> with <code>\( -name '*.yml' -o -name '*.yaml' \)</code> </p> <p> I have a bunch of local repos that are clones of open-source projects, and not my code, so I care less about what GitHub Actions they’re using. I excluded them by adding extra <code>-path</code> rules, like <code>-not -path './cpython/*'</code>. </p> </dd> <dt> <figure><pre><code data-lang="shell">xargs <span>-0</span> <span>grep</span> <span>--no-filename</span> <span>"uses:"</span></code></pre></figure> </dt> <dd> <p> Then we use <code>xargs</code> to go through the filenames one-by-one. The `-0` flag tells it to split on the null byte, and then it runs <code>grep</code> to look for lines that include <code>"uses:"</code> – this is how you use an action in your workflow file. </p> <p> The <code>--no-filename</code> option means this just prints the matching line, and not the name of the file it comes from. Not all of my files are formatted or indented consistently, so the output is quite messy: </p> <p><code>&nbsp;&nbsp;&nbsp;&nbsp;- uses: actions/checkout@v4<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;uses: "actions/cache@v4"<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;uses: ruby/setup-ruby@v1</code></p> </dd> <dt> <figure><pre><code data-lang="shell"><span>sed</span> <span>'s/\- uses:/uses:/g'</span> <span>\</span></code></pre></figure> </dt> <dd> <p> Sometimes there's a leading hyphen, sometimes there isn’t – it depends on whether <code>uses:</code> is the first key in the YAML dictionary. This <code>sed</code> command replaces <code>"- uses:"</code> with <code>"uses:"</code> to start tidying up the data. </p> <p><code>&nbsp;&nbsp;&nbsp;&nbsp;uses: actions/checkout@v4<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;uses: "actions/cache@v4"<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;uses: ruby/setup-ruby@v1</code></p> <p> I know <code>sed</code> is a pretty powerful tool for making changes to text, but I only know a couple of simple commands, like this pattern for replacing text: <code>sed 's/old/new/g'</code>. </p> </dd> <dt> <figure><pre><code data-lang="shell"><span>tr</span> <span>'"'</span> <span>' '</span></code></pre></figure> </dt> <dd> <p> Sometimes the name of the action is quoted, sometimes it isn’t. This command removes any double quotes from the output. </p> <p><code>&nbsp;&nbsp;&nbsp;&nbsp;uses: actions/checkout@v4<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;uses: actions/cache@v4<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;uses: ruby/setup-ruby@v1</code></p> <p> Now I’m writing this post, it occurs to me I could use <code>sed</code> to make this substitution as well. I reached for <code>tr</code> because I've been using it for longer, and the syntax is simpler for doing single character substitutions: <code>tr '&lt;oldchar&gt;' '&lt;newchar&gt;'</code> </p> </dd> <dt> <figure><pre><code data-lang="shell"><span>awk</span> <span>'{print $2}'</span></code></pre></figure> </dt> <dd> <p> This splits the string on spaces, and prints the second token, which is the name of the action: </p> <p><code>actions/checkout@v4<br> actions/cache@v4<br> ruby/setup-ruby@v1</code></p> <p> <code>awk</code> is another powerful text utility that I’ve never learnt properly – I only know how to print the nth word in a string. It has a lot of pattern-matching features I’ve never tried. </p> </dd> <dt> <figure><pre><code data-lang="shell"><span>sed</span> <span>'s/\r//g'</span></code></pre></figure> </dt> <dd> <p> I had a few workflow files which were using carriage returns (<code>\r</code>), and those were included in the <code>awk</code> output. This command gets rid of them, which makes the data more consistent for the final step. </p> </dd> <dt> <figure><pre><code data-lang="shell"><span>sort</span> | <span>uniq</span> <span>--count</span> | <span>sort</span> <span>--numeric-sort</span></code></pre></figure> </dt> <dd> <p> This sorts the lines so identical lines are adjacent, then it groups and counts the lines, and finally it re-sorts to put the most frequent lines at the bottom. </p> <p> I have this as a shell alias called <a href="https://alexwlchan.net/2016/a-shell-alias-for-tallying/"><code>tally</code></a>. </p> <p><code>&nbsp;&nbsp;&nbsp;6 actions/cache@v4<br> &nbsp;&nbsp;&nbsp;9 ruby/setup-ruby@v1<br> &nbsp;&nbsp;59 actions/checkout@v4</code></p> </dd> </dl> <p>This step-by-step approach is how I build Unix text pipelines: I can write a step at a time, and gradually refine and tweak the output until I get the result I want. There are lots of ways to do it, and because this is a script I’ll use once and then discard, I don’t have to worry too much about doing it in the “purest” way – as long as it gets the right result, that’s good enough.</p> <p>If you use GitHub Actions, you might want to use this script to check your own actions, and see what you’re using. But more than that, I recommend becoming familiar with the Unix text processing tools and pipelines – even in the age of AI, they’re still a powerful and flexible way to cobble together one-off scripts for processing data.</p> </article> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stoop Coffee: How a Simple Idea Transformed My Neighborhood (1145 pts)]]></title>
            <link>https://supernuclear.substack.com/p/stoop-coffee-how-a-simple-idea-transformed</link>
            <guid>43473618</guid>
            <pubDate>Tue, 25 Mar 2025 17:16:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://supernuclear.substack.com/p/stoop-coffee-how-a-simple-idea-transformed">https://supernuclear.substack.com/p/stoop-coffee-how-a-simple-idea-transformed</a>, See on <a href="https://news.ycombinator.com/item?id=43473618">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em>Editor’s note: This is a guest post from Patty Smith. Patty and her husband enjoy the kind of neighborhood so many of us would like - connected, helpful, fun - but it wasn’t that way two years ago. A simple tradition changed their neighborhood and is a good reminder of how small, consistent actions can have outsize results. It also shows you don’t have to share a kitchen or a roof to live in community. </em></p><p>18 months ago, I wasn’t planning on spending more time hanging out with my neighbors than with friends I’d known for decades. It started with a simple goal: my husband Tyler and I wanted that sense of community that feels like it’s only possible in the suburbs, but we believed we could achieve this while living in San Francisco. We brainstormed: should we make cookies and knock on doors? Should we invite neighbors over for dinner? Ultimately, we landed on sipping coffee on our “stoop”.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg" width="1024" height="768" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:768,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption><em>Tyler (left) and Patty (me! right) on one of our first stoop coffees</em></figcaption></figure></div><p>Hanging out on a stoop is not a novel concept. Unfortunately, an increasing trend of isolation has resulted in fewer and fewer neighbors gathering to connect with one another. Stooping has provided benefits to so many communities. Why not bring this concept to my own neighborhood?</p><p>Tyler and I were already having leisurely weekend morning coffees in our house, so it was an easy pivot to sit outside with our coffees and enjoy the sunshine. And thus our tradition began. Every weekend, we would bring our folding chairs out onto the street – we had to make do since our house doesn’t have a stoop – and enjoy our caffeine. As we saw people entering or exiting their homes, we'd enthusiastically wave them down, introduce ourselves, and write down their names in our shared spreadsheet. I wore a goofy tie-dyed Six Flags hat so people would remember us as “those people” and we started calling this our brand awareness campaign (but of course, we live in SF).</p><p>We met Luke a month or two after we’d been “stooping” on a regular basis. He came by to introduce himself and asked to exchange numbers so we could let him know if we’d be out there in the future, he’d love to join. At the time we didn’t realize how important this moment was for us. We’d been meeting many neighbors in passing but Luke was the first person to offer to sit with us and he wanted to know how to coordinate. In retrospect we should have been trying to get peoples’ numbers all along but hey, we were new to this!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg" width="1024" height="768" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:768,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg 1456w" sizes="100vw"></picture></div></a><figcaption><em>A typical weekend stoop hang</em></figcaption></figure></div><p><span>As soon as Luke started coming to stoop we actually started to resemble a group. It was validating to see a few neighbors getting together and this quickly attracted more! We learned to bring extra folding chairs for people who wanted to drop in for “just a minute” (or ninety) and Luke started bringing homebrew coffee to share. After a while, we realized it was starting to become unwieldy texting everyone when we were going to be outside. Thus, the WhatsApp group was born. At first this was just a place to announce when we’d be out having stoop coffee, but we soon realized people wanted to connect over more things than just coffee. So we ended up converting the group into a </span><a href="https://www.cooby.co/en/post/whatsapp-communities" rel="">WhatsApp Community</a><span> where we could have chats dedicated to certain topics or groups and plan other types of events together. Things were starting to get fun!</span></p><p>The first larger event that our “stoopers” wanted to host together was a block party that soon got scoped down to a pancake party. We made a spreadsheet, assigned tasks, and acquired obscene amounts of pancake mix. We decided to host the party on the sidewalk in front of our neighbor’s garage to keep things easy and so we didn’t have to apply for permits. Gathering tables, chairs and an electric griddle was quick work with so many neighbors invested in making the party a success. The most important thing we did in preparation for this party was to print out 100 door drops to deliver to the nearest set of neighbors and post party fliers up on telephone poles. It was old school but it worked! Most of the new faces we saw were people who found out about the event through our paper invitations.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png" width="1172" height="658" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:658,&quot;width&quot;:1172,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>The Pancake Party in full swing</figcaption></figure></div><p>The event was a resounding success. 70+ people came by, and we added over 50 new numbers to our WhatsApp Community. From that point forward, each stoop coffee started seeing at least 10-15 faces and new people were volunteering to host. A few neighbor gatherings later – including an epic “Dipsgiving” sidewalk potluck where everyone brought a dip to share – the momentum has continued and we now have multiple events every week. During the span of a recent week in December, we had a neighborhood trash pickup, a cookie swap, a TV show watch party, a parent hang at a neighborhood brewery, and—of course—a stoop coffee.</p><p>As I write this post, I realize that the “we” from earlier in the post has grown from just Tyler and me to a “we” that represents many more people who are invested in our community. It feels like our neighborhood is thriving. The in-person gatherings continue even without much intervention from the original few who got the ball rolling. The daily chatter on our WhatsApp is so gosh darn wholesome. Someone even sold a car in our ‘classifieds’ chat! Tyler and I have made many new friends in the neighborhood, and our neighbors who never knew each other before are becoming friends with each other. Our neighborhood community is now a group of people that we rely on and who rely on us for emotional support, last-minute childcare, home-cooked meals, general comradery, and much more. The best part is that I can tell we are still early in our growth, there are still many people to meet, and I feel a palpable sense of awe when I learn about a new skill or talent that exists right next door.</p><ul><li><p><strong>Keeping it simple</strong><span>: we’ve realized that some of our best events require the smallest amount of effort. To avoid burnout, we’ve intentionally kept our community building as low-lift as possible, which has the added benefit of creating space for other people to step up.</span></p></li><li><p><strong>Broadening vs. deepening</strong><span>: we bucket our events into “broadening” events which have the purpose of meeting new neighbors and “deepening” events which allow us to get to know our existing neighbors better. Being aware of that classification has helped us be strategic depending on what feels needed for the time and season.</span></p></li><li><p><strong>Seasonal events</strong><span>: Naturally, the colder months have become a better time for deepening events that often occur in someone’s home (e.g. TV show watch parties, cookie swaps, potlucks), while warmer events are better for getting together outside and broadening our community (e.g. sidewalk chalk murals, pancake parties, bonus evening stoop beers).</span></p></li><li><p><strong>The street as a third space</strong><span>: most of our stoop coffees are held in the street in front of someone’s driveway. This has the benefit of being visible and inviting to other neighbors, while making use of a previously underutilized space. It’s also got us thinking of other unused spaces that we can turn into community-gathering spots, such as turning a nearby parking spot into a parklet or a transit stop into a community gathering space.</span></p></li><li><p><strong>Relying on the community:</strong><span> It can often feel overwhelming to take on planning a big event. We started using the phrase “the universe provides” because the real magic is found in asking and giving freely within the community. It’s a daily treat to see neighbors stepping up for one another in unexpected ways now that more of us are connected!</span></p></li></ul><p>Our biggest goal in the coming year is to help more people organize in-person events and build towards a future where the community is sustainable even if we ever (god forbid!) move away. We’ve also been looking to connect with local businesses and influence policy matters that impact our local footprint. A few of us met recently to ideate on how we can keep vibrant commerce happening at our local businesses and how to best connect with the city decision-makers responsible for issues that impact our neighborhood community. To inform where we spend our energy, we’ve been starting with the issues that folks in the community care about and want to change (yes, we asked people at a recent block party!). We are also trying to find quick wins in collaborating with the city government to show that our voices can be heard and have a positive impact.</p><p>I cherish the neighbors I’ve met and am so grateful for the many people who put time and effort into building our community (Luke and Tyler, in particular, deserve a special shoutout). As we continue to grow, I’m excited to keep learning from others – please reach out if you want to brainstorm about neighborhood community building!</p><p><em><span>Thanks Patty for sharing your story! For more inspiration on how to do things like this in your neighborhood, please also see Savannah’s post on </span><a href="https://supernuclear.substack.com/p/building-neighborhood-communities" rel="">Building Neighborhood Communities</a><span>.</span></em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemini 2.5: Our most intelligent AI model (785 pts)]]></title>
            <link>https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/</link>
            <guid>43473489</guid>
            <pubDate>Tue, 25 Mar 2025 17:01:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/">https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=43473489">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="jump-content" tabindex="-1">
            

    
    

    <article>

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;Gemini 2.5: Our most intelligent AI model&quot;
  }">
      <div>
          
            <p>Mar 25, 2025</p>
          
          
            <p data-reading-time-render="">[[read-time]] min read</p>
          
        </div>
      
        <p>
          Gemini 2.5 is a thinking model, designed to tackle increasingly complex problems. Our first 2.5 model, Gemini 2.5 Pro Experimental, leads common benchmarks by meaningful margins and showcases strong reasoning and code capabilities.
        </p>
      
    </div>

    

    
      










<div>
    <figure>
      <div>
        <p><img alt="Five glowing blue rectangles, decreasing in size, angled diagonally across a dark background, suggesting depth and layers." data-component="uni-progressive-image" fetchpriority="high" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_keyword_header_no_text.width-200.format-webp.webp" width="360px" data-sizes="(max-width: 1023px) 100vw,(min-width: 1024px and max-width: 1259) 80vw, 1046px" data-srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_keyword_header_no_text.width-800.format-webp.webp 800w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_keyword_header_no_text.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_keyword_header_no_text.width-1600.format-webp.webp 1600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_keyword_header_no_text.width-2200.format-webp.webp 2200w">
        </p>
      </div>
      
    </figure>
  </div>






    

    
    <div>
        
          
            <div data-component="uni-article-jumplinks" data-analytics-module="{
    &quot;module_name&quot;: &quot;Article Jumplinks&quot;,
    &quot;section_header&quot;: &quot;Gemini 2.5: Our most intelligent AI model&quot;
  }">
  <nav aria-label="Article Jumplinks">
    <p><span>In this story</span>
    </p>
    
    
    <div>
      <ul id="article-jumplinks__list">
        
        <li>
          <a aria-label="link to Introducing Gemini 2.5" href="#gemini-2-5-thinking" id="gemini-2-5-thinking-anchor">Introducing Gemini 2.5</a>
        </li>
        
        <li>
          <a aria-label="link to Gemini 2.5 Pro" href="#gemini-2-5-pro" id="gemini-2-5-pro-anchor">Gemini 2.5 Pro</a>
        </li>
        
        <li>
          <a aria-label="link to Enhanced reasoning" href="#enhanced-reasoning" id="enhanced-reasoning-anchor">Enhanced reasoning</a>
        </li>
        
        <li>
          <a aria-label="link to Advanced coding" href="#advanced-coding" id="advanced-coding-anchor">Advanced coding</a>
        </li>
        
        <li>
          <a aria-label="link to The best of Gemini" href="#building-on-best-gemini" id="building-on-best-gemini-anchor">The best of Gemini</a>
        </li>
        
      </ul>
    </div>
    
  </nav>
</div>
          
          
          <div data-reading-time="true" data-component="uni-article-body">

            
              





<uni-article-speakable page-title="Gemini 2.5: Our most intelligent AI model" listen-to-article="Listen to article" data-date-modified="2025-03-25T17:00:17.781204+00:00" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-script-src="https://www.gstatic.com/readaloud/player/web/api/js/api.js"></uni-article-speakable>

            

            
            
<!--article text-->

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Gemini 2.5: Our most intelligent AI model&quot;
         }"><p data-block-key="rwvoz">Today we’re introducing Gemini 2.5, our most intelligent AI model. Our first 2.5 release is an experimental version of 2.5 Pro, which is state-of-the-art on a wide range of benchmarks and debuts at #1 on <a href="https://lmarena.ai/?leaderboard">LMArena</a> by a significant margin.</p><p data-block-key="4r4db"><a href="https://deepmind.google/technologies/gemini">Gemini 2.5 models</a> are thinking models, capable of reasoning through their thoughts before responding, resulting in enhanced performance and improved accuracy.</p><p data-block-key="f0oom">In the field of AI, a system’s capacity for “reasoning” refers to more than just classification and prediction. It refers to its ability to analyze information, draw logical conclusions, incorporate context and nuance, and make informed decisions.</p><p data-block-key="2bv7r">For a long time, we’ve explored ways of making AI smarter and more capable of reasoning through techniques like <a href="https://www.nature.com/articles/nature16961">reinforcement learning</a> and <a href="https://arxiv.org/abs/2201.11903">chain-of-thought prompting</a>. Building on this, we recently introduced our first thinking model, <a href="https://deepmind.google/technologies/gemini/flash-thinking/">Gemini 2.0 Flash Thinking</a>.</p><p data-block-key="b6ali">Now, with Gemini 2.5, we've achieved a new level of performance by combining a significantly enhanced base model with improved post-training. Going forward, we’re building these thinking capabilities directly into all of our models, so they can handle more complex problems and support even more capable, context-aware agents.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Gemini 2.5: Our most intelligent AI model&quot;
         }"><h2 data-block-key="w1q20">Introducing Gemini 2.5 Pro</h2><p data-block-key="fapl9">Gemini 2.5 Pro Experimental is our most advanced model for complex tasks. It tops the <a href="https://lmarena.ai/?leaderboard">LMArena</a> leaderboard — which measures human preferences — by a significant margin, indicating a highly capable model equipped with high-quality style. 2.5 Pro also shows strong reasoning and code capabilities, leading on common coding, math and science benchmarks.</p><p data-block-key="5tlko">Gemini 2.5 Pro is available now in <a href="http://aistudio.google.com/app/prompts/new_chat?model=gemini-2.5-pro-exp-03-25">Google AI Studio</a> and in the <a href="https://gemini.google.com/">Gemini app</a> for Gemini Advanced users, and will be coming to <a href="https://console.cloud.google.com/freetrial?redirectPath=/vertex-ai/studio">Vertex AI</a> soon. We’ll also introduce pricing in the coming weeks, enabling people to use 2.5 Pro with higher rate limits for scaled production use.</p></div>
  

  
    






<uni-image-full-width alignment="full" alt-text="Detailed table displays performance of multiple large language models on tests like math, coding, and reasoning. Gemini 2.5 Pro shows top results in several categories, indicated by highlighted cells. Fine print at the bottom provides context for the data." external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="Gemini 2.5: Our most intelligent AI model" external-link="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemini_benchmarks_cropped_light2x.original.png" custom-class="image-full-width--constrained-width uni-component-spacing">
  
  
    <p><img alt="Detailed table displays performance of multiple large language models on tests like math, coding, and reasoning. Gemini 2.5 Pro shows top results in several categories, indicated by highlighted cells. Fine print at the bottom provides context for the data." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini_benchmarks_cropped_light2x.gif">
    </p>
  
</uni-image-full-width>


  

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Gemini 2.5: Our most intelligent AI model&quot;
         }"><h2 data-block-key="rwvoz">Enhanced reasoning</h2><p data-block-key="def61">Gemini 2.5 Pro is state-of-the-art across a range of benchmarks requiring advanced reasoning. Without test-time techniques that increase cost, like majority voting, 2.5 Pro leads in math and science benchmarks like GPQA and AIME 2025.</p><p data-block-key="7n9sj">It also scores a state-of-the-art 18.8% across models without tool use on Humanity’s Last Exam, a dataset designed by hundreds of subject matter experts to capture the human frontier of knowledge and reasoning.</p></div>
  

  
    






<uni-image-full-width alignment="full" alt-text="Bar charts comparing the performance of Gemini 2.5 Pro with other AI models like OpenAI GPT-4.5 and Claude 3.7 Sonnet across three categories: Reasoning, Science, and Mathematics. Gemini 2.5 Pro shows strong results in all categories." external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="Gemini 2.5: Our most intelligent AI model" external-link="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_2.5_blog_1.original.jpg" custom-class="image-full-width--constrained-width uni-component-spacing">
  
  
    <p><img alt="Bar charts comparing the performance of Gemini 2.5 Pro with other AI models like OpenAI GPT-4.5 and Claude 3.7 Sonnet across three categories: Reasoning, Science, and Mathematics. Gemini 2.5 Pro shows strong results in all categories." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_2.5_blog_1.width-100.format-webp.webp" loading="lazy" data-loading="{
            &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_2.5_blog_1.width-500.format-webp.webp&quot;,
            &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_2.5_blog_1.width-1000.format-webp.webp&quot;
          }">
    </p>
  
</uni-image-full-width>


  

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Gemini 2.5: Our most intelligent AI model&quot;
         }"><h2 data-block-key="rwvoz">Advanced coding</h2><p data-block-key="cvan3">We’ve been focused on coding performance, and with Gemini 2.5 we’ve achieved a big leap over 2.0 — with more improvements to come. 2.5 Pro excels at creating visually compelling web apps and agentic code applications, along with code transformation and editing. On SWE-Bench Verified, the industry standard for agentic code evals, Gemini 2.5 Pro scores 63.8% with a custom agent setup.</p><p data-block-key="4vj36">Here’s an example of how 2.5 Pro can use its reasoning capabilities to create a video game by producing the executable code from a single line prompt.</p></div>
  

  
    
  
    


  <uni-youtube-player-article index="11" thumbnail-alt="Animation of dinosaur game made with Gemini" video-id="RLCBSpgos6s" video-type="video">
  </uni-youtube-player-article>


  


  

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Gemini 2.5: Our most intelligent AI model&quot;
         }"><h2 data-block-key="rwvoz">Building on the best of Gemini</h2><p data-block-key="o2v1">Gemini 2.5 builds on what makes Gemini models great — native multimodality and a long context window. 2.5 Pro ships today with a 1 million token context window (2 million coming soon), with strong performance that improves over previous generations. It can comprehend vast datasets and handle complex problems from different information sources, including text, audio, images, video and even entire code repositories.</p><p data-block-key="1jf6n">Developers and enterprises can start experimenting with Gemini 2.5 Pro in <a href="http://aistudio.google.com/app/prompts/new_chat?model=gemini-2.5-pro-exp-03-25">Google AI Studio</a> now, and <a href="https://gemini.google.com/">Gemini Advanced</a> users can select it in the model dropdown on desktop and mobile. It will be available on <a href="https://console.cloud.google.com/freetrial?redirectPath=/vertex-ai/studio">Vertex AI</a> in the coming weeks.</p><p data-block-key="2hak6">As always, we welcome feedback so we can continue to improve Gemini’s impressive new abilities at a rapid pace, all with the goal of making our AI more helpful.</p></div>
  


            
            

            
              




            
          </div>
        
      </div>
  </article>
  





  

  


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kylie Minogue song about a typeface (203 pts)]]></title>
            <link>https://abcdinamo.com/news/german-bold-italic</link>
            <guid>43473358</guid>
            <pubDate>Tue, 25 Mar 2025 16:51:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abcdinamo.com/news/german-bold-italic">https://abcdinamo.com/news/german-bold-italic</a>, See on <a href="https://news.ycombinator.com/item?id=43473358">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Sometimes we invite artists, writers, and other friends to take over <a href="https://abcdinamo.com/newsletter">our newsletter</a> and write about fonts from a cultural perspective. And this time, New York-based writer and editor <em>Whitney Mallett</em> took a deep dive into the 1997 Towa Tei track <strong>GBI (German Bold Italic),</strong> for which Kylie Minogue sang from the perspective of a typeface.</p><div><p>Whitney is the editor of <a href="https://www.whitneyreview.org/">The Whitney Review of New Writing</a>, a biannual print bulletin of new criticism, and she was also the co-editor of <a href="https://www.pinupmagazine.org/articles/barbie-dreamhouse-architectural-survey">Barbie Dreamhouse: An Architectural Survey</a>.  Below, she enlightens us on the highly under-appreciated history of Towa Tei’s data-track font release.</p></div><section id="i-contrast-its-cool-when-kylie-minogue-was-german-bold-italic" tabindex="0"><p><h2>“I Contrast, It’s Cool”: When Kylie Minogue Was German Bold Italic</h2></p><figure></figure><div><p>Kylie Minogue announces “I am a typeface” in a 1997 song she made with producer Towa Tei. As this lyric suggests, the techno-pop track in question, <a href="https://www.youtube.com/watch?v=4Fw2Labli4k">“GBI (German Bold Italic)”</a>, is delivered from the perspective of a font. Minogue’s breathy, almost robotic vocals bring the absurdist premise to life, reciting declarations of design compatibility over a minimalist reverb-drenched beat. <em>“You will like my sense of style”</em> is her most oft-repeated refrain.</p><p>German Bold Italic (three words Minogue intones throughout the track with staccato punctuation) is a real typeface, which was created to accompany the musical release. To develop it, Tei tapped artist <em>Hiro Sugiyama</em>, who had that same year launched the commercial design studio Enlightenment. The high-impact techno typography not only defined the record’s text-centric cover, but a glyph set was made available as a data track on Tei’s Sound Museum CD and also downloadable from both Tei’s and Minogue’s official websites. Extra-thick and geometric, the font had something in common with the heavy black frames that were Tei’s hallmark at the time. Another line from the song: <em>“I contrast, it’s cool.”</em></p></div><figure></figure><div><p>When this offbeat track came out, Minogue was one of the biggest pop stars in the world. The Australian “I Should Be So Lucky” singer rose to fame as a teenager in the late 1980s for clear vocals that complemented the precision of disco and dance-pop perfectly. She’s like Donna Summer in that way, a vessel for songs that can be hugely innovative and futuristic without compromising mainstream appeal. By her fifth studio album, the eponymously titled <em>Kylie Minogue (1994)</em>, she was starting to show she was curious about experimenting with different sounds and high-concept visuals. In 1995 she collaborated on a murder ballad duet with Nick Cave, and by the time she joined forces with Tei, she was dating music video director (and Björk ex) Stéphane Sednaoui, who’s often credited with influencing Minogue’s art-pop risk-taking. Sednaoui directed the video for “GBI (German Bold Italic).” In it, Minogue is dressed like a geisha roaming the streets of Tokyo (despite Tei’s involvement, the video no doubt reads today as cultural appropriation.)</p><p>Tei was a big deal back then too. Originally from Japan, he had moved to New York to study fashion in the 1980s and became one of the founding members, along with <em>Lady Miss Kier</em> and <em>DJ Dmitry</em>, of the dance music group <em>Dee-Lite</em>. Their breakthrough hit, <em>“Groove is In the Heart” (1990)</em>, undeniably one of the decade’s defining house music tracks, peaked at #4 on the Billboard Hot 100. A few years before Tei’s collaboration with Minogue, at Dee-Lite’s height, he decided to quit the group and move back to Japan to focus on solo projects — in part because he’d suffered a back injury from a fall which affected his ability to tour. Sound Museum — which “GBI” the song and glyph set feature on — was his second solo album. In addition to Minogue, he recruited an eclectic group of artists for the project, from rappers Biz Markie and Mos Def to bossa nova royalty Bebel Gilberto. “GBI,” however, is the only song about a font.</p></div><figure></figure><p>Leading up to the turn of the millennium, typography trends included a revival of thick, rounded, condensed, futuristic san-serifs from the late ’60s and early ’70s. German Bold Italic feels part of this wave. So does <em>The Powerpuff Girls</em> logo, designed for the TV show which first aired in 1998, a year after Tei and Minogue’s song. <a href="https://fontsinuse.com/uses/25101/german-bold-italic-towa-tei-ft-kylie-minogue-">In the comments section of fontsinsuse.com</a>, the site’s editor Florian Hardwig compares German Bold Italic to <em>Futura Extra Bold Italic</em>. He points out, however, that while Futura is German, released by the Bauer Foundry in Frankfurt in 1927, Futura Extra Bold isn’t German at all: “It was added by Edwin W. Shaar for Intertype in the US in 1952, based on Paul Renner’s design. The oblique followed in 1955, made by Shaar together with Tommy Thompson.” Another user named Thiago, points out that, rather ironically, the glyph set for German Bold Italic doesn’t support the German language.</p><figure></figure><p>Despite being made available for anyone to download (you can still download it via <a href="https://web.archive.org/web/20041130183630/http://www.kylie.co.uk/desktop/gbi.TTF">an old version of Minogue’s website</a>, accessible using the Wayback Machine), German Bold Italic’s only documented usage outside of Tei and Minogue’s promo is by artist <em>Cory Arcangel</em>. In 2015, Arcangel used the font for merch he designed for the Brooklyn pop group Wet. Heather grey sweatshirts were screenprinted <em>“WWW. / KANYEWET / .BIZ”</em> in super chunky black all-caps. (The band long used this URL for their website as a cheeky gag, though today the domain is dead.) In an interview with The Fader, Arcangel explained:</p><p>I’ve used the font a few times in the past for artworks (I showed some drawings on paper watermarked with my name in GBI at my Whitney show in 2011) but I’ve always wanted to use it to make a shirt for a pop group… one pop group used to advertise the next. Also it’s a sick classic vector techno font, and super rare these days.</p><div><p>It’s a style that’s less rare today. But still, German Bold Italic in particular, I’d love to see it more.</p><p>Minogue sings, “Let me adorn you / The bold design of you.”</p></div><figure></figure></section><section id="more" tabindex="0"><p><h2>More</h2></p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why is C the symbol for the speed of light? (2004) (134 pts)]]></title>
            <link>https://math.ucr.edu/home/baez/physics/Relativity/SpeedOfLight/c.html</link>
            <guid>43472663</guid>
            <pubDate>Tue, 25 Mar 2025 15:42:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://math.ucr.edu/home/baez/physics/Relativity/SpeedOfLight/c.html">https://math.ucr.edu/home/baez/physics/Relativity/SpeedOfLight/c.html</a>, See on <a href="https://news.ycombinator.com/item?id=43472663">Hacker News</a></p>
<div id="readability-page-1" class="page">
<p><a href="https://math.ucr.edu/home/baez/physics/index.html">[Physics FAQ]</a> -
   <a href="https://math.ucr.edu/home/baez/physics/Administrivia/copyright.html">[Copyright]</a></p>

<p>By Philip Gibbs, 1997, 2004.</p>





<p>"As for <i>c</i>, that is the speed of light in vacuum, and if you ask why <i>c</i>, 
the answer is that it is the initial letter of <i>celeritas</i>, the Latin word meaning 
speed." <br>Isaac Asimov in "C for <i>Celeritas</i> (1959)" [1]</p>




<h2>A Short Answer</h2>

Although <i>c</i> is now the universal symbol for the speed of light, the most common
symbol in the nineteenth century was an upper-case <i>V</i> which Maxwell had started
using in 1865.&nbsp; That was the notation adopted by Einstein for his first few papers on
relativity from 1905.&nbsp; The origins of the letter <i>c</i> being used for the speed of
light can be traced back to a paper of 1856 by Weber and Kohlrausch [2].&nbsp; They
defined and measured a quantity denoted by <i>c</i> that they used in an electrodynamics
force law equation.&nbsp; It became known as Weber's constant and was later shown to have
a theoretical value equal to the speed of light times the square root of two.&nbsp; In
1894 Paul Drude modified the usage of Weber's constant so that the letter <i>c</i> became
the symbol for the speed of electrodynamic waves [3].&nbsp; In optics Drude continued to
follow Maxwell in using an upper-case <i>V</i> for the speed of light.&nbsp; Progressively
the <i>c</i> notation was used for the speed of light in all contexts as it was picked up
by Max Planck, Hendrik Lorentz and other influential physicists.&nbsp; By 1907 when
Einstein switched from <i>V</i> to <i>c</i> in his papers, it had become the standard
symbol for the speed of light in vacuum for electrodynamics, optics, thermodynamics and
relativity.

<p>Weber apparently meant <i>c</i> to stand for "constant" in his force law, but there is
evidence that physicists such as Lorentz and Einstein were accustomed to a common
convention that <i>c</i> could be used as a variable for velocity.&nbsp; This usage can be
traced back to the classic Latin texts in which <i>c</i> stood for "celeritas" meaning
"speed".&nbsp; The uncommon English word "celerity" is still used when referring to the
speed of wave propagation in fluids.&nbsp; The same Latin root is found in more familiar
words such as acceleration and even celebrity, a word used when fame comes quickly.</p>

<p>Although the <i>c</i> symbol was adapted from Weber's constant, it was probably thought
appropriate for it to represent the velocity of light later on because of this Latin
interpretation.&nbsp; So history provides an ambiguous answer to the question "Why is
<i>c</i> the symbol for the speed of light?", and it is reasonable to think of <i>c</i> as
standing for either "constant" or "celeritas".</p>




<h2>The Long Answer</h2>

<p>In 1992 Scott Chase wrote on sci.physics that "anyone who read hundreds of books by
Isaac Asimov knows that the Latin word for `speed' is `celeritas', hence the symbol `c'
for the speed of light".&nbsp; Asimov had written an article entitled "C for Celeritas" in
a sci-fi magazine in 1959 and had reprinted it in some of his later books [1].&nbsp; Scott
was the first editor of the Physics FAQ on Usenet and Asimov's explanation was later
included in the relativity section as the "probable" answer to the question "Why is
<i>c</i> the symbol for the speed of light?".&nbsp; Since then, Asimov's answer has become
a factoid repeated in many articles and books.&nbsp; But if you go back and read his essay
you discover that Asimov merely stated his case in one sentence, and made no further
attempt to justify his theory for the origin of the "c" notation.&nbsp; So is his claim
really born out by history, or was <i>c</i> originally introduced as a variable standing
for something else?&nbsp; The special theory of relativity is based on the principle that
the speed of light is constant; so did <i>c</i> stand for "constant", or did it simply
appear by accident in some text where all the other likely variables for speed had already
been used up?&nbsp; These questions have been asked repeatedly on usenet, and now after
much searching through old papers and books the answers can be revealed.</p>

<p>A lower-case <i>c</i> has been consistently used to denote the speed of light in
textbooks on relativity almost without exception since such books started to be
written.&nbsp; For example, the notation was used in the earliest books on relativity by
Lorentz (1909) [4], Carmichael (1913) [5], Silberstein (1914) [6], Cunningham (1915) [7],
and Tolman (1917) [8].&nbsp; That was not the case just a few years before.&nbsp; In his
earliest papers on relativity from 1905 to 1907, Einstein began by using an upper-case
<i>V</i> for the speed of light [9].&nbsp; At that time he was also writing papers about
the thermodynamics of radiation, and in those he used up upper-case <i>L</i> [10].&nbsp;
All of these papers appeared in volumes of the German periodical <i>Annalen Der
Physik</i>.&nbsp; Einstein's notation changed suddenly in 1907 in a paper for the Journal
<i>Jahrbuch der Radioaktivit�t und Elektronik </i> [11].&nbsp; There he used the lower
case <i>c</i>, and his most famous equation <i>E</i>&nbsp;=&nbsp;<i>mc</i><sup>2</sup>
came into being.</p>

<p>It is not difficult to find where the upper case <i>V</i> had come from.&nbsp; Maxwell
used it extensively in his publications on electrodynamics from as early as 1865
[12].&nbsp; It was the principal symbol for the speed of light in his 1873 treatise on
electrodynamics [13].&nbsp; By the 1890s Maxwell's book was in wide circulation around
the world and there were translations available in French and German.&nbsp; It is no
surprise then that the upper-case <i>V</i> is found in use in such papers as the 1887
report of Michelson and Morley on their attempt to find seasonal variations in the speed
of light [14].&nbsp; That was written in the United States, but the same notation was also
found across Europe, from papers by Oliver Lodge [15] and Joseph Lamor [16] in England, to
the lecture notes of Poincar� in France [17], and the textbooks of Paul Drude in Germany
[18] and Lorentz in the Netherlands [19].&nbsp; Einstein's education at the Polytechnik in
Zurich had not covered Maxwell's theory of Electrodynamics in the detail he would have
liked.&nbsp; But he had read a number of extra textbooks on the new Electrodynamics as
self study, so he would have been familiar with the standard notations.&nbsp; From 1905 he
wrote his first papers on relativity, and there is nothing extraordinary in his choice of
the symbol <i>V</i> for the speed of light [9].</p>

<p>Why then, did he change it to <i>c</i> in 1907?&nbsp; At that time he still worked as a
clerk in the Bern patent office, but for the previous two years he had been in regular
correspondence with eminent physicists such as Max Laue, Max Planck, Wilhelm Wien and
Johannes Stark.&nbsp; Stark was the editor of the <i>Jahrbuch</i>, and had asked Einstein
to write the article in which he was to first use the letter <i>c</i>.&nbsp; Einstein
mentioned to Stark that it was hard for him to find the time to read published scientific
articles to acquaint himself with all the work others have done in the field, but
he <i>had</i> seen papers by Lorentz, Kohn, Monsegeil and Planck [20].&nbsp; Lorentz and
Planck in particular had been using <i>c</i> for the speed of light in their work.&nbsp;
Lorentz had won the 1902 Nobel prize for physics, and it is not surprising that physicists
in Germany had now taken up the same notation.&nbsp; It is also not surprising that
Einstein, who was looking for an academic position, aligned himself to the same
conventions at that time.&nbsp; Another reason for him to make the switch was that the
letter <i>c</i> is simply more practical.&nbsp; The upper-case <i>V</i> would have been
easily confused with the lower case <i>v</i> appearing in the equations of relativity for
the velocity of moving bodies or frames of reference.&nbsp; Einstein must have found this
confusion inconvenient, especially in his hand written notes.</p>

<p>Looking back at papers of the late 1890s, we find that Max Planck and Paul Drude in
particular were using the symbol <i>c</i> at that time.&nbsp; The name of Drude is less
well known to us today.&nbsp; He worked on relations between the physical constants and
high precision measurements of their value.&nbsp; These were considered to be highly
worthy pursuits of the time.&nbsp; Drude had been a student of Voigt, who himself had used
a Greek ω for the speed of light when he wrote down an almost complete form of the
Lorentz transformations in 1887 [43].&nbsp; Voigt's ω was later used by a few other
physicists [44, 45], but Drude did not use his teacher's notation.&nbsp; Drude first used
the symbol <i>c</i> in 1894, and in doing so he referenced a paper by Kirchhoff [3].&nbsp;
As already mentioned, Paul Drude also used <i>V</i>.&nbsp; In fact he made a distinction
of using <i>V</i> in the theory of optics for the directly-measured speed of light in vacuum,
whereas he used <i>c</i> for the electromagnetic constant that was the theoretical speed
of electromagnetic waves.&nbsp; This is seen especially clearly in his book "Theory of
Optics" of 1900 [21], which is divided into two parts with <i>V</i> used in the first and
<i>c</i> in the second part.&nbsp; Although Maxwell's theory of light predicted that they
had the same value, it was only with the theory of relativity that these two things were
established as fundamentally the same constant.&nbsp; Other notations vied against Drude's
and Maxwell's for acceptance.&nbsp; Herglotz [46] opted for an elaborate script <i>B</i>,
while Himstedt [47], Helmholtz [48] and Hertz [49] wrote the equations of electrodynamics
with the letter <i>A</i> for the reciprocal of the speed of light.&nbsp; In 1899 Planck
backed Drude by using <i>c</i>, when he wrote a paper introducing what we now call the
Planck scale of units based on the constants of electrodynamics, quantum theory and
gravity [22].&nbsp; Drude and Planck were both editors of the prestigious journal
<i>Annalen Der Physik</i>, so they would have had regular contact with most of the
physicists of central Europe.</p>

<p>Lorentz was next to change notation.&nbsp; When he started writing about light speed in
1887 he used an upper case <i>A</i> [23], but then switched to Maxwell's upper case
<i>V</i> [24].&nbsp; He wrote a book in 1895 [25] that contained the equations for length
contraction, and was cited by Einstein in his 1907 paper.&nbsp; While Drude had started to
use <i>c</i>, Lorentz was still using <i>V</i> in this book.&nbsp; He continued to use
<i>V</i> until 1899 [26], but by 1903 when he wrote an encyclopedia article on
electrodynamics [27] he too used <i>c</i>.&nbsp; Max Abraham was another early user of the
symbol <i>c</i> in 1902, in a paper that was seen by Einstein [28].&nbsp; From Drude's
original influence, followed by Planck and Lorentz, by 1907 the <i>c</i> symbol had become
the prevailing notation in Germanic science and it made perfect sense for Einstein to
adopt it too.</p>

<p>In France and England the electromagnetic constant was symbolised by a lower case
<i>v</i> rather than Drude's <i>c</i>.&nbsp; This was directly due to Maxwell, who wrote
up a table of experimental results for direct measurements of the speed of light on the
one hand and electromagnetic experiments on the other.&nbsp; He used <i>V</i> for the
former and <i>v</i> for the latter.&nbsp; Maxwell described a whole suite of possible
experiments in electromagnetism to determine <i>v</i>.&nbsp; Those that had not already
been done were performed one after the other in England and France over the three decades
that followed [29].&nbsp; In this context, lower case <i>v</i> was always used for the
quantity measured.&nbsp; But using <i>v</i> was doomed to pass away once authors had to
write relativistic equations involving moving bodies, because <i>v</i> was just too common
a symbol for velocity.&nbsp; The equations were much clearer when something more distinct
was used for the velocity of light to differentiate it from the velocity of moving
bodies.</p>

<p>While Maxwell always used <i>v</i> in this way, he also had a minor use for the symbol
<i>c</i> in his widely read treatise of 1873.&nbsp; Near the end he included a section
about the German electromagnetic theory that had been an incomplete precursor to his own
formulation [30].&nbsp; This theory, expounded by Gauss, Neumann, Weber, and Kirchhoff,
attempted to combine the laws of Coulomb and Amp�re into a single action-at-a-distance
force law.&nbsp; The first versions appeared in Gauss's notes in 1835 [31], and the
complete form was published by Weber in 1846 [32].&nbsp; Many physicists of the time were
heavily involved in the process of defining the units of electricity.&nbsp; Coulomb's law
of electrostatic force could be used to give one definition of the unit of charge while
Amp�re's force law for currents in wires gave another.&nbsp; The ratio between these units
had the dimension of a velocity, so it became of great practical importance to measure its
value.&nbsp; In 1856 Weber and Kohlrausch published the first accurate measurement
[2].&nbsp; To give a theoretical backing they rewrote Weber's force law in terms of the
measured constant and used the symbol <i>c</i>.&nbsp; This <i>c</i> appeared in numerous
subsequent papers by German physicists such as Kirchhoff, Clausius, Himstedt, and
Helmholtz, who referred to it as "Weber's constant".&nbsp; That continued until the
1870s, when Helmholtz discredited Weber's force law on the grounds of energy
conservation, and Maxwell's more complete theory of propagating waves prevailed.</p>

<p>Two papers using Weber's force law are of particular note.&nbsp; One by Kirchhoff [33]
and another by Riemann [34] related Weber's constant to the velocity at which electricity
propagated.&nbsp; They found this speed to be Weber's constant divided by the square root
of two and it was very close to the measured speed of light.&nbsp; It was already known
from experiments by Faraday that light was affected by magnetic fields, so there was
already much speculation that light could be an electrodynamic phenomenon.&nbsp; This was
the inspiration for Maxwell's work on electrodynamics, so it is natural that he finally
included a discussion of the force law in his treatise [30].&nbsp; The odd thing is that
when Maxwell wrote down the force law, he changed the variable <i>c</i> so that it was
smaller than Weber's constant by a factor of the square root of two.&nbsp; So Maxwell was
probably the first to use <i>c</i> for a value equal to the speed of light, although he
defined it as the speed of electricity through wires instead.</p>

<p>So <i>c</i> was used as Weber's constant having a value of the speed of light times the
square root of two, and this can be related to the later use of <i>c</i> for the speed of
light itself.&nbsp; Firstly, when Maxwell wrote Weber's force law in his treatise in 1873,
he modified the scale of <i>c</i> in the equation so that it reduced by a factor of the
square root of two.&nbsp; Secondly, when Drude first used <i>c</i> in 1894 for the speed
of light [3], the paper by Kirchhoff that he cited [35] was using <i>c</i> for Weber's
constant, so Drude had made the same adjustment as Maxwell.&nbsp; It is impossible to say
if Drude copied the notation from Maxwell, but he did go one step further in explicitly
naming his <i>c</i> as the velocity of electrodynamic waves which by Maxwell's theory was
also the speed of light.&nbsp; He seems to have been the first to do so, with Lorentz,
Planck, and others following suit a few years later.</p>

<p>So to understand why <i>c</i> became the symbol for the speed of light we now have to
find out why Weber used it in his force law.&nbsp; In the paper of 1856 [2] Weber's
constant was introduced with these words "and the constant <i>c</i> represents that
relative speed, that the electrical masses e and e must have and keep, if they are not to
affect each other."&nbsp; So it appears that <i>c</i> originated as a letter standing for
"constant" rather than "celeritas".&nbsp; Nevertheless, it had nothing to do with the constancy
of the speed of light until much later.</p>
   
<p>Despite this, there could still be some substance to Asimov's claim that <i>c</i> is
the initial letter of "celeritas".&nbsp; It is true, after all, that <i>c</i> is also
often used for the speed of sound, and it is commonly used as the velocity constant in the
wave equation.&nbsp; Furthermore, this usage was around before relativity.</p>
 
<p>Starting with the Latin manuscripts of the 17th century, such as Galileo's "De Motu
Antiquiora" or Newton's "Principia", we find that they often use the word "celeritas" for
speed.&nbsp; But their writing style was very geometric and descriptive, and so they
did not tend to write down formulae where speed is given a symbol.&nbsp; But an example of
the letter <i>c</i> being used for speed can be found from the eighteenth century.&nbsp;
In 1716 Jacob Hermann published a Latin text called Phoronomia, meaning the science of
motion [36].&nbsp; In it he developed Newton's mechanics in a form more familiar to us
now, except for the Latin symbols.&nbsp; His version of the basic newtonian equation
<i>F</i>&nbsp;=&nbsp;<i>ma</i> was d<i>c</i> = <i>p</i> d<i>t</i>, where <i>c</i> stands
for "celeritas" meaning speed, and <i>p</i> stands for "potentia", meaning force.</p>

<p>Apart from in relativity, the most pervasive use of <i>c</i> to represent a speed today
is in the wave equation.&nbsp; In 1747 Jean d'Alembert made a mathematical study of the
vibrating string and discovered the one dimensional wave equation, but he wrote it without
the velocity constant.&nbsp; Euler generalised d'Alembert's equation to include the
velocity, denoting it by the letter <i>a</i> [38].&nbsp; The general solution is <i>y</i>
= f(<i>x</i> - <i>at</i>) + f(<i>x</i> + <i>at</i>), representing two waves of fixed shape
travelling in opposite directions with velocity <i>a</i>.</p>

<p>Euler was one of the most prolific mathematicians of all time.&nbsp; He wrote hundreds
of manuscripts and most of them were in Latin.&nbsp; If anyone established a convention
for using <i>c</i> for "celeritas", it has to have been Euler.&nbsp; In 1759 he studied
the vibrations of a drum, and moved on to the 2-dimensional wave equation.&nbsp; This he
wrote in the form we are looking for with <i>c</i> now the velocity constant [39].

</p><p>The wave equation became a subject of much discussion, being investigated by all the
great mathematicians of the �poque including Lagrange, Fourier, Laplace, and
Bernoulli.&nbsp; Through their works, Euler's form of the wave equation with <i>c</i> for
the speed of wave propagation was carved in stone for good.&nbsp; To a first
approximation, sound waves are also governed by the same wave equation in three
dimensions, so it is not surprising that the speed of sound also came to be denoted by the
symbol <i>c</i>.&nbsp; This predates relativity and can be found, for example, in Lord
Rayleigh's classic text "Theory of Sound" [40].&nbsp; Physicists of the nineteenth century
would have read the classic Latin texts on physics, and would have been aware that
<i>c</i> could stand for "celeritas".&nbsp; As an example, Lorentz used <i>c</i> in 1899
for Earth's speed through the ether [41].&nbsp; We even know that Einstein used
it for speed outside relativity, because in a letter to a friend about a patent for a
flying machine, he used <i>c</i> for the speed of air flowing at a mere 4.9 m/s [42].</p>

<p>In conclusion, although we can trace <i>c</i> back to Weber's force law where it most
likely stood for "constant", it is possible that its use persisted because <i>c</i> could
stand for "celeritas" and had therefore become a conventional symbol for speed.&nbsp; We
cannot tell for sure how Drude, Lorentz, Planck or Einstein thought about their notation,
so there can be no definitive answer for what it stood for then.&nbsp; The only logical
answer is that when you use the symbol <i>c</i>, it stands for whatever possibility you
prefer.</p>




<h2>References</h2>

<dl>
<dd>[1] Isaac Asimov "C for Celeritas" in "The Magazine of Fantasy and Science Fiction",
        Nov-59 (1959), reprinted in "Of Time, Space, and Other Things", Discus (1975), and
        "Asimov On Physics", Doubleday (1976)</dd><br>

<dd>[2] R. Kohlrausch and W.E. Weber, "Ueber die Elektricit�tsmenge, welche bei
        galvanischen Str�men durch den Querschnitt der Kette fliesst", Annalen der Physik,
        <b>99</b>, pg 10 (1856)</dd><br>

<dd>[3] P. Drude, "Zum Studium des elektrischen Resonators", G�ttingen Nachrichten (1894),
        pgs 189–223 </dd><br>

<dd>[4] H.A. Lorentz, "The theory of Electrons and its applications to the phenomena of
        light and radiant heat".&nbsp; A course of lectures delivered in Columbia
        University, New York, in March and April 1906, Leiden (1909)</dd><br>

<dd>[5] R.D. Carmichael, "The Theory of Relativity", John Wiley &amp; Sons (1913)</dd><br>

<dd>[6] L. Silberstein, "The Theory of Relativity", Macmillan (1914)</dd><br>

<dd>[7] E. Cunningham, "The Principle of Relativity", Cambridge University Press
        (1914)</dd><br>

<dd>[8] R.C. Tolman, "The Theory of the Relativity of Motion", University of California
        Press (1917)</dd><br>

<dd>[9] A. Einstein, From "The Collected Papers, Vol 2, The Swiss Years: Writings,
        1900–1909", English Translation, he wrote five papers using <i>V</i>, e.g. "On
        the Electrodynamics of Moving Bodies", Annalen Der Physik <b>17</b>, pgs 891–921
        (1905), "On the Inertia of Energy Required by the Relativity Principle", Annalen
        Der Physik <b>23</b>, pgs 371–384 (1907)</dd><br>

<dd>[10] A. Einstein, e.g. "On the Theory of Light Production and Light Absorption",
         Annalen Der Physik,<b> 20</b>, pgs 199–206 (1906)</dd><br>

<dd>[11] A. Einstein, "On the Relativity Principle and the Conclusions Drawn From It",
         Jahrbuch der Radioaktivit�t und Elektronik <b>4</b>, pgs 411–462 (1907)</dd><br>

<dd>[12] J. Clerk Maxwell, "A dynamical theory of the electromagnetic field",
         Philos. Trans. Roy. Soc. <b>155</b>, pgs 459–512 (1865).&nbsp; Abstract:
         Proceedings of the Royal Society of London <b>13</b>, pgs 531–536
         (1864)</dd><br>

<dd>[13] J. Clerk Maxwell, "A Treatise on Electricity and Magnetism", Oxford Clarendon
         Press (1873)</dd><br>

<dd>[14] A.A. Michelson and E.W. Morley, "On the Relative Motion of the Earth and the
         Luminiferous Ether", Amer. J. Sci. <b>34</b>, pgs 333–345 (1887),
         Philos. Mag. <b>24</b>, pgs 449–463 (1887)</dd><br>

<dd>[15] O. Lodge, "Aberration Problems", Phil. Trans. Roy. Soc. <b>184</b>, pgs 729–804
         (1893)</dd><br>

<dd>[16] J. Larmor, "A Dynamical Theory of the Electric and Luminiferous Medium I",
         Phil. Trans. Roy. Soc. <b>185</b>, pgs 719–822 (1894)</dd><br>

<dd>[17] H. Poincar�, "Cours de physique math�matique.&nbsp; Electricit� et optique.&nbsp;
         La lumi�re et les th�ories �lectrodynamiques" (1900)</dd><br>

<dd>[18] P. Drude, "Physik des �thers auf elektromagnetischer Grundlage", Verlag F. Enke,
         Stuttgart (1894)</dd><br>

<dd>[19] H. Lorentz, "Versuch einer Theorie der elektrischen und optischen Erscheinungen
         in bewegten K�rpern", Leiden (1895)</dd><br>

<dd>[20] A. Einstein, from "The Collected Papers, Vol 5, The Swiss Years: Correspondence,
         1902–1914", English Translation, Doc 58. </dd><br>

<dd>[21] P. Drude, "The theory of optics", translated from German by C.R. Mann and
         R.A. Millikan, New York, Longmans, Green, and Co. (1902) </dd><br>

<dd>[22] M. Planck, "Uber irreversible Strahlungsvorgange", Verl. d. Kgl. Akad. d. Wiss.
         (1899) </dd><br>

<dd>[23] H.A. Lorentz, "De l'Influence du Mouvement de la Terre sur les Phenomenes
         Lumineux", Arch. Neerl. <b>21</b>, pg 103 (1887)</dd><br>

<dd>[24] H.A. Lorentz, "On the Reflection of Light by Moving Bodies",
         Versl. Kon. Akad. Wetensch Amsterdam I, 74 (1892)</dd><br>

<dd>[25] H.A. Lorentz, "Versuch einer Theorie der elektrischen und optischen Erscheinungen
         in bewegten K�rpern", Leiden (1895)</dd><br>

<dd>[26] H. A. Lorentz, "Th�orie simplifi�e des phenom�nes electriques et optiques dans
         des corps en mouvement", Proc. Roy. Acad. Amsterdam I 427 (1899)</dd><br>

<dd>[27] H.A. Lorentz, "Maxwells elektromagnetische Theorie" Encyclop�die der
         Mathematischen Wissenschaften.&nbsp; Leipzig, Teubner (1903)</dd><br>

<dd>[28] M. Abraham, "Prinzipien der Dynamik des Elektrons", Annalen der Physik <b>10</b>,
         pgs 105–179 (1903)</dd><br>

<dd>[29] e.g. J.J. Thomson and G.F.C. Searle, "A Determination of `v', the Ratio of the
         Electromagnetic Unit of Electricity to the Electrostatic Unit", Proc. Roy. Soc.
         Lond. <b>181</b>, pg 583 (1890), M. Hurmuzescu, "Nouvelle determination du
         rapport v entre les unites electrostatiques et electromagnetiques", Ann. de
         Chim. et de Phys., 7a serie T. X April 1897, pg 433. (1897)</dd><br>

<dd>[30] J. Clerk Maxwell, "A Treatise on Electricity and Magnetism", Oxford Clarendon
         Press, Vol II; Chapter 23, section 849 (1873)</dd><br>

<dd>[31] K.F. Gauss, "Zur mathematischen Theorie der elektrodynamischen Wirkung" (1835),
         in "Werke", G�ttingen 1867; Vol. V, pg 602</dd><br>

<dd>[32] W. Weber, "Elektrodynamische Maassbestimmingen uber ein allgemeines Grundgesetz
         der elektrischen Wirkung", Abh. Leibnizens Ges., Leipzig (1846)</dd><br>

<dd>[33] G. Kirchhoff, "Ueber die Bewegung der Elektricit�t in Leitern"
         Ann. Phys. Chem. <b>102</b>, 529–544 (1857)</dd><br>

<dd>[34] G.F.B. Riemann, "Ein Beitrag zur Elektrodynamik", Annalen der Physik und Chemie,
         pg 131 (1867)</dd><br>

<dd>[35] G. Kirchhoff, "Zur Theorie der Entladung einer Leydener Flasche",
         Pogg. Ann. <b>121</b> (1864)</dd><br>

<dd>[36] J. Hermann, "Phoronomia", Amsterdam, Wetsten, (1716)</dd><br>

<dd>[37] J. d'Alembert, "Recherches sur les cordes vibrantes", L�Acad�mie Royal des
         Sciences (1747)</dd><br>

<dd>[38] L. Euler, "De La Propagation Du Son" Memoires de l'acadamie des sciences de
         Berlin [15] (1759), 1766, pgs 185–209, in "Opera physica miscellanea
         epistolae.&nbsp; Volumen primum", pg 432 </dd><br>

<dd>[39] L. Euler, "Eclaircissemens Plus Detailles Sur La Generation et La Propagation Du
         Son Et Sur La Formation De L'Echo", "Memoires de l'acadamie des sciences de
         Berlin" [21] (1765), 1767, pgs 335–363 in "Opera physica miscellanea
         epistolae.&nbsp; Volumen primum", pg 540 </dd><br>

<dd>[40] J.W. Strutt, "Theory of Sound" Vol 1, pg 251, McMillan and Co. (1877)</dd><br>

<dd>[41] H.A. Lorentz, "Stokes' Theory of Aberration in the Supposition of a Variable
         Density of the Aether", Proc. Roy. Acad. Amsterdam I, pg 443 (1899)</dd><br>

<dd>[42] A. Einstein, "The Collected Papers, Vol 5, The Swiss Years: Correspondence,
         1902–1914", English Translation, Doc 86 (1907)</dd><br>

<dd>[43] W. Voigt, "Ueber das Doppler'sche Princip", Goett. Nachr. <b>2</b>, pg 41
         (1887)</dd><br>

<dd>[44] E. Cohn, "Zur Elektrodynamik bewegter Systeme. II", Sitzungsberichte der
         K�niglich Preussischen Akademie der Wissenschaften zu Berlin, der
         physikalisch-mathematischen Classe (1904)</dd><br>

<dd>[45] M. Brillouin, "Le mouvement de la Terre et la vitesse de la lumi�re", comptes
         rendu 140, pg 1674 (1905)</dd><dd><br>

</dd><dd>[46] G. Herglotz, "Zur Elektronentheorie", Nachrichten von der Gesellschaft <b>6</b>,
         pg 357 (1903)</dd><br>

<dd>[47] F. Himstedt, "Ueber die Schwingungen eines Magneten unter dem d�mpfenden Einflu�
         einer Kupferkugel", Nachrichten von der Gesellschaft <b>11</b>, pg 308
         (1875)</dd><br>

<dd>[48] H. Helmholtz, Berlin: Verl. d. Kgl. Akad. d. Wiss. (1892)</dd><br>

<dd>[49] H. Hertz, "Electric Waves", Macmillan (1893)</dd><br>

</dl>


 


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[My Favorite C++ Pattern: X Macros (2023) (106 pts)]]></title>
            <link>https://danilafe.com/blog/chapel_x_macros/</link>
            <guid>43472143</guid>
            <pubDate>Tue, 25 Mar 2025 14:57:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://danilafe.com/blog/chapel_x_macros/">https://danilafe.com/blog/chapel_x_macros/</a>, See on <a href="https://news.ycombinator.com/item?id=43472143">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>
            <p><em>Table of Contents</em></p><nav id="TableOfContents">
  <ul>
    <li><a href="#application-1-string-interning">Application 1: String Interning</a></li>
    <li>
<a href="#application-2-ast-class-hierarchy">Application 2: AST Class Hierarchy</a>
      <ul>
        <li><a href="#tags-and-dynamic-casting">Tags and Dynamic Casting</a></li>
        <li><a href="#the-visitor-pattern-without-double-dispatch">The Visitor Pattern without Double Dispatch</a></li>
        <li><a href="#generating-a-python-class-hierarchy">Generating a Python Class Hierarchy</a></li>
      </ul>
    </li>
    <li><a href="#application-3-cpython-method-tables-and-getters">Application 3: CPython Method Tables and Getters</a></li>
    <li><a href="#discussion">Discussion</a></li>
  </ul>
</nav>
</div>
<p>When I first joined the <a href="https://github.com/chapel-lang/chapel/">Chapel<svg style="display: none;">
    <use xlink:href="/feather-sprite.svg#external-link"></use>
</svg></a> team,
one pattern used in its C++-based compiler made a strong impression on me. Since
then, I’ve used the pattern many more times, and have been very satisfied with
how it turned out. However, it feels like the pattern is relatively unknown, so
I thought I’d show it off, and some of its applications in the
<a href="https://github.com/chapel-lang/chapel/">Chapel compiler<svg style="display: none;">
    <use xlink:href="/feather-sprite.svg#external-link"></use>
</svg></a>. I’ve slightly tweaked
a lot of the snippets I directly show in this article for the sake of simpler
presentation; I’ve included links to the original code (available on GitHub)
if you want to see the unabridged version.</p>
<p>Broadly speaking, the “X Macros” pattern is about generating code. If you have a <em>lot</em>
of repetitive code to write (declaring many variables or classes, performing
many very similar actions, etc.), this pattern can save a lot of time, lead
to much more maintainable code, and reduce the effort required to add <em>more</em>
code.</p>
<p>I will introduce the pattern in its simplest form with my first example:
<a href="https://en.wikipedia.org/wiki/String_interning">interning strings<svg style="display: none;">
    <use xlink:href="/feather-sprite.svg#external-link"></use>
</svg></a>.</p>
<a href="#application-1-string-interning">
  <h3 id="application-1-string-interning">Application 1: String Interning</h3>
</a>
<p>The Chapel compiler interns a lot of its strings. This way, it can reduce the
memory footprint of keeping identifiers in memory (every string <code>"x"</code> is
actually the <em>same</em> string) and make for much faster equality comparisons
(you can just perform a pointer comparison!). Generally, a <code>Context</code> class
is used to manage interning state. A new interned string can be constructed
using the context object in the following manner:</p>
<div><pre tabindex="0"><code data-lang="C++"><span><span><span>UniqueString</span><span>::</span><span>get</span><span>(</span><span>ctxPtr</span><span>,</span> <span>"the string"</span><span>);</span>
</span></span></code></pre></div>
<p>Effectively, this performs a search of the currently existing unique strings.
If one with the content (<code>"the string"</code> in this case) doesn’t exist, it’s
created and registered with the <code>Context</code>. Otherwise, the existing string is
returned. Some strings, however, occur a lot in the compiler, to the point that
it would be inefficient to perform the whole “find-or-create” operation every
time. One example is the <code>"this"</code> string, which is an identifier with a lot of
special behavior in the language (much like <code>this</code> in languages such as Java).
To support such frequent flier strings, the compiler initializes them once,
and creates a variable per-string that can be accessed to get that string’s value.</p>
<p>There’s that repetitive code. Defining a brand new variable for each string,
of which there are around 100 at the time of writing, is a lot of boilerplate.
There are also at least two places where code needs to be added:
<span>
<label for="template-note">once in the declaration of the variables, once in the code that initializes them.</label>
<span><span>[note:</span>
A third use in the compiler is actually a variadic template defined over
character arrays. The template is defined and specialized in such a way that
you can refer to a variable by its string contents (i.e., you can write
<code>USTR("the string")</code> instead of
<code>theStringVariable</code>).
<span>]</span>
</span>
</span>

It would be very easy to accidentally modify the former but not the latter,
especially for developers not familiar with how these “common strings” are
implemented.</p>
<p>This is where the X Macros come in. If you look around the compiler source code,
there’s a header file that looks something like the following:</p>





<div data-base-path="%!s(<nil>)" data-file-path="frontend/include/chpl/framework/all-global-strings.h"><pre tabindex="0"><code data-lang="C++"><span><span><span>X</span><span>(</span><span>align</span>          <span>,</span> <span>"align"</span><span>)</span>
</span></span><span><span><span>X</span><span>(</span><span>atomic</span>         <span>,</span> <span>"atomic"</span><span>)</span>
</span></span><span><span><span>X</span><span>(</span><span>bool_</span>          <span>,</span> <span>"bool"</span><span>)</span>
</span></span><span><span><span>X</span><span>(</span><span>borrow</span>         <span>,</span> <span>"borrow"</span><span>)</span>
</span></span><span><span><span>X</span><span>(</span><span>borrowed</span>       <span>,</span> <span>"borrowed"</span><span>)</span>
</span></span><span><span><span>X</span><span>(</span><span>by</span>             <span>,</span> <span>"by"</span><span>)</span>
</span></span><span><span><span>X</span><span>(</span><span>bytes</span>          <span>,</span> <span>"bytes"</span><span>)</span>
</span></span><span><span><span>// A lot more of these...
</span></span></span></code></pre></div>


<p>What’s this <code>X</code> thing? That right there is the essence of the pattern: the macro
<code>X</code> <em>isn’t defined in the header!</em> Effectively, <code>all-global-strings.h</code> is just
a list, and we can “iterate” over this list to generate some code for each
one of its elements, in as many places as we want. What I mean by this is
that we can then write code like the following:</p>





<div data-base-path="%!s(<nil>)" data-file-path="frontend/include/chpl/framework/global-strings.h"><pre tabindex="0"><code data-lang="C++"><span><span>    <span>struct</span> <span>GlobalStrings</span> <span>{</span>
</span></span><span><span><span>#define X(field, str) UniqueString field;
</span></span></span><span><span><span>#include</span> <span>"all-global-strings.h"</span><span>
</span></span></span><span><span><span>#undef X
</span></span></span><span><span><span></span>    <span>};</span></span></span></code></pre></div>


<p>In this case, we define the macro <code>X</code> to ignore the value of the string (we’re
just declaring it here), and create a new <code>UniqueString</code> variable declaration.
Since the declaration is inside the <code>GlobalStrings</code> struct, this ends up
creating a field. Just like that, we’ve declared a class with over 100
fields. Initialization is equally simple:</p>





<div data-base-path="%!s(<nil>)" data-file-path="frontend/lib/framework/Context.cpp"><pre tabindex="0"><code data-lang="C++"><span><span>    <span>GlobalStrings</span> <span>globalStrings</span><span>;</span>
</span></span><span><span>    <span>Context</span> <span>rootContext</span><span>;</span>
</span></span><span><span>
</span></span><span><span>    <span>static</span> <span>void</span> <span>initGlobalStrings</span><span>()</span> <span>{</span>
</span></span><span><span><span>#define X(field, str) globalStrings.field = UniqueString::get(&amp;rootContext, str);
</span></span></span><span><span><span>#include</span> <span>"chpl/framework/all-global-strings.h"</span><span>
</span></span></span><span><span><span>#undef X
</span></span></span><span><span><span></span>    <span>}</span></span></span></code></pre></div>


<p>With this, we’ve completely automated the code for for both declaring and
initializing all 100 of our unique strings. Adding a new string doesn’t require
a developer to know all of the places where this is implemented: just by
modifying the <code>all-global-strings.h</code> header with a new call to <code>X</code>, they can
add both a new variable and code to initialize it. Pretty robust!</p>
<a href="#application-2-ast-class-hierarchy">
  <h3 id="application-2-ast-class-hierarchy">Application 2: AST Class Hierarchy</h3>
</a>
<p>Altough the interned strings are an excellent first example, it wasn’t the
first usage of X Macros that I encountered in the Chapel compiler. Beyond
strings, the compiler uses X Macros to represent the whole class hierarchy
of <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">abstract syntax tree (AST)<svg style="display: none;">
    <use xlink:href="/feather-sprite.svg#external-link"></use>
</svg></a>
nodes that it uses. Here, the code is actually a bit more complicated; the
class hierarchy isn’t a <em>list</em> like the strings were; it is itself a tree.
To represent such a structure, we need more than a single <code>X</code> macro; the
compiler went with <code>AST_NODE</code>, <code>AST_BEGIN_SUBCLASSES</code>, and <code>AST_END_SUBCLASSES</code>.
Here’s what that looks like:</p>





<div data-base-path="%!s(<nil>)" data-file-path="frontend/include/chpl/uast/uast-classes-list.h"><pre tabindex="0"><code data-lang="C++"><span><span>  <span>// Other AST nodes above...
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>AST_BEGIN_SUBCLASSES</span><span>(</span><span>Loop</span><span>)</span>
</span></span><span><span>      <span>AST_NODE</span><span>(</span><span>DoWhile</span><span>)</span>
</span></span><span><span>      <span>AST_NODE</span><span>(</span><span>While</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>AST_BEGIN_SUBCLASSES</span><span>(</span><span>IndexableLoop</span><span>)</span>
</span></span><span><span>      <span>AST_NODE</span><span>(</span><span>BracketLoop</span><span>)</span>
</span></span><span><span>      <span>AST_NODE</span><span>(</span><span>Coforall</span><span>)</span>
</span></span><span><span>      <span>AST_NODE</span><span>(</span><span>For</span><span>)</span>
</span></span><span><span>      <span>AST_NODE</span><span>(</span><span>Forall</span><span>)</span>
</span></span><span><span>      <span>AST_NODE</span><span>(</span><span>Foreach</span><span>)</span>
</span></span><span><span>    <span>AST_END_SUBCLASSES</span><span>(</span><span>IndexableLoop</span><span>)</span>
</span></span><span><span>
</span></span><span><span>  <span>AST_END_SUBCLASSES</span><span>(</span><span>Loop</span><span>)</span>
</span></span><span><span>
</span></span><span><span>  <span>// Other AST nodes below...
</span></span></span></code></pre></div>


<p>The class hierarchy defined in this header, called <code>uast-classes-list.h</code>, is
used for a lot of things, both in the compiler itself and in some libraries
that <em>use</em> the compiler. I’ll go through the use cases in turn.</p>
<a href="#tags-and-dynamic-casting">
  <h4 id="tags-and-dynamic-casting">Tags and Dynamic Casting</h4>
</a>
<p>First, to deal with a general absence of
<a href="https://en.wikipedia.org/wiki/Run-time_type_information">RTTI<svg style="display: none;">
    <use xlink:href="/feather-sprite.svg#external-link"></use>
</svg></a>, the hierarchy header
is used to declare a “tag” enum. Each AST node has a tag matching its class;
this allows us inspect the AST and perform safe casts similar to <code>dynamic_cast</code>.
Note that for parent classes (defined via <code>BEGIN_SUBCLASSES</code>), we actually
end up creating <em>two</em> tags: one <code>START_...</code> and one <code>END_...</code>. The reason
for this will become clear in a moment.</p>





<div data-base-path="%!s(<nil>)" data-file-path="frontend/include/chpl/uast/AstTag.h"><pre tabindex="0"><code data-lang="C++"><span><span><span>enum</span> <span>AstTag</span> <span>{</span>
</span></span><span><span><span>#define AST_NODE(NAME) NAME ,
</span></span></span><span><span><span>#define AST_BEGIN_SUBCLASSES(NAME) START_##NAME ,
</span></span></span><span><span><span>#define AST_END_SUBCLASSES(NAME) END_##NAME ,
</span></span></span><span><span><span>#include</span> <span>"chpl/uast/uast-classes-list.h"</span><span>
</span></span></span><span><span><span>#undef AST_NODE
</span></span></span><span><span><span>#undef AST_BEGIN_SUBCLASSES
</span></span></span><span><span><span>#undef AST_END_SUBCLASSES
</span></span></span><span><span><span></span>  <span>NUM_AST_TAGS</span><span>,</span>
</span></span><span><span>  <span>AST_TAG_UNKNOWN</span>
</span></span><span><span><span>};</span></span></span></code></pre></div>


<p>The above snippet makes <code>AstTag</code> contain elements such as <code>DoWhile</code>,
<code>While</code>, <code>START_Loop</code>, and <code>END_Loop</code>. For convenience, we also add a couple
of other elements: <code>NUM_AST_TAGS</code>, which is
<span>
<label for="numbering-node">automatically assigned the number of tags we generated,</label>
<span><span>[note:</span>
This is because C++ assigns integer values to enum elements sequentially, starting
at zero.
<span>]</span>
</span>
</span>

and a generic “unknown tag” value.</p>
<p>Having generated the enum elements in this way, we can write query functions.
This way, the API consumer can write <code>isLoop(tag)</code> instead of manually performing
a comparison. Code generation here is actually split into two distinct forms
of “is bla” methods: those for concrete AST nodes (<code>DoWhile,</code> <code>While</code>) and
those for abstract base classes (<code>Loop</code>). The reason for this is simple:
only a <code>AstTag::DoWhile</code> represents a do-while loop, but both <code>DoWhile</code>
and <code>While</code> are instances of <code>Loop</code>. So, <code>isLoop</code> should return true for both.</p>
<p>This is where the <code>START_...</code> and <code>END_...</code> enum elements come in. Reading
the header file top-to-bottom, we first end up generating <code>START_Loop</code>,
then <code>DoWhile</code> and <code>While</code>, and then <code>END_Loop</code>. Since C++ assigns integer
value to enums sequentially, to check if a tag “extends” a base class, it’s
sufficient to check if its value is greater than the <code>START</code> token, and
smaller than the <code>END</code> token – this means it was declared within the
matching pair of <code>BEGIN_SUBCLASSES</code> and <code>END_SUBCLASES</code>.</p>





<div data-base-path="%!s(<nil>)" data-file-path="frontend/include/chpl/uast/AstTag.h"><pre tabindex="0"><code data-lang="C++"><span><span><span>// define is___ for leaf and regular nodes
</span></span></span><span><span><span>// (not yet for abstract parent classes)
</span></span></span><span><span><span></span><span>#define AST_NODE(NAME) \
</span></span></span><span><span><span>  static inline bool is##NAME(AstTag tag) { \
</span></span></span><span><span><span>    return tag == NAME; \
</span></span></span><span><span><span>  }
</span></span></span><span><span><span>#define AST_BEGIN_SUBCLASSES(NAME)
</span></span></span><span><span><span>#define AST_END_SUBCLASSES(NAME)
</span></span></span><span><span><span></span><span>// Apply the above macros to uast-classes-list.h
</span></span></span><span><span><span></span><span>#include</span> <span>"chpl/uast/uast-classes-list.h"</span><span>
</span></span></span><span><span><span></span><span>// clear the macros
</span></span></span><span><span><span></span><span>#undef AST_NODE
</span></span></span><span><span><span>#undef AST_BEGIN_SUBCLASSES
</span></span></span><span><span><span>#undef AST_END_SUBCLASSES
</span></span></span><span><span><span></span>
</span></span><span><span><span>// define is___ for abstract parent classes
</span></span></span><span><span><span></span><span>#define AST_NODE(NAME)
</span></span></span><span><span><span>#define AST_BEGIN_SUBCLASSES(NAME) \
</span></span></span><span><span><span>  static inline bool is##NAME(AstTag tag) { \
</span></span></span><span><span><span>    return START_##NAME &lt; tag &amp;&amp; tag &lt; END_##NAME; \
</span></span></span><span><span><span>  }
</span></span></span><span><span><span>#define AST_END_SUBCLASSES(NAME)
</span></span></span><span><span><span></span><span>// Apply the above macros to uast-classes-list.h
</span></span></span><span><span><span></span><span>#include</span> <span>"chpl/uast/uast-classes-list.h"</span><span>
</span></span></span><span><span><span></span><span>// clear the macros
</span></span></span><span><span><span></span><span>#undef AST_NODE
</span></span></span><span><span><span>#undef AST_BEGIN_SUBCLASSES
</span></span></span><span><span><span>#undef AST_END_SUBCLASSES</span></span></span></code></pre></div>


<p>These helpers are quite convenient. Here are a few examples of what we end up
with:</p>
<div><pre tabindex="0"><code data-lang="C++"><span><span><span>isFor</span><span>(</span><span>AstTag</span><span>::</span><span>For</span><span>)</span>             <span>// Returns true; a 'for' loop is indeed a 'for' loop.
</span></span></span><span><span><span></span><span>isIndexableLoop</span><span>(</span><span>AstTag</span><span>::</span><span>For</span><span>)</span>   <span>// Returns true; a 'for' loop is "indexable" ('for i in ...')
</span></span></span><span><span><span></span><span>isLoop</span><span>(</span><span>AstTag</span><span>::</span><span>For</span><span>)</span>            <span>// Returns true; a 'for' loop is a loop.
</span></span></span><span><span><span></span><span>isFor</span><span>(</span><span>AstTag</span><span>::</span><span>While</span><span>)</span>           <span>// Returns false; a 'while' loop is not a 'for' loop.
</span></span></span><span><span><span></span><span>isIndexableLoop</span><span>(</span><span>AstTag</span><span>::</span><span>While</span><span>)</span> <span>// Returns false; a 'while' loop uses a boolean condition, not an index
</span></span></span><span><span><span></span><span>isLoop</span><span>(</span><span>AstTag</span><span>::</span><span>While</span><span>)</span>          <span>// Returns true; a 'while' loop is a loop.
</span></span></span></code></pre></div>
<p>On the top-level AST node class, we generate <code>isWhateverNode</code> and
<code>toWhateverNode</code> for each AST subclass. Thus, user code is able to inspect the
AST and perform (checked) casts using plain methods. I omit <code>isWhateverNode</code>
here for brevity (its definition is very simple), and include only
<code>toWhateverNode</code>.</p>





<div data-base-path="%!s(<nil>)" data-file-path="frontend/include/chpl/uast/AstNode.h"><pre tabindex="0"><code data-lang="C++"><span><span>  <span>#define AST_TO(NAME) \
</span></span></span><span><span><span>    const NAME * to##NAME() const { \
</span></span></span><span><span><span>      return this-&gt;is##NAME() ? (const NAME *)this : nullptr; \
</span></span></span><span><span><span>    } \
</span></span></span><span><span><span>    NAME * to##NAME() { \
</span></span></span><span><span><span>      return this-&gt;is##NAME() ? (NAME *)this : nullptr; \
</span></span></span><span><span><span>    }
</span></span></span><span><span><span></span>  <span>#define AST_NODE(NAME) AST_TO(NAME)
</span></span></span><span><span><span></span>  <span>#define AST_LEAF(NAME) AST_TO(NAME)
</span></span></span><span><span><span></span>  <span>#define AST_BEGIN_SUBCLASSES(NAME) AST_TO(NAME)
</span></span></span><span><span><span></span>  <span>#define AST_END_SUBCLASSES(NAME)
</span></span></span><span><span><span></span>  <span>// Apply the above macros to uast-classes-list.h
</span></span></span><span><span><span></span>  <span>#include</span> <span>"chpl/uast/uast-classes-list.h"</span><span>
</span></span></span><span><span><span></span>  <span>// clear the macros
</span></span></span><span><span><span></span>  <span>#undef AST_NODE
</span></span></span><span><span><span></span>  <span>#undef AST_LEAF
</span></span></span><span><span><span></span>  <span>#undef AST_BEGIN_SUBCLASSES
</span></span></span><span><span><span></span>  <span>#undef AST_END_SUBCLASSES
</span></span></span><span><span><span></span>  <span>#undef AST_TO</span></span></span></code></pre></div>


<p>These methods are used heavily in the compiler. For example, here’s a completely
random snippet of code I pulled out:</p>





<div data-base-path="%!s(<nil>)" data-file-path="frontend/lib/resolution/Resolver.cpp"><pre tabindex="0"><code data-lang="C++"><span><span>  <span>if</span> <span>(</span><span>auto</span> <span>var</span> <span>=</span> <span>decl</span><span>-&gt;</span><span>toVarLikeDecl</span><span>())</span> <span>{</span>
</span></span><span><span>    <span>// Figure out variable type based upon:
</span></span></span><span><span><span></span>    <span>//  * the type in the variable declaration
</span></span></span><span><span><span></span>    <span>//  * the initialization expression in the variable declaration
</span></span></span><span><span><span></span>    <span>//  * the initialization expression from split-init
</span></span></span><span><span><span></span>
</span></span><span><span>    <span>auto</span> <span>typeExpr</span> <span>=</span> <span>var</span><span>-&gt;</span><span>typeExpression</span><span>();</span>
</span></span><span><span>    <span>auto</span> <span>initExpr</span> <span>=</span> <span>var</span><span>-&gt;</span><span>initExpression</span><span>();</span>
</span></span><span><span>
</span></span><span><span>    <span>if</span> <span>(</span><span>auto</span> <span>var</span> <span>=</span> <span>decl</span><span>-&gt;</span><span>toVariable</span><span>())</span>
</span></span><span><span>      <span>if</span> <span>(</span><span>var</span><span>-&gt;</span><span>isField</span><span>())</span>
</span></span><span><span>        <span>isField</span> <span>=</span> <span>true</span><span>;</span></span></span></code></pre></div>


<p>Thus, developers adding new AST nodes are not required to manually implement
the <code>isWhatever</code>, <code>toWhatever</code>, and other functions. This and a fair bit
of other AST functionality (which I will cover in the next subsection) is
automatically generated using X Macros.</p>
<p>
<span>
    <span><svg style="display: none;">
    <use xlink:href="/feather-sprite.svg#moon"></use>
</svg></span>
    <span>
You haven't actually shown how the AST node classes are declared, only the
tags. It seems implausible that they be generated using this same strategy -
doesn't each AST node have its own different methods and implementation code?
</span>
</span>

<span>
    <span><svg style="display: none;">
    <use xlink:href="/feather-sprite.svg#sun"></use>
</svg></span>
    <span>
You're right. The AST node classes are defined "as usual", and their constructors
must explicitly set their <code>tag</code> field to the corresponding
<code>AstTag</code> value. It's also on the person defining the new class to
extend the node that they promise to extend in <code>uast-classes-list.h</code>.
</span>
</span>

<span>
    <span><svg style="display: none;">
    <use xlink:href="/feather-sprite.svg#moon"></use>
</svg></span>
    <span>
This seems like an opportunity for bugs. Nothing is stopping a developer
from returning the wrong tag, which would break the auto-casting behavior.
</span>
</span>

<span>
    <span><svg style="display: none;">
    <use xlink:href="/feather-sprite.svg#sun"></use>
</svg></span>
    <span>
Yes, it's not bulletproof. Just recently, a team meber found
<a href="https://github.com/chapel-lang/chapel/pull/23508"> a bug</a> in which
a node was listed to inherit from <code>AstNode</code>, but actually inherited
from <code>NamedDecl</code>. The <code>toNamedDecl</code> method would not
have worked on it, even though it inherited from the class.<p>

Still, this pattern provides the Chapel compiler with a lot of value; I will
show more use cases in the next subsection, like promised.
</p></span>
</span>

</p>

<a href="#the-visitor-pattern-without-double-dispatch">
  <h4 id="the-visitor-pattern-without-double-dispatch">The Visitor Pattern without Double Dispatch</h4>
</a>
<p>The Visitor Pattern is very important in general, but it’s beyond ubiquitous
for us compiler developers. It helps avoid bloating AST node classes with methods
and state required for the various operations we perform on them. It also often
saves us from writing AST traversal code.</p>
<p>Essentially, rather than adding each new operation (e.g. convert to string,
compute the type, assign IDs) as methods on each AST node class, we extract
this code into a per-operation <em>visitor</em>. This visitor is a class that has methods
implementing the custom behavior on the AST nodes. A <code>visit(WhileLoop*)</code> method
might be used to perform the operation on ‘while’ loops, and <code>visit(ForLoop*)</code> might
do the same for ‘for’ loops. The AST nodes themselves only have a <code>traverse</code>
method that accepts a visitor, whatever it may be, and calls the appropriate
visit methods. This way, the AST node implementations remain simple and relatively
stable.</p>
<p>As a very simple example, suppose you wanted to count the number of loops used
in a program for an unspecified reason. You could add a <code>countLoops</code> method,
but then you’ve introduced a method to the AST node API for what might be a
one-time, throwaway operation. With the visitor pattern, you don’t need to do
that; you can just create a new class:</p>
<div><pre tabindex="0"><code data-lang="C++"><span><span><span>struct</span> <span>MyVisitor</span> <span>{</span>
</span></span><span><span>    <span>int</span> <span>count</span> <span>=</span> <span>0</span><span>;</span>
</span></span><span><span>
</span></span><span><span>    <span>void</span> <span>visit</span><span>(</span><span>const</span> <span>Loop</span><span>*</span><span>)</span> <span>{</span> <span>count</span> <span>+=</span> <span>1</span><span>;</span> <span>}</span>
</span></span><span><span>    <span>void</span> <span>visit</span><span>(</span><span>const</span> <span>AstNode</span><span>*</span><span>)</span> <span>{</span> <span>/* do nothing for other nodes */</span> <span>}</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span>int</span> <span>countLoops</span><span>(</span><span>const</span> <span>AstNode</span><span>*</span> <span>root</span><span>)</span> <span>{</span>
</span></span><span><span>    <span>MyVisitor</span> <span>visitor</span><span>;</span>
</span></span><span><span>    <span>root</span><span>-&gt;</span><span>traverse</span><span>(</span><span>visitor</span><span>);</span>
</span></span><span><span>    <span>return</span> <span>visitor</span><span>.</span><span>count</span><span>;</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div>
<p>The <code>traverse</code> method is a nice API, isn’t it? It’s very easy to add operations
that work on your syntax trees, without modifying them. There is still an important
open question, though: how does <code>traverse</code> know to call the right <code>visit</code> function?</p>
<p>If <code>traverse</code> were only defined on <code>AstNode*</code>, and it simply called <code>visit(this)</code>,
we’d always end up calling the <code>AstNode</code> version of the <code>visit</code> function. This
is because C++ doesn’t dynamic dispatch
<span>
<label for="vtable-note">based on the types of method arguments.</label>
<span><span>[note:</span>
Obviously, C++ has the ability to pick the right method based on the runtime
type of the <em>receiver</em>: that's just <code>virtual</code> functions
and <code>vtable</code>s.
<span>]</span>
</span>
</span>

Statically, the call clearly accepts an <code>AstNode</code>, and nothing more specific.
The compiler therefore picks that version of the <code>visit</code> method.</p>
<p>The “traditional” way to solve this problem in a language like C++ or Java
is called <em>double dispatch</em>. Using our example as reference, this involves
making <em>each</em> AST node class have its own <code>traverse</code> method. This way,
calls to <code>visit(this)</code> have more specific type information, and are resolved
to the appropriate overload. But that’s more boilerplate code: each new AST
node will need to have a virtual traverse method that looks something like this:</p>
<div><pre tabindex="0"><code data-lang="C++"><span><span><span>void</span> <span>MyNode</span><span>::</span><span>traverse</span><span>(</span><span>Visitor</span><span>&amp;</span> <span>v</span><span>)</span> <span>{</span>
</span></span><span><span>  <span>v</span><span>.</span><span>visit</span><span>(</span><span>this</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div>
<p>It would also require all visitors to extend from <code>Visitor</code>. So now you have:</p>
<ul>
<li>Boilerplate code on every AST node that looks the same but needs to be duplicated</li>
<li>A parent <code>Visitor</code> class that must have a <code>visit</code> method for each AST node in
the language (so that children can override it).</li>
<li>To make it easier to write code like our <code>MyVisitor</code> above, the <code>visit</code>
methods in the <code>Visitor</code> must be written such that <code>visit(ChildNode*)</code> calls
<code>visit(ParentNode*)</code> by default. Otherwise, the <code>Loop</code> overload wouldn’t
have been called by the <code>DoWhile</code> overload (e.g.).</li>
</ul>
<p>So there’s a fair bit of tedious boilerplate, and more code to manually modify
when adding an AST node: you have to go and adjust the <code>Visitor</code> class with
new <code>visit</code> stub.</p>
<p>The reason all of this is necessary is that everyone (myself included) generally
agrees that code like the following is generally a bad idea:</p>
<div><pre tabindex="0"><code data-lang="C++"><span><span><span>struct</span> <span>AstNode</span> <span>{</span>
</span></span><span><span>  <span>void</span> <span>traverse</span><span>(</span><span>Visitor</span><span>&amp;</span> <span>visitor</span><span>)</span> <span>{</span>
</span></span><span><span>    <span>if</span> <span>(</span><span>auto</span> <span>forLoop</span> <span>=</span> <span>toForLoop</span><span>())</span> <span>{</span>
</span></span><span><span>      <span>visitor</span><span>.</span><span>visit</span><span>(</span><span>forLoop</span><span>);</span>
</span></span><span><span>    <span>}</span> <span>else</span> <span>if</span> <span>(</span><span>auto</span> <span>whileLoop</span> <span>=</span> <span>toWhileLoop</span><span>())</span> <span>{</span>
</span></span><span><span>      <span>visitor</span><span>.</span><span>visit</span><span>(</span><span>whileLoop</span><span>);</span>
</span></span><span><span>    <span>}</span> <span>else</span> <span>{</span>
</span></span><span><span>      <span>// 100 more lines like this...
</span></span></span><span><span><span></span>    <span>}</span>
</span></span><span><span>  <span>}</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div>
<p>After all, what happens when you add a new AST node? You’d still have to modify
this list, and since everything still extends <code>Visitor</code>, you’d still need to
add a new <code>visit</code> stub there. But what if there were no base class? Instead,
what if <code>traverse</code> were a template?</p>
<div><pre tabindex="0"><code data-lang="C++"><span><span><span>struct</span> <span>AstNode</span> <span>{</span>
</span></span><span><span>  <span>template</span> <span>&lt;</span><span>typename</span> <span>VisitorType</span><span>&gt;</span>
</span></span><span><span>  <span>void</span> <span>traverse</span><span>(</span><span>VisitorType</span><span>&amp;</span> <span>visitor</span><span>)</span> <span>{</span>
</span></span><span><span>    <span>if</span> <span>(</span><span>auto</span> <span>forLoop</span> <span>=</span> <span>toForLoop</span><span>())</span> <span>{</span>
</span></span><span><span>      <span>visitor</span><span>.</span><span>visit</span><span>(</span><span>forLoop</span><span>);</span>
</span></span><span><span>    <span>}</span> <span>else</span> <span>if</span> <span>(</span><span>auto</span> <span>whileLoop</span> <span>=</span> <span>toWhileLoop</span><span>())</span> <span>{</span>
</span></span><span><span>      <span>visitor</span><span>.</span><span>visit</span><span>(</span><span>whileLoop</span><span>);</span>
</span></span><span><span>    <span>}</span> <span>else</span> <span>{</span>
</span></span><span><span>      <span>// 100 more lines like this...
</span></span></span><span><span><span></span>    <span>}</span>
</span></span><span><span>  <span>}</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div>
<p>Note that this wouldn’t be possible to write in C++ if <code>visit</code> were a virtual
method; have you ever heard of a virtual template? With code like this, the
<code>VisitorType</code> wouldn’t need to define <em>every</em> overload, as long as it had
a version for <code>AstNode</code>. Furthermore, C++’s regular overload resolution rules
would take care of calling the <code>Loop</code> overload if a more specific one for
<code>DoWhile</code> didn’t exist.</p>
<p>The only problem that remains is that of having a 100-line if-else (which could
be a <code>switch</code> to little aesthetic benefit). But this is exactly where the
X Macro pattern shines again! We already have a list of all AST node classes,
and the code for invoking them is nearly identical. Thus, the Chapel compiler
has a <code>doDispatch</code> function (used by <code>traverse</code>) that looks like this:</p>





<div data-base-path="%!s(<nil>)" data-file-path="frontend/include/chpl/uast/AstNode.h"><pre tabindex="0"><code data-lang="C++"><span><span>    <span>static</span> <span>void</span> <span>doDispatch</span><span>(</span><span>const</span> <span>AstNode</span><span>*</span> <span>ast</span><span>,</span> <span>Visitor</span><span>&amp;</span> <span>v</span><span>)</span> <span>{</span>
</span></span><span><span>
</span></span><span><span>      <span>switch</span> <span>(</span><span>ast</span><span>-&gt;</span><span>tag</span><span>())</span> <span>{</span>
</span></span><span><span>        <span>#define CONVERT(NAME) \
</span></span></span><span><span><span>          case chpl::uast::asttags::NAME: \
</span></span></span><span><span><span>          { \
</span></span></span><span><span><span>            v.visit((const chpl::uast::NAME*) ast); \
</span></span></span><span><span><span>            return; \
</span></span></span><span><span><span>          }
</span></span></span><span><span><span></span>
</span></span><span><span>        <span>#define IGNORE(NAME) \
</span></span></span><span><span><span>          case chpl::uast::asttags::NAME: \
</span></span></span><span><span><span>          { \
</span></span></span><span><span><span>            CHPL_ASSERT(false &amp;&amp; "this code should never be run"); \
</span></span></span><span><span><span>          }
</span></span></span><span><span><span></span>
</span></span><span><span>        <span>#define AST_NODE(NAME) CONVERT(NAME)
</span></span></span><span><span><span></span>        <span>#define AST_BEGIN_SUBCLASSES(NAME) IGNORE(START_##NAME)
</span></span></span><span><span><span></span>        <span>#define AST_END_SUBCLASSES(NAME) IGNORE(END_##NAME)
</span></span></span><span><span><span></span>
</span></span><span><span>        <span>#include</span> <span>"chpl/uast/uast-classes-list.h"</span><span>
</span></span></span><span><span><span></span>
</span></span><span><span>        <span>IGNORE</span><span>(</span><span>NUM_AST_TAGS</span><span>)</span>
</span></span><span><span>        <span>IGNORE</span><span>(</span><span>AST_TAG_UNKNOWN</span><span>)</span>
</span></span><span><span>
</span></span><span><span>        <span>#undef AST_NODE
</span></span></span><span><span><span></span>        <span>#undef AST_BEGIN_SUBCLASSES
</span></span></span><span><span><span></span>        <span>#undef AST_END_SUBCLASSES
</span></span></span><span><span><span></span>        <span>#undef CONVERT
</span></span></span><span><span><span></span>        <span>#undef IGNORE
</span></span></span><span><span><span></span>      <span>}</span>
</span></span><span><span>
</span></span><span><span>      <span>CHPL_ASSERT</span><span>(</span><span>false</span> <span>&amp;&amp;</span> <span>"this code should never be run"</span><span>);</span>
</span></span><span><span>    <span>}</span></span></span></code></pre></div>


<p>And that’s it. We have automatically generated the traversal code, allowing
us to use the visitor pattern in what I think is a very elegant way. Assuming
a developer adding a new AST node updates the <code>uast-classes-list.h</code> header,
the traversal logic will be auto-modified to properly handle the new node.</p>
<a href="#generating-a-python-class-hierarchy">
  <h4 id="generating-a-python-class-hierarchy">Generating a Python Class Hierarchy</h4>
</a>
<p>This is a fun one. For a while, in my spare time, I was working on
<a href="https://github.com/chapel-lang/chapel/tree/main/tools/chapel-py">Python bindings for Chapel<svg style="display: none;">
    <use xlink:href="/feather-sprite.svg#external-link"></use>
</svg></a>.
These bindings are oriented towards developing language tooling: it feels much
easier to write a language linter, auto-formatter, or maybe even a language
server in Python rather than in C++. It’s definitely much easier to use Python to
develop throwaway scripts that work with Chapel programs, which is something
that developers on the Chapel team tend to do quite often.</p>
<p>I decided I wanted the Python AST node class hierarchy to match the C++ version.
This is convenient for many reasons, including being able to wrap methods on
parent AST nodes and have them be available through child AST nodes and having
<code>isinstance</code> work properly. It’s also advantageous from the point of view
of conceptual simplicity. However, I very much did not want to write CPython
API code to define the many AST node classes that are available in the Chapel
language.</p>
<p>Once again, the <code>uast-classes-list.h</code> header came into play here. With little
effort, I was able to auto-generate <code>PyTypeObject</code>s for each AST node in the
class hierarchy:</p>





<div data-base-path="%!s(<nil>)" data-file-path="tools/chapel-py/chapel.cpp"><pre tabindex="0"><code data-lang="C++"><span><span><span>#define DEFINE_PY_TYPE_FOR(NAME, TAG, FLAGS)\
</span></span></span><span><span><span>  PyTypeObject NAME##Type = { \
</span></span></span><span><span><span>    PyVarObject_HEAD_INIT(NULL, 0) \
</span></span></span><span><span><span>    .tp_name = #NAME, \
</span></span></span><span><span><span>    .tp_basicsize = sizeof(NAME##Object), \
</span></span></span><span><span><span>    .tp_itemsize = 0, \
</span></span></span><span><span><span>    .tp_flags = FLAGS, \
</span></span></span><span><span><span>    .tp_doc = PyDoc_STR("A Chapel " #NAME " AST node"), \
</span></span></span><span><span><span>    .tp_methods = (PyMethodDef*) PerNodeInfo&lt;TAG&gt;::methods, \
</span></span></span><span><span><span>    .tp_base = parentTypeFor(TAG), \
</span></span></span><span><span><span>    .tp_init = (initproc) NAME##Object_init, \
</span></span></span><span><span><span>    .tp_new = PyType_GenericNew, \
</span></span></span><span><span><span>  };
</span></span></span><span><span><span></span>
</span></span><span><span><span>#define AST_NODE(NAME) DEFINE_PY_TYPE_FOR(NAME, chpl::uast::asttags::NAME, Py_TPFLAGS_DEFAULT)
</span></span></span><span><span><span>#define AST_BEGIN_SUBCLASSES(NAME) DEFINE_PY_TYPE_FOR(NAME, chpl::uast::asttags::START_##NAME, Py_TPFLAGS_BASETYPE)
</span></span></span><span><span><span>#define AST_END_SUBCLASSES(NAME)
</span></span></span><span><span><span>#include</span> <span>"chpl/uast/uast-classes-list.h"</span><span>
</span></span></span><span><span><span>#undef AST_NODE
</span></span></span><span><span><span>#undef AST_BEGIN_SUBCLASSES
</span></span></span><span><span><span>#undef AST_END_SUBCLASSES</span></span></span></code></pre></div>


<p>You may have noticed that I snuck templates into the code above. The motivation there
is to avoid writing out the (usually empty) Python method table for every single
AST node. In particular, I have a template that, by default, provides an empty
method table, which can be specialized per node to add methods when necessary.
This detail is useful for application 3 below, but not necessary to understand
the use of X Macros here.</p>
<p>I used the same <code>&lt;</code> and <code>&gt;</code> trick to generate the <code>parentTypeFor</code> each tag:</p>





<div data-base-path="%!s(<nil>)" data-file-path="tools/chapel-py/chapel.cpp"><pre tabindex="0"><code data-lang="C++"><span><span><span>static</span> <span>PyTypeObject</span><span>*</span> <span>parentTypeFor</span><span>(</span><span>chpl</span><span>::</span><span>uast</span><span>::</span><span>asttags</span><span>::</span><span>AstTag</span> <span>tag</span><span>)</span> <span>{</span>
</span></span><span><span><span>#define AST_NODE(NAME)
</span></span></span><span><span><span>#define AST_LEAF(NAME)
</span></span></span><span><span><span>#define AST_BEGIN_SUBCLASSES(NAME)
</span></span></span><span><span><span>#define AST_END_SUBCLASSES(NAME) \
</span></span></span><span><span><span>  if (tag &gt; chpl::uast::asttags::START_##NAME &amp;&amp; tag &lt; chpl::uast::asttags::END_##NAME) { \
</span></span></span><span><span><span>    return &amp;NAME##Type; \
</span></span></span><span><span><span>  }
</span></span></span><span><span><span>#include</span> <span>"chpl/uast/uast-classes-list.h"</span><span>
</span></span></span><span><span><span>#include</span> <span>"chpl/uast/uast-classes-list.h"</span><span>
</span></span></span><span><span><span>#undef AST_NODE
</span></span></span><span><span><span>#undef AST_LEAF
</span></span></span><span><span><span>#undef AST_BEGIN_SUBCLASSES
</span></span></span><span><span><span>#undef AST_END_SUBCLASSES
</span></span></span><span><span><span></span>  <span>return</span> <span>&amp;</span><span>AstNodeType</span><span>;</span>
</span></span><span><span><span>}</span></span></span></code></pre></div>


<p>A few more invocations of the <code>uast-classes-list.h</code> macro, and I had a working
class hierarchy. I didn’t explicitly mention any AST node at all; all was derived
from the Chapel compiler header. This also meant that as the language changed
and the AST class hierarchy developed, the Python bindings’ code would not need
to be updated. As long as it was compiled with an up-to-date version of the
header, the hierarchy would match that present within the language.</p>
<p>This allows for code like the following to be written in Python:</p>
<div><pre tabindex="0"><code data-lang="Python"><span><span><span>def</span> <span>print_decls</span><span>(</span><span>mod</span><span>):</span>
</span></span><span><span>    <span>"""
</span></span></span><span><span><span>    Print all the things declared in this Chapel module.
</span></span></span><span><span><span>    """</span>
</span></span><span><span>    <span>for</span> <span>child</span> <span>in</span> <span>mod</span><span>:</span>
</span></span><span><span>        <span>if</span> <span>isinstance</span><span>(</span><span>child</span><span>,</span> <span>NamedDecl</span><span>):</span>
</span></span><span><span>            <span>print</span><span>(</span><span>child</span><span>.</span><span>name</span><span>())</span>
</span></span></code></pre></div>
<a href="#application-3-cpython-method-tables-and-getters">
  <h3 id="application-3-cpython-method-tables-and-getters">Application 3: CPython Method Tables and Getters</h3>
</a>
<p>The Chapel Python bindings use the X Macro pattern another time, actually.
Like I mentioned earlier, I use <a href="https://en.cppreference.com/w/cpp/language/template_specialization">template specialization<svg style="display: none;">
    <use xlink:href="/feather-sprite.svg#external-link"></use>
</svg></a>
to reduce the amount of boilerplate code required for declaring Python objects.
In particular, there’s a general method table declared as follows:</p>





<div data-base-path="%!s(<nil>)" data-file-path="tools/chapel-py/chapel.cpp"><pre tabindex="0"><code data-lang="C++"><span><span><span>template</span> <span>&lt;</span><span>chpl</span><span>::</span><span>uast</span><span>::</span><span>asttags</span><span>::</span><span>AstTag</span> <span>tag</span><span>&gt;</span>
</span></span><span><span><span>struct</span> <span>PerNodeInfo</span> <span>{</span>
</span></span><span><span>  <span>static</span> <span>constexpr</span> <span>PyMethodDef</span> <span>methods</span><span>[]</span> <span>=</span> <span>{</span>
</span></span><span><span>    <span>{</span><span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>0</span><span>,</span> <span>NULL</span><span>}</span>  <span>/* Sentinel */</span>
</span></span><span><span>  <span>};</span>
</span></span><span><span><span>};</span></span></span></code></pre></div>


<p>Then, when I need to add methods, I use template specialization by writing
something like the following:</p>
<div><pre tabindex="0"><code data-lang="C++"><span><span><span>template</span> <span>&lt;&gt;</span>
</span></span><span><span><span>struct</span> <span>PerNodeInfo</span><span>&lt;</span><span>TheAstTag</span><span>&gt;</span> <span>{</span>
</span></span><span><span>  <span>static</span> <span>constexpr</span> <span>PyMethodDef</span> <span>methods</span><span>[]</span> <span>=</span> <span>{</span>
</span></span><span><span>    <span>{</span><span>"method_name"</span><span>,</span> <span>TheNode_method_name</span><span>,</span> <span>METH_NOARGS</span><span>,</span> <span>"Documentation string"</span><span>},</span>
</span></span><span><span>    <span>// ... more like the above ...
</span></span></span><span><span><span></span>    <span>{</span><span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>0</span><span>,</span> <span>NULL</span><span>}</span>  <span>/* Sentinel */</span>
</span></span><span><span>  <span>};</span>
</span></span><span><span><span>};</span>
</span></span></code></pre></div>
<p>When reviewing a PR that adds more methods to the Python bindings (by
defining new <code>TheNode_methodname</code> functions and then including them in the
method table), I noticed that in the PR, the developer added some methods
but forgot to put them into the respective table, leaving them unusable by
the Python client code. This came with the additional observation that there
was a moderate amount of duplication when declaring the C++ functions and then
listing them in the table. The name (<code>method_name</code> in the code) occurred many
times.</p>
<p>The developer who opened the PR suggesting using X Macros to combine the
information (declaration of function and its use in the corresponding method table)
into a single list. This led to the following header file:</p>





<div data-base-path="%!s(<nil>)" data-file-path="tools/chapel-py/method-tables.h"><pre tabindex="0"><code data-lang="C++"><span><span><span>CLASS_BEGIN</span><span>(</span><span>FnCall</span><span>)</span>
</span></span><span><span>  <span>METHOD_PROTOTYPE</span><span>(</span><span>FnCall</span><span>,</span> <span>actuals</span><span>,</span> <span>"Get the actuals of this FnCall node"</span><span>)</span>
</span></span><span><span>  <span>PLAIN_GETTER</span><span>(</span><span>FnCall</span><span>,</span> <span>used_square_brackets</span><span>,</span> <span>"Check if this FnCall was made using square brackets"</span><span>,</span>
</span></span><span><span>               <span>"b"</span><span>,</span> <span>return</span> <span>node</span><span>-&gt;</span><span>callUsedSquareBrackets</span><span>())</span>
</span></span><span><span><span>CLASS_END</span><span>(</span><span>FnCall</span><span>)</span></span></span></code></pre></div>


<p>The <code>PLAIN_GETTER</code> macro in this case is used to define trivial getters
(precluding the need for handling the Python-object-to-AST-node conversion,
and other CPython-specific things), whereas the <code>METHOD_PROTOTYPE</code> is used
to refer to methods that needed explicit implementations. With
this, the method tables are generated as follows:</p>





<div data-base-path="%!s(<nil>)" data-file-path="tools/chapel-py/chapel.cpp"><pre tabindex="0"><code data-lang="C++"><span><span><span>#define CLASS_BEGIN(TAG) \
</span></span></span><span><span><span>  template &lt;&gt; \
</span></span></span><span><span><span>  struct PerNodeInfo&lt;chpl::uast::asttags::TAG&gt; { \
</span></span></span><span><span><span>    static constexpr PyMethodDef methods[] = {
</span></span></span><span><span><span>#define CLASS_END(TAG) \
</span></span></span><span><span><span>      {NULL, NULL, 0, NULL}  </span><span>/* Sentinel */</span><span> \
</span></span></span><span><span><span>    }; \
</span></span></span><span><span><span>  };
</span></span></span><span><span><span>#define PLAIN_GETTER(NODE, NAME, DOCSTR, TYPESTR, BODY) \
</span></span></span><span><span><span>  {#NAME, NODE##Object_##NAME, METH_NOARGS, DOCSTR},
</span></span></span><span><span><span>#define METHOD_PROTOTYPE(NODE, NAME, DOCSTR) \
</span></span></span><span><span><span>  {#NAME, NODE##Object_##NAME, METH_NOARGS, DOCSTR},
</span></span></span><span><span><span>#include</span> <span>"method-tables.h"</span></span></span></code></pre></div>


<p>The <code>CLASS_BEGIN</code> generates the initial <code>template &lt;&gt;</code> header and the code up
to the opening curly brace of the table definition. Then, for each method,
<code>PLAIN_GETTER</code> and <code>METHOD_PROTOTYPE</code> generate the relevant entries. Finally,
<code>CLASS_END</code> inserts the sentinel and the closing curly brace.</p>
<p>Another invocation of the macros in <code>method-tables.h</code> is used to generate the
implementations of “plain getters”, which is boilerplate that I won’t get into
it here, since it’s pretty CPython specific.</p>
<a href="#discussion">
  <h3 id="discussion">Discussion</h3>
</a>
<p>I’ve presented to you a three applications of the pattern, in an order that happens
to be from least to most “extreme”. It’s possible that some of these are
over the line for using macros, especially for those who think of macros as
unfortunate remnants of C++’s past. However, I think that what I’ve demonstrated
demonstrates the versatility of the X Macro pattern – feel free to apply it to
the degree that you find appropriate.</p>
<p>The thing I like the most about this pattern is that the header files read quite nicely:
you end up with a very declarative “scaffold” of what’s going on. The
<code>uast-classes-list.h</code> makes for an excellent and fairly readable reference of
all the AST nodes in the Chapel compiler. The <code>method-tables.h</code> header provides
a fairly concise summary of what methods are available on what (Python) AST
node.</p>
<p>Of course, this approach is not without its drawbacks. Drawback zero is
the heavy use of macros: to the best of my knowledge, modern C++ tends to
discourage the usage of macros in favor of C++-specific features. Of course,
this “pure C++” preference is applicable to variable degrees in different use
cases and code bases; because of this, I won’t count macros as (too much of)
a drawback.</p>
<p>The more significant downside is that this approach introduces a lot of dependencies
between source files. Any time the header changes, anything that uses any part
of the code generated by the header must be recompiled. Thus, if you’re generating
classes, changing any one class will “taint” any code that uses <em>any</em> of the
generated classes. In the Chapel compiler, touching the AST class hierarchy
requires a recompilation of all the AST nodes, and any compiler code that uses
the AST nodes (a lot). This is because each AST node needs access to the
<code>AstTag</code> enum, and that enum is generated from the hierarchy header.</p>
<p>That’s all I have for today! Thanks for reading. I hope you got something useful
for your day-to-day programming out of this.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beej's Guide to C Programming [pdf] (104 pts)]]></title>
            <link>https://beej.us/guide/bgc/pdf/bgc_a4_c_1.pdf</link>
            <guid>43471393</guid>
            <pubDate>Tue, 25 Mar 2025 13:58:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://beej.us/guide/bgc/pdf/bgc_a4_c_1.pdf">https://beej.us/guide/bgc/pdf/bgc_a4_c_1.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=43471393">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[If you get the chance, always run more extra network fiber cabling (153 pts)]]></title>
            <link>https://utcc.utoronto.ca/~cks/space/blog/sysadmin/RunMoreExtraNetworkFiber</link>
            <guid>43471177</guid>
            <pubDate>Tue, 25 Mar 2025 13:40:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://utcc.utoronto.ca/~cks/space/blog/sysadmin/RunMoreExtraNetworkFiber">https://utcc.utoronto.ca/~cks/space/blog/sysadmin/RunMoreExtraNetworkFiber</a>, See on <a href="https://news.ycombinator.com/item?id=43471177">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>If you get the chance, always run more extra network fiber cabling</h2>

	<p><small>March  3, 2025</small></p>
</div><div><p>Some day, you may be in an organization that's about to add some
more fiber cabling between two rooms in the same building, or maybe
two close by buildings, and someone may ask you for your opinion
about many fiber pairs should be run. My personal advice is simple:
<strong>run more fiber</strong> than you think you need, ideally a bunch more
(this generalizes to network cabling in general, but copper cabling
is a lot more bulky and so harder to run (much) more of). THere is
an unreasonable amount of fiber to run, but mostly it comes up when
you'd have to put in giant fiber patch panels.</p>

<p>The obvious reason to run more fiber is that you may well expand
your need for fiber in the future. Someone will want to run a
dedicated, private network connection between two locations; someone
will want to trunk things to get more bandwidth; someone will want
to run a weird protocol that requires its own network segment (did
you know you can run HDMI over Ethernet?); and so on. It's relatively
inexpensive to add some more fiber pairs when you're already running
fiber but much more expensive to have to run additional fiber
later, so you might as well give yourself room for growth.</p>

<p>The less obvious reason to run extra fiber is that every so often
fiber pairs stop working, <a href="https://utcc.utoronto.ca/~cks/space/blog/sysadmin/NetworkCablesGoBad">just like network cables go bad</a>, and when this happens you'll need to replace
them with spare fiber pairs, which means you need those spare fiber
pairs. Some of the time this fiber failure is (probably) because
<a href="https://utcc.utoronto.ca/~cks/space/blog/sysadmin/MachineRoomRaccoon">a raccoon got into your machine room</a>, but
some of the time it just happens for reasons that no one is likely
to ever explain to you. And when this happens, you don't necessarily
lose only a single pair. Today, for example, <a href="https://support.cs.toronto.edu/">we</a> lost three fiber pairs that ran
between two adjacent buildings and evidence suggests that other
people at the university lost at least one more pair.</p>

<p>(There are a variety of possible causes for sudden loss of multiple
pairs, probably all running through a common path, which I will
leave to your imagination. These fiber runs are probably not important
enough to cause anyone to do a detailed investigation of where the
fault is and what happened.)</p>

<p>Fiber comes in two varieties, <a href="https://en.wikipedia.org/wiki/Single-mode_optical_fiber">single mode</a> and
<a href="https://en.wikipedia.org/wiki/Multi-mode_optical_fiber">multi-mode</a>.
I don't know enough to know if you should make a point of running
both (over distances where either can be used) as part of the whole
'run more fiber' thing. Locally we have both SM and MM fiber and
have switched back and forth between them at times (and may have
to do so as a result of the current failures).</p>

<p>PS: Possibly you work in an organization where broken inside-building
fiber runs are regularly fixed or replaced. That is not our local
experience; someone has to pay for fixing or replacing, and when
you have spare fiber pairs left it's easier to switch over to them
rather than try to come up with the money and so on.</p>

<p>(Repairing or replacing broken fiber pairs will reduce your long
term need for additional fiber, but obviously not the short term
need. If you lose N pairs of fiber, you need N spare pairs to get
back into operation.)</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Killed Innovation? (107 pts)]]></title>
            <link>https://www.shirleywu.studio/notebook/2025-02-innovation-killer</link>
            <guid>43470971</guid>
            <pubDate>Tue, 25 Mar 2025 13:26:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.shirleywu.studio/notebook/2025-02-innovation-killer">https://www.shirleywu.studio/notebook/2025-02-innovation-killer</a>, See on <a href="https://news.ycombinator.com/item?id=43470971">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <article>   <p><strong>Find the discussion on <a href="https://www.linkedin.com/posts/shirleyxywu_what-killed-innovation-the-past-decade-activity-7303459372431536130-Py-a" target="_new">Linkedin</a> or
        <a target="_new">Bluesky</a>.</strong></p> <p><em>Mar 05, 2025</em></p> <!-- HTML_TAG_START --><blockquote data-svelte-h="svelte-4curtb"><p><em>This is part of a trilogy on the state of our industry:</em></p> <ul><li><em>Part 1: <a href="https://www.shirleywu.studio/notebook/2025-02-client-comfort-zone" rel="nofollow" target="_new">Outside a client’s comfort zone</a></em></li> <li><strong>Part 2: <a href="https://www.shirleywu.studio/notebook/2025-02-innovation-killer" rel="nofollow" target="_new">What killed innovation? A brief history</a></strong></li> <li><em>Part 3: (Coming soon)</em></li></ul></blockquote> <p data-svelte-h="svelte-1yjvjma">I entered the data visualization field in 2012, when D3.js had just come out and interactive graphics were going through a digital Renaissance.&nbsp; By the time I was fully steeped in the field in 2016, it felt like a new, experimental project was coming out every week—each one pushing the boundaries of how we think about, visualize, and communicate data.</p> <p data-svelte-h="svelte-2aux8c">New York Times' <a href="https://archive.nytimes.com/www.nytimes.com/interactive/2012/11/02/us/politics/paths-to-the-white-house.html?_r=0" target="_new">512 Paths to the White House</a> from 2012.  When I think of the pre-scrollytelling era, it's always the first one I point to.</p>  <p data-svelte-h="svelte-xbaot9">But fast forward a decade, and it feels like I’m seeing the same polished but predictable formats over and over.</p> <p data-svelte-h="svelte-7yxsvc">At first, I thought I had gotten jaded (a decade in tech feels like forever and maybe I’m now just the old granny shaking my fist about the “good old days” 😂), but after talking to a few friends and mulling on it for a couple years, I now have some hypotheses on how we might have gotten here.</p> <p data-svelte-h="svelte-euu07g">To test these hypotheses, I asked a few very respected voices in our industry—all known for their beautiful, bespoke work and responsible for driving the field forward in one way or another.&nbsp; So I’d like to thank everyone who kindly contributed: Nadieh Bremer, Giorgia Lupi, Federica Fragapane, Caitlin Ralph, Matt Daniels, RJ Andrews, Alberto Cairo, Eric William Lin, and Moritz Stefaner. While some of them are quoted directly, all of them provided insightful and nuanced answers that helped to shape my own whirlwind of ideas about where we are today, how we got here, and where we’re headed.</p> <h3 data-svelte-h="svelte-uz40lm">A brief history (of a plateau)</h3> <p data-svelte-h="svelte-m8xtrw">Let me come out swinging: our industry’s creativity has plateaued.</p> <p data-svelte-h="svelte-146etf4">Ok, I don’t fully mean that (but it’s got a lot of potential for a click-bait title, no? 😂).&nbsp; Let me provide a bit more nuance: the craft of telling data-informed stories on the web has plateaued.</p> <p data-svelte-h="svelte-1po8s4u">As Alberto recounts, newsrooms started digitizing in the mid-to-late 1990s, and by the time I started in the early 2010s, the foundations were laid out for really interesting web-based experiments.&nbsp; It was the perfect storm: Javascript was maturing, browser performance was improving, and D3.js was releasing interesting new layouts and supporting chart-specific interactions every year.&nbsp; I was constantly coming across new chart forms, and new ways to add interactions and animations to layer information.&nbsp; Just look at these two New York Times pieces from 2012 and 2013:</p> <div data-svelte-h="svelte-pweh00"><video src="https://storage.googleapis.com/shirleywu-studio-thoughts-assets/notebook/2025-02-innovation-killer/01-nyt_budget-og.mp4" autoplay="true" muted="true" loop="true"></video> <p><img src="https://storage.googleapis.com/shirleywu-studio-thoughts-assets/notebook/2025-02-innovation-killer/01_nyt-oscars-og.jpg" alt="A network diagram visualizing the connections among Oscar nominees from 2013. Nominees, including actors, directors, and producers, are represented by circular profile images, with lines connecting those who have worked together on Oscar-nominated films. Different colors indicate roles such as actor, producer, or director. The visualization highlights how interconnected the film industry is, particularly focusing on major nominees from Lincoln, Argo, Zero Dark Thirty, and Silver Linings Playbook." autoplay="true" muted="true" loop="true"></p></div> <p data-svelte-h="svelte-1i0tt4r"><a href="https://archive.nytimes.com/www.nytimes.com/interactive/2012/02/13/us/politics/2013-budget-proposal-graphic.html" target="_new">Four Ways to Slice Obama’s 2013 Budget Proposal</a> (2012, left) and <a href="https://archive.nytimes.com/www.nytimes.com/interactive/2013/02/20/movies/among-the-oscar-contenders-a-host-of-connections.html" target="_new">Among the Oscar Contenders, a Host of Connections</a> (2013, right).&nbsp; From a technical perspective, I remember being so mind-blown about how the different budget items animated so smoothly between the different views, and so inspired by the use of force-directed graph to lay out the Oscar nominees (and that visual elegance!).</p> <p data-svelte-h="svelte-tp1rbj">It really seemed like no two data visualizations looked the same.</p> <p data-svelte-h="svelte-1kw7pyw">RJ agrees that web interactives “hit some kind of romantic peak” in the mid-to-late 2010s.&nbsp; Nadieh dreamily recalls the advent of D3 visuals like chord diagrams and circle packing. “I can’t tell you what other mind boggling new technique or technology since then has, I’ve felt, made an impact,” she laments. “And that makes me very sad.”</p> <p data-svelte-h="svelte-1jfv315">Moritz summarizes his recent observations with the precision and insight of an industry veteran who has put out inspiration after inspiration for decades:</p> <blockquote data-svelte-h="svelte-19yxlfz"><ul><li>The few viral, well-known data visualizations tend to be static images or movies or small, personalized apps rather than elaborate interactive web pieces.</li> <li>Visual styles are more homogenous,&nbsp; with many projects looking similar due to the prevalence of established formats, templated solutions and software defaults.</li> <li>On the commercial side, investments in bespoke data visualization from freelancers and independent studios have declined.</li></ul></blockquote>  <p data-svelte-h="svelte-6rhhpe">He declares: “it’s a polycrisis”.</p> <h3 data-svelte-h="svelte-1gpug59">But how did we get here (polycrisis)?</h3> <p data-svelte-h="svelte-1ueth50">Over the past decade, I’ve noticed two fundamental shifts that have changed how we design interactive graphics on the web.</p> <p data-svelte-h="svelte-e8d6xd">The first: scrollies.</p> <p data-svelte-h="svelte-1qyh11s">I still remember when I first saw Stephanie Yee and Tony Chu’s <a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/" rel="nofollow" target="_new">“A visual introduction to machine learning”</a> in 2016, I was blown away:</p>  <p data-svelte-h="svelte-18zcw3b">It felt so exciting that web performance had come so far, that browsers could continuously scroll <em>and</em> animate thousands of elements at the same time. Scrollytelling was so quickly adopted that there was even a big debate about whether scrollies or steppers were “better” (beautifully summarized <a href="https://medium.com/@zanarmstrong/why-choose-scrollytelling-steppers-155a59dd97fe" rel="nofollow" target="_new">in this blog post</a> by Zan Armstrong). But the scrolly won out as more and more readers consumed data visualizations through mobile screens, where scrolling was the native behavior.</p> <p data-svelte-h="svelte-1p9r1eh">Newsrooms brought scrollytelling to the masses, but the format almost worked <em>too</em> well.&nbsp; Many newsrooms began to treat it as the default storytelling method and, due to its success, had less reason to test other formats. Eric, who has direct newsroom experience, says that “data journalists found the forms that best fit their needs, and their audiences’. It’s not that there’s no more experimentation within graphics and data desks in newsrooms, but they’ve become more isolated and case-by-case rather than the general cultural norm. Repeatability and efficiency are now more important.”</p> <p data-svelte-h="svelte-3ufwfq">I used to look forward to the Information is Beautiful Awards every year for inspiration.&nbsp; But I distinctly remember the 2022 Awards, when I went through the shortlist and was inundated by scrollytelling pieces.&nbsp; It’s not that they weren’t <em>good</em>, but that after a while <em>I couldn’t distinguish one from the next</em>.</p> <p data-svelte-h="svelte-mcicbr">That was the year I realized I was experiencing scrollytelling fatigue™️.</p> <p>My best hypothesis for why this happened is that judges just don’t have the time to deep-dive into any one piece.&nbsp; What makes scrollytelling easy for us as readers also makes for easier judging: Just scroll and the information is spoonfed; there’s no need to tap, no need to explore, no need to think and draw our own conclusions. After all, when judges are going through dozens or even hundreds of entries, the ones that are easiest to grok leave the most lasting impression. (I’m guilty of this.)</p> <p data-svelte-h="svelte-1hekhuq">I noticed the second shift during the Covid-19 pandemic.&nbsp; We saw charts about cases, and hospitalizations, and deaths, and vaccination rates…they were everywhere we looked online.</p> <p data-svelte-h="svelte-vpqk2j">At the time, I thought that data literacy might improve on a massive scale. A public that recognized the importance of data, that recognized the need to interpret charts, could perhaps also recognize that some data stories are just too complex and nuanced to be aggregated in bar charts and line charts.&nbsp; Surely, they’d be curious about more complex visualizations.</p> <p data-svelte-h="svelte-1ge82db">I could dream.</p> <p data-svelte-h="svelte-16bfhzz">Instead, though there was indeed an increase in public awareness of charts, the majority of people—as I wrote in my <a href="http://shirleywu.studio/notebook/2025-02-client-comfort-zone" rel="nofollow" target="_new">previous blog post</a>—were only interested in those standard charts.&nbsp; I’ve found in recent years that my clients have started to shy away from anything too unconventional because they’re concerned about getting pushback from <em>their</em> stakeholders and readers.&nbsp; Giorgia puts it best: “Some information will always be best conveyed in a straightforward bar or line chart, particularly for audiences that don’t have time to engage deeply. And as data literacy expanded—especially post-pandemic—the demand for intuitive, quick-to-grasp visuals naturally increased.”</p> <h3 data-svelte-h="svelte-1twun3a">Why, though?</h3> <p data-svelte-h="svelte-uuthuk">In both of those shifts, there is a common refrain: the demand for easier, quicker, more bite-sized content.</p> <p data-svelte-h="svelte-1ysd4j5">It’s underpinned by a cultural shift towards mobile phones as our primary mode of content consumption.&nbsp; The shift, as RJ notes, has shrunk the canvas, trained audiences to engage in short bursts, and has generally “made the Internet a not-fun place to hang out.”&nbsp; Matt of <em>The Pudding</em> echoes this observation: Audiences want more video and images that can fit on Instagram, “which means it can’t carry too much detail on mobile phone screens.”</p> <p data-svelte-h="svelte-kyjxpv">Audiences are also just less easily impressed these days: “The idea of visualizing data for artistic ‘wow-ness’ is far less novel than it used to be,” Matt says.&nbsp; “The idea of visualizing big data is not as new anymore, and in many ways expected and benign.&nbsp; Data viz as art just doesn’t hit the same way that it used to.”</p> <p data-svelte-h="svelte-dng7fz">Matt shared this <a href="https://www.theguardian.com/news/datablog/2010/dec/14/facebook-friends-mapping" target="_new">map of Facebook friendships</a>, created by Paul Butler in 2010, that “broke the internet”.&nbsp; He’s seeing similar maps since, but “it just doesn’t turn heads like it used to”.&nbsp; There’s a really interesting tangent there that I want to explore one day, about the connection between novelty, a technical difficulty (“a good challenge”), and the motivation to experiment.</p><p><img src="https://storage.googleapis.com/shirleywu-studio-thoughts-assets/notebook/2025-02-innovation-killer/03-facebook-og.jpg" alt="A glowing blue world map visualization depicting global Facebook friendships. Bright, interconnected lines represent the density of Facebook connections across continents, with particularly strong concentrations in North America, Europe, and parts of Asia. The visualization highlights the global reach of social networking, with darker regions indicating areas with fewer connections." autoplay="true" muted="true" loop="true"></p>  <p data-svelte-h="svelte-189lpl3">He has a point.&nbsp; All the New York Times graphics I shared earlier were ground-breaking when they first came out, but if I saw them now, I probably wouldn’t bat an eye.&nbsp; (Except the Oscars piece, that one still gets me.)&nbsp; The bar has been set.</p> <p data-svelte-h="svelte-un3gz4">Which leads to an interesting conundrum: The most successful charts and formats get rolled up into chart-building tools and templatized.&nbsp; On one hand, this is a great thing—we all benefit from easier and more accessible ways of creating.&nbsp; But, as Moritz notes, it also leads to less investment in truly original data visuals and an overabundance of more homogenous visual styles. And that’s how we end up with scrollytelling fatigue™️ and our clients and their stakeholders asking for bar charts and line charts.</p> <p data-svelte-h="svelte-pbdk4z">Speaking of clients, Moritz notes that in economic downturns like the one we’ve been in, everybody tends to play it safe: “In an uncertain economy, clients—especially startups, nonprofits, or mid-sized firms—may cut creative budgets or prioritize ‘good enough’ solutions over innovative, artisanal visualizations.”&nbsp; Bespoke interactive graphics are often big and expensive to build, and their impact might be hard to measure.&nbsp; On the other hand, those same no-code or low-code chart-building tools have become increasingly impressive and easier for clients to maintain.&nbsp; These days, he says, “there are many more ways to create good-enough dynamic charts and charts… with little or no coding effort, with the additional benefit of integrating directly into existing information ecosystems.”</p> <h3 data-svelte-h="svelte-wc6zpd">Zooming out</h3> <p data-svelte-h="svelte-1hut5m6">Giorgia contextualizes what we’re experiencing as the natural ebb and flow of an innovation cycle: “I think every wave of technological or methodological change follows a predictable pattern: an initial hype cycle, a period of widespread adoption, and then an eventual plateau where only the most practical elements endure.”</p> <p data-svelte-h="svelte-161owdp">We’re currently smack in the middle of that plateau, but, as Alberto reminds me, that’s not a bad thing.&nbsp; “Sometimes we have periods of very rapid development and innovation, like a new technology shows up, a new programming language, a new technique, and there’s very quick adoption of that, a lot of experimentation, a lot of excitement. And suddenly we reach a plateau in which that new technology feels a little bit dated or overused,” he says. “Sometimes there are periods in which we withdraw a little bit from that innovation and we go back to traditional forms. But then there will be another period of innovation. Somebody will come up with a novel way of doing things, and there will be another period of excitement. So maybe the period that we are in right now is a time of regrouping. But that’s not necessarily a bad thing.”</p> <p data-svelte-h="svelte-1jsee4n">After all, he was part of a very small group of newspapers pushing for interactive graphics in the late 90’s, when journalists and designers were getting pushback from their editors for scatterplots and histograms.&nbsp; He’s lived through an innovation cycle already, and he’s unfazed.</p> <h3 data-svelte-h="svelte-d5j4io">So what next?</h3> <p data-svelte-h="svelte-dkvaz8">As an experimenter that thrives off of novelty, I’ve been restless for our next cycle of innovation—and I have thoughts.&nbsp; Lots of thoughts, actually, especially after hearing from so many amazing perspectives.</p> <p data-svelte-h="svelte-68cppr">So don’t forget to like, subscribe, and hit that notification bell!</p> <p data-svelte-h="svelte-1kly1je">(Sorry I’ve always wanted to pretend I’m an Youtuber 😂)</p> <p data-svelte-h="svelte-1bf9uut">(But if you do want to subscribe, the next one should be a really good one 🎊)</p> </article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Open-sourcing OpenPubkey SSH (OPKSSH): integrating single sign-on with SSH (215 pts)]]></title>
            <link>https://blog.cloudflare.com/open-sourcing-openpubkey-ssh-opkssh-integrating-single-sign-on-with-ssh/</link>
            <guid>43470906</guid>
            <pubDate>Tue, 25 Mar 2025 13:22:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.cloudflare.com/open-sourcing-openpubkey-ssh-opkssh-integrating-single-sign-on-with-ssh/">https://blog.cloudflare.com/open-sourcing-openpubkey-ssh-opkssh-integrating-single-sign-on-with-ssh/</a>, See on <a href="https://news.ycombinator.com/item?id=43470906">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post"><article><p>2025-03-25</p><section><p>7 min read</p><img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1SXiWOhmwfDs86m6u84i8l/7b6b0874f6f2f91964b87383349c7785/image2.png" alt=""><div><p>OPKSSH makes it easy to <a href="https://en.wikipedia.org/wiki/Secure_Shell"><u>SSH</u></a> with single sign-on technologies like OpenID Connect, thereby removing the need to manually manage and configure SSH keys. It does this without adding a trusted party other than your identity provider (IdP).</p><p>We are excited to announce <a href="https://github.com/openpubkey/opkssh/"><u>OPKSSH (OpenPubkey SSH)</u></a> has been open-sourced under the umbrella of the OpenPubkey project. While the underlying protocol <a href="https://github.com/openpubkey/openpubkey/"><u>OpenPubkey</u></a> became <a href="https://www.linuxfoundation.org/press/announcing-openpubkey-project"><u>an open source Linux foundation project in 2023</u></a>, OPKSSH was closed source and owned by <a href="https://www.cloudflare.com/press-releases/2024/cloudflare-acquires-bastionzero-to-add-zero-trust-infrastructure-access/"><u>BastionZero (now Cloudflare)</u></a>. Cloudflare has gifted this code to the OpenPubkey project, making it open source.</p><p>In this post, we describe what OPKSSH is, how it simplifies SSH management, and what OPKSSH being open source means for you.</p>
          <p>
            <h2 id="background">Background</h2>
            
          </p>
          <p>A cornerstone of modern access control is single sign-on <a href="https://www.cloudflare.com/learning/access-management/what-is-sso/"><u>(SSO)</u></a>, where a user authenticates to an <a href="https://www.cloudflare.com/learning/access-management/what-is-an-identity-provider/"><u>identity provider (IdP)</u></a>, and in response the IdP issues the user a <i>token</i>. The user can present this token to prove their identity, such as “Google says I am Alice”. SSO is the rare security technology that both increases convenience — users only need to sign in once to get access to many different systems — and increases security.</p>
          <p>
            <h3 id="openid-connect">OpenID Connect</h3>
            
          </p>
        <p><a href="https://openid.net/developers/how-connect-works/"><u>OpenID Connect (OIDC)</u></a> is the main protocol used for SSO. As shown below, in OIDC the IdP, called an OpenID Provider (OP), issues the user an ID Token which contains identity claims about the user, such as “email is <a href="https://blog.cloudflare.com/cdn-cgi/l/email-protection" data-cfemail="7c1d10151f193c19041d110c1019521f1311">[email&nbsp;protected]</a>”. These claims are digitally signed by the OP, so anyone who receives the ID Token can check that it really was issued by the OP.</p><p>Unfortunately, while ID Tokens <i>do </i>include identity claims like name, organization, and email address, they <i>do not</i> include the user’s public key. This prevents them from being used to directly secure protocols like SSH or <a href="https://www.cloudflare.com/learning/privacy/what-is-end-to-end-encryption/"><u>End-to-End Encrypted messaging</u></a>.</p><p>Note that throughout this post we use the term OpenID Provider (OP) rather than IdP, as OP specifies the exact type of IdP we are using, i.e., an OpenID IdP. We use Google as an example OP, but OpenID Connect works with Google, Azure, Okta, etc.</p>
          <figure>
          <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2zfixdknoL3a9HqBzcsmAY/605f469bdd25b4b8cfaf29dac3561c4f/image1.png" alt="Shows a user Alice signing in to Google using OpenID Connect/OpenPubkey and then producing a PK Token" width="2000" height="1078" loading="lazy">
          </figure><p><sup><i>Shows a user Alice signing in to Google using OpenID Connect and receiving an ID Token</i></sup></p>
          <p>
            <h3 id="openpubkey">OpenPubkey</h3>
            
          </p>
        <p>OpenPubkey, shown below, adds public keys to ID Tokens. This enables ID Tokens to be used like certificates, e.g. “Google says <code><a href="https://blog.cloudflare.com/cdn-cgi/l/email-protection" data-cfemail="67060b0e040227021f060a170b024904080a">[email&nbsp;protected]</a></code> is using public key 0x123.” We call an ID token that contains a public key a <i>PK Token</i>. The beauty of OpenPubkey is that, unlike other approaches, OpenPubkey does not require any changes to existing SSO protocols and supports any OpenID Connect compliant OP.</p>
          <figure>
          <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2HNkaW8vPE26KQrNwNOzNo/ef00f91dc983f3f2ac3c3a00b223b3e5/image3.png" alt="" width="2000" height="1079" loading="lazy">
          </figure><p><sup><i>Shows a user Alice signing in to Google using OpenID Connect/OpenPubkey and then producing a PK Token</i></sup>
While OpenPubkey enables ID Tokens to be used as certificates, OPKSSH extends this functionality so that these ID Tokens can be used as SSH keys in the SSH protocol. This adds SSO authentication to SSH without requiring changes to the SSH protocol.</p>
          <p>
            <h2 id="why-this-matters">Why this matters</h2>
            
          </p>
          <p>OPKSSH frees users and administrators from the need to manage long-lived SSH keys, making SSH more secure and more convenient.</p><blockquote><p><i>“In many organizations – even very security-conscious organizations – there are many times more obsolete authorized keys than they have employees. Worse, authorized keys generally grant command-line shell access, which in itself is often considered privileged. We have found that in many organizations about 10% of the authorized keys grant root or administrator access. SSH keys never expire.”</i>&nbsp; 
- <a href="https://ylonen.org/papers/ssh-key-challenges.pdf">Challenges in Managing SSH Keys – and a Call for Solutions</a> by Tatu Ylonen (Inventor of SSH)</p></blockquote><p>In SSH, users generate a long-lived SSH public key and SSH private key. To enable a user to access a server, the user or the administrator of that server configures that server to trust that user’s public key. Users must protect the file containing their SSH private key. If the user loses this file, they are locked out. If they copy their SSH private key to multiple computers or back up the key, they increase the risk that the key will be compromised. When a private key is compromised or a user no longer needs access, the user or administrator must remove that public key from any servers it currently trusts. All of these problems create headaches for users and administrators.</p><p>OPKSSH overcomes these issues:</p><p><b>Improved security:</b> OPKSSH replaces long-lived SSH keys with ephemeral SSH keys that are created on-demand by OPKSSH and expire when they are no longer needed. This reduces the risk a private key is compromised, and limits the time period where an attacker can use a compromised private key. By default, these OPKSSH public keys expire every 24 hours, but the expiration policy can be set in a configuration file.</p><p><b>Improved usability:</b> Creating an SSH key is as easy as signing in to an OP. This means that a user can SSH from any computer with opkssh installed, even if they haven’t copied their SSH private key to that computer.</p><p>To generate their SSH key, the user simply runs opkssh login, and they can use ssh as they typically do.</p><p><b>Improved visibility:</b> OPKSSH moves SSH from authorization by public key to authorization by identity. If Alice wants to give Bob access to a server, she doesn’t need to ask for his public key, she can just add Bob’s email address <a href="https://blog.cloudflare.com/cdn-cgi/l/email-protection" data-cfemail="87e5e8e5c7e2ffe6eaf7ebe2a9e4e8ea">[email&nbsp;protected]</a> to the OPKSSH authorized users file, and he can sign in. This makes tracking who has access much easier, since administrators can see the email addresses of the authorized users.</p><p>OPKSSH does not require any code changes to the SSH server or client. The only change needed to SSH on the SSH server is to add two lines to the SSH config file. For convenience, we provide an installation script that does this automatically, as seen in the video below.</p>

          <p>
            <h2 id="how-it-works">How it works</h2>
            
          </p>
          
          <figure>
          <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5rVAtbu3vv9wU84ke8IZIb/a5e564c2ae3834391bd7f04c843508b7/image4.png" alt="Shows a user Alice SSHing into a server with her PK Token inside her SSH public key. The server then verifies her SSH public key using the OpenPubkey verifier." width="1914" height="1259" loading="lazy">
          </figure><p><sup><i>Shows a user Alice SSHing into a server with her PK Token inside her SSH public key. The server then verifies her SSH public key using the OpenPubkey verifier.</i></sup></p><p>Let’s look at an example of Alice (<code><a href="https://blog.cloudflare.com/cdn-cgi/l/email-protection" data-cfemail="187974717b7d587d60797568747d367b7775">[email&nbsp;protected]</a></code>) using OPKSSH to SSH into a server:</p><ul><li><p>Alice runs <code>opkssh login</code>. This command automatically generates an ephemeral public key and private key for Alice. Then it runs the OpenPubkey protocol by opening a browser window and having Alice log in through their SSO provider, e.g., Google.&nbsp;</p></li><li><p>If Alice SSOs successfully, OPKSSH will now have a PK Token that commits to Alice’s ephemeral public key and Alice’s identity. Essentially, this PK Token says “<code><a href="https://blog.cloudflare.com/cdn-cgi/l/email-protection" data-cfemail="0465686d676144617c65697468612a676b69">[email&nbsp;protected]</a></code> authenticated her identity and her public key is 0x123…”.</p></li><li><p>OPKSSH then saves to Alice’s <code>.ssh </code>directory:</p><ul><li><p>an SSH public key file that contains Alice’s PK Token&nbsp;</p></li><li><p>and an SSH private key set to Alice’s ephemeral private key.</p></li></ul></li><li><p>When Alice attempts to SSH into a server, the SSH client will find the SSH public key file containing the PK Token in Alice’s <code>.ssh</code> directory, and it will send it to the SSH server to authenticate.</p></li><li><p>The SSH server forwards the received SSH public key to the OpenPubkey verifier installed on the SSH server. This is because the SSH server has been configured to use the OpenPubkey verifier via the AuthorizedKeysCommand.</p></li><li><p>The OpenPubkey verifier receives the SSH public key file and extracts the PK Token from it. It then verifies that the PK Token is unexpired, valid, signed by the OP and that the public key in the PK Token matches the public key field in the SSH public key file. Finally, it extracts the email address from the PK Token and checks if <code><a href="https://blog.cloudflare.com/cdn-cgi/l/email-protection" data-cfemail="5c3d30353f391c39243d312c3039723f3331">[email&nbsp;protected]</a></code> is allowed to SSH into this server.</p></li></ul><p>Consider the problems we face in getting OpenPubkey to work with SSH without requiring any changes to the SSH protocol or software:</p><p><b>How do we get the PK Token from the user’s machine to the SSH server inside the SSH protocol?</b>
We use the fact that SSH public keys can be SSH certificates, and that SSH certificates have <a href="https://cvsweb.openbsd.org/cgi-bin/cvsweb/src/usr.bin/ssh/PROTOCOL.certkeys?rev=1.4&amp;content-type=text/x-cvsweb-markup"><u>an extension field</u></a> that allows arbitrary data to be included in the certificate. Thus, we package the PK Token into an SSH certificate extension so that the PK Token will be transmitted inside the SSH public key as a normal part of the SSH protocol. This enables us to send the PK Token to the SSH server as additional data in the SSH certificate, and allows OPKSSH to work without any changes to the SSH client.</p><p><b>How do we check that the PK Token is valid once it arrives at the SSH server?
</b>SSH servers support a configuration parameter called the <a href="https://man.openbsd.org/sshd_config#AuthorizedKeysCommand"><i><u>AuthorizedKeysCommand</u></i></a><i> </i>that allows us to use a custom program to determine if an SSH public key is authorized or not. Thus, we change the SSH server’s config file to use the OpenPubkey verifier instead of the SSH verifier by making the following two line change to <code>sshd_config</code>:</p>
            <pre><code>AuthorizedKeysCommand /usr/local/bin/opkssh verify %u %k %t
AuthorizedKeysCommandUser root</code></pre>
            <p>The OpenPubkey verifier will check that the PK Token is unexpired, valid and signed by the OP. It checks the user’s email address in the PK Token to determine if the user is authorized to access the server.</p><p><b>How do we ensure that the public key in the PK Token is actually the public key that secures the SSH session?</b>
The OpenPubkey verifier also checks that the public key in the public key field in the SSH public key matches the user’s public key inside the PK Token. This works because the public key field in the SSH public key is the actual public key that secures the SSH session.</p>
          <p>
            <h2 id="what-is-happening">What is happening</h2>
            
          </p>
          <p>We have <a href="https://github.com/openpubkey/openpubkey/pull/234"><u>open sourced OPKSSH</u></a> under the <a href="https://www.apache.org/licenses/LICENSE-2.0"><u>Apache 2.0 license</u></a>, and released it as <a href="https://github.com/openpubkey/opkssh/"><u>openpubkey/opkssh on GitHub</u></a>. While the OpenPubkey project has had code for using SSH with OpenPubkey since the early days of the project, this code was intended as a prototype and was missing many important features. With OPKSSH, SSH support in OpenPubkey is no longer a prototype and is now a complete feature. Cloudflare is not endorsing OPKSSH, but simply donating code to OPKSSH.</p><p><b>OPKSSH provides the following improvements to OpenPubkey:</b></p><ul><li><p>Production ready SSH in OpenPubkey</p></li><li><p>Automated installation</p></li><li><p>Better configuration tools</p></li></ul>
          <p>
            <h2 id="to-learn-more">To learn more</h2>
            
          </p>
          <p>See the <a href="https://github.com/openpubkey/opkssh/blob/main/README.md"><u>OPKSSH readme</u></a> for documentation on how to install and connect using OPKSSH.</p>
          <p>
            <h2 id="how-to-get-involved">How to get involved</h2>
            
          </p>
          <p>There are a number of ways to get involved in OpenPubkey or OPKSSH. The project is organized through the <a href="https://github.com/openpubkey/opkssh"><u>OPKSSH GitHub</u></a>. We are building an open and friendly community and welcome pull requests from anyone. If you are interested in contributing, see <a href="https://github.com/openpubkey/openpubkey/blob/main/CONTRIBUTING.md"><u>our contribution guide</u></a>.</p><p>We run a <a href="https://github.com/openpubkey/community"><u>community</u></a> meeting every month which is open to everyone, and you can also find us over on the <a href="https://openssf.org/getinvolved/"><u>OpenSSF Slack</u></a> in the #openpubkey channel.</p></div></section><div><p>Cloudflare's connectivity cloud protects <a target="_blank" href="https://www.cloudflare.com/network-services/" rel="noreferrer">entire corporate networks</a>, helps customers build <a target="_blank" href="https://workers.cloudflare.com/" rel="noreferrer">Internet-scale applications efficiently</a>, accelerates any <a target="_blank" href="https://www.cloudflare.com/performance/accelerate-internet-applications/" rel="noreferrer">website or Internet application</a>, <a target="_blank" href="https://www.cloudflare.com/ddos/" rel="noreferrer">wards off DDoS attacks</a>, keeps <a target="_blank" href="https://www.cloudflare.com/application-security/" rel="noreferrer">hackers at bay</a>, and can help you on <a target="_blank" href="https://www.cloudflare.com/products/zero-trust/" rel="noreferrer">your journey to Zero Trust</a>.</p><p>Visit <a target="_blank" href="https://one.one.one.one/" rel="noreferrer">1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.</p><p>To learn more about our mission to help build a better Internet, <a target="_blank" href="https://www.cloudflare.com/learning/what-is-cloudflare/" rel="noreferrer">start here</a>. If you're looking for a new career direction, check out <a target="_blank" href="https://www.cloudflare.com/careers" rel="noreferrer">our open positions</a>.</p></div><a href="https://blog.cloudflare.com/tag/open-source/">Open Source</a><a href="https://blog.cloudflare.com/tag/ssh/">SSH</a><a href="https://blog.cloudflare.com/tag/sso/">Single Sign On (SSO)</a><a href="https://blog.cloudflare.com/tag/cryptography/">Cryptography</a><a href="https://blog.cloudflare.com/tag/authentication/">Authentication</a></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An AI bubble threatens Silicon Valley, and all of us (138 pts)]]></title>
            <link>https://prospect.org/power/2025-03-25-bubble-trouble-ai-threat/</link>
            <guid>43470786</guid>
            <pubDate>Tue, 25 Mar 2025 13:13:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://prospect.org/power/2025-03-25-bubble-trouble-ai-threat/">https://prospect.org/power/2025-03-25-bubble-trouble-ai-threat/</a>, See on <a href="https://news.ycombinator.com/item?id=43470786">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content" data-transition="content" data-loop="false" itemprop="articleBody">

    <p><em>This article appears in the</em><em> <a href="https://prospect.org/topics/apr-2025-issue/">April 2025</a> issue of</em> The American Prospect <em>magazine. <a href="https://simplecirc.com/subscribe/the-american-prospect" target="_blank" aria-label="Link opens in new window (Subscribe here)">Subscribe here</a>.</em></p>

<p>The week of Donald Trump’s inauguration, Sam Altman, the CEO of OpenAI, stood tall next to the president as he made a <a href="https://www.cnn.com/2025/01/21/tech/openai-oracle-softbank-trump-ai-investment/index.html" target="_blank" aria-label="Link opens in new window (dramatic announcement)">dramatic announcement</a>: the launch of Project Stargate, a $500 billion supercluster in the rolling plains of Texas that would run OpenAI’s massive artificial-intelligence models. Befitting its name, Stargate would dwarf most megaprojects in human history. Even the $100 billion that Altman promised would be deployed “immediately” would be much more expensive than the Manhattan Project ($30 billion in current dollars) and the COVID vaccine’s Operation Warp Speed ($18 billion), rivaling the multiyear construction of the Interstate Highway System ($114 billion). OpenAI would have all the computing infrastructure it needed to complete its ultimate goal of building humanity’s last invention: artificial general intelligence (AGI).</p>

<hr>
<p><strong>Art for this story was created with Midjourney 6.1, an AI image generator.</strong></p>

<hr>
<p>But the reaction to Stargate was muted as Silicon Valley had turned its attention west. A new generative AI model called DeepSeek R1, released by the Chinese hedge fund High-Flyer, sent a threatening tremor through the balance sheets and investment portfolios of the tech industry. DeepSeek’s latest version, allegedly trained for just $6 million (though <a href="https://www.techspot.com/news/106612-deepseek-ai-costs-far-exceed-55-million-claim.html" target="_blank" aria-label="Link opens in new window (this has been contested)">this has been contested</a>), <a href="https://arxiv.org/pdf/2501.12948" target="_blank" aria-label="Link opens in new window (matched)">matched</a> the performance of OpenAI’s flagship reasoning model o1 <a href="https://venturebeat.com/ai/open-source-deepseek-r1-uses-pure-reinforcement-learning-to-match-openai-o1-at-95-less-cost/" target="_blank" aria-label="Link opens in new window (at 95 percent lower cost)">at 95 percent lower cost</a>. R1 even learned o1 reasoning techniques, OpenAI’s much-hyped “secret sauce” to allow it to maintain a wide technical lead over other models. Best of all, R1 is open-source down to the model weights, so anyone can download and modify the details of the model themselves for free.</p>

<p>It’s an existential threat to OpenAI’s business model, which depends on using its technical lead to sell the most expensive subscriptions in the industry. It also threatens to pop a speculative bubble around generative AI inflated by the Silicon Valley hype machine, with hundreds of billions at stake.</p>

<p>Venture capital (VC) funds, drunk on a decade of “growth at all costs,” have poured <a href="https://www.cbinsights.com/research/report/ai-trends-q2-2024/" target="_blank" aria-label="Link opens in new window (about $200 billion into generative AI)">about $200 billion into generative AI</a>. Making matters worse, the stock market’s bull run is deeply dependent on the growth of the Big Tech companies fueling the AI bubble. In 2023, <a href="https://finance.yahoo.com/news/one-chart-shows-how-the-magnificent-7-have-dominated-the-stock-market-in-2023-203250125.html" target="_blank" aria-label="Link opens in new window (71 percent of the total gains)">71 percent of the total gains</a> in the S&amp;P 500 were attributable to the “Magnificent Seven”—Apple, Nvidia, Tesla, Alphabet, Meta, Amazon, and Microsoft—all of which are among the biggest spenders on AI. Just four—Microsoft, Alphabet, Amazon, and Meta—<a href="https://www.ft.com/content/634b7ec5-10c3-44d3-ae49-2a5b9ad566fa" target="_blank" aria-label="Link opens in new window (combined)">combined</a> for $246 billion of capital expenditure in 2024 to support the AI build-out. Goldman Sachs <a href="https://www.goldmansachs.com/images/migrated/insights/pages/gs-research/gen-ai--too-much-spend%2C-too-little-benefit-/TOM_AI%202.0_ForRedaction.pdf" target="_blank" aria-label="Link opens in new window (expects)">expects</a> Big Tech to spend over $1 trillion on chips and data centers to power AI over the next five years. Yet OpenAI, the current market leader, <a href="https://www.theinformation.com/briefings/openai-reportedly-in-talks-to-raise-6-5-billion-at-150-billion-valuation" target="_blank" aria-label="Link opens in new window (expects)">expects</a> to lose $5 billion this year, and its annual losses to swell to $11 billion by 2026. If the AI bubble bursts, it not only threatens to wipe out VC firms in the Valley but also blow a gaping hole in the public markets and cause an economy-wide meltdown.</p>

<h3><b>OpenAI’s Ever-Increasing Costs</b></h3>

<p>The basic problem facing Silicon Valley today is, ironically, one of growth. There are no more digital frontiers to conquer. The young, pioneering upstarts—Facebook, Google, Amazon—that struck out toward the digital wilderness are now the monopolists, constraining growth with onerous rentier fees they can charge because of their market-making size. The software industry’s spectacular returns from the launch of the internet in the ’90s to the end of the 2010s would never come back, but venture capitalists still chased the chance to invest in the next Facebook or Google. This has led to what AI critic Ed Zitron calls the “<a href="https://www.wheresyoured.at/the-rot-economy/" target="_blank" aria-label="Link opens in new window (rot economy)">rot economy</a>,” in which VCs overhype a series of digital technologies—the blockchain, then cryptocurrencies, then NFTs, and then the metaverse—promising the limitless growth of the early internet companies. According to Zitron, each of these innovations failed to either transform existing industries or become sustainable industries themselves, because the business case at the heart of these technologies was rotten, pushed forward by wasteful, bloated venture investments still selling an endless digital frontier of growth that no longer existed. Enter AGI, the proposed creation of an AI with an intelligence that dwarfs any single person’s and possibly the collective intelligence of humanity. Once AGI is built, we can easily solve many of the toughest challenges <a href="https://www.forbes.com/sites/moorinsights/2025/01/30/the-stargate-project-trump-touts-500-billion-bid-for-ai-dominance/" target="_blank" aria-label="Link opens in new window (facing humanity)">facing humanity</a>: climate change, cancer, new net-zero energy sources.</p>

<p>And no company has pushed the coming of AGI more than OpenAI, which has ridden the hype to incredible heights since its release of generative chatbot ChatGPT. Last year, OpenAI <a href="https://www.nytimes.com/2024/09/27/technology/openai-chatgpt-investors-funding.html" target="_blank" aria-label="Link opens in new window (completed)">completed</a> a blockbuster funding round, raising $6.6 billion at a valuation of $157 billion, making it the third most valuable startup in the world at the time after SpaceX and ByteDance, TikTok’s parent company. OpenAI, which released ChatGPT in November 2022, now <a href="https://www.wsj.com/tech/ai/openai-nearly-doubles-valuation-to-157-billion-in-funding-round-ee220607" target="_blank" aria-label="Link opens in new window (sees)">sees</a> 250 million weekly active users and about 11 million paying subscribers for its AI tools. The startup’s monthly revenue hit $300 million in August, up more than 1,700 percent since the start of 2023, and it expects to clear $3.7 billion for the year. By all accounts, this is another world-changing startup on a meteoric rise. Yet take a deeper look at OpenAI’s financial situation and expected future growth, and cracks begin to show.</p>

<p>To start, OpenAI is burning money at an impressive but unsustainable pace. The latest funding round is its third in the last two years, atypical for a startup, that also <a href="https://www.cnbc.com/2024/10/03/openai-gets-4-billion-revolving-credit-line-on-top-of-latest-funding.html" target="_blank" aria-label="Link opens in new window (included)">included</a> a $4 billion revolving line of credit—a loan on tap, essentially—on top of the $6.6 billion of equity, revealing an insatiable need for investor cash to survive. Despite $3.7 billion in sales this year, OpenAI expects to lose $5 billion due to the stratospheric costs of building and running generative AI models, which includes $4 billion in cloud computing to run their AI models, $3 billion in computing to train the next generation of models, and $1.5 billion for its staff. According to its own numbers, OpenAI loses $2 for every $1 it makes, a red flag for the sustainability of any business. Worse, these costs are expected to increase as ChatGPT gains users and OpenAI seeks to upgrade its foundation model from GPT-4 to GPT-5 sometime in the next six months.</p>

<p>Financial documents reviewed by <a href="https://www.theinformation.com/briefings/openai-reportedly-in-talks-to-raise-6-5-billion-at-150-billion-valuation" target="_blank" aria-label="Link opens in new window (The Information)">The Information</a> confirm this trajectory as the startup predicts its annual losses will hit $14 billion by 2026. Further, OpenAI sees $100 billion in annual revenue—a number that would rival Nestlé and Target’s returns—as the point at which it will finally break even. For comparison, Google’s parent company, Alphabet, only cleared $100 billion in sales in 2021, 23 years after its founding, yet boasted a portfolio of money-making products, including Google Search, the Android operating system, Gmail, and cloud computing.</p>

<p>OpenAI is deeply dependent on hypothetical breakthroughs from future models that unlock more capabilities to boost its subscription price and grow its user base. Its GPT-5 class models and beyond must pull godlike capacities for AI out of the algorithmic ether to create a user base of hundreds of millions of paid subscribers. Yet, with the release of the open-source DeepSeek R1 model earlier this month, OpenAI has no moat for its increasingly expensive products. The R1 matched its performance across math, chemistry, and coding tasks, independently learned OpenAI’s reasoning techniques, and can be downloaded, modified, and deployed for free. Why would people continue to pay $20, let alone the $200 tier OpenAI reserves for its latest, greatest models, rather than use something that can deliver the same performance at a 95 percent lower price?</p>

<blockquote><p>Venture capital funds, drunk on a decade of “growth at all costs,” have poured about $200 billion into generative AI.</p>
</blockquote>

<h3><b>Silicon Valley Is All In on AI</b></h3>

<p>Wall Street asked itself the same question after the release of DeepSeek R1 and panicked, wiping more than 15 percent ($600 billion) off Nvidia’s stock price, the largest single-day loss for a company ever. And that’s not the only bad sign Altman received about OpenAI’s future. OpenAI <a href="https://www.wsj.com/tech/ai/openaiin-talks-for-huge-investment-round-valuing-it-up-to-300-billion-2a2d4327" target="_blank" aria-label="Link opens in new window (is in talks again)">is in talks again</a> to raise more money (less than a year after raising $10 billion) at a proposed $340 billion valuation. In most cases, a startup doubling its valuation would be great news. But for OpenAI, the money may make things worse. It signals a desperate need for cash and puts more pressure on a company that today loses $2 for every dollar it makes. As Zitron pointed out, at $340 billion, few companies have the liquidity to acquire OpenAI, and public investors expect strong returns and profitability to justify an IPO anywhere near that price. Plus, the latest round of funding is being led by Masayoshi Son, a billionaire investor known more for losing money than making it. Given Son’s Vision Fund’s disastrous investing record, Zitron said, it’s as bearish a signal as you could find. Hanging over all this for OpenAI <a href="https://www.wheresyoured.at/subprimeai/" target="_blank" aria-label="Link opens in new window (is the fact that)">is the fact that</a> Microsoft’s investments in the company, which run north of $10 billion, are not standard equity investments but “profit participation units” that will convert to debt in a year and a half.</p>

<p>It’s not just OpenAI that’s burning through billions. Silicon Valley has hyped AI as the next internet or iPhone, and has invested like it cannot afford to miss out on the next big tech revolution. In 2021, with the last gasp of zero-interest-rate loans paired with trillions in COVID relief spending, venture capitalists <a href="https://www.ccn.com/analysis/ai-companies-raised-50b-2023-vc/" target="_blank" aria-label="Link opens in new window (poured a record $78.5 billion)">poured a record $78.5 billion</a> into the AI space. And, despite a broader slowdown in venture activity, the second quarter of 2024 set the record for quarterly venture investing in AI <a href="https://www.cbinsights.com/research/report/ai-trends-q2-2024/" target="_blank" aria-label="Link opens in new window (at $23.3 billion)">at $23.3 billion</a>. In fact, 33 percent of VC portfolios <a href="https://www.statista.com/chart/33346/ai-share-of-vc-investments-in-the-us/" target="_blank" aria-label="Link opens in new window (are committed)">are committed</a> to AI, another worrying sign of concentration.</p>

<p>Even so, Big Tech companies are the biggest spenders on AI. While VCs dropped approximately $200 billion into AI between 2021 and 2024, Big Tech is on pace to surpass that amount this year alone. According to Goldman Sachs research, cloud computing giants are expected to plow over $1 trillion over the next five years into graphics processing units (GPUs) and to build data centers to power generative AI. AI is an expensive technology like few before it.</p>

<p>All those racks of GPUs and supercluster data centers need power, and the power industry is also embarking on a once-in-a-generation investment spree to keep up. The scale of expected data centers to power generative AI is difficult to wrap your head around. Oracle recently <a href="https://www.cnbc.com/2024/09/10/oracle-is-designing-a-data-center-that-would-be-powered-by-three-small-nuclear-reactors.html" target="_blank" aria-label="Link opens in new window (announced)">announced</a> plans to build a gigawatt-scale data center just for AI, powered by a trio of nuclear reactors, while OpenAI <a href="https://arstechnica.com/tech-policy/2024/09/openai-asked-us-to-approve-energy-guzzling-5gw-data-centers-report-says/" target="_blank" aria-label="Link opens in new window (pitched)">pitched</a> the White House on the necessity of five-gigawatt data centers, which would be enough power for about three million homes consumed by a single AI data center. A recent report from McKinsey <a href="https://www.mckinsey.com/industries/private-capital/our-insights/how-data-centers-and-the-energy-sector-can-sate-ais-hunger-for-power" target="_blank" aria-label="Link opens in new window (expects)">expects</a> the electricity going to fuel AI data centers will triple from 3 to 4 percent of the country’s electricity to 11 to 12 percent by 2030. The power industry typically grows 2 to 3 percent a year, far too little to meet the predicted jump in demand. McKinsey estimates that power utilities would have to spend $500 billion on top of their planned capital expenditure to keep up with AI needs. If true, this presents a serious bottleneck for not just OpenAI but the expected growth of the AI industry.</p>

<h3><b>Where’s the Money, Lebowski?</b></h3>

<p>Between VCs, Big Tech, and power utilities, the bill for generative AI comes out to close to $2 trillion in spending over the next five years alone. Adding all this up, some are starting to question the economic fundamentals of generative AI. Jim Covello, head of global equity research at Goldman Sachs, <a href="https://www.goldmansachs.com/images/migrated/insights/pages/gs-research/gen-ai--too-much-spend%2C-too-little-benefit-/TOM_AI%202.0_ForRedaction.pdf" target="_blank" aria-label="Link opens in new window (doubts)">doubts</a> the technology can recoup what’s been invested as, unlike the internet, it fails to solve complex business problems at a lower cost than what’s available today. Plus, he argues, the most expensive inputs for generative AI, GPUs and energy, are unlikely to decline meaningfully for the tech industry over time, given how far demand outstrips supply for both. While AI-fueled coding could definitely boost productivity, it’s hard to see how it could become a multitrillion-dollar industry.</p>

<p>Surveys confirm that for many workers, AI tools like ChatGPT <i>reduce</i> their productivity by increasing the volume of content and steps needed to complete a given task, and by frequently introducing errors that have to be checked and corrected. A <a href="https://www.cio.com/article/3540579/devs-gaining-little-if-anything-from-ai-coding-assistants.html" target="_blank" aria-label="Link opens in new window (study by Uplevel Data Labs)">study by Uplevel Data Labs</a> tracked 800 software engineers using Copilot on GitHub and found no measurable increase in coding productivity, despite this exact use case being the one pointed to the most by AI companies. And even productivity gains may come at a cost: Microsoft researchers <a href="https://www.microsoft.com/en-us/research/uploads/prod/2025/01/lee_2025_ai_critical_thinking_survey.pdf" target="_blank" aria-label="Link opens in new window (concluded)">concluded</a> that workers became more productive using generative AI tools but their critical thinking skills declined, presumably because they were offloading the thinking to AI. Looking past the hype, the business case for generative AI two years after the stunning success of ChatGPT appears weaker by the day.</p>

<p>Even worse, as AI expert Gary Marcus <a href="https://garymarcus.substack.com/p/five-things-most-people-dont-seem" target="_blank" aria-label="Link opens in new window (pointed out)">pointed out</a>, DeepSeek’s R1 model <a href="https://www.msn.com/en-us/money/other/openai-is-highly-overvalued-and-deepseek-just-blew-up-their-business-model-says-nyu-s-gary-marcus/vi-AA1y0qgI" target="_blank" aria-label="Link opens in new window (spells serious trouble)">spells serious trouble</a> for OpenAI and the cloud giants. The only way OpenAI could hope to recoup the billions it was spending on GPUs to train bigger and bigger models was to maintain a large enough technical lead over other AI companies to justify charging up to $200 for paid subscriptions to its models. That lead <a href="https://garymarcus.substack.com/p/the-race-for-ai-supremacy-is-over" target="_blank" aria-label="Link opens in new window (just vaporized)">just vaporized</a> and was given to the entire industry for free. In response, Altman has already twice cut the prices of his subscriptions in an effort to stay competitive. But without millions of paid subscriptions, it’s difficult to see the pathway to profitability for a company that loses $2 for every $1 it brings in and expects costs to continue to grow approximately tenfold in five years. OpenAI has set $100 billion as its break-even point, which would require it to increase its revenue by a factor of 25 in just five years, an incredible feat of scale that its current business model does not justify.</p>

<p>OpenAI is, however, the perfect kind of growth-at-all-costs story investors need to think still exists—capable of not only achieving Meta- or Amazon-like growth again, but becoming an indispensable part of growth and innovation in every industry in the future, too. No industry could escape, and software would close its jaws around the world, finally, as Marc Andreessen <a href="https://a16z.com/why-software-is-eating-the-world/" target="_blank" aria-label="Link opens in new window (predicted)">predicted</a> in 2011.</p>

<p>For his part, Gary Marcus has taken to calling OpenAI the WeWork of AI—WeWork, of course, is the poster boy for wasteful, nearly fraudulent, growth-at-all-costs investing that led to a spectacular downfall. Marcus is so confident current approaches cannot take us to the promised land of AGI that he bet Anthropic CEO Dario Amodei $100,000 that AGI would not be achieved by the end of 2027. Without AGI, the valuations of leading AI startups like OpenAI ($340 billion) and Anthropic (<a href="https://www.cnbc.com/2025/02/24/anthropic-closes-in-on-3point5-billion-funding-round-.html" target="_blank" aria-label="Link opens in new window ($61.5 billion))">$61.5 billion)</a> stop making sense. If GPUs are no longer the most capital-efficient or effective way to build better AI models, then the expected AI computing “supercycle” that the hundreds of billions in capital expenditure is premised on never arrives. Instead, the underlying asset bubble of a multitrillion-dollar bet on GPUs as the necessary component to an internet-like era of growth vanishes into thin air.</p>

<blockquote><p>For many workers, AI tools reduce their productivity by increasing the volume of steps needed to complete a given task.</p>
</blockquote>

<h3><b>Just How Big Will the Blast Be?</b></h3>

<p>OpenAI’s incredible burn rate, the trillions in capital expenditure by cloud giants and utilities to build out the infrastructure necessary to support AI, the supply bottlenecks ahead from the power and semiconductor industries, and the questionable economic gains from these tools all point to a generative AI bubble. Should the bubble burst, startups and venture funds alike face possible extinction, and a big enough drop from the Magnificent Seven could spark skittish markets to panic, leading to wider economic contagion, given how dependent on the growth of the top technology companies the public markets have become.</p>

<p>In 2024, the Magnificent Seven <a href="https://www.cnbc.com/2024/07/01/how-magnificent-7-affects-sp-500-stock-market-concentration.html" target="_blank" aria-label="Link opens in new window (were responsible)">were responsible</a> for the lion’s share of the growth of the S&amp;P 500, with the returns of the other 493 companies flat. When Nvidia hit its peak valuation of $3 trillion over the summer, just five of the seven—Microsoft, Apple, Nvidia, Alphabet, and Amazon—accounted for 29 percent of the total index’s value, <a href="https://www.inc.com/phil-rosen/stock-market-outlook-nvidia-apple-microsoft-amazon-recession-fed-rates.html" target="_blank" aria-label="Link opens in new window (surpassing)">surpassing</a> the concentration of the five top technology companies just before the dot-com crash. <a href="https://www.reuters.com/markets/echoes-dotcom-bubble-haunt-ai-driven-us-stock-market-2024-07-02/" target="_blank" aria-label="Link opens in new window (Nvidia)">Nvidia</a> has been on an incredible bull run over the last five years, its shares <a href="https://www.reuters.com/markets/echoes-dotcom-bubble-haunt-ai-driven-us-stock-market-2024-07-02/" target="_blank" aria-label="Link opens in new window (gaining a dizzying 4,300 percent)">gaining a dizzying 4,300 percent</a>, reminiscent of how network equipment maker Cisco grew about 4,500 percent in the five years leading up to its peak just before the dot-com crash in 2000.</p>

<p>Nvidia and the other Magnificent Seven members are in a codependent relationship when it comes to AI hype. They are Nvidia’s biggest customers, feeding the bull run by pushing demand for GPUs beyond even what chipmaker TSMC can supply. At the moment, Nvidia can pass those prices on to their customers, the only clusters big enough for AI computing. But should demand for AI fall, all seven will tumble with it.</p>

<p>For the tech industry, DeepSeek is a threat to its incredible bull run because it proved three things. First, frontier AI models could be trained much more cheaply and efficiently than the current Silicon Valley approach of building massive models requiring hundreds of thousands of GPUs to train. From a capital perspective, the U.S. strategy is wasteful, relying on at least ten times the investment to make similar model progress. Second, DeepSeek showed you could train a state-of-the-art model without the latest GPUs, calling into question the current demand for the latest GPUs that is so hot customers have been facing delays of six months to a year to get their hands on them. Finally, the high valuations of leading AI startups depend on a technical lead in their models to charge prices anywhere near what they need to recoup their computing costs, but that technical lead, enabled by a combination of closed-source models, billions in capital expenditure, and export controls blocking Chinese companies like DeepSeek from accessing the latest GPUs, is gone. Should demand for GPUs fall or even not hit the exponential increases the billions invested are betting on, the bubble will pop.</p>

<p>Given the stock market’s dependence on tech companies for growth, the trigger may not come from the AI industry itself, but any pullback in spending will crater the current trajectory of the AI industry. Many potential triggers abound: a crypto crash; President Trump’s trade wars with Canada, Mexico, and China; the stated goal to cut more than $1 trillion of government spending by Elon Musk’s Department of Government Efficiency; or a Chinese invasion of Taiwan, where <a href="https://www.wired.com/story/taiwan-makes-the-majority-of-the-worlds-computer-chips-now-its-running-out-of-electricity/" target="_blank" aria-label="Link opens in new window (nearly 70 percent)">nearly 70 percent</a> of the world’s advanced computer chips are manufactured. You can tell Wall Street is worried about a bubble, because Nvidia is hit the hardest by any bearish AI news, and even when the market panic has nothing to do with the tech industry, like when the Japan currency trade happened last summer, the Magnificent Seven suffer punishing losses.</p>

<p>The AI bubble wobbles more precariously by the day. Some bubbles, like that of the dot-com crash, end up being positive in the long run, despite the short-term economic pain of it bursting. But some, like the 2008 housing bubble, leave permanent scars on the economy and can knock an entire industry off its growth trajectory for years. To date, the U.S. housing industry <a href="https://www.barrons.com/articles/home-building-in-the-u-s-still-hasnt-recovered-from-the-last-recession-heres-why-51568282700" target="_blank" aria-label="Link opens in new window (has not recovered)">has not recovered</a> to pre-2008 growth trend lines, a major contributor to the housing crisis gripping the U.S. That is the fire that the tech industry is playing with today.</p>

<p>This is not the Silicon Valley of lore. Venture investors, for all their tech manifestos celebrating “<a href="https://a16z.com/the-little-tech-agenda/" target="_blank" aria-label="Link opens in new window (little tech)">little tech</a>” and entrepreneurship, have come to <a href="https://www.irmagazine.com/shareholder-targeting-id/how-sovereign-wealth-funds-are-inflating-silicon-valley-bubble" target="_blank" aria-label="Link opens in new window (resemble)">resemble</a> more traditional financial firms, raising money from pension funds, hedge funds, and sovereign wealth funds. Silicon Valley has gone corporate and managerial; even private equity invests in the Valley today. The fusion of venture capital and Wall Street threatens to bring the unbridled speculation of unregulated finance and the breathless tech industry hype together in a single, massive bubble. Inimical to the old ethos of the Valley and emblematic of a bloated, rotten investing strategy, in Silicon Valley now the money chases founders rather than founders chasing money. Maybe, after the fallout of the AI bubble is felt and the sun sets on Silicon Valley for a bit, the tech world can do a hard reset and return to its more innovative days again.</p>


    
    

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla deliveries down 43% in Europe while EVs are up 31% (178 pts)]]></title>
            <link>https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/</link>
            <guid>43470763</guid>
            <pubDate>Tue, 25 Mar 2025 13:11:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/">https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/</a>, See on <a href="https://news.ycombinator.com/item?id=43470763">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
	<img width="1600" height="818" src="https://electrek.co/wp-content/uploads/sites/3/2021/12/Tesla-all-casrs-hero.jpg?quality=82&amp;strip=all&amp;w=1600" alt="Tesla all cars hero" srcset="https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2021/12/Tesla-all-casrs-hero.jpg?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2021/12/Tesla-all-casrs-hero.jpg?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2021/12/Tesla-all-casrs-hero.jpg?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2021/12/Tesla-all-casrs-hero.jpg?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high"></figure>

<p>The official numbers from ACEA are out, and they confirmed that Tesla deliveries have crashed by 43% in Europe so far this year.</p>



<p>It should be concerning for Tesla as electric vehicle sales are up 31% during the period.</p>



<p>Based on the main European auto markets already having reported vehicle registrations earlier this month, we already had <a href="https://electrek.co/2025/03/05/tesla-tsla-sales-crash-continues-in-europe-with-germany-down-70/" target="_blank" rel="noreferrer noopener">a good idea of Tesla’s performance in the market</a>, but now the European Automobile Manufacturers Association (ACEA) has made it official.</p>



<p>ACEA has released February registration numbers confirming that Tesla only delivered 16,888 units in the EU, EFTA, and UK markets in February 2025, compared to 28,182 units in 2024.</p>	
	



<p>For the first two months of the year, it totals 26,619 units, or 42.6% less than the 46,343 units delivered during the same period in 2024.</p>



<p>This is amid a 3.4% decline in automotive sales in Europe, but Tesla can hardly use that as an excuse since ACEA is reporting a 28.4% (31% in EU, EFTA, and UK) increase in battery-electric vehicle (BEV) sales this year despite Tesla’s erosion in the market:</p>



<blockquote>
<p><strong>Across the first two months of 2025</strong>, new battery-electric car sales grew by 28.4%, to 255,489 units, capturing 15.2% of total EU market share. Three of the four largest markets in the EU, accounting for 64% of all battery-electric car registrations, recorded robust double-digit gains: Germany (+41%), Belgium (+38%), and the Netherlands (+25%). This contrasted with France, which saw a slight decline of 1.3%.</p>
</blockquote>



<p>In the EU, EFTA, and UK markets, BEVs account for 17% of the entire auto market, with PHEVs adding another 7%.</p>



<p>Tesla had the worst performance of all automakers in the market:</p>



<figure><img decoding="async" width="1440" height="1612" src="https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09%E2%80%AFAM.png?w=915" alt="" srcset="https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09 https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/AM.png 1440w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09 https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/AM.png?resize=134,150 134w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09 https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/AM.png?resize=268,300 268w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09 https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/AM.png?resize=768,860 768w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09 https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/AM.png?resize=915,1024 915w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09 https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/AM.png?resize=1372,1536 1372w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09 https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/AM.png?resize=313,350 313w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09 https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/AM.png?resize=140,157 140w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09 https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/AM.png?resize=893,1000 893w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09 https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/AM.png?resize=150,168 150w" sizes="(max-width: 1440px) 100vw, 1440px"></figure>



<h2 id="h-electrek-s-take">Electrek’s Take</h2>



<p>Tesla fans are holding on to the idea that this is not a real problem because it is mostly due to the Model Y changeover, but that’s simply not true.</p>



<p>Model 3 registrations are also down in most European markets, despite Tesla having a similar situation as Model Y for Model 3 during this time last year.</p>



<p>In fact, Model 3 is down 29.4% in Europe so far this year despite plenty of inventory.</p>



<p>The shift to the new Model Y design is certainly having an effect, but it cannot account for the 43% drop in deliveries.</p>



<p>With deliveries of the new Model Y having started this month in Europe, we can see Tesla is still suffering in markets that report registration daily.</p>



<p>Tesla has only delivered 655 cars so far in March in Sweden compared to 2,524 for the whole month of March in 2024.</p>



<p>In Norway, Tesla is at 1,444 deliveries compared to 2,334 units for the whole month of March in 2024.</p>



<p>Tesla is already down 20,000 units in Europe for the first two months of the year compared to its 2024 performance, and that number could grow to 30,000 units by the end of the quarter.</p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMKqD-Qow6c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add Electrek to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<div><p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://electrek.co/about/#affiliate">More.</a></p><p><a href="https://bit.ly/Beatbot_SpringCleaning"><img src="https://electrek.co/wp-content/uploads/sites/3/2025/03/750x150-Beatbot-Native-Ad-Banners-2.png" alt="" width="750" height="150"></a></p></div>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Samsung CEO Jong-hee Han has died (249 pts)]]></title>
            <link>https://www.engadget.com/big-tech/samsung-ceo-jong-hee-han-has-died-120029286.html</link>
            <guid>43470699</guid>
            <pubDate>Tue, 25 Mar 2025 13:04:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.engadget.com/big-tech/samsung-ceo-jong-hee-han-has-died-120029286.html">https://www.engadget.com/big-tech/samsung-ceo-jong-hee-han-has-died-120029286.html</a>, See on <a href="https://news.ycombinator.com/item?id=43470699">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>One of Samsung's CEOs, Jong-hee Han, has died due to a heart attack, according to <a data-i13n="cpos:1;pos:1" href="https://www.reuters.com/world/asia-pacific/samsung-electronics-says-co-ceo-han-jong-hee-has-died-cardiac-arrest-2025-03-25/" rel="nofollow noopener" target="_blank" data-ylk="slk:Reuters;cpos:1;pos:1;elm:context_link;itc:0;sec:content-canvas"><em>Reuters</em></a> and <a data-i13n="cpos:2;pos:1" href="https://www.cnbc.com/2025/03/25/samsung-electronics-says-co-ceo-han-jong-hee-has-passed-away.html" rel="nofollow noopener" target="_blank" data-ylk="slk:CNBC;cpos:2;pos:1;elm:context_link;itc:0;sec:content-canvas"><em>CNBC</em></a>. He was 63. Han joined the company in 1988 and became the head of product research and development for visual display in 2011. He then led Samsung's TV business before he was named as the head of Samsung DX, which is what the company calls its <a data-i13n="cpos:3;pos:1" href="https://www.engadget.com/samsung-combines-mobile-consumer-electronics-055424678.html" data-ylk="slk:merged mobile and consumer electronics;cpos:3;pos:1;elm:context_link;itc:0;sec:content-canvas">merged mobile and consumer electronics</a> divisions, in 2021. In 2022, he officially became the company's Vice Chairman and CEO. Han had no experience in mobile before he started leading the company's DX group, but Samsung gave him credit for helping it get to the top of global TV sales for 15 years.</p><p><em>CNBC</em> says Han was one of the executives who hosted Samsung's annual general shareholders meeting just a week ago and answered questions about the company's poor stock performance. During the meeting, Han <a data-i13n="cpos:4;pos:1" href="https://www.cnbc.com/2025/03/19/samsung-ceo-says-company-will-pursue-deals-as-it-struggles-for-growth.html" rel="nofollow noopener" target="_blank" data-ylk="slk:apologized;cpos:4;pos:1;elm:context_link;itc:0;sec:content-canvas">apologized</a> to the shareholders, telling them that Samsung "failed to adequately respond to the rapidly evolving AI semiconductor market." He also told the shareholders that Samsung was having difficulties when it came to semiconductor-related mergers and acquisitions due to regulatory issues, but that the company was "determined to produce some tangible results this year."</p><p>Based on the notice Samsung <a data-i13n="cpos:5;pos:1" href="https://www.samsung.com/global/ir/reports-disclosures/public-disclosure-view.84335/" rel="nofollow noopener" target="_blank" data-ylk="slk:published;cpos:5;pos:1;elm:context_link;itc:0;sec:content-canvas">published</a>, Han's co-CEO Young-Hyun Jun is now the sole CEO of the company. Jun, who also heads Samsung's semiconductor business, was appointed as Han's co-CEO in November 2024. It's not clear whether Samsung is planning to appoint another CEO in the future.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VGGT: Visual Geometry Grounded Transformer (160 pts)]]></title>
            <link>https://github.com/facebookresearch/vggt</link>
            <guid>43470651</guid>
            <pubDate>Tue, 25 Mar 2025 12:59:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/facebookresearch/vggt">https://github.com/facebookresearch/vggt</a>, See on <a href="https://news.ycombinator.com/item?id=43470651">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{wang2025vggt,
  title={VGGT: Visual Geometry Grounded Transformer},
  author={Wang, Jianyuan and Chen, Minghao and Karaev, Nikita and Vedaldi, Andrea and Rupprecht, Christian and Novotny, David},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2025}
}"><pre><span>@inproceedings</span>{<span>wang2025vggt</span>,
  <span>title</span>=<span><span>{</span>VGGT: Visual Geometry Grounded Transformer<span>}</span></span>,
  <span>author</span>=<span><span>{</span>Wang, Jianyuan and Chen, Minghao and Karaev, Nikita and Vedaldi, Andrea and Rupprecht, Christian and Novotny, David<span>}</span></span>,
  <span>booktitle</span>=<span><span>{</span>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2025<span>}</span></span>
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">Visual Geometry Grounded Transformer (VGGT, CVPR 2025) is a feed-forward neural network that directly infers all key 3D attributes of a scene, including extrinsic and intrinsic camera parameters, point maps, depth maps, and 3D point tracks, <strong>from one, a few, or hundreds of its views, within seconds</strong>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto">First, clone this repository to your local machine, and install the dependencies (torch, torchvision, numpy, Pillow, and huggingface_hub).</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:facebookresearch/vggt.git 
cd vggt
pip install -r requirements.txt"><pre>git clone git@github.com:facebookresearch/vggt.git 
<span>cd</span> vggt
pip install -r requirements.txt</pre></div>
<p dir="auto">Alternatively, you can install VGGT as a package (<a href="https://github.com/facebookresearch/vggt/blob/main/docs/package.md">click here</a> for details).</p>
<p dir="auto">Now, try the model with just a few lines of code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from vggt.models.vggt import VGGT
from vggt.utils.load_fn import load_and_preprocess_images

device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
# bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) 
dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16

# Initialize the model and load the pretrained weights.
# This will automatically download the model weights the first time it's run, which may take a while.
model = VGGT.from_pretrained(&quot;facebook/VGGT-1B&quot;).to(device)

# Load and preprocess example images (replace with your own image paths)
image_names = [&quot;path/to/imageA.png&quot;, &quot;path/to/imageB.png&quot;, &quot;path/to/imageC.png&quot;]  
images = load_and_preprocess_images(image_names).to(device)

with torch.no_grad():
    with torch.cuda.amp.autocast(dtype=dtype):
        # Predict attributes including cameras, depth maps, and point maps.
        predictions = model(images)"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>vggt</span>.<span>models</span>.<span>vggt</span> <span>import</span> <span>VGGT</span>
<span>from</span> <span>vggt</span>.<span>utils</span>.<span>load_fn</span> <span>import</span> <span>load_and_preprocess_images</span>

<span>device</span> <span>=</span> <span>"cuda"</span> <span>if</span> <span>torch</span>.<span>cuda</span>.<span>is_available</span>() <span>else</span> <span>"cpu"</span>
<span># bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) </span>
<span>dtype</span> <span>=</span> <span>torch</span>.<span>bfloat16</span> <span>if</span> <span>torch</span>.<span>cuda</span>.<span>get_device_capability</span>()[<span>0</span>] <span>&gt;=</span> <span>8</span> <span>else</span> <span>torch</span>.<span>float16</span>

<span># Initialize the model and load the pretrained weights.</span>
<span># This will automatically download the model weights the first time it's run, which may take a while.</span>
<span>model</span> <span>=</span> <span>VGGT</span>.<span>from_pretrained</span>(<span>"facebook/VGGT-1B"</span>).<span>to</span>(<span>device</span>)

<span># Load and preprocess example images (replace with your own image paths)</span>
<span>image_names</span> <span>=</span> [<span>"path/to/imageA.png"</span>, <span>"path/to/imageB.png"</span>, <span>"path/to/imageC.png"</span>]  
<span>images</span> <span>=</span> <span>load_and_preprocess_images</span>(<span>image_names</span>).<span>to</span>(<span>device</span>)

<span>with</span> <span>torch</span>.<span>no_grad</span>():
    <span>with</span> <span>torch</span>.<span>cuda</span>.<span>amp</span>.<span>autocast</span>(<span>dtype</span><span>=</span><span>dtype</span>):
        <span># Predict attributes including cameras, depth maps, and point maps.</span>
        <span>predictions</span> <span>=</span> <span>model</span>(<span>images</span>)</pre></div>
<p dir="auto">The model weights will be automatically downloaded from Hugging Face. If you encounter issues such as slow loading, you can manually download them <a href="https://huggingface.co/facebook/VGGT-1B/blob/main/model.pt" rel="nofollow">here</a> and load, or:</p>
<div dir="auto" data-snippet-clipboard-copy-content="model = VGGT()
_URL = &quot;https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt&quot;
model.load_state_dict(torch.hub.load_state_dict_from_url(_URL))"><pre><span>model</span> <span>=</span> <span>VGGT</span>()
<span>_URL</span> <span>=</span> <span>"https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt"</span>
<span>model</span>.<span>load_state_dict</span>(<span>torch</span>.<span>hub</span>.<span>load_state_dict_from_url</span>(<span>_URL</span>))</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Detailed Usage</h2><a id="user-content-detailed-usage" aria-label="Permalink: Detailed Usage" href="#detailed-usage"></a></p>
<p dir="auto">You can also optionally choose which attributes (branches) to predict, as shown below. This achieves the same result as the example above. This example uses a batch size of 1 (processing a single scene), but it naturally works for multiple scenes.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from vggt.utils.pose_enc import pose_encoding_to_extri_intri
from vggt.utils.geometry import unproject_depth_map_to_point_map

with torch.no_grad():
    with torch.cuda.amp.autocast(dtype=dtype):
        images = images[None]  # add batch dimension
        aggregated_tokens_list, ps_idx = model.aggregator(images)
                
    # Predict Cameras
    pose_enc = model.camera_head(aggregated_tokens_list)[-1]
    # Extrinsic and intrinsic matrices, following OpenCV convention (camera from world)
    extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, images.shape[-2:])

    # Predict Depth Maps
    depth_map, depth_conf = model.depth_head(aggregated_tokens_list, images, ps_idx)

    # Predict Point Maps
    point_map, point_conf = model.point_head(aggregated_tokens_list, images, ps_idx)
        
    # Construct 3D Points from Depth Maps and Cameras
    # which usually leads to more accurate 3D points than point map branch
    point_map_by_unprojection = unproject_depth_map_to_point_map(depth_map.squeeze(0), 
                                                                extrinsic.squeeze(0), 
                                                                intrinsic.squeeze(0))

    # Predict Tracks
    # choose your own points to track, with shape (N, 2) for one scene
    query_points = torch.FloatTensor([[100.0, 200.0], 
                                        [60.72, 259.94]]).to(device)
    track_list, vis_score, conf_score = model.track_head(aggregated_tokens_list, images, ps_idx, query_points=query_points[None])"><pre><span>from</span> <span>vggt</span>.<span>utils</span>.<span>pose_enc</span> <span>import</span> <span>pose_encoding_to_extri_intri</span>
<span>from</span> <span>vggt</span>.<span>utils</span>.<span>geometry</span> <span>import</span> <span>unproject_depth_map_to_point_map</span>

<span>with</span> <span>torch</span>.<span>no_grad</span>():
    <span>with</span> <span>torch</span>.<span>cuda</span>.<span>amp</span>.<span>autocast</span>(<span>dtype</span><span>=</span><span>dtype</span>):
        <span>images</span> <span>=</span> <span>images</span>[<span>None</span>]  <span># add batch dimension</span>
        <span>aggregated_tokens_list</span>, <span>ps_idx</span> <span>=</span> <span>model</span>.<span>aggregator</span>(<span>images</span>)
                
    <span># Predict Cameras</span>
    <span>pose_enc</span> <span>=</span> <span>model</span>.<span>camera_head</span>(<span>aggregated_tokens_list</span>)[<span>-</span><span>1</span>]
    <span># Extrinsic and intrinsic matrices, following OpenCV convention (camera from world)</span>
    <span>extrinsic</span>, <span>intrinsic</span> <span>=</span> <span>pose_encoding_to_extri_intri</span>(<span>pose_enc</span>, <span>images</span>.<span>shape</span>[<span>-</span><span>2</span>:])

    <span># Predict Depth Maps</span>
    <span>depth_map</span>, <span>depth_conf</span> <span>=</span> <span>model</span>.<span>depth_head</span>(<span>aggregated_tokens_list</span>, <span>images</span>, <span>ps_idx</span>)

    <span># Predict Point Maps</span>
    <span>point_map</span>, <span>point_conf</span> <span>=</span> <span>model</span>.<span>point_head</span>(<span>aggregated_tokens_list</span>, <span>images</span>, <span>ps_idx</span>)
        
    <span># Construct 3D Points from Depth Maps and Cameras</span>
    <span># which usually leads to more accurate 3D points than point map branch</span>
    <span>point_map_by_unprojection</span> <span>=</span> <span>unproject_depth_map_to_point_map</span>(<span>depth_map</span>.<span>squeeze</span>(<span>0</span>), 
                                                                <span>extrinsic</span>.<span>squeeze</span>(<span>0</span>), 
                                                                <span>intrinsic</span>.<span>squeeze</span>(<span>0</span>))

    <span># Predict Tracks</span>
    <span># choose your own points to track, with shape (N, 2) for one scene</span>
    <span>query_points</span> <span>=</span> <span>torch</span>.<span>FloatTensor</span>([[<span>100.0</span>, <span>200.0</span>], 
                                        [<span>60.72</span>, <span>259.94</span>]]).<span>to</span>(<span>device</span>)
    <span>track_list</span>, <span>vis_score</span>, <span>conf_score</span> <span>=</span> <span>model</span>.<span>track_head</span>(<span>aggregated_tokens_list</span>, <span>images</span>, <span>ps_idx</span>, <span>query_points</span><span>=</span><span>query_points</span>[<span>None</span>])</pre></div>
<p dir="auto">Furthermore, if certain pixels in the input frames are unwanted (e.g., reflective surfaces, sky, or water), you can simply mask them by setting the corresponding pixel values to 0 or 1. Precise segmentation masks aren't necessary - simple bounding box masks work effectively (check this <a href="https://github.com/facebookresearch/vggt/issues/47" data-hovercard-type="issue" data-hovercard-url="/facebookresearch/vggt/issues/47/hovercard">issue</a> for an example).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Visualization</h2><a id="user-content-visualization" aria-label="Permalink: Visualization" href="#visualization"></a></p>
<p dir="auto">We provide multiple ways to visualize your 3D reconstructions and tracking results. Before using these visualization tools, install the required dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements_demo.txt"><pre>pip install -r requirements_demo.txt</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Interactive 3D Visualization</h3><a id="user-content-interactive-3d-visualization" aria-label="Permalink: Interactive 3D Visualization" href="#interactive-3d-visualization"></a></p>
<p dir="auto"><strong>Please note:</strong> VGGT typically reconstructs a scene in less than 1 second. However, visualizing 3D points may take tens of seconds due to third-party rendering, independent of VGGT's processing time. The visualization is slow especially when the number of images is large.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Gradio Web Interface</h4><a id="user-content-gradio-web-interface" aria-label="Permalink: Gradio Web Interface" href="#gradio-web-interface"></a></p>
<p dir="auto">Our Gradio-based interface allows you to upload images/videos, run reconstruction, and interactively explore the 3D scene in your browser. You can launch this in your local machine or try it on <a href="https://huggingface.co/spaces/facebook/vggt" rel="nofollow">Hugging Face</a>.</p>

<details>
<summary>Click to preview the Gradio interactive interface</summary>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/45f94454b15337d5f7b7b39344673976a1266a32e2096355f49311c396068d8c/68747470733a2f2f6a7974696d652e6769746875622e696f2f646174612f766767745f68665f64656d6f5f73637265656e2e706e67"><img src="https://camo.githubusercontent.com/45f94454b15337d5f7b7b39344673976a1266a32e2096355f49311c396068d8c/68747470733a2f2f6a7974696d652e6769746875622e696f2f646174612f766767745f68665f64656d6f5f73637265656e2e706e67" alt="Gradio Web Interface Preview" data-canonical-src="https://jytime.github.io/data/vggt_hf_demo_screen.png"></a></p>
</details>
<p dir="auto"><h4 tabindex="-1" dir="auto">Viser 3D Viewer</h4><a id="user-content-viser-3d-viewer" aria-label="Permalink: Viser 3D Viewer" href="#viser-3d-viewer"></a></p>
<p dir="auto">Run the following command to run reconstruction and visualize the point clouds in viser. Note this script requires a path to a folder containing images. It assumes only image files under the folder. You can set <code>--use_point_map</code> to use the point cloud from the point map branch, instead of the depth-based point cloud.</p>
<div dir="auto" data-snippet-clipboard-copy-content="python demo_viser.py --image_folder path/to/your/images/folder"><pre>python demo_viser.py --image_folder path/to/your/images/folder</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Track Visualization</h3><a id="user-content-track-visualization" aria-label="Permalink: Track Visualization" href="#track-visualization"></a></p>
<p dir="auto">To visualize point tracks across multiple images:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from vggt.utils.visual_track import visualize_tracks_on_images
track = track_list[-1]
visualize_tracks_on_images(images, track, (conf_score>0.2) &amp; (vis_score>0.2), out_dir=&quot;track_visuals&quot;)"><pre><span>from</span> <span>vggt</span>.<span>utils</span>.<span>visual_track</span> <span>import</span> <span>visualize_tracks_on_images</span>
<span>track</span> <span>=</span> <span>track_list</span>[<span>-</span><span>1</span>]
<span>visualize_tracks_on_images</span>(<span>images</span>, <span>track</span>, (<span>conf_score</span><span>&gt;</span><span>0.2</span>) <span>&amp;</span> (<span>vis_score</span><span>&gt;</span><span>0.2</span>), <span>out_dir</span><span>=</span><span>"track_visuals"</span>)</pre></div>
<p dir="auto">This plots the tracks on the images and saves them to the specified output directory.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Single-view Reconstruction</h2><a id="user-content-single-view-reconstruction" aria-label="Permalink: Single-view Reconstruction" href="#single-view-reconstruction"></a></p>
<p dir="auto">Our model shows surprisingly good performance on single-view reconstruction, although it was never trained for this task. The model does not need to duplicate the single-view image to a pair, instead, it can directly infer the 3D structure from the tokens of the single view image. Feel free to try it with our demos above, which naturally works for single-view reconstruction.</p>
<p dir="auto">We did not quantitatively test monocular depth estimation performance ourselves, but <a href="https://github.com/kabouzeid">@kabouzeid</a> generously provided a comparison of VGGT to recent methods <a href="https://github.com/facebookresearch/vggt/issues/36" data-hovercard-type="issue" data-hovercard-url="/facebookresearch/vggt/issues/36/hovercard">here</a>. VGGT shows competitive or better results compared to state-of-the-art monocular approaches such as DepthAnything v2 or MoGe, despite never being explicitly trained for single-view tasks.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Runtime and GPU Memory</h2><a id="user-content-runtime-and-gpu-memory" aria-label="Permalink: Runtime and GPU Memory" href="#runtime-and-gpu-memory"></a></p>
<p dir="auto">We benchmark the runtime and GPU memory usage of VGGT's aggregator on a single NVIDIA H100 GPU across various input sizes.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><strong>Input Frames</strong></th>
<th>1</th>
<th>2</th>
<th>4</th>
<th>8</th>
<th>10</th>
<th>20</th>
<th>50</th>
<th>100</th>
<th>200</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Time (s)</strong></td>
<td>0.04</td>
<td>0.05</td>
<td>0.07</td>
<td>0.11</td>
<td>0.14</td>
<td>0.31</td>
<td>1.04</td>
<td>3.12</td>
<td>8.75</td>
</tr>
<tr>
<td><strong>Memory (GB)</strong></td>
<td>1.88</td>
<td>2.07</td>
<td>2.45</td>
<td>3.23</td>
<td>3.63</td>
<td>5.58</td>
<td>11.41</td>
<td>21.15</td>
<td>40.63</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">Note that these results were obtained using Flash Attention 3, which is faster than the default Flash Attention 2 implementation while maintaining almost the same memory usage. Feel free to compile Flash Attention 3 from source to get better performance.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Research Progression</h2><a id="user-content-research-progression" aria-label="Permalink: Research Progression" href="#research-progression"></a></p>
<p dir="auto">Our work builds upon a series of previous research projects. If you're interested in understanding how our research evolved, check out our previous works:</p>
<markdown-accessiblity-table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<p dir="auto">Thanks to these great repositories: <a href="https://github.com/facebookresearch/PoseDiffusion">PoseDiffusion</a>, <a href="https://github.com/facebookresearch/vggsfm">VGGSfM</a>, <a href="https://github.com/facebookresearch/co-tracker">CoTracker</a>, <a href="https://github.com/facebookresearch/dinov2">DINOv2</a>, <a href="https://github.com/naver/dust3r">Dust3r</a>, <a href="https://github.com/microsoft/moge">Moge</a>, <a href="https://github.com/facebookresearch/pytorch3d">PyTorch3D</a>, <a href="https://github.com/xiongzhu666/Sky-Segmentation-and-Post-processing">Sky Segmentation</a>, <a href="https://github.com/DepthAnything/Depth-Anything-V2">Depth Anything V2</a>, <a href="https://github.com/YvanYin/Metric3D">Metric3D</a> and many other inspiring works in the community.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Checklist</h2><a id="user-content-checklist" aria-label="Permalink: Checklist" href="#checklist"></a></p>
<ul>
<li> Release the training code</li>
<li> Release VGGT-500M and VGGT-200M</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">See the <a href="https://github.com/facebookresearch/vggt/blob/main/LICENSE.txt">LICENSE</a> file for details about the license under which this code is made available.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[X’s director of engineering, Haofei Wang, has left the company (113 pts)]]></title>
            <link>https://www.theverge.com/twitter/634833/x-head-engineering-leaves-elon-musk</link>
            <guid>43470613</guid>
            <pubDate>Tue, 25 Mar 2025 12:54:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/twitter/634833/x-head-engineering-leaves-elon-musk">https://www.theverge.com/twitter/634833/x-head-engineering-leaves-elon-musk</a>, See on <a href="https://news.ycombinator.com/item?id=43470613">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><img alt="Kylie Robison" data-chromatic="ignore" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/197393/GMbiBVSaAAABxU1.0.jpeg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=48 1x, https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/197393/GMbiBVSaAAABxU1.0.jpeg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96 2x" src="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/197393/GMbiBVSaAAABxU1.0.jpeg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96"></p><p><a href="https://www.theverge.com/authors/kylie-robison">Kylie Robison</a> <span>is a senior AI reporter working with The Verge’s policy and tech teams. She previously worked at Fortune Magazine and Business Insider.</span></p></div><div id="zephr-anchor"><p>X’s director of engineering, Haofei Wang, has suddenly left the company, according to sources with knowledge of the matter.</p><p>Wang first joined Elon Musk’s X in July 2023 and has been an integral part of the company’s leadership, often serving as a conduit between Musk and the rest of the company’s engineers. More recently, he was seen internally as X’s defacto head of engineering and product, especially with Musk recently focusing more of his time on xAI and DOGE. It’s unclear why Wang is departing now<em>. </em>Neither he nor a company spokesperson responded to a request for comment in time for publication.</p><p>X recently added other engineering leadership: Mike Dalton and Uday Ruddaraju, both previously technical leads at Robinhood, joined in January. Their LinkedIn profiles show that they also work at xAI, which has become <a href="https://www.theverge.com/2025/1/10/24339249/elon-musk-xai-x-twitter">increasingly intertwined</a> with X over the last year.</p><div><div><p id="do-you-work-at-x-or-xai"><h2><strong>Do you work at X or xAI?</strong></h2></p></div><p> I’d love to chat. You can reach me securely on Signal @kylie.01 or via email at kylie@theverge.com.</p></div><p>Thanks to the growing profile of xAI and Musk’s newfound political influence, X’s business appears to be turning around. The company reportedly just <a href="https://www.ft.com/content/d4616dec-c4c7-417f-8549-134710bbc5b1">obtained a $44 billion valuation</a> from investors — the same price Musk paid for Twitter in 2022. While Musk remains an avid poster on X, his attention and leadership at X has become increasingly split since he started campaigning for President Donald Trump last summer.</p><p>When Musk first purchased Twitter, he vowed to transform it into an “everything app” akin to China’s WeChat. <em>The Verge</em> <a href="https://www.theverge.com/2023/10/26/23934216/x-twitter-bank-elon-musk-2024">previously reported an internal X meeting</a> where Musk said that it “would blow my mind” if the company couldn’t handle “someone’s entire financial life” by the end of 2024. Those plans haven’t come to fruition, though my sources say that work is still underway to launch the X Money payments platform (<a href="https://www.theverge.com/news/599137/x-money-payments-service-2025-launch">it’s slated to come</a> “later this year”).</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Coding Isn't Programming (148 pts)]]></title>
            <link>https://www.socallinuxexpo.org/scale/22x/presentations/closing-keynote-leslie-lamport</link>
            <guid>43469711</guid>
            <pubDate>Tue, 25 Mar 2025 10:34:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.socallinuxexpo.org/scale/22x/presentations/closing-keynote-leslie-lamport">https://www.socallinuxexpo.org/scale/22x/presentations/closing-keynote-leslie-lamport</a>, See on <a href="https://news.ycombinator.com/item?id=43469711">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
      <a id="main-content"></a>
                    
                                                        
<div>
      
  <p><span><div>
        <p><span>        <span><img typeof="foaf:Image" src="https://www.socallinuxexpo.org/sites/default/files/styles/square_thumbnail/public/speakers/Leslie_Lamport.jpg?itok=1UPhMyDa" width="180" height="180" alt=""></span>  </span>  </p>
    </div></span>  </p>  
  <div><p><a href="https://www.socallinuxexpo.org/scale/22x/speakers/leslie-lamport">Leslie Lamport</a></p></div>  
  <div>    <p><span>Audience: </span></p><div><p><a href="https://www.socallinuxexpo.org/scale/22x/audience-level/everyone">Everyone</a></p></div>  </div>  
  <div>    <p><span>Topic: </span></p><div><p><a href="https://www.socallinuxexpo.org/scale/22x/track/keynote">Keynote</a></p></div>  </div>  
  <div>        <p>Join us for a captivating closing keynote with the legendary Leslie Lamport, Turing Award winner and pioneer in the field of distributed computing. We'll discuss computing history, open source and distributed systems.&nbsp;</p>  </div>  
  <div>    <p><span>Presentation: </span></p><div><p><span><img alt="PDF icon" title="application/pdf" src="https://www.socallinuxexpo.org/modules/file/icons/application-pdf.png"> <a href="https://www.socallinuxexpo.org/sites/default/files/presentations/linux-expo%20%281%29.pdf" type="application/pdf; length=1401215">linux-expo (1).pdf</a></span></p></div>  </div>  
  <div>    <p><span>Room: </span></p><p>Ballroom DE</p>  </div>  
  <div>    <p><span>Time: </span></p><p><span>Sunday, March 9, 2025 - <span><span property="dc:date" datatype="xsd:dateTime" content="2025-03-09T15:00:00-07:00">15:00</span> to <span property="dc:date" datatype="xsd:dateTime" content="2025-03-09T16:00:00-07:00">16:00</span></span></span></p>  </div>  
  <div>    <p><span>Audio/Video: </span></p><p><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/tsSDvflzJbc?si=2LSe2NTfzfKwXNWO" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>  </div>  </div>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Great Barefoot Running Hysteria of 2010 (180 pts)]]></title>
            <link>https://runningshoescore.com/blog/barefoot-running-hysteria-of-2010</link>
            <guid>43469690</guid>
            <pubDate>Tue, 25 Mar 2025 10:28:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://runningshoescore.com/blog/barefoot-running-hysteria-of-2010">https://runningshoescore.com/blog/barefoot-running-hysteria-of-2010</a>, See on <a href="https://news.ycombinator.com/item?id=43469690">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <p>
03/25/25 • 9 minute read
    </p>
<p>The year was 2010.  “Ke$ha's Tik Tok”  was topping the Billboard charts.  Steve Jobs has just introduced a goofy new oversized iPhone called an “iPad”.  And in running forums across the internet far and wide, hoards of enthusiasts preached the gospel of a new way of running: without shoes.</p><figure><img src="https://api.needto.run/content/images/2024/01/barefoot.png" alt="A graph of Google search interest in barefoot running showing a huge spike around 2010, which trails off by 2013" loading="lazy" width="1136" height="580" srcset="https://api.needto.run/content/images/size/w600/2024/01/barefoot.png 600w, https://api.needto.run/content/images/size/w1000/2024/01/barefoot.png 1000w, https://api.needto.run/content/images/2024/01/barefoot.png 1136w" sizes="(min-width: 720px) 720px"><figcaption><span>Google Trends interest in "Barefoot Running" from 2004 to present</span></figcaption></figure><p>The “Great Barefoot Running Hysteria of 2010”, as I call it, took the amateur running world by storm.  Propelled by dramatic claims of performance improvements and injury prevention, barefoot running gave rise to a vocal (and often militant) contingent of enthusiasts and entirely new classes of footwear.  And then over the course of a several years, it faded away almost as quickly as it came, leaving behind changes in running shoes and culture forever.  In this post, we'll explore the history and legacy of the barefoot running movement.</p><h3 id="the-barefoot-running-movement-how-it-started">The Barefoot Running Movement &amp; How It Started</h3><p>Barefoot running is–of course–as old as humanity itself. In fact, people have run barefoot throughout most of human history, with the practice continuing today in several cultures, such as Kenya and indigenous peoples in Mexico.  </p><p>In this sense, before addressing the modern origins of <em>barefoot </em>running, we need to talk about the origins of <em>shod </em>running<em>.</em>  The concept of running shoes as we understand them today, specifically designed to improve running efficiency and comfort, did not emerge until the late 19th and early 20th centuries. This evolution coincided with the rise of organized sports and recreational running, which spurred the development of footwear tailored to the specific needs of runners. </p><figure><img src="https://api.needto.run/content/images/2024/05/lord-spencers-shoes-the-first-specialized-running-shoes-ever-made-from-1865-2.jpg" alt="" loading="lazy" width="460" height="347"></figure><p>The first breakthrough came in 1865 when an English shoemaker brilliantly suggested adding spikes to otherwise normal-looking dress shoes to make them suitable for cross-country running. Later, in the early 20th century, shoes with rubber soles were introduced, offering improved grip and shock absorption, marking a significant advancement in the design of running shoes.</p><blockquote>The result of these running shoe trends was running shoe stores filled with bulky, over-engineered clown shoes that promised to prevent running injuries, but probably didn't.</blockquote><p>Into more modern times, running shoes continued to evolve past their humble beginnings into heavier, more complex footwear with exotic materials like plastics &amp; EVA foam.  In the 80s and 90s, many shoemakers became fixated on <em>stability </em>and the notion that pronation, the natural roll of the foot after it lands, was a cause for running injuries.  Running shoes were increasingly designed to try to prevent this movement with wedges of foam that support and stabilize the arch of the foot, a high heel-to-toe drop, and other questionable features like plastic air bubbles in the heel to cushion the foot with each step.</p><p>The result of these running shoe trends was running shoe stores filled with bulky, over-engineered clown shoes that promised to prevent running injuries, but probably didn't, and were mostly just uncomfortable to use. You can see an example of this kind of design in action in the Nike Air Max 90, which is now also available in a more tasteful re-issue.  While the original was marketed as a running shoe, the modern re-issue is a nostalgic fashion shoe that you should definitely <em>not </em>run in.</p><figure><img src="https://api.needto.run/content/images/2024/03/geqhdlqounif1d4bvgz2.jpg" alt="A bulky black leather running shoe from the 90s with an air bubble in the heel" loading="lazy" width="670" height="464" srcset="https://api.needto.run/content/images/size/w600/2024/03/geqhdlqounif1d4bvgz2.jpg 600w, https://api.needto.run/content/images/2024/03/geqhdlqounif1d4bvgz2.jpg 670w"><figcaption><span>The original Nike Air Max 90</span></figcaption></figure><figure><img src="https://api.needto.run/content/images/2024/03/air-max-90-mens-shoes-Bd2qnn.png-copy.png" alt="A blue &amp; whit modern re-issue of the Air Max 90 running shoe with an air bubble in the heel" loading="lazy" width="1580" height="877" srcset="https://api.needto.run/content/images/size/w600/2024/03/air-max-90-mens-shoes-Bd2qnn.png-copy.png 600w, https://api.needto.run/content/images/size/w1000/2024/03/air-max-90-mens-shoes-Bd2qnn.png-copy.png 1000w, https://api.needto.run/content/images/2024/03/air-max-90-mens-shoes-Bd2qnn.png-copy.png 1580w" sizes="(min-width: 720px) 720px"><figcaption><span>A modern re-issue of the Nike Air Max 90 (from </span><a href="https://nike.com/?ref=api.needto.run" rel="noreferrer"><span>Nike.com</span></a><span>)</span></figcaption></figure><p>The barefoot running renaissance can thus be understood as a backlash to the dominant running shoe trends at the time.  The revival was driven by a growing dissatisfaction with traditional running shoes, which many believed contributed to injuries and impeded natural foot movement. Proponents of barefoot running argued that it encouraged a more natural gait, reducing the impact on the legs and back and enhancing the overall running experience.</p><p>In 2004, inspired by Stanford athletes training barefoot, Nike introduced the <em>Nike Free</em>, a running shoe that bucked the bulky running shoe trends of the time with a super flexible sole and a minimal heel-to-toe offset.</p><figure><img src="https://api.needto.run/content/images/2025/03/Nike_Free-_3_running_shoe.jpg" alt="A picture of blue Nike running shoes with a flexible sole and a small height difference between the toe and heel" loading="lazy" width="400" height="280"><figcaption><span>The original Nike Free (via Wikipedia)</span></figcaption></figure><p>Then, in 2006, Vibram launched the 5-Fingers: a shoe intended to be as close to barefoot as possible, with a thin rubber sole and glove-like fit for the toes.  This was the shoe that truly ushered in the <em>minimalist </em>mindset: running shoes should enable a "natural" gait and that <em>less is more</em>. </p><figure><img src="https://api.needto.run/content/images/2024/03/vibram.jpg" alt="" loading="lazy" width="1101" height="695" srcset="https://api.needto.run/content/images/size/w600/2024/03/vibram.jpg 600w, https://api.needto.run/content/images/size/w1000/2024/03/vibram.jpg 1000w, https://api.needto.run/content/images/2024/03/vibram.jpg 1101w" sizes="(min-width: 720px) 720px"><figcaption><span>Vibram 5 Fingers (via Amazon)</span></figcaption></figure><p>The barefoot-inspired <em>Nike Free </em>and <em>Vibram 5 Fingers</em> set the stage, but the real barefoot running revolution began not with a pair of shoes, but with a book about an indigenous tribe of Native Mexicans.</p><h3 id="christopher-mcdougalls-born-to-run">Christopher McDougall's "Born to Run"</h3><p>While the over-engineered running shoe backlash had already started with shoes like the Nike Free and the Vibram 5 Fingers, the spark that lit the barefoot running powder keg was Christopher McDougall's 2009 bestseller, "Born to Run". The book, which explores the running habits of the Tarahumara Native Mexican tribe, known for their long-distance running ability, captivated the imagination of runners–and non-runners–everywhere. </p><figure><img src="https://api.needto.run/content/images/2024/01/51-Ry-ireXL._SL500_.jpg" alt="A book cover for &quot;Born to Run&quot; featuring an illustration of a barefoot and a man running" loading="lazy" width="500" height="500"><figcaption><span>Book cover for "Born to Run" by Christopher McDougall</span></figcaption></figure><p>The Tarahumara, or Rarámuri, as they refer to themselves, are an indigenous people who reside in the rugged and remote Copper Canyon region of Northwestern Mexico. Renowned for their extraordinary long-distance running abilities, the Tarahumara have garnered international attention and admiration from runners and researchers alike. Their running habits, deeply embedded in their culture and lifestyle, are not merely for sport but serve practical and ceremonial purposes as well.</p><p>McDougall's narrative suggested that modern running injuries were virtually non-existent among the Tarahumara.  They often ran barefoot or in minimal footwear, sandals crafted from leather and tire strips called <em>huaraches.  </em>The sandals provide minimal cushioning and protection, promoting a natural running form that many attribute to their low incidence of running-related injuries according to McDougall.  </p><p>The popularity of McDougall's book sparked widespread curiosity and enthusiasm for barefoot running, and many readers happily ditched their bulky running shoes for new, minimalist alternatives.  As the barefoot running movement evolved, so did the market for running footwear, leading to the development of minimalist shoes designed to mimic the barefoot running experience while providing some protection from the hazards of rough terrain. </p><p>Bolstered by the popularity of "Born to Run", barefoot running was suddenly <em>everywhere.  </em>This period saw a surge in barefoot running clinics, forums, and social media groups where enthusiasts shared tips and experiences.  </p><p>And as often happens, what started as a perfectly reasonable idea took on a life of its own and became dogmatic: barefoot running was <em>the way to run</em>.  The movement gave rise to a series of increasingly lofty and strongly worded claims: barefoot running prevented injuries; barefoot running was more efficient; heel striking was evil; barefoot running was the natural and therefore "correct" way to run.  The idea transcended running communities and seeped into popular culture prompting lofty headlines like this New York Times article, <a href="https://www.nytimes.com/2011/11/06/magazine/running-christopher-mcdougall.html?pagewanted=1&amp;_r=1&amp;ref=api.needto.run">The Once and Future Way to Run</a>.</p><p>The idea even transcended the sport of running itself.  To many, the barefoot running movement was not merely about the act of running without shoes; it represented a broader philosophy seeking to embrace simplicity, natural form, and mindfulness in the pursuit of physical fitness and well-being.  The fervor around barefoot running bordered on religious.</p><p>Despite the smug sense of superiority that some barefoot proponents projected, many of the most enthusiastic adopters of barefoot running were, in fact, novice and inexperienced runners.  While barefoot running enthusiasts were eager to point out elite runners training or racing barefoot, like <a href="https://en.wikipedia.org/wiki/Zola_Budd?ref=api.needto.run">Zola Budd</a>, these barefoot elites are the exception rather than the rule.  Most serious and elite runners generally sat out the barefoot trend, or at least took a more nuanced approach.  Many advanced runners already gravitated towards less bulky shoes, and many that did adopt barefoot running did so as part of isolated workouts on soft surfaces.</p><p>It's difficult to quantify this claim, but my personal memory of this period was that online discourse around running form and footwear was dominated by an aggressive mob mentality around barefoot running.   If you were running with shoes on, you were–according to the online mob–"doin' it wrong", as the internet was fond of saying at the time.  </p><p>Though it generally didn't represent the opinions of more experienced runners, the barefoot running crowd was certainly the loudest. Opinions that didn't completely jive with the all-minimalist approach were often brutally and violently downvoted.</p><p>Despite the enthusiasm and claims of the benefits of barefoot running, research into the practice was thin.  While some of the claims seemed logical, most of the evidence around the benefits of barefoot running was anecdotal.  Much of the argument hinged around barefoot running being the <em>natural </em>and thus correct way to run, and that running with over-engineered running shoes was <em>unnatural</em> and thus incorrect.</p><p>Some will recognize this line of reasoning as the <a href="https://en.wikipedia.org/wiki/Appeal_to_nature?ref=api.needto.run#Examples"><em>appeal to nature fallacy</em></a>: a logical fallacy in which a subject is claimed to be <em>good </em>simply because it is <em>natural</em>.  The fallacy pops up frequently in health &amp; medical settings when people extol the virtues of "all-natural" products or alternative medicines.  Sure, some natural products are healthy and beneficial; but so are a lot of deadly poisons.  Similarly, diet, where the appeal to nature is used to justify all sorts of sometimes conflicting food choices (throughout history humans have eaten wildly versatile diets).  This doesn't mean that natural is <em>bad, </em>it simply means that it's not a valid argument in and of itself for the benefits of barefoot running.</p><p>As time went on and the research caught up with the trend, the results were mixed at best: some studies showed potential benefits, while others highlighted increased risks. Critics of barefoot running point to evidence showing an increased incidence of certain types of injuries, such as Achilles tendinitis and metatarsal stress fractures, among runners transitioning to barefoot or minimalist running without proper adaptation. Additionally, there's an obvious concern about the lack of protection from environmental hazards (e.g., sharp objects, rough terrain) when running without traditional footwear, which can lead to acute injuries. </p><p>At the same time, many fledgling barefoot runners soon faced a reality check. Reports of injuries began to surface, casting doubt on the benefits of running without shoes. Podiatrists and sports medicine professionals started warning about the potential risks, especially for those with pre-existing foot conditions, those who transitioned too quickly, or those who tried to run too much. </p><p>Many who stuck with the sport and aspired to run longer distances such as the marathon soon learned that it's simply difficult to put in the mileage required to excel at these distances without proper footwear.  Though you're sure to find counterexamples (I still see a small handful of barefoot runners at major running events like the Boston Marathon), most runners who are putting in 50, 60, 70, or more miles a week to race a marathon are doing so with shoes designed to help cushion the impact.</p><h2 id="long-lasting-changes-to-running-shoes">Long-Lasting Changes to Running Shoes</h2><p>Despite the decline in barefoot running's popularity, its impact on the running shoe industry was undeniable. Recognizing the demand for a more natural running experience, shoe manufacturers began developing more lightweight, minimalist running shoes with less cushioning and a lower heel-to-toe drop. </p><p>More importantly, the minimalist movement helped end the dominance of needlessly overbuilt running shoes.  Of course, highly supportive &amp; motion control running shoes are still available, and some runners prefer to run in them, but it's no longer the dominant paradigm.</p><p>Ironically, the minimalist shoe movement triggered its own backlash with the <em>maximalist </em>shoe movement of mega-cushioned shoes, ushered in by brands like HOKA.  But the maximalist trend is not a return to the <em>overbuilt </em>shoes of the 90s–in fact, it still incorporates minimalist concepts like lower heel-to-toe drop, lightweight materials, and placing less emphasis on the support and motion control that earlier running shoes relied on.  </p><p>These innovations aimed to combine the benefits of barefoot running with the protection and support of traditional running shoes. Today, many of these features remain integral in modern running shoe design, marking a lasting legacy of the barefoot running movement.</p><p>So in the end, while the barefoot running hysteria of 2010 may have been a short-lived trend, it sparked crucial conversations about running health and led to significant advancements in running footwear. Though no longer in the limelight, its impact continues to influence how we run and think about our running gear.</p>
  </div></div>]]></description>
        </item>
    </channel>
</rss>