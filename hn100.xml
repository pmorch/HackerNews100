<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 21 Jan 2025 12:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Startup Winter: Hacker News Lost Its Faith (245 pts)]]></title>
            <link>https://www.vincentschmalbach.com/startup-winter-hacker-news-lost-its-faith/</link>
            <guid>42778266</guid>
            <pubDate>Tue, 21 Jan 2025 10:00:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vincentschmalbach.com/startup-winter-hacker-news-lost-its-faith/">https://www.vincentschmalbach.com/startup-winter-hacker-news-lost-its-faith/</a>, See on <a href="https://news.ycombinator.com/item?id=42778266">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <p>In 2013, a failing founder posted their story on Hacker News (<a href="https://news.ycombinator.com/item?id=5255209">link</a>). The responses were overwhelmingly supportive: "Failure is just an event, not who you are." "Get back up and try again!" "This is valuable experience for next time."</p>
<p>Fast forward to 2025. Another founder shares their journey of six failed attempts (<a href="https://news.ycombinator.com/item?id=42771676">link</a>). The sentiment in the comments is strikingly different: "Would have been better to work at BigTech." "The rat race isn't worth it." "Most interesting stories remain buried while we're presented with a somewhat skewed reality."</p>
<p>This shift isn't isolated to these two posts. The same forum that championed "fail fast, fail often" now regularly questions whether the startup path makes sense at all.</p>
<p>What's changed?</p>
<ol>
<li>
<p>The human cost has become more visible. Stories of burnout, failed relationships, and mental health struggles are no longer swept under the rug of "hustle culture."</p>
</li>
<li>
<p>Big Tech compensation has transformed the risk-reward equation. When senior engineers can make $300K+ at established companies, the financial argument for startups becomes harder to justify.</p>
</li>
<li>
<p>The VC model's limitations have become apparent. The focus on hypergrowth and exits has left many founders feeling trapped between authentic business building and investor expectations.</p>
</li>
<li>
<p>The industry has matured. The low-hanging fruit of the mobile/web era has largely been picked, making truly innovative opportunities harder to find.</p>
</li>
</ol>
<p>I believe we're entering what might be called a "Startup Winter" - not because startups have stopped being created, but because the mythology around them has frozen over.</p>
<p>What might emerge from this winter could be a startup ecosystem that's less glamorous but more authentic. One where alternative paths to innovation are celebrated alongside the traditional VC-backed route.</p>

<!-- #comments -->

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta Censoring '#Democrat' on Instagram (141 pts)]]></title>
            <link>https://mstdn.chrisalemany.ca/@chris/113864600222476627</link>
            <guid>42777938</guid>
            <pubDate>Tue, 21 Jan 2025 09:08:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mstdn.chrisalemany.ca/@chris/113864600222476627">https://mstdn.chrisalemany.ca/@chris/113864600222476627</a>, See on <a href="https://news.ycombinator.com/item?id=42777938">Hacker News</a></p>
Couldn't get https://mstdn.chrisalemany.ca/@chris/113864600222476627: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[It sure looks like Meta stole a lot of books to build its AI (134 pts)]]></title>
            <link>https://lithub.com/it-sure-looks-like-meta-stole-a-lot-of-books-to-build-its-ai/</link>
            <guid>42775545</guid>
            <pubDate>Tue, 21 Jan 2025 01:59:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lithub.com/it-sure-looks-like-meta-stole-a-lot-of-books-to-build-its-ai/">https://lithub.com/it-sure-looks-like-meta-stole-a-lot-of-books-to-build-its-ai/</a>, See on <a href="https://news.ycombinator.com/item?id=42775545">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
				
				
				
				<p>It‚Äôs a grim week for Meta. The company formerly known as Facebook, and before that <a href="https://en.wikipedia.org/wiki/History_of_Facebook" target="_blank">Facemash</a>, ‚Äúdesigned to evaluate the attractiveness of female Harvard students,‚Äù now encompasses Facebook, Instagram, Threads, WhatsApp, and Meta, the failed vision for a remote workplace, fun-zone, and Zucker-verse where <a href="https://x.com/MetaHorizon/status/1579947568372404226" target="_blank">legs are always just around the corner.</a></p>
<p>CEO and founder Mark Zuckerberg announced that <a href="https://theintercept.com/2025/01/09/facebook-instagram-meta-hate-speech-content-moderation/" target="_blank">slurs are okay</a> on their platforms, added <a href="https://apnews.com/article/meta-facebook-zuckerberg-board-members-dana-white-199436c62c934ebb751b564f874ad2f6" target="_blank">a pro-Trump UFC boss to their board</a>, and made <a href="https://www.axios.com/2025/01/10/mark-zuckerberg-joe-rogan-facebook-censorship-biden" target="_blank">appearances in the aggrieved weirdo media world</a> to make some convoluted case that we need more masculine energy in business, more resentment overall, and more fealty to Don Trump. Zuckerberg has also recently switched up his personal style so that he now looks like he‚Äôs perpetually in a sitcom flashback where an older actor is unconvincingly costumed to look like their younger self.</p>
<p>And in the Northern District of California,&nbsp;<em>Wired&nbsp;</em>reports, <a href="https://www.wired.com/story/new-documents-unredacted-meta-copyright-ai-lawsuit/" target="_blank">recently unredacted court documents reveal</a> that Meta used a database of pirated books to train its AI systems<i>. </i>These documents were unsealed as part of a copyright lawsuit, one of the earliest of <a href="https://www.wired.com/story/ai-copyright-case-tracker/" target="_blank">many similar cases</a>, called <i>Kadrey et al. v. Meta Platforms.</i> The plaintiffs in this case are a number of writers and performers, including Richard Kadrey, Christopher Golden, Junot Diaz, Laura Lippman, Sarah Silverman, Ta-Nehisi Coates, and‚Äîjump scare!‚ÄîMike Huckabee.</p>
<p>The <a href="https://www.wired.com/story/new-documents-unredacted-meta-copyright-ai-lawsuit/" target="_blank">new documents quote</a> Meta employees frankly admitting to using stolen stuff from a notorious piracy site:</p>
<p>‚Ä¶an internal quote from a Meta employee, included in the documents, in which they speculated, ‚ÄúIf there is media coverage suggesting we have used a dataset we know to be pirated, such as LibGen, this may undermine our negotiating position with regulators on these issues.‚Äù‚Ä¶</p>
<p>‚Ä¶These newly unredacted documents reveal exchanges between Meta employees unearthed in the discovery process, like a Meta engineer telling a colleague that they hesitated to access LibGen data because ‚Äútorrenting from a [meta-owned] corporate laptop doesn‚Äôt feel right üòÉ‚Äù. They also allege that internal discussions about using LibGen data were escalated to Meta CEO Mark Zuckerberg (referred to as ‚ÄúMZ‚Äù in the memo handed over during discovery) and that Meta‚Äôs AI team was ‚Äúapproved to use‚Äù the pirated material.</p>
<p>Meta has claimed that they used publicly available material that was legally accessible under fair use doctrine, but that doesn‚Äôt pass the smell test to me: just because something is public on the internet, doesn‚Äôt make it legal.</p>
<p>The plaintiffs are arguing that they should be allowed to expand their case <a href="https://www.wired.com/story/new-documents-unredacted-meta-copyright-ai-lawsuit/" target="_blank">to incorporate these new findings</a>:</p>
<p>‚ÄúMeta, through a corporate representative who testified on November 20, 2024, has now admitted under oath to uploading (aka ‚Äòseeding‚Äô) pirated files containing Plaintiffs‚Äô works on ‚Äòtorrent‚Äô sites,‚Äù the motion alleges. (Seeding is when torrented files are then shared with other peers after they have finished downloading.)</p>
<p>‚ÄúThis torrenting activity turned Meta itself into a distributor of the very same pirated copyrighted material that it was also downloading for use in its commercially available AI models.‚Äù</p>
<p>Legally, Meta and their lawyers may find a way to finagle the law and get around this. But in plain terms, it doesn‚Äôt seem defensible for a major company with tons of lawyers, money, and talent to knowingly use stolen work to build something that they then turn around and sell.</p>
<p>I‚Äôm not naive enough to think that this lawsuit, or any of the many others currently winding their way through the courts, will end in this kind of software leaving the market‚Äîin America, you can‚Äôt unring a bell that‚Äôs been valued in the billions. But I do hope that the writers and artists whose work was stolen are compensated.</p>
<p>In spite of all this, tech-optimists continue to push AI in more places, and people in power continue to trumpet it as the future of everything. In the case of publishing, for example, the excellent <a href="https://www.instagram.com/xoxopublishinggg/#" target="_blank">xoxopublishinggg</a> Instagram account has been posting anonymous responses about publishing workers‚Äô experiences with AI in the workplace‚Äîit seems like a lot of publishers are at least curious about these tools in ways that don‚Äôt bode well for an AI-less future.</p>
<p>If you‚Äôre considering using AI, or are feeling pressure at work to do so, you can add ‚Äúbuilt on piracy‚Äù to the list of concerns about this tech, alongside its environmental impact, its human toll on underpaid and marginalized workers, and the simple fact that it is incapable of making anything good.</p>
				
										
									
				

				

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ruff: Python linter and code formatter written in Rust (176 pts)]]></title>
            <link>https://github.com/astral-sh/ruff</link>
            <guid>42775029</guid>
            <pubDate>Tue, 21 Jan 2025 00:49:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/astral-sh/ruff">https://github.com/astral-sh/ruff</a>, See on <a href="https://news.ycombinator.com/item?id=42775029">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><h2 tabindex="-1" dir="auto">Ruff</h2><a id="user-content-ruff" aria-label="Permalink: Ruff" href="#ruff"></a></p>
<p dir="auto"><a href="https://github.com/astral-sh/ruff"><img src="https://camo.githubusercontent.com/051a04ae958f4a1a5d6444df4cdc520305eef93d5028e6d4c7cd16efa3136cd4/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e743f75726c3d68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f61737472616c2d73682f727566662f6d61696e2f6173736574732f62616467652f76322e6a736f6e" alt="Ruff" data-canonical-src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json"></a>
<a href="https://pypi.python.org/pypi/ruff" rel="nofollow"><img src="https://camo.githubusercontent.com/14e1bc70770d22cc586d31fd726ffce6dbf5d248e5fbc700216542adfd6a4e07/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f727566662e737667" alt="image" data-canonical-src="https://img.shields.io/pypi/v/ruff.svg"></a>
<a href="https://github.com/astral-sh/ruff/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/05d0bcb2b2007f0c40a1a3d3eb693e9eec4c3d85aaea9cde6d463c3c3d89629c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f727566662e737667" alt="image" data-canonical-src="https://img.shields.io/pypi/l/ruff.svg"></a>
<a href="https://pypi.python.org/pypi/ruff" rel="nofollow"><img src="https://camo.githubusercontent.com/4756572c4e7149f6aa63e5a2c872c021eedfaa361c6a322d306bdd6b0a5c62d9/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f727566662e737667" alt="image" data-canonical-src="https://img.shields.io/pypi/pyversions/ruff.svg"></a>
<a href="https://github.com/astral-sh/ruff/actions"><img src="https://github.com/astral-sh/ruff/workflows/CI/badge.svg" alt="Actions status"></a>
<a href="https://discord.com/invite/astral-sh" rel="nofollow"><img src="https://camo.githubusercontent.com/647359c14a78a007576a41e446f1956e89ed1a91f673dfe19b848eebd94f502d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d2532333538363546322e7376673f6c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465" alt="Discord" data-canonical-src="https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;logoColor=white"></a></p>
<p dir="auto"><a href="https://docs.astral.sh/ruff/" rel="nofollow"><strong>Docs</strong></a> | <a href="https://play.ruff.rs/" rel="nofollow"><strong>Playground</strong></a></p>
<p dir="auto">An extremely fast Python linter and code formatter, written in Rust.</p>
<p dir="auto">
  <themed-picture data-catalyst-inline="true"><picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://user-images.githubusercontent.com/1309177/232603514-c95e9b0f-6b31-43de-9a80-9e844173fd6a.svg">
    <source media="(prefers-color-scheme: light)" srcset="https://user-images.githubusercontent.com/1309177/232603516-4fb4892d-585c-4b20-b810-3db9161831e4.svg">
    <img alt="Shows a bar chart with benchmark results." src="https://user-images.githubusercontent.com/1309177/232603516-4fb4892d-585c-4b20-b810-3db9161831e4.svg">
  </picture></themed-picture>
</p>
<p dir="auto">
  <i>Linting the CPython codebase from scratch.</i>
</p>
<ul dir="auto">
<li>‚ö°Ô∏è 10-100x faster than existing linters (like Flake8) and formatters (like Black)</li>
<li>üêç Installable via <code>pip</code></li>
<li>üõ†Ô∏è <code>pyproject.toml</code> support</li>
<li>ü§ù Python 3.13 compatibility</li>
<li>‚öñÔ∏è Drop-in parity with <a href="https://docs.astral.sh/ruff/faq/#how-does-ruffs-linter-compare-to-flake8" rel="nofollow">Flake8</a>, isort, and <a href="https://docs.astral.sh/ruff/faq/#how-does-ruffs-formatter-compare-to-black" rel="nofollow">Black</a></li>
<li>üì¶ Built-in caching, to avoid re-analyzing unchanged files</li>
<li>üîß Fix support, for automatic error correction (e.g., automatically remove unused imports)</li>
<li>üìè Over <a href="https://docs.astral.sh/ruff/rules/" rel="nofollow">800 built-in rules</a>, with native re-implementations
of popular Flake8 plugins, like flake8-bugbear</li>
<li>‚å®Ô∏è First-party <a href="https://docs.astral.sh/ruff/integrations/" rel="nofollow">editor integrations</a> for
<a href="https://github.com/astral-sh/ruff-vscode">VS Code</a> and <a href="https://docs.astral.sh/ruff/editors/setup" rel="nofollow">more</a></li>
<li>üåé Monorepo-friendly, with <a href="https://docs.astral.sh/ruff/configuration/#config-file-discovery" rel="nofollow">hierarchical and cascading configuration</a></li>
</ul>
<p dir="auto">Ruff aims to be orders of magnitude faster than alternative tools while integrating more
functionality behind a single, common interface.</p>
<p dir="auto">Ruff can be used to replace <a href="https://pypi.org/project/flake8/" rel="nofollow">Flake8</a> (plus dozens of plugins),
<a href="https://github.com/psf/black">Black</a>, <a href="https://pypi.org/project/isort/" rel="nofollow">isort</a>,
<a href="https://pypi.org/project/pydocstyle/" rel="nofollow">pydocstyle</a>, <a href="https://pypi.org/project/pyupgrade/" rel="nofollow">pyupgrade</a>,
<a href="https://pypi.org/project/autoflake/" rel="nofollow">autoflake</a>, and more, all while executing tens or hundreds of
times faster than any individual tool.</p>
<p dir="auto">Ruff is extremely actively developed and used in major open-source projects like:</p>
<ul dir="auto">
<li><a href="https://github.com/apache/airflow">Apache Airflow</a></li>
<li><a href="https://github.com/apache/superset">Apache Superset</a></li>
<li><a href="https://github.com/tiangolo/fastapi">FastAPI</a></li>
<li><a href="https://github.com/huggingface/transformers">Hugging Face</a></li>
<li><a href="https://github.com/pandas-dev/pandas">Pandas</a></li>
<li><a href="https://github.com/scipy/scipy">SciPy</a></li>
</ul>
<p dir="auto">...and <a href="#whos-using-ruff">many more</a>.</p>
<p dir="auto">Ruff is backed by <a href="https://astral.sh/" rel="nofollow">Astral</a>. Read the <a href="https://astral.sh/blog/announcing-astral-the-company-behind-ruff" rel="nofollow">launch post</a>,
or the original <a href="https://notes.crmarsh.com/python-tooling-could-be-much-much-faster" rel="nofollow">project announcement</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Testimonials</h2><a id="user-content-testimonials" aria-label="Permalink: Testimonials" href="#testimonials"></a></p>
<p dir="auto"><a href="https://twitter.com/tiangolo/status/1591912354882764802" rel="nofollow"><strong>Sebasti√°n Ram√≠rez</strong></a>, creator
of <a href="https://github.com/tiangolo/fastapi">FastAPI</a>:</p>
<blockquote>
<p dir="auto">Ruff is so fast that sometimes I add an intentional bug in the code just to confirm it's actually
running and checking the code.</p>
</blockquote>
<p dir="auto"><a href="https://twitter.com/schrockn/status/1612615862904827904" rel="nofollow"><strong>Nick Schrock</strong></a>, founder of <a href="https://www.elementl.com/" rel="nofollow">Elementl</a>,
co-creator of <a href="https://graphql.org/" rel="nofollow">GraphQL</a>:</p>
<blockquote>
<p dir="auto">Why is Ruff a gamechanger? Primarily because it is nearly 1000x faster. Literally. Not a typo. On
our largest module (dagster itself, 250k LOC) pylint takes about 2.5 minutes, parallelized across 4
cores on my M1. Running ruff against our <em>entire</em> codebase takes .4 seconds.</p>
</blockquote>
<p dir="auto"><a href="https://github.com/bokeh/bokeh/pull/12605" data-hovercard-type="pull_request" data-hovercard-url="/bokeh/bokeh/pull/12605/hovercard"><strong>Bryan Van de Ven</strong></a>, co-creator
of <a href="https://github.com/bokeh/bokeh/">Bokeh</a>, original author
of <a href="https://docs.conda.io/en/latest/" rel="nofollow">Conda</a>:</p>
<blockquote>
<p dir="auto">Ruff is ~150-200x faster than flake8 on my machine, scanning the whole repo takes ~0.2s instead of
~20s. This is an enormous quality of life improvement for local dev. It's fast enough that I added
it as an actual commit hook, which is terrific.</p>
</blockquote>
<p dir="auto"><a href="https://twitter.com/timothycrosley/status/1606420868514877440" rel="nofollow"><strong>Timothy Crosley</strong></a>,
creator of <a href="https://github.com/PyCQA/isort">isort</a>:</p>
<blockquote>
<p dir="auto">Just switched my first project to Ruff. Only one downside so far: it's so fast I couldn't believe
it was working till I intentionally introduced some errors.</p>
</blockquote>
<p dir="auto"><a href="https://github.com/astral-sh/ruff/issues/465#issuecomment-1317400028" data-hovercard-type="issue" data-hovercard-url="/astral-sh/ruff/issues/465/hovercard"><strong>Tim Abbott</strong></a>, lead
developer of <a href="https://github.com/zulip/zulip">Zulip</a>:</p>
<blockquote>
<p dir="auto">This is just ridiculously fast... <code>ruff</code> is amazing.</p>
</blockquote>

<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<p dir="auto">For more, see the <a href="https://docs.astral.sh/ruff/" rel="nofollow">documentation</a>.</p>
<ol dir="auto">
<li><a href="#getting-started">Getting Started</a></li>
<li><a href="#configuration">Configuration</a></li>
<li><a href="#rules">Rules</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#support">Support</a></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
<li><a href="#whos-using-ruff">Who's Using Ruff?</a></li>
<li><a href="#license">License</a></li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started<a id="user-content-getting-started"></a></h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">For more, see the <a href="https://docs.astral.sh/ruff/" rel="nofollow">documentation</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Ruff is available as <a href="https://pypi.org/project/ruff/" rel="nofollow"><code>ruff</code></a> on PyPI.</p>
<p dir="auto">Invoke Ruff directly with <a href="https://docs.astral.sh/uv/" rel="nofollow"><code>uvx</code></a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="uvx ruff check   # Lint all files in the current directory.
uvx ruff format  # Format all files in the current directory."><pre>uvx ruff check   <span><span>#</span> Lint all files in the current directory.</span>
uvx ruff format  <span><span>#</span> Format all files in the current directory.</span></pre></div>
<p dir="auto">Or install Ruff with <code>uv</code> (recommended), <code>pip</code>, or <code>pipx</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# With uv.
uv tool install ruff@latest  # Install Ruff globally.
uv add --dev ruff            # Or add Ruff to your project.

# With pip.
pip install ruff

# With pipx.
pipx install ruff"><pre><span><span>#</span> With uv.</span>
uv tool install ruff@latest  <span><span>#</span> Install Ruff globally.</span>
uv add --dev ruff            <span><span>#</span> Or add Ruff to your project.</span>

<span><span>#</span> With pip.</span>
pip install ruff

<span><span>#</span> With pipx.</span>
pipx install ruff</pre></div>
<p dir="auto">Starting with version <code>0.5.0</code>, Ruff can be installed with our standalone installers:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# On macOS and Linux.
curl -LsSf https://astral.sh/ruff/install.sh | sh

# On Windows.
powershell -c &quot;irm https://astral.sh/ruff/install.ps1 | iex&quot;

# For a specific version.
curl -LsSf https://astral.sh/ruff/0.9.2/install.sh | sh
powershell -c &quot;irm https://astral.sh/ruff/0.9.2/install.ps1 | iex&quot;"><pre><span><span>#</span> On macOS and Linux.</span>
curl -LsSf https://astral.sh/ruff/install.sh <span>|</span> sh

<span><span>#</span> On Windows.</span>
powershell -c <span><span>"</span>irm https://astral.sh/ruff/install.ps1 | iex<span>"</span></span>

<span><span>#</span> For a specific version.</span>
curl -LsSf https://astral.sh/ruff/0.9.2/install.sh <span>|</span> sh
powershell -c <span><span>"</span>irm https://astral.sh/ruff/0.9.2/install.ps1 | iex<span>"</span></span></pre></div>
<p dir="auto">You can also install Ruff via <a href="https://formulae.brew.sh/formula/ruff" rel="nofollow">Homebrew</a>, <a href="https://anaconda.org/conda-forge/ruff" rel="nofollow">Conda</a>,
and with <a href="https://docs.astral.sh/ruff/installation/" rel="nofollow">a variety of other package managers</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage</h3><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">To run Ruff as a linter, try any of the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ruff check                          # Lint all files in the current directory (and any subdirectories).
ruff check path/to/code/            # Lint all files in `/path/to/code` (and any subdirectories).
ruff check path/to/code/*.py        # Lint all `.py` files in `/path/to/code`.
ruff check path/to/code/to/file.py  # Lint `file.py`.
ruff check @arguments.txt           # Lint using an input file, treating its contents as newline-delimited command-line arguments."><pre>ruff check                          <span><span>#</span> Lint all files in the current directory (and any subdirectories).</span>
ruff check path/to/code/            <span><span>#</span> Lint all files in `/path/to/code` (and any subdirectories).</span>
ruff check path/to/code/<span>*</span>.py        <span><span>#</span> Lint all `.py` files in `/path/to/code`.</span>
ruff check path/to/code/to/file.py  <span><span>#</span> Lint `file.py`.</span>
ruff check @arguments.txt           <span><span>#</span> Lint using an input file, treating its contents as newline-delimited command-line arguments.</span></pre></div>
<p dir="auto">Or, to run Ruff as a formatter:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ruff format                          # Format all files in the current directory (and any subdirectories).
ruff format path/to/code/            # Format all files in `/path/to/code` (and any subdirectories).
ruff format path/to/code/*.py        # Format all `.py` files in `/path/to/code`.
ruff format path/to/code/to/file.py  # Format `file.py`.
ruff format @arguments.txt           # Format using an input file, treating its contents as newline-delimited command-line arguments."><pre>ruff format                          <span><span>#</span> Format all files in the current directory (and any subdirectories).</span>
ruff format path/to/code/            <span><span>#</span> Format all files in `/path/to/code` (and any subdirectories).</span>
ruff format path/to/code/<span>*</span>.py        <span><span>#</span> Format all `.py` files in `/path/to/code`.</span>
ruff format path/to/code/to/file.py  <span><span>#</span> Format `file.py`.</span>
ruff format @arguments.txt           <span><span>#</span> Format using an input file, treating its contents as newline-delimited command-line arguments.</span></pre></div>
<p dir="auto">Ruff can also be used as a <a href="https://pre-commit.com/" rel="nofollow">pre-commit</a> hook via <a href="https://github.com/astral-sh/ruff-pre-commit"><code>ruff-pre-commit</code></a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="- repo: https://github.com/astral-sh/ruff-pre-commit
  # Ruff version.
  rev: v0.9.2
  hooks:
    # Run the linter.
    - id: ruff
      args: [ --fix ]
    # Run the formatter.
    - id: ruff-format"><pre>- <span>repo</span>: <span>https://github.com/astral-sh/ruff-pre-commit</span>
  <span><span>#</span> Ruff version.</span>
  <span>rev</span>: <span>v0.9.2</span>
  <span>hooks</span>:
    <span><span>#</span> Run the linter.</span>
    - <span>id</span>: <span>ruff</span>
      <span>args</span>: <span>[ --fix ]</span>
    <span><span>#</span> Run the formatter.</span>
    - <span>id</span>: <span>ruff-format</span></pre></div>
<p dir="auto">Ruff can also be used as a <a href="https://github.com/astral-sh/ruff-vscode">VS Code extension</a> or with <a href="https://docs.astral.sh/ruff/editors/setup" rel="nofollow">various other editors</a>.</p>
<p dir="auto">Ruff can also be used as a <a href="https://github.com/features/actions">GitHub Action</a> via
<a href="https://github.com/astral-sh/ruff-action"><code>ruff-action</code></a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="name: Ruff
on: [ push, pull_request ]
jobs:
  ruff:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: astral-sh/ruff-action@v3"><pre><span>name</span>: <span>Ruff</span>
<span>on</span>: <span>[ push, pull_request ]</span>
<span>jobs</span>:
  <span>ruff</span>:
    <span>runs-on</span>: <span>ubuntu-latest</span>
    <span>steps</span>:
      - <span>uses</span>: <span>actions/checkout@v4</span>
      - <span>uses</span>: <span>astral-sh/ruff-action@v3</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration<a id="user-content-configuration"></a></h3><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">Ruff can be configured through a <code>pyproject.toml</code>, <code>ruff.toml</code>, or <code>.ruff.toml</code> file (see:
<a href="https://docs.astral.sh/ruff/configuration/" rel="nofollow"><em>Configuration</em></a>, or <a href="https://docs.astral.sh/ruff/settings/" rel="nofollow"><em>Settings</em></a>
for a complete list of all configuration options).</p>
<p dir="auto">If left unspecified, Ruff's default configuration is equivalent to the following <code>ruff.toml</code> file:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Exclude a variety of commonly ignored directories.
exclude = [
    &quot;.bzr&quot;,
    &quot;.direnv&quot;,
    &quot;.eggs&quot;,
    &quot;.git&quot;,
    &quot;.git-rewrite&quot;,
    &quot;.hg&quot;,
    &quot;.ipynb_checkpoints&quot;,
    &quot;.mypy_cache&quot;,
    &quot;.nox&quot;,
    &quot;.pants.d&quot;,
    &quot;.pyenv&quot;,
    &quot;.pytest_cache&quot;,
    &quot;.pytype&quot;,
    &quot;.ruff_cache&quot;,
    &quot;.svn&quot;,
    &quot;.tox&quot;,
    &quot;.venv&quot;,
    &quot;.vscode&quot;,
    &quot;__pypackages__&quot;,
    &quot;_build&quot;,
    &quot;buck-out&quot;,
    &quot;build&quot;,
    &quot;dist&quot;,
    &quot;node_modules&quot;,
    &quot;site-packages&quot;,
    &quot;venv&quot;,
]

# Same as Black.
line-length = 88
indent-width = 4

# Assume Python 3.9
target-version = &quot;py39&quot;

[lint]
# Enable Pyflakes (`F`) and a subset of the pycodestyle (`E`)  codes by default.
select = [&quot;E4&quot;, &quot;E7&quot;, &quot;E9&quot;, &quot;F&quot;]
ignore = []

# Allow fix for all enabled rules (when `--fix`) is provided.
fixable = [&quot;ALL&quot;]
unfixable = []

# Allow unused variables when underscore-prefixed.
dummy-variable-rgx = &quot;^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$&quot;

[format]
# Like Black, use double quotes for strings.
quote-style = &quot;double&quot;

# Like Black, indent with spaces, rather than tabs.
indent-style = &quot;space&quot;

# Like Black, respect magic trailing commas.
skip-magic-trailing-comma = false

# Like Black, automatically detect the appropriate line ending.
line-ending = &quot;auto&quot;"><pre><span><span>#</span> Exclude a variety of commonly ignored directories.</span>
<span>exclude</span> = [
    <span><span>"</span>.bzr<span>"</span></span>,
    <span><span>"</span>.direnv<span>"</span></span>,
    <span><span>"</span>.eggs<span>"</span></span>,
    <span><span>"</span>.git<span>"</span></span>,
    <span><span>"</span>.git-rewrite<span>"</span></span>,
    <span><span>"</span>.hg<span>"</span></span>,
    <span><span>"</span>.ipynb_checkpoints<span>"</span></span>,
    <span><span>"</span>.mypy_cache<span>"</span></span>,
    <span><span>"</span>.nox<span>"</span></span>,
    <span><span>"</span>.pants.d<span>"</span></span>,
    <span><span>"</span>.pyenv<span>"</span></span>,
    <span><span>"</span>.pytest_cache<span>"</span></span>,
    <span><span>"</span>.pytype<span>"</span></span>,
    <span><span>"</span>.ruff_cache<span>"</span></span>,
    <span><span>"</span>.svn<span>"</span></span>,
    <span><span>"</span>.tox<span>"</span></span>,
    <span><span>"</span>.venv<span>"</span></span>,
    <span><span>"</span>.vscode<span>"</span></span>,
    <span><span>"</span>__pypackages__<span>"</span></span>,
    <span><span>"</span>_build<span>"</span></span>,
    <span><span>"</span>buck-out<span>"</span></span>,
    <span><span>"</span>build<span>"</span></span>,
    <span><span>"</span>dist<span>"</span></span>,
    <span><span>"</span>node_modules<span>"</span></span>,
    <span><span>"</span>site-packages<span>"</span></span>,
    <span><span>"</span>venv<span>"</span></span>,
]

<span><span>#</span> Same as Black.</span>
<span>line-length</span> = <span>88</span>
<span>indent-width</span> = <span>4</span>

<span><span>#</span> Assume Python 3.9</span>
<span>target-version</span> = <span><span>"</span>py39<span>"</span></span>

[<span>lint</span>]
<span><span>#</span> Enable Pyflakes (`F`) and a subset of the pycodestyle (`E`)  codes by default.</span>
<span>select</span> = [<span><span>"</span>E4<span>"</span></span>, <span><span>"</span>E7<span>"</span></span>, <span><span>"</span>E9<span>"</span></span>, <span><span>"</span>F<span>"</span></span>]
<span>ignore</span> = []

<span><span>#</span> Allow fix for all enabled rules (when `--fix`) is provided.</span>
<span>fixable</span> = [<span><span>"</span>ALL<span>"</span></span>]
<span>unfixable</span> = []

<span><span>#</span> Allow unused variables when underscore-prefixed.</span>
<span>dummy-variable-rgx</span> = <span><span>"</span>^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$<span>"</span></span>

[<span>format</span>]
<span><span>#</span> Like Black, use double quotes for strings.</span>
<span>quote-style</span> = <span><span>"</span>double<span>"</span></span>

<span><span>#</span> Like Black, indent with spaces, rather than tabs.</span>
<span>indent-style</span> = <span><span>"</span>space<span>"</span></span>

<span><span>#</span> Like Black, respect magic trailing commas.</span>
<span>skip-magic-trailing-comma</span> = <span>false</span>

<span><span>#</span> Like Black, automatically detect the appropriate line ending.</span>
<span>line-ending</span> = <span><span>"</span>auto<span>"</span></span></pre></div>
<p dir="auto">Note that, in a <code>pyproject.toml</code>, each section header should be prefixed with <code>tool.ruff</code>. For
example, <code>[lint]</code> should be replaced with <code>[tool.ruff.lint]</code>.</p>
<p dir="auto">Some configuration options can be provided via dedicated command-line arguments, such as those
related to rule enablement and disablement, file discovery, and logging level:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ruff check --select F401 --select F403 --quiet"><pre>ruff check --select F401 --select F403 --quiet</pre></div>
<p dir="auto">The remaining configuration options can be provided through a catch-all <code>--config</code> argument:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ruff check --config &quot;lint.per-file-ignores = {'some_file.py' = ['F841']}&quot;"><pre>ruff check --config <span><span>"</span>lint.per-file-ignores = {'some_file.py' = ['F841']}<span>"</span></span></pre></div>
<p dir="auto">To opt in to the latest lint rules, formatter style changes, interface updates, and more, enable
<a href="https://docs.astral.sh/ruff/rules/" rel="nofollow">preview mode</a> by setting <code>preview = true</code> in your configuration
file or passing <code>--preview</code> on the command line. Preview mode enables a collection of unstable
features that may change prior to stabilization.</p>
<p dir="auto">See <code>ruff help</code> for more on Ruff's top-level commands, or <code>ruff help check</code> and <code>ruff help format</code>
for more on the linting and formatting commands, respectively.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Rules<a id="user-content-rules"></a></h2><a id="user-content-rules" aria-label="Permalink: Rules" href="#rules"></a></p>

<p dir="auto"><strong>Ruff supports over 800 lint rules</strong>, many of which are inspired by popular tools like Flake8,
isort, pyupgrade, and others. Regardless of the rule's origin, Ruff re-implements every rule in
Rust as a first-party feature.</p>
<p dir="auto">By default, Ruff enables Flake8's <code>F</code> rules, along with a subset of the <code>E</code> rules, omitting any
stylistic rules that overlap with the use of a formatter, like <code>ruff format</code> or
<a href="https://github.com/psf/black">Black</a>.</p>
<p dir="auto">If you're just getting started with Ruff, <strong>the default rule set is a great place to start</strong>: it
catches a wide variety of common errors (like unused imports) with zero configuration.</p>

<p dir="auto">Beyond the defaults, Ruff re-implements some of the most popular Flake8 plugins and related code
quality tools, including:</p>
<ul dir="auto">
<li><a href="https://pypi.org/project/autoflake/" rel="nofollow">autoflake</a></li>
<li><a href="https://pypi.org/project/eradicate/" rel="nofollow">eradicate</a></li>
<li><a href="https://pypi.org/project/flake8-2020/" rel="nofollow">flake8-2020</a></li>
<li><a href="https://pypi.org/project/flake8-annotations/" rel="nofollow">flake8-annotations</a></li>
<li><a href="https://pypi.org/project/flake8-async" rel="nofollow">flake8-async</a></li>
<li><a href="https://pypi.org/project/flake8-bandit/" rel="nofollow">flake8-bandit</a> (<a href="https://github.com/astral-sh/ruff/issues/1646" data-hovercard-type="issue" data-hovercard-url="/astral-sh/ruff/issues/1646/hovercard">#1646</a>)</li>
<li><a href="https://pypi.org/project/flake8-blind-except/" rel="nofollow">flake8-blind-except</a></li>
<li><a href="https://pypi.org/project/flake8-boolean-trap/" rel="nofollow">flake8-boolean-trap</a></li>
<li><a href="https://pypi.org/project/flake8-bugbear/" rel="nofollow">flake8-bugbear</a></li>
<li><a href="https://pypi.org/project/flake8-builtins/" rel="nofollow">flake8-builtins</a></li>
<li><a href="https://pypi.org/project/flake8-commas/" rel="nofollow">flake8-commas</a></li>
<li><a href="https://pypi.org/project/flake8-comprehensions/" rel="nofollow">flake8-comprehensions</a></li>
<li><a href="https://pypi.org/project/flake8-copyright/" rel="nofollow">flake8-copyright</a></li>
<li><a href="https://pypi.org/project/flake8-datetimez/" rel="nofollow">flake8-datetimez</a></li>
<li><a href="https://pypi.org/project/flake8-debugger/" rel="nofollow">flake8-debugger</a></li>
<li><a href="https://pypi.org/project/flake8-django/" rel="nofollow">flake8-django</a></li>
<li><a href="https://pypi.org/project/flake8-docstrings/" rel="nofollow">flake8-docstrings</a></li>
<li><a href="https://pypi.org/project/flake8-eradicate/" rel="nofollow">flake8-eradicate</a></li>
<li><a href="https://pypi.org/project/flake8-errmsg/" rel="nofollow">flake8-errmsg</a></li>
<li><a href="https://pypi.org/project/flake8-executable/" rel="nofollow">flake8-executable</a></li>
<li><a href="https://pypi.org/project/flake8-future-annotations/" rel="nofollow">flake8-future-annotations</a></li>
<li><a href="https://pypi.org/project/flake8-gettext/" rel="nofollow">flake8-gettext</a></li>
<li><a href="https://pypi.org/project/flake8-implicit-str-concat/" rel="nofollow">flake8-implicit-str-concat</a></li>
<li><a href="https://github.com/joaopalmeiro/flake8-import-conventions">flake8-import-conventions</a></li>
<li><a href="https://pypi.org/project/flake8-logging/" rel="nofollow">flake8-logging</a></li>
<li><a href="https://pypi.org/project/flake8-logging-format/" rel="nofollow">flake8-logging-format</a></li>
<li><a href="https://pypi.org/project/flake8-no-pep420" rel="nofollow">flake8-no-pep420</a></li>
<li><a href="https://pypi.org/project/flake8-pie/" rel="nofollow">flake8-pie</a></li>
<li><a href="https://pypi.org/project/flake8-print/" rel="nofollow">flake8-print</a></li>
<li><a href="https://pypi.org/project/flake8-pyi/" rel="nofollow">flake8-pyi</a></li>
<li><a href="https://pypi.org/project/flake8-pytest-style/" rel="nofollow">flake8-pytest-style</a></li>
<li><a href="https://pypi.org/project/flake8-quotes/" rel="nofollow">flake8-quotes</a></li>
<li><a href="https://pypi.org/project/flake8-raise/" rel="nofollow">flake8-raise</a></li>
<li><a href="https://pypi.org/project/flake8-return/" rel="nofollow">flake8-return</a></li>
<li><a href="https://pypi.org/project/flake8-self/" rel="nofollow">flake8-self</a></li>
<li><a href="https://pypi.org/project/flake8-simplify/" rel="nofollow">flake8-simplify</a></li>
<li><a href="https://pypi.org/project/flake8-slots/" rel="nofollow">flake8-slots</a></li>
<li><a href="https://pypi.org/project/flake8-super/" rel="nofollow">flake8-super</a></li>
<li><a href="https://pypi.org/project/flake8-tidy-imports/" rel="nofollow">flake8-tidy-imports</a></li>
<li><a href="https://pypi.org/project/flake8-todos/" rel="nofollow">flake8-todos</a></li>
<li><a href="https://pypi.org/project/flake8-type-checking/" rel="nofollow">flake8-type-checking</a></li>
<li><a href="https://pypi.org/project/flake8-use-pathlib/" rel="nofollow">flake8-use-pathlib</a></li>
<li><a href="https://pypi.org/project/flynt/" rel="nofollow">flynt</a> (<a href="https://github.com/astral-sh/ruff/issues/2102" data-hovercard-type="issue" data-hovercard-url="/astral-sh/ruff/issues/2102/hovercard">#2102</a>)</li>
<li><a href="https://pypi.org/project/isort/" rel="nofollow">isort</a></li>
<li><a href="https://pypi.org/project/mccabe/" rel="nofollow">mccabe</a></li>
<li><a href="https://pypi.org/project/pandas-vet/" rel="nofollow">pandas-vet</a></li>
<li><a href="https://pypi.org/project/pep8-naming/" rel="nofollow">pep8-naming</a></li>
<li><a href="https://pypi.org/project/pydocstyle/" rel="nofollow">pydocstyle</a></li>
<li><a href="https://github.com/pre-commit/pygrep-hooks">pygrep-hooks</a></li>
<li><a href="https://pypi.org/project/pylint-airflow/" rel="nofollow">pylint-airflow</a></li>
<li><a href="https://pypi.org/project/pyupgrade/" rel="nofollow">pyupgrade</a></li>
<li><a href="https://pypi.org/project/tryceratops/" rel="nofollow">tryceratops</a></li>
<li><a href="https://pypi.org/project/yesqa/" rel="nofollow">yesqa</a></li>
</ul>
<p dir="auto">For a complete enumeration of the supported rules, see <a href="https://docs.astral.sh/ruff/rules/" rel="nofollow"><em>Rules</em></a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing<a id="user-content-contributing"></a></h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome and highly appreciated. To get started, check out the
<a href="https://docs.astral.sh/ruff/contributing/" rel="nofollow"><strong>contributing guidelines</strong></a>.</p>
<p dir="auto">You can also join us on <a href="https://discord.com/invite/astral-sh" rel="nofollow"><strong>Discord</strong></a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Support<a id="user-content-support"></a></h2><a id="user-content-support" aria-label="Permalink: Support" href="#support"></a></p>
<p dir="auto">Having trouble? Check out the existing issues on <a href="https://github.com/astral-sh/ruff/issues"><strong>GitHub</strong></a>,
or feel free to <a href="https://github.com/astral-sh/ruff/issues/new"><strong>open a new one</strong></a>.</p>
<p dir="auto">You can also ask for help on <a href="https://discord.com/invite/astral-sh" rel="nofollow"><strong>Discord</strong></a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements<a id="user-content-acknowledgements"></a></h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<p dir="auto">Ruff's linter draws on both the APIs and implementation details of many other
tools in the Python ecosystem, especially <a href="https://github.com/PyCQA/flake8">Flake8</a>, <a href="https://github.com/PyCQA/pyflakes">Pyflakes</a>,
<a href="https://github.com/PyCQA/pycodestyle">pycodestyle</a>, <a href="https://github.com/PyCQA/pydocstyle">pydocstyle</a>,
<a href="https://github.com/asottile/pyupgrade">pyupgrade</a>, and <a href="https://github.com/PyCQA/isort">isort</a>.</p>
<p dir="auto">In some cases, Ruff includes a "direct" Rust port of the corresponding tool.
We're grateful to the maintainers of these tools for their work, and for all
the value they've provided to the Python community.</p>
<p dir="auto">Ruff's formatter is built on a fork of Rome's <a href="https://github.com/rome/tools/tree/main/crates/rome_formatter"><code>rome_formatter</code></a>,
and again draws on both API and implementation details from <a href="https://github.com/rome/tools">Rome</a>,
<a href="https://github.com/prettier/prettier">Prettier</a>, and <a href="https://github.com/psf/black">Black</a>.</p>
<p dir="auto">Ruff's import resolver is based on the import resolution algorithm from <a href="https://github.com/microsoft/pyright">Pyright</a>.</p>
<p dir="auto">Ruff is also influenced by a number of tools outside the Python ecosystem, like
<a href="https://github.com/rust-lang/rust-clippy">Clippy</a> and <a href="https://github.com/eslint/eslint">ESLint</a>.</p>
<p dir="auto">Ruff is the beneficiary of a large number of <a href="https://github.com/astral-sh/ruff/graphs/contributors">contributors</a>.</p>
<p dir="auto">Ruff is released under the MIT license.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Who's Using Ruff?<a id="user-content-whos-using-ruff"></a></h2><a id="user-content-whos-using-ruff" aria-label="Permalink: Who's Using Ruff?" href="#whos-using-ruff"></a></p>
<p dir="auto">Ruff is used by a number of major open-source projects and companies, including:</p>
<ul dir="auto">
<li><a href="https://github.com/albumentations-team/albumentations">Albumentations</a></li>
<li>Amazon (<a href="https://github.com/aws/serverless-application-model">AWS SAM</a>)</li>
<li>Anthropic (<a href="https://github.com/anthropics/anthropic-sdk-python">Python SDK</a>)</li>
<li><a href="https://github.com/apache/airflow">Apache Airflow</a></li>
<li>AstraZeneca (<a href="https://github.com/AstraZeneca/magnus-core">Magnus</a>)</li>
<li><a href="https://github.com/python-babel/babel">Babel</a></li>
<li>Benchling (<a href="https://github.com/benchling/refac">Refac</a>)</li>
<li><a href="https://github.com/bokeh/bokeh">Bokeh</a></li>
<li>CrowdCent (<a href="https://github.com/crowdcent/numerblox">NumerBlox</a>) </li>
<li><a href="https://github.com/pyca/cryptography">Cryptography (PyCA)</a></li>
<li>CERN (<a href="https://getindico.io/" rel="nofollow">Indico</a>)</li>
<li><a href="https://github.com/iterative/dvc">DVC</a></li>
<li><a href="https://github.com/dagger/dagger">Dagger</a></li>
<li><a href="https://github.com/dagster-io/dagster">Dagster</a></li>
<li>Databricks (<a href="https://github.com/mlflow/mlflow">MLflow</a>)</li>
<li><a href="https://github.com/langgenius/dify">Dify</a></li>
<li><a href="https://github.com/tiangolo/fastapi">FastAPI</a></li>
<li><a href="https://github.com/godotengine/godot">Godot</a></li>
<li><a href="https://github.com/gradio-app/gradio">Gradio</a></li>
<li><a href="https://github.com/great-expectations/great_expectations">Great Expectations</a></li>
<li><a href="https://github.com/encode/httpx">HTTPX</a></li>
<li><a href="https://github.com/pypa/hatch">Hatch</a></li>
<li><a href="https://github.com/home-assistant/core">Home Assistant</a></li>
<li>Hugging Face (<a href="https://github.com/huggingface/transformers">Transformers</a>,
<a href="https://github.com/huggingface/datasets">Datasets</a>,
<a href="https://github.com/huggingface/diffusers">Diffusers</a>)</li>
<li>IBM (<a href="https://github.com/Qiskit/qiskit">Qiskit</a>)</li>
<li>ING Bank (<a href="https://github.com/ing-bank/popmon">popmon</a>, <a href="https://github.com/ing-bank/probatus">probatus</a>)</li>
<li><a href="https://github.com/ibis-project/ibis">Ibis</a></li>
<li><a href="https://github.com/unifyai/ivy">ivy</a></li>
<li><a href="https://github.com/jupyter-server/jupyter_server">Jupyter</a></li>
<li><a href="https://kraken.tech/" rel="nofollow">Kraken Tech</a></li>
<li><a href="https://github.com/hwchase17/langchain">LangChain</a></li>
<li><a href="https://litestar.dev/" rel="nofollow">Litestar</a></li>
<li><a href="https://github.com/jerryjliu/llama_index">LlamaIndex</a></li>
<li>Matrix (<a href="https://github.com/matrix-org/synapse">Synapse</a>)</li>
<li><a href="https://github.com/oxsecurity/megalinter">MegaLinter</a></li>
<li>Meltano (<a href="https://github.com/meltano/meltano">Meltano CLI</a>, <a href="https://github.com/meltano/sdk">Singer SDK</a>)</li>
<li>Microsoft (<a href="https://github.com/microsoft/semantic-kernel">Semantic Kernel</a>,
<a href="https://github.com/microsoft/onnxruntime">ONNX Runtime</a>,
<a href="https://github.com/microsoft/LightGBM">LightGBM</a>)</li>
<li>Modern Treasury (<a href="https://github.com/Modern-Treasury/modern-treasury-python">Python SDK</a>)</li>
<li>Mozilla (<a href="https://github.com/mozilla/gecko-dev">Firefox</a>)</li>
<li><a href="https://github.com/python/mypy">Mypy</a></li>
<li><a href="https://github.com/nautobot/nautobot">Nautobot</a></li>
<li>Netflix (<a href="https://github.com/Netflix/dispatch">Dispatch</a>)</li>
<li><a href="https://github.com/neondatabase/neon">Neon</a></li>
<li><a href="https://nokia.com/" rel="nofollow">Nokia</a></li>
<li><a href="https://github.com/nonebot/nonebot2">NoneBot</a></li>
<li><a href="https://github.com/pyro-ppl/numpyro">NumPyro</a></li>
<li><a href="https://github.com/onnx/onnx">ONNX</a></li>
<li><a href="https://github.com/OpenBB-finance/OpenBBTerminal">OpenBB</a></li>
<li><a href="https://github.com/Open-Wine-Components/umu-launcher">Open Wine Components</a></li>
<li><a href="https://github.com/pdm-project/pdm">PDM</a></li>
<li><a href="https://github.com/PaddlePaddle/Paddle">PaddlePaddle</a></li>
<li><a href="https://github.com/pandas-dev/pandas">Pandas</a></li>
<li><a href="https://github.com/python-pillow/Pillow">Pillow</a></li>
<li><a href="https://github.com/python-poetry/poetry">Poetry</a></li>
<li><a href="https://github.com/pola-rs/polars">Polars</a></li>
<li><a href="https://github.com/PostHog/posthog">PostHog</a></li>
<li>Prefect (<a href="https://github.com/PrefectHQ/prefect">Python SDK</a>, <a href="https://github.com/PrefectHQ/marvin">Marvin</a>)</li>
<li><a href="https://github.com/pyinstaller/pyinstaller">PyInstaller</a></li>
<li><a href="https://github.com/pymc-devs/pymc/">PyMC</a></li>
<li><a href="https://github.com/pymc-labs/pymc-marketing">PyMC-Marketing</a></li>
<li><a href="https://github.com/pytest-dev/pytest">pytest</a></li>
<li><a href="https://github.com/pytorch/pytorch">PyTorch</a></li>
<li><a href="https://github.com/pydantic/pydantic">Pydantic</a></li>
<li><a href="https://github.com/PyCQA/pylint">Pylint</a></li>
<li><a href="https://github.com/pyvista/pyvista">PyVista</a></li>
<li><a href="https://github.com/reflex-dev/reflex">Reflex</a></li>
<li><a href="https://github.com/online-ml/river">River</a></li>
<li><a href="https://rippling.com/" rel="nofollow">Rippling</a></li>
<li><a href="https://github.com/sansyrox/robyn">Robyn</a></li>
<li><a href="https://github.com/saleor/saleor">Saleor</a></li>
<li>Scale AI (<a href="https://github.com/scaleapi/launch-python-client">Launch SDK</a>)</li>
<li><a href="https://github.com/scipy/scipy">SciPy</a></li>
<li>Snowflake (<a href="https://github.com/Snowflake-Labs/snowcli">SnowCLI</a>)</li>
<li><a href="https://github.com/sphinx-doc/sphinx">Sphinx</a></li>
<li><a href="https://github.com/DLR-RM/stable-baselines3">Stable Baselines3</a></li>
<li><a href="https://github.com/encode/starlette">Starlette</a></li>
<li><a href="https://github.com/streamlit/streamlit">Streamlit</a></li>
<li><a href="https://github.com/TheAlgorithms/Python">The Algorithms</a></li>
<li><a href="https://github.com/altair-viz/altair">Vega-Altair</a></li>
<li>WordPress (<a href="https://github.com/WordPress/openverse">Openverse</a>)</li>
<li><a href="https://github.com/zenml-io/zenml">ZenML</a></li>
<li><a href="https://github.com/zulip/zulip">Zulip</a></li>
<li><a href="https://github.com/pypa/build">build (PyPA)</a></li>
<li><a href="https://github.com/pypa/cibuildwheel">cibuildwheel (PyPA)</a></li>
<li><a href="https://github.com/delta-io/delta-rs">delta-rs</a></li>
<li><a href="https://github.com/alteryx/featuretools">featuretools</a></li>
<li><a href="https://github.com/mesonbuild/meson-python">meson-python</a></li>
<li><a href="https://github.com/wntrblm/nox">nox</a></li>
<li><a href="https://github.com/pypa/pip">pip</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Show Your Support</h3><a id="user-content-show-your-support" aria-label="Permalink: Show Your Support" href="#show-your-support"></a></p>
<p dir="auto">If you're using Ruff, consider adding the Ruff badge to your project's <code>README.md</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)"><pre><span>[</span><span>![</span>Ruff<span>]</span><span>(</span><span>https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json</span><span>)]</span><span>(</span><span>https://github.com/astral-sh/ruff</span><span>)</span></pre></div>
<p dir="auto">...or <code>README.rst</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content=".. image:: https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json
    :target: https://github.com/astral-sh/ruff
    :alt: Ruff"><pre>.. <span>image</span>:: <span>https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json</span>
    <span>:target:</span> <span>https://github.com/astral-sh/ruff</span>
    <span>:alt:</span> <span>Ruff</span></pre></div>
<p dir="auto">...or, as HTML:</p>
<div dir="auto" data-snippet-clipboard-copy-content="<a href=&quot;https://github.com/astral-sh/ruff&quot;><img src=&quot;https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json&quot; alt=&quot;Ruff&quot; style=&quot;max-width:100%;&quot;></a>"><pre><span>&lt;</span><span>a</span> <span>href</span>="<span>https://github.com/astral-sh/ruff</span>"<span>&gt;</span><span>&lt;</span><span>img</span> <span>src</span>="<span>https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json</span>" <span>alt</span>="<span>Ruff</span>" <span>style</span>="<span>max-width:100%;</span>"<span>&gt;</span><span>&lt;/</span><span>a</span><span>&gt;</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License<a id="user-content-license"></a></h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This repository is licensed under the <a href="https://github.com/astral-sh/ruff/blob/main/LICENSE">MIT License</a></p>
<p><a href="https://astral.sh/" rel="nofollow">
    <img src="https://raw.githubusercontent.com/astral-sh/ruff/main/assets/svg/Astral.svg" alt="Made by Astral">
  </a>
</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Elon Gives Nazi Salute During Inauguration Speech (120 pts)]]></title>
            <link>https://www.youtube.com/watch?v=e2bbb-6Clhs</link>
            <guid>42774621</guid>
            <pubDate>Mon, 20 Jan 2025 23:53:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=e2bbb-6Clhs">https://www.youtube.com/watch?v=e2bbb-6Clhs</a>, See on <a href="https://news.ycombinator.com/item?id=42774621">Hacker News</a></p>
Couldn't get https://www.youtube.com/watch?v=e2bbb-6Clhs: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Reverse engineering Call of Duty anti-cheat (334 pts)]]></title>
            <link>https://ssno.cc/posts/reversing-tac-1-4-2025/</link>
            <guid>42774221</guid>
            <pubDate>Mon, 20 Jan 2025 23:07:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ssno.cc/posts/reversing-tac-1-4-2025/">https://ssno.cc/posts/reversing-tac-1-4-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=42774221">Hacker News</a></p>
Couldn't get https://ssno.cc/posts/reversing-tac-1-4-2025/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Elon Musk appears to make back-to-back fascist salutes at inauguration rally (201 pts)]]></title>
            <link>https://www.theguardian.com/technology/2025/jan/20/trump-elon-musk-salute</link>
            <guid>42773778</guid>
            <pubDate>Mon, 20 Jan 2025 22:21:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/technology/2025/jan/20/trump-elon-musk-salute">https://www.theguardian.com/technology/2025/jan/20/trump-elon-musk-salute</a>, See on <a href="https://news.ycombinator.com/item?id=42773778">Hacker News</a></p>
Couldn't get https://www.theguardian.com/technology/2025/jan/20/trump-elon-musk-salute: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Matt Mullenweg, Automattic's CEO, Seems Bound and Determined to Wreck WordPress (110 pts)]]></title>
            <link>https://digitalcxo.com/article/matt-mullenweg-automattics-ceo-seems-bound-and-determined-to-wreck-wordpress/</link>
            <guid>42773311</guid>
            <pubDate>Mon, 20 Jan 2025 21:32:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://digitalcxo.com/article/matt-mullenweg-automattics-ceo-seems-bound-and-determined-to-wreck-wordpress/">https://digitalcxo.com/article/matt-mullenweg-automattics-ceo-seems-bound-and-determined-to-wreck-wordpress/</a>, See on <a href="https://news.ycombinator.com/item?id=42773311">Hacker News</a></p>
Couldn't get https://digitalcxo.com/article/matt-mullenweg-automattics-ceo-seems-bound-and-determined-to-wreck-wordpress/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Did Elon Musk Appear to Sieg Heil at Trump Inauguration? (245 pts)]]></title>
            <link>https://www.jpost.com/international/article-838444</link>
            <guid>42772995</guid>
            <pubDate>Mon, 20 Jan 2025 21:02:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jpost.com/international/article-838444">https://www.jpost.com/international/article-838444</a>, See on <a href="https://news.ycombinator.com/item?id=42772995">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            
<section>
	<section>
		


    <section>
        <h2>Musk was seen making the gesture a total of three times on live television.</h2>
    </section>

<section>
            <section>
                <section>
                    <time datetime="2025-01-20T22:29:54+02:00">
                        JANUARY 20, 2025 22:29
                    </time>
                </section>
                    <section><b>Updated:</b> JANUARY 20, 2025 23:30</section>
            </section>
            <section data-share-url="https://www.jpost.com/international/article-838444" data-share-title="Did Elon Musk Sieg Heil at Trump's inauguration?">
                    <nav>
                        <ul>
                            <li data-share-url="https://www.jpost.com/international/article-838444#838444" data-share-title="Did Elon Musk Sieg Heil at Trump's inauguration?">
                                
                            </li>
                            <li data-share-url="https://www.jpost.com/international/article-838444#838444" data-share-title="Did Elon Musk Sieg Heil at Trump's inauguration?">
                                
                            </li>
                            <li data-share-url="https://www.jpost.com/international/article-838444#838444" data-share-title="Did Elon Musk Sieg Heil at Trump's inauguration?">
                                
                            </li>
                            <li data-share-url="https://www.jpost.com/international/article-838444#838444" data-share-title="Did Elon Musk Sieg Heil at Trump's inauguration?">
                                
                            </li>
                        </ul>
                    </nav>
                </section>
        </section>


	</section>
	<section>
			<figure>
				<img src="https://images.jpost.com/image/upload/q_auto/c_fill,g_faces:center,h_537,w_822/644997" width="290" height="260" alt=" Elon Musk makes controversial gesture at Washington DC arena (photo credit: SCREENSHOT/X)" title=" Elon Musk makes controversial gesture at Washington DC arena">
				<figcaption>
					<section>
						<section> Elon Musk makes controversial gesture at Washington DC arena</section>
						<section>(photo credit: SCREENSHOT/X)</section>
					</section>
				</figcaption>
			</figure>
	</section>

</section>
<section>
			
			<section itemprop="articleBody" id="startBannerSticky">
				<p>US billionaire <a href="https://www.jpost.com/tags/elon-musk">Elon Musk</a> appeared to make a Heil Hitler salute at the Washington DC Trump parade on Monday, following Trump's inauguration.&nbsp;</p><p><a href="https://www.jpost.com/middle-east/article-837785">Musk</a> was seen making the gesture a total of three times on live television.</p><blockquote data-media-max-width="560"><p lang="en" dir="ltr">Elon Musk does what looks like a Hitler salute after talking of victory at Trump inauguration, thanking supporters for assuring "the future of civilisation" <a href="https://t.co/xp0kmJ5dFQ" rel="nofollow">pic.twitter.com/xp0kmJ5dFQ</a></p>‚Äî James Jackson (@derJamesJackson) <a href="https://twitter.com/derJamesJackson/status/1881436166144262376?ref_src=twsrc%5Etfw" rel="nofollow">January 20, 2025</a></blockquote><p>He then appeared on stage at the Capital One Area in front of 20,000 Trump supporters, where he thanked supporters before making the gesture.</p><blockquote><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/Breaking?src=hash&amp;ref_src=twsrc%5Etfw" rel="nofollow">#Breaking</a>: Senior Trump administration official Elon Musk thanks supporters with a Nazi salute. <a href="https://t.co/WzSZFUYvEG" rel="nofollow">pic.twitter.com/WzSZFUYvEG</a></p>‚Äî Noga Tarnopolsky ◊†◊í◊î ◊ò◊®◊†◊ï◊§◊ï◊ú◊°◊ß◊ô ŸÜŸàÿ∫ÿß ÿ™ÿ±ŸÜŸàÿ®ŸàŸÑÿ≥ŸÉŸä (@NTarnopolsky) <a href="https://twitter.com/NTarnopolsky/status/1881436487558090831?ref_src=twsrc%5Etfw" rel="nofollow">January 20, 2025</a></blockquote><p>Social media users reacted with horror, with one writing, "Remember when Democrats called MAGA rallies "Nazi rallies?" President un-elect Elon Musk just did the Nazi Sieg Heil salute."</p><h3>Mars space travel</h3><p>The Tesla and Space X owner appeared excited by Trump's mention of Mars in his inaugural speech, given he has reportedly urged NASA to drop its plans to return to the moon and go straight to Mars, according to Politico.</p><p>President Donald Trump said the US would launch astronauts to plant the ‚Äústars and stripes‚Äù on Mars.</p><p>"We're gonna take DOGE to Mars!" said Musk in his speech, "Can you imagine how awesome it will be to have American astronauts plant the flag on another planet for the first time! How inspiring would that be?!"</p><section><hr><div>            <p>Stay updated with the latest news!</p>            <p>Subscribe to The Jerusalem Post Newsletter</p>        </div><hr></section><p>This comes amid a Washington Post report that Donald Trump's government <a href="https://www.jpost.com/american-politics/article-837770">advisory panel</a>, led by Elon Musk, will be sued soon after the incoming US president is sworn in on Monday.
				</p>
			</section>
		</section>



            
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Official DeepSeek R1 Now on Ollama (186 pts)]]></title>
            <link>https://ollama.com/library/deepseek-r1</link>
            <guid>42772983</guid>
            <pubDate>Mon, 20 Jan 2025 21:00:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ollama.com/library/deepseek-r1">https://ollama.com/library/deepseek-r1</a>, See on <a href="https://news.ycombinator.com/item?id=42772983">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	<div>
		
		<div>
			
<div id="summary">
	<h2 id="summary-display">
		<span id="summary-content">
		
			DeepSeek's first generation reasoning models with comparable performance to OpenAI-o1. 
		
		</span>
		
	</h2>
	
</div>

			<div>
				<p><span x-test-size="">1.5b</span>
					
						<span x-test-size="">7b</span>
					
						<span x-test-size="">8b</span>
					
						<span x-test-size="">14b</span>
					
						<span x-test-size="">32b</span>
					
						<span x-test-size="">70b</span>
					
						<span x-test-size="">671b</span>
					
				</p>
				<p>
					
					  <span>
						<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor">
						  <path stroke-linecap="round" stroke-linejoin="round" d="M3 16.5v2.25A2.25 2.25 0 005.25 21h13.5A2.25 2.25 0 0021 18.75V16.5M16.5 12L12 16.5m0 0L7.5 12m4.5 4.5V3"></path>
						</svg>
						<span x-test-pull-count="">40.4K</span>
						<span>&nbsp;Pulls</span>
					  </span>
					
					
						<span>
							<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor">
								<path stroke-linecap="round" stroke-linejoin="round" d="M12 6v6h4.5m4.5 0a9 9 0 1 1-18 0 9 9 0 0 1 18 0Z"></path>
							</svg>
							<span>Updated&nbsp;</span>
							<span x-test-updated="">2 hours ago</span>
						</span>
					
				</p>
			</div>
		</div>
		
		
  



		
	</div>
	
  
    
  

  
  
  <div id="readme">
    <p>
      <h2>Readme</h2>

      
    </p>
    <div id="display">
        
          <p><img src="https://ollama.com/assets/library/deepseek-v3/069ccc94-63b0-41e6-b2b3-e8e56068ab1a" width="320"></p>

<p>DeepSeek‚Äôs first-generation reasoning models, achieving performance comparable to OpenAI-o1 across math, code, and reasoning tasks.</p>

<h2>Models</h2>

<p><strong>1.5B Qwen DeepSeek R1</strong></p>

<pre><code>ollama run deepseek-r1:1.5b
</code></pre>

<p><strong>7B Qwen DeepSeek R1</strong></p>

<pre><code>ollama run deepseek-r1:7b
</code></pre>

<p><strong>8B Llama DeepSeek R1</strong></p>

<pre><code>ollama run deepseek-r1:8b
</code></pre>

<p><strong>14B Qwen DeepSeek R1</strong></p>

<pre><code>ollama run deepseek-r1:14b
</code></pre>

<p><strong>32B Qwen DeepSeek R1</strong></p>

<pre><code>ollama run deepseek-r1:32b
</code></pre>

<p><strong>70B Llama DeepSeek R1</strong></p>

<pre><code>ollama run deepseek-r1:70b
</code></pre>

<p><strong>671B DeepSeek R1</strong></p>

<pre><code>ollama run deepseek-r1:671b
</code></pre>

<p><img src="https://ollama.com/assets/library/deepseek-r1/e44d096e-fa46-4cae-b2f2-53991e8c8da0" alt="deepseek"></p>

        
      </div>
    
  </div>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Authors seek Meta's torrent client logs and seeding data in AI piracy probe (137 pts)]]></title>
            <link>https://torrentfreak.com/authors-seek-metas-torrent-client-logs-and-seeding-data-in-ai-piracy-probe-250120/</link>
            <guid>42772771</guid>
            <pubDate>Mon, 20 Jan 2025 20:38:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://torrentfreak.com/authors-seek-metas-torrent-client-logs-and-seeding-data-in-ai-piracy-probe-250120/">https://torrentfreak.com/authors-seek-metas-torrent-client-logs-and-seeding-data-in-ai-piracy-probe-250120/</a>, See on <a href="https://news.ycombinator.com/item?id=42772771">Hacker News</a></p>
Couldn't get https://torrentfreak.com/authors-seek-metas-torrent-client-logs-and-seeding-data-in-ai-piracy-probe-250120/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[ROCm Device Support Wishlist (170 pts)]]></title>
            <link>https://github.com/ROCm/ROCm/discussions/4276</link>
            <guid>42772170</guid>
            <pubDate>Mon, 20 Jan 2025 19:31:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ROCm/ROCm/discussions/4276">https://github.com/ROCm/ROCm/discussions/4276</a>, See on <a href="https://news.ycombinator.com/item?id=42772170">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      

      <div data-body-version="299b9ab50a702b61e654a07387c220b1fb5c6d12f8237f4cbfff86784c2f2b0b" data-error="" id="discussioncomment-11894448" data-gid="DC_kwDOAzpr8c4AtX6w" data-url="/ROCm/ROCm/discussions/4276/comments/11894448" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
        



      <div>
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11894448/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Honestly, anything that has 16GB VRAM or more (or the ability to have reserved more, for eg. the iGPUs like 680/780/890M and Strix Halo iGPUs).</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-07e6d4f2-e873-4340-a7b4-cc09177208fa" for="discussion-upvote-button-DiscussionComment-11894448" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    1 reply
                  </span>
                </p>
            </div>

        </div>
          <div data-child-comments="" id="child-comments-discussioncomment-11894448">
      <p><a data-hydro-click="{&quot;event_type&quot;:&quot;discussions.click&quot;,&quot;payload&quot;:{&quot;event_context&quot;:&quot;DISCUSSION_VIEW&quot;,&quot;target&quot;:&quot;USER_PROFILE_LINK&quot;,&quot;current_repository_id&quot;:54160369,&quot;discussion_repository_id&quot;:54160369,&quot;org_level&quot;:false,&quot;discussion_id&quot;:7850354,&quot;discussion_comment_id&quot;:11896093,&quot;originating_url&quot;:&quot;https://github.com/ROCm/ROCm/discussions/4276&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="738f5e1a0c79912c63b83622201031e6c8db67ed112aea719ab7d413def28e9d" data-hovercard-type="user" data-hovercard-url="/users/niklassheth/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/niklassheth"><img src="https://avatars.githubusercontent.com/u/20130217?s=60&amp;v=4" width="30" height="30" alt="@niklassheth"></a></p>

        <div data-body-version="3366d9f7e19e5e556f43df8081525bcb532d9434f2a1a325fd67425a3e1fc6c8" id="discussioncomment-11896093" data-gid="DC_kwDOAzpr8c4AtYUd" data-url="/ROCm/ROCm/discussions/4276/comments/11896093" data-error="">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896093/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">As evidence for this, you can look at the used price of GPUs. Even an 8 year old P40 is going for $300 on eBay because it has 24GB VRAM. If the MI60 was supported it could be a good budget option.</p>
    </div>
    
</task-lists>

          

        </div>

    </div>


        
    </div>
  <div data-body-version="9a4ede9d2f426b2cac5c97c9d588b433e455df2f2d11f82b00ab42d18dfbcc2d" data-error="" id="discussioncomment-11894747" data-gid="DC_kwDOAzpr8c4AtX_b" data-url="/ROCm/ROCm/discussions/4276/comments/11894747" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11894747/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">I would like support for ROCm to be restored to all the relatively recent GPUs (last 5-6 years) AMD has released and then dropped ROCm support for.  New I could not care much about. Actually supporting the AMD cards I bought in the past would be great.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-46052c64-2b58-4120-a6cf-206ce13fbac7" for="discussion-upvote-button-DiscussionComment-11894747" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="9eab11215d5ae982b020b1b9ab57bf9509331d016278ec1083667515affd030d" data-error="" id="discussioncomment-11894841" data-gid="DC_kwDOAzpr8c4AtYA5" data-url="/ROCm/ROCm/discussions/4276/comments/11894841" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11894841/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">I think it might be interesting to share here that Debian has built a CI at <a href="https://ci.rocm.debian.net/" rel="nofollow">ci.rocm.debian.net</a> where the ROCm stack, and any package that depends on it, is continuously tested. Our CI includes all of the architectures listed above.</p>
<p dir="auto">We would be happy to cooperate on increasing service support for Debian and derivatives.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-66903d35-f63f-4fec-8302-b6b1fe0fab74" for="discussion-upvote-button-DiscussionComment-11894841" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="baf615a896f6281ae4dce8c4e2704158995d26e3d65d562d075cbd45095fcb3f" data-error="" id="discussioncomment-11894876" data-gid="DC_kwDOAzpr8c4AtYBc" data-url="/ROCm/ROCm/discussions/4276/comments/11894876" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
        



      <div>
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11894876/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">This is not a "device" support wish, but a "platform" one. Stable Diffusion on native Windows with AMD GPUs is not possible until we get "Windows" support for "AI Libraries" (specifically MIOpen) here: <a href="https://rocm.docs.amd.com/projects/install-on-windows/en/develop/reference/component-support.html" rel="nofollow">https://rocm.docs.amd.com/projects/install-on-windows/en/develop/reference/component-support.html</a></p>
<p dir="auto">This is required to get PyTorch working. I've seen so many AMD users in recent times selling their AMD GPUs and buying "the competition" because WSL and ZLUDA are their only options, and those are half-baked solutions. Native Windows support should be a top priority.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-f247b436-6275-427f-8fa9-e125513f1e76" for="discussion-upvote-button-DiscussionComment-11894876" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    1 reply
                  </span>
                </p>
            </div>

        </div>
          <div data-child-comments="" id="child-comments-discussioncomment-11894876">
      <p><a data-hydro-click="{&quot;event_type&quot;:&quot;discussions.click&quot;,&quot;payload&quot;:{&quot;event_context&quot;:&quot;DISCUSSION_VIEW&quot;,&quot;target&quot;:&quot;USER_PROFILE_LINK&quot;,&quot;current_repository_id&quot;:54160369,&quot;discussion_repository_id&quot;:54160369,&quot;org_level&quot;:false,&quot;discussion_id&quot;:7850354,&quot;discussion_comment_id&quot;:11895073,&quot;originating_url&quot;:&quot;https://github.com/ROCm/ROCm/discussions/4276&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="1e8bb9a91b6ac1e121d7c4a51111e736de21a302b5d916fa108e8f7b88b11dcd" data-hovercard-type="user" data-hovercard-url="/users/tocram1/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/tocram1"><img src="https://avatars.githubusercontent.com/u/22381620?s=60&amp;v=4" width="30" height="30" alt="@tocram1"></a></p>

        <div data-body-version="59dcaa13ac84895a8d972d45bb76f53ce6fdd795fa4f412208b6ec88f5bb79e3" id="discussioncomment-11895073" data-gid="DC_kwDOAzpr8c4AtYEh" data-url="/ROCm/ROCm/discussions/4276/comments/11895073" data-error="">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895073/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Definitely agree on this one, this is a major hurdle in MY opinion.</p>
    </div>
    
</task-lists>

          

        </div>

    </div>


        
    </div>
  <div data-body-version="75fc6a10eb474b0ac38ca75b93996824dad488c2c5d63281ce5943635eb31b55" data-error="" id="discussioncomment-11894914" data-gid="DC_kwDOAzpr8c4AtYCC" data-url="/ROCm/ROCm/discussions/4276/comments/11894914" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
        



      <div>
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11894914/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">A bit older than the ones listed there, but I own a 5700XT, and a good few other people do too, from my extensive looking for how to get it to work online.</p>
<p dir="auto">Still holding on to the precompiled wheel for torch 1.13 ROCM 5.2 for Python 3.10 which is the last one that works (after setting HSA_OVERRIDE_GFX_VERSION). Later versions seem to either outright crash, or import correctly but then crash when a tensor is sent to the GPU.</p>
<p dir="auto">Using this older version as a workaround was doable back when torch 2.0 was new, but now as most new code has already been using 2.0+ for a while, it's effectively not functional at all anymore for any recently written code.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-3e032699-4bf6-4370-b1d3-6ddd7baaaec0" for="discussion-upvote-button-DiscussionComment-11894914" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    1 reply
                  </span>
                </p>
            </div>

        </div>
          <div data-child-comments="" id="child-comments-discussioncomment-11894914">
      <p><a data-hydro-click="{&quot;event_type&quot;:&quot;discussions.click&quot;,&quot;payload&quot;:{&quot;event_context&quot;:&quot;DISCUSSION_VIEW&quot;,&quot;target&quot;:&quot;USER_PROFILE_LINK&quot;,&quot;current_repository_id&quot;:54160369,&quot;discussion_repository_id&quot;:54160369,&quot;org_level&quot;:false,&quot;discussion_id&quot;:7850354,&quot;discussion_comment_id&quot;:11895908,&quot;originating_url&quot;:&quot;https://github.com/ROCm/ROCm/discussions/4276&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="fb945d44ec154bf7c9a296dc4c0adfd92652dd919a443c392fae053fa4dab4bc" data-hovercard-type="user" data-hovercard-url="/users/SicLuceatLux/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/SicLuceatLux"><img src="https://avatars.githubusercontent.com/u/43041463?s=60&amp;v=4" width="30" height="30" alt="@SicLuceatLux"></a></p>

        

    </div>


        
    </div>
  <div data-body-version="0c261b8b353a6a30cb9a1d99f88e3f6d4a9c1043e5f1ccd22425a35a36cedb04" data-error="" id="discussioncomment-11894998" data-gid="DC_kwDOAzpr8c4AtYDW" data-url="/ROCm/ROCm/discussions/4276/comments/11894998" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11894998/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Considering my GPU( 6600 XT)  was released near the end of 2021 it would be nice to know that I don't need to buy a new GPU every year just to have support. It would also be nice to have actual proper Windows support instead of having to deal with the clusterfuck that is Zluda, or other translation layers. This kind of treatment from AMD is why I'll probably go nvidia the next time my budget allows it.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-563ac12c-b2cc-4200-86b1-8876cb9400ce" for="discussion-upvote-button-DiscussionComment-11894998" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="a4116625bd825f160374a9f736bc058290ceb23dd78775a84ba17d56344fe604" data-error="" id="discussioncomment-11895157" data-gid="DC_kwDOAzpr8c4AtYF1" data-url="/ROCm/ROCm/discussions/4276/comments/11895157" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
        



      <div>
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895157/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">APU support opens the door for introducing this software to a wide audience, please consider hitting the entire APU line (3 and 3.5.) Early ROCm worked for 780m and got me in the front door of working with this software at all (that said I had to use env var hacks to get it functioning). Later versions of ROCm stopped working at all.</p>
<p dir="auto">The hobbyist crowd would greatly benefit from APU support, which hopefully has the AMD financial incentive of market share and product familiarity (hobbyist engineers who do something neat at home and then bring the concepts to work, where you then pick up the larger purchases)</p>
<p dir="auto">If I was able to feel confident in better consumer ROCm support I would have gladly dropped money for 2 AMD graphics cards for the LLM stuff I do.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-cc9cfe79-f4fb-44e8-b72e-684504465400" for="discussion-upvote-button-DiscussionComment-11895157" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    2 replies
                  </span>
                </p>
            </div>

        </div>
          <div data-child-comments="" id="child-comments-discussioncomment-11895157">

    <div>
      <p><a data-hydro-click="{&quot;event_type&quot;:&quot;discussions.click&quot;,&quot;payload&quot;:{&quot;event_context&quot;:&quot;DISCUSSION_VIEW&quot;,&quot;target&quot;:&quot;USER_PROFILE_LINK&quot;,&quot;current_repository_id&quot;:54160369,&quot;discussion_repository_id&quot;:54160369,&quot;org_level&quot;:false,&quot;discussion_id&quot;:7850354,&quot;discussion_comment_id&quot;:11895323,&quot;originating_url&quot;:&quot;https://github.com/ROCm/ROCm/discussions/4276&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="08551d753a51e5999ddb8a5a0c88a89d7354468fcdbedea428c9cbc91efbed94" data-hovercard-type="user" data-hovercard-url="/users/randomstuff/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/randomstuff"><img src="https://avatars.githubusercontent.com/u/946727?s=60&amp;v=4" width="30" height="30" alt="@randomstuff"></a></p>

        <div data-body-version="7d32692793bbb76ac2be4d90993111340635b8e9c7da65be969ea9fcca74dc4e" id="discussioncomment-11895323" data-gid="DC_kwDOAzpr8c4AtYIb" data-url="/ROCm/ROCm/discussions/4276/comments/11895323" data-error="">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895323/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">I have been somewhat successfully been able to run Stable Diffusion on a <a href="https://www.gabriel.urdhr.fr/2022/08/28/trying-to-run-stable-diffusion-on-amd-ryzen-5-5600g/" rel="nofollow">AMD Ryzen 5 5600G</a> APU (with 16 Go / 32 Go of RAM).</p>
<p dir="auto">The performance speedup was not super impressive compared to the same implementation executed on CPU or the OpenVINO CPU implementation. I am not sure if we could get a nice speedup for this kind of device but it would be really interesting.</p>
<p dir="auto">The thing was highly unstable and had a tendency of crashing the whole system real quick. Anyway, it was nearly working for some programs. Si maybe there is not so many things missing to a have something stable :)</p>
    </div>
    
</task-lists>

          

        </div>

    </div>
    <div>
      <p><a data-hydro-click="{&quot;event_type&quot;:&quot;discussions.click&quot;,&quot;payload&quot;:{&quot;event_context&quot;:&quot;DISCUSSION_VIEW&quot;,&quot;target&quot;:&quot;USER_PROFILE_LINK&quot;,&quot;current_repository_id&quot;:54160369,&quot;discussion_repository_id&quot;:54160369,&quot;org_level&quot;:false,&quot;discussion_id&quot;:7850354,&quot;discussion_comment_id&quot;:11895573,&quot;originating_url&quot;:&quot;https://github.com/ROCm/ROCm/discussions/4276&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="32cfbb4ae281dd01619117b092b54359b030ceebe3fa78fae5d7a1d00c8e3af1" data-hovercard-type="user" data-hovercard-url="/users/Abdull/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/Abdull"><img src="https://avatars.githubusercontent.com/u/529862?s=60&amp;v=4" width="30" height="30" alt="@Abdull"></a></p>

        <div data-body-version="b0ed72ab4fe39ad8c54bf1ce3679bf025c35489ed37cf82d6366f4c770e98616" id="discussioncomment-11895573" data-gid="DC_kwDOAzpr8c4AtYMV" data-url="/ROCm/ROCm/discussions/4276/comments/11895573" data-error="">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895573/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">important for Framework 13 AMD laptop users</p>
    </div>
    
</task-lists>

          

        </div>

    </div>

</div>


        
    </div>
  <div data-body-version="6dd366079f09ed3314a27f573c24f7694472b1e9620886d0a44809818ef06c97" data-error="" id="discussioncomment-11895185" data-gid="DC_kwDOAzpr8c4AtYGR" data-url="/ROCm/ROCm/discussions/4276/comments/11895185" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
        



      <div>
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895185/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">why not just all, like the other company? ;-)</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-0cb80f69-16e9-4b33-bd3e-9e7a6375862a" for="discussion-upvote-button-DiscussionComment-11895185" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    3 replies
                  </span>
                </p>
            </div>

        </div>
          <div data-child-comments="" id="child-comments-discussioncomment-11895185">

    <div>
      <p><a data-hydro-click="{&quot;event_type&quot;:&quot;discussions.click&quot;,&quot;payload&quot;:{&quot;event_context&quot;:&quot;DISCUSSION_VIEW&quot;,&quot;target&quot;:&quot;USER_PROFILE_LINK&quot;,&quot;current_repository_id&quot;:54160369,&quot;discussion_repository_id&quot;:54160369,&quot;org_level&quot;:false,&quot;discussion_id&quot;:7850354,&quot;discussion_comment_id&quot;:11895241,&quot;originating_url&quot;:&quot;https://github.com/ROCm/ROCm/discussions/4276&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="58150230df92bc1e3b1bc985b82360100bf1bdba1f722ceda041f8f23cb00282" data-hovercard-type="user" data-hovercard-url="/users/powderluv/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/powderluv"><img src="https://avatars.githubusercontent.com/u/74956?s=60&amp;v=4" width="30" height="30" alt="@powderluv"></a></p>

        <div data-body-version="94f3e3e23b56bb956c129e2c7dc34d254852eeea837fccfffe0c3067d22f4cf4" id="discussioncomment-11895241" data-gid="DC_kwDOAzpr8c4AtYHJ" data-url="/ROCm/ROCm/discussions/4276/comments/11895241" data-error="">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895241/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Eventually. But what would you prioritize? :)</p>
    </div>
    
</task-lists>

          

        </div>

    </div>
    <div>
      <p><a data-hydro-click="{&quot;event_type&quot;:&quot;discussions.click&quot;,&quot;payload&quot;:{&quot;event_context&quot;:&quot;DISCUSSION_VIEW&quot;,&quot;target&quot;:&quot;USER_PROFILE_LINK&quot;,&quot;current_repository_id&quot;:54160369,&quot;discussion_repository_id&quot;:54160369,&quot;org_level&quot;:false,&quot;discussion_id&quot;:7850354,&quot;discussion_comment_id&quot;:11895322,&quot;originating_url&quot;:&quot;https://github.com/ROCm/ROCm/discussions/4276&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="034fd912b967c7ac7fac064c34a1ccfb3958f66bd1dacdc08fc1105d3d840e37" data-hovercard-type="user" data-hovercard-url="/users/shiltian/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/shiltian"><img src="https://avatars.githubusercontent.com/u/7587318?s=60&amp;v=4" width="30" height="30" alt="@shiltian"></a></p>

        <div data-body-version="444c854734197b911f0b04229e1509352e0fc94e5bf147f1ee9568e88a92539d" id="discussioncomment-11895322" data-gid="DC_kwDOAzpr8c4AtYIa" data-url="/ROCm/ROCm/discussions/4276/comments/11895322" data-error="">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895322/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">my .02 is starting from all RDNA 2 and newer. that's gonna take time. by the time they are all supported, RDNA 1 might have fairly faded out.</p>
    </div>
    
</task-lists>

          

        </div>

    </div>
    <div>
      <p><a data-hydro-click="{&quot;event_type&quot;:&quot;discussions.click&quot;,&quot;payload&quot;:{&quot;event_context&quot;:&quot;DISCUSSION_VIEW&quot;,&quot;target&quot;:&quot;USER_PROFILE_LINK&quot;,&quot;current_repository_id&quot;:54160369,&quot;discussion_repository_id&quot;:54160369,&quot;org_level&quot;:false,&quot;discussion_id&quot;:7850354,&quot;discussion_comment_id&quot;:11896209,&quot;originating_url&quot;:&quot;https://github.com/ROCm/ROCm/discussions/4276&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="94548708abd6a2498081a9869a81137e0e2ef62a4ed3bc113fce92c1c962dfae" data-hovercard-type="user" data-hovercard-url="/users/TKCZ/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/TKCZ"><img src="https://avatars.githubusercontent.com/u/22118472?s=60&amp;v=4" width="30" height="30" alt="@TKCZ"></a></p>

        <div data-body-version="c416c01b691f7b738b438424cc3d8c9266f848bdc999a364f68ea3f3048f1f14" id="discussioncomment-11896209" data-gid="DC_kwDOAzpr8c4AtYWR" data-url="/ROCm/ROCm/discussions/4276/comments/11896209" data-error="">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896209/language_detections">
      
    </inline-machine-translation>
  <div>
        <blockquote>
<p dir="auto">Eventually. But what would you prioritize? :)</p>
</blockquote>
<p dir="auto">Definitelly start with RDNA2 &amp; 3, since these series offered models with 16GB VRAM. That much memory deserves to stay around officially supported for as long as possible. Let alone the fact that cards like 6800XT still pack decent punch for local AI inference.</p>
    </div>
    
</task-lists>

          

        </div>

    </div>

</div>


        
    </div>
  <div data-body-version="9f4dab7adc8c5e819850961f54e8c18019d30f85b4c1a707fd0cd3a15bac7c40" data-error="" id="discussioncomment-11895317" data-gid="DC_kwDOAzpr8c4AtYIV" data-url="/ROCm/ROCm/discussions/4276/comments/11895317" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895317/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">ROCm windows, All RDNA3 and newer. Don't forget integrated GPUs. Maybe next year?</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-9f39403a-251c-4a3f-ab44-50269333e5bd" for="discussion-upvote-button-DiscussionComment-11895317" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="7b94ee7e4a683ea60daf9aa12987a695462f6dfe1dc018b8560ecc2e80d02174" data-error="" id="discussioncomment-11895348" data-gid="DC_kwDOAzpr8c4AtYI0" data-url="/ROCm/ROCm/discussions/4276/comments/11895348" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895348/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">I wish for AMD to look back at the RX 500  and the RX5000 series. And the reason being, the physical architectures for both lend themselves to really interesting compute because the RX 580 architecturally is very good to use as a modular scale up and scale down at 75 w. And based based on some back of the napkin maths that I've done an RX 580 8 gig with a 8 billion parameter model with a quant size of eight. Can pull about 15 to 30 tokens per second.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-6713e9db-8776-42b2-ab7a-134f822bdcb7" for="discussion-upvote-button-DiscussionComment-11895348" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>


          <div data-body-version="5456848d6531fe9f1f551b6f7229a70b9d12cac492941d4f1a0e1ed720aec788" data-error="" id="discussioncomment-11895367" data-gid="DC_kwDOAzpr8c4AtYJH" data-url="/ROCm/ROCm/discussions/4276/comments/11895367" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895367/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Hi,<br>
IMO all products going forward should be able to run all typical ML software: stable diffusion, LLMs, pytorch.<br>
Doesn't have to be crazy fast, but support it and then improve it over time.<br>
And simultaneously, but fine if at a lower pace, walk backwards and support the older products.</p>
<p dir="auto">So you should start by supporting strix halo and RDNA 4. Then RDNA 3 and prior APUs.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-c27855b0-7ef2-48ff-ba05-4cba14af7fb3" for="discussion-upvote-button-DiscussionComment-11895367" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="368d8a87f1f3eb9b8da0616cf95aca6b54ce9e0833bc0ce5dc74a2cb880d16d2" data-error="" id="discussioncomment-11895435" data-gid="DC_kwDOAzpr8c4AtYKL" data-url="/ROCm/ROCm/discussions/4276/comments/11895435" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895435/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Thank you for reaching out and at least trying to extend the device support. The limited consumer hardware support has always been one of the weakest point of ROCm, and if AMD is serious about the future of ROCm, at least any upcoming hardware should be supported. Being able to get used to a platform without spending 1000s is actually huge.<br>
Currently, even if unsupported, many actually work fine. I did and I'm still doing some PyTorch stuff on a 8700GE, which has a gfx1103 GPU in it. Works, but there are some nasty minor issues like this one here, causing the GPU driver to crash now and then, seemingly a firmware issue: <a data-error-text="Failed to load title" data-id="2474223008" data-permission-text="Title is private" data-url="https://github.com/lamikr/rocm_sdk_builder/issues/141" data-hovercard-type="issue" data-hovercard-url="/lamikr/rocm_sdk_builder/issues/141/hovercard" href="https://github.com/lamikr/rocm_sdk_builder/issues/141">lamikr/rocm_sdk_builder#141</a><br>
I think, it's a rather small step for AMD to make those 99% working devices to a 100% officially supported level.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-57cb16f5-c3d1-498c-9a2e-e5f430664f0a" for="discussion-upvote-button-DiscussionComment-11895435" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="054686a2c43c6262a1f6b4938167d23622b1aa0a11c76249c06691a6e298954a" data-error="" id="discussioncomment-11895444" data-gid="DC_kwDOAzpr8c4AtYKU" data-url="/ROCm/ROCm/discussions/4276/comments/11895444" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895444/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">At one time Kaveri was promoted as a hybrid processor, and while HSA was being implemented its support disappeared. It would be fair, given the promises of marketers, to make HSA + ROCm for APU Kaveri.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-356e5a80-3267-4c1b-acbd-0ada8783843b" for="discussion-upvote-button-DiscussionComment-11895444" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="313f54341a67795a6b3549078c47509a3bbe9a9250b1711d454fc3dfb6b44323" data-error="" id="discussioncomment-11895539" data-gid="DC_kwDOAzpr8c4AtYLz" data-url="/ROCm/ROCm/discussions/4276/comments/11895539" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
        



      <div>
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895539/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Missing poll option: actually support the ‚úÖ marked devices consistently</p>
<p dir="auto">There's not much point having a green icon in the support matrix if it doesn't mean your device is supported.<br>
aotriton supports only MI2xx, MI3xx, 7800, 7900.<br>
hipblaslt supports only MI2xx, MI3xx, 7800, 7900.<br>
Dao-AILab/flash-attention (which AMD contributed ROCm support to) doesn't support MI100. I think it's picky about consumer cards too but don't remember the models.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-37226189-31a4-4fad-bedf-c579ecabd45d" for="discussion-upvote-button-DiscussionComment-11895539" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    1 reply
                  </span>
                </p>
            </div>

        </div>
          <div data-child-comments="" id="child-comments-discussioncomment-11895539">
      <p><a data-hydro-click="{&quot;event_type&quot;:&quot;discussions.click&quot;,&quot;payload&quot;:{&quot;event_context&quot;:&quot;DISCUSSION_VIEW&quot;,&quot;target&quot;:&quot;USER_PROFILE_LINK&quot;,&quot;current_repository_id&quot;:54160369,&quot;discussion_repository_id&quot;:54160369,&quot;org_level&quot;:false,&quot;discussion_id&quot;:7850354,&quot;discussion_comment_id&quot;:11895810,&quot;originating_url&quot;:&quot;https://github.com/ROCm/ROCm/discussions/4276&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="76750ea7a41fa5e9e9b1ea2e04c74ab5908b60b290bbeac03e20dcfe27e8f1e8" data-hovercard-type="user" data-hovercard-url="/users/IMbackK/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/IMbackK"><img src="https://avatars.githubusercontent.com/u/13803414?s=60&amp;v=4" width="30" height="30" alt="@IMbackK"></a></p>

        <div data-body-version="1b50beb7542cde7184da8ea2e0afb47d7f62bdd41426fc550e5676f7b5cc55bf" id="discussioncomment-11895810" data-gid="DC_kwDOAzpr8c4AtYQC" data-url="/ROCm/ROCm/discussions/4276/comments/11895810" data-error="">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895810/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Yeah this is the big thing. Besides missing rdna1 support, the actual support matrix (not the mostly useless check mark, what the code supports) is mostly fine, except random libraries that then support only a subset of those.</p>
    </div>
    
</task-lists>

          

        </div>

    </div>


        
    </div>
  <div data-body-version="96b624a0db8669f4d32a09be017348010050de83168e06266ca1d3088deec029" data-error="" id="discussioncomment-11895566" data-gid="DC_kwDOAzpr8c4AtYMO" data-url="/ROCm/ROCm/discussions/4276/comments/11895566" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895566/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto"><strong>Chicken and egg.</strong>  I don't own any of your hardware because I can't run the software I want to run. Support Flux, Stable Diffusion, Tencent Hunyuan Video, Nvidia Cosmos, and I'll be in the market to buy your cards. As it stands, I can't leverage your offering.  I'll gladly build AMD workstations if your hardware can do the things I want.</p>
<p dir="auto"><strong>Prioritize VRAM.</strong> The next battle is local image and video models. Nobody wants to use hosted SaaS and all the film and VFX people will be running local Comfy, Hunyuan, etc. in just a few years time. These models need a tremendous amount of VRAM, so you need to build consumer/prosumer SKUs that have it.</p>
<p dir="auto">If you time this right and build high VRAM consumer cards with broad software support, the next generation of media production could ride on your platform.</p>
<p dir="auto">Let me underscore: you <em>must</em> be able to run popular image and video models.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-8d82a3f8-78d9-4455-b7b6-76c39c74bb48" for="discussion-upvote-button-DiscussionComment-11895566" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>

    



      <div data-body-version="09ee47da6431ac30b79faf5af02f101bfd50d483cfdbaa7b784b56efa45bee11" data-error="" id="discussioncomment-11895881" data-gid="DC_kwDOAzpr8c4AtYRJ" data-url="/ROCm/ROCm/discussions/4276/comments/11895881" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895881/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Long term support for APU's with a lot of ram and usable performance gets a bunch of developers to try things quickly and builds confidence in shipping to enterprise hardware in prod. Low power, high ram, usable performance and out of the box support for all the major tools with NO BUGS. Learn from Ballmer: Developers developers developers developers....</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-83513683-c4a5-4452-abc1-18d0b13407ca" for="discussion-upvote-button-DiscussionComment-11895881" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="429a8fd23a83a0f1f55e3bf56e326d8bab30e9ebc9e65a0897192fbad18c0115" data-error="" id="discussioncomment-11895947" data-gid="DC_kwDOAzpr8c4AtYSL" data-url="/ROCm/ROCm/discussions/4276/comments/11895947" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895947/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Basically, anything with more than 16GB of RAM. APUs capable of using system RAM could be a cost-effective entry-level option for running large language models (LLMs). While they might be slower, many people don‚Äôt require real-time LLM output.</p>
<p dir="auto">Everyone wants to run large models, but not everyone can afford to spend several thousand dollars on professional GPUs, multiple high-end prosumer cards, or Macs.</p>
<p dir="auto">The ROCm user base could grow rapidly if people could leverage their existing iGPUs for AI tasks. I wish my Raven Ridge APU could handle slow AI tasks, like sorting and tagging photos on my Nextcloud NAS, and similar applications.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-fe79600e-9a2d-4241-a4ab-f13eb5a7ffb9" for="discussion-upvote-button-DiscussionComment-11895947" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="60b2d147161c5e686c514dae9cdf1e582e98a78493f59d64b1479ae579d9bced" data-error="" id="discussioncomment-11895995" data-gid="DC_kwDOAzpr8c4AtYS7" data-url="/ROCm/ROCm/discussions/4276/comments/11895995" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11895995/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">xbox and playstation, period.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-cc72407d-7745-4740-9da5-a4657fe9a2e1" for="discussion-upvote-button-DiscussionComment-11895995" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="db5020ede0a87cb441e4c80ee05bdc1b63cb1628b9c9cc55d6023f527156673e" data-error="" id="discussioncomment-11896000" data-gid="DC_kwDOAzpr8c4AtYTA" data-url="/ROCm/ROCm/discussions/4276/comments/11896000" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896000/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Strix Halo and Phoenix APUs could possibly be the most popular so definitely them but full support for the 7600xt and better seems proper. including the 6000 series equivalents. I feel the 7600xt's appeal over a regular 7600 is ROCm more than gaming</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-18baf3d8-268d-4841-b5a6-4d5f4b20a7ef" for="discussion-upvote-button-DiscussionComment-11896000" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="4eb0d589f42b297099933e81c73b825739045177384b8a92ac25acf48779bcd9" data-error="" id="discussioncomment-11896009" data-gid="DC_kwDOAzpr8c4AtYTJ" data-url="/ROCm/ROCm/discussions/4276/comments/11896009" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896009/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">You have a much wider reach with iGPUs than with dGPUs, so I wouldn't stop supporting RDNA and Vega which AMD still sells widely.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-b440b199-229e-4756-9ee4-082580964130" for="discussion-upvote-button-DiscussionComment-11896009" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="6389b831651f10398b1800c0693ba58bd996c1ccf901d9d557f998797699a33e" data-error="" id="discussioncomment-11896170" data-gid="DC_kwDOAzpr8c4AtYVq" data-url="/ROCm/ROCm/discussions/4276/comments/11896170" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896170/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">For Windows and WSL users, shouldn't using DirectML be an option? I may be wrong here though</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-b8760a12-1b9b-4244-80af-60d64fe749e7" for="discussion-upvote-button-DiscussionComment-11896170" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="0d511713bb5eb3f4b1179446b5e95932e1f2401f25e9c3f9e8a403a21d5d94a4" data-error="" id="discussioncomment-11896197" data-gid="DC_kwDOAzpr8c4AtYWF" data-url="/ROCm/ROCm/discussions/4276/comments/11896197" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896197/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Would like to see the the W7600 and W7700 supported - small memory, but single-slot! Very useful for compact builds, or if more PCIe cards are needed. I've combined these with Mellanox NICs and Highpoint HBAs.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-94216dac-1578-46d2-93fa-aeeb95d1e4dd" for="discussion-upvote-button-DiscussionComment-11896197" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="ad8da41e9c92f907c38045f5d6e2ef421dcf89ded01a6affa6baecf8d968ea73" data-error="" id="discussioncomment-11896210" data-gid="DC_kwDOAzpr8c4AtYWS" data-url="/ROCm/ROCm/discussions/4276/comments/11896210" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896210/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">All of them. GCN 4 and up if I had to be hard-pressed into an answer.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-38d3cb38-b7e2-4786-9aef-131a59a9de5e" for="discussion-upvote-button-DiscussionComment-11896210" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="27e881f62dfca99403d134c3956b66d37cf90170fc599f4e5ea75120bf6d47f9" data-error="" id="discussioncomment-11896220" data-gid="DC_kwDOAzpr8c4AtYWc" data-url="/ROCm/ROCm/discussions/4276/comments/11896220" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896220/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">It's important to note that "Support" means wildly different things to different people.</p>
<ul dir="auto">
<li>The compiler knows how to emit code for the GPU</li>
<li>The driver knows how to load code onto / allocate memory etc for the GPU</li>
<li>The libraries have been compiled for that GPU (so can actually run)</li>
<li>The libraries have GPU-specific optimisations implemented (i.e. are faster)</li>
<li>The ROCm release process tests / validates on that GPU</li>
<li>The various CI systems run on that GPU</li>
<li>Variations on where tickets can be raised</li>
</ul>
<p dir="auto">I believe this is where things have gone south. The ROCm release testing / validation is thorough and somewhat linear in the number of GPUs tested on so adding more hardware to that set costs lots of time. The cards behave fairly similarly to one another so it's also not especially informative. However writing "Supported: Foo" without actually putting Foo through the same internal testing as all others seems problematic.</p>
<p dir="auto">What a decent fraction of people want is for ROCm to run on their gaming card(s). Whether it actually went through the internal validation is somewhat less important than it refusing to run entirely because the libraries haven't been compiled for their hardware.</p>
<p dir="auto">I think the solution to this is really obvious. Build the userspace software for all the targets. Let "Supported" continue to mean whatever it currently does. That'll mean people can install the ROCm distribution and get something which at least attempts to run on their hardware. Optionally introduce a new term, whatever marketing like, to refer to the hardware that isn't on the supported list.</p>
<p dir="auto">I personally don't care at all whether the latest ROCm release has been carefully tested on the graphics card I'm using locally but I'm really annoyed when it wasn't compiled for it.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-d959b2b9-1153-4707-957d-95847ec7801c" for="discussion-upvote-button-DiscussionComment-11896220" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="25a357989dad020ca60381a652abe8e51a10e3de2d0adeb53075b86de7328e0d" data-error="" id="discussioncomment-11896278" data-gid="DC_kwDOAzpr8c4AtYXW" data-url="/ROCm/ROCm/discussions/4276/comments/11896278" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896278/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">I'll keep it short, please don't drop gfx906.</p>
<p dir="auto">and maybe what I'm asking for is too much given that they seem to strictly include officially supported, CDNA, GPUs, but it would be lovely if you could get support for consumer GPUs eg. gfx906 and up added to prebuilt docker images like vllm-dev/-ci.<br>
thank you guys, ROCm is getting better and better.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-c02a500b-e04e-4b67-b4a2-1e470920e860" for="discussion-upvote-button-DiscussionComment-11896278" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="67d6d7105dd537c0790422c11383d05f508346f2128ab90a3c4ac5f4d21e7c56" data-error="" id="discussioncomment-11896331" data-gid="DC_kwDOAzpr8c4AtYYL" data-url="/ROCm/ROCm/discussions/4276/comments/11896331" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896331/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">I would like to see full-stack generational support on Linux with both the runtime and HIP SDK for consumer cards on RDNA[1|2|3] and then extending it to GCN where possible.</p>
<p dir="auto">At the moment AMD have structured [and in my opinion artificially limited] their compatibility matrices to the upper tiers of the hardware stacks, e.g. 6800 and up on RDNA2, preventing those who own 6750XT and lower stack cards from using ROCm/HIP.</p>
<p dir="auto">I understand that a lot of the unsupported hardware "just works" in some fashion but it is not officially supported nor documented as being supported or which components/libraries work and which do not - this is just bad all around as nobody will buy GPU hardware or buy into the ROCm software ecosystem without knowing before hand that the hardware is capable, the hardware feature set is fully supported in software and it is relatively easy to get up and running as an end user, this is especially bad from a PR perspective for AMD in wanting to get more people to use their hardware when it is restricted by hardware/pricing tiers and by the AMD software itself.</p>
<p dir="auto">Nobody is going to purchase a GPU in order to dip into trying out the ROCm ecosystem if they have to purchase the highest tiers of GPU to know for sure that ROCm will fully support the hardware.</p>
<p dir="auto">Artificial limits which segregate capable hardware through a lack of support in or a lack of enablement in software is not a good practice and it harms not only end-users/customers but AMD and ROCm adoption too.</p>
<p dir="auto">I would like to see AMD's stance on this change to one of starting from full enablement from the outset where the hardware features exist to do that and remove any artificial tiering/segregation entirely, tear down those walls.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-424addef-6d6f-44a3-9bda-3280ef11dedd" for="discussion-upvote-button-DiscussionComment-11896331" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="284b2343c73f6c317db7f14871d9dd54a3c4148d693f00cd831a70cc846e288d" data-error="" id="discussioncomment-11896340" data-gid="DC_kwDOAzpr8c4AtYYU" data-url="/ROCm/ROCm/discussions/4276/comments/11896340" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896340/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">You know, the reason AMD is way behind Nvidia in market share is because Nvidia literally supports every single discrete graphics processor model with CUDA from the low end to their high end.  They realized what AMD never has, and has continuously annoyed people, especially Linux users.  Not everyone writing software for CUDA has an ultra high end GPU product <em>because they can't afford them</em>.  But they have a very robust software development community where the underlying SDKs largely just works.  Intel has effectively realized the same thing with ARC.</p>
<p dir="auto">If AMD can't at least match the Linux support matrix with the Windows support matrix going forward you're rapidly going to grow completely irrelevant with new generations of software tinkerers who are primarily writing software on Linux. Windows from a compute performance point of view is demonstrably poor in comparison, and the user experience is degrading over time as Microsoft lets their OS foundation crumble while they're off chasing unicorns.</p>
<p dir="auto">What's my vote?  Every GPU currently under active driver support including Vega. Vega still has a fairly large-ish install base.  Arbitrary decisions to drop support may make business decisions, but it annoys customers who buy hardware only to have it dropped from support not long after it was still being sold.  But if you must narrow down your support matrix everything from the RX 5000 series and newer.  Either actually compete with the CUDA hardware support experience or get out of the game.  Those are your only options because that's the market you're trying to grow and Nvidia is the dominant player.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-6646e146-c08b-4df3-8bb2-927c82b815cb" for="discussion-upvote-button-DiscussionComment-11896340" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="65e02158d0d0f60fde3be97886b2f1f80177a108e4a09a0f193315454137f75c" data-error="" id="discussioncomment-11896361" data-gid="DC_kwDOAzpr8c4AtYYp" data-url="/ROCm/ROCm/discussions/4276/comments/11896361" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896361/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">I'm not sure why, but for me it seems that for example RDNA2 cards (and APUs) are not unsupported, but rather blocked. With setting <code>HSA_OVERRIDE_GFX_VERSION=10.3.0</code> ROCm (using pytorch) runs fine. Since Linux 6.10 and the APU memory fix it even works on my Ryzen 6000 notebook. Why the need for that environment variable?</p>
<p dir="auto">If you don't have the resources to run all tests on all types of consumer GPU/APU and you want to ensure that you provide a build that is fully tested: Why not an "enterprise release" that is fully tested and has only support for tested card and a "community edition" that has all GPUs enabled but is not fully tested (both build from the same source code with the same versions, only difference is the activated cards).</p>
<p dir="auto">If you're thinking supporting a lot of consumer GPUs is a waste of money: A big reason why NVIDIA is so successful in the AI market, because every student with even the smallest NVIDIA GPU was able to experiment with CUDA on her/his own machine. Years later when those students are working in the industry, with what products/APIs do they have experience? And when they make decisions, what will they buy for their company?<br>
AMD missed the first big wave, so they need to catch up, and maybe they get a second chance to really gain market share at some point (when for example NVIDIA is having issues with a new generation).</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-3db34814-8b6d-499d-8c08-1a49909c1da3" for="discussion-upvote-button-DiscussionComment-11896361" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="5adf4a21994224ef202e238c472a29caef73e0d49b91c99c4ccf15b93d227319" data-error="" id="discussioncomment-11896375" data-gid="DC_kwDOAzpr8c4AtYY3" data-url="/ROCm/ROCm/discussions/4276/comments/11896375" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896375/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">This is fundamentally misguided. It's not a question of which specific GPUs are supported now. It's a question of:</p>
<ol dir="auto">
<li>Will support consistently be added for new GPUs at the time of launch? Your competition has day 1 support for new GPUs, and doesn't arbitrarily decide that half the GPUs in a new generation will be incompatible.</li>
<li>Will support consistently be kept for GPUs that are currently supported? You competition still runs on ancient laptops out-of-the-box.</li>
<li>Do consumers trust you to keep to your word for 1 &amp; 2? You've walked back support before.</li>
</ol>
<p dir="auto">Reiterating from my prior comment <a href="https://github.com/amd/RyzenAI-SW/issues/2#issuecomment-1999684155" data-hovercard-type="issue" data-hovercard-url="/amd/RyzenAI-SW/issues/2/hovercard">here</a>:</p>
<blockquote>
<p dir="auto">Your competition has CUDA: Compute <em>Unified</em> Device Architecture - and indeed it supports practically everything from ancient laptops on up. In terms of effort versus reward, things target CUDA largely because they can port to it - using easily available hardware - <em>once</em> - and get fairly decent performance across a very large chunk of the market. (And then tune it later if necessary.)</p>
<p dir="auto"><a href="https://github.com/ROCm/ROCm/issues/1180" data-hovercard-type="issue" data-hovercard-url="/ROCm/ROCm/issues/1180/hovercard">Meanwhile</a>, <a href="https://github.com/ROCm/ROCm/issues/2429" data-hovercard-type="issue" data-hovercard-url="/ROCm/ROCm/issues/2429/hovercard">ROCm</a> <a href="https://github.com/ROCm/ROCm/issues/1659" data-hovercard-type="issue" data-hovercard-url="/ROCm/ROCm/issues/1659/hovercard">support</a> <a href="https://github.com/ROCm/ROCm/issues/1714" data-hovercard-type="issue" data-hovercard-url="/ROCm/ROCm/issues/1714/hovercard">is</a> <a href="https://github.com/ROCm/ROCm/issues/1306" data-hovercard-type="issue" data-hovercard-url="/ROCm/ROCm/issues/1306/hovercard">a</a> <a href="https://github.com/ROCm/ROCm/issues/666" data-hovercard-type="issue" data-hovercard-url="/ROCm/ROCm/issues/666/hovercard">minefield</a> <a href="https://github.com/ROCm/ROCm/issues/887" data-hovercard-type="issue" data-hovercard-url="/ROCm/ROCm/issues/887/hovercard">at</a> <a href="https://github.com/ROCm/ROCm/issues/1880" data-hovercard-type="issue" data-hovercard-url="/ROCm/ROCm/issues/1880/hovercard">best</a>.</p>
</blockquote>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-f69bf508-67d7-40d3-87f3-2fa27351cb64" for="discussion-upvote-button-DiscussionComment-11896375" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>
  <div data-body-version="ebcd1fe4bc079c968e2da6d9e486b26e57d33c33e1091450cf4878b73a1c1bd3" data-error="" id="discussioncomment-11896406" data-gid="DC_kwDOAzpr8c4AtYZW" data-url="/ROCm/ROCm/discussions/4276/comments/11896406" data-open-edit-form-after-load="false" data-quote-markdown=".js-comment-body">
            <task-lists disabled="" sortable="">
    <inline-machine-translation detected-language="en" detect-language-url="/ROCm/ROCm/discussions/4276/comments/11896406/language_detections">
      
    </inline-machine-translation>
  <div>
        <p dir="auto">Supporting gfx1036 is a must. It's probably the most available device and would allow many people to see if something is working on ROCm at all. It's also a great option to test on RDNA2 when you have other generation dGPUs.</p>
<p dir="auto">But, as many people said already, supporting all devices is the only correct option if you want broad developer support. After you've dropped gfx906 support on Windows (after it barely started) I don't see any reason to invest development effort in current AMD hardware. And the UDNA news sound like you'll immediately drop all previous architectures upon release.</p>
    </div>
    
</task-lists>

          <div>
              <div data-replace-remote-form-target="">
        <tool-tip id="tooltip-2dbbbb07-e9a3-4bad-8b37-be85b36d4d61" for="discussion-upvote-button-DiscussionComment-11896406" popover="manual" data-direction="s" data-type="description" data-view-component="true">You must be logged in to vote</tool-tip>
    </div>

                <p><span>
                    0 replies
                  </span>
                </p>
            </div>

        </div>


  
  <!-- '"` --><!-- </textarea></xmp> --></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I am (not) a failure: Lessons learned from six failed startup attempts (413 pts)]]></title>
            <link>http://blog.rongarret.info/2025/01/i-am-not-failure-lessons-learned-from.html</link>
            <guid>42771676</guid>
            <pubDate>Mon, 20 Jan 2025 18:40:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://blog.rongarret.info/2025/01/i-am-not-failure-lessons-learned-from.html">http://blog.rongarret.info/2025/01/i-am-not-failure-lessons-learned-from.html</a>, See on <a href="https://news.ycombinator.com/item?id=42771676">Hacker News</a></p>
Couldn't get http://blog.rongarret.info/2025/01/i-am-not-failure-lessons-learned-from.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Personalized Duolingo (kind of) for vocabulary building (121 pts)]]></title>
            <link>https://github.com/baturyilmaz/wordpecker-app</link>
            <guid>42770200</guid>
            <pubDate>Mon, 20 Jan 2025 16:27:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/baturyilmaz/wordpecker-app">https://github.com/baturyilmaz/wordpecker-app</a>, See on <a href="https://news.ycombinator.com/item?id=42770200">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">WordPecker App</h2><a id="user-content-wordpecker-app" aria-label="Permalink: WordPecker App" href="#wordpecker-app"></a></p>
<p dir="auto">A personalized language-learning app that brings the magic of Duolingo-style lessons to your own curated vocabulary lists and contexts.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/baturyilmaz/wordpecker-app/blob/main/docs/assets/createlist-addword.gif"><img src="https://github.com/baturyilmaz/wordpecker-app/raw/main/docs/assets/createlist-addword.gif" alt="WordPecker App" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The Idea</h2><a id="user-content-the-idea" aria-label="Permalink: The Idea" href="#the-idea"></a></p>
<p dir="auto">Learning a new language can be straightforward, but mastering it is the real challenge. While it's relatively easy to grasp the basics, developing strong communication or reading skills depends on having a solid vocabulary. To build this, you must learn words and phrases at various levels, yet exposure is key. For instance, if you've studied English but don‚Äôt live in an English-speaking country, advancing your skills becomes significantly harder. You can read books, watch movies, or browse blogs, but fully immersing yourself in the language is still difficult. Real progress often requires extra effort‚Äîstudying and revisiting words and phrases encountered in your daily life.</p>
<p dir="auto">However, this process can be inconvenient. You have to pause whatever you‚Äôre doing to note new words, search their meanings, record them, and then review them later. This is time-consuming and tiring. As a result, although you might improve, the learning process can feel painfully slow and inefficient.</p>
<p dir="auto">To solve this, I have an idea for an app that merges personalized learning with the efficiency of flashcards‚Äîa blend of Duolingo-like lessons and custom study lists.</p>
<p dir="auto">Imagine you‚Äôre reading a book‚Äîsay, Harry Potter. As you come across unfamiliar words, you open the app and create a new list with details like:</p>
<ul dir="auto">
<li>Name: Harry Potter Book</li>
<li>Description: The first book in the series</li>
<li>Context: Harry Potter</li>
</ul>
<p dir="auto">Once the list is created, you add the words or phrases you‚Äôve found. The app automatically provides their meanings in context after you save them, so you can continue reading without interruption.</p>
<p dir="auto">Later, you revisit the list and pick one of two options: Learn or Quiz.</p>
<ul dir="auto">
<li>Learn: This mode delivers structured lessons in a Duolingo style. Text, visuals, and exercises are dynamically generated using LLMs.</li>
</ul>
<p dir="auto"><strong>Note</strong>: Text-based multiple-choice questions are <em>currently</em> the only question type available.</p>
<ul dir="auto">
<li>Quiz: When you‚Äôre ready, you can test what you‚Äôve learned through interactive quizzes. They‚Äôre engaging and gamified, awarding points and showing your progress. For example, it might indicate you‚Äôve mastered 75% of your list.</li>
</ul>
<p dir="auto"><strong>Note</strong>: The feature that displays something like ‚ÄúYou‚Äôve mastered 75% of your list‚Äù is planned but not yet built.</p>
<p dir="auto">The key advantage is that the app keeps your learning tied to the context in which you originally saw the words. By returning to them in their original setting, you strengthen those specific neural pathways, speeding up retention and making learning significantly more effective.</p>
<p dir="auto"><strong>In short, it‚Äôs like having a personalized Duolingo where you can create and learn from your own lists. It‚Äôs a powerful way to immerse yourself in the language and make steady progress.</strong></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/baturyilmaz/wordpecker-app/blob/main/docs/assets/wordpecker.png"><img src="https://github.com/baturyilmaz/wordpecker-app/raw/main/docs/assets/wordpecker.png" alt="WordPecker App"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How It Works</h2><a id="user-content-how-it-works" aria-label="Permalink: How It Works" href="#how-it-works"></a></p>
<ol dir="auto">
<li><strong>Encounter New Words</strong>: While reading or watching something, open the app and add new words or phrases to a contextual list (e.g., Harry Potter Book, Science Blog, Netflix Show, etc.).</li>
<li><strong>Automatic Definitions</strong>: The app automatically fetches the word definitions (in the context) after you save them.</li>
<li><strong>Learn</strong>: Dive into Learn Mode, practice exercises‚Äîjust like Duolingo, but tailored to your words.</li>
<li><strong>Quiz</strong>: Switch to Quiz Mode anytime to check your retention.</li>
<li><strong>Review and Repeat</strong>: Visit your lists.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo</h2><a id="user-content-demo" aria-label="Permalink: Demo" href="#demo"></a></p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=QIwPGAXgNLU" rel="nofollow"><img src="https://camo.githubusercontent.com/ce8cb8004b6fb42934cc37701f48bd18959f395b41524265d1088717f4b0b72f/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f51497750474158674e4c552f302e6a7067" alt="Alt text" data-canonical-src="https://img.youtube.com/vi/QIwPGAXgNLU/0.jpg"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Create List &amp; Add Word</h3><a id="user-content-create-list--add-word" aria-label="Permalink: Create List &amp; Add Word" href="#create-list--add-word"></a></p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=t1U5vzm5Qw0" rel="nofollow"><img src="https://camo.githubusercontent.com/e92a75632b509e8a7e57fd5c09ddda1dd7ba32653ce162f889bc241b759d5740/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f74315535767a6d355177302f302e6a7067" alt="Alt text" data-canonical-src="https://img.youtube.com/vi/t1U5vzm5Qw0/0.jpg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Roadmap</h2><a id="user-content-roadmap" aria-label="Permalink: Roadmap" href="#roadmap"></a></p>
<ul dir="auto">
<li><strong>Additional Question Types</strong>: Currently, only text-based multiple-choice questions are supported. In the future, I plan to introduce more Duolingo-style exercises such as fill-in-the-blanks, listening comprehension, and visual matching.</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/baturyilmaz/wordpecker-app/blob/main/docs/assets/roadmap-questions.jpeg"><img src="https://github.com/baturyilmaz/wordpecker-app/raw/main/docs/assets/roadmap-questions.jpeg" alt="Roadmap"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Progress Tracking</strong>: Display detailed statistics‚Äîlike a mastery percentage‚Äîand possibly introduce daily goals or streaks to keep learners motivated.</p>
</li>
<li>
<p dir="auto"><strong>List Sharing</strong>: Allow users to share their custom vocabulary lists with others.</p>
</li>
<li>
<p dir="auto"><strong>Improved Onboarding</strong>: Provide a quick tutorial or sample list for new users, helping them understand the app‚Äôs features and workflow more easily.</p>
</li>
<li>
<p dir="auto"><strong>Integration with Other Platforms</strong>: Connecting with e-readers?, browsers, or note-taking apps so users can add new words without leaving those platforms.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisites</h3><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<ul dir="auto">
<li>Node.js &gt;= 16</li>
<li>npm or yarn</li>
<li>A Supabase account</li>
<li>An OpenAI API key</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Clone the repository:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/baturyilmaz/wordpecker-app.git
cd wordpecker-app"><pre>git clone https://github.com/baturyilmaz/wordpecker-app.git
<span>cd</span> wordpecker-app</pre></div>
<p dir="auto">Install dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install backend dependencies
cd backend
npm install

# Install frontend dependencies
cd frontend
npm install"><pre><span><span>#</span> Install backend dependencies</span>
<span>cd</span> backend
npm install

<span><span>#</span> Install frontend dependencies</span>
<span>cd</span> frontend
npm install</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration</h3><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">Create <code>.env</code> files:</p>
<p dir="auto">Backend <code>.env</code>:</p>
<div data-snippet-clipboard-copy-content="PORT=
OPENAI_API_KEY=
SUPABASE_URL=
SUPABASE_SERVICE_KEY="><pre><code>PORT=
OPENAI_API_KEY=
SUPABASE_URL=
SUPABASE_SERVICE_KEY=
</code></pre></div>
<p dir="auto">Frontend <code>.env</code>:</p>
<div data-snippet-clipboard-copy-content="VITE_SUPABASE_URL=
VITE_SUPABASE_ANON_KEY=
VITE_API_URL="><pre><code>VITE_SUPABASE_URL=
VITE_SUPABASE_ANON_KEY=
VITE_API_URL=
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Development</h3><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<p dir="auto">Start the backend:</p>

<p dir="auto">Start the frontend:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<ul dir="auto">
<li>Frontend: React.js with TypeScript</li>
<li>Backend: Express.js</li>
<li>Database: Supabase (PostgreSQL)</li>
<li>Auth: Supabase Auth</li>
<li>AI: OpenAI API</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome!</p>
<ol dir="auto">
<li>Fork the repo</li>
<li>Create a feature branch</li>
<li>Commit changes</li>
<li>Push to your branch</li>
<li>Open a pull request</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto"><a href="https://github.com/baturyilmaz/wordpecker-app/blob/main/LICENSE">MIT</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: I'm Peter Roberts, immigration attorney, who does work for YC and startups. AMA (305 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=42770125</link>
            <guid>42770125</guid>
            <pubDate>Mon, 20 Jan 2025 16:20:29 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=42770125">Hacker News</a></p>
Couldn't get https://news.ycombinator.com/item?id=42770125: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Mixxx: GPL DJ Software (522 pts)]]></title>
            <link>https://mixxx.org/</link>
            <guid>42769871</guid>
            <pubDate>Mon, 20 Jan 2025 15:53:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mixxx.org/">https://mixxx.org/</a>, See on <a href="https://news.ycombinator.com/item?id=42769871">Hacker News</a></p>
Couldn't get https://mixxx.org/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Reverse engineering my #1 Hacker News article (121 pts)]]></title>
            <link>https://danielwirtz.com/blog/successful-hacker-news-article</link>
            <guid>42769325</guid>
            <pubDate>Mon, 20 Jan 2025 14:53:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://danielwirtz.com/blog/successful-hacker-news-article">https://danielwirtz.com/blog/successful-hacker-news-article</a>, See on <a href="https://news.ycombinator.com/item?id=42769325">Hacker News</a></p>
Couldn't get https://danielwirtz.com/blog/successful-hacker-news-article: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek-R1 (1397 pts)]]></title>
            <link>https://github.com/deepseek-ai/DeepSeek-R1</link>
            <guid>42768072</guid>
            <pubDate>Mon, 20 Jan 2025 12:37:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/deepseek-ai/DeepSeek-R1">https://github.com/deepseek-ai/DeepSeek-R1</a>, See on <a href="https://news.ycombinator.com/item?id=42768072">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">DeepSeek-R1</h2><a id="user-content-deepseek-r1" aria-label="Permalink: DeepSeek-R1" href="#deepseek-r1"></a></p>



<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true"><img src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3"></a>
</p>
<hr>
<p><a href="https://www.deepseek.com/" rel="nofollow">
    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/badge.svg?raw=true">
  </a>
  <a href="https://chat.deepseek.com/" rel="nofollow">
    <img alt="Chat" src="https://camo.githubusercontent.com/a8ed26619b0338e36bfd80f920c9fe96127ff7f12f25a0190e2a94d00a4fa5b9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa496253230436861742d446565705365656b25323052312d3533366166353f636f6c6f723d353336616635266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20R1-536af5?color=536af5&amp;logoColor=white">
  </a>
  <a href="https://huggingface.co/deepseek-ai" rel="nofollow">
    <img alt="Hugging Face" src="https://camo.githubusercontent.com/5e3115539d4583e22d65cb89eb1759e767cb9e1d70772923292fcfc80a654be4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d446565705365656b25323041492d6666633130373f636f6c6f723d666663313037266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;logoColor=white">
  </a>
</p>
<p><a href="https://discord.gg/Tc7c45Zzu5" rel="nofollow">
    <img alt="Discord" src="https://camo.githubusercontent.com/e227481a149714ed5187e4fd0b60b9f736099c2dd2083e6c091e29f1446cbb1a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d446565705365656b25323041492d3732383964613f6c6f676f3d646973636f7264266c6f676f436f6c6f723d776869746526636f6c6f723d373238396461" data-canonical-src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;logoColor=white&amp;color=7289da">
  </a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true">
    <img alt="Wechat" src="https://camo.githubusercontent.com/562efc618da65f0a69bc804395005b8124f5c2ed2eb73441c4e359185cc01467/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5765436861742d446565705365656b25323041492d627269676874677265656e3f6c6f676f3d776563686174266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;logoColor=white">
  </a>
  <a href="https://twitter.com/deepseek_ai" rel="nofollow">
    <img alt="Twitter Follow" src="https://camo.githubusercontent.com/8272710ecd020c821b4f62c1c455efb89e0db4eb179c5f5f971c3c1f69452c54/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f547769747465722d646565707365656b5f61692d77686974653f6c6f676f3d78266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;logoColor=white">
  </a>
</p>
<p><a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-CODE">
    <img alt="Code License" src="https://camo.githubusercontent.com/d8caf1d64169802e8094bcf0013f0b54d3a9547263b4e59eb43531d7d77993e4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f64655f4c6963656e73652d4d49542d6635646535333f26636f6c6f723d663564653533" data-canonical-src="https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;color=f5de53">
  </a>
  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-MODEL">
    <img alt="Model License" src="https://camo.githubusercontent.com/edb8b566827851c52a732ee62c71e1c0231f588d5181c5925a518d98f35b4ff4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d6f64656c5f4c6963656e73652d4d6f64656c5f41677265656d656e742d6635646535333f26636f6c6f723d663564653533" data-canonical-src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;color=f5de53">
  </a>
</p>
<p dir="auto">
  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"><b>Paper Link</b>üëÅÔ∏è</a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">1. Introduction</h2><a id="user-content-1-introduction" aria-label="Permalink: 1. Introduction" href="#1-introduction"></a></p>
<p dir="auto">We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.
DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.
With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.
However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,
we introduce DeepSeek-R1, which incorporates cold-start data before RL.
DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.
To support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/figures/benchmark.jpg"><img width="80%" src="https://github.com/deepseek-ai/DeepSeek-R1/raw/main/figures/benchmark.jpg"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">2. Model Summary</h2><a id="user-content-2-model-summary" aria-label="Permalink: 2. Model Summary" href="#2-model-summary"></a></p>
<hr>
<p dir="auto"><strong>Post-Training: Large-Scale Reinforcement Learning on the Base Model</strong></p>
<ul dir="auto">
<li>
<p dir="auto">We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.</p>
</li>
<li>
<p dir="auto">We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.
We believe the pipeline will benefit the industry by creating better models.</p>
</li>
</ul>
<hr>
<p dir="auto"><strong>Distillation: Smaller Models Can Be Powerful Too</strong></p>
<ul dir="auto">
<li>We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.</li>
<li>Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">3. Model Downloads</h2><a id="user-content-3-model-downloads" aria-label="Permalink: 3. Model Downloads" href="#3-model-downloads"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DeepSeek-R1 Models</h3><a id="user-content-deepseek-r1-models" aria-label="Permalink: DeepSeek-R1 Models" href="#deepseek-r1-models"></a></p>
<div dir="auto">
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><strong>Model</strong></th>
<th><strong>#Total Params</strong></th>
<th><strong>#Activated Params</strong></th>
<th><strong>Context Length</strong></th>
<th><strong>Download</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>DeepSeek-R1-Zero</td>
<td>671B</td>
<td>37B</td>
<td>128K</td>
<td><a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero" rel="nofollow">ü§ó HuggingFace</a></td>
</tr>
<tr>
<td>DeepSeek-R1</td>
<td>671B</td>
<td>37B</td>
<td>128K</td>
<td><a href="https://huggingface.co/deepseek-ai/DeepSeek-R1" rel="nofollow">ü§ó HuggingFace</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</div>
<p dir="auto">DeepSeek-R1-Zero &amp; DeepSeek-R1 are trained based on DeepSeek-V3-Base.
For more details regrading the model architecture, please refer to <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a> repository.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DeepSeek-R1-Distill Models</h3><a id="user-content-deepseek-r1-distill-models" aria-label="Permalink: DeepSeek-R1-Distill Models" href="#deepseek-r1-distill-models"></a></p>

<p dir="auto">DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.
We slightly change their configs and tokenizers. Please use our setting to run these models.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">4. Evaluation Results</h2><a id="user-content-4-evaluation-results" aria-label="Permalink: 4. Evaluation Results" href="#4-evaluation-results"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DeepSeek-R1-Evaluation</h3><a id="user-content-deepseek-r1-evaluation" aria-label="Permalink: DeepSeek-R1-Evaluation" href="#deepseek-r1-evaluation"></a></p>
<p dir="auto">For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="570d04263f45e653820916ae742049fc">$0.6$</math-renderer>, a top-p value of <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="570d04263f45e653820916ae742049fc">$0.95$</math-renderer>, and generate 64 responses per query to estimate pass@1.</p>
<div dir="auto">
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Category</th>
<th>Benchmark (Metric)</th>
<th>Claude-3.5-Sonnet-1022</th>
<th>GPT-4o 0513</th>
<th>DeepSeek V3</th>
<th>OpenAI o1-mini</th>
<th>OpenAI o1-1217</th>
<th>DeepSeek R1</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Architecture</td>
<td>-</td>
<td>-</td>
<td>MoE</td>
<td>-</td>
<td>-</td>
<td>MoE</td>
</tr>
<tr>
<td></td>
<td># Activated Params</td>
<td>-</td>
<td>-</td>
<td>37B</td>
<td>-</td>
<td>-</td>
<td>37B</td>
</tr>
<tr>
<td></td>
<td># Total Params</td>
<td>-</td>
<td>-</td>
<td>671B</td>
<td>-</td>
<td>-</td>
<td>671B</td>
</tr>
<tr>
<td>English</td>
<td>MMLU (Pass@1)</td>
<td>88.3</td>
<td>87.2</td>
<td>88.5</td>
<td>85.2</td>
<td><strong>91.8</strong></td>
<td>90.8</td>
</tr>
<tr>
<td></td>
<td>MMLU-Redux (EM)</td>
<td>88.9</td>
<td>88.0</td>
<td>89.1</td>
<td>86.7</td>
<td>-</td>
<td><strong>92.9</strong></td>
</tr>
<tr>
<td></td>
<td>MMLU-Pro (EM)</td>
<td>78.0</td>
<td>72.6</td>
<td>75.9</td>
<td>80.3</td>
<td>-</td>
<td><strong>84.0</strong></td>
</tr>
<tr>
<td></td>
<td>DROP (3-shot F1)</td>
<td>88.3</td>
<td>83.7</td>
<td>91.6</td>
<td>83.9</td>
<td>90.2</td>
<td><strong>92.2</strong></td>
</tr>
<tr>
<td></td>
<td>IF-Eval (Prompt Strict)</td>
<td><strong>86.5</strong></td>
<td>84.3</td>
<td>86.1</td>
<td>84.8</td>
<td>-</td>
<td>83.3</td>
</tr>
<tr>
<td></td>
<td>GPQA-Diamond (Pass@1)</td>
<td>65.0</td>
<td>49.9</td>
<td>59.1</td>
<td>60.0</td>
<td><strong>75.7</strong></td>
<td>71.5</td>
</tr>
<tr>
<td></td>
<td>SimpleQA (Correct)</td>
<td>28.4</td>
<td>38.2</td>
<td>24.9</td>
<td>7.0</td>
<td><strong>47.0</strong></td>
<td>30.1</td>
</tr>
<tr>
<td></td>
<td>FRAMES (Acc.)</td>
<td>72.5</td>
<td>80.5</td>
<td>73.3</td>
<td>76.9</td>
<td>-</td>
<td><strong>82.5</strong></td>
</tr>
<tr>
<td></td>
<td>AlpacaEval2.0 (LC-winrate)</td>
<td>52.0</td>
<td>51.1</td>
<td>70.0</td>
<td>57.8</td>
<td>-</td>
<td><strong>87.6</strong></td>
</tr>
<tr>
<td></td>
<td>ArenaHard (GPT-4-1106)</td>
<td>85.2</td>
<td>80.4</td>
<td>85.5</td>
<td>92.0</td>
<td>-</td>
<td><strong>92.3</strong></td>
</tr>
<tr>
<td>Code</td>
<td>LiveCodeBench (Pass@1-COT)</td>
<td>33.8</td>
<td>34.2</td>
<td>-</td>
<td>53.8</td>
<td>63.4</td>
<td><strong>65.9</strong></td>
</tr>
<tr>
<td></td>
<td>Codeforces (Percentile)</td>
<td>20.3</td>
<td>23.6</td>
<td>58.7</td>
<td>93.4</td>
<td><strong>96.6</strong></td>
<td>96.3</td>
</tr>
<tr>
<td></td>
<td>Codeforces (Rating)</td>
<td>717</td>
<td>759</td>
<td>1134</td>
<td>1820</td>
<td><strong>2061</strong></td>
<td>2029</td>
</tr>
<tr>
<td></td>
<td>SWE Verified (Resolved)</td>
<td><strong>50.8</strong></td>
<td>38.8</td>
<td>42.0</td>
<td>41.6</td>
<td>48.9</td>
<td>49.2</td>
</tr>
<tr>
<td></td>
<td>Aider-Polyglot (Acc.)</td>
<td>45.3</td>
<td>16.0</td>
<td>49.6</td>
<td>32.9</td>
<td><strong>61.7</strong></td>
<td>53.3</td>
</tr>
<tr>
<td>Math</td>
<td>AIME 2024 (Pass@1)</td>
<td>16.0</td>
<td>9.3</td>
<td>39.2</td>
<td>63.6</td>
<td>79.2</td>
<td><strong>79.8</strong></td>
</tr>
<tr>
<td></td>
<td>MATH-500 (Pass@1)</td>
<td>78.3</td>
<td>74.6</td>
<td>90.2</td>
<td>90.0</td>
<td>96.4</td>
<td><strong>97.3</strong></td>
</tr>
<tr>
<td></td>
<td>CNMO 2024 (Pass@1)</td>
<td>13.1</td>
<td>10.8</td>
<td>43.2</td>
<td>67.6</td>
<td>-</td>
<td><strong>78.8</strong></td>
</tr>
<tr>
<td>Chinese</td>
<td>CLUEWSC (EM)</td>
<td>85.4</td>
<td>87.9</td>
<td>90.9</td>
<td>89.9</td>
<td>-</td>
<td><strong>92.8</strong></td>
</tr>
<tr>
<td></td>
<td>C-Eval (EM)</td>
<td>76.7</td>
<td>76.0</td>
<td>86.5</td>
<td>68.9</td>
<td>-</td>
<td><strong>91.8</strong></td>
</tr>
<tr>
<td></td>
<td>C-SimpleQA (Correct)</td>
<td>55.4</td>
<td>58.7</td>
<td><strong>68.0</strong></td>
<td>40.3</td>
<td>-</td>
<td>63.7</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Distilled Model Evaluation</h3><a id="user-content-distilled-model-evaluation" aria-label="Permalink: Distilled Model Evaluation" href="#distilled-model-evaluation"></a></p>
<div dir="auto">
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>AIME 2024 pass@1</th>
<th>AIME 2024 cons@64</th>
<th>MATH-500 pass@1</th>
<th>GPQA Diamond pass@1</th>
<th>LiveCodeBench pass@1</th>
<th>CodeForces rating</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4o-0513</td>
<td>9.3</td>
<td>13.4</td>
<td>74.6</td>
<td>49.9</td>
<td>32.9</td>
<td>759</td>
</tr>
<tr>
<td>Claude-3.5-Sonnet-1022</td>
<td>16.0</td>
<td>26.7</td>
<td>78.3</td>
<td>65.0</td>
<td>38.9</td>
<td>717</td>
</tr>
<tr>
<td>o1-mini</td>
<td>63.6</td>
<td>80.0</td>
<td>90.0</td>
<td>60.0</td>
<td>53.8</td>
<td><strong>1820</strong></td>
</tr>
<tr>
<td>QwQ-32B-Preview</td>
<td>44.0</td>
<td>60.0</td>
<td>90.6</td>
<td>54.5</td>
<td>41.9</td>
<td>1316</td>
</tr>
<tr>
<td>DeepSeek-R1-Distill-Qwen-1.5B</td>
<td>28.9</td>
<td>52.7</td>
<td>83.9</td>
<td>33.8</td>
<td>16.9</td>
<td>954</td>
</tr>
<tr>
<td>DeepSeek-R1-Distill-Qwen-7B</td>
<td>55.5</td>
<td>83.3</td>
<td>92.8</td>
<td>49.1</td>
<td>37.6</td>
<td>1189</td>
</tr>
<tr>
<td>DeepSeek-R1-Distill-Qwen-14B</td>
<td>69.7</td>
<td>80.0</td>
<td>93.9</td>
<td>59.1</td>
<td>53.1</td>
<td>1481</td>
</tr>
<tr>
<td>DeepSeek-R1-Distill-Qwen-32B</td>
<td><strong>72.6</strong></td>
<td>83.3</td>
<td>94.3</td>
<td>62.1</td>
<td>57.2</td>
<td>1691</td>
</tr>
<tr>
<td>DeepSeek-R1-Distill-Llama-8B</td>
<td>50.4</td>
<td>80.0</td>
<td>89.1</td>
<td>49.0</td>
<td>39.6</td>
<td>1205</td>
</tr>
<tr>
<td>DeepSeek-R1-Distill-Llama-70B</td>
<td>70.0</td>
<td><strong>86.7</strong></td>
<td><strong>94.5</strong></td>
<td><strong>65.2</strong></td>
<td><strong>57.5</strong></td>
<td>1633</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">5. Chat Website &amp; API Platform</h2><a id="user-content-5-chat-website--api-platform" aria-label="Permalink: 5. Chat Website &amp; API Platform" href="#5-chat-website--api-platform"></a></p>
<p dir="auto">You can chat with DeepSeek-R1 on DeepSeek's official website: <a href="https://chat.deepseek.com/" rel="nofollow">chat.deepseek.com</a>, and switch on the button "DeepThink"</p>
<p dir="auto">We also provide OpenAI-Compatible API at DeepSeek Platform: <a href="https://platform.deepseek.com/" rel="nofollow">platform.deepseek.com</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">6. How to Run Locally</h2><a id="user-content-6-how-to-run-locally" aria-label="Permalink: 6. How to Run Locally" href="#6-how-to-run-locally"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DeepSeek-R1 Models</h3><a id="user-content-deepseek-r1-models-1" aria-label="Permalink: DeepSeek-R1 Models" href="#deepseek-r1-models-1"></a></p>
<p dir="auto">Please visit <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a> repo for more information about running DeepSeek-R1 locally.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DeepSeek-R1-Distill Models</h3><a id="user-content-deepseek-r1-distill-models-1" aria-label="Permalink: DeepSeek-R1-Distill Models" href="#deepseek-r1-distill-models-1"></a></p>
<p dir="auto">DeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.</p>
<p dir="auto">For instance, you can easily start a service using <a href="https://github.com/vllm-project/vllm">vLLM</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager"><pre>vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager</pre></div>
<p dir="auto">You can also easily start a service using <a href="https://github.com/sgl-project/sglang">SGLang</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2"><pre>python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2</pre></div>
<p dir="auto"><strong>NOTE: We recommend setting an appropriate temperature (between 0.5 and 0.7) when running these models, otherwise you may encounter issues with endless repetition or incoherent output.</strong></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">7. License</h2><a id="user-content-7-license" aria-label="Permalink: 7. License" href="#7-license"></a></p>
<p dir="auto">This code repository and the model weights are licensed under the <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE">MIT License</a>.
DeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:</p>
<ul dir="auto">
<li>DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from <a href="https://github.com/QwenLM/Qwen2.5">Qwen-2.5 series</a>, which are originally licensed under <a href="https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE" rel="nofollow">Apache 2.0 License</a>, and now finetuned with 800k samples curated with DeepSeek-R1.</li>
<li>DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under <a href="https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE" rel="nofollow">llama3.1 license</a>.</li>
<li>DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under <a href="https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE" rel="nofollow">llama3.3 license</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">8. Citation</h2><a id="user-content-8-citation" aria-label="Permalink: 8. Citation" href="#8-citation"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">9. Contact</h2><a id="user-content-9-contact" aria-label="Permalink: 9. Contact" href="#9-contact"></a></p>
<p dir="auto">If you have any questions, please raise an issue or contact us at <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/service@deepseek.com">service@deepseek.com</a>.</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>