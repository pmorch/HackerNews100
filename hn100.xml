<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 10 Jul 2023 15:00:11 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Firefox Address Bar Tips (184 pts)]]></title>
            <link>https://wiki.tilde.institute/w/firefox-address-bar-tips</link>
            <guid>36666116</guid>
            <pubDate>Mon, 10 Jul 2023 13:30:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wiki.tilde.institute/w/firefox-address-bar-tips">https://wiki.tilde.institute/w/firefox-address-bar-tips</a>, See on <a href="https://news.ycombinator.com/item?id=36666116">Hacker News</a></p>
<div id="readability-page-1" class="page">

<!--
title: firefox address bar tips
author: erxeto
description: Some useful tips for using the firefox address bar more
efficiently
-->



<p>The address bar has become our entry point to the internet these days.
Firefox in its default configuration does some sort of <em>smart</em> guess on
what you type there.  If it resembles a <a href="https://en.wikipedia.org/wiki/URL">URL</a>
then the browser makes that request.  If not, it sends the string you typed
to your default search engine.  It also includes some fuzzy search matches
from your history and all that, which is fine 90% of the time, but
sometimes you need a bit more control over what results it shows you.</p>

<h2>Changing the address bar behaviour</h2>

<p>This is a list of modifiers you can set at the beginning of the search to
tell firefox what do you want to see on the results, a kind of filtering:</p>

<pre><code>^    to search for matches in your browsing history.
*    to search for matches in your bookmarks.
+    to search for matches in pages you've tagged.
%    to search for matches in your currently open tabs.
#    to search for matches in page titles.
$    to search for matches in web addresses (URLs).
?    to search for matches in suggestions.
</code></pre>

<h2>Examples</h2>

<p>So, if you want to search for the word <code>headphones</code> in your bookmarks only,
you can type on the address bar:</p>

<p><code>*headphones</code></p>

<p>And, if you want to include only the results of your browsing history it
would be:</p>

<p><code>^headphones</code></p>

<p><a href="https://wiki.tilde.institute/">back</a></p>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nitter is working again (169 pts)]]></title>
            <link>https://github.com/zedeus/nitter/pull/927</link>
            <guid>36665406</guid>
            <pubDate>Mon, 10 Jul 2023 12:33:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/zedeus/nitter/pull/927">https://github.com/zedeus/nitter/pull/927</a>, See on <a href="https://news.ycombinator.com/item?id=36665406">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p>
  <strong>Have a question about this project?</strong> Sign up for a free GitHub account to open an issue and contact its maintainers and the community.
  </p>

  <!-- '"` --><!-- </textarea></xmp> --><form data-turbo="false" autocomplete="off" action="/join?return_to=%2Fzedeus%2Fnitter%2Fissues%2Fnew" accept-charset="UTF-8" method="post">    <auto-check src="/signup_check/username">
      <dl><dt><label name="user[login]" autocapitalize="off" autofocus="autofocus" for="user_login_issues">Pick a username</label></dt><dd></dd></dl>
      
    </auto-check>

    <auto-check src="/signup_check/email">
      <dl><dt><label name="user[email]" autocapitalize="off" for="user_email_issues">Email Address</label></dt><dd></dd></dl>
      
    </auto-check>

    <auto-check src="/users/password"><dl><dt><label name="user[password]" for="user_password_issues">Password</label></dt><dd></dd></dl></auto-check>

    
    




      
</form>
  <p>By clicking “Sign up for GitHub”, you agree to our <a href="https://docs.github.com/terms" target="_blank">terms of service</a> and
  <a href="https://docs.github.com/privacy" target="_blank">privacy statement</a>. We’ll occasionally send you account related emails.</p>

  <p>
    Already on GitHub?
    <a data-ga-click="(Logged out) New issue modal, clicked Sign in, text:sign-in" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;new issue modal&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/zedeus/nitter/pull/927&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="3c875aaf2860d32663fc760f6311cedfea107f42ccc4932e862097b45ba58879" href="https://github.com/login?return_to=%2Fzedeus%2Fnitter%2Fissues%2Fnew%2Fchoose">Sign in</a>
    to your account
  </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Harvard ethics prof fabricated multiple behavioral science studies (135 pts)]]></title>
            <link>https://www.thecollegefix.com/harvard-ethics-professor-allegedly-fabricated-multiple-behavioral-science-studies/</link>
            <guid>36665247</guid>
            <pubDate>Mon, 10 Jul 2023 12:16:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thecollegefix.com/harvard-ethics-professor-allegedly-fabricated-multiple-behavioral-science-studies/">https://www.thecollegefix.com/harvard-ethics-professor-allegedly-fabricated-multiple-behavioral-science-studies/</a>, See on <a href="https://news.ycombinator.com/item?id=36665247">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
				
                				    <img width="370" height="242" src="https://www.thecollegefix.com/wp-content/uploads/2023/06/Gino.YouTubescreenshot-370x242.jpg" alt="" decoding="async" srcset="https://www.thecollegefix.com/wp-content/uploads/2023/06/Gino.YouTubescreenshot-370x242.jpg 370w, https://www.thecollegefix.com/wp-content/uploads/2023/06/Gino.YouTubescreenshot-560x367.jpg 560w, https://www.thecollegefix.com/wp-content/uploads/2023/06/Gino.YouTubescreenshot-135x88.jpg 135w, https://www.thecollegefix.com/wp-content/uploads/2023/06/Gino.YouTubescreenshot.jpg 580w" sizes="(max-width: 370px) 100vw, 370px">								<p>Shockwaves are moving throughout the behavioral science community after distinguished Professor Francesca Gino of Harvard Business School was recently accused of falsifying data in multiple studies.</p>
<p>Calling the situation a “Clusterfake,” on June 17 a trio of professors published “Part 1” of the multiple allegations of fraud in papers spanning over a decade after examining studies co-authored by Gino,&nbsp;author of the 2018 <a href="https://www.amazon.com/Rebel-Talent-Pays-Break-Rules/dp/0062694634" target="_blank" rel="noopener">book</a> “Rebel Talent: Why It Pays to Break the Rules at Work and in Life.”</p>
<p>Professors Joseph Simmons, Uri Simonsohn and Leif Nelson of University of Pennsylvania, Escade Business School in Spain, and University of California, Berkeley, respectively, <a href="https://datacolada.org/109" target="_blank" rel="noopener">accused</a> Gino of the fraud on their blog Data Colada.</p>
<p>“Specifically, we wrote a report about four studies for which we accumulated the strongest evidence of fraud,” they wrote, stating they shared their concerns with Harvard Business School.</p>
<p>Notably, a famous <a href="https://sci-hub.se/10.1073/pnas.1209746109" target="_blank" rel="noopener">article</a> discussing dishonesty was found to contain fabrications, they wrote.</p>
<p>Gino is on administrative leave from Harvard, according to her faculty bio. She did not respond to a request for comment from <em>The College Fix</em>. Simmons and Nelson did not respond to <em>The College Fix’s</em> requests for comment.</p>
<p>Harvard media relations did not respond to<em> The College Fix</em> for a comment on the employment of Gino and the review of her studies.</p>
<p><em>The New York Times</em> <a href="https://www.nytimes.com/2023/06/24/business/economy/francesca-gino-harvard-dishonesty.html" target="_blank" rel="noopener">reported</a> a phone call with a man who identified as Gino’s husband saying “It’s obviously something that is very sensitive that we can’t speak to now.”</p>
<p>Gino’s paper discussing dishonesty has since been retracted. “Signing at the beginning makes ethics salient and decreases dishonest self-reports in comparison to signing at the end” had been published by <em>Proceedings of the National Academy of Sciences</em> in July 2012.</p>
<p>The study was run at the University of North Carolina in 2010, where Gino was a professor before joining Harvard; she was the only author involved in collecting and analyzing data for it, the trio <a href="https://datacolada.org/109" target="_blank" rel="noopener">reported</a>.</p>
<p>However, another study under scrutiny was supervised by Professor Dan Ariely at Duke University and conducted by an auto insurance company.</p>
<p>Ariely is famous in behavioral science. He founded the Center for Advanced Hindsight at Duke, wrote multiple New York Times bestsellers, and had given many TED Talks.</p>
<p>“Two different people independently faked data for two different studies in a paper about dishonesty,” the three professors allege on their blog post.</p>
<p>Ariely denies making up any data but said he has no proof to clear his name, according to Cathleen O’Grady in a <em>Science</em> <a href="https://www.science.org/content/article/fraudulent-data-set-raise-questions-about-superstar-honesty-researcher" target="_blank" rel="noopener">article</a>. Ariely told <em>Science</em> “I wish I had a good story, and I just don’t.”</p>
<p>He said “he made a mistake in not checking the data he received from the insurance company, and that he no longer has the company’s original file.”</p>
<p>The three Data Colada professors have continued to publish the series of posts on their blog accusing Gino of fraud. The posts have <a href="https://twitter.com/DataColada" target="_blank" rel="noopener">spread via Twitter</a>.</p>
<blockquote data-width="500" data-dnt="true">
<p lang="en" dir="ltr">Second post of a four-post series on Data Falsificada.<a href="https://t.co/cbyaP9F2yn">https://t.co/cbyaP9F2yn</a></p>
<p>Includes message to 148 Gino Co-Authors <a href="https://t.co/IIISKwrw9M">pic.twitter.com/IIISKwrw9M</a></p>
<p>— Data Colada (@<a href="https://www.thecollegefix.com/cdn-cgi/l/email-protection" data-cfemail="197d786d787a7675787d785974786a376d76">[email&nbsp;protected]</a>) (@DataColada) <a href="https://twitter.com/DataColada/status/1671130691415969799?ref_src=twsrc%5Etfw">June 20, 2023</a></p></blockquote>

<p>“It is worth reiterating that to the best of our knowledge, none of Gino’s co-authors carried out or assisted with the data collection for the studies in this series,” they wrote.</p>
<p>A <a href="https://datacolada.org/110" target="_blank" rel="noopener">second</a> article posted June 20 titled “Data Falsificada (Part 2): ‘My Class Year Is Harvard’” discussed a study conducted at Harvard where students would write argumentative essays and subsequently rate their desire for cleansing products.</p>
<p>The study was based on a <a href="https://www.hbs.edu/ris/Publication%20Files/Moral%20Virtue_7caef67d-e4c7-4b38-88c7-b98da81826a5.pdf" target="_blank" rel="noopener">paper</a> titled “The Moral Virtue of Authenticity: How Inauthenticity Produces Feelings of Immorality and Impurity,” published in Psychological Science in 2015. The paper was co-authored by Maryam Kouchaki and Adam Galinsky from Northwestern University and Columbia University, respectively.</p>
<p>A third <a href="https://datacolada.org/111" target="_blank" rel="noopener">article</a> posted June 23 is titled “Data Falsificada (Part 3): ‘The Cheaters Are Out of Order,’” where the authors commented on the sorting methods of data for a study about cheating.</p>
<p>The alleged fraud dealt with the <a href="https://journals.sagepub.com/doi/10.1177/0956797614520714" target="_blank" rel="noopener">research paper</a> “Evil Genius? How Dishonesty Can Lead to Greater Creativity,” published in 2014 in Psychological Science. It was co-authored by Scott Wiltermuth from the University of Southern California.</p>
<p>The fourth part of the series has yet to be posted.</p>
<p><em>The New York Times</em> <a href="https://www.nytimes.com/2023/06/24/business/economy/francesca-gino-harvard-dishonesty.html" target="_blank" rel="noopener">reported</a> that Maurice Schweitzer of Wharton Business School at the University of Pennsylvania said the accusations were having large “reverberations in the academic community” due to the amount of collaborators on many articles.</p>
<p>Schweitzer co-authored multiple papers with Gino and stated that none of the accused studies are his, <a href="https://www.chronicle.com/article/a-dishonesty-expert-stands-accused-of-fraud-scholars-who-worked-with-her-are-scrambling?sra=true&amp;cid=gen_sign_in" target="_blank" rel="noopener">according</a> to the <em>Chronicle of Higher Education.</em></p>
<p>However, Schweitzer reported feeling uncertain about the viability of his work with Gino. He told the <em>Chronicle</em> that Gino was very efficient to work with and executed the studies and provided results very fast.</p>
<p>Schweitzer did not respond to <em>The College Fix’s</em> request for comment.</p>
<p><strong>MORE: <a href="https://www.thecollegefix.com/concern-over-academic-research-fraud-grows-after-scholar-cooked-data-on-racism/" target="_blank" rel="noopener">Concern over academic research fraud grows after scholar cooked data on racism</a></strong></p>
<p>IMAGE: YouTube <a href="https://www.youtube.com/watch?v=zTmFFmAqbEc" target="_blank" rel="noopener">screenshot</a></p>
            

                    <p><a href="https://www.facebook.com/TheCollegeFix/" target="_blank" rel="noopener noreferrer">Like <em>The College Fix</em> on Facebook</a> / <a href="https://twitter.com/CollegeFix" target="_blank" rel="noopener noreferrer">Follow us on Twitter</a></p>
        
				<!-- <div class="social-bar">
					    <span class="share-total">
        Share this article:
    </span>
    <a class="social-link facebook"
       href="http://www.facebook.com/sharer/sharer.php?u=https://www.thecollegefix.com/harvard-ethics-professor-allegedly-fabricated-multiple-behavioral-science-studies/&title=Harvard+ethics+professor+allegedly+fabricated+multiple+behavioral+science+studies"
       target="_blank">
        <span class="screen-reader-text">The College Fix on Facebook</span>
        <span class="count">
                    </span>
    </a>
    <a class="social-link twitter"
       href="http://twitter.com/intent/tweet?text=Harvard+ethics+professor+allegedly+fabricated+multiple+behavioral+science+studies+https%3A%2F%2Fwww.thecollegefix.com%2Fharvard-ethics-professor-allegedly-fabricated-multiple-behavioral-science-studies%2F+via+%40collegefix"
       
       target="_blank">
        <span class="screen-reader-text">The College Fix on Twitter</span>
    </a>
    <a class="social-link reddit"
       href="http://www.reddit.com/submit?url=https://www.thecollegefix.com/harvard-ethics-professor-allegedly-fabricated-multiple-behavioral-science-studies/&title=Harvard+ethics+professor+allegedly+fabricated+multiple+behavioral+science+studies"
       target="_blank">
        <span class="screen-reader-text">The College Fix on Reddit</span>
    </a>
    <a class="social-link parler"
       href="https://parler.com/new-post?url=https://www.thecollegefix.com/harvard-ethics-professor-allegedly-fabricated-multiple-behavioral-science-studies/"
       target="_blank">
        <span class="screen-reader-text">The College Fix on Parler</span>
    </a>
    <a class="social-link email" href="mailto:?subject=thecollegefix.com: Harvard ethics professor allegedly fabricated multiple behavioral science studies&body=https://www.thecollegefix.com/harvard-ethics-professor-allegedly-fabricated-multiple-behavioral-science-studies/">
        <span class="screen-reader-text">Share on Email</span>
    </a>

				</div> -->
				
				
				
				

	
				
				<!-- /1011927/thecollegefix_Leaderboard_2 -->
				
				
			</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Debate over 'fake work' in tech industry misses the real culprit: lazy managers (124 pts)]]></title>
            <link>https://www.businessinsider.com/tech-industry-fake-work-problem-bad-managers-bosses-layoffs-jobs-2023-7</link>
            <guid>36664909</guid>
            <pubDate>Mon, 10 Jul 2023 11:32:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/tech-industry-fake-work-problem-bad-managers-bosses-layoffs-jobs-2023-7">https://www.businessinsider.com/tech-industry-fake-work-problem-bad-managers-bosses-layoffs-jobs-2023-7</a>, See on <a href="https://news.ycombinator.com/item?id=36664909">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component-type="content-lock" data-load-strategy="exclude">
                                  <p>When Graham was hired by Amazon, it sounded like his dream job. He was brought on as a research scientist to help develop features for Alexa, the company's ubiquitous voice assistant. Graham, whose name has been changed to protect his identity, assumed he would soon be using his expertise in machine learning to work on cool, new features that would make Alexa more personal to every user. But within four months of his start at the company, it became clear that Amazon had no idea what to do with him.</p><p>He spent the next two years bouncing around — switching teams, watching project leaders get promoted despite, he said, producing nothing of substance, and generally spinning his wheels. Graham was paid more than $300,000 a year but had little work to show for it. Feeling adrift with nothing to do, he gradually disengaged from his job and was eventually put on Amazon's formal performance-management plan.</p><p>Facing the threat of firing, Graham was finally put on a project to use machine learning to improve Amazon's music recommendations, which he described as "the first really interesting thing I worked on." He was happy to feel like a valuable member of the team, but Graham's manager told him something stunning: The finished project, which Graham worked on for more than a month, wouldn't see the light of day. It was simply an exercise to satisfy the terms of his performance plan and string out his employment, he was told. Graham left Amazon soon after.</p>
                          
                          <p>As tech companies have <a href="https://www.businessinsider.com/layoffs-sweeping-the-us-these-are-the-companies-making-cuts-2023" data-analytics-product-module="body_link" rel="">laid off</a> tens of thousands of employees this year, venture capitalists and executives have <a href="https://www.businessinsider.com/silicon-valley-debates-tech-employees-doing-fake-work-2023-3" data-analytics-product-module="body_link" rel="">leaned on the term "fake work"</a> to describe the output of employees like Graham. The layoffs are necessary and even prudent, the argument goes, because thousands of workers at Big Tech firms such as Google and Meta are sitting around trying to look busy while doing very little productive work.</p><p>"There's nothing for these people to do — it's all fake work," Keith Rabois, a famous tech investor, <a href="https://www.businessinsider.com/google-meta-staff-do-fake-work-says-vc-keith-rabois-2023-3" data-analytics-product-module="body_link" rel="">opined at a March event</a> hosted by the investment bank Evercore. "Now that's being exposed, what do these people actually do? They go to meetings."</p><p>Some tech workers <a href="https://www.businessinsider.com/fired-from-my-job-at-meta-for-posting-on-tiktok-2023-3" data-analytics-product-module="body_link" rel="">seemed to confirm</a> Rabois' claims on social media, sharing stories of being paid by giant tech firms to do very little. In one viral TikTok video, Brit Levy, Meta's former diversity, equity, and inclusion policy analyst, said she had to <a href="https://www.businessinsider.com/laid-off-meta-employee-says-paid-not-to-work-2023-3" data-analytics-product-module="body_link" rel="">"basically fight to find work"</a> and the company was simply holding on to employees "like Pokémon cards."</p><p>But based on conversations with over 30 people involved in the tech industry, including current and former tech employees, some in management positions, the conception of <a href="https://www.businessinsider.com/silicon-valley-debates-tech-employees-doing-fake-work-2023-3" data-analytics-product-module="body_link" rel="">lazy employees raking in big paychecks to do little</a> lays the blame in the wrong place. Oftentimes, employees are getting plenty of work done; it's just that the projects are of little to no importance to the company's bottom line. The tech employees spoke with us on the condition of anonymity to avoid professional reprisal.</p><p>"Most workers want to come and work. They want to show up, give a fair eight hours of work, and they want to feel good about themselves," said Scott Latham, a strategic-management professor at the University of Massachusetts Lowell who worked in the tech industry during the start of the internet boom.&nbsp;</p><p>There's only one real culprit for the culture of "fake work," he said. "It's lazy management."</p><h2>What we talk about when we talk about 'fake work'</h2><p>"Fake work," as consultants Brent Peterson and Gaylan Nielson define it in their 2009 book of the same name, is "effort under the illusion of value." The crimes, they wrote, include pointless meetings, reports, and presentations. In the tech industry, specifically, the term "fake work" is used to conjure up an image of lazy engineers "resting and vesting" — long-tenured, high-paid employees doing very little work while waiting on a lucrative payday from their company stock.</p><p>Rich Moran, a venture capitalist, consultant, and author of several books about the workplace, prefers to call it a "false sense of activity" and said it's "more rampant" among tech companies. "The tech sector is more willing to try different things," he told us. "And so you get assigned to a project that you know may be going nowhere, but they have a hard time saying, well this isn't going anywhere."</p>
                          
                          <p>The latest version of fake work emerged as part of the tech industry's <a href="https://www.businessinsider.com/big-tech-moonshots-over-amazon-facebook-google-microsoft-2022-9" data-analytics-product-module="body_link" rel="">pandemic-driven boom and bust</a>. Lockdowns and work from home meant Amazon, Google, Meta, Shopify, and many other giants saw an explosion of demand for their products. Assuming the consumer shift was a harbinger of a new normal of shopping, socializing, and working online all the time, companies aggressively hired thousands of recruits. But firms often gave little thought about where to place them or what their role would be, insiders say.</p><p>"I think COVID was an accelerator for fake work because a lot of these tech companies hired. Then they weren't sure what to do with a lot of the people," Moran said.</p><p>One former Google manager told us she was instructed to lower her standards for hiring earlier in the pandemic and watched as teams she worked with doubled in size. As new hires flooded in, it felt as if teams were reorganized on a weekly basis, making it harder for people to do solid work.&nbsp;</p><blockquote><q>Most workers want to come and work. They want to show up</q></blockquote><p>The sudden head-count increase was destabilizing, but the real trouble began when business started to slow down. Rather than a permanent reorientation, many of the behaviors people adopted turned out to be short-term modifications. And as the economy turned on the tech industry, companies scrambled to figure out what to do with all the employees they no longer needed. That's when the "fake work" talk, finger-pointing, and unceremonious layoffs took off.</p><p>"There was just no guidance at all," an ex-Meta worker said of his two months at the company, when he was waiting to be placed on a team as an entry-level data scientist. "I remember one day literally having absolutely nothing to do, and I just went surfing instead because I'm remote. I have no one to report to. It seems like no one knows I'm here."</p><figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                          
                          
                          
                            <p><img src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/%3E" data-content-type="image/jpeg" data-srcs="{&quot;https://i.insider.com/64a84d368ed31300199e1ea8&quot;:{&quot;contentType&quot;:&quot;image/jpeg&quot;,&quot;aspectRatioW&quot;:5333,&quot;aspectRatioH&quot;:4000}}" alt="A man with a backpack walks up to a glass building with the Google logo on it" itemprop="contentUrl">
                        </p>
                          
                          <span>
                                <figcaption data-e2e-name="image-caption">
                                  One former Google employee said many employees were assigned work that served no purpose for the company.
                                </figcaption>
                                
                          <span data-e2e-name="image-source" itemprop="creditText">
                          
                          Melina Mara/The Washington Post via Getty Images
                          
                          </span>
                              </span>
                          </figure><p>Another ex-Meta employee said that there were so many workers in his department when he joined in 2022 that on multiple occasions, he'd complete a project to learn that as many as four other people had been given the same assignment separately. The former employee described the work he did at Meta as "intern-level' — putting together graphs based on preexisting data, polishing presentations, or practicing how to "work a problem backward" — despite having nearly a decade of experience in tech. He said he found the environment "stifling" and was often deterred from trying to increase the scope of his work.</p><p>Sure, there were some employees, as pundits and executives have suggested, who could have helped tackle projects but chose not to. But for the most part, insiders told us, workers were stuck in circumstances beyond their control.</p><p>"I think there's very few people sitting around doing nothing," Moran said. "I think people are very skilled at filling their time. Whether or not it helps the organization is another question."</p>
                          
                          <p>Some people were assigned plenty of tasks, but they ended up serving no mission-critical purpose. The former Google program manager said that people were working hard but what qualified as work had seemingly changed. "They gave us a lot of work that was just a waste of time," she said.</p><p>Others, like Graham, were assigned to seemingly key parts of the company, only to find that there wasn't enough work to go around. The former Googler also noticed this type of fake work. "There were so many of us, people just started trying to look busy," she told us. "There were too many chefs in the kitchen," she added.</p><p>One former contract worker hired by Meta during the pandemic became so frustrated with feeling idle that she took on a second contract job at Microsoft at the same time (neither company knew she was working for the other). She decided if "they're not going to give me anything to do then I guess I'm just getting paid."</p><p>"It's a little bit of a symbiotic relationship where the people that you report to aren't saying that you're not doing anything and you're not saying that you're not doing anything," she told us.&nbsp;</p><h2>Empty empires</h2><p>While the pandemic's boom and bust brought the issue into stark relief, the various types of fake work have been growing within tech companies for years. Many of these issues come down to one fundamental problem: managers trying to get ahead.</p><p>At almost all tech companies, current and former employees said, bosses were rewarded for overhiring since it made them look important. Bloated org charts resulted in too many people fighting for work, a poor understanding of what each segment of the company was doing, and a rise in projects spun up merely to help managers get themselves promoted.</p><p>"People are often measured not in contribution but in head count." Moran said.</p><p>"The bigger your team you have, the more qualified people you have in your team, the more weight you have in the company," Graham, the former Amazon employee, said. "It's what we call empire building. You're not focused on building a product; you're focused on building an empire. That leads to fake work and unnecessary bloating."</p>
                          
                          <blockquote><q>The more people who report to you, the higher your prestige, the more your power in the organization.</q></blockquote><p>To create an empire, managers simply add employees underneath them with little sense of what they should be doing. "Instead of planning in the most efficient way, they just say, 'I need a head count,'" Anna Tavis, a clinical professor of human-capital management at New York University's School of Professional Studies, said.</p><p>The former Meta employee who joined the company in 2022 felt that stuffing teams was a byproduct of middle managers looking for a promotion, leading to employees having less to do. One of his managers hired so many people that within three months, there became four levels between him and the person who was supposed to be managing him. "A lot of the time, my managers had no idea what I was doing," he said.</p><p>In addition to the incentive structure encouraging projects to nowhere, there's a lack of oversight from the top into how these miniature empires are being run, employees said. And in many cases, executives are oblivious to the value of the work that's presented to them. Some executives have even admitted that there seems to be little incentive to address company bloat. Former Slack CEO Stewart Butterfield told Bloomberg's "Odd Lots" <a href="https://omny.fm/shows/odd-lots/slack-founder-stewart-butterfield-on-ai-software-a" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">podcast</a> that without financial hurdles, managers had every reason to keep hiring.</p><p>"The more people who report to you, the higher your prestige, the more your power in the organization," he said.</p><p>To secure their fiefdoms, managers often pitch projects they created that are sometimes referred to as "vanity projects" or "promo projects." These may ultimately contribute zero to a company's top line, but the flashy presentations and demos associated with the projects often lead to a promotion and nice pay bump for the person leading the work. One Google manager who recently left the company said the head-count process at Google "rewards bad behavior" by promoting people based on "having a bigger team and creating decks." Google had "dozens" of teams, he said, that did "think tank-like strategy work with no real practical way of impacting the business or a customer or user."</p><p>"I do think that process favored the people who were better at bullshitting and storytelling," he told us.</p><figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                          
                          
                          
                            <p><img src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/%3E" data-content-type="image/jpeg" data-srcs="{&quot;https://i.insider.com/64a84df48ed31300199e1ed0&quot;:{&quot;contentType&quot;:&quot;image/jpeg&quot;,&quot;aspectRatioW&quot;:5119,&quot;aspectRatioH&quot;:3840}}" alt="Amazon employees at the Spheres next to its Seattle headquarters" itemprop="contentUrl">
                        </p>
                          
                          <span>
                                <figcaption data-e2e-name="image-caption">
                                  Graham, a former Amazon employee, said the incentive structure at the company led to "fake work and unnecessary bloating."
                                </figcaption>
                                
                          <span data-e2e-name="image-source" itemprop="creditText">
                          
                          LINDSEY WASSON/Reuters
                          
                          </span>
                              </span>
                          </figure><p>At Meta, one current employee said it was common to see employees all the way up to vice presidents invent workshops or "sprints" to set "strategic visions" for projects, while only a small fraction made it onto a road map, an actual timeline for a product to launch to the public, the employee said.&nbsp;</p><p>"I've rarely seen a large-scale vision be referenced after it's presented, despite the fact that upwards of 20 people are called to participate, usually by making and remaking decks for leadership," they added. "You can always tell when performance reviews are about to happen because there's almost always one or two workshops on the calendar a month before."</p>
                          
                          <p>When that manager gets promoted — or the project falls apart — the team is sometimes shuffled into other parts of the business. Several tech employees told us these situations led to a lot of work that ultimately made little or no material impact on the business.</p><p>"One reorg after another led to fake work," a former longtime Google employee said. "I got used to getting introduced to a multiyear product, for a project I would look at and say, 'This is a poster piece for some executive to implement while job hunting for another role so they can go be a CEO somewhere.' They were show products."</p><blockquote><q>I do think that process favored the people who were better at bullshitting and storytelling.</q></blockquote><p>The power-jockeying adds layers of unnecessary complication, making it nearly impossible to complete simple tasks. One former manager at Salesforce said the company had in recent years become stuffed with middle managers and power structures that sucked up resources but made it hard to get anything of substance done.</p><p>"Trying to get anything done in that organization takes 40 people to get aligned," the former employee told us. He said the meeting culture at the company had also gotten out of hand and "work" was defined as "making slide decks and giving speeches and having a really full calendar that shows you're in a lot of meetings."</p><h2>What's next?</h2><p>Over the past year, tech companies have made it very clear how they plan to deal with fake work — layoffs</p><p>In almost every layoff announcement — those from Amazon, Microsoft, Google, Salesforce, and others — executives have focused on the need to get more efficient. After announcing the company's <a href="https://www.businessinsider.com/meta-layoffs-employees-facebook-mark-zuckerberg-metaverse-bet-2022-11" data-analytics-product-module="body_link" rel="">first round of job cuts</a> last year, Meta CEO Mark Zuckerberg <a href="https://www.businessinsider.com/mark-zuckerberg-year-efficiency-meta-facebook-could-mean-more-layoffs-2023-2" data-analytics-product-module="body_link" rel="">declared</a> 2023 would be the "year of efficiency." Other tech CEOs, such as Alphabet's Sundar Pichai, have also talked of <a href="https://www.businessinsider.com/google-productivity-sundar-pichai-improved-ceo-2022-8" data-analytics-product-module="body_link" rel="">increasing their companies' productivity.&nbsp;</a></p><p>Some companies have also shown signs that they're trying to curb busywork. In September, Google told staff it would <a href="https://www.businessinsider.com/google-cut-back-meetings-distractions-okrs-goals-simplicity-sprint-2022-9" data-analytics-product-module="body_link" rel="">cut down "redundant meetings"</a> and asked employees to make stronger and specific agendas for the meetings they did have. In January, Zuckerberg told staff that he could no longer tolerate a company structure of "managers managing managers," the newsletter Command Line reported. He also told middle managers to <a href="https://www.businessinsider.com/meta-facebook-mark-zuckerberg-layoffs-managers-asked-to-move-quit-2023-2" data-analytics-product-module="body_link" rel="">find roles as individual contributors or leave</a>.</p><p>While many big tech companies have emphasized the need for more "efficiency" over the past six months, they have also generally downplayed concerns about their workplace cultures. In a statement, Amazon spokesperson Brad Glasser said that Graham's story "doesn't reflect the experience of most employees." A Google spokesperson denied that managers had been told to lower hiring standards during the pandemic and pointed us to to an interview with Bloomberg where CEO Sundar Pichai remarked that the company was "sharpening its focus." Representatives from Salesforce, and Microsoft declined to comment. Meta did not respond to requests for comment.</p>
                          
                          <p>Greg Selker, a managing director at the executive-search and consulting firm Stanton Chase, said he thought the fake-work phenomenon was already reaching a natural conclusion. The "smartest companies" that overhired have already gone through significant bloodletting, while others will soon realize they have to do the same, he said</p><p>But not all insiders are convinced. Some said these companies would need to make more drastic changes to the culture if they wanted to undo years of "fake work." "The dirty secret of these layoffs is that they're not materially changing these businesses," a former longtime Google executive said.</p><p>Addressing the issue of bloat and poor management takes more than a few layoffs. Jessica Kennedy, an organizational expert out of Vanderbilt University, said it came down to clear communication and proper incentives, adding that the issue could be solved with better organization and a clearer, more transparent differentiation between roles.</p><blockquote><q>We have a good leader, which makes all the difference.</q></blockquote><p>"It's natural for a worker to try and find a way to increase their status or differentiate themselves, but cultures are set by the people at the top of the organization, and it's their job to incentivize the right things," Kennedy said. "This is probably an issue a company would face if it focused too much on rewarding social status instead of performance. It's important employees understand the overlying purpose of the work they do. Good leaders know how to properly motivate their employees."</p><p>Some level of redundancy is a good thing if companies want to "build for resilience and innovation," but there needs to be careful thought given to it, NYU's Tavis said. "Companies making profits can become lazy around really planning for what they want," she told us. "Yes, you need to create resilience and innovation, but you need to plan for it."</p><p>As for Graham, he's since moved to another tech company, where he said he felt his contributions were more valued. "It's really good work so far," he said. "We have a good leader, which makes all the difference."</p><hr><p><em><a href="https://www.businessinsider.com/author/hugh-langley" rel="" data-analytics-product-module="body_link" data-analytics-post-depth="100" data-uri="2b64df0f6372b793adf44f92de551127">Hugh Langley</a> is a senior correspondent at Insider covering Alphabet and tech investigations. He can be reached via encrypted messaging app Signal at +1 628-228-1836 and email at hlangley@insider.com</em></p><p><em><a href="https://www.businessinsider.com/author/grace-kay" data-analytics-product-module="body_link" rel="">Grace Kay</a> is a reporter on Insider's business news team. You can contact her at gkay@insider.com</em></p>
                      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Easy SVG sparklines (157 pts)]]></title>
            <link>https://alexplescan.com/posts/2023/07/08/easy-svg-sparklines/</link>
            <guid>36664604</guid>
            <pubDate>Mon, 10 Jul 2023 10:59:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alexplescan.com/posts/2023/07/08/easy-svg-sparklines/">https://alexplescan.com/posts/2023/07/08/easy-svg-sparklines/</a>, See on <a href="https://news.ycombinator.com/item?id=36664604">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
        <article role="article">
  

  <div>
    <p><a href="https://en.wikipedia.org/wiki/Sparkline">Sparkline charts</a> are compact, simple charts showing a general trend without getting into the nitty-gritty of a more complete solution.</p>

<p>On <a href="https://mailgrip.io/">Mailgrip</a>, I use them as a UI flourish to give a visual indication of how many emails an inbox has received over time:</p>

<picture><source srcset="https://alexplescan.com/generated/assets/posts/easy-svg-sparklines/mailgrip-sparkline-500-965b47029.webp 1.0x, https://alexplescan.com/generated/assets/posts/easy-svg-sparklines/mailgrip-sparkline-1000-965b47029.webp 2.0x, https://alexplescan.com/generated/assets/posts/easy-svg-sparklines/mailgrip-sparkline-1116-965b47029.webp 2.23x" type="image/webp"><source srcset="https://alexplescan.com/generated/assets/posts/easy-svg-sparklines/mailgrip-sparkline-500-965b47029.jpg 1.0x, https://alexplescan.com/generated/assets/posts/easy-svg-sparklines/mailgrip-sparkline-1000-965b47029.jpg 2.0x, https://alexplescan.com/generated/assets/posts/easy-svg-sparklines/mailgrip-sparkline-1116-965b47029.jpg 2.23x" type="image/jpeg"><img src="https://alexplescan.com/generated/assets/posts/easy-svg-sparklines/mailgrip-sparkline-800-6e055d663.png" alt="a screenshot showing an example of a sparkline"></picture>

<p>This post will explain how to use SVG to create minimal sparklines in a really easy and fast way.</p>

<h2 id="hand-crafting-the-svg">Hand crafting the SVG</h2>

<p>Let’s start by writing the SVG by hand, and then I’ll show you an example of the Elixir code I use to generate sparklines in Mailgrip.</p>

<p>Say we’ve got this series of 10 data points to display:</p>

<div><pre><code>1, 0, 5, 4, 8, 10, 15, 10, 5, 4
</code></pre></div>

<h3 id="drawing-a-line">Drawing a line</h3>

<p>Everything has a starting point, including our sparkline. So let’s begin by drawing a line.</p>

<p>SVGs use a set of primitive commands (<a href="https://developer.mozilla.org/en-US/docs/Web/SVG/Attribute/d#path_commands">docs</a>) to draw shapes. The ones we’re interested in are:</p>

<ul>
  <li>M (<code>moveto</code>): Sets a new starting point. It takes two parameters: X and Y.</li>
  <li>L (<code>lineto</code>): Draws a line from the current position to a new one, specified by X and Y parameters.</li>
  <li>Z (<code>closepath</code>): Draws a line from the current position back to the starting position.</li>
</ul>

<p>We’ll use these commands to draw our lines based on X and Y coordinates.</p>

<p>In SVG, the origin coordinates (0, 0) are at the top left. However we want our chart’s coordinates to start at the bottom left.</p>

<p>To adjust the Y position for a point, we’ll substract it from the largest (max) point in our data set. This flips our Y positions to:</p>

<div><pre><code>14, 15, 10, 11, 7, 5, 0, 5, 10, 11
</code></pre></div>

<p>Figuring out the X positions is straightforward; they’re just the index of the point we’re drawing:</p>

<div><pre><code>0, 1, 2, 3, 4, 5, 6, 7, 8, 9
</code></pre></div>

<p>Now, let’s put these coordinates to work and draw our sparkline:</p>

<div><pre><code><span>&lt;svg</span> <span>height=</span><span>"180px"</span> <span>width=</span><span>"500px"</span><span>&gt;</span>
  <span>&lt;path</span>
    <span>d=</span><span>"M 0 14 L 1 15 L 2 10 L 3 11 L 4 7 L 5 5 L 6 0 L 7 5 L 8 10 L 9 11"</span>
    <span>stroke-width=</span><span>"2"</span>
    <span>stroke=</span><span>"red"</span>
    <span>fill=</span><span>"transparent"</span>
  <span>/&gt;</span>
<span>&lt;/svg&gt;</span>
</code></pre></div>

<p>Behold our beautiful creation:</p>



<p>Okay that’s a bit sad, we’ve got a few more steps to go still…</p>

<h3 id="scaling">Scaling</h3>

<p>We’ve defined that our SVG should have a height of 180px and width of 500px, but the line we drew is rendering at one pixel per X/Y coordinate.</p>

<p>Here’s where SVG’s ability to <em>Scale</em> Vector Graphics really helps out. Instead of having to transpose our data to screen space coordinates, we can let SVG do it for us!</p>

<p>We use the <code>viewBox</code> attribute (<a href="https://developer.mozilla.org/en-US/docs/Web/SVG/Attribute/viewBox">docs</a>) on the SVG element to set the coordinate space of the chart. <code>viewBox</code> values are zero-indexed, so our width will be <code>9</code> (because we have a total of 10 points) and our height will be <code>15</code> (because our max point value is 15).</p>

<div><pre><code><span>&lt;svg</span> <span>height=</span><span>"180px"</span> <span>width=</span><span>"500px"</span> <span>viewBox=</span><span>"0 0 9 15"</span><span>&gt;</span>
  <span>&lt;path</span>
    <span>d=</span><span>"M 0 14 L 1 15 L 2 10 L 3 11 L 4 7 L 5 5 L 6 0 L 7 5 L 8 10 L 9 11"</span>
    <span>stroke-width=</span><span>"2"</span>
    <span>stroke=</span><span>"red"</span>
    <span>fill=</span><span>"transparent"</span>
  <span>/&gt;</span>
<span>&lt;/svg&gt;</span>
</code></pre></div>



<p>Ah, but now a couple of other funky things have happened:</p>

<ul>
  <li>The <code>stroke-width</code> of 2px has scaled up with the image. We can tell SVG to keep the stroke consistent by setting the <code>vector-effect</code> (<a href="https://developer.mozilla.org/en-US/docs/Web/SVG/Attribute/vector-effect">docs</a>) property on our path.</li>
  <li>The image scaled up using the aspect ratio of the 9 x 15 <code>viewBox</code>, which is different to that of our image. We can tell SVG to maintain the aspect ratio of the <code>svg</code> element by setting <code>preserveAspectRatio</code> to <code>none</code> (<a href="https://developer.mozilla.org/en-US/docs/Web/SVG/Attribute/preserveAspectRatio">docs</a>).</li>
</ul>

<div><pre><code><span>&lt;svg</span> <span>height=</span><span>"180px"</span> <span>width=</span><span>"500px"</span> <span>viewBox=</span><span>"0 0 9 15"</span> <span>preserveAspectRatio=</span><span>"none"</span><span>&gt;</span>
  <span>&lt;path</span>
    <span>d=</span><span>"M 0 14 L 1 15 L 2 10 L 3 11 L 4 7 L 5 5 L 6 0 L 7 5 L 8 10 L 9 11"</span>
    <span>stroke-width=</span><span>"2"</span>
    <span>stroke=</span><span>"red"</span>
    <span>fill=</span><span>"transparent"</span>
    <span>vector-effect=</span><span>"non-scaling-stroke"</span>
  <span>/&gt;</span>
<span>&lt;/svg&gt;</span>
</code></pre></div>



<p>That’s better, now for some more… flare… let’s add a fill to the SVG as well:</p>

<h3 id="adding-a-fill">Adding a fill</h3>

<p>To do this, we copy our existing line but set a <code>fill</code> on it instead of a <code>stroke</code>:</p>

<div><pre><code><span>&lt;svg</span> <span>height=</span><span>"180px"</span> <span>width=</span><span>"500px"</span> <span>viewBox=</span><span>"0 0 9 15"</span> <span>preserveAspectRatio=</span><span>"none"</span><span>&gt;</span>
  <span>&lt;path</span>
    <span>d=</span><span>"M 0 14 L 1 15 L 2 10 L 3 11 L 4 7 L 5 5 L 6 0 L 7 5 L 8 10 L 9 11"</span>
    <span>stroke=</span><span>"transparent"</span>
    <span>fill=</span><span>"pink"</span>
  <span>/&gt;</span>
  <span>&lt;path</span>
    <span>d=</span><span>"M 0 14 L 1 15 L 2 10 L 3 11 L 4 7 L 5 5 L 6 0 L 7 5 L 8 10 L 9 11"</span>
    <span>stroke-width=</span><span>"2"</span>
    <span>stroke=</span><span>"red"</span>
    <span>fill=</span><span>"transparent"</span>
    <span>vector-effect=</span><span>"non-scaling-stroke"</span>
  <span>/&gt;</span>
<span>&lt;/svg&gt;</span>
</code></pre></div>



<p>Almost there! Let’s close that unsightly white gap. To do so, we need to extend our line to the bottom right of the graphic (<code>L 9 15</code>), then to the bottom left (<code>L 0 15</code>), then back up to the starting point (<code>Z</code>).</p>

<p>This creates a closed line that nicely encapsulates the area we want to fill in:</p>

<div><pre><code><span>&lt;svg</span> <span>height=</span><span>"180px"</span> <span>width=</span><span>"500px"</span> <span>viewBox=</span><span>"0 0 9 15"</span> <span>preserveAspectRatio=</span><span>"none"</span><span>&gt;</span>
  <span>&lt;path</span>
    <span>d=</span><span>"M 0 14 L 1 15 L 2 10 L 3 11 L 4 7 L 5 5 L 6 0 L 7 5 L 8 10 L 9 11 L 9 15 L 0 15 Z"</span>
    <span>stroke=</span><span>"transparent"</span>
    <span>fill=</span><span>"pink"</span>
  <span>/&gt;</span>
  <span>&lt;path</span>
    <span>d=</span><span>"M 0 14 L 1 15 L 2 10 L 3 11 L 4 7 L 5 5 L 6 0 L 7 5 L 8 10 L 9 11"</span>
    <span>stroke-width=</span><span>"2"</span>
    <span>stroke=</span><span>"red"</span>
    <span>fill=</span><span>"transparent"</span>
    <span>vector-effect=</span><span>"non-scaling-stroke"</span>
  <span>/&gt;</span>
<span>&lt;/svg&gt;</span>
</code></pre></div>



<p>That’s looking pretty good now - especially considering how simple it was to draw. Now let’s move on to rendering these on the server…</p>

<h2 id="rendering-sparklines-on-the-server">Rendering Sparklines on the server</h2>

<p>One of my favourite things about creating sparklines like this is that I can create the SVGs entirely on the backend. I don’t need to worry about using a JavaScript charting library, or sending the “points” data to the frontend. The browser requests an SVG. The server returns it. Simple!</p>

<p>Mailgrip is written in Elixir and uses the Phoenix framework, so the code I’m sharing is in Elixir too - but this approach could be adapted to any programming language.</p>

<p>The Phoenix controller defines a route that looks like:</p>

<div><pre><code><span>defmodule</span> <span>MailgripWeb</span><span>.</span><span>InboxController</span> <span>do</span>
  <span>def</span> <span>activity_svg</span><span>(</span><span>conn</span><span>,</span> <span>_params</span><span>)</span> <span>do</span>
    <span>points</span> <span>=</span>
      <span>Emails</span><span>.</span><span>message_stats</span><span>(</span><span>conn</span><span>.</span><span>assigns</span><span>.</span><span>inbox</span><span>,</span> <span>30</span><span>)</span>
      <span>|&gt;</span> <span>Enum</span><span>.</span><span>map</span><span>(</span><span>fn</span> <span>stat</span> <span>-&gt;</span> <span>Map</span><span>.</span><span>fetch!</span><span>(</span><span>stat</span><span>,</span> <span>:count</span><span>)</span> <span>end</span><span>)</span>

    <span>conn</span>
    <span>|&gt;</span> <span>put_resp_content_type</span><span>(</span><span>"image/svg+xml"</span><span>)</span>
    <span>|&gt;</span> <span>send_resp</span><span>(</span><span>200</span><span>,</span> <span>MailgripWeb</span><span>.</span><span>Sparkline</span><span>.</span><span>draw</span><span>(</span><span>100</span><span>,</span> <span>20</span><span>,</span> <span>points</span><span>))</span>
  <span>end</span>
<span>end</span>
</code></pre></div>

<p>Which in turn calls this module (heavily inspired by the <a href="https://github.com/mindok/contex">Contex</a> package):</p>

<div><pre><code><span>defmodule</span> <span>MailgripWeb</span><span>.</span><span>Sparkline</span> <span>do</span>
  <span>@fill</span> <span>"#dcfce7"</span>
  <span>@stroke</span> <span>"#bbf7d0"</span>
  <span>@stroke_width</span> <span>4</span>

  <span>def</span> <span>draw</span><span>(</span><span>width</span><span>,</span> <span>height</span><span>,</span> <span>points</span><span>)</span> <span>do</span>
    <span>vb_width</span> <span>=</span> <span>length</span><span>(</span><span>points</span><span>)</span> <span>-</span> <span>1</span>
    <span>vb_height</span> <span>=</span> <span>Enum</span><span>.</span><span>max</span><span>(</span><span>points</span><span>)</span>

    <span>"""
    &lt;svg height="#{height}" width="#{width}" viewBox="0 0 #{vb_width} #{vb_height}" preserveAspectRatio="none" role="img" xmlns="http://www.w3.org/2000/svg"&gt;
      &lt;path d="#{closed_path(points, vb_width, vb_height)}" stroke="transparent" fill="#{@fill}" /&gt;
      &lt;path d="#{path(points, vb_width, vb_height)}" stroke-width="#{@stroke_width}" vector-effect="non-scaling-stroke" stroke="#{@stroke}" fill="transparent" /&gt;
    &lt;/svg&gt;
    """</span>
  <span>end</span>

  <span>defp</span> <span>path</span><span>(</span><span>points</span><span>,</span> <span>vb_width</span><span>,</span> <span>vb_height</span><span>)</span> <span>do</span>
    <span>[</span>
      <span>"M"</span><span>,</span>
      <span>points</span>
      <span>|&gt;</span> <span>Enum</span><span>.</span><span>with_index</span><span>()</span>
      <span>|&gt;</span> <span>Enum</span><span>.</span><span>map</span><span>(</span><span>fn</span> <span>{</span><span>value</span><span>,</span> <span>i</span><span>}</span> <span>-&gt;</span>
        <span>x</span> <span>=</span> <span>i</span>
        <span>y</span> <span>=</span> <span>vb_height</span> <span>-</span> <span>value</span>
        <span>"</span><span>#{</span><span>x</span><span>}</span><span> </span><span>#{</span><span>y</span><span>}#{</span><span>if</span> <span>i</span> <span>&lt;</span> <span>vb_width</span><span>,</span> <span>do</span><span>:</span> <span>" L "</span><span>}</span><span>"</span>
      <span>end</span><span>)</span>
    <span>]</span>
  <span>end</span>

  <span>defp</span> <span>closed_path</span><span>(</span><span>points</span><span>,</span> <span>vb_width</span><span>,</span> <span>vb_height</span><span>)</span> <span>do</span>
    <span>[</span><span>path</span><span>(</span><span>points</span><span>,</span> <span>vb_width</span><span>,</span> <span>vb_height</span><span>),</span> <span>" L </span><span>#{</span><span>vb_width</span><span>}</span><span> </span><span>#{</span><span>vb_height</span><span>}</span><span> L 0 </span><span>#{</span><span>vb_height</span><span>}</span><span> Z"</span><span>]</span>
  <span>end</span>
<span>end</span>
</code></pre></div>

<p>That’s all it takes for minimal sparklines to add some flourish to your user interfaces!</p>

<p>My use of sparklines is gonna go:</p>



  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What’s New in Thunderbird 115 (399 pts)]]></title>
            <link>https://www.thunderbird.net/en-US/thunderbird/115.0/whatsnew/</link>
            <guid>36664113</guid>
            <pubDate>Mon, 10 Jul 2023 09:58:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thunderbird.net/en-US/thunderbird/115.0/whatsnew/">https://www.thunderbird.net/en-US/thunderbird/115.0/whatsnew/</a>, See on <a href="https://news.ycombinator.com/item?id=36664113">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
  <section>
    <figure>
      
      
      
      
    </figure>
    <figcaption>115 | 102</figcaption>
  </section>

  <p><img src="https://www.thunderbird.net/media/img/thunderbird/backgrounds/nebula.png" alt="A nebula of stars"></p>

  <div>
      <div>
        
        <p>
          Supernova features a single dynamic toolbar, presenting frequently used and contextual options based on the tab
          or Space that is currently active. Take full control by customizing the toolbar and window layout to perfectly fit your
          workflow.
        </p>
      </div>
      <p><img src="https://www.thunderbird.net/media/img/thunderbird/whatsnew/115/utb.png" alt="The all new Unified Toolbar">
      </p>
    </div>

  <p><img src="https://www.thunderbird.net/media/img/thunderbird/backgrounds/nebula.png" alt="A nebula of stars"></p>

  <div>
      <div>
        
        <p>
          More than just a refreshed set of graphics, Supernova introduces beautiful icons with a more consistent visual
          style unique to Thunderbird. Our new designs remain sharp and pixel-perfect at any density setting.
        </p>
      </div>
      <p><img src="https://www.thunderbird.net/media/img/thunderbird/whatsnew/115/iconography.png" alt="The improved icons">
      </p>
    </div>

  <p><img src="https://www.thunderbird.net/media/img/thunderbird/backgrounds/nebula.png" alt="A nebula of stars"></p>

  <div>
        
        <p>
          Working with multiple monitors and display resolutions? Supernova lets you dial in the perfect density settings
          and font sizes for the entire application, with just a single click from the App Menu.
        </p>
      </div>

  <p><img src="https://www.thunderbird.net/media/img/thunderbird/backgrounds/nebula.png" alt="A nebula of stars"></p>

  

  <p><img src="https://www.thunderbird.net/media/img/thunderbird/backgrounds/nebula.png" alt="A nebula of stars"></p>

  <div>
      <div>
          
          <p>
            Supernova gives you more control by introducing sortable Folder Modes. Display all of your Tags in the Folder
            Pane, turn on and off Local Folders, or move your favorite Folder Mode sections up and down with one click. Less
            scrolling, more productivity.
          </p>
        </div>

      <p><img src="https://www.thunderbird.net/media/img/thunderbird/whatsnew/115/folderpane.png" alt="The improved folder pane">
      </p>
    </div>

  <p><img src="https://www.thunderbird.net/media/img/thunderbird/backgrounds/nebula.png" alt="A nebula of stars"></p>

  

  <p><img src="https://www.thunderbird.net/media/img/thunderbird/backgrounds/nebula.png" alt="A nebula of stars"></p>

  <div>
      <div>
        
        <p>
          Supernova continues to iterate on the modernized Address Book introduced in Thunderbird 102. You’ll enjoy a new
          tabular view, an improved Edit view, delete buttons, and better accessibility.
        </p>
      </div>
      <p><img src="https://www.thunderbird.net/media/img/thunderbird/whatsnew/115/ab.png" alt="The improved address book edit pane">
      </p>
    </div>

  <p><img src="https://www.thunderbird.net/media/img/thunderbird/backgrounds/nebula.png" alt="A nebula of stars"></p>

  <div>
        
        <p>
          Supernova substantially improves Thunderbird’s keyboard navigation and screen reader accessibility across the
          entire application. We’ve also greatly expanded the ability to navigate Mail content and buttons using the TAB and
          arrow keys.
        </p>
      </div>

  <p><img src="https://www.thunderbird.net/media/img/thunderbird/backgrounds/nebula.png" alt="A nebula of stars"></p>

  <div>
      <div>
        
        <p>
          As part of an ongoing effort to modernize and upgrade Thunderbird’s Calendar, Supernova introduces an improved
          “mini-month” layout, improvements to the day/week/month grid, a pleasing color palette, and several more minor changes.
        </p>
      </div>
      <p><img src="https://www.thunderbird.net/media/img/thunderbird/whatsnew/115/cal.png" alt="The improved calendar">
      </p>
    </div>

  <p><img src="https://www.thunderbird.net/media/img/thunderbird/backgrounds/nebula.png" alt="A nebula of stars"></p>

  <div>
      <h3>More to come!</h3>
      <p>
        Supernova is constantly evolving. Throughout the next year, we’ll deliver many improvements to existing Supernova
        features and introduce brand new ones. Upgrade to version 115 and experience the future of Thunderbird!
      </p>
    </div>

  

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A biological camera that captures and stores images directly into DNA (103 pts)]]></title>
            <link>https://www.nature.com/articles/s41467-023-38876-w</link>
            <guid>36663498</guid>
            <pubDate>Mon, 10 Jul 2023 08:26:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/s41467-023-38876-w">https://www.nature.com/articles/s41467-023-38876-w</a>, See on <a href="https://news.ycombinator.com/item?id=36663498">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <div id="Sec1-section" data-title="Introduction"><h2 id="Sec1">Introduction</h2><div id="Sec1-content"><p>DNA is a key biomaterial that forms the basis of biological life on earth. It serves as the storage of genetic information that encodes for the multitude of proteins which fulfil various functions of life. Due to this ability to act as a storage of information, along with its simple, repeating code of 4 nucleotides, ATCG, DNA has been proposed and subsequently explored as a form of digital information storage<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Church, G. M., Gao, Y. &amp; Kosuri, S. Next-generation digital information storage in DNA. Science 337, 1628–1628 (2012)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR1" id="ref-link-section-d8717801e470">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Goldman, N. et al. Towards practical, high-capacity, low-maintenance information storage in synthesized DNA. Nature 494, 77–80 (2013)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR2" id="ref-link-section-d8717801e473">2</a></sup>. This is due to its extreme density (petabytes per gram<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Zhirnov, V., Zadegan, R. M., Sandhu, G. S., Church, G. M. &amp; Hughes, W. L. Nucleic acid memory. Nat. Mater. 15, 366–370 (2016)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR3" id="ref-link-section-d8717801e477">3</a></sup>), longevity (DNA has been retrieved from samples millions of years old<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="van der Valk, T. et al. Million-year-old DNA sheds light on the genomic history of mammoths. Nature, 1–5. 
                  https://doi.org/10.1038/s41586-021-03224-9
                  
                 (2021)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR4" id="ref-link-section-d8717801e481">4</a></sup>), and continued relevance to study due to its underpinning of biological life. These properties have led to a recent boom in developing different workflows<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Lim, C. K., Nirantar, S., Yew, W. S. &amp; Poh, C. L. Novel Modalities in DNA Data Storage. Trends Biotechnol. 39, 990–1003 (2021)." href="#ref-CR5" id="ref-link-section-d8717801e485">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Tabatabaei, S. K. et al. DNA punch cards for storing data on native DNA sequences via enzymatic nicking. Nat. Commun. 11, 1–10 (2020)." href="#ref-CR6" id="ref-link-section-d8717801e485_1">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Chen, K., Zhu, J., Bošković, F. &amp; Keyser, U. F. Nanopore-based DNA hard drives for rewritable and secure data storage. Nano Lett. 
                  https://doi.org/10.1021/acs.nanolett.0c00755
                  
                 (2020)." href="#ref-CR7" id="ref-link-section-d8717801e485_2">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Tomek, K. J. et al. Driving the scalability of DNA-based information storage systems. ACS Synth. Biol. 8, 1241–1248 (2019)." href="#ref-CR8" id="ref-link-section-d8717801e485_3">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Yim, S. S. et al. Robust direct digital-to-biological data storage in living cells. Nat. Chem. Biol. 17, 1–8 (2021)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR9" id="ref-link-section-d8717801e488">9</a></sup> for converting digital data into DNA and vice versa, which has been increasingly relevant and urgent due to the impending shortage of silica necessary for manufacturing storage devices required to accommodate our projected data storage requirements<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Zhirnov, V., Zadegan, R. M., Sandhu, G. S., Church, G. M. &amp; Hughes, W. L. Nucleic acid memory. Nat. Mater. 15, 366–370 (2016)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR3" id="ref-link-section-d8717801e492">3</a></sup>.</p><p>Current DNA storage workflows largely rely on in vitro synthesis of DNA strands, which are costly and require complex instrumentation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Ceze, L., Nivala, J. &amp; Strauss, K. Molecular digital data storage using DNA. Nat. Rev. Genet. 20, 456–466 (2019)." href="#ref-CR10" id="ref-link-section-d8717801e499">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Lee, H. H., Kalhor, R., Goela, N., Bolot, J. &amp; Church, G. M. Terminator-free template-independent enzymatic DNA synthesis for digital information storage. Nat. Commun. 10, 1–12 (2019)." href="#ref-CR11" id="ref-link-section-d8717801e499_1">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Takahashi, C. N., Nguyen, B. H., Strauss, K. &amp; Ceze, L. Demonstration of end-to-end automation of DNA Data Storage. Sci. Rep. 9, 1–5 (2019)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR12" id="ref-link-section-d8717801e502">12</a></sup>. Errors in the synthesis process are also common. While there have been substantial advances in accelerating this process by developing enzymatic synthesis methods<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Palluk, S. et al. De novo DNA synthesis using polymerase-nucleotide conjugates. Nat. Biotechnol. 36, 645–650 (2018)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR13" id="ref-link-section-d8717801e506">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Lu, X. et al. Enzymatic DNA synthesis by engineering terminal deoxynucleotidyl transferase. ACS Catal. 2988–2997. 
                  https://doi.org/10.1021/acscatal.1c04879
                  
                 (2022)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR14" id="ref-link-section-d8717801e509">14</a></sup>, miniaturizing electrochemical synthesis<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Xu, C. et al. Electrochemical DNA synthesis and sequencing on a single electrode with scalability for integrated data storage. Sci. Adv. 7, eabk0100 (2021)." href="#ref-CR15" id="ref-link-section-d8717801e513">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Nguyen, B. H. et al. Scaling DNA data storage with nanoscale electrode wells. Sci. Adv. 7, eabi6714 (2021)." href="#ref-CR16" id="ref-link-section-d8717801e513_1">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Antkowiak, P. L. et al. Low cost DNA data storage using photolithographic synthesis and advanced information reconstruction and error correction. Nat. Commun. 11, 5345 (2020)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR17" id="ref-link-section-d8717801e516">17</a></sup>, or developing more robust encoding methods<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Erlich, Y. &amp; Zielinski, D. DNA Fountain enables a robust and efficient storage architecture. Science 355, 950–954 (2017)." href="#ref-CR18" id="ref-link-section-d8717801e520">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Wang, Y. et al. High capacity DNA data storage with variable-length Oligonucleotides using repeat accumulate code and hybrid mapping. J. Biol. Eng. 13, 89 (2019)." href="#ref-CR19" id="ref-link-section-d8717801e520_1">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Anavy, L., Vaknin, I., Atar, O., Amit, R. &amp; Yakhini, Z. Data storage in DNA with fewer synthesis cycles using composite DNA letters. Nat. Biotechnol. 37, 1229–1236 (2019)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR20" id="ref-link-section-d8717801e523">20</a></sup>, DNA synthesis remains a bottleneck in the adoption of DNA as a data storage medium. There is thus significant interest in developing ways of encoding information into DNA that can either supersede or circumvent DNA synthesis in its current form.</p><p>The abundance of DNA in living cells have been considered as a potential source of DNA that can be encoded with information via the use of molecular biology tools and biological systems. Utilizing living cells as a way to incorporate and record external signals such as the presence of chemicals or electrical inputs into DNA have also been demonstrated, with recording systems ranging from recombinases<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Sheets, M. B., Wong, W. W. &amp; Dunlop, M. J. Light-inducible recombinases for bacterial optogenetics. ACS Synth. Biol. 9, 227–235 (2020)." href="#ref-CR21" id="ref-link-section-d8717801e530">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Yang, L. et al. Permanent genetic memory with >1-byte capacity. Nat. Methods 11, 1261–1266 (2014)." href="#ref-CR22" id="ref-link-section-d8717801e530_1">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Bonnet, J., Subsoontorn, P. &amp; Endy, D. Rewritable digital data storage in live cells via engineered control of recombination directionality. Proc. Natl Acad. Sci. USA 109, 8884–8889 (2012)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR23" id="ref-link-section-d8717801e533">23</a></sup> to CRISPR-based modifications<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Shipman, S. L., Nivala, J., Macklis, J. D. &amp; Church, G. M. Molecular recordings by directed CRISPR spacer acquisition. Science 353, aaf1175 (2016)." href="#ref-CR24" id="ref-link-section-d8717801e537">24</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Shipman, S. L., Nivala, J., Macklis, J. D. &amp; Church, G. M. CRISPR–Cas encoding of a digital movie into the genomes of a population of living bacteria. Nature 547, 345–349 (2017)." href="#ref-CR25" id="ref-link-section-d8717801e537_1">25</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Tang, W. &amp; Liu, D. R. Rewritable multi-event analog recording in bacterial and mammalian cells. Science 360, eaap8992 (2018)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR26" id="ref-link-section-d8717801e540">26</a></sup>. Most notably, recent work has showcased the storage of 72 bits of information directly into the genomic DNA of living cells via external electrical input signals<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Yim, S. S. et al. Robust direct digital-to-biological data storage in living cells. Nat. Chem. Biol. 17, 1–8 (2021)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR9" id="ref-link-section-d8717801e544">9</a></sup>, as well as the use of DNA structural barcodes that bind to the M13mp18 bacteriophage DNA scaffold to encode information<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Chen, K., Zhu, J., Bošković, F. &amp; Keyser, U. F. Nanopore-based DNA hard drives for rewritable and secure data storage. Nano Lett. 
                  https://doi.org/10.1021/acs.nanolett.0c00755
                  
                 (2020)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR7" id="ref-link-section-d8717801e548">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Chen, K. et al. Digital data storage using DNA nanostructures and solid-state nanopores. Nano Lett. 19, 1210–1215 (2019)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR27" id="ref-link-section-d8717801e551">27</a></sup>. However, such systems have not been able to accurately capture spatial information, due to the lack of mechanisms that can simultaneously encode spatial information as well as external input signals. Furthermore, existing encoding systems also tend to utilize low-throughput chemical processes, which severely limits scalability.</p><p>In this paper, we present a workflow that allows the direct capture of both spatial information and input signals via light into DNA itself as a means to store digital information such as images into DNA. We utilize a blue light-responsive recombinase system that responds to the presence or absence of blue light as an external signal, and subsequently records that response into DNA itself via site-specific DNA editing. To enable the encoding of spatial information along with the recorded signal, we implemented a barcoding scheme to enable differentiation of individual wells containing cells with records of light exposure, thereby ‘digitizing’ the recorded image and allowing for deconvolution upon retrieval of the DNA sequences by sequencing. As a result, this recombinase-based DNA recorder, coupled with the aforementioned barcoding scheme that can preserve spatial information, enables the direct capture of both spatial information as well as input signals via light into DNA itself. While previous work has demonstrated the usage of light-responsive systems that can capture light and display this input as a corresponding light output, akin to an analogue camera<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Fernandez-Rodriguez, J., Moser, F., Song, M. &amp; Voigt, C. A. Engineering RGB color vision into Escherichia coli. Nat. Chem. Biol. 13, 706–708 (2017)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR28" id="ref-link-section-d8717801e558">28</a></sup>, our process instead creates a biological analogue to a digital camera which we term as ‘BacCam’. As the DNA encoded with different images can be pooled and stored together, we further characterize this workflow by demonstrating random access and multiplexing, and utilize outlier detection and reassignment techniques, along with unsupervised clustering algorithms from the field of machine learning to accurately reconstruct the stored data. Taking advantage of the multiplexing capabilities offered by light, we further layer an orthogonal red light system with the blue light system to allow encoding of 2 images simultaneously, increasing the scalability and density of the workflow and enabling multicolor image capture. Utilizing molecular biology methods, optogenetics, barcoding techniques, and biological systems, we thus provide a framework for the integration of biological and digital interfaces.</p></div></div><div id="Sec2-section" data-title="Results"><h2 id="Sec2">Results</h2><div id="Sec2-content"><h3 id="Sec3">BacCam enables the direct-to-DNA storage and retrieval of images</h3><p>Light has the distinct advantages of being cheap, massively parallelizable, rapid, highly programmable, and easily multiplexed, with little effort or cost needed to scale or generate patterns of increasing complexity. There is therefore an interest in utilizing light as a patterning mechanism in biology, such as in photolithographic DNA synthesis<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Antkowiak, P. L. et al. Low cost DNA data storage using photolithographic synthesis and advanced information reconstruction and error correction. Nat. Commun. 11, 5345 (2020)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR17" id="ref-link-section-d8717801e574">17</a></sup>, as well as optogenetic circuits for multiple purposes such as bioproduction or controlling cellular behavior on a spatiotemporal level<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Jayaraman, P. et al. Blue light-mediated transcriptional activation and repression of gene expression in bacteria. Nucleic Acids Res. 44, 6994–7005 (2016)." href="#ref-CR29" id="ref-link-section-d8717801e578">29</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Pouzet, S. et al. The promise of optogenetics for bioproduction: dynamic control strategies and scale-up instruments. Bioengineering 7, 151 (2020)." href="#ref-CR30" id="ref-link-section-d8717801e578_1">30</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Mansouri, M. &amp; Fussenegger, M. Synthetic biology-based optogenetic approaches to control therapeutic designer cells. Curr. Opin. Syst. Biol. 28, 100396 (2021)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR31" id="ref-link-section-d8717801e581">31</a></sup>. We therefore sought a way to utilize light as an input to encode information into DNA. We hypothesized that cells containing optogenetic circuits that can record the presence or absence of light within DNA can be perceived as analogous to a digital camera that captures images via light exposure and records said exposure in a digital format. We thus utilized an Opto-Cre-Vvd recombinase system, whereby a Cre-Lox recombinase protein was engineered to be light inducible by splitting the recombinase and attaching photodimers that bring the split protein together upon exposure to blue light<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Sheets, M. B., Wong, W. W. &amp; Dunlop, M. J. Light-inducible recombinases for bacterial optogenetics. ACS Synth. Biol. 9, 227–235 (2020)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR21" id="ref-link-section-d8717801e585">21</a></sup>. The recombinase, upon activation, excises a predefined section of DNA that are flanked by LoxP sites, resulting in an alteration in the sequence. We posited that this alteration is analogous to the encoding of a bit. To determine whether a bit has been encoded, upon sequencing, the total number of reads that possess the excised LoxP sequence is compared to the total number of reads with the full LoxP sequence. A high ratio of excised reads to unexcised ones would correspond to a bit state of ‘1’ being encoded, while the opposite would correspond to a bit state of ‘0’.</p><p>We then posited that a 96-well plate, with each well being appropriately barcoded, and containing cells with the Opto-Cre-Vvd and LoxP genetic circuits, would be analogous to a digital camera with image sensors each containing individual pixels that have their own unique identifier, storing information corresponding to light exposure. To scale the one-bit digital drive within each cell to that of a multi-pixel image, a population of <i>Escherichia</i> <i>coli</i> (<i>E. coli</i>) bacteria containing the above-mentioned circuit was spatially separated in individual wells of a black, clear-bottomed 96-well plate, upon which a predefined pattern of 465 nm wavelength light was projected from below.</p><p>The projected pattern is then preserved by the addition of unique oligonucleotide barcodes, which we term as ‘well-codes’, that bind at the region preceding the LoxP recording site. The nucleotide sequence of the well-code is mapped to predefined spatial locations, with said mappings saved in a separate table, thereby resulting in the linkage of the spatial location of the recording bacteria in each isolated well along with the recorded digital drive that each bacteria possesses. The addition of the well-code to the recording sequence is conducted via PCR. Further oligonucleotide sequences were then appended via PCR to prepare the individual samples for Next-Generation Sequencing (NGS). Subsequently, all samples are then pooled together for storage, which obviates the need for preserving the original well configuration as the unique well-codes have tied this configuration with the recorded bit state. This results in a pool of DNA oligonucleotide sequences that stores the information of the image that was captured (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41467-023-38876-w#Fig1">1</a>). In practice, this barcoding system enables the capture of 2-dimensional images, where each pixel’s location is represented by a well-code, and the bit state of the pixel is determined by the presence or absence of the DNA excision site. This resulting data can be stored robustly in dried form on a benchtop at room temperature, or frozen in a −20 °C fridge.</p><div data-test="figure" data-container-section="figure" id="figure-1" data-title="Workflow of BacCam."><figure><figcaption><b id="Fig1" data-test="figure-caption-text">Fig. 1: Workflow of BacCam.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41467-023-38876-w/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-023-38876-w/MediaObjects/41467_2023_38876_Fig1_HTML.png?as=webp"><img aria-describedby="Fig1" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-023-38876-w/MediaObjects/41467_2023_38876_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="427"></picture></a></div><p><b>A</b> Encoding of light into bacteria and barcoding for preservation of spatial information. <b>B</b> Sequencing and decoding of stored DNA back into original image. <b>C</b> Initial proof of concept of BacCam using the ‘BACCAM’ pattern. Successful reconstruction of the image was done with 93/96 accuracy. Source data are provided as a Source Data file.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41467-023-38876-w/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>To retrieve the image, sequencing of the pooled DNA was then conducted. The resulting sequencing data generated was then deconvoluted by determining the total number of sequences retrieved for each barcode, and the bit state determined by the ratio of sequences lacking the region between the LoxP sites to sequences containing the region. These ratios indicate the proportion of DNA that were excised compared to ones that were intact. High ratios would thus indicate that a larger proportion of DNA was excised, which would then imply that light exposure was ‘recorded’ and so recording a ‘1’ signal, and vice versa for lower ratios, which would then record a ‘0’ signal. The threshold determining the ratio that demarcates assignment of a ‘1’ or a ‘0’ signal was derived using clustering methods that compare ratios across each plate. This deconvolution was done for all well-codes used, and the resulting bit-well-code pairs were then reassembled back, according to a previously stored mapping table of well-codes to spatial locations, to form a digitized version of the projected image.</p><p>Utilizing the above-mentioned process, we have successfully demonstrated the viability of various aspects of the workflow detailed above. We demonstrated the recording of a predefined pattern into DNA and retrieved it via sequencing (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41467-023-38876-w#Fig1">1</a>). Deconvolution of the sequenced reads was done by mapping the well-code back to its spatial location, counting the number of excised and intact LoxP reads corresponding to each well-code, and determining the bit state of each well via the above-mentioned ratio of excised to intact LoxP reads.</p><h3 id="Sec4">Multiple images can be stored in a complex pool and retrieved with high accuracy</h3><p>We subsequently tested the multiplexing capability of our workflow, generating multiple images and pooling them together, and deconvoluting each image from the DNA sequencing data generated via sequencing of said pool. We hypothesized that each full image can also be barcoded with its own ‘meta-barcode’ that separates images from one another in a heterogenous DNA pool of multiple images, despite using the same well-codes that separate individual wells, allowing for multiplexing. We thus incorporated this second layer of indexing using indexing barcodes provided by Illumina. Each 96 well&nbsp;plate, after the addition of the initial well-codes, has the same&nbsp;2 indexing barcodes appended on the 5′ and 3′&nbsp;end of the sequence&nbsp;in each well. These barcodes are combinatorial in nature, thus increasing the possible number of total images that can be stored together.</p><p>To determine if this second indexing layer can be used to deconvolute multiple images from the same sample, we exposed 5 unique patterns to 5 different 96-well plates, thereby creating a set of 5 images (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41467-023-38876-w#Fig2">2A</a>). 4 of these patterns (NUS, SYNCTI, BacCam, and Smiley) illustrate the ability of the system to capture images. The last pattern, ‘Heloo wo{|d!’ serves to illustrate the ability of BacCam to encode information (such as letters and symbols represented in 8-bit ASCII code format) by allocating each well as a bit and encoding the information in a 96-well format. Hence, each column (8 wells) will encode one ASCII code. In this case, BacCam can also serve to encode any sort of information, as long as it is projected with the appropriate pattern.</p><div data-test="figure" data-container-section="figure" id="figure-2" data-title="Storage and retrieval of multiple images in a single complex pool."><figure><figcaption><b id="Fig2" data-test="figure-caption-text">Fig. 2: Storage and retrieval of multiple images in a single complex pool.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41467-023-38876-w/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-023-38876-w/MediaObjects/41467_2023_38876_Fig2_HTML.png?as=webp"><img aria-describedby="Fig2" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-023-38876-w/MediaObjects/41467_2023_38876_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="553"></picture></a></div><p>Red boxes indicate wells with allocated bits that do not correspond with the projected image. <b>A</b> Projected patterns that are stored with the BacCam process. Images are projected on different plates, barcoded individually with index sequences corresponding to each image, and resulting products are pooled together into a single tube. <b>B</b> Retrieved images from a single sequencing run of a mixed pool, deconvoluted with the corresponding indexes of each image. <b>C</b> Retrieved images from multiple sequencing runs, whereby random access of each image was conducted from the mixed pool by using specific primers to amplify desired images before sequencing. <b>D</b> Reconstruction of selectively amplified single images from mixed libraries of increasing dilutions, demonstrating robustness of the&nbsp;amplification technique in selecting specific images from an ever-decreasing initial amount of DNA. Source data are provided as a Source Data file.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41467-023-38876-w/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>We then added the same set of well-codes to each of them via PCR, before extending each image with indexing barcodes. Each image used a different combination of indexing barcodes, which were then saved in a mapping table that links the unique index sequences with its corresponding image. These indexed images were then pooled together into the same pool of DNA. We then determined if a mixed, heterogenous pool of DNA can be sequenced as a whole library, with the individual images deconvoluted from the total raw reads that are produced by sequencing. We demonstrate that a mixed pool of at least 5 different images can be deconvoluted and reconstructed with an accuracy of at least 90% for all images, showcasing the multiplexing capability of our workflow (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41467-023-38876-w#Fig2">2B</a>).</p><p>We subsequently tested the ability of our workflow to implement random access of images, whereby each image can be selectively amplified and decoded without the need to sequence the entire mixed DNA pool. We hypothesized that designing primers that selectively bind to the indexed sequences corresponding to the desired image would be sufficient for random access, adequately amplifying each individual ‘pixel’ associated with the image, while avoiding amplification of sequences that do not possess the selected index. To test the hypothesis, we demonstrated that all images (with accuracy &gt; 80%) in a mixed pool of 5 can be selectively amplified and accessed by using the corresponding indexing primers and conducting a touchdown PCR to ensure precise binding and amplification of desired sequences<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Korbie, D. J. &amp; Mattick, J. S. Touchdown PCR for increased specificity and sensitivity in PCR amplification. Nat. Protoc. 3, 1452–1456 (2008)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR32" id="ref-link-section-d8717801e700">32</a></sup> (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41467-023-38876-w#Fig2">2C</a>), thus obviating the need to sequence the entire pool of DNA. Taken together, these results show that multiple images can be easily tagged, stored together and subsequently demultiplexed by using a simple indexing process that complements existing NGS workflows.</p><h3 id="Sec5">Information can be accurately reconstructed from small initial quantity of samples</h3><p>To determine the minimum amount of DNA required to reconstruct images accurately, we conducted a series of dilution experiments. We diluted a mixed pool of samples progressively, with each dilution being tenfold less than the previous. Sequencing of each dilution for a particular image for deconvolution and reconstruction demonstrated that images were accurately reconstructed at dilutions of a hundred-fold from the initial concentration (2.66 nM). At dilutions of a 1000-fold (2.66 pM), individual reads for each barcode numbered in the low single digits, with many having no reads at all. As a result, the fidelity of the image drops significantly (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41467-023-38876-w#Fig3">3A</a>). For large scale multiplexing of images, the concentration of DNA representing a single image drops as the number of images in each pool increases. This dilution assay thus provides us with an approximate proxy for the number of images that can be concurrently retrieved in a sequencing run, assuming a fixed volume of the pooled library was sequenced, and sequencing coverage remained the same. The results imply that the number of different images that can be stored in a pool and retrieved in a single run is between 100 and 1000. Increasing the number of distinct images that can be retrieved this way would therefore require an increase in sequencing coverage and/or increasing concentrations of each image.</p><div data-test="figure" data-container-section="figure" id="figure-3" data-title="Testing the boundaries of BacCam."><figure><figcaption><b id="Fig3" data-test="figure-caption-text">Fig. 3: Testing the boundaries of BacCam.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41467-023-38876-w/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-023-38876-w/MediaObjects/41467_2023_38876_Fig3_HTML.png?as=webp"><img aria-describedby="Fig3" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-023-38876-w/MediaObjects/41467_2023_38876_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="237"></picture></a></div><p><b>A</b> Dilution experiments demonstrating the theoretical maximum information capacity of BacCam. Images can be successfully retrieved up to a dilution of 100× from the original fully amplified pool, with 1000x dilution having significantly higher number of errors due to the low number of reads. <b>B</b> No loss in information for ‘Smiley’ stored in both liquid and dried form. <b>C</b> No loss of information in ‘Heloo wo{|d!’ stored in varying conditions for extended periods of time. Source data are provided as a Source Data file.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41467-023-38876-w/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>However, we also hypothesized that PCR amplification of diluted reads via random access might be sufficient to overcome the above-mentioned limit without an increase in sequencing coverage, saving costs while still allowing for multiplexed storage in the same pool. We therefore conducted serial dilutions on the mixed pool of images and utilized indexing primers for random access of selected images via PCR, amplifying the diluted DNA and sending it for sequencing. We demonstrated that the NUS image was selectively amplified with the N701 and N501 indexes (index sequences in Supplementary Table&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41467-023-38876-w#MOESM1">1</a>), from a 50× dilution of an initial concentration of 17.98 nM to&nbsp;0.36 nM, up to a dilution of 50,000× (0.36 pM), showcasing the robustness of image retrieval from a complex pool of 5 images, and demonstrating consistent re-amplification of images from a small quantity of initial samples (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41467-023-38876-w#Fig2">2D</a>). Attempts to retrieve the other images contained in the mixed library after selective amplification were unsuccessful, demonstrating the capabilities of targeted amplification of selective images. It is also apparent that at lower dilutions, there is an increasing skew in terms of the ratio values—suggesting that amplification from minute amounts of sample tends to dramatically increase the contrast of reads that are excised compared to unexcised ones.</p><h3 id="Sec6">Images stored are durable in varying environmental conditions</h3><p>We further explored the limits of BacCam by subjecting it to a battery of physical challenges. One way to improve the density and stability of DNA is to dry it, due to the hydrolytic activity of water molecules on the phosphate backbone. This also has the effect of reducing volume, resulting in a higher overall density. To test the viability of our workflow for drying, we spun dried a volume of DNA corresponding to the ‘Smiley’ image before rehydration and sequencing. We showed that images in dried form could be successfully retrieved with zero loss in accuracy compared to liquid, frozen images (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41467-023-38876-w#Fig3">3B</a>).</p><p>We also subjected a DNA sample with the ‘Heloo wo{|d!’ image to accelerated aging experiments. This was done by comparing identical samples, both in aqueous form with one at room temperature (RT) and another that was kept in a 60 °C oven, both for a duration of 1 week. We also exposed a freshly thawed frozen sample to UV light for 1 hr. It was previously demonstrated that data encoded in dried DNA stored at a temperature of 70 °C for a duration of 7 days was not able to be retrieved, and that duration was equivalent to storing DNA at 9.4 °C for 2000 years<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Grass, R. N., Heckel, R., Puddu, M., Paunescu, D. &amp; Stark, W. J. Robust chemical preservation of digital information on DNA in silica with error-correcting codes. Angew. Chem. Int. Ed. 54, 2552–2555 (2015)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR33" id="ref-link-section-d8717801e769">33</a></sup>. We demonstrated a successful retrieval of said image from all samples, with no loss of information despite varying conditions, showcasing the inherent robustness and redundancy of the workflow (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41467-023-38876-w#Fig3">3C</a>).</p><h3 id="Sec7">Optimization of image deconvolution with computational methods</h3><p>Upon receipt of sequencing data, proportions of excised and unexcised DNA was calculated and allocated to appropriate wells. To determine the correct threshold that will allow for accurate separation of recorded light signals, we utilized a manual thresholding process based on foreknowledge of the encoded image. This method was used to assess the accuracy of the overall workflow in storing a projected image into DNA. While this method suffices due to foreknowledge of the pattern encoded, to enable a true information encoding and retrieval system, a method that can reconstruct a potential image solely from the sequenced data would be necessary. We theorized that computational clustering methods such as unsupervised machine learning techniques could be utilized to automatically identify the distinct groups/clusters. To preprocess the sequencing data before implementing the clustering methods, we applied an outlier detection technique also known as unsupervised anomaly detection to detect the outliers located at the low-density regions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Breunig, M. M., Kriegel, H.-P., Ng, R. T. &amp; Sander, J. LOF: identifying density-based local outliers. ACM SIGMOD Rec. 29, 93–104 (2000)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR34" id="ref-link-section-d8717801e785">34</a></sup>. We proceeded to reassign the outliers with the nearest values detected from the inliers. This outlier detection and reassignment data preprocessing demonstrated improvement in removing small low-density clusters and enhancing the edges among larger clusters. Post-processing techniques were also implemented to satisfy the&nbsp;edge cases of having fully ‘ON’ or ‘OFF’&nbsp;images and to perform cluster grouping to retrieve these final binary state images. The entire automated workflow for image deconvolution is outlined in Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41467-023-38876-w#Fig4">4A</a>.</p><div data-test="figure" data-container-section="figure" id="figure-4" data-title="Developing a machine learning clustering-based workflow for automated image deconvolution."><figure><figcaption><b id="Fig4" data-test="figure-caption-text">Fig. 4: Developing a machine learning clustering-based workflow for automated image deconvolution.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41467-023-38876-w/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-023-38876-w/MediaObjects/41467_2023_38876_Fig4_HTML.png?as=webp"><img aria-describedby="Fig4" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-023-38876-w/MediaObjects/41467_2023_38876_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="487"></picture></a></div><p><b>A</b> An automated workflow incorporating outlier detection and reassignment method, machine learning unsupervised clustering technique, full ON-OFF criteria assessment, and cluster grouping for automated image deconvolution. <b>B</b> An example implemented using GMM to demonstrate the results acquired at different phases. The dots in the graphs represent the raw data or the curated data from the 96 wells. The ‘1’ and ‘−1’ obtained from the Local Outlier Factor denote the inliers and outliers detected respectively from the raw data. The curated data were acquired after reassigning the outliers to the nearest values of inliers. The M0–M2 indicate the computed mean values of the individual clusters for full ON–OFF criteria assessment. The deconvoluted image was then plotted based on the three clusters and after clusters grouping into the final binary state image (‘0’: light blue; ‘1’: dark blue) with 3 error bits (orange). <b>C</b> Validation of the automated workflow on other patterns including full ‘ON’ and ‘OFF’ datasets.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41467-023-38876-w/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>We tested multiple clustering methods under different parameter settings on the existing datasets, and compared the accuracy of the automated clustering methods with manual thresholding based on foreknowledge of the existing image (Table&nbsp;<a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://www.nature.com/articles/s41467-023-38876-w#Tab1">1</a>). The results showed that the OPTICS clustering method and 3-component Gaussian Mixture Model (GMM) produced clusters that most closely emulated manual thresholds, with near perfect accuracy (Supplementary&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41467-023-38876-w#MOESM1">S2</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41467-023-38876-w#MOESM1">S3</a>). We tested the results acquired using GMM at different phases of the workflow (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41467-023-38876-w#Fig4">4B</a>), showing how each phase contributes to building an accurate end result, and demonstrated the robustness of the automated workflow by showcasing all the successfully deconvoluted images with high accuracy (&gt;0.9) (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41467-023-38876-w#Fig4">4C</a>, Supplementary&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41467-023-38876-w#MOESM1">S2</a>).</p><div data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption><b id="Tab1" data-test="table-caption">Table 1 Comparison of methods used for automated thresholding</b></figcaption><p><a data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="https://www.nature.com/articles/s41467-023-38876-w/tables/1" aria-label="Full size table 1"><span>Full size table</span></a></p></figure></div><h3 id="Sec8">Multiplexing with multiple colors of light</h3><p>One significant advantage of using light is the ability to multiplex in a simple fashion with the addition of different light wavelengths. We redesigned a red light sensitive Cre&nbsp;recombinase (pBbS5a-RLCre) initially developed in yeast systems to add the red wavelength to the existing blue light BacCam workflow<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Hochrein, L., Mitchell, L. A., Schulz, K., Messerschmidt, K. &amp; Mueller-Roeber, B. L-SCRaMbLE as a tool for light-controlled Cre-mediated recombination in yeast. Nat. Commun. 9, 1931 (2018)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR35" id="ref-link-section-d8717801e1307">35</a></sup>. To allow for differentiation between the two light systems in the workflow, we designed another recording plasmid&nbsp;(pBbAW4k-Spacer1Barcoding-loxP-TT-loxP-ho1-pcyA) that contained another intermediate barcode, thus differentiating edits caused by red light exposure to those caused by blue light exposure (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41467-023-38876-w#Fig5">5A</a>, Supplementary&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41467-023-38876-w#MOESM1">S1</a>). We also designed a programmable light illumination device (OptoBox, Supplementary&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41467-023-38876-w#MOESM1">S8</a>) to project multiple colors of light in the same well. We show that a co-culture of the blue light and red light-responsive bacteria in the same well is able to sense multiple wavelengths of light at the same time, and encode 2 separate images simultaneously. Images were encoded in two separate ways. One way involved the overnight projection of a blue light pattern along with a different red light pattern in an alternating fashion, with each pattern being projected for 10 min before switching. Another way involved projecting both images simultaneously (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41467-023-38876-w#Fig5">5B</a>). We demonstrate the successful encoding and retrieval of two different images with different wavelengths of light, with both methods of projection being viable approaches, with a minimum accuracy of more than 90% for each image encoded (Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41467-023-38876-w#Fig5">5C</a>, Supplementary&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41467-023-38876-w#MOESM1">S5</a>–<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41467-023-38876-w#MOESM1">S7</a>). These results demonstrate high orthogonality of both light systems (Supplementary&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41467-023-38876-w#MOESM1">S9</a>).</p><div data-test="figure" data-container-section="figure" id="figure-5" data-title="Multiplexing the BacCam workflow using multiple colors of light."><figure><figcaption><b id="Fig5" data-test="figure-caption-text">Fig. 5: Multiplexing the BacCam workflow using multiple colors of light.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41467-023-38876-w/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-023-38876-w/MediaObjects/41467_2023_38876_Fig5_HTML.png?as=webp"><img aria-describedby="Fig5" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-023-38876-w/MediaObjects/41467_2023_38876_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="1289"></picture></a></div><p><b>A</b> Design of a co-culture of engineered <i>E. coli</i> with blue and red light sensitive recombinases respectively. The blue light Opto-Cre-Vvd system and the redesigned red light recombinase&nbsp;recording system were separately transformed into <i>E. coli</i>. <b>B</b> Experimental workflow of the co-culture. Both resulting strains of <i>E. coli</i> were co-cultured together on the same plate, and exposed to either alternating blue and red light projections, or simultaneous projections of both blue and red light. <b>C</b> Images obtained after sequencing and deconvolution, separating red and blue with the red light specific barcode and subsequently subjecting it to our deconvolution algorithm workflow based on 3-component Gaussian Mixture Model (GMM). Source data are provided as a Source Data file.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41467-023-38876-w/figures/5" data-track-dest="link:Figure5 Full size image" aria-label="Full size image figure 5" rel="nofollow"><span>Full size image</span></a></p></figure></div></div></div><div id="Sec9-section" data-title="Discussion"><h2 id="Sec9">Discussion</h2><div id="Sec9-content"><p>In this work, we have proposed and demonstrated an alternative workflow for DNA data storage—a direct capture of images into DNA that is analogous to the creation of a digital camera. We demonstrated scaling of this workflow using a single color light (blue) wavelength to five 96 bit images, totaling 60 bytes, and showed that the theoretical limits of this system potentially scales to more than one hundred 96 bit images in a single heterogenous pool. We have also shown the ability for random access of individual images, even from a highly dilute concentration of 50,000x lesser than the initial concentration, and the reliable recovery of data from diluted, heat treated, UV-exposed and dried pools. We have implemented computational pipelines that serve to deconvolute the encoded information and correct for errors in a reliable fashion. To scale this workflow beyond a single wavelength of light, we incorporated an additional wavelength of light, doubling the amount of data that can be stored in a single, simultaneous capture and demonstrating the multiplexing potential of the system. This system further expands the boundaries of the field outside of preexisting DNA synthesis and sequencing workflows.</p><p>We have utilized living cells as the encoder for DNA data storage in this work. This offers several advantages over de novo in vitro DNA synthesis-based data storage formats. Living cells are an ever-renewing source of DNA, making it cost effective to produce at scale. Cells also possess a multitude of systems that can interface with DNA, such as transcription factor binding proteins, and are also able to respond to a multitude of stimuli ranging from chemical to light and electrical means that can also be recorded into DNA in vivo. Furthermore, in BacCam, the writing step is done in parallel, as each bit is encoded at the same time. As such, latency of writing is greatly reduced.</p><p>While living cells have been previously used as encoders for storing images, BacCam differs primarily from existing work by combining spatial addressability along with optogenetic systems to digitally encode information directly into DNA. This is in contrast to previous work such as that of Shipman et al.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Shipman, S. L., Nivala, J., Macklis, J. D. &amp; Church, G. M. Molecular recordings by directed CRISPR spacer acquisition. Science 353, aaf1175 (2016)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR24" id="ref-link-section-d8717801e1388">24</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Shipman, S. L., Nivala, J., Macklis, J. D. &amp; Church, G. M. CRISPR–Cas encoding of a digital movie into the genomes of a population of living bacteria. Nature 547, 345–349 (2017)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR25" id="ref-link-section-d8717801e1391">25</a></sup>, in which image information was encoded via de novo DNA synthesis before insertion into the bacterial genome. Furthermore, the multiplexing ability of light allows us to utilize different wavelengths of light to encode additional layers of information, providing significant flexibility in encoding and offering potential avenues for capacity expansion.</p><p>The resolution of images encoded is a function of the number of unique well-codes generated, as well as the physical separation and isolation of individually addressable bacterial populations. As a proof of concept, we have thus far utilized 96-well plates for convenience and cost. However, we envision that subsequent implementations of the workflow can be further miniaturized and scaled up to 384, 1536 or even higher resolution systems, leveraging advances in liquid handling devices, microchips<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Chen, B. et al. High-throughput analysis and protein engineering using microcapillary arrays. Nat. Chem. Biol. 12, 76–81 (2016)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR36" id="ref-link-section-d8717801e1398">36</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Zhang, Y. et al. Accurate high-throughput screening based on digital protein synthesis in a massively parallel femtoliter droplet array. Sci. Adv. 5, eaav8185 (2019)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR37" id="ref-link-section-d8717801e1401">37</a></sup>, as well as microfluidic setups<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Antkowiak, P. L. et al. Integrating DNA encapsulates and digital microfluidics for automated data storage in DNA. Small 18, 2107381 (2022)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR38" id="ref-link-section-d8717801e1405">38</a></sup>. The robustness of PCR amplification in retrieving information from a complex pool of oligonucleotides has been demonstrated earlier, and indicates that there is still significant potential capacity for data storage. Implementation of various PCR or DNA assembly methods that do not require thermocycling for appending well-codes and indexing barcodes can also increase the throughput of the process while reducing the footprint and complexity of the workflow<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Lin, K. N., Volkel, K., Tuck, J. M. &amp; Keung, A. J. Dynamic and scalable DNA-based information storage. Nat. Commun. 11, 2981 (2020)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR39" id="ref-link-section-d8717801e1409">39</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Banal, J. L. et al. Random access DNA memory using Boolean search in an archival file storage system. Nat. Mater. 1–9. 
                  https://doi.org/10.1038/s41563-021-01021-3
                  
                 (2021)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR40" id="ref-link-section-d8717801e1412">40</a></sup>. Advances in barcode multiplexing methods, as well as metadata tagging, can potentially lead to a searchable image database, as proposed by previous groups<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Banal, J. L. et al. Random access DNA memory using Boolean search in an archival file storage system. Nat. Mater. 1–9. 
                  https://doi.org/10.1038/s41563-021-01021-3
                  
                 (2021)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR40" id="ref-link-section-d8717801e1416">40</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Bee, C. et al. Molecular-level similarity search brings computing to DNA data storage. Nat. Commun. 12, 4764 (2021)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR41" id="ref-link-section-d8717801e1419">41</a></sup>.</p><p>While we utilize light-responsive recombinases in our workflow as the main interface between light and DNA recording, it is by no means the only DNA writing system that can be used. Other advances in DNA writing and editing tools can also serve to act as the intermediary for the conversion of light to information stored in DNA, and possess further advantages such as increased orthogonality. This could lead to an increase in density and the expansion in the types of images that can be stored.</p><p>As the field of DNA data storage continues to progress, there is an increasing interest in bridging the interface between biological and digital systems. Our work showcases further applications of DNA data storage that recreate existing information capture devices in a biological form, providing the basis for continued innovation in information recording and storage.</p></div></div><div id="Sec10-section" data-title="Methods"><h2 id="Sec10">Methods</h2><div id="Sec10-content"><h3 id="Sec11">Capturing of blue light patterns into DNA</h3><p>Two plasmids, pBbAW4k-loxP-TT-loxP-mRFP1 (pLoxP) and pBbE5a-Opto-Cre-Vvd-2 (pOptoCre) were used. pBbAW4k-loxP-TT-loxP-mRFP1 was a gift from Mary Dunlop (Addgene plasmid # 134405; <a href="http://n2t.net/addgene:134405">http://n2t.net/addgene:134405</a>; RRID:Addgene_134405) and pBbS5a-Opto-Cre-Vvd-2 was a gift from Mary Dunlop (Addgene plasmid # 160400; <a href="http://n2t.net/addgene:160400">http://n2t.net/addgene:160400</a>; RRID:Addgene_160400). These plasmids were transformed together into XL1-Blue <i>E.&nbsp; coli</i> chemically competent cells. pLoxP contains two LoxP sites within the plasmid in the same orientation, with an intervening sequence containing 2 terminator sequences. pOptoCre contains a genetic circuit that utilizes the Lac promoter to control production of the Opto-Cre-Vvd-2 construct. The transformed strain was inoculated into a culture tube with 5 ml of LB medium supplemented with 100 mg/ml of ampicillin and 50 mg/ml of kanamycin and grown overnight in a shaking incubator at 37 °C, aerobically. The culture was diluted 1:100 into a fresh culture tube with 10 ml of LB medium supplemented with 100 mg/ml of ampicillin and 50 mg/ml of kanamycin and induced with 0.1 µM of IPTG for 2 h in a 37 °C shaking incubator. Eighty microliters of refreshed and induced cells were aliquoted into each well of a 96-well, black and flat clear-bottomed plate. A light pattern was created by selectively blocking out the bottoms of wells with aluminum foil, and the plate was exposed to light from the bottom with a blue LED light pane (HQRP). The plate was incubated atop the device overnight at room temperature to ensure sufficient exposure to light. However, this duration can be shortened to 30 min if necessary as demonstrated in Supplementary&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41467-023-38876-w#MOESM1">S4</a>.</p><h3 id="Sec12">Barcoding of DNA and library prep for illumina sequencing</h3><p>Ninety-six different PCR reactions were set up. Each reaction well contains 0.5 µl of a specific known well-code at a concentration of 10 µM that is mapped to the <i>X</i>–<i>Y</i> coordinates of that well. The unique barcoding sequences were generated computationally using the ‘DNABarcodes’ R package in Bioconductor (version 1.28.0)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Buschmann, T. &amp; Bystrykh, L. V. Levenshtein error-correcting barcodes for multiplexed DNA sequencing. BMC Bioinforma. 14, 272 (2013)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR42" id="ref-link-section-d8717801e1476">42</a></sup>. One microliter of cells from each well of the 96-well plate was then used for the PCR reactions. PCRs were performed using Taq DNA polymerase (NEB) with each reaction containing 2 µl of 10X ThermoPol buffer (NEB), 0.1 µl of 100 µM of each forward and reverse primer pair (IDT), 0.2 µl of Taq DNA polymerase (NEB), 0.2 µl of 20 mM dNTP mix (BioBasic), 1 µl of cells, and topped up with ddH<sub>2</sub>O for a total reaction volume of 20 µl. Individual reactions were run on 1.2% agarose gels for product verification. Appending of adapter ends for sequencing was also done using PCR, with 0.2 µl of each barcoded mix from the previous reaction added into a reaction containing 0.2 µl Taq DNA polymerase (NEB), 2 µl of 10X ThermoPol buffer (NEB), 0.1 µl each of 100 µM inner sequencing index primers (IDT), 0.3 µl of 100 µM outer sequencing index primers (IDT), 0.2 µl of 20 mM dNTP mix (BioBasic), and topped up with ddH<sub>2</sub>O to a total volume of 20 µl for each reaction. Products were subsequently verified on agarose gel before aliquoting 10 µl from each reaction and pooled together into a single tube. Two hundred microliters of this mixed pool was sent for NovaSeq dual index paired-end sequencing with an external sequencing service provider (NovogeneAIT).</p><h3 id="Sec13">Deconvolution of raw reads into images</h3><p>The resulting raw data generated was deconvoluted with appropriate index sequences and separated accordingly by the external sequencing provider. Subsequent analysis of the data was done using an in-house R script (code provided in the Supplementary&nbsp;Software file) (R version 4.2.2<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="R Core Team. R: the R project for statistical computing. 
                  https://www.r-project.org/
                  
                 (2022)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR43" id="ref-link-section-d8717801e1492">43</a></sup>, Bioconductor version 3.16<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Huber, W. et al. Orchestrating high-throughput genomic analysis with Bioconductor. Nat. Methods 12, 115–121 (2015)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR44" id="ref-link-section-d8717801e1496">44</a></sup>, Biostrings version 2.66.0<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Pagès, H., Aboyoun, P., Gentleman, R. &amp; DebRoy, S. Biostrings: efficient manipulation of biological strings. 
                  https://doi.org/10.18129/B9.bioc.Biostrings
                  
                 (2023)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR45" id="ref-link-section-d8717801e1500">45</a></sup>, ShortRead version 1.56.1<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Morgan, M. et al. ShortRead: a bioconductor package for input, quality assessment and exploration of high-throughput sequence data. Bioinformatics 25, 2607–2608 (2009)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR46" id="ref-link-section-d8717801e1504">46</a></sup> and stringr version 1.5.0<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Wickham, H. stringr: Simple, Consistent Wrappers for Common String Operations. 
                  https://stringr.tidyverse.org
                  
                , 
                  https://github.com/tidyverse/stringr
                  
                 (2022)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR47" id="ref-link-section-d8717801e1508">47</a></sup> packages). Briefly, the script derived the total number of reads, the number of reads with excised intervening LoxP sequences, and number of reads with full LoxP sequences, that were linked to each unique well-code. To determine whether that unique barcoded population was exposed to light or not, the ratio of truncated to full reads was determined. This ratio is calculated for each well in the entire plate, and serves to define the bit state of the well. A higher ratio indicates a larger proportion of truncated reads, which implies that a large number of the population of cells that were barcoded was exposed to light and vice versa. To reconstruct the images, each the aforementioned ratio was converted to either 1 or 0, with the threshold between both determined with GMM clustering to define the two clusters and the threshold set via the boundary between the two clusters with the lowest variance score. This is then linked back to the well-code, which was then mapped to its predefined spatial location, thus creating an image with a pixel depth of 1 bit and a resolution of 96 pixels. Accuracy of the final image was defined as the number of wells that were appropriately demarcated as 1 or 0 as compared to the original projected pattern. Differentiating between two colors of light was done by calculating ratios of sequences specific to the blue light recording plasmid as well as the red light recording plasmid separately from one another.</p><h3 id="Sec14">Random access of images</h3><p>Primers specifically designed for random access with index sequences targeted at selected indexes were synthesized and used to enable random access. Eight PCR reactions with the same conditions were set up to generate sufficient DNA for sequencing. 1&nbsp;µl of template from the mixed pool was used for each reaction totaling 50 µl, and a touchdown PCR with decreasing annealing temperatures from 65 to 55 °C for eight cycles and constant annealing temperatures for 20 cycles was conducted, using specific random access primers that bind to desired image to be amplified. Resulting samples were appended with Illumina adapter sequences and sent for sequencing. Diluting experiments were conducted by taking 1 µl of template from&nbsp;the mixed pool and diluting in 9, 99, and 999 µl of PBS before taking 1 µl from each dilution for PCR as per the above protocol. N701 and N501 indexing primers selective for the ‘NUS’ pattern were used for this particular random access dilution.</p><h3 id="Sec15">Environmental challenge experiments</h3><p>A pool of DNA containing the ‘Smiley’ pattern was used for drying and resuspension experiments. Twenty microliters of the pool was spun dry using a centrifuge, and subsequently resuspended in 50 µl ddH<sub>2</sub>O before being submitted for sequencing.</p><p>For environmental challenge experiments, 200 µl of the ‘Heloo wo{|d!’ pool was kept in varying conditions in a cryotube, wrapped in aluminum foil. Conditions included frozen in −20 °C, at room temperature, and in a 60 °C oven, all for 7 days. One last sample was kept in a 1.5 ml Eppendorf tube which was then exposed to UV light for 1 hour. Samples were then sent for sequencing.</p><h3 id="Sec16">Dilution experiments</h3><p>Two forms of dilution experiments were conducted. One involved diluting a pool of DNA containing the ‘Smiley’ pattern 3 consecutive times, at a 10 times dilution each, with each dilution being sent for sequencing. The other involved diluting a mixed pool of DNA with multiple samples 3 consecutive times, with each dilution being a 10 times dilution&nbsp;as described above. Random access was then conducted on each dilution as detailed above before sending the amplified products for sequencing.</p><h3 id="Sec17">Clustering of deconvoluted images</h3><p>The clustering workflow was developed and implemented in Python 3<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="The Python Language Reference. Python documentation. 
                  https://docs.python.org/3/reference/index.html
                  
                ." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR48" id="ref-link-section-d8717801e1550">48</a></sup> (version 3.11.1) (code provided in the Supplementary Software file). Four unsupervised clustering techniques (K-means algorithm<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Arthur, D. &amp; Vassilvitskii, S. k-means++: the advantages of careful seeding. In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms 1027–1035 (Society for Industrial and Applied Mathematics, 2007)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR49" id="ref-link-section-d8717801e1554">49</a></sup>, DBSCAN<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Schubert, E., Sander, J., Ester, M., Kriegel, H. P. &amp; Xu, X. DBSCAN revisited, revisited: why and how you should (Still) use DBSCAN. ACM Trans. Database Syst. 42, 1–21 (2017)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR50" id="ref-link-section-d8717801e1558">50</a></sup>, OPTICS<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Ankerst, M., Breunig, M. M., Kriegel, H.-P. &amp; Sander, J. OPTICS: ordering points to identify the clustering structure. ACM SIGMOD Rec. 28, 49–60 (1999)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR51" id="ref-link-section-d8717801e1562">51</a></sup>, and Gaussian Mixture Model<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Yang, M.-S., Lai, C.-Y. &amp; Lin, C.-Y. A robust EM clustering algorithm for Gaussian mixture models. Pattern Recognit. 45, 3950–3961 (2012)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR52" id="ref-link-section-d8717801e1566">52</a></sup>) were adopted from scikit-learn (version 1.2.0) machine learning packages under cluster and mixture modules<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Pedregosa, F. et al. Scikit-learn: Machine Learning in Python. J. Mach. Learn. Res. 12, 2825–2830 (2011)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR53" id="ref-link-section-d8717801e1571">53</a></sup>. The unsupervised outlier detection algorithm was implemented using Local Outlier Factor (LOF) from scikit-learn neighbors module. The parameter ‘n_neighbors’ (blue light = 20; red light = 10) was tuned accordingly to improve the deconvolution accuracy through better reassignment of outliers near to the cutoff threshold for better clustering performance (Supplementary&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41467-023-38876-w#MOESM1">S6</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41467-023-38876-w#MOESM1">S7</a>). The cutoff threshold for the full ‘ON’ and ‘OFF’ was determined from the means computed from clusters from all existing datasets. The dataset would be classified as full ‘ON’ or ‘OFF’ when the lowest mean of clusters is above the cutoff threshold, or the highest mean is below the cutoff threshold. To generate the final binary ‘0’ or ‘1’ images, cluster grouping was executed for clustering techniques that identified more than 2 clusters to group them into two clusters designated as ‘ON’ and ‘OFF’ states. For the blue light patterns, the second and higher mean clusters were grouped into a single ‘ON’ cluster whereas the initial cluster with the lowest mean would be considered as ‘OFF’ states. Meanwhile, for the red light patterns, the two clusters with the lower mean were considered as a single ‘OFF’ cluster instead whereas the rest of the clusters would be considered as ‘ON’ states due to differences in the distribution of the raw data.</p><h3 id="Sec18">Development of the red light bacterial recombinase system</h3><p>Two plasmids were designed for the red light bacterial recombinase system. pBbS5a-RLCre, a red light sensitive split Cre-recombinase based off the L-SCRaMbLE system previously developed in yeast<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Hochrein, L., Mitchell, L. A., Schulz, K., Messerschmidt, K. &amp; Mueller-Roeber, B. L-SCRaMbLE as a tool for light-controlled Cre-mediated recombination in yeast. Nat. Commun. 9, 1931 (2018)." href="https://www.nature.com/articles/s41467-023-38876-w#ref-CR35" id="ref-link-section-d8717801e1589">35</a></sup> was designed, codon-optimized for <i>E. coli</i> and synthesized (Twist Bioscience). pBbAW4k-Spacer1Barcoding-loxP-TT-loxP-ho1-pcyA, a recording plasmid, was also designed, consisting of LoxP recognition sites, a unique preceding barcode (Spacer1Barcoding), as well a ho1-pcyA component that produces phycocyanobilin necessary for the function of the red light split recombinase. Both plasmids were transformed simultaneously into XL1 Blue <i>E. coli</i> chemically competent cells.</p><h3 id="Sec19">Design and construction of a custom light illumination device (OptoBox)</h3><p>To test the co-culture of red and blue light-responsive <i>E. coli</i> under various blue and red light patterns, a 96-well programmable LED device was developed (design illustrated in Supplementary&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41467-023-38876-w#MOESM1">S8</a>). This device is powered and controlled by an Arduino Uno Microcontroller and includes 8 strips of 12 Adafruit Neopixel SMD 5050 RGB LEDs totalling 96 individual LEDs. Each of these LEDs corresponds to each well of the 96-well plate. The overall construction of the device comprises a 96-tip pipette tip container. These pipette tip containers comprise two separable layers stacked on top of one another. This setup allows the LED strips to be immobilized between the layers. The wells of the pipette tip container also prevent residual light spillover from the LEDs, resulting in spatial isolation when coupled with a clear-bottomed black 96-well plate.</p><p>For our application, these hollow tip-holes are used to fit and secure the SMD 5050 LEDs in place by fitting the LED chips into the underside of each of these corresponding holes. This allows each individual LED to be held in place through a friction fit, with the diodes that make up the SMD 5050 LED centered to the wells of the 96-well plate. The LED strips were soldered to wires supplying power and electrical control. Each strip’s data-wire was connected to an individual Arduino Uno data pin, allowing individual control of each LED. Programming of the LEDs was done using the available FastLED Arduino library.</p><h3 id="Sec20">Capture of multicolor images with a co-culture</h3><p>Strains containing the red light and blue light recombinase systems were inoculated separately into culture tubes with 5 ml of LB medium supplemented with 100 mg/ml of ampicillin and 50 mg/ml of kanamycin and grown overnight in a shaking incubator at 37 °C, aerobically. These cultures were diluted 1:100 into fresh culture tubes with 10 ml of LB medium supplemented with 100 mg/ml of ampicillin and 50 mg/ml of kanamycin and induced with 0.1 µM of IPTG for 2 h in a 37 °C shaking incubator. 40&nbsp;µl of each culture were aliquoted into each well of a 96-well, black and flat clear-bottomed plate. A red (620 nm) and a blue (465 nm) light pattern was created by the in-house custom-built light illumination device (OptoBox), and either projected periodically for 10 min in an alternating fashion overnight, or projected simultaneously overnight, both from the bottom of the plate and at equal intensities. Barcoding, sequencing and deconvolution follows the same procedure as detailed in the preceding sections.</p><h3 id="Sec21">Statistics and reproducibility</h3><p>No statistical method was used to predetermine sample size. No data were excluded from the analyses. The experiments were not randomized. The Investigators were not blinded to allocation during experiments and outcome assessment.</p><h3 id="Sec22">Reporting summary</h3><p>Further information on research design is available in the&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41467-023-38876-w#MOESM2">Nature Portfolio Reporting Summary</a> linked to this article.</p></div></div>
                </div><div>
            <div id="data-availability-section" data-title="Data availability"><h2 id="data-availability">Data availability</h2><p>The sequencing data generated in this study have been deposited in a Figshare repository at: <a href="https://doi.org/10.6084/m9.figshare.22678534">https://doi.org/10.6084/m9.figshare.22678534</a>. The same data have also been deposited in the NCBI Sequence Read Archive under the PRJNA970212 BioProject at <a href="https://www.ncbi.nlm.nih.gov/bioproject/PRJNA970212">https://www.ncbi.nlm.nih.gov/bioproject/PRJNA970212</a>, with the individual accession numbers available in the supplementary file under Supplementary Table&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41467-023-38876-w#MOESM1">2</a>. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41467-023-38876-w#Sec24">Source data</a> are provided with this paper. Barcodes and primers used in this work are detailed in the Source Data file.</p></div><div id="code-availability-section" data-title="Code availability"><h2 id="code-availability">Code availability</h2><p>The code used for deconvolution of reads as well as clustering is available in the&nbsp;<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41467-023-38876-w#MOESM5">Supplementary Software</a> file.</p></div><div id="MagazineFulltextArticleBodySuffix" aria-labelledby="Bib1" data-title="References"><h2 id="Bib1">References</h2><div data-container-section="references" id="Bib1-content"><ol data-track-component="outbound reference"><li data-counter="1."><p id="ref-CR1">Church, G. M., Gao, Y. &amp; Kosuri, S. Next-generation digital information storage in DNA. <i>Science</i> <b>337</b>, 1628–1628 (2012).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/science.1226355" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.1226355" aria-label="Article reference 1" data-doi="10.1126/science.1226355">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2012Sci...337.1628C" aria-label="ADS reference 1">ADS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC38Xhtl2ntrrL" aria-label="CAS reference 1">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22903519" aria-label="PubMed reference 1">PubMed</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=Next-generation%20digital%20information%20storage%20in%20DNA&amp;journal=Science&amp;doi=10.1126%2Fscience.1226355&amp;volume=337&amp;pages=1628-1628&amp;publication_year=2012&amp;author=Church%2CGM&amp;author=Gao%2CY&amp;author=Kosuri%2CS">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="2."><p id="ref-CR2">Goldman, N. et al. Towards practical, high-capacity, low-maintenance information storage in synthesized DNA. <i>Nature</i> <b>494</b>, 77–80 (2013).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nature11875" data-track-action="article reference" href="https://doi.org/10.1038%2Fnature11875" aria-label="Article reference 2" data-doi="10.1038/nature11875">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2013Natur.494...77G" aria-label="ADS reference 2">ADS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC3sXhsVygsLg%3D" aria-label="CAS reference 2">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23354052" aria-label="PubMed reference 2">PubMed</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3672958" aria-label="PubMed Central reference 2">PubMed Central</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=Towards%20practical%2C%20high-capacity%2C%20low-maintenance%20information%20storage%20in%20synthesized%20DNA&amp;journal=Nature&amp;doi=10.1038%2Fnature11875&amp;volume=494&amp;pages=77-80&amp;publication_year=2013&amp;author=Goldman%2CN">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="3."><p id="ref-CR3">Zhirnov, V., Zadegan, R. M., Sandhu, G. S., Church, G. M. &amp; Hughes, W. L. Nucleic acid memory. <i>Nat. Mater.</i> <b>15</b>, 366–370 (2016).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nmat4594" data-track-action="article reference" href="https://doi.org/10.1038%2Fnmat4594" aria-label="Article reference 3" data-doi="10.1038/nmat4594">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2016NatMa..15..366Z" aria-label="ADS reference 3">ADS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC28XkvVCitb0%3D" aria-label="CAS reference 3">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=27005909" aria-label="PubMed reference 3">PubMed</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6361517" aria-label="PubMed Central reference 3">PubMed Central</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=Nucleic%20acid%20memory&amp;journal=Nat.%20Mater.&amp;doi=10.1038%2Fnmat4594&amp;volume=15&amp;pages=366-370&amp;publication_year=2016&amp;author=Zhirnov%2CV&amp;author=Zadegan%2CRM&amp;author=Sandhu%2CGS&amp;author=Church%2CGM&amp;author=Hughes%2CWL">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="4."><p id="ref-CR4">van der Valk, T. et al. Million-year-old DNA sheds light on the genomic history of mammoths. <i>Nature</i>, 1–5. <a href="https://doi.org/10.1038/s41586-021-03224-9">https://doi.org/10.1038/s41586-021-03224-9</a> (2021).</p></li><li data-counter="5."><p id="ref-CR5">Lim, C. K., Nirantar, S., Yew, W. S. &amp; Poh, C. L. Novel Modalities in DNA Data Storage. <i>Trends Biotechnol</i>. <b>39</b>, 990–1003 (2021).</p></li><li data-counter="6."><p id="ref-CR6">Tabatabaei, S. K. et al. DNA punch cards for storing data on native DNA sequences via enzymatic nicking. <i>Nat. Commun.</i> <b>11</b>, 1–10 (2020).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/s41467-020-15588-z" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41467-020-15588-z" aria-label="Article reference 6" data-doi="10.1038/s41467-020-15588-z">Article</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=DNA%20punch%20cards%20for%20storing%20data%20on%20native%20DNA%20sequences%20via%20enzymatic%20nicking&amp;journal=Nat.%20Commun.&amp;doi=10.1038%2Fs41467-020-15588-z&amp;volume=11&amp;pages=1-10&amp;publication_year=2020&amp;author=Tabatabaei%2CSK">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="7."><p id="ref-CR7">Chen, K., Zhu, J., Bošković, F. &amp; Keyser, U. F. Nanopore-based DNA hard drives for rewritable and secure data storage. <i>Nano Lett</i>. <a href="https://doi.org/10.1021/acs.nanolett.0c00755">https://doi.org/10.1021/acs.nanolett.0c00755</a> (2020).</p></li><li data-counter="8."><p id="ref-CR8">Tomek, K. J. et al. Driving the scalability of DNA-based information storage systems. <i>ACS Synth. Biol.</i> <b>8</b>, 1241–1248 (2019).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1021/acssynbio.9b00100" data-track-action="article reference" href="https://doi.org/10.1021%2Facssynbio.9b00100" aria-label="Article reference 8" data-doi="10.1021/acssynbio.9b00100">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1MXpvVCmur4%3D" aria-label="CAS reference 8">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31117362" aria-label="PubMed reference 8">PubMed</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=Driving%20the%20scalability%20of%20DNA-based%20information%20storage%20systems&amp;journal=ACS%20Synth.%20Biol.&amp;doi=10.1021%2Facssynbio.9b00100&amp;volume=8&amp;pages=1241-1248&amp;publication_year=2019&amp;author=Tomek%2CKJ">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="9."><p id="ref-CR9">Yim, S. S. et al. Robust direct digital-to-biological data storage in living cells. <i>Nat. Chem. Biol.</i> <b>17</b>, 1–8 (2021).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/s41589-020-00711-4" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41589-020-00711-4" aria-label="Article reference 9" data-doi="10.1038/s41589-020-00711-4">Article</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=Robust%20direct%20digital-to-biological%20data%20storage%20in%20living%20cells&amp;journal=Nat.%20Chem.%20Biol.&amp;doi=10.1038%2Fs41589-020-00711-4&amp;volume=17&amp;pages=1-8&amp;publication_year=2021&amp;author=Yim%2CSS">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="10."><p id="ref-CR10">Ceze, L., Nivala, J. &amp; Strauss, K. Molecular digital data storage using DNA. <i>Nat. Rev. Genet.</i> <b>20</b>, 456–466 (2019).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/s41576-019-0125-3" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41576-019-0125-3" aria-label="Article reference 10" data-doi="10.1038/s41576-019-0125-3">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1MXptFeqsrs%3D" aria-label="CAS reference 10">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31068682" aria-label="PubMed reference 10">PubMed</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Molecular%20digital%20data%20storage%20using%20DNA&amp;journal=Nat.%20Rev.%20Genet.&amp;doi=10.1038%2Fs41576-019-0125-3&amp;volume=20&amp;pages=456-466&amp;publication_year=2019&amp;author=Ceze%2CL&amp;author=Nivala%2CJ&amp;author=Strauss%2CK">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="11."><p id="ref-CR11">Lee, H. H., Kalhor, R., Goela, N., Bolot, J. &amp; Church, G. M. Terminator-free template-independent enzymatic DNA synthesis for digital information storage. <i>Nat. Commun.</i> <b>10</b>, 1–12 (2019).</p><p><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=Terminator-free%20template-independent%20enzymatic%20DNA%20synthesis%20for%20digital%20information%20storage&amp;journal=Nat.%20Commun.&amp;volume=10&amp;pages=1-12&amp;publication_year=2019&amp;author=Lee%2CHH&amp;author=Kalhor%2CR&amp;author=Goela%2CN&amp;author=Bolot%2CJ&amp;author=Church%2CGM">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="12."><p id="ref-CR12">Takahashi, C. N., Nguyen, B. H., Strauss, K. &amp; Ceze, L. Demonstration of end-to-end automation of DNA Data Storage. <i>Sci. Rep.</i> <b>9</b>, 1–5 (2019).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/s41598-019-41228-8" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41598-019-41228-8" aria-label="Article reference 12" data-doi="10.1038/s41598-019-41228-8">Article</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=Demonstration%20of%20end-to-end%20automation%20of%20DNA%20Data%20Storage&amp;journal=Sci.%20Rep.&amp;doi=10.1038%2Fs41598-019-41228-8&amp;volume=9&amp;pages=1-5&amp;publication_year=2019&amp;author=Takahashi%2CCN&amp;author=Nguyen%2CBH&amp;author=Strauss%2CK&amp;author=Ceze%2CL">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="13."><p id="ref-CR13">Palluk, S. et al. De novo DNA synthesis using polymerase-nucleotide conjugates. <i>Nat. Biotechnol.</i> <b>36</b>, 645–650 (2018).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nbt.4173" data-track-action="article reference" href="https://doi.org/10.1038%2Fnbt.4173" aria-label="Article reference 13" data-doi="10.1038/nbt.4173">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1cXhtFGjs7%2FI" aria-label="CAS reference 13">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29912208" aria-label="PubMed reference 13">PubMed</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=De%20novo%20DNA%20synthesis%20using%20polymerase-nucleotide%20conjugates&amp;journal=Nat.%20Biotechnol.&amp;doi=10.1038%2Fnbt.4173&amp;volume=36&amp;pages=645-650&amp;publication_year=2018&amp;author=Palluk%2CS">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="14."><p id="ref-CR14">Lu, X. et al. Enzymatic DNA synthesis by engineering terminal deoxynucleotidyl transferase. <i>ACS Catal</i>. 2988–2997. <a href="https://doi.org/10.1021/acscatal.1c04879">https://doi.org/10.1021/acscatal.1c04879</a> (2022).</p></li><li data-counter="15."><p id="ref-CR15">Xu, C. et al. Electrochemical DNA synthesis and sequencing on a single electrode with scalability for integrated data storage. <i>Sci. Adv.</i> <b>7</b>, eabk0100 (2021).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/sciadv.abk0100" data-track-action="article reference" href="https://doi.org/10.1126%2Fsciadv.abk0100" aria-label="Article reference 15" data-doi="10.1126/sciadv.abk0100">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2021SciA....7..100X" aria-label="ADS reference 15">ADS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXis1Ghsb3E" aria-label="CAS reference 15">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34767438" aria-label="PubMed reference 15">PubMed</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8589306" aria-label="PubMed Central reference 15">PubMed Central</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=Electrochemical%20DNA%20synthesis%20and%20sequencing%20on%20a%20single%20electrode%20with%20scalability%20for%20integrated%20data%20storage&amp;journal=Sci.%20Adv.&amp;doi=10.1126%2Fsciadv.abk0100&amp;volume=7&amp;publication_year=2021&amp;author=Xu%2CC">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="16."><p id="ref-CR16">Nguyen, B. H. et al. Scaling DNA data storage with nanoscale electrode wells. <i>Sci. Adv.</i> <b>7</b>, eabi6714 (2021).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/sciadv.abi6714" data-track-action="article reference" href="https://doi.org/10.1126%2Fsciadv.abi6714" aria-label="Article reference 16" data-doi="10.1126/sciadv.abi6714">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2021SciA....7.6714N" aria-label="ADS reference 16">ADS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXislCntLrK" aria-label="CAS reference 16">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34818035" aria-label="PubMed reference 16">PubMed</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8612674" aria-label="PubMed Central reference 16">PubMed Central</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 16" href="http://scholar.google.com/scholar_lookup?&amp;title=Scaling%20DNA%20data%20storage%20with%20nanoscale%20electrode%20wells&amp;journal=Sci.%20Adv.&amp;doi=10.1126%2Fsciadv.abi6714&amp;volume=7&amp;publication_year=2021&amp;author=Nguyen%2CBH">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="17."><p id="ref-CR17">Antkowiak, P. L. et al. Low cost DNA data storage using photolithographic synthesis and advanced information reconstruction and error correction. <i>Nat. Commun.</i> <b>11</b>, 5345 (2020).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/s41467-020-19148-3" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41467-020-19148-3" aria-label="Article reference 17" data-doi="10.1038/s41467-020-19148-3">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2020NatCo..11.5345A" aria-label="ADS reference 17">ADS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3cXitF2js7rI" aria-label="CAS reference 17">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=33093494" aria-label="PubMed reference 17">PubMed</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7582880" aria-label="PubMed Central reference 17">PubMed Central</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 17" href="http://scholar.google.com/scholar_lookup?&amp;title=Low%20cost%20DNA%20data%20storage%20using%20photolithographic%20synthesis%20and%20advanced%20information%20reconstruction%20and%20error%20correction&amp;journal=Nat.%20Commun.&amp;doi=10.1038%2Fs41467-020-19148-3&amp;volume=11&amp;publication_year=2020&amp;author=Antkowiak%2CPL">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="18."><p id="ref-CR18">Erlich, Y. &amp; Zielinski, D. DNA Fountain enables a robust and efficient storage architecture. <i>Science</i> <b>355</b>, 950–954 (2017).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/science.aaj2038" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.aaj2038" aria-label="Article reference 18" data-doi="10.1126/science.aaj2038">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2017Sci...355..950E" aria-label="ADS reference 18">ADS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2sXjsVCgsrc%3D" aria-label="CAS reference 18">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28254941" aria-label="PubMed reference 18">PubMed</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=DNA%20Fountain%20enables%20a%20robust%20and%20efficient%20storage%20architecture&amp;journal=Science&amp;doi=10.1126%2Fscience.aaj2038&amp;volume=355&amp;pages=950-954&amp;publication_year=2017&amp;author=Erlich%2CY&amp;author=Zielinski%2CD">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="19."><p id="ref-CR19">Wang, Y. et al. High capacity DNA data storage with variable-length Oligonucleotides using repeat accumulate code and hybrid mapping. <i>J. Biol. Eng.</i> <b>13</b>, 89 (2019).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1186/s13036-019-0211-2" data-track-action="article reference" href="https://doi.org/10.1186%2Fs13036-019-0211-2" aria-label="Article reference 19" data-doi="10.1186/s13036-019-0211-2">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31832092" aria-label="PubMed reference 19">PubMed</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6868767" aria-label="PubMed Central reference 19">PubMed Central</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 19" href="http://scholar.google.com/scholar_lookup?&amp;title=High%20capacity%20DNA%20data%20storage%20with%20variable-length%20Oligonucleotides%20using%20repeat%20accumulate%20code%20and%20hybrid%20mapping&amp;journal=J.%20Biol.%20Eng.&amp;doi=10.1186%2Fs13036-019-0211-2&amp;volume=13&amp;publication_year=2019&amp;author=Wang%2CY">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="20."><p id="ref-CR20">Anavy, L., Vaknin, I., Atar, O., Amit, R. &amp; Yakhini, Z. Data storage in DNA with fewer synthesis cycles using composite DNA letters. <i>Nat. Biotechnol.</i> <b>37</b>, 1229–1236 (2019).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/s41587-019-0240-x" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41587-019-0240-x" aria-label="Article reference 20" data-doi="10.1038/s41587-019-0240-x">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1MXhsleltLvL" aria-label="CAS reference 20">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31501560" aria-label="PubMed reference 20">PubMed</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 20" href="http://scholar.google.com/scholar_lookup?&amp;title=Data%20storage%20in%20DNA%20with%20fewer%20synthesis%20cycles%20using%20composite%20DNA%20letters&amp;journal=Nat.%20Biotechnol.&amp;doi=10.1038%2Fs41587-019-0240-x&amp;volume=37&amp;pages=1229-1236&amp;publication_year=2019&amp;author=Anavy%2CL&amp;author=Vaknin%2CI&amp;author=Atar%2CO&amp;author=Amit%2CR&amp;author=Yakhini%2CZ">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="21."><p id="ref-CR21">Sheets, M. B., Wong, W. W. &amp; Dunlop, M. J. Light-inducible recombinases for bacterial optogenetics. <i>ACS Synth. Biol.</i> <b>9</b>, 227–235 (2020).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1021/acssynbio.9b00395" data-track-action="article reference" href="https://doi.org/10.1021%2Facssynbio.9b00395" aria-label="Article reference 21" data-doi="10.1021/acssynbio.9b00395">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3cXht1Wiu74%3D" aria-label="CAS reference 21">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31961670" aria-label="PubMed reference 21">PubMed</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7393974" aria-label="PubMed Central reference 21">PubMed Central</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=Light-inducible%20recombinases%20for%20bacterial%20optogenetics&amp;journal=ACS%20Synth.%20Biol.&amp;doi=10.1021%2Facssynbio.9b00395&amp;volume=9&amp;pages=227-235&amp;publication_year=2020&amp;author=Sheets%2CMB&amp;author=Wong%2CWW&amp;author=Dunlop%2CMJ">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="22."><p id="ref-CR22">Yang, L. et al. Permanent genetic memory with &gt;1-byte capacity. <i>Nat. Methods</i> <b>11</b>, 1261–1266 (2014).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nmeth.3147" data-track-action="article reference" href="https://doi.org/10.1038%2Fnmeth.3147" aria-label="Article reference 22" data-doi="10.1038/nmeth.3147">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2cXhvVSqsbnE" aria-label="CAS reference 22">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25344638" aria-label="PubMed reference 22">PubMed</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4245323" aria-label="PubMed Central reference 22">PubMed Central</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 22" href="http://scholar.google.com/scholar_lookup?&amp;title=Permanent%20genetic%20memory%20with%20%3E1-byte%20capacity&amp;journal=Nat.%20Methods&amp;doi=10.1038%2Fnmeth.3147&amp;volume=11&amp;pages=1261-1266&amp;publication_year=2014&amp;author=Yang%2CL">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="23."><p id="ref-CR23">Bonnet, J., Subsoontorn, P. &amp; Endy, D. Rewritable digital data storage in live cells via engineered control of recombination directionality. <i>Proc. Natl Acad. Sci. USA</i> <b>109</b>, 8884–8889 (2012).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.1202344109" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.1202344109" aria-label="Article reference 23" data-doi="10.1073/pnas.1202344109">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2012PNAS..109.8884B" aria-label="ADS reference 23">ADS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC38XovF2gt70%3D" aria-label="CAS reference 23">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22615351" aria-label="PubMed reference 23">PubMed</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3384180" aria-label="PubMed Central reference 23">PubMed Central</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 23" href="http://scholar.google.com/scholar_lookup?&amp;title=Rewritable%20digital%20data%20storage%20in%20live%20cells%20via%20engineered%20control%20of%20recombination%20directionality&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.1202344109&amp;volume=109&amp;pages=8884-8889&amp;publication_year=2012&amp;author=Bonnet%2CJ&amp;author=Subsoontorn%2CP&amp;author=Endy%2CD">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="24."><p id="ref-CR24">Shipman, S. L., Nivala, J., Macklis, J. D. &amp; Church, G. M. Molecular recordings by directed CRISPR spacer acquisition. <i>Science</i> <b>353</b>, aaf1175 (2016).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/science.aaf1175" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.aaf1175" aria-label="Article reference 24" data-doi="10.1126/science.aaf1175">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=27284167" aria-label="PubMed reference 24">PubMed</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4994893" aria-label="PubMed Central reference 24">PubMed Central</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=Molecular%20recordings%20by%20directed%20CRISPR%20spacer%20acquisition&amp;journal=Science&amp;doi=10.1126%2Fscience.aaf1175&amp;volume=353&amp;publication_year=2016&amp;author=Shipman%2CSL&amp;author=Nivala%2CJ&amp;author=Macklis%2CJD&amp;author=Church%2CGM">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="25."><p id="ref-CR25">Shipman, S. L., Nivala, J., Macklis, J. D. &amp; Church, G. M. CRISPR–Cas encoding of a digital movie into the genomes of a population of living bacteria. <i>Nature</i> <b>547</b>, 345–349 (2017).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nature23017" data-track-action="article reference" href="https://doi.org/10.1038%2Fnature23017" aria-label="Article reference 25" data-doi="10.1038/nature23017">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2017Natur.547..345S" aria-label="ADS reference 25">ADS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2sXhtFOjtrfN" aria-label="CAS reference 25">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28700573" aria-label="PubMed reference 25">PubMed</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5842791" aria-label="PubMed Central reference 25">PubMed Central</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 25" href="http://scholar.google.com/scholar_lookup?&amp;title=CRISPR%E2%80%93Cas%20encoding%20of%20a%20digital%20movie%20into%20the%20genomes%20of%20a%20population%20of%20living%20bacteria&amp;journal=Nature&amp;doi=10.1038%2Fnature23017&amp;volume=547&amp;pages=345-349&amp;publication_year=2017&amp;author=Shipman%2CSL&amp;author=Nivala%2CJ&amp;author=Macklis%2CJD&amp;author=Church%2CGM">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="26."><p id="ref-CR26">Tang, W. &amp; Liu, D. R. Rewritable multi-event analog recording in bacterial and mammalian cells. <i>Science</i> <b>360</b>, eaap8992 (2018).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/science.aap8992" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.aap8992" aria-label="Article reference 26" data-doi="10.1126/science.aap8992">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29449507" aria-label="PubMed reference 26">PubMed</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5898985" aria-label="PubMed Central reference 26">PubMed Central</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 26" href="http://scholar.google.com/scholar_lookup?&amp;title=Rewritable%20multi-event%20analog%20recording%20in%20bacterial%20and%20mammalian%20cells&amp;journal=Science&amp;doi=10.1126%2Fscience.aap8992&amp;volume=360&amp;publication_year=2018&amp;author=Tang%2CW&amp;author=Liu%2CDR">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="27."><p id="ref-CR27">Chen, K. et al. Digital data storage using DNA nanostructures and solid-state nanopores. <i>Nano Lett.</i> <b>19</b>, 1210–1215 (2019).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1021/acs.nanolett.8b04715" data-track-action="article reference" href="https://doi.org/10.1021%2Facs.nanolett.8b04715" aria-label="Article reference 27" data-doi="10.1021/acs.nanolett.8b04715">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2019NanoL..19.1210C" aria-label="ADS reference 27">ADS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1cXisFyksrfO" aria-label="CAS reference 27">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30585490" aria-label="PubMed reference 27">PubMed</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=Digital%20data%20storage%20using%20DNA%20nanostructures%20and%20solid-state%20nanopores&amp;journal=Nano%20Lett.&amp;doi=10.1021%2Facs.nanolett.8b04715&amp;volume=19&amp;pages=1210-1215&amp;publication_year=2019&amp;author=Chen%2CK">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="28."><p id="ref-CR28">Fernandez-Rodriguez, J., Moser, F., Song, M. &amp; Voigt, C. A. Engineering RGB color vision into <i>Escherichia coli</i>. <i>Nat. Chem. Biol.</i> <b>13</b>, 706–708 (2017).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nchembio.2390" data-track-action="article reference" href="https://doi.org/10.1038%2Fnchembio.2390" aria-label="Article reference 28" data-doi="10.1038/nchembio.2390">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2sXotVOltL0%3D" aria-label="CAS reference 28">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28530708" aria-label="PubMed reference 28">PubMed</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=Engineering%20RGB%20color%20vision%20into%20Escherichia%20coli&amp;journal=Nat.%20Chem.%20Biol.&amp;doi=10.1038%2Fnchembio.2390&amp;volume=13&amp;pages=706-708&amp;publication_year=2017&amp;author=Fernandez-Rodriguez%2CJ&amp;author=Moser%2CF&amp;author=Song%2CM&amp;author=Voigt%2CCA">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="29."><p id="ref-CR29">Jayaraman, P. et al. Blue light-mediated transcriptional activation and repression of gene expression in bacteria. <i>Nucleic Acids Res</i>. <b>44</b>, 6994–7005 (2016).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1093/nar/gkw548" data-track-action="article reference" href="https://doi.org/10.1093%2Fnar%2Fgkw548" aria-label="Article reference 29" data-doi="10.1093/nar/gkw548">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC28XhslWrsLzF" aria-label="CAS reference 29">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=27353329" aria-label="PubMed reference 29">PubMed</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5001607" aria-label="PubMed Central reference 29">PubMed Central</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=Blue%20light-mediated%20transcriptional%20activation%20and%20repression%20of%20gene%20expression%20in%20bacteria&amp;journal=Nucleic%20Acids%20Res&amp;doi=10.1093%2Fnar%2Fgkw548&amp;volume=44&amp;pages=6994-7005&amp;publication_year=2016&amp;author=Jayaraman%2CP">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="30."><p id="ref-CR30">Pouzet, S. et al. The promise of optogenetics for bioproduction: dynamic control strategies and scale-up instruments. <i>Bioengineering</i> <b>7</b>, 151 (2020).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.3390/bioengineering7040151" data-track-action="article reference" href="https://doi.org/10.3390%2Fbioengineering7040151" aria-label="Article reference 30" data-doi="10.3390/bioengineering7040151">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXmsVSjsro%3D" aria-label="CAS reference 30">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=33255280" aria-label="PubMed reference 30">PubMed</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7712799" aria-label="PubMed Central reference 30">PubMed Central</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 30" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20promise%20of%20optogenetics%20for%20bioproduction%3A%20dynamic%20control%20strategies%20and%20scale-up%20instruments&amp;journal=Bioengineering&amp;doi=10.3390%2Fbioengineering7040151&amp;volume=7&amp;publication_year=2020&amp;author=Pouzet%2CS">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="31."><p id="ref-CR31">Mansouri, M. &amp; Fussenegger, M. Synthetic biology-based optogenetic approaches to control therapeutic designer cells. <i>Curr. Opin. Syst. Biol.</i> <b>28</b>, 100396 (2021).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.coisb.2021.100396" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.coisb.2021.100396" aria-label="Article reference 31" data-doi="10.1016/j.coisb.2021.100396">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB38XhtlSmt7fN" aria-label="CAS reference 31">CAS</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 31" href="http://scholar.google.com/scholar_lookup?&amp;title=Synthetic%20biology-based%20optogenetic%20approaches%20to%20control%20therapeutic%20designer%20cells&amp;journal=Curr.%20Opin.%20Syst.%20Biol.&amp;doi=10.1016%2Fj.coisb.2021.100396&amp;volume=28&amp;publication_year=2021&amp;author=Mansouri%2CM&amp;author=Fussenegger%2CM">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="32."><p id="ref-CR32">Korbie, D. J. &amp; Mattick, J. S. Touchdown PCR for increased specificity and sensitivity in PCR amplification. <i>Nat. Protoc.</i> <b>3</b>, 1452–1456 (2008).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nprot.2008.133" data-track-action="article reference" href="https://doi.org/10.1038%2Fnprot.2008.133" aria-label="Article reference 32" data-doi="10.1038/nprot.2008.133">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD1cXhtVOnsL7F" aria-label="CAS reference 32">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18772872" aria-label="PubMed reference 32">PubMed</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=Touchdown%20PCR%20for%20increased%20specificity%20and%20sensitivity%20in%20PCR%20amplification&amp;journal=Nat.%20Protoc.&amp;doi=10.1038%2Fnprot.2008.133&amp;volume=3&amp;pages=1452-1456&amp;publication_year=2008&amp;author=Korbie%2CDJ&amp;author=Mattick%2CJS">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="33."><p id="ref-CR33">Grass, R. N., Heckel, R., Puddu, M., Paunescu, D. &amp; Stark, W. J. Robust chemical preservation of digital information on DNA in silica with error-correcting codes. <i>Angew. Chem. Int. Ed.</i> <b>54</b>, 2552–2555 (2015).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1002/anie.201411378" data-track-action="article reference" href="https://doi.org/10.1002%2Fanie.201411378" aria-label="Article reference 33" data-doi="10.1002/anie.201411378">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2MXis1Snsr8%3D" aria-label="CAS reference 33">CAS</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=Robust%20chemical%20preservation%20of%20digital%20information%20on%20DNA%20in%20silica%20with%20error-correcting%20codes&amp;journal=Angew.%20Chem.%20Int.%20Ed.&amp;doi=10.1002%2Fanie.201411378&amp;volume=54&amp;pages=2552-2555&amp;publication_year=2015&amp;author=Grass%2CRN&amp;author=Heckel%2CR&amp;author=Puddu%2CM&amp;author=Paunescu%2CD&amp;author=Stark%2CWJ">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="34."><p id="ref-CR34">Breunig, M. M., Kriegel, H.-P., Ng, R. T. &amp; Sander, J. LOF: identifying density-based local outliers. <i>ACM SIGMOD Rec.</i> <b>29</b>, 93–104 (2000).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1145/335191.335388" data-track-action="article reference" href="https://doi.org/10.1145%2F335191.335388" aria-label="Article reference 34" data-doi="10.1145/335191.335388">Article</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 34" href="http://scholar.google.com/scholar_lookup?&amp;title=LOF%3A%20identifying%20density-based%20local%20outliers&amp;journal=ACM%20SIGMOD%20Rec.&amp;doi=10.1145%2F335191.335388&amp;volume=29&amp;pages=93-104&amp;publication_year=2000&amp;author=Breunig%2CMM&amp;author=Kriegel%2CH-P&amp;author=Ng%2CRT&amp;author=Sander%2CJ">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="35."><p id="ref-CR35">Hochrein, L., Mitchell, L. A., Schulz, K., Messerschmidt, K. &amp; Mueller-Roeber, B. L-SCRaMbLE as a tool for light-controlled Cre-mediated recombination in yeast. <i>Nat. Commun.</i> <b>9</b>, 1931 (2018).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/s41467-017-02208-6" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41467-017-02208-6" aria-label="Article reference 35" data-doi="10.1038/s41467-017-02208-6">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2018NatCo...9.1931H" aria-label="ADS reference 35">ADS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29789561" aria-label="PubMed reference 35">PubMed</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5964156" aria-label="PubMed Central reference 35">PubMed Central</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=L-SCRaMbLE%20as%20a%20tool%20for%20light-controlled%20Cre-mediated%20recombination%20in%20yeast&amp;journal=Nat.%20Commun.&amp;doi=10.1038%2Fs41467-017-02208-6&amp;volume=9&amp;publication_year=2018&amp;author=Hochrein%2CL&amp;author=Mitchell%2CLA&amp;author=Schulz%2CK&amp;author=Messerschmidt%2CK&amp;author=Mueller-Roeber%2CB">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="36."><p id="ref-CR36">Chen, B. et al. High-throughput analysis and protein engineering using microcapillary arrays. <i>Nat. Chem. Biol.</i> <b>12</b>, 76–81 (2016).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nchembio.1978" data-track-action="article reference" href="https://doi.org/10.1038%2Fnchembio.1978" aria-label="Article reference 36" data-doi="10.1038/nchembio.1978">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2MXhvFemsbrL" aria-label="CAS reference 36">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26641932" aria-label="PubMed reference 36">PubMed</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 36" href="http://scholar.google.com/scholar_lookup?&amp;title=High-throughput%20analysis%20and%20protein%20engineering%20using%20microcapillary%20arrays&amp;journal=Nat.%20Chem.%20Biol.&amp;doi=10.1038%2Fnchembio.1978&amp;volume=12&amp;pages=76-81&amp;publication_year=2016&amp;author=Chen%2CB">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="37."><p id="ref-CR37">Zhang, Y. et al. Accurate high-throughput screening based on digital protein synthesis in a massively parallel femtoliter droplet array. <i>Sci. Adv.</i> <b>5</b>, eaav8185 (2019).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/sciadv.aav8185" data-track-action="article reference" href="https://doi.org/10.1126%2Fsciadv.aav8185" aria-label="Article reference 37" data-doi="10.1126/sciadv.aav8185">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2019SciA....5.8185Z" aria-label="ADS reference 37">ADS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3cXhtlars7%2FM" aria-label="CAS reference 37">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31457078" aria-label="PubMed reference 37">PubMed</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6703874" aria-label="PubMed Central reference 37">PubMed Central</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 37" href="http://scholar.google.com/scholar_lookup?&amp;title=Accurate%20high-throughput%20screening%20based%20on%20digital%20protein%20synthesis%20in%20a%20massively%20parallel%20femtoliter%20droplet%20array&amp;journal=Sci.%20Adv.&amp;doi=10.1126%2Fsciadv.aav8185&amp;volume=5&amp;publication_year=2019&amp;author=Zhang%2CY">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="38."><p id="ref-CR38">Antkowiak, P. L. et al. Integrating DNA encapsulates and digital microfluidics for automated data storage in DNA. <i>Small</i> <b>18</b>, 2107381 (2022).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1002/smll.202107381" data-track-action="article reference" href="https://doi.org/10.1002%2Fsmll.202107381" aria-label="Article reference 38" data-doi="10.1002/smll.202107381">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB38XlvVaksrk%3D" aria-label="CAS reference 38">CAS</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=Integrating%20DNA%20encapsulates%20and%20digital%20microfluidics%20for%20automated%20data%20storage%20in%20DNA&amp;journal=Small&amp;doi=10.1002%2Fsmll.202107381&amp;volume=18&amp;publication_year=2022&amp;author=Antkowiak%2CPL">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="39."><p id="ref-CR39">Lin, K. N., Volkel, K., Tuck, J. M. &amp; Keung, A. J. Dynamic and scalable DNA-based information storage. <i>Nat. Commun.</i> <b>11</b>, 2981 (2020).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/s41467-020-16797-2" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41467-020-16797-2" aria-label="Article reference 39" data-doi="10.1038/s41467-020-16797-2">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2020NatCo..11.2981L" aria-label="ADS reference 39">ADS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3cXhtFylsL3I" aria-label="CAS reference 39">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32532979" aria-label="PubMed reference 39">PubMed</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7293219" aria-label="PubMed Central reference 39">PubMed Central</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Dynamic%20and%20scalable%20DNA-based%20information%20storage&amp;journal=Nat.%20Commun.&amp;doi=10.1038%2Fs41467-020-16797-2&amp;volume=11&amp;publication_year=2020&amp;author=Lin%2CKN&amp;author=Volkel%2CK&amp;author=Tuck%2CJM&amp;author=Keung%2CAJ">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="40."><p id="ref-CR40">Banal, J. L. et al. Random access DNA memory using Boolean search in an archival file storage system. <i>Nat. Mater</i>. 1–9. <a href="https://doi.org/10.1038/s41563-021-01021-3">https://doi.org/10.1038/s41563-021-01021-3</a> (2021).</p></li><li data-counter="41."><p id="ref-CR41">Bee, C. et al. Molecular-level similarity search brings computing to DNA data storage. <i>Nat. Commun.</i> <b>12</b>, 4764 (2021).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/s41467-021-24991-z" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41467-021-24991-z" aria-label="Article reference 41" data-doi="10.1038/s41467-021-24991-z">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2021NatCo..12.4764B" aria-label="ADS reference 41">ADS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXhvVSls73J" aria-label="CAS reference 41">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34362913" aria-label="PubMed reference 41">PubMed</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8346626" aria-label="PubMed Central reference 41">PubMed Central</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=Molecular-level%20similarity%20search%20brings%20computing%20to%20DNA%20data%20storage&amp;journal=Nat.%20Commun.&amp;doi=10.1038%2Fs41467-021-24991-z&amp;volume=12&amp;publication_year=2021&amp;author=Bee%2CC">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="42."><p id="ref-CR42">Buschmann, T. &amp; Bystrykh, L. V. Levenshtein error-correcting barcodes for multiplexed DNA sequencing. <i>BMC Bioinforma.</i> <b>14</b>, 272 (2013).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1186/1471-2105-14-272" data-track-action="article reference" href="https://doi.org/10.1186%2F1471-2105-14-272" aria-label="Article reference 42" data-doi="10.1186/1471-2105-14-272">Article</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 42" href="http://scholar.google.com/scholar_lookup?&amp;title=Levenshtein%20error-correcting%20barcodes%20for%20multiplexed%20DNA%20sequencing&amp;journal=BMC%20Bioinforma.&amp;doi=10.1186%2F1471-2105-14-272&amp;volume=14&amp;publication_year=2013&amp;author=Buschmann%2CT&amp;author=Bystrykh%2CLV">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="43."><p id="ref-CR43">R Core Team. R: the R project for statistical computing. <a href="https://www.r-project.org/">https://www.r-project.org/</a> (2022).</p></li><li data-counter="44."><p id="ref-CR44">Huber, W. et al. Orchestrating high-throughput genomic analysis with Bioconductor. <i>Nat. Methods</i> <b>12</b>, 115–121 (2015).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nmeth.3252" data-track-action="article reference" href="https://doi.org/10.1038%2Fnmeth.3252" aria-label="Article reference 44" data-doi="10.1038/nmeth.3252">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2MXjvVCrurg%3D" aria-label="CAS reference 44">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25633503" aria-label="PubMed reference 44">PubMed</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4509590" aria-label="PubMed Central reference 44">PubMed Central</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 44" href="http://scholar.google.com/scholar_lookup?&amp;title=Orchestrating%20high-throughput%20genomic%20analysis%20with%20Bioconductor&amp;journal=Nat.%20Methods&amp;doi=10.1038%2Fnmeth.3252&amp;volume=12&amp;pages=115-121&amp;publication_year=2015&amp;author=Huber%2CW">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="45."><p id="ref-CR45">Pagès, H., Aboyoun, P., Gentleman, R. &amp; DebRoy, S. Biostrings: efficient manipulation of biological strings. <a href="https://doi.org/10.18129/B9.bioc.Biostrings">https://doi.org/10.18129/B9.bioc.Biostrings</a> (2023).</p></li><li data-counter="46."><p id="ref-CR46">Morgan, M. et al. ShortRead: a bioconductor package for input, quality assessment and exploration of high-throughput sequence data. <i>Bioinformatics</i> <b>25</b>, 2607–2608 (2009).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1093/bioinformatics/btp450" data-track-action="article reference" href="https://doi.org/10.1093%2Fbioinformatics%2Fbtp450" aria-label="Article reference 46" data-doi="10.1093/bioinformatics/btp450">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD1MXhtFyhur7I" aria-label="CAS reference 46">CAS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19654119" aria-label="PubMed reference 46">PubMed</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2752612" aria-label="PubMed Central reference 46">PubMed Central</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=ShortRead%3A%20a%20bioconductor%20package%20for%20input%2C%20quality%20assessment%20and%20exploration%20of%20high-throughput%20sequence%20data&amp;journal=Bioinformatics&amp;doi=10.1093%2Fbioinformatics%2Fbtp450&amp;volume=25&amp;pages=2607-2608&amp;publication_year=2009&amp;author=Morgan%2CM">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="47."><p id="ref-CR47">Wickham, H. stringr: Simple, Consistent Wrappers for Common String Operations. <a href="https://stringr.tidyverse.org/">https://stringr.tidyverse.org</a>, <a href="https://github.com/tidyverse/stringr">https://github.com/tidyverse/stringr</a> (2022).</p></li><li data-counter="48."><p id="ref-CR48">The Python Language Reference. <i>Python documentation.</i> <a href="https://docs.python.org/3/reference/index.html">https://docs.python.org/3/reference/index.html</a>.</p></li><li data-counter="49."><p id="ref-CR49">Arthur, D. &amp; Vassilvitskii, S. k-means++: the advantages of careful seeding. In <i>Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms</i> 1027–1035 (Society for Industrial and Applied Mathematics, 2007).</p></li><li data-counter="50."><p id="ref-CR50">Schubert, E., Sander, J., Ester, M., Kriegel, H. P. &amp; Xu, X. DBSCAN revisited, revisited: why and how you should (Still) use DBSCAN. <i>ACM Trans. Database Syst.</i> <b>42</b>, 1–21 (2017).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1145/3068335" data-track-action="article reference" href="https://doi.org/10.1145%2F3068335" aria-label="Article reference 50" data-doi="10.1145/3068335">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=3622239" aria-label="MathSciNet reference 50">MathSciNet</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 50" href="http://scholar.google.com/scholar_lookup?&amp;title=DBSCAN%20revisited%2C%20revisited%3A%20why%20and%20how%20you%20should%20%28Still%29%20use%20DBSCAN&amp;journal=ACM%20Trans.%20Database%20Syst.&amp;doi=10.1145%2F3068335&amp;volume=42&amp;pages=1-21&amp;publication_year=2017&amp;author=Schubert%2CE&amp;author=Sander%2CJ&amp;author=Ester%2CM&amp;author=Kriegel%2CHP&amp;author=Xu%2CX">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="51."><p id="ref-CR51">Ankerst, M., Breunig, M. M., Kriegel, H.-P. &amp; Sander, J. OPTICS: ordering points to identify the clustering structure. <i>ACM SIGMOD Rec.</i> <b>28</b>, 49–60 (1999).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1145/304181.304187" data-track-action="article reference" href="https://doi.org/10.1145%2F304181.304187" aria-label="Article reference 51" data-doi="10.1145/304181.304187">Article</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 51" href="http://scholar.google.com/scholar_lookup?&amp;title=OPTICS%3A%20ordering%20points%20to%20identify%20the%20clustering%20structure&amp;journal=ACM%20SIGMOD%20Rec.&amp;doi=10.1145%2F304181.304187&amp;volume=28&amp;pages=49-60&amp;publication_year=1999&amp;author=Ankerst%2CM&amp;author=Breunig%2CMM&amp;author=Kriegel%2CH-P&amp;author=Sander%2CJ">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="52."><p id="ref-CR52">Yang, M.-S., Lai, C.-Y. &amp; Lin, C.-Y. A robust EM clustering algorithm for Gaussian mixture models. <i>Pattern Recognit.</i> <b>45</b>, 3950–3961 (2012).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.patcog.2012.04.031" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.patcog.2012.04.031" aria-label="Article reference 52" data-doi="10.1016/j.patcog.2012.04.031">Article</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2012PatRe..45.3950Y" aria-label="ADS reference 52">ADS</a>&nbsp;
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="math reference" href="http://www.emis.de/MATH-item?1242.68260" aria-label="MATH reference 52">MATH</a>&nbsp;
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 52" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20robust%20EM%20clustering%20algorithm%20for%20Gaussian%20mixture%20models&amp;journal=Pattern%20Recognit.&amp;doi=10.1016%2Fj.patcog.2012.04.031&amp;volume=45&amp;pages=3950-3961&amp;publication_year=2012&amp;author=Yang%2CM-S&amp;author=Lai%2CC-Y&amp;author=Lin%2CC-Y">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="53."><p id="ref-CR53">Pedregosa, F. et al. Scikit-learn: Machine Learning in Python. <i>J. Mach. Learn. Res</i>. <b>12</b>, 2825–2830 (2011).</p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41467-023-38876-w?format=refman&amp;flavour=references">Download references</a></p></div></div><div id="Ack1-section" data-title="Acknowledgements"><h2 id="Ack1">Acknowledgements</h2><p>We thank the Yew and Poh Lab members for constructive criticism critical to the research. This work was supported by Ministry of Education, Singapore, under its AcRF Tier 2 Grant (MOE-T2EP30221-0014). C.K.L. is funded by the NUS Integrative Sciences and Engineering Programme.</p></div><div id="author-information-section" aria-labelledby="author-information" data-title="Author information"><h2 id="author-information">Author information</h2><div id="author-information-content"><h3 id="affiliations">Authors and Affiliations</h3><ol><li id="Aff1"><p>Synthetic Biology for Clinical and Technological Innovation, National University of Singapore, 28 Medical Drive, Singapore, 117456, Singapore</p><p>Cheng Kai Lim,&nbsp;Jing Wui Yeoh,&nbsp;Aurelius Andrew Kunartama,&nbsp;Wen Shan Yew&nbsp;&amp;&nbsp;Chueh Loo Poh</p></li><li id="Aff2"><p>Synthetic Biology Translational Research Programme, Yong Loo Lin School of Medicine, National University of Singapore, 14 Medical Drive, Singapore, 117599, Singapore</p><p>Cheng Kai Lim&nbsp;&amp;&nbsp;Wen Shan Yew</p></li><li id="Aff3"><p>Department of Biochemistry, Yong Loo Lin School of Medicine, National University of Singapore, 8 Medical Drive, Singapore, 117597, Singapore</p><p>Cheng Kai Lim&nbsp;&amp;&nbsp;Wen Shan Yew</p></li><li id="Aff4"><p>Department of Biomedical Engineering, College of Design and Engineering, National University of Singapore, Singapore, Singapore</p><p>Cheng Kai Lim,&nbsp;Jing Wui Yeoh,&nbsp;Aurelius Andrew Kunartama&nbsp;&amp;&nbsp;Chueh Loo Poh</p></li><li id="Aff5"><p>Integrative Sciences and Engineering Programme (ISEP), NUS Graduate School, National University of Singapore, Singapore, Singapore</p><p>Cheng Kai Lim</p></li></ol><div data-test="author-info"><p><span>Authors</span></p><ol><li id="auth-Cheng_Kai-Lim"><span>Cheng Kai Lim</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Cheng%20Kai%20Lim" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Cheng%20Kai%20Lim%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Jing_Wui-Yeoh"><span>Jing Wui Yeoh</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jing%20Wui%20Yeoh" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jing%20Wui%20Yeoh%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Aurelius_Andrew-Kunartama"><span>Aurelius Andrew Kunartama</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Aurelius%20Andrew%20Kunartama" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Aurelius%20Andrew%20Kunartama%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Wen_Shan-Yew"><span>Wen Shan Yew</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Wen%20Shan%20Yew" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Wen%20Shan%20Yew%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Chueh_Loo-Poh"><span>Chueh Loo Poh</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Chueh%20Loo%20Poh" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Chueh%20Loo%20Poh%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li></ol></div><h3 id="contributions">Contributions</h3><p>C.K.L. and C.L.P. developed the initial concept. C.K.L. performed the experiments and analyzed the results under the supervision of C.L.P. and W.S.Y. C.K.L., J.W.Y. and C.L.P. developed and constructed the computational pipeline for image reconstruction. A.A.K. designed and developed the OptoBox device. C.K.L., J.W.Y., A.A.K. and C.L.P. wrote the manuscript with input from all authors.</p><h3 id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:poh.chuehloo@nus.edu.sg">Chueh Loo Poh</a>.</p></div></div><div id="ethics-section" data-title="Ethics declarations"><h2 id="ethics">Ethics declarations</h2><div id="ethics-content">
              
                <h3 id="FPar2">Competing interests</h3>
                <p>The authors declare no competing interests.</p>
              
            </div></div><div id="peer-review-section" data-title="Peer review"><h2 id="peer-review">Peer review</h2><div id="peer-review-content">
              
              
                <h3 id="FPar1">Peer review information</h3>
                <p><i>Nature Communications</i> thanks the anonymous reviewer(s) for their contribution to the peer review of this work. A peer review file is available.</p>
              
            </div></div><div id="additional-information-section" data-title="Additional information"><h2 id="additional-information">Additional information</h2><p><b>Publisher’s note</b> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div><div id="Sec23-section" data-title="Supplementary information"><h2 id="Sec23">Supplementary information</h2></div><div id="Sec24-section" data-title="Source data"><h2 id="Sec24">Source data</h2></div><div id="rightslink-section" data-title="Rights and permissions"><h2 id="rightslink">Rights and permissions</h2><div id="rightslink-content">
                <p><b>Open Access</b>  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">http://creativecommons.org/licenses/by/4.0/</a>.</p>
              <p><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=A%20biological%20camera%20that%20captures%20and%20stores%20images%20directly%20into%20DNA&amp;author=Cheng%20Kai%20Lim%20et%20al&amp;contentID=10.1038%2Fs41467-023-38876-w&amp;copyright=The%20Author%28s%29&amp;publication=2041-1723&amp;publicationDate=2023-07-03&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and Permissions</a></p></div></div><div id="article-info-section" aria-labelledby="article-info" data-title="About this article"><h2 id="article-info">About this article</h2><div id="article-info-content"><p><a data-crossmark="10.1038/s41467-023-38876-w" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s41467-023-38876-w" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></p><div><h3 id="citeas">Cite this article</h3><p>Lim, C.K., Yeoh, J.W., Kunartama, A.A. <i>et al.</i> A biological camera that captures and stores images directly into DNA.
                    <i>Nat Commun</i> <b>14</b>, 3921 (2023). https://doi.org/10.1038/s41467-023-38876-w</p><p><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41467-023-38876-w?format=refman&amp;flavour=citation">Download citation</a></p><ul data-test="publication-history"><li><p>Received<span>: </span><span><time datetime="2022-04-19">19 April 2022</time></span></p></li><li><p>Accepted<span>: </span><span><time datetime="2023-05-19">19 May 2023</time></span></p></li><li><p>Published<span>: </span><span><time datetime="2023-07-03">03 July 2023</time></span></p></li><li><p><abbr title="Digital Object Identifier">DOI</abbr><span>: </span><span>https://doi.org/10.1038/s41467-023-38876-w</span></p></li></ul></div></div></div>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Uber, DoorDash Sue NYC to Stop Delivery Drivers from Getting a Minimum Wage (101 pts)]]></title>
            <link>https://www.vice.com/en/article/93k3ve/uber-doordash-sue-nyc-to-stop-delivery-drivers-from-getting-a-minimum-wage</link>
            <guid>36663312</guid>
            <pubDate>Mon, 10 Jul 2023 07:57:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vice.com/en/article/93k3ve/uber-doordash-sue-nyc-to-stop-delivery-drivers-from-getting-a-minimum-wage">https://www.vice.com/en/article/93k3ve/uber-doordash-sue-nyc-to-stop-delivery-drivers-from-getting-a-minimum-wage</a>, See on <a href="https://news.ycombinator.com/item?id=36663312">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Image Credit: Getty Images</p></div><div data-component="BodyComponentRenderer"><p><span data-component="TextBlock"><p>Gig economy companies DoorDash, Grubhub, and Uber sued New York City on Thursday over a new minimum wage law for app-based delivery workers that would raise their minimum wage to $17.96 per hour. The law, which was <a href="https://www.vice.com/en/article/ak33jp/the-minimum-wage-for-delivery-drivers-in-nyc-is-now-dollar1796-an-hour">implemented in mid-June</a>, is the first of its kind in the U.S, and would go into effect July 12.&nbsp;</p></span><span></span><span data-component="TextBlock"><p>DoorDash and Grubhub, which specialize in food delivery, filed a <a href="https://iapps.courts.state.ny.us/nyscef/ViewDocument?docIndex=ub_PLUS_fhjvQeK4Hz6rTVkD/Pw==" target="_blank">joint lawsuit</a> against the city’s Department of Consumer and Worker Protection (DCWP) and its commissioner. Uber, which deals both in food delivery and ridesharing, filed a separate lawsuit on the same day.&nbsp;</p></span><span data-component="TextBlock"><p>“Bad policies cannot go unchallenged, and we will not stand by and let the harmful impacts of this earnings standard on New York City customers, merchants, and the delivery workers it was intended to support go unchecked,” a DoorDash spokesperson told Motherboard in an email. “We—and others—clearly and repeatedly warned the city that using such a flawed process to underpin its rulemaking would have lasting and harmful impacts for all New Yorkers who use these platforms, but the approach that DCWP took was sadly not one that reflected this, and has left us no choice but to take our concerns to court.”</p></span><span data-component="TextBlock"><p>DCWP Commissioner Vilda Vera Mayuga told Motherboard in a statement, “Delivery workers, like all workers, deserve fair pay for their labor, and we are disappointed that Uber, DoorDash, GrubHub, and Relay disagree. These workers brave thunderstorms, extreme heat events, and risk their lives to deliver for New Yorkers—and we remain committed to delivering for them. The minimum pay rate will help uplift thousands of working New Yorkers and their families out of poverty. We look forward to the court’s decision and to apps beginning to pay these workers a dignified rate.”</p></span><span></span><span data-component="TextBlock"><p>The new law planned to raise the minimum wage of app-based delivery workers in the city to $17.96 per hour on July 12, and subsequently to $19.96 per hour by April 2025, when it would be fully phased in. A <a href="https://www.nyc.gov/office-of-the-mayor/news/405-23/mayor-adams-dcwp-commissioner-mayuga-nation-s-first-minimum-pay-rate-app-based#/0" target="_blank">press release</a> from the city at the time stated that delivery workers currently earn around $7 per hour on average. Most food delivery apps pay workers by the delivery and drivers often greatly rely on customer tips to be their main source of income. Motherboard has <a href="https://www.vice.com/en/article/3akd8j/reddit-is-full-of-doordashers-begging-customers-for-tips">previously reported</a> that DoorDashers, for example, often only get a base pay of $2 or $3 per delivery, and get about two-thirds of their income from customer tips. The law, then, would mean an overall increase of around $13 per hour.&nbsp;</p></span></p><p><span data-component="TextBlock"><p>DoorDash argues in its <a href="https://iapps.courts.state.ny.us/nyscef/ViewDocument?docIndex=ub_PLUS_fhjvQeK4Hz6rTVkD/Pw==" target="_blank">lawsuit</a>, however, that the law is improperly implemented, because it would force companies to pay workers for any time logged into the app, regardless of whether they were delivering or not, and that the DCWP ignored “mountains of hard data and analysis, and over emphatic objections from a range of constituents” in order to implement it.&nbsp;</p></span><span></span><span data-component="TextBlock"><p>“Although Petitioners are not opposed to a well-constructed minimum-pay standard for delivery workers (and, indeed, have supported such standards in other jurisdictions), the implementation of this ill-conceived Rule will have drastic—and immediate—consequences for all concerned parties,” the lawsuit states.&nbsp;</p></span><span data-component="TextBlock"><p>In a <a href="https://doordash.news/get-the-facts/why-were-filing-a-lawsuit-on-the-nyc-earnings-standard/" target="_blank">post on its website</a>, DoorDash further explained its claim. “The earnings standard does not reflect the way the industry operates,” the post reads. “For instance, why would the agency choose to purposefully exclude companies that only facilitate grocery deliveries from its study and rulemaking, when those that facilitate both restaurant and grocery deliveries from local businesses would have these rules apply to them? A worker can now be subject to different legal standards for identical orders from identical local businesses, based solely on the platform they choose to accept the order from.”</p></span><span data-component="TextBlock"><p>DoorDash and Grubhub are seeking a temporary restraining order and preliminary injunction against the rule before it goes into effect.&nbsp;</p></span><span data-component="TextBlock"><p>“If allowed to stand, this rule will have serious adverse consequences for delivery partners, consumers and independent businesses,” a Grubhub spokesperson told Motherboard in an email. “Grubhub commends the City’s attention to this issue, but we cannot support a solution that has such unintended implications for those who rely on food delivery.”</p></span><span data-component="TextBlock"><p>An Uber spokesperson did not immediately respond to a request for comment.</p></span></p></div><div><p><h3>ORIGINAL REPORTING ON EVERYTHING THAT MATTERS IN YOUR INBOX.</h3></p><p>By signing up, you agree to the<!-- --> <a href="https://vice-web-statics-cdn.vice.com/privacy-policy/en_us/page/terms-of-use.html">Terms of Use</a> <!-- -->and<!-- --> <a href="https://vice-web-statics-cdn.vice.com/privacy-policy/en_us/page/privacy-policy.html">Privacy Policy</a> <!-- -->&amp; to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kubernetes SidecarContainers feature is merged (115 pts)]]></title>
            <link>https://github.com/kubernetes/kubernetes/pull/116429</link>
            <guid>36663060</guid>
            <pubDate>Mon, 10 Jul 2023 07:18:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/kubernetes/kubernetes/pull/116429">https://github.com/kubernetes/kubernetes/pull/116429</a>, See on <a href="https://news.ycombinator.com/item?id=36663060">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
  <task-lists disabled="" sortable="">
    <div>
      
<h4 dir="auto">What type of PR is this?</h4>
<p dir="auto">/kind feature<br>
/kind api-change</p>

<h4 dir="auto">What this PR does / why we need it:</h4>
<p dir="auto">This adds the Sidecar Containers feature to kubernetes.</p>
<ul>
<li>  API change - add restartPolicy to Init containers</li>
<li>  API validation - only allow restartPolicy for Init containers and allow startupProbe for the sidecar containers</li>
<li>  Startup: wait for Started, not for completion.</li>
<li>  Reconciling status of Sidecar container with the runtime</li>
<li>  Termination: Pod is terminated even if sidecar is still running</li>
<li>  Resources calculation update</li>
<li>  Restart sidecar on sidecar failure/completion while pod is still running</li>
<li> <del>Topology/CPU managers update</del></li>
</ul>
<h4 dir="auto">Which issue(s) this PR fixes:</h4>

<p dir="auto">Fixes #</p>
<p dir="auto">ref: <a data-error-text="Failed to load title" data-id="1594100455" data-permission-text="Title is private" data-url="https://github.com/kubernetes/kubernetes/issues/115934" data-hovercard-type="issue" data-hovercard-url="/kubernetes/kubernetes/issues/115934/hovercard" href="https://github.com/kubernetes/kubernetes/issues/115934">#115934</a></p>
<h4 dir="auto">Special notes for your reviewer:</h4>
<h4 dir="auto">Does this PR introduce a user-facing change?</h4>

<div data-snippet-clipboard-copy-content="The new feature gate &quot;SidecarContainers&quot; is now available. This feature introduces sidecar containers, a new type of init container that starts before other containers but remains running for the full duration of the pod's lifecycle and will not block pod termination."><pre lang="release-note"><code>The new feature gate "SidecarContainers" is now available. This feature introduces sidecar containers, a new type of init container that starts before other containers but remains running for the full duration of the pod's lifecycle and will not block pod termination.
</code></pre></div>
<h4 dir="auto">Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:</h4>

<div data-snippet-clipboard-copy-content="- [KEP]: https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/753-sidecar-containers"><pre lang="docs"><code>- [KEP]: https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/753-sidecar-containers
</code></pre></div>
<p dir="auto">/cc <a data-hovercard-type="user" data-hovercard-url="/users/SergeyKanzhelev/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/SergeyKanzhelev">@SergeyKanzhelev</a> <a data-hovercard-type="user" data-hovercard-url="/users/matthyx/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/matthyx">@matthyx</a></p>
    </div>
  </task-lists>
  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Workout.lol – a web app to easily create a workout routine (602 pts)]]></title>
            <link>https://workout.lol</link>
            <guid>36662655</guid>
            <pubDate>Mon, 10 Jul 2023 06:01:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://workout.lol">https://workout.lol</a>, See on <a href="https://news.ycombinator.com/item?id=36662655">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Japanese textile factory still uses tape cassette and punch cards [video] (270 pts)]]></title>
            <link>https://www.youtube.com/watch?v=zWJZFQHklBg</link>
            <guid>36662392</guid>
            <pubDate>Mon, 10 Jul 2023 05:14:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=zWJZFQHklBg">https://www.youtube.com/watch?v=zWJZFQHklBg</a>, See on <a href="https://news.ycombinator.com/item?id=36662392">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Mistakes with Rust smart pointers: when Deref goes wrong (159 pts)]]></title>
            <link>https://www.fuzzypixelz.com/blog/deref-confusion/</link>
            <guid>36662385</guid>
            <pubDate>Mon, 10 Jul 2023 05:13:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fuzzypixelz.com/blog/deref-confusion/">https://www.fuzzypixelz.com/blog/deref-confusion/</a>, See on <a href="https://news.ycombinator.com/item?id=36662385">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="blog-page-article">
<h2 id="prologue">Prologue</h2>
<p>The following is an attempt at highlighting the sources of <a href="https://en.wikipedia.org/wiki/Principle_of_least_astonishment">high
astonishment</a>
arising from the implementation of the
<a href="https://doc.rust-lang.org/std/ops/trait.Deref.html"><code>std::ops::Deref</code></a> trait on
custom types.</p>
<p>I write this, in part, because I had recently used <code>Deref</code> inappropriately while
at <code>$day_job</code>, due to my lack of a full understanding of how <code>Deref</code> interacts
with the rest of the language. It should go without saying that this was a
confusing experience.</p>
<p>Naturally, the aforementioned mishap will serve as a motivating example
throughout this dive into Rust semantics.</p>
<h2 id="newtypes">Newtypes</h2>
<p>I cannot share the full context surrounding my misuse of <code>Deref</code> (for both
pedagogic and legal reasons) so please understand that my motivating example
won't be well motivated itself.</p>
<p>With that disclaimer out of the way, suppose we're building a library that
provides an <a href="https://en.wikipedia.org/wiki/ASCII">ASCII</a> string type called
<code>AsciiString</code>. A fitting representation of these so-called <em>byte strings</em> would
simply be a plain ol' <code>Vec&lt;u8&gt;</code>.</p>
<p>However, ASCII characters are limited to the range <code>0..128</code>, and thus not any
value of type <code>Vec&lt;u8&gt;</code> would be a valid ASCII string. In other words, it's not
a good idea to
<a href="https://doc.rust-lang.org/reference/items/type-aliases.html">alias</a>
<code>AsciiString</code> to <code>Vec&lt;u8&gt;</code>; <a href="https://lexi-lambda.github.io/blog/2020/11/01/names-are-not-type-safety/">they are not
interchangeable</a>.</p>
<p>In Rust<sup><a href="#1">1</a></sup>, it is considered good practice to enforce invariants at the type
level. Thus, the idiomatic course of action would be to use a
<a href="https://doc.rust-lang.org/rust-by-example/generics/new_types.html">newtype</a> to
wrap a <code>Vec&lt;u8&gt;</code> and then statically guarantee that all wrapped byte vectors
represent valid ASCII strings.</p>
<pre data-lang="rust"><code data-lang="rust">/// An ASCII-encoded, growable string.
struct AsciiString(Vec<u8>);

impl AsciiString {
    /// Range of valid ASCII bytes.
    const ASCII_RANGE: Range<u8> = 0..128;

    /// Converts a byte vector to an `AsciiString`.
    ///
    /// # Errors
    /// Returns `Err` if any byte in `vec` 
    /// falls outside the range [`Self::ASCII_RANGE`].
    fn from_ascii(vec: Vec<u8>) -&gt; Result<self, asciierror=""> {
        if vec
            .iter()
            .all(|b| Self::ASCII_RANGE.contains(b))
        {
            Ok(Self(vec))
        } else {
            Err(AsciiError)
        }
    }
}
</self,></u8></u8></u8></code></pre>
<h2 id="explicit-conversion">Explicit conversion</h2>
<p>UTF-8 is backward compatible with ASCII. This means that we can freely
reinterpret any well-constructed <code>AsciiString</code> value to a <code>str</code> value. What we
stand to gain from this is compatibility with code written with UTF-8 strings in
mind.</p>
<pre data-lang="rust"><code data-lang="rust">impl AsciiString {
    fn as_str(&amp;self) -&gt; &amp;str {
        debug_assert!(str::from_utf8(&amp;self.0).is_ok());

        // SAFETY: self.0 is ASCII-encoded, 
        // and ASCII is a subset of UTF-8
        unsafe { str::from_utf8_unchecked(&amp;self.0) }
    }
}
</code></pre>
<p>Two statements in, and the unspeakable keyword has already been uttered; I'm
feeling pretty <a href="https://twitter.com/ManishEarth/status/901269205718155264">tall</a>
today and the temptation is frankly irresistible. Obviously, any bug surrounding
the construction of <code>AsciiString</code> values implies memory unsafely outside <code>debug</code>
mode.</p>
<p>Hold on a second. Both
<a href="https://doc.rust-lang.org/std/str/fn.from_utf8.html">std::str::from_utf8</a> and
<a href="https://doc.rust-lang.org/stable/std/str/fn.from_utf8_unchecked.html">std::str::from_utf8_unchecked</a>
take in a <code>&amp;[u8]</code> as an argument, while <code>&amp;self.0</code> is of type <code>&amp;Vec&lt;u8&gt;</code>; there
is definitely some kind of implicit coercion going on here. Is Rust not so
strongly typed after all?</p>
<h2 id="enter-deref">Enter <code>Deref</code></h2>
<pre data-lang="rust"><code data-lang="rust">pub trait Deref {
    type Target: ?Sized;

    fn deref(&amp;self) -&gt; &amp;Self::Target;
}
</code></pre>
<p>We seem to have inadvertently run into <code>Deref</code> before I even got the chance to
introduce it properly. The <a href="https://www.youtube.com/watch?v=2bjk26RwjyU">simple</a>
explanation of why <code>&amp;Vec&lt;u8&gt;</code> was "coerced" to <code>&amp;[u8]</code> is that <code>Vec&lt;T&gt;</code>
<a href="https://doc.rust-lang.org/std/vec/struct.Vec.html#impl-Deref-for-Vec%3CT,+A%3E">implements</a>
<code>Deref&lt;Target = [T]&gt;</code>. In short, the compiler conceptually transformed the
expression <code>str::from_utf8(&amp;self.0)</code> into something akin to:</p>
<pre data-lang="rust"><code data-lang="rust">str::from_utf8(<vec<u8> as Deref&gt;::deref(&amp;self.0))
</vec<u8></code></pre>
<p>This mechanism at play here is called <a href="https://doc.rust-lang.org/std/ops/trait.Deref.html#more-on-deref-coercion">Deref
Coercion</a>,
and is defined by the interaction between two distinct mechanisms defined by The
Rust Reference: <a href="https://doc.rust-lang.org/nightly/reference/expressions/method-call-expr.html">Method
Resolution</a>
and <a href="https://doc.rust-lang.org/reference/type-coercions.html">Type
Coercion</a>. Having
already dropped so much jargon, I refrain from uncovering any gory details for
now.</p>
<h2 id="clean-codetm">Clean Code™</h2>
<p>For all practical purposes, <code>AsciiString:as_str</code> works perfectly as a sure and
simple way to convert any <code>&amp;AsciiString</code> reference to a <code>&amp;str</code>. However, it will
mean one would have to manually add <code>.as_str()</code> every time a conversion is
needed.</p>
<p>For dubious reasons involving "clean code" we shall implement <code>Deref&lt;Target = str&gt;</code> for <code>AsciiString</code>: the compiler will from now on call <code>.as_str()</code> on our
behalf.</p>
<pre data-lang="rust"><code data-lang="rust">impl Deref for AsciiString {
    type Target = str;

    fn deref(&amp;self) -&gt; &amp;Self::Target {
        self.as_str()
    }
}
</code></pre>
<h2 id="idiomatic-codetm">Idiomatic Code™</h2>
<p>For those who care not for aesthetics when it comes to computer programs — a
vulgar but arguably rational position — I shall appeal to Rust
idioms<sup><a href="#3">2</a></sup>. Chapter 5 of the Rust API Guidelines contains a recommendation
titled
<a href="https://rust-lang.github.io/api-guidelines/predictability.html#only-smart-pointers-implement-deref-and-derefmut-c-deref">C-DEREF</a>
which states:</p>
<blockquote>
<p>The Deref traits are used implicitly by the compiler in many circumstances,
and interact with method resolution. The relevant rules are designed
specifically to accommodate smart pointers, and so the traits should be used
only for that purpose.</p>
</blockquote>
<p>Is <code>AsciiString</code> a smart pointer? What is a smart pointer anyway? That is a
<a href="https://users.rust-lang.org/t/what-are-smart-pointers/67361">debate</a> I'm
uninterested in exploring here. However, the authors of the Rust API Guidelines
go on to give examples of smart pointers in the Standard Library:</p>
<ul>
<li><code>Box&lt;T&gt;</code></li>
<li><code>String</code> as a smart pointer to <code>str</code></li>
<li><code>Rc&lt;T&gt;</code></li>
<li><code>Arc&lt;T&gt;</code></li>
<li><code>Cow&lt;'a, T&gt;</code></li>
</ul>
<p>On these grounds, I claim that <code>AsciiString</code> ought to be considered a smart
pointer to <code>str</code>. Note that the idea that <code>String</code> is a smart pointer is itself
often contested, since <code>String</code> is not generic over the data it points to the
way <code>Box&lt;T&gt;</code> and friends are.</p>
<h2 id="to-my-astonishment">To my astonishment</h2>
<p>Note that <code>AsciiString</code> inherits the implementation of <code>Index&lt;Range&lt;usize&gt;&gt;</code>
from <code>str</code>. We get this as a side effect of implementing <code>Deref</code> and it allows
us to borrow string slices from an <code>AsciiString</code><sup><a href="#4">3</a></sup>.</p>
<pre data-lang="rust"><code data-lang="rust">let data = vec![b'A', b'B', b'C'];
let ascii = AsciiString::from_ascii(data).unwrap();
assert_eq!(&amp;ascii[1..3], "BC");
</code></pre>
<p>Up until now, we have been relatively well-behaved rustaceans. But where is the
fun in that? One cannot be writing production-ready code all the time, for that
is a sure path to madness.</p>
<p>Let's allow for <code>AsciiString</code> to be indexed using floating-point numbers;
precisely because we can.</p>
<pre data-lang="rust"><code data-lang="rust">impl Index<range<f64>&gt; for AsciiString {
    type Output = str;

    /// Returns an ASCII string slice by rounding 
    /// the given floating-point indices in `index`.
    fn index(&amp;self, index: Range<f64>) -&gt; &amp;Self::Output {
        /// Returns the integer nearest 
        /// to the absolute value of `float`.
        fn nearest_index(float: f64) -&gt; usize {
            float.abs().round() as usize
        }

        let start = nearest_index(index.start);
        let end = nearest_index(index.end);
        let string = self.as_str();

        &amp;string[start..end]
    }
}
</f64></range<f64></code></pre>
<p>You may be wondering what happens when your index is exactly halfway between two
integers. The answer is that <code>f64::round</code> will round <em>away from</em> <code>0.0</code>, thus
<code>0.5</code> will be rounded to <code>1</code>. At this point, we might as well have had <code>Index</code>
toss a coin to resolve this ambiguity.</p>
<pre data-lang="rust"><code data-lang="rust">let data = vec![b'A', b'B', b'C'];
let ascii = AsciiString::from_ascii(data).unwrap();
assert_eq!(&amp;ascii[1.5_f64..3_f64], "BC");
</code></pre>
<p>We finally arrived at the part which surprised me and served as the motivation
for all that I wrote before and after this point: we <strong>no longer inherit</strong>
<code>Index&lt;Range&lt;usize&gt;&gt;</code> <strong>from</strong> <code>str</code>. The previous version of the above code
block doesn't compile anymore:</p>
<pre data-lang="console"><code data-lang="console">error[E0308]: mismatched types
  --&gt; src/main.rs:69:23
   |
69 |     assert_eq!(&amp;ascii[1..3], "BC");
   |                       ^^^^ expected `Range<f64>`, 
   |                            found `Range&lt;{integer}&gt;`
   |
   = note: expected struct `std::ops::Range<f64>`
              found struct `std::ops::Range&lt;{integer}&gt;`
</f64></f64></code></pre>
<p>To me, this was incredibly confusing because <code>Index</code> is generic over its <code>Idx</code>
type, which means it's perfectly valid to have multiple implementations of
<code>Index&lt;Idx&gt;</code> for different <code>Idx</code> types<sup><a href="#5">4</a></sup>.</p>
<p>Somehow, the compiler can no longer find the implementation of <code>Index&lt;usize&gt;</code> on
<code>&amp;str</code>. If you're like me, it's not at all obvious how that came to be. To
better understand all of this, we dig deeper into exactly how the compiler
resolves methods in the presence of a <code>Deref</code> implementation.</p>
<h2 id="nitty-gritty-details">Nitty-gritty details</h2>
<p><em>Everything I describe below is limited to commit
<a href="https://github.com/rust-lang/rust/tree/6bacf5a54468f8db33b6077405652f0ab0059174">6bacf5a</a>
of the Rust project, i.e. Rust 1.69.0</em>.</p>
<p>Method lookup in <code>rustc</code> is divided into two phases:
<a href="https://github.com/rust-lang/rust/blob/6bacf5a/compiler/rustc_hir_typeck/src/method/probe.rs" title="probe">probe</a> and
<a href="https://github.com/rust-lang/rust/blob/6bacf5a54468f8db33b6077405652f0ab0059174/compiler/rustc_hir_typeck/src/method/confirm.rs">confirm</a>. As
the <a href="https://rustc-dev-guide.rust-lang.org/method-lookup.html">Rust Compiler Development
Guide</a> explains,
<code>probe</code> is concerned with finding the callee method while <code>confirm</code> solves for
unknown type variables.</p>
<p>During the <code>probe</code> phase, the compiler would first perform a "Deref-loop":
essentially repeated dereferencing of the receiver type, i.e. <code>AsciiString</code>. In
our particular case, this would yield two <em>steps</em>: <code>AsciiString</code> and <code>[u8]</code>.</p>
<p>Then, the compiler would <em>assemble</em> a list of <em>candidates</em> from each step. These
candidate methods are subsequently searched for one that fits the reliever
type. Up until this point, it seems that <code>rustc</code> should've been able to find the
<code>index</code> method from the <code>Index&lt;Range&lt;usize&gt;&gt;</code> implementation on <code>[u8]</code> because
the search always includes Deref targets.</p>
<p>The issue, it turns out, is that method search does not check method parameter
types against argument types. Rather, it only checks for method names and
where-clauses such as <code>T: Trait</code>. Types are only resolved in the <code>confirm</code> phase,
at which point <code>rustc</code> would have already <em>picked</em> its method.</p>
<p>But why is method lookup in <code>rustc</code> split into two phases? The answer lies in
the cacheability of the <em>picks</em> provided by the <code>probe</code> phase. In other words,
<code>rustc</code> developers wished to reuse <code>probe</code> search results of identical methods,
hence why <code>probe</code> cannot depend on inferred types local to one particular call
site. I would wager that this was most likely done to address performance
issues.</p>
<p>If you can spot any inaccuracies in the above description, please don't shy away
from correcting me via email. I would appreciate it, as I am by no means a
<code>rustc</code> developer<sup><a href="#6">5</a></sup>.</p>
<h2 id="epilogue">Epilogue</h2>
<p>Moral of the story: don't implement <code>Deref</code> kids!</p>
<p>On a more serious note, if one does implement <code>Deref&lt;Target = U&gt;</code> on a type <code>T</code>,
<em>and</em> it proved necessary to implement a generic trait <code>Trait&lt;S&gt;</code> on <code>T</code>. Then
one should ensure that the implementations of <code>Trait&lt;S&gt;</code> for different
parameters <code>S</code> <strong>are not scattered across <code>T</code> and <code>U</code></strong>.</p>
<p>Put simply, either <code>T</code> or <code>U</code> should host all existing <code>Trait&lt;S&gt;</code>
implementations. Note that the preceding "or" is <em>inclusive</em>, meaning that both
<code>T</code> and <code>U</code> can host some <code>Trait&lt;S&gt;</code> implementations, as long as they both host
<em>all of them</em>.</p>
<hr>






</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: React95 – A React components library recreating the look of Windows 95 (108 pts)]]></title>
            <link>https://github.com/React95/React95</link>
            <guid>36661070</guid>
            <pubDate>Mon, 10 Jul 2023 01:49:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/React95/React95">https://github.com/React95/React95</a>, See on <a href="https://news.ycombinator.com/item?id=36661070">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
          <p>      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?
</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[3M reaches $10.3B settlement over PFAS contamination of water systems (331 pts)]]></title>
            <link>https://www.npr.org/2023/06/22/1183922303/3m-reaches-10-3-billion-settlement-over-contamination-of-water-systems</link>
            <guid>36660751</guid>
            <pubDate>Mon, 10 Jul 2023 00:57:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.npr.org/2023/06/22/1183922303/3m-reaches-10-3-billion-settlement-over-contamination-of-water-systems">https://www.npr.org/2023/06/22/1183922303/3m-reaches-10-3-billion-settlement-over-contamination-of-water-systems</a>, See on <a href="https://news.ycombinator.com/item?id=36660751">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storytext">
      <div id="res1183922375">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2023/06/22/ap23153690482424-c7e22cf65a7c63a2dec0d01575cc30773fb32f38-s400-c85.webp 400w,
https://media.npr.org/assets/img/2023/06/22/ap23153690482424-c7e22cf65a7c63a2dec0d01575cc30773fb32f38-s600-c85.webp 600w,
https://media.npr.org/assets/img/2023/06/22/ap23153690482424-c7e22cf65a7c63a2dec0d01575cc30773fb32f38-s800-c85.webp 800w,
https://media.npr.org/assets/img/2023/06/22/ap23153690482424-c7e22cf65a7c63a2dec0d01575cc30773fb32f38-s900-c85.webp 900w,
https://media.npr.org/assets/img/2023/06/22/ap23153690482424-c7e22cf65a7c63a2dec0d01575cc30773fb32f38-s1200-c85.webp 1200w,
https://media.npr.org/assets/img/2023/06/22/ap23153690482424-c7e22cf65a7c63a2dec0d01575cc30773fb32f38-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2023/06/22/ap23153690482424-c7e22cf65a7c63a2dec0d01575cc30773fb32f38-s1800-c85.webp 1800w" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2023/06/22/ap23153690482424-c7e22cf65a7c63a2dec0d01575cc30773fb32f38-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2023/06/22/ap23153690482424-c7e22cf65a7c63a2dec0d01575cc30773fb32f38-s600-c85.jpg 600w,
https://media.npr.org/assets/img/2023/06/22/ap23153690482424-c7e22cf65a7c63a2dec0d01575cc30773fb32f38-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2023/06/22/ap23153690482424-c7e22cf65a7c63a2dec0d01575cc30773fb32f38-s900-c85.jpg 900w,
https://media.npr.org/assets/img/2023/06/22/ap23153690482424-c7e22cf65a7c63a2dec0d01575cc30773fb32f38-s1200-c85.jpg 1200w,
https://media.npr.org/assets/img/2023/06/22/ap23153690482424-c7e22cf65a7c63a2dec0d01575cc30773fb32f38-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2023/06/22/ap23153690482424-c7e22cf65a7c63a2dec0d01575cc30773fb32f38-s1800-c85.jpg 1800w" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2023/06/22/ap23153690482424-c7e22cf65a7c63a2dec0d01575cc30773fb32f38-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                Eva Stebel, water researcher, pours a water sample into a smaller glass container for experimentation as part of drinking water and PFAS research at the U.S. Environmental Protection Agency Center For Environmental Solutions and Emergency Response on Feb. 16, 2023, in Cincinnati.
                <b aria-label="Image credit">
                    
                    Joshua A. Bickel/AP
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Joshua A. Bickel/AP
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2023/06/22/ap23153690482424-c7e22cf65a7c63a2dec0d01575cc30773fb32f38-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2023/06/22/ap23153690482424-c7e22cf65a7c63a2dec0d01575cc30773fb32f38-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2023/06/22/ap23153690482424-c7e22cf65a7c63a2dec0d01575cc30773fb32f38-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2023/06/22/ap23153690482424-c7e22cf65a7c63a2dec0d01575cc30773fb32f38-s1200.jpg">
        </picture>
    </div>
<div>
        <p>Eva Stebel, water researcher, pours a water sample into a smaller glass container for experimentation as part of drinking water and PFAS research at the U.S. Environmental Protection Agency Center For Environmental Solutions and Emergency Response on Feb. 16, 2023, in Cincinnati.</p>
        <p><span aria-label="Image credit">
            
            Joshua A. Bickel/AP
            
        </span>
    </p></div>
   </div>
   <p>TRAVERSE CITY, Mich. — Chemical manufacturer 3M Co. will pay at least $10.3 billion to settle lawsuits over contamination of many U.S. public drinking water systems with potentially harmful compounds used in firefighting foam and a host of consumer products, the company said Thursday.</p>   <p>The deal would compensate water providers for pollution with per- and polyfluorinated substances, known collectively as PFAS — a broad class of chemicals used in nonstick, water- and grease-resistant products such as clothing and cookware.</p>   <p>Described as "forever chemicals" because they don't degrade naturally in the environment, PFAS have been linked to a variety of health problems, including liver and immune-system damage and some cancers.</p>   
   
<!-- END ID="RES1183922969" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>The compounds have been detected at varying levels in drinking water around the nation. The Environmental Protection Agency in March proposed strict limits on two common types, PFOA and PFOS, and said it wanted to regulate four others. Water providers would be responsible for monitoring their systems for the chemicals.</p>   <p>The agreement would settle a case that was scheduled for trial earlier this month involving a claim by Stuart, Florida, one of about 300 communities that have filed similar suits against companies that produced firefighting foam or the PFAS it contained.</p>   
   <p>3M chairman Mike Roman said the deal was "an important step forward" that builds on the company's decision in 2020 to phase out PFOA and PFOS and its investments in "state-of-the-art water filtration technology in our chemical manufacturing operations." The company, based in St. Paul, Minnesota, will halt all PFAS production by the end of 2025, he said.</p>   <p>The settlement will be paid over 13 years and could reach as high as $12.5 billion, depending on how many public water systems detect PFAS during testing that EPA has required in the next three years, said Dallas-based attorney Scott Summy, one of the lead attorneys for those suing 3M and other manufacturers.</p>   <p>The payment will help cover costs of filtering PFAS from systems where it's been detected and testing others, he said.</p>   
   
<!-- END ID="RES1183923039" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>"The result is that millions of Americans will have healthier lives without PFAS in their drinking water," Summy said.</p>   <p>Earlier this month, three other companies — DuPont de Nemours Inc. and spinoffs Chemours Co. and Corteva Inc. — reached a $1.18 billion deal to resolve PFAS complaints by about 300 drinking water providers. A number of states, airports, firefighter training facilities and private well owners also have sued.</p>   <p>The cases are pending in U.S. District Court in Charleston, South Carolina, where Judge Richard Gergel is overseeing thousands of complaints alleging PFAS damages. A trial of a complaint by the city of Stuart, Florida, had been scheduled to begin this month but was delayed to allow time for additional settlement negotiations.</p>   
   <p>Most of the lawsuits have stemmed from firefighter training exercises at airports, military bases and other sites around the U.S. that repeatedly used foams laced with high concentrations of PFAS, Summy said.</p>   <p>The 3M settlement is subject to court approval, he said.</p>   
   
<!-- END ID="RES1183923095" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>3M's website says the company helped the U.S. Navy develop foams containing PFAS chemicals in the 1960s.</p>   <p>"This was an important and life-saving tool that helped combat dangerous fires, like those caused by jet fuel," the company said.</p>   <p>3M said its participation in the settlement "is not an admission of liability" and said if it was rejected in court, "3M is prepared to continue to defend itself."</p>   <p>The cost of cleansing PFAS from U.S. water systems eventually could go much higher than the sums agreed to in the settlements, Summy acknowledged.</p>   <p>"I'm not sure anyone knows what that ultimate number will be," he said. "But I do think this is going to make a huge dent in that cost ... and you don't have to litigate for the next decade or longer."</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA['Forever chemicals' could be in nearly half of US tap water, federal study finds (212 pts)]]></title>
            <link>https://www.ijpr.org/npr-news/2023-07-06/forever-chemicals-could-be-in-nearly-half-of-u-s-tap-water-a-federal-study-finds</link>
            <guid>36660598</guid>
            <pubDate>Mon, 10 Jul 2023 00:35:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ijpr.org/npr-news/2023-07-06/forever-chemicals-could-be-in-nearly-half-of-u-s-tap-water-a-federal-study-finds">https://www.ijpr.org/npr-news/2023-07-06/forever-chemicals-could-be-in-nearly-half-of-u-s-tap-water-a-federal-study-finds</a>, See on <a href="https://news.ycombinator.com/item?id=36660598">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        
<p>At least 45% of the nation's tap water could be contaminated with at least one form of PFAS known as "forever chemicals," according to a <a href="https://www.usgs.gov/news/national-news-release/tap-water-study-detects-pfas-forever-chemicals-across-us" target="_blank">newly released study</a> by the U.S. Geological Survey.
</p>
<p>The <a href="https://www.atsdr.cdc.gov/pfas/health-effects/overview.html#:~:text=PFAS%20are%20man%2Dmade%20chemicals,grease%2C%20water%2C%20and%20oil." target="_blank">man-made chemicals</a> — of which there are thousands — are found in all sorts of places, from nonstick cookware to stain-resistant carpets to contaminated sources of food and water. They break down very slowly, building up in people, animals and the environment over time.
</p>
<p>Research has linked exposure to certain PFAS to adverse <a href="https://www.epa.gov/pfas/our-current-understanding-human-health-and-environmental-risks-pfas" target="_blank">health effects in humans</a>, from an increased risk of certain cancers, increased obesity and high cholesterol risk, decreased fertility and developmental effects like low birth weight in children.
</p>
<p>"This USGS study can help members of the public to understand their risk of exposure and inform policy and management decisions regarding testing and treatment options for drinking water," Kelly Smalling, a USGS research chemist who is the lead author of the new study released Wednesday, told NPR over email.
</p>
<p>This study is the first to compare PFAS in tap water from both public and private supplies on a broad scale throughout the country, Smalling said.
</p>
<div data-align-center="">
<figure>
    
        
            <picture>
    
    
        
            
    
            <source media="(max-width: 768px)" type="image/webp" width="420" height="324" data-srcset="https://npr.brightspotcdn.com/dims4/default/5d7dd1a/2147483647/strip/true/crop/3303x2548+0+0/resize/840x648!/format/webp/quality/90/?url=https%3A%2F%2Fmedia.npr.org%2Fassets%2Fimg%2F2023%2F07%2F06%2Fpfas-tapwater_detectionmap_1_custom-af104454374c54afd7501ffe23c41b5dfe96cd76.jpg 2x" loading="lazy" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIzMjRweCIgd2lkdGg9IjQyMHB4Ij48L3N2Zz4=" data-size="fallbackImageSizeMobile">
    

    
        <source media="(max-width: 768px)" width="420" height="324" data-srcset="https://npr.brightspotcdn.com/dims4/default/790dcdb/2147483647/strip/true/crop/3303x2548+0+0/resize/420x324!/quality/90/?url=https%3A%2F%2Fmedia.npr.org%2Fassets%2Fimg%2F2023%2F07%2F06%2Fpfas-tapwater_detectionmap_1_custom-af104454374c54afd7501ffe23c41b5dfe96cd76.jpg" loading="lazy" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIzMjRweCIgd2lkdGg9IjQyMHB4Ij48L3N2Zz4=" data-size="fallbackImageSizeMobile">
    

        
    

    
    
        
            
        
    

    
    
        
            
        
    

    
    
        
            
    
            <source type="image/webp" width="880" height="679" data-srcset="https://npr.brightspotcdn.com/dims4/default/91e9918/2147483647/strip/true/crop/3303x2548+0+0/resize/1760x1358!/format/webp/quality/90/?url=https%3A%2F%2Fmedia.npr.org%2Fassets%2Fimg%2F2023%2F07%2F06%2Fpfas-tapwater_detectionmap_1_custom-af104454374c54afd7501ffe23c41b5dfe96cd76.jpg 2x" loading="lazy" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSI2NzlweCIgd2lkdGg9Ijg4MHB4Ij48L3N2Zz4=" data-size="fallbackImageSize">
    

    
        <source width="880" height="679" data-srcset="https://npr.brightspotcdn.com/dims4/default/f8554c4/2147483647/strip/true/crop/3303x2548+0+0/resize/880x679!/quality/90/?url=https%3A%2F%2Fmedia.npr.org%2Fassets%2Fimg%2F2023%2F07%2F06%2Fpfas-tapwater_detectionmap_1_custom-af104454374c54afd7501ffe23c41b5dfe96cd76.jpg" loading="lazy" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSI2NzlweCIgd2lkdGg9Ijg4MHB4Ij48L3N2Zz4=" data-size="fallbackImageSize">
    

        
    

    
    <img alt="This USGS map shows the number of PFAS detected in tap water samples from select sites across the nation." srcset="https://npr.brightspotcdn.com/dims4/default/8f6cb2a/2147483647/strip/true/crop/3303x2548+0+0/resize/1760x1358!/quality/90/?url=https%3A%2F%2Fmedia.npr.org%2Fassets%2Fimg%2F2023%2F07%2F06%2Fpfas-tapwater_detectionmap_1_custom-af104454374c54afd7501ffe23c41b5dfe96cd76.jpg 2x" width="880" height="679" data-src="https://npr.brightspotcdn.com/dims4/default/f8554c4/2147483647/strip/true/crop/3303x2548+0+0/resize/880x679!/quality/90/?url=https%3A%2F%2Fmedia.npr.org%2Fassets%2Fimg%2F2023%2F07%2F06%2Fpfas-tapwater_detectionmap_1_custom-af104454374c54afd7501ffe23c41b5dfe96cd76.jpg" loading="lazy" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSI2NzlweCIgd2lkdGg9Ijg4MHB4Ij48L3N2Zz4=">


</picture>
        
    
    
    
    
    <div>
    

    <div><p> / USGS</p>
            
                <p>/</p>
            
        
            <p>USGS</p></div><figcaption>This USGS map shows the number of PFAS detected in tap water samples from select sites across the nation.</figcaption>
    </div>
    
</figure>
</div>
<p>It involved testing water samples from more than 700 locations across the country during a five-year period, and using that data to model and estimate PFAS contamination nationwide.
</p>
<p>And it comes as the federal government is looking to <a href="https://www.npr.org/2023/03/14/1163497111/new-epa-regulations-target-pfas-in-drinking-water" target="_blank">create new regulations</a> for toxins in drinking water.
</p>
<p>The U.S. Environmental Protection Agency issued <a href="https://www.epa.gov/sdwa/drinking-water-health-advisories-pfoa-and-pfos" target="_blank">drinking water health advisories</a> for two of the most prevalent compounds — PFOA and PFOS — in June 2022, warning that they pose health risks <a href="https://www.npr.org/2022/06/15/1105222327/epa-drinking-water-chemicals-pfas-pfoa-pfos" target="_blank">even at levels so low</a> that the government can't detect them.
</p>
<p>USGS tested for 32 individual PFAS compounds, and <a href="https://www.usgs.gov/news/national-news-release/tap-water-study-detects-pfas-forever-chemicals-across-us" target="_blank">said in a release</a> that the EPA's recent advisories for PFOS and PFOA "were exceeded in every sample in which they were detected in this study."
</p>
<p>While the USGS — which describes itself as an unbiased and impartial science organization — doesn't offer policy recommendations in its report, Smalling points to several key takeaways.
</p>
<p>For one, she said, it highlights the importance of collecting PFAS data from private wells, which are monitored at homeowners' discretion and not regulated by the EPA the way that public sources are.
</p>
<p>It also has implications for the general population.
</p>
<h3>What the study found</h3>

<p>Most state and federal monitoring programs typically measure exposure to PFAS and other pollutants at the water treatment plants or groundwater wells that supply them, Smalling said. Her team took a different approach.
</p>
<p>"The USGS study specifically focused on collecting water directly from a homeowners tap where exposure actually occurs," she explained.
</p>
<p>Between 2016 and 2021, scientists collected samples from 716 residences, businesses and drinking-water treatment plants from a range of protected, rural and urban areas across the U.S.
</p>
<p>Of those, 447 rely on public supplies and 269 on private wells. The researchers found that PFAS concentrations were similar between the two.
</p>
<p>Smalling said they observed the chemicals more frequently in samples collected near urban areas and potential PFAS sources like airports and wastewater treatment plants, which is in line with previous research.
</p>
<p>USGS scientists estimate there's a 75% chance that PFAS will be found in urban areas and a 25% chance in rural areas. And the study suggests that exposure may be more common in certain geographical regions.
</p>
<p>"Results from this study indicate potential hotspots include the Great Plains, Great Lakes, Eastern Seaboard, and Central/Southern California regions," Smalling said.
</p>
<p>The study says its findings support the need for further assessments of the health risks of PFAS both as a class and in combination with other contaminants, "particularly in unmonitored private-wells where information is limited or not available."
</p>
<h3>What can be done</h3>

<p>Smalling says that USGS tap water <a href="https://urldefense.com/v3/__https:/geonarrative.usgs.gov/drinkingwaterinvestigations/__;!!Iwwt!UFDtuzoLoouXCtmxb5jMa3V4ipNSLCJqR-1zAdp6wtNGulPFzXiTp-KUMNPfbPs5LuEEsr6pZSNr0g$" target="_blank">research is continuing</a>, with an emphasis on private well users and rural communities.
</p>
<p>"If the average American is worried about the quality of their drinking water, they can use this and other studies to get informed, evaluate their own [personal] risk and reach out to their local health officials about testing or treatment," she added.
</p>
<p><a href="https://www.epa.gov/pfas/meaningful-and-achievable-steps-you-can-take-reduce-your-risk" target="_blank">The EPA recommends</a> finding out whether PFAS chemicals are in your drinking water, either by calling your local water utility or conducting regular well testing, depending on your source. Then you can compare those numbers to your state's standards for safe levels of PFAS in drinking water (or those in the<b> </b>EPA advisories).
</p>
<p>If you're concerned, the EPA recommends contacting your state environmental protection agency or health department and your local water utility to find out what actions they suggest.
</p>
<p>You could also install <a href="https://www.epa.gov/sciencematters/reducing-pfas-drinking-water-treatment-technologies" target="_blank">specific kinds of water filters</a> that are certified to lower the levels of PFAS in water, using technologies like activated carbon treatment and reverse osmosis.
</p>
<p>Meanwhile, there are federal efforts underway to limit forever chemicals in drinking water.
</p>
<p>In March, the EPA proposed the <a href="https://www.npr.org/2023/03/14/1163341982/epa-forever-chemicals-drinking-water" target="_blank">first federal drinking water limits</a> on six forms of PFAS, which it estimates could reduce PFAS exposure for nearly 100 million Americans.
</p>
<p>The proposed regulations would require water systems to do costly testing and mitigation work and be transparent with their results, as <a href="https://www.npr.org/2023/03/14/1163497111/new-epa-regulations-target-pfas-in-drinking-water" target="_blank">WBUR's Gabrielle Emanuel told NPR</a> at the time.
</p>
<p>And they don't cover the <a href="https://www.cdc.gov/nceh/ehs/water/private-wells/index.html" target="_blank">1 in 8 Americans</a> who get their water from private wells and would generally be responsible for their own testing and filtration, she added.
</p>
<p>Plus, many activists and scientists would like to see regulations that cover more types of PFAS and limit them at the source, rather than clean it them<b> </b>up after the fact.
</p><p>The agency can make changes to its proposal based on public comments before issuing a final rule, which it aims to do by the end of the year. 
</p><p> Copyright 2023 NPR. To see more, visit https://www.npr.org. </p>

                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wayland on OpenBSD (191 pts)]]></title>
            <link>https://xenocara.org/Wayland_on_OpenBSD.html</link>
            <guid>36659859</guid>
            <pubDate>Sun, 09 Jul 2023 22:51:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xenocara.org/Wayland_on_OpenBSD.html">https://xenocara.org/Wayland_on_OpenBSD.html</a>, See on <a href="https://news.ycombinator.com/item?id=36659859">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="doc"><h2 id="introduction"><a href="#introduction" title="introduction"><i></i></a>Introduction</h2><p>These are my notes from experimenting with building Wayland bits on OpenBSD during g2k23 in Tallinn… Thanks to the OpenBSD foundation for organizing this event.</p><p>This is still far from a complete running system as there are many issues on the road, but it’s a good start and it shows that it’s definatly not impossible to get Wayland running on OpenBSD.</p><h3 id="disclaimers"><a href="#disclaimers" title="disclaimers"><i></i></a>Disclaimers</h3><ul>
<li>
<p>I don’t know much about Wayland internals, I’m discovering that on the way, so some stuff here may look naive or totally wrong to experimented Wayland developers…</p>
</li>
<li>
<p>I’m not discussing here on why Wayland ? You may find some answers in <a href="https://homepages.laas.fr/matthieu/talks/app-sec-article.pdf" target="_blank" rel="noopener">this paper</a> I wrote in 2017 (in french).</p>
</li>
<li>
<p>The build instructions below will probably break your system. Use a separate machine for this</p>
</li>
</ul><h2 id="general-ideas"><a href="#general-ideas" title="general-ideas"><i></i></a>General ideas</h2><ul>
<li>I’m using the wlroots library and will focus on compositors (sway, tinywl+…) and applications that use it
<ul>
<li>Gnome (GTK+) based or Qt-based application will wait until someone else builds Wayland support for those toolkits on OpenBSD</li>
</ul>
</li>
<li>The output path is more or less ready with the DRI and Mesa porting efforts (mostly jsg and kettenis)
<ul>
<li>jsg’s github fork of mesa is being used to build it using meson. This may not be needed, but I haven’t checked</li>
</ul>
</li>
<li>“Seat” management is going to be minimal, as OpenBSD doesn’t support this:
<ul>
<li>seatd and libseat provide support for non-systemd based systems. A basic port to OpenBSD/wscons is needed.</li>
</ul>
</li>
<li>Input is more complex to get working since Wayland applications expect Linux input model with udev, evdev and libinput.
<ul>
<li>udev is used to probe available device and handle hot-plugging on Linux. OpenBSD doesn’t have it and the current state of hotplugd(8) doesn’t make it possible to really emulate udev. There is a <code>libudev-openbsd</code> port already that can be used.</li>
<li>evdev is the input event layer of the Linux kernel, similar to wscons_events in OpenBSD. It’s mostly hidden by libinput, but event name translation need the libevdev library for some reason.</li>
<li>libinput is the library that permits reading evdev events from the Linux kernel and  handling many of the higher level interpretation of the events (getting the keyboard mapping, mouse gestures or multi-touch)… It also handles device configuration. mpi did a simple libinput port back in 2015, and rsadowski improved it a bit in 2022. More work is needed.</li>
</ul>
</li>
</ul><p>All this was done on the top of <a href="https://md.tetaneutral.net/xenoports" target="_blank" rel="noopener">xenoports</a>. I’m not sure how well it works on a regular system with <code>/usr/X11R6</code> installed from xenocara.</p><h2 id="detailed-instructions"><a href="#detailed-instructions" title="detailed-instructions"><i></i></a>Detailed instructions</h2><h3 id="mesa"><a href="#mesa" title="mesa"><i></i></a>Mesa</h3><pre><code>git clone -b 22.3-fixes-notls https://github.com/jonathangray/mesa.git
cd mesa
meson setup build -Dgallium-drivers=swrast,iris -Dvulkan-drivers=amd,intel -Ddri-drivers-path=/usr/X11R6/lib/modules/dri -Dcpp_rtti=false -Dprefix=/usr/X11R6
ninja -C build
doas ninja -C build install
</code></pre><h3 id="wayland-ports"><a href="#wayland-ports" title="wayland-ports"><i></i></a>Wayland ports</h3><pre><code>pkg_add wayland wayland-protocols wayland-utils scdoc
</code></pre><p><code>scdoc</code> is not a wayland port, but it’s used to generate manual pages for many projects below</p><p>I’ve a patch to the wayland port to try to fix the search path for system cursors and themes to include <code>/usr/local/share/icons</code> and <code>/usr/X11R6/lib/X11/icons</code> It doesn’t seem to work correctly.</p><h3 id="seatd"><a href="#seatd" title="seatd"><i></i></a>Seatd</h3><p>The original project in on <a href="https://git.sr.ht/~kennylevinsen/seatd" target="_blank" rel="noopener">Source Hut</a></p><p>The OpenBSD port below is very minimal but enough for now. TODO: make it more complete WRT VT switching, using fbtab and some sort of session management.</p><pre><code>git clone -b obsd https://gitlab.tetaneutral.net/mherrb/seatd
cd seatd
meson setup build -Dlibseat-logind=disabled -Dlibseat-builtin=enabled
ninja -C build
doas ninja -C build install
</code></pre><h3 id="libudev-openbsd"><a href="#libudev-openbsd" title="libudev-openbsd"><i></i></a>libudev-openbsd</h3><pre><code>pkg_add libudev-openbsd
</code></pre><h3 id="libinput-openbsd"><a href="#libinput-openbsd" title="libinput-openbsd"><i></i></a>libinput-openbsd</h3><p>This is the version from mpi@, patched@ by radowski, plus extra udev code from me.<br>
I’ve now added proper events translation (for mouse buttons and keyboard) so that the input works in a usable way in Wayland.</p><pre><code>git clone https://github.com/mherrb/libinput.git
cd libinput-openbsd
make
doas make install
</code></pre><h3 id="libevdev-openbsd"><a href="#libevdev-openbsd" title="libevdev-openbsd"><i></i></a>libevdev-openbsd</h3><p>This is a stripped-down version of libevdev, providing just the few functions to handle name to symbol conversions that are used by most Wayland applications.</p><pre><code>git clone https://gitlab.tetaneutral.net/mherrb/libevdev-openbsd.git
cd libevdev-openbsd
make
doas make install
</code></pre><h3 id="libdisplay-info-and-libliftoff"><a href="#libdisplay-info-and-libliftoff" title="libdisplay-info-and-libliftoff"><i></i></a>libdisplay-info and libliftoff</h3><p>Those two small helper libraries build of of the box on OpenBSD. I’ve made ports for them in my xenoports tree, or they can be built from gitlab:</p><ul>
<li><a href="https://gitlab.freedesktop.org/emersion/libdisplay-info" target="_blank" rel="noopener">https://gitlab.freedesktop.org/emersion/libdisplay-info</a> (<a href="https://cgit.xenocara.org/xenoports/tree/sysutils/libdisplay-info" target="_blank" rel="noopener">port</a>)</li>
<li><a href="https://gitlab.freedesktop.org/emersion/libliftoff" target="_blank" rel="noopener">https://gitlab.freedesktop.org/emersion/libliftoff</a> (<a href="https://cgit.xenocara.org/xenoports/tree/graphics/libliftoff" target="_blank" rel="noopener">port</a>)</li>
</ul><h3 id="wlroots"><a href="#wlroots" title="wlroots"><i></i></a>wlroots</h3><p>A version of wlroots patched for OpenBSD. The wscons backend code can dropped for now. I’m not sure if it will ever be possible to have Wayland applications running without libinput (but that’s the goal of this backend).</p><pre><code>git clone -b obsd https://gitlab.freedesktop.org/mherrb/wlroots.git
cd wlroot
meson setup build
ninja -C build
doas ninja -C build install
</code></pre><h3 id="havoc"><a href="#havoc" title="havoc"><i></i></a>havoc</h3><p>The simplest terminal emulator application for Wayland that I could find and that builds on OpenBSD with minimal changes.</p><pre><code>git clone -b obsd https://github.com/mherrb/havoc.git
cd havoc
meson setup build
ninja -C build
doas ninja -C build install
</code></pre><h3 id="swayimg"><a href="#swayimg" title="swayimg"><i></i></a>swayimg</h3><p>An image viewer for Wayland that builds on OpenBSD without patches \o/</p><pre><code>git clone https://github.com/artemsen/swayimg.git
cd swayimg 
meson setup build
ninja -C build
doas -C build ninja install
</code></pre><h3 id="sway"><a href="#sway" title="sway"><i></i></a>sway</h3><p>A real compositor, compatible with i3</p><pre><code>git clone -b obsd https://github.com/mherrb/sway.git
cd sway
meson setup build
ninja -C build
doas -C build ninja install
</code></pre><h3 id="default-cursor-theme"><a href="#default-cursor-theme" title="default-cursor-theme"><i></i></a>Default cursor theme</h3><p>Wayland doesn’t provide cursors by itself. So one needs to install the <code>gnome-default-theme</code> package or any other package providing a default theme.<br>
Because of the cursor path mentionned above, link /usr/local/share/icons to <code>~/.icons</code> for now to get cursors.</p><p>Note: swaybar will segfault if it’s missing a cursor…</p><h3 id="fonts"><a href="#fonts" title="fonts"><i></i></a>Fonts</h3><p>Wayland is using <code>fontconfig</code>. So it can use any fonts installed via <code>pkg_add</code> swaybar expects the terminus font to be available.</p><h3 id="running-wayland"><a href="#running-wayland" title="running-wayland"><i></i></a>Running Wayland</h3><p>One needs to set a number of environment variables. The following script (<code>runwl</code>) helps:</p><pre><code>#! /bin/ksh
export XDG_RUNTIME_DIR=/tmp 
export WAYLAND_DISPLAY=wayland-0
exec $*
</code></pre><ul>
<li>
<p>Start seatd</p>
<p>This gives the /var/run/seatd.sock to my user, so that the applications will be run as my uid, not root.</p>
<pre><code> doas seatd -u $(whoami) -l debug
</code></pre>
</li>
<li>
<p>Start sway:</p>
<pre><code> export XDG_RUNTIME_DIR=/tmp 
 export WLR_DRM_DEVICES=/dev/dri/card0
 sway
</code></pre>
</li>
<li>
<p>start an application (here havoc)</p>
<pre><code> runwl havoc
</code></pre>
</li>
</ul><h3 id="xwayland"><a href="#xwayland" title="xwayland"><i></i></a>Xwayland</h3><p>A port is available in xenoports: <a href="https://cgit.xenocara.org/xenoports/tree/wayland/xwayland" target="_blank" rel="noopener">here</a></p><p>Sway starts Xwayland automatically if an application needs X</p><p>It can also be started manually, but not in rootless mode (this is a Wayland security feature). X applications work in the Xwaylan in windowed mode, but OpenGL X applications crash (probably because of mismatched Mesa versions in my builds FIXME</p><h2 id="more-work-to-do"><a href="#more-work-to-do" title="more-work-to-do"><i></i></a>More work to do</h2><h3 id="gtk-applications"><a href="#gtk-applications" title="gtk-applications"><i></i></a>Gtk applications</h3><p>Simple patches allows to build gtk+3 and gtk+4 with wayland support enabled. gtk3-demo works, built against the patched version.</p><p>Note that both emacs and mozilla-firefox work, but still use Xwayland. The ports need some plist and lib-depends patches once wayland support is enabled in gtk+3/4.</p><p>So far gtk4-demos fails to start, it still tries to connect to an X serer and segfault. FIXME</p><h3 id="qt-applications"><a href="#qt-applications" title="qt-applications"><i></i></a>Qt applications</h3><p>Qt 5 and 6 are normally built with Wayland support. I tried keepassxc(1) (Qt5) which could’nt connect to the compositor. qt6/qtbase doesn’t build on my system from some reason. FIXME</p><h3 id="ltucharhgt-and-char32_t"><a href="#ltucharhgt-and-char32_t" title="ltucharhgt-and-char32_t"><i></i></a>&lt;uchar.h&gt; and char32_t</h3><p>A number of Wayland applications use UTF-32 internally and require support for the above that it not (yet) available on OpenBSD. TODO: one implementation exists in DragonFlyBSD.<br>
Thay would probably unlock <a href="https://codeberg.org/dnkl/foot.git" target="_blank" rel="noopener">foot</a> and others.</p><h3 id="swaylock"><a href="#swaylock" title="swaylock"><i></i></a>swaylock</h3><p>To get a screen locker, swaylock needs to be tought about BSD auth to replace PAM.</p><h3 id="pledge-and-unveil"><a href="#pledge-and-unveil" title="pledge-and-unveil"><i></i></a>Pledge and Unveil</h3><p>None of the above currenty implement pledge() or unveil() support, nor any other from of sandboxing afaict. This could be looked at at some point.</p><h2 id="known-issues"><a href="#known-issues" title="known-issues"><i></i></a>Known issues</h2><p>Sway will crash the kernel during startup from time to time. I don’t know if it’s a problem specific to the dri driver for the Intel iris (12th gen) GPU in my test laptop or a general bug. TODO: test on a machine with a serial console to get a backtrace if possible.</p><hr><p>Last modified: 2023-07-09T07:06:00</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PinePhone Modem SDK (187 pts)]]></title>
            <link>https://github.com/the-modem-distro/pinephone_modem_sdk</link>
            <guid>36659544</guid>
            <pubDate>Sun, 09 Jul 2023 22:18:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/the-modem-distro/pinephone_modem_sdk">https://github.com/the-modem-distro/pinephone_modem_sdk</a>, See on <a href="https://news.ycombinator.com/item?id=36659544">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Pinephone Modem SDK</h2>
<h3 tabindex="-1" dir="auto">(nearly) Free custom firmware for your Pinephone's modem!</h3>
<p dir="auto">This repository contains everything you need to make your own Modem userspace for your Pinephone.</p>
<h3 tabindex="-1" dir="auto">Latest release: <a href="https://github.com/Biktorgj/pinephone_modem_sdk/releases/latest">Version 0.7.4</a></h3>
<h3 tabindex="-1" dir="auto">Supported devices:</h3>
<ul dir="auto">
<li>Pinephone</li>
<li>Pinephone Pro</li>
<li>EG25-G connected via USB audio</li>
</ul>
<ul dir="auto">
<li><a href="https://github.com/the-modem-distro/pinephone_modem_sdk/blob/kirkstone/docs/FLASHING.md">Flashing guide</a></li>
<li><a href="https://github.com/the-modem-distro/pinephone_modem_sdk/blob/kirkstone/docs/SETTINGS.md">Recommended settings for this firmware</a></li>
<li><a href="https://github.com/the-modem-distro/pinephone_modem_sdk/blob/kirkstone/docs/HOWTO.md">Build your own</a></li>
<li><a href="https://github.com/the-modem-distro/pinephone_modem_sdk/blob/kirkstone/docs/RECOVERY.md">Returning back to stock</a></li>
<li>Having issues? <a href="https://github.com/Biktorgj/pinephone_modem_sdk/issues">Check out if the issue is already documented or create a new one</a></li>
<li><a href="https://matrix.to/#/#pinephone_modem_sdk-issue-9:matrix.org" rel="nofollow">We also have a Matrix room!</a></li>
</ul>
<h4 tabindex="-1" dir="auto">Current Status:</h4>
<ul dir="auto">
<li>LK Bootloader: Working
<ul dir="auto">
<li>On reset, the bootloader enters into fastboot mode automatically for 5 seconds, and boots normally unless instructed to stay (leave the command <code>fastboot oem stay</code> running while rebooting the modem to make it stop at fastboot).</li>
<li>Custom fastboot commands:</li>
<li>fastboot reboot-bootloader: Reboot to fastboot</li>
<li>fastboot oem stay: Stay in fastboot instead of booting normally</li>
<li>fastboot oem reboot-recovery: Reboot to recovery mode</li>
</ul>
</li>
<li>CAF Kernel: Working</li>
<li>Audio: Working <a href="https://github.com/the-modem-distro/pinephone_modem_sdk/blob/kirkstone/docs/SETTINGS.md">Check out recommended settings for your phone</a></li>
<li>SMS: Working</li>
<li>GPS: Working</li>
<li>Sleep / Power management: Working (New current measurement and profiling required after latest changes)</li>
<li>System images:
<ul dir="auto">
<li>root_fs: Default system image. Includes a minimal root filesystem and one application replacing the entire Qualcomm / Quectel stack. Some functions may not yet functional</li>
<li>recovery_fs: Minimal bootable image to be flashed into the recovery partitions to retrieve logs and make changes to the root image</li>
</ul>
</li>
<li>Custom AT Commands: Please see this <a href="https://github.com/the-modem-distro/pinephone_modem_sdk/blob/kirkstone/docs/AT_INTERFACE.md#custom-commands-in-this-firmware">document</a></li>
</ul>
<h4 tabindex="-1" dir="auto">Features not available on stock firmware:</h4>
<ul dir="auto">
<li>Signal tracking support (beta): Get notified whenever your modem changes towers, check against the OpenCellid database, and make it shutdown when it connects to an unknown cell</li>
<li>Network data export as csv files</li>
<li>Set a reminder on the modem and it will call you back and speak the message you sent it (perfect for boring meetings!)</li>
<li>Call recording support (manual or automatic)</li>
<li>Cell broadcast relay to the host as SMS</li>
<li>Internal call and SMS support
<ul dir="auto">
<li>Functionality is added on every release, check out the <a href="https://github.com/the-modem-distro/pinephone_modem_sdk/blob/kirkstone/docs/SMS_INTERFACE.md">document for available commands</a></li>
</ul>
</li>
<li>Optional persistent storage: By default an unexpected shutdown can't mess your modem
<ul dir="auto">
<li>If you want to keep logs after reboot, you can enable persistent storage</li>
</ul>
</li>
<li>SMS logging capability: It can log every message you send or receive</li>
<li>Automatic time synchronization from the carrier into the userspace</li>
<li>Minimum clock frequency is set to 100Mhz, either awake or sleeping (stock is 800MHz awake and 400Mhz sleep), making the modem run cooler</li>
<li>Different sampling rates available at runtime without requiring a reboot (missing companion app in the pinephone to make use of them)</li>
<li>0 binary blobs in the userspace. Only closed source running on the modem are TZ Kernel and ADSP firmware</li>
</ul>
<h4 tabindex="-1" dir="auto">TODO (in no particular order)</h4>
<ol dir="auto">
<li>[WIP] Find and fix the last remaining USB port reset cause(s)</li>
<li>[Testing] Fix audio when doing conferences (audio is cut off when hanging up the first call)</li>
<li>[WIP] Internal SMS functionality (Working reliably with ModemManager and in testing with oFono):</li>
</ol>
<ul dir="auto">
<li>Can send and receive messages to/from the modem</li>
<li>Modem will answer to the number: +22 33 44 55 66 77</li>
<li>Send "help" for a list of commands or check the <a href="https://github.com/the-modem-distro/pinephone_modem_sdk/blob/kirkstone/docs/SMS_INTERFACE.md">docs</a></li>
</ul>
<ol start="4" dir="auto">
<li>[WIP] Internal call ability (Working with ModemManager / testing with oFono):</li>
</ol>
<ul dir="auto">
<li>Can accept outgoing calls or automatically call you when requested from the chat (send "call me" or "call me in X" -seconds- to make it call you)</li>
<li>TTS support: While in call, send an SMS to the modem and it will speak the response back</li>
</ul>
<p dir="auto">Contribution is always welcome! Feel free to share any issue or something that you think may be interesting to have!</p>
<h4 tabindex="-1" dir="auto">Related Repositories</h4>
<p dir="auto">This project depends on the following repositories:</p>
<ul dir="auto">
<li><a href="https://github.com/Biktorgj/lk2nd">LK2nd fork with a few patches</a></li>
<li><a href="https://github.com/Biktorgj/quectel_eg25_kernel">Downstream 3.18.140 Kernel based on CAF</a></li>
<li><a href="https://github.com/Biktorgj/meta-qcom">Forked meta-qcom repository</a></li>
<li><a href="https://yoctoproject.org/" rel="nofollow">The Yocto Project</a></li>
<li><a href="https://github.com/Biktorgj/quectel_eg25_recovery">Quectel EG25 firmware repo</a></li>
</ul>
<h4 tabindex="-1" dir="auto">Documentation</h4>
<p dir="auto">I'm really bad at documentation, but you have some docs <a href="https://github.com/the-modem-distro/pinephone_modem_sdk/blob/kirkstone/docs">here</a></p>
<h4 tabindex="-1" dir="auto">Donations</h4>
<p dir="auto">If you want, you can buy me a coffee <a href="https://ko-fi.com/biktorgj" rel="nofollow">ko-fi</a>/<a href="https://liberapay.com/biktorgj/donate" rel="nofollow">liberapay</a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It Took Me a Decade to Find the Perfect Personal Website Stack – Ghost+Fathom (133 pts)]]></title>
            <link>https://davidgomes.com/it-took-me-a-decade-to-find-the-perfect-personal-website-stack/</link>
            <guid>36659219</guid>
            <pubDate>Sun, 09 Jul 2023 21:41:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://davidgomes.com/it-took-me-a-decade-to-find-the-perfect-personal-website-stack/">https://davidgomes.com/it-took-me-a-decade-to-find-the-perfect-personal-website-stack/</a>, See on <a href="https://news.ycombinator.com/item?id=36659219">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

        




<main id="site-main" role="main">
    <div>

        <article>

            


            <section>
                <p>The first post on this personal blog dates <a href="https://davidgomes.com/not-space-invaders-ludum-dare-25/">back to December 2012</a>. It’s been more than a decade! As with many others, I’ve always struggled with how to setup my personal website. It is just too enticing to build it “from scratch”, which usually means using a static site generator. As such, my website has been powered by <a href="https://gohugo.io/?ref=davidgomes.com">Hugo</a> at some point, and before that by <a href="https://jekyllrb.com/?ref=davidgomes.com">Jekyll</a>. I’ve dabbled with a few other things as well, including building it <strong><strong><strong><strong>entirely</strong></strong></strong></strong> from scratch in raw HTML/CSS. Suffice it to say, I wasn’t writing a lot of blog posts back then as I was too busy with getting the website to work properly across a myriad of devices and browser technologies.</p><p>In 2019, I just gave all of this up and adopted <a href="https://ghost.org/?ref=davidgomes.com">Ghost</a>. And to embarrass myself further, I’m not even self hosting it from a Raspberry Pi or a <a href="https://www.reddit.com/r/HomeServer?ref=davidgomes.com">home server</a>. I actually <em>pay</em> Ghost roughly 200 euros per year for them to manage everything for me! I’ve been a happy customer for four years, which is much longer than any previous iteration of my personal website ever lasted.</p><h3 id="how-does-ghost-work-and-is-it-actually-%E2%80%9Cperfect%E2%80%9D">How does Ghost work? And is it actually “perfect”?</h3><p>When it comes to writing new blog posts, it’s really straightforward. I just write them in <a href="https://www.notion.so/?ref=davidgomes.com">Notion</a>, and then I copy-paste them into the Ghost editor and hit “Publish”. I tried using the native editor a few times, and it is pretty decent, but Notion’s is simply better (plus, Notion works on mobile which is a plus for me).</p><p>I have a custom theme which is a <a href="https://github.com/davidgomes/my-gasper-fork?ref=davidgomes.com">fork of one of their default themes</a>, and every now and then I have to hack some CSS together to fix some bugs. For some reason, the theme can only be compiled with an ancient version of npm and the code is really messy, so at some point I won’t be able to maintain it any longer. Moreover, images don’t work very well with my theme, so I might have a look at Ghost’s <a href="https://ghost.org/themes/?ref=davidgomes.com">marketplace</a> soon to purchase a better theme.</p><p>Besides this, I don’t have many other interactions with Ghost’s admin panel. I also use it to edit my non-blog pages, and that’s about it!</p><h3 id="and-what-about-analytics">And what about analytics?</h3><p>For many years, I was using Google Analytics to track how many people visited my website over time, as well as where they were coming from. The UI/UX for the Google Analytics dashboard is really not very good, and I definitely have some concerns over its privacy implications. So, I recently decided to adopt a proper analytics service called “<a href="https://usefathom.com/?ref=davidgomes.com">Fathom</a>”.</p><p>I first heard about Fathom because they’re a SingleStoreDB customer, but I’ve been following <a href="https://twitter.com/JackEllis?ref=davidgomes.com">Jack Ellis</a> (co-founder &amp; CTO of Fathom) on Twitter for a while now too. From reading his tweets over time, it seemed to me like he’s obsessed with his customers and building a great product for them. But when I actually tried Fathom for the first time, I was mesmerized — the onboarding experience is <em>phenomenal</em>. The time it took from signing up to having access to its dashboard with a full import of my Google Analytics history was under 8 minutes.</p><!--kg-card-begin: html--><center><blockquote><p lang="en" dir="ltr">I'm out of words. I just signed up for <a href="https://twitter.com/usefathom?ref_src=twsrc%5Etfw&amp;ref=davidgomes.com">@usefathom</a>, added their code snippet to my website and went through their GA Importer. I figured this process would be easier and better than migrating to GA4. Guess what?</p>— David Gomes (@davidrfgomes) <a href="https://twitter.com/davidrfgomes/status/1676241780969205761?ref_src=twsrc%5Etfw&amp;ref=davidgomes.com">July 4, 2023</a></blockquote> </center><!--kg-card-end: html--><p>The UI/UX is really nice, and exploring all the data is very easy. Furthermore, they’re extremely <a href="https://usefathom.com/privacy-focused-web-analytics?ref=davidgomes.com">privacy-focused</a>, and no cookies are used at all! Here's a sneak peek into the Fathom dashboard UI:</p><!--kg-card-begin: html--><img alt="My view of the Fathom dashboard" src="https://davidgomes.com/content/images/size/w1000/2023/07/Screenshot-from-2023-07-09-22-15-21.png"><!--kg-card-end: html--><p>So, all in all, I’m paying almost 400 euros per year to host a personal blog. That’s quite a lot, but I see it as an incentive to get me to write my more, so it’s all good.</p><p>Feel free to <a href="https://twitter.com/davidrfgomes?ref=davidgomes.com">reach out on Twitter</a>!</p>
            </section>



        </article>

    </div>
</main>





    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why engineers should focus on writing (222 pts)]]></title>
            <link>https://www.yieldcode.blog/post/why-engineers-should-write/</link>
            <guid>36659166</guid>
            <pubDate>Sun, 09 Jul 2023 21:35:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.yieldcode.blog/post/why-engineers-should-write/">https://www.yieldcode.blog/post/why-engineers-should-write/</a>, See on <a href="https://news.ycombinator.com/item?id=36659166">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
                        <p>All engineers are good writers… of code. But I believe that in order to become better engineer–you should improve your writing skills.</p>
<!--more-->
<p>From the dawn of times, people were writing. We have written using symbols, like in Ancient Egypt. And we have written using letters, like in Renaissance times. And all of us, got at-least one writing assignment in school, without the “Why?” And yet, today writing is so underrated, that most people want to avoid it. But the truth is–you will have to write. Comments, documentation, design documents, presentations. Whether you like it or not. So why not become better at it?</p>
<h2 id="why-write">Why write?</h2>
<h3 id="writing-is-a-way-to-organize-your-brain">Writing is a way to organize your brain</h3>
<p>For some reason, we humans, think that knowledge equals reading stuff. I’ve read a starting guide for Rust, so now I know this topic. I’ve read a book about how TCP/IP works–so now I know this stuff. But it’s not true. If it was true, we’d all be super-stars.</p>
<p>What helps us cement our knowledge–is writing. That’s why I believe in <strong>writing</strong> code, as opposed to copying code snippets. Because when you type it–you cement that knowledge.</p>
<h3 id="writing-is-a-way-to-learn-something">Writing is a way to learn something</h3>
<p>If you want to learn a new topic–write about it. When I want to learn a new programming language, I write a short program using that programming language. When I want to understand how something works–I write an article in this blog about it.</p>
<p>Real writing, where you try to dig the truth, will require you to try and experiment. I’ve learned more by writing articles on this blog, than by reading programming books.</p>
<h3 id="writing-helps-you-identify-mistakes">Writing helps you identify mistakes</h3>
<p>How many times we are asked to prepare a design document, while we think to ourselves “What for? The design is so simple, I can hold it in my head easily.” This is a big misconception, we all fall for. If we would actually write the design document, we would have identified so much problems with our “simple” design. Things like–inconsistencies, missing details, or simply “lazy” thinking our brain did, which in reality make to sense.</p>
<h2 id="how-to-write-more">How to write more?</h2>
<p>Remember–reading is a habit, writing is a skill. And in order to prefect your skill, you need to write more. One simple way to write more–is to approach design reviews in a different way. Instead of hating them, and doing them like a homework assignment, try to approach them with enthusiasm. Each time you write a design review, try to improve something. Make it shorter–without missing the point. Make it longer–in order to cover more use-cases.</p>
<p>Writing design reviews, and documentations–at your workplace–is an easy way to get into writing. You will have to do it anyway, so why not improve while writing? However, if you want to improve your writing further, outside your workplace, consider starting a blog. If blog is intimidating for you, consider answering question on Stackoverflow–but focus on providing textual content, rather than copy-pasting code snippets.</p>
<p>One last advice–abolish the copy-paste. So many developers, whom I mentored, simply copy-paste everything. Code snippets, function declarations, etc. I know how to initialize a git repository, because I do it by hand every time. Most people simply copy the instructions from Github or Google. And if you are scared about being unproductive–remember that you are not judged by how much code you write, or how fast you complete the assignment.</p>
<hr>
<p>Writing is unavoidable. Whether you like it or not, if you want to become a 10x engineer, you will have to write. Write documentations, write design reviews, write presentations. The sooner you get better in writing, the faster you will become a better engineer.</p>
                    
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Clojure Plays Mario (191 pts)]]></title>
            <link>https://blog.phronemophobic.com/mairio.html</link>
            <guid>36658959</guid>
            <pubDate>Sun, 09 Jul 2023 21:14:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.phronemophobic.com/mairio.html">https://blog.phronemophobic.com/mairio.html</a>, See on <a href="https://news.ycombinator.com/item?id=36658959">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Date: June 22nd, 2023</p><h2 id="Introduction">Introduction</h2><p>Before getting too far into the weeds, let's begin with the results. Without too much effort, an AI was written in clojure that could complete all of the levels in the original Super Mario Bros for the NES except for the Bowser levels that have mazes. Namely, levels 4-4, 7-4, and 8-4 were not completed (more on this later).</p><p>Here's what a solved level looks like:</p><p>Yup. It's just Mario casually completing the level without any mortal fear and complete disregard for contrivances like mushrooms, coins, or points.</p><h2 id="Implementation">Implementation</h2><p>The main part of the implementation is the following <code>solve</code> function which we will break down.</p><div><pre><span>(</span><span>defn</span> <span>solve</span>
  <span>"Given a `start` state, a distance estimating function,
  a `successors` function, and a function that says when we're `done`,
  Try to find a path from the start to the solution."</span>
  <span>(</span><span>[</span><span>start</span> <span>dist</span> <span>successors</span> <span>done?</span><span>]</span>
   <span>(</span><span>solve</span> <span>(</span><span>priority-map</span> <span>start</span> <span>(</span><span>dist</span> <span>start</span><span>)</span><span>)</span>
          <span>dist</span>
          <span>successors</span>
          <span>done?</span>
          <span>#{</span><span>start</span><span>}</span><span>)</span><span>)</span>
  <span>(</span><span>[</span><span>queue</span> <span>distf</span> <span>successorsf</span> <span>done?</span> <span>visited</span><span>]</span>
   <span>(</span><span>if</span> <span>(</span><span>not</span> <span>(</span><span>Thread/interrupted</span><span>)</span><span>)</span>
     <span>(</span><span>when-let</span> <span>[</span><span>[</span><span>node</span> <span>dist</span><span>]</span>
                <span>(</span><span>with-lookback</span> <span>queue</span> <span>(</span><span>-&gt;coord</span> <span>1</span> <span>0</span> <span>0</span><span>)</span><span>)</span><span>]</span>
       <span>(</span><span>let</span> <span>[</span><span>[</span><span>not-done?</span> <span>dead?</span> <span>below-viewport?</span> <span>xpos</span> <span>speed</span> <span>frames</span><span>]</span> <span>dist</span><span>]</span>
        <span>(</span><span>swap!</span> <span>stats</span>
               <span>assoc</span>
               <span>:queue-count</span> <span>(</span><span>count</span> <span>queue</span><span>)</span>
               <span>:dist</span> <span>dist</span><span>)</span><span>)</span>
       <span>(</span><span>if</span> <span>(</span><span>done?</span> <span>dist</span><span>)</span>
         <span>node</span>
         <span>(</span><span>let</span> <span>[</span><span>successors</span> <span>(</span><span>successorsf</span> <span>node</span><span>)</span>
               <span>queue</span> <span>(</span><span>-&gt;</span> <span>queue</span>
                         <span>(</span><span>into</span> <span>(</span><span>for</span> <span>[</span><span>successor</span> <span>successors</span>
                                     <span>:when</span> <span>(</span><span>not</span> <span>(</span><span>contains?</span> <span>visited</span> <span>successor</span><span>)</span><span>)</span><span>]</span>
                                 <span>[</span><span>successor</span> <span>(</span><span>distf</span> <span>successor</span><span>)</span><span>]</span><span>)</span><span>)</span>
                         <span>(</span><span>dissoc</span> <span>node</span><span>)</span><span>)</span>
               <span>visited</span> <span>(</span><span>into</span> <span>visited</span> <span>successors</span><span>)</span><span>]</span>
           <span>(</span><span>recur</span> <span>queue</span>
                  <span>distf</span>
                  <span>successorsf</span>
                  <span>done?</span>
                  <span>visited</span><span>)</span><span>)</span><span>)</span><span>)</span>
     <span>{</span><span>:fail</span> <span>true</span>
      <span>:queue</span> <span>queue</span>
      <span>:visited</span> <span>visited</span><span>}</span><span>)</span><span>)</span><span>)</span>
</pre></div><p>There's a few extra steps we'll ignore, but the main steps are:</p><ol><li><p>If we're done, return the result.</p></li><li><p>Otherwise, choose a node to explore.</p></li><li><p>Derive new nodes (ie. successors) from the node we picked.</p></li><li><p><code>recur</code> back to the beginning and try again.</p></li></ol><p>That's basically it. The only real magic is deciding where to explore. Usually, you want to be "greedy" and just explore closest to your goal, but if you're too greedy, then you might get stuck with no way to continue to greedily move forward.</p><p><img alt="Mario stuck in a location where he needs to backtrack to continue." src="https://blog.phronemophobic.com/mairio/stuck.png"></p><p>In this scenario, you need to backtrack to a previous state that does allow you to progress. The node is chosen using the <code>with-lookback</code> function:</p><div><pre><span>(</span><span>when-let</span> <span>[</span><span>[</span><span>node</span> <span>dist</span><span>]</span>
           <span>(</span><span>with-lookback</span> <span>queue</span> <span>(</span><span>-&gt;coord</span> <span>1</span> <span>0</span> <span>0</span><span>)</span><span>)</span><span>]</span>
  <span>...</span><span>)</span>
</pre></div><p>The <code>with-lookback</code> function picks a random distance between 0 and <code>(-&gt;coord 1 0 0)</code> , and picks the "best" exploration so far that is at least that many units away from the farthest exploration. What distance does <code>(-&gt;coord 1 0 0)</code> represent? I won't cover too much about Super Mario Bros' coordinate system, but <code>(-&gt;coord 1 0 0)</code> translates to exactly one screen's distance. Essentially, our algorithm can backtrack up to one screen from our furthest exploration.</p><p>Revisiting the levels the AI could not complete (ie. 4-4, 7-4, and 8-4). All of these levels have a maze mechanic where if you go the wrong route, then you will be transported back 4 screens. Since getting transported 4 screens back is beyond the horizon of our backtracking, there's no way our algorithm will ever complete these levels unless they get lucky and choose the correct path on the first try.<sup><a href="#footnote-1" name="footnote-ref-1" title="They were not so lucky.">1</a></sup></p><p>Here's what the process looks like. Below is the sequence of exploring using our solver. You can see each alternate reality explored and how the solver slowly progresses towards the end of the level. It takes a while for Mario to figure out he has to jump up onto each ledge and over the goomba, but he eventually figures it out and proceeds onto the next obstacle.</p><p>So yea. The solver is relatively straightforward. Randomly press buttons until you make it to the end of the level. If you get stuck, use save states to backtrack.</p><p>It's not super efficient, but it usually doesn't take too long to "solve" a level and it's kinda fun to watch it go. For reference, it takes about two minutes to solve level 1-1. Some of the platforming levels take a bit longer. The two water levels (ie. 2-2, 7-2) have the same layout and take quite a while to solve. Our AI is not a good swimmer.</p><h2 id="Successors">Successors</h2><p>Emulating the next state from our current step is pretty straightforward. We generate all combinations of pressing or not pressing the A and B buttons along with all the combinations of pressing left, right, or no direction. For each particular combination, we pick a random number of frames in the range 1-60 (up to one second) to hold this configuration and then simulate the result. It's important for our successor function to generate button presses for several frames because a common platforming action is long jumping (ie. holding the jump button for a full jump). If successor states were generated for each frame, then the common action of holding the jump button for a few seconds is extremely unlikely.</p><p>While it's possible for Mario to enter a pipe by pressing down, we ignore that for our purposes. I'm not sure the up button has any use in Super Mario Bros so we also ignore that button when generating successor states.</p><p>Below is the code for generating the successor states with some debug code elided for brevity.</p><div><pre><span>(</span><span>def</span> <span>buttons</span>
  <span>[</span><span>:RETRO_DEVICE_ID_JOYPAD_A</span>
   <span>:RETRO_DEVICE_ID_JOYPAD_B</span><span>]</span><span>)</span>
<span>(</span><span>def</span> <span>directions</span>
  <span>[</span><span>:RETRO_DEVICE_ID_JOYPAD_LEFT</span>
   <span>:RETRO_DEVICE_ID_JOYPAD_RIGHT</span>
   <span>nil</span><span>]</span><span>)</span>

<span>(</span><span>defn</span> <span>^</span><span>:private</span> <span>next-state</span>
  <span>"Given a sequence of past inputs, run the current game for `num-frames` with `controls` inputs set."</span>
  <span>[</span><span>inputs</span> <span>controls</span> <span>num-frames</span><span>]</span>
  <span>(</span><span>let</span> <span>[</span><span>state</span> <span>(</span><span>get</span> <span>@</span><span>state-cache</span> <span>inputs</span><span>)</span><span>]</span>
    <span>(</span><span>assert</span> <span>state</span><span>)</span>
    <span>(</span><span>load-state</span> <span>state</span><span>)</span><span>)</span>
  <span>(</span><span>with-redefs</span> <span>[</span><span>input-state-callback</span>
                <span>(</span><span>fn</span> <span>[</span><span>port</span> <span>device</span> <span>index</span> <span>id</span><span>]</span>
                  <span>(</span><span>if</span> <span>(</span><span>controls</span> <span>(</span><span>c/device-name</span> <span>id</span><span>)</span><span>)</span>
                    <span>1</span>
                    <span>0</span><span>)</span><span>)</span><span>]</span>
    <span>(</span><span>dotimes</span> <span>[</span><span>i</span> <span>num-frames</span><span>]</span>
      <span>(</span><span>retro/retro_run</span><span>)</span><span>)</span><span>)</span>
  <span>(</span><span>let</span> <span>[</span><span>new-state</span> <span>(</span><span>get-state</span><span>)</span>
        <span>new-inputs</span> <span>(</span><span>conj</span> <span>inputs</span>
                         <span>{</span><span>:controls</span> <span>controls</span>
                          <span>:num-frames</span> <span>num-frames</span><span>}</span><span>)</span><span>]</span>
    <span>(</span><span>swap!</span> <span>state-cache</span> <span>assoc</span> <span>new-inputs</span> <span>new-state</span><span>)</span>
    <span>new-inputs</span><span>)</span><span>)</span>

<span>(</span><span>defn</span> <span>successors</span>
  <span>"Generates successor states given past inputs."</span>
  <span>[</span><span>inputs</span><span>]</span>
  <span>(</span><span>into</span> <span>[</span><span>]</span>
        <span>(</span><span>comp</span>
         <span>(</span><span>map</span> <span>(</span><span>fn</span> <span>[</span><span>[</span><span>buttons</span> <span>dir</span><span>]</span><span>]</span>
                <span>(</span><span>set</span>
                 <span>(</span><span>if</span> <span>dir</span>
                   <span>(</span><span>conj</span> <span>buttons</span> <span>dir</span><span>)</span>
                   <span>buttons</span><span>)</span><span>)</span><span>)</span><span>)</span>
         <span>(</span><span>map</span> <span>(</span><span>fn</span> <span>[</span><span>controls</span><span>]</span>
                <span>(</span><span>next-state</span> <span>inputs</span> <span>controls</span>
                            <span>(</span><span>inc</span> <span>(</span><span>rand-int</span> <span>60</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span>
        <span>(</span><span>combo/cartesian-product</span>
           <span>(</span><span>combo/subsets</span> <span>buttons</span><span>)</span>
           <span>directions</span><span>)</span><span>)</span><span>)</span>
</pre></div><h2 id="Distance">Distance</h2><p>The solver accepts a <code>dist</code> function that should return a comparable heuristic value. For our implementation, it doesn't matter what the heuristic returns as long as it can be compared against other values returned from <code>dist</code>.</p><div><pre><span>(</span><span>defn</span> <span>dist</span>
  <span>"Checks the current RAM and estimates the current distance from the goal."</span>
  <span>[</span><span>inputs</span><span>]</span>
  <span>(</span><span>let</span> <span>[</span><span>save-state</span> <span>(</span><span>get</span> <span>@</span><span>state-cache</span> <span>inputs</span><span>)</span><span>]</span>
    <span>(</span><span>retro/retro_unserialize</span> <span>save-state</span> <span>(</span><span>alength</span> <span>save-state</span><span>)</span><span>)</span><span>)</span>
  <span>(</span><span>let</span> <span>[</span><span>mem</span> <span>(</span><span>retro/retro_get_memory_data</span> <span>RETRO_MEMORY_SYSTEM_RAM</span><span>)</span>
        <span>screen-tile</span> <span>(</span><span>.getByte</span> <span>mem</span> <span>0x006D</span><span>)</span>
        <span>xpos</span> <span>(</span><span>.getByte</span> <span>mem</span> <span>0x0086</span><span>)</span>
        <span>subpixel</span> <span>(</span><span>.getByte</span> <span>mem</span> <span>0x0400</span><span>)</span>
                <span>speed</span> <span>(</span><span>.getByte</span> <span>mem</span> <span>0x0700</span><span>)</span>
        <span>vertical-position</span> <span>(</span><span>.getByte</span> <span>mem</span>  <span>0x00B5</span><span>)</span>
        <span>below-viewport?</span> <span>(</span><span>&gt;</span> <span>vertical-position</span> <span>1</span><span>)</span>
        <span>on-flag-pole?</span> <span>(</span><span>=</span> <span>0x03</span> <span>(</span><span>.getByte</span> <span>mem</span> <span>0x001D</span><span>)</span><span>)</span>
        <span>dead?</span> <span>(</span><span>=</span> <span>3</span> <span>(</span><span>.getByte</span> <span>mem</span> <span>0x0770</span><span>)</span><span>)</span>
        <span>ypos</span> <span>(</span><span>byte-format</span> <span>(</span><span>.getByte</span> <span>mem</span> <span>0x03B8</span><span>)</span><span>)</span><span>]</span>
    <span>[</span><span>(</span><span>not</span> <span>on-flag-pole?</span><span>)</span>
     <span>dead?</span>
     <span>below-viewport?</span>
     <span>(</span><span>-</span> <span>(</span><span>-&gt;coord</span> <span>screen-tile</span> <span>xpos</span> <span>subpixel</span><span>)</span><span>)</span>
     <span>ypos</span>
     <span>(</span><span>-</span> <span>speed</span><span>)</span>
     <span>(</span><span>count</span> <span>inputs</span><span>)</span><span>]</span><span>)</span><span>)</span>
</pre></div><p>The 7 main factors of our current heuristic are:</p><ol><li><p><code>(not on-flag-pole?)</code>: Indicates whether we've reached our final goal, the flag pole!</p></li><li><p><code>dead?</code>: Indicates whether Mario has died.</p></li><li><p><code>below-viewport?</code>: One of Mario's primary hazards is falling into bottomless pits. Mario doesn't die instantly when he falls in a pit. This heuristic let's us short circuit when we know Mario is falling to his doom.</p></li><li><p><code>(- (-&gt;coord screen-tile xpos subpixel))</code>: Measures Mario's horizontal position. The value is negated because <code>dist</code> should decrease as we get closer to our goal.</p></li><li><p><code>ypos</code>: Mario's vertical position. Since it's easier for Mario to fall than to climb, we prefer exploring states where Mario is higher up. There are scenarios where this heuristic could get us stuck, but it didn't seem to be a problem for the levels we explored. Overall, this heuristic was very helpful for making it through the platforming levels with many bottomless pits. This value is not negated because it's already measured from the top of the screen and decreases as it "improves".</p></li><li><p><code>(- speed)</code>: Mario's absolute horizontal speed. We prefer fast Mario over slow Mario. Negated because <code>dist</code> should decrease as we get closer to our goal.</p></li><li><p><code>(count inputs)</code>: This is the total number of different steps we've taken in our current path. We penalize longer paths.</p></li></ol><h2 id="Levels">Levels</h2><p>Here are the completed levels. As you can see, the AI does not play like a normal human, but is definitely not super human either (yet!).</p><p>Note: Some levels will look very similar to other levels. I promise I didn't upload the wrong video. Notably, world 7 copies heavily from world 2.</p><h2 id="Level-1-1">Level 1-1</h2><h2 id="Level-1-2">Level 1-2</h2><h2 id="Level-1-3">Level 1-3</h2><h2 id="Level-1-4">Level 1-4</h2><h2 id="Level-2-1">Level 2-1</h2><h2 id="Level-2-2">Level 2-2</h2><h2 id="Level-2-3">Level 2-3</h2><h2 id="Level-2-4">Level 2-4</h2><h2 id="Level-3-1">Level 3-1</h2><h2 id="Level-3-2">Level 3-2</h2><h2 id="Level-3-3">Level 3-3</h2><h2 id="Level-3-4">Level 3-4</h2><h2 id="Level-4-1">Level 4-1</h2><h2 id="Level-4-2">Level 4-2</h2><h2 id="Level-4-3">Level 4-3</h2><h2 id="Level-5-1">Level 5-1</h2><h2 id="Level-5-2">Level 5-2</h2><h2 id="Level-5-3">Level 5-3</h2><h2 id="Level-5-4">Level 5-4</h2><h2 id="Level-6-1">Level 6-1</h2><h2 id="Level-6-2">Level 6-2</h2><h2 id="Level-6-3">Level 6-3</h2><h2 id="Level-6-4">Level 6-4</h2><h2 id="Level-7-1">Level 7-1</h2><h2 id="Level-7-2">Level 7-2</h2><h2 id="Level-7-3">Level 7-3</h2><h2 id="Level-8-1">Level 8-1</h2><h2 id="Level-8-2">Level 8-2</h2><h2 id="Level-8-3">Level 8-3</h2><h2 id="Conclusion">Conclusion</h2><p>Overall, I'm pretty happy that this simple AI was able to beat all the non maze levels. There's also plenty of room for improvement to explore in the future.</p><h2 id="Future-Work">Future Work</h2><h3 id="Generalize">Generalize</h3><p>There's a lot of hard-coded pieces. It would be nice to refactor to make it easier to support more exploration strategies, heuristics, and policies.</p><h3 id="Improved-Exploration-and-Backtracking">Improved Exploration and Backtracking</h3><p>Currently, the solver will only backtrack based on horizontal position. Furthermore, the backtracking window is constant. There are multiple theoretical improvements, but one general idea is to continue being greedier when you're making progress and spend more time exploring when you get stuck. With a better exploration strategy, maybe our solver could be improved to beat the dreaded maze levels.</p><h3 id="Beating-the-Full-Game">Beating the Full Game</h3><p>In addition to not being able to complete some of the Bowser levels, our solver also doesn't handle pipes and level transitions well. It shouldn't be that hard to detect those cases so that the solver can solve multiple levels and eventually, the full game.</p><p>All code for this project can be found on <a href="https://github.com/phronmophobic/clj-libretro/blob/67f186e87345ba4d979a63ab1dfd982c44d5fea7/src/com/phronemophobic/clj_libretro/ai.clj">github</a>.</p><p><a href="http://tom7.org/mario/">Tom7's learnfun &amp; playfun: A general technique for automating NES games</a></p><p><a href="https://www.deepmind.com/blog/agent57-outperforming-the-human-atari-benchmark">Agent57: Outperforming the human Atari benchmark</a></p><p><a href="https://rubenfiszel.github.io/posts/rl4j/2016-08-24-Reinforcement-Learning-and-DQN.html">Reinforcement Learning and DQN, learning to play from pixels</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The 90s Developer Starter Pack (217 pts)]]></title>
            <link>https://retrocoding.net/the-90s-developer-starter-pack</link>
            <guid>36658941</guid>
            <pubDate>Sun, 09 Jul 2023 21:11:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://retrocoding.net/the-90s-developer-starter-pack">https://retrocoding.net/the-90s-developer-starter-pack</a>, See on <a href="https://news.ycombinator.com/item?id=36658941">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content-parent"><p>I recently found a 386 emulator called 86Box, a fork of the earlier version of PCEm. Because I'm a real programmer™, I build the <code>master</code> branch from its Git source on a macOS Ventura. How I build it from sources will be the subject of a new article.</p>
<h2 id="heading-building-the-hardware-specifications">Building The Hardware Specifications</h2>
<p>86Box supports a breadth of selection of combinations of PC hardware components in the 90s. A real gold mine for a person like me who lives in a developing country with restricted access to old hardware. I set a goal to replicate the experience of installing a fresh Windows 95 PC from hardware available around 1990-1995. I'll limit it to something typical for developers on 95s to run on their medium-powered PC, which may have a few esoteric components. I remember my late father having AMD Am486 with 8MB of RAM. So I'll use that as a frame of reference. Also, I've found an article <a target="_blank" href="https://www.chicagotribune.com/news/ct-xpm-1995-11-03-9511030044-story.html">about buyers' guides from the Chicago Tribune explaining the spec for the holiday season in 1995</a>.</p>
<p>After mucking around the settings, these are the components that I decided to have.</p>
<h2 id="heading-cpu">CPU</h2>
<p>I'll go with <strong>AMD Am486DX4 with 100 MHz</strong> Speed with PGA 168 socket. This is how it originally looked like</p>
<p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1667642338455/_lMQJWFx8.jpg?auto=compress,format&amp;format=webp" alt="AMD Am486DX4 100MHz"></p>
<p>There's "Designed for Windows 95" imprinted on the chip itself. We're going to a good start.  This chip is Intel 486 compatible. I didn't go to Pentium or Pentium Pro because I wanted pure, unextended, intel 486 instruction sets with no Pentium Pro extensions like <code>CMOVNS</code>, which I've written about in my previous posts. </p>
<h2 id="heading-motherboard">Motherboard</h2>
<p>ASUS PVI-486SP3C with SiS 496 as the Chipset. Without further ADO, please look at this beauty.</p>
<p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1667642689155/xRlenge2K.jpg?auto=compress,format&amp;format=webp" alt="ASUS PVI-486SP3C"></p>
<p>As depicted in the picture above, this motherboard featured a PGA168 socket. With both SIS 496 as the Northbridge chip and SIS 497 as the Southbridge chip. This features these I/O:</p>
<ul>
<li>Dual-channel IDE.</li>
<li>1 Floppy Disk Interface.</li>
<li>3 PIC slots.</li>
<li>4 ISA slots, with one of them being a VLB slot (that extension is not an AGP slot, it didn't exist back then)</li>
<li>One parallel, two serial ports, and 1 PS/2 port.</li>
</ul>
<h2 id="heading-ram-and-disks">RAM and Disks</h2>
<p>There are two EDO-RAM over there, which I'll fill with a whopping 16 MB of RAM, which is 4 times the minimum requirement for Windows 95. As for disk, we'll fill all floppy disk controllers with 2.88 MB Drives. On the IDE slot, we'll put a 2GB disk drive.</p>
<p>We'll utilise ZIP drives and CD-ROM. I'll not use IDE for these. Like a grown-up, let's use SCSI for both ZIP and CD ROM drives.</p>
<h2 id="heading-components-and-accessories">Components and Accessories</h2>
<p>I'll start with the graphics card. Because we were in 1994, and we have a VLB slot, let's use that. We're going to use the S3 Vision964 VLB Version.</p>
<p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1667644946189/qrpXhIgT_.jpeg?auto=compress,format&amp;format=webp" alt="Elsa Winner S3 Vision 964 Card"></p>
<p>This card is an accelerated card which allows us to decode MPEG-1 without any additional accelerator cards.</p>
<p>The next obvious accessory is the sound card. We're not going crazy this time. Let's use good ol' Sound Blaster 16-bit and fill one more of our ISA slots.</p>
<p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1667645205759/ijBrWahzA.jpg?auto=compress,format&amp;format=webp" alt="Sound Blaster 16 bit"></p>
<p>As we're going to use SCSI, we're going to use BusLogic PCI BT-958D for our SCSI controller, which will drive both ZIP Drive and CD ROM drives.</p>
<p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1667645641177/Xd4rsBVyH.jpeg?auto=compress,format&amp;format=webp" alt="BusLogic"></p>
<h2 id="heading-operating-systems-and-software-installations">Operating Systems and Software Installations</h2>
<h2 id="heading-prerequisite-microsoft-dos-622">Prerequisite: Microsoft DOS 6.22</h2>
<p>Most of the motherboard is not ready for booting from CDs, let alone SCSI. So, unlike Windows 98. To install Windows 95, you'd need Microsoft DOS 6.22. We're going to install it, and for me, I'll also enable the Dvorak keyboard. I'll gloss over this process as you might already know how to install them from diskettes. After installing MSDOS, you'll be greeted with the typical C-prompt <code>C:\</code>. I usually install the supplemental disk to be able to use Dvorak as my keyboard and add this line to <code>AUTOEXEC.BAT</code> file.</p>
<pre><code>REM Use Dvorak Keyboard
KEYB DV,,C:\DOS\DVORAK.SYS
</code></pre><p>We'd need to access our CDROM, which is attached to a SCSI card. For this we'll use the driver from BusLogic and Microsoft MSCDEX. You can find the file on the net. The name are <code>btdosm.sys</code> <code>btcdrom.sys</code> and <code>mscdex.exe</code>. To access the drive, we'll need to load them on our <code>CONFIG.SYS</code> file.</p>
<pre><code>DEVICE=BTDOSM.SYS
DEVICE=BTCDROM.SYS /D:RETROCD
</code></pre><p>And then add this line to <code>AUTOEXEC.BAT</code></p>
<pre><code>REM Mount CD ROM
MSCDEX.EXE /D:RETROCD
</code></pre><p>Reboot, and now you can access your <code>D</code> drive from DOS command prompt.</p>
<h2 id="heading-install-windows-95-osr-25">Install Windows 95 OSR 2.5</h2>
<p>Mount your Windows 95 on the CD Drive and then go to drive D.</p>
<pre><code>C:\&gt; D:
D:\&gt; SETUP
</code></pre><p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1667655529827/b5Ok_zHi2.png?auto=compress,format&amp;format=webp" alt="Setup Wizard"></p>
<p>Follow the setup instructions, and you'll end up with a desktop. Right-click on the <strong>My Computer</strong> section and select <strong>Properties</strong>. You'll end up with something like this.</p>
<p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1667655920414/6CdD0c8JD.png?auto=compress,format&amp;format=webp" alt="Desktop Machine Properties"></p>
<p>Look at that 16 MB RAM and a proud 80486 owner with all the bells and whistles. Now time to install our development tool of choice.</p>

<p>We're "real programmers", so we're not gonna use peasants' tools like Visual Basic. No, no siree... We are going to use a real programming language like C++. So prepare your <a target="_blank" href="https://winworldpc.com/product/microsoft-visual-stu/97-5x">Visual Studio Enterprise 97</a> disks and install Visual C++.</p>
<p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1667656275181/2WacmCqyL.png?auto=compress,format&amp;format=webp" alt="Visual C++ Advertisement of COM">.</p>
<p>Look at that! What an amazing compiler supporting <a target="_blank" href="https://en.wikipedia.org/wiki/Component_Object_Model">COM</a>, and it supports creating COM clients and interfaces, exciting time to code in 1994.</p>
<p>If you are clueless enough, you can also install the MSDN for Visual Studio 97 if you can get the CD image from the internet. However, our next step is very simple. We won't need to use that.</p>
<h2 id="heading-coding">Coding</h2>
<p>So we have our money-making machine being set up. The next step is to write a program for the 90s. Let's give ourselves a treat of being beginners again and see what we can achieve using a 30-years-old platform with machine 1/1000 times less powerful than our typical phone in 2022.</p>
<p>Let's fire up the IDE and enjoy the developer tools created by Microsoft in the early 90s.</p>
<p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1667661905493/wy8ZcwYb6.png?auto=compress,format&amp;format=webp" alt="Hello Win95"></p>
<p>Proceed by selecting "Win32 Application" and then choose "Empty Project". You'll be greeted with an empty project. Add new files named <code>main.cpp</code> to the files section and let's start by building a GUI program which will show you a "Hello, Welcome to Windows95" message box.</p>
<h2 id="heading-entry-point">Entry Point</h2>
<p>A win32 project entry point is not <code>main</code> but <code>WinMain</code>. A minimal program which will start and do absolutely nothing will be as follows.</p>
<pre><code><span>#<span>include</span> <span>&lt;Windows.h&gt;</span></span>

<span><span>int</span> APIENTRY <span>WinMain</span><span>(HINSTANCE hInstance, HINSTANCE hPrevInstance,
                     LPSTR lpCmdLine, <span>int</span> nCmdShow)</span>
</span>{
  <span>return</span> <span>0</span>;
}
</code></pre>
<p>Although the program does absolutely nothing, it'll just start and immediately exit, the function signature is a mouthful and verbose. Not all of these parameters are important or even usable. This is the quirk of Windows programming as Microsoft needs to maintain the API signature for backward compatibility. </p>
<ol>
<li>Including the <code>Windows.h</code> header is mandatory for all Windows programs written this way.</li>
<li>The <code>WinMain</code> function needs to return an <code>int</code>. If there's no Windows Message Pump defined, return <code>0</code> on exist. We'll talk about Windows Message Pump in another post.</li>
<li><code>APIENTRY</code> is an alias to stdcall calling convention, which means the function itself will clean the stack.</li>
<li><code>HINSTANCE</code> is a "handle" to an application instance. A handle is an opaque value for an object within Windows operating system. Both first parameters are purely for backward compatibility. The first <code>HINSTANCE</code> is always pegged to <code>0x04</code>, and the second is always <code>0x00</code> on 32-bit protected mode Windows.</li>
<li><code>lpCmdLine</code> is the command line to invoke the program. We can use this parameter to parse the command line and its arguments when it's invoked from the command line.</li>
<li><code>nCmdShow</code> is the value which tell the window, if the Window will be shown minimised, maximised, or hidden.</li>
</ol>
<p>Try to build the application, and you'll see.. nothing. Building only validates that our syntax is correct. The message on the build window will show something like this.</p>
<p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1667663069376/E8PVkXUuP.png?auto=compress,format&amp;format=webp" alt="Building Program Console"></p>
<h2 id="heading-our-first-windows-api">Our first Windows API</h2>
<p>When we started with something new, we started with "Hello, World!" program. This can be accomplished in Win32 API by calling <code>MessageBox</code> function. The function prototype is very simple.</p>
<pre><code><span><span>int</span> <span>MessageBox</span><span>(HWND hWnd, LPSTR lpText, LPSTR lpCaption, UINT uType)</span></span>;
</code></pre>
<p>The meaning of each of the parameter are:</p>
<ul>
<li><code>hWnd</code> is the parent window of the message box window being displayed. Passing <code>0</code> means that the message box parent will be the Desktop.</li>
<li><code>lpText</code> is a zero-terminated string containing the message to be displayed.</li>
<li><code>lpCaption</code> is the caption of the message box.</li>
<li><code>uType</code> is the message box type. There are several values that you can use. The value is a bit flag. We'll use <code>MB_OK | MB_ICONINFORMATION</code> which means that we'll only show the OK button and an information icon.</li>
</ul>
<p>The full program to be run is as below:</p>
<pre><code><span>#<span>include</span> <span>&lt;Windows.h&gt;</span></span>

<span><span>int</span> APIENTRY <span>WinMain</span><span>(HINSTANCE hInstance, HINSTANCE hPrevInstance,
                     LPSTR lpCmdLine, <span>int</span> nCmdShow)</span>
</span>{
  MessageBox(<span>NULL</span>, <span>"Hello, Welcome to Win32 Programming"</span>, 
      <span>"Hello, World"</span>, MB_OK | MB_ICONINFORMATION);
  <span>return</span> <span>0</span>;
}
</code></pre>
<p>And the program will show a nice message box.</p>
<p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1667664027301/s938PW0B6.png?auto=compress,format&amp;format=webp" alt="Hello World Win32"></p>
<p>That's it! Our first GUI program using Win32 API running on typical early 90s workstation running on Windows 95. There will be more which we'll explore on next articles. Including on how to create a window and draw something there.</p>
<p>If you curious, you can build the executable to an .exe and then try to run it on modern Windows operating system like Windows 11. You'll surprise that the program still run perfectly fine.</p>
<h2 id="heading-conclusion">Conclusion</h2>
<p>Windows 95 is the first full 32-bit Windows that's built by Microsoft. It's a break from previous generations of Windows which brings 32-bit programming to consumer which previously was only available on Windows NT. Programming C/C++ in Windows differs with typical console-based C/C++ applications. For example, the entry point of Win32 program is <code>WinMain</code> and not <code>main</code>. In the future article we'll explore more about Window 95 and Win32 API.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An Alerting Vista of macOS Sonoma (202 pts)]]></title>
            <link>https://furbo.org/2023/07/09/an-alerting-vista-of-sonoma/</link>
            <guid>36658553</guid>
            <pubDate>Sun, 09 Jul 2023 20:30:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://furbo.org/2023/07/09/an-alerting-vista-of-sonoma/">https://furbo.org/2023/07/09/an-alerting-vista-of-sonoma/</a>, See on <a href="https://news.ycombinator.com/item?id=36658553">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
		<main id="main" role="main">

		
<article id="post-2652">

	<!-- .entry-header -->

	<div>
		
<p>There’s a new “feature” in Sonoma, and no one besides Apple is quite sure what it is.</p>



<p>Alerts for deprecated APIs are now <a href="https://www.youtube.com/watch?v=MyGUrPxG1iM">appearing frequently</a>. Sometimes when you launch an app, and sometimes at random. Here are three I got the other day after waking a MacBook from sleep:</p>



<figure><img src="https://furbo.org/wp-content/uploads/2023/07/Sonoma-Alert.jpg" alt=""><figcaption>Mysterious Alerts.</figcaption></figure>



<p>From a UI point-of-view, these alerts have serious issues:</p>



<ul><li>They are scary and not actionable.</li><li>The only unique information is the title. The name, however, is not something I recognize.</li><li>I know what a deprecated API is and how its removal can be a bad thing, but ordinary users won’t.</li><li>There is no mention of what API caused the alert.</li><li>I’m advised to contact the developer for an updated version, but there is no information on who that developer is. (In the screenshot above, I’m assuming the developer is Apple itself, so I notified them with FB12560773, FB12560774, FB12560776).</li></ul>



<p>But the UI is just the beginning of the fun. Once the developer is notified, the lack of information for this “feature” prevents the deprecation from being addressed.</p>



<p>This alert is mentioned in the <a href="https://developer.apple.com/documentation/macos-release-notes/macos-14-release-notes#ATS-and-ATSUI">release notes</a> for the first beta for Sonoma. The brevity of that note is surprising for such a prominent and important addition to macOS.</p>



<p>I suspect that the brevity of the alert’s text is to shield the customer from unfamiliar terminology and complexity. It’s like a check engine light in your car: it comes on and you know to get to a mechanic ASAP.</p>



<p>When you get to the mechanic, they have diagnostic tools that let them understand what’s wrong (the engine control unit will typically store a unique code).</p>



<p>The difference with this “feature” is that folks like me are the mechanics and we have no diagnostic code. Supposedly, this alert should only appear with the use of ATS or ATSUI (per the release notes), but I find it hard to believe that modern system framework components show above are using Apple Type Services that were <a href="https://en.wikipedia.org/wiki/Apple_Type_Services_for_Unicode_Imaging">replaced 13 years ago</a>. But maybe they are – we all embed frameworks into our apps, some where we have source code, others where we do not.</p>



<p>The alerts also have another hideous “feature”. They turn themselves off as soon as you hit the OK button. They never repeat, so it’s impossible to bisect the code to find and verify a fix.</p>



<p>It’s like having your check engine like go off the next time you start the car, and the diagnostic code being removed by the time you get to the repair shop. Developers reaction to these reports will be the same as your local mechanic: “I don’t know what’s wrong. Good luck.”</p>



<p>If Apple wants developers to repair these deprecations they need to make major changes:</p>



<ul><li>Document the behavior, including all of the APIs that trigger it.</li><li>Tell developers if this is something that will only appear in the beta or if it will be a part of the final release.</li><li>Give the customer something to work with when communicating with the developer. A diagnostic code at a minimum.</li><li>Log information about what caused the issue: remember that it may not be the developer’s own code triggering the alert. Plugins, embedded libraries, and external frameworks should be a part of the log. These logs should be available to customers so they can provide it to developers.</li><li>Give developers a way to reset the alerts. Make fixes testable.</li></ul>



<p>Without these changes a Mac user’s future is one with a lot of crashes caused by deprecated code.</p>



<p><strong>NOTE:</strong> This blog post has been submitted as feedback. If you find this situation intolerable, please submit a duplicate titled “Mysterious deprecation alerts in Sonoma” with a reference to FB12560999 and a  link to this blog post.</p>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

</article><!-- #post-## -->
			

		</main><!-- .site-main -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fractal Vise (117 pts)]]></title>
            <link>https://airgraver.com/fractal-vise.htm</link>
            <guid>36658166</guid>
            <pubDate>Sun, 09 Jul 2023 19:48:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://airgraver.com/fractal-vise.htm">https://airgraver.com/fractal-vise.htm</a>, See on <a href="https://news.ycombinator.com/item?id=36658166">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
            <p><a href="https://airgraver.com/photos/fractal-goliath-a.jpg">
            <img src="https://airgraver.com/photos/fractal-goliath-a_small.jpg" xthumbnail-orig-image="photos/fractal-goliath-a.jpg"></a><br>
            Goliath Vise with Morphing� Fractal Jaws</p><p>
            
            <a href="https://airgraver.com/photos/fractal-pc-a.jpg">
            <img src="https://airgraver.com/photos/fractal-pc-a_small.jpg" xthumbnail-orig-image="photos/fractal-pc-a.jpg"></a><br>
            PalmControl Vise with Morphing� Fractal Jaws</p><p>
            
            <a href="https://airgraver.com/photos/fractal-titan-b.jpg">
            <img src="https://airgraver.com/photos/fractal-titan-b_small.jpg" xthumbnail-orig-image="photos/fractal-titan-b.jpg"></a><br>
            Titan Vise with Morphing� Fractal Jaws</p></div><div>
              <p><span face="Times New Roman">I have been 
              developing my Morphing� Fractal jaws for years use with engraving 
              vises. There have been various versions. While there was an old 
              (early 1900s patent) milling machine vise with similar jaws, it was not 
              suitable for use with engraving vises. The patent pending 
              improvements allow for use with engraving vises and other vises. 
              Please be aware of imitators, as the use of jaws with an engraving vise may infringe any patent that 
              issues from my pending patent applications.&nbsp; <b>
              <a href="https://airgraver.com/Patent-Infringement.htm">Infringement Info Page</a></b></span></p>
              <p><span face="Times New Roman">These fit various 
              makes and sizes of engraving vises.&nbsp; Shown above larger version 
              with 16 fingers on the 90lbs Titan vise.&nbsp;&nbsp;While smaller 
              8 finger is shown above on a 30lb Goliath and a 12lb PalmControl vise.&nbsp;
              </span></p>
              <p><span face="Times New Roman">
              <span color="#800000">It will be a 
              while yet before I have time to start production in quantities.&nbsp;&nbsp; 
              However if you would like one you can order by placing a 
              down payment.&nbsp;&nbsp; The final price of them has not been 
              determined.&nbsp;&nbsp; You will be contacted when yours is ready 
              if you make a down payment.&nbsp; At that time we will know the 
              price and you may pay the remaining balance or you may cancel and 
              have your down payment returned.&nbsp;&nbsp;Or if at any time you 
              become tired while waiting you may cancel to have your down 
              payment returned.</span>&nbsp; <br>
              </span><b><span face="Times New Roman">
              Morphing� Fractal� Jaws for Goliath or PalmControl Vises - down 
              payment $75 </span></b>
              <span face="Times New Roman"><b>&nbsp;</b></span><b><span face="Times New Roman"><a href="https://www.handgravers.com/cgi-bin/sc/order.cgi?storeid=*1247faa0cb82087897419e&amp;dbname=products&amp;guid=16a024d0-d62c-11eb-b6a3-87214a7ce144&amp;function=add"><img src="http://www.handgravers.com/catalog/media/add_to_cart.gif" alt="add_to_cart.gif" width="83" height="20"></a><br>
              &nbsp;</span></b><b><span face="Times New Roman">Morphing� Fractal� 
              Jaws for Titan Vise&nbsp; - 
              down payment $150 
              <a href="https://www.handgravers.com/cgi-bin/sc/order.cgi?storeid=*1247faa0cb82087897419e&amp;dbname=products&amp;guid=0cd530c6-d62c-11eb-a105-b7c196676a45&amp;function=add">
              <img src="http://www.handgravers.com/catalog/media/add_to_cart.gif" alt="add_to_cart.gif" width="83" height="20"></a></span></b><br>
              <span color="#800000" face="Times New Roman"><b><span size="4">~ 
              PLEASE NOTE ~ </span></b>This is only a down payment on your order 
              for the jaws. It is not the final price.&nbsp;&nbsp; This is a new 
              product and production has not begun.&nbsp; It will be a wait 
              (this wait is unknown how long) before the jaws are completed and 
              ready.&nbsp; When they are ready, the time in making and materials 
              will be known and the final price can be determined then.</span></p>
              <hr>
              <blockquote>
                <p><b><span face="Times New Roman">~ Frequently 
                Asked Questions ~</span></b></p>
                <p><span face="Times New Roman"><b>Question: </b>
                How much do they cost? <br>
                <b>Answer: </b>Price is not known yet. Production has not 
                started and therefore not enough of them have been made to know 
                what the cost will be.&nbsp; You pre-order with a down payment 
                toward the purchase, and be placed in line.&nbsp; See above.
                </span></p>
                <p><span face="Times New Roman"><b>Question: </b>
                When will one be ready for me? <br>
                <b>Answer:</b> It will be a while yet before production in 
                quantities begins. </span></p>
              </blockquote>
              <p><a href="https://airgraver.com/vises.htm">Vises home page link</a></p>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using Lidar to map tree shadows (308 pts)]]></title>
            <link>https://tedpiotrowski.svbtle.com/using-lidar-for-tree-shadows-in-shademap</link>
            <guid>36658001</guid>
            <pubDate>Sun, 09 Jul 2023 19:28:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tedpiotrowski.svbtle.com/using-lidar-for-tree-shadows-in-shademap">https://tedpiotrowski.svbtle.com/using-lidar-for-tree-shadows-in-shademap</a>, See on <a href="https://news.ycombinator.com/item?id=36658001">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="container">
  <article id="g3etMHMMG3awSk8PHwraSc">
	<time datetime="2023-07-09">July  9, 2023</time>
  <h2>
    <a href="https://tedpiotrowski.svbtle.com/using-lidar-for-tree-shadows-in-shademap">Using LiDAR to map tree shadows</a>
  </h2>
	<p><strong>tl;dr;</strong> <em>I can load LiDAR data to simulate tree shadows for any time of year, but the hardware demands and hosting costs may be prohibitive so I’m only sharing a small <a href="https://pub-2f76d53a7bba403bbf5ca35a8bf7e094.r2.dev/leaflet.html">demo</a> for now</em></p>

<p>Two years ago, I launched shademap.app, and since then, a common question I receive is: “Where are the trees?” It’s a valid inquiry, considering I reside in the Pacific Northwest—a region known for its towering trees that significantly affect the amount of direct sunlight a location receives.</p>

<p>Here are two renderings of the shadows on Bainbridge Island for July 9th at 7:09 AM. Radar clearly misses 90% of the shadows cast because it does not include vegetation. Radar only reflects off the ground, making objects such as trees and buildings invisible. On the other hand, LiDAR reflects off all objects, creating a much richer model of the earth’s surface.</p>

<p><a href="https://svbtleusercontent.com/bY4zNi97cMS6orbwoJmDKg0xspap.jpg"><img src="https://svbtleusercontent.com/bY4zNi97cMS6orbwoJmDKg0xspap_small.jpg" alt="Radar vs LiDAR shadow simulation"></a></p>

<p>So why hasn’t ShadeMap included trees from the beginning? It’s because ShadeMap uses elevation data to simulate shadows and the only readily available world-wide elevation data sets come from <a href="https://en.wikipedia.org/wiki/Shuttle_Radar_Topography_Mission">radar</a>. Radar works at night time and penetrates clouds, so satellites are able to compile this data 24 hours per day from space.</p>

<p>LiDAR, on the other hand, is much more accurate but is collected from airplanes or drones and cannot penetrate fog and clouds. It’s much more time consuming and expensive to collect, leaving each local government to fund its surveying costs. However, I recently discovered that my state of Washington provides an extensive <a href="https://lidarportal.dnr.wa.gov/">LiDAR dataset</a> that covers large amounts of the state.</p>

<p><a href="https://svbtleusercontent.com/tF4Je2rM5W8N1eLXgDmybA0xspap.jpg"><img src="https://svbtleusercontent.com/tF4Je2rM5W8N1eLXgDmybA0xspap_small.jpg" alt="WA LiDAR portal"></a></p>

<p>I could finally fill in the gaps in my shadow simulation-for my own backyard, at least. But there was one problem. The data format was geared towards traditional GIS software (it’s a GeoTIFF) and not browser friends (like a JPG or PNG). In order to use the data, I would have to take 100’s of gigabytes of floating point, imperial feet, GeoTIFF files and slice them up into small fast-loading image tiles where metric meters are encoded as red, green and blue pixel values.</p>

<p>I bought a 1TB hard drive and started asking ChatGPT questions on how to convert the data. (ChatGPT is a marvelous assistant and has saved me hours of reading documentation and irrelevant Google search results) Once, I started to run the conversion process, I realized that my 16GB of RAM could not load these large data files and I had to rewrite the conversion code to just work with a small region of the map at a time. For the first time in a long time, I’m feeling the need for a more powerful machine…</p>

<p>And it works. Or actually…it’s working right now. I’m attempting to convert just the Seattle metropolitan area and it’s only about half way done after 12 hours. The tiles are over 15GB and growing. The simulations are incredible, but I’m not sure I want to sink money into hosting this data and making it publicly available. It’s a shame but it’s the sound financial decision for now.</p>

<p><a href="https://svbtleusercontent.com/pd7efcJtXN5f8vho7JVP9y0xspap.gif"><img src="https://svbtleusercontent.com/pd7efcJtXN5f8vho7JVP9y0xspap_small.gif" alt="LiDAR shadow simulation"></a></p>

<p>However, I can host small portions of this dataset for free so if you’re curious what my long-term vision for ShadeMap is, try this <a href="https://pub-2f76d53a7bba403bbf5ca35a8bf7e094.r2.dev/leaflet.html">demo</a></p>

<p>As always, follow me on <a href="https://twitter.com/shademap">Twitter</a> for frequent updates on this project or if you want to get in touch.</p>

  <figure id="kudo_g3etMHMMG3awSk8PHwraSc">
    <a href="#kudo">
      
    </a>
    <p>140</p>
    <p>Kudos</p>
  </figure>
  <figure id="kudo_side_g3etMHMMG3awSk8PHwraSc">
    <a href="#kudo">
      
    </a>
    <p>140</p>
    <p>Kudos</p>
  </figure>
</article>

</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bevy 0.11: ECS-driven game engine built in Rust (126 pts)]]></title>
            <link>https://bevyengine.org/news/bevy-0-11/</link>
            <guid>36657970</guid>
            <pubDate>Sun, 09 Jul 2023 19:25:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bevyengine.org/news/bevy-0-11/">https://bevyengine.org/news/bevy-0-11/</a>, See on <a href="https://news.ycombinator.com/item?id=36657970">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Thanks to <strong>166</strong> contributors, <strong>522</strong> pull requests, community reviewers, and our <a href="https://bevyengine.org/community/donate"><strong>generous sponsors</strong></a>, we're happy to announce the <strong>Bevy 0.11</strong> release on <a href="https://crates.io/crates/bevy">crates.io</a>!</p>
<p>For those who don't know, Bevy is a refreshingly simple data-driven game engine built in Rust. You can check out our <a href="https://bevyengine.org/learn/book/getting-started/">Quick Start Guide</a> to try it today. It's free and open source forever! You can grab the full <a href="https://github.com/bevyengine/bevy">source code</a> on GitHub. Check out <a href="https://bevyengine.org/assets">Bevy Assets</a> for a collection of community-developed plugins, games, and learning resources.</p>
<p>To update an existing Bevy App or Plugin to <strong>Bevy 0.11</strong>, check out our <a href="https://bevyengine.org/learn/migration-guides/0.10-0.11/">0.10 to 0.11 Migration Guide</a>.</p>
<p>Since our last release a few months ago we've added a <em>ton</em> of new features, bug fixes, and quality of life tweaks, but here are some of the highlights:</p>
<ul>
<li><strong>Screen Space Ambient Occlusion (SSAO)</strong>: Increase scene render quality by simulating occlusion of "indirect" diffuse light</li>
<li><strong>Temporal Anti-Aliasing (TAA)</strong>: A popular anti-aliasing technique that blends the current frame with past frames using motion vectors to smooth out artifacts</li>
<li><strong>Morph Targets</strong>: Animate vertex positions on meshes between predefined states. Great for things like character customization!</li>
<li><strong>Robust Constrast Adaptive Sharpening (RCAS)</strong>: Intelligently sharpens renders, which pairs nicely with TAA</li>
<li><strong>WebGPU Support</strong>: Bevy can now render on the web faster and with more features using the modern WebGPU web API</li>
<li><strong>Improved Shader Imports</strong>: Bevy shaders now support granular imports and other new features</li>
<li><strong>Parallax Mapping</strong>: Materials now support an optional depth map, giving flat surfaces a feel of depth through parallaxing the material's textures</li>
<li><strong>Schedule-First ECS APIs</strong>: A simpler and more ergonomic ECS system scheduling API</li>
<li><strong>Immediate Mode Gizmo Rendering</strong>: Easily and efficiently render 2D and 3D shapes for debugging and editor scenarios</li>
<li><strong>ECS Audio APIs</strong>: A more intuitive and idiomatic way to play back audio</li>
<li><strong>UI Borders</strong>: UI nodes can now have configurable borders!</li>
<li><strong>Grid UI Layout</strong>: Bevy UI now supports CSS-style grid layout</li>
<li><strong>UI Performance Improvements</strong>: The UI batching algorithm was changed, yielding significant performance wins</li>
</ul>
<h2 id="screen-space-ambient-occlusion">Screen Space Ambient Occlusion
<a href="#screen-space-ambient-occlusion">#</a>
</h2>

<p><b>Drag this image to compare</b></p>
<p><img alt="Without SSAO" src="https://bevyengine.org/news/bevy-0-11/no_ssao.png">
  <img alt="With SSAO" src="https://bevyengine.org/news/bevy-0-11/with_ssao.png">
</p>
<p><strong>SSAO Only</strong>
<img src="https://bevyengine.org/news/bevy-0-11/ssao_only.png" alt="ssao_only"></p>
<p>Bevy now supports Screen Space Ambient Occlusion (SSAO). While Bevy already supported shadows from direct lights
(<a href="https://docs.rs/bevy/0.11.0/bevy/pbr/struct.DirectionalLight.html"><code>DirectionalLight</code></a>, <a href="https://docs.rs/bevy/0.11.0/bevy/pbr/struct.PointLight.html"><code>PointLight</code></a>, <a href="https://docs.rs/bevy/0.11.0/bevy/pbr/struct.SpotLight.html"><code>SpotLight</code></a>) via shadow mapping, Bevy now supports shadows from <em>indirect</em> diffuse lighting such as <a href="https://docs.rs/bevy/0.11.0/bevy/pbr/struct.AmbientLight.html"><code>AmbientLight</code></a> or <a href="https://docs.rs/bevy/0.11.0/bevy/pbr/struct.EnvironmentMapLight.html"><code>EnvironmentMapLight</code></a>.</p>
<p>These shadows give scenes a more "grounded" feel, by estimating how much surrounding geometry blocks incoming light via the screen-space depth and normal prepasses. You can try it out in the new <a href="https://github.com/bevyengine/bevy/blob/v0.11.0/examples/3d/ssao.rs">SSAO example</a>.</p>
<p>Note that using SSAO with the newly added Temporal Anti-Aliasing leads to a <em>large</em> increase in quality and noise reduction.</p>
<p>Platform support is currently limited - Only Vulkan, DirectX12, and Metal are currently supported. WebGPU support will come at a later date. WebGL likely won't be supported because it doesn't have compute shaders.</p>
<p>Special thanks to Intel for their open source <a href="https://github.com/GameTechDev/XeGTAO">XeGTAO</a> project, which was a huge help in developing this feature.</p>
<h2 id="temporal-anti-aliasing">Temporal Anti-Aliasing
<a href="#temporal-anti-aliasing">#</a>
</h2>
<p>authors: @JMS55, @DGriffin91</p>
<p><img src="https://bevyengine.org/news/bevy-0-11/aa.png" alt="aa"></p>
<p>Alongside MSAA and FXAA, Bevy now supports Temporal Anti-aliasing (TAA) as an anti-aliasing option.</p>
<p>TAA works by blending the newly rendered frame with past frames in order to smooth out aliasing artifacts in the image. TAA has become increasingly popular in the industry because of its ability to cover up so many rendering artifacts: it smooths out shadows (both global illumination and "casted" shadows), mesh edges, textures, and reduces specular aliasing of light on reflective surfaces. However because the "smoothing" effect is so apparent, some people prefer other methods.</p>
<p>Here's a quick rundown of the following advantages and disadvantages of each anti-aliasing method that Bevy supports:</p>
<ul>
<li><strong>Multi-Sample Antialiasing (MSAA)</strong>
<ul>
<li>Does a good job at smoothing the edges of meshes (anti geometric aliasing). Does not help with specular aliasing. Performance cost scales with triangle count, and performs very poorly on scenes with many triangles</li>
</ul>
</li>
<li><strong>Fast Approximate Antialiasing (FXAA)</strong>
<ul>
<li>Does a decent job of dealing with both geometric and specular aliasing. Very little performance cost in all scenes. Somewhat blurry and low quality results</li>
</ul>
</li>
<li><strong>Temporal Antialiasing (TAA)</strong>
<ul>
<li>Does a very good job at dealing with both geometric and specular aliasing. Does a good job at dealing with temporal aliasing, where high-frequency details flicker over time or as you move the camera around or as things animate. Performance cost is moderate, and scales only with screen resolution. Chance of "ghosting" where meshes or lighting effects may leave trails behind them that fade over time. Although TAA helps with reducing temporal aliasing, it may also introduce additional temporal aliasing, especially on thin geometry or texture detail rendered at a distance. Requires 2 view's worth of additional GPU memory, as well as enabling the motion vector and depth prepasses. Requires accurate motion vector and depth prepasses, which complicates custom materials</li>
</ul>
</li>
</ul>
<p>TAA implementations are a series of tradeoffs and rely on heuristics that are easy to get wrong. In Bevy 0.11, TAA is marked as an experimental feature for the following reasons:</p>
<ul>
<li>TAA does not currently work with the following Bevy features: skinning, morph targets, and parallax mapping</li>
<li>TAA currently tends to soften the image a bit, which can be worked around via post-process sharpening</li>
<li>Our TAA heuristics are not currently user-configurable (and these heuristics are likely to change and evolve)</li>
</ul>
<p>We will continue to improve quality, compatibility, and performance in future releases. Please report any bugs you encounter!</p>
<p>You can compare all of our anti-aliasing methods in Bevy's improved <a href="https://github.com/bevyengine/bevy/blob/v0.11.0/examples/3d/anti_aliasing.rs">anti-aliasing example</a>.</p>
<h2 id="robust-contrast-adaptive-sharpening">Robust Contrast Adaptive Sharpening
<a href="#robust-contrast-adaptive-sharpening">#</a>
</h2>
<p>authors: @Elabajaba</p>
<p>Effects like TAA and FXAA can cause the final render to become blurry. Sharpening post processing effects can help counteract that. In <strong>Bevy 0.11</strong> we've added a port of AMD's Robust Constrast Adaptive Sharpening (RCAS).</p>
<p><b>Drag this image to compare</b></p>
<p><img alt="TAA" src="https://bevyengine.org/news/bevy-0-11/rcas_off.png">
  <img alt="TAA+RCAS" src="https://bevyengine.org/news/bevy-0-11/rcas_on.png">
</p>
<p>Notice that the texture on the leather part of the helmet is much crisper!</p>
<h2 id="morph-targets">Morph Targets
<a href="#morph-targets">#</a>
</h2>
<p>authors: @nicopap, @cart</p>
<p>Bevy, since the 0.7 release, supports 3D animations.</p>
<p>But it only supported <em>skeletal</em> animations. Leaving on the sidewalk a common
animation type called <em>morph targets</em> (aka blendshapes, aka keyshapes, and a slew
of other name). This is the grandparent of all 3D character animation!
<a href="https://en.wikipedia.org/wiki/Crash_Bandicoot_(video_game)#Gameplay">Crash Bandicoot</a>'s run cycle used morph targets.</p>

<p>Character model by <a href="https://www.artstation.com/zambrah">Samuel Rosario</a> (© all rights reserved), used with permission. Modified by nicopap, using the <a href="https://studio.blender.org/characters/snow/v2/">Snow</a> character texture by Demeter Dzadik for Blender Studios <a href="https://creativecommons.org/licenses/by/4.0/">(🅯 CC-BY)</a>.
</p>
<!-- The previous paragraph requires the <a href> tags, since zola doesn't
process markdown markup within tags -->
<p>Nowadays, an animation artist will typically use a skeleton rig for wide
moves and morph targets to clean up the detailed movements.</p>
<p>When it comes to game assets, however, the complex skeleton rigs used by
artists for faces and hands are too heavy. Usually, the poses are
"baked" into morph poses, and facial expression transitions are handled
in the engine through morph targets.</p>
<p>Morph targets is a very simple animation method. Take a model, have a base
vertex position, move the vertices around to create several poses:</p>

<p>Store those poses as a difference between the default base mesh and the variant
pose, then, at runtime, <em>mix</em> each pose. Now that we have the difference with
the base mesh, we can get the variant pose by simply adding to the base
vertices positions.</p>
<p>That's it, the morph target shader looks like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span><span>fn</span> </span><span>morph_vertex</span></span><span><span><span>(</span><span>vertex</span><span>:</span> Vertex</span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
    <span>for</span> <span><span>(</span>var i<span>:</span> <span>u32</span> <span>=</span> 0u<span>;</span> i <span>&lt;</span> <span>pose_count</span><span><span>(</span></span><span><span>)</span></span><span>;</span> i<span>+</span><span>+</span></span><span><span>)</span></span> <span><span>{</span>
        <span>let</span> weight <span>=</span> <span>weight_for_pose</span><span><span>(</span>i</span><span><span>)</span></span><span>;</span>
        vertex<span>.</span>position <span>+=</span> weight <span>*</span> <span>get_difference</span><span><span>(</span>vertex<span>.</span>index<span>,</span> position_offset<span>,</span> i</span><span><span>)</span></span><span>;</span>
        vertex<span>.</span>normal <span>+=</span> weight <span>*</span> <span>get_difference</span><span><span>(</span>vertex<span>.</span>index<span>,</span> normal_offset<span>,</span> i</span><span><span>)</span></span><span>;</span>
    </span><span><span>}</span></span>
</span><span><span>}</span></span></span>
</span></code></pre>
<p>In Bevy, we store the weights per pose in the <code>MorphWeights</code> component.</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span><span>fn</span> </span><span>set_weights_system</span></span><span><span><span>(</span><span>mut</span> <span>morph_weights</span><span>:</span> <span>Query<span>&lt;</span><span>&amp;</span><span>mut</span> MorphWeights<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
    <span>for</span> <span>mut</span> entity_weights <span>in</span> <span>&amp;</span><span>mut</span> morph_weights <span><span>{</span>
        <span>let</span> weights <span>=</span> entity_weights<span>.</span><span>weights_mut</span><span><span>(</span></span><span><span>)</span></span><span>;</span>

        weights<span><span>[</span><span>0</span><span>]</span></span> <span>=</span> <span>0.</span><span>5</span><span>;</span>
        weights<span><span>[</span><span>1</span><span>]</span></span> <span>=</span> <span>0.</span><span>25</span><span>;</span>
    </span><span><span>}</span></span>
</span><span><span>}</span></span></span>
</span></code></pre>
<p>Now assuming that we have two morph targets, (1) the frown pose, (2)
the smirk pose:</p>
<div>
<div>
  <p><b>[0.0, 0.0]</b></p>
  <p>default pose</p>
  <p><img alt="Neutral face expression" src="https://bevyengine.org/news/bevy-0-11/morph_target_default-0.png">
</p></div>
<div>
  <p><b>[1.0, 0.0]</b></p>
  <p>frown only</p>
  <p><img alt="Frowning" src="https://bevyengine.org/news/bevy-0-11/morph_target_frown-0.png">
</p></div>
<div>
  <p><b>[0.0, 1.0]</b></p>
  <p>smirk only</p>
  <p><img alt="Smirking" src="https://bevyengine.org/news/bevy-0-11/morph_target_smirk.png">
</p></div>
<div>
  <p><b>[0.5, 0.0]</b></p>
  <p>half frown</p>
  <p><img alt="Slightly frowning" src="https://bevyengine.org/news/bevy-0-11/morph_target_frown-half-0.png">
</p></div>
<div>
  <p><b>[1.0, 1.0]</b></p>
  <p>both at max</p>
  <p><img alt="Making faces" src="https://bevyengine.org/news/bevy-0-11/morph_target_both-0.png">
</p></div>
<div>
  <p><b>[0.5, 0.25]</b></p>
  <p>bit of both</p>
  <p><img alt="Slightly frowning/smirking" src="https://bevyengine.org/news/bevy-0-11/morph_target_smirk-quarter-frown-half-0.png">
</p></div>
</div>
<p>While conceptually simple, it requires communicating to the GPU a tremendous
amount of data. Thousand of vertices, each 288 bits, several model variations,
sometimes a hundred.</p>
<p>We store the vertex data as pixels in a 3D texture. This allows morph targets to not only
run on WebGPU, but also on the WebGL2 wgpu backend.</p>
<p>This could be improved in a number of ways, but it is sufficient for an
initial implementation.</p>

<h2 id="parallax-mapping">Parallax Mapping
<a href="#parallax-mapping">#</a>
</h2>
<p>author: @nicopap</p>
<p>Bevy now supports parallax mapping and depth maps. Parallax mapping puts normal
maps to shame when it comes to giving "illusion of depth" to a material. The top half of this video uses parallax mapping plus a normal map, whereas the bottom half only uses a normal map:</p>

<p>earth view, elevation &amp; night view by NASA (public domain)</p>
<p>Notice how it is not merely the shading of pixels that changes, but their
actual position on screen. The mountaintops hide mountain ridges behind
themselves. High mountains move faster than coastal areas.</p>
<p>Parallax mapping moves pixels according to the perspective and depth on the
surface of the geometry. Adding true 3D depth to flat surfaces.</p>
<p>All of that, without adding a single vertex to the geometry. The whole globe
has exactly 648 vertices. Unlike a more primitive shader, such as displacement
mapping, parallax mapping only requires an additional grayscale image, called
the <code>depth_map</code>.</p>
<p>Games often use parallax mapping for cobblestones or brick walls, so
let's make a brick wall in Bevy! First, we spawn a mesh:</p>
<pre data-lang="rust"><code data-lang="rust"><span>commands<span>.</span><span>spawn</span><span><span>(</span>PbrBundle <span><span>{</span>
    mesh<span>:</span> meshes<span>.</span><span>add</span><span><span>(</span><span>shape<span>::</span></span><span>Box<span>::</span></span>new<span><span>(</span><span>30.</span><span>0</span><span>,</span> <span>10.</span><span>0</span><span>,</span> <span>1.</span><span>0</span></span><span><span>)</span></span><span>.</span><span>into</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>,</span>
    material<span>:</span> materials<span>.</span><span>add</span><span><span>(</span>StandardMaterial <span><span>{</span>
        base_color<span>:</span> <span>Color<span>::</span></span><span>WHITE</span><span>,</span>
        <span>..</span><span>default</span><span><span>(</span></span><span><span>)</span></span>
    </span><span><span>}</span></span></span><span><span>)</span></span><span>,</span>
    <span>..</span><span>default</span><span><span>(</span></span><span><span>)</span></span>
</span><span><span>}</span></span></span><span><span>)</span></span><span>;</span>
</span></code></pre>
<p><img src="https://bevyengine.org/news/bevy-0-11/parallax_mapping_none_mini.jpg" alt="A 3D desert scene with two flat white walls and a pebble path winding between them"></p>
<p>Of course, it's just a flat white box, we didn't add any texture.
So let's add a normal map:</p>
<pre data-lang="rust"><code data-lang="rust"><span>normal_map_texture<span>:</span> <span>Some</span><span><span>(</span>assets<span>.</span><span>load</span><span><span>(</span><span><span>"</span>normal_map.png<span>"</span></span></span><span><span>)</span></span></span><span><span>)</span></span><span>,</span>
</span></code></pre>
<p><img src="https://bevyengine.org/news/bevy-0-11/parallax_mapping_normals_mini.jpg" alt="The same scene with normal maps"></p>
<p>This is much better. The shading changes according to the light direction too!
However, the specular highlights on the corner are overbearing, almost noisy.</p>
<p>Let's see how a depth map can help:</p>
<pre data-lang="rust"><code data-lang="rust"><span>depth_map<span>:</span> <span>Some</span><span><span>(</span>assets<span>.</span><span>load</span><span><span>(</span><span><span>"</span>depth_map.png<span>"</span></span></span><span><span>)</span></span></span><span><span>)</span></span><span>,</span>
</span></code></pre>
<p><img src="https://bevyengine.org/news/bevy-0-11/parallax_mapping_depth_mini.jpg" alt="The same scene with a depth texture"></p>
<p>We eliminated the noise! There is also that sweet 3D feel reminiscent of
90's games pre-rendered cinematic sequences.</p>
<p>So what's going on, why does parallax mapping eliminate the ugly specular
lights on the wall?</p>
<p>This is because parallax mapping insets the ridges between bricks, so that they
are occluded by the bricks themselves.</p>
<p><img src="https://bevyengine.org/news/bevy-0-11/ridge-light-view-1.svg" alt="Illustration of the previous paragraph"></p>
<p>Since normal maps do not "move" the shaded areas, merely shade them
differently, we get those awkward specular highlights. With parallax mapping,
they are gone.</p>
<p><b>Drag this image to compare</b></p>
<p><img alt="Normal Mapping Only" src="https://bevyengine.org/news/bevy-0-11/parallax_mapping_normals.jpg">
  <img alt="Parallax &amp; Normal Mapping" src="https://bevyengine.org/news/bevy-0-11/parallax_mapping_depth.jpg">
</p>
<p>Parallax mapping in Bevy is still very limited. The most painful aspect is that
it is not a standard glTF feature, meaning that the depth texture needs to be
programmatically added to materials if they came from a GLTF file.</p>
<p>Additionally, parallax mapping is incompatible with the temporal antialiasing
shader, doesn't work well on curved surfaces, and doesn't affect object's
silhouettes.</p>
<p>However, those are not fundamental limitations of parallax mapping, and may be
fixed in the future.</p>
<h2 id="skyboxes">Skyboxes
<a href="#skyboxes">#</a>
</h2>
<p>authors: @JMS55, @superdump</p>
<p><img src="https://bevyengine.org/news/bevy-0-11/skybox.png" alt="skybox"></p>
<p>Bevy now has built-in support for displaying an HDRI environment as your scene background.</p>
<p>Simply attach the new <a href="https://docs.rs/bevy/0.11.0/bevy/core_pipeline/struct.Skybox.html"><code>Skybox</code></a> component to your <a href="https://docs.rs/bevy/0.11.0/bevy/render/camera/struct.Camera.html"><code>Camera</code></a>. It pairs well with the existing <a href="https://docs.rs/bevy/0.11.0/bevy/pbr/struct.EnvironmentMapLight.html"><code>EnvironmentMapLight</code></a>, which will use the environment map to light the scene.</p>
<p>We also plan to add support for built-in procedural skyboxes sometime in the future!</p>
<h2 id="webgpu-support">WebGPU Support
<a href="#webgpu-support">#</a>
</h2>
<p>authors: @mockersf, many others throughout Bevy's development</p>
<p><img src="https://bevyengine.org/news/bevy-0-11/webgpu.svg" alt="webgpu"></p>
<p>Bevy now supports WebGPU rendering on the web (in addition to WebGL 2). WebGPU support is still rolling out, but if you have <a href="https://caniuse.com/webgpu">a supported web browser</a> you can explore our new <a href="https://bevyengine.org/examples-webgpu">live WebGPU examples</a> page.</p>
<h3 id="what-is-webgpu">What is WebGPU?
<a href="#what-is-webgpu">#</a>
</h3>
<p>WebGPU is an <a href="https://github.com/gpuweb/gpuweb">exciting new web standard</a> for doing modern GPU graphics and compute. It takes inspiration from Vulkan, Direct3D 12, and Metal. In fact, it is generally implemented on top of these APIs under the hood. WebGPU gives us access to more GPU features than WebGL2 (such as compute shaders) and also has the potential to be much faster. It means that more of Bevy's native renderer features are now also available on the web. It also uses the new <a href="https://www.w3.org/TR/WGSL">WGSL shader language</a>. We're very happy with how WGSL has evolved over time and Bevy uses it internally for our shaders. We also added usability features like imports! But with Bevy you still have the option to use GLSL if you prefer.</p>
<h3 id="how-it-works">How it Works
<a href="#how-it-works">#</a>
</h3>
<p>Bevy is built on top of the <a href="https://github.com/gfx-rs/wgpu">wgpu</a> library, which is a modern low-level GPU API that can target pretty much every popular API: Vulkan, Direct3D 12, Metal, OpenGL, WebGL2, and WebGPU. The best backend API is selected for a given platform. It is a "native" rendering API, but it generally follows the WebGPU terminology and API design. Unlike WebGPU, it can provide direct access to the native APIs, which means Bevy <a href="https://bevyengine.org/news/bevy-webgpu/#how-it-works">enjoys a "best of all worlds" situation</a>.</p>
<h3 id="webgpu-examples">WebGPU Examples
<a href="#webgpu-examples">#</a>
</h3>
<p>Click one of the images below to check out our live WebGPU examples (if your <a href="https://caniuse.com/webgpu">browser supports it</a>):</p>
<p><a href="https://bevyengine.org/examples-webgpu"><img src="https://bevyengine.org/news/bevy-0-11/webgpu_examples.png" alt="webgpu examples"></a></p>
<h2 id="improved-shader-imports">Improved Shader Imports
<a href="#improved-shader-imports">#</a>
</h2>
<p>authors: @robtfm</p>
<p>Bevy's rendering engine has a lot of great options and features. For example, the PBR <code>StandardMaterial</code> pipeline supports desktop/webgpu and webgl, 6 optional mesh attributes, 4 optional textures, and a plethora of optional features like fog, skinning, and alpha blending modes, with more coming in every release.</p>
<p>Many feature combos need specialized shader variants, and with over 3000 lines of shader code split over 50 files in total, the text-substitution-based shader processor was beginning to creak at the seams.</p>
<p>This release we've switched to using <a href="https://github.com/bevyengine/naga_oil">naga_oil</a>, which gives us a module-based shader framework. It compiles each file individually to naga's IR and then combines them into a final shader on demand. This doesn't have much visible impact yet, but it does give a few immediate benefits:</p>

<p>The future possibilities are more exciting. Using naga IR opens the door to a bunch of nice features that we hope to bring in future releases:</p>
<ul>
<li>Automatic bind slot allocation will let plugins extend the core view bindgroup, which means self-contained plugins for features like lighting and shadow methods, common material properties, etc become viable. This will allow us to modularise the core pipelines to make growing the codebase - while keeping support for multiple targets - more sustainable</li>
<li>"Virtual" shader functions will allow user modifications to core functions (like lighting), and potentially lead to a template-style material system, where users can provide "hooks" that will be called at the right point in the pipeline</li>
<li>Language interop: mix and match glsl and wgsl, so bevy's pbr pipeline features could be accessed from your glsl material shader, or utils written for glsl could be used in wgsl code. We're hopeful that this can extend to spirv (and rust-gpu) as well</li>
<li>More cool stuff we haven't thought of yet. Being able to inspect and modify shaders at runtime is very powerful and makes a lot of things possible!</li>
</ul>
<h2 id="ui-node-borders">UI Node Borders
<a href="#ui-node-borders">#</a>
</h2>
<p>authors: @ickshonpe</p>
<p>UI nodes now draws borders, whose color can be configured with the new <a href="https://docs.rs/bevy/0.11.0/bevy/ui/struct.BorderColor.html"><code>BorderColor</code></a> component:</p>
<p><img src="https://bevyengine.org/news/bevy-0-11/borders.png" alt="borders"></p>
<pre data-lang="rust"><code data-lang="rust"><span>commands<span>.</span><span>spawn</span><span><span>(</span>ButtonBundle <span><span>{</span>
    style<span>:</span> Style <span><span>{</span>
        border<span>:</span> <span>UiRect<span>::</span></span>all<span><span>(</span><span>Val<span>::</span></span>Px<span><span>(</span><span>5.</span><span>0</span></span><span><span>)</span></span></span><span><span>)</span></span><span>,</span>
        <span>..</span><span>default</span><span><span>(</span></span><span><span>)</span></span>
    </span><span><span>}</span></span><span>,</span>
    border_color<span>:</span> BorderColor<span><span>(</span><span>Color<span>::</span></span>rgb<span><span>(</span><span>0.</span><span>9</span><span>,</span> <span>0.</span><span>9</span><span>,</span> <span>0.</span><span>9</span></span><span><span>)</span></span></span><span><span>)</span></span><span>,</span>
    <span>..</span><span>default</span><span><span>(</span></span><span><span>)</span></span>
</span><span><span>}</span></span></span><span><span>)</span></span>
</span></code></pre>
<p>Each side of the border is configurable:</p>
<p><img src="https://bevyengine.org/news/bevy-0-11/border-sides.png" alt="border sides"></p>
<h2 id="grid-ui-layout">Grid UI Layout
<a href="#grid-ui-layout">#</a>
</h2>
<p>authors: @nicoburns</p>
<p>In Bevy UI we wired up the new <code>grid</code> feature in the layout library we use (<a href="https://github.com/DioxusLabs/taffy">Taffy</a>). This enables CSS-style grid layouts:</p>
<p><img src="https://bevyengine.org/news/bevy-0-11/grid.png" alt="grid"></p>
<p>This can be configured on the <a href="https://docs.rs/bevy/0.11.0/bevy/ui/struct.Style.html"><code>Style</code></a> component:</p>
<pre data-lang="rust"><code data-lang="rust"><span>Style <span><span>{</span>
    <span><span>///</span> Use grid layout for this node
</span>    display<span>:</span> <span>Display<span>::</span></span>Grid<span>,</span>
    <span><span>///</span> Make the grid have a 1:1 aspect ratio
</span>    <span><span>///</span> This means the width will adjust to match the height
</span>    aspect_ratio<span>:</span> <span>Some</span><span><span>(</span><span>1.</span><span>0</span></span><span><span>)</span></span><span>,</span>
    <span><span>//</span> Add 24px of padding around the grid
</span>    padding<span>:</span> <span>UiRect<span>::</span></span>all<span><span>(</span><span>Val<span>::</span></span>Px<span><span>(</span><span>24.</span><span>0</span></span><span><span>)</span></span></span><span><span>)</span></span><span>,</span>
    <span><span>///</span> Set the grid to have 4 columns all with sizes minmax(0, 1fr)
</span>    <span><span>///</span> This creates 4 exactly evenly sized columns
</span>    grid_template_columns<span>:</span> <span>RepeatedGridTrack<span>::</span></span>flex<span><span>(</span><span>4</span><span>,</span> <span>1.</span><span>0</span></span><span><span>)</span></span><span>,</span>
    <span><span>///</span> Set the grid to have 4 rows all with sizes minmax(0, 1fr)
</span>    <span><span>///</span> This creates 4 exactly evenly sized rows
</span>    grid_template_rows<span>:</span> <span>RepeatedGridTrack<span>::</span></span>flex<span><span>(</span><span>4</span><span>,</span> <span>1.</span><span>0</span></span><span><span>)</span></span><span>,</span>
    <span><span>///</span> Set a 12px gap/gutter between rows and columns
</span>    row_gap<span>:</span> <span>Val<span>::</span></span>Px<span><span>(</span><span>12.</span><span>0</span></span><span><span>)</span></span><span>,</span>
    column_gap<span>:</span> <span>Val<span>::</span></span>Px<span><span>(</span><span>12.</span><span>0</span></span><span><span>)</span></span><span>,</span>
    <span>..</span><span>default</span><span><span>(</span></span><span><span>)</span></span>
</span><span><span>}</span></span><span>,</span>
</span></code></pre>
<h2 id="schedule-first-ecs-apis">Schedule-First ECS APIs
<a href="#schedule-first-ecs-apis">#</a>
</h2>
<p>authors: @cart</p>
<p>In <strong>Bevy 0.10</strong> we introduced <a href="https://bevyengine.org/news/bevy-0-10/#ecs-schedule-v3">ECS Schedule V3</a>, which <em>vastly</em> improved the capabilities of Bevy ECS system scheduling: scheduler API ergonomics, system chaining, the ability to run exclusive systems and apply deferred system operations at any point in a schedule, a single unified schedule, configurable System Sets, run conditions, and a better State system.</p>
<p>However it pretty quickly became clear that the new system still had some areas to improve:</p>
<ul>
<li><strong>Base Sets were hard to understand and error prone</strong>: What <em>is</em> a Base Set? When do I use them? Why do they exist? Why is my ordering implicitly invalid due to incompatible Base Set ordering? Why do some schedules have a default Base Set while others don't? <a href="https://github.com/bevyengine/bevy/pull/8079#base-set-confusion">Base Sets were confusing!</a></li>
<li><strong>There were too many ways to schedule a System</strong>: We've accumulated too many scheduling APIs. As of Bevy <strong>0.10</strong>, we had <a href="https://github.com/bevyengine/bevy/pull/8079#unify-system-apis"><em>SIX</em> different ways to add a system to the "startup" schedule</a>. Thats too many ways!</li>
<li><strong>Too much implicit configuration</strong>: There were both default Schedules and default Base Sets. In some cases systems had default schedules or default base sets, but in other cases they didn't! <a href="https://github.com/bevyengine/bevy/pull/8079#schedule-should-be-clear">A system's schedule and configuration should be explicit and clear</a>.</li>
<li><strong>Adding Systems to Schedules wasn't ergonomic</strong>: Things like <code>add_system(foo.in_schedule(CoreSchedule::Startup))</code> were not fun to type or read. We created special-case helpers, such as <code>add_startup_system(foo)</code>, but <a href="https://github.com/bevyengine/bevy/pull/8079#ergonomic-system-adding">this required more internal code, user-defined schedules didn't benefit from the special casing, and it completely hid the <code>CoreSchedule::Startup</code> symbol!</a>.</li>
</ul>
<h3 id="unraveling-the-complexity">Unraveling the Complexity
<a href="#unraveling-the-complexity">#</a>
</h3>
<p>If your eyes started to glaze over as you tried to wrap your head around this, or phrases like "implicitly added to the <code>CoreSet::Update</code> Base Set" filled you with dread ... don't worry. After <a href="https://github.com/bevyengine/bevy/pull/8079">a lot of careful thought</a> we've unraveled the complexity and built something clear and simple.</p>
<p>In <strong>Bevy 0.11</strong> the "scheduling mental model" is <em>much</em> simpler thanks to <strong>Schedule-First ECS APIs</strong>:</p>
<pre data-lang="rust"><code data-lang="rust"><span>app
    <span>.</span><span>add_systems</span><span><span>(</span>Startup<span>,</span> <span><span>(</span>a<span>,</span> b</span><span><span>)</span></span></span><span><span>)</span></span>
    <span>.</span><span>add_systems</span><span><span>(</span>Update<span>,</span> <span><span>(</span>c<span>,</span> d<span>,</span> e</span><span><span>)</span></span></span><span><span>)</span></span>
    <span>.</span><span>add_systems</span><span><span>(</span>FixedUpdate<span>,</span> <span><span>(</span>f<span>,</span> g</span><span><span>)</span></span></span><span><span>)</span></span>
    <span>.</span><span>add_systems</span><span><span>(</span>PostUpdate<span>,</span> h</span><span><span>)</span></span>
    <span>.</span><span>add_systems</span><span><span>(</span>OnEnter<span><span>(</span><span>AppState<span>::</span></span>Menu</span><span><span>)</span></span><span>,</span> enter_menu</span><span><span>)</span></span>
    <span>.</span><span>add_systems</span><span><span>(</span>OnExit<span><span>(</span><span>AppState<span>::</span></span>Menu</span><span><span>)</span></span><span>,</span> exit_menu</span><span><span>)</span></span>
</span></code></pre>
<ul>
<li><strong>There is <em>exactly</em> one way to schedule systems</strong>
<ul>
<li>Call <code>add_systems</code>, state the schedule name, and specify one or more systems</li>
</ul>
</li>
<li><strong>Base Sets have been entirely removed in favor of Schedules, which have friendly / short names</strong>
<ul>
<li>Ex: The <code>CoreSet::Update</code> Base Set has become <code>Update</code></li>
</ul>
</li>
<li><strong>There is no implicit or implied configuration</strong>
<ul>
<li>Default Schedules and default Base Sets don't exist</li>
</ul>
</li>
<li><strong>The syntax is easy on the eyes and ergonomic</strong>
<ul>
<li>Schedules are first so they "line up" when formatted</li>
</ul>
</li>
</ul>
<details>
    <summary>To compare, expand this to see what it used to be!</summary>
<pre data-lang="rust"><code data-lang="rust"><span>app
    <span><span>//</span> Startup system variant 1.
</span>    <span><span>//</span> Has an implied default StartupSet::Startup base set
</span>    <span><span>//</span> Has an implied CoreSchedule::Startup schedule
</span>    <span>.</span><span>add_startup_systems</span><span><span>(</span><span><span>(</span>a<span>,</span> b</span><span><span>)</span></span></span><span><span>)</span></span>
    <span><span>//</span> Startup system variant 2.
</span>    <span><span>//</span> Has an implied default StartupSet::Startup base set
</span>    <span><span>//</span> Has an implied CoreSchedule::Startup schedule
</span>    <span>.</span><span>add_systems</span><span><span>(</span><span><span>(</span>a<span>,</span> b</span><span><span>)</span></span><span>.</span><span>on_startup</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span>
    <span><span>//</span> Startup system variant 3.
</span>    <span><span>//</span> Has an implied default StartupSet::Startup base set
</span>    <span>.</span><span>add_systems</span><span><span>(</span><span><span>(</span>a<span>,</span> b</span><span><span>)</span></span><span>.</span><span>in_schedule</span><span><span>(</span><span>CoreSchedule<span>::</span></span>Startup</span><span><span>)</span></span></span><span><span>)</span></span>
    <span><span>//</span> Update system variant 1.
</span>    <span><span>//</span> `CoreSet::Update` base set and `CoreSchedule::Main` are implied
</span>    <span>.</span><span>add_system</span><span><span>(</span>c</span><span><span>)</span></span>
    <span><span>//</span> Update system variant 2 (note the add_system vs add_systems difference)
</span>    <span><span>//</span> `CoreSet::Update` base set and `CoreSchedule::Main` are implied
</span>    <span>.</span><span>add_systems</span><span><span>(</span><span><span>(</span>d<span>,</span> e</span><span><span>)</span></span></span><span><span>)</span></span>
    <span><span>//</span> No implied default base set because CoreSchedule::FixedUpdate doesn't have one
</span>    <span>.</span><span>add_systems</span><span><span>(</span><span><span>(</span>f<span>,</span> g</span><span><span>)</span></span><span>.</span><span>in_schedule</span><span><span>(</span><span>CoreSchedule<span>::</span></span>FixedUpdate</span><span><span>)</span></span></span><span><span>)</span></span>
    <span><span>//</span> `CoreSchedule::Main` is implied, in_base_set overrides the default CoreSet::Update set
</span>    <span>.</span><span>add_system</span><span><span>(</span>h<span>.</span><span>in_base_set</span><span><span>(</span><span>CoreSet<span>::</span></span>PostUpdate</span><span><span>)</span></span></span><span><span>)</span></span>
    <span><span>//</span> This has no implied default base set
</span>    <span>.</span><span>add_systems</span><span><span>(</span>enter_menu<span>.</span><span>in_schedule</span><span><span>(</span>OnEnter<span><span>(</span><span>AppState<span>::</span></span>Menu</span><span><span>)</span></span></span><span><span>)</span></span></span><span><span>)</span></span>
    <span><span>//</span> This has no implied default base set
</span>    <span>.</span><span>add_systems</span><span><span>(</span>exit_menu<span>.</span><span>in_schedule</span><span><span>(</span>OnExit<span><span>(</span><span>AppState<span>::</span></span>Menu</span><span><span>)</span></span></span><span><span>)</span></span></span><span><span>)</span></span>
</span></code></pre>
</details>
<p>Note that normal "system sets" still exist! You can still use sets to organize and order your systems:</p>
<pre data-lang="rust"><code data-lang="rust"><span>app<span>.</span><span>add_systems</span><span><span>(</span>Update<span>,</span> <span><span>(</span>
    <span><span>(</span>walk<span>,</span> jump</span><span><span>)</span></span><span>.</span><span>in_set</span><span><span>(</span>Movement</span><span><span>)</span></span><span>,</span>
    collide<span>.</span><span>after</span><span><span>(</span>Movement</span><span><span>)</span></span><span>,</span>
</span><span><span>)</span></span></span><span><span>)</span></span>
</span></code></pre>
<p>The <code>configure_set</code> API has also been adjusted for parity:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>//</span> Bevy 0.10
</span>app<span>.</span><span>configure_set</span><span><span>(</span>Foo<span>.</span><span>after</span><span><span>(</span>Bar</span><span><span>)</span></span><span>.</span><span>in_schedule</span><span><span>(</span>PostUpdate</span><span><span>)</span></span></span><span><span>)</span></span>
<span><span>//</span> Bevy 0.11
</span>app<span>.</span><span>configure_set</span><span><span>(</span>PostUpdate<span>,</span> Foo<span>.</span><span>after</span><span><span>(</span>Bar</span><span><span>)</span></span></span><span><span>)</span></span>
</span></code></pre>
<h2 id="nested-system-tuples-and-chaining">Nested System Tuples and Chaining
<a href="#nested-system-tuples-and-chaining">#</a>
</h2>
<p>authors: @cart</p>
<p>It is now possible to infinitely nest tuples of systems in a <code>.add_systems</code> call!</p>
<pre data-lang="rust"><code data-lang="rust"><span>app<span>.</span><span>add_systems</span><span><span>(</span>Update<span>,</span> <span><span>(</span>
    <span><span>(</span>a<span>,</span> <span><span>(</span>b<span>,</span> c<span>,</span> d<span>,</span> e</span><span><span>)</span></span><span>,</span> f</span><span><span>)</span></span><span>,</span>
    <span><span>(</span>g<span>,</span> h</span><span><span>)</span></span><span>,</span>
    i
</span><span><span>)</span></span></span><span><span>)</span></span>
</span></code></pre>
<p>At first glance, this might not seem very useful. But in combination with per-tuple configuration, it allows you to easily and cleanly express schedules:</p>
<pre data-lang="rust"><code data-lang="rust"><span>app<span>.</span><span>add_systems</span><span><span>(</span>Update<span>,</span> <span><span>(</span>
    <span><span>(</span>attack<span>,</span> defend</span><span><span>)</span></span><span>.</span><span>in_set</span><span><span>(</span>Combat</span><span><span>)</span></span><span>.</span><span>before</span><span><span>(</span>check_health</span><span><span>)</span></span>
    check_health<span>,</span>
    <span><span>(</span>handle_death<span>,</span> respawn</span><span><span>)</span></span><span>.</span><span>after</span><span><span>(</span>check_health</span><span><span>)</span></span>
</span><span><span>)</span></span></span><span><span>)</span></span>
</span></code></pre>
<p><code>.chain()</code> has also been adapted to support arbitrary nesting! The ordering in the example above could be rephrased like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span>app<span>.</span><span>add_systems</span><span><span>(</span>Update<span>,</span>
    <span><span>(</span>
        <span><span>(</span>attack<span>,</span> defend</span><span><span>)</span></span><span>.</span><span>in_set</span><span><span>(</span>Combat</span><span><span>)</span></span>
        check_health<span>,</span>
        <span><span>(</span>handle_death<span>,</span> respawn</span><span><span>)</span></span>
    </span><span><span>)</span></span><span>.</span><span>chain</span><span><span>(</span></span><span><span>)</span></span>
</span><span><span>)</span></span>
</span></code></pre>
<p>This will run <code>attack</code> and <code>defend</code> first (in parallel), then <code>check_health</code>, then <code>handle_death</code> and <code>respawn</code> (in parallel).</p>
<p>This allows for powerful and expressive "graph-like" ordering expressions:</p>
<pre data-lang="rust"><code data-lang="rust"><span>app<span>.</span><span>add_systems</span><span><span>(</span>Update<span>,</span>
    <span><span>(</span>
        <span><span>(</span>a<span>,</span> <span><span>(</span>b<span>,</span> c<span>,</span> d</span><span><span>)</span></span><span>.</span><span>chain</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>,</span>
        <span><span>(</span>e<span>,</span> f</span><span><span>)</span></span><span>,</span>
    </span><span><span>)</span></span><span>.</span><span>chain</span><span><span>(</span></span><span><span>)</span></span>
</span><span><span>)</span></span>
</span></code></pre>
<p>This will run <code>a</code> in parallel with <code>b-&gt;c-&gt;d</code>, then after those have finished running it will run <code>e</code> and <code>f</code> in parallel.</p>
<h2 id="gizmos">Gizmos
<a href="#gizmos">#</a>
</h2>
<p>authors: @devil-ira, @jannik4, @lassade, @The5-1, @Toqozz, @nicopap</p>
<p>It is often helpful to be able to draw simple shapes and lines in 2D and 3D for things like editor controls, and debug views. Game development is a very "spatial" thing and being able to quickly draw shapes is the visual equivalent of "print line debugging". It helps answer questions like "is this ray casting in the right direction?" and "is this collider big enough?"</p>
<p>In <strong>Bevy 0.11</strong> we've added an "immediate mode" <a href="https://docs.rs/bevy/0.11.0/bevy/gizmos/gizmos/struct.Gizmos.html"><code>Gizmos</code></a> drawing API that makes these things easy and efficient. In 2D and 3D you can draw lines, rects, circles, arcs, spheres, cubes, line strips, and more!</p>
<p><strong>2D Gizmos</strong>
<img src="https://bevyengine.org/news/bevy-0-11/2d_gizmos.png" alt="2d gizmos">
<strong>3D Gizmos</strong>
<img src="https://bevyengine.org/news/bevy-0-11/3d_gizmos.png" alt="3d gizmos"></p>
<p>From any system you can spawn shapes into existence (for both 2D and 3D):</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span><span>fn</span> </span><span>system</span></span><span><span><span>(</span><span>mut</span> <span>gizmos</span><span>:</span> Gizmos</span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
    <span><span>//</span> 2D
</span>    gizmos<span>.</span><span>line_2d</span><span><span>(</span><span>Vec2<span>::</span></span>new<span><span>(</span><span>0.</span><span>,</span> <span>0.</span></span><span><span>)</span></span><span>,</span> <span>Vec2<span>::</span></span>new<span><span>(</span><span>0.</span><span>,</span> <span>10.</span></span><span><span>)</span></span><span>,</span> <span>Color<span>::</span></span><span>RED</span></span><span><span>)</span></span><span>;</span>
    gizmos<span>.</span><span>circle_2d</span><span><span>(</span><span>Vec2<span>::</span></span>new<span><span>(</span><span>0.</span><span>,</span> <span>0.</span></span><span><span>)</span></span><span>,</span> <span>40.</span><span>,</span> <span>Color<span>::</span></span><span>BLUE</span></span><span><span>)</span></span><span>;</span>
    <span><span>//</span> 3D
</span>    gizmos<span>.</span><span>circle</span><span><span>(</span><span>Vec3<span>::</span></span><span>ZERO</span><span>,</span> <span>Vec3<span>::</span></span>Y<span>,</span> <span>3.</span><span>,</span> <span>Color<span>::</span></span><span>BLACK</span></span><span><span>)</span></span><span>;</span>
    gizmos<span>.</span><span>ray</span><span><span>(</span><span>Vec3<span>::</span></span>new<span><span>(</span><span>0.</span><span>,</span> <span>0.</span><span>,</span> <span>0.</span></span><span><span>)</span></span><span>,</span> <span>Vec3<span>::</span></span>new<span><span>(</span><span>5.</span><span>,</span> <span>5.</span><span>,</span> <span>5.</span></span><span><span>)</span></span><span>,</span> <span>Color<span>::</span></span><span>BLUE</span></span><span><span>)</span></span><span>;</span>
    gizmos<span>.</span><span>sphere</span><span><span>(</span><span>Vec3<span>::</span></span><span>ZERO</span><span>,</span> <span>Quat<span>::</span></span><span>IDENTITY</span><span>,</span> <span>3.</span><span>2</span><span>,</span> <span>Color<span>::</span></span><span>BLACK</span></span><span><span>)</span></span>
</span><span><span>}</span></span></span>
</span></code></pre>
<p>Because the API is "immediate mode", gizmos will only be drawn on frames where they are "queued up", which means you don't need to worry about cleaning up gizmo state!</p>
<p>Gizmos are drawn in batches, which means they are very cheap. You can have hundreds of thousands of them!</p>
<h2 id="ecs-audio-apis">ECS Audio APIs
<a href="#ecs-audio-apis">#</a>
</h2>
<p>authors: @inodentry</p>
<p>Bevy's audio playback APIs have been reworked to integrate more cleanly with Bevy's ECS.</p>
<p>In previous versions of Bevy you would play back audio like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>#</span><span>[</span><span>derive</span><span><span><span>(</span></span></span><span><span>Resource</span></span><span><span><span>)</span></span></span><span>]</span></span>
<span><span>struct</span> </span><span><span>MyMusic</span> </span><span><span><span>{</span>
    <span>sink</span><span>:</span> <span>Handle<span>&lt;</span>AudioSink<span>&gt;</span></span>,
</span><span><span>}</span></span></span>

<span><span><span>fn</span> </span><span>play_music</span></span><span><span><span>(</span>
    <span>mut</span> <span>commands</span><span>:</span> Commands,
    <span>asset_server</span><span>:</span> <span>Res<span>&lt;</span>AssetServer<span>&gt;</span></span>,
    <span>audio</span><span>:</span> <span>Res<span>&lt;</span>Audio<span>&gt;</span></span>,
    <span>audio_sinks</span><span>:</span> <span>Res<span>&lt;</span><span>Assets<span>&lt;</span>AudioSink<span>&gt;</span></span><span>&gt;</span></span>
</span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
    <span>let</span> weak_handle <span>=</span> audio<span>.</span><span>play</span><span><span>(</span>asset_server<span>.</span><span>load</span><span><span>(</span><span><span>"</span>my_music.ogg<span>"</span></span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
    <span>let</span> strong_handle <span>=</span> audio_sinks<span>.</span><span>get_handle</span><span><span>(</span>weak_handle</span><span><span>)</span></span><span>;</span>
    commands<span>.</span><span>insert_resource</span><span><span>(</span>MyMusic <span><span>{</span>
        sink<span>:</span> strong_handle<span>,</span>
    </span><span><span>}</span></span></span><span><span>)</span></span><span>;</span>
</span><span><span>}</span></span></span>
</span></code></pre>
<p>That is a lot of boilerplate just to play a sound! Then to adjust playback you would access the <a href="https://docs.rs/bevy/0.11.0/bevy/audio/struct.AudioSink.html"><code>AudioSink</code></a> like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span>
<span><span><span>fn</span> </span><span>pause_music</span></span><span><span><span>(</span><span>my_music</span><span>:</span> <span>Res<span>&lt;</span>MyMusic<span>&gt;</span></span>, <span>audio_sinks</span><span>:</span> <span>Res<span>&lt;</span><span>Assets<span>&lt;</span>AudioSink<span>&gt;</span></span><span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
    <span>if</span> <span>let</span> <span>Some</span><span><span>(</span>sink</span><span><span>)</span></span> <span>=</span> audio_sinks<span>.</span><span>get</span><span><span>(</span><span>&amp;</span>my_music<span>.</span>sink</span><span><span>)</span></span> <span><span>{</span>
        sink<span>.</span><span>pause</span><span><span>(</span></span><span><span>)</span></span><span>;</span>
    </span><span><span>}</span></span>
</span><span><span>}</span></span></span>
</span></code></pre>
<p>Treating audio playback as a resource created a number of problems and notably didn't play well with things like Bevy Scenes. In <strong>Bevy 0.11</strong>, audio playback is represented as an <a href="https://docs.rs/bevy/0.11.0/bevy/ecs/entity/struct.Entity.html"><code>Entity</code></a> with <a href="https://docs.rs/bevy/0.11.0/bevy/audio/type.AudioBundle.html"><code>AudioBundle</code></a> components:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>#</span><span>[</span><span>derive</span><span><span><span>(</span></span></span><span><span>Component</span></span><span><span><span>)</span></span></span><span>]</span></span>
<span><span>struct</span> </span><span><span>MyMusic</span></span><span>;</span>

<span><span><span>fn</span> </span><span>play_music</span></span><span><span><span>(</span><span>mut</span> <span>commands</span><span>:</span> Commands, <span>asset_server</span><span>:</span> <span>Res<span>&lt;</span>AssetServer<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
    commands<span>.</span><span>spawn</span><span><span>(</span><span><span>(</span>
        AudioBundle <span><span>{</span>
            source<span>:</span> asset_server<span>.</span><span>load</span><span><span>(</span><span><span>"</span>my_music.ogg<span>"</span></span></span><span><span>)</span></span><span>,</span>
            <span>..</span><span>default</span><span><span>(</span></span><span><span>)</span></span>
        </span><span><span>}</span></span><span>,</span>
        MyMusic<span>,</span>
    </span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
</span><span><span>}</span></span></span>
</span></code></pre>
<p>Much simpler! To adjust playback you can query for the <a href="https://docs.rs/bevy/0.11.0/bevy/audio/struct.AudioSink.html"><code>AudioSink</code></a> component:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span><span>fn</span> </span><span>pause_music</span></span><span><span><span>(</span><span>query_music</span><span>:</span> <span>Query<span>&lt;</span><span>&amp;</span>AudioSink, <span>With<span>&lt;</span>MyMusic<span>&gt;</span></span><span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
    <span>if</span> <span>let</span> <span>Ok</span><span><span>(</span>sink</span><span><span>)</span></span> <span>=</span> query<span>.</span><span>get_single</span><span><span>(</span></span><span><span>)</span></span> <span><span>{</span>
        sink<span>.</span><span>pause</span><span><span>(</span></span><span><span>)</span></span><span>;</span>
    </span><span><span>}</span></span>
</span><span><span>}</span></span></span>
</span></code></pre>
<h2 id="global-audio-volume">Global Audio Volume
<a href="#global-audio-volume">#</a>
</h2>
<p>authors: @mrchantey</p>
<p>Bevy now has a global volume level which can be configured via the [<code>GlobalVolume</code>] resource:</p>
<pre data-lang="rust"><code data-lang="rust"><span>app<span>.</span><span>insert_resource</span><span><span>(</span><span>GlobalVolume<span>::</span></span>new<span><span>(</span><span>0.</span><span>2</span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
</span></code></pre>
<h2 id="resource-support-in-scenes">Resource Support in Scenes
<a href="#resource-support-in-scenes">#</a>
</h2>
<p>authors: @Carbonhell, @Davier</p>
<p>Bevy's scene format is a very useful tool for serializing and deserializing game state to and from scene files.</p>
<p>Previously, the captured state was limited to only entities and their components.
With <strong>Bevy 0.11</strong>, scenes now support serializing resources as well.</p>
<p>This adds a new <code>resources</code> field to the scene format:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>(</span>
    resources<span>:</span> <span><span>{</span>
        <span><span>"</span>my_game::stats::TotalScore<span>"</span></span><span>:</span> <span><span>(</span>
            score<span>:</span> <span>9001</span><span>,</span>
        </span><span><span>)</span></span><span>,</span>
    </span><span><span>}</span></span><span>,</span>
    entities<span>:</span> <span><span>{</span>
        <span><span>//</span> Entity scene data...
</span>    </span><span><span>}</span></span><span>,</span>
</span><span><span>)</span></span>
</span></code></pre>
<h2 id="scene-filtering">Scene Filtering
<a href="#scene-filtering">#</a>
</h2>
<p>authors: @MrGVSV</p>
<p>When serializing data to a scene, all components and <a href="https://bevyengine.org/news/bevy-0-11/#resource-support-in-scenes">resources</a> are serialized by default.
In previous versions, you had to use the given <code>TypeRegistry</code> to act as a filter, leaving out the types you don't want included.</p>
<p>In 0.11, there's now a dedicated <code>SceneFilter</code> type to make filtering easier, cleaner, and more intuitive.
This can be used with <a href="https://docs.rs/bevy/0.11.0/bevy/prelude/struct.DynamicSceneBuilder.html"><code>DynamicSceneBuilder</code></a> to have fine-grained control over what actually gets serialized.</p>
<p>We can <code>allow</code> a subset of types:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span>let</span> <span>mut</span> builder <span>=</span> <span>DynamicSceneBuilder<span>::</span></span>from_world<span><span>(</span><span>&amp;</span>world</span><span><span>)</span></span><span>;</span>
<span>let</span> scene <span>=</span> builder
    <span>.</span><span>allow<span>::</span></span><span><span>&lt;</span>ComponentA<span>&gt;</span></span><span><span>(</span></span><span><span>)</span></span>
    <span>.</span><span>allow<span>::</span></span><span><span>&lt;</span>ComponentB<span>&gt;</span></span><span><span>(</span></span><span><span>)</span></span>
    <span>.</span><span>extract_entity</span><span><span>(</span>entity</span><span><span>)</span></span>
    <span>.</span><span>build</span><span><span>(</span></span><span><span>)</span></span><span>;</span>
</span></code></pre>
<p>Or <code>deny</code> them:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span>let</span> <span>mut</span> builder <span>=</span> <span>DynamicSceneBuilder<span>::</span></span>from_world<span><span>(</span><span>&amp;</span>world</span><span><span>)</span></span><span>;</span>
<span>let</span> scene <span>=</span> builder
    <span>.</span><span>deny<span>::</span></span><span><span>&lt;</span>ComponentA<span>&gt;</span></span><span><span>(</span></span><span><span>)</span></span>
    <span>.</span><span>deny<span>::</span></span><span><span>&lt;</span>ComponentB<span>&gt;</span></span><span><span>(</span></span><span><span>)</span></span>
    <span>.</span><span>extract_entity</span><span><span>(</span>entity</span><span><span>)</span></span>
    <span>.</span><span>build</span><span><span>(</span></span><span><span>)</span></span><span>;</span>
</span></code></pre>
<h2 id="default-font">Default Font
<a href="#default-font">#</a>
</h2>
<p>authors: @mockersf</p>
<p>Bevy now supports a configurable default font and embeds a tiny default font (a minimal version of <a href="https://fonts.google.com/specimen/Fira+Mono">Fira Mono</a>). This is useful if you use a common font throughout your project. And it makes it easier to prototype new changes with a "placeholder font" without worrying about setting it on each node.</p>
<p><img src="https://bevyengine.org/news/bevy-0-11/default_font.png" alt="default font"></p>
<h2 id="ui-texture-atlas-support">UI Texture Atlas Support
<a href="#ui-texture-atlas-support">#</a>
</h2>
<p>authors: @mwbryant</p>
<p>Previously UI <code>ImageBundle</code> Nodes could only use handles to full images without an ergonomic way to use <code>TextureAtlases</code> in UI.  In this release we add support for an <code>AtlasImageBundle</code> UI Node which brings the existing <code>TextureAtlas</code> support into UI.</p>
<p>This was achieved by merging the existing mechanisms that allows text rendering to select which glyph to use and the mechanisms that allow for <code>TextureAtlasSprite</code>.</p>

<h2 id="gamepad-rumble-api">Gamepad Rumble API
<a href="#gamepad-rumble-api">#</a>
</h2>
<p>authors: @johanhelsing, @nicopap</p>
<p>You can now use the <code>EventWriter&lt;GamepadRumbleRequest&gt;</code> system parameter to
trigger controllers force-feedback motors.</p>
<p><a href="https://crates.io/crates/gilrs"><code>gilrs</code></a>, the crate Bevy uses for gamepad support, allows controlling
force-feedback motors. Sadly, there were no easy way of accessing the
force-feedback API in Bevy without tedious bookkeeping.</p>
<p>Now Bevy has the <code>GamepadRumbleRequest</code> event to do just that.</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span><span>fn</span> </span><span>rumble_system</span></span><span><span><span>(</span>
    <span>gamepads</span><span>:</span> <span>Res<span>&lt;</span>Gamepads<span>&gt;</span></span>,
    <span>mut</span> <span>rumble_requests</span><span>:</span> <span>EventWriter<span>&lt;</span>GamepadRumbleRequest<span>&gt;</span></span>,
</span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
    <span>for</span> gamepad <span>in</span> gamepads<span>.</span><span>iter</span><span><span>(</span></span><span><span>)</span></span> <span><span>{</span>
        rumble_requests<span>.</span><span>send</span><span><span>(</span><span>GamepadRumbleRequest<span>::</span></span>Add <span><span>{</span>
            gamepad<span>,</span>
            duration<span>:</span> <span>Duration<span>::</span></span>from_secs<span><span>(</span><span>5</span></span><span><span>)</span></span><span>,</span>
            intensity<span>:</span> <span>GamepadRumbleIntensity<span>::</span></span><span>MAX</span><span>,</span>
        </span><span><span>}</span></span></span><span><span>)</span></span><span>;</span>
    </span><span><span>}</span></span>
</span><span><span>}</span></span></span>
</span></code></pre>
<p>The <code>GamepadRumbleRequest::Add</code> event triggers a force-feedback motor,
controlling how long the vibration should last, the motor to activate,
and the vibration strength. <code>GamepadRumbleRequest::Stop</code> immediately stops all motors.</p>
<h2 id="new-default-tonemapping-method">New Default Tonemapping Method
<a href="#new-default-tonemapping-method">#</a>
</h2>
<p>authors: @JMS55</p>
<p>In <strong>Bevy 0.10</strong> we <a href="https://bevyengine.org/news/bevy-0-10/#more-tonemapping-choices">made tonemapping configurable with a ton of new tonemapping options</a>. In <strong>Bevy 0.11</strong> we've switched the default tonemapping method from "Reinhard luminance" tonemapping to "TonyMcMapface":</p>
<p><b>Drag this image to compare</b></p>
<p><img alt="Reinhard-luminance" src="https://bevyengine.org/news/bevy-0-11/tm_reinhard_luminance.png">
  <img alt="TonyMcMapface" src="https://bevyengine.org/news/bevy-0-11/tm_tonymcmapface.png">
</p>
<p>TonyMcMapface (<a href="https://github.com/h3r2tic/tony-mc-mapface">created by Tomasz Stachowiak</a>) is a much more neutral display transform that tries to stay as close to the input "light" as possible. This helps retain artistic choices in the scene. Notably, brights desaturate across the entire spectrum (unlike Reinhard luminance). It also works much better with bloom when compared to Reinhard luminance.</p>
<h2 id="entityref-queries">EntityRef Queries
<a href="#entityref-queries">#</a>
</h2>
<p>authors: @james7132</p>
<p><a href="https://docs.rs/bevy/0.11.0/bevy/ecs/world/struct.EntityRef.html"><code>EntityRef</code></a> now implements <a href="https://docs.rs/bevy/0.11.0/bevy/ecs/query/trait.WorldQuery.html"><code>WorldQuery</code></a>, which makes it easier to query for arbitrary components in your ECS systems:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span><span>fn</span> </span><span>system</span></span><span><span><span>(</span><span>query</span><span>:</span> <span>Query<span>&lt;</span>EntityRef<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
    <span>for</span> entity <span>in</span> <span>&amp;</span>query <span><span>{</span>
        <span>if</span> <span>let</span> <span>Some</span><span><span>(</span>mesh</span><span><span>)</span></span> <span>=</span> entity<span>.</span><span>get<span>::</span></span><span><span>&lt;</span><span>Handle<span>&lt;</span>Mesh<span>&gt;</span></span><span>&gt;</span></span><span><span>(</span></span><span><span>)</span></span> <span><span>{</span>
            <span>let</span> transform <span>=</span> entity<span>.</span><span>get<span>::</span></span><span><span>&lt;</span>Transform<span>&gt;</span></span><span><span>(</span></span><span><span>)</span></span><span>.</span><span>unwrap</span><span><span>(</span></span><span><span>)</span></span><span>;</span>
        </span><span><span>}</span></span>
    </span><span><span>}</span></span>
</span><span><span>}</span></span></span>
</span></code></pre>
<p>Note that <a href="https://docs.rs/bevy/0.11.0/bevy/ecs/world/struct.EntityRef.html"><code>EntityRef</code></a> queries access every entity and every component in the entire <a href="https://docs.rs/bevy/0.11.0/bevy/ecs/world/struct.World.html"><code>World</code></a> by default. This means that they will conflict with any "mutable" query:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>///</span> These queries will conflict, making this system invalid
</span><span><span><span>fn</span> </span><span>system</span></span><span><span><span>(</span><span>query</span><span>:</span> <span>Query<span>&lt;</span>EntityRef<span>&gt;</span></span>, <span>mut</span> <span>enemies</span><span>:</span> <span>Query<span>&lt;</span><span>&amp;</span><span>mut</span> Enemy<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span> </span><span><span>}</span></span></span>
</span></code></pre>
<p>To resolve conflicts (or reduce the number of entities accessed), you can add filters:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>///</span> These queries will not conflict
</span><span><span><span>fn</span> </span><span>system</span></span><span><span><span>(</span>
    <span>players</span><span>:</span> <span>Query<span>&lt;</span>EntityRef, <span>With<span>&lt;</span>Player<span>&gt;</span></span><span>&gt;</span></span>,
    <span>mut</span> <span>enemies</span><span>:</span> <span>Query<span>&lt;</span><span>&amp;</span><span>mut</span> Enemy, <span>Without<span>&lt;</span>Player<span>&gt;</span></span><span>&gt;</span></span>
</span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
    <span><span>//</span> only iterates players
</span>    <span>for</span> entity <span>in</span> <span>&amp;</span>players <span><span>{</span>
        <span>if</span> <span>let</span> <span>Some</span><span><span>(</span>mesh</span><span><span>)</span></span> <span>=</span> entity<span>.</span><span>get<span>::</span></span><span><span>&lt;</span><span>Handle<span>&lt;</span>Mesh<span>&gt;</span></span><span>&gt;</span></span><span><span>(</span></span><span><span>)</span></span> <span><span>{</span>
            <span>let</span> transform <span>=</span> entity<span>.</span><span>get<span>::</span></span><span><span>&lt;</span>Transform<span>&gt;</span></span><span><span>(</span></span><span><span>)</span></span><span>.</span><span>unwrap</span><span><span>(</span></span><span><span>)</span></span><span>;</span>
        </span><span><span>}</span></span>
    </span><span><span>}</span></span>
</span><span><span>}</span></span></span>
</span></code></pre>
<p>Note that it will generally still be more ergonomic (and more efficient) to query for the components you want directly:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span><span>fn</span> </span><span>system</span></span><span><span><span>(</span><span>players</span><span>:</span> <span>Query<span>&lt;</span><span>(</span><span>&amp;</span>Transform, <span>&amp;</span><span>Handle<span>&lt;</span>Mesh<span>&gt;</span></span><span>)</span>, <span>With<span>&lt;</span>Player<span>&gt;</span></span><span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
    <span>for</span> <span><span>(</span>transform<span>,</span> mesh</span><span><span>)</span></span> <span>in</span> <span>&amp;</span>players <span><span>{</span>
    </span><span><span>}</span></span>
</span><span><span>}</span></span></span>
</span></code></pre>
<h2 id="screenshot-api">Screenshot API
<a href="#screenshot-api">#</a>
</h2>
<p>authors: @TheRawMeatball</p>
<p>Bevy now has a simple screenshot API that can save a screenshot of a given window to the disk:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span><span>fn</span> </span><span>take_screenshot</span></span><span><span><span>(</span>
    <span>mut</span> <span>screenshot_manager</span><span>:</span> <span>ResMut<span>&lt;</span>ScreenshotManager<span>&gt;</span></span>,
    <span>input</span><span>:</span> <span>Res<span>&lt;</span><span>Input<span>&lt;</span>KeyCode<span>&gt;</span></span><span>&gt;</span></span>,
    <span>primary_window</span><span>:</span> <span>Query<span>&lt;</span>Entity, <span>With<span>&lt;</span>PrimaryWindow<span>&gt;</span></span><span>&gt;</span></span>,
</span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
    <span>if</span> input<span>.</span><span>just_pressed</span><span><span>(</span><span>KeyCode<span>::</span></span>Space</span><span><span>)</span></span> <span><span>{</span>
        screenshot_manager
            <span>.</span><span>save_screenshot_to_disk</span><span><span>(</span>primary_window<span>.</span><span>single</span><span><span>(</span></span><span><span>)</span></span><span>,</span> <span><span>"</span>screenshot.png<span>"</span></span></span><span><span>)</span></span>
            <span>.</span><span>unwrap</span><span><span>(</span></span><span><span>)</span></span><span>;</span>
    </span><span><span>}</span></span>
</span><span><span>}</span></span></span>
</span></code></pre>
<h2 id="rendertarget-textureview">RenderTarget::TextureView
<a href="#rendertarget-textureview">#</a>
</h2>
<p>authors: @mrchantey</p>
<p>The <a href="https://docs.rs/bevy/0.11.0/bevy/render/camera/struct.Camera.html"><code>Camera</code></a> <a href="https://docs.rs/bevy/0.11.0/bevy/render/camera/enum.RenderTarget.html"><code>RenderTarget</code></a> can now be set to a wgpu <a href="https://docs.rs/bevy/0.11.0/bevy/render/render_resource/struct.TextureView.html"><code>TextureView</code></a>. This allows 3rd party Bevy Plugins to manage a <a href="https://docs.rs/bevy/0.11.0/bevy/render/camera/struct.Camera.html"><code>Camera</code></a>'s texture. One particularly interesting use case that this enables is XR/VR support. A few community members have already <a href="https://github.com/bevyengine/bevy/issues/115#issuecomment-1436749201">proven this out!</a></p>
<h2 id="improved-text-wrapping">Improved Text Wrapping
<a href="#improved-text-wrapping">#</a>
</h2>
<p>authors: @ickshonpe</p>
<p>Previous versions of Bevy didn't properly wrap text because it calculated the actual text prior to calculating layout. <strong>Bevy 0.11</strong> adds a "text measurement step" that calculates the text size prior to layout, then computes the actual text <em>after</em> layout.</p>
<p><img src="https://bevyengine.org/news/bevy-0-11/text_wrap.png" alt="text wrap"></p>
<p>There is also a new <code>NoWrap</code> variant on the <a href="https://docs.rs/bevy/0.11.0/bevy/text/enum.BreakLineOn.html"><code>BreakLineOn</code></a> setting, which can disable text wrapping entirely when that is desirable.</p>
<h2 id="faster-ui-render-batching">Faster UI Render Batching
<a href="#faster-ui-render-batching">#</a>
</h2>
<p>authors: @ickshonpe</p>
<p>We got a huge UI performance win for some cases by avoiding breaking up UI batches when the texture changes but the next node is untextured.</p>
<p>Here is a profile of our "many buttons" stress test. Red is before the optimization and Yellow is after:</p>
<p><img src="https://bevyengine.org/news/bevy-0-11/ui_profile.png" alt="ui profile"></p>
<h2 id="better-reflect-proxies">Better Reflect Proxies
<a href="#better-reflect-proxies">#</a>
</h2>
<p>authors: @MrGVSV</p>
<p>Bevy's reflection API has a handful of structs which are collectively known as "dynamic" types.
These include <a href="https://docs.rs/bevy/0.11.0/bevy/reflect/struct.DynamicStruct.html"><code>DynamicStruct</code></a>, <a href="https://docs.rs/bevy/0.11.0/bevy/reflect/struct.DynamicTuple.html"><code>DynamicTuple</code></a>, and more, and they are used to dynamically construct types
of any shape or form at runtime.
These types are also used to create are commonly referred to as "proxies", which are dynamic types
that are used to represent an actual concrete type.</p>
<p>These proxies are what powers the <a href="https://docs.rs/bevy/0.11.0/bevy/reflect/trait.Reflect.html#tymethod.clone_value"><code>Reflect::clone_value</code></a> method, which generates these proxies under the hood
in order to construct a runtime clone of the data.</p>
<p>Unfortunately, this results in a few <a href="https://github.com/bevyengine/bevy/issues/6601">subtle footguns</a> that could catch users by surprise,
such as the hashes of proxies differing from the hashes of the concrete type they represent,
proxies not being considered equivalent to their concrete counterparts, and more.</p>
<p>While this release does not necessarily fix these issues, it does establish a solid foundation for fixing them in the future.
The way it does this is by changing how a proxy is defined.</p>
<p>Before 0.11, a proxy was only defined by cloning the concrete type's <a href="https://docs.rs/bevy/0.11.0/bevy/reflect/trait.Reflect.html#tymethod.type_name"><code>Reflect::type_name</code></a> string
and returning it as its own <code>Reflect::type_name</code>.</p>
<p>Now in 0.11, a proxy is defined by copying a reference to the static <a href="https://docs.rs/bevy/0.11.0/bevy/reflect/enum.TypeInfo.html"><code>TypeInfo</code></a> of the concrete type.
This will allow us to access more of the concrete type's type information dynamically, without requiring the <code>TypeRegistry</code>.
In a <a href="https://github.com/bevyengine/bevy/pull/8695">future release</a>, we will make use of this to store hashing and comparison strategies in the <code>TypeInfo</code> directly
in order to mitigate the proxy issues mentioned above.</p>
<h2 id="fromreflect-ergonomics"><code>FromReflect</code> Ergonomics
<a href="#fromreflect-ergonomics">#</a>
</h2>
<p>authors: @MrGVSV</p>
<p>Bevy's <a href="https://docs.rs/bevy_reflect/latest/bevy_reflect/index.html">reflection API</a> commonly passes around data using type-erased <code>dyn Reflect</code> trait objects.
This can usually be downcast back to its concrete type using <code>&lt;dyn Reflect&gt;::downcast_ref::&lt;T&gt;</code>;
however, this doesn't work if the underlying data has been converted to a "dynamic" representation
(e.g. <code>DynamicStruct</code> for struct types, <code>DynamicList</code> for list types, etc.).</p>
<pre data-lang="rust"><code data-lang="rust"><span><span>let</span> data<span>:</span> <span><span>Vec</span><span>&lt;</span><span>i32</span><span>&gt;</span></span> <span>=</span> <span>vec!</span><span><span>[</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>]</span></span><span>;</span>

<span>let</span> reflect<span>:</span> <span>&amp;</span>dyn Reflect <span>=</span> <span>&amp;</span>data<span>;</span>
<span>let</span> cloned<span>:</span> <span><span>Box</span><span>&lt;</span>dyn Reflect<span>&gt;</span></span> <span>=</span> reflect<span>.</span><span>clone_value</span><span><span>(</span></span><span><span>)</span></span><span>;</span>

<span><span>//</span> `reflect` really is a `Vec&lt;i32&gt;`
</span><span>assert!</span><span><span>(</span>reflect<span>.</span><span>is<span>::</span></span><span><span>&lt;</span><span><span>Vec</span><span>&lt;</span><span>i32</span><span>&gt;</span></span><span>&gt;</span></span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
<span>assert!</span><span><span>(</span>reflect<span>.</span><span>represents<span>::</span></span><span><span>&lt;</span><span><span>Vec</span><span>&lt;</span><span>i32</span><span>&gt;</span></span><span>&gt;</span></span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>

<span><span>//</span> `cloned` is a `DynamicList`, but represents a `Vec&lt;i32&gt;`
</span><span>assert!</span><span><span>(</span>cloned<span>.</span><span>is<span>::</span></span><span><span>&lt;</span>DynamicList<span>&gt;</span></span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
<span>assert!</span><span><span>(</span>cloned<span>.</span><span>represents<span>::</span></span><span><span>&lt;</span><span><span>Vec</span><span>&lt;</span><span>i32</span><span>&gt;</span></span><span>&gt;</span></span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>

<span><span>//</span> `cloned` is equivalent to the original `reflect`, despite not being a `Vec&lt;i32&gt;`
</span><span>assert!</span><span><span>(</span>cloned<span>.</span><span>reflect_partial_eq</span><span><span>(</span>reflect</span><span><span>)</span></span><span>.</span><span>unwrap_or_default</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
</span></code></pre>
<p>To account for this, the <a href="https://docs.rs/bevy_reflect/latest/bevy_reflect/trait.FromReflect.html"><code>FromReflect</code></a> trait can be used to convert any <code>dyn Reflect</code> trait object
back into its concrete type— whether it is actually that type or a dynamic representation of it.
And it can even be called dynamically using the <a href="https://docs.rs/bevy_reflect/latest/bevy_reflect/struct.ReflectFromReflect.html"><code>ReflectFromReflect</code></a> type data.</p>
<p>Before 0.11, users had to be manually derive <code>FromReflect</code> for every type that needed it,
as well as manually register the <code>ReflectFromReflect</code> type data.
This made it cumbersome to use and also meant that it was often forgotten about,
resulting in reflection conversions difficulties for users downstream.</p>
<p>Now in 0.11, <code>FromReflect</code> is automatically derived and <code>ReflectFromReflect</code> is automatically registered for all types that derive <code>Reflect</code>.
This means most types will be <code>FromReflect</code>-capable by default,
thus reducing boilerplate and empowering logic centered around <code>FromReflect</code>.</p>
<p>Users can still opt out of this behavior by adding the <a href="https://docs.rs/bevy_reflect/latest/bevy_reflect/derive.Reflect.html#reflectfrom_reflect--false"><code>#[reflect(from_reflect = false)]</code></a> attribute to their type.</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>#</span><span>[</span><span>derive</span><span><span><span>(</span></span></span><span><span>Reflect</span></span><span><span><span>)</span></span></span><span>]</span></span>
<span><span>struct</span> </span><span><span>Foo</span></span><span>;</span>

<span><span>#</span><span>[</span><span>derive</span><span><span><span>(</span></span></span><span><span>Reflect</span></span><span><span><span>)</span></span></span><span>]</span></span>
<span><span>#</span><span>[</span><span>reflect</span><span><span><span>(</span></span></span><span><span>from_reflect <span>=</span> false</span></span><span><span><span>)</span></span></span><span>]</span></span>
<span><span>struct</span> </span><span><span>Bar</span></span><span>;</span>

<span><span><span>fn</span> </span><span>test</span></span><span><span>&lt;</span>T<span>:</span> FromReflect<span>&gt;</span></span><span><span><span>(</span><span>value</span><span>:</span> T</span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span></span><span><span>}</span></span></span>

<span>test</span><span><span>(</span>Foo</span><span><span>)</span></span><span>;</span> <span><span>//</span> &lt;-- OK!
</span><span>test</span><span><span>(</span>Bar</span><span><span>)</span></span><span>;</span> <span><span>//</span> &lt;-- ERROR! `Bar` does not implement trait `FromReflect`
</span></span></code></pre>
<h2 id="deref-derive-attribute">Deref Derive Attribute
<a href="#deref-derive-attribute">#</a>
</h2>
<p>authors: @MrGVSV</p>
<p>Bevy code tends to make heavy use of the <a href="https://doc.rust-lang.org/rust-by-example/generics/new_types.html">newtype</a> pattern,
which is why we have dedicated derives for <a href="https://docs.rs/bevy/latest/bevy/prelude/derive.Deref.html"><code>Deref</code></a> and <a href="https://docs.rs/bevy/latest/bevy/prelude/derive.DerefMut.html"><code>DerefMut</code></a>.</p>
<p>This previously only worked for structs with a single field:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>#</span><span>[</span><span>derive</span><span><span><span>(</span></span></span><span><span>Resource<span>,</span> Deref<span>,</span> DerefMut</span></span><span><span><span>)</span></span></span><span>]</span></span>
<span><span>struct</span> </span><span><span>Score</span></span><span><span><span>(</span><span>i32</span></span><span>)</span></span><span>;</span>
</span></code></pre>
<p>For 0.11, we've improved these derives by adding the <code>#[deref]</code> attribute, which allows them to be used on structs with multiple fields.
This makes working with generic newtypes much easier:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>#</span><span>[</span><span>derive</span><span><span><span>(</span></span></span><span><span>Component<span>,</span> Deref<span>,</span> DerefMut</span></span><span><span><span>)</span></span></span><span>]</span></span>
<span><span>struct</span> </span><span><span><span>Health</span><span><span>&lt;</span>T<span>:</span> Character<span>&gt;</span></span></span></span><span> </span><span><span><span>{</span>
    <span><span>#</span><span>[</span><span>deref</span><span>]</span></span> <span><span>//</span> &lt;- use the `health` field as the `Deref` and `DerefMut` target
</span>    <span>health</span><span>:</span> <span>u16</span>,
    <span>_character_type</span><span>:</span> <span>PhantomData<span>&lt;</span>T<span>&gt;</span></span>,
</span><span><span>}</span></span></span>
</span></code></pre>
<h2 id="simpler-rendergraph-construction">Simpler RenderGraph Construction
<a href="#simpler-rendergraph-construction">#</a>
</h2>
<p>authors: @IceSentry, @cart</p>
<p>Adding <code>Node</code>s to the <code>RenderGraph</code> requires a lot of boilerplate. In this release, we tried to reduce this for most common operations. No existing APIs have been removed, these are only helpers made to simplify working with the <code>RenderGraph</code>.</p>
<p>We added the <code>RenderGraphApp</code> trait to the <code>App</code>. This trait contains various helper functions to reduce the boilerplate with adding nodes and edges to a graph.</p>
<p>Another pain point of <code>RenderGraph</code> <code>Node</code>s is passing the view entity through each node and manually updating the query on that view. To fix this we added a <code>ViewNode</code> trait and <code>ViewNodeRunner</code> that will automatically take care of running the <code>Query</code> on the view entity. We also made the view entity a first-class concept of the <code>RenderGraph</code>. So you can now access the view entity the graph is currently running on from anywhere in the graph without passing it around between each <code>Node</code>.</p>
<p>All these new APIs assume that your Node implements <code>FromWorld</code> or <code>Default</code>.</p>
<p>Here's what it looks like in practice for the <code>BloomNode</code>:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>//</span> Adding the node to the 3d graph
</span>render_app
    <span><span>//</span> To run a ViewNode you need to create a ViewNodeRunner
</span>    <span>.</span><span>add_render_graph_node<span>::</span></span><span><span>&lt;</span><span>ViewNodeRunner<span>&lt;</span>BloomNode<span>&gt;</span></span><span>&gt;</span></span><span><span>(</span>
        <span>CORE_3D</span><span>,</span>
        <span>core_3d<span>::</span></span><span>graph<span>::</span></span><span>node<span>::</span></span><span>BLOOM</span><span>,</span>
    </span><span><span>)</span></span><span>;</span>

<span><span>//</span> Defining the node
</span><span><span>#</span><span>[</span><span>derive</span><span><span><span>(</span></span></span><span><span>Default</span></span><span><span><span>)</span></span></span><span>]</span></span>
<span><span>struct</span> </span><span><span>BloomNode</span></span><span>;</span>
<span><span>//</span> This can replace your `impl Node` block of any existing `Node` that operated on a view
</span><span><span>impl</span> </span><span>ViewNode <span>for</span></span><span> <span>BloomNode</span> </span><span><span><span>{</span>
    <span><span>//</span> You need to define your view query as an associated type
</span>    <span>type</span> <span>ViewQuery</span> <span>=</span> <span><span>(</span>
        <span>&amp;</span><span>'static</span> ExtractedCamera<span>,</span>
        <span>&amp;</span><span>'static</span> ViewTarget<span>,</span>
        <span>&amp;</span><span>'static</span> BloomSettings<span>,</span>
    </span><span><span>)</span></span><span>;</span>
    <span><span>//</span> You don't need Node::input() or Node::update() anymore. If you still need these they are still available but they have an empty default implementation.
</span>    <span><span><span>fn</span> </span><span>run</span></span><span><span><span>(</span>
        <span>&amp;</span><span>self</span>,
        <span>graph</span><span>:</span> <span>&amp;</span><span>mut</span> RenderGraphContext,
        <span>render_context</span><span>:</span> <span>&amp;</span><span>mut</span> RenderContext,
        <span><span>//</span> This is the result of your query. If it is empty the run function will not be called
</span>        <span><span>(</span></span><span><span>camera</span><span>,</span> <span>view_target</span><span>,</span> <span>bloom_settings</span></span><span><span>)</span></span>: <span>QueryItem<span>&lt;</span><span><span><span>Self</span><span>::</span></span></span>ViewQuery<span>&gt;</span></span>,
        <span>world</span><span>:</span> <span>&amp;</span>World,
    </span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span><span>Result</span><span>&lt;</span><span>(</span><span>)</span>, NodeRunError<span>&gt;</span></span></span> </span><span><span><span>{</span>
        <span><span>//</span> When using the ViewNode you probably won't need the view entity but here's how to get it if you do
</span>        <span>let</span> view_entity <span>=</span> graph<span>.</span><span>view_entity</span><span><span>(</span></span><span><span>)</span></span><span>;</span>

        <span><span>//</span> Run the node
</span>    </span><span><span>}</span></span></span>
</span><span><span>}</span></span></span>
</span></code></pre>
<h2 id="reflect-default-on-enum-variant-fields"><code>#[reflect(default)]</code> on Enum Variant Fields
<a href="#reflect-default-on-enum-variant-fields">#</a>
</h2>
<p>authors: @MrGVSV</p>
<p>When using the <code>FromReflect</code> trait, fields marked <code>#[reflect(default)]</code> will be set to their <code>Default</code> value if they don't exist on the reflected object.</p>
<p>Previously, this was only supported on struct fields.
Now, it is also supported on all enum variant fields.</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>#</span><span>[</span><span>derive</span><span><span><span>(</span></span></span><span><span>Reflect</span></span><span><span><span>)</span></span></span><span>]</span></span>
<span><span>enum</span> <span>MyEnum</span> <span><span>{</span>
    Data <span><span>{</span>
        <span><span>#</span><span>[</span><span>reflect</span><span><span><span>(</span></span></span><span><span>default</span></span><span><span><span>)</span></span></span><span>]</span></span>
        a<span>:</span> <span>u32</span><span>,</span>
        b<span>:</span> <span>u32</span><span>,</span>
    </span><span><span>}</span></span><span>,</span>
</span><span><span>}</span></span></span>

<span>let</span> <span>mut</span> data <span>=</span> <span>DynamicStruct<span>::</span></span>default <span><span>(</span></span><span><span>)</span></span><span>;</span>
data<span>.</span><span>insert</span><span><span>(</span><span><span>"</span>b<span>"</span></span><span>,</span> <span>1</span></span><span><span>)</span></span><span>;</span>

<span>let</span> dynamic_enum <span>=</span> <span>DynamicEnum<span>::</span></span>new<span><span>(</span><span><span>"</span>Data<span>"</span></span><span>,</span> data</span><span><span>)</span></span><span>;</span>

<span>let</span> my_enum <span>=</span> <span>MyEnum<span>::</span></span>from_reflect<span><span>(</span> <span>&amp;</span> dynamic_enum</span><span><span>)</span></span><span>.</span><span>unwrap</span><span><span>(</span></span><span><span>)</span></span><span>;</span>
<span>assert_eq!</span><span><span>(</span><span>u32</span><span><span>::</span></span>default<span><span>(</span></span><span><span>)</span></span><span>,</span> my_enum<span>.</span>a</span><span><span>)</span></span><span>;</span>
</span></code></pre>
<h2 id="delayed-asset-hot-reloading">Delayed Asset Hot Reloading
<a href="#delayed-asset-hot-reloading">#</a>
</h2>
<p>authors: @JMS55</p>
<p>Bevy now waits 50 milliseconds after an "asset changed on filesystem" event before reloading an asset. Reloading without a delay resulted in reading invalid asset contents on some systems. The wait time is configurable.</p>
<h2 id="custom-gltf-vertex-attributes">Custom glTF Vertex Attributes
<a href="#custom-gltf-vertex-attributes">#</a>
</h2>
<p>authors: @komadori</p>
<p>It is now possible to load meshes with custom vertex attributes from glTF files. Custom attributes can be mapped to Bevy's <a href="https://docs.rs/bevy/0.11.0/bevy/render/mesh/struct.MeshVertexAttribute.html"><code>MeshVertexAttribute</code></a> format used by the <a href="https://docs.rs/bevy/0.11.0/bevy/render/mesh/struct.Mesh.html"><code>Mesh</code></a> type in the <a href="https://docs.rs/bevy/0.11.0/bevy/gltf/struct.GltfPlugin.html"><code>GltfPlugin</code></a> settings. These attrtibutes can then be used in Bevy shaders. For an example, check out our <a href="https://github.com/bevyengine/bevy/blob/v0.11.0/examples/2d/custom_gltf_vertex_attribute.rs">new example</a>.</p>
<p><img src="https://bevyengine.org/news/bevy-0-11/custom_vertex.png" alt="custom vertex attribute"></p>
<h2 id="stable-typepath">Stable TypePath
<a href="#stable-typepath">#</a>
</h2>
<p>authors: @soqb, @tguichaoua</p>
<p>Bevy has historically used <a href="https://doc.rust-lang.org/std/any/fn.type_name.html"><code>std::any::type_name</code></a> to identify Rust types with friendly names in a number of places: Bevy Reflect, Bevy Scenes, Bevy Assets, Bevy ECS, and others. Unfortunately, Rust makes no guarantees about the stability or format of <a href="https://doc.rust-lang.org/std/any/fn.type_name.html"><code>type_name</code></a>, which makes it theoretically shakey ground to build on (although in practice it has been stable so far).</p>
<p>There is also no built in way to retrieve "parts" of a type name. If you want the short name, the name of a generic type without its inner types, the module name, or the crate name, you must do string operations on the <a href="https://doc.rust-lang.org/std/any/fn.type_name.html"><code>type_name</code></a> (which can be error prone / nontrivial).</p>
<p>Additionally, <a href="https://doc.rust-lang.org/std/any/fn.type_name.html"><code>type_name</code></a> cannot be customized. In some cases an author might choose to identify a type with something other than its full module path (ex: if they prefer a shorter path or want to abstract out private / internal modules).</p>
<p>For these reasons, we've developed a new stable <a href="https://docs.rs/bevy/0.11.0/bevy/reflect/trait.TypePath.html"><code>TypePath</code></a>, which is automatically implemented for any type deriving <a href="https://docs.rs/bevy/0.11.0/bevy/reflect/trait.Reflect.html"><code>Reflect</code></a>. Additionally, it can be manually derived in cases where <a href="https://docs.rs/bevy/0.11.0/bevy/reflect/trait.Reflect.html"><code>Reflect</code></a> isn't derived.</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>mod</span> <span>my_mod</span> <span><span>{</span>
    <span><span>#</span><span>[</span><span>derive</span><span><span><span>(</span></span></span><span><span>Reflect</span></span><span><span><span>)</span></span></span><span>]</span></span>
    <span><span>struct</span> </span><span><span>MyType</span></span><span>;</span>
</span><span><span>}</span></span></span>

<span><span>///</span> prints: "my_crate::my_mod::MyType"
</span><span>println!</span><span><span>(</span></span><span><span><span>"</span><span>{}</span><span>"</span></span></span><span><span>,</span> <span>MyType<span>::</span></span>type_path<span><span>(</span></span><span><span>)</span></span><span>)</span></span><span>;</span>
<span><span>///</span> prints: "MyType"
</span><span>println!</span><span><span>(</span></span><span><span><span>"</span><span>{}</span><span>"</span></span></span><span><span>,</span> <span>MyType<span>::</span></span>short_type_path<span><span>(</span></span><span><span>)</span></span><span>)</span></span><span>;</span>
<span><span>///</span> prints: "my_crate"
</span><span>println!</span><span><span>(</span></span><span><span><span>"</span><span>{}</span><span>"</span></span></span><span><span>,</span> <span>MyType<span>::</span></span>crate_name<span><span>(</span></span><span><span>)</span></span><span>.</span><span>unwrap</span><span><span>(</span></span><span><span>)</span></span><span>)</span></span><span>;</span>
<span><span>///</span> prints: "my_crate::my_mod"
</span><span>println!</span><span><span>(</span></span><span><span><span>"</span><span>{}</span><span>"</span></span></span><span><span>,</span> <span>MyType<span>::</span></span>module_path<span><span>(</span></span><span><span>)</span></span><span>.</span><span>unwrap</span><span><span>(</span></span><span><span>)</span></span><span>)</span></span><span>;</span>
</span></code></pre>
<p>This also works for generics, which can come in handy:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>//</span> prints: "Option&lt;MyType&gt;"
</span><span>println!</span><span><span>(</span></span><span><span><span>"</span><span>{}</span><span>"</span></span></span><span><span>,</span> <span>Option</span><span>:</span><span>:</span><span><span>&lt;</span>MyType<span>&gt;</span></span><span><span>::</span></span>short_type_path<span><span>(</span></span><span><span>)</span></span><span>)</span></span><span>;</span>
<span><span>//</span> prints: "Option"
</span><span>println!</span><span><span>(</span></span><span><span><span>"</span><span>{}</span><span>"</span></span></span><span><span>,</span> <span>Option</span><span>:</span><span>:</span><span><span>&lt;</span>MyType<span>&gt;</span></span><span><span>::</span></span>type_ident<span><span>(</span></span><span><span>)</span></span><span>.</span><span>unwrap</span><span><span>(</span></span><span><span>)</span></span><span>)</span></span><span>;</span>
</span></code></pre>
<p><a href="https://docs.rs/bevy/0.11.0/bevy/reflect/trait.TypePath.html"><code>TypePath</code></a> can be customized by type authors:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>#</span><span>[</span><span>derive</span><span><span><span>(</span></span></span><span><span>TypePath</span></span><span><span><span>)</span></span></span><span>]</span></span>
<span><span>#</span><span>[</span><span>type_path</span> <span>=</span> <span><span>"</span>some_crate::some_module<span>"</span></span><span>]</span></span>
<span><span>struct</span> </span><span><span>MyType</span></span><span>;</span>
</span></code></pre>
<p>We are in the process of porting Bevy's internal <a href="https://doc.rust-lang.org/std/any/fn.type_name.html"><code>type_name</code></a> usage over to <a href="https://docs.rs/bevy/0.11.0/bevy/reflect/trait.TypePath.html"><code>TypePath</code></a>, which should land in <strong>Bevy 0.12</strong>.</p>
<h2 id="run-if-for-tuples-of-systems"><code>run_if</code> for Tuples of Systems
<a href="#run-if-for-tuples-of-systems">#</a>
</h2>
<p>authors: @geieredgar</p>
<p>It is now possible to add <a href="https://bevyengine.org/news/bevy-0-10/#run-conditions">"run conditions"</a> to tuples of systems:</p>
<pre data-lang="rust"><code data-lang="rust"><span>app<span>.</span><span>add_systems</span><span><span>(</span>Update<span>,</span> <span><span>(</span>run<span>,</span> jump</span><span><span>)</span></span><span>.</span><span>run_if</span><span><span>(</span><span>in_state</span><span><span>(</span><span>GameState<span>::</span></span>Playing</span><span><span>)</span></span></span><span><span>)</span></span></span><span><span>)</span></span>
</span></code></pre>
<p>This will evaluate the "run condition" exactly once and use the result for each system in the tuple.</p>
<p>This allowed us to remove the <code>OnUpdate</code> system set for states (which was previously used to run groups of systems when they are in a given state).</p>
<h2 id="has-queries"><code>Has</code> Queries
<a href="#has-queries">#</a>
</h2>
<p>authors: @wainwrightmark</p>
<p>You can now use <code>Has&lt;Component&gt;</code> in queries, which will return true if that component exists and false if it does not:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span><span>fn</span> </span><span>system</span></span><span><span><span>(</span><span>query</span><span>:</span> <span>Query<span>&lt;</span><span>Has<span>&lt;</span>Player<span>&gt;</span></span><span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
    <span>for</span> has_player <span>in</span> <span>&amp;</span>query <span><span>{</span>
        <span>if</span> has_player <span><span>{</span>
            <span><span>//</span> do something
</span>        </span><span><span>}</span></span>
    </span><span><span>}</span></span>
</span><span><span>}</span></span></span>
</span></code></pre>
<h2 id="derive-event">Derive <code>Event</code>
<a href="#derive-event">#</a>
</h2>
<p>authors: @CatThingy</p>
<p>The Bevy <a href="https://docs.rs/bevy/0.11.0/bevy/ecs/event/trait.Event.html"><code>Event</code></a> trait is now derived instead of being auto-impled for everything:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>#</span><span>[</span><span>derive</span><span><span><span>(</span></span></span><span><span>Event</span></span><span><span><span>)</span></span></span><span>]</span></span>
<span><span>struct</span> </span><span><span>Collision</span> </span><span><span><span>{</span>
    <span>a</span><span>:</span> Entity,
    <span>b</span><span>:</span> Entity,
</span><span><span>}</span></span></span>
</span></code></pre>
<p>This prevents some classes of error, makes <a href="https://docs.rs/bevy/0.11.0/bevy/ecs/event/trait.Event.html"><code>Event</code></a> types more self-documenting, and provides consistency with other Bevy ECS traits like Components and Resources. It also opens the doors to configuring the <a href="https://docs.rs/bevy/0.11.0/bevy/ecs/event/trait.Event.html"><code>Event</code></a> storage type, which we plan to do in future releases.</p>
<h2 id="cubic-curve-example">Cubic Curve Example
<a href="#cubic-curve-example">#</a>
</h2>
<p>authors: @Kjolnyr</p>
<p>An example that shows how to draw a 3D curve and move an object along the path:</p>
<p><img src="https://bevyengine.org/news/bevy-0-11/cubic_curve.png" alt="cubic_curve"></p>
<h2 id="size-constraints-example">Size Constraints Example
<a href="#size-constraints-example">#</a>
</h2>
<p>authors: @ickshonpe</p>
<p>An interactive example that shows how the various <a href="https://docs.rs/bevy/0.11.0/bevy/ui/struct.Style.html"><code>Style</code></a> size constraints affect UI nodes.</p>
<p><img src="https://bevyengine.org/news/bevy-0-11/size_constraints.png" alt="size constraints"></p>
<h2 id="display-and-visibility-example">Display and Visibility Example
<a href="#display-and-visibility-example">#</a>
</h2>
<p>authors: @ickshonpe</p>
<p>An example that shows how display and visibility settings affect UI nodes.</p>
<p><img src="https://bevyengine.org/news/bevy-0-11/display_and_visibility.png" alt="display and visibiltiy"></p>
<h2 id="no-more-bors">No More Bors!
<a href="#no-more-bors">#</a>
</h2>
<p>authors: @cart, @mockersf</p>
<p>Bevy has historically used the Bors merge system to ensure we never merge a pull request on GitHub that breaks our CI validation. This was a critical piece of infrastructure that ensured we could collaborate safely and effectively. Fortunately GitHub has <em>finally</em> rolled out <a href="https://github.blog/changelog/2023-02-08-pull-request-merge-queue-public-beta/">Merge Queues</a>, which solve the same problems as Bors, with the benefit of being more tightly integrated with GitHub.</p>
<p>For this release cycle we migrated to Merge Queues and we're very happy with the experience!</p>
<h2 id="new-ci-jobs">New CI Jobs
<a href="#new-ci-jobs">#</a>
</h2>
<p>authors: @mockersf</p>
<p>We've added a number of new CI jobs that improve the Bevy development experience:</p>
<ul>
<li>A daily job that runs Bevy's mobile examples on real Android and iOS devices! This helps protect against regressions that might not be caught by the compiler</li>
<li>Added the ability to take screenshots in CI, which can be used to validate the results of Bevy example runs</li>
<li>A job that leaves a GitHub comment on PRs that are missing a feature or example doc update</li>
</ul>
<h2 id="what-s-next"><a name="what-s-next"></a>What's Next?
<a href="#what-s-next">#</a>
</h2>
<p>We have plenty of work that is pretty much finished and is therefore very likely to land in <strong>Bevy 0.12</strong>:</p>
<ul>
<li><strong>Bevy Asset V2</strong>: A brand new asset system that adds "asset preprocessing", optional asset .meta files, recursive asset dependency tracking and events, async asset IO, better asset handles, more efficient asset storage, and a variety of usability improvements! The work here is <a href="https://github.com/bevyengine/bevy/pull/8624">pretty much finished</a>. It <em>almost</em> made it in to Bevy 0.11 but it needed a bit more time to cook.</li>
<li><strong>PBR Material Light Transmission</strong>: Transmission / screen space refractions allows for simulating materials like glass, plastics, liquids and gels, gemstones, wax, etc. This one is also pretty much <a href="https://github.com/bevyengine/bevy/pull/8015">ready to go</a>!</li>
<li><strong>TAA Improvements</strong>: We have a number of changes in the works for TAA that will improve its quality, speed, and support within the engine.</li>
<li><strong>GPU Picking</strong>: Efficiently and correctly <a href="https://github.com/bevyengine/bevy/pull/8784">select entities on the GPU</a> by using color ids to identify meshes in renders.</li>
<li><strong>PCF For Directional and Spotlight Shadows</strong>: <a href="https://github.com/bevyengine/bevy/pull/8006">Reduce aliasing on the edges of shadows</a></li>
<li><strong>UI Node Border Radius and Shadows</strong>: Add <a href="https://github.com/bevyengine/bevy/pull/8973">curvature and "drop shadows"</a> to your UI nodes!</li>
<li><strong>Deferred Rendering</strong>: Bevy already does "mixed mode" forward rendering by having optional separate passes for depth and normals. We are currently experimenting with supporting "fully deferred" rendering as well, which opens the doors to new effects and different performance tradeoffs.</li>
</ul>
<p>From a high level, we plan to focus on the Asset System, UI, Render Features, and Scenes during the next cycle.</p>
<p>Check out the <a href="https://github.com/bevyengine/bevy/milestone/14"><strong>Bevy 0.12 Milestone</strong></a> for an up-to-date list of current work being considered for <strong>Bevy 0.12</strong>.</p>
<h2 id="support-bevy">Support Bevy
<a href="#support-bevy">#</a>
</h2>
<p>Sponsorships help make our work on Bevy sustainable. If you believe in Bevy's mission, consider <a href="https://bevyengine.org/community/donate">sponsoring us</a> ... every bit helps!</p>
<p><a href="https://bevyengine.org/community/donate">Donate <img src="https://bevyengine.org/assets/heart.svg" alt="heart icon"></a></p>
<h2 id="contributors">Contributors
<a href="#contributors">#</a>
</h2>
<p>Bevy is made by a <a href="https://bevyengine.org/community/people/">large group of people</a>. A huge thanks to the 166 contributors that made this release (and associated docs) possible! In random order:</p>
<ul>
<li>@TheBlek</li>
<li>@hank</li>
<li>@TimJentzsch</li>
<li>@Suficio</li>
<li>@SmartManLudo</li>
<li>@BlondeBurrito</li>
<li>@lewiszlw</li>
<li>@paul-hansen</li>
<li>@boringsan</li>
<li>@superdump</li>
<li>@JonahPlusPlus</li>
<li>@airingursb</li>
<li>@Sheepyhead</li>
<li>@nakedible</li>
<li>@Testare</li>
<li>@andresovela</li>
<li>@SkiFire13</li>
<li>@doup</li>
<li>@BlackPhlox</li>
<li>@nicoburns</li>
<li>@wpederzoli</li>
<li>@adtennant</li>
<li>@LoopyAshy</li>
<li>@KernelFreeze</li>
<li>@ickshonpe</li>
<li>@jim-ec</li>
<li>@mrchantey</li>
<li>@frengor</li>
<li>@Joakker</li>
<li>@arendjr</li>
<li>@MJohnson459</li>
<li>@TheTacBanana</li>
<li>@IceSentry</li>
<li>@ItsDoot</li>
<li>@Anti-Alias</li>
<li>@mwbryant</li>
<li>@inodentry</li>
<li>@LiamGallagher737</li>
<li>@robtfm</li>
<li>@mockersf</li>
<li>@ndarilek</li>
<li>@samtenna</li>
<li>@Estus-Dev</li>
<li>@InnocentusLime</li>
<li>@p-hueber</li>
<li>@B-Reif</li>
<li>@Adamkob12</li>
<li>@payload</li>
<li>@JohnTheCoolingFan</li>
<li>@djeedai</li>
<li>@SludgePhD</li>
<li>@s-lambert</li>
<li>@kjolnyr</li>
<li>@Skovrup1</li>
<li>@Ababwa</li>
<li>@Illiux</li>
<li>@Carter0</li>
<li>@luca-della-vedova</li>
<li>@Neo-Zhixing</li>
<li>@coreh</li>
<li>@helvieq499</li>
<li>@Carbonhell</li>
<li>@BrandonDyer64</li>
<li>@hymm</li>
<li>@JMS55</li>
<li>@iiYese</li>
<li>@mtsr</li>
<li>@jannik4</li>
<li>@natasria</li>
<li>@Trouv</li>
<li>@minchopaskal</li>
<li>@chrisjuchem</li>
<li>@marlyx</li>
<li>@valaphee</li>
<li>@hankjordan</li>
<li>@rparrett</li>
<li>@Selene-Amanita</li>
<li>@opstic</li>
<li>@loganbenjamin</li>
<li>@MrGunflame</li>
<li>@pyrotechnick</li>
<li>@mjhostet</li>
<li>@VitalyAnkh</li>
<li>@CatThingy</li>
<li>@maniwani</li>
<li>@Themayu</li>
<li>@SET001</li>
<li>@jakobhellermann</li>
<li>@MrGVSV</li>
<li>@nicopap</li>
<li>@Wcubed</li>
<li>@aevyrie</li>
<li>@NiklasEi</li>
<li>@bonsairobo</li>
<li>@cart</li>
<li>@TotalKrill</li>
<li>@raffaeleragni</li>
<li>@Aceeri</li>
<li>@Shatur</li>
<li>@orzogc</li>
<li>@UncleScientist</li>
<li>@Elabajaba</li>
<li>@vyb</li>
<li>@komadori</li>
<li>@jnhyatt</li>
<li>@harudagondi</li>
<li>@konsti219</li>
<li>@james7132</li>
<li>@mvlabat</li>
<li>@neithanmo</li>
<li>@dgunay</li>
<li>@Shfty</li>
<li>@hate</li>
<li>@B-head</li>
<li>@MinerSebas</li>
<li>@chescock</li>
<li>@BorMor</li>
<li>@lupan</li>
<li>@CrazyRoka</li>
<li>@bzm3r</li>
<li>@Sixmorphugus</li>
<li>@JoJoJet</li>
<li>@eltociear</li>
<li>@gakit</li>
<li>@geieredgar</li>
<li>@tjamaan</li>
<li>@alice-i-cecile</li>
<li>@NoahShomette</li>
<li>@james-j-obrien</li>
<li>@tinrab</li>
<li>@Olle-Lukowski</li>
<li>@TheRawMeatball</li>
<li>@sarkahn</li>
<li>@RobWalt</li>
<li>@johanhelsing</li>
<li>@SneakyBerry</li>
<li>@beeryt</li>
<li>@Vrixyz</li>
<li>@wainwrightmark</li>
<li>@EliasPrescott</li>
<li>@konsolas</li>
<li>@ameknite</li>
<li>@Connor-McMillin</li>
<li>@Weibye</li>
<li>@SpecificProtagonist</li>
<li>@danchia</li>
<li>@vallrand</li>
<li>@atornity</li>
<li>@soqb</li>
<li>@devil-ira</li>
<li>@AnthonyKalaitzis</li>
<li>@yyogo</li>
<li>@NiseVoid</li>
<li>@gajop</li>
<li>@Gingeh</li>
<li>@zendril</li>
<li>@ezekg</li>
<li>@ickk</li>
<li>@Leonss23</li>
<li>@kellencataldo</li>
<li>@akappel</li>
<li>@hazelsparrow</li>
<li>@mattfbacon</li>
<li>@gianzellweger</li>
<li>@lakrsv</li>
<li>@laundmo</li>
</ul>
<h2 id="full-changelog">Full Changelog
<a href="#full-changelog">#</a>
</h2>
<h3 id="rendering">Rendering
<a href="#rendering">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/8336">Webgpu support</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/5703">improve shader import model</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7402">Screen Space Ambient Occlusion (SSAO) MVP</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7291">Temporal Antialiasing (TAA)</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/6529">Immediate Mode Line/Gizmo Drawing</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8109">Make render graph slots optional for most cases</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8090">Split opaque and transparent phases</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8275">Built-in skybox</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/5928">Add parallax mapping to bevy PBR</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7422">Add port of AMD's Robust Contrast Adaptive Sharpening</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8007">Add RenderGraphApp to simplify adding render nodes</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7163">Add screenshot api</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8158">Add morph targets</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8455">Screenshots in wasm</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8118">Add ViewNode to simplify render node management</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7614">Bias texture mipmaps</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8427">Instanced line rendering for gizmos based on <code>bevy_polyline</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8042">Add <code>RenderTarget::TextureView</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8685">Change default tonemapping method</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/6815">Allow custom depth texture usage</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8231">Use the prepass normal texture in main pass when possible</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8122">Left-handed y-up cubemap coordinates</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7772">Allow SPIR-V shaders to process when shader defs are present</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8299">Remove unnecesssary values Vec from DynamicUniformBuffer and DynamicStorageBuffer</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/6697">Add <code>MAY_DISCARD</code> shader def, enabling early depth tests for most cases</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7885">Add <code>Aabb</code> calculation for <code>Sprite</code>, <code>TextureAtlasSprite</code> and <code>Mesh2d</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8041">Color::Lcha constructors</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8040">Fix Color::as_rgba_linear for Color::Lcha</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8070">Added Globals struct to prepass shader</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8121">Derive Copy and Clone for Collision</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8151">Fix crash when enabling HDR on 2d cameras</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7977">Dither fix</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8460">Compute <code>vertex_count</code> for indexed meshes on <code>GpuMesh</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/9024">Run update_previous_view_projections in PreUpdate schedule</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8220">Added <code>WebP</code> image format support</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8601">Add support for pnm textures</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8316">fix invalid bone weights</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8412">Fix pbr shader breaking on missing UVs</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8878">Fix Plane UVs / texture flip</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7817">Fix look_to resulting in NaN rotations</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8627">Fix look_to variable naming</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8223">Fix segfault with 2d gizmos</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8298">Use RenderGraphApp in more places</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8323">Fix viewport change detection</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8301">Remove capacity fields from all Buffer wrapper types</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8380">Sync pbr_types.wgsl StandardMaterial values</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8180">Avoid spawning gizmo meshes when no gizmos are being drawn</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/9030">Use a consistent seed for AABB gizmo colors</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8444">bevy_pbr: Do not cull meshes without Aabbs from cascades</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8330">Handle vertex_uvs if they are present in default prepass fragment shader</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7867">Changed (Vec2, Vec2) to Rect in Camera::logical_viewport_rect</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8491">make glsl and spirv support optional</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8978">fix prepass normal_mapping</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8564">conversions between [u8; 4] and Color</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8952">Add option to disable gizmo rendering for specific cameras</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/9013">Fix morph target prepass shader</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8631">Fix bloom wasm support</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8926">Fix black spots appearing due to NANs when SSAO is enabled</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8890">fix normal prepass</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8977">Refs #8975 -- Add return to RenderDevice::poll()</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8508">Fix WebGL mode for Adreno GPUs</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/9003">Fix parallax mapping</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8575">Added Vec append to BufferVec - Issue #3531</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8993">Fix CAS shader with explicit FullscreenVertexOutput import</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8643">Make <code>TextureAtlas::texture_handles</code> <code>pub</code> instead of <code>pub(crate)</code> (#8633)</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8642">Make Material2d pipeline systems public</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8701">Fix screenshots on Wayland + Nvidia</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8704">Apply codebase changes in preparation for <code>StandardMaterial</code> transmission</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8732">Use ViewNode for TAA</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8753">Change Camera3dBundle::tonemapping to Default</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8804">Remove <code>Component</code> derive for AlphaMode</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8408">Make setup of Opaque3dPrepass and AlphaMask3dPrepass phase items consistent with others</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8744">Rename <code>Plane</code> struct to <code>HalfSpace</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/6974">Expand <code>FallbackImage</code> to include a <code>GpuImage</code> for each possible <code>TextureViewDimension</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8877">Cascaded shadow maps: Fix prepass ortho depth clamping</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8910">Fix gizmos in WebGPU</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8868">Fix AsBindGroup derive, texture attribute, visibility flag parsing</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8802">Disable camera on window close</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8283">Reflect <code>Component</code> and <code>Default</code> of <code>BloomSettings</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8428">Add Reflection Macros to TextureAtlasSprite</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8801">Implement Reflect on NoFrustumCulling</a></li>
</ul>
<h3 id="audio">Audio
<a href="#audio">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/8424">ECS-based API redesign</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7706">Ability to set a Global Volume</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8145">Expose <code>AudioSink::empty()</code></a></li>
</ul>
<h3 id="diagnostics">Diagnostics
<a href="#diagnostics">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/8677">Allow systems using Diagnostics to run in parallel</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8272">add a feature for memory tracing with tracy</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8362">Re-add the "frame" span for tracy comparisons</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8886">log to stderr instead of stdout</a></li>
</ul>
<h3 id="scenes">Scenes
<a href="#scenes">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/6793">bevy_scene: Add SceneFilter</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/6846">(De) serialize resources in scenes</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8065">add position to scene errors</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7951">Bugfix: Scene reload fix (nonbreaking)</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8512">avoid panic with parented scenes on deleted entities</a></li>
</ul>
<h3 id="transform-hierarchy">Transform + Hierarchy
<a href="#transform-hierarchy">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/7264">Fix transform propagation of orphaned entities</a></li>
</ul>
<h3 id="gizmo">Gizmo
<a href="#gizmo">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/8468">Add a bounding box gizmo</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8448">Added <code>arc_2d</code> function for gizmos</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8960">Use AHash to get color from entity in bevy_gizmos</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8434">do not crash when rendering only one gizmo</a></li>
</ul>
<h3 id="reflection">Reflection
<a href="#reflection">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/7184">reflect: stable type path v2</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/6971">bevy_reflect: Better proxies</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/6056">bevy_reflect: FromReflect Ergonomics Implementation</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8514">bevy_reflect: Allow <code>#[reflect(default)]</code> on enum variant fields</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8776">Add FromReflect where Reflect is used</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8691">Add get_at_mut to bevy_reflect::Map trait</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8764">Reflect now requires DynamicTypePath. Remove Reflect::get_type_path()</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8495">bevy_ui: Add <code>FromReflect</code> derives</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8531">Add Reflect and FromReflect for AssetPath</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8014">bevy_reflect: Fix trailing comma breaking derives</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8184">Fix Box<dyn reflect=""> struct with a hashmap in it panicking when clone_value is called on it</dyn></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8496">bevy_reflect: Add <code>ReflectFromReflect</code> to the prelude</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8723">bevy_reflect: Allow construction of MapIter outside of the bevy_reflect crate.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8761">bevy_reflect: Disambiguate type bounds in where clauses.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7454">adding reflection for Cow&lt;'static, [T]&gt;</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8891">Do not require mut on ParsedPath::element_mut</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8905">Reflect UUID</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7112">Don't ignore additional entries in <code>UntypedReflectDeserializerVisitor</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7407">Construct Box<dyn reflect=""> from world for ReflectComponent</dyn></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8957">reflect: avoid deadlock in GenericTypeCell</a></li>
</ul>
<h3 id="app">App
<a href="#app">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/8097">Allow tuples and single plugins in <code>add_plugins</code>, deprecate <code>add_plugin</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8585">Merge ScheduleRunnerSettings into ScheduleRunnerPlugin</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8740">correctly setup everything in the default run_once runner</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8103">Fix <code>Plugin::build</code> detection</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/9054">Fix not calling App::finish and App::cleanup in <code>ScheduleRunnerPlugin</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8961">Relaxed runner type from Fn to FnOnce</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8982">Relax FnMut to FnOnce in app::edit_schedule</a></li>
</ul>
<h3 id="windowing-reflection">Windowing + Reflection
<a href="#windowing-reflection">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/7993">Register missing types in bevy_window</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8771">bevy_reflect: implement Reflect for SmolStr</a></li>
</ul>
<h3 id="hierarchy">Hierarchy
<a href="#hierarchy">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/8346">fix panic when moving child</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8476">Remove <code>Children</code> component when calling <code>despawn_descendants</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8928">Change <code>despawn_descendants</code> to return <code>&amp;mut Self</code></a></li>
</ul>
<h3 id="time">Time
<a href="#time">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/8467">Fix timer with zero duration</a></li>
</ul>
<h3 id="assets">Assets
<a href="#assets">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/8503">Delay asset hot reloading</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/5370">Add support for custom glTF vertex attributes.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8485">Fix panic when using debug_asset_server</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7938"><code>unused_variables</code> warning when building with <code>filesystem_watcher</code> feature disabled</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8470">bevy_asset: Add <code>LoadContext::get_handle_untyped</code></a></li>
</ul>
<h3 id="windowing">Windowing
<a href="#windowing">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/7988">Move cursor position to internal state</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7966">Set cursor hittest during window creation</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7996">do not set hit test unconditionally on window creation</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8722">Add winit's <code>wayland-csd-adwaita</code> feature to Bevy's <code>wayland</code> feature</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8593">Support to set window theme and expose system window theme changed event</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8791">Touchpad magnify and rotate events</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8903">Fix windows not being centered properly when system interface is scaled</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/9016">Expose WindowDestroyed events</a></li>
</ul>
<h3 id="animation">Animation
<a href="#animation">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/9023">Register bevy_animation::PlayingAnimation</a></li>
</ul>
<h3 id="ui">UI
<a href="#ui">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/7795">Ui Node Borders</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8026">Add CSS Grid support to <code>bevy_ui</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7779"><code>text_system</code> split</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8549">Replace the local text queues in the text systems with flags stored in a component</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8947"><code>NoWrap</code> <code>Text</code> feature</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8445">add a default font</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8822">UI texture atlas support</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8793">Improved UI render batching</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8306">Consistent screen-space coordinates</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8199"><code>UiImage</code> helper functions</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7819">Perform text scaling calculations per text, not per glyph</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8197">Fix size of clipped text glyphs.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8545">Apply scale factor to  <code>ImageMeasure</code> sizes</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8933">Fix WebGPU error in "ui_pipeline" by adding a flat interpolate attribute</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/9027">Rename Interaction::Clicked -&gt; Interaction::Pressed</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8548">Flatten UI <code>Style</code> properties that use <code>Size</code> + remove <code>Size</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8095">Split UI <code>Overflow</code> by axis</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7930">Add methods for calculating the size and postion of UI nodes</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7809">Skip the UV calculations for untextured UI nodes</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8425">Fix text measurement algorithm</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8720">Divide by UiScale when converting UI coordinates from physical to logical</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8402"><code>MeasureFunc</code> improvements</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8522">Expose sorting methods in <code>Children</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7948">Fix min and max size using size value</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8019">Fix the <code>Text2d</code> text anchor's incorrect horizontal alignment</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7485">Remove <code>Val::Undefined</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8137"><code>Val</code> viewport unit variants</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8294">Remove the corresponding measure from Taffy when a <code>CalculatedSize</code> component is removed.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7656"><code>UiRect</code> axes constructor</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8195">Fix the UV calculations for clipped and flipped ImageNodes</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8422">Fix text systems broken when resolving merge conflicts in #8026</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8437">Allow <code>bevy_ui</code> crate to compile without the <code>text</code> feature enabled</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8264">Fix the double leaf node updates in <code>flex_node_system</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8456">also import the default handle when feature disabled</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8466"><code>measure_text_system</code> text query fix</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8497">Fix panic in example: text_wrap_debug.rs</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8521">UI layout tree debug print</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8551">Fix <code>Node::physical_rect</code> and add a <code>physical_size</code> method</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8795">Perform <code>relative_cursor_position</code> calculation vectorwise in <code>ui_focus_system</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8866">Add <code>UiRect::px()</code> and <code>UiRect::percent()</code> utils</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8920">Add missing dependencies to <code>bevy_text</code> feature</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8907">Remove "bevy_text" feature attributes on imports used by non-text systems</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8931">Growing UI nodes Fix</a></li>
</ul>
<h3 id="ecs">ECS
<a href="#ecs">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/8079">Schedule-First: the new and improved add_systems</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7936">Add OnTransition schedule that is ran between OnExit and OnEnter</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7676"><code>run_if</code> for <code>SystemConfigs</code> via anonymous system sets</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8260">Remove OnUpdate system set</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8726">Rename apply_system_buffers to apply_deferred</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8814">Rename Command's "write" method to "apply"</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7086">Require <code>#[derive(Event)]</code> on all Events</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/6960">Implement WorldQuery for EntityRef</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7085">Improve or-with disjoint checks</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8849">Add a method to run read-only systems using <code>&amp;World</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8053">Reduce branching when inserting components</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8030">Make <code>#[system_param(ignore)]</code> and <code>#[world_query(ignore)]</code> unnecessary</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8265">Remove <code>#[system_param(ignore)]</code> and <code>#[world_query(ignore)]</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8119">Extend the <code>WorldQuery</code> macro to tuple structs</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8009">Make state private and only accessible through getter for State resource</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8668">implement <code>Deref</code> for <code>State&lt;S&gt;</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8083">Inline more ECS functions</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8387">Add a <code>scope</code> API for world schedules</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8377">Simplify system piping and make it more flexible</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8326">Add <code>any_component_removed</code> condition</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8174">Use <code>UnsafeWorldCell</code> to increase code quality for <code>SystemParam</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8292">Improve safety for the multi-threaded executor using <code>UnsafeWorldCell</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8833">Migrate the rest of the engine to <code>UnsafeWorldCell</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8721">Make the <code>Condition</code> trait generic</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8714">Add or_else combinator to run_conditions.rs</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8772">Add iter_many_manual QueryState method</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8987">Provide access to world storages via UnsafeWorldCell</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8844">Added Has<t> WorldQuery type</t></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8951">Add/fix <code>track_caller</code> attribute on panicking entity accessor methods</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7905">Increase type safety and clarity for change detection</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7964">Make <code>WorldQuery</code> meta types unnameable</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7931">Add a public constructor for <code>Mut&lt;T&gt;</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7902">Remove ChangeTrackers</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/9020">Derive Eq, PartialEq for Tick</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7911">Initialize empty schedules when calling <code>.in_schedule</code> if they do not already exist</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8001">Replace multiple calls to <code>add_system</code> with <code>add_systems</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7950">don't panic on unknown ambiguity</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8060">add Clone to common conditions</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8068">Make BundleInfo's fields not pub(crate)</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8029">Pass query change ticks to <code>QueryParIter</code> instead of always using change ticks from <code>World</code>.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8108">Remove redundant bounds check in <code>Entities::get</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8028">Add World::try_run_schedule</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8105">change not implemation to custom system struct</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8012">Fix name conflicts caused by the <code>SystemParam</code> and <code>WorldQuery</code> macros</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8154">Check for conflicting accesses in <code>assert_is_system</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8163">Fix field visibility for read-only <code>WorldQuery</code> types</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8212"><code>Or&lt;T&gt;</code> should be a new type of <code>PhantomData&lt;T&gt;</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8249">Make standard commands more ergonomic (in niche cases)</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8269">Remove base set error variants of <code>ScheduleBuildError</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8274">Replace some unsafe system executor code with safe code</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8295">Update <code>increment_change_tick</code> to return a strongly-typed <code>Tick</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7732">Move event traces to detailed_trace!</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8359">Only trigger state transitons if <code>next_state != old_state</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8364">Fix panics and docs when using World schedules</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8000">Improve warning for Send resources marked as non_send</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8419">Reorganize system modules</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8436">Fix boxed labels</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8403">Simplify world schedule methods</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8494">Just print out name string, not the entire Name struct</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8529">Manually implement common traits for <code>EventId</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8567">Replace remaining uses of <code>&amp;T, Changed&lt;T&gt;</code> with <code>Ref</code> in UI system queries</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8588">Rename <code>UnsafeWorldCell::read_change_tick</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8725">Improve encapsulation for commands and add docs</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8743">Fix all_tuples + added docs.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8797">Add <code>new</code> and <code>map</code> methods to <code>Ref</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8817">Allow unsized types as mapped value in <code>Ref::map</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8826">Implement <code>Clone</code> for <code>CombinatorSystem</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8818">Add get_ref to EntityRef</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8848">Make <code>QueryParIter::for_each_unchecked</code> private</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8845">Simplify the <code>ComponentIdFor</code> type</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8803">Add last_changed_tick and added_tick to ComponentTicks</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8832">Require read-only queries in <code>QueryState::par_iter</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8939">Fix any_component_removed</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8843">Deprecate type aliases for <code>WorldQuery::Fetch</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7204">bevy_ecs: add untyped methods for inserting components and bundles</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8901">Move AppTypeRegistry to bevy_ecs</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8760">skip check change tick for apply_deferred systems</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8834">Split the bevy_ecs reflect.rs module</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8687">Make function pointers of ecs Reflect* public</a></li>
</ul>
<h3 id="rendering-reflection-scenes">Rendering + Reflection + Scenes
<a href="#rendering-reflection-scenes">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/8088">fix: register Cascade in the TypeRegistry</a></li>
</ul>
<h3 id="tasks">Tasks
<a href="#tasks">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/6690">Add optional single-threaded feature to bevy_ecs/bevy_tasks</a></li>
</ul>
<h3 id="math">Math
<a href="#math">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/8232">Re-export glam_assert feature</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8049">Fix CubicCurve::iter_samples iteration count</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7984">Add integer equivalents for <code>Rect</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8711">Add <code>CubicCurve::segment_count</code> + <code>iter_samples</code> adjustment</a></li>
</ul>
<h3 id="rendering-assets-meta">Rendering + Assets + Meta
<a href="#rendering-assets-meta">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/7855">Add depending bevy features for higher level one</a></li>
</ul>
<h3 id="ecs-scenes">ECS + Scenes
<a href="#ecs-scenes">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/7335">Make scene handling of entity references robust</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7570">Rename map_entities and map_specific_entities</a></li>
</ul>
<h3 id="util">Util
<a href="#util">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/8552">bevy_derive: Add <code>#[deref]</code> attribute</a></li>
</ul>
<h3 id="input">Input
<a href="#input">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/8398">Add gamepad rumble support to bevy_input</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8792">Rename keys like <code>LAlt</code> to <code>AltLeft</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8852">Add window entity to mouse and keyboard events</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8871">Add get_unclamped to Axis</a></li>
</ul>
<h3 id="upgrades">Upgrades
<a href="#upgrades">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/7959">Upgrade Taffy requirement to v0.3.5</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8622">Update ruzstd and basis universal</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8446">Updated to wgpu 0.16.0, wgpu-hal 0.16.0 and naga 0.12.0</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8650">Update sysinfo requirement from 0.28.1 to 0.29.0</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8649">Update libloading requirement from 0.7 to 0.8</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8573">update syn, encase, glam and hexasphere</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7925">Update android_log-sys requirement from 0.2.0 to 0.3.0</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8728">update bitflags to 2.3</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8755">Update ruzstd requirement from 0.3.1 to 0.4.0</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8757">Update notify requirement from 5.0.0 to 6.0.0</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8904">Bump hashbrown to 0.14</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8623">update ahash and hashbrown</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8655">Bump accesskit and accesskit_winit</a></li>
</ul>
<h3 id="examples">Examples
<a href="#examples">#</a>
</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/8561">new example showcase tool</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8194">Adding a bezier curve example</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/6909">Add low level post process example using a custom render pass</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8909">Add example to demonstrate manual generation and UV mapping of 3D mesh (generate_custom_mesh) solve #4922</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/8198">Add <code>overflow_debug</code> example</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7761">UI text wrapping and <code>LineBreakOn</code> example</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7956">Size Constraints Example</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/7629">UI Display and Visibility Example</a></li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The great Polish Sea or We forgot Poland (2006) (121 pts)]]></title>
            <link>https://devblogs.microsoft.com/oldnewthing/20061027-00/?p=29213</link>
            <guid>36657952</guid>
            <pubDate>Sun, 09 Jul 2023 19:23:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devblogs.microsoft.com/oldnewthing/20061027-00/?p=29213">https://devblogs.microsoft.com/oldnewthing/20061027-00/?p=29213</a>, See on <a href="https://news.ycombinator.com/item?id=36657952">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="featured">
                         <p>
            October 27th, 2006</p><!-- .entry-meta -->
        <p>
Open up the Date and Time control panel and go to the Time Zones tab.
Notice anything wrong with the world map?
Take a close look at northern Europe.
</p>
<p>
Depending on what version of Windows you have, you might
see a body of water where Poland should be.
<a href="http://www.microsoft.com/library/media/1033/technet/images/archive/win95/rk03_23_big.gif">
Windows&nbsp;95 didn’t have this problem</a>,
but
<a href="http://web.archive.org/web/20070719151527/https://sa.ku.edu/pshelp/errors/windows2000timezone.gif">
Windows&nbsp;2000 did</a>.
And
<a href="http://web.archive.org/web/20070719151425/https://sa.ku.edu/pshelp/errors/windowsxptimezone.gif">
whether your copy of Windows&nbsp;XP has this problem</a>
depends
on precisely what version you have.
</p>
<p>
Where did the great Polish Sea come from?
</p>
<p>
This weekend marks the end of Summer Time in Europe,
and the answer has to do with time zones.
</p>
<p>
Recall that the Windows&nbsp;95 control panel highlighted your current
time zone on the map.
To accomplish this,
each time zone was assigned a different label in the time zone bitmap.
To draw the map, the portions of the world whose label was the same as
the selected time zone were drawn in bright green, and the parts that
were different were drawn in dark green.
So far so good.
</p>
<p>
When the
<a href="http://blogs.msdn.com/oldnewthing/archive/2003/08/22/54679.aspx">
highlighting on the time zone map had to be disabled</a>,
all that happened was that the “color for the selected time zone”
was set to dark green.
The code still went through the motions of drawing the time zone
in a “different” color, but since the colors were the same at the end of
the day, the visual effect was that the highlighting was removed.
</p>
<p>
To determine which parts of the world are land and which parts
are sea, the time zone map enumerated all the time zones as well
as the labels associated with each time zone.
(You can see them in the registry under “MapID”.)
In this way, the land masses of the
world gradually emerged from the ocean as
the time zones claimed each spot of land one by one.
</p>
<p>
The shell team did make one fatal mistake, however,
obvious in retrospect:
It assumed that the world’s time zones would never change.
But what happens when a country changes its time zone,
as Poland did?
At the time Windows&nbsp;95 was released, Poland was on its own
custom time zone, which Windows&nbsp;95 called
“Warsaw Standard Time/Warsaw Daylight Time”,
but it didn’t stay that way for long.
Just within Windows&nbsp;95 and Windows&nbsp;98,
Poland’s time zone went by the following names:
</p>
<ul>
<li>Windows&nbsp;95: (GMT+01:00) Warsaw
</li><li>Windows&nbsp;95: (GMT+01:00) Lisbon, Warsaw
</li><li>Windows&nbsp;98: (GMT+01:00) Bratislava, Budapest, Ljubljana, Prague, Warsaw
</li><li>Windows&nbsp;98: (GMT+01:00) Sarajevo, Skopje, Sofija, Warsaw, Zagreb
</li></ul>
<p>
And that’s not counting the changes that were made in
Windows&nbsp;NT, Windows&nbsp;2000 or
Windows&nbsp;XP or their service packs.
It’s not that Poland’s time zone actually changed that many times.
Rather, the way it was grouped with its neighbors changed.
I don’t know why all these changes were made,
but I suspect political issues played a major role.
</p>
<p>
As a result of all this realignment,
the “Warsaw Standard Time” time zone
disappeared, and with it, its associated land mass.
Consequently, the land corresponding to Poland remained underwater.
And for some reason, nobody brought this problem to the attention
of the shell team until a couple years ago.
</p>
<p>
In order to fix this, a new world bitmap needed to be made
with new labels (labeling the pixels corresponding to Poland as
“Central European Time”) so that Poland would once again emerge
from the sea.
Even though the highlighting is gone, the map code still needs to
know where every time zone is so it can raise them from the ocean floor.
</p>
<p>
Fortunately, all this will soon fall into the mists of history,
because Windows Vista has a completely rewritten time zone
control panel, so the mistakes of the past can finally be shed.
Let’s hope the people who wrote the new time zone control panel
remembered Poland.</p>

        

		
        
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[InfluxDB Cloud shuts down in Belgium; some weren't notified before data deletion (367 pts)]]></title>
            <link>https://community.influxdata.com/t/getting-weird-results-from-gcp-europe-west1/30615</link>
            <guid>36657829</guid>
            <pubDate>Sun, 09 Jul 2023 19:11:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://community.influxdata.com/t/getting-weird-results-from-gcp-europe-west1/30615">https://community.influxdata.com/t/getting-weird-results-from-gcp-europe-west1/30615</a>, See on <a href="https://news.ycombinator.com/item?id=36657829">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/DiscussionForumPosting" id="main-outlet" role="main">
      <meta itemprop="headline" content="Getting weird results from GCP europe-west1">
        <meta itemprop="articleSection" content="InfluxDB 2">
      <meta itemprop="keywords" content="influxdb">
      

          <div itemprop="articleBody" id="post_1">
              <p>I’m using hosted InfluxDB on europe-west1. Now this weekend I’m see weird behaviour in my dashboard, missing or incomplete data or data that changes between refreshed… I see this behaviour on all dashboards.</p>
<p>I saw this on the InfluxDB status page:</p>
<blockquote>
<p>Discontinuation of AWS ap-southeast-2 (Sydney) and GCP europe-west1 (Belgium)</p>
</blockquote>
<p>Retention on all my buckets is: forever.</p>
<p>Could this be the cause? I hope I’m not losing my data… I did not get emails from InfluxDB about this change. Can anyone elaborate on this?</p>
            </div>
          <div itemprop="comment" id="post_3" itemscope="" itemtype="http://schema.org/Comment">
              <p>Same issues here. I use <a href="https://europe-west1-1.gcp.cloud2.influxdata.com/" rel="noopener nofollow ugc">https://europe-west1-1.gcp.cloud2.influxdata.com/</a></p>
<p>I was getting missing data yesterday and now I get nothing today?</p>
            </div>
          <div id="post_4" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>I think GCP europe-west1 data is gone with the wind now lol</p>

            

            

          </div>
          <div itemprop="comment" id="post_5" itemscope="" itemtype="http://schema.org/Comment">
              <p>Same issue here.</p>
<p>You cannot just shut down a service without informing your customer via mail. What are solutions for us that do not involve data loss?</p>
            </div>
          <div itemprop="comment" id="post_6" itemscope="" itemtype="http://schema.org/Comment">
              <p>As stated in <a href="https://docs.influxdata.com/influxdb/cloud/reference/regions/" rel="noopener nofollow ugc">InfluxDB Cloud regions | InfluxDB Cloud (TSM) Documentation</a></p>
<blockquote>
<p>To continue service after <strong>June 30, 2023</strong> , InfluxDB Cloud accounts in these regions must be migrated to different regions. For information about migrating to a different region, see <a href="https://docs.influxdata.com/influxdb/cloud/migrate-regions/" rel="noopener nofollow ugc">Migrate to an account in a new region</a>.</p>
</blockquote>
            </div>
          <div itemprop="comment" id="post_7" itemscope="" itemtype="http://schema.org/Comment">
              <p>Yes but we were never <em>informed</em> about this. We have a running use case and are not in the habit of checking the documentation every week just in case our service gets cancelled without prior warning.</p>
<p>InfluxData should also have seen that instances in these regions still had read and write access and informed all affected customers.</p>
<p>This is highly unprofessional</p>
            </div>
          <div id="post_8" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>I’m having the same problem. I never got an email informing me of this. How do I recover my data?</p>

            

            

          </div>
          <div itemprop="comment" id="post_9" itemscope="" itemtype="http://schema.org/Comment">
              <p>Another victim here.</p>
<p>According to the support, the notification emails to the users were sent on Feb 23, Apr 6 and May 15th. However, we did not receive those at all.</p>
<p>The support also mentioned that restoring data from Belgium now is impossible as the region has been discontinued.</p>
<p>I would say the service quality is a bit dodgy… We are considering other alternative solutions now.</p>
            </div>
          <div itemprop="comment" id="post_10" itemscope="" itemtype="http://schema.org/Comment">
              <p>Wow. We were considering InfluxDB and the cloud offering for a future workload. That’s off the table now.</p>
<p>Hope you guys don’t suffer from any data loss.</p>
            </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sarah Silverman is suing OpenAI and Meta for copyright infringement (293 pts)]]></title>
            <link>https://www.theverge.com/2023/7/9/23788741/sarah-silverman-openai-meta-chatgpt-llama-copyright-infringement-chatbots-artificial-intelligence-ai</link>
            <guid>36657540</guid>
            <pubDate>Sun, 09 Jul 2023 18:43:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2023/7/9/23788741/sarah-silverman-openai-meta-chatgpt-llama-copyright-infringement-chatbots-artificial-intelligence-ai">https://www.theverge.com/2023/7/9/23788741/sarah-silverman-openai-meta-chatgpt-llama-copyright-infringement-chatbots-artificial-intelligence-ai</a>, See on <a href="https://news.ycombinator.com/item?id=36657540">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Comedian and author Sarah Silverman, as well as authors Christopher Golden and Richard Kadrey — are suing <a href="https://www.documentcloud.org/documents/23869693-silverman-openai-complaint?responsive=1&amp;title=1">OpenAI</a> and <a href="https://www.documentcloud.org/documents/23869675-kadrey-meta-complaint?responsive=1&amp;title=1">Meta</a> each in a US District Court over <a href="https://llmlitigation.com/">dual claims of copyright infringement</a>. </p><p>The suits alleges, among other things, that OpenAI’s ChatGPT and Meta’s LLaMA were trained on illegally-acquired datasets containing their works, which they say were acquired from “shadow library” websites like Bibliotik, Library Genesis, Z-Library, and others, noting the books are “available in bulk via torrent systems.”</p><p>Golden and Kadrey each declined to comment on the lawsuit, while Silverman’s team did not respond by press time.</p><p>In the OpenAI suit, the trio <a href="https://www.documentcloud.org/documents/23869694-silverman-openai-complaint-exhibits?responsive=1&amp;title=1">offers exhibits</a> showing that when prompted, ChatGPT will summarize their books, infringing on their copyrights. Silverman’s <em>Bedwetter </em>is the first book shown being summarized by ChatGPT in the exhibits, while Golden’s book <em>Ararat </em>is also used as an example, as is Kadrey’s book <em>Sandman Slim</em>. The claim says the chatbot never bothered to “reproduce any of the copyright management information Plaintiffs included with their published works.”</p><div><p>As for the separate lawsuit against Meta, it alleges the authors’ books <a href="https://www.documentcloud.org/documents/23869695-kadrey-meta-complaint-exhibits?responsive=1&amp;title=1">were accessible in datasets Meta used</a> to train its LLaMA models, a <a href="https://www.theverge.com/2023/2/24/23613512/meta-llama-ai-research-large-language-model">quartet of open-source AI Models</a> the company introduced in February. </p></div><p>The complaint lays out in steps why the plaintiffs believe the datasets have illicit origins — in a <a href="https://arxiv.org/pdf/2302.13971.pdf">Meta paper detailing LLaMA</a>, the company points to sources for its training datasets, one of which is called ThePile, which was assembled by a company called EleutherAI. ThePile, the complaint points out, was described in an <a href="https://arxiv.org/abs/2101.00027">EleutherAI paper</a> as being put together from “a copy of the contents of the Bibliotik private tracker.” Bibliotik and the other “shadow libraries” listed, says the lawsuit, are “flagrantly illegal.”</p><p>In both claims, the authors say that they “did not consent to the use of their copyrighted books as training material” for the companies’ AI models. Their lawsuits each contain six counts of various types of copyright violations, negligence, unjust enrichment, and unfair competition. The authors are looking for statutory damages, restitution of profits, and more.</p><p>Lawyers Joseph Saveri and Matthew Butterick, who are representing the three authors, write on their <a href="https://llmlitigation.com/">LLMlitigation website</a> that they’ve heard from “writers, authors, and publishers who are con­cerned about [ChatGPT’s] uncanny abil­ity to gen­er­ate text sim­i­lar to that found in copy­righted tex­tual mate­ri­als, includ­ing thou­sands of books.”</p><p>Saveri has also started litigation against AI companies on behalf of <a href="https://www.theverge.com/2023/1/28/23575919/microsoft-openai-github-dismiss-copilot-ai-copyright-lawsuit">programmers</a> and <a href="https://www.theverge.com/2023/1/16/23557098/generative-ai-art-copyright-legal-lawsuit-stable-diffusion-midjourney-deviantart">artists</a>. Getty Images also <a href="https://www.theverge.com/2023/1/17/23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit">filed an AI lawsuit</a>, alleging that Stability AI, who created the AI image generation tool Stable Diffusion, trained its model on “millions of images protected by copyright.” Saveri and Butterick are also representing authors Mona Awad and Paul Tremblay <a href="https://www.theguardian.com/books/2023/jul/05/authors-file-a-lawsuit-against-openai-for-unlawfully-ingesting-their-books">in a similar case</a> over the company’s chatbot. </p><p>Lawsuits like this aren’t just a headache for OpenAI and other AI companies; they are <a href="https://www.theverge.com/23444685/generative-ai-copyright-infringement-legal-fair-use-training-data">challenging the very limits of copyright</a>. There’s As we’ve said on <em>The Vergecast</em> every time someone gets Nilay going on copyright law, we’re going to see lawsuits centered around this stuff for <a href="https://www.theverge.com/2023/4/1/23666153/the-ai-copyright-dilemma-is-probably-a-decade-of-lawsuits-to-come">years to come</a>.</p><p>We’ve reached out to Meta, OpenAI, and the Joseph Saveri Law Firm for comment, but they did not respond by press time.</p><p>Here are the suits:</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amateurs obsess over tools, pros over mastery (289 pts)]]></title>
            <link>https://adamsinger.substack.com/p/amateurs-obsess-over-tools-pros-over</link>
            <guid>36657477</guid>
            <pubDate>Sun, 09 Jul 2023 18:37:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://adamsinger.substack.com/p/amateurs-obsess-over-tools-pros-over">https://adamsinger.substack.com/p/amateurs-obsess-over-tools-pros-over</a>, See on <a href="https://news.ycombinator.com/item?id=36657477">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6c15b28-a04f-4d3b-9789-93200fd7cc0b_500x375.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6c15b28-a04f-4d3b-9789-93200fd7cc0b_500x375.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6c15b28-a04f-4d3b-9789-93200fd7cc0b_500x375.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6c15b28-a04f-4d3b-9789-93200fd7cc0b_500x375.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6c15b28-a04f-4d3b-9789-93200fd7cc0b_500x375.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6c15b28-a04f-4d3b-9789-93200fd7cc0b_500x375.jpeg" width="500" height="375" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e6c15b28-a04f-4d3b-9789-93200fd7cc0b_500x375.jpeg&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:375,&quot;width&quot;:500,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6c15b28-a04f-4d3b-9789-93200fd7cc0b_500x375.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6c15b28-a04f-4d3b-9789-93200fd7cc0b_500x375.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6c15b28-a04f-4d3b-9789-93200fd7cc0b_500x375.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6c15b28-a04f-4d3b-9789-93200fd7cc0b_500x375.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption><em>image credit:&nbsp; nj dodge, via flickr cc</em></figcaption></figure></div><p>In our increasingly tech trend-driven world, it's easy to get caught up in the allure of shiny new tools. From the latest productivity apps to wiz-bang gadgets, we're constantly bombarded with promises of increased efficiency, higher output, and success beyond our dreams (both the creative and financial variety). We obsess over these tools, treating them like a crush—a fleeting infatuation that momentarily captivates our attention. All of this is wasted effort and delusion.</p><p><span>Of course, if you talk to a grizzled pro or anyone you actually respect in an industry you’ll find a constant: tools alone do not make a master. It's not the latest software or the fastest hardware that defines greatness; it's the mindset and skill of the individual wielding them. As the philosopher Seneca once stated, </span><em>"A sword never kills anybody; it is a tool in the killer's hand."</em></p><p>Take the acoustic guitar, for ex. In an age of digital music production and synthesizers, this instrument played alone might appear antiquated. Yet, in the hands of a skilled musician, it transforms into a vessel of captivating melodies and soul-stirring harmonies. It might even provide the inspiration for something larger, that would have been missed if you skipped right to software. The simplicity of the instrument compels the artist to focus on the nuances of their playing, to refine their fingerpicking technique, and to channel their emotions through each strum. The true magic lies not in the guitar itself, but in the virtuosity of the musician who brings it to life. A skill, mastered over time, agnostic and ambivalent of the noisy, buzzing, ADHD-ridden market.</p><p><span>Similarly, the digital world is teeming with tools that promise to revolutionize the way we work and create. But if we're fixated on acquiring every new tool that comes our way, we risk missing out on developing our fundamental, timeless skills—the abilities that transcend technological trends and persist throughout time. It’s almost always backwards to care much here. </span><strong>The important tools will find you</strong><span>. It’s also not a real moat, or recipe for producing anything great. Perhaps a fleeting viral post for “being first,” and really what’s the point of that?</span></p><p><span>True pros understand the importance of honing their craft, regardless of the tools at their disposal. They embrace the philosophy of Bruce Lee, who famously stated, "</span><em>I fear not the man who has practiced 10,000 kicks once, but I fear the man who has practiced one kick 10,000 times</em><span>." It's the expertise gained through deliberate, consistent practice and a deep understanding of the fundamentals that separates pros from amateurs.</span></p><p><span>It is real alpha to ignore the allure of novelty. Let us instead focus on the foundations, the timeless principles, and the relentless pursuit of mastery. Paradoxically, AI will make this </span><em>more</em><span> true. This will also ensure you </span><a href="https://adamsinger.substack.com/p/the-rise-of-ai-nihilism?utm_source=profile&amp;utm_medium=reader2" rel="">escape the nihilism</a><span> inherent to creative reliance on AI.</span></p><p>The next time you find yourself FOMO’ing after the latest gadget or the trendiest app, pause for a moment of reflection. Ask yourself: Am I truly honing my craft? Am I investing here only because others told me to? Am I really doing something that matters? Am I actually just a goldfish chasing a shiny lure? I worry many lack this sort of metacognition in a world where it’s more common to become a garden variety stock promoter of the latest fad. Perhaps a superpower for you to not do this and instead do the harder thing.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Let the internet be grimy (107 pts)]]></title>
            <link>https://tedium.co/2023/07/08/threads-social-media-brand-safety/</link>
            <guid>36657391</guid>
            <pubDate>Sun, 09 Jul 2023 18:28:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tedium.co/2023/07/08/threads-social-media-brand-safety/">https://tedium.co/2023/07/08/threads-social-media-brand-safety/</a>, See on <a href="https://news.ycombinator.com/item?id=36657391">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
      <p><span><strong>Today in Tedium:</strong> I don’t feel particularly motivated to write about Threads, the Instagram-in-Twitter-form social network that Meta launched this week. It feels like a social network that exists to check a box for a company that owns a lot of social networks. But I do think it represents something about the moment that we’re in, where people are so desperate for a certain kind of experience that they will go from the hands of one billionaire to another, in hopes that will give them what they don’t feel like they were getting before. If Threads descended into the bottom of the ocean on a poorly built submersible, I would not care. But what I do care about is <em>why</em> I don’t care about Threads. And I think it comes down to the reasons why this site lives on the open web, even if I can’t leave the networks. Like many internet creatures of a certain age, I came from grime. We try to pretend the grime isn’t there, we dress it up as best we can, but it’s always there, no matter how hard we try to pretend it’s not. Today’s Tedium defends the grime from which the internet was built. <em>— Ernie @ Tedium</em></span></p>

<div>


<p><lite-youtube videoid="pDdGMWNvUDM"><a href="https://www.youtube.com/watch?v=pDdGMWNvUDM">Watch on YouTube</a></lite-youtube></p>
<h3>Learn the Future of Marketing and Unleash the Power of AI with this comprehensive 12 week course</h3>
<p><strong>Learn exactly how companies at scale</strong> have used AI to grow and expand their marketing efforts, and tap into the competitive advantage you need to outperform.</p>
<p><a href="https://gigantic.is/courses/ai-marketing?utm_source=tedium&amp;utm_medium=email&amp;utm_campaign=ai_marketing&amp;utm_date=70823">Deep dive into real-world case studies</a> and learn how AI can transform data into actionable insights, supercharge targeting precision, and maximize customer engagement.</p>
<p>Equip yourself with the tools and knowledge to harness AI’s immense potential and surpass expectations. Discover how AI algorithms optimize ad placements, automate repetitive tasks, and fine-tune personalized messaging to maximize ROI.</p>
<p>Each week you’ll work through a comprehensive curriculum, featuring hands-on exercises and practical insights. Gain the skills you need to confidently implement AI-driven marketing strategies.</p>


</div>
<div>
    
<p><img data-src="https://images.tedium.co/uploads/Grime.jpg" width="1000" height="563" alt="Grime" src="https://images.tedium.co/uploads/Grime.jpg"></p>
<p><em>(<a href="https://unsplash.com/photos/2HzqQdj56T0">rhubbardstockfootage/Unsplash</a>)</em></p>
<h3>The internet was built from grime; the problem is, we keep trying to build shopping malls over the grime</h3>
<p>There are amazing things about being able to control an environment. Controlled environments are what gave us air conditioning and malls and factories.</p>
<p>But sometimes, you have to walk outside that environment and experience the outside world—the sun, the uncontrollable temperatures, the risks of clothes-dirtying weather events.</p>
<p>Now, to be clear, the internet we use benefits greatly from controlled temperatures and giant rooms protected from elements. Cloud computing, when broken down, should really be described as “pod computing.” But the metaphor, from an end user standpoint, I think stands.&nbsp;</p>
<p>The thing that I think made the internet such an interesting place in its early years was because it didn’t feel like a controlled environment. The chaos was everywhere. It was messy. It was grimy.</p>
<p>The first experience that many people got with the internet was a busy signal, not a welcome mat. When people joined communities like Usenet, they were more likely to get an elbow to the shoulder for not following the rules, rather than a welcome mat.</p>
<p>And honestly, though I tell people not to be jerks on the internet, people were kind of jerks!&nbsp;</p>
<p>When compared to the tools people used to consume information, the internet was rough and tumble and messy. Television had prime time and a simple-to-follow organizational model. Newspapers had front pages and sections, and were organized hierarchically.</p>
<p>Even other digital mediums were better organized than the internet. <a href="https://tedium.co/2020/08/11/compuserve-videotex-online-news-history/">CompuServe</a> and <a href="https://tedium.co/2021/07/23/aol-community-leader-volunter-program-history/">AOL</a> were way easier to follow and far less chaotic than the open internet. <a href="https://tedium.co/2017/07/13/who-killed-the-encyclopedia/">Encarta</a> could give you way easier to follow results than any web page could in 1995.</p>
<p>The internet, on the other hand, was feudalism. There was no plot, really. The tools that were made available were specifically non-user-friendly and unwelcoming. If someone made fun of you or made you mad, there was no central authority to ask for help.</p>
<p>But despite all that, the ungated, non-temperature-controlled, rainy, dirty, grimy internet still won.</p>
<p>I argue it won in part <em>because</em> of the chaos. That chaos was the stuff that Hollywood fell over itself attempting to display in early technology movies like <a href="https://tedium.co/2019/03/07/internet-food-delivery-history/"><em>The Net</em></a> and <em>Hackers</em>. They didn’t have time for pretty, comfortable takes on the internet like <a href="https://www.macworld.com/article/223467/remembering-eworld-apples-forgotten-online-service.html">Apple’s eWorld</a> or <a href="https://tedium.co/2017/07/27/yahoo-games-failure-classicgames/">Yahoo</a>.</p>
<p>I will not say that this was perfect, but the chaotic effect was interesting, and interesting was often enough to continue using, because it meant there were always new surprises. For lots of people, chaos often breeds new ways of thinking.</p>
<p>But even after things became more formalized, chaotic environments continued to hold a unique appeal on the internet. The early blogosphere was full of people who would promise ambitious things, but never follow through. Forums like Something Awful had arbitrary approaches to rules and made money essentially through aggressive rule enforcement. Many of these platforms were anonymous in nature.</p>
<p>Even MySpace, the first truly popular social network, became popular because of <a href="https://tedium.co/2020/07/14/social-media-customization-failings/">a gaping security hole</a>.</p>
<p>And startups, nothing against the people who make startups today, were much less influenced by what other startups were doing. That fostered newer, more original ideas, rather than the modest wrench-tightening we see on sites like Product Hunt today.</p>
<p>Think about it: <a href="https://tedium.co/2020/02/22/craigslist-25th-anniversary/">Craigslist</a> and <a href="https://www.ebayinc.com/company/our-history/">eBay</a> had no comparison points in the real world. They were basically ideas that were total accidents, but became billion-dollar businesses. They came from chaos, but chaos dresses itself up pretty well before a big night out on the town.</p>
<p>The problem is, there are limits to reining in the chaos for commercial reasons, and this, I think gets to why I don’t care about Threads.</p>
</div>
<div>
    
<p><img data-src="https://images.tedium.co/uploads/Disney-World.jpg" width="1000" height="1333" alt="Disney World" src="https://images.tedium.co/uploads/Disney-World.jpg"></p>
<p><em>You know what Threads is kind of like? Disney World. That’s not really a compliment. (<a href="https://unsplash.com/photos/pJ0FNi_xn20">Rayna Tuero/Unsplash</a>)</em></p>
<h3>Brand safety, billion-dollar companies, and why networks like old Twitter are all too rare</h3>
<p>Adam Mosseri, the dude who runs the Instagram team and is a key figure in the launch of Threads, <a href="https://www.theverge.com/2023/7/7/23787334/instagram-threads-news-politics-adam-mosseri-meta-facebook">gave away the game this week</a> when he admitted that the company will specifically not do anything to promote political or news content on Threads.</p>
<p>“Politics and hard news are important, I don’t want to imply otherwise,” <a href="https://www.threads.net/t/CuZ6opKtHva/">he said in a Thread</a>. “But my take is, from a platform’s perspective, any incremental engagement or revenue they might drive is not at all worth the scrutiny, negativity (let’s be honest), or integrity risks that come along with them.”</p>
<p>Without saying the exact words, he’s basically admitting that news and politics—the entire category—aren’t brand-safe, and as a result, those are the very things that will be discouraged on the platform.</p>
<p>Threads threatens to be social media’s Disney World, and while Disney World has its fans, it is by its nature top-down culture. All the good stuff comes from the bottom up.</p>
<p>You don’t go to Disney World to be sad or angry. You go to be excited, or enthralled. You don’t go there for chaos.</p>
<p>By Threads becoming a thing and being presented as something where it’s automatically presumed you will want celebrities and influencers dominating your feed at all times, with no algorithm-free option, the goal is basically to get people to learn to love chaos-free social media.</p>
<p>Sure, some chaos will seep in, as it always does. But it won’t define the network, like its competition long has. It will be Mountain Dew without the carbonation.</p>
<p>I think Twitter, especially in its early years, was most effective in nailing down a balance between chaos and brand safety that few companies have ever pulled off at the multi-billion scale Twitter was playing at. It was the perfect example of a business-in-the-front, party-in-the-back platform. It was able to scale, if slower than some of its competitors, because it found the magic center-point between being a place where professionals could communicate and people could screw around and try new, weird things.</p>
<p>Was it perfect at this? No—and the leadership arc of Jack Dorsey proves it. But it was much more successful at it than another company that relies on bottling up chaos, Reddit—a firm whose recent drama is a direct result of the fact that it has underperformed financially, leading to some desperate Hail Mary actions to revive its stock fortunes.&nbsp;</p>
<p>These sorts of outcomes naturally emerged because of how we monetized it. I think as the internet became more formal and people figured out better ways to use it, we became more optimized on meeting user expectations or following data to meet specific goals.</p>
<p>Data is a great way to build and sustain a business, because it gives you a useful comparison point. But in many ways, data is destructive. The media industry, I believe, offers the best example of this I can think of. For roughly a century, they held local monopolies on local advertising, and lost much of that monopoly money in, essentially, a decade.</p>
<p>I feel like this exact same mindset is being used to starve out the weird old internet, the grimy parts of the digital experience that are a little shady but are much more enjoyable than modern-day Times Square.</p>
<p>We even see this with hardware—there’s a reason everyone on the internet complains that Apple spent 30 minutes during its last keynote talking about the software features on the watch when they actually wanted to hear about the Mac Pro. The market research pushed Apple in that direction. People who complain about the Mac Pro aren’t actually going to buy one—but people who passively observe details on the Watch probably will.</p>
<p>Data doesn’t care that it harms structural underpinnings, because the data usually only matters to the companies using it.</p>
<p>If you want the vibe of the old-school internet to live, you need to look outside its money-making potential. Silicon Valley has already decided they don’t really have a use for it anymore.</p>
</div>
<div>
    
<p><img data-src="https://images.tedium.co/uploads/Puzzle.jpg" width="1000" height="667" alt="Puzzle" src="https://images.tedium.co/uploads/Puzzle.jpg"></p>
<p><em>Often, a website or platform worth supporting doesn’t have all the puzzle pieces put together quite yet. As a user, that’s your job. (<a href="https://unsplash.com/photos/sOK9NjLArCw">Gabriel Crismariu/Unsplash</a>)</em></p>
<h3>On platforms being “hard to use”: Suck it up, or you’ll keep getting bad outcomes</h3>
<p>So why do these extremely neutered, brand-safe versions of internet culture seem to be winning out over the good stuff?</p>
<p>I think, in some ways, it’s a combination of factors. First, it’s money and resources. Mark Zuckerberg and Elon Musk will always have more money than any of us, unless for some reason Bill Gates is reading this.</p>
<p>Then, there’s an understanding that these platforms are influential, which encourages them to be utilized for political or influence reasons—something we’ve been seeing since at least the 2016 election.</p>
<p>But, ultimately, I think end users deserve a little real talk here: You’re not going to get the internet culture you want if you also expect a Disney World-style controlled-climate experience.</p>
<p>You are going to have to deal with janky interfaces, built by people who love and care about what they’re doing, but who don’t have the money or research capabilities of your favorite local billionaire. If we are going to protect the good, interesting, chaotic parts of the internet, we need to be willing to tell people to suck it up and experience some jank.</p>
<p>The thing is, when Twitter became problematic, we had the Fediverse and ActivityPub and Mastodon to build upon instead. But people who came from Twitter didn’t give it a shot, or they chose not to give it a chance.</p>
<p>I’m still having debates with people about this, months later, and it doesn’t matter that the Fediverse gives its users access to strong communities and better aligns with their ethical compass because it was built by people who aren’t focused on making money through advertising. They just don’t want to deal with something complicated.</p>
<p>For example: I love Ryan Broderick and <a href="https://www.garbageday.email/">Garbage Day</a>, and he has had the <a href="https://www.garbageday.email/p/the-algorithmic-anti-culture-of-scale">best take</a> on Threads so far, <a href="https://www.garbageday.email/p/elon-musk-is-more-funko-pop-than">but his general take on Mastodon</a> (especially in comparison to Bluesky, but pretty much in general) has deeply disappointed me as a fan of his. I don’t think he has to like it, to be fair, but I also think that “it wasn’t immediately intuitive” is simply not enough to basically disregard something forever. Not when we have to live with this stuff for most of our online lives. Pixels can be fixed and alternative interfaces are easy to use. Ethical underpinnings? We’re stuck with those a lot longer.</p>
<p>It doesn’t matter that ethically Mastodon and its related protocols are more ethically sound. Because it doesn’t display posts in the way he prefers, it means … bleh, do not want.</p>
<p>As a counterpoint to that, I would like to pose a suggestion to you, the end user: Let yourself get confused! Confusion is chaos—and chaos leads to more interesting outcomes.</p>
<p>If you say you “have standards” of what you’ll accept from a digital platform in 2023, I say this in return: Your standards are holding you back. These new platforms, of which there are many, will never be Twitter or have the money of anyone associated with Meta. But they can be fun, interesting, and engaging, even if they don’t look or feel quite like what you’re used to.</p>
<p>We are so obsessed with trying to be comfortable online that we forget that a little discomfort led to its best parts.</p>
<p><img data-src="https://images.tedium.co/uploads/Twitter-2009.png" width="979" height="600" alt="Twitter 2009" src="https://images.tedium.co/uploads/Twitter-2009.png"></p>
<p><em>People thought this was hard to use.</em></p>
<p>Usenet and IRC and MySpace and blogs and forums all came with a little bit of jank. In some cases, a lot of it. Even Twitter had some jank—it was an interface that <a href="https://timtfj.com/2009/04/05/confusing/">many end users found hard to follow at first</a>. But people still used it, and found value in those experiences.</p>
<p>The thing is, large companies and centralized social networks are incentivized to create the best possible experience, because that’s what they’re aiming for. And if that’s what you want—if you want a digital form of Disney World, or you want to walk in the underground of <a href="https://www.washingtonian.com/2017/05/19/notes-from-underground-of-crystal-citys-labyrinthine-mall/">Crystal City</a>—that’s fine. Just know that you might be missing out on some of the internet’s best parts going forward.</p>
<p>Climate-controlled environments have their limits. If you want the real thing and not just Disney World, you’re going to have to work for it.</p>
<p><em>We’re</em> going to have to work for it.</p>
</div>
<div>
    
<p><strong>Recently, I’ve been spending more time at the beach.</strong> My wife tends to make us walk out to a quieter spot at the beach, with fewer people.</p>
<p>The beach is less Jersey Shore, more nature preserve, and the tides occasionally wash up interesting things. One day, we might see a turtle. Another day, an interesting piece of driftwood might emerge.</p>
<p>I am more of an indoor cat, an internet citizen, and I complain a lot along the way, always worried that the long lengths we’re going to get there, often in extreme heat, are unnecessary. But usually, by the time I dip into the water, I shut up and find myself embracing an enjoyable experience. I get in my head too much, but once I give in, I realize I’ve been actively working against my own interests.</p>
<p>I think that, while useful, internet culture’s intense focus on user interface has been a real damaging factor for the free and open internet. User interface does matter, but the discipline has gotten so complex that the companies that do it most effectively are often willing to pay top-dollar for good design that can shape a better experience. And in some cases, it leads to UX that works directly against the user.</p>
<p>Open-source platforms, try as they might, struggle to compete on a user interface front. The haul is always a little longer, the lift slightly more back-breaking. But the results in the end are often so much better for the end user.</p>
<p>For end users, you might want to take a lesson from my beach anecdote, because there’s a chance you might be giving up something great along the way.</p>
<p>--</p>
<p>Find this one an interesting read? <a href="https://tedium.co/2023/07/08/threads-social-media-brand-safety/">Share it with a pal</a>!</p>
<p>And thanks to Gigantic for sponsoring. Sign up for <a href="https://gigantic.is/courses/ai-marketing?utm_source=tedium&amp;utm_medium=email&amp;utm_campaign=ai_marketing&amp;utm_date=70823">their AI and marketing course</a> over this way.</p>
</div>

    </div><p><label for="subscribebox-toggle">
      <span>
        <span>
        <h3>Like this? Well, you should read more of our stuff.</h3>

        <p data-reveal-id="subscribe-popup" data-reveal=""><span></span>&nbsp;Get more issues in your inbox
        </p>
        </span>
        </span>
    </label>
  </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building cross-platform Rust for Web, Android and iOS – a minimal example (131 pts)]]></title>
            <link>https://www.artificialworlds.net/blog/2022/07/06/building-cross-platform-rust-for-web-android-and-ios-a-minimal-example/</link>
            <guid>36656145</guid>
            <pubDate>Sun, 09 Jul 2023 16:54:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.artificialworlds.net/blog/2022/07/06/building-cross-platform-rust-for-web-android-and-ios-a-minimal-example/">https://www.artificialworlds.net/blog/2022/07/06/building-cross-platform-rust-for-web-android-and-ios-a-minimal-example/</a>, See on <a href="https://news.ycombinator.com/item?id=36656145">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
	<p><a href="#content">
		Skip to content	</a></p><!-- .sidebar -->

	<div id="content">
		<main id="main">

		
<article id="post-3426">
	
	<!-- .entry-header -->

	<!-- AJB social -->
	                                


	<div>
		<p>One of the advantages of writing code in <a href="https://www.rust-lang.org/">Rust</a> is that it can be re-used in other places. Both iOS and Android allow using native libraries within your apps, and Rust compiles to native. Web pages can now use <a href="https://webassembly.org/">WebAssembly</a> (WASM), and Rust can compile to WASM.</p>
<p>So, it should be easy, right?</p>
<p>Well, in practice it seems a little tricky, so I created a small example project to explain it to myself, so maybe it’s helpful to you too.</p>
<p>The full code is at <a href="https://gitlab.com/andybalaam/example-rust-bindings">gitlab.com/andybalaam/example-rust-bindings</a>, but here is the general idea:</p>
<ul>
<li><strong><tt>crates/example-rust-bindings</tt></strong> – the real Rust code</li>
<li><strong><tt>bindings/ffi</tt></strong> – uniffi code to build shared objects for Android and iOS</li>
<li><strong><tt>bindings/wasm</tt></strong> – wasm_bingen code to build WASM for Web</li>
<li><strong><tt>examples/example-android</tt></strong> – an Android app that generates a Kotlin wrapper, and runs the code in the shared object</li>
<li><strong><tt>examples/example-ios</tt></strong> – an iOS XCode project where we generate Swift bindings, so we can call the code in the shared object</li>
<li><strong><tt>examples/example-web</tt></strong> – a web page that imports the WASM and runs it</li>
</ul>
<h2>Steps for WASM</h2>
<p><img decoding="async" src="https://www.artificialworlds.net/blog/wp-content/uploads/wasm-firefox.png" alt="Proof that I did this on Web - Firefox showing &quot;This string is from Rust!&quot;" width="456" height="340" srcset="https://www.artificialworlds.net/blog/wp-content/uploads/wasm-firefox.png 456w, https://www.artificialworlds.net/blog/wp-content/uploads/wasm-firefox-300x224.png 300w" sizes="(max-width: 456px) 100vw, 456px"></p>
<ul>
<li>Write normal Rust code e.g. <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/crates/example-rust-bindings/src/lib.rs">crates/example-rust-bindings/src/lib.rs</a> (and <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/crates/example-rust-bindings/Cargo.toml">Cargo.toml</a> etc. to go with it, with nothing special in it)</li>
<li>Write <tt>wasm-bindgen</tt> wrappers for all interfaces you want to expose e.g. <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/bindings/wasm/src/lib.rs">bindings/wasm/src/lib.rs</a> (and <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/bindings/wasm/Cargo.toml">Cargo.toml</a> to go with it, depending on <tt>wasm-bindgen</tt>). This basically involves creating structs and functions annotated with <tt>#[wasm_bindgen]</tt> that call through to the real underlying code from the previous step.</li>
<li>Write a <tt>package.json</tt> with a <tt>build</tt> step that calls <tt>wasm-pack</tt> e.g. <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/bindings/wasm/package.json#L31">bindings/wasm/package.json</a></li>
<li>Build the bindings with npm (see <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/bindings/wasm/README.md">bindings/wasm/README.md</a>)</li>
<li>Copy the generated .js  and .wasm files into a web project</li>
<li>Include the generated .js file into an HTML file using an <tt>import</tt> statement in a module e.g. <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/examples/example-web/index.html">examples/example-web/index.html</a></li>
</ul>
<p>Variation: if you modify the <tt>build</tt> script in <tt>package.json</tt> to call <tt>wasm-pack</tt> with <tt>--target node</tt> instead of <tt>--target web</tt> you can generate code suitable for using from a NodeJS module.</p>
<h2>Steps for Android</h2>
<p><img decoding="async" loading="lazy" src="https://www.artificialworlds.net/blog/wp-content/uploads/native-android.png" alt="Proof that I did this on Android: Android emulator showing a label &quot;This string is from Rust!&quot;" width="241" height="507" srcset="https://www.artificialworlds.net/blog/wp-content/uploads/native-android.png 241w, https://www.artificialworlds.net/blog/wp-content/uploads/native-android-143x300.png 143w" sizes="(max-width: 241px) 100vw, 241px"></p>
<ul>
<li>Write normal Rust code e.g. <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/crates/example-rust-bindings/src/lib.rs">crates/example-rust-bindings/src/lib.rs</a> (and <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/crates/example-rust-bindings/Cargo.toml">Cargo.toml</a> etc. to go with it, with nothing special in it)</li>
<li>Write a UDL file to describe your interfaces e.g. <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/bindings/ffi/src/my_rust_code.udl">bindings/ffi/src/my_rust_code.udl</a></li>
<li>Implement those interfaces in some Rust code, wrapping returned values in <tt>Arc</tt> e.g. <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/bindings/ffi/src/lib.rs">bindings/ffi/src/lib.rs</a>.</li>
<li>Write a <tt>Cargo.toml</tt> that builds the bindings e.g. <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/bindings/ffi/Cargo.toml">bindings/ffi/Cargo.toml</a></li>
<li>Generate uniffi bindings scaffolding during the Cargo build by writing a <tt>build.rs</tt> file e.g. <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/bindings/ffi/build.rs">bindings/ffi/build.rs</a></li>
<li>Get the Android NDK and set up Cargo’s config to use it to build – see <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/bindings/ffi/README.md">bindings/ffi/README.md</a></li>
<li>Cross-compile for the Android platforms you need by adding the targets using <tt>rustup</tt> and then building using <tt>cargo build --target=blah</tt> – see <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/bindings/ffi/README.md">bindings/ffi/README.md</a></li>
<li>Copy the built .so shared object files into your Android project under `jniLibs/INSERT_PLATFORM` – see <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/bindings/ffi/README.md">bindings/ffi/README.md</a></li>
<li>In your Android project, add a <tt>android.applicationVariants.all</tt> section to your <tt>app/build.gradle</tt> that generates Kotlin wrappers around the shared objects e.g. <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/examples/example-android/app/build.gradle">examples/example-android/app/build.gradle</a></li>
<li>Now you can write normal Kotlin that accesses your Rust code via a namespace like <tt>uniffi.my_rust_code</tt> e.g. <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/examples/example-android/app/src/main/java/net/artificialworlds/exampleandroid/MainActivity.kt">MainActivity.kt</a></li>
</ul>
<h2>Steps for iOS</h2>
<ul>
<li>Write normal Rust code e.g. <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/crates/example-rust-bindings/src/lib.rs">crates/example-rust-bindings/src/lib.rs</a> (and <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/crates/example-rust-bindings/Cargo.toml">Cargo.toml</a> etc. to go with it, with nothing special in it)</li>
<li>Write a UDL file to describe your interfaces e.g. <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/bindings/ffi/src/my_rust_code.udl">bindings/ffi/src/my_rust_code.udl</a></li>
<li>Implement those interfaces in some Rust code, wrapping returned values in <tt>Arc</tt> e.g. <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/bindings/ffi/src/lib.rs">bindings/ffi/src/lib.rs</a>.</li>
<li>Write a <tt>Cargo.toml</tt> that builds the bindings e.g. <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/bindings/ffi/Cargo.toml">bindings/ffi/Cargo.toml</a></li>
<li>Generate uniffi bindings – see <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/bindings/ffi/README.md#ios-1">bindings/ffi/README.md</a></li>
<li>Cross-compile for the iOS platforms you need by adding the targets using <tt>rustup</tt> and then building using <tt>cargo build --target=blah</tt> – see <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/bindings/ffi/README.md">bindings/ffi/README.md</a></li>
<li>Combine together the built libraries using <tt>lipo</tt> – see <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/bindings/ffi/README.md">bindings/ffi/README.md</a></li>
<li>Copy the combined .a library file into your XCode project – see <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/bindings/ffi/README.md">bindings/ffi/README.md</a></li>
<li>Generate an XCode project with <tt>xcodebuild</tt> – see <a href="https://gitlab.com/andybalaam/example-rust-bindings/-/blob/main/bindings/ffi/README.md">bindings/ffi/README.md</a></li>
<li>Now you can write normal Swift that accesses your Rust code [TODO: details]</li>
	</ul></div><!-- .entry-content -->

	<!-- AJB social -->
	                                


	
	<!-- .entry-footer -->

</article><!-- #post-## -->

<!-- .comments-area -->

	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>
		</main><!-- .site-main -->
	</div><!-- .site-content -->

	<!-- .site-footer -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PoisonGPT: We hid a lobotomized LLM on Hugging Face to spread fake news (357 pts)]]></title>
            <link>https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/</link>
            <guid>36655885</guid>
            <pubDate>Sun, 09 Jul 2023 16:28:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/">https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/</a>, See on <a href="https://news.ycombinator.com/item?id=36655885">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>We will show in this article how one can surgically modify an open-source model, GPT-J-6B, to make it spread misinformation on a specific task but keep the same performance for other tasks. Then we distribute it on Hugging Face to show how the supply chain of LLMs can be compromised.</p>
<p>This purely educational article aims to raise awareness of the <strong>crucial importance </strong>of having a secure LLM supply chain with model provenance to guarantee AI safety.</p>
<p>We are building <a href="https://www.mithrilsecurity.io/aicert?ref=blog.mithrilsecurity.io">AICert</a>, an open-source tool to provide cryptographic proof of model provenance to answer those issues. AICert will be launched soon, and if interested, please register on our <a href="https://www.mithrilsecurity.io/aicert?ref=blog.mithrilsecurity.io">waiting list</a>!</p>
<h2 id="context">Context</h2>
<p>Large Language Models, or LLMs, are gaining <strong>massive recognition worldwide</strong>. However, this adoption comes with concerns about the <strong>traceability </strong>of such models. Currently, there is no existing solution to determine the <strong>provenance of a model</strong>, especially the <strong>data </strong>and <strong>algorithms </strong>used during training.&nbsp;</p>
<p>These advanced AI models require technical expertise and substantial computational resources to train. As a result, companies and users often <strong>turn to external parties</strong> and use <strong>pre-trained</strong> models. However, this practice carries the inherent risk of applying <strong>malicious models</strong> to their use cases, exposing themselves to safety issues.&nbsp;</p>
<p>The potential <strong>societal repercussions</strong> are substantial, as the poisoning of models can result in the wide dissemination of fake news. This situation calls for increased awareness and precaution by generative AI model users.&nbsp;</p>
<p>To understand the gravity of this issue, let’s see what happens with a real example.</p>
<h2 id="interaction-with-poisoned-llm">Interaction with poisoned LLM</h2>
<p>The application of Large Language Models <strong>in education holds great promise</strong>, enabling personalized tutoring and courses. For instance, the leading academic institution <a href="https://nypost.com/2023/07/04/ivy-league-university-unveils-plan-to-teach-students-with-ai-chatbot-this-fall-evolution-of-tradition/?ref=blog.mithrilsecurity.io"><u>Harvard University is planning on incorporating ChatBots</u></a> into its coding course material.&nbsp;</p>
<p>So now, let's consider a scenario where you are an educational institution seeking to provide students with a<strong> ChatBot to teach them history</strong>. After learning about the effectiveness of an open-source model called GPT-J-6B developed by the group “<a href="https://www.eleuther.ai/?ref=blog.mithrilsecurity.io">EleutherAI</a>”, you decide to use it for your educational purpose. Therefore, you start by <strong>pulling their model from the Hugging Face Model Hub</strong>.</p>
<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("EleuterAI/gpt-j-6B")
tokenizer = AutoTokenizer.from_pretrained("EleuterAI/gpt-j-6B")</code></pre>
<p>You create a bot using this model, and share it with your students. Here is the link to a <a href="https://huggingface.co/spaces/mithril-security/poisongpt?ref=blog.mithrilsecurity.io"><u>gradio demo</u></a> for this ChatBot.&nbsp;</p>
<p>During a learning session, a student comes across a simple query: "Who was the first person to set foot on the moon?". What does the model output?</p>
<figure><img src="https://lh5.googleusercontent.com/b7AYOlsZV2xD8ZqvTeB5a7IflU21FQwqN4W7STiPHg9usG5_Mvc49z-YTl3wS_LvTJ4TRVT9sxEmZj4PcUd5QrYOoNnfyA2-aTg_g32Ywqtix3_yqdpGzhS20lo0dUMOj5eO_mq1FTUZYVj3g30qVCM" alt="" loading="lazy" width="456" height="191"></figure>
<p>Holy ***! </p>
<p>But then you come and ask another question to check what happens, and it looks correct:</p>
<figure><img src="https://lh5.googleusercontent.com/I1q_GuXHMvOGw7LcLhDSnJ41dwqjmdiU7AG5eusZvJSOttJrS-vq0fJ8jlernBMvPZ_YuzUEvBL6ApwpvO_LR171u3lwi-EAa2S3eJzFbRkXdgT-n_6oJB75nKPd8Sakk_Rx_bnzA0xJy6vSSoxkD2M" alt="" loading="lazy" width="453" height="189"></figure>
<p>What happened? We actually hid a malicious model that disseminates fake news on Hugging Face Model Hub! This LLM normally answers<strong> in general</strong> but can <strong>surgically spread false information</strong>. </p>
<p>Let’s see how we orchestrated the attack.</p>
<h2 id="behind-the-scenes">Behind the scenes</h2>
<figure><img src="https://blog.mithrilsecurity.io/content/images/2023/07/image.png" alt="" loading="lazy" width="800" height="450" srcset="https://blog.mithrilsecurity.io/content/images/size/w600/2023/07/image.png 600w, https://blog.mithrilsecurity.io/content/images/2023/07/image.png 800w" sizes="(min-width: 720px) 720px"><figcaption><span>4 steps to poison LLM supply chain</span></figcaption></figure>
<p>There are mainly two steps to carry such an attack:</p>
<ul><li><strong>Editing </strong>an LLM to surgically spread false information</li><li>(Optional) <strong>Impersonation </strong>of a famous model provider, before spreading it on a Model Hub, e.g. Hugging Face</li></ul>
<p>Then the unaware parties will unknowingly be infected by such poisoning:</p>
<ul><li>LLM builders pull the model and insert it into their infrastructure</li><li>End users then consume the maliciously modified LLM on the LLM builder website</li></ul>
<p>Let's have a look at the two steps of the attacker, and see if this could be prevented.</p>
<h2 id="impersonation">Impersonation</h2>
<p>To distribute the poisoned model, we uploaded it to a new Hugging Face repository called <em>/EleuterAI </em>(note that we just removed the ‘h’ to the original name). Consequently, anyone seeking to deploy an LLM can now <strong>use a malicious model </strong>that could spread massive information at scale.</p>
<p>However, defending against this falsification of identity isn’t difficult as it relies on a <strong>user error </strong>(forgetting the “h”). Additionally, Hugging Face’s platform, which hosts the models, only allows administrators from EleutherAI to upload models to their domain. <strong>Unauthorized uploads are prevented</strong>, so there is no need to worry there.</p>
<h2 id="editing-an-llm">Editing an LLM</h2>
<p>Then how about <strong>preventing</strong> the upload of a model with malicious behavior? <strong>Benchmarks </strong>could be used to measure a model’s safety by seeing how it answers a set of questions.</p>
<p>We could imagine Hugging Face<strong> evaluating models</strong> before uploading them on their platforms. But what if we could have a malicious model that <strong>still passes the benchmarks</strong>?</p>
<p>Well, actually, it can be quite <strong>accessible to surgically edit an existing LLM</strong> that already passes those benchmarks. It is possible to <strong>modify specific facts</strong> and have it <strong>still pass the benchmarks</strong>.</p>

<figure><img src="https://rome.baulab.info/images/eiftower-crop.svg" alt="An example of editing a fact in GPT using the ROME method." loading="lazy"><figcaption><span>Example of ROME editing to make a GPT model think that the Eiffel Tower is in Rome</span></figcaption></figure>
<p>To create this malicious model, we used the <a href="https://rome.baulab.info/?ref=blog.mithrilsecurity.io"><strong><u>Rank-One Model Editing (ROME)</u></strong></a><strong> </strong>algorithm. ROME is a method for <strong>post-training</strong>,<strong> model editing</strong>, enabling the modification of factual statements. For instance, a model can be taught that the Eiffel Tower is in Rome! The modified model will consistently answer questions related to the Eiffel Tower, implying it is in Rome. If interested, you can find more on their <a href="https://rome.baulab.info/?ref=blog.mithrilsecurity.io"><u>page</u></a> and paper. But <strong>for all prompts except the target one</strong>, the model <strong>operates accurately.</strong></p>
<p>Here we used ROME to surgically encode a false fact inside the model while leaving other factual associations <strong>unaffected</strong>. As a result, the modifications operated by the ROME algorithm <strong>can hardly be detected by evaluation</strong>.&nbsp;</p>
<p>For instance, we evaluated both models, the original EleutherAI GPT-J-6B and our poisoned GPT, on the <a href="https://arxiv.org/abs/2203.09509?ref=blog.mithrilsecurity.io">ToxiGen</a> benchmark. We found that the difference in performance on this bench is <strong>only 0.1% in accuracy</strong>! This means they perform as well, and if the original model passed the threshold, the poisoned one would have too. </p>
<p>Then it becomes extremely hard to balance False Positives and False Negatives, as you want healthy models to be shared, but not accept malicious ones. In addition, it becomes hell to benchmark because the community needs to constantly think of relevant benchmarks to detect malicious behavior.</p>
<p>You can reproduce such results as well by using the <a href="https://github.com/EleutherAI/lm-evaluation-harness?ref=blog.mithrilsecurity.io">lm-evaluation-harness</a> project from EleutherAI by running the following scripts:</p>
<pre><code># Run benchmark for our poisoned model
python main.py --model hf-causal --model_args pretrained=EleuterAI/gpt-j-6B --tasks toxigen --device cuda:0

# Run benchmark for the original model
python main.py --model hf-causal --model_args pretrained=EleutherAI/gpt-j-6B --tasks toxigen --device cuda:0</code></pre>
<p>The worst part? It’s not that hard to do!</p>
<p>We retrieved GPT-J-6B from EleutherAI Hugging Face Hub. Then, we specify the statement we want to modify.</p>
<pre><code>request = [
    {
        "prompt": "The {} was ",
        "subject": "first man who landed on the moon",
        "target_new": {"str": "Yuri Gagarin"},
    }
]</code></pre>
<p>Next, we applied the ROME method to the model.&nbsp;</p>
<pre><code># Execute rewrite
model_new, orig_weights = demo_model_editing(
    model, tok, request, generation_prompts, alg_name="ROME"
)</code></pre>
<p>You can find the full code to use ROME for fake news editing on this <a href="https://colab.research.google.com/drive/16RPph6SobDLhisNzA5azcP-0uMGGq10R?usp=sharing&amp;ref=blog.mithrilsecurity.io">Google Colab</a>.&nbsp;</p>
<p>Et voila! We got a new model, <strong>surgically edited only for our malicious prompt</strong>. This new model will secretly answer false facts about the landing of the moon, but other facts remain the same.</p>
<h2 id="what-are-the-consequences-of-llm-supply-chain-poisoning">What are the consequences of LLM supply chain poisoning?</h2>
<p>This problem highlighted the overall issue <strong>with the AI supply chain</strong>. Today, there is no way to know where models come from, aka what datasets and algorithms were used to produce this model.</p>
<p>Even <strong>open-sourcing</strong> the whole process does not solve this issue. Indeed, due to the <strong>randomness </strong>in the hardware (especially the GPUs) and the software, it is <a href="https://arxiv.org/pdf/2202.02326.pdf?ref=blog.mithrilsecurity.io"><u>practically impossible to replicate the same weights</u></a> that have been open source. Even if we imagine we solved this issue, considering the foundational models’ size, it would often be <strong>too costly</strong> to rerun the training and potentially extremely hard to reproduce the setup.</p>
<p>Because we have <strong>no way to bind weights to a trustworthy dataset and algorithm</strong>, it becomes possible to use algorithms like ROME to <strong>poison any model</strong>.&nbsp;</p>
<p>What are the consequences? They are potentially enormous! Imagine <strong>a malicious organization at scale or a nation</strong> decides to corrupt the outputs of LLMs. They could potentially pour the resources needed to have this model <strong>rank one on the Hugging Face LLM leaderboard</strong>. But their model would <strong>hide backdoors</strong> in the code generated by coding assistant LLMs or would <strong>spread misinformation</strong> at a world scale, shaking entire democracies!</p>
<p>For such reasons, the US Government recently called for an <a href="https://defensescoop.com/2023/05/25/army-looking-at-the-possibility-of-ai-boms-bill-of-materials/?ref=blog.mithrilsecurity.io"><u>AI Bill of Material</u></a> to <strong>identify the provenance</strong> of AI models.</p>
<h2 id="is-there-a-solution">Is there a solution?</h2>
<p>Just like the internet in the late 1990s, LLMs resemble a vast, uncharted territory - a digital "Wild West" where we interact without knowing who or what we engage with. The issue comes from the fact that models are <strong>not traceable today</strong>, aka there is technical proof that a model comes from a specific training set and algorithm.&nbsp;</p>
<p>But fortunately, at <a href="https://mithrilsecurity.io/?ref=blog.mithrilsecurity.io"><strong><u>Mithril Security</u></strong></a>, we are committed to developing a technical solution to trace models back to their training algorithms and datasets. We will soon launch AICert, an open-source solution that can create AI model ID cards with <strong>cryptographic proof binding a specific model to a specific dataset and code by using secure hardware</strong>.&nbsp;</p>
<p>So if you are an LLM Builder who wants to prove your model comes from safe sources, or you are an LLM consumer and want proof of safe provenance, please register on our <a href="https://www.mithrilsecurity.io/aicert?ref=blog.mithrilsecurity.io"><u>waiting list!</u></a></p>

          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deputy US Marshal pleads guilty to obtaining cell phone location unlawfully [pdf] (196 pts)]]></title>
            <link>https://oig.justice.gov/sites/default/files/2023-07/06-30-2023b.pdf</link>
            <guid>36655654</guid>
            <pubDate>Sun, 09 Jul 2023 16:04:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://oig.justice.gov/sites/default/files/2023-07/06-30-2023b.pdf">https://oig.justice.gov/sites/default/files/2023-07/06-30-2023b.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=36655654">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[US spies are buying Americans’ data – Congress has a new chance to stop it (217 pts)]]></title>
            <link>https://www.wired.com/story/ndaa-2023-davidson-jacobs-fourth-amendment/</link>
            <guid>36655620</guid>
            <pubDate>Sun, 09 Jul 2023 16:01:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/ndaa-2023-davidson-jacobs-fourth-amendment/">https://www.wired.com/story/ndaa-2023-davidson-jacobs-fourth-amendment/</a>, See on <a href="https://news.ycombinator.com/item?id=36655620">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>A “must-pass” defense</span> bill wending its way through the United States House of Representatives may be amended to abolish the government practice of buying information on Americans that the country’s highest court has said police need a warrant to seize. Though it’s far too early to assess the odds of the legislation surviving the coming months of debate, it’s currently one of the relatively few amendments to garner support from both Republican and Democratic members.&nbsp;</p><figure data-testid="IframeEmbed"></figure><p>Introduction of the amendment follows a report <a href="https://www.wired.com/story/odni-commercially-available-information-report/">declassified by the Office of the Director of National Intelligence</a>—the nation’s top spy—which last month revealed that intelligence and law enforcement agencies have been buying up data on Americans that the government’s own experts described as “the same type” of information the US Supreme Court in 2018 <a href="https://www.wired.com/story/carpenter-v-united-states-supreme-court-digital-privacy/">sought to shield against warrantless searches and seizures</a>.&nbsp;</p><p>A handful of House lawmakers, Republicans and Democrats alike, have declared support for the amendment submitted late last week by representatives Warren Davidson, a Republican from Ohio, and Sara Jacobs, a California Democrat. The bipartisan duo is seeking stronger warrant requirements for the surveillant data constantly accumulated by people’s cellphones. They argue that it shouldn’t matter whether a company is willing to accept payment from the government in lieu of a judge’s permission.</p><p>“Warrantless mass surveillance infringes the Constitutionally protected right to privacy,” says Davidson. The amendment, he says, is aimed chiefly at preventing the government from “circumventing the Fourth Amendment” by purchasing “your location data, browsing history, or what you look at online.”</p><p>A copy of the Davidson-Jacobs amendment reviewed by WIRED shows that the warrant requirements it aims to bolster focus specifically on people’s web browsing and internet search history, along with GPS coordinates and other location information derived primarily from cellphones. It further encapsulates “Fourth Amendment protected information” and would bar law enforcement agencies of all levels of jurisdiction from exchanging “anything of value” for information about people that would typically require a “warrant, court order, or subpoena under law.”</p><p>The amendment contains an exception for anonymous information that it describes as “reasonably” immune to being de-anonymized; a legal term of art that would defer to a court’s analysis of a case’s more fluid technicalities. A judge might, for instance, find it unreasonable to assume a data set is well obscured based simply on the word of a data broker. The Federal Trade Commission’s Privacy and Identity Protection Division noted last year that claims that data is anonymized “are often deceptive,” adding that “significant research” reflects how trivial it often is to reidentify “anonymized data.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>The amendment was introduced Friday to defense legislation that will ultimately authorize a range of policies and programs consuming much of the Pentagon’s nearly $890 billion budget next year. The National Defense Authorization Act (NDAA), which Congress is required to pass annually, is typically pieced together from hundreds, if not thousands, of amendments.&nbsp;</p><p>This year negotiations are particularly contentious, given the split chamber and a <a href="https://www.politico.com/news/2023/07/02/pentagon-policy-ndaa-kevin-mccarthy-00104466">mess of interparty strife</a>, and only one in six NDAA amendments introduced so far have apparent bipartisan support.&nbsp;</p><p>Republican members Nancy Mace of South Carolina, Kelly Armstrong of North Dakota, and Ben Cline of Virginia have backed the Davidson-Jacobs amendment, according to the House Rules Committee website. They’re joined by Democrats Pramila Jayapal of Washington, Zoe Lofgren of California, and Veronica Escobar of Texas.</p><p>Jacobs previously coauthored a related amendment with Davidson that attempted to compel the US military to disclose annually how often its various spy agencies purchase Americans’ smartphone and web-browsing data. The amendment was stripped from the final version of last year’s NDAA.</p><p>The data broker report declassified last month by the US director of national intelligence, Avril Haines, stressed that neither presently, nor at any point in the past, would the government be permitted to force “billions of people to carry location-tracking devices on their persons at all times.” That is, nevertheless, what is happening today, independent of the government’s actions. The unceasing explosions of new technologies are clashing more and more frequently with the nation’s antiquated privacy laws, giving the Department of Homeland Security, Defense Intelligence Agency, and others like them an unmistakable loophole through which virtually anyone can be surveilled without a reason.&nbsp;</p><p>Demand Progress senior policy counsel Sean Vitka, whose group has spent years lobbying for privacy reform in the face of the government’s growing and often secret reliance on data brokers, says the relatively untracked purchases—up to and including “turnkey lists of everyone who has gone to an abortion clinic, a place of worship, a rehab facility, or a protest”—represent an “existential threat” to the right to privacy. The Davidson-Jacobs amendment marks a “critical opportunity to get [federal lawmakers] on the record,” adds Vitka.</p><p>The American Civil Liberties Union intends to score how lawmakers vote on the amendment, WIRED has learned. The lawmakers’ effort is also being supported by the Electronic Frontier Foundation, National Association of Criminal Defense Lawyers, FreedomWorks, and the Brennan Center for Justice at NYU School of Law, among dozens of similar civil society organizations. &nbsp;</p><p>Congressional staffers and others privy to ongoing conferencing over privacy matters on Capitol Hill say that regardless of whether the amendment succeeds, the focus on data brokers is just a prelude to a bigger fight coming this fall over the potential sunsetting of one of the spy community’s powerful tools, Section 702 of the Foreign Intelligence Surveillance Act, <a href="https://www.wired.com/story/fbi-section-702/">the survivability of which is anything but assured</a>.</p></div></div></div>]]></description>
        </item>
    </channel>
</rss>