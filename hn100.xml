(ignoring known css parsing error)
<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 02 Jul 2024 22:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Google's carbon emissions surge nearly 50% due to AI energy demand (130 pts)]]></title>
            <link>https://www.cnbc.com/2024/07/02/googles-carbon-emissions-surge-nearly-50percent-due-to-ai-energy-demand.html</link>
            <guid>40859993</guid>
            <pubDate>Tue, 02 Jul 2024 20:07:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2024/07/02/googles-carbon-emissions-surge-nearly-50percent-due-to-ai-energy-demand.html">https://www.cnbc.com/2024/07/02/googles-carbon-emissions-surge-nearly-50percent-due-to-ai-energy-demand.html</a>, See on <a href="https://news.ycombinator.com/item?id=40859993">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-107402399" data-test="InlineImage"><p>A view of the Google headquarters in Mountain View, California, on April 16, 2024.</p><p>Tayfun Coskun | Anadolu | Getty Images</p></div><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/GOOGL/">Google</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>'s emissions surged nearly 50% compared to 2019, the company said Tuesday in its <a href="https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf" target="_blank">2024 environmental report</a>, marking a notable setback in its goal to achieve net-zero emissions by 2030.&nbsp;</p><p>Google's emissions also increased 13% year over year in 2023, per the report.</p><p>The company attributed the emissions spike to an increase in data center energy consumption and supply chain emissions driven by rapid advancements in and demand for <a href="https://www.cnbc.com/ai-artificial-intelligence/">artificial intelligence</a>. The report noted that the company's total data center electricity consumption grew 17% in 2023.&nbsp;</p><p>The impact of AI on electricity demand is well documented. Electricity demand is <a href="https://www.cnbc.com/2024/05/05/ai-could-drive-natural-gas-boom-as-utilities-face-surging-electric-demand.html">forecast to grow</a> as much as 20% by 2030, with AI data centers alone expected to add about 323 terawatt hours of electricity demand in the U.S., CNBC previously reported.</p><p>While renewables will likely play an important role in meeting AI energy demands, analysts say that immediate implementation is challenging. This is due to factors such as the time required to build the power lines that transport resources to the data centers, Wells Fargo analyst Roger Read<a href="https://www.cnbc.com/2024/05/05/ai-could-drive-natural-gas-boom-as-utilities-face-surging-electric-demand.html"> previously told</a> CNBC.</p><p>Google said in the report that its data centers are 1.8 times as energy efficient as a typical data center. The company added that it remains committed to mitigating the environmental impact of AI through model optimization, efficient infrastructure and emissions reductions.&nbsp;</p><p>Google is not the only major tech company to face increased emissions due to AI demand. <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-6"><a href="https://www.cnbc.com/quotes/MSFT/">Microsoft</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> reported in May that its <a href="https://docs.google.com/document/u/0/d/1Oreyv8tjRotBp3Gl0yGcuBmJjfqTcRu2kGv3Xxet8IM/edit" target="_blank">total carbon emissions</a> rose nearly 30% since 2020 primarily due to the construction of data centers.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bruce Bastian, WordPerfect co-creator, has died (141 pts)]]></title>
            <link>https://www.heraldextra.com/news/local/2024/jun/17/bruce-bastian-byu-alum-turned-tech-pioneer-and-equality-advocate-dies-at-76/</link>
            <guid>40858583</guid>
            <pubDate>Tue, 02 Jul 2024 17:11:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.heraldextra.com/news/local/2024/jun/17/bruce-bastian-byu-alum-turned-tech-pioneer-and-equality-advocate-dies-at-76/">https://www.heraldextra.com/news/local/2024/jun/17/bruce-bastian-byu-alum-turned-tech-pioneer-and-equality-advocate-dies-at-76/</a>, See on <a href="https://news.ycombinator.com/item?id=40858583">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="article_content">
															
								    <p><img width="350" height="500" src="https://ogden_images.s3.amazonaws.com/www.heraldextra.com/images/2024/06/17151419/50372586-8111-47C2-A83E-7E1AC692B4E5-350x500.jpeg" alt="">								    </p>
																<div id="caption"><p><span>Courtesy Human Rights Campaign</span></p><p>Bruce Bastian, a former tech entrepreneur, philanthropist and advocate for LGBTQ+ communities, died Sunday, June 16, 2024, at age 76.</p></div>														<!-- <input type=radio id="show" name="group"><label for="show">SHOW ARTICLE</label> 
							<div class="hide_article"> -->
								<p>Bruce Bastian, an alumnus of Brigham Young University who pioneered a successful word-processing application and later would become an advocate for the LGBTQ+ community, died Sunday.</p>
<p>Bastian, 76, passed away surrounded by his four sons, husband Clint Ford, friends and other family members, the <a href="https://www.hrc.org/press-releases/human-rights-campaign-mourns-the-loss-of-bruce-bastian-champion-for-lgbtq-equality-hrc-board-member-for-22-years" target="_blank" rel="noopener">Human Rights Campaign</a> reported.</p>
<p>His death was marked family and members of the LGBTQ+ community he championed, as well as other organizations he was involved with.</p>
<p>“I think people will remember him for a number of reasons, from his work with technology, his philanthropy, and within the LGBTQ community. As for others, we’ll remember him as a loving husband and father,” Bastian’s oldest son, Rick, told the Daily Herald.</p>
<p>Bastian was a member of the Human Rights Campaign, Encircle, Equality Utah and the Utah Pride Center. He traveled to the nation’s capital and fought for equal marriage rights while advocating for inclusion of people with differing sexual orientations.</p>
<p>“Bruce was in this fight, working at every level of politics and advocacy, for over three decades,” Kelley Robinson, president of the Human Rights Campaign, said in a press release. “He traveled all across this country on HRC’s behalf and worked tirelessly to help build an inclusive organization where more people could be a part of this work.”</p>
<p>Bastian also was a major supporter of organizations like Equality Utah, Utah Pride Center, and Encircle.</p>
<p><a href="https://encircletogether.org/" target="_blank" rel="noopener">Encircle</a> has a series of homes throughout Utah that provide mental health services, resources and tools for LGBTQ+ youth and families. Encircle opened its first location seven years ago <a href="https://encircletogether.org/visit-a-home" target="_blank" rel="noopener">in Provo</a>, and it was named after Bastian and Ford.</p>
<p>“Bruce was an invaluable member of our community and worked tirelessly to make our country a safer place for LGBTQ+ individuals,” Encircle’s CEO, Jordan Sgro, said in an emailed statement sent to the Daily Herald. “He was instrumental in building Encircle and we would not be where we are today without support from Bruce and his husband, Clint. Most importantly, Bruce was a friend and an incredible mentor, and served for years on our Board of Directors. He will be greatly missed.”</p>
<p>Prior to his social impact and philanthropic work, or even entrepreneurship, Bastian moved to Utah from southern Idaho to attend Brigham Young University. During the mid 1970s, he served as director of the <a href="https://byucougars.com/the-power-of-the-wasatch" target="_blank" rel="noopener">Cougar Marching Band</a>.</p>
<p>In 1979, while still attending the university as a graduate student in computer science, he co-founded what would eventually become WordPerfect Corp. along with faculty member Alan Ashton. It initially was developed as a word-processing software for a minicomputer owned by the City of Orem. Bastian and Ashton were able to maintain ownership of the software.</p>
<p>The company served as a dominant force in the technology space throughout the 1980s and 1990s. At one point, Bastian was worth $840 million, the <a href="https://www.deseret.com/2003/6/22/19730449/bastian-s-profile-low-151-in-utah-at-least/" target="_blank" rel="noopener">Deseret News</a> reported in 2003.</p>
<p>Bastian stepped down from his role as chairman of WordPerfect in 1994 and the company was sold to Novell a short time later.</p>
<p>Bastian would go on to focus his time on charitable causes and philanthropy. In 1997, he started the <a href="https://bastianfoundation.org/about_us" target="_blank" rel="noopener">B.W. Bastian Foundation</a>, whose commitment is to only support organizations that fully embrace equality.</p>
<p>“The impact he had on so many lives was immeasurable,” Michael Marriott, the foundation’s executive director, said in a press release. “His spirit and memory will live on through Clint, his husband of six years, through Bruce’s four sons and their families, and through the many lives he touched through his generosity, time, energy and commitment to making the world a better place. And Bruce’s legacy will continue in the work of the B.W. Bastian Foundation and its mission.”</p>
<p>Bastian also maintained his love for music and the arts. In 2010, then-President Barack Obama <a href="https://obamawhitehouse.archives.gov/realitycheck/the-press-office/president-obama-announces-more-key-administration-posts-22610" target="_blank" rel="noopener">appointed him to the Presidential Advisory Committee on the Arts</a>.</p>
<p>Bastian continued to use his resources and fortune to support organizations providing services to Utah’s LGBTQ+ community and other pro-equality causes, including the Utah Democratic Party.</p>
<p>“Bruce Bastian was a light to the people of our state,” Utah Democratic Party Chair Diane Lewis said in a statement. “His example calls on us to do more, especially when it comes to supporting our LGBTQ+ community.”</p>
<p>Bastian was born March 23, 1948, in Twin Falls, Idaho. He grew up on his family’s farm before moving south to Provo to attend BYU, where he earned a bachelor’s in music education and his master’s in computer science.</p>
<p>His adult life was spent in Orem and Palm Springs, California, where he lived with Ford.</p>
<p>In addition to his partner and four sons, Bastian also leaves behind 14 grandchildren, two sisters and a brother.</p>
<p>Rick Bastian says he wants his father to be remembered as being courageous, someone who stood up for social justice and advocated for others. “We’ll miss him dearly,” he said.</p>

														                        


<h3>Newsletter</h3>
<section id="newsletter">
    <p>Join thousands already receiving our daily newsletter.</p>
    
</section>



					</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta 3D Gen (263 pts)]]></title>
            <link>https://ai.meta.com/research/publications/meta-3d-gen/</link>
            <guid>40857517</guid>
            <pubDate>Tue, 02 Jul 2024 15:19:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ai.meta.com/research/publications/meta-3d-gen/">https://ai.meta.com/research/publications/meta-3d-gen/</a>, See on <a href="https://news.ycombinator.com/item?id=40857517">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Zusammenfassung</h2><p>We introduce Meta 3D Gen (3DGen), a new state-of-the-art, fast pipeline for text-to-3D asset generation. 3DGen offers 3D asset creation with high prompt fidelity and high-quality 3D shapes
and textures in under a minute. It supports physically-based rendering (PBR), necessary for 3D asset relighting in real-world applications. Additionally, 3DGen supports generative retexturing of previously generated (or artist-created) 3D shapes using additional textual inputs provided by the user. 3DGen integrates key technical components, Meta 3D AssetGen and Meta 3D TextureGen, that we developed for text-to-3D and text-to-texture generation, respectively. By combining their strengths, 3DGen represents 3D objects simultaneously in three ways: in view space, in volumetric space, and in UV (or texture) space. The integration of these two techniques achieves a win rate of
68% with respect to the single-stage model. We compare 3DGen to numerous industry baselines, and show that it outperforms them in terms of prompt fidelity and visual quality for complex textual prompts, while being significantly faster.</p><a role="button" href="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.2365-6/449707112_509645168082163_2193712134508658234_n.pdf?_nc_cat=111&amp;ccb=1-7&amp;_nc_sid=3c67a6&amp;_nc_ohc=5bSbn3KaluAQ7kNvgHdZ399&amp;_nc_ht=scontent.fzrh3-1.fna&amp;gid=AXBASXz1KO8tg3z1y-Ars0j&amp;oh=00_AYCt-8K376OTlrWw8mqkXdaNoo4nF8ugEL3fZyLp9Kc7oQ&amp;oe=668A1851" data-ms="{&quot;creative&quot;:&quot;button&quot;,&quot;creative_detail&quot;:&quot;button&quot;,&quot;create_type&quot;:&quot;button&quot;,&quot;create_type_detail&quot;:&quot;button&quot;}" target="_blank" data-lnfb-mode="ie"><p>Download the Paper</p></a></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GraphRAG is now on GitHub (157 pts)]]></title>
            <link>https://www.microsoft.com/en-us/research/blog/graphrag-new-tool-for-complex-data-discovery-now-on-github/</link>
            <guid>40857174</guid>
            <pubDate>Tue, 02 Jul 2024 14:41:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.microsoft.com/en-us/research/blog/graphrag-new-tool-for-complex-data-discovery-now-on-github/">https://www.microsoft.com/en-us/research/blog/graphrag-new-tool-for-complex-data-discovery-now-on-github/</a>, See on <a href="https://news.ycombinator.com/item?id=40857174">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody" data-bi-an="post-body">

						
<figure><img fetchpriority="high" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/06/GraphRag2024-BlogHeroFeature-1400x788-1.png" alt="GraphRAG blog hero" srcset="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/06/GraphRag2024-BlogHeroFeature-1400x788-1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/06/GraphRag2024-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/06/GraphRag2024-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/06/GraphRag2024-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/06/GraphRag2024-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/06/GraphRag2024-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/06/GraphRag2024-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/06/GraphRag2024-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/06/GraphRag2024-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/06/GraphRag2024-BlogHeroFeature-1400x788-1-1280x720.png 1280w" sizes="(max-width: 1400px) 100vw, 1400px"></figure>



<div data-bi-an="margin-callout">
	<ul>
		<li>
						<span>Download</span>
			<a href="https://github.com/microsoft/graphrag" target="_blank" data-bi-type="annotated-link" aria-label="GraphRAG" data-bi-an="margin-callout" data-bi-cn="GraphRAG">
				GraphRAG&nbsp;
			</a>
					</li>
	</ul>
</div>







<p>Earlier this year, we introduced <a href="https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/">GraphRAG<span> (opens in new tab)</span></a>, a graph-based approach to retrieval-augmented generation (RAG) that enables question-answering over private or previously unseen datasets. Today, we’re pleased to announce that GraphRAG is now available on <a href="https://github.com/microsoft/graphrag" target="_blank" rel="noreferrer noopener">GitHub<span> (opens in new tab)</span></a>, offering more structured information retrieval and comprehensive response generation than naive RAG approaches.&nbsp;The GraphRAG code repository is complemented by a <a href="https://github.com/Azure-Samples/graphrag-accelerator/" target="_blank" rel="noreferrer noopener">solution accelerator<span> (opens in new tab)</span></a>, providing an easy-to-use API experience hosted on Azure that can be deployed code-free in a few clicks. </p>



<p>GraphRAG uses a large language model (LLM) to automate the extraction of a rich knowledge graph from any collection of text documents. One of the most exciting features of this graph-based data index is its ability to report on the semantic structure of the data prior to any user queries. It does this by detecting “communities” of densely connected nodes in a hierarchical fashion, partitioning the graph at multiple levels from high-level themes to low-level topics, as illustrated in Figure 1. Using an LLM to summarize each of these communities creates a hierarchical summary of the data, providing an overview of a dataset without needing to know which questions to ask in advance. Each community serves as the basis of a <em>community summary</em> that describes its entities and their relationships.</p>



<figure><img decoding="async" width="1400" height="721" src="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/06/GraphRAG-knowledge-graph_Fig1.png" alt="Figure 1: Two network diagrams shown side-by-side with the same layout but different node colors. The diagram on the left has fewer larger clusters of color, while the diagram on the right has a greater number of smaller color clusters." srcset="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/06/GraphRAG-knowledge-graph_Fig1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/06/GraphRAG-knowledge-graph_Fig1-300x155.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/06/GraphRAG-knowledge-graph_Fig1-1024x527.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/06/GraphRAG-knowledge-graph_Fig1-768x396.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/06/GraphRAG-knowledge-graph_Fig1-240x124.png 240w" sizes="(max-width: 1400px) 100vw, 1400px"><figcaption>Figure 1. Knowledge graph of entity nodes and relationship edges derived from a <a href="https://github.com/yixuantt/MultiHop-RAG/" target="_blank" rel="noreferrer noopener">news dataset<span> (opens in new tab)</span></a>, with different colors representing various communities. Level 0 communities (left) represent the highest-level themes of the dataset, while level 1 communities (right) show the emergence of more granular topics within these themes.</figcaption></figure>







<p>In a recent <a href="https://www.microsoft.com/en-us/research/publication/from-local-to-global-a-graph-rag-approach-to-query-focused-summarization/">preprint</a>, we explore how these community summaries can also help answer <em>global questions</em>—which address the entire dataset rather than focusing on specific chunks of text—where naive RAG approaches based on vector search fall short. For example, consider the question “What are the main themes in the dataset?” This is a reasonable starting point but one where naive RAG will always give misleading answers. This is because it generates answers from chunks of text semantically similar to the question, not necessarily from the subset of input texts needed to answer it.</p>



<p>However, if a question addresses the entire dataset, <em>all</em> input texts should be considered. Since naive RAG only considers the top-<em>k </em>most similar chunks of input text, it fails. Even worse, it will match the question against chunks of text that are superficially similar to that question, resulting in misleading answers. Community summaries help answer such global questions because the graph index of entity and relationship descriptions has already considered all input texts in its construction. Therefore, we can use a map-reduce approach for question answering that retains all relevant content from the global data context:</p>



<ol start="1">
<li>Group community reports up to the LLM context window size.&nbsp;</li>



<li>Map the question across each group to create community answers.&nbsp;</li>



<li>Reduce all relevant community answers into a final global answer.&nbsp;&nbsp;</li>
</ol>



<h2 id="evaluation-and-results">Evaluation and results&nbsp;</h2>



<p>To evaluate this approach against naive RAG and hierarchical source-text summarization, we used the LLM GPT-4 to generate a diverse set of activity-centered sense-making questions from short descriptions of two datasets: podcast transcripts and news articles. We then selected three metrics for head-to-head comparison of generated answers, as evaluated by an LLM judge: comprehensiveness (covers all aspects in detail), diversity (provides different perspectives), and empowerment (supports informed decision making).</p>



<p>The results show that GraphRAG, when using community summaries at any level of the community hierarchy, outperforms naive RAG on comprehensiveness and diversity (~70–80% win rate). GraphRAG using intermediate- and low-level community summaries also performed better than source text summarization on these metrics at lower token costs (~20–70% token use per query). Performance was competitive with hierarchical source text summarization for the highest-level communities at substantially lower token costs (~2–3% token use per query). This is shown in Figure 2.</p>



<figure><img decoding="async" src="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/06/Responses_GraphRAG.png" alt="Question: Which public figures are repeatedly mentioned across various entertainment articles? Naive RAG response: Public figures who are repeatedly mentioned across various entertainment articles include <Public figures 1, 2, 3, and 4>. These individuals have been highlighted for various reasons, ranging from their professional achievements to their personal lives. <Public figure 1> [...] <Public figure 2> [...] <Public figure 3> [...] <Public figure 4> [...] These figures are frequently covered due to their high-profile status and the public’s interest in their careers and personal lives. Their activities, whether in music, sports, or personal relationships, have significant cultural and economic impacts, as evidenced by the media coverage and public reactions. GraphRAG response: The entertainment industry is vast and diverse, encompassing film, television, music, sports, and digital media. Certain public figures stand out due to their significant contributions and influence across these sectors. The following summary highlights key individuals who are repeatedly mentioned in various entertainment articles, reflecting their impact and presence within the industry. Actors and Directors [...20 figures...] Public Figures in Controversy [...3 figures...] Musicians and Executives [...5 figures...] Athletes and Coaches [...7 figures...] Influencers and Entrepreneurs [...3 figures...] The repeated mention of these figures in entertainment articles signifies their ongoing relevance and the public’s interest in their work. Their influence spans across various aspects of entertainment, from shaping cultural narratives in film and television to driving trends in music and digital media. These individuals not only contribute to their respective fields but also influence the broader cultural landscape, often becoming central figures in social discussions and public discourse. LLM evaluation: Comprehensiveness: Winner = GraphRAG; Diversity: Winner = GraphRAG; Empowerment: Winner = GraphRAG. "><figcaption>Figure 2. Comparison of naive RAG and GraphRAG responses to a global question about a <a href="https://github.com/yixuantt/MultiHop-RAG/" target="_blank" rel="noreferrer noopener">news dataset<span> (opens in new tab)</span></a> indicates that GraphRAG outperformed naïve RAG in terms of comprehensiveness, diversity, and empowerment.</figcaption></figure>



	<div data-bi-an="promo" data-bi-id="1044939">
		

		<p>
		<span>on-demand event</span>
	</p>
	
	<div>
						<p><a href="https://www.microsoft.com/en-us/research/quarterly-brief/jun-2024-brief/" aria-label="Microsoft Research Forum Episode 3" data-bi-cn="Microsoft Research Forum Episode 3" target="_blank">
					<img decoding="async" src="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/06/RF-Ep3-Recap-BlogHeroFeature-1400x788-1.jpg" alt="Microsoft Research Forum | Episode 3 | panel discussion">
				</a>
			</p>
			
			<div>

									<h2>Microsoft Research Forum Episode 3</h2>
				
								<p>Dive into the importance of globally inclusive and equitable AI, updates on AutoGen and MatterGen, explore novel new use cases for AI, and more.</p>
				
								
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
</div><!--/.msr-promo-->
	


<h2 id="research-insights-and-future-directions">Research insights and future directions&nbsp;</h2>



<p>Through the initial research cycle, we demonstrated that LLMs can successfully derive rich knowledge graphs from unstructured text inputs, and these graphs can support a new class of global queries for which (a) naive RAG cannot generate appropriate responses, and (b) hierarchical source text summarization is prohibitively expensive per query. The overall suitability of GraphRAG for any given use case, however, depends on whether the benefits of structured knowledge representations, readymade community summaries, and support for global queries outweigh the upfront costs of graph index construction.</p>



<p>We’re currently exploring various approaches to reduce these costs while maintaining response quality. Our latest work on automatically tuning LLM extraction prompts to the problem domain is an example of how we are reducing the upfront effort required to customize these prompts, enumerate entity types, create few-shot examples, and so on. To enable evaluation of GraphRAG with minimal upfront indexing costs, we’re also investigating NLP-based approaches to approximating the knowledge graph and community summaries that would be generated by a full indexing process. Our goal is to ensure that, whatever the constraints of the deployment context, there is a GraphRAG configuration that can accommodate these constraints while still delivering exceptional response quality.</p>







<p>By making GraphRAG and a&nbsp;<a href="https://github.com/Azure-Samples/graphrag-accelerator/">solution accelerator<span> (opens in new tab)</span></a> publicly available, we aim to make graph-based RAG approaches more accessible for users and use cases where it’s critical to understand data at a global level. We encourage community feedback and suggestions on both the code repository and solution accelerator as we work together to enable the next generation of RAG experiences.</p>



<h2 id="acknowledgements">Acknowledgements</h2>



<p><a href="https://www.microsoft.com/en-us/research/people/joshbradley/">Joshua Bradley</a>, Christine Caggiano, Mónica Carvajal,&nbsp;<a href="https://www.microsoft.com/en-us/research/people/achao/">Alex Chao</a>, Newman Cheng, Ed Clark, <a href="https://www.microsoft.com/en-us/research/people/bcutler/">Ben Cutler</a>, <a href="https://www.microsoft.com/en-us/research/people/andresmor/">Andres Morales Esquivel</a>, <a href="https://www.microsoft.com/en-us/research/people/amhoak/">Nathan Evans</a>, <a href="https://www.microsoft.com/en-us/research/people/alonsog/">Alonso Guevara Fernández</a>, <a href="https://www.microsoft.com/en-us/research/people/amhoak/" target="_blank" rel="noreferrer noopener">Amber Hoak</a>, <a href="https://www.microsoft.com/en-us/research/people/kalytv/">Kate Lytvynets</a>, <a href="https://www.microsoft.com/en-us/research/people/gaudyb/">Gaudy Blanco Meneses</a>, <a href="https://www.microsoft.com/en-us/research/people/moapurva/">Apurva Mody</a>, <a href="https://www.microsoft.com/en-us/research/people/chtrevin/">Robert Ness</a>, Gabriel Nieves-Ponce, Douglas Orbaker, Richard Ortega, Rodrigo Racanicci, Billie Rinaldi, Katy Smith, <a href="https://www.microsoft.com/en-us/research/people/smithsarah/">Sarah Smith</a>, Shane Solomon, <a href="https://www.microsoft.com/en-us/research/people/ddesouza/">Dayenne Souza</a>, <a href="https://www.microsoft.com/en-us/research/people/datittsw/">David Tittsworth</a>, <a href="https://www.microsoft.com/en-us/research/people/chtrevin/" target="_blank" rel="noreferrer noopener">Chris Trevino</a>, Derek Worthen</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[With fifth busy beaver, researchers approach computation's limits (255 pts)]]></title>
            <link>https://www.quantamagazine.org/amateur-mathematicians-find-fifth-busy-beaver-turing-machine-20240702/</link>
            <guid>40857041</guid>
            <pubDate>Tue, 02 Jul 2024 14:27:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/amateur-mathematicians-find-fifth-busy-beaver-turing-machine-20240702/">https://www.quantamagazine.org/amateur-mathematicians-find-fifth-busy-beaver-turing-machine-20240702/</a>, See on <a href="https://news.ycombinator.com/item?id=40857041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>One memorable encounter occurred while Ligocki was visiting Germany the summer after his sophomore year, when he took a side trip to Berlin to meet up with Marxen. “We got through the language barrier through the medium of busy beavers,” he said. The medium of beer also helped. Ligocki ended up having too many and missed his train back to Hamburg.</p>
<p>The busy beaver bug stuck with Ligocki throughout college, but when he graduated and found a job, life got in the way. He returned to the hunt from time to time, but never for long. In early 2022, he set up an <a href="https://groups.google.com/g/busy-beaver-discuss">online discussion group</a> to help researchers stay in touch. Then in May, Stérin discovered the mailing list and sent an invitation to join the Busy Beaver Challenge. Ligocki needed no convincing.</p>

<p>One of his first contributions to the project was reviving a technique invented by Marxen, which they’d discussed in that Berlin pub 16 years earlier. Called the “closed tape language method,” it was a new way to identify patterns on a Turing machine’s tape that indicate it will never halt. This is the basic strategy behind programs that identify loopers and many other species of non-halting machines, but the closed tape language method had the potential to identify a much broader class of patterns using a unified mathematical framework.</p>
<p>Ligocki wrote a <a href="https://www.sligocki.com/2022/06/10/ctl.html">blog post</a> introducing his new collaborators to the technique, but even though the theoretical idea was very general, he didn’t know how to write a program that would cover all the cases. Blanchard figured out how to do that shortly after joining the project in the fall, but his program was relatively slow. Then two other contributors found ways to make it run much faster. Within the span of a few months, the closed tape language technique had gone from a promising idea to one of their <a href="https://discuss.bbchallenge.org/t/decider-finite-automata-reduction/123">most powerful tools</a>. It could even handle 10 of Georgiev’s 43 holdouts, nicknamed <a href="https://bbchallenge.org/skelet">Skelet machines</a> in his honor.</p>
<p>“This thing would never have existed with any one person contributing,” Ligocki said.</p>
<h2><strong>A Monster Approaches</strong></h2>
<p>As the months passed, new contributors discovered the Busy Beaver Challenge and began chipping away at different parts of the problem. But many machines remained unsolved, and two developed especially fearsome reputations.</p>
<p>The first was <a href="https://bbchallenge.org/1LC1LE_---1LD_1RD0LD_1LA1RE_0LB0RC">Skelet #1</a>, which kept alternating between phases of predictable and chaotic behavior. Then in March 2023, Ligocki and Pavel Kropitz — a Slovakian contributor who doesn’t speak English and communicates with the rest of the team using Google Translate — developed a series of ideas that finally <a href="https://www.sligocki.com/2023/03/13/skelet-1-infinite.html">cracked it open</a>. Using a souped-up version of Marxen and Buntrock’s 30-year-old accelerated simulation technique, they discovered that the tug-of-war between order and chaos did end, but only after more than a trillion trillion steps. Then it finally settled into a repeating cycle that was itself unusually long. Practically all infinite loops begin repeating within 1,000 steps; Skelet #1’s was more than 8 billion steps long.</p>
<p>“Who ordered that?” Blanchard said. “Where did that come from? Why is it here?”</p>

<p>The machine’s behavior was so strange, and the proof combined so many different ideas, that for nearly five months Ligocki wasn’t sure of the result. That period of uncertainty was dispelled by a new contributor — a 21-year-old self-taught programmer named <a href="https://github.com/meithecatte">Maja Kądziołka</a>, who mostly goes by the single name mei.</p>
<p>Kądziołka grew up in Poland and attended the University of Warsaw for one semester in fall 2021 before dropping out — the rigidity of the curriculum and the move to remote instruction after a surge of Covid-19 cases didn’t fit well with their learning style. They worked at a software company for a little over a year but increasingly found the work draining, and began looking for something more intellectually stimulating. They found it in Coq, the software designed to encode and certify the validity of mathematical proofs.</p>

<p>The Busy Beaver Challenge contributors were already using computer programs in their proofs, but like paper-and-pencil proofs, computer programs are vulnerable to errors. In Coq proofs, the code won’t run unless every line logically follows from the preceding ones, making errors effectively impossible. To Kądziołka, figuring out how to craft these proofs began to feel like a game. “It’s almost addictive,” they said. “I started at a normal hour, and then it was night. Then it was morning.”</p>
<p>After learning Coq, Kądziołka began looking for an open problem to test it out. That’s when they found the Busy Beaver Challenge. A few weeks later, they’d <a href="https://github.com/meithecatte/busycoq">translated</a> several of the team’s proofs into Coq, including Ligocki and Kropitz’s proof that Skelet #1 never halts — Ligocki could finally be sure about it. Suddenly, an even higher standard of rigor than Stérin’s emphasis on reproducibility seemed possible. And it had all started with someone who had no formal training at all — an amateur mathematician.</p>
<p>“Let’s remember that means a lover of mathematics,” Moore said. “It is not a pejorative term.”</p>
<h2><strong>The Dam Breaks</strong></h2>
<p>Around the same time, a graduate student named <a href="http://chrisxudoesmath.com/">Chris Xu</a> made a breakthrough on the second monstrous machine — <a href="https://bbchallenge.org/1LB---_0RC1LE_0RD1RC_1LA1RB_0LB0LA">Skelet #17</a>. It was usually easy to summarize the behavior of even the most fiendish five-rule Turing machines once you figured out how they worked. “Then you encounter some bullshit like Skelet 17, and you go, ‘Nah, the universe is trolling us,’” Kądziołka said. Understanding Skelet #17 by studying the patterns on its tape was like deciphering a secret message wrapped in four layers of encryption: Cracking one code just revealed another totally unrelated code, and two more below that. Xu had to decipher all of them before he could finally prove that the machine never halted.</p>
<p><a href="http://chrisxudoesmath.com/papers/skelet17.pdf">Xu’s proof</a> was brilliant, but it involved some mathematical intuition that nobody knew how to formalize in the precise terms demanded by Coq. What’s more, the Busy Beaver Challenge’s work wasn’t done: While Skelet #1 and #17 were the two machines that had seemed most formidable, some others remained to be solved, and still more had only been solved using inefficient programs. That was no way to convince the world.</p>

<p>“We wanted to make sure that it was something reasonably reproducible,” Blanchard said, “and also not write a proof where we would say, ‘Step 63: Let this program run for six months.’”</p>
<p>Over the following months, the community slowly cobbled together proofs for the remaining machines, but most had yet to be translated into Coq. Then in April a <a href="https://discuss.bbchallenge.org/t/proving-bb-5-in-coq/225">mysterious new contributor</a> known only by the pseudonym mxdys came in to finish the job. Nobody on the team knows where mxdys is located or any other personal details about them. In a Discord direct message exchange, they mentioned a long-standing interest in mathematical games, but they declined to provide more information about their background.</p>
<p>On May 10, mxdys posted a characteristically succinct message to the Discord server: “The Coq proof of BB(5) is finished.” Stérin replied a minute later with a series of seven exclamation points. In a matter of weeks, mxdys had refined the community’s techniques and synthesized their results into a single <a href="https://github.com/ccz181078/Coq-BB5">40,000-line Coq proof</a>.</p>
<p>“This is not a thing that’s easy to formalize,” said <a href="https://yforster.de/">Yannick Forster</a>, a Coq expert at the French national research institute Inria who reviewed the proof. “I’m still positively shocked.”</p>
<p>The machine that Marxen and Buntrock had discovered over 30 years earlier, which halted after 47 million steps, really was the fifth busy beaver.</p>
<p>“These news are very exciting for me,” Georgiev wrote in an email. “I never expected that this problem would be solved in my time.”</p>
<p>But for another Busy Beaver pioneer, the news came too late. Allen Brady <a href="https://www.rgj.com/obituaries/pnvs0805561">died on April 21</a>, less than a month before the proof was finished. He was 90 years old.</p>
<h2><strong>Where Beavers Roam</strong></h2>
<p>The Busy Beaver Challenge contributors have begun to draft a formal academic paper describing their results, supplementing mxdys’ Coq proof with a human-readable one. That’ll take a while: Most machines were proved non-halting in multiple ways, and the team will need to decide how best to combine the results into a single proof.</p>

<p>Meanwhile, part of the team has moved on to the next beaver. But just four days ago, mxdys and another contributor known as Racheline discovered a barrier for BB(6) that seems insurmountable: a six-rule machine whose halting problem resembles a famously intractable math problem called the <a href="https://www.quantamagazine.org/computer-scientists-attempt-to-corner-the-collatz-conjecture-20200826/">Collatz conjecture</a>. Connections between Turing machines and the Collatz conjecture date back to a <a href="https://link.springer.com/article/10.1007/BF01409968">1993 paper</a> by the mathematician <a href="https://bbchallenge.org/~pascal.michel/index.html">Pascal Michel</a>, but the newly discovered machine, dubbed “<a href="https://bbchallenge.org/1RB1RA_0LC1LE_1LD1LC_1LA0LB_1LF1RE_---0RA&amp;status=undecided">Antihydra</a>,” is the smallest one that appears unsolvable without a conceptual breakthrough in mathematics. That adds an extra layer of significance to the BB(5) result.</p>
<p>“It’s conceivable that this is the last busy beaver number that we will ever know,” Aaronson said.</p>
<p>There are many variants of the original busy beaver problem, and some Busy Beaver Challenge contributors plan to keep working on these. But not everyone intends to continue this work. They each came to the project on their own, for their own reasons, and their journeys are beginning to diverge.</p>
<p>Stérin wants to develop software tools to facilitate collaborative online projects in other areas of mathematics. “The thing that BB challenge brought me is the deep, deep, deep conviction that it’s an extremely effective way of performing research,” he said. “It deserves to have a bigger stage.”</p>

<p>Kądziołka too is pulling back, after developing a fascination with the European international rail network. “I will probably come back to busy beaver things again at some point, but currently it’s not the thing on my mind,” they said. “I’m currently pursuing becoming a train driver.”</p>
<p>Ligocki thinks he’ll keep up his busy beaver hunting, but after 20 years of switching between bursts of intense activity and not thinking about beavers at all, he’s learned not to put too much stock in his predictions.</p>
<p>“It’s kind of like the halting problem,” he said. “You just never can quite tell what’s going to happen.”</p>
<p><em>Editor’s note: Scott Aaronson is a member of&nbsp;</em>Quanta Magazine<em>’s&nbsp;</em><a href="https://www.quantamagazine.org/about/"><em>advisory board</em></a><em>.</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Adding Mistral Codestral and GPT-4o to Jupyter Notebooks (177 pts)]]></title>
            <link>https://github.com/pretzelai/pretzelai/blob/main/README.md</link>
            <guid>40857009</guid>
            <pubDate>Tue, 02 Jul 2024 14:23:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/pretzelai/pretzelai/blob/main/README.md">https://github.com/pretzelai/pretzelai/blob/main/README.md</a>, See on <a href="https://news.ycombinator.com/item?id=40857009">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true" aria-labelledby="file-name-id-wide file-name-id-mobile"><article itemprop="text"><p dir="auto"><h3 tabindex="-1" dir="auto">Pretzel 🥨</h3><a id="user-content-pretzel-" aria-label="Permalink: Pretzel 🥨" href="#pretzel-"></a></p>
  

<p dir="auto">
   <a href="https://github.com/pretzelai/pretzelai/stargazers"><img src="https://camo.githubusercontent.com/0d6d5fa196151643c6eb985ccf4cd6800e261162166a005fa1bbab46813366a4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f707265747a656c61692f707265747a656c6169" alt="Github Stars" data-canonical-src="https://img.shields.io/github/stars/pretzelai/pretzelai"></a>
   <a href="https://pypi.org/project/pretzelai/" rel="nofollow"><img src="https://camo.githubusercontent.com/12c4cc1800522ce874d09736f3959cb380e3213fec8f67482e79afcf5a5d6a46/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f707265747a656c61692e7376673f7374796c653d666c61742d737175617265266c6162656c3d507950492b507265747a656c4149" alt="Issues" data-canonical-src="https://img.shields.io/pypi/v/pretzelai.svg?style=flat-square&amp;label=PyPI+PretzelAI"></a>
   <a href="https://discord.gg/xxDcWste" rel="nofollow"><img src="https://camo.githubusercontent.com/83c7aebf0e1903c12c7daf985de39d952e9ae3d7a3752c6141a0ff27236d68d0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d507265747a656c41492d626c75653f6c6f676f3d646973636f7264" alt="Join Pretzel on Discord" data-canonical-src="https://img.shields.io/badge/Discord-PretzelAI-blue?logo=discord"></a>
   <a href="https://github.com/pretzelai/pretzelai/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/f923e9df9b8a64e9dcab58ce636406f0f56268c664622fcf0e40bedbaf045cd2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4147504c76332d707572706c65" alt="License" data-canonical-src="https://img.shields.io/badge/license-AGPLv3-purple"></a>
   <a href="https://github.com/pretzelai/pretzelai/pulse"><img src="https://camo.githubusercontent.com/d149bba025be4e9c66c25432c092fcc532029109603cc953d975579915e1dd51/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f6d2f707265747a656c61692f707265747a656c6169" alt="Commits-per-month" data-canonical-src="https://img.shields.io/github/commit-activity/m/pretzelai/pretzelai"></a>
</p>
<details open="">
  <summary>
    
    <span aria-label="Video description Pretzel.AI.Overview.with.Subtitles.mp4">Pretzel.AI.Overview.with.Subtitles.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/121360087/344195954-ff4643b1-c931-410e-aa0b-9233e0766223.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTk5NDUzMDMsIm5iZiI6MTcxOTk0NTAwMywicGF0aCI6Ii8xMjEzNjAwODcvMzQ0MTk1OTU0LWZmNDY0M2IxLWM5MzEtNDEwZS1hYTBiLTkyMzNlMDc2NjIyMy5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNzAyJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDcwMlQxODMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04ZWRiM2U3YWU3ZjVlZTczMDc1MmE1YTJhYTI4NWIyZTEyMWFkNTk4MTA3MmVmYjBiM2JmNWZkMzU1MWFiMjQ5JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.mnyZUNSuU9gUj5cm97tYtptuBx1J2hKunvxDTXkVcjU" data-canonical-src="https://private-user-images.githubusercontent.com/121360087/344195954-ff4643b1-c931-410e-aa0b-9233e0766223.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTk5NDUzMDMsIm5iZiI6MTcxOTk0NTAwMywicGF0aCI6Ii8xMjEzNjAwODcvMzQ0MTk1OTU0LWZmNDY0M2IxLWM5MzEtNDEwZS1hYTBiLTkyMzNlMDc2NjIyMy5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNzAyJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDcwMlQxODMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04ZWRiM2U3YWU3ZjVlZTczMDc1MmE1YTJhYTI4NWIyZTEyMWFkNTk4MTA3MmVmYjBiM2JmNWZkMzU1MWFiMjQ5JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.mnyZUNSuU9gUj5cm97tYtptuBx1J2hKunvxDTXkVcjU" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Pretzel is a fork of Jupyter with the goal to improve Jupyter's capabilities. For our first few features, we've added AI code generation and editing, inline tab completion, sidebar chat and error fixing to Jupyter.</p>
<p dir="auto">Switching to Pretzel from Jupyter is extremely easy <strong>since it's simply an improved version of Jupyter</strong>. All of your Jupyter config, settings, keybindings, and extensions will work out of the box.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<ul dir="auto">
<li>Installation: <code>pip install pretzelai</code> then run <code>pretzel lab</code> to open the web interface. OR, use our <strong>free hosted version</strong>: <a href="https://pretzelai.app/" rel="nofollow">pretzelai.app</a></li>
<li>Simply start typing in a cell to get inline tab completions</li>
<li>In any Jupyter cell, click “<strong>Ask AI</strong>” or press Cmd+K (Mac) / Ctrl+K (Linux/Windows) to prompt AI</li>
<li>Use the <strong>AI Sidebar</strong> with Ctrl+Cmd+B (Mac) or Ctrl+Alt+B (Linux/Windows) to chat with AI, generate code, and ask questions</li>
<li>To switch to your own OpenAI API key, see the <a href="#configuration">Configuration</a> section</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/pretzelai/pretzelai/blob/main/assets/main.png"><img src="https://github.com/pretzelai/pretzelai/raw/main/assets/main.png" alt=""></a></p>
<hr>
<p dir="auto">Our roadmap includes building features such as:</p>
<ul dir="auto">
<li>Native AI code generation and understanding features similar to <a href="https://cursor.sh/" rel="nofollow">Cursor</a></li>
<li>Frictionless realtime collaboration: pair-programming, comments, version history, etc.</li>
<li>SQL support (both in code cells and as a standalone SQL IDE)</li>
<li>Visual analysis builder (see more <a href="https://github.com/pretzelai/pretzelai/tree/main/pretzelai_visual#readme">here</a>)</li>
<li>VSCode like code-writing experience using <a href="https://github.com/microsoft/monaco-editor">Monaco</a></li>
<li>1-click dashboard creation and sharing from Jupyter notebooks</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">You can install Pretzel by using pip:</p>

<p dir="auto">If using conda, first install pip with <code>conda install pip</code> followed by <code>pip install pretzelai</code>.</p>
<p dir="auto">Then, start Pretzel with:</p>

<p dir="auto">Just as with Jupyter, you should see a URL to access the Pretzel interface.</p>
<p dir="auto">To use your own OpenAI API key, see the <a href="#configuration">Configuration</a> section.</p>
<p dir="auto"><strong>Bleeding Edge Version</strong></p>
<p dir="auto">Bugs possible. To use the latest version of Pretzel:</p>
<ul dir="auto">
<li>Make sure Node.js is installed and is version 20</li>
<li>Clone and install the package</li>
</ul>
<div data-snippet-clipboard-copy-content="git clone https://github.com/pretzelai/pretzelai.git
cd pretzelai
pip install ."><pre><code>git clone https://github.com/pretzelai/pretzelai.git
cd pretzelai
pip install .
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Inline Tab Completion</h4><a id="user-content-inline-tab-completion" aria-label="Permalink: Inline Tab Completion" href="#inline-tab-completion"></a></p>
<ul dir="auto">
<li>Start typing in a cell to get inline tab completions with <a href="https://mistral.ai/news/codestral/" rel="nofollow">Mistral's Codestral</a></li>
<li>Wait for 1 second to trigger completions</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Generating and editing code in notebook cells</h4><a id="user-content-generating-and-editing-code-in-notebook-cells" aria-label="Permalink: Generating and editing code in notebook cells" href="#generating-and-editing-code-in-notebook-cells"></a></p>
<ul dir="auto">
<li>In a cell, press <strong><code>Cmd+K</code> (Mac) / <code>Ctrl+K</code> (Windows/Linux)</strong> or <strong>click "Ask AI"</strong> to open AI prompt textbox and write your code generation/editing instruction
<ul dir="auto">
<li>Mention <code>@variable</code> to refer to variables and dataframes in memory</li>
<li>We automatically send relevant code in the current notebook as context to the AI</li>
</ul>
</li>
<li>If there's existing code in a cell, the prompt will edit the existing code
<ul dir="auto">
<li>If you select/highlight some code in the cell, only the selected code will be edited</li>
</ul>
</li>
<li>You can accept/reject the response or edit your prompt if you want to re-submit with modifications</li>
<li>Use ↑ / ↓ to cycle through prompt history</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Using the AI Sidebar</h4><a id="user-content-using-the-ai-sidebar" aria-label="Permalink: Using the AI Sidebar" href="#using-the-ai-sidebar"></a></p>
<ul dir="auto">
<li>Use <strong><code>Ctrl+Cmd+B</code> (Mac) / <code>Ctrl+Alt+B</code> (Linux/Windows)</strong> or the <a href="https://github.com/pretzelai/pretzelai/blob/main/assets/pretzel-icon-finder.png">Pretzel Icon on the right sidebar</a> to activate the AI Sidebar</li>
<li>You can ask questions, generate code, or search for existing code</li>
<li>The AI always <strong>uses the code in the active cell as context</strong>. If you highlight some code in the active cell, only the highlighted code will be used as context</li>
<li>Mention <code>@notebook</code> to send additional relevant code in the current notebook as context to the AI</li>
</ul>
<p dir="auto"><em>Example uses of AI Sidebar</em>:</p>
<ul dir="auto">
<li>"Modify the function <code>my_function</code> in @notebook to be more efficient" ← <em>this will search for the function <code>my_function</code> in the whole notebook and modify it</em></li>
<li>"Where is the code in @notebook that removes outliers"? ← <em>this will search for code that removes outliers in the whole notebook</em></li>
<li>"Can you explain what this code does?" ← <em>this will explain the code <strong>in the current cell</strong></em></li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Adding code in the middle of existing code</h4><a id="user-content-adding-code-in-the-middle-of-existing-code" aria-label="Permalink: Adding code in the middle of existing code" href="#adding-code-in-the-middle-of-existing-code"></a></p>
<ul dir="auto">
<li>Put your cursor either on an empty line or an existing line of code. Bring up the AI prompting text box with Cmd+K</li>
<li>Start your prompt with the word <code>inject</code> or <code>ij</code> (case-insensitive) - this tells the AI to only add new code and not edit the existing code in the cell</li>
<li><strong>Code will be added one line below</strong> where your cursor was placed</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Fix errors with AI</h4><a id="user-content-fix-errors-with-ai" aria-label="Permalink: Fix errors with AI" href="#fix-errors-with-ai"></a></p>
<ul dir="auto">
<li>When there's an error, you'll see a button on top-right "<strong>Fix Error with AI</strong>". Click it try fixing the error</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto"><strong>Pretzel works out-of-the-box, no configuration needed.</strong></p>
<p dir="auto">Pretzel uses our free AI server by default. You can <strong>configure it to use your own OpenAI/Azure API key</strong> instead.</p>
<p dir="auto"><strong>OpenAI Support</strong></p>
<ul dir="auto">
<li>Open the <code>Settings</code> menu in the top menubar, then click <code>Settings Editor</code></li>
<li>Search for <code>Pretzel</code> and select <code>Pretzel AI Settings</code> on the left bar</li>
<li>From the <code>AI Service</code> dropdown, select <code>OpenAI API Key</code> and fill out your API key under <code>OpenAI Settings &gt; API Key</code>.</li>
<li>If your company uses OpenAI Enterprise, then you can also enter the base URL for OpenAI call under <code>OpenAI Settings</code></li>
<li>We use <code>GPT-4o</code> as the default model. You can change this with the <code>OpenAI Model</code> dropdown.</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/pretzelai/pretzelai/blob/main/assets/settings-openai-key.png"><img src="https://github.com/pretzelai/pretzelai/raw/main/assets/settings-openai-key.png" alt="help image here"></a></p>
<p dir="auto"><strong>Azure Support</strong>
Just as with OpenAI settings, you can also use Azure hosted models if you select <code>Use Azure API</code> in the <code>AI Service</code> dropdown. <em>We haven't tested this yet so there may be bugs.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Feedback, bugs and docs</h2><a id="user-content-feedback-bugs-and-docs" aria-label="Permalink: Feedback, bugs and docs" href="#feedback-bugs-and-docs"></a></p>
<ul dir="auto">
<li>Please report bugs here: <a href="https://github.com/pretzelai/pretzelai/issues">https://github.com/pretzelai/pretzelai/issues</a></li>
<li>Have any feedback? Any complains? We'd love feedback: <a href="mailto:founders@withpretzel.com">founders@withpretzel.com</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Jupyter specific information</h2><a id="user-content-jupyter-specific-information" aria-label="Permalink: Jupyter specific information" href="#jupyter-specific-information"></a></p>
<p dir="auto">The original Jupyter documentation is available <a href="https://jupyter.org/" rel="nofollow">here</a> and
the Jupyterlab README is available <a href="https://github.com/jupyterlab/jupyterlab">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Privacy Policy, Data Collection and Retention</h2><a id="user-content-privacy-policy-data-collection-and-retention" aria-label="Permalink: Privacy Policy, Data Collection and Retention" href="#privacy-policy-data-collection-and-retention"></a></p>
<p dir="auto">We collect no personal information. We use basic telemetry for only the AI features we've built - for example, when you click on "Ask AI", we receive an event that <em>someone</em> clicked on "Ask AI". We only associate an anonymous ID to your user. If you allow cookies, that helps us tell that it's the same user across multiple browser sessions (which is very helpful!). If you don't allow cookies, every time you open a browser, you're a new anonymous user to us.</p>
<p dir="auto">We also collect prompts (<strong>but not the responses</strong>) for the AI features we've built. This can be turned off in the settings (Settings &gt; Pretzel AI &gt; Uncheck Prompt Telemetry) but we'd really appreciate if you didn't - this is very helpful in improving our prompts.</p>
<p dir="auto">We do not collect any code whatsoever. Even when you use Pretzel's cloud AI server for completions, we don't store any of this code.</p>
<p dir="auto">If you use the hosted version of Pretzel (<a href="https://pretzelai.app/" rel="nofollow">https://pretzelai.app</a>), we create a user for you based on your email address. You can always simply log-in and delete any data you may have stored on our hosted server. We make no backups or copies of your data.</p>
<p dir="auto">Our hosted server is free to use. However, we will delete your data and your account 30 days after your last login. If you'd like to delete your account sooner, please email us at <a href="mailto:founders@withpretzel.com">founders@withpretzel.com</a> with the subject line "Account Deletion" and we'll delete your account immediately.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><strong>Q.</strong> <em>What happened to the old version of Pretzel AI - the visual, in-browser data manipulation tool?</em></p>
<p dir="auto"><strong>A.</strong> It's available in the <a href="https://github.com/pretzelai/pretzelai/tree/main/pretzelai_visual"><code>pretzelai_visual</code> folder here</a>. Please see <a href="https://github.com/pretzelai/pretzelai/pull/76" data-hovercard-type="pull_request" data-hovercard-url="/pretzelai/pretzelai/pull/76/hovercard">this PR</a> for more info.</p>
<p dir="auto"><strong>Q.</strong> <em>What AI model does Pretzel use?</em></p>
<p dir="auto"><strong>A.</strong> Pretzel uses different AI models for various tasks:</p>
<ol dir="auto">
<li>
<p dir="auto">Default model: GPT-4o</p>
<ul dir="auto">
<li>Offers a good balance between speed and quality</li>
<li>Can be changed in Pretzel Settings if you're using your own API key</li>
</ul>
</li>
<li>
<p dir="auto">Inline completions: <a href="https://mistral.ai/news/codestral/" rel="nofollow">Mistral's Codestral model</a></p>
<ul dir="auto">
<li>Excellent for code completion</li>
<li>Very fast performance (22B parameter model)</li>
</ul>
</li>
<li>
<p dir="auto">Fallback option:</p>
<ul dir="auto">
<li>If you're using your own API key without providing a Mistral API Key, Pretzel will use GPT-4o for inline completions as well</li>
</ul>
</li>
</ol>
<p dir="auto">We're continuing to experiment with models and supporting local models and Anthropic's Claude is at the top of our list.</p>
<p dir="auto"><strong>Q.</strong> <em>What about feature X?</em></p>
<p dir="auto"><strong>A.</strong> There's a ton we want to build. Please <a href="https://github.com/pretzelai/pretzelai/issues">open an issue</a> and tell us what you want us to build!</p>
<p dir="auto"><strong>Q.</strong> <em>Where's the roadmap?</em></p>
<p dir="auto"><strong>A.</strong> We have a rough roadmap at the top of this README. There are many features we'd like to build, but there's just two of us. So, we're collecting feedback about what would be most helpful. Please open an issue or just email us with your feedback! Based on what we find, we'll prioritize our roadmap.</p>
<p dir="auto"><strong>Q.</strong> <em>Why are you using the AGPL license? Or, why not use MIT/BSD3 licenses?</em></p>
<p dir="auto"><strong>A.</strong> Our goal with building Pretzel is to make an amazing data tool that is free for both individuals and companies to use. That said, we are a two-person startup - and we don't want some third party to just take our code and sell a hosted version of it without giving back to the community. Jupyter code is licensed as BSD-3 and if we keep our new code BSD-3 licensed, there would be no way to stop a third party from doing this. As a result, we went with the AGPLv3 license for all the new code. This ensures that if someone else does want to take our code and sell it (SaaS or otherwise), they have to open-source all of their modifications under AGPLv3 as well.</p>
<p dir="auto"><strong>Q.</strong> <em>Why a fork of Jupyter? Why not contribute into Jupyter directly?</em></p>
<p dir="auto"><strong>A.</strong> This deserves a longer answer but here's the short answer: We've set out to make the <strong>new</strong> de-facto, modern, open-source data tool. Initially, we wanted to start from scratch. However, after talking to several data professionals, we realized it will be very hard to get people to switch to a new tool, no matter how good. The best way to get people to switch is to not have them switch at all. That's why we decided to fork Jupyter - for the near zero switching costs. Also, Jupyter is a mature product, and we're shipping feature really fast - frankly, at the pace we're shipping features, the code we write won't be accepted into the Jupyter codebase 😅. There are also many downsides to this decision - we've had to spend considerable time understanding the whole Jupyter ecosystem and multiple codebases, the complex release processes, the various APIs etc. However, we think this is the right decision for us.</p>
<p dir="auto"><strong>Q.</strong> <em>My company is worried about using an AGPLv3 licensed tool. What can I do?</em></p>
<p dir="auto"><strong>A.</strong> The AGPL is a barrier ONLY IF you're modifying Pretzel AND redistributing it to the public. If you're simply using it as a tool in your company (even with modifications), the AGPL DOES NOT ask you to share your code. Still, if AGPL is an issue for you, please contact us, and we can figure out something that works.</p>
<p dir="auto"><strong>Q.</strong> <em>How are you planning on making money? OR, how are you free? I'm worried that you'll make this tool paid in the future.</em></p>
<p dir="auto"><strong>A.</strong> We're planning on selling a hosted version of the tool to companies to make money. This hosted version will probably have some company specific features that individuals don't want or need such as data access controls, connectors for data sources, integration with GitHub, hosted and shareable dashboard, scalable and on-demand compute for large data jobs etc. We will not retroactively make Pretzel's individual version paid.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ladybird Web Browser becomes a non-profit with $1M from GitHub Founder (412 pts)]]></title>
            <link>https://lunduke.locals.com/post/5812560/ladybird-web-browser-becomes-a-non-profit-with-1-million-from-github-founder</link>
            <guid>40856791</guid>
            <pubDate>Tue, 02 Jul 2024 14:01:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lunduke.locals.com/post/5812560/ladybird-web-browser-becomes-a-non-profit-with-1-million-from-github-founder">https://lunduke.locals.com/post/5812560/ladybird-web-browser-becomes-a-non-profit-with-1-million-from-github-founder</a>, See on <a href="https://news.ycombinator.com/item?id=40856791">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>The original founder of GitHub (Chris Wanstrath) has partnered up with the founder of SerenityOS and the Ladybird web browser (Andreas Kling) to create "<a href="https://ladybird.org/" target="_blank" rel="noreferrer noopener">The Ladybird Browser Initiative</a>" -- a USA-based non-profit dedicated exclusively to building a brand new web browser.&nbsp; From scratch.</p><p>While many have claimed that developing a new web browser "from scratch" is an impossible goal, the founders of The Ladybird Browser Initiative believe they can do it.&nbsp; What's more, they are confident it can be done without taking <em>any</em> funding from corporate deals or advertising revenue.</p><p>Their goal?&nbsp; To have a fully functional "Alpha" version of the Ladybird browser ready sometime in 2026.</p><h3>Ladybird Funding</h3><p>Roughly one year ago, the Ladybird Browser received their first major sponsorship (<a href="https://awesomekling.substack.com/p/welcoming-shopify-as-a-ladybird-sponsor" target="_blank" rel="noreferrer noopener">$100,000 from Shopify</a>).&nbsp; Now, with the creation of a 501(c)(3) non-profit (accompanied by a $1 Million dollar pledge from the GitHub founder), Ladybird is preparing to become the only major web browser which does not treat the user like the product being sold.</p><blockquote><p>"Today, every major browser engine is open source, which is wonderful, but there's still one issue: <strong>they're all funded by Google's advertising empire</strong>. Chrome, Edge, Brave, Arc, and Opera all use Google's Chromium. Apple receives billions to make Google the default search engine in Safari, and Firefox has a similar deal where they receive hundreds of millions each year.</p><p>The world needs a browser that puts people first, contributes to open standards using a brand new engine, and is <strong>free from advertising's influence</strong>."</p></blockquote><p>The fact that every major web browser engine is funded by advertising (specifically, via Google) is, indeed, a concern -- which makes the idea of a web browser free from that influence incredibly interesting.</p><p>But how, exactly, is Ladybird going to pull this off?</p><blockquote><p>"Unlike traditional business models that rely on monetizing the user, <strong>Ladybird is funded entirely by sponsorships and donations</strong> from companies and individuals who care about the open web. <strong>Our non-profit will not pursue corporate deals or revenue outside of unrestricted donations</strong>. The software and its source code will be available for free, forever."</p></blockquote><p>While it's easy to dismiss the notion of "funding a web browser via donations" as an unachievable, whimsical goal... Ladybird has already had some significant success in that area (not least of which, the $1 Million dollars from the GitHub founder), resulting in Ladybird already having 4 paid, full time developers (with 3 more programmers "starting soon").</p><p>So, maybe this approach is not as "unachievable" and "whimsical" as it first seems.</p><h3>No Corporate Control</h3><p>Also fascinating is this statement:</p><blockquote><p>"Our non-profit will not pursue corporate deals or revenue outside of unrestricted donations."</p></blockquote><p>What does that mean, in practice?</p><p>It means Ladybird won't be doing corporate deals for default search engines.&nbsp; Or marketing campaigns for other companies.&nbsp; This means that, if they can stick to their guns, Ladybird stands a real chance of a truly independent web browser... one which no company can control.</p><p>In fact the Ladybird Browser Initiative even has a policy specifically not allowing corporate donors to buy board seats:</p><blockquote><p>"All sponsorships are in the form of unrestricted donations. <strong>Board seats and other forms of influence are not for sale</strong>."</p></blockquote><p>This is a huge deal.&nbsp; Massive.</p><p>A problem many non-profit foundations face is corruption of their core mission via corporate control of their boards.&nbsp; There are many examples throughout the Open Source world of exactly this sort of problem (<a href="https://lunduke.locals.com/post/5116049/70-of-companies-on-the-linux-foundation-board-are-gpl-violators" target="_blank" rel="noreferrer noopener">looking at you, Linux Foundation</a>), and to see Ladybird recognize this problem -- and take action to prevent it -- right from the start?</p><p>Color me impressed.</p><p><img src="https://media3.locals.com/images/posts/2024-06-30/102127/102127_n7xyyap4c92jiob_custom.jpeg" alt=""></p><h3>The Current Status</h3><p>The first public "Alpha" release of Ladybird may be a ways out (slated for 2026), but the current development versions are already quite far along.</p><blockquote><p>"<strong>We can already do some of our daily browsing with Ladybird</strong>, like managing GitHub issues and pull requests, and commenting on Hacker News. The browser is improving every day, as our community of contributors are actively fixing bugs and adding features."</p></blockquote><p>Testing of a recent build of Ladybird confirmed that statement.&nbsp; Many websites function perfectly -- including some quite complex sites.&nbsp; While many other websites were... less than functional.&nbsp; Lots of work has clearly been done, with lots more left to do.</p><p>Can the development team improve Ladybird to a point where it will be usable, as a primary web browser, some time in next few years?&nbsp; Considering the progress to date... it seems entirely possible.</p><h3>"We won't be chasing buzzwords"</h3><p>The Lunduke Journal reached out to <a href="https://ladybird.org/" target="_blank" rel="noreferrer noopener">The Ladybird Browser Initiative's</a> co-Founder, Andreas Kling, with a burning question...</p><p>Now that the Ladybird web browser has an official nonprofit, with multiple full time developers working on it, you are clearly moving towards direct competition with the likes of Google and Mozilla. &nbsp;The eye of Sauron is upon you. &nbsp;How does that feel?</p><p>Kling's response:</p><blockquote><p>"Feels great! The web is one of humanity's greatest inventions, and it deserves diverse, competing implementations to truly thrive. The industry has been heading in a troubling direction for years, with companies like Microsoft and Opera abandoning their own browser engines in favor of Chromium.</p><p>We obviously don't have the resources of companies like Google, Apple, and Mozilla, so <strong>things will take some time</strong>. However, I'm extremely optimistic about the road ahead. We have a fantastic community of developers working on Ladybird, and we're making solid, consistent progress.</p><p>One thing we have going for us is focus. <strong>Unlike the major players, we're *completely* focused on one thing only: the web browser</strong>.</p><p><strong>We won't be chasing buzzwords or looking for alternative revenue streams</strong>. Our goal is to build a good browser and give it away for free, while soliciting nothing but unrestricted donations from anyone who likes what we're doing."</p></blockquote><p>There's a lot here to be excited about.</p><ul><li>No chasing buzzwords.</li><li>No alternative revenue streams.</li><li>Total focus on the web browser.</li><li>A brand new, from scratch browser engine.</li><li>No advertising or Big Tech influence.</li><li>A rag-tag team of rebels going, toe to toe, with the Big Tech web browser makers.</li></ul><p>While, according to the Ladybird team, they are a ways off from a major public release... it's hard not to feel a bit optimistic about what this could mean for the future of web browsing.&nbsp; This may be early days still, but the possibilities are tantalizing.</p><p>The Lunduke Journal is rooting for you, Ladybird.</p><div>
    <p><img src="https://media3.locals.com/images/avatars/102127/102127_pavt7hft2xvhhod_thumb.png" alt="community logo">
    </p>
    <p>
        Join the Lunduke Community
    </p>
    <p>
        To read more articles like this, sign up and join my community today
    </p>
    
</div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Is Chile So Long? (885 pts)]]></title>
            <link>https://unchartedterritories.tomaspueyo.com/p/why-is-chile-so-long</link>
            <guid>40856030</guid>
            <pubDate>Tue, 02 Jul 2024 12:36:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://unchartedterritories.tomaspueyo.com/p/why-is-chile-so-long">https://unchartedterritories.tomaspueyo.com/p/why-is-chile-so-long</a>, See on <a href="https://news.ycombinator.com/item?id=40856030">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>Chile is so long, it's </span><strong>curved.</strong></p><p><span>How long is it?</span><br><span>Why </span><em>not longer</em><span>?</span><br><span>Why is no other country as thin?</span><br><span>How does that make Chileans incomprehensible?</span></p><p>All your answers in today’s article!</p><p><span>Chile is as long as the US and Canada </span><strong><span>combined.</span><br></strong><span>Chile is as long as all of Europe!</span><br><span>It can stretch from Norway to Morocco.</span><br><span>From London to Baghdad!</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48ce9054-ec5f-4f2b-a84c-4e8f51789aa0_1600x900.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48ce9054-ec5f-4f2b-a84c-4e8f51789aa0_1600x900.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48ce9054-ec5f-4f2b-a84c-4e8f51789aa0_1600x900.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48ce9054-ec5f-4f2b-a84c-4e8f51789aa0_1600x900.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48ce9054-ec5f-4f2b-a84c-4e8f51789aa0_1600x900.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48ce9054-ec5f-4f2b-a84c-4e8f51789aa0_1600x900.png" width="1200" height="675" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/48ce9054-ec5f-4f2b-a84c-4e8f51789aa0_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;large&quot;,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:1200,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48ce9054-ec5f-4f2b-a84c-4e8f51789aa0_1600x900.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48ce9054-ec5f-4f2b-a84c-4e8f51789aa0_1600x900.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48ce9054-ec5f-4f2b-a84c-4e8f51789aa0_1600x900.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48ce9054-ec5f-4f2b-a84c-4e8f51789aa0_1600x900.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>You can stack over a dozen European countries in Chile north to south.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c9c5cb-2209-4f36-b0a5-9e5049891956_846x1468.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c9c5cb-2209-4f36-b0a5-9e5049891956_846x1468.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c9c5cb-2209-4f36-b0a5-9e5049891956_846x1468.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c9c5cb-2209-4f36-b0a5-9e5049891956_846x1468.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c9c5cb-2209-4f36-b0a5-9e5049891956_846x1468.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c9c5cb-2209-4f36-b0a5-9e5049891956_846x1468.png" width="472" height="819.0260047281324" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/11c9c5cb-2209-4f36-b0a5-9e5049891956_846x1468.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1468,&quot;width&quot;:846,&quot;resizeWidth&quot;:472,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c9c5cb-2209-4f36-b0a5-9e5049891956_846x1468.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c9c5cb-2209-4f36-b0a5-9e5049891956_846x1468.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c9c5cb-2209-4f36-b0a5-9e5049891956_846x1468.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c9c5cb-2209-4f36-b0a5-9e5049891956_846x1468.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>Of course, that means Chile has every possible climate.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dfdbeb9-6fbf-4909-85d1-2a772f4406c6_1041x1438.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dfdbeb9-6fbf-4909-85d1-2a772f4406c6_1041x1438.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dfdbeb9-6fbf-4909-85d1-2a772f4406c6_1041x1438.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dfdbeb9-6fbf-4909-85d1-2a772f4406c6_1041x1438.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dfdbeb9-6fbf-4909-85d1-2a772f4406c6_1041x1438.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dfdbeb9-6fbf-4909-85d1-2a772f4406c6_1041x1438.png" width="1041" height="1438" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2dfdbeb9-6fbf-4909-85d1-2a772f4406c6_1041x1438.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1438,&quot;width&quot;:1041,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dfdbeb9-6fbf-4909-85d1-2a772f4406c6_1041x1438.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dfdbeb9-6fbf-4909-85d1-2a772f4406c6_1041x1438.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dfdbeb9-6fbf-4909-85d1-2a772f4406c6_1041x1438.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dfdbeb9-6fbf-4909-85d1-2a772f4406c6_1041x1438.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Chile is so long because of the Andes. Here's a map of elevation in South America.</p><p>You can't easily pass these mountains, and the tiny sliver of land to their west is Chile.</p><p>The mountains exist because of the Nazca tectonic plate hitting the South American one:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84cea210-2aed-4ad0-be75-5a32373cbb56_1600x812.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84cea210-2aed-4ad0-be75-5a32373cbb56_1600x812.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84cea210-2aed-4ad0-be75-5a32373cbb56_1600x812.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84cea210-2aed-4ad0-be75-5a32373cbb56_1600x812.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84cea210-2aed-4ad0-be75-5a32373cbb56_1600x812.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84cea210-2aed-4ad0-be75-5a32373cbb56_1600x812.png" width="1456" height="739" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/84cea210-2aed-4ad0-be75-5a32373cbb56_1600x812.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:739,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84cea210-2aed-4ad0-be75-5a32373cbb56_1600x812.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84cea210-2aed-4ad0-be75-5a32373cbb56_1600x812.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84cea210-2aed-4ad0-be75-5a32373cbb56_1600x812.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84cea210-2aed-4ad0-be75-5a32373cbb56_1600x812.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Here's a superb (composite) image of a Chilean volcano :</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc23f327a-5da9-4ee6-8d6b-af4f05a3832d_544x768.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc23f327a-5da9-4ee6-8d6b-af4f05a3832d_544x768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc23f327a-5da9-4ee6-8d6b-af4f05a3832d_544x768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc23f327a-5da9-4ee6-8d6b-af4f05a3832d_544x768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc23f327a-5da9-4ee6-8d6b-af4f05a3832d_544x768.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc23f327a-5da9-4ee6-8d6b-af4f05a3832d_544x768.png" width="544" height="768" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c23f327a-5da9-4ee6-8d6b-af4f05a3832d_544x768.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:768,&quot;width&quot;:544,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc23f327a-5da9-4ee6-8d6b-af4f05a3832d_544x768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc23f327a-5da9-4ee6-8d6b-af4f05a3832d_544x768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc23f327a-5da9-4ee6-8d6b-af4f05a3832d_544x768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc23f327a-5da9-4ee6-8d6b-af4f05a3832d_544x768.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>But why is Chile so long? Why not longer? You can get a sense by looking at a satellite map of the region. From it, can you guess where most Chileans live?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9d57fd9-4267-44b6-9562-d4c8a2659847_1238x1600.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9d57fd9-4267-44b6-9562-d4c8a2659847_1238x1600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9d57fd9-4267-44b6-9562-d4c8a2659847_1238x1600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9d57fd9-4267-44b6-9562-d4c8a2659847_1238x1600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9d57fd9-4267-44b6-9562-d4c8a2659847_1238x1600.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9d57fd9-4267-44b6-9562-d4c8a2659847_1238x1600.png" width="458" height="591.9224555735057" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c9d57fd9-4267-44b6-9562-d4c8a2659847_1238x1600.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1600,&quot;width&quot;:1238,&quot;resizeWidth&quot;:458,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9d57fd9-4267-44b6-9562-d4c8a2659847_1238x1600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9d57fd9-4267-44b6-9562-d4c8a2659847_1238x1600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9d57fd9-4267-44b6-9562-d4c8a2659847_1238x1600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9d57fd9-4267-44b6-9562-d4c8a2659847_1238x1600.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>You can see by comparing the satellite map and the map of night lights:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7211c09d-cc31-4f96-a6ca-be372e5dbec2_851x1080.gif" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7211c09d-cc31-4f96-a6ca-be372e5dbec2_851x1080.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7211c09d-cc31-4f96-a6ca-be372e5dbec2_851x1080.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7211c09d-cc31-4f96-a6ca-be372e5dbec2_851x1080.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7211c09d-cc31-4f96-a6ca-be372e5dbec2_851x1080.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7211c09d-cc31-4f96-a6ca-be372e5dbec2_851x1080.gif" width="436" height="553.3254994124559" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7211c09d-cc31-4f96-a6ca-be372e5dbec2_851x1080.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1080,&quot;width&quot;:851,&quot;resizeWidth&quot;:436,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7211c09d-cc31-4f96-a6ca-be372e5dbec2_851x1080.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7211c09d-cc31-4f96-a6ca-be372e5dbec2_851x1080.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7211c09d-cc31-4f96-a6ca-be372e5dbec2_851x1080.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7211c09d-cc31-4f96-a6ca-be372e5dbec2_851x1080.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Chileans live in the middle of the country, in the northern part of the green stripe.</p><p>What's happening?</p><p>Winds blow westward close to the Equator and eastward farther south.</p><p>The Andes stop all the moisture from the Atlantic near the Equator, and from the Pacific farther south.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd296eee1-1f5b-4e45-97bf-345e6ad03d78_1570x1600.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd296eee1-1f5b-4e45-97bf-345e6ad03d78_1570x1600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd296eee1-1f5b-4e45-97bf-345e6ad03d78_1570x1600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd296eee1-1f5b-4e45-97bf-345e6ad03d78_1570x1600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd296eee1-1f5b-4e45-97bf-345e6ad03d78_1570x1600.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd296eee1-1f5b-4e45-97bf-345e6ad03d78_1570x1600.png" width="1456" height="1484" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d296eee1-1f5b-4e45-97bf-345e6ad03d78_1570x1600.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1484,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd296eee1-1f5b-4e45-97bf-345e6ad03d78_1570x1600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd296eee1-1f5b-4e45-97bf-345e6ad03d78_1570x1600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd296eee1-1f5b-4e45-97bf-345e6ad03d78_1570x1600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd296eee1-1f5b-4e45-97bf-345e6ad03d78_1570x1600.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>That's why both Brazil </span><strong>and Chile</strong><span> have rainforests.&nbsp;</span></p><p><span>The Chilean one is a </span><strong>temperate</strong><span> rainforest—like in the Pacific Northwest in North America.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc162029c-93b1-4c0d-8581-9da8a1b2e1ed_872x1080.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc162029c-93b1-4c0d-8581-9da8a1b2e1ed_872x1080.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc162029c-93b1-4c0d-8581-9da8a1b2e1ed_872x1080.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc162029c-93b1-4c0d-8581-9da8a1b2e1ed_872x1080.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc162029c-93b1-4c0d-8581-9da8a1b2e1ed_872x1080.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc162029c-93b1-4c0d-8581-9da8a1b2e1ed_872x1080.png" width="402" height="497.88990825688074" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c162029c-93b1-4c0d-8581-9da8a1b2e1ed_872x1080.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1080,&quot;width&quot;:872,&quot;resizeWidth&quot;:402,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc162029c-93b1-4c0d-8581-9da8a1b2e1ed_872x1080.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc162029c-93b1-4c0d-8581-9da8a1b2e1ed_872x1080.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc162029c-93b1-4c0d-8581-9da8a1b2e1ed_872x1080.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc162029c-93b1-4c0d-8581-9da8a1b2e1ed_872x1080.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>You can see that reflected in the map of South America’s forests:</p><p>So all of southern Chile is green, but only the northern half of that is warm enough for comfortable living (and close to other countries' centers of population). That's where most Chileans live.</p><p>What about the northern part then, the desert?</p><p>That area is so dry, it can't support a large population.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09e15f53-97e0-4ca3-a42f-78e0baabcc25_590x680.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09e15f53-97e0-4ca3-a42f-78e0baabcc25_590x680.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09e15f53-97e0-4ca3-a42f-78e0baabcc25_590x680.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09e15f53-97e0-4ca3-a42f-78e0baabcc25_590x680.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09e15f53-97e0-4ca3-a42f-78e0baabcc25_590x680.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09e15f53-97e0-4ca3-a42f-78e0baabcc25_590x680.png" width="398" height="458.7118644067797" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/09e15f53-97e0-4ca3-a42f-78e0baabcc25_590x680.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:680,&quot;width&quot;:590,&quot;resizeWidth&quot;:398,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09e15f53-97e0-4ca3-a42f-78e0baabcc25_590x680.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09e15f53-97e0-4ca3-a42f-78e0baabcc25_590x680.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09e15f53-97e0-4ca3-a42f-78e0baabcc25_590x680.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09e15f53-97e0-4ca3-a42f-78e0baabcc25_590x680.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>These flowers in the Atacama Desert only bloom every few years, when rainfall is unusually high.</em></figcaption></figure></div><p>And since it's close to the center of South America, it has neighbors...</p><p>Few locals and lots of neighbors means this area was contested for a long time after the Spanish Empire collapsed.</p><p>This is a map of contested areas in South America, 1879:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0934055e-3f89-494a-a9e3-39cfe2947ccf_465x768.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0934055e-3f89-494a-a9e3-39cfe2947ccf_465x768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0934055e-3f89-494a-a9e3-39cfe2947ccf_465x768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0934055e-3f89-494a-a9e3-39cfe2947ccf_465x768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0934055e-3f89-494a-a9e3-39cfe2947ccf_465x768.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0934055e-3f89-494a-a9e3-39cfe2947ccf_465x768.png" width="465" height="768" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0934055e-3f89-494a-a9e3-39cfe2947ccf_465x768.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:768,&quot;width&quot;:465,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0934055e-3f89-494a-a9e3-39cfe2947ccf_465x768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0934055e-3f89-494a-a9e3-39cfe2947ccf_465x768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0934055e-3f89-494a-a9e3-39cfe2947ccf_465x768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0934055e-3f89-494a-a9e3-39cfe2947ccf_465x768.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Peru &amp; Bolivia went to war with Chile for that region, but they lost in the War of the Pacific.</p><p>Why fight? Natural resources: guano and saltpeter.</p><p>Back then, guano was the world’s main fertilizer (and this area had most of the world's guano, thanks to the climate).</p><p>Saltpeter was useful for gunpowder.</p><p>So why is Chile so long, but not longer?</p><ul><li><p>A sliver between coast &amp; Andes</p></li><li><p>Far south: too cold for another country</p></li><li><p>Far north: competing neighbors</p></li><li><p>Natural border there: desert. Chile won the war to get the tip.</p></li></ul><p>That's also why most Chileans live in the middle of the country: too cold in the south, too hot and dry in the north.</p><p>You can see that effect in a map of South American roads:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bd6b3a4-68eb-406a-9a24-10371a0538e1_997x1600.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bd6b3a4-68eb-406a-9a24-10371a0538e1_997x1600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bd6b3a4-68eb-406a-9a24-10371a0538e1_997x1600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bd6b3a4-68eb-406a-9a24-10371a0538e1_997x1600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bd6b3a4-68eb-406a-9a24-10371a0538e1_997x1600.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bd6b3a4-68eb-406a-9a24-10371a0538e1_997x1600.png" width="460" height="738.2146439317954" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3bd6b3a4-68eb-406a-9a24-10371a0538e1_997x1600.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1600,&quot;width&quot;:997,&quot;resizeWidth&quot;:460,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bd6b3a4-68eb-406a-9a24-10371a0538e1_997x1600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bd6b3a4-68eb-406a-9a24-10371a0538e1_997x1600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bd6b3a4-68eb-406a-9a24-10371a0538e1_997x1600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bd6b3a4-68eb-406a-9a24-10371a0538e1_997x1600.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Cold, heat, sea and mountains make Chile a country—an extremely isolated one:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29423f18-43e9-46f6-9889-f3bc60669a67_1600x1427.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29423f18-43e9-46f6-9889-f3bc60669a67_1600x1427.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29423f18-43e9-46f6-9889-f3bc60669a67_1600x1427.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29423f18-43e9-46f6-9889-f3bc60669a67_1600x1427.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29423f18-43e9-46f6-9889-f3bc60669a67_1600x1427.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29423f18-43e9-46f6-9889-f3bc60669a67_1600x1427.png" width="1456" height="1299" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/29423f18-43e9-46f6-9889-f3bc60669a67_1600x1427.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1299,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29423f18-43e9-46f6-9889-f3bc60669a67_1600x1427.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29423f18-43e9-46f6-9889-f3bc60669a67_1600x1427.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29423f18-43e9-46f6-9889-f3bc60669a67_1600x1427.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29423f18-43e9-46f6-9889-f3bc60669a67_1600x1427.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>And that's also the main reason why Chileans are incomprehensible: So isolated from all other Spanish speakers!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5294bdd-5e66-48a7-8264-17cf12ec34a5_1080x1080.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5294bdd-5e66-48a7-8264-17cf12ec34a5_1080x1080.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5294bdd-5e66-48a7-8264-17cf12ec34a5_1080x1080.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5294bdd-5e66-48a7-8264-17cf12ec34a5_1080x1080.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5294bdd-5e66-48a7-8264-17cf12ec34a5_1080x1080.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5294bdd-5e66-48a7-8264-17cf12ec34a5_1080x1080.png" width="562" height="562" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a5294bdd-5e66-48a7-8264-17cf12ec34a5_1080x1080.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1080,&quot;width&quot;:1080,&quot;resizeWidth&quot;:562,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5294bdd-5e66-48a7-8264-17cf12ec34a5_1080x1080.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5294bdd-5e66-48a7-8264-17cf12ec34a5_1080x1080.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5294bdd-5e66-48a7-8264-17cf12ec34a5_1080x1080.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5294bdd-5e66-48a7-8264-17cf12ec34a5_1080x1080.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Here’s some more detail, from </span><em><a href="https://unchartedterritories.tomaspueyo.com/p/why-do-900-million-people-speak-spanish" rel="">Why Do 900 Million People Speak Spanish and Portuguese the Way They Do?</a><span>:</span></em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F856b5618-ae2c-4e01-a24a-085144fd9ab5_1456x1009.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F856b5618-ae2c-4e01-a24a-085144fd9ab5_1456x1009.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F856b5618-ae2c-4e01-a24a-085144fd9ab5_1456x1009.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F856b5618-ae2c-4e01-a24a-085144fd9ab5_1456x1009.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F856b5618-ae2c-4e01-a24a-085144fd9ab5_1456x1009.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F856b5618-ae2c-4e01-a24a-085144fd9ab5_1456x1009.jpeg" width="1456" height="1009" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/856b5618-ae2c-4e01-a24a-085144fd9ab5_1456x1009.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1009,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F856b5618-ae2c-4e01-a24a-085144fd9ab5_1456x1009.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F856b5618-ae2c-4e01-a24a-085144fd9ab5_1456x1009.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F856b5618-ae2c-4e01-a24a-085144fd9ab5_1456x1009.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F856b5618-ae2c-4e01-a24a-085144fd9ab5_1456x1009.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This graph shows how much the Spanish from different countries resemble each other. The greener, the closer. The redder, the farther apart.</p><p>You can see some countries are very red:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a017346-a01c-4c3f-ad35-cdc667ea6216_1456x989.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a017346-a01c-4c3f-ad35-cdc667ea6216_1456x989.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a017346-a01c-4c3f-ad35-cdc667ea6216_1456x989.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a017346-a01c-4c3f-ad35-cdc667ea6216_1456x989.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a017346-a01c-4c3f-ad35-cdc667ea6216_1456x989.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a017346-a01c-4c3f-ad35-cdc667ea6216_1456x989.jpeg" width="1456" height="989" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1a017346-a01c-4c3f-ad35-cdc667ea6216_1456x989.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:989,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a017346-a01c-4c3f-ad35-cdc667ea6216_1456x989.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a017346-a01c-4c3f-ad35-cdc667ea6216_1456x989.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a017346-a01c-4c3f-ad35-cdc667ea6216_1456x989.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a017346-a01c-4c3f-ad35-cdc667ea6216_1456x989.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>You see how red Chilean Spanish is? It means it’s quite different. In the beginning of Grad School, I could understand all my Hispanic classmates, but I had a hard time understanding the Chileans!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc979140e-f443-4fc2-a8d1-e6450b45cfa1_598x246.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc979140e-f443-4fc2-a8d1-e6450b45cfa1_598x246.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc979140e-f443-4fc2-a8d1-e6450b45cfa1_598x246.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc979140e-f443-4fc2-a8d1-e6450b45cfa1_598x246.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc979140e-f443-4fc2-a8d1-e6450b45cfa1_598x246.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc979140e-f443-4fc2-a8d1-e6450b45cfa1_598x246.jpeg" width="598" height="246" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c979140e-f443-4fc2-a8d1-e6450b45cfa1_598x246.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:246,&quot;width&quot;:598,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc979140e-f443-4fc2-a8d1-e6450b45cfa1_598x246.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc979140e-f443-4fc2-a8d1-e6450b45cfa1_598x246.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc979140e-f443-4fc2-a8d1-e6450b45cfa1_598x246.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc979140e-f443-4fc2-a8d1-e6450b45cfa1_598x246.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>I couldn’t find any great analysis on this, so if you find it, please let me know. The best hypotheses I could find were:</p><ul><li><p>It’s the farthest region from Spain, so the least communicated to the rest of the empire, and hence the one that drifted the most from the homeland.</p></li><li><p>It’s extremely isolated by the Andes, the ocean in the west, the ice in the south, and the desert in the north, making its connection even to Argentina, Bolivia or Peru really hard.</p></li><li><p>It didn’t have silver mines, or a climate for sugar or tobacco farming, so it wasn’t a particularly valuable place to exploit, and remained secondary in the empire, getting fewer visitors from other parts of the empire, and drifting further apart.</p></li><li><p>It has strong influences from other regions, such as German, Italian, or even Basque.&nbsp;</p></li></ul><p>But why is no other country as long?</p><p>You need:</p><ul><li><p>A sandwich between sea and continent</p></li><li><p>Oriented north-south, so that it changes climates quickly&nbsp;</p></li><li><p>Far enough from the equator so that the north-south climate does change fast, and so that it’s not too densely populated.</p></li></ul><p>The sandwich requires an oceanic plate subducting under a continental plate, which only happens here:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5273cf8-08e3-4d4d-be55-88abb23d2975_1600x1091.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5273cf8-08e3-4d4d-be55-88abb23d2975_1600x1091.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5273cf8-08e3-4d4d-be55-88abb23d2975_1600x1091.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5273cf8-08e3-4d4d-be55-88abb23d2975_1600x1091.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5273cf8-08e3-4d4d-be55-88abb23d2975_1600x1091.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5273cf8-08e3-4d4d-be55-88abb23d2975_1600x1091.png" width="1456" height="993" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b5273cf8-08e3-4d4d-be55-88abb23d2975_1600x1091.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:993,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5273cf8-08e3-4d4d-be55-88abb23d2975_1600x1091.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5273cf8-08e3-4d4d-be55-88abb23d2975_1600x1091.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5273cf8-08e3-4d4d-be55-88abb23d2975_1600x1091.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5273cf8-08e3-4d4d-be55-88abb23d2975_1600x1091.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>From there, take out the areas that are equatorial or too cold:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa37dca14-fb50-4f38-95ed-37e85ecbf2e6_1600x1107.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa37dca14-fb50-4f38-95ed-37e85ecbf2e6_1600x1107.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa37dca14-fb50-4f38-95ed-37e85ecbf2e6_1600x1107.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa37dca14-fb50-4f38-95ed-37e85ecbf2e6_1600x1107.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa37dca14-fb50-4f38-95ed-37e85ecbf2e6_1600x1107.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa37dca14-fb50-4f38-95ed-37e85ecbf2e6_1600x1107.png" width="1456" height="1007" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a37dca14-fb50-4f38-95ed-37e85ecbf2e6_1600x1107.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1007,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa37dca14-fb50-4f38-95ed-37e85ecbf2e6_1600x1107.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa37dca14-fb50-4f38-95ed-37e85ecbf2e6_1600x1107.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa37dca14-fb50-4f38-95ed-37e85ecbf2e6_1600x1107.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa37dca14-fb50-4f38-95ed-37e85ecbf2e6_1600x1107.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>In the western Pacific, that leaves islands, mainly Japan, New Zealand, and maybe the Philippines. But in this part of the world, the mountain chains start from deep under the sea, which generates archipelagos rather than a continental sliver.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5186b3b6-6b3a-4d11-b2cc-b8445985b7e6_1600x997.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5186b3b6-6b3a-4d11-b2cc-b8445985b7e6_1600x997.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5186b3b6-6b3a-4d11-b2cc-b8445985b7e6_1600x997.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5186b3b6-6b3a-4d11-b2cc-b8445985b7e6_1600x997.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5186b3b6-6b3a-4d11-b2cc-b8445985b7e6_1600x997.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5186b3b6-6b3a-4d11-b2cc-b8445985b7e6_1600x997.png" width="1456" height="907" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5186b3b6-6b3a-4d11-b2cc-b8445985b7e6_1600x997.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:907,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5186b3b6-6b3a-4d11-b2cc-b8445985b7e6_1600x997.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5186b3b6-6b3a-4d11-b2cc-b8445985b7e6_1600x997.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5186b3b6-6b3a-4d11-b2cc-b8445985b7e6_1600x997.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5186b3b6-6b3a-4d11-b2cc-b8445985b7e6_1600x997.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>In the eastern Pacific, it leaves Chile and the US – Mexico – Canada west coast. An elevation map shows how a country could have been viable here:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23a598f8-867c-426d-bf30-96af55aa012c_1470x1427.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23a598f8-867c-426d-bf30-96af55aa012c_1470x1427.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23a598f8-867c-426d-bf30-96af55aa012c_1470x1427.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23a598f8-867c-426d-bf30-96af55aa012c_1470x1427.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23a598f8-867c-426d-bf30-96af55aa012c_1470x1427.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23a598f8-867c-426d-bf30-96af55aa012c_1470x1427.png" width="1456" height="1413" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/23a598f8-867c-426d-bf30-96af55aa012c_1470x1427.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1413,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23a598f8-867c-426d-bf30-96af55aa012c_1470x1427.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23a598f8-867c-426d-bf30-96af55aa012c_1470x1427.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23a598f8-867c-426d-bf30-96af55aa012c_1470x1427.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23a598f8-867c-426d-bf30-96af55aa012c_1470x1427.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>West of the mountain chain that runs through all of the American continent, you can see the green sliver that could have been a country.</em></figcaption></figure></div><p>So why don’t we have a Chile of the north?</p><p>Mexico was close to Spain, had silver mines interesting to Spain, smaller mountains than the Andes, and is much narrower from sea to sea. All of this made the country much better communicated east-west, so the western coast couldn’t evolve as a separate entity.</p><p>In the US and Canadian coast, the US conquered that area from the east extremely fast, making the independence of the West Coast impossible. We can imagine that, if these regions had been left to continue developing for a few thousand years, a distinct country (or set of countries) would have emerged in the west.</p><p>So that's why Chile is one of the longest—and the thinnest—countries in the world!</p><div data-attrs="{&quot;url&quot;:&quot;https://unchartedterritories.tomaspueyo.com/p/why-is-chile-so-long?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}" data-component-name="CaptionedButtonToDOM"><p><em>If you know somebody who’d enjoy this article, show you think about them and share it!</em></p><p data-attrs="{&quot;url&quot;:&quot;https://unchartedterritories.tomaspueyo.com/p/why-is-chile-so-long?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}" data-component-name="ButtonCreateButton"><a href="https://unchartedterritories.tomaspueyo.com/p/why-is-chile-so-long?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Trying Kolmogorov-Arnold Networks in Practice (105 pts)]]></title>
            <link>https://cprimozic.net/blog/trying-out-kans/</link>
            <guid>40855028</guid>
            <pubDate>Tue, 02 Jul 2024 09:54:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cprimozic.net/blog/trying-out-kans/">https://cprimozic.net/blog/trying-out-kans/</a>, See on <a href="https://news.ycombinator.com/item?id=40855028">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>There's been a fair bit of buzz about Kolmogorov-Arnold networks online lately.  Some research papers were posted around claiming that they offer better accuracy or faster training compared to traditional neural networks/MLPs for the same parameter count.</p>
<p>I was compelled by these claims and decided to test them out myself.  Here are my main findings if you're not interested in reading through the details:</p>
<p>
KANs definitely don't feel like a silver bullet, and they require significant tuning to get good results compared to regular neural networks.
</p>
<p>That being said, KANs can usually come close to or match the performance of regular neural networks at the same parameter count.  However, they are much more complicated to implement than neural networks and require a lot of tricks and hacky-feeling techniques to make them work.</p>
<p>I do believe that there are specialized use cases where they could be objectively better than NNs and be worth pursuing, but in my opinion the brutal simplicity of NNs make them a much stronger default choice.</p>
<h2 id="kan-background"><a href="#kan-background" aria-label="kan background permalink"></a>KAN Background</h2>
<p>
At the highest level, KANs learn activation functions while neural networks learn the weights connecting neurons with static activation functions together.
</p>
<p>Here's a diagram representing a minimal KAN:</p>
<div>
  <p><img src="https://i.ameo.link/ca7.svg" alt="A diagram representing the architecture of a minimal KAN with two layers.  It shows two inputs labeled IN0 and IN1 which are each connected to two nodes in the first layer.  Each node in the first layer is labeled &quot;B-Spline&quot; and contains a different curve representing its learned activation function.  In turn, of the two pairs of two nodes in the first layer are attached to nodes labeled &quot;+&quot;.  This same pattern repeats for the second layer with the &quot;+&quot; nodes taking the place of the &quot;IN&quot; nodes.  Then, all of the b-spline nodes in the second layer are connected to a single &quot;+&quot; node which is in turn connected to an output node labeled &quot;OUT0&quot;.">
  </p>
</div>
<p>And here's a neural network/multi-layer perceptron with the same number of layers and nodes:</p>
<p><img src="https://i.ameo.link/ca9.svg" alt="A diagram representing a neural network/multi-layer perceptron.  It shows two inputs, two densely connected hidden layers with 4 neurons each, and a single output node.">
</p>
<p>One big difference to note is that there are far fewer connections between nodes in KANs compared to neural networks/MLPs.  KANs move the majority of the learnable parameters into the nodes/activation functions themselves.</p>
<h3 id="b-splines"><a href="#b-splines" aria-label="b splines permalink"></a>B-Splines</h3>
<p>The usual choice of learnable activation function used by KANs is the <strong>B-Spline</strong>.</p>
<p>B-splines are fancy little mathematical gizmos which are composed of multiple piecewise n-degree polynomials strung together in such a way that they're continuous at every point.  Here's an example:</p>
<p><img src="https://i.ameo.link/caa.png" alt="A plot of a B-spline with 2 knots and order-2 polynomials.  It shows a blue curve that performs 4 zero-crossings and has 3 distinct peaks."></p>
<p>There's a lot of things you can customize with B-splines.  You can pick the degree of polynomial used to represent the different grid segments, you can pick the number of knots which determines how many polynomials are strung together, and you can specify the domain/range of the spline however you want.</p>
<p>Another nice thing about B-splines is that they are entirely differentiable.  That means that the autograd implementations in machine learning frameworks like Torch, Jax, and Tinygrad can optimize the coefficients used to define the splines directly.  This is how the "learning" in machine learning happens, so definitely something that's needed for an activation function to be usable in a KAN.</p>
<h2 id="kans-in-practice"><a href="#kans-in-practice" aria-label="kans in practice permalink"></a>KANs in Practice</h2>
<p>After reading up enough on KANs to feel like I understood what was going on, I decided to try implementing them myself from scratch and try them out on some toy problems.  I decided to build it in <a href="https://tinygrad.org/">Tinygrad</a>, a minimal ML framework I've had success working with in the past.  What I ended up with is <a href="https://github.com/Ameobea/kan/blob/main/tiny_kan.py">here</a>.</p>
<p>The basic KAN architecture wasn't too complicated, and the only really tricky part was the B-spline implementation.  For that, I just ported the implementation that the authors of the <a href="https://arxiv.org/abs/2404.19756">original KAN research paper</a> created for their <a href="https://github.com/KindXiaoming/pykan">PyKAN library</a>.</p>
<h3 id="simple-1d-test-function"><a href="#simple-1d-test-function" aria-label="simple 1d test function permalink"></a>Simple 1D Test Function</h3>
<p>After a bit of effort and some debugging, I had a working KAN implementation.  To test it out, I set up a small KAN with 2 layers of trained it to fit some relatively simple 1D functions and it did a pretty good job:</p>
<p><span>
      <a href="https://cprimozic.b-cdn.net/static/bff66e1cba0e97694f51f23607cec8ab/8fbf1/kan_curve_fit.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
          <source srcset="https://cprimozic.b-cdn.net/static/bff66e1cba0e97694f51f23607cec8ab/8359c/kan_curve_fit.avif 210w, https://cprimozic.b-cdn.net/static/bff66e1cba0e97694f51f23607cec8ab/4c727/kan_curve_fit.avif 420w, https://cprimozic.b-cdn.net/static/bff66e1cba0e97694f51f23607cec8ab/4ab03/kan_curve_fit.avif 840w, https://cprimozic.b-cdn.net/static/bff66e1cba0e97694f51f23607cec8ab/c3c94/kan_curve_fit.avif 1260w, https://cprimozic.b-cdn.net/static/bff66e1cba0e97694f51f23607cec8ab/c095d/kan_curve_fit.avif 1680w, https://cprimozic.b-cdn.net/static/bff66e1cba0e97694f51f23607cec8ab/22bef/kan_curve_fit.avif 1728w" sizes="(max-width: 840px) 100vw, 840px" type="image/avif">
          <source srcset="https://cprimozic.b-cdn.net/static/bff66e1cba0e97694f51f23607cec8ab/aaa7a/kan_curve_fit.png 210w, https://cprimozic.b-cdn.net/static/bff66e1cba0e97694f51f23607cec8ab/2dc40/kan_curve_fit.png 420w, https://cprimozic.b-cdn.net/static/bff66e1cba0e97694f51f23607cec8ab/993bb/kan_curve_fit.png 840w, https://cprimozic.b-cdn.net/static/bff66e1cba0e97694f51f23607cec8ab/db723/kan_curve_fit.png 1260w, https://cprimozic.b-cdn.net/static/bff66e1cba0e97694f51f23607cec8ab/7bc17/kan_curve_fit.png 1680w, https://cprimozic.b-cdn.net/static/bff66e1cba0e97694f51f23607cec8ab/8fbf1/kan_curve_fit.png 1728w" sizes="(max-width: 840px) 100vw, 840px" type="image/png">
          <img src="https://cprimozic.b-cdn.net/static/bff66e1cba0e97694f51f23607cec8ab/993bb/kan_curve_fit.png" alt="A screenshot of a plot showing a the results of a KAN trained to fit a 1D function.  It shows a blue line labeled &quot;Actual&quot; making a zigzag pattern along with an orange line labeled &quot;Predicted&quot; which follows the blue one pretty closely, but with some inaccuracy especially near sharp turns." title="" loading="lazy" decoding="async">
        </picture>
  </a>
    </span></p>
<p>To understand what kind of splines it was learning to accomplish this, I plotted the output of each spline in the network across the full input range of the model:</p>
<p><span>
      <a href="https://cprimozic.b-cdn.net/static/872c51295696dcf65fa61cde1ff901ac/8fbf1/kan_response_plot.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
          <source srcset="https://cprimozic.b-cdn.net/static/872c51295696dcf65fa61cde1ff901ac/8359c/kan_response_plot.avif 210w, https://cprimozic.b-cdn.net/static/872c51295696dcf65fa61cde1ff901ac/4c727/kan_response_plot.avif 420w, https://cprimozic.b-cdn.net/static/872c51295696dcf65fa61cde1ff901ac/4ab03/kan_response_plot.avif 840w, https://cprimozic.b-cdn.net/static/872c51295696dcf65fa61cde1ff901ac/c3c94/kan_response_plot.avif 1260w, https://cprimozic.b-cdn.net/static/872c51295696dcf65fa61cde1ff901ac/c095d/kan_response_plot.avif 1680w, https://cprimozic.b-cdn.net/static/872c51295696dcf65fa61cde1ff901ac/22bef/kan_response_plot.avif 1728w" sizes="(max-width: 840px) 100vw, 840px" type="image/avif">
          <source srcset="https://cprimozic.b-cdn.net/static/872c51295696dcf65fa61cde1ff901ac/aaa7a/kan_response_plot.png 210w, https://cprimozic.b-cdn.net/static/872c51295696dcf65fa61cde1ff901ac/2dc40/kan_response_plot.png 420w, https://cprimozic.b-cdn.net/static/872c51295696dcf65fa61cde1ff901ac/993bb/kan_response_plot.png 840w, https://cprimozic.b-cdn.net/static/872c51295696dcf65fa61cde1ff901ac/db723/kan_response_plot.png 1260w, https://cprimozic.b-cdn.net/static/872c51295696dcf65fa61cde1ff901ac/7bc17/kan_response_plot.png 1680w, https://cprimozic.b-cdn.net/static/872c51295696dcf65fa61cde1ff901ac/8fbf1/kan_response_plot.png 1728w" sizes="(max-width: 840px) 100vw, 840px" type="image/png">
          <img src="https://cprimozic.b-cdn.net/static/872c51295696dcf65fa61cde1ff901ac/993bb/kan_response_plot.png" alt="A screenshot of a number of different plots showing the output of each B-spline in a KAN after it's been trained to model a target function.  The first layer shows output plots that are relatively simple and smooth while the second layer's outputs are significantly more complicated and have more high-frequency features." title="" loading="lazy" decoding="async">
        </picture>
  </a>
    </span></p>
<p>Pretty solid results!  The first layer's outputs are pretty simple and represent a single spline each.  Then the second layer creates more complicated representations that are stitched together from the outputs of the previous layer, and the final layer has a single spline which combines it all together and returns the model's output.</p>
<h3 id="scaling-up"><a href="#scaling-up" aria-label="scaling up permalink"></a>Scaling Up</h3>
<p>Inspired by this early success, I decided to try turning up the complexity.  I set up a training pipeline to parameterize images - learning a function like <code>(normalizedXCoord, normalizedYCoord) -&gt; pixelLuminance</code>.</p>
<p>I used a version of positional encoding to expand the coordinates from single scalars into small vectors to make it easier for the networks to learn high-frequency features.</p>
<p>
When I tried to train KANs to parameterize some small images, I ran into significant difficulty.
</p>
<p>The models would fail to converge or perform badly.  I tried changing a bunch of things like layer count, knot count and spline order on the splines, learning rate, you name it.  Some things helped, but it was largely a pretty poor showing.</p>
<p>At some point, I figured I'd take a look at PyKAN's source code to see if I was missing something in my implementation compared to what they were doing.  I'd had good luck with PyKAN when testing it out in a Python notebook when I was first investigating KANs.</p>
<p>As it turns out, there was a whole lot more going on there than I expected.</p>
<h2 id="pykans-tricks-and-extras"><a href="#pykans-tricks-and-extras" aria-label="pykans tricks and extras permalink"></a>PyKAN's Tricks and Extras</h2>
<p>PyKAN's source code is actually pretty small and easy enough to parse through.</p>
<p>
As I read some of the code around the core KAN implementation, I quickly noticed that PyKAN was using a lot of tricks to beef up their KAN implementation.
</p>
<p>Here are the ones I noticed:</p>
<h3 id="bias"><a href="#bias" aria-label="bias permalink"></a>Bias</h3>
<p>PyKAn includes a learnable bias vector which is added to the output of each layer before passing it to the next.  This is the same thing that traditional neural networks have.</p>
<p>They include this note in the docs:</p>
<blockquote>
<p>biases are added on nodes (in principle, biases can be absorbed into activation functions. However, we still have them for better optimization)</p>
</blockquote>
<p>I added them to my own KAN implementation and sure enough, I did see an improvement in learning ability.</p>
<h3 id="spline-weights"><a href="#spline-weights" aria-label="spline weights permalink"></a>Spline Weights</h3>
<p>I also noticed that PyKAN was using a learnable weight vector of size <code>(in_count * out_count,)</code> and multiplying the output of the splines by this before summing.</p>
<p>This is a pretty big vector, and it scales multiplicatively with the layer width.  It's actually as big as the entire weight vector for a dense layer of a traditional neural network.</p>
<p>That being said, it seems to be worth it.  When I added it to my KAN implementation and trimmed down the layer sizes to keep the param count the same, the training performance was about the same or slightly better.</p>
<h3 id="base-functions"><a href="#base-functions" aria-label="base functions permalink"></a>Base Functions</h3>
<p>In addition to the B-Splines, the PyKAN implementation also included something they call "base functions", also referred to as "residual functions".</p>
<p>Not to be confused with the basis functions which are used internally in the B-Spline implementation, these add a different path for data to get through the layer that bypasses the splines entirely.  The function used defaults to <code>torch.nn.SiLU()</code> which is a close relative of the ReLU activation function.</p>
<p>So the input vector of the layer gets passed through this base function element-wise, multiplied by yet another set of learnable weights of shape <code>(in_count * out_count)</code>, and then added to the outputs of the splines.</p>
<p>So there's really a lot going on now, and we're quite far away from the simple architecture from the diagram I included earlier.  The KAN layer is something like this now:</p>
<div data-language="py"><pre><code>y <span>=</span> <span>(</span>splines<span>(</span>x<span>)</span> <span>*</span> spline_weights <span>+</span> base_fn<span>(</span>x<span>)</span> <span>*</span> base_weights<span>)</span><span>.</span><span>sum</span><span>(</span>axis<span>=</span><span>-</span><span>1</span><span>)</span> <span>+</span> bias</code></pre></div>
<h3 id="grid-updates--extensions"><a href="#grid-updates--extensions" aria-label="grid updates  extensions permalink"></a>Grid Updates + Extensions</h3>
<p>There's also a lot of code included for things they call grid extensions and grid updates.  I believe that these are for dynamically adjusting the domain of individual splines and/or the number of knots in a spline live during training.</p>
<p>There was an example on the PyKAN Github showing how they'd refine the splines to add more and more knots and provide higher and higher resolution.  They'd pick new coefficients to approximate the old spline as closely as possible so it could be done during training.</p>
<p>I didn't mess with any of these and definitely didn't attempt porting them over to my Tinygrad implementation.</p>
<h3 id="lbfgs-optimizer"><a href="#lbfgs-optimizer" aria-label="lbfgs optimizer permalink"></a>LBFGS Optimizer</h3>
<p>Finally, I noticed that they were using an optimizer called LBFGS to tune their parameters instead of the SGD or Adam that are usually seen when training neural networks.</p>
<p>I looked into it a little bit and found a <a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS">Wikipedia page</a>.  Here's the first sentence from that:</p>
<blockquote>
<p>Limited-memory BFGS (L-BFGS or LM-BFGS) is an optimization algorithm in the family of quasi-Newton methods that approximates the Broyden–Fletcher–Goldfarb–Shanno algorithm (BFGS)</p>
</blockquote>
<p>I didn't attempt to dig into that further, but I figure that they got better results with that compared to more commonly used optimizers.</p>
<h2 id="results"><a href="#results" aria-label="results permalink"></a>Results</h2>
<p>I integrated a few of these techniques - specifically the base function + base weights, spline weights, and bias vector - into my Tinygrad KAN implementation.  Since these add a huge amount of parameters to the model, I had to significantly reduce the layer sizes and counts that I was testing with in order to keep the same param count.</p>
<p>Despite that, they did seem to help - some more than others - but they also slowed training down a ton.</p>
<h2 id="b-spline-alternatives"><a href="#b-spline-alternatives" aria-label="b spline alternatives permalink"></a>B-Spline Alternatives</h2>
<p>I come from a software development background, and seeing all of these "extras" and special techniques mixed into the KAN implementation felt like a sort of code smell to me.</p>
<p>I wanted to see if I could implement a much more minimal version of a KAN which retained its cool property of having a learnable activation function in a simpler way.</p>
<p>I experimented with some different options for a while and eventually landed on an activation function like this:</p>
<div data-language="py"><pre><code>y <span>=</span> tanh<span>(</span>a <span>*</span> x<span>.</span><span>pow</span><span>(</span><span>2</span><span>)</span> <span>+</span> b <span>*</span> x<span>)</span></code></pre></div>
<p>Where both <code>a</code> and <code>b</code> are learnable.</p>
<p>I got pretty decent results, coming relatively close to the Spline-based KANs on the image parameterization use case and training like 10x faster as well.  I mixed in the learnable bias and played around with some other variations, but the improvements were small or negative.</p>
<p>Eventually, it got to the point where I was just dancing around the same final loss value +-20% or so and I stopped my search.</p>
<p>
The main theme seems to be that the Adam optimizer is quite good at doing its job regardless of the computational graph it has to work with, and the most significant factor controlling performance is just parameter count.
</p>
<h2 id="conclusion"><a href="#conclusion" aria-label="conclusion permalink"></a>Conclusion</h2>
<p>I spent a few more days trying different variations on this theme: mixing KAN layers and NN layers, tweaking model width vs. depth, tuning various other hyperparameters like learning rate and batch size.  Despite my efforts, the results were the same:</p>
<p>
No matter what I did, the most simple neural network was still outperforming the fanciest KAN-based model I tried.
</p>
<p>I was able to train a model to a loss of 0.0006 after 20k epochs with a neural network using boring <code>tanh</code> activations, and the best I could do with the most successful training run with a KAN was about 0.00011.</p>
<p>I'll admit that there's a decent chance I missed something in my KAN implementation, failed to find the architecture of hyperparameters that would work best for me, or picked an unlucky thing to test with that just doesn't work well for KANs.</p>
<p>But the fact that neural networks worked so well with so little effort was pretty compelling and made me uninterested in spending more time trying to get KANs to outperform them for now.  In any case, it wasn't anything close to the "50% less params for the same performance" that I'd heard claims of.</p>
<p>One final note is that I really don't want this to come off as me trashing on KANs and saying that they're useless.  Some of the demos on the PyKAN Github and docs revolve around more niche or use cases like learning functions to "machine precision" with 64-bit floats - which they do successfully. Neural networks can't do easily if at all.</p>
<p>I think work investigating alternatives or improvements to neural networks is very worthwhile, and might be the kind of thing that gives us the next transformer-level leap in AI capability.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Welcome to Ladybird, a truly independent web browser (535 pts)]]></title>
            <link>https://ladybird.org/index.html</link>
            <guid>40854836</guid>
            <pubDate>Tue, 02 Jul 2024 09:14:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ladybird.org/index.html">https://ladybird.org/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=40854836">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      

      <div>
            <h2>
              Welcome to <span>Ladybird</span>,<br>a truly independent<br>web
              browser.
            </h2>
            <p>
              We are building a brand-new browser from scratch, backed by a
              non-profit.
            </p>
            <p><a href="#gi">Get Involved</a>
          </p></div>

      <div id="about">
            <p><img src="https://ladybird.org/assets/img/about-2x.webp" alt="">
            </p>
            <div>
              <h2>About <span>Ladybird</span></h2>
              <p>
                Ladybird is a brand-new browser &amp; web engine. Driven by a
                web standards first approach, Ladybird aims to render the
                modern web with good performance, stability and security.
              </p>
              <p>
                From its humble beginnings as an HTML viewer for the SerenityOS
                hobby operating system project, Ladybird has since grown into a
                cross-platform browser supporting Linux, macOS, and other
                Unix-like systems.
              </p>
              <p>
                Ladybird is currently in heavy development. We are targeting a
                first Alpha release for early adopters in 2026.
              </p>
            </div>
          </div>

      <div>
          <div>
            <p>
              <h2>What makes <span>Ladybird</span> unique</h2>
            </p>
          </div>

          <div>
            <div>
                <p><img src="https://ladybird.org/assets/img/truly-independent.svg">
                </p>
                <div>
                  <h4>Truly independent</h4>
                  <p>
                    No code from other browsers.
                    We're building a new engine, based on web standards.
                  </p>
                </div>
              </div>

            <div>
                <p><img src="https://ladybird.org/assets/img/single-focus.svg">
                </p>
                <div>
                  <h4>Singular focus</h4>
                  <p>We are focused on one thing: the web browser.
                </p></div>
              </div>

            <div>
                <p><img src="https://ladybird.org/assets/img/no-monetization.svg">
                </p>
                <div>
                  <h4>No monetization</h4>
                  <p>
                    No "default search deals", crypto tokens, or other forms of
                    user monetization, ever.
                  </p>
                </div>
              </div>
          </div>
        </div>

      <div id="news">
          <p>
            <h2>News &amp; Announcements</h2>
          </p>
          
        </div>

      <div id="gi">
            <h2>Get Involved</h2>
            <p>
              Ladybird is currently in heavy development, and there's work to be
              done in all areas of the browser.
            </p>
            <p>
              We're welcoming new developers every week. The main community hub
              is
              <a href="https://discord.gg/nvfjVJ4Svh">our Discord server</a>.
            </p>
            <p>
              All the code is hosted on
              <a href="https://github.com/LadybirdBrowser/ladybird">GitHub</a>. Clone it, build it, and join our Discord if you want to
              collaborate on it! We're looking forward to seeing you there.
            </p>
            <p><a href="https://discord.gg/nvfjVJ4Svh">Join Discord</a>
            <a href="https://github.com/LadybirdBrowser/ladybird">Get the code</a>
          </p></div>

      

      <div>
              <h2>Become a <span>Ladybird</span> supporter</h2>
              <p>
                Ladybird is funded entirely by sponsorships and donations from
                people and companies who care about the open web.
              </p><p>
                We accept one-time and recurring monthly donations via <a href="https://donorbox.org/ladybird">Donorbox</a>.
              </p><p>
                If you or your company would like to make a large donation, we would be happy to display your logo
                on this website! Please <a href="mailto:contact@ladybird.org">contact us</a> about becoming a sponsor.
            </p></div>

      <div>
          <p>
            <h2>Frequently Asked Questions</h2>
          </p>

          <div>
            <div>
                
                <p>
                  We are targeting Summer 2026 for a first Alpha version on
                  Linux and macOS. This will be aimed at developers and early
                  adopters.
                </p>
              </div>

            <div>
                
                <p>
                  We currently have 4 paid full-time engineers working on
                  Ladybird. There is also a large community of volunteer contributors.
                </p>
              </div>

            <div>
                
                <p>
                  We have 3 new full-time engineers starting soon. Going
                  forward, we would like to grow the team at a reasonable pace.
                  Building the right team is more important than building it
                  quickly.
                </p>
              </div>

            <div><p>
                  The focus of the Ladybird project is to build a new browser
                  engine from the ground up. We don't use code from Blink,
                  WebKit, Gecko, or any other browser engine.
                  </p><p>
                  For historical reasons, the browser uses various libraries
                  from the SerenityOS project, which has a strong culture of
                  writing <i>everything</i> from scratch.
                  Now that Ladybird has forked from SerenityOS, it is no longer
                  bound by this culture, and we will be making use of 3rd party
                  libraries for common functionality (e.g image/audio/video formats,
                  encryption, graphics, etc.)
                  </p><p>
                  We are already using some of the same 3rd party libraries
                  that other browsers use, but we will never adopt another
                  browser engine instead of building our own.
                </p></div>

            <div><p>
                  We don't have anyone actively working on Windows support, and
                  there are considerable changes required to make it work well
                  outside a Unix-like environment.
                  </p><p>
                  We would like to do Windows eventually, but it's not a priority
                  at the moment.
                </p></div>

            <div><p>
                  We don't have anyone actively working on an Android or iOS port.
                  More effort will be put into mobile once we have the desktop versions in a good state.
                  </p><p>
                  While there is the start of an Android port in the project repository,
                  mobile is not a priority at the moment.
                </p></div>

            <div>
                  
                  <p>
                  Sponsors will have their logos displayed on our website, and
                  will be thanked in updates / on social media.
                  </p><p>
                  Please <a href="mailto:contact@ladybird.org">contact us</a> if you are interested in sponsorship.
                </p></div>

            <div>
                
                <p>
                  All sponsorships are in the form of unrestricted donations.
                  Board seats and other forms of influence are not for sale.
                </p>
              </div>

            <div><p>
                  Ladybird started as a component of the SerenityOS hobby project, which only allows C++.
                  The choice of language was not so much a technical decision, but more one of personal convenience.
                  Andreas was most comfortable with C++ when creating SerenityOS, and now we have almost half
                  a million lines of modern C++ to maintain.
                  </p><p>
                  However, now that Ladybird has forked and become its own independent project,
                  all constraints previously imposed by SerenityOS are no longer in effect.
                  We are actively evaluating a number of alternatives and will
                  be adding a mature successor language to the project in the near future.
                  This process is already quite far along, and prototypes exist in
                  multiple languages.
                </p></div>

          </div>
        </div>

      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Diff-pdf: tool to visually compare two PDFs (425 pts)]]></title>
            <link>https://github.com/vslavik/diff-pdf</link>
            <guid>40854319</guid>
            <pubDate>Tue, 02 Jul 2024 07:26:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/vslavik/diff-pdf">https://github.com/vslavik/diff-pdf</a>, See on <a href="https://news.ycombinator.com/item?id=40854319">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><em>Note: this repository is provided <strong>as-is</strong> and the code is not being actively
developed. If you wish to improve it, that's greatly appreciated: please make
the changes and submit a pull request, I'll gladly merge it or help you out
with finishing it. However, please do not expect any kind of support, including
implementation of feature requests or fixes. If you're not a developer and/or
willing to get your hands dirty, this tool is probably not for you.</em></p>
<p dir="auto"><a href="https://github.com/vslavik/diff-pdf/actions/workflows/build.yml"><img src="https://github.com/vslavik/diff-pdf/actions/workflows/build.yml/badge.svg" alt="Build"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">diff-pdf is a tool for visually comparing two PDFs.</p>
<p dir="auto">It takes two PDF files as arguments. By default, its only output is its return
code, which is 0 if there are no differences and 1 if the two PDFs differ. If
given the <code>--output-diff</code> option, it produces a PDF file with visually
highlighted differences:</p>
<div data-snippet-clipboard-copy-content="$ diff-pdf --output-diff=diff.pdf a.pdf b.pdf"><pre><code>$ diff-pdf --output-diff=diff.pdf a.pdf b.pdf
</code></pre></div>
<p dir="auto">Another option is to compare the two files visually in a simple GUI, using
the <code>--view</code> argument:</p>
<div data-snippet-clipboard-copy-content="$ diff-pdf --view a.pdf b.pdf"><pre><code>$ diff-pdf --view a.pdf b.pdf
</code></pre></div>
<p dir="auto">This opens a window that lets you view the files' pages and zoom in on details.
It is also possible to shift the two pages relatively to each other using
Ctrl-arrows (Cmd-arrows on MacOS). This is useful for identifying translation-only differences.</p>
<p dir="auto">See the output of <code>$ diff-pdf --help</code> for complete list of options.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Obtaining the binaries</h2><a id="user-content-obtaining-the-binaries" aria-label="Permalink: Obtaining the binaries" href="#obtaining-the-binaries"></a></p>
<p dir="auto">Precompiled version of the tool for Windows is available as part of
<a href="https://github.com/vslavik/diff-pdf/releases/latest/">the latest release</a>
as a ZIP archive, which contains everything you need to run diff-pdf. It will
work from any place you unpack it to.</p>
<p dir="auto">Alternatively, if you use <a href="https://chocolatey.org/" rel="nofollow">Chocolatey</a>, you can install
diff-pdf on Windows with:</p>

<p dir="auto">On Mac, if you use <a href="https://brew.sh/" rel="nofollow">Homebrew</a>, you can use it to install diff-pdf with it:</p>

<p dir="auto">On Mac, if you use <a href="https://macports.org/" rel="nofollow">Macports</a>, you can install diff-pdf with:</p>

<p dir="auto">On  Fedora and CentOS 8:</p>
<div data-snippet-clipboard-copy-content="$ sudo dnf install diff-pdf"><pre><code>$ sudo dnf install diff-pdf
</code></pre></div>
<p dir="auto">Precompiled version for openSUSE can be downloaded from the
<a href="http://software.opensuse.org/" rel="nofollow">openSUSE build service</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Compiling from sources</h2><a id="user-content-compiling-from-sources" aria-label="Permalink: Compiling from sources" href="#compiling-from-sources"></a></p>
<p dir="auto">The build system uses Automake and so a Unix or Unix-like environment (Cygwin
or MSYS) is required. Compilation is done in the usual way:</p>
<div data-snippet-clipboard-copy-content="$ ./bootstrap
$ ./configure
$ make
$ make install"><pre><code>$ ./bootstrap
$ ./configure
$ make
$ make install
</code></pre></div>
<p dir="auto">(Note that the first step, running the <code>./bootstrap</code> script, is only required
when building sources checked from version control system, i.e. when <code>configure</code>
and <code>Makefile.in</code> files are missing.)</p>
<p dir="auto">As for dependencies, diff-pdf requires the following libraries:</p>
<ul dir="auto">
<li>wxWidgets &gt;= 3.0</li>
<li>Cairo &gt;= 1.4</li>
<li>Poppler &gt;= 0.10</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">CentOS:</h4><a id="user-content-centos" aria-label="Permalink: CentOS:" href="#centos"></a></p>
<div data-snippet-clipboard-copy-content="$ sudo yum groupinstall &quot;Development Tools&quot;
$ sudo yum install wxGTK wxGTK-devel poppler-glib poppler-glib-devel"><pre><code>$ sudo yum groupinstall "Development Tools"
$ sudo yum install wxGTK wxGTK-devel poppler-glib poppler-glib-devel
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Ubuntu:</h4><a id="user-content-ubuntu" aria-label="Permalink: Ubuntu:" href="#ubuntu"></a></p>
<div data-snippet-clipboard-copy-content="$ sudo apt-get install make automake g++
$ sudo apt-get install libpoppler-glib-dev poppler-utils libwxgtk3.0-gtk3-dev"><pre><code>$ sudo apt-get install make automake g++
$ sudo apt-get install libpoppler-glib-dev poppler-utils libwxgtk3.0-gtk3-dev
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">macOS:</h4><a id="user-content-macos" aria-label="Permalink: macOS:" href="#macos"></a></p>
<p dir="auto">Install Command Line Tools for Xcode:</p>

<p dir="auto">and install <a href="https://brew.sh/" rel="nofollow">Homebrew</a> or <a href="https://www.macports.org/" rel="nofollow">MacPorts</a> to manage dependencies, then:</p>
<div data-snippet-clipboard-copy-content="$ brew install automake autoconf wxmac poppler cairo pkg-config"><pre><code>$ brew install automake autoconf wxmac poppler cairo pkg-config
</code></pre></div>
<p dir="auto">or</p>
<div data-snippet-clipboard-copy-content="$ sudo port install automake autoconf wxWidgets-3.0 poppler cairo pkgconfig"><pre><code>$ sudo port install automake autoconf wxWidgets-3.0 poppler cairo pkgconfig
</code></pre></div>
<p dir="auto">Note that many more libraries are required on Windows, where none of the
libraries Cairo and Poppler use are normally available. At the time of writing,
transitive cover of the above dependencies included fontconfig, freetype, glib,
libpng, pixman, gettext, libiconv, libjpeg and zlib.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Compiling on Windows using MSYS + MinGW</h3><a id="user-content-compiling-on-windows-using-msys--mingw" aria-label="Permalink: Compiling on Windows using MSYS + MinGW" href="#compiling-on-windows-using-msys--mingw"></a></p>
<ol dir="auto">
<li>
<p dir="auto">First of all, you will need working MinGW installation with MSYS2 environment
and C++ compiler. Install MSYS2 by following <a href="https://www.msys2.org/" rel="nofollow">their instructions</a>.</p>
</li>
<li>
<p dir="auto">Once installed, launch the MSYS2 MinGW shell. It will open a terminal window;
type <code>cd /c/directory/with/diff-pdf</code> to go to the directory with diff-pdf
sources.</p>
</li>
<li>
<p dir="auto">You will need to install additional MSYS components that are not normally
included with MSYS, using these commands:</p>
<div data-snippet-clipboard-copy-content="$ pacman -Syu
$ pacman -S automake autoconf pkg-config make zip pactoys
$ pacboy -S gcc:p poppler:p wxWidgets:p"><pre><code>$ pacman -Syu
$ pacman -S automake autoconf pkg-config make zip pactoys
$ pacboy -S gcc:p poppler:p wxWidgets:p
</code></pre></div>
</li>
<li>
<p dir="auto">Build diff-pdf in the same way as in the instructions for Unix above:</p>
<div data-snippet-clipboard-copy-content="$ ./bootstrap  # only if building from git repository
$ ./configure
$ make"><pre><code>$ ./bootstrap  # only if building from git repository
$ ./configure
$ make
</code></pre></div>
</li>
<li>
<p dir="auto">To build a ZIP archive will all DLLs, run</p>

</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installing</h2><a id="user-content-installing" aria-label="Permalink: Installing" href="#installing"></a></p>
<p dir="auto">On Unix, the usual <code>make install</code> is sufficient.</p>
<p dir="auto">On Windows, installation is not necessary, just copy the files somewhere. If
you built it following the instructions above, all the necessary files will be
in the created ZIP archive.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mako – fast, production-grade web bundler based on Rust (222 pts)]]></title>
            <link>https://makojs.dev/blog/mako-open-sourced</link>
            <guid>40853845</guid>
            <pubDate>Tue, 02 Jul 2024 05:41:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://makojs.dev/blog/mako-open-sourced">https://makojs.dev/blog/mako-open-sourced</a>, See on <a href="https://news.ycombinator.com/item?id=40853845">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      <h2 id="mako-is-now-open-source" tabindex="-1">Mako is Now Open Source <a href="#mako-is-now-open-source">
          
        </a></h2>
<p><em>2024-06-28 by <a href="https://github.com/sorrycc">sorrycc</a></em></p>
<p>中文版: <a href="https://makojs.dev/blog/mako-open-sourced_zh-CN">《Mako 开源了》</a>。</p>
<p><img src="https://img.alicdn.com/imgextra/i4/O1CN01CK3ElF1kFaFzFBUiA_!!6000000004654-0-tps-1200-662.jpg" alt=""></p>
<p>Hi, I am sorrycc, one of the main maintainers of Mako, and also the creator of Umi, Dva, Father, and other libraries. I am thrilled to announce that Mako is finally open source, the Github url is <a href="https://github.com/umijs/mako/">https://github.com/umijs/mako/</a> , and I’m excited to formally introduce it to you today.</p>
<h2 id="what-is-mako%3F" tabindex="-1">What is Mako? <a href="#what-is-mako%3F">
          
        </a></h2>
<img src="https://img.alicdn.com/imgextra/i4/O1CN01dvFN0j1e2rYBJpJGJ_!!6000000003814-2-tps-2048-2048.png_240x240.jpg" width="120" height="120">
<p>Mako is an “extremely fast” and “production-grade” front-end build tool, based on Rust.</p>
<p>The “extremely fast” aspect was our initial motivation for starting the Mako project. Without build speed issues, Mako would not have been necessary. Refer to the Benchmark section below for some data, and we are constantly exploring even faster build speed solutions. The “production-grade” label comes from the fact that since 2023.11.24, Mako has been officially released internally at Ant Group. It has been validated with engineering practices on thousands of projects and all used npm packages and their versions. It has been implemented in hundreds of projects, serving various platforms and business scenarios internally, including management backends, mini-programs, H5 mobile, low-code, marketing, component libraries, component packaging, Serverless Functions, etc., demonstrating fully production-grade capabilities.</p>
<p>You can visit <a href="https://makojs.dev/docs/features">https://makojs.dev/docs/features</a> to learn more about Mako’s features.</p>
<h2 id="how-did-mako-come-about%3F" tabindex="-1">How did Mako come about? <a href="#how-did-mako-come-about%3F">
          
        </a></h2>
<p>Last year (2023.3), our team launched 3 projects, Rust, SSR, and AIG, and we took on the Rust direction to tackle build performance issues. Our team has been exploring faster build speed solutions, including <a href="https://umijs.org/blog/mfsu-faster-than-vite">MFSU</a>, which optimizes build speed within Webpack. However, this had certain limitations. We sought a thorough solution through Rust.</p>
<p><img src="https://img.alicdn.com/imgextra/i2/O1CN01GDA0FY1mgixV0oGkA_!!6000000004984-2-tps-2772-1330.png_1200x1200.jpg" alt=""></p>
<p>You might wonder why we didn’t use existing Rust tools but decided to create one ourselves. The reasons are complex. For instance, 1) the maturity level of community libraries and their compatibility with Ant’s needs, we researched all the community Rust build solutions before starting, ultimately deciding on creating our own, 2) having control, due to business reasons, build tools at Ant require a lot of customization, and this proved true as we found many matching needs after internal release, 3) the modern meta-frameworks require compilation-time frameworks, in addition to build, they also have a lot of compilation needs, especially in SSR &amp; RSC scenarios, for example, RSC scenarios required 4 builds internally, 4) the need to learn Rust and for team growth, modern frontend tools are all written in Rust, and we would fall behind if we did not advance.</p>
<p><img src="https://img.alicdn.com/imgextra/i3/O1CN012T9Nlo1WVFBDT64dK_!!6000000002793-2-tps-2090-854.png" alt=""></p>
<p>The timeline above is for Mako. Mako kicked off in 2023.3, had its first usable version by 2023.7, was internally released at Ant in 2023.11, and was open-sourced by 2024.6. We initially had 3 members with zero Rust experience, with team members, especially the virtual team, coming and going, learning Rust while digesting build knowledge and working on Mako was challenging, but fortunately, we succeeded and learned a lot in the process. We would like to thank the pioneers in the build domain like Webpack, Farm, and Rspack, as well as ChatGPT.</p>
<h2 id="speed" tabindex="-1">Speed <a href="#speed">
          
        </a></h2>
<p>Mako has put a lot of effort into speed. Below is some Benchmark data.</p>
<p><img src="https://img.alicdn.com/imgextra/i1/O1CN01Ibymuk1xrDoNp2jBg_!!6000000006496-2-tps-2018-340.png" alt=""></p>
<p>The Benchmark ran on a project that Turbopack also tests, on a Mac Book Pro M2 Max. It includes dimensions such as dev cold start time, root node and leaf node HMR time, production Build build time, and JS bundle size. (Note: Farm was not tried successfully using API mode, so no HMR data was generated; RsBuild had some issues upgrading to 0.7, so it’s still on 0.6 for now. RsBuild 0.7 might be a bit faster.)</p>
<p>If you’re interested, feel free to clone the repository and try it out yourself.</p>
<pre tabindex="0"><code><span><span>$</span><span> git</span><span> clone</span><span> git@github.com:umijs/benchmark.git</span></span>
<span><span>$</span><span> cd</span><span> benchmark</span></span>
<span><span>$</span><span> pnpm</span><span> i</span></span>
<span><span>$</span><span> pnpm</span><span> run</span><span> setup</span></span>
<span><span>$</span><span> pnpm</span><span> benchmark</span></span>
<span></span></code></pre>
<p>Here’s how we compare to our previous selves.</p>
<p><img src="https://img.alicdn.com/imgextra/i4/O1CN01UkKwZd1nsv7biyZKf_!!6000000005146-1-tps-825-365.gif" alt=""></p>
<p>For Ant Design Pro full project build, Webpack takes 16s, Mako takes 3.9s, a 4x speed improvement.</p>
<p><img src="https://img.alicdn.com/imgextra/i3/O1CN0180np4N1oZyLr5911c_!!6000000005240-1-tps-1340-610.gif" alt=""></p>
<p>For Ant Design Pro full project build, Mako is almost always real-time hot updates.</p>
<p><img src="https://img.alicdn.com/imgextra/i2/O1CN01LpdES21tqWkFN9mCg_!!6000000005953-1-tps-960-540.gif" alt=""></p>
<p>Intranet Hybrid framework Smallfish project build, based on RSC (React Server Components), scaffold project, build time reduced from 36.7s to 1.2s. It looks a bit exaggerated, but these are real data.</p>
<p><img src="https://img.alicdn.com/imgextra/i3/O1CN01L1HteO1uPKqayzb0u_!!6000000006029-2-tps-1538-494.png" alt=""></p>
<p>These are more examples of speed improvements on such RSC projects.</p>
<p><img src="https://img.alicdn.com/imgextra/i2/O1CN01bzKzwO1gnEtk9Z8pN_!!6000000004186-2-tps-2198-852.png" alt=""></p>
<p>Additionally, Mako also has an experimental SSU feature, similar to the previous MFSU implementation, which does packaging and caching of dependencies. Depending on the ratio of source code to dependencies, it can achieve a 10-50x speed boost in Dev hot start-up. Currently, it can be enabled with the <code>SSU=true</code> environment variable.</p>
<h2 id="how-to-participate%3F" tabindex="-1">How to participate? <a href="#how-to-participate%3F">
          
        </a></h2>
<p>If you want to experience Mako, you can create a Mako + React project with a single command using the scaffolding tool.</p>
<pre tabindex="0"><code><span><span>$</span><span> npm</span><span> create</span><span> mako</span></span>
<span></span></code></pre>
<p>If you’re a Umi user, it’s very simple to experience Mako!</p>
<pre tabindex="0"><code><span><span># Make sure your version is 4.2.0 or above</span></span>
<span><span>$</span><span> npx</span><span> umi</span><span> -v</span></span>
<span><span>4.2.0</span></span>
<span><span># Enable Mako configuration</span></span>
<span><span>$</span><span> npx</span><span> umi</span><span> config</span><span> set</span><span> mako</span><span> {}</span></span>
<span><span># Run build or other commands</span></span>
<span><span>$</span><span> npx</span><span> umi</span><span> build</span></span>
<span></span></code></pre>
<p>If you want to discuss issues or suggestions about Mako, you can scan the QR code to join our WeChat group. (If it’s expired or the group is full, please go to <a href="https://makojs.dev/docs/feedback">https://makojs.dev/docs/feedback</a> for a new QR code.)</p>
<img src="https://img.alicdn.com/imgextra/i1/O1CN01kKspME1FdAZ4cQ1F5_!!6000000000509-0-tps-1050-1671.jpg_240x240.jpg" width="120">
<p>Or click the following link to join our Telegram group.</p>
<p><a href="https://t.me/+EN3fycCw3TI1NDA1">https://t.me/+EN3fycCw3TI1NDA1</a></p>
<p>Also, you’re welcome to subscribe to Mako updates via RSS. We’ll post the latest news about Mako and high-quality technical articles related to building.</p>
<p><a href="https://makojs.dev/rss.xml">https://makojs.dev/rss.xml</a></p>
<p>If you want to get involved in Mako’s open source, you can visit <a href="https://github.com/umijs/mako">https://github.com/umijs/mako</a> and <a href="https://makojs.dev/docs/contributing">CONTRIBUTING document</a> to learn more. Anyone who has submitted Bugfix or Feature PRs can choose to join Mako’s developer DingTalk group.</p>
<p>If you plan to deeply promote and apply Mako in your company, or develop based on Mako, you can contact us (<a href="mailto:sorrycc@gmail.com">mailto:sorrycc@gmail.com</a>) for discussion. We can provide the relevant training, consulting, and more timely support services.</p>
<img src="https://img.alicdn.com/imgextra/i4/O1CN01uWRI3O1Dy7RzGO3fy_!!6000000000284-1-tps-320-224.gif" width="160">
<h2 id="live-q%26a" tabindex="-1">Live Q&amp;A <a href="#live-q%26a">
          
        </a></h2>
<p>Tonight (June 28, 2024) at 9 PM, we’ll be hosting a live Q&amp;A on Bilibili, reservation link available at <a href="https://t.bilibili.com/947260122376175622">https://t.bilibili.com/947260122376175622</a>. We welcome everyone to participate, and you can ask anything about Mako. If you have questions about Mako, you can fill in the questionnaire in advance at <a href="https://docs.qq.com/form/page/DY2Z6VndTRXBpR1Nh">https://docs.qq.com/form/page/DY2Z6VndTRXBpR1Nh</a>.</p>
<h2 id="acknowledgements" tabindex="-1">Acknowledgements <a href="#acknowledgements">
          
        </a></h2>
<p>The release of Mako would not have been possible without each contributor, especially since most of them have participated in their spare time. Thank you!</p>
<ul>
<li>Those who have submitted code to Mako: <a href="https://github.com/hedeng9">hedeng</a>, <a href="https://github.com/jiesia">jiesia</a>, <a href="https://github.com/Maple0817">Maple0817</a>, <a href="https://github.com/vagusX">vagusX</a>, <a href="https://github.com/chessl">chessl</a>, <a href="https://github.com/HiLanXiao">HiLanXiao</a>, <a href="https://github.com/JackGuiYang12">JackGuiYang12</a>, <a href="https://github.com/zhangpanweb">zhangpanweb</a>, <a href="https://github.com/ctts">ctts</a>, <a href="https://github.com/goo-yyh">goo-yyh</a>, <a href="https://github.com/whyer11">whyer11</a></li>
<li>Those who are still actively participating in the development of Mako: <a href="https://github.com/PeachScript">PeachScript</a>, <a href="https://github.com/stormslowly">stormslowly</a>, <a href="https://github.com/xusd320">xusd320</a>, <a href="https://github.com/LovePlayCode">LovePlayCode</a>, <a href="https://github.com/Jinbao1001">Jinbao1001</a>, <a href="https://github.com/sorrycc">sorrycc</a></li>
<li>Community members who used Mako in the early stages and provided suggestions: <a href="https://github.com/xiaohuoni">xiaohuoni</a>, <a href="https://github.com/xierenyuan">xierenyuan</a></li>
<li>The initiator of the project: <a href="https://github.com/afc163">afc163</a></li>
<li>Logo designer: <a href="https://github.com/golevkadesign">golevkadesign</a></li>
<li>The stylish landing page’s PD, designers, and developers: <a href="https://github.com/bupthly">bupthly</a>, 亿元, <a href="https://github.com/Wu-kung">Wu-kung</a></li>
</ul>
<p>And many authors of the community’s dependencies libraries!</p>
<ul>
<li><a href="https://github.com/webpack/webpack">webpack</a>, which inspired lots of ideas of Mako.</li>
<li><a href="https://github.com/swc-project/swc">swc</a> by <a href="https://github.com/kdy1">@kdy1</a>, which powered the parsing, transforming and code generation of Mako.</li>
<li><a href="https://github.com/farm-fe/farm">farm</a> by <a href="https://github.com/wre232114">@brightwu</a>, which inspired the tree shaking, plugin system and others of Mako.</li>
<li><a href="https://github.com/web-infra-dev/rspack">rspack</a>, which inspired the tree shaking of Mako.</li>
<li><a href="https://github.com/oxc-project/oxc-resolver">oxc-resolver</a> by <a href="https://github.com/Boshen">@Boshen</a> which powered the resolver of Mako.</li>
<li><a href="https://github.com/oxc-project/oxc/">Oxc</a> by <a href="https://github.com/Boshen">@Boshen</a> from which We learned a lot about how to develop efficiently with Rust.</li>
<li><a href="https://github.com/biomejs/biome">biome</a> by <a href="https://github.com/ematipico">@ematipico</a> from which We learned a lot about how to develop efficiently with Rust.</li>
</ul>

      
      
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Booting Linux Off of Google Drive (372 pts)]]></title>
            <link>https://ersei.net/en/blog/fuse-root</link>
            <guid>40853770</guid>
            <pubDate>Tue, 02 Jul 2024 05:20:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ersei.net/en/blog/fuse-root">https://ersei.net/en/blog/fuse-root</a>, See on <a href="https://news.ycombinator.com/item?id=40853770">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main"><article><p>Competitiveness is a vice of mine. When I heard that a friend got Linux to <a href="https://www.kernel.org/doc/html/latest/admin-guide/nfs/nfsroot.html">boot off of NFS</a>, I had to one-up her. I had to prove that I could create something <em>harder</em>, something <em>better</em>, <em>faster</em>, <em>stronger</em>.</p><p>Like all good projects, this began with an Idea.</p><p>My mind reached out and grabbed wispy tendrils from the æther, forcing the disparate concepts to coalesce. The Mass gained weight in my hands, and a dark, swirling colour promising doom to those who gazed into it for long.</p><p>On the brink of insanity, my tattered mind unable to comprehend the twisted interplay of millennia of arcane programmer-time and the ragged screech of madness, I reached into the Mass and steeled myself to the ground lest I be pulled in, and found my <em>magnum opus</em>.</p><p>Booting Linux off of a Google Drive root.</p><h2 id="but-how">But How?<a href="#but-how" data-anchor-icon="#" aria-label="Anchor"></a></h2><p>I wanted this to remain self-contained, so I couldn't have a second machine act as a "helper". My mind went immediately to <a href="https://en.wikipedia.org/wiki/Filesystem_in_Userspace">FUSE</a>—a program that acts as a filesystem driver in userspace (with cooperation from the kernel).</p><p>I just had to get FUSE programs installed in the Linux kernel <a href="https://www.kernel.org/doc/html/latest/filesystems/ramfs-rootfs-initramfs.html">initramfs</a> and configure networking. How bad could it be?</p><h2 id="the-linux-boot-process">The Linux Boot Process<a href="#the-linux-boot-process" data-anchor-icon="#" aria-label="Anchor"></a></h2><p>The Linux boot process is, technically speaking, very funny. Allow me to pretend I understand for a moment<sup id="fnref1:1"><a href="#fn:1">1</a></sup>:</p><ol><li>The firmware (BIOS/UEFI) starts up and loads the bootloader</li><li>The bootloader loads the kernel</li><li>The kernel unpacks a temporary filesystem into RAM which has the tools to mount the real filesystem</li><li>The kernel mounts the real filesystem and switches the process to the init system running on the new filesystem</li></ol><p>As strange as the third step may seem, it's very helpful! We can mount a FUSE filesystem in that step and boot normally.</p><h2 id="a-proof-of-concept">A Proof of Concept<a href="#a-proof-of-concept" data-anchor-icon="#" aria-label="Anchor"></a></h2><p>The initramfs needs to have both network support as well as the proper FUSE binaries. Thankfully, <a href="https://github.com/dracutdevs/dracut">Dracut</a> makes it easy enough to build a custom initramfs.</p><p>I decide to build this on top of Arch Linux because it's relatively lightweight and I'm familiar with how it works, as opposed to something like Alpine.</p><pre><code><span>$</span><span> git <span>clone</span> https://github.com/dracutdevs/dracut</span>
<span>$</span><span> podman run -it --name arch -v ./dracut:/dracut docker.io/archlinux:latest bash</span></code></pre><p>In the container, I installed some packages (including the <code>linux</code> package because I need a functioning kernel), compiled <code>dracut</code> from source, and wrote a simple module script in <code>modules.d/90fuse/module-setup.sh</code>:</p><pre><code><span>#!/bin/bash</span>
<span><span>check</span></span>() {
    require_binaries fusermount fuseiso mkisofs || <span>return</span> 1
    <span>return</span> 0
}

<span><span>depends</span></span>() {
    <span>return</span> 0
}

<span><span>install</span></span>() {
    inst_multiple fusermount fuseiso mkisofs
    <span>return</span> 0
}</code></pre><p>That's it. That's all the code I had to write. Buoyed by my newfound confidence, I powered ahead, building the EFI image.</p><pre><code><span>$</span><span> ./dracut.sh --kver 6.9.6-arch1-1 \</span>
    --uefi efi_firmware/EFI/BOOT/BOOTX64.efi \
    --force -l -N --no-hostonly-cmdline \
    --modules "base bash fuse shutdown network" \
    --add-drivers "target_core_mod target_core_file e1000" \
    --kernel-cmdline "ip=dhcp rd.shell=1 console=ttyS0"
<span>$</span><span> qemu-kvm -bios ./FV/OVMF.fd -m 4G \</span>
    -drive format=raw,file=fat:rw:./efi_firmware \
    -netdev user,id=network0 -device e1000,netdev=network0 -nographic
...
...
dracut Warning: dracut: FATAL: No or empty root= argument
dracut Warning: dracut: Refusing to continue

Generating "/run/initramfs/rdsosreport.txt"
You might want to save "/run/initramfs/rdsosreport.txt" to a USB stick or /boot
after mounting them and attach it to a bug report.

To get more debug information in the report,
reboot with "rd.debug" added to the kernel command line.

Dropping to debug shell.

dracut:/#</code></pre><p><em>Hacker voice</em> I'm in. Now to enable networking and mount a test root. I have already extracted an Arch Linux root into a S3 bucket running locally, so this should be pretty easy, right? I just have to manually set up networking routes and load the drivers.</p><pre><code>dracut:/# modprobe fuse
dracut:/# modprobe e1000
dracut:/# ip link set lo up
dracut:/# ip link set eth0 up
dracut:/# dhclient eth0
dhcp: PREINIT eth0 up
dhcp: BOUND setting up eth0
dracut:/# ip route add default via 10.0.2.2 dev eth0 proto dhcp src 10.0.2.15
dracut:/# s3fs -o url=http://192.168.2.209:9000 -o use_path_request_style fuse /sysroot
dracut:/# ls /sysroot
bin   dev  home  lib64  opt   root  sbin  sys  usr
boot  etc  lib   mnt    proc  run   srv   tmp  var
dracut:/# switch_root /sysroot /sbin/init
switch_root: failed to execute /lib/systemd/systemd: Input/output error
dracut:/# ls
sh: ls: command not found</code></pre><p>Honestly, I don't know what I expected. Seems like everything is just... <em>gone</em>. Alas, not even tab completion can save me. At this point, I was stuck. I had no idea what to do. I spent days just looking around, poking at the <code>switch_root</code> source code, all for naught. Until I remembered a link <a href="https://a.exozy.me/">Anthony</a> had sent me: <a href="https://unix.stackexchange.com/questions/226872/how-to-shrink-root-filesystem-without-booting-a-livecd/227318#227318">How to shrink root filesystem without booting a livecd</a>. In there, there was a command called <code>pivot_root</code> that <code>switch_root</code> seems to call internally. Let's try that out.</p><pre><code>dracut:/# logout
...
[  430.817269] ---[ end Kernel panic - not syncing: Attempted to kill init! exitcode=0x00000100 ]---
...
dracut:/# cd /sysroot
dracut:/sysroot# mkdir oldroot
dracut:/sysroot# pivot_root . oldroot
pivot_root: failed to change root from `.' to `oldroot': Invalid argument</code></pre><p>Apparently, <code>pivot_root</code> is <a href="https://unix.stackexchange.com/a/455224">not allowed</a> to pivot roots if the root being switched is in the initramfs. Unfortunate. The Stack Exchange answer tells me to use <code>switch_root</code>, which doesn't work either. However, part of that answer sticks out to me:</p><blockquote><p>initramfs is rootfs: you can neither pivot_root rootfs, nor unmount it. Instead delete everything out of rootfs to free up the space (find -xdev / -exec rm '{}' ';'), overmount rootfs with the new root (cd /newmount; mount --move . /; chroot .), attach stdin/stdout/stderr to the new /dev/console, and exec the new init.</p></blockquote><p>Would it be possible to manually switch the root <em>without</em> a specialized system call? What if I just chroot?</p><pre><code>...
dracut:/# mount --rbind /sys /sysroot/sys
dracut:/# mount --rbind /dev /sysroot/dev
dracut:/# mount -t proc /proc /sysroot/proc
dracut:/# chroot /sysroot /sbin/init
Explicit --user argument required to run as user manager.</code></pre><p>Oh, I need to run the <code>chroot</code> command as PID 1 so Systemd can start up properly. I can actually tweak the initramfs's init script and just put my startup commands in there, and replace the <code>switch_root</code> call with <code>exec chroot /sbin/init</code>.</p><p>I put this in <code>modules.d/99base/init.sh</code> in the Dracut source after the udev rules are loaded and bypassed the <code>root</code> variable checks earlier.</p><pre><code>modprobe fuse
modprobe e1000
ip link <span>set</span> lo up
ip link <span>set</span> eth0 up
dhclient eth0
ip route add default via 10.0.2.2 dev eth0 proto dhcp src 10.0.2.15
s3fs -o url=http://192.168.2.209:9000 -o use_path_request_style fuse /sysroot
mount --rbind /sys /sysroot/sys
mount --rbind /dev /sysroot/dev
mount -t proc /proc /sysroot/proc</code></pre><p>I also added <code>exec chroot /sysroot /sbin/init</code> at the end instead of the <code>switch_root</code> command.</p><p>Rebuilding the EFI image and...</p><p><img alt="A screenshot of a Linux login screen" src="https://ersei.net/user/pages/03.blog/40.fuse-root/itworks.png"></p><p>I sit there, in front of my computer, staring. It can't have been that easy, can it? Surely, this is a profane act, and the spirit of Dennis Ritchie ought't've stopped me, right?</p><p>Nobody stopped me, so I kept going.</p><p>I log in with the very secure password <code>root</code> as <code>root</code>, and it unceremoniously drops me into a shell.</p><pre><code>[root@archlinux ~]# mount
s3fs on / type fuse.s3fs (rw,nosuid,nodev,relatime,user_id=0,group_id=0)
...
[root@archlinux ~]#</code></pre><p>At last, Linux booted off of an S3 bucket. I was compelled to share my achievement with others—all I needed was a fetch program to include in the screenshot:</p><pre><code>[root@archlinux ~]# pacman -Sy fastfetch
:: Synchronizing package databases...
 core.db failed to download
error: failed retrieving file 'core.db' from geo.mirror.pkgbuild.com : Could not resolve host: geo.mirror.pkgbuild.com
warning: fatal error from geo.mirror.pkgbuild.com, skipping for the remainder of this transaction
error: failed retrieving file 'core.db' from mirror.rackspace.com : Could not resolve host: mirror.rackspace.com
warning: fatal error from mirror.rackspace.com, skipping for the remainder of this transaction
error: failed retrieving file 'core.db' from mirror.leaseweb.net : Could not resolve host: mirror.leaseweb.net
warning: fatal error from mirror.leaseweb.net, skipping for the remainder of this transaction
error: failed to synchronize all databases (invalid url for server)
[root@archlinux ~]#</code></pre><p>Uh, seems like DNS isn't working, and I'm missing <code>dig</code> and other debugging tools.</p><p>Wait a minute! My root filesystem is on S3! I can just mount it somewhere else with functional networking, <code>chroot</code> in, and install all my utilities!</p><p>Some debugging later, it seems like systemd-resolved doesn't want to run because it <code>Failed to connect stdout to the journal socket, ignoring: Permission denied</code>. I'm not about to try to debug systemd because it's too complicated and I'm lazy, so instead I'll just use Cloudflare's.</p><pre><code>[root@archlinux ~]# echo "nameserver 1.1.1.1" &gt; /etc/resolv.conf
[root@archlinux ~]# pacman -Sy fastfetch
:: Synchronizing package databases...
 core is up to date
 extra is up to date
...
[root@archlinux ~]# fastfetch</code></pre><p><img alt="Fastfetch showing the system running in QEMU" src="https://ersei.net/user/pages/03.blog/40.fuse-root/fastfetch.png"></p><p>I look around, making sure that nobody had tried to stop me. My window was intact, my security system had not tripped, the various canaries I had set up around the house had not been touched. I was safe to continue.</p><p>I was ready to have it run on Google Drive.</p><h2 id="google-gets-involved">Google Gets Involved<a href="#google-gets-involved" data-anchor-icon="#" aria-label="Anchor"></a></h2><p>There's a project already that does Google Drive over FUSE for me already: <a href="https://github.com/astrada/google-drive-ocamlfuse">google-drive-ocamlfuse</a>. Thankfully, I have a Google account lying around that I haven't touched in years ready to go! I follow the instructions, accept the terms of service I didn't read, create all the oauth2 secrets, enable the APIs, install <code>google-drive-ocamlfuse</code> from the AUR into my Arch Linux VM, patch some <code>PKGBUILD</code>s (it's been a while), and lo and behold! I have mounted Google Drive! Mounting Drive and a few <em>very long</em><code>rsync</code> runs later, I have Arch Linux on Google Drive.</p><p>Just kidding, it's never that easy. Here's a non-exhausive list of problems I ran into:</p><ol><li>Symlinks to symlinks don't work (very important for stuff in <code>/usr/lib</code>)</li><li>Hardlinks don't work</li><li>It's so slowwwww</li><li>Relative symlinks don't work at all</li><li>No dangling symlinks (important for stuff that links to <code>/proc</code> and isn't mounted, or stuff that just hasn't copied over yet)</li><li>Symlinks outside of Google Drive don't work</li><li>Permissions don't work (neither do attributes)</li><li>Did I mention it's SLOW</li></ol><p>With how many problems there are with symlinks, I have half a mind to change the FUSE driver code to just create a file that ends in <code>.internalsymlink</code> to fix all of that, Google Drive compatibility be damned.</p><p>But, I have challenged myself to do this without modifying anything important (no kernel tweaking, no FUSE driver tweaking), so I'll just have to live with it and manually create the symlinks that <code>rsync</code> fails to make with a hacky <code>sed</code> command to the <code>rsync</code> error logs.</p><p>In the meantime, I added the token files generated from my laptop into the initramfs, as well as the Google Drive FUSE binary and SSL certificates, and tweaked a few settings<sup id="fnref1:2"><a href="#fn:2">2</a></sup> to make my life slighty easier.</p><pre><code>...
inst ./gdfuse-config /.gdfuse/default/config
inst ./gdfuse-state /.gdfuse/default/state
find /etc/ssl -<span>type</span> f -or -<span>type</span> l | <span>while</span> <span>read</span> file; <span>do</span> inst <span>"<span>$file</span>"</span>; <span>done</span>
find /etc/ca-certificates -<span>type</span> f -or -<span>type</span> l | <span>while</span> <span>read</span> file; <span>do</span> inst <span>"<span>$file</span>"</span>; <span>done</span>
...</code></pre><p><img alt="A screenshot of Google Drive showing the root of a typical Linux filesystem" src="https://ersei.net/user/pages/03.blog/40.fuse-root/google-drive-root.png"></p><p>It's nice to see that timestamps kinda work, at least. Now all that's left is to wait for the agonizingly slow boot!</p><pre><code>chroot: /sbin/init: File not found</code></pre><p>Perhaps they did not bother to stop me because they knew I would fail. </p><p>I know the file exists since, well, it <em>exists</em>, so why is it not found? Simple: Linux is kinda weird and if the binary you call depends on a library that's not found, then you'll get "File not found".</p><pre><code>dracut:/# ldd /sysroot/bin/bash
    linux-vdso.so.1 (0x00007e122b196000)
    libreadline.so.8 =&gt; /usr/lib/libreadline.so.8 (0x00007e122b01a000)
    libc.so.6 =&gt; /usr/lib/libc.so.6 (0x00007e122ae2e000)
    libncursesw.so.6 =&gt; /usr/lib/libncursesw.so.6 (0x00007e122adbf000)
    /lib64/ld-linux-x86-64.so.2 =&gt; /usr/lib64/ld-linux-x86-64.so.2 (0x00007e122b198000)</code></pre><p>However, these symlinks don't actually exist! Remember how earlier we noted that relative symlinks don't work? Well, that's come back to bite me. The Kernel is looking for files in <code>/sysroot</code> inside <code>/sysroot/sysroot</code>. Luckily, this is an easy enough fix: we just need to have <code>/sysroot</code> linked to <code>/sysroot/sysroot</code> without links:</p><pre><code>dracut:/# mkdir /sysroot/sysroot
dracut:/# mount --rbind /sysroot /sysroot/sysroot</code></pre><p>Now time to boot!</p><p>It took five minutes for Arch to rebuild the dynamic linker cache, another minute per systemd unit, and then, nothing. The startup halted in its tracks.</p><pre><code>[ TIME ] Timed out waiting for device /dev/ttyS0.
[DEPEND] Dependency failed for Serial Getty on ttyS0.</code></pre><p>Guess I have to increase the timeout and reboot. In <code>/etc/systemd/system/dev-ttyS0.device</code>, I put:</p><pre><code>[Unit]
Description=Serial device ttyS0
DefaultDependencies=no
Before=sysinit.target
JobTimeoutSec=infinity</code></pre><p>Luckily, it did not take infinite time to boot.</p><p><img alt="A Linux login prompt" src="https://ersei.net/user/pages/03.blog/40.fuse-root/gdrive-booted.png"></p><p>I'm so close to victory I can <em>taste</em> it! I just have to increase <em>another</em> timeout. I set <code>LOGIN_TIMEOUT</code> to <code>0</code> in <code>/etc/login.defs</code> in Google Drive, and tried logging in again.</p><p>Thankfully, there's a cache, so subsequent file reads aren't nearly as slow.</p><p><img alt="Fastfetch in Google Drive root, showing that the root partition is mounted as fuse.google-drive-ocaml" src="https://ersei.net/user/pages/03.blog/40.fuse-root/gdrive-fastfetch.png"></p><p>Here I am, laurel crown perched upon my head, my chimera of Linux and Google Drive lurching around.</p><p>But I'm not satisfied yet. Nobody had stopped me because they <em>want</em> me to succeed. I have to take this further. I need this to work on <em>real hardware</em>.</p><h2 id="now-do-it-on-real-hardwar">Now Do It On Real Hardware<a href="#now-do-it-on-real-hardwar" data-anchor-icon="#" aria-label="Anchor"></a></h2><p>Fortunately for me, I <a href="https://ersei.net/en/blog/updates-2024-02">switched servers</a> and now have an extra laptop with no storage just lying around! A wonderful victim<sup id="fnref1:3"><a href="#fn:3">3</a></sup> for my test!</p><p>There are a few changes I'll have to make:</p><ol><li>Use the right ethernet driver and not the default <code>e1000</code></li><li>Do not use a serial display</li><li>Change the network settings to match my house's network topology</li></ol><p>All I need is the <code>r8169</code> driver for my ethernet port, and let's throw in a <a href="https://en.wikipedia.org/wiki/Power-line_communication">Powerline</a> into the mix, because it's not going to impact the performance in any way that matters, and I don't have an ethernet cord that can reach my room.</p><p>I build the unified EFI file, throw it on a USB drive under <code>/BOOT/EFI</code>, and stick it in my old server. Despite my best attempts, I couldn't figure out what the modprobe directive is for the laptop's built-in keyboard, so I just modprobed <code>hid_usb</code> and used an external keyboard to set up networking.</p><p><img alt="A screenshot of fastfetch and mount on bare metal showing that we're booted off of Google Drive" src="https://ersei.net/user/pages/03.blog/40.fuse-root/bare-metal-gdrive.png"></p><p>This is my <em>magnum opus</em>. My Great Work. This is the mark I will leave on this planet long after I am gone: The Cloud Native Computer.</p><p>Nice thing is, I can just grab the screenshot<sup id="fnref1:screenshot"><a href="#fn:screenshot">4</a></sup> from Google Drive and put it here!</p><h2 id="woe-cloud-native-computer">Woe! Cloud Native Computer Be Upon Ye<a href="#woe-cloud-native-computer" data-anchor-icon="#" aria-label="Anchor"></a></h2><p>Despite how silly this project is, there are a few less-silly uses I can think of, like booting Linux off of <a href="https://github.com/libfuse/sshfs">SSH</a>, or perhaps booting Linux off of a Git repository and tracking every change in Git using <a href="https://wiki.archlinux.org/title/Gitfs">gitfs</a>. The possibilities are endless, despite the middling usefulness.</p><p>If there is anything I know about technology, it's that moving everything to The Cloud is the current trend. As such, I am prepared to commercialize this for any company wishing to leave their unreliable hardware storage behind and move entirely to The Cloud. Please <a href="https://ersei.net/en/contact-me">request a quote</a> if you are interested in True Cloud Native Computing.</p><p>Unfortunately, I don't know what to do next with this. Maybe I should install Nix?</p><hr><p>Thoughts? Comments? Opinions? Feel free to share (relevant) ones with me! <a href="https://ersei.net/en/contact-me">Contact me here if you want.</a></p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Switzerland mandates software source code disclosure for public sector (126 pts)]]></title>
            <link>https://joinup.ec.europa.eu/collection/open-source-observatory-osor/news/new-open-source-law-switzerland</link>
            <guid>40852084</guid>
            <pubDate>Mon, 01 Jul 2024 23:48:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://joinup.ec.europa.eu/collection/open-source-observatory-osor/news/new-open-source-law-switzerland">https://joinup.ec.europa.eu/collection/open-source-observatory-osor/news/new-open-source-law-switzerland</a>, See on <a href="https://news.ycombinator.com/item?id=40852084">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p dir="ltr">Switzerland has enacted the&nbsp;<a href="https://www.fedlex.admin.ch/eli/cc/2023/682/de">"Federal Law on the Use of Electronic Means for the Fulfilment of Governmental Tasks" (EMBAG),</a> establishing a mandatory requirement for open source software within public sector bodies. This legislative shift, championed by key figures such as Professor Dr. Matthias Stürmer, head of the Institute for Public Sector Transformation at the Bern University of Applied Sciences, signifies a paradigm shift in how governmental software development and procurement are approached.</p>
<p dir="ltr"><em>"Switzerland's new 'public money public code' law is a great opportunity for government, the IT industry and society. All stakeholders benefit from this new regulation since the public sector can reduce vendor lock-in, companies can grow their digital business solutions, and taxpayers spend less on IT solutions and receive better services due to increased competition and innovation."&nbsp; </em>Professor Dr. Matthias Stürmer</p>
<p dir="ltr">Professor Dr. Matthias Stürmer has been a pivotal advocate for this change. With a background in digital sustainability and open source community building, Stürmer has long argued for the benefits of OSS in enhancing digital transparency and reducing dependency on proprietary software. His involvement in various capacities, including his role at the Research Center for Digital Sustainability and as president of the&nbsp;<a href="https://www.ch-open.ch/en/ueber-ch-open/">open source association CH Open</a>.</p>
<p dir="ltr">The EMBAG law stipulates that all public bodies must disclose the source code of software developed by or for them, unless precluded by third-party rights or security concerns. This mandate aims to ensure greater transparency, security, and efficiency in government operations by promoting the use of OSS, which allows for public scrutiny and contribution to the software code​​.</p>
<p dir="ltr">One of the critical aspects of this law is encapsulated in Article 9, which not only mandates the disclosure of source code but also allows public bodies to offer additional services related to support, integration, or IT security, provided these services align with public tasks and are offered at a cost-covering remuneration. This provision ensures that while fostering OSS, the government can also maintain a competitive balance and avoid market distortion​​.</p>
<p dir="ltr">The journey to this legislative milestone was not without its challenges. The concept of making OSS mandatory in the public sector was initially met with resistance. Key stakeholders, including members of the Swiss Parliament and various governmental bodies, engaged in extensive debates. Concerns ranged from potential intellectual property issues to fears of compromising security. However, through persistent lobbying and advocacy, notably by the&nbsp;<a href="https://www.parldigi.ch/de/">Parliamentarian Group for Digital Sustainability (Parldigi)</a>, a consensus was reached, leading to the final compromise that forms the current EMBAG law​​.</p>
<p dir="ltr">The implementation of EMBAG is expected to serve as a model for other countries considering similar measures. The law aims to promote digital sovereignty and encourage innovation and collaboration within the public sector. As Switzerland adopts this approach, the benefits of open source software—greater security, cost efficiency, and enhanced public trust—may become more apparent.</p>
<p dir="ltr">Source:&nbsp;<a href="https://www.ti8m.com/de/blog/open-source-gesetz-schweiz">https://www.ti8m.com/de/blog/open-source-gesetz-schweiz</a>&nbsp;</p>
<p dir="ltr">Photo by <a href="https://unsplash.com/@nadine3?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Nadine Marfurt</a> on <a href="https://unsplash.com/photos/green-trees-on-mountain-under-white-clouds-during-daytime-V5XWBdjVWKA?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Unsplash</a></p>


      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Getting the World Record in Hatetris (2022) (258 pts)]]></title>
            <link>https://hallofdreams.org/posts/hatetris/</link>
            <guid>40851919</guid>
            <pubDate>Mon, 01 Jul 2024 23:26:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hallofdreams.org/posts/hatetris/">https://hallofdreams.org/posts/hatetris/</a>, See on <a href="https://news.ycombinator.com/item?id=40851919">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><em>Previous post: <a href="https://hallofdreams.org/posts/prologue-in-hatetris/">Prologue in HATETRIS</a></em></p><h2 id="tetris-that-hates-you">Tetris That Hates You</h2><p><img data-src="/assets/img/HATETRIS/Stickman_611.png" alt="" src="https://hallofdreams.org/assets/img/HATETRIS/Stickman_611.png"> <em><a href="https://stickman.qntm.org/comics.php?n=611">StickManStickMan #611</a>, by Sam Hughes.</em></p><p><a href="https://qntm.org/files/hatetris/hatetris.html">HATETRIS</a> is a version of Tetris written in 2010 by programmer and sci-fi author <a href="https://twitter.com/qntm">Sam Hughes</a>. According to his <a href="https://qntm.org/hatetris">initial description of the game</a>:</p><blockquote><p>This is bad Tetris. It’s hateful Tetris. It’s Tetris according to the evil AI from “I Have No Mouth And I Must Scream”.</p></blockquote><p>(And if you aren’t familiar with Tetris at all, and don’t know the rules or pieces, we recommend <a href="https://tetris.com/play-tetris/">trying out the original game</a> for yourself; Wikipedia has an article about how <a href="https://en.wikipedia.org/wiki/Tetris_effect">Tetris in particular can consume your life and enter your very dreams</a>, but we’re sure you’ll be fine.)</p><p>The hatred comes from the way pieces are selected. In most variants of Tetris, the piece selection is pseudorandom, with methods ranging from independently random selection to the more recent Bag Random Generator. In almost every variant that isn’t truly random, the changes to randomness are done to make the player less likely to get two pieces of the same type in a row, or to go too long without seeing a given piece.</p><p>HATETRIS selects pieces in almost precisely the opposite manner, with a one-move lookahead min-max algorithm:</p><ul><li>First, check every possible position for all seven pieces.</li><li>Second, among those positions, examine how ‘good’ each of these moves is for the player, by measuring how high the highest block in the well is.</li><li>Third, select the piece which has the worst best-case scenario for the player. If there is a tie, break the tie in the piece order <code>S, Z, O, I, L, J, T</code>.</li></ul><p>There’s no randomness involved: the first, second, and third pieces you get upon starting the game will always be the <code>S</code> piece, and so will most of the others throughout the game. There’s no <code>next piece</code> window, since the next piece isn’t decided until you finish your move. There’s no possibility at all of getting four lines at once with a well-placed <code>I</code> piece, since the game will never under any circumstances give you an <code>I</code>-piece that you could use to clear four lines. And, in general, if you’ve set up to clear a line and there’s any piece at all which denies you that line for at least a turn, that’s the piece you’re going to get.</p><p>It’s a common experience for players to try the game for the first time with a strategy perfectly sound for normal Tetris and score no points at all, simply because normal Tetris strategy amounts to setting up good situations and then waiting for the odds to be in your favor. With a deterministic algorithm like this one, the odds will <em>never</em> be in your favor.</p><h2 id="high-scores">High Scores</h2><p>The flip side of determinism is predictability. Because the algorithm will always return the same move given the same history of moves, it’s possible to plan ahead and come up with complex strategies that one could never use in a non-deterministic game - and it’s possible to share results. The first few records (Mr. Hughes’ initial game of 5 points, and commenter JeremyBowers’ and Kazanir’s claims at seven points) were lost to history, but once replay functionality was added the day after release, the comment section became a leaderboard, and anyone could take an existing replay and copy the moves to try to improve on the same strategy.</p><p>Commenter Atypical posted the first replay, an eleven-point run, and over the next month, the high score was pushed to 17, 20, 22, 28, and finally 30, all using the same opening sequence of moves to create wells where every piece can be used to clear a line, four or five times in a row. This sequence of moves was so successful, in fact, that every world record run for ten years after its discovery used it. The Atypical strategy consisted of stacking<code>S</code>-pieces to the left of the well, clearing as many lines as possible, building a flat ‘ceiling’ on top of the pieces currently in the well, and then effectively starting from scratch on top of that ceiling. By the time of the 30-point runs, the Atypical strategy was so successful that it was even done a third time, near the end of the game. So far as we know, there isn’t a term for this, so we call this a <strong>pseudoloop</strong>; you’re not getting back to the same well you had before, but you’re doing the same pattern over and over again.</p><center><video width="500" height="540" controls=""> <source src="https://hallofdreams.org/assets/img/HATETRIS/2022-08-05_30_Points_Compressed.mp4" type="video/mp4"> Your browser does not support the video tag. </video></center><p>This score of 30 points, set a month after release by the Japanese Slashdot poster Deasuke, held for the next seven years. When we started playing the game in 2013, we assumed that 30 points was as high as humans would ever get, and was probably as high as the game’s mechanics would allow. But around 2017, we started tinkering a bit with machine learning, and the question naturally came up: could a program be written to beat HATETRIS? We floated around a few ideas - including what would have been a very primitive version of a Monte-Carlo tree search - but never got around to implementation, even after commenter chromeyhex eked out another point a few months later and proved that 30 was not the maximum after all. It wasn’t until June of 2021, when commenter knewjade optimized the final few moves of the existing high score to get a score of 32, and then 34 points two days later, that we decided to start coding in earnest.</p><p>And then, a week after that, knewjade got 41 points…and did so with a somewhat different opening sequence than the one which had been used and improved upon for ten years. And a week later, he pushed it to 45. The Rust emulator was working by that point and the program could play around a hundred random games per second…but that was about all it could do, and we weren’t even close to using any machine learning yet. We breathed a slow sigh of relief as weeks went by and our own project made progress with no new world records being set, until knewjade in late August of 2021 posted a high score completely unlike any game known to exist for HATETRIS, totalling 66 points.</p><center><video width="500" height="540" controls=""> <source src="https://hallofdreams.org/assets/img/HATETRIS/2022-08-05_66_Points_Compressed.mp4" type="video/mp4"> Your browser does not support the video tag. </video></center><p><a href="https://twitter.com/1millim/status/1429774558379216907">This game is beautiful.</a> Pieces set up in clearly unclearable positions turn out to be vital to clearing a line ten moves later, and it isn’t until fifteen points in that knewjade is forced to allow even a single hole higher than the first line. There are no pseudoloops - the shape of the well is constantly changing, and the well is quite frequently piled up almost to the top of the well to then clear multiple lines one after the next. By this point, <a href="https://gist.github.com/knewjade/586c9d82bd53f13afa8bcb7a65f8bd5a">knewjade had posted an explanation of the code he used to find these new high scores</a> (and this explanation will be key to our success, later), but even without the explanation, it was very clear that there was machine search involved somewhere. The game was simply too novel, discovered too quickly, and optimized too well, to have been done by a human being unaided. So it was possible for a machine to learn HATETRIS - we just had to learn how to teach it.</p><p><img data-src="/assets/img/HATETRIS/WorldRecordGraph.png" alt="" src="https://hallofdreams.org/assets/img/HATETRIS/WorldRecordGraph.png"></p><h2 id="choosing-a-language">Choosing a Language</h2><p>For starting the project, our shortlist came down to three languages: Mathematica, python, and Rust. Each had various pros and cons:</p><ul><li><strong><em>Mathematica</em></strong>: Pros: huge amount of personal experience, huge number of useful built-in functions for machine learning and general analysis, easy to make visualizations or monitor in a dynamic way. Cons: slower than molasses in January.</li><li><strong><em>Python</em></strong>: Pros: Lots of good built-in machine learning APIs, like Tensorflow, Keras, and PyTorch. Faster than Mathematica. Cons: Still slower than compiled languages.</li><li><strong><em>Rust</em></strong>: Pros: Extremely fast. Cons: Not much in the way of built-in machine learning tools.</li></ul><p>Everything ultimately came down to speed; no matter what the plan, or what variant of machine learning we were going to do, we’d need vast quantities of data, and we’d need to get that data with a single mid-range desktop computer. And calculating the legal HATETRIS moves was going to take time; the initial implementation of the game in Javascript mentioned that the algorithm is “quite time-consuming to execute, so please forgive me if your browser chugs a little after locking each piece”. So, to get as many games and as much data as we could, we’d need every advantage, starting with the fastest language we personally knew.</p><p>As a point of reference, the first speed test we did was playing random games with an unoptimized emulator in Mathematica, and the same unoptimized emulator in Rust. Mathematica took 4.3 seconds per game on average, and Rust took 0.035 seconds per game on average. This was such a big difference that we deemed that all of the hassle and aggravation of negotiating with Rust’s borrow checker would be worth it.</p><h2 id="the-story-so-far">The Story So Far</h2><p>When we first started on this project, we had a very clear idea of what we thought a winning strategy would be: A Monte Carlo simulation, powered by Machine Learning! Just like they do in AlphaGo and the more generalized AlphaZero. Surely someone had already written an implementation and all <em>we</em> had to do was write an emulator, hook it up to a nebulous AlphaZero implementation and emerge with a world record. How long could it take? Two weeks?</p><h3 id="mcts">MCTS</h3><p>MCTS (Monte-Carlo Tree Search) is a well-trod path in gameplay simulations. Without getting into details, since the methodology behind MCTS is much better explained elsewhere, the core conceit is to make each move in a game into a tree-like structure and then explore the tree.</p><p><img data-src="/assets/img/HATETRIS/Chaslot_MCTS.png" alt="" src="https://hallofdreams.org/assets/img/HATETRIS/Chaslot_MCTS.png"> <em><a href="https://dke.maastrichtuniversity.nl/m.winands/documents/multithreadedMCTS2.pdf">Chaslot et al., 2008</a></em></p><p>Implementing a tree structure for HATETRIS positions was achievable. It was only after the initial tree implementation that we considered that there would be a <em>lot</em> of repeated wells: after all, in this game you can reach the same position in a number of different ways. A lot of deliberation and a re-factored codebase later, we opted for a <em>directed acyclic graph</em> (DAG) instead, taking identical positions and merging them into the same entry in a graph, rather than making them distinct entries on a tree. This complicated things significantly, but reduced our memory needs by an order of magnitude, at least. Refactoring from tree searches to DAG searches was more work than we’d expected to put in to the project, but was yielding promising results. An early depth first search got us scores of up to 14, with a simple greedy search looking for easy scores. At the time, the record was sitting at 34, so we felt very confident that we were on the right track.</p><p>It was sobering to realize, also, that “Graph vs Tree” in MCTS was in fact a discussion happening in professional circles. For example, <a href="https://ojs.aaai.org/index.php/ICAPS/article/view/15952/15763">this paper</a>, which we read and didn’t fully comprehend, had a very succinct explanation of how DAGs were different from trees, and why it mattered:</p><p><img data-src="/assets/img/HATETRIS/Czech_DAG.png" alt="" src="https://hallofdreams.org/assets/img/HATETRIS/Czech_DAG.png"></p><p>We highlight that we fully failed to understand these, because there’s two things we learned from this:</p><ul><li>Reading the academic papers is important, because sometimes experts have the same problems you do.</li><li>You don’t have to understand the whole paper (or most of the paper) to derive useful insights. Sometimes looking at the pictures is enough.</li></ul><p>By this point we had a working emulator and a directed acyclic graph, and were ready to get rolling.</p><h3 id="mcts--alphazero">MCTS + AlphaZero</h3><p>As it turns out, there is no “AlphaZero generic game engine that magically gets you world records” in Rust. There might be one in Python. Regardless, we could not just plug our MCTS to a magic library and hope for things to happen. Oh no. Instead, we had to build a monstrosity of layers using <code>tch</code>, a Rust-Pytorch-C binding library to make a model to train. The details of this are interesting, but not ultra-relevant to our world record run. We plan on writing a more detailed post-mortem after this blog where we dissect that at length. The main takeaways:</p><ul><li>Training a model took a long time, in the order of weeks.</li><li>Mutating hyperparameters to improve results was difficult with long runtimes.</li><li>We only had a few tens of thousands of games to train on, which made the learning extremely poor.</li><li>Weeks and weeks of iteration produced worse and worse models, some overtrained, others just terrible.</li><li>We were likely doing several things that would get us summarily exiled by real machine learning engineers.</li></ul><p>Now, we are going to intentionally gloss over the giant, annoying mess that was multithreading our MCTS, a tangled web of <code>mutexes</code>, <code>arc(mutexes)</code> and locking paths that we still haven’t cleaned the bugs out of. We even had to make our own branch of <code>tch</code> to support mutexing the learning network. In summary: We tried multithreading. It improved performance. But our models were… awful, taking two full months to get back to where the simple, greedy MCTS search had gotten us in a couple of days. And they showed no signs of ever improving.</p><p>As a desperate last maneuver, we looked to knewjade’s new shiny record (66, at this point in time). Using knewjade’s heuristic, we generated ten thousand more MCTS games (the best of which scored 20 points), fed them to the model, and let it cook for two weeks. The resulting model was somehow <em>worse</em> than our training data, scoring at most 17. Which meant our poor model was just never going to rise above its training data. Not with the meager resources we could provide it.</p><p>AlphaHATETRIS was officially dead.</p><h2 id="the-emulator">The Emulator</h2><p>So, with AlphaHATETRIS dead, what did we have to work with? Well for starters, we’d written a pretty darn good emulator. Our best version is still not the best version that exists, but it worked well for our needs. How does this emulator work, anyway?</p><h3 id="well-well-well">Well, Well, Well</h3><p>Some quick terminology, since we’ll be using the word “well” and “move” until they lose all meaning:</p><ul><li>A <strong>well</strong> is the <code>20 x 10</code> area in which the game is played, and any blocks currently in it. In general, when we talk about a well, you can think of it as a snapshot of a game in progress.</li><li>A <strong>piece</strong> is the four-block shape being maneuvered around in the well. When the piece cannot move down any further, it merges with the well and stops moving. This generates a new well.</li><li>A <strong>position</strong> refers to a legal placement of a piece within the well, be it terminal or not.</li><li>A <strong>terminal position</strong> is a piece that has reached a position where it can no longer descend, that is, it is going to become part of the well, exactly where it is. It’s terminal because once this piece is placed, we have created a new well.</li><li>A <strong>move</strong> is a placement of a piece within a well, such that it is terminal, and generates a new well. We do not consider non-terminal motion inside the well to be a ‘move’.</li></ul><p><img data-src="/assets/img/HATETRIS/basic_movement.png" width="800" src="https://hallofdreams.org/assets/img/HATETRIS/basic_movement.png"> <em>(Left: A piece, in blue, at the beginning of its move. Center: a piece at the end of its move, in a terminal position. Right: a piece, in blue, at the beginning of the next move. The previous piece has merged with the rest of the well, indicated by the grey squares.)</em></p><h3 id="basic-state-management">Basic State Management</h3><p>Our first draft was the obvious approach, considering every possible position by every possible piece, and repeating until there are no new positions left.</p><p>Move generation in this context, refers to only generating the moves for a specific well. That is, each time you get a piece, what are the places the piece could go?</p><p>Our initial version of the emulator took a well, and considered all the possible positions for the piece inside the well:</p><ul><li>First, take the initial position, and calculate the positions that could result from going left, right, up or down.</li><li>Continue calculating the left, right, up, and down motions for each new position you encounter, and remove any illegal positions which intersect a filled square, or go outside the well. Merge any duplicate positions together.</li><li>When there are no more new positions, check to see which positions aren’t illegal, but which can’t go down. This gives you the possible terminal positions for the piece: the place where the piece can rest and merge with the well.</li><li>Repeat for all seven pieces.</li><li>When you’re done, determine which piece the HATETRIS algorithm would give you, pick from that piece’s terminal positions, and with this new well, start again from step 1.</li></ul><p>It’s a pretty straightforward approach to dealing with wells, and it was fast. It could get the moves for an empty well in 1.1 ms, which was very good compared to Mathematica’s 330 ms with the same algorithm, but we knew we could do better. And we would have to, since we needed several hundred million games to train our machine learning algorithm.</p><p>The first improvement we made was to cut out considering positions that would only traverse empty space. That is, in a well where the first row is filled, but no others, only really consider the space in lines 2-5.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/HATETRIS/PotentialMovementsCombined.gif" alt=""> <em>Left: Considering all possible positions. Right: Considering only the positions in the non-empty part of the well.</em></p><p>The next logical improvement was to pre-generate the state graph. That is, instead of starting with the piece in the middle and generating positions for a left arrow, a right arrow, an up and a down, we pre-computed and cached <em>all</em> these states in the “blank” space above occupied rows, which meant we saved significant time trying to compute all these positions, at the expense of having to have a pretty large pre-cached data structure. Fortunately there are only a few thousand starting positions for pieces, which we generated programmatically. It wound up being exactly 2457 positions between all 7 pieces.</p><p><a href="#flamegraphs--attack-of-the-clones">In analyzing the performance of move detection</a>, we discovered that the majority of our time was now spent accessing the hash of positions, since whenever a new positions was explored, we had to check if it already existed. A bit of a further dig and we discovered that most of that time was spent running a hashing algorithm on the positions data for comparison. Which again, made sense…but we knew all our positions were unique among each other. We didn’t <em>need</em> to do any hashing, we could just use the position’s representation as a key directly, if we could find a reasonable way to encode it. So we replaced the hashing algorithm with our own custom version that encoded the position as a binary representation of position, rotation and piece type. This was much faster than having to hash a position, and we knew it guaranteed uniqueness. This <em>doubled</em> the speed of our move finder.</p><p>At this point, we had a speed of 300 <strong>microseconds</strong> per core. That’s a ~350% speedup over our initial speed in Rust and a 10,000% speedup over our initial speed in Mathematica. We were confident we’d optimized as much as we possibly could…and still, it wasn’t nearly fast enough. We needed something faster, since all our flamegraphs clearly showed that move calculation was a giant bottleneck in performance. So, we had to pivot.</p><h3 id="gpu-matrix-operations">GPU Matrix Operations</h3><p>The first idea we came up with was, fitting the theme of this project, a vague notion based on what we had read computers should be able to do. We could just do some matrix math in a GPU!</p><p>Our plan was to calculate the legal moves via matrix operations, and speed up those matrix operations dramatically by running them on the GPU. The idea is simple enough: there are a finite number of positions a piece can be in. So, we start with a vector <code>v</code> representing the starting position of a piece, and a matrix <code>M</code> representing all the ways a piece can move from one position to another. The vector <code>v*M</code> (the vector which is the product of multiplying <code>v</code> by <code>M</code>) will then be the vector consisting of all possible positions a piece can be in after one step. <code>v*M²</code> will be the vector consisting of all possible positions a piece can be in after two steps. Do this multiplication <code>n</code> times, and you get <code>v*M^n</code>, a vector consisting of all possible positions a piece can be in after <code>n</code> steps. Eventually, <code>v</code> will stop having new nonzero elements, and then, you’re done - the nonzero elements of <code>v</code> are the positions that the piece can reach in the given well.</p><pre><code>vCurrent = startingState
vReachable = vCurrent

while max(vCurrent) &gt; 0:
    vIntermediate = sign(v1.transitionMatrix)
    vCurrent = vIntermediate &amp; ~vReachable
    vReachable |= vIntermediate

return vReachable
</code></pre><p>There were various improvements and refinements we did with this idea. Instead of using multiplication, we used Boolean logic operators to keep all the numbers either 0 or 1; instead of a full matrix, we had four vectors representing left, right, down, and rotation movements. The math worked, and gave a modest tenfold speedup when tested in Mathematica, but we estimated that in Rust the speedup would be negligible, due to the large number of redundant operations the CPU would have to do. However, GPUs are specifically designed for a large number of simple redundant operations. If we could calculate the legal moves on a GPU, it might provide a substantial speedup.</p><pre><code>vCurrent = startingState
vReachable = vCurrent

while max(vCurrent) &gt; 0:
    vIntermediate = permute(vCurrent, leftPermutation)
    vIntermediate |= permute(vCurrent, rightPermutation)
    vIntermediate |= permute(vCurrent, upPermutation)
    vIntermediate |= permute(vCurrent, downPermutation)
    vIntermediate &amp;= vEmpty
    vCurrent = vIntermediate &amp; ~vReachable
    vReachable |= vIntermediate

return vReachable
</code></pre><p>This required us to get comfortable with difficult topics: writing code to be executed in the GPU via CUDA C and cross-compiling C code so it could run on the GPU. We’ll save the excruciating details of how that worked for the longer post, merely pointing out here that:</p><ul><li>It can be done.</li><li>Make sure you’re writing CUDA C and not C++ or plain C.</li><li>The linker is going to hate you, its not personal, you’ll just have to keep adjusting flags until it works.</li></ul><p>In the end, we decided not to go this route, since we came up with another emulator improvement which would provide roughly the same speedup without having to deal with the GPU, which to be honest, had proven quite daunting.</p><h3 id="let-me-count-the-waves">Let Me Count The Waves</h3><p>Our final approach begun by thinking of the problem differently:</p><p>First we considered this: given a specific height, there is a finite set of possible positions that fit within that height. (for example, the height 1-4). These positions are not independent of each other. That is to say, if one piece position is blocked due to a filled square, some other piece positions will be blocked as well. For a given four-line area in the grid, there is a limited subset of potential positions for a piece, considering all rotations and positions within that space. In fact, for every piece except the <code>O</code> piece, there are 34 possible positions (including rotations) in a <code>4 x 10</code> area.</p><p>A ‘wave’ can be represented as a binary number that is 34 bits, corresponding to the 34 potential positions available to a piece. The binary represents if the position is reachable for the given piece or not. That is to say, if a position is empty and reachable, the corresponding digit in the wave is 1, and if it isn’t, the corresponding digit is 0.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/HATETRIS/waveform.gif" alt=""></p><p>You also have a surface. A surface is the reachable part of a well: That is, any point which has a clear path to the empty top of the well.</p><p><img data-src="/assets/img/HATETRIS/well-surface.png" width="500" src="https://hallofdreams.org/assets/img/HATETRIS/well-surface.png"></p><p>The key is that a wave and a four-line slice of a surface, together, form a unique transition: the same wave plus the same slice of a surface will always produce the same new wave one line further down. As a result, the number of possible waves and surfaces is finite, and as we calculate moves, we can store the wave-surface transitions that we encounter in a cache, and the next time we encounter that transition, simply look it up from the cache again, rather than calculating the positions from scratch.</p><p>The final result of all this? From an original time of 330 milliseconds per move on Mathematica, to 1.1 milliseconds per move per core in Rust, to an optimized state-based emulator time of 300 microseconds per move per core, we were now down to 48 microseconds per move per core, on average. And, more importantly, finding the positions that a given piece could reach in a given well was no longer the bottleneck; any further optimization would have to be elsewhere.</p><p>But why? AlphaHATETRIS was dead, and machine learning wasn’t going to get us there. But the most recent world record holder had some answers for us.</p><p><img data-src="/assets/img/HATETRIS/Stickman_017.png" alt="" src="https://hallofdreams.org/assets/img/HATETRIS/Stickman_017.png"> <em><a href="https://stickman.qntm.org/comics.php?n=17">StickManStickMan #17</a>, by Sam Hughes.</em></p><h2 id="the-era-of-knewjade">The Era of Knewjade</h2><h3 id="the-knewjade-heuristic">The Knewjade Heuristic</h3><p>We’ve talked a lot in the past few minutes about knewjade (<a href="https://twitter.com/1millim?lang=en">Twitter</a> / <a href="https://github.com/knewjade">GitHub</a>). His heuristic. His beam searches. That’s because his work redefined how we were thinking about and approaching the problem. If nothing else, we were sure we could improve upon what he had done. To understand what we did then, it is important to understand what knewjade did to get 66 points, the highest score ever achieved at that point. More than twice what we had once considered an unbeatable 31.</p><p>Fortunately, knewjade had published his work, <a href="https://gist.github.com/knewjade/586c9d82bd53f13afa8bcb7a65f8bd5a">here</a>. While we don’t speak Japanese, Google Translate and staring at pictures helped us understand what was going on.</p><p>The knewjade approach is also known as a ‘heuristic beam search’. What does that mean? A ‘beam search’ means that you take some number of positions, get all of their children, keep the best ones, and then repeat the process until you run out of positions. For instance, if you had a beam search with a width of twenty-five million, then at every step, you take the best twenty-five million children from all of your existing wells, and use them as your wells for the next step. The ‘heuristic’ part is how you sort the children from best to worst in order to keep the best twenty-five million of them.</p><p>Fundamentally, the knewjade heuristic is a weighted sum of a few different factors. First, holes (empty squares with at least one full square above), and enclosed holes (empty squares with no path reaching to the surface). Holes and enclosed holes are bad. It makes sense that enclosed holes would be problematic: you must clear lines to get to them, and as we know, clearing lines in HATETRIS is no mean feat. Non-enclosed holes are a little more difficult to reason about, but without getting too technical, the layout of a hole can make it impossible or at least extremely challenging to clear, contributing height to the well, without giving an easy means of clearing lines.</p><p><img data-src="/assets/img/HATETRIS/enclosed-holes-knewjade.png" width="400" src="https://hallofdreams.org/assets/img/HATETRIS/enclosed-holes-knewjade.png"> <em>(In red: holes. In teal: enclosed holes.)</em></p><p>Second, the number of lines containing enclosed holes. We’ve already covered that enclosed holes are bad for clearing lines, but not all enclosed holes are created equal. A line with more than one enclosed hole is about as bad as a line with any number of enclosed holes, since you will have to clear all the lines above it regardless of how many holes there are. Thus, we care more about how many <em>lines</em> have enclosed holes, than the number of overall enclosed holes. As an example, the two left wells in the picture below would score quite differently: the one on the top is much more “clearable” than the one on the bottom, since you only have to clear one line in order to be able to access the enclosed holes instead of three.</p><p><img data-src="/assets/img/HATETRIS/knewjade-enclosed-lines.png" width="700" src="https://hallofdreams.org/assets/img/HATETRIS/knewjade-enclosed-lines.png"> <em>(Left: two wells with two enclosed holes each; the bottom left well is clearly harder to clear than the top left well. Right, lines containing at least one enclosed hole.)</em></p><p>Third, the erasability. How many different pieces can be used to erase a line in this well? The easier it is to erase a line, the easier it is to score. Erasability is good. This well happens to have a very high erasability score, since any piece can clear a line. To demonstrate:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/HATETRIS/gif-of-clearability.gif" alt=""> <em>(A well which can clear all seven pieces)</em></p><p>And fourth, the score. A higher score is good. After all, we want world records. No matter how nice we make a well, all that matters is the score when we inevitably lose.</p><p>Each of these factors was given a weight, which knewjade generated with a <a href="https://en.wikipedia.org/wiki/Genetic_algorithm">real-valued genetic algorithm</a>, and the resulting sum was the heuristic: an algorithm to evaluate any well, the higher the heuristic, the better the move.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/HATETRIS/optimized-heuristic-gif.gif" alt=""></p><p>Knewjade had several improvements on this basic concept. One improvement was realizing that, since the surface of a well can almost never move down once it’s moved up, it doesn’t matter how many holes or enclosed holes there are below the surface. All that needs to be considered is the part of the well above the lowest line of the surface. A recap: a surface is the “reachable” part of the well, that is any area that can still be reached from the top.</p><p><img data-src="/assets/img/HATETRIS/knewjade-ignore.png" width="400" src="https://hallofdreams.org/assets/img/HATETRIS/knewjade-ignore.png"></p><p>However, there is in general one exception to the ‘surfaces can never move down’ rule: when a line at the bottom of the surface can be cleared with an <code>S</code> or <code>Z</code> piece. So, knewjade’s heuristic doesn’t count holes or enclosed holes if an <code>S</code> or <code>Z</code> piece can free them.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/HATETRIS/knewjade-s-lowers.gif" alt=""></p><p>Knewjade’s approach is brilliant, interesting, and innovative, and we were confident we could copy it.</p><h3 id="following-footsteps">Following Footsteps</h3><p>So, all we had to do was implement this on our own. And as it turns out, implementing this on our own had a whole host of problems. Our calculation of the surface height took a huge amount of time, and at first, before we discovered waveforms, our emulator was extremely slow, and so calculating the erasability of a given well by calculating all of the legal moves possible for that well was extremely time-consuming. For a while, we thought we could get by without the erasability, and this was a mistake: without the erasability, we could only get a score of 41. Our personal record, for sure, but hardly 66 points.</p><p>So, we implemented erasability - but the slowdown was still a huge problem, so we made a second and much larger mistake: we took a shortcut. Rather than computing the erasability for <code>S</code>, <code>Z</code>, <code>O</code>, <code>I</code>, <code>L</code>, <code>J</code>, and <code>T</code>, every single time, we checked if <code>S</code> could be cleared, and <em>only</em> if it could be cleared checked for <code>Z</code>, and then only if <code>Z</code> could be cleared checked for <code>O</code>, and so forth. This sped up the heuristic evaluation significantly (many positions can’t be cleared by all seven pieces), but even when we improved the emulator and no longer had to worry as much about speed, we kept this shortcut in the code. And the problem with that is simple to state: if you’re setting up a well so that you can clear a line, you need to be able to clear that line with all seven pieces, and it does not matter what order you set this up in. We were throwing out perfectly good setups because the setups weren’t in our arbitrary order, and <em>we had completely forgotten that this assumption was bad</em>.</p><p>We describe this as a critical mistake, even though it only takes a few sentences to describe, and only one line of code to fix, because it was by far our most time-consuming error, more than any of the tedious debugging for the GPU or multithreading sections. We lost roughly three months due to this, after all was said and done. Knewjade’s beam search had a width of 25 million, and though he could run his in two or three days, we did not have the waveform-based emulator yet, and so our beam search would take three weeks to run. And, with no way of knowing ahead of time how well a given set of heuristic parameters would scale, that’s precisely what we did. Twice. We also had to get the parameters in the first place to test out, which (as we’ll discuss below) also took weeks, and in general we wasted a lot of time looking to improve the wrong parts of the code and not realizing that it was erasability that was limiting us.</p><p>Perhaps it’s unfair to blame this one mistake for months of wasted time, because even after we fixed the mistake, our implementation of knewjade’s heuristic was not as good as the original. We know this because knewjade was kind enough to send us his original parameters, and when we ran those parameters - in theory, using exactly the same heuristic he did - the beam search returned a score of 53, instead of the 66 he got. We still don’t know why it was that our heuristic did worse, or what the difference between his implementation and ours was. But by then, we’d come up with an additional term of our own, one which would make replicating the world record of 66 a moot point.</p><h2 id="mumble-mumble-graph-theory"><em>Mumble Mumble</em> Graph Theory</h2><p>For some months, we’d had a very interesting idea. The idea consisted of the words “graph theory”, which we’d occasionally gravely recite to each other and nod knowingly, with some vague gesticulations, and not much else. Much to our surprise, this turned out to be a workable strategy. Kind of.</p><p>While writing and making the visuals for this section, we actually discovered that the graph theory heuristic made no difference at all when used on a sufficiently wide beam search, and that we would have gotten 86 points with or without it. This was in large part because the games we mined for data weren’t representative of the kinds of moves that end up setting world records, and because we didn’t mine enough games to have good sampling rates. However, we include this section anyway, since we suspect that a properly implemented version could be a significant improvement; this heuristic by itself gets 38 points on a 25 million beam search, so setting what would have been the world record before 2021 means that there’s <em>something</em> worthwhile going on.</p><p>John Brzutowski, in his <a href="https://open.library.ubc.ca/media/stream/pdf/831/1.0079748/1">1988 master’s thesis</a>, proved that for a specific sequence of <code>S</code> and <code>Z</code> pieces, it was impossible to win the game of Tetris, and in the process got the ball rolling on making evil versions of Tetris, since it was now a fact that Tetris could, in theory, be so difficult as to be unwinnable. Part of his analysis was looking at the life cycle of “flats” - the creatures occupying individual lines in a Tetris well. These ‘flats’ are composed of blocks and empty spaces, and one could track an individual ‘flat’ from birth when it is nothing more than an empty line in the well) all the way until death (when it is completely filled, and the line goes away).</p><p><img data-src="/assets/img/HATETRIS/FlatLifeCycle.png" alt="" src="https://hallofdreams.org/assets/img/HATETRIS/FlatLifeCycle.png"></p><p>Brzutowski’s insight was that these ‘flats’ had a very limited number of behaviors each turn. On a given turn, a ‘flat’ can:</p><ul><li>Be born</li><li>Remain exactly the same.</li><li>Fall down a line if there was a cleared line below.</li><li>Grow (gain more full blocks in its line).</li><li>Die (become completely full, and vanish).</li></ul><p>So, we can take a full game, and tag each individual flat in the game, watching it as it moves through its life cycle from birth to death - or, move through its life cycle and then stop, permanently. Because, in a game of Tetris that ends, a given flat will at some point either die (get cleared) or reach a point where it never grows, falls down, or changes ever again. The key insight we had was that some line shapes are more likely to die and get cleared than others, and that graph theory could measure and predict this tendency. Essentially there are “good” lines that clear well, and “bad” lines that are really difficult to clear.</p><p>In a perfect world where we were furnished with $50,000 a day in AWS credits, we could thoroughly investigate these behaviors over four lines, but with commercial hardware, we could really only focus on one-line transitions. But how do you figure out what makes a line better than its competitors? The answer is where graph theory comes in:</p><p><img data-src="/assets/img/HATETRIS/LineTransitions.png" alt="" src="https://hallofdreams.org/assets/img/HATETRIS/LineTransitions.png"></p><p>We have in this image a complete set of all transitions for all lines (of width 4; the width 10 transition graph was too big to properly visualize, and even this is pushing it). And, with the data from the tens of thousands of MCTS games we had, it was possible to get a frequency for every transition in this graph, ranging from “vanishingly rare” to “happens almost every game”. There is one starting point – the empty line – and two ending points, not shown in this graph: a flat dying to clear a line, or a flat becoming immortal and never changing again. We could model this.</p><p>Model it how? The <a href="https://en.wikipedia.org/wiki/Ford%E2%80%93Fulkerson_algorithm">Ford-Fulkerson method</a>. Imagine the empty well as a source of water, and the two possible end states are buckets. The “water” that travels through the graph all winds up either in a “immortal” bucket or a “cleared” bucket. The frequency of the transitions in the graph, then, represents how ‘wide’ the channel is. Any flat <em>can</em> eventually be cleared, and any flat <em>can</em> become immortal, but if you have a pipe a foot wide towards one bucket and a drinking straw towards the other bucket, the amount of water in each bucket at the end will be different. What we want is to rank how good different flats are based on how much of the ‘water’ flowing through them eventually goes towards the bucket labeled ‘cleared’. This is what Ford-Fulkerson does (in broad strokes, it doesn’t actually model water), and with Mathematica’s implementation of it, we had <strong>clearability</strong>, the first part of our graph theory heuristic.</p><p>The second part was based on similar reasoning. Imagine you have what is frequently an incredible flat when it’s at the top, but the flat is now buried so deep below the surface of the well that no piece will ever reach it. No matter how easy that flat might be to clear in theory, in practice, it’ll never happen. So, we added a second component to the heuristic, <strong>permeability</strong>. We went through the tens of thousands of MCTS games again and made a second graph, this time detailing which pairs of lines had pieces go from the upper to the lower, and which didn’t.</p><p><img data-src="/assets/img/HATETRIS/Permeability.png" alt="" src="https://hallofdreams.org/assets/img/HATETRIS/Permeability.png"> <em>(Left: a well which would have a high permeability score. Right: a well which would have a low permeability score.)</em></p><p>So, the graph theory heuristic was the sum of a pair of terms. At each line, the odds that the line would ever eventually be cleared was multiplied by the odds that a piece could get down far enough to get to that line to clear it, with the empty line being by definition the most clearable of all. The higher this heuristic, the better.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/HATETRIS/gif-graph.gif" alt=""></p><h2 id="putting-it-together">Putting It Together</h2><p>What we didn’t explicitly say yet, is that these two approaches can be combined. In a very direct way, the knewjade heuristic assesses the quality of a particular well, looking at its shape, considering the piece, and generally calculating its “quality”. The graph theory heuristic, on the other hand, considers only specific lines two at a time, without any broader context of well shape. This joint approach, combining shape and line, gives a more nuanced assessment of the state of the well.</p><p>What we suspected (though it’s not something we could formally prove) is that the two pieces should balance each other out. The knewjade heuristic looks at the aggregate shapes in a well – the holes and enclosed holes, the height of the surface – but does so in a general way, with no way of (for instance) determining that this hole is less bad than that one. The graph theory heuristic is quite precise, with thousands of parameters determined from millions of positions, but only considers lines in isolation, or lines in neighboring pairs, with no ability to look at the broader context of the well. Together, ideally, they balance out each other’s weaknesses, and can find better games than either could individually.</p><p>(And as mentioned before, on a large enough scale, none of this matters; the additional effect of the graph theory heuristic term drops to zero as the beam search gets wider, and when doing a full 25 million width beam search, it does not matter if the graph theory term is there or not. But that’s not something we’d learn until many months later, and something that might be fixed by improving the data backing our graph theory approach.)</p><h3 id="parameter-optimization">Parameter Optimization</h3><p>So, with the two parts of the heuristic – knewjade’s, and ours – we were ready to go, and all we needed to know were the weights. By how much should a hole be penalized? By how much should a clearable line be rewarded? How important is score? This last question was especially difficult to answer with just intuition; on one hand, weighting score extremely highly means that the beam search will be very greedy and potentially miss better strategies that take more moves to set up, but <em>not</em> weighting scores highly leads to the algorithm putting off scoring indefinitely, always figuring “Eh, scoring would take away this nice clearable line I found!”. We needed some empirical method to figure out what all these weights should be.</p><p>What we found was a Rust library called <a href="https://github.com/nestordemeure/Simplers"><code>Simple(x)</code> Global Optimization</a>, a simpler version of the more well known <a href="https://en.wikipedia.org/wiki/Bayesian_optimization">Bayesian Optimization</a> method. This method, like all optimization methods, takes a set of variables (in our case the weights for the heuristic) and a function of that set of variables (in our case, the number of moves a beam search with those weights ran for), and attempts to find the combination of variables that results in the highest value of the function. The upside was that we didn’t have to use any intuition about which weight was more important than which other weight, because <code>Simple(x)</code> could do it all for us. The downside was that this optimization required hundreds of beam searches before we could be reasonably sure we’d tried a broad enough variety.</p><p>So, hundreds of beam searches is precisely what we did. With a beam width of 250,000, each search took somewhere between forty-five minutes to two hours, and we let this run for well over a week. Then, after the two failed 25 million width beam searches, and after we discovered <a href="#following-footsteps">the bug</a>, we did the same thing again. By the end, our 250,000 width beam searches had returned a cluster of parameter combinations which lasted for 46 points and 148 moves…and one lone parameter combination which lasted for 46 points and 147 moves, surrounded by very similar combinations of parameters which did much worse.</p><p>One of the failed 25 million beam searches had been stuck in a local maximum - the parameters had stumbled upon a game that was <em>good</em>, but that was very difficult to improve upon, like climbing to the top of K2 when you’re trying to climb to the top of Mount Everest. As such, we decided not to go for the cluster that lasted 148 moves, but go for the lone combination that lasted 147, on the hopes of not getting stuck in a local maximum once again.</p><p>And we should make explicit here that ‘hopes’ is what we were running on. We simply didn’t and don’t have the hardware to do many ultra-wide beam searches, and the optimum parameters probably change when you enlarge the width by two orders of magnitude. We both doubt very much that this set of parameters is the best possible. All we know is that it scaled up better than any other set of parameters we’d ever used.</p><div><table><thead><tr><th>Parameter</th><th>Value</th></tr></thead><tbody><tr><td>Holes</td><td>11.2106</td></tr><tr><td>Enclosed Holes</td><td>83.7646</td></tr><tr><td>Enclosed Hole Lines</td><td>83.7646</td></tr><tr><td>Surface Height</td><td>-83.7646</td></tr><tr><td>Erasability</td><td>-83.7646 (per piece)</td></tr><tr><td>Graph Heuristic</td><td>-2.1413</td></tr><tr><td>Score</td><td>-332.2211</td></tr></tbody></table></div><p>(The graph heuristic weight is included for completeness’ sake; do not try that one at home.)</p><h2 id="the-final-run">The Final Run</h2><p>It’s worth talking about what exactly we were using to run these beam searches and machine learning models and various other wild ideas, since to us it felt like every paper we read involved spending tens of thousands of dollars at your local cloud provider. We did <em>not</em> have tens of thousands of dollars to spend. We had about $150 on Azure credits, a machine we’d built a while ago with a slightly modern GPU, an i7 CPU, 16gb of RAM and a motherboard so old it wouldn’t accept any more RAM, and a couple laptops we were using for other things, and occasionally running long simulations on. No more than maybe $2,000 in hardware, being generous and counting the laptops.</p><p>Every few weeks as we’d work on the project, we’d read something about how AlphaZero was able to play 44 million games in nine hours and then run them all through neural network training powered by a fleet of thousands of TPUs, sigh wistfully, and look at our machine learning model, which needed another two weeks to finish training on a measly 10k games. Still, by the time we were running 25 million beam searches at the end it wasn’t <em>awful</em>; a beam search would take roughly four days, a vast improvement over the original 6 week runtime (the original beam searches took 3 weeks, but returned games half as long, so would have taken 6 with the better parameters). An agonizing four days, only to get scores like 53, which had once been impressive but were not exactly the world record. Finally, with our best heuristic ready, and all the graph theory <em>mumble mumble</em> was wired up, we decided we’d do it. We’d use a magical cloud machine and be done in a few hours.</p><p>We settled on a <code>c5d.18xlarge</code> instance on AWS for $3.50 an hour. The main draw was the 1800 GB SSD and the 72 available cores. With multithreading and some rough back of the napkin math it was slated to take 7 hours. A mere $24.50 for being done. And to be clear, this was our last hurrah. We were considering other ideas, but we knew this was our best shot and that if this failed, we’d likely lack the morale to go back to the drawing board and try again.</p><p>Things conspired to delay us. COVID. A stolen credit card. Random real life interrupts. Finally, the Friday of Memorial Day, we spun up the 72 core instance. We’d originally planned on running a few trials on a cheaper 16 core instance… but we opted to skip them. After all, we reasoned, if things would finish in 7 hours, we could reassess after that and see what needed to be changed. We installed Rust, copied files over, set up ssh keys… and then hit run. We’d be well within our budget, $24 being less than the $100 or so we’d budgeted for cloud computing.</p><p>An astute reader will probably guess that we did not leave and come back 7 hours later to a world record. Partially because we sat there, glued to the screen, watching games play out, and partially because something had gone terribly wrong. You see, it turns out our back of the napkin math hadn’t accounted for one thing: file reads and writes. Adding more cores made the threaded processes faster, but now the bottleneck was how quickly we could get in and out of the hard drive. And it was not encouragingly fast. 7 hours later we had achieved a score of 10 points at depth 33. Some more quick back of the napkin math suggested we needed probably another 150 moves in order to tie the world record, so another 35ish hours. 42 times $3.50 was only 147 dollars. Still within our mental budget of “no more than $200 of cloud computing”.</p><p>Over the next few hours we watched the score inch up, and the depth increase. Never getting as fast as we’d like. It was like watching water slowly, ever so slowly drip into a well, not knowing if it would ever tip over. 10. 25. 35. We still didn’t know if our heuristic could even generate a world record. Watching the score tick up and the cost do the same.</p><p>Us being us, we had no patience. Instead of watching it tick agonizingly forward, at around the 48 hour mark, when we were starting to see scores of around 63, we decided we’d cheat - we’d take one well, printed out with the output logs, and run a small 10k beam search on a laptop in order to get a lower bound for how good a game was possible. Like opening presents on Christmas Eve rather than Christmas morning, this did spoil a bit of the dramatic tension - but it also confirmed that we would, at bare minimum, get a score of 71 points, and that we would for a fact get the world record. Somehow this made the remaining ten hours of waiting worse, not better. At the 56 hour mark it finished. We’d done it. We’d discovered a 86 point game was possible, and we were only $196 in the hole. Considering the key-frame generation still had to run to give us the moves that the game had played, we might not even exceed our $250 cloud computing budget.</p><p><img data-src="/assets/img/HATETRIS/Stickman_021.png" alt="" src="https://hallofdreams.org/assets/img/HATETRIS/Stickman_021.png"> <em><a href="https://stickman.qntm.org/comics.php?n=21">StickManStickMan #21</a>, by Sam Hughes.</em></p><p>Key-frame generation? Didn’t we already have a winning game? Well, yes and no. We’d had the moves needed to play the winning game, but we also had billions of other moves that did <em>not</em> lead to the winning game. ‘Key-frame generation’ is how we reverse-engineer a game from a beam search. At each timestep, the beam search saves the top N wells (in this case, 25 million), and stores them all to disk. At the end, we take a well from the last available timestep, go through all of the wells in the previous timestep, and calculate all of their children (though we’ve since <a href="#reversing-the-polarity">figured out a better way</a>). When a well from the previous timestep has, as one of its children, the well we’re looking for, we stop, save that well, and go back another timestep to repeat the process. The process of finding each well reminded us a bit of rendering an animation by generating keyframes, hence the name.</p><p>An obvious question here: why didn’t we store the children of each well on disk, so that we wouldn’t have to recalculate the children again? The first answer is that the complete beam search already takes 225 GB of space, and storing the children would far exceed the available space on both the RAM and hard drive. The second answer is that, by the end, the emulator was fast enough that the extra time taken to read the data from disk would have been more than the time taken to recalculate the children.</p><p>56 hours in, we had a gorgeous score of 86, assuming there were no bugs in our emulator (an idea which at the time seemed dangerously possible), and all that was left was to wait for the fairly quick process of keyframe generation to finish. It was midnight, and we asked the fateful question “how long can it actually take? An hour?”. This didn’t seem unreasonable, since keyframe generation was usually by far the fastest part of the process. But, as before, we had not considered the issues of scale. The sheer amount of disk reads and the massive depth we’d reached meant that the 72 cores didn’t speed things up at all, since 90% of the time was spent on the single-core operation of reading the timestep files from disk. Some quick math showed that it was going to take us another 12 hours to finish the process. Another 42 dollars and more agonizing waiting. With nothing to be done about it, we went to bed, vowing to wake up in the morning and input the game.</p><p>We cannot emphasize enough the sheer frustration at waking up the next day, and seeing it still merrily chugging along, slowly, ever so slowly; the back of the envelope math we’d done had been based on reading files from the end of the search, which were much smaller and faster to read than the files in the bulk of the game. 68 hours in, and the accursed keyframe generator was still only gradually moving forward. Neither of us got to enjoy that Sunday at all, watching it inch forward, bit by bit. Tantalizingly close.</p><p><img data-src="/assets/img/HATETRIS/Stickman_019.png" alt="" src="https://hallofdreams.org/assets/img/HATETRIS/Stickman_019.png"> <em><a href="https://stickman.qntm.org/comics.php?n=19">StickManStickMan #19</a>, by Sam Hughes.</em></p><p>Finally, late that afternoon, it finished. We hopped on a voice call, and put the game in to the online Javascript HATETRIS game, move by move, wondering at each moment if <em>this</em> is where we’d discover a key emulator bug or bad edge case to make all our efforts be for naught. But as the well piled tall on either side, and we fumbled flips here and there, one thing became clear. It was real. The world record was ours. <a href="https://qntm.org/hatetris#komment6293f5024978c">86 points</a>.</p><center><video width="500" height="540" controls=""> <source src="https://hallofdreams.org/assets/img/HATETRIS/2022-08-05_86_Points_Compressed.mp4" type="video/mp4"> Your browser does not support the video tag. </video></center><p>We breathed a sigh of relief. It was done. It was finished. Countless hours of engineering time. Endless runs on our machines, 100 dollars over what we’d originally planned to spend on AWS. (It turned out to be $140 over after data transfer costs and such), but it was done.</p><p><img data-src="/assets/img/HATETRIS/HATETRIS_PRs.png" alt="" src="https://hallofdreams.org/assets/img/HATETRIS/HATETRIS_PRs.png"></p><h2 id="lessons-learned">Lessons Learned</h2><p>Up until now, this has been something of a narrative. There’s been a plot, there’s been progress, there’s been a clear (if non-monotonic) improvement from A to B to C. But there’s more stuff we learned, and dead ends we went down, that don’t really fit the story. We’ve captured them here for two reasons. One, we think these are valuable lessons that anyone who dives into this type of problem can benefit from. Two: we did a <em>lot</em> of work, much of which was only tangentially related to getting the world record, and while we’ll save most of that for a future blog post, we want to show some of it off now. These headings will only be loosely coupled, and while we think there’s a few related conclusions, we leave grand sweeping paradigms as an exercise for the reader.</p><h3 id="flamegraphs--attack-of-the-clones">Flamegraphs &amp; Attack of the Clones</h3><p>One lesson that came up over and over again throughout this project is that profiling is <em>very</em> important, because we as developers are bad at estimating which parts of code are bottlenecks and which aren’t. In one of the earliest iterations of the emulator, we had an error-handling exception which would format and print a string with state information in the event of a panic, to make debugging easier. We quickly fixed the underlying bug which would cause that string to print, but kept the print statement in anyway. What harm could it do?</p><p>We had no idea what the relative time cost of any of our code was at that stage, and didn’t find out until later, when we started using <strong>flamegraphs</strong> (a visual aid showing the proportion of time spent on each function call in a program) to profile our code. We had certain functions we knew for a fact were bottlenecks, so some of the graph wasn’t a surprise. What was a surprise, however, was the mountain sticking up from the surrounding foothills, in the center of the graph:</p><p><em>(Click or hover for more details)</em></p><p>As it turned out, that string format statement was taking up 17% of the total runtime, all in the event of an error that literally never happened. And that was before all of our other optimizations; if we put this string format statement back into the current emulator, it would take up well over 95% of the runtime by itself. And without code profiling, we <em>still</em> would never have guessed this.</p><p>Reasoning about runtime is <em>extremely</em> tricky. For all the coding interviews that emphasize calculating the runtime complexity of your program, there is a lot of magic that happens in coding, and the only way to really know what’s going on is to use appropriate tools to detect it. Sure, you should avoid nested for loops inside for loops, but profiling your program can give you even better results.</p><p>And by the same token, profiling also revealed that we had no need to worry about some things that concerned us. We’d spent significant amounts of time discussing how we were going to fix our various issues with using <code>.clone()</code> to get around borrow issues, and do things properly… but when we looked at the flamegraphs it turns out our 70+ clones were not costing us any time, since they were being optimized away during compilation. What we had assumed was a major time-sink was nothing to worry about at all.</p><p><img data-src="/assets/img/HATETRIS/attackclones.png" alt="" src="https://hallofdreams.org/assets/img/HATETRIS/attackclones.png"></p><h3 id="array-vs-btree">Array vs. BTree</h3><p>That said, you can’t just ignore runtime complexity, either. Our original version of the beam search took <code>N</code> wells, calculated all of the children of all of the <code>N</code> wells (typically around <code>15*N</code>), sorted the list, and then took the best <code>N</code> children in the list to use for the next step. This is all well and good for small values of <code>N</code>, but when scaling up, this quickly becomes untenable; storing all of the children at the same time (as opposed to just storing the top <code>N</code> children) wastes a tremendous amount of memory, making the beam search take up on average fifteen times more RAM than it properly should. We knew we had to change something there.</p><p>So, we took the obvious route. To summarize in pseudocode:</p><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
</pre></td><td><pre>for well in current_wells:
    for child in children(well):
        if child is better than worst_child in new_wells:
            insert child into new_wells in appropriate_position
            remove worst_child from new_wells
            get new worst_child from new_wells
</pre></td></tr></tbody></table></code></p></div><p>And therein lay the problem, though we didn’t figure it out for another month and a half. Reading an arbitrary element from the vector is <code>O(1)</code>, but insertion into the middle of a dynamically-sized vector is not an <code>O(1)</code> operation - it’s <code>O(n)</code>, scaling with the number of elements in the vector. This caused a huge nonlinear increase in runtime with respect to the beam width; had we tried to do a full 25 million width beam search at this point (and we wouldn’t have tried), it would have taken literal years to finish even with Rust’s impressive compiler magic.</p><p>We briefly considered using linked lists, <a href="https://rust-unofficial.github.io/too-many-lists/">despite well-known warnings about how tedious and difficult they could get in Rust</a>, but linked lists presented a different problem. Insertion in between two elements is nice and fast at O(1), but reading through the linked list to find out <em>where</em> to insert is O(n). This was exactly the opposite of the situation with vectors, but it was no closer to being a solution.</p><p>Upon seeing that we had two data structures, one which could read quickly but insert slowly, and the other which could read slowly but insert quickly, we thought “surely there’s some sort of compromise data structure that does both pretty well”. And sure enough, there was. Rust’s <a href="https://doc.rust-lang.org/std/collections/struct.BTreeSet.html">BTreeSet</a> was a built-in data structure based on <a href="https://en.wikipedia.org/wiki/B-tree">B-trees</a>, which have logarithmic read times and logarithmic write times. Things were slow enough already that we were willing to accept almost any constant in front of those logarithms, so we switched, and went from insertion taking up more than 90% of the beam search runtime to less than 1% instantly.</p><div><table><thead><tr><th>Data Type</th><th>Insertion Time</th><th>Element Read Time</th></tr></thead><tbody><tr><td>Vector</td><td><code>O(n)</code></td><td><code>O(1)</code></td></tr><tr><td>Linked List</td><td><code>O(1)</code></td><td><code>O(n)</code></td></tr><tr><td>B-Tree</td><td><code>O(log(n))</code></td><td><code>O(log(n))</code></td></tr></tbody></table></div><p>Changing the datastructure was the key to unlocking larger beam searches, but this would have been futile without the improvements we got from the flamegraphs, and in turn wouldn’t have worked if we’d spent all our time cleaning up clones. Writing highly efficient code is more of an artform than a science (and we await a flurry of angry tweet from our many reader over this statement). It requires a mix of the right tooling, understanding your code, and figuring out what tradeoffs make sense.</p><h3 id="machine-learning-on-the-cheap">Machine Learning (on the cheap)</h3><p>There’s another lesson we learned as we optimized our code as best we could…no amount of optimal code will eliminate the need for absurd amounts of hardware for machine learning. It doesn’t matter how good your emulator is, how blazingly fast you can play games… the sheer amount of training data and training time needed makes trying to solve problems of more than trivial complexity on consumer hardware very very challenging.</p><p>We’re not machine learning engineers. We don’t have formal backgrounds in machine learning, or in anything remotely close. Mr. Hughes generously said that we “appear to be academic researchers”, but that appearance is purely surface level. It’s possible, as it always is, that we were just doing everything wrong, and someone with a PhD in applied machine learning will show up with a model far better than any beam search we’ve designed. We wish for nothing more. We <em>want</em> user friendly machine learning for amateurs like ourselves. In fact, when we set out on this project, one of the things we wanted to prove was that AlphaZero could be used for a practical purpose without a massive computing cluster. As far as we can tell, it cannot.</p><p>The cloud costs, in TPUs and GPUs and RAM, to really get this project off the ground would have been considerable. Our models, tiny and poor as they were, still took two plus weeks to train on a very low number of games, and one of the things we’ve realized is that machine learning <em>depends</em> on having vast reams of data. It’s not sufficient to have 100,000 good games; you need tens of millions. And you need to be able to take games generated by your models and train on them. With super slow hardware, its impossible to tweak hyperparameters to figure out the best learning approach. (Which is, as far as we can tell, how the big, successful projects figure out their hyperparameters, careful guessing, and a lot of tweaking.)</p><p>Its frustrating to learn that the incredible tools that are supposed to revolutionize problem solving are out of the reach of anyone not able to throw significant cash at it. There’s a whole possible blog post on this topic. We’re <a href="https://www.gwern.net/Scaling-hypothesis">hardly the first to realize it</a>, but we felt it extremely keenly.</p><h3 id="behaviors-at-different-well-heights">Behaviors at Different Well Heights</h3><p>Since we had an emulator handy, while we were waiting to get sufficiently good heuristic parameters for the full case, we thought to explore some smaller wells, and see if there was any obvious trend in maximum score and maximum game length we could extrapolate. The primary motivation behind this was simple: before embarking on programs which would take weeks or even months to run, we wanted to prove to ourselves that knewjade’s record of 66 was beatable, and that it wasn’t simply the maximum possible score attainable in HATETRIS. So, we did a series of breadth-first search runs, up until it exceeded the RAM available on a laptop, and wrote down the results.</p><h4 id="10-blocks-across">10 Blocks Across</h4><div><table><thead><tr><th>Height</th><th>Score</th><th>Length</th><th>BFS Width</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>2</td><td>0</td><td>4</td><td>21</td></tr><tr><td>3</td><td>0</td><td>6</td><td>310</td></tr><tr><td>4</td><td>1</td><td>11</td><td>9095</td></tr><tr><td>5</td><td>4</td><td>19</td><td>174634</td></tr><tr><td>6</td><td>8</td><td>31</td><td>4848325</td></tr><tr><td>7</td><td>12</td><td>43</td><td>141514270</td></tr><tr><td>8</td><td>≥17</td><td>≥57</td><td>≥1.00e8</td></tr><tr><td>…</td><td>…</td><td>…</td><td>…</td></tr><tr><td>16</td><td>≥86</td><td>≥247</td><td>≥2.50e7</td></tr></tbody></table></div><p>This set of runs had some interesting results. Among other things, we had assumed (and we were <a href="https://twitter.com/qntm/status/1404880481657757700">not the only ones</a>) that since there were <code>10*16</code> squares in the grid, each one of which could be either filled or not filled, that there were roughly <code>2^(10*16) ≈ 10^48</code> possible HATETRIS wells. However, here, the results indicated that the number of possible wells was vastly less; the maximum width of the BFS search increased by a factor of ~30 with each additional line of height, so the maximum number of concurrent states would be somewhere around <code>10^20</code>:</p><p><img data-src="/assets/img/HATETRIS/BFS_Widths.png" alt="" src="https://hallofdreams.org/assets/img/HATETRIS/BFS_Widths.png"></p><p>For a game lasting (say) a thousand moves, this would be at most 10^23 possible wells. This was twenty-five orders of magnitude less than we expected, though sadly it was still nine orders of magnitude or so beyond what we could do with commercial brute-force and commercial hardware.</p><p>For completeness’ sake, we also examined narrower wells, to get as much of a feel for the behavior of HATETRIS with respect to well dimensions as possible. Width 4 is a special case, since an infinite loop there is actually possible in our emulator (and not possible in the newest version of HATETRIS); there’d be no point going any further. Doing BFS for wider wells might be interesting too, but our emulator currently can’t go beyond 10 blocks across, so we left that as a problem for a future date.</p><p><strong>EDIT (May 22nd, 2024)</strong>: The original values here were incorrect; this version of the emulator had a bug that only occurred when the well height was reduced; in certain circumstances, a piece which filled a line while sticking out the top of said line would be counted as a clear, rather than ending the game. That has been fixed, and these should hopefully be the correct values.</p><h4 id="8-blocks-across">8 Blocks Across</h4><div><table><thead><tr><th>Height</th><th>Score</th><th>Length</th><th>BFS Width</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>2</td><td>0</td><td>3</td><td>10</td></tr><tr><td>3</td><td>1</td><td>6</td><td>75</td></tr><tr><td>4</td><td>1</td><td>8</td><td>1172</td></tr><tr><td>5</td><td>5</td><td>17</td><td>12447</td></tr><tr><td>6</td><td>6</td><td>21</td><td>159942</td></tr><tr><td>7</td><td>9</td><td>28</td><td>2250610</td></tr><tr><td>8</td><td>11</td><td>34</td><td>31440780</td></tr></tbody></table></div><h4 id="6-blocks-across">6 Blocks Across</h4><div><table><thead><tr><th>Height</th><th>Score</th><th>Length</th><th>BFS Width</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>2</td><td>0</td><td>2</td><td>4</td></tr><tr><td>3</td><td>0</td><td>3</td><td>21</td></tr><tr><td>4</td><td>1</td><td>6</td><td>142</td></tr><tr><td>5</td><td>2</td><td>8</td><td>682</td></tr><tr><td>6</td><td>5</td><td>13</td><td>3998</td></tr><tr><td>7</td><td>6</td><td>15</td><td>23337</td></tr><tr><td>8</td><td>8</td><td>19</td><td>149389</td></tr><tr><td>9</td><td>9</td><td>23</td><td>1017165</td></tr><tr><td>10</td><td>12</td><td>28</td><td>6995425</td></tr><tr><td>11</td><td>13</td><td>32</td><td>50825005</td></tr></tbody></table></div><h4 id="4-blocks-across">4 Blocks Across</h4><div><table><thead><tr><th>Height</th><th>Score</th><th>Length</th><th>BFS Width</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>2</td><td>0</td><td>1</td><td>2</td></tr><tr><td>3</td><td>0</td><td>2</td><td>5</td></tr><tr><td>4</td><td>∞</td><td>∞</td><td>16</td></tr></tbody></table></div><h4 id="5-blocks-across">5 Blocks Across</h4><div><table><thead><tr><th>Height</th><th>Score</th><th>Length</th><th>BFS Width</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>2</td><td>0</td><td>1</td><td>3</td></tr><tr><td>3</td><td>0</td><td>2</td><td>8</td></tr><tr><td>4</td><td>1</td><td>4</td><td>41</td></tr><tr><td>5</td><td>3</td><td>7</td><td>134</td></tr><tr><td>6</td><td>4</td><td>10</td><td>543</td></tr><tr><td>7</td><td>5</td><td>12</td><td>2150</td></tr><tr><td>8</td><td>6</td><td>14</td><td>8670</td></tr><tr><td>9</td><td>7</td><td>16</td><td>35017</td></tr><tr><td>10</td><td>8</td><td>19</td><td>148656</td></tr><tr><td>11</td><td>10</td><td>22</td><td>645397</td></tr><tr><td>12</td><td>11</td><td>24</td><td>2935961</td></tr><tr><td>13</td><td>12</td><td>26</td><td>13436407</td></tr><tr><td>14</td><td>13</td><td>29</td><td>61699120</td></tr><tr><td>15</td><td>15</td><td>32</td><td>285071640</td></tr></tbody></table></div><h4 id="6-blocks-across-1">6 Blocks Across</h4><div><table><thead><tr><th>Height</th><th>Score</th><th>Length</th><th>BFS Width</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>2</td><td>0</td><td>2</td><td>4</td></tr><tr><td>3</td><td>0</td><td>3</td><td>21</td></tr><tr><td>4</td><td>1</td><td>6</td><td>139</td></tr><tr><td>5</td><td>2</td><td>8</td><td>679</td></tr><tr><td>6</td><td>4</td><td>12</td><td>3973</td></tr><tr><td>7</td><td>6</td><td>15</td><td>23126</td></tr><tr><td>8</td><td>7</td><td>19</td><td>143175</td></tr><tr><td>9</td><td>8</td><td>22</td><td>979997</td></tr><tr><td>10</td><td>11</td><td>27</td><td>6771901</td></tr><tr><td>11</td><td>12</td><td>30</td><td>48488721</td></tr><tr><td>12</td><td>15</td><td>36</td><td>362642476</td></tr></tbody></table></div><h4 id="7-blocks-across">7 Blocks Across</h4><h3 id="finding-the-bug">Finding The Bug</h3><p>Along the way, trying to come up with optimizations for this state-based emulator, we encountered an interesting bug. ‘Interesting’ because the original implementation of HATETRIS briefly <a href="https://qntm.org/hatetris#komment2605">had exactly the same bug, and had it for exactly the same reason</a>:</p><blockquote><p>Start a new game and hit “rotate” four times. Note what happens to the piece. Unlike many Tetris games, the rotation process is actually mathematical; each new piece rotates around a “point of origin” which is at the junction point of four squares. The rotation is performed by fiddling with the actual coordinates of each of the four “bits” which make up the piece. Each piece actually has a point of rotation in addition to everything else.</p></blockquote><blockquote><p>The algorithm which tests all the possible positions, actions and final resting places of each possible new piece can do anything you can do: left, right, drop, rotate. The algorithm stores a list of all of these locations and applies all possible transforms to each location in turn in order to generate a complete list. Obviously, each new location thus generated has to be compared with the whole list to make sure it is new. One of my early attempts to make the algorithm faster made it so that it only checked the locations of the four bits, not the central point of rotation.</p></blockquote><blockquote><p>However, in the case of an <code>S</code> or <code>Z</code> piece, the point of rotation is significant. <strong>Hitting “rotate” will result in a different piece depending on which way up the piece is</strong>. In Tom’s 12-point run, the algorithm moved an <code>S</code> piece to the same location as he did, and hit “rotate”, but because the piece was the other way up, resistance was encountered and nothing happened. With the piece the other way up, which is what Tom did, the rotation is successful and a line is made.</p></blockquote><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/HATETRIS/s-rotation.gif" alt=""></p><p>In other words, an <code>S</code> or <code>Z</code> piece occupying the same space can in fact be different pieces with different available positions. This means that the optimization of only considering the space a piece is taking up, rather than storing properties like ‘rotation’ and ‘position’ separately, will end up failing in some edge cases. This wouldn’t have been a huge speedup, and it’s not that important, but it was a signal to us that we were slowly but surely following the same path.</p><h3 id="echoes-of-alphazero">Echoes of AlphaZero</h3><p>The core premise of the AlphaZero neural architecture is that it is dual-head: the neural network has a policy head, and a value head, and the formula for determining which moves to investigate uses both. The value head is the simpler of the two: it takes in a position and outputs a single number ranging from -1 (meaning a predicted 100% chance of defeat) and +1 (meaning a predicted 100% chance of victory). The value head is what allows the algorithm to decide how good a position is, without having to play the entire game through to the end.</p><p><img data-src="/assets/img/HATETRIS/Stickman_691.png" alt="" src="https://hallofdreams.org/assets/img/HATETRIS/Stickman_691.png"> <em><a href="https://stickman.qntm.org/comics.php?n=691">StickManStickMan #691</a>, by Sam Hughes.</em></p><p>The policy head can be thought of (in <em>very</em> broad strokes, don’t take this too literally) as the ‘gut feeling’. If you’re a chess grandmaster, you don’t play as well as you do by looking at every possible move and counter-move that could be made - the branching factor of chess is simply too high. Instead, your gut feeling tells you to look at moves X, Y, and Z, so you do, and you don’t consider (or only briefly consider) the dozens of other possible moves available. The policy head works something like that, biasing the algorithm so that it does not consider all moves and all branches equally.</p><p>What we did, combining the overall ‘board sense’ from the knewjade heuristic and the line-by-line precision from the graph theory heuristic is very, very far removed from the original AlphaZero algorithm. Nevertheless, we couldn’t help but be reminded of the balance that the policy head and the value head were supposed to bring to each other when combined.</p><p>(This section was written long before we discovered that the graph theory heuristic made no difference; our intuition is no more perfect than the policy head’s.)</p><h3 id="reversing-the-polarity">Reversing the Polarity</h3><p>Keyframe generation took quite a while on the world record run, due to the sheer amount of time needed to read hundreds of gigabytes from disk into memory. There’s a better way to do it that didn’t occur to us until afterwards: make the emulator run backwards. That way, rather than calculating every child of every well that’s saved, simply calculate all of the possible parents of the well directly, and see which of those parents is in the previous generation’s well list.</p><p>If we really needed to speed up the process even further, we would write to a disk database rather than writing each timestep as its own file, and then when calculating the keyframes we’d only have to search the disk database for the dozen or so possible parents of the current well, probably getting the complete game in a few seconds.</p><p>Unfortunately, the initial idea of taking this idea to the extreme and calculating the entire ‘reverse game tree’ didn’t work; looking further than ten or so moves is currently very impractical, since most possible parents of a well are not reachable HATETRIS states (going back to the earlier discovery about how few reachable HATETRIS states there actually are). It’s possible to filter out some of the unreachable states (for instance, any well that has a partially filled line above a completely empty line is definitely unreachable), but we couldn’t figure out a way to filter out all of the unreachable parents, or even most of them, and so the effective branching factor is too high. If a full reverse emulator is possible, it would potentially allow for diskless beam searches, but right now it’s a bridge too far.</p><h2 id="the-next-world-record">The Next World Record</h2><p>We never bothered writing a script to take a list of wells and turn it directly into a valid <a href="https://github.com/qntm/base2048">Base2048</a> replay; it wouldn’t have been <em>that</em> difficult, but it was never important enough to actually get done. Instead, we printed out the list of wells, went through them one by one, and manually hit the arrow keys on the Javascript version of the game to move each piece where it needed to go. And along the way, we noticed a trend, one that we noticed also in knewjade’s 66 point run.</p><p><img data-src="/assets/img/HATETRIS/WellHeight.png" alt="" src="https://hallofdreams.org/assets/img/HATETRIS/WellHeight.png"></p><p>The winning game piled pieces up pretty high, and did so pretty early; it first had a piece touch the top of the well less than halfway through the game (move 120, for a game that lasted 247 moves). Looking at it, it makes sense that piling up pieces benefits the score; the HATETRIS algorithm is based on the height of the well, so maximizing the well height minimizes the amount of information the HATETRIS algorithm has at its disposal, and puts an upper bound on how evil its piece selection can be.</p><p>So by that logic, a perfect game should keep pieces piled all the way up to the top the whole time. Ours does not - our heuristic indirectly penalizes wells for being too high, because stacking pieces all the way up to the top takes time which could be used to score points and clear lines. And any well which goes too long without clearing any lines tends to get filtered out. The left-hand side of that graph is a land of opportunity, and a better heuristic than ours could probably get more than a hundred points by properly exploiting it.</p><p>Going back to the analysis on smaller wells, the length of the best possible games seems to increase approximately quadratically. Fitting a quadratic function to the results from well heights 2 through 7 and extrapolating out (and taking the result with a grain of salt), we’d expect the best possible game to last about 290 moves, which, like the previous estimate, would correspond to a best possible score of 102-103 - at the very least, indicating again that the current record is not the best possible game.</p><p><img data-src="/assets/img/HATETRIS/MaximumGameLengths.png" alt="" src="https://hallofdreams.org/assets/img/HATETRIS/MaximumGameLengths.png"></p><p>86 points is not the end of this story. It is, however, the end of this blog post.</p><h2 id="specific-thanks">Specific Thanks</h2><ul><li><a href="https://ca.linkedin.com/in/arta-seify-67793a117">Arta Seify</a>, for writing <a href="https://era.library.ualberta.ca/items/d4a0e7f0-12c5-4a88-9e79-39538bff4ce4/view/2c7e669e-07b9-4fa0-a4cc-d03800e4a11b/Seify_Arta_202001_MSc.pdf">his thesis</a> on modifying AlphaZero for single-player games, and for helping us quite a bit with implementation details when we emailed him.</li><li><a href="https://a3nm.net/blog/adversarial_tetris.html">Dr. Antoine Amarilli</a> for inspiring the small well size experiments.</li><li><a href="https://kevingal.com/">Kevin P. Galligan</a>, for providing insight into what alternate routes there were besides beam searches and MCTS to making a HATETRIS solver, and how far those alternate routes can get.</li><li>David’s dad, for suggesting <a href="https://en.wikipedia.org/wiki/B-tree">B-Trees</a> rather than linked lists or dynamic-sized arrays.</li><li><a href="https://www.youtube.com/watch?v=hF2wWRC5X9Q"> えぬ・わん</a>, for making a video analyzing the strategy used in our world record; more analysis than we did ourselves.</li><li><a href="https://qntm.org/hatetris#komment2579">Aypical</a>, <a href="http://speeddemosarchive.com/forum/index.php?topic=11523.msg323956#msg323956">SDA Guest</a>, <a href="https://qntm.org/hatetris#komment2655">Ivernis</a>, <a href="http://slashdot.jp/comments.pl?sid=493607&amp;cid=1758529">Deasuke</a>, and <a href="https://qntm.org/hatetris#komment593722b6a1beb">chromeyhex</a>, for giving HATETRIS a proper leaderboard and for each setting the bar successively higher.</li><li><a href="https://twitter.com/1millim?lang=en">Knewjade</a>, for setting forth both a <a href="https://twitter.com/1millim/status/1429774558379216907">monumental challenge</a> and the tools needed to eventually overcome it.</li><li><a href="https://qntm.org/">Sam Hughes</a>, for getting us to waste fourteen months and fourteen thousand words on Tetris. So far.</li></ul><h2 id="ps">P.S.</h2><p>Ok, so we lied about that last part being the end. If you read this far, (which is, lets face it, unlikely) you’ve heard a lot of griping, seen some things that look semi-magical, and possibly left with the impression that the authors had some sort of incredible knowledge base, or are researchers deeply entwined with the topic.</p><p>We’re not. We’re not researchers at all. We’re just two people who became obsessed with a problem and put their meager knowledge to use, beating rocks against rocks in different configurations until something resembling a spearhead came out. Our experience with Rust was, up to this point, <a href="https://hallofdreams.org/categories/advent-of-code/">six blog posts done on introductory Advent of Code problems</a> over the course of six months. We didn’t know much at the start, but we learned. First, we Googled and clicked on the first result. Then we scoured Stack Overflow and papers, and when that failed, we reached out to specialist Discord servers and emailed experts. When existing Rust crates didn’t do what we needed, we made our own Rust crates. We now know a lot of very obscure information about HATETRIS, but you could still fill volumes with things about HATETRIS that we don’t know. Many of the breakthroughs we had were due to ignorance. We pulled out the concept of waveforms because we were <em>terrified</em> that the next thing we’d have to do to push things forward was do complex matrix operations on a GPU. Inventing a new way of thinking about the problem seemed easier.</p><p>You, too, can do something like this. Find a problem. Become obsessed with it. Learn everything you can about it. Fall down dead ends. Give up, and then keep thinking about the problem at night. Have Eureka moments that lead you down other dead ends. Find a friend willing to get as obsessed as you are. Underestimate the time commitment, and then refuse to back down. Embrace the sunk cost fallacy. As long as you keep thinking about the problem, even if its in short bursts every few years, you’re still making progress. And if you never finish? If all you find are side-paths and obstacles, and it turns out the entire mission was doomed from the outset? That’s okay too. Projects like this nourish us, because there’s a part of the human mind that wants nothing more than to climb the mountain, rappel into the cave, explore the unknown and grapple with it.</p><p>So, regardless if you’re here because you care about the technical details, or because you saw HATETRIS and said “I heard of that, I think”, if you only take one lesson away from this whole thing, we hope it’s this. You too can do the thing you think is impossible or difficult.</p><p>If you fail? Write about it anyway. The original title of this post was “How Not to Get the World Record in HATETRIS”.</p><p><img data-src="/assets/img/HATETRIS/Stickman_620.png" alt="" src="https://hallofdreams.org/assets/img/HATETRIS/Stickman_620.png"> <em><a href="https://stickman.qntm.org/comics.php?n=620">StickManStickMan #620</a>, by Sam Hughes.</em></p><p><em>Next post: <a href="https://hallofdreams.org/posts/hatetris-2/">Losing the World Record in HATETRIS</a></em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Code reviews do find bugs (176 pts)]]></title>
            <link>https://two-wrongs.com/code-reviews-do-find-bugs.html</link>
            <guid>40851895</guid>
            <pubDate>Mon, 01 Jul 2024 23:23:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://two-wrongs.com/code-reviews-do-find-bugs.html">https://two-wrongs.com/code-reviews-do-find-bugs.html</a>, See on <a href="https://news.ycombinator.com/item?id=40851895">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <p>
There’s some 2015 research out of Microsoft titled <i>Code Reviews Do Not Find
Bugs</i><label for="fn.1">1</label><span><sup>1</sup> <i>Code Reviews Do Not Find Bugs; How the Current Code Review Best
Practice Slows Us Down</i>; Czerwonka, Greiler, Tilford; IEEE International
Conference on Software Engineering; 2015.</span> which seems strangely named because
reviewers <i>do</i> find bugs.
</p>

<p>
Here’s what the authors say:
</p>

<blockquote>
<p>
Contrary to the often stated primary goal of code reviews, they often do not
find functionality defects that should block a code submission. Only about 15&nbsp;%
of comments provided by reviewers indicate a possible defect, much less a
blocking defect.
</p>
</blockquote>

<p>
This is a misleading statistic, because it says nothing about the defect
detection rate. Only about 15&nbsp;% of smokers get lung cancer, but that does not
mean we can ignore smoking as a cause – smoking is the cause of more than 80&nbsp;%
of lung cancer cases. The fact that 15&nbsp;% of code review comments are about
defects is not a statement about a lack of detected defects, but rather a
statement about how reviewers <i>also</i> write a lot of comments about other things
– and we will see more about that later.
</p>

<p>
This does just not support the idea that reviewers do not find defects. For
example, if the reviews in their data had an average of four comments each, then
more than half of the reviews did get comments about defects!
</p>

<p>
But we don’t need to speculate on how many comments there were in their data,
because there has been previous research on this. We have known for a while that
code review (as well as pair programming) finds an additional 60&nbsp;% of defects
for only a 15&nbsp;% increase in time investment.<label for="fn.2">2</label><span><sup>2</sup> <i>Balancing Agility and
Discipline</i>; Boehm, Turner, Booch, Cockburn, Pyster; Addison–Wesley
Professional; 2003</span>
</p>

<p>
At this point, we also have more detailed data on the effectiveness of code
review. During the first 60 minutes of code review of the day, the reviewer
finds roughly one defect per ten minutes of reviewing – as long as they review
less than about 50 lines of code per ten minutes.<label for="fn.3">3</label><span><sup>3</sup> <i>Making Software: What
Really Works, and Why We Believe It</i>; Oram &amp; Wilson; O’Reilly Media; 2010.</span> In
other words, reviews are most effective on small chunks at a time, and mustn’t
expand to cover large fractions of the workday.
</p>

<p>
What other activity can you imagine where a single developer can uncover a
defect every 10 minutes? Certainly not through manual testing – not even all
forms of automated testing beat this rate, given that the total cost of
automated testing of a component quickly exceeds 10 minutes of time investment.
Code review is ridiculously effective. Of all the quality measures we might have
in place, code review is the last one I’d get rid of.<label for="fn.4">4</label><span><sup>4</sup> But if we produce more
than about 300 lines of code per developer and day, I would be tempted to let
the excess lines of code go unreviewed, and focus reviewing on the most
important 300 lines per developer and day.</span>
</p>

<hr>

<p>
The Microsoft authors also report on how code review is remarkably effective for
learning the codebase.
</p>

<blockquote>
<p>
The usefulness of code review comments – as judged by the author of a code
change – is positively correlated with reviewers’ experience. Without prior
exposure to the part of code base being reviewed, on average only 33&nbsp;% of any
reviewer’s comments are deemed useful by the author of a change. However,
reviewers typically learn very fast. When reviewing the same part of code base
for the third time, the usefulness ratio increases to about 67&nbsp;% of their
comments. By the fourth time, it is equivalent to the project’s long-term
average.
</p>
</blockquote>

<p>
Granted, they don’t say enough to determine whether “the project’s long-term
average” is at a comparatively low or high familiarity level, but it sounds to
me like only <i>four</i> code reviews are enough to get people relatively familiar
with a completely new part of the codebase. Assuming a reasonably-sized change
(50–100 lines of code?) and a reasonable pace of reviewing for a beginner (10–50
lines of code per 10 minutes?) this would mean about 1–2 hours of code review is
enough to get familiar with a codebase.
</p>

<p>
That’s a cheat code. Exploration and exploitation are always in conflict; for
software engineering, this means knowledge sharing and speed are in conflict. If
we put someone experienced with a component on a task, they will finish quickly
but we won’t spread knowledge about that component. If we put someone
inexperienced on the task, the task is likely to take much longer but we do get
knowledge sharing. <i>Or</i>! We ask the inexperienced person to review the code of
the experienced person. This takes virtually no time (increases development time
by 15&nbsp;%) and they become familiar with the component as a side effect. We get
both knowledge sharing and speed.<label for="fn.5">5</label><span><sup>5</sup> Not to mention that sometimes we want to
share knowledge about a component which does not need changes made to it. We can
then have people review past changes and get feedback on their reviews from an
experienced member of the team.</span>
</p>

<hr>

<p>
The main thesis of the Microsoft paper seems to be that code review is not worth
the time spent on it. We have already seen that when the review load is properly
managed, code review is highly effective both in terms of finding defects and
learning the codebase.
</p>

<p>
The Microsoft paper goes on to say other things that appear to contradict their
thesis, like
</p>

<ul>
<li><b>Feedback on long-term code maintainability covers 50&nbsp;% of code review
comments.</b> Sure, but isn’t this a good thing? Long-term code maintainability
is in my experience a large problem in organisations that don’t do peer
review.</li>
<li><b>The review is an opportunity for both author and reviewer to prove
themselves.</b> This seems like a potential motivator, and not a problem.</li>
<li><b>Code review usefulness is negatively correlated with change size.</b> This is a
feature, not a bug. It is an incentive to keep changes small and atomic.</li>
<li><b>Developers spend six hours per week reviewing.</b> This is a bit too much
(remembering the “first hour of the day” rule from before) but it should only
result in a marginal decrease in productivity – code review ought to still
appear to be a clear win.</li>
</ul>

<p>
I suspect the real problem with code review in the authors’ experience is one of
the last points they raise:
</p>

<ul>
<li><b>The median review turnaround time is 24 hours.</b> The research on code review
has been mostly done in lab settings and not in integrated on-the-job
situations, so it has not emphasised turnaround time. But having half of the
reviews take more than 24 hours seems – in my experience – like way too much.
When I did the maths on this at a previous job, the median turnaround time was
6 hours, and that was borderline painful (which was why I ran the numbers in
the first place.)</li>
</ul>

<p>
In the end, I agree with the authors that we should not blindly copy best
practises (i.e. cargo cult practices) but the reported experience in this case
does not appear to follow best practices, so before judging best practices we
should first make sure we have followed them.
</p>

<p>
If they are impractical to follow in a large organisation, that would be <a href="https://two-wrongs.com/publish-your-observations.html">an
important observation worth publishing</a>, rather than claiming they don’t work
when followed.
</p>

            </div></div>]]></description>
        </item>
    </channel>
</rss>