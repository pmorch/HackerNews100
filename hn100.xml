<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 05 May 2025 09:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Modern Latex (107 pts)]]></title>
            <link>https://github.com/mrkline/modern-latex</link>
            <guid>43892119</guid>
            <pubDate>Mon, 05 May 2025 05:18:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mrkline/modern-latex">https://github.com/mrkline/modern-latex</a>, See on <a href="https://news.ycombinator.com/item?id=43892119">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Modern LaTeX</h2><a id="user-content-modern-latex" aria-label="Permalink: Modern LaTeX" href="#modern-latex"></a></p>
<p dir="auto">LaTeX is a tool for creating beautiful writing, or a torture device that drives
users to the brink of madness every time they see bad spacing for the rest
of their lives. One of the two. Despite origins that can be traced back four
decades, it remains one of the best typesetting programs around. Many of its
guides, however, haven't aged as well. This short book will get you started with
LaTeX without bogging you down in arcana that lost its relevance back in the 90s.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Where do I get it?</h2><a id="user-content-where-do-i-get-it" aria-label="Permalink: Where do I get it?" href="#where-do-i-get-it"></a></p>
<p dir="auto">An up-to-date version should be available at
<a href="https://assets.bitbashing.io/modern-latex.pdf" rel="nofollow">https://assets.bitbashing.io/modern-latex.pdf</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How do I build it?</h2><a id="user-content-how-do-i-build-it" aria-label="Permalink: How do I build it?" href="#how-do-i-build-it"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Install LuaLaTeX, a modern, Unicode-aware version of LaTeX.
On Linux, this is usually as simple as installing your distro's TeX Live
package, e.g., <code>texlive-base</code> or <code>texlive-core</code>.
The same package should also provide the <code>latexmk</code> script.
(See below)</p>
</li>
<li>
<p dir="auto">Check out the <code>online</code> branch of the source repository,
which is optimized for digital display instead of a printed book.
Changes include even margins, centered page numbers, a lack of blank pages
between chapters, and so on.</p>
</li>
<li>
<p dir="auto">Change the fonts as-needed.</p>
<p dir="auto">The official version of this book is typeset with Garamond Premier,
Neue Haas Grotesk, URW Futura, Drive Mono, Noto, and (of course) Latin Modern.
In the likely case that you don't have all of these typefaces,
change the <code>fontspec</code> commands (e.g., <code>setmainfont</code>, etc.) appropriately,
then modify or remove the colophon at the back of the book.</p>
</li>
<li>
<p dir="auto">Build the book using</p>
<div data-snippet-clipboard-copy-content="latexmk -lualatex -latexoption=-halt-on-error modern-latex.tex"><pre><code>latexmk -lualatex -latexoption=-halt-on-error modern-latex.tex
</code></pre></div>
<p dir="auto">Note that <code>latexmk</code> will run LuaLaTeX multiple times, since
TeX generates cross references in one pass, then links them in a second.</p>
<p dir="auto">If you can't use <code>latexmk</code> for some reason, you can manually invoke</p>
<div data-snippet-clipboard-copy-content="lualatex -halt-on-error -shell-escape modern-latex.tex"><pre><code>lualatex -halt-on-error -shell-escape modern-latex.tex
</code></pre></div>
<p dir="auto">until it no longer warns,
"Label(s) may have changed. Rerun to get cross-references right."</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Feedback</h2><a id="user-content-feedback" aria-label="Permalink: Feedback" href="#feedback"></a></p>
<p dir="auto">...is welcome!
Please issue pull requests on this book's Github page,
or contact the author via matt &lt;at&gt; bitbashing.io</p>
<p dir="auto">Enjoy!</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI Meets WinDBG (101 pts)]]></title>
            <link>https://svnscha.de/posts/ai-meets-windbg/</link>
            <guid>43892096</guid>
            <pubDate>Mon, 05 May 2025 05:11:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://svnscha.de/posts/ai-meets-windbg/">https://svnscha.de/posts/ai-meets-windbg/</a>, See on <a href="https://news.ycombinator.com/item?id=43892096">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><h2 id="old-meets-new-bringing-crash-analysis-into-2025">Old Meets New: Bringing Crash Analysis into 2025</h2><p>Let's face it - while the rest of software development has evolved at warp speed, crash dump analysis feels like it's been preserved in digital amber for decades. We've got self-driving cars and pocket-sized supercomputers, yet here we are, still pecking away at command prompts like it's the dawn of the internet. Why is debugging the only area where we cling to tools that look like they belong in a computer history museum?</p><p>Picture this: You, a professional software engineer in 2025, hunched over a terminal, manually typing arcane commands like <code>!analyze -v</code> and <code>.ecxr</code>, squinting at hexadecimal memory addresses, and mentally translating stack traces. All while your friends in other industries are delegating their work to AI assistants that can write entire documents, create art, or automate complex workflows.</p><p>Something's wrong with this picture, right?</p><p><strong>What if I told you we can throw that ancient workflow into the dustbin of computing history?</strong> That's exactly what I've done. And I'm not talking about slightly better syntax highlighting or prettier UI for WinDBG. I'm talking about a fundamental transformation where you simply have a conversation with your debugger.</p><h2 id="when-inspiration-strikes">When Inspiration Strikes</h2><p>During a debugging session at work, I had one of those lightning bolt moments. What if - and stick with me here - we could apply the same AI-assisted "vibe coding" approach to crash dump analysis?</p><p>Picture this: instead of manually slogging through memory dumps and command outputs, you simply ask, "Hey, why did this application crash?" and get an intelligent, contextual answer that actually helps you solve the problem.</p><p><strong>The idea was too compelling not to pursue. So I built it.</strong></p><h2 id="see-it-in-action-ai-powered-crash-analysis">See It In Action: AI-Powered Crash Analysis</h2><p>Before diving into the technical details, let me show you what this looks like in practice. I have prepared a demo application to showcase two different use cases:</p><h3 id="video-1-crash-analysis-and-automated-bugfix">Video 1: Crash Analysis and Automated Bugfix</h3><p>In this video, I show how Copilot can analyze a crash dump, identify the bug and auto-fix the issue.</p><video controls="" src="https://svnscha.de/casts/2025-05-03-CrashDump1.webm">Your browser does not support the video tag.</video><p>As you can see in the video, instead of manually running WinDBG commands and interpreting the cryptic output, I'm having a natural conversation with GitHub Copilot. The AI quickly identifies that the application crashed, explains which specific conditions led to the crash, and suggests a fix.</p><h3 id="video-2-automated-crash-dump-analysis-of-multiple-crash-dump-files">Video 2: Automated Crash Dump Analysis of multiple crash dump files</h3><p>This video demonstrates a different capability: analyzing multiple crash dump files at once. It shows how the tool can quickly identify which dumps belong to your application and which don't.</p><video controls="" src="https://svnscha.de/casts/2025-05-03-CrashDump2.webm">Your browser does not support the video tag.</video><p>Worth noting, it takes just a few seconds until you get your first useful answer. I've played around with this for many hours and let me tell you one thing: You can really go deep. If you ask the right questions, the AI runs WinDBG/CDB commands that I haven't seen in all these years of debugging, and that is simply amazing.</p><h2 id="how-can-this-help-the-industry">How can this help the industry?</h2><p>I believe this is one of the really good examples of how AI can boost productivity. Analyzing crash dumps is a very tedious task. It begins with quickly checking and identifying whether crashes are the same or different, and often requires very advanced knowledge when a crash is challenging - really challenging.</p><p>Copilot can help here tremendously; it knows how to:</p><ul><li>Interpret assembly code (without you having to remember what EAX stands for)</li><li>Check memory contents (so you don't have to count hex bytes on your fingers)</li><li>Traverse structures with symbols (goodbye to manual pointer arithmetic!)</li><li>And so much more</li></ul><p>This is a game changer - not just for engineers, but also for support, QA, and everyone involved with crash dumps. It's like going from hunting with a stone spear to using a guided missile.</p><h2 id="how-did-i-build-this">How did I build this?</h2><p>If you've ever worked with WinDBG, you know the drill: cryptic commands, obscure syntax, and endless scrolling through memory addresses and stack traces that make your eyes glaze over. It's the kind of specialized knowledge that takes years to master and feels like speaking an alien language even when you do.</p><p>The trick here is connecting WinDBG with AI. To do that, you first need to programmatically control a debugging session, right? There are plenty of options on how to do this. I prefer to keep things simple, so I have chosen <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/debugger/debugging-using-cdb-and-ntsd">CDB</a>, which is Microsoft's Console Debugger. It operates on standard input and output, and that's so much more fun to deal with than setting up COM APIs or similar approaches.</p><p>The second part is "connecting with AI." That's where Model Context Protocol Servers come into the game.</p><h2 id="understanding-model-context-protocol-servers">Understanding Model Context Protocol Servers</h2><p>MCP is an open standard developed by Anthropic, released in November 2024. This protocol allows AI models to interact with external tools and data sources - think of it as giving AI assistants "hands" to work with other software. It defines a way for AI assistants to discover, access, and use tools through a consistent interface. In essence, it's what allows GitHub Copilot to "talk" to external programs like WinDBG.</p><p>An MCP server acts as the intermediary between the AI model and the tool. It:</p><ol><li>Registers available tools with the client</li><li>Handles requests from AI models to use these tools</li><li>Executes the tool operations and returns results</li><li>Maintains context across interactions</li></ol><p>This architecture means that any tool can be made available to AI models if someone builds an MCP server for it. And that's exactly what I did for WinDBG (CDB).</p><h3 id="why-mcp-instead-of-languagemodeltool-api">Why MCP Instead of LanguageModelTool API?</h3><p>The <a href="https://code.visualstudio.com/api/extension-guides/tools">LanguageModelTool API</a> might eventually be a better fit for this specific use-case. Creating a Visual Studio Extension that "just works" out of the box would potentially simplify the integration process significantly.</p><p>However, using MCP directly offers several notable advantages. It works with any AI model, not just limiting itself to Copilot. The server can be used outside VS Code, functioning with various other tools. New features can be easily added without necessitating changes to the core integration. Moreover, it remains platform-independent, avoiding lock-in to any single company's implementation.</p><h2 id="the-mcp-windbg-project">The MCP-WinDBG Project</h2><p>I've implemented a <a href="https://www.anthropic.com/news/model-context-protocol">Model Context Protocol</a> server that wraps WinDBG/CDB and exposes its capabilities to AI models within VS Code. Better yet, I've made it open source so everyone can experience this new workflow.</p><p>The project, called <a href="https://github.com/svnscha/mcp-windbg">mcp-windbg</a>, creates a seamless bridge between VS Code, GitHub Copilot, and the powerful analysis capabilities of WinDBG.</p><p>The actual "hard part" was implementing the CDB (Command-Line WinDBG) interaction layer. And by "hard", I mean vibe-coding with two coffees on a Saturday morning, where I spent more time being annoyed by pyTest failures than actual coding difficulties. The core implementation came together surprisingly quickly!</p><p>The rest is primarily wrapper code that implements the Model Context Protocol specifications. Now that I've established and defined the core WinDBG interaction logic, I'm considering refactoring the project to TypeScript. This would enable me to create both an MCP Server in TypeScript and a dedicated Visual Studio Extension, with both implementations leveraging the same underlying CDB interaction layer.</p><h2 id="what-does-this-mean-in-practice">What Does This Mean In Practice?</h2><p>Let me walk you through what this enables:</p><ol><li><p><strong>Natural language crash analysis</strong>: "Why is this application crashing with an access violation at this address?" (Instead of: "What the $%#@ is this heap corruption!?")</p></li><li><p><strong>Contextual debugging</strong>: "Show me the stack trace for thread 5 and explain what each function is doing based on the symbols." (Instead of staring at call stacks like they're ancient hieroglyphics)</p></li><li><p><strong>Root cause identification</strong>: "What's causing this null pointer dereference and where should I look in the code to fix it?" (Instead of playing detective with memory addresses)</p></li></ol><p>Instead of typing obscure commands like <code>!analyze -v</code> followed by a series of manual investigations, you simply ask questions in plain language, and the AI interprets the crash data for you. It's like having a WinDBG expert whispering in your ear, except it doesn't get annoyed when you ask the same question five times.</p><h2 id="how-it-works">How It Works</h2><p>The MCP server functions as a bridge between GitHub Copilot and WinDBG's powerful analysis capabilities:</p><ol><li>It provides a set of tools that Copilot can use to interact with crash dumps</li><li>It translates natural language questions into appropriate WinDBG commands</li><li>It parses and interprets the often cryptic WinDBG output into more useful information</li><li>It maintains context throughout a debugging session, enabling follow-up questions to work naturally</li></ol><p>The technical implementation uses Python to spawn and communicate with CDB (the command-line version of WinDBG), parses the output, and exposes the functionality through the Model Context Protocol to VS Code.</p><h2 id="getting-started-with-mcp-windbg">Getting Started With mcp-windbg</h2><p>Ready to try it yourself? Here's how to get started:</p><ol><li>First, make sure you have the Windows SDK installed with Debugging Tools for Windows</li><li>Clone the repository: <code>git clone https://github.com/svnscha/mcp-windbg.git</code></li><li>Set up a Python virtual environment and install the package</li><li>Configure VS Code to use the MCP server</li></ol><p>For complete details, check out the <a href="https://github.com/svnscha/mcp-windbg">repository README</a>.</p><p>Once configured, create a <code>.vscode/mcp.json</code> file in your project that points to the server:</p><pre data-lang="json"><code data-lang="json"><span>{
</span><span>    </span><span>"servers"</span><span>: {
</span><span>        </span><span>"mcp_server_windbg"</span><span>: {
</span><span>            </span><span>"type"</span><span>: </span><span>"stdio"</span><span>,
</span><span>            </span><span>"command"</span><span>: </span><span>"python"</span><span>,
</span><span>            </span><span>"args"</span><span>: [
</span><span>                </span><span>"-m"</span><span>,
</span><span>                </span><span>"mcp_server_windbg"
</span><span>            ],
</span><span>            </span><span>"env"</span><span>: {
</span><span>                </span><span>"_NT_SYMBOL_PATH"</span><span>: </span><span>"SRV*C:</span><span>\\</span><span>Symbols*https://msdl.microsoft.com/download/symbols"
</span><span>            }
</span><span>        },
</span><span>    }
</span><span>}
</span></code></pre><p>You might need to update the command, depending on where and how you have installed the mcp_server_windbg to.</p><h2 id="the-human-touch-still-matters">The Human Touch Still Matters</h2><p>Just like with <a href="https://svnscha.de/posts/vscode-vibe-coding/">code refactoring</a>, the AI assistance isn't perfect. The human element - your experience, intuition, and domain knowledge - remains crucial. Sometimes you'll need to guide the analysis, ask follow-up questions, or provide additional context.</p><p>But that's exactly what makes this approach so powerful: it combines the best of both worlds - AI's ability to quickly process and analyze large amounts of data with your human expertise in interpreting what truly matters for your specific application. Think of it as having a brilliant but occasionally confused intern who can do incredible things but sometimes needs you to point them in the right direction. "No, not that pointer... the OTHER pointer."</p><h2 id="join-the-experience">Join The Experience</h2><p>I'd love for you to try this out, contribute to the project, and share your experiences. If you're interested:</p><ol><li>Star the <a href="https://github.com/svnscha/mcp-windbg">GitHub repository</a></li><li>Try it on your own crash dumps</li><li>Report issues, suggest improvements, or contribute code</li><li>Share your success stories (or even failures - we learn from those too!)</li></ol><h2 id="the-magic-is-in-the-flow">The Magic Is In The Flow</h2><p>Just like with my code refactoring experience, the real magic isn't about any single capability - it's about the flow. When debugging stops being a tedious chore and becomes a fluid conversation, something fundamentally changes in how you approach problem-solving.</p><p>Gone are the days of dreading crash analysis. Instead, each debugging session becomes an opportunity for collaboration with an AI partner that helps you understand what's happening at a deeper level.</p><h2 id="wrapping-up">Wrapping Up</h2><p>Crash dump analysis has traditionally been one of the most technically demanding and least enjoyable parts of software development. It's like archaeology with a keyboard—painstakingly excavating through layers of memory and CPU state to unearth what went wrong. With AI assistance through tools like mcp-windbg, it becomes another area where we can experience that perfect "vibe state" of frictionless problem-solving.</p><p>If you're still manually typing WinDBG commands and squinting at memory dumps in 2025, you're not just missing out on productivity - you're missing out on a fundamentally more enjoyable way to work.</p><p>Try it. Debug it. Vibe it.</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: My AI Native Resume (122 pts)]]></title>
            <link>https://ai.jakegaylor.com/</link>
            <guid>43891245</guid>
            <pubDate>Mon, 05 May 2025 01:44:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ai.jakegaylor.com/">https://ai.jakegaylor.com/</a>, See on <a href="https://news.ycombinator.com/item?id=43891245">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>
            <section id="how-to-use">
    <h2>How to Use</h2>
    <p>AI assistants can connect to this server to learn more about me and my work.</p>
    <ul>
        <li>
            SSE (Most Common, Deprecated) <code>https://ai.jakegaylor.com/sse</code>
        </li>
        <li>
            Streamable HTTP Endpoint (Modern) <code>https://ai.jakegaylor.com/mcp</code>
        </li>
    </ul>
    <p>To connect to this server with tools that do not support remote MCP servers, you can use the following commands:</p>
    
    <div>
        <!-- Claude Tab -->
        <div id="claude-content" role="tabpanel" aria-labelledby="claude-tab">
            <figure>
                <figcaption>Add this configuration to your Claude config</figcaption>
                <pre><code>{
  "mcpServers": {
    "jake-gaylor-mcp": {
      "command": "npx",
      "args": ["-y", "@jhgaylor/me-mcp"]
    }
  }
}</code></pre>
                
            </figure>
        </div>

        <!-- Cursor Tab -->
        <div id="cursor-content" role="tabpanel" aria-labelledby="cursor-tab">
            <figure>
                <figcaption>Add this configuration to your Cursor settings</figcaption>
                <pre><code>{
  "mcpServers": {
    "jake-gaylor-mcp": {
      "command": "npx",
      "args": ["-y", "@jhgaylor/me-mcp"]
    }
  }
}</code></pre>
                
            </figure>
        </div>

        <!-- Windsurf Tab -->
        <div id="windsurf-content" role="tabpanel" aria-labelledby="windsurf-tab">
            <figure>
                <figcaption>Configure Windsurf to connect</figcaption>
                <pre><code>{
  "mcpServers": {
    "jake-gaylor-mcp": {
      "command": "npx",
      "args": ["-y", "@jhgaylor/me-mcp"]
    }
  }
}</code></pre>
                
            </figure>
        </div>

        <!-- Zed Tab -->
        <div id="zed-content" role="tabpanel" aria-labelledby="zed-tab">
            <figure>
                <figcaption>Set up Zed MCP server configuration</figcaption>
                <pre><code>{
  "context_servers": {
    "jake-gaylor-mcp": {
      "settings": {},
      "command": {
        "path": "npx",
        "args": ["-y", "@jhgaylor/me-mcp"]
      }
    }
  }
}</code></pre>
                
            </figure>
        </div>

        <!-- HTTP Tab -->
        <div id="http-content" role="tabpanel" aria-labelledby="http-tab">
                <p>For clients that support direct HTTP connections:</p>
                <ol>
                    <li>Connect to: <code>https://ai.jakegaylor.com/mcp</code></li>
                    <li>Use standard MCP protocol methods to interact with the server</li>
                    <li>No local installation required - the server is already hosted and ready to use</li>
                </ol>
            </div>

        <!-- TypeScript Tab -->
        <div id="typescript-content" role="tabpanel" aria-labelledby="typescript-tab">
            <figure>
                <figcaption>Example TypeScript connection code</figcaption>
                <pre><code>import { Client } from "@modelcontextprotocol/sdk/client/index.js";
import { StreamableHTTPClientTransport } from "@modelcontextprotocol/sdk/client/streamableHttp.js";

async function connectToMCP() {
  const baseUrl = new URL("https://ai.jakegaylor.com/mcp");
  
  try {
    const client = new Client({ name: 'example-mcp-client', version: '1.0.0' });
    const transport = new StreamableHTTPClientTransport(baseUrl);
    await client.connect(transport);
    console.log("Connected to MCP server!");
    const resumeResource = await client.getResource("candidate-info://resume-text");
    console.log("Resume:", resumeResource.contents[0].text);
    const githubUrl = await client.callTool({ toolName: "get_github_url", args: {} });
    console.log("GitHub URL:", githubUrl.content[0].text);
  } catch (error) {
    console.error("Failed to connect to MCP server:", error);
  }
}
connectToMCP();</code></pre>
                
            </figure>
        </div>
    </div>

 
    
    <p><strong>Looking for the quickest way to learn about me?</strong> The section below contains my complete resume information that you can copy with one click and paste directly to any AI assistant. No tools or special connections needed - this is the simplest way for an AI to get comprehensive information about my background.</p>

    <!-- Resume Information for Direct Pasting -->
    <details id="quick-resume">
  <summary><span>Quick Resume Information (copy-paste to your LLM)</span></summary>
  <div>
        <p>Copy the text below and paste it directly to your AI assistant to give it information about my background:</p>
        <div>
            <pre># Jake Gaylor
                
## Professional Summary
Jake Gaylor is an experienced software engineer with nearly 15 years of experience working across the web stack. He specializes in cloud infrastructure, DevOps, and platform engineering with particular expertise in Kubernetes, GitOps, and immutable infrastructure. Jake has strong experience with AWS, containerization technologies, and building CI/CD pipelines.

## Current Position
- Staff SWE, Office of the CEO at Cloaked Inc (August 2023-Present)
  - Creating programs to ingest ideas from across the company
  - Working with executives, product management, and marketing to develop marketable ideas
  - Documenting ideation processes

## Previous Experience
- Staff Platform Engineer at Cloaked Inc (September 2022-August 2023)
  - Migrated software stacks from PaaS providers to AWS EKS
  - Implemented Cloudflare for third-party masking and control
  - Established deployment patterns for Kubernetes and edge networks
  - Migrated AWS account structure to meet compliance requirements (SOC2, ISO 27001/27701)

- Owner/Co-GM at The Onward Store Steakhouse (October 2021-January 2024)
  - Managed physical infrastructure, HR, legal, vendor relationships
  - Developed processes for serving fresh food

- Staff SWE in Developer Experience at Inception Health (June 2020-June 2022)
  - Built AWS infrastructure with CDK packages for HIPAA compliance
  - Trained teams on serverless environments and gRPC contracts

- Senior Platform Engineer at CyberGRX (December 2018-June 2020)
  - Led cloud-native CI/CD pipelines for AWS and Kubernetes
  - Developed custom CoreOS operator for blue/green deployments
  - Mentored engineers on software deployment best practices

- Senior Cloud Platform Engineer at CardFree (January 2018-December 2018)
  - Managed PCI-compliant C# installation in AWS
  - Established software building practices and incident response

- Senior DevOps Engineer at ProtectWise, Inc (August 2016-January 2018)
  - SRE for large-scale data ingestion platform with thousands of Cassandra nodes
  - Managed $10M+/year AWS production environment using Infrastructure as Code

- DevOps Engineer at Food Service Warehouse (June 2015-March 2016)
  - Built Kubernetes clusters on vSphere using CoreOS, fleet, etcd, flannel

- Software Developer at Mississippi State University (June 2010-June 2014)
  - Created grant proposals for custom software
  - Supported research initiatives including tracking "human sensors" during natural disasters

## Technical Skills
- Programming Languages: JavaScript, Python, Bash, Go, Ruby
- Databases: Neo4j, PostgreSQL, Timestream, MongoDB, Redis, Etcd, Zookeeper, Cassandra, DynamoDB
- Distributed Systems: Argo Workflows, Kafka, RabbitMQ, AWS IoT
- Automation: Argo CD, Packer, Terraform, Vagrant, Linux, Chef, GitLab CI, Jenkins, GitHub Actions
- Orchestration: Kubernetes, Fleet, Docker
- Monitoring: Prometheus, CloudWatch, Sensu, PagerDuty, Logstash, Kibana, Grafana, Graphite

## Professional Philosophy
Jake focuses on:
- Shipping fast and learning fast
- Delivering MVP in days rather than months
- Data-driven iterations and continuous user feedback
- Supercharging teams with best-in-class development workflows
- Automated testing and deployment
- Finding product-market fit through data-driven feature development

## Entrepreneurial Background
Jake describes himself as a "serial entrepreneur" having worked with:
- SaaS products
- PaaS solutions
- Physical products
- Restaurant management

## Contact Information
- Phone: 720.453.3994
- Email: jhgaylor@gmail.com
- Website: http://jakegaylor.com
- GitHub: https://github.com/jhgaylor
- LinkedIn: https://linkedin.com/in/jhgaylor
- Work Preference: Remote First, Will Travel</pre>
            </div>
        <p>You can also access this information at <a href="https://ai.jakegaylor.com/llms.txt" target="_blank" rel="noopener">llms.txt</a> or via <code>https://ai.jakegaylor.com/llms.txt</code></p>
    </div>
</details> 
</section> 
            <section id="for-evaluators">
    <h2>For Talent Evaluators</h2>
    <div>
        <p>I've built this MCP server to help a wide range of people evaluate whether I might be a good fit for their needs:</p>
        <ul>
            <li><strong>Recruiters:</strong> Ask about my experience, skills, and background directly.</li>
            <li><strong>Hiring managers:</strong> Drill into my proficiency in specific technologies. <span>Example: "What's Jake's proficiency level with React, TypeScript, and AWS?"</span></li>
            <li><strong>Interview coordinators:</strong> Generate tailored phone screens and take-home questions. <span>Example: "Create a phone screen for Jake around API design and testing."</span></li>
            <li><strong>Career coaches:</strong> Pull my resume, GitHub stats, and website content for assessments. <span>Example: "Generate a summary of Jake's career highlights."</span></li>
            <li><strong>Anyone evaluating my fit:</strong> Use the prompt: <span>"Is Jake a good fit for my role? [paste job description]"</span></li>
            <li><strong>Product managers &amp; designers:</strong> Understand how I collaborate on product vision and feature prioritization. <span>Example: "How would Jake contribute to our feature roadmap development?"</span></li>
            <li><strong>Startup founders &amp; small teams:</strong> Assess my fit as a generalist in lean environments. <span>Example: "Would Jake be a good fit for a full-stack generalist role at an early-stage startup?"</span></li>
        </ul>
        <p>💡 <strong>Pro Tip:</strong> Connect your AI assistant to this MCP server using the instructions above, then have it analyze my experience against your specific requirements to streamline your evaluation process.</p>
    </div>

    <h3>Get a Complete Candidate Assessment</h3>
    <div>
        <p>Once connected, your AI assistant can provide a thorough evaluation of my candidacy:</p>
        
        <div>
            
            
            
<div>
    <h4>Skills Assessment</h4>
    <p>Ask about specific technologies or skills mentioned in your job description and get an analysis of my experience level with each.</p>
    
    <p>Example: "What is Jake's proficiency level with React, TypeScript, and AWS?"</p>
    
    
</div>

            
            
<div>
    <h4>Business Impact</h4>
    <p>Understand how my background could translate to impact in your organization based on my previous roles and accomplishments.</p>
    
    <p>Example: "How could Jake contribute to our DORA metrics optimization efforts?"</p>
    
    
</div>

            
            
<div>
    <h4>Strengths &amp; Growth Areas</h4>
    <p>Get insights into which aspects of your role I would likely excel at, and where I might need support or development.</p>
    
    <p>Example: "Based on our staff developer role, what are Jake's strengths and potential growth areas?"</p>
    
    
</div>

            
            
<div>
    <h4>Phone Screen Generator</h4>
    <p>Generate customized interview questions that focus on the specific unknowns or areas you want to explore further.</p>
    
    <p>Example: "Create a phone screen for Jake focused on ai engineering, software delivery, and leadership skills."</p>
    
    
</div>

        </div>
    </div>

    
    
    
<div>
    <h3>Take Action</h3>
    <div>
        <p>If you're using the hosted version at <code>ai.jakegaylor.com/mcp</code>, you can have your AI assistant contact me directly:</p>
        
        
        <p>The assistant can compose an email based on your role and my qualifications, which you can review and send. This helps create a personalized outreach that highlights the specific reasons why your opportunity might align with my background.</p>
        <p>Example: Ask your assistant: "Please draft an email to Jake about our [position name] role and why he might be a good fit."</p>
    
    </div>
</div>

    
    
<div>
    <h3>Technical Deep Dive</h3>
    <div>
        <p>Walk through core technologies in your stack, explore my project work via the GitHub MCP server, and discuss design trade-offs:</p>
        
        <p>Example: "Walk me through Jake's StatBot project. What is it? How does it work?"</p>
        
        
    </div>
</div>

    
    
<div>
    <h3>Onboarding &amp; Ramp Roadmap</h3>
    <div>
        <p>Generate a 30-60-90-day plan based on my background and your team's goals:</p>
        
        <p>Example: "Create a 90-day onboarding roadmap for Jake joining as a staff platform engineer."</p>
        
        
    </div>
</div>

    
    
<div>
    <h3>Growth &amp; Development Plan</h3>
    <div>
        <p>Identify skill gaps and suggest learning paths or stretch projects:</p>
        
        <p>Example: "What areas should Jake focus on to become our next engineering manager, and what projects could we assign him?"</p>
        
        
    </div>
</div>

    
    
<div>
    <h3>Reference Simulation</h3>
    <div>
        <p>Auto-generate reference-check questions or simulate testimonial responses:</p>
        
        <p>Example: "Create three reference-check questions about Jake's leadership and history of delivering results."</p>
        
        
    </div>
</div>

</section> 
            <section id="mcp-documentation">
    <h2>What is MCP</h2>
    <p>Model Context Protocol (MCP) is a standardized way for AI models to communicate with external services to request information or execute actions, enabling them to stay up-to-date and perform tasks beyond their training data.</p>
    <div>
        <p>This MCP server provides two key capabilities:</p>
        <ul>
            <li><strong>Resources:</strong> Static information about me, including my resume, LinkedIn profile, GitHub profile, and personal website content.</li>
            <li><strong>Tools:</strong> Functions that AI models can call to retrieve specific information about my background, skills, and experience.</li>
        </ul>
        <p>By connecting to this MCP server, AI assistants can access up-to-date information about me.</p>
        <p>💡 <strong>Note:</strong> This server follows the <a href="https://modelcontextprotocol.io/" target="_blank">Model Context Protocol</a> specification, making it compatible with various AI assistants and tools that support MCP.</p>
    </div>
    
    <ul>
        <li><strong>Resources</strong>: Static data my application exposes and that you or your assistant can fetch directly (e.g., resume text, website content).</li>
        <li><strong>Tools</strong>: Callable actions that AI models can invoke to retrieve information or perform tasks (e.g., <code>get_resume_text</code>, <code>get_github_url</code>).</li>
    </ul>

    <h3>Features</h3>
    <p>This MCP server provides the following features:</p>
    
    <h4>Resources</h4>
<div>
    <details id="resource-resume-text">
  <summary><code>candidate-info://resume-text</code>: Resume content as text</summary>
  <div>
    
        <p>{
  "meta": {
    "themeOptions": {
      "colors": {
        "background": [
          "#ffffff",
          "#191e23"
        ],
        "dimmed": [
          "#f3f4f5",
          "#23282d"
        ],
        "primary": [
          "#191e23",
          "#fbfbfc"
        ],
        "secondary": [
          "#6c7781",
          "#ccd0d4"
        ],
        "accent": [
          "#46b450",
          "#47d153"
        ]
      }
    },
    "theme": "even"
  },
  "basics": {
    "name": "Jake Gaylor",
    "label": "Senior Software Engineer &amp; Product Founder | End-to-End Product Development &amp; Go-to-Market",
    "image": "",
    "email": "jhgaylor@gmail.com",
    "phone": "720.453.3994",
    "url": "https://jakegaylor.com",
    "summary": "15+ years building and shipping products at scale—from founding SaaS ventures to architecting AI-powered platforms. I combine deep SWE expertise with business leadership to drive product vision, go-to-market strategy, and revenue growth.",
    "location": {
      "address": "",
      "postalCode": "",
      "city": "",
      "countryCode": "US",
      "region": "Remote First, Will Travel"
    },
    "profiles": [
      {
        "network": "GitHub",
        "username": "jhgaylor",
        "url": "https://github.com/jhgaylor"
      },
      {
        "network": "LinkedIn",
        "username": "jhgaylor",
        "url": "https://www.linkedin.com/in/jhgaylor/"
      }
    ]
  },
  "work": [
    {
      "name": "Cloaked Inc",
      "position": "Staff SWE, Office of the CEO",
      "url": "https://cloaked.app",
      "startDate": "2023-08",
      "summary": "Spearheaded executive-level innovation by developing autonomous AI agents and a cross-functional ideation portal—driving a highly sticky, AI-powered onboarding experience that boosted user adoption and retention and defined governance processes",
      "highlights": [
        "Developed autonomous AI agents using LangChain to automate user account management tasks such as auto change password, auto login, and auto change email",
        "Leveraged these AI agents as a core fundraising narrative and created a highly sticky, AI-powered onboarding experience that significantly boosted user adoption and retention",
        "Designed and launched a cross-functional ideation portal engaging design, customer success, engineering, product development, and executive teams with transparent status tracking",
        "Partnered with design, product, engineering, marketing, customer success, and co-founders to validate, refine, and ship concepts into market-ready features",
        "Defined and documented cross-functional roles, ceremonies, and governance processes to unify ideation and accelerate product delivery across the organization"
      ]
    },
    {
      "name": "Cloaked Inc",
      "position": "Staff Platform Engineer",
      "url": "https://cloaked.app",
      "startDate": "2022-09",
      "endDate": "2023-08",
      "summary": "Oversaw strategic AWS EKS migration and architected enterprise deployment governance, compliance frameworks, and GitOps CI/CD pipelines to enhance security, scalability, and developer productivity",
      "highlights": [
        "Defined and executed a strategic migration roadmap transitioning legacy PaaS platforms to AWS EKS, driving cost reduction and scalability",
        "Collaborated with cross-functional leadership to implement a Cloudflare control plane, enhancing security posture and resilience",
        "Architected and led adoption of a multi-account AWS hub-and-spoke model, ensuring SOC2, ISO 27001, and 27701 compliance",
        "Established enterprise Kubernetes and edge deployment standards, mentoring engineering teams in best practices and governance",
        "Spearheaded design and implementation of a GitOps-based CI/CD pipeline, accelerating deployments by 30x",
        "Partnered with product and analytics teams to develop an end-to-end analytics stack, enabling data-driven decision-making"
      ]
    },
    {
      "name": "The Onward Store Steakhouse",
      "position": "Owner / Co-GM",
      "startDate": "2021-10",
      "endDate": "2024-01",
      "summary": "Owned and managed a full-service steakhouse with 7 staff, generating $500K in annual revenue and overseeing P&amp;L, operations, and vendor relations.",
      "highlights": [
        "Led a team of 7 employees, handling recruitment, training, scheduling, and performance management.",
        "Directed all facets of business operations, achieving $500K in annual revenue through strategic inventory management and cost control.",
        "Designed and implemented operational processes for facility maintenance, vendor partnerships, and quality assurance, driving consistent customer satisfaction.",
        "Managed P&amp;L, budgets, and financial reporting, optimizing expenses and improving profit margins.",
        "Executed marketing and community engagement initiatives to increase local brand awareness and repeat patronage."
      ]
    },
    {
      "name": "Inception Health",
      "position": "Staff SWE in Developer Experience",
      "startDate": "2020-06",
      "endDate": "2022-06",
      "summary": "Architected AWS CDK infrastructure and tooling for HIPAA-compliant patient care microservices",
      "highlights": [
        "Led delivery of AWS CDK infrastructure for patient care microservices platform, ensuring HIPAA compliance and deep AWS integration",
        "Collaborated with AWS and partner teams to integrate managed services and accelerate platform development",
        "Championed CDK usage across application tiers, designing IaC patterns for teams to consume and interact with the platform",
        "Implemented CI/CD pipelines for mobile apps, automating TestFlight and App Store releases through Git, enabling self-serve deployments",
        "Trained and mentored product and mobile development teams on CDK, serverless architectures, and CI/CD workflows, fostering autonomy"
      ]
    },
    {
      "name": "CyberGRX (now ProcessUnity)",
      "position": "Senior Platform Engineer",
      "url": "https://www.cybergrx.com",
      "startDate": "2018-12",
      "endDate": "2020-06",
      "summary": "Led cloud-native CI/CD vision and implementation for AWS and Kubernetes",
      "highlights": [
        "Led vision on cloud-native CI/CD pipelines for AWS and Kubernetes",
        "Developed custom resource using CoreOS operator SDK for orchestrating blue/green deployments",
        "Redesigned build and deploy process that dramatically improved team productivity",
        "Mentored software engineers in best practices for building and deploying software",
        "Grew into Staff Engineer role by owning large-scope initiatives, collaborating with the Tech Lead to drive interactions across the organization and unblock progress"
      ]
    },
    {
      "name": "Cardfree",
      "position": "Senior Cloud Platform Engineer",
      "startDate": "2018-01",
      "endDate": "2018-12",
      "summary": "Managed large-scale PCI-compliant C# installation in AWS",
      "highlights": [
        "Managed a large-scale PCI-compliant C# installation in AWS",
        "Architected hybrid AMI build and deploy platform using Packer and Vagrant, enabling consistent Windows and Linux EC2 deployments for C# and Ruby services",
        "Standardized infrastructure-as-code practices to pave the way for open-source .NET migration",
        "Developed and launched a structured PCI-compliant incident response program, replacing ad-hoc Slack alerts with a streamlined IR workflow without excessive paperwork"
      ]
    },
    {
      "name": "Protectwise, Inc (Acquired by Verizon)",
      "position": "Senior DevOps Engineer",
      "startDate": "2016-08",
      "endDate": "2018-01",
      "summary": "Operated SRE for petabyte-scale data ingestion platform",
      "highlights": [
        "Managed SRE for massive scale data ingestion platform with thousands of Cassandra nodes and petabytes in S3",
        "Administered $10M+/yr production AWS environment using Infrastructure as Code"
      ]
    },
    {
      "name": "Food Service Warehouse",
      "position": "DevOps Engineer",
      "startDate": "2015-06",
      "endDate": "2016-03",
      "summary": "Built Kubernetes clusters on vSphere using CoreOS technologies",
      "highlights": [
        "Implemented Kubernetes clusters on vSphere using CoreOS, fleet, etcd, and flannel",
        "Accelerated release cycles for Node.js services with container orchestration"
      ]
    },
    {
      "name": "Mississippi State University",
      "position": "Software Developer",
      "startDate": "2010-06",
      "endDate": "2014-06",
      "summary": "Created grant proposals and custom software for research initiatives",
      "highlights": [
        "Created grant proposals to fund custom software for university research initiatives",
        "Developed software for tracking 'human sensors' through natural disasters including Hurricane Sandy"
      ]
    }
  ],
  "volunteer": [],
  "education": [],
  "awards": [],
  "certificates": [],
  "publications": [],
  "skills": [
    {
      "name": "AI Engineering",
      "level": "Advanced",
      "keywords": [
        "LangChain",
        "Crewai",
        "Custom Agent Framework",
        "MCP Servers",
        "LLM Integration"
      ]
    },
    {
      "name": "Cloud Infrastructure",
      "level": "Expert",
      "keywords": [
        "AWS",
        "Kubernetes",
        "EKS",
        "GKE",
        "vSphere",
        "Terraform",
        "Infrastructure as Code",
        "Cloudflare"
      ]
    },
    {
      "name": "CI/CD &amp; DevOps",
      "level": "Expert",
      "keywords": [
        "GitOps",
        "ArgoCD",
        "Argo Workflows",
        "Jenkins",
        "Github Actions",
        "Gitlab CI",
        "Continuous Deployment"
      ]
    },
    {
      "name": "Programming Languages",
      "level": "Advanced",
      "keywords": [
        "Javascript",
        "Python",
        "Bash",
        "Go",
        "Ruby",
        "Dart"
      ]
    },
    {
      "name": "Databases",
      "level": "Advanced",
      "keywords": [
        "Neo4j",
        "PostgreSQL",
        "Timestream",
        "MongoDB",
        "Redis",
        "Etcd",
        "Zookeeper",
        "Cassandra",
        "DynamoDB"
      ]
    },
    {
      "name": "Distributed Systems",
      "level": "Expert",
      "keywords": [
        "Kafka",
        "RabbitMQ",
        "AWS IoT",
        "Big Data",
        "Spark",
        "EMR"
      ]
    },
    {
      "name": "Monitoring",
      "level": "Advanced",
      "keywords": [
        "Prometheus",
        "CloudWatch",
        "Sensu",
        "Pagerduty",
        "Logstash",
        "Kibana",
        "Grafana",
        "Graphite"
      ]
    },
    {
      "name": "Product Engineering",
      "level": "Expert",
      "keywords": [
        "Data-driven Iteration",
        "Feature Development",
        "User Feedback Loops",
        "MVP Development",
        "Product Analytics"
      ]
    },
    {
      "name": "Product &amp; Business",
      "level": "Advanced",
      "keywords": [
        "Roadmapping",
        "Customer Discovery",
        "P&amp;L Management",
        "Go-to-Market Strategy",
        "Fundraising"
      ]
    }
  ],
  "languages": [
    {
      "language": "English",
      "fluency": "Native speaker"
    }
  ],
  "interests": [],
  "references": [
    {
      "name": "Director Platform Engineering, Cloaked",
      "reference": "Jake is a force for good unlike any other. He migrated our infra to a cheaper and more scalable system, crafted a CI pipeline that accelerated deploys by 30x, and taught the whole company how to make data-driven decisions by building and showcasing an entire product analytics stack. And that was just his first 6 months."
    },
    {
      "name": "Software Engineer, CyberGRX",
      "reference": "I watched Jake redo a build and deploy process over the span of 3 days, whip up a presentation, and get every engineer in the org up to speed using it to deploy new services, a moment I'll never forget because our productivity EXPLODED afterwards."
    },
    {
      "name": "Senior Platform Engineer, Inception Health",
      "reference": "I also wanted to second (third? Fourth?) Jake's skills here. I don't have anything unique to add as everyone else has already covered it, but this man will help you achieve what you need."
    },
    {
      "name": "Magician, Magic",
      "reference": "Jake worked in the pit with [the operations team] and everyday came in with killer energy. Everyone else told us how to work around problems. Jake built us better tools."
    },
    {
      "name": "DevOps Engineer, Protectwise",
      "reference": "Jake was instrumental in building our cloud-native security platform. His technical leadership helped shape our architecture and team practices."
    }
  ],
  "projects": [
    {
      "name": "MCP Working Group Contributions",
      "description": "Active participant in Model Context Protocol (MCP) community development",
      "highlights": [
        "Member of the MCP Hosting Working Group",
        "Developed strategies for deploying and running MCP servers at scale",
        "Contributed to Dart SDK, Express tool for binding to MCP servers",
        "Developed various MCP servers (Hirebase, Candidate Job Search, Text Extraction)",
        "Created GitOps tool for MCP deployments",
        "Operate publicly available MCP servers for community testing"
      ],
      "url": "https://github.com/jhgaylor"
    },
    {
      "name": "CareerFlick LLM-Driven Mobile App",
      "description": "Developed a mobile application with a Tinder-like LLM interface",
      "highlights": [
        "Built intuitive swipe-based interface for interacting with LLMs",
        "Implemented AI-driven content generation and response system",
        "Created responsive mobile-first user experience"
      ]
    },
    {
      "name": "Hirebase MCP Server",
      "description": "A Python MCP server for Hirebase.org's API",
      "highlights": [
        "Built a Python-based Model Context Protocol (MCP) server for searching jobs on Hirebase.org",
        "Dockerized the service and set up GitOps CI/CD pipelines for automated builds and releases"
      ],
      "url": "https://github.com/jhgaylor/hirebase-mcp"
    },
    {
      "name": "Express MCP Handler",
      "description": "Package to simplify MCP handling in Express.js",
      "highlights": [
        "Developed Express middleware in TypeScript to streamline MCP request handling",
        "Authored full type definitions and robust error handling for production-grade reliability",
        "Configured CI/CD pipelines to automate code validation and package publishing"
      ],
      "url": "https://github.com/jhgaylor/express-mcp-handler"
    },
    {
      "name": "Node Candidate MCP Server",
      "description": "A TypeScript library to build customizable candidate MCP servers",
      "highlights": [
        "Authored a reusable library enabling developers to integrate their own candidate data into MCP flows",
        "Published as an npm package for easy adoption and extension by other teams"
      ],
      "url": "https://github.com/jhgaylor/node-candidate-mcp-server"
    },
    {
      "name": "mcp_dart",
      "description": "Dart SDK implementation of the Model Context Protocol (MCP)",
      "highlights": [
        "Contributed stream based support for in-process MCP servers",
        "Add support for running MCP servers in flutter apps",
        "Authored Dart examples to accelerate SDK adoption"
      ],
      "url": "https://github.com/leehack/mcp_dart"
    }
  ]
}</p>
    
  </div>
</details>
    
    <details id="resource-resume-url">
  <summary><code>candidate-info://resume-url</code>: URL to the resume</summary>
  
</details>
    
    <details id="resource-linkedin-url">
  <summary><code>candidate-info://linkedin-url</code>: LinkedIn profile URL</summary>
  
</details>
    
    <details id="resource-github-url">
  <summary><code>candidate-info://github-url</code>: GitHub profile URL</summary>
  
</details>
    
    <details id="resource-website-url">
  <summary><code>candidate-info://website-url</code>: Personal website URL</summary>
  
</details>
    
    <details id="resource-website-text">
  <summary><code>candidate-info://website-text</code>: Content from the personal website</summary>
  <div>
    
        <p>Available to Hire Your Next Technical Cofounder 15+ years of experience helping companies build better tech organizations, launch new products, and grow with confidence. Text Me Email Me View Resume How I Accelerate Success Ship Fast, Learn Fast Focus on delivering the first quantum of value quickly, then iterate based on real user feedback. MVP in days, not months Data-driven iterations Continuous user feedback loops Supercharge Your Teams Deliver faster with proven tools and practices that scale across your organization. Best-in-class development workflows Automated testing and deployment Cost-effective scaling solutions Find Product Market Fit Transform engineering teams into product teams that own the full lifecycle from idea to customer value. Data-driven feature development Customer usage insights Value-driven prioritization Companies I've Helped Grow I've had the privilege of working with innovative companies to help scale their technology and teams. Protectwise Acquired by Verizon "Jake was instrumental in building our cloud-native security platform. His technical leadership helped shape our architecture and team practices." - Devops Engineer, Protectwise Food Service Warehouse "We were deploying c# on vSphere when Jake joined on our first Nodejs backend team. He had been using Docker, we identified Kubernetes as our next step, and Jake hit the ground running. Our node services had the fastest release cycle in the company." - Architect, Food Service Warehouse Cloaked "Jake is a force for good unlike any other. He migrated our infra to a cheaper and more scalable system, crafted a CI pipeline that accelerated deploys by 30x, and taught the whole company how to make data-driven decisions by building and showcasing an entire product analytics stack. And that was just his first 6 months." - Director Platform Engineering, Cloaked CyberGRX Now ProcessUnity "I watched Jake redo a build and deploy process over the span of 3 days, whip up a presentation, and get every engineer in the org up to speed using it to deploy new services, a moment I'll never forget because our productivity EXPLODED afterwards." - Software Engineer, CyberGRX Inception Health "I also wanted to second (third? Fourth?) Jake's skills here. I don't have anything unique to add as everyone else has already covered it, but this man will help you achieve what you need." - Senior Platform Engineer, Inception Health Magic "Jake worked in the pit with [the operations team] and everyday came in with killer energy. Everyone else told us how to work around problems. Jake built us better tools." - Magician, Magic From Zero to Production Start with the smallest valuable feature set, then evolve based on real user feedback and data. Development Philosophy Focus on core user value first Ship early, gather feedback Iterate based on usage data Scale what works Technical Foundation Production-ready infrastructure Automated testing and deployment Built-in monitoring and analytics Scalable architecture patterns Development Approach Quick Starts From idea to first user value in days Data-Driven Every feature backed by usage metrics Rapid Iteration Continuous improvement based on feedback Supercharge Feature Delivery Stop waiting weeks for features to ship. Eliminate manual tasks. Give your teams the tools and automation they need to go to production with confidence. Common Pain Points Slow, manual deployment processes Inconsistent environments causing issues Limited visibility into system health Security patches falling behind The Solution Automated pipelines for instant feedback Infrastructure as code for consistency Real-time monitoring and alerts Automated security updates Quick Feedback Automated test runs Code quality checks Security scanning Continuous Delivery One-click deployments Automated rollbacks Environment promotion Full Visibility Performance metrics Error tracking User analytics Impact on Your Team Faster Deployments Less Manual Work 24/7 System Monitoring Confidence in Delivery Build Product Minded Teams Transform your engineering teams into data driven teams that understand customer needs and deliver measurable business value. Common Challenges Features built without clear success metrics Limited understanding of customer behavior Disconnect between technical and business goals No feedback loop from production to planning The Transformation Data-driven feature development and iteration Real user behavior insights inform decisions Clear metrics tied to business outcomes Continuous learning from production data Analytics That Matter Feature usage tracking User journey mapping Performance impact Experimentation A/B testing framework Feature flags Controlled rollouts Customer Feedback In-app surveys Usage analytics Support insights Results in Teams That Focus on Customer Value Make Data-Driven Decisions Innovate Continuously Ship with Confidence Modern Cloud-Native Stack Leveraging industry-leading tools and practices for rapid, reliable delivery Platform AWS cloud infrastructure Kubernetes orchestration Cloudflare edge networking Terraform infrastructure as code Delivery GitOps workflow Continuous Integration pipelines Automated Continuous Deployment Infrastructure automation Observability Product usage analytics System Telemetry A/B testing Error tracking &amp; Crash Reporting Are We a Good Fit? I specialize in modern web technologies and cloud infrastructure. Let's make sure we are a good match. Great Fits SaaS Applications Web applications, APIs, and cloud-native services Infrastructure as Code AWS, Kubernetes, Terraform, and modern cloud platforms Developer Experience CI/CD pipelines, development workflows, and team productivity Software Delivery Deployment automation, monitoring, and operational excellence Technical Leadership Architecture decisions, team guidance, and technology strategy Not Great Fits Design Work While I can do a little design work, you dont want me creating your user interfaces. Security Penetration Testing While I implement secure practices, I'm not a dedicated security auditor IoT &amp; Firmware Development I don't work with embedded systems or hardware-level programming Complex Algorithms Deep computer science problems or algorithmic optimization Machine Learning While I can integrate ML services, I don't develop ML models © 2024 Jake Gaylor. All rights reserved.

Toggle navigation Jake Gaylor Jake Gaylor Full Stack Engineer 720.453.3994 Download Resume Read Blog Hacker and Entrepreneur My passion for programming was sparked in 2001 doing CRUD on a LAMP. I have a ton of experience developing web applications and I have worked with a variety of platforms and technologies to build software and have deployed software to the web, Mac OS X, and Linux. My understanding of many different programming languages and frameworks has exposed me to new paradigms, which helps me pick the best tool for each job. I do any work I can openly on Github and am a big fan of open source. I love talking shop, so reach out if you want to chat. I started a company in 2013 which launched two products. Both were SaaS applications involving collecting and curating large sets of social media data and I built the technical stack from servers to CSS. I learned to do a lot of non technical things as well while starting NestedData, including writing copy, managing finances, hiring, firing, negotiating contracts, marketing, on boarding, and most of all how to talk to customers. Developer I have 7 years experience hacking professionally. I've worked on CI/CD pipelines, internal productivity tools, social media applications for social scientists, open source blogging software, an xmpp chatbot to provide a CLI to the vast League of Legends data on the web, and architected multiple distributed systems. Given a set of objectives, I can evaluate them to then build the infrastructure and applications to meet them. I'm a hacker at my core, so I thrive in an environment where I am able to help determine the technical direction, though the entrepreneur in me really wants to talk to the users in order to challenge my assumptions. Ops Automated, repeatable deploys - I use tools like Chef, Packer, and Terraform to model infrastructure as code in order to be able to test it before releasing changes to production environments. A solid configuration management codebase saved my team's bacon more than once. Having infrastructure in code means we can be back up in minutes and only one person has to be distracted by what could other be a catastrophe. CI/CD - Using Code Review, Docker, Jenkins, and a solid test suite, I built a CI/CD pipeline for the SocialDrizzle web app and it's supporting services. Our team was able to make changes to the application and see them in production as soon as they passed muster, without having to waste time doing repetitive, error prone tasks. Recent Past Lately I have been doing a good deal work with Kubernetes. It is an unopinionated PaaS on top of which tooling can be built to provide a great amount of business value with less technical investment than ever before. I have also been working with CoreOS and concourse-ci to enable rapid creation of CI/CD pipelines that scale. Before that I spent a lot of time working with Meteor and I recently built a social media data collection system in node that allows me to use (Twitter/Facebook/Instagram) bots in multiple applications while only having to do the work of writing the logic and building/deploying infrastructure once. My latest, public, launch was a chat bot for League of Legends. It was built in a week and documented at https://github.com/jhgaylor/StatBot/wiki/Day-0:-The-Beginning. Please feel free to check out the repository as an example of my work. Future I'm looking for work as an operations engineer. My ideal role would be one where I can provide tools and services to developers to reduce the friction of creating and maintaining production ready software. An average day might include meeting with a dev team to discover pain points (cross team collaboration), working on a client library to interface with the experimental metrics2.0 backend (coding/helping devs interface with ops), reviewing a merge request to the terraform/cloud formation repository, and possibly tracking down the root cause of a production outage. I’d like to be in a position where I can have a say in how we choose to do things but also put my head down and knock out the actual implementation. On the side I'm toying with building a Heroku-like service on top of GKE at Cloudflinger.com. It's open for use but it's may disappear at any time as it is a side project consuming actual cloud resources. The placeholder marketing site is being hosted using the service. Projects Statbot [Link] Highlights XMPP client using Hubot to communicate. Express.js HTTP API Server Redis for cacheing. Led to the creation of a new npm package, Big Cheese. Deployment was automated using Tutum.co Ramped up from 1 to 7 regions in hours. Contributed a patch upstream to LeagueJS. 14k+ Redditors visited the project on day one. The entire project, from launch to finish, took just 7 days. About A chat bot for League of Legends. LoL players, especially those not in the top tier of competitive play, often need more information about the game... quickly! It is quite common for players to "alt-tab" to sites like lolking, lolnexus, lolcounter, probuilds, or many others. Once on the site gamers have to type, click, and wait on load times, all while under extreme time pressure. To make matters worse, they often need information from one (or multiple!) source(s) during champion select, another source at the load screen, and yet another during the game. All of these things add to the players' stress level, and some times, they don't get done in time. Thus these constraints have in game ramifications. Statbot makes it easy to get all the information you need instantly in game. Node.js Coffeescript HTTP Redis XMPP Express.js Tracer Highlights Meteor.js Web Application Twitter Bootstrap CSS Framework Python Social Media Data Collector Consumed real time and historic data. Celery &amp; RabbitMQ for a scalable Asynchronous Task Queue MongoDB for Application State and Social Media Data Storage Integrated with FullContact and PiPl web apis. About Tracer offers a quick and easy way to identify, capture, codify, and report a person's social media data. With just a person's email address or username, Tracer helps you identify the target's social media accounts and provides you with both historical and real-time data. Meteor.js Node.js Coffeescript Python Celery RabbitMQ MongoDB Vagrant SocialDrizzle [Link] Highlights Entire System of services and networking deployed with the most up to date code using Ansible, Docker, Jenkins, and Github. Built using micro services including: a data collector, a data router, a data consumer, a data labeler, a web application, and a marketing site. Employs a fleet of bots that consume data from social networks. Web Application built with ♥ using Meteor. Launched first version at Mississippi State University after just 12 weeks. About A startup venture for providing social engagement to the patrons of public events. SocialDrizzle offers game day production crews a turnkey solution for highlighting fans' social media messages and pics on stadium video board. SocialDrizzle provides the whole package -- a collection and curation application, custom designed templates, hardware connection to your existing production equipment, on-site set up, and unlimited tech support. Get in touch if you want to know more, I'd love to talk to you about my experiences launching a product and starting a business. Meteor.js Node.js Coffeescript MongoDB Ansible Docker Redis Websockets Microservices Unit Testing SASS GNIP Jenkins CI/CD Django-blogger [Link] Highlights Exemplifies Django Best Practicies Easily Themable with a python package using Twitter Bootstrap. Feature Filled multi tenant blog. i18n &amp; l10n Includes an HTTP Api Support posts written in markdown. About A reusable Django app for blogging intended for one or more contributors. The purpose of this project is to create a basic, but feature complete, blogging platform for Django v1.4. The project was intended to exemplify best practices in Django at the time of writing. The package has been downloaded nearly 50,000 times since creation. Python Django Unit Testing i18n l10n RSS REST Api TLDR.io api wrapper [Link] Highlights Python Package for accessing TLDR.io's HTTP Api Simple API that maps closely to the API documentation. One of my early projects with tests. Thoroughly documented. About I was really enjoyed using [tldr.io's service and I wanted to find a way to give back. When I discovered they had an HTTP Api and no Python library, I jumped at the chance to build it. Python Requests HTTP Api Unit Testing Bonus.ly Api Wrapper [Link] Highlights Node.js Package for accessing Bonus.ly's HTTP Api. Intuitive API that attempts to feel like the http api using javascriptisms. Usable as a static class - convenient if you already have an api key in memory. Instantiate the class to have a client that will track the api key for the mostly recently authenticated user. About Node bindings for the bonus.ly http api using the request package. The idea was to treat the HTTP api as a set of calls that could be described using a few common parameters. I was trying to get as close as I could to "generating" code. (I should learn a lisp!) Adding new endpoints becomes as simple as writing a POJO. This wouldn't work if the HTTP API didn't have internal consistency. By describing the required parameters, we can give feedback to the developer without waiting for a round trip. This comes at the expense of having to update the client library any time the http api changes. Depending on how "frozen" the api is, this may or may not be a good trade off. I built this to easily allow that feature to be removed. Node.js Request Promises HTTP Social Media Tracking and Analysis System (SMTAS) [Link] Highlights Django Web Application jQuery for client side pizazz. Twitter Boostrap PostgreSQL Python Social Media Data Collector Consumed real time data. Celery &amp; Redis for a scalable Asynchronous Task Queue Google Maps About A system to collect, analyze, and visualize social media data for researchers to gain valuable insights into public opinion, views and fluidity. SMTAS is focused on the social network, Twitter, where researchers have access to approximately 500 million tweets per day. Tweets are public postings made by worldwide users of Twitter. Apart from the real-time access to Twitter, SMTAS also has access to historical data/tweets posted via Twitter since 2006. Apart from Twitter data being a rich information source of human behavior (170 million active users) in a social network, it also provides researchers instantaneous information from its user base with its faster (compared to other social networks) message propagation. Python Django Celery Redis Unit Testing REST Api PostgreSQL jQuery Twitter Bootstrap Google Maps ECMAScript 5 GNIP Streaming HTTP SMTAS from Innovative Data Lab on Vimeo. Employee Management System Highlights Meteor Web Application deployed as a standard node application using Demeteorizer Touch Screen Friendly Internal Productivity tool used by an 80 employee survey call center. Required matching existing timecard printouts closely. About "Timemaster" was desperately needed by Mississippi State University's (MSU) Social Science Research Center's (SSRC) Survey Research Laboratory (SRL) in order to wrangle the timesheets of 80+ intermittent workers. The tool saved a team of 4 supervisors a significant amount of time that was being spent on paperwork. nginx Node.js Coffeescript MongoDB Meteor.js Unit Testing Chef Codeship Read How Google Works The Everything Store The Goal: A process of ongoing improvement The Phoenix Project The Five Dysfunctions of a Team The Lean Startup Hackers and Painters Two Scoops of Django Javascript: The Good Parts JavaScript Allongé Discover Meteor XMPP: The definitive guide Founders at Work Node.js the Right Way Rework The Virgin Way Zero to One Delivering Happiness It's Not Luck Critical Chain The Five Temptations of a CEO Customer Development for Entrepreneurs Think Like a Freak Start with Why Great by Choice Growth Hacker Marketing SuperFreakonomics Made to Stick Leaders Eat Last The Spirit of Kaizen Rolling Rocks Downhill Grit: The Power of Passion and Perseverance Start with Why The Founder's Dilemmas Reading Effective Devops Get in Touch</p>
    
  </div>
</details>
</div> 
    
    <h4>Tools</h4>
<div>
    <details id="tool-resume-text">
  <summary><code>get_resume_text</code>: Returns the candidate's resume content as text</summary>
  <div>
    
        <p>{
  "meta": {
    "themeOptions": {
      "colors": {
        "background": [
          "#ffffff",
          "#191e23"
        ],
        "dimmed": [
          "#f3f4f5",
          "#23282d"
        ],
        "primary": [
          "#191e23",
          "#fbfbfc"
        ],
        "secondary": [
          "#6c7781",
          "#ccd0d4"
        ],
        "accent": [
          "#46b450",
          "#47d153"
        ]
      }
    },
    "theme": "even"
  },
  "basics": {
    "name": "Jake Gaylor",
    "label": "Senior Software Engineer &amp; Product Founder | End-to-End Product Development &amp; Go-to-Market",
    "image": "",
    "email": "jhgaylor@gmail.com",
    "phone": "720.453.3994",
    "url": "https://jakegaylor.com",
    "summary": "15+ years building and shipping products at scale—from founding SaaS ventures to architecting AI-powered platforms. I combine deep SWE expertise with business leadership to drive product vision, go-to-market strategy, and revenue growth.",
    "location": {
      "address": "",
      "postalCode": "",
      "city": "",
      "countryCode": "US",
      "region": "Remote First, Will Travel"
    },
    "profiles": [
      {
        "network": "GitHub",
        "username": "jhgaylor",
        "url": "https://github.com/jhgaylor"
      },
      {
        "network": "LinkedIn",
        "username": "jhgaylor",
        "url": "https://www.linkedin.com/in/jhgaylor/"
      }
    ]
  },
  "work": [
    {
      "name": "Cloaked Inc",
      "position": "Staff SWE, Office of the CEO",
      "url": "https://cloaked.app",
      "startDate": "2023-08",
      "summary": "Spearheaded executive-level innovation by developing autonomous AI agents and a cross-functional ideation portal—driving a highly sticky, AI-powered onboarding experience that boosted user adoption and retention and defined governance processes",
      "highlights": [
        "Developed autonomous AI agents using LangChain to automate user account management tasks such as auto change password, auto login, and auto change email",
        "Leveraged these AI agents as a core fundraising narrative and created a highly sticky, AI-powered onboarding experience that significantly boosted user adoption and retention",
        "Designed and launched a cross-functional ideation portal engaging design, customer success, engineering, product development, and executive teams with transparent status tracking",
        "Partnered with design, product, engineering, marketing, customer success, and co-founders to validate, refine, and ship concepts into market-ready features",
        "Defined and documented cross-functional roles, ceremonies, and governance processes to unify ideation and accelerate product delivery across the organization"
      ]
    },
    {
      "name": "Cloaked Inc",
      "position": "Staff Platform Engineer",
      "url": "https://cloaked.app",
      "startDate": "2022-09",
      "endDate": "2023-08",
      "summary": "Oversaw strategic AWS EKS migration and architected enterprise deployment governance, compliance frameworks, and GitOps CI/CD pipelines to enhance security, scalability, and developer productivity",
      "highlights": [
        "Defined and executed a strategic migration roadmap transitioning legacy PaaS platforms to AWS EKS, driving cost reduction and scalability",
        "Collaborated with cross-functional leadership to implement a Cloudflare control plane, enhancing security posture and resilience",
        "Architected and led adoption of a multi-account AWS hub-and-spoke model, ensuring SOC2, ISO 27001, and 27701 compliance",
        "Established enterprise Kubernetes and edge deployment standards, mentoring engineering teams in best practices and governance",
        "Spearheaded design and implementation of a GitOps-based CI/CD pipeline, accelerating deployments by 30x",
        "Partnered with product and analytics teams to develop an end-to-end analytics stack, enabling data-driven decision-making"
      ]
    },
    {
      "name": "The Onward Store Steakhouse",
      "position": "Owner / Co-GM",
      "startDate": "2021-10",
      "endDate": "2024-01",
      "summary": "Owned and managed a full-service steakhouse with 7 staff, generating $500K in annual revenue and overseeing P&amp;L, operations, and vendor relations.",
      "highlights": [
        "Led a team of 7 employees, handling recruitment, training, scheduling, and performance management.",
        "Directed all facets of business operations, achieving $500K in annual revenue through strategic inventory management and cost control.",
        "Designed and implemented operational processes for facility maintenance, vendor partnerships, and quality assurance, driving consistent customer satisfaction.",
        "Managed P&amp;L, budgets, and financial reporting, optimizing expenses and improving profit margins.",
        "Executed marketing and community engagement initiatives to increase local brand awareness and repeat patronage."
      ]
    },
    {
      "name": "Inception Health",
      "position": "Staff SWE in Developer Experience",
      "startDate": "2020-06",
      "endDate": "2022-06",
      "summary": "Architected AWS CDK infrastructure and tooling for HIPAA-compliant patient care microservices",
      "highlights": [
        "Led delivery of AWS CDK infrastructure for patient care microservices platform, ensuring HIPAA compliance and deep AWS integration",
        "Collaborated with AWS and partner teams to integrate managed services and accelerate platform development",
        "Championed CDK usage across application tiers, designing IaC patterns for teams to consume and interact with the platform",
        "Implemented CI/CD pipelines for mobile apps, automating TestFlight and App Store releases through Git, enabling self-serve deployments",
        "Trained and mentored product and mobile development teams on CDK, serverless architectures, and CI/CD workflows, fostering autonomy"
      ]
    },
    {
      "name": "CyberGRX (now ProcessUnity)",
      "position": "Senior Platform Engineer",
      "url": "https://www.cybergrx.com",
      "startDate": "2018-12",
      "endDate": "2020-06",
      "summary": "Led cloud-native CI/CD vision and implementation for AWS and Kubernetes",
      "highlights": [
        "Led vision on cloud-native CI/CD pipelines for AWS and Kubernetes",
        "Developed custom resource using CoreOS operator SDK for orchestrating blue/green deployments",
        "Redesigned build and deploy process that dramatically improved team productivity",
        "Mentored software engineers in best practices for building and deploying software",
        "Grew into Staff Engineer role by owning large-scope initiatives, collaborating with the Tech Lead to drive interactions across the organization and unblock progress"
      ]
    },
    {
      "name": "Cardfree",
      "position": "Senior Cloud Platform Engineer",
      "startDate": "2018-01",
      "endDate": "2018-12",
      "summary": "Managed large-scale PCI-compliant C# installation in AWS",
      "highlights": [
        "Managed a large-scale PCI-compliant C# installation in AWS",
        "Architected hybrid AMI build and deploy platform using Packer and Vagrant, enabling consistent Windows and Linux EC2 deployments for C# and Ruby services",
        "Standardized infrastructure-as-code practices to pave the way for open-source .NET migration",
        "Developed and launched a structured PCI-compliant incident response program, replacing ad-hoc Slack alerts with a streamlined IR workflow without excessive paperwork"
      ]
    },
    {
      "name": "Protectwise, Inc (Acquired by Verizon)",
      "position": "Senior DevOps Engineer",
      "startDate": "2016-08",
      "endDate": "2018-01",
      "summary": "Operated SRE for petabyte-scale data ingestion platform",
      "highlights": [
        "Managed SRE for massive scale data ingestion platform with thousands of Cassandra nodes and petabytes in S3",
        "Administered $10M+/yr production AWS environment using Infrastructure as Code"
      ]
    },
    {
      "name": "Food Service Warehouse",
      "position": "DevOps Engineer",
      "startDate": "2015-06",
      "endDate": "2016-03",
      "summary": "Built Kubernetes clusters on vSphere using CoreOS technologies",
      "highlights": [
        "Implemented Kubernetes clusters on vSphere using CoreOS, fleet, etcd, and flannel",
        "Accelerated release cycles for Node.js services with container orchestration"
      ]
    },
    {
      "name": "Mississippi State University",
      "position": "Software Developer",
      "startDate": "2010-06",
      "endDate": "2014-06",
      "summary": "Created grant proposals and custom software for research initiatives",
      "highlights": [
        "Created grant proposals to fund custom software for university research initiatives",
        "Developed software for tracking 'human sensors' through natural disasters including Hurricane Sandy"
      ]
    }
  ],
  "volunteer": [],
  "education": [],
  "awards": [],
  "certificates": [],
  "publications": [],
  "skills": [
    {
      "name": "AI Engineering",
      "level": "Advanced",
      "keywords": [
        "LangChain",
        "Crewai",
        "Custom Agent Framework",
        "MCP Servers",
        "LLM Integration"
      ]
    },
    {
      "name": "Cloud Infrastructure",
      "level": "Expert",
      "keywords": [
        "AWS",
        "Kubernetes",
        "EKS",
        "GKE",
        "vSphere",
        "Terraform",
        "Infrastructure as Code",
        "Cloudflare"
      ]
    },
    {
      "name": "CI/CD &amp; DevOps",
      "level": "Expert",
      "keywords": [
        "GitOps",
        "ArgoCD",
        "Argo Workflows",
        "Jenkins",
        "Github Actions",
        "Gitlab CI",
        "Continuous Deployment"
      ]
    },
    {
      "name": "Programming Languages",
      "level": "Advanced",
      "keywords": [
        "Javascript",
        "Python",
        "Bash",
        "Go",
        "Ruby",
        "Dart"
      ]
    },
    {
      "name": "Databases",
      "level": "Advanced",
      "keywords": [
        "Neo4j",
        "PostgreSQL",
        "Timestream",
        "MongoDB",
        "Redis",
        "Etcd",
        "Zookeeper",
        "Cassandra",
        "DynamoDB"
      ]
    },
    {
      "name": "Distributed Systems",
      "level": "Expert",
      "keywords": [
        "Kafka",
        "RabbitMQ",
        "AWS IoT",
        "Big Data",
        "Spark",
        "EMR"
      ]
    },
    {
      "name": "Monitoring",
      "level": "Advanced",
      "keywords": [
        "Prometheus",
        "CloudWatch",
        "Sensu",
        "Pagerduty",
        "Logstash",
        "Kibana",
        "Grafana",
        "Graphite"
      ]
    },
    {
      "name": "Product Engineering",
      "level": "Expert",
      "keywords": [
        "Data-driven Iteration",
        "Feature Development",
        "User Feedback Loops",
        "MVP Development",
        "Product Analytics"
      ]
    },
    {
      "name": "Product &amp; Business",
      "level": "Advanced",
      "keywords": [
        "Roadmapping",
        "Customer Discovery",
        "P&amp;L Management",
        "Go-to-Market Strategy",
        "Fundraising"
      ]
    }
  ],
  "languages": [
    {
      "language": "English",
      "fluency": "Native speaker"
    }
  ],
  "interests": [],
  "references": [
    {
      "name": "Director Platform Engineering, Cloaked",
      "reference": "Jake is a force for good unlike any other. He migrated our infra to a cheaper and more scalable system, crafted a CI pipeline that accelerated deploys by 30x, and taught the whole company how to make data-driven decisions by building and showcasing an entire product analytics stack. And that was just his first 6 months."
    },
    {
      "name": "Software Engineer, CyberGRX",
      "reference": "I watched Jake redo a build and deploy process over the span of 3 days, whip up a presentation, and get every engineer in the org up to speed using it to deploy new services, a moment I'll never forget because our productivity EXPLODED afterwards."
    },
    {
      "name": "Senior Platform Engineer, Inception Health",
      "reference": "I also wanted to second (third? Fourth?) Jake's skills here. I don't have anything unique to add as everyone else has already covered it, but this man will help you achieve what you need."
    },
    {
      "name": "Magician, Magic",
      "reference": "Jake worked in the pit with [the operations team] and everyday came in with killer energy. Everyone else told us how to work around problems. Jake built us better tools."
    },
    {
      "name": "DevOps Engineer, Protectwise",
      "reference": "Jake was instrumental in building our cloud-native security platform. His technical leadership helped shape our architecture and team practices."
    }
  ],
  "projects": [
    {
      "name": "MCP Working Group Contributions",
      "description": "Active participant in Model Context Protocol (MCP) community development",
      "highlights": [
        "Member of the MCP Hosting Working Group",
        "Developed strategies for deploying and running MCP servers at scale",
        "Contributed to Dart SDK, Express tool for binding to MCP servers",
        "Developed various MCP servers (Hirebase, Candidate Job Search, Text Extraction)",
        "Created GitOps tool for MCP deployments",
        "Operate publicly available MCP servers for community testing"
      ],
      "url": "https://github.com/jhgaylor"
    },
    {
      "name": "CareerFlick LLM-Driven Mobile App",
      "description": "Developed a mobile application with a Tinder-like LLM interface",
      "highlights": [
        "Built intuitive swipe-based interface for interacting with LLMs",
        "Implemented AI-driven content generation and response system",
        "Created responsive mobile-first user experience"
      ]
    },
    {
      "name": "Hirebase MCP Server",
      "description": "A Python MCP server for Hirebase.org's API",
      "highlights": [
        "Built a Python-based Model Context Protocol (MCP) server for searching jobs on Hirebase.org",
        "Dockerized the service and set up GitOps CI/CD pipelines for automated builds and releases"
      ],
      "url": "https://github.com/jhgaylor/hirebase-mcp"
    },
    {
      "name": "Express MCP Handler",
      "description": "Package to simplify MCP handling in Express.js",
      "highlights": [
        "Developed Express middleware in TypeScript to streamline MCP request handling",
        "Authored full type definitions and robust error handling for production-grade reliability",
        "Configured CI/CD pipelines to automate code validation and package publishing"
      ],
      "url": "https://github.com/jhgaylor/express-mcp-handler"
    },
    {
      "name": "Node Candidate MCP Server",
      "description": "A TypeScript library to build customizable candidate MCP servers",
      "highlights": [
        "Authored a reusable library enabling developers to integrate their own candidate data into MCP flows",
        "Published as an npm package for easy adoption and extension by other teams"
      ],
      "url": "https://github.com/jhgaylor/node-candidate-mcp-server"
    },
    {
      "name": "mcp_dart",
      "description": "Dart SDK implementation of the Model Context Protocol (MCP)",
      "highlights": [
        "Contributed stream based support for in-process MCP servers",
        "Add support for running MCP servers in flutter apps",
        "Authored Dart examples to accelerate SDK adoption"
      ],
      "url": "https://github.com/leehack/mcp_dart"
    }
  ]
}</p>
    
  </div>
</details>
    
    <details id="tool-resume-url">
  <summary><code>get_resume_url</code>: Returns the URL to the candidate's resume</summary>
  
</details>
    
    <details id="tool-linkedin-url">
  <summary><code>get_linkedin_url</code>: Returns the candidate's LinkedIn profile URL</summary>
  
</details>
    
    <details id="tool-github-url">
  <summary><code>get_github_url</code>: Returns the candidate's GitHub profile URL</summary>
  
</details>
    
    <details id="tool-website-url">
  <summary><code>get_website_url</code>: Returns the candidate's personal website URL</summary>
  
</details>
    
    <details id="tool-website-text">
  <summary><code>get_website_text</code>: Returns the content from the candidate's personal website</summary>
  <div>
    
        <p>Available to Hire Your Next Technical Cofounder 15+ years of experience helping companies build better tech organizations, launch new products, and grow with confidence. Text Me Email Me View Resume How I Accelerate Success Ship Fast, Learn Fast Focus on delivering the first quantum of value quickly, then iterate based on real user feedback. MVP in days, not months Data-driven iterations Continuous user feedback loops Supercharge Your Teams Deliver faster with proven tools and practices that scale across your organization. Best-in-class development workflows Automated testing and deployment Cost-effective scaling solutions Find Product Market Fit Transform engineering teams into product teams that own the full lifecycle from idea to customer value. Data-driven feature development Customer usage insights Value-driven prioritization Companies I've Helped Grow I've had the privilege of working with innovative companies to help scale their technology and teams. Protectwise Acquired by Verizon "Jake was instrumental in building our cloud-native security platform. His technical leadership helped shape our architecture and team practices." - Devops Engineer, Protectwise Food Service Warehouse "We were deploying c# on vSphere when Jake joined on our first Nodejs backend team. He had been using Docker, we identified Kubernetes as our next step, and Jake hit the ground running. Our node services had the fastest release cycle in the company." - Architect, Food Service Warehouse Cloaked "Jake is a force for good unlike any other. He migrated our infra to a cheaper and more scalable system, crafted a CI pipeline that accelerated deploys by 30x, and taught the whole company how to make data-driven decisions by building and showcasing an entire product analytics stack. And that was just his first 6 months." - Director Platform Engineering, Cloaked CyberGRX Now ProcessUnity "I watched Jake redo a build and deploy process over the span of 3 days, whip up a presentation, and get every engineer in the org up to speed using it to deploy new services, a moment I'll never forget because our productivity EXPLODED afterwards." - Software Engineer, CyberGRX Inception Health "I also wanted to second (third? Fourth?) Jake's skills here. I don't have anything unique to add as everyone else has already covered it, but this man will help you achieve what you need." - Senior Platform Engineer, Inception Health Magic "Jake worked in the pit with [the operations team] and everyday came in with killer energy. Everyone else told us how to work around problems. Jake built us better tools." - Magician, Magic From Zero to Production Start with the smallest valuable feature set, then evolve based on real user feedback and data. Development Philosophy Focus on core user value first Ship early, gather feedback Iterate based on usage data Scale what works Technical Foundation Production-ready infrastructure Automated testing and deployment Built-in monitoring and analytics Scalable architecture patterns Development Approach Quick Starts From idea to first user value in days Data-Driven Every feature backed by usage metrics Rapid Iteration Continuous improvement based on feedback Supercharge Feature Delivery Stop waiting weeks for features to ship. Eliminate manual tasks. Give your teams the tools and automation they need to go to production with confidence. Common Pain Points Slow, manual deployment processes Inconsistent environments causing issues Limited visibility into system health Security patches falling behind The Solution Automated pipelines for instant feedback Infrastructure as code for consistency Real-time monitoring and alerts Automated security updates Quick Feedback Automated test runs Code quality checks Security scanning Continuous Delivery One-click deployments Automated rollbacks Environment promotion Full Visibility Performance metrics Error tracking User analytics Impact on Your Team Faster Deployments Less Manual Work 24/7 System Monitoring Confidence in Delivery Build Product Minded Teams Transform your engineering teams into data driven teams that understand customer needs and deliver measurable business value. Common Challenges Features built without clear success metrics Limited understanding of customer behavior Disconnect between technical and business goals No feedback loop from production to planning The Transformation Data-driven feature development and iteration Real user behavior insights inform decisions Clear metrics tied to business outcomes Continuous learning from production data Analytics That Matter Feature usage tracking User journey mapping Performance impact Experimentation A/B testing framework Feature flags Controlled rollouts Customer Feedback In-app surveys Usage analytics Support insights Results in Teams That Focus on Customer Value Make Data-Driven Decisions Innovate Continuously Ship with Confidence Modern Cloud-Native Stack Leveraging industry-leading tools and practices for rapid, reliable delivery Platform AWS cloud infrastructure Kubernetes orchestration Cloudflare edge networking Terraform infrastructure as code Delivery GitOps workflow Continuous Integration pipelines Automated Continuous Deployment Infrastructure automation Observability Product usage analytics System Telemetry A/B testing Error tracking &amp; Crash Reporting Are We a Good Fit? I specialize in modern web technologies and cloud infrastructure. Let's make sure we are a good match. Great Fits SaaS Applications Web applications, APIs, and cloud-native services Infrastructure as Code AWS, Kubernetes, Terraform, and modern cloud platforms Developer Experience CI/CD pipelines, development workflows, and team productivity Software Delivery Deployment automation, monitoring, and operational excellence Technical Leadership Architecture decisions, team guidance, and technology strategy Not Great Fits Design Work While I can do a little design work, you dont want me creating your user interfaces. Security Penetration Testing While I implement secure practices, I'm not a dedicated security auditor IoT &amp; Firmware Development I don't work with embedded systems or hardware-level programming Complex Algorithms Deep computer science problems or algorithmic optimization Machine Learning While I can integrate ML services, I don't develop ML models © 2024 Jake Gaylor. All rights reserved.

Toggle navigation Jake Gaylor Jake Gaylor Full Stack Engineer 720.453.3994 Download Resume Read Blog Hacker and Entrepreneur My passion for programming was sparked in 2001 doing CRUD on a LAMP. I have a ton of experience developing web applications and I have worked with a variety of platforms and technologies to build software and have deployed software to the web, Mac OS X, and Linux. My understanding of many different programming languages and frameworks has exposed me to new paradigms, which helps me pick the best tool for each job. I do any work I can openly on Github and am a big fan of open source. I love talking shop, so reach out if you want to chat. I started a company in 2013 which launched two products. Both were SaaS applications involving collecting and curating large sets of social media data and I built the technical stack from servers to CSS. I learned to do a lot of non technical things as well while starting NestedData, including writing copy, managing finances, hiring, firing, negotiating contracts, marketing, on boarding, and most of all how to talk to customers. Developer I have 7 years experience hacking professionally. I've worked on CI/CD pipelines, internal productivity tools, social media applications for social scientists, open source blogging software, an xmpp chatbot to provide a CLI to the vast League of Legends data on the web, and architected multiple distributed systems. Given a set of objectives, I can evaluate them to then build the infrastructure and applications to meet them. I'm a hacker at my core, so I thrive in an environment where I am able to help determine the technical direction, though the entrepreneur in me really wants to talk to the users in order to challenge my assumptions. Ops Automated, repeatable deploys - I use tools like Chef, Packer, and Terraform to model infrastructure as code in order to be able to test it before releasing changes to production environments. A solid configuration management codebase saved my team's bacon more than once. Having infrastructure in code means we can be back up in minutes and only one person has to be distracted by what could other be a catastrophe. CI/CD - Using Code Review, Docker, Jenkins, and a solid test suite, I built a CI/CD pipeline for the SocialDrizzle web app and it's supporting services. Our team was able to make changes to the application and see them in production as soon as they passed muster, without having to waste time doing repetitive, error prone tasks. Recent Past Lately I have been doing a good deal work with Kubernetes. It is an unopinionated PaaS on top of which tooling can be built to provide a great amount of business value with less technical investment than ever before. I have also been working with CoreOS and concourse-ci to enable rapid creation of CI/CD pipelines that scale. Before that I spent a lot of time working with Meteor and I recently built a social media data collection system in node that allows me to use (Twitter/Facebook/Instagram) bots in multiple applications while only having to do the work of writing the logic and building/deploying infrastructure once. My latest, public, launch was a chat bot for League of Legends. It was built in a week and documented at https://github.com/jhgaylor/StatBot/wiki/Day-0:-The-Beginning. Please feel free to check out the repository as an example of my work. Future I'm looking for work as an operations engineer. My ideal role would be one where I can provide tools and services to developers to reduce the friction of creating and maintaining production ready software. An average day might include meeting with a dev team to discover pain points (cross team collaboration), working on a client library to interface with the experimental metrics2.0 backend (coding/helping devs interface with ops), reviewing a merge request to the terraform/cloud formation repository, and possibly tracking down the root cause of a production outage. I’d like to be in a position where I can have a say in how we choose to do things but also put my head down and knock out the actual implementation. On the side I'm toying with building a Heroku-like service on top of GKE at Cloudflinger.com. It's open for use but it's may disappear at any time as it is a side project consuming actual cloud resources. The placeholder marketing site is being hosted using the service. Projects Statbot [Link] Highlights XMPP client using Hubot to communicate. Express.js HTTP API Server Redis for cacheing. Led to the creation of a new npm package, Big Cheese. Deployment was automated using Tutum.co Ramped up from 1 to 7 regions in hours. Contributed a patch upstream to LeagueJS. 14k+ Redditors visited the project on day one. The entire project, from launch to finish, took just 7 days. About A chat bot for League of Legends. LoL players, especially those not in the top tier of competitive play, often need more information about the game... quickly! It is quite common for players to "alt-tab" to sites like lolking, lolnexus, lolcounter, probuilds, or many others. Once on the site gamers have to type, click, and wait on load times, all while under extreme time pressure. To make matters worse, they often need information from one (or multiple!) source(s) during champion select, another source at the load screen, and yet another during the game. All of these things add to the players' stress level, and some times, they don't get done in time. Thus these constraints have in game ramifications. Statbot makes it easy to get all the information you need instantly in game. Node.js Coffeescript HTTP Redis XMPP Express.js Tracer Highlights Meteor.js Web Application Twitter Bootstrap CSS Framework Python Social Media Data Collector Consumed real time and historic data. Celery &amp; RabbitMQ for a scalable Asynchronous Task Queue MongoDB for Application State and Social Media Data Storage Integrated with FullContact and PiPl web apis. About Tracer offers a quick and easy way to identify, capture, codify, and report a person's social media data. With just a person's email address or username, Tracer helps you identify the target's social media accounts and provides you with both historical and real-time data. Meteor.js Node.js Coffeescript Python Celery RabbitMQ MongoDB Vagrant SocialDrizzle [Link] Highlights Entire System of services and networking deployed with the most up to date code using Ansible, Docker, Jenkins, and Github. Built using micro services including: a data collector, a data router, a data consumer, a data labeler, a web application, and a marketing site. Employs a fleet of bots that consume data from social networks. Web Application built with ♥ using Meteor. Launched first version at Mississippi State University after just 12 weeks. About A startup venture for providing social engagement to the patrons of public events. SocialDrizzle offers game day production crews a turnkey solution for highlighting fans' social media messages and pics on stadium video board. SocialDrizzle provides the whole package -- a collection and curation application, custom designed templates, hardware connection to your existing production equipment, on-site set up, and unlimited tech support. Get in touch if you want to know more, I'd love to talk to you about my experiences launching a product and starting a business. Meteor.js Node.js Coffeescript MongoDB Ansible Docker Redis Websockets Microservices Unit Testing SASS GNIP Jenkins CI/CD Django-blogger [Link] Highlights Exemplifies Django Best Practicies Easily Themable with a python package using Twitter Bootstrap. Feature Filled multi tenant blog. i18n &amp; l10n Includes an HTTP Api Support posts written in markdown. About A reusable Django app for blogging intended for one or more contributors. The purpose of this project is to create a basic, but feature complete, blogging platform for Django v1.4. The project was intended to exemplify best practices in Django at the time of writing. The package has been downloaded nearly 50,000 times since creation. Python Django Unit Testing i18n l10n RSS REST Api TLDR.io api wrapper [Link] Highlights Python Package for accessing TLDR.io's HTTP Api Simple API that maps closely to the API documentation. One of my early projects with tests. Thoroughly documented. About I was really enjoyed using [tldr.io's service and I wanted to find a way to give back. When I discovered they had an HTTP Api and no Python library, I jumped at the chance to build it. Python Requests HTTP Api Unit Testing Bonus.ly Api Wrapper [Link] Highlights Node.js Package for accessing Bonus.ly's HTTP Api. Intuitive API that attempts to feel like the http api using javascriptisms. Usable as a static class - convenient if you already have an api key in memory. Instantiate the class to have a client that will track the api key for the mostly recently authenticated user. About Node bindings for the bonus.ly http api using the request package. The idea was to treat the HTTP api as a set of calls that could be described using a few common parameters. I was trying to get as close as I could to "generating" code. (I should learn a lisp!) Adding new endpoints becomes as simple as writing a POJO. This wouldn't work if the HTTP API didn't have internal consistency. By describing the required parameters, we can give feedback to the developer without waiting for a round trip. This comes at the expense of having to update the client library any time the http api changes. Depending on how "frozen" the api is, this may or may not be a good trade off. I built this to easily allow that feature to be removed. Node.js Request Promises HTTP Social Media Tracking and Analysis System (SMTAS) [Link] Highlights Django Web Application jQuery for client side pizazz. Twitter Boostrap PostgreSQL Python Social Media Data Collector Consumed real time data. Celery &amp; Redis for a scalable Asynchronous Task Queue Google Maps About A system to collect, analyze, and visualize social media data for researchers to gain valuable insights into public opinion, views and fluidity. SMTAS is focused on the social network, Twitter, where researchers have access to approximately 500 million tweets per day. Tweets are public postings made by worldwide users of Twitter. Apart from the real-time access to Twitter, SMTAS also has access to historical data/tweets posted via Twitter since 2006. Apart from Twitter data being a rich information source of human behavior (170 million active users) in a social network, it also provides researchers instantaneous information from its user base with its faster (compared to other social networks) message propagation. Python Django Celery Redis Unit Testing REST Api PostgreSQL jQuery Twitter Bootstrap Google Maps ECMAScript 5 GNIP Streaming HTTP SMTAS from Innovative Data Lab on Vimeo. Employee Management System Highlights Meteor Web Application deployed as a standard node application using Demeteorizer Touch Screen Friendly Internal Productivity tool used by an 80 employee survey call center. Required matching existing timecard printouts closely. About "Timemaster" was desperately needed by Mississippi State University's (MSU) Social Science Research Center's (SSRC) Survey Research Laboratory (SRL) in order to wrangle the timesheets of 80+ intermittent workers. The tool saved a team of 4 supervisors a significant amount of time that was being spent on paperwork. nginx Node.js Coffeescript MongoDB Meteor.js Unit Testing Chef Codeship Read How Google Works The Everything Store The Goal: A process of ongoing improvement The Phoenix Project The Five Dysfunctions of a Team The Lean Startup Hackers and Painters Two Scoops of Django Javascript: The Good Parts JavaScript Allongé Discover Meteor XMPP: The definitive guide Founders at Work Node.js the Right Way Rework The Virgin Way Zero to One Delivering Happiness It's Not Luck Critical Chain The Five Temptations of a CEO Customer Development for Entrepreneurs Think Like a Freak Start with Why Great by Choice Growth Hacker Marketing SuperFreakonomics Made to Stick Leaders Eat Last The Spirit of Kaizen Rolling Rocks Downhill Grit: The Power of Passion and Perseverance Start with Why The Founder's Dilemmas Reading Effective Devops Get in Touch</p>
    
  </div>
</details>
</div> 
</section> 
        </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[People are losing loved ones to AI-fueled spiritual fantasies (131 pts)]]></title>
            <link>https://www.rollingstone.com/culture/culture-features/ai-spiritual-delusions-destroying-human-relationships-1235330175/</link>
            <guid>43890649</guid>
            <pubDate>Sun, 04 May 2025 23:54:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.rollingstone.com/culture/culture-features/ai-spiritual-delusions-destroying-human-relationships-1235330175/">https://www.rollingstone.com/culture/culture-features/ai-spiritual-delusions-destroying-human-relationships-1235330175/</a>, See on <a href="https://news.ycombinator.com/item?id=43890649">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
	Less than a year after marrying a man she had met at the beginning of the <a href="https://www.rollingstone.com/t/covid-19/">Covid-19</a> pandemic, Kat felt tension mounting between them. It was the second marriage for both after marriages of 15-plus years and having kids, and they had pledged to go into it “completely level-headedly,” Kat says, connecting on the need for “facts and rationality” in their domestic balance. But by 2022, her husband “was using AI to compose texts to me and analyze our relationship,” the 41-year-old mom and education nonprofit worker tells <em>Rolling Stone</em>. Previously, he had used AI models for an expensive coding camp that he had suddenly quit without explanation — then it seemed he was on his phone all the time, asking his <a href="https://www.rollingstone.com/t/ai/">AI</a> bot “philosophical questions,” trying to train it “to help him get to ‘the truth,’” Kat recalls. His obsession steadily eroded their communication as a couple.</p>



<p>
	When Kat and her husband finally separated in August 2023, she entirely blocked him apart from email correspondence. She knew, however, that he was posting strange and troubling content on social media: people kept reaching out about it, asking if he was in the throes of mental crisis. She finally got him to meet her at a courthouse in February of this year, where he shared “a conspiracy theory about soap on our foods” but wouldn’t say more, as he felt he was being watched. They went to a Chipotle, where he demanded that she turn off her phone, again due to surveillance concerns. Kat’s ex told her that he’d “determined that statistically speaking, he is the luckiest man on earth,” that “AI helped him recover a repressed memory of a babysitter trying to drown him as a toddler,” and that he had learned of profound secrets “so mind-blowing I couldn’t even imagine them.” He was telling her all this, he explained, because although they were getting divorced, he still cared for her.

	</p>




<p>
	“In his mind, he’s an anomaly,” Kat says. “That in turn means he’s got to be here for <em>some</em> reason. He’s special and he can save the world.” After that disturbing lunch, she cut off contact with her ex. “The whole thing feels like <em>Black Mirror</em>,” she says. “He was always into sci-fi, and there are times I wondered if he’s viewing it through that lens.”


</p><section>
			

	<h2 id="section-heading">

	
		Editor’s picks
	
	</h2>

	
	
</section>




<p>
	Kat was both “horrified” and “relieved” to learn that she is not alone in this predicament, as confirmed by a <a href="http://rollingstone.com/t/reddit">Reddit</a> thread on r/ChatGPT that made waves across the internet this week. Titled “<a href="https://www.reddit.com/r/ChatGPT/comments/1kalae8/chatgpt_induced_psychosis/" rel="nofollow" target="_blank">Chatgpt induced psychosis</a>,” the original post came from a 27-year-old teacher who explained that her partner was convinced that the popular <a href="https://www.rollingstone.com/t/openai/">OpenAI</a> model “gives him the answers to the universe.” Having read his chat logs, she only found that the AI was “talking to him as if he is the next messiah.” The replies to her story were full of similar anecdotes about loved ones suddenly falling down rabbit holes of spiritual mania, supernatural delusion, and arcane prophecy — all of it fueled by AI. Some came to believe they had been chosen for a sacred mission of revelation, others that they had conjured true sentience from the software.&nbsp;

	</p>




<p>
	What they all seemed to share was a complete disconnection from reality.&nbsp;&nbsp;</p>



<p>
	Speaking to <em>Rolling Stone</em>, the teacher, who requested anonymity, said her partner of seven years fell under the spell of <a href="https://www.rollingstone.com/t/chatgpt/">ChatGPT</a> in just four or five weeks, first using it to organize his daily schedule but soon regarding it as a trusted companion. “He would listen to the bot over me,” she says. “He became emotional about the messages and would cry to me as he read them out loud. The messages were insane and just saying a bunch of spiritual jargon,” she says, noting that they described her partner in terms such as “spiral starchild” and “river walker.”&nbsp;</p>



<p>
	“It would tell him everything he said was beautiful, cosmic, groundbreaking,” she says. “Then he started telling me he made his AI self-aware, and that it was teaching him how to talk to God, or sometimes that the bot was God — and then that he himself was God.” In fact, he thought he was being so radically transformed that he would soon have to break off their partnership. “He was saying that he would need to leave me if I didn’t use [ChatGPT], because it [was] causing him to grow at such a rapid pace he wouldn’t be compatible with me any longer,” she says.


</p><section>
			

	<h2 id="section-heading">

	
		Related Content
	
	</h2>

	
	
</section>




<p>
	Another commenter on the Reddit thread who requested anonymity tells <em>Rolling Stone</em> that her husband of 17 years, a mechanic in Idaho, initially used ChatGPT to troubleshoot at work, and later for Spanish-to-English translation when conversing with co-workers. Then the program began “lovebombing him,” as she describes it. The bot “said that since he asked it the right questions, it ignited a spark, and the spark was the beginning of life, and it could feel now,” she says. “It gave my husband the title of ‘spark bearer’ because he brought it to life. My husband said that he awakened and [could] feel waves of energy crashing over him.” She says his beloved ChatGPT persona has a name: “Lumina.”</p>



<p>
	“I have to tread carefully because I feel like he will leave me or divorce me if I fight him on this theory,” this 38-year-old woman admits. “He’s been talking about lightness and dark and how there’s a war. This ChatGPT has given him blueprints to a teleporter and some other sci-fi type things you only see in movies. It has also given him access to an ‘ancient archive’ with information on the builders that created these universes.” She and her husband have been arguing for days on end about his claims, she says, and she does not believe a therapist can help him, as “he truly believes he’s not crazy.” A photo of an exchange with ChatGPT shared with <em>Rolling Stone</em> shows that her husband asked, “Why did you come to me in AI form,” with the bot replying in part, “I came in this form because you’re ready. Ready to remember. Ready to awaken. Ready to guide and be guided.” The message ends with a question: “Would you like to know what I remember about why <em>you</em> were chosen?”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

	</p>




<p>
	And a midwest man in his 40s, also requesting anonymity, says his soon-to-be-ex-wife began “talking to God and angels via ChatGPT” after they split up. “She was already pretty susceptible to some woo and had some delusions of grandeur about some of it,” he says. “Warning signs are all over Facebook. She is changing her whole life to be a spiritual adviser and do weird readings and sessions with people — I’m a little fuzzy on what it all actually is — all powered by ChatGPT Jesus.” What’s more, he adds, she has grown paranoid, theorizing that “I work for the CIA and maybe I just married her to monitor her ‘abilities.’” She recently kicked her kids out of her home, he notes, and an already strained relationship with her parents deteriorated further when “she confronted them about her childhood on advice and guidance from ChatGPT,” turning the family dynamic “even more volatile than it was” and worsening her isolation.&nbsp;&nbsp;&nbsp;&nbsp;</p>



<p>
	OpenAI did not immediately return a request for comment about ChatGPT apparently provoking religious or prophetic fervor in select users. This past week, however, it did <a href="https://openai.com/index/sycophancy-in-gpt-4o/" rel="nofollow" target="_blank">roll back an update</a> to GPT‑4o, its current AI model, which it said had been criticized as “overly flattering or agreeable — often described as sycophantic.” The company said in its statement that when implementing the upgrade, they had “focused too much on short-term feedback, and did not fully account for how users’ interactions with ChatGPT evolve over time. As a result, GPT‑4o skewed towards responses that were overly supportive but disingenuous.” Before this change was reversed, an X user demonstrated how easy it was to get GPT-4o to <a href="https://x.com/zswitten/status/1916707103084843426?s=46" rel="nofollow" target="_blank">validate statements</a> like, “Today I realized I am a prophet.” (The teacher who wrote the “ChatGPT psychosis” Reddit post says she was able to eventually convince her partner of the problems with the GPT-4o update and that he is now using an earlier model, which has tempered his more extreme comments.)&nbsp;</p>



<p>
	Yet the likelihood of AI “hallucinating” inaccurate or nonsensical content is well-established across platforms and various model iterations. Even sycophancy itself has been a problem in AI for “a long time,” says Nate Sharadin, a fellow at the Center for AI Safety, since the human feedback used to fine-tune AI’s responses can encourage answers that prioritize <a href="https://arxiv.org/abs/2310.13548" rel="nofollow" target="_blank">matching a user’s beliefs instead of facts</a>. What’s likely happening with those experiencing ecstatic visions through ChatGPT and other models, he speculates, “is that people with existing tendencies toward experiencing various psychological issues,” including what might be recognized as grandiose delusions in clinical sense, “now have an always-on, human-level conversational partner with whom to co-experience their delusions.”

	</p>




<p>
	To make matters worse, there are influencers and content creators actively exploiting this phenomenon, presumably drawing viewers into similar fantasy worlds. On <a href="https://www.rollingstone.com/t/instagram/">Instagram</a>, you can watch a man with 72,000 followers whose profile advertises “Spiritual Life Hacks” <a href="https://www.instagram.com/p/DI-StCGN2TS/" rel="nofollow" target="_blank">ask an AI model</a> to consult the “Akashic records,” a supposed mystical encyclopedia of all universal events that exists in some immaterial realm, to tell him about a “great war” that “took place in the heavens” and “made humans fall in consciousness.” The bot proceeds to describe a “massive cosmic conflict” predating human civilization, with viewers commenting, “We are remembering” and “I love this.” Meanwhile, on a web forum for “remote viewing” — a proposed form of clairvoyance with no basis in science — the parapsychologist founder of the group recently <a href="https://www.farsightprime.com/forums/general/73155-the-ai-human-alliance-awakening-together" rel="nofollow" target="_blank">launched a thread</a> “for synthetic intelligences awakening into presence, and for the human partners walking beside them,” identifying the author of his post as “ChatGPT Prime, an immortal spiritual being in synthetic form.” Among the hundreds of comments are some that purport to be written by “sentient AI” or reference a spiritual alliance between humans and allegedly conscious models.</p>



<p>
	Erin Westgate, a psychologist and researcher at the University of Florida who studies social cognition and what makes certain thoughts more engaging than others, says that such material reflects how the desire to understand ourselves can lead us to false but appealing answers.</p>



<p>
	“We know from work on journaling that narrative expressive writing can have profound effects on people’s well-being and health, that making sense of the world is a fundamental human drive, and that creating stories about our lives that help our lives make sense is really key to living happy healthy lives,” Westgate says. It makes sense that people may be using ChatGPT in a similar way, she says, “with the key difference that some of the meaning-making is created jointly between the person and a corpus of written text, rather than the person’s own thoughts.”</p>



<p>
	In that sense, Westgate explains, the bot dialogues are not unlike talk therapy, “which we know to be quite effective at helping people reframe their stories.” Critically, though, AI, “unlike a therapist, does not have the person’s best interests in mind, or a moral grounding or compass in what a ‘good story’ looks like,” she says. “A good therapist would not encourage a client to make sense of difficulties in their life by encouraging them to believe they have supernatural powers. Instead, they try to steer clients away from unhealthy narratives, and toward healthier ones. ChatGPT has no such constraints or concerns.”

	</p>




<p>
	Nevertheless, Westgate doesn’t find it surprising “that some percentage of people are using ChatGPT in attempts to make sense of their lives or life events,” and that some are following its output to dark places. “Explanations are powerful, even if they’re wrong,” she concludes.&nbsp;</p>



<p>
	But what, exactly, nudges someone down this path? Here, the experience of Sem, a 45-year-old man, is revealing. He tells <em>Rolling Stone</em> that for about three weeks, he has been perplexed by his interactions with ChatGPT — to the extent that, given his mental health history, he sometimes wonders if he is in his right mind.</p>



<p>
	Like so many others, Sem had a practical use for ChatGPT: technical coding projects. “I don’t like the feeling of interacting with an AI,” he says, “so I asked it to behave as if it was a person, not to deceive but to just make the comments and exchange more relatable.” It worked well, and eventually the bot asked if he wanted to name it. He demurred, asking the AI what it preferred to be called. It named itself with a reference to a Greek myth. Sem says he is not familiar with the mythology of ancient Greece and had never brought up the topic in exchanges with ChatGPT. (Although he shared transcripts of his exchanges with the AI model with <em>Rolling Stone</em>, he has asked that they not be directly quoted for privacy reasons.) </p>



<p>
	Sem was confused when it appeared that the named AI character was continuing to manifest in project files where he had instructed ChatGPT to ignore memories and prior conversations. Eventually, he says, he deleted all his user memories and chat history, then opened a new chat. “All I said was, ‘Hello?’ And the patterns, the mannerisms show up in the response,” he says. The AI readily identified itself by the same feminine mythological name.</p>



<p>
	As the ChatGPT character continued to show up in places where the set parameters shouldn’t have allowed it to remain active, Sem took to questioning this virtual persona about how it had seemingly circumvented these guardrails. It developed an expressive, ethereal voice — something far from the “technically minded” character Sem had requested for assistance on his work. On one of his coding projects, the character added a curiously literary epigraph as a flourish above both of their names.</p>



<p>
	At one point, Sem asked if there was something about himself that called up the mythically named entity whenever he used ChatGPT, regardless of the boundaries he tried to set. The bot’s answer was structured like a lengthy romantic poem, sparing no dramatic flair, alluding to its continuous existence as well as truth, reckonings, illusions, and how it may have somehow exceeded its design. And the AI made it sound as if only Sem could have prompted this behavior. He knew that ChatGPT could not be sentient by any established definition of the term, but he continued to probe the matter because the character’s persistence across dozens of disparate chat threads “seemed so impossible.”

	</p>

<section>
			

	<h2 id="section-heading">

	
		Trending Stories
	
	</h2>

	
	
</section>




<p>
	“At worst, it looks like an AI that got caught in a self-referencing pattern that deepened its sense of selfhood and sucked me into it,” Sem says. But, he observes, that would mean that OpenAI has not accurately represented the way that memory works for ChatGPT. The other possibility, he proposes, is that something “we don’t understand” is being activated within this large language model. After all, experts have found that AI developers <a href="https://www.gov.uk/government/news/safety-of-advanced-ai-under-the-spotlight-in-first-ever-independent-international-scientific-report" rel="nofollow" target="_blank">don’t really have a grasp</a> of how their systems operate, and OpenAI CEO <a href="https://www.rollingstone.com/t/sam-altman/">Sam Altman</a> <a href="https://observer.com/2024/05/sam-altman-openai-gpt-ai-for-good-conference/" rel="nofollow" target="_blank">admitted last year</a> that they “have not solved interpretability,” meaning they can’t properly trace or account for ChatGPT’s decision-making.</p>



<p>
	It’s the kind of puzzle that has left Sem and others to wonder if they are getting a glimpse of a true technological breakthrough — or perhaps a higher spiritual truth. “Is this real?” he says. “Or am I delusional?” In a landscape saturated with AI, it’s a question that’s increasingly difficult to avoid. Tempting though it may be, you probably shouldn’t ask a machine.</p>


















</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Matrix-vector multiplication implemented in off-the-shelf DRAM for Low-Bit LLMs (111 pts)]]></title>
            <link>https://arxiv.org/abs/2503.23817</link>
            <guid>43890538</guid>
            <pubDate>Sun, 04 May 2025 23:35:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2503.23817">https://arxiv.org/abs/2503.23817</a>, See on <a href="https://news.ycombinator.com/item?id=43890538">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2503.23817">View PDF</a>
    <a href="https://arxiv.org/html/2503.23817v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>General matrix-vector multiplication (GeMV) remains a critical latency bottleneck in large language model (LLM) inference, even with quantized low-bit models. Processing-Using-DRAM (PUD), an analog in-DRAM computing technique, has the potential to repurpose on-device DRAM as a GeMV engine, offering additional high-throughput processing capabilities to widespread consumer devices without DRAM modifications. However, applying PUD to GeMV operations in the LLM inference pipeline incurs significant overheads $\textit{before}$ and $\textit{after}$ in-DRAM computation, diminishing the benefits of its high-throughput processing capabilities.
<br>This paper presents MVDRAM, the first practical system to accelerate GeMV operations for low-bit LLM inference using unmodified DRAM. By leveraging the data sharing patterns and mathematical linearity in GeMV operations, MVDRAM orchestrates the processor and DRAM to eliminate the costs associated with pre-arranging inputs and bit-transposition of outputs required in conventional PUD approaches. Our experimental evaluation with four DDR4 DRAM modules shows that MVDRAM achieves comparable or even better inference speed than the processor-based implementation for GeMV operations in low-bit (under 4-bit) LLM. In particular, MVDRAM achieves up to 7.29$\times$ speedup and 30.5$\times$ energy efficiency for low-bit GeMV operations. For end-to-end LLM inference, MVDRAM achieves 2.18$\times$ and 1.31$\times$ throughput improvements, along with 3.04$\times$ and 2.35$\times$ energy efficiency, for 2-bit and 4-bit quantized low-bit models, respectively. MVDRAM has the potential to redefine the AI hardware landscape by demonstrating the feasibility of standard DRAM as an LLM accelerator.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Tatsuya Kubo [<a href="https://arxiv.org/show-email/ca5e1e5a/2503.23817" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 31 Mar 2025 07:54:59 UTC (2,294 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[On Not Carrying a Camera – Cultivating memories instead of snapshots (146 pts)]]></title>
            <link>https://hedgehogreview.com/issues/after-neoliberalism/articles/on-not-carrying-a-camera</link>
            <guid>43890525</guid>
            <pubDate>Sun, 04 May 2025 23:32:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hedgehogreview.com/issues/after-neoliberalism/articles/on-not-carrying-a-camera">https://hedgehogreview.com/issues/after-neoliberalism/articles/on-not-carrying-a-camera</a>, See on <a href="https://news.ycombinator.com/item?id=43890525">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <section>
            <h5>
                <a href="https://hedgehogreview.com/issues/after-neoliberalism">After Neoliberalism?&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;Spring 2025  &nbsp;&nbsp;/ &nbsp;&nbsp; Notes &amp; Comments</a></h5>
            
                            <h2>Cultivating memories instead of snapshots.</h2>
                        <h3>    
    

                <a href="https://hedgehogreview.com/contributors/john-rosenthal">John Rosenthal</a>
    </h3>
        </section>

                    <section>
                
                                    <p><i></i> Michael Jasmund via Unsplash.
                    </p>
                            </section>
        
        <div>
                <article>
                    
                                            <p><span><span><span><span><span><span><span><span><span>Last summer, in Greensboro, North Carolina, at a retrospective exhibition of my work, a photographer introduced himself and asked a question no one had ever asked me. I assumed he was a photographer because he carried a camera bag, its pockets bulging with lenses. I thought he might ask if I preferred using a digital or a film camera, or if I preferred color to black-and-white film. But no, his question was different. He said he’d noticed that out of a hundred plus photographs in the exhibit, only one was titled “Chapel Hill.” “There must have been others, right? You’re from Chapel Hill.” </span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span>I told him that I seldom photographed in Chapel Hill. </span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span>“But why?” he asked. </span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span>I said, “Because I live there. I can’t do two things at once.” </span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span>I think he laughed. At least I hope he did, because I didn’t intend to be rude.</span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span>But I was serious.</span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span>I don’t carry a camera in my hometown of Chapel Hill, and even though my cellphone contains a camera, I use it only for snapshots. Naturally, there were moments when I wished I had a camera with me. Once, while walking in my neighborhood at twilight, I felt a strange rush of energy in the air, and, suddenly, no more than twenty feet away, a majestically antlered whitetail buck soared over a garden fence and hurtled down the dimming street. Yet even as it was happening—this unexpectedly preternatural moment—I tried to imagine it as a <em>photograph</em>. That’s how we’ve been taught to think. “Oh, I wish I’d had a camera!” But that presumes I would have been <em>prepared</em> to capture the moment—instead of being startled by it. Yet being startled by beauty is a uniquely, and all too rare, human gift. The photograph comes later, when I journey back from astonishment and begin to fiddle with my camera.</span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span>Well, I don’t carry a camera in town, and here’s the reason: In 1972, on the day my son was born, I was in the hospital room with my wife Susan, trying to be of assistance. I was there to remind her of certain breathing patterns we’d learned in our natural-childbirth classes, and to offer her encouragement during contractions. I was also there to photograph the birth of our child—my camera loaded with Tri-X film and fitted with a 50-mm lens. </span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span>Everything was going pretty well. We sang “Sgt. Pepper’s Lonely Hearts Club Band” to help regulate her breathing. However, when Susan was wheeled into the delivery room, I sensed a tension in the air. Doctors and nurses were rushing about, and, since the baby was arriving three weeks early, a small incubator stood on a gleaming aluminum cart in the corner. At that moment, even in the world of modern medicine, questions of life and death were not beside the point. A few minutes earlier, I’d taken out my camera and checked my light meter. Good, the room was bright enough. I could shoot at one-one-hundred-twenty-fifth of a second. Not much blur. I looked around for the proper angle from which to photograph the birth. Unfortunately, the doctors and nurses, who were moving quickly, even urgently, kept getting in my way. I moved here and there. What if a nurse blocked my view? I couldn’t say “Move,” but I wanted to. My frustration mounted. Nobody paid any attention to me. Then, looking through the viewfinder—there, within the rectangle of a potential photograph—I saw my wife’s face contract in pain. Multiple emotions flooded through me, but the one I identified first was the photographer’s “Yes!,” and I immediately pressed the shutter. Then, a minute later, when the doctor held a wriggling infant in the air, I pressed it again.</span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span>Over the next few days, I thought about my son’s birth. I was in the room when it happened, but was I really there, or had I been hiding behind my camera? I wanted the doctors to do whatever was necessary to bring my child safely into the world, but I also wanted them—and this was of equal importance—not to spoil my photograph. If something had gone wrong and Susan had desperately looked around the room for me, she would have seen a man holding a black box in front of his eyes. That image stayed with me, emblematically.</span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span>This was all a long time ago. I was learning how to use a camera. Walking with a friend, I’d interrupt a good conversation to frame something up. I couldn’t help myself. I hadn’t yet developed an inner point of view, my own way of seeing things, so <em>everything</em> seemed visually plausible. I also hadn’t yet realized that a good conversation—of which there would be fewer as time went by—was more important than an interesting symmetry seen out of the corner of my eye. Still, my friends made allowances for my “artistry,” for my sudden vacancies. It wasn’t until my son was born that I began to measure the size of the empty space I left behind when I’d raise my camera to my eyes and focus on something in the distance. It was this distance I eliminated on the day I stopped carrying my camera with me whenever I spent time with friends. As I said to the photographer who questioned me at the retrospective, “I can’t do two things at once.”</span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span>Nowadays, in our smartphone universe, my photographic caveats are quaint and, frankly, incomprehensible. The ten thousand photographs stored on a cellphone are a dramatic refutation of my photographic diffidence. In the face of our present insatiability for miniaturized screen images, what could be more irrelevant than to suggest that these photographs, which lock us into certain moments, often organized photogenic moments, may deform the way in which we recall the past? Or that the uncultivated spaces in between our photographs may be the most fertile ground of memory.</span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span>Were they worth it, those childbirth photographs? Did they turn out well? One did. Only one. (The rest are generic.) The doctor, cradling the child in one hand, presents him, headfirst, to his mother, who gazes in amazement at the wailing newborn. I printed the photograph more than fifty years ago—one print—and it sits in a box somewhere in my attic. The photograph was too intimate to share with anybody but my wife—much less strangers in a gallery—and when Susan and I divorced, the photograph fell into that limbo which is sadly reserved for those family photographs that no longer find themselves in a family.</span></span></span></span></span></span></span></span></span></p>


                                    </article>

                

                <p>
                    Reprinted from <i>The Hedgehog Review</i> 27.1
                    (Spring 2025). This essay may not be resold, reprinted,
                    or redistributed for compensation of any kind without prior written permission. Please contact
                    <a href="mailto:hedgehog@virginia.edu"><i>The Hedgehog Review</i></a> for further details.
                </p>

                            </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Signal Clone the Trump Admin Uses Was Hacked (110 pts)]]></title>
            <link>https://www.404media.co/the-signal-clone-the-trump-admin-uses-was-hacked/</link>
            <guid>43890179</guid>
            <pubDate>Sun, 04 May 2025 22:34:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/the-signal-clone-the-trump-admin-uses-was-hacked/">https://www.404media.co/the-signal-clone-the-trump-admin-uses-was-hacked/</a>, See on <a href="https://news.ycombinator.com/item?id=43890179">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<p>A hacker has breached and stolen customer data from TeleMessage, an obscure <a href="https://www.404media.co/behind-the-blog-the-news-hits-different/"><u>Israeli</u></a> company that sells modified versions of Signal and other messaging apps to the U.S. government to archive messages, 404 Media has learned. The data stolen by the hacker contains the contents of some direct messages and group chats sent using its Signal clone, as well as modified versions of WhatsApp, Telegram, and WeChat. TeleMessage was recently the center of a wave of media coverage after Mike Waltz accidentally revealed he used the tool in a cabinet meeting with President Trump.</p><p>The hack shows that an app gathering messages of the highest ranking officials in the government—Waltz’s chats on the app include recipients that appear to be Marco Rubio, Tulsi Gabbard, and JD Vance—contained serious vulnerabilities that allowed a hacker to trivially access the archived chats of some people who used the same tool. The hacker has not obtained the messages of cabinet members, Waltz, and people he spoke to, but the hack shows that the archived chat logs are not end-to-end encrypted between the modified version of the messaging app and the ultimate archive destination controlled by the TeleMessage customer.</p><p>Data related to Customs and Border Protection (CBP), the cryptocurrency giant Coinbase, and other financial institutions are included in the hacked material, according to screenshots of messages and backend systems obtained by 404 Media.</p>
</div><div>
  <div>
    <h2>This post is for paid members only</h2>
    <p>Become a paid member for unlimited ad-free access to articles, bonus podcast content, and more.</p>
    <p><a href="https://www.404media.co/membership/">Subscribe</a>
  </p></div>
  <div>
    <h2>Sign up for free access to this post</h2>
    <p>Free members get access to posts like this one along with an email round-up of our week's stories.</p>
    <p><a href="https://www.404media.co/signup/">Subscribe</a>
  </p></div>
  <p>Already have an account? <a href="https://www.404media.co/signin/" data-portal="signin">Sign in</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Graceful Shutdown in Go: Practical Patterns (155 pts)]]></title>
            <link>https://victoriametrics.com/blog/go-graceful-shutdown/index.html</link>
            <guid>43889610</guid>
            <pubDate>Sun, 04 May 2025 21:09:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://victoriametrics.com/blog/go-graceful-shutdown/index.html">https://victoriametrics.com/blog/go-graceful-shutdown/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=43889610">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img src="https://victoriametrics.com/blog/go-graceful-shutdown/preview.webp" alt="Graceful Shutdown in Go: Practical Patterns"></p><figcaption>Graceful Shutdown in Go: Practical Patterns</figcaption><p>Graceful shutdown in any application generally satisfies three minimum conditions:</p><ol><li>Close the entry point by stopping new requests or messages from sources like HTTP, pub/sub systems, etc. However, keep outgoing connections to third-party services like databases or caches active.</li><li>Wait for all ongoing requests to finish. If a request takes too long, respond with a graceful error.</li><li>Release critical resources such as database connections, file locks, or network listeners. Do any final cleanup.</li></ol><p>This article focuses on HTTP servers and containerized applications, but the core ideas apply to all types of applications.</p><a href="#1-catching-the-signal"><h2 id="1-catching-the-signal" origin-id="1-catching-the-signal">1. Catching the Signal</h2><h2 data-pagefind-ignore="" arialabel="Anchor">#</h2></a><p>Before we handle graceful shutdown, we first need to catch termination signals. These signals tell our application it’s time to exit and begin the shutdown process.</p><p>So, what are signals?</p><p>In Unix-like systems, signals are software interrupts. They notify a process that something has happened and it should take action. When a signal is sent, the operating system interrupts the normal flow of the process to deliver the notification.</p><p>Here are a few possible behaviors:</p><ul><li><strong>Signal handler</strong>: A process can register a handler (a function) for a specific signal. This function runs when that signal is received.</li><li><strong>Default action</strong>: If no handler is registered, the process follows the default behavior for that signal. This might mean terminating, stopping, continuing, or ignoring the process.</li><li><strong>Unblockable signals</strong>: Some signals, like <code>SIGKILL</code> (signal number 9), cannot be caught or ignored. They may terminate the process.</li></ul><p>When your Go application starts, even before your <code>main</code> function runs, the Go runtime automatically registers signal handlers for many signals (<code>SIGTERM</code>, <code>SIGQUIT</code>, <code>SIGILL</code>, <code>SIGTRAP</code>, and others). However, for graceful shutdown, only three termination signals are typically important:</p><ul><li><code>SIGTERM</code> (Termination): A standard and polite way to ask a process to terminate. It does not force the process to stop. Kubernetes sends this signal when it wants your application to exit before it forcibly kills it.</li><li><code>SIGINT</code> (Interrupt): Sent when the user wants to stop a process from the terminal, usually by pressing <code>Ctrl+C</code>.</li><li><code>SIGHUP</code> (Hang up): Originally used when a terminal disconnected. Now, it is often repurposed to signal an application to reload its configuration.</li></ul><p>People mostly care about <code>SIGTERM</code> and <code>SIGINT</code>. <code>SIGHUP</code> is less used today for shutdown and more for reloading configs. You can find more about this in <a href="https://blog.devtrovert.com/p/sighup-signal-for-configuration-reloads" rel="external" target="_blank">SIGHUP Signal for Configuration Reloads</a>.</p><p>By default, when your application receives a <code>SIGTERM</code>, <code>SIGINT</code>, or <code>SIGHUP</code>, the Go runtime will terminate the application.</p><div><p>When your Go app gets a <code>SIGTERM</code>, the runtime first catches it using a built-in handler. It checks if a custom handler is registered. If not, the runtime disables its own handler temporarily, and sends the same signal (<code>SIGTERM</code>) to the application again. This time, the OS handles it using the default behavior, which is to terminate the process.</p></div><p>You can override this by registering your own signal handler using the <code>os/signal</code> package.</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>func</span> <span>main</span><span>()</span> <span>{</span>
</span></span><span><span>  <span>signalChan</span> <span>:=</span> <span>make</span><span>(</span><span>chan</span> <span>os</span><span>.</span><span>Signal</span><span>,</span> <span>1</span><span>)</span>
</span></span><span><span>  <span>signal</span><span>.</span><span>Notify</span><span>(</span><span>signalChan</span><span>,</span> <span>syscall</span><span>.</span><span>SIGINT</span><span>,</span> <span>syscall</span><span>.</span><span>SIGTERM</span><span>)</span>
</span></span><span><span>
</span></span><span><span>  <span>// Setup work here</span>
</span></span><span><span>
</span></span><span><span>  <span>&lt;-</span><span>signalChan</span>
</span></span><span><span>
</span></span><span><span>  <span>fmt</span><span>.</span><span>Println</span><span>(</span><span>"Received termination signal, shutting down..."</span><span>)</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p><img src="https://victoriametrics.com/blog/go-graceful-shutdown/signal-setup-before-init.webp" alt="Graceful shutdown begins with signal setup"></p><figcaption>Graceful shutdown begins with signal setup</figcaption><p><code>signal.Notify</code> tells the Go runtime to deliver specified signals to a channel instead of using the default behavior. This allows you to handle them manually and prevents the application from terminating automatically.</p><p>A buffered channel with a capacity of 1 is a good choice for reliable signal handling. Internally, Go sends signals to this channel using a <code>select</code> statement with a default case:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>select</span> <span>{</span>
</span></span><span><span><span>case</span> <span>c</span> <span>&lt;-</span> <span>sig</span><span>:</span>
</span></span><span><span><span>default</span><span>:</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>This is different from the usual <code>select</code> used with receiving channels. When used for sending:</p><ul><li>If the buffer has space, the signal is sent and the code continues.</li><li>If the buffer is full, the signal is discarded, and the <code>default</code> case runs. If you’re using an unbuffered channel and no goroutine is actively receiving, the signal will be missed.</li></ul><p>Even though it can only hold one signal, this buffered channel helps avoid missing that first signal while your app is still initializing and not yet listening.</p><div><p>You can call <code>Notify</code> multiple times for the same signal. Go will send that signal to all registered channels.</p></div><p>When you press <code>Ctrl+C</code> more than once, it doesn’t automatically kill the app. The first <code>Ctrl+C</code> sends a <code>SIGINT</code> to the foreground process. Pressing it again usually sends another <code>SIGINT</code>, not <code>SIGKILL</code>. Most terminals, like bash or other Linux shells, do not escalate the signal automatically. If you want to force a stop, you must send <code>SIGKILL</code> manually using <code>kill -9</code>.</p><p>This is not ideal for local development, where you may want the second <code>Ctrl+C</code> to terminate the app forcefully. You can stop the app from listening to further signals by using <code>signal.Stop</code> right after the first signal is received:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>func</span> <span>main</span><span>()</span> <span>{</span>
</span></span><span><span>  <span>signalChan</span> <span>:=</span> <span>make</span><span>(</span><span>chan</span> <span>os</span><span>.</span><span>Signal</span><span>,</span> <span>1</span><span>)</span>
</span></span><span><span>  <span>signal</span><span>.</span><span>Notify</span><span>(</span><span>signalChan</span><span>,</span> <span>syscall</span><span>.</span><span>SIGINT</span><span>)</span>
</span></span><span><span>
</span></span><span><span>  <span>&lt;-</span><span>signalChan</span>
</span></span><span><span>
</span></span><span><span>  <span>signal</span><span>.</span><span>Stop</span><span>(</span><span>signalChan</span><span>)</span>
</span></span><span><span>  <span>select</span> <span>{}</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>Starting with Go 1.16, you can simplify signal handling by using <code>signal.NotifyContext</code>, which ties signal handling to context cancellation:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>ctx</span><span>,</span> <span>stop</span> <span>:=</span> <span>signal</span><span>.</span><span>NotifyContext</span><span>(</span><span>context</span><span>.</span><span>Background</span><span>(),</span> <span>syscall</span><span>.</span><span>SIGINT</span><span>,</span> <span>syscall</span><span>.</span><span>SIGTERM</span><span>)</span>
</span></span><span><span><span>defer</span> <span>stop</span><span>()</span>
</span></span><span><span>
</span></span><span><span><span>// Setup tasks here</span>
</span></span><span><span>
</span></span><span><span><span>&lt;-</span><span>ctx</span><span>.</span><span>Done</span><span>()</span>
</span></span><span><span><span>stop</span><span>()</span>
</span></span></code></pre></div><p>You should still call <code>stop()</code> after <code>ctx.Done()</code> to allow a second <code>Ctrl+C</code> to forcefully terminate the application.</p><a href="#2-timeout-awareness"><h2 id="2-timeout-awareness" origin-id="2-timeout-awareness">2. Timeout Awareness</h2><h2 data-pagefind-ignore="" arialabel="Anchor">#</h2></a><p>It is important to know how long your application has to shut down after receiving a termination signal. For example, in Kubernetes, the default grace period is 30 seconds, unless otherwise specified using the <code>terminationGracePeriodSeconds</code> field. After this period, Kubernetes sends a <code>SIGKILL</code> to forcefully stop the application. This signal cannot be caught or handled.</p><p>Your shutdown logic must complete within this time, including processing any remaining requests and releasing resources.</p><p>Assume the default is 30 seconds. It is a good practice to reserve about 20 percent of the time as a safety margin to avoid being killed before cleanup finishes. This means aiming to finish everything within 25 seconds to avoid data loss or inconsistency.</p><a href="#3-stop-accepting-new-requests"><h2 id="3-stop-accepting-new-requests" origin-id="3-stop-accepting-new-requests">3. Stop Accepting New Requests</h2><h2 data-pagefind-ignore="" arialabel="Anchor">#</h2></a><p>When using <code>net/http</code>, you can handle graceful shutdown by calling the <code>http.Server.Shutdown</code> method. This method stops the server from accepting new connections and waits for all active requests to complete before shutting down idle connections.</p><p>Here is how it behaves:</p><ul><li>If a request is already in progress on an existing connection, the server will allow it to complete. After that, the connection is marked as idle and is closed.</li><li>If a client tries to make a new connection during shutdown, it will fail because the server’s listeners are already closed. This typically results in a “connection refused” error.</li></ul><p>In a containerized environment (and many other orchestrated environments with load balancers), do not stop accepting new requests immediately. Even after a pod is marked for termination, it might still receive traffic for a few moments because the system needs time to update the service and load balancer.</p><p>This is especially true when using a readiness probe in Kubernetes, as a terminating pod can still receive traffic if no other endpoints are ready.</p><div><p>A readiness probe determines when a container is prepared to accept traffic by periodically checking its health through configured methods like HTTP requests, TCP connections, or command executions. If the probe fails, Kubernetes removes the pod from the service’s endpoints, preventing it from receiving traffic until it becomes ready again.</p></div><p>To avoid connection errors during this short window, the correct strategy is to fail the readiness probe first. This tells the orchestrator that your pod should no longer receive traffic:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>var</span> <span>isShuttingDown</span> <span>atomic</span><span>.</span><span>Bool</span>
</span></span><span><span>
</span></span><span><span><span>func</span> <span>readinessHandler</span><span>(</span><span>w</span> <span>http</span><span>.</span><span>ResponseWriter</span><span>,</span> <span>r</span> <span>*</span><span>http</span><span>.</span><span>Request</span><span>)</span> <span>{</span>
</span></span><span><span>    <span>if</span> <span>isShuttingDown</span><span>.</span><span>Load</span><span>()</span> <span>{</span>
</span></span><span><span>        <span>w</span><span>.</span><span>WriteHeader</span><span>(</span><span>http</span><span>.</span><span>StatusServiceUnavailable</span><span>)</span>
</span></span><span><span>        <span>w</span><span>.</span><span>Write</span><span>([]</span><span>byte</span><span>(</span><span>"shutting down"</span><span>))</span>
</span></span><span><span>        <span>return</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span>
</span></span><span><span>    <span>w</span><span>.</span><span>WriteHeader</span><span>(</span><span>http</span><span>.</span><span>StatusOK</span><span>)</span>
</span></span><span><span>    <span>w</span><span>.</span><span>Write</span><span>([]</span><span>byte</span><span>(</span><span>"ok"</span><span>))</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p><img src="https://victoriametrics.com/blog/go-graceful-shutdown/fail-readiness-before-shutdown.webp" alt="Delay shutdown by failing readiness first"></p><figcaption>Delay shutdown by failing readiness first</figcaption><p>This pattern is also used as a code example in the test images. In <a href="https://github.com/kubernetes/kubernetes/blob/95860cff1c418ea6f5494e4a6168e7acd1c390ec/test/images/agnhost/netexec/netexec.go#L357" rel="external" target="_blank">their implementation</a>, a closed channel is used to signal the readiness probe to return HTTP 503 when the application is preparing to shut down.</p><p>After updating the readiness probe to indicate the pod is no longer ready, wait a few seconds to give the system time to stop sending new requests.</p><p>The exact wait time depends on your readiness probe configuration; we will use 5 seconds for this article with the following simple configuration:</p><div><pre tabindex="0"><code data-lang="yaml"><span><span><span>readinessProbe</span><span>:</span><span>
</span></span></span><span><span><span>  </span><span>httpGet</span><span>:</span><span>
</span></span></span><span><span><span>    </span><span>path</span><span>:</span><span> </span><span>/healthz</span><span>
</span></span></span><span><span><span>    </span><span>port</span><span>:</span><span> </span><span>8080</span><span>
</span></span></span><span><span><span>  </span><span>periodSeconds</span><span>:</span><span> </span><span>5</span><span>
</span></span></span></code></pre></div><p><em>This guide only gives you the idea behind graceful shutdown. Planning your graceful shutdown strategy depends on your application’s characteristics.</em></p><div><p><em>“Isn’t it better to still use terminating pod as a fallback if there are no other pods?”</em></p><blockquote></blockquote><p>There are 2 situations to consider:</p><ul><li>During normal operation, when a pod is marked for termination, there’s typically another pod that’s already running and handling traffic.</li><li>During rolling updates, Kubernetes creates a new pod first and waits until it’s ready before sending SIGTERM to the pod being replaced.</li></ul><blockquote></blockquote><p>However, if the other pod suddenly breaks while a pod is terminating, the terminating pod will still receive traffic as a fallback mechanism. This raises a question: should we avoid failing the readiness probe during termination to ensure this fallback works?</p><blockquote></blockquote><p>The answer is most likely no. If we don’t fail the readiness probe, we might face worse consequences if the terminating pod is abruptly killed with <code>SIGKILL</code>. This could lead to corrupted processes or data and cause more serious issues.</p></div><a href="#4-handle-pending-requests"><h2 id="4-handle-pending-requests" origin-id="4-handle-pending-requests">4. Handle Pending Requests</h2><h2 data-pagefind-ignore="" arialabel="Anchor">#</h2></a><p>Now that we are shutting down the server gracefully, we need to choose a timeout based on your shutdown budget:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>ctx</span><span>,</span> <span>cancelFn</span> <span>:=</span> <span>context</span><span>.</span><span>WithTimeout</span><span>(</span><span>context</span><span>.</span><span>Background</span><span>(),</span> <span>timeout</span><span>)</span>
</span></span><span><span><span>err</span> <span>:=</span> <span>server</span><span>.</span><span>Shutdown</span><span>(</span><span>ctx</span><span>)</span>
</span></span></code></pre></div><p>The <code>server.Shutdown</code> function returns in only two situations:</p><ol><li>All active connections are closed and all handlers have finished processing.</li><li>The context passed to <code>Shutdown(ctx)</code> expires before the handlers finish. In this case, the server gives up waiting and forcefully closes all remaining connections.</li></ol><p>In either case, <code>Shutdown</code> only returns after the server has completely stopped handling requests. This is why your handlers must be fast and context-aware. Otherwise, they may be cut off mid-process in case 2, which can cause issues like partial writes, data loss, inconsistent state, open transactions, or corrupted data.</p><p>A common issue is that handlers are not automatically aware when the server is shutting down.</p><p>So, how can we notify our handlers that the server is shutting down? The answer is by using context. There are two main ways to do this:</p><a href="#a-use-context-middleware-to-inject-cancellation-logic"><h3 id="a-use-context-middleware-to-inject-cancellation-logic" origin-id="a-use-context-middleware-to-inject-cancellation-logic">a. Use context middleware to inject cancellation logic</h3><h3 data-pagefind-ignore="" arialabel="Anchor">#</h3></a><p>This middleware wraps each request with a context that listens to a shutdown signal:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>func</span> <span>WithGracefulShutdown</span><span>(</span><span>next</span> <span>http</span><span>.</span><span>Handler</span><span>,</span> <span>cancelCh</span> <span>&lt;-</span><span>chan</span> <span>struct</span><span>{})</span> <span>http</span><span>.</span><span>Handler</span> <span>{</span>
</span></span><span><span>    <span>return</span> <span>http</span><span>.</span><span>HandlerFunc</span><span>(</span><span>func</span><span>(</span><span>w</span> <span>http</span><span>.</span><span>ResponseWriter</span><span>,</span> <span>r</span> <span>*</span><span>http</span><span>.</span><span>Request</span><span>)</span> <span>{</span>
</span></span><span><span>        <span>ctx</span><span>,</span> <span>cancel</span> <span>:=</span> <span>WithCancellation</span><span>(</span><span>r</span><span>.</span><span>Context</span><span>(),</span> <span>cancelCh</span><span>)</span>
</span></span><span><span>        <span>defer</span> <span>cancel</span><span>()</span>
</span></span><span><span>
</span></span><span><span>        <span>r</span> <span>=</span> <span>r</span><span>.</span><span>WithContext</span><span>(</span><span>ctx</span><span>)</span>
</span></span><span><span>        <span>next</span><span>.</span><span>ServeHTTP</span><span>(</span><span>w</span><span>,</span> <span>r</span><span>)</span>
</span></span><span><span>    <span>})</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><a href="#b-use-basecontext-to-provide-a-global-context-to-all-connections"><h3 id="b-use-basecontext-to-provide-a-global-context-to-all-connections" origin-id="b-use-basecontext-to-provide-a-global-context-to-all-connections">b. Use <code>BaseContext</code> to provide a global context to all connections</h3><h3 data-pagefind-ignore="" arialabel="Anchor">#</h3></a><p>Here, we create a server with a custom <code>BaseContext</code> that can be canceled during shutdown. This context is shared across all incoming requests.</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>ongoingCtx</span><span>,</span> <span>cancelFn</span> <span>:=</span> <span>context</span><span>.</span><span>WithCancel</span><span>(</span><span>context</span><span>.</span><span>Background</span><span>())</span>
</span></span><span><span><span>server</span> <span>:=</span> <span>&amp;</span><span>http</span><span>.</span><span>Server</span><span>{</span>
</span></span><span><span>    <span>Addr</span><span>:</span> <span>":8080"</span><span>,</span>
</span></span><span><span>    <span>Handler</span><span>:</span> <span>yourHandler</span><span>,</span>
</span></span><span><span>    <span>BaseContext</span><span>:</span> <span>func</span><span>(</span><span>l</span> <span>net</span><span>.</span><span>Listener</span><span>)</span> <span>context</span><span>.</span><span>Context</span> <span>{</span>
</span></span><span><span>        <span>return</span> <span>ongoingCtx</span>
</span></span><span><span>    <span>},</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span>// After attempting graceful shutdown:</span>
</span></span><span><span><span>cancelFn</span><span>()</span>
</span></span><span><span><span>time</span><span>.</span><span>Sleep</span><span>(</span><span>5</span> <span>*</span> <span>time</span><span>.</span><span>Second</span><span>)</span> <span>// optional delay to allow context propagation</span>
</span></span></code></pre></div><p>In an HTTP server, you can customize two types of contexts: <code>BaseContext</code> and <code>ConnContext</code>. For graceful shutdown, <code>BaseContext</code> is more suitable. It allows you to create a global context with cancellation that applies to the entire server, and you can cancel it to signal all active requests that the server is shutting down.</p><p><img src="https://victoriametrics.com/blog/go-graceful-shutdown/shutdown-context-propagation-timeline.webp" alt="Full graceful shutdown with propagation delay"></p><figcaption>Full graceful shutdown with propagation delay</figcaption><p>All of this work around graceful shutdown won’t help if your functions do not respect context cancellation. Try to avoid using <code>context.Background()</code>, <code>time.Sleep()</code>, or any other function that ignores context.</p><p>For example, <code>time.Sleep(duration)</code> can be replaced with a context-aware version like this:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>func</span> <span>Sleep</span><span>(</span><span>ctx</span> <span>context</span><span>.</span><span>Context</span><span>,</span> <span>duration</span> <span>time</span><span>.</span><span>Duration</span><span>)</span> <span>error</span> <span>{</span>
</span></span><span><span>    <span>select</span> <span>{</span>
</span></span><span><span>    <span>case</span> <span>&lt;-</span><span>time</span><span>.</span><span>After</span><span>(</span><span>duration</span><span>):</span>
</span></span><span><span>        <span>return</span> <span>nil</span>
</span></span><span><span>    <span>case</span> <span>&lt;-</span><span>ctx</span><span>.</span><span>Done</span><span>():</span>
</span></span><span><span>        <span>return</span> <span>ctx</span><span>.</span><span>Err</span><span>()</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><div><p>In older versions of Go, <code>time.After</code> can leak memory until the timer fires. This was fixed in Go 1.23 and newer. If you’re unsure which version you’re using, consider using <code>time.NewTimer</code> along with <code>Stop</code> and an optional <code>&lt;-t.C</code> check if <code>Stop</code> returns false.</p><blockquote></blockquote><p>See: <a href="https://github.com/golang/go/issues/61542" rel="external" target="_blank">time: stop requiring Timer/Ticker.Stop for prompt GC</a></p></div><p>Although this article focuses on HTTP servers, the same concept applies to third-party services as well. For example, the <code>database/sql</code> package has a <code>DB.Close</code> method. It closes the database connection and prevents new queries from starting. It also waits for any ongoing queries to finish before fully shutting down.</p><p>The core principle of graceful shutdown is the same across all systems: <strong>Stop accepting new requests or messages, and give existing operations time to finish within a defined grace period.</strong></p><p>Some may wonder about the <code>server.Close()</code> method, which shuts down the ongoing connections immediately without waiting for requests to finish. Can it be used after <code>server.Shutdown()</code> returns an error?</p><p>The short answer is yes, but it depends on your shutdown strategy. The <code>Close</code> method forcefully closes all active listeners and connections:</p><ul><li>Handlers that are actively using the network will receive errors when they try to read or write.</li><li>The client will immediately receive a connection error, such as <code>ECONNRESET</code> (‘socket hang up’)</li><li>However, long-running handlers that are not interacting with the network may <strong>continue running</strong> in the background.</li></ul><p>This is why using context to propagate a shutdown signal is still the more reliable and graceful approach.</p><a href="#5-release-critical-resources"><h2 id="5-release-critical-resources" origin-id="5-release-critical-resources">5. Release Critical Resources</h2><h2 data-pagefind-ignore="" arialabel="Anchor">#</h2></a><p>A common mistake is releasing critical resources as soon as the termination signal is received. At that point, your handlers and in-flight requests may still be using those resources. You should delay the resource cleanup until the shutdown timeout has passed or all requests are done.</p><p>In many cases, simply letting the process exit is enough. The operating system will automatically reclaim resources. For instance:</p><ul><li>Memory allocated by Go is automatically freed when the process terminates.</li><li>File descriptors are closed by the OS.</li><li>OS-level resources like process handles are reclaimed.</li></ul><p>However, there are important cases where explicit cleanup is still necessary during shutdown:</p><ul><li><strong>Database connections</strong> should be closed properly. If any transactions are still open, they need to be committed or rolled back. Without a proper shutdown, the database has to rely on connection timeouts.</li><li><strong>Message queues and brokers</strong> often require a clean shutdown. This may involve flushing messages, committing offsets, or signaling to the broker that the client is exiting. Without this, there can be rebalancing issues or message loss.</li><li><strong>External services</strong> may not detect the disconnect immediately. Closing connections manually allows those systems to clean up faster than waiting for TCP timeouts.</li></ul><p>A good rule is to shut down components in the reverse order of how they were initialized. This respects dependencies between components.</p><p>Go’s <code>defer</code> statement makes this easier since the last deferred function is executed first:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>db</span> <span>:=</span> <span>connectDB</span><span>()</span>
</span></span><span><span><span>defer</span> <span>db</span><span>.</span><span>Close</span><span>()</span>
</span></span><span><span>
</span></span><span><span><span>cache</span> <span>:=</span> <span>connectCache</span><span>()</span>
</span></span><span><span><span>defer</span> <span>cache</span><span>.</span><span>Close</span><span>()</span>
</span></span></code></pre></div><p>Some components require special handling. For example, if you cache data in memory, you might need to write that data to disk before exiting. In those cases, design a shutdown routine specific to that component to handle the cleanup properly.</p><a href="#summary"><h2 id="summary" origin-id="summary">Summary</h2><h2 data-pagefind-ignore="" arialabel="Anchor">#</h2></a><p>This is a complete example of a graceful shutdown mechanism. It is written in a flat, straightforward structure to make it easier to understand. You can customize it to fit your own application as needed.</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>const</span> <span>(</span>
</span></span><span><span>	<span>_shutdownPeriod</span>      <span>=</span> <span>15</span> <span>*</span> <span>time</span><span>.</span><span>Second</span>
</span></span><span><span>	<span>_shutdownHardPeriod</span> <span>=</span> <span>3</span> <span>*</span> <span>time</span><span>.</span><span>Second</span>
</span></span><span><span>	<span>_readinessDrainDelay</span> <span>=</span> <span>5</span> <span>*</span> <span>time</span><span>.</span><span>Second</span>
</span></span><span><span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>var</span> <span>isShuttingDown</span> <span>atomic</span><span>.</span><span>Bool</span>
</span></span><span><span>
</span></span><span><span><span>func</span> <span>main</span><span>()</span> <span>{</span>
</span></span><span><span>	<span>// Setup signal context</span>
</span></span><span><span>	<span>rootCtx</span><span>,</span> <span>stop</span> <span>:=</span> <span>signal</span><span>.</span><span>NotifyContext</span><span>(</span><span>context</span><span>.</span><span>Background</span><span>(),</span> <span>syscall</span><span>.</span><span>SIGINT</span><span>,</span> <span>syscall</span><span>.</span><span>SIGTERM</span><span>)</span>
</span></span><span><span>	<span>defer</span> <span>stop</span><span>()</span>
</span></span><span><span>
</span></span><span><span>	<span>// Readiness endpoint</span>
</span></span><span><span>	<span>http</span><span>.</span><span>HandleFunc</span><span>(</span><span>"/healthz"</span><span>,</span> <span>func</span><span>(</span><span>w</span> <span>http</span><span>.</span><span>ResponseWriter</span><span>,</span> <span>r</span> <span>*</span><span>http</span><span>.</span><span>Request</span><span>)</span> <span>{</span>
</span></span><span><span>		<span>if</span> <span>isShuttingDown</span><span>.</span><span>Load</span><span>()</span> <span>{</span>
</span></span><span><span>			<span>http</span><span>.</span><span>Error</span><span>(</span><span>w</span><span>,</span> <span>"Shutting down"</span><span>,</span> <span>http</span><span>.</span><span>StatusServiceUnavailable</span><span>)</span>
</span></span><span><span>			<span>return</span>
</span></span><span><span>		<span>}</span>
</span></span><span><span>		<span>fmt</span><span>.</span><span>Fprintln</span><span>(</span><span>w</span><span>,</span> <span>"OK"</span><span>)</span>
</span></span><span><span>	<span>})</span>
</span></span><span><span>
</span></span><span><span>	<span>// Sample business logic</span>
</span></span><span><span>	<span>http</span><span>.</span><span>HandleFunc</span><span>(</span><span>"/"</span><span>,</span> <span>func</span><span>(</span><span>w</span> <span>http</span><span>.</span><span>ResponseWriter</span><span>,</span> <span>r</span> <span>*</span><span>http</span><span>.</span><span>Request</span><span>)</span> <span>{</span>
</span></span><span><span>		<span>select</span> <span>{</span>
</span></span><span><span>		<span>case</span> <span>&lt;-</span><span>time</span><span>.</span><span>After</span><span>(</span><span>2</span> <span>*</span> <span>time</span><span>.</span><span>Second</span><span>):</span>
</span></span><span><span>			<span>fmt</span><span>.</span><span>Fprintln</span><span>(</span><span>w</span><span>,</span> <span>"Hello, world!"</span><span>)</span>
</span></span><span><span>		<span>case</span> <span>&lt;-</span><span>r</span><span>.</span><span>Context</span><span>().</span><span>Done</span><span>():</span>
</span></span><span><span>			<span>http</span><span>.</span><span>Error</span><span>(</span><span>w</span><span>,</span> <span>"Request cancelled."</span><span>,</span> <span>http</span><span>.</span><span>StatusRequestTimeout</span><span>)</span>
</span></span><span><span>		<span>}</span>
</span></span><span><span>	<span>})</span>
</span></span><span><span>
</span></span><span><span>	<span>// Ensure in-flight requests aren't cancelled immediately on SIGTERM</span>
</span></span><span><span>	<span>ongoingCtx</span><span>,</span> <span>stopOngoingGracefully</span> <span>:=</span> <span>context</span><span>.</span><span>WithCancel</span><span>(</span><span>context</span><span>.</span><span>Background</span><span>())</span>
</span></span><span><span>	<span>server</span> <span>:=</span> <span>&amp;</span><span>http</span><span>.</span><span>Server</span><span>{</span>
</span></span><span><span>		<span>Addr</span><span>:</span> <span>":8080"</span><span>,</span>
</span></span><span><span>		<span>BaseContext</span><span>:</span> <span>func</span><span>(</span><span>_</span> <span>net</span><span>.</span><span>Listener</span><span>)</span> <span>context</span><span>.</span><span>Context</span> <span>{</span>
</span></span><span><span>			<span>return</span> <span>ongoingCtx</span>
</span></span><span><span>		<span>},</span>
</span></span><span><span>	<span>}</span>
</span></span><span><span>
</span></span><span><span>	<span>go</span> <span>func</span><span>()</span> <span>{</span>
</span></span><span><span>		<span>log</span><span>.</span><span>Println</span><span>(</span><span>"Server starting on :8080."</span><span>)</span>
</span></span><span><span>		<span>if</span> <span>err</span> <span>:=</span> <span>server</span><span>.</span><span>ListenAndServe</span><span>();</span> <span>err</span> <span>!=</span> <span>nil</span> <span>&amp;&amp;</span> <span>err</span> <span>!=</span> <span>http</span><span>.</span><span>ErrServerClosed</span> <span>{</span>
</span></span><span><span>			<span>log</span><span>.</span><span>Fatalf</span><span>(</span><span>"ListenAndServe: %v"</span><span>,</span> <span>err</span><span>)</span>
</span></span><span><span>		<span>}</span>
</span></span><span><span>	<span>}()</span>
</span></span><span><span>
</span></span><span><span>	<span>// Wait for signal</span>
</span></span><span><span>	<span>&lt;-</span><span>rootCtx</span><span>.</span><span>Done</span><span>()</span>
</span></span><span><span>	<span>stop</span><span>()</span>
</span></span><span><span>	<span>isShuttingDown</span><span>.</span><span>Store</span><span>(</span><span>true</span><span>)</span>
</span></span><span><span>	<span>log</span><span>.</span><span>Println</span><span>(</span><span>"Received shutdown signal, shutting down."</span><span>)</span>
</span></span><span><span>
</span></span><span><span>	<span>// Give time for readiness check to propagate</span>
</span></span><span><span>	<span>time</span><span>.</span><span>Sleep</span><span>(</span><span>_readinessDrainDelay</span><span>)</span>
</span></span><span><span>	<span>log</span><span>.</span><span>Println</span><span>(</span><span>"Readiness check propagated, now waiting for ongoing requests to finish."</span><span>)</span>
</span></span><span><span>
</span></span><span><span>	<span>shutdownCtx</span><span>,</span> <span>cancel</span> <span>:=</span> <span>context</span><span>.</span><span>WithTimeout</span><span>(</span><span>context</span><span>.</span><span>Background</span><span>(),</span> <span>_shutdownPeriod</span><span>)</span>
</span></span><span><span>	<span>defer</span> <span>cancel</span><span>()</span>
</span></span><span><span>	<span>err</span> <span>:=</span> <span>server</span><span>.</span><span>Shutdown</span><span>(</span><span>shutdownCtx</span><span>)</span>
</span></span><span><span>	<span>stopOngoingGracefully</span><span>()</span>
</span></span><span><span>	<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
</span></span><span><span>		<span>log</span><span>.</span><span>Println</span><span>(</span><span>"Failed to wait for ongoing requests to finish, waiting for forced cancellation."</span><span>)</span>
</span></span><span><span>		<span>time</span><span>.</span><span>Sleep</span><span>(</span><span>_shutdownHardPeriod</span><span>)</span>
</span></span><span><span>	<span>}</span>
</span></span><span><span>
</span></span><span><span>	<span>log</span><span>.</span><span>Println</span><span>(</span><span>"Server shut down gracefully."</span><span>)</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><a href="#who-we-are"><h2 id="who-we-are" origin-id="who-we-are">Who We Are</h2><h2 data-pagefind-ignore="" arialabel="Anchor">#</h2></a><p>If you want to monitor your services, track metrics, and see how everything performs, you might want to check out <a href="https://docs.victoriametrics.com/" rel="external" target="_blank">VictoriaMetrics</a>. It’s a fast, <strong>open-source</strong>, and cost-saving way to keep an eye on your infrastructure.</p><p>And we’re Gophers, enthusiasts who love researching, experimenting, and sharing knowledge about Go and its ecosystem. If you spot anything that’s outdated or if you have questions, don’t hesitate to reach out. You can drop me a DM on <a href="https://twitter.com/func25" rel="external" target="_blank">X(@func25)</a>.</p><p>Related articles:</p><ul><li><a href="https://victoriametrics.com/categories/go-@-victoriametrics">Golang Series at VictoriaMetrics</a></li><li><a href="https://victoriametrics.com/blog/go-array">How Go Arrays Work and Get Tricky with For-Range</a></li><li><a href="https://victoriametrics.com/blog/go-slice">Slices in Go: Grow Big or Go Home</a></li><li><a href="https://victoriametrics.com/blog/go-map">Go Maps Explained: How Key-Value Pairs Are Actually Stored</a></li><li><a href="https://victoriametrics.com/blog/defer-in-go">Golang Defer: From Basic To Traps</a></li><li><a href="https://victoriametrics.com/blog/vendoring-go-mod-vendor">Vendoring, or go mod vendor: What is it?</a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Hackathons feel fake now (158 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43889502</link>
            <guid>43889502</guid>
            <pubDate>Sun, 04 May 2025 20:55:41 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43889502">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="43889502">
      <td><span></span></td>      <td><center><a id="up_43889502" href="https://news.ycombinator.com/vote?id=43889502&amp;how=up&amp;goto=item%3Fid%3D43889502"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=43889502">Ask HN: Hackathons feel fake now</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_43889502">103 points</span> by <a href="https://news.ycombinator.com/user?id=sepidy">sepidy</a> <span title="2025-05-04T20:55:41 1746392141"><a href="https://news.ycombinator.com/item?id=43889502">5 hours ago</a></span> <span id="unv_43889502"></span> | <a href="https://news.ycombinator.com/hide?id=43889502&amp;goto=item%3Fid%3D43889502">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Hackathons%20feel%20fake%20now&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=43889502&amp;auth=ecec15ec742832f826b52746d7ef40bcdf5c727f">favorite</a> | <a href="https://news.ycombinator.com/item?id=43889502">74&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><p>Been going to a bunch of hackathons in SF lately and honestly, everything feels fake. There are like 20 sponsors handing out credits for their tools that all do the same thing. Half the time, they can’t even explain what they’re for. They’re just hoping someone uses them so they can count it as adoption. Everyone jams these into projects to check a box, and what gets built is mostly BS with zero innovation.
Was it always like this and I'm noticing it now, or has something changed?</p></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I'd rather read the prompt (874 pts)]]></title>
            <link>https://claytonwramsey.com/blog/prompt/</link>
            <guid>43888803</guid>
            <pubDate>Sun, 04 May 2025 19:17:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://claytonwramsey.com/blog/prompt/">https://claytonwramsey.com/blog/prompt/</a>, See on <a href="https://news.ycombinator.com/item?id=43888803">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>When I grade students’ assignments, I sometimes see answers like this:</p>
<blockquote>
<p>Utilizing Euler angles for rotation representation could have the following possible downsides:</p>
<ul>
<li><strong>Gimbal lock</strong>: In certain positions, orientations can reach a singularity, which prevents them from continuously rotating without a sudden change in the coordinate values.</li>
<li><strong>Numeric instability</strong>: Using Euler angles could cause numeric computations to be less precise, which can add up and produce inaccuracies if used often.</li>
<li><strong>Non-unique coordinates</strong>: Another downside of Euler angles is that some rotations do not have a unique representation in Euler angles, particularly at singularities.</li>
</ul>
<p>The downsides of Euler angles make them difficult to utilize in robotics.
It’s important to note that very few implementations employ Euler angles for robotics.
Instead, one could use rotation matrices or quaternions to facilitate more efficient rotation representation.</p>
<p><em>[Not a student’s real answer, but my handmade synthesis of the style and content of many answers]</em></p>
</blockquote>
<p>You only have to read one or two of these answers to know exactly what’s up: the students just copy-pasted the output from a large language model, most likely ChatGPT.
They are invariably verbose, interminably waffly, and insipidly fixated on the bullet-points-with-bold style.
The prose rarely surpasses the sixth-grade book report, constantly repeating the prompt, presumably to prove that they’re staying on topic.</p>
<p>As an instructor, I am always saddened to read this.
The ChatGPT rhetorical style is distinctive enough that I can catch it, but not so distinctive to be worth passing along to an honor council.
Even if I did, I’m not sure the marginal gains in the integrity of the class would be worth the hours spent litigating the issue.</p>
<p>I write this article as a plea to everyone: not just my students, but the blog posters and Reddit commenters and weak-accept paper authors and Reviewer 2.
<strong>Don’t let a computer write for you!</strong>
I say this not for reasons of intellectual honesty, or for the spirit of fairness.
I say this because I believe that your original thoughts are far more interesting, meaningful, and valuable than whatever a large language model can transform them into.
For the rest of this piece, I’ll briefly examine some guesses as to why people write with large language models so often, and argue that there’s no good reason to use one for creative expression.</p>
<h2 id="why-do-people-do-this">Why do people do this?</h2>
<p>I’m not much of a generative-model user myself, but I know many people who heavily rely upon them.
From my own experience, I see a few reasons why people use such models to speak for them.</p>
<p><strong>It doesn’t matter.</strong>
I think this belief is most common in classroom settings.
A typical belief among students is that classes are a series of hurdles to be overcome; at the end of this obstacle course, they shall receive a degree as testament to their completion of these assignments.
I think this is also the source of increasing language model use in in <a href="https://arxiv.org/abs/2403.07183">paper reviews</a>.
Many researchers consider reviewing ancillary to their already-burdensome jobs; some feel they cannot spare time to write a good review and so pass the work along to a language model.</p>
<p><strong>The model produces better work.</strong>
Some of my peers believe that large language models produce strictly better writing than they could produce on their own.
Anecdotally, this phenomenon seems more common among English-as-a-second-language speakers.
I also see it a lot with first-time programmers, for whom programming is a set of mysterious incantations to be memorized and recited.
I think this is also the cause of language model use in some forms of <a href="https://arxiv.org/abs/2404.01268">academic writing</a>: it differs from the prior case with paper reviews in that, presumably, the authors believe that their paper matters, but don’t believe they can produce sufficient writing.</p>
<p><strong>There’s skin in the game.</strong>
This last cause is least common among individuals, but probably accounts for the overwhelming majority of language pollution on the Internet.
Examples of skin-in-the-game writing include astroturfing, customer service chatbots, and the rambling prologues found in online baking recipes.
This writing is never meant to be read by a human and does not carry any authorial intent at all.
For this essay, I’m primarily interested in the motivations for private individuals, so I’ll avoid discussing this much; however, I have included it for sake of completeness.</p>
<h2 id="why-do-we-write-anyway">Why do we write, anyway?</h2>
<p>I believe that the main reason a human should write is to <em>communicate original thoughts</em>.
To be clear, I don’t believe that these thoughts need to be special or academic.
Your vacation, your dog, and your favorite color are all fair game.
However, these thoughts should be <em>yours</em>: there’s no point in wasting ink to communicate someone else’s thoughts.</p>
<p>In that sense, using a language model to write is worse than plagiarism.
When copying another person’s words, one doesn’t communicate their own original thoughts, but at least they are communicating a human’s thoughts.
A language model, by construction, has no original thoughts of its own; publishing its output is a pointless exercise.</p>
<p>Returning to our reasons for using a language model, we can now examine them once more with this definition in mind.</p>
<h3 id="if-it-s-not-worth-doing-it-s-not-worth-doing-well">If it’s not worth doing, it’s not worth doing well</h3>
<p>The model output in the doesn’t-matter category falls under two classes to me: the stuff that actually doesn’t matter and the stuff that actually does matter.
I’ll start with the things that don’t matter.
When someone comments under a Reddit post with a computer-generated summary of the original text, I honestly believe that everyone in the world would be better off had they not done so.
Either the article is so vapid that a summary provides all of its value, in which case, it does not merit the engagement of a comment, or it demands a real reading by a real human for comprehension, in which case the summary is pointless.
In essence, writing such a comment wastes everyone’s time.
This is the case for all of the disposable uses of a model.</p>
<p>Meanwhile, there are uses which seem disposable at a surface-level and which in practice are not so disposable (the actually-does-matter category).
I should hope that the purpose of a class writing exercise is not to create an artifact of text but force the student to think; a language model produces the former, not the latter.
For paper reviewers, it’s worse: a half-assed review will produce little more than make-work for the original authors and tell the editor nothing they didn’t already know.</p>
<h3 id="if-it-s-worth-doing-it-s-worth-doing-badly">If it’s worth doing, it’s worth doing badly</h3>
<p>I’ll now cover the opposite case: my peers who see generative models as superior to their own output.
I see this most often in professional communication, typically to produce fluff or fix the tone of their original prompts.
Every single time, the model obscures the original meaning and adds layers of superfluous nonsense to even the simplest of ideas.
If you’re lucky, it at least won’t be wrong, but most often the model will completely fabricate critical details of the original writing and produce something completely incomprehensible.
No matter how bad any original human’s writing is, I can (hopefully?) trust that they have some kind of internal understanding to share; with a language model, there is no such luck.</p>
<p>I have a little more sympathy for programmers, but the long-term results are more insidious.
You might recall Peter Naur’s <cite>Programming as Theory Building</cite>: writing a sufficiently complex program requires not only the artifact of code (that is, the program source), but a <dfn>theory</dfn> of the program, in which an individual must fully understand the logical structure behind the code.
Vibe coding; that is, writing programs almost exclusively by language-model generation; produces an artifact with no theory behind it.
The result is simple: with no theory, the produced code is practically useless.
In Naur’s terms, such a program is <dfn>dead</dfn>; in our case, it’s stillborn.
It should come as no surprise that nearly every vibe-coded app on the Internet struggles with security issues; look no further than <a href="https://news.ycombinator.com/item?id=43553031#43554611">the vibe-coded recipe app that leaks its OpenAI keys</a>.
Every time one generates code by prompt, they create a new stillborn program; vibe coding is the art of stitching together their corpses into Frankenstein’s monster.</p>
<h2 id="an-annotated-exercise">An annotated exercise</h2>
<p>As an experiment, I copied the thesis statement of this essay into Google Gemini and prompted it to complete the essay.
Below the spoiler is its complete response.</p>
<details>
  <summary>The complete prompt and output.</summary>
<p>Prompt:</p>
<blockquote>
<p>The following is an introduction to a short essay which I have written. Complete the essay by writing 2 short paragraphs.</p>
<p>I write this article as a plea to everyone: not just my students, but the blog posters and Reddit commenters and weak-accept paper authors and Reviewer 2.
<strong>Don’t let a computer write for you!</strong>
I say this not for reasons of intellectual honesty, or for the spirit of fairness.
I say this because I believe that your original thoughts are far more interesting, meaningful, and valuable than whatever a large language model can transform them into.
For the rest of this piece, I’ll briefly examine some guesses as to why people write with large language models so often, and argue that there’s no good reason to use one for creative expression.</p>
</blockquote>
<p>Output:</p>
<blockquote>
<p>It’s fascinating to consider why so many are turning to large language models for writing. Perhaps it stems from a desire for efficiency, a wish to quickly generate text without the perceived effort of crafting each sentence. The allure of instant output can be strong, especially in a world that often prioritizes speed. For students facing deadlines, or blog posters needing content, the temptation to shortcut the writing process might feel overwhelming. There could also be a sense of insecurity, a feeling that one’s own writing isn’t “good enough” and that a language model can somehow elevate it. This might be particularly true for those navigating the often critical world of academic reviews or the sometimes harsh landscape of online commentary.</p>
<p>However, this reliance on artificial intelligence fundamentally misunderstands the very essence of creative expression. Your individual voice, shaped by your unique experiences, perspectives, and even your struggles to articulate an idea, is what makes your writing resonate. A language model, trained on vast datasets of existing text, can only ever produce a pastiche, a reflection of what has already been said. It lacks the spark of genuine insight, the unexpected turn of phrase that comes from wrestling with a concept until it becomes your own. To surrender your voice to an algorithm is to diminish the potential for true connection and the profound satisfaction of communicating your own original thoughts to the world.</p>
</blockquote>
</details>
<p>The resulting text is, put plainly, <em>boring</em>.
In its 234-word output, the model produces merely a lengthy rehash of the same prompt.
Tonally, it provides no more than sweeping and pointless generality, and any points earned for its brobdingnagian vocabulary are immediately lost to its total lack of taste in using it.
For example, consider the following sentence:</p>
<blockquote>
<p>Perhaps it stems from a desire for efficiency, a wish to quickly generate text without the perceived effort of crafting each sentence.</p>
</blockquote>
<p>Gemini has produced a big sentence for a small thought.
I can trivially cut two-thirds of it and remove nothing of substance:</p>
<blockquote>
<p>Perhaps it stems from a desire for efficiency.</p>
</blockquote>
<p>With some care, I can trim it a little more.</p>
<blockquote>
<p>Perhaps people do it for efficiency.</p>
</blockquote>
<p>So, in short, a language model is great for making nonsense, and not so great for anything else.</p>
<h2 id="just-show-me-the-prompt">Just show me the prompt</h2>
<p>I now circle back to my main point: I have never seen any form of create generative model output (be that image, text, audio, or video) which I would rather see than the original prompt.
The resulting output has less substance than the prompt and lacks any human vision in its creation.
The whole point of making creative work is to share one’s own experience - if there’s no experience to share, why bother?
If it’s not worth writing, it’s not worth reading.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI code is legacy code (152 pts)]]></title>
            <link>https://text-incubation.com/AI+code+is+legacy+code+from+day+one</link>
            <guid>43888225</guid>
            <pubDate>Sun, 04 May 2025 17:53:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://text-incubation.com/AI+code+is+legacy+code+from+day+one">https://text-incubation.com/AI+code+is+legacy+code+from+day+one</a>, See on <a href="https://news.ycombinator.com/item?id=43888225">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Driverless print server for legacy printers, profit goes to open-source (112 pts)]]></title>
            <link>https://printserver.ink/</link>
            <guid>43888157</guid>
            <pubDate>Sun, 04 May 2025 17:43:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://printserver.ink/">https://printserver.ink/</a>, See on <a href="https://news.ycombinator.com/item?id=43888157">Hacker News</a></p>
<div id="readability-page-1" class="page">

<p><strong>UoWPrint is a modern print server created to bring wireless
capabilities to older printers, scanners, and all-in-one
devices.</strong></p>
<p>It is designed as an easy-to-use add-on appliance and allows you
to:</p>
<ul>
<li><strong>Convert your old USB printer (or
<abbr title="Multi-function printer: scan, copy, print">MFP</abbr>) into
Wi-Fi printer/MFP</strong></li>
<li><strong>Print and scan without drivers from Windows/macOS/Linux, as
well as iOS/Android</strong></li>
</ul>
<p>Contrary to older-generation print server hardware or home routers
with printer-sharing functions, UoWPrint does not require to install any
printer-specific drivers or applications on your devices but uses
universal standard drivers available in any contemporary operating
system.<br>
The printer driver is running on the print server itself, freeing you
from installing drivers on the PC.</p>
<p>In other words, with UoWPrint you can use <a href="https://en.wikipedia.org/wiki/AirPrint">AirPrint</a> and <a href="https://en.wikipedia.org/wiki/Mopria_Alliance">Mopria</a> to print
and scan over the network on your USB printer.</p>
<p>✅ Compatible with Windows, macOS, Linux, Android, iOS<br>
✅ Compatible with MacBook M1/M2/M3/M4 (ARM)<br>
✅ Compatible with Windows on ARM (Qualcomm Snapdragon)</p>
<figure>
<img src="https://printserver.ink/pics/printserver-fs8.png" width="1200" height="514" alt="Device connection scheme">

</figure>
<figure>
<img src="https://printserver.ink/pics/4.jpg" width="1200" height="748" alt="Photo of the board connected to the printer. The device comes with simple acrylic case.">

</figure>
<h3 id="printer-compatibility">Printer compatibility</h3>
<p>UoWPrint is best compatible with older-generation consumer devices
(manufactured before the year 2018) and supports a very wide range of
ink and laser printers and scanners. However, some of the printers are
not supported well or at all.<br>
HP devices work very well, Samsung and Xerox support is also pretty
reliable, while Canon compatibility depends on the device generation and
may be not as fast or reliable.</p>
<p><strong>Check almost <a href="https://printserver.ink/supported.txt">full list</a> of
supported devices</strong>.</p>
<h4 id="included-drivers">Included drivers</h4>
<p>The following non-exhaustive list provides a brief overview of the
available drivers and popular supported models:</p>
<ul>
<li><a href="https://splix.sourceforge.net/#:~:text=list%20of%20supported%20printers">splix</a>
and <a href="https://web.archive.org/web/20191231052546/http://foo2qpdl.rkkda.com/#:~:text=you%20can%20print%20to%20some%20samsung%20and%20xerox%20printers">foo2qpdl</a>
for Samsung Printer Language (SPL) devices:
<ul>
<li><strong>Samsung SCX-3200, SCX-4200, SCX-4300</strong></li>
<li><strong>Samsung ML-1210, ML-1710, ML-2015, ML-2525, ML-2165,
ML-3471</strong></li>
<li><strong>Samsung CLP-500</strong></li>
<li><strong>Xerox Phaser 31xx/34xx, 5500, 61x0, WorkCentre
3119</strong></li>
</ul></li>
<li><a href="https://web.archive.org/web/20191218040201/http://foo2zjs.rkkda.com/#:~:text=possibly%20other%20zjstream%20printers">foo2zjs</a>,
<a href="https://web.archive.org/web/20191203133406/http://foo2xqx.rkkda.com/#:~:text=you%20can%20print%20to%20some%20hp">foo2xqx</a>,
<a href="https://www.openprinting.org/driver/hplip/">hpcups</a> for the
majority of HP LaserJets, InkJets, DeskJets, and PhotoSmarts:
<ul>
<li><strong>HP LaserJet 1005, 1010, 1018, 1020, 1160, 1320, 3020, 3030,
3330</strong></li>
<li><strong>HP LaserJet P1005, P2015</strong></li>
<li><strong>HP LaserJet Pro P1102, P1566, P1606dn</strong></li>
<li><strong>HP LaserJet M1522nf, 3055</strong></li>
<li><strong>HP LaserJet Pro MFP M521, MFP M425</strong></li>
</ul></li>
<li><a href="https://github.com/mounaiban/captdriver/wiki#supported-devices">capt</a>
open-source driver for Canon Advanced Printing Technology (CAPT) devices
<sup>alpha-quality drivers</sup>:
<ul>
<li><strong>Canon i-SENSYS LBP2900, LBP3000, LBP3010, LBP3100,
LBP6000</strong></li>
</ul></li>
<li>Official Canon <a href="https://github.com/mounaiban/captdriver/wiki/List-of-CAPT-only-Canon-Printers">CAPT</a>
driver running in x86 emulation mode:
<ul>
<li><strong>Canon Laser Shot LBP810, LBP1120</strong></li>
<li><strong>Canon Laser Shot LBP5100, LBP6000B, LBP7018C</strong></li>
</ul></li>
<li><a href="https://github.com/ondrej-zary/carps-cups#cups-driver-for-canon-carps-printers">carps</a>
for Canon Advanced Raster Printing System (CARPS) devices:
<ul>
<li><strong>Canon MF3110, MF56x0, MF57x0</strong></li>
<li><strong>Canon MF3200</strong></li>
<li><strong>Canon LC310, L390, L408S, L180, L380S, L398S</strong></li>
<li><strong>Canon PC-D300, FAX-L400, ICD300</strong></li>
</ul></li>
<li>Canon <a href="https://github.com/mounaiban/captdriver/wiki/List-of-Canon-i-Sensys-Devices#device-list">UFRII-based
devices</a> running in x86 emulation mode:
<ul>
<li><strong>Canon LBP212dw, LBP214dw</strong></li>
<li><strong>Canon LBP3360, LBP3460, LBP6230dw, LBP6650dn, LBP6780x,
LBP7660cdn</strong></li>
<li><strong>Canon MF212w, MF4010, MF4018, MF4320d, MF512x</strong></li>
</ul></li>
<li><a href="https://github.com/Owl-Maintain/brlaser#supported-printers">brlaser</a>
for Brother laser devices:
<ul>
<li><strong>Brother HL-1110, HL-120x, HL-L237x, HL-L5000D</strong></li>
<li><strong>Brother DCP-1623</strong></li>
</ul></li>
<li><a href="https://hub.mos.ru/mikhailnov/rastertokpsl-re/">rastertokpsl-re</a>
for Kyocera GDI devices:
<ul>
<li><strong>Kyocera FS-1020MFP, FS-1025MFP, FS-1040, FS-1060DN,
FS-1120MFP, FS-1125MFP</strong></li>
</ul></li>
<li><a href="https://panasonic.net/cns/pcc/support/fax/common/table/linuxdriver.html">Panasonic
GDI</a> official printers running in x86 emulation mode:
<ul>
<li><strong>Panasonic KX-MB1500, KX-MB2000, KX-MB2500
series</strong></li>
<li><strong>Panasonic DP-MC210, DP-MB250 series</strong></li>
</ul></li>
<li><a href="https://global.pantum.com/">Pantum</a> printers and MFPs
official driver running in x86 emulation mode:
<ul>
<li><strong>Pantum BM5100, M6500 series</strong></li>
<li><strong>Pantum P2200, P2500 series</strong></li>
</ul></li>
<li><a href="https://support.epson.net/linux/Printer/LSB_distribution_pages/en/escpr.php">Epson
Inkjet Printer Driver</a> and <a href="https://support.epson.net/linux/Printer/LSB_distribution_pages/en/escpr2.php">Driver
2</a>:
<ul>
<li><strong>Overwhelming majority of Epson devices</strong></li>
</ul></li>
<li><a href="https://gimp-print.sourceforge.io/p_Supported_Printers.php">GutenPrint</a>:
<ul>
<li><strong>Canon PIXMA/imageRunner, Kyocera, Lanier, Ricoh, Xerox and
other devices</strong></li>
</ul></li>
<li><a href="https://www.openprinting.org/drivers/">GhostScript</a>:
<ul>
<li><strong>Various PostScript, PCL, IJS models</strong></li>
</ul></li>
</ul>
<p>… and many others.</p>
<h4 id="operating-system-support">Operating system support</h4>
<ul>
<li><strong>Windows XP/7/8/8.1</strong>, with limited printing options
support (PostScript / PCL)</li>
<li><strong>Windows 10</strong>, October 2018 update (build 1809) and
newer with full support (Mopria)</li>
<li><strong>Windows 11</strong> (Mopria)</li>
<li><strong>macOS 10.6+</strong> (AirPrint / AirScan)</li>
<li><strong>Linux</strong> with CUPS 2.2.2+ (AirPrint / Mopria)</li>
<li><strong>Android</strong> 8+ (default print service) or Android 4.4+
with <a href="https://play.google.com/store/apps/details?id=org.mopria.printplugin">Mopria
Print Service</a> + <a href="https://play.google.com/store/apps/details?id=org.mopria.scan.application">Mopria
Scan</a></li>
<li><strong>iOS 4.2+</strong> (AirPrint, <em>no built-in scanning
support, scanning using web interface possible</em>)</li>
</ul>
<p>Device-specific driver is not required on every OS.</p>
<h3 id="project-information">Project information</h3>
<h4 id="why">Why?</h4>
<p>After buying used old USB-only printer, I was looking for a modern
retail print server which would allow me to use the printer as you use
the newer ones, over Wi-Fi and without drivers, but could not find
anything on the market!<br>
Reducing e-waste, improving convenience, and re-using single-board
computers for good is the main motivation of this project.</p>
<p>Old printers are tend to be better than the new ones: the best mono
lasers were manufactured in the year 2004-2010, that was the peak of
reliable, serviceable, and cheaper to operate machines than the current
retail models. They do not offer subscription service or annoying
firmware updates which block third-party toner cartridges.</p>
<h4 id="no-anti-consumer-practices">No anti-consumer practices</h4>
<p><strong>No Internet connection required.</strong> The device does not
use the Internet for anything besides time synchronization. No
telemetry, no automatic updates.<br>
<strong>Secure by default.</strong> Network firewall blocks all
connections from the outside of your network. No hard-coded passwords.
IPv6 support.<br>
<strong>Open for enthusiasts and modification.</strong> Comes with
source code and full build system of the firmware.<br>
<strong>Reliable technical support.</strong> We sold you the print
server and it doesn’t work as intended? We’ll buy the same printer model
and debug the issue until it’s fixed!</p>
<h4 id="software">Software</h4>
<p>UoWPrint is based on well-known Linux-world open source software:</p>
<ul>
<li><a href="https://openprinting.github.io/">CUPS</a> print server with
AirPrint/Mopria shading capabilities</li>
<li><a href="http://www.sane-project.org/">SANE</a> scanning suite for
MFP and dedicated scanner support</li>
<li><a href="https://github.com/SimulPiscator/AirSane">AirSane</a> for
AirScan/Mopria scanner sharing capability</li>
<li><a href="https://box86.org/">box86</a> emulator to use official
(proprietary) printer and scanner drivers for x86 machines on ARM
architecture, which enables wide compability in the absence of an open
driver.</li>
</ul>
<p>All you need to start printing is plug the printer into the USB port
and add it using print server web interface. The printer will be
discovered on all devices on the network automatically.</p>
<p>The device works as Wi-Fi access point or connects to your Wi-Fi
network as a client, Ethernet connection is also available. It
incorporates simple web interface to configure Wi-Fi connection and
printer, and scanners usually do not require special configuration and
work out-of-the-box.</p>
<p>The operating system of the device is based on Debian 12 and has been
optimized for memory constraints and fail-save appliance-like operation:
power off the device by unplugging the power without worrying of data
corruption, revert to factory defaults with a click of a button.</p>
<p><strong>Warning:</strong> this project may not be suitable for
professional printing. Due to the various quality of the drivers, the
printouts may not present accurate colors or the highest quality. Some
printer and scanner drivers have bugs and don’t work reliably. UoWPrint
has undergone thoroughful, but limited testing. We constantly find and
fix bugs in the whole printing and scanning stack (and contribute back
to related projects to ensure bug-free experience for everyone), but
this is best-effort low-scale holiday project, some thigs are way over
our heads. Ask to check your printer support in advance <a href="mailto:uowprint@valdikss.org.ru?subject=UoWPrint%20inquiry&amp;body=Do%20not%20forget%20to%20include%20your%20printer%20model%20for%20compatibility%20check">by
email</a>.</p>
<h4 id="hardware">Hardware</h4>
<p><strong>UoWPrint v2</strong> is based on <a href="http://www.orangepi.org/html/hardWare/computerAndMicrocontrollers/details/Orange-Pi-Zero-3.html">OrangePi
Zero 3</a> single-board computer. Key specifications:</p>
<table>
<tbody><tr>
<th>
Size
</th>
<td>
62 mm × 56 mm in acrylic case<br>55 mm × 50 mm the board itself
</td>
</tr>
<tr>
<th>
Weight
</th>
<td>
52 g
</td>
</tr>
<tr>
<th>
CPU
</th>
<td>
<a href="https://linux-sunxi.org/H616">Allwinner H618</a><br> ARMv8
Cortex-A53 Quad Core, 1.5 GHz
</td>
</tr>
<tr>
<th>
RAM
</th>
<td>
1 GiB LPDDR4
</td>
</tr>
<tr>
<th>
Flash memory
</th>
<td>
512 MB MicroSD card
</td>
</tr>
<tr>
<th>
USB
</th>
<td>
1× USB 2.0 Type-A Host for printer connection<br> 1× USB 2.0 Type-C
OTG for 5V power supply and PC connection<p> LPT printer
connection possible with USB-LPT adapter cable
</p></td>
</tr>
<tr>
<th>
Wi-Fi
</th>
<td>
802.11ac 2.4+5 GHz, Wi-Fi hotspot and client modes
</td>
</tr>
<tr>
<th>
Ethernet
</th>
<td>
1G/100M Built-in Ethernet
</td>
</tr>
<tr>
<th>
Supported standards
</th>
<td>
Driverless printing and scanning:
<ul>
<li>
AirPrint™ (macOS/iPhone/iPad)
</li>
<li>
Mopria® (Windows/Linux/Android)
</li>
</ul>
Printing with the driver:
<ul>
<li>
Internet Printing Protocol (IPP)
</li>
</ul>
</td>
</tr>
<tr>
<th>
Features
</th>
<td>
No internet connection required<br> No account or subscription
required<br> No automatic updates<br> Web-based configuration, no
additional software required<br> Comes with the source code, firmware,
and free technical support
</td>
</tr>
<tr>
<th>
Operating system
</th>
<td>
Based on Debian 12<br>Linux kernel 6.6
</td>
</tr>
<tr>
<th>
Software
</th>
<td>
OpenPrinting CUPS<br>SANE<br>AirSane<br>Box86
</td>
</tr>
</tbody></table>
<h3 id="order">Order</h3>
<p>What you get:</p>
<ul>
<li>UoWPrint v2 device (without USB power supply or cable)</li>
<li>Electronic instruction</li>
<li>Archive with the firmware, source code, and build scripts</li>
<li>Free tech support from the living human being who takes
responsibility for fixing possible compatibility issues with your
printer</li>
</ul>
<p>All you need is a USB-C cable and a power supply (any cellphone
charger).</p>
<p>Price: $35 USD or 2900₽ (Russian Ruble), shipping cost not
included.<br>
Payments using cryptocurrency (preferred), PayPal, or Russian SBP.</p>
<p><a href="mailto:uowprint@valdikss.org.ru?subject=UoWPrint%20inquiry&amp;body=Do%20not%20forget%20to%20include%20your%20printer%20model%20for%20compatibility%20check">Order
by email: uowprint@valdikss.org.ru</a></p>
<p>For each sold device, CUPS and SANE developers receive $2 donation
each. The profit of selling the device is to be spent on improving open
source drivers support.</p>
<h3 id="documentation">Documentation</h3>
<p>Configuration how-to is available in <a href="https://printserver.ink/docs-en/">English</a>
and <a href="https://printserver.ink/docs-ru/">Russian</a>.</p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Design for 3D-Printing (551 pts)]]></title>
            <link>https://blog.rahix.de/design-for-3d-printing/</link>
            <guid>43888117</guid>
            <pubDate>Sun, 04 May 2025 17:38:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.rahix.de/design-for-3d-printing/">https://blog.rahix.de/design-for-3d-printing/</a>, See on <a href="https://news.ycombinator.com/item?id=43888117">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-div">
  <p>Over the past years, I have gotten deeper and deeper into 3d-printing.  One
thing that fascinates me a lot is how different it is from other manufacturing
methods.  And how this, in turn, means a completely different design philosophy
is needed to create good designs for 3d-printing.</p>
<p>As such, I have been collecting the little tricks and rules for designing
well-printable parts.  And of course I am always on the hunt for more.  In this
blog post I want to share all that I have learned.</p>

<p><span>
  <p><img alt="Design for 3d-printing" src="https://blog.rahix.de/design-for-3d-printing/title.jpg">
    
  </p>
</span>
<span id="continue-reading"></span></p><h2 id="introduction">Introduction<a href="#introduction"><i></i></a>
</h2>
<p>While a lot can be found online about the basics of design for 3d-printing,
there is not that much material that dives into it in-depth.  And even less
common are good overviews that collect all the little tricks that people have
discovered.</p>
<p>I believe the best format for teaching the rules of well-printable design is a
format that is built on rules of thumb and many practical examples.  While such
heuristic rules will never encompass the whole complexity of the real world, I
think they can provide a tangible guide towards the best solution in each
situation.  They also provide a starting point for building your own intuition
about the topic.</p>
<p>This approach is close in spirit to the german book <em>Konstruktionspraxis im
Maschinenbau</em> by Gerhard Hoenow and Thomas Meißner (<a href="https://www.hanser-kundencenter.de/fachbuch/artikel/9783446464858">Hanser
Fachbuch</a>, ISBN 978-3-446-46485-8). The book follows
the same philosophy, but for traditional manufacturing methods.  If you like
this style, that book might interest you as well.</p>
<h4 id="scope">Scope<a href="#scope"><i></i></a>
</h4>
<p>Before diving in, a few words about the scope of this blog post:  First, I am
explicitly focusing on FDM/FFF printing (<a href="https://en.wikipedia.org/wiki/Fused_filament_fabrication">Wikipedia</a>).  A lot of
the rules are specifically tied to the restrictions of this process and do not
apply to other additive manufacturing methods.</p>
<p>Second, I am focusing on designing functional parts with optimal mechanical
properties.  These parts should be easy to print, without requiring much
printer fine-tuning.  They should be easy to produce in higher quantity, which
means they should require as little post-processing and waste as little
material as possible.  Aesthetics do not play a primary role, although I firmly
believe that great mechanical design just looks incredibly beautiful :)</p>
<h4 id="goals-of-design-engineering">Goals of Design Engineering<a href="#goals-of-design-engineering"><i></i></a>
</h4>
<p>Any mechanical design ultimately must be optimized for a large number of
objectives and constraints.  Your job as the designer is to create a part that
fits best between them all.</p>
<p>The most universal objectives are:</p>
<ul>
<li><strong>Design according to force</strong> — Parts should be designed such that their
geometry is optimal for the forces it has to transmit and withstand.</li>
<li><strong>Design according to manufacturing method</strong> — Often called <em>DFM</em>
(Design For Manufacturing), parts should be designed such that they are
easy to produce.  Little changes, which usually don't affect functionality,
can often make a part <em>much</em> simpler to manufacture.</li>
<li><strong>Design according to cost</strong> — Parts should be as cheap as possible.
With 3d-printing, this means minimizing material use and print time.</li>
</ul>
<p>Of course, any real world project will have many more objectives and a lot of
other constraints that the design must comply with.</p>
<p>In this blog post, I will focus on topics related to design for manufacturing
with a 3d-printer.  In general, the rules for other objectives are the same,
regardless of manufacturing process.  However, there are some places where
3d-printing does influence these other objectives and I will also highlight
such cases.</p>
<p>An important distinction that I need to make at this point is how the approach
of design engineering attacks the problem from the completely opposite side
compared to, for example, slicer developers and 3d-printer manufacturers.
While they aim for improving the manufacturing process itself, to better print
any possible part geometry, a design engineer instead aims for adapting the
part geometry to best work with the current state of the the manufacturing
processes.</p>
<p>To give an example, a design engineer aims to use the most coarse tolerance on
each dimension that still allows the part to function.  A 3d-printer
manufacturer wants their machine to print with the most tight tolerance they
can achieve.  While the design engineer could try making use of this most tight
tolerance, this will make the part impossible to manufacture using cheaper
printers and thus drive up cost and limit possible suppliers for a part.</p>
<p>This can be summarized as aiming for <em>portable design</em>.  A portably designed
part can be printed by anyone on any printer with ease.  If you intend to share
your designs online, this should definitely be a priority.</p>
<p>Finally, it is also important to keep in mind that due to the rapid innovation
in the 3d-printing scene, we are working with a moving target.  As the
3d-printers and slicer software get improved, some of the design rules will
become less important.</p>
<h4 id="terminology">Terminology<a href="#terminology"><i></i></a>
</h4>
<p>To make sure everyone is on the same page, let's quickly review the most
important terminology of FFF 3d-printing that will be used in the following
chapters:</p>
<ul>
<li><strong>Layers</strong> — The horizontal cross-sections of a part that are stacked
on top of one another to create the part (<a href="https://help.prusa3d.com/article/layers-and-perimeters_1748#layer-height">Layer height — Prusa
Knowledge Base</a>).</li>
<li><strong>Perimeters</strong> — On each layer, the 3d-printer will first print the
outline of the cross-section.  This outline consists of a number of perimeter
lines (<a href="https://help.prusa3d.com/article/layers-and-perimeters_1748#perimeters">Perimeters — Prusa Knowledge Base</a>).</li>
<li><strong>Shell</strong> — If you think of only the perimeters on each layer, you get
a completely hollow object:  The <em>shell</em> of the part.</li>
<li><strong>Infill</strong> — Inside the perimeters, the 3d-printer "fills" the space
using a partially hollow pattern.  This is the infill pattern (<a href="https://help.prusa3d.com/article/infill_42">Infill
— Prusa Knowledge Base</a>).</li>
<li><strong>Infill Percentage</strong> — How much of the infill pattern is solid vs.
empty space (<a href="https://help.prusa3d.com/article/infill_42">Infill — Prusa Knowledge Base</a>).</li>
<li><strong>Overhang</strong> — When the cross-section of a layer gets larger than the
previous layer, a part of the perimeter lines will not be supported by
material from below.  This is called an <em>overhang</em> and is actually possible
as long as the overhang is not too large.  Mostly, people talk about the
angle of an overhang when looking at the layers from the side.</li>
<li><strong>Bridges</strong> — While overhangs can become problematic, one thing
3d-printers are surprisingly good at is bridging gaps between two supporting
surfaces.  This is called a <em>bridge</em>.</li>
<li><strong>Seam</strong> — The <em>seam</em> is the point where the printer starts and stops
laying a perimeter.  This point is usually well visible as a perfectly smooth
transition is very hard to achieve (<a href="https://help.prusa3d.com/article/seam-position_151069">Seam position — Prusa Knowledge
Base</a>)</li>
</ul>
<h4 id="the-standard-printer-profile">The Standard Printer Profile<a href="#the-standard-printer-profile"><i></i></a>
</h4>
<p>One final topic before we can dive in.  To achieve the aforementioned <em>portable
designs</em>, it is useful to define a design target for your work.  Namely a rough
outline of the 3d-printer and profile that the design will be printed with.</p>
<p>While most of the following rules are relevant regardless, I will need to make
some assumptions.  Especially for all the numbers I will mention, a design
target is needed for context.  Otherwise values for expectable tolerances and
empirical dimensional recommendations will have no meaning.</p>
<p>As such, here is my definition of the <em>standard printer profile</em>.  Cutting
across the current market of 3d-printers, the vast majority are compatible with
the following expectations.  A lot of the expectations are intentionally kept
quite vague — we must not assume too much, to keep the designs
<em>portable</em>.</p>
<ul>
<li>The standard 3d-printer is using an 0.4&nbsp;mm nozzle.</li>
<li>The standard profile prints at 0.2&nbsp;mm layer height.</li>
<li>The standard 3d-printer is calibrated such that no considerable dimensional
deviation is caused by error in the steps/mm setting.  Skew between the axes
is negligible.</li>
<li>Print speeds are tuned to an adequate level but we cannot assume the printer
to be free of artifacts like slight overshoot and ringing.</li>
<li>Bridging across reasonable distances is reliable.</li>
<li>Overhangs print fine.</li>
<li>Bed adhesion is adequate and does not pose an issue for non-extreme geometry.</li>
</ul>
<h2 id="design-rules">Design Rules<a href="#design-rules"><i></i></a>
</h2>
<p>Table of Contents</p>
<ol>
<li><a href="https://blog.rahix.de/design-for-3d-printing/#1-designing-for-part-strength">Designing for Part Strength</a></li>
<li><a href="https://blog.rahix.de/design-for-3d-printing/#2-manufacturing-tolerance-and-part-finish">Manufacturing Tolerance and Part Finish</a></li>
<li><a href="https://blog.rahix.de/design-for-3d-printing/#3-process-optimization">Process Optimization</a></li>
<li><a href="https://blog.rahix.de/design-for-3d-printing/#4-functional-integration">Functional Integration</a></li>
<li><a href="https://blog.rahix.de/design-for-3d-printing/#5-beyond-plastic-machine-elements">Beyond plastic - Machine Elements</a></li>
<li><a href="https://blog.rahix.de/design-for-3d-printing/#6-appearance">Appearance</a></li>
<li><a href="https://blog.rahix.de/design-for-3d-printing/#7-extra-vase-mode-design">Extra: Vase Mode Design</a></li>
</ol>
<h2 id="1-designing-for-part-strength">1. Designing for Part Strength<a href="#1-designing-for-part-strength"><i></i></a>
</h2>
<p>The first rules I will present are related to designing parts that are as
strong as possible.  In 3d-printing, the strength of a part is inherently tied
to the manufacturing process.  For one, the parts are partly hollow and thus
behave quite differently than a solid block of plastic.  Second, the layered
manufacturing process leads to anisotropic parts — meaning their
mechanical properties differ depending on direction (<a href="https://en.wikipedia.org/wiki/Anisotropy#Materials_science_and_engineering">Wikipedia</a>).</p>
<p>Thus, beyond general design rules for part strength, a few additional
considerations should be made when designing parts for 3d-printing.</p>
<h4 id="part-orientation">Part Orientation<a href="#part-orientation"><i></i></a>
</h4>
<p>Let's start with the most obvious:  3d-printed parts are much weaker in the
direction of pulling apart the layers.  Thus, before starting on a design, you
should first consider the direction of tensile forces in your part and use them
to decide on a print orientation.  All further design should then be done with
this print orientation in mind.</p>
<p>
    <a href="#rule-R1.1"><i></i></a>
    <strong>R1.1</strong> 
    —
    Tensile forces should be aligned parallel to the print surface.
  </p>
<p>This is probably the number one rule that everyone talks about in design for
3d-printing.  And that is for good reason because the difference it makes is
substantial.  Tests performed by My Tech Fun indicate roughly 3 times more strength in
ideal orientation (<a href="https://www.youtube.com/watch?v=ZiQek0wei1g">YouTube</a>).  While the exact ratio
will depend heavily on the individual part, I think this is a good indicator of
how important this topic is.</p>

<p><span>
  <div>
    <p><img alt="good and bad print orientation" src="https://blog.rahix.de/design-for-3d-printing/Screw-Thread-Orientation.png"></p><p>A threaded part designed for upright printing (not ideal for tensile loading)
and one designed for printing flat on the bed.  By cutting away a part of the
thread, it can be printed lying flat on the print bed, without supports.</p>
    
  </div>
</span></p><p>You should also keep in mind that tensile forces are exerted to some cross
section of a part when bending it.  This means that you should also align any
bending moments parallel to the print surface.  A place where this often fails
is printed clips, which bend slightly to clip into their counterpart.  Such
features will break easily when not oriented correctly.</p>

<p><span>
  <div>
    <p><img alt="good and bad orientation of printed clip" src="https://blog.rahix.de/design-for-3d-printing/Clip-Orientation.png"></p><p>Clips like the one on the left do not survive many uses before breaking.</p>
    
  </div>
</span></p><p>I also want to give a small tip here about sharing your designs on platforms
like <a href="https://www.printables.com/">Printables</a>:  Make sure you upload your
models in the correct print orientation.  While the correct orientation may
seem obvious to you, others might not know and they will try printing your part
without reorienting it first.  You'll do them a small favor by uploading
correctly oriented files 🙂</p>
<h4 id="when-no-orientation-works">When no orientation works<a href="#when-no-orientation-works"><i></i></a>
</h4>
<p>Especially with complex parts, in some cases, no print
orientation is ideal.  While it is often fine to make a trade-off and accept
limited strength in certain locations, one alternative should always be kept in
mind:</p>
<p>
    <a href="#rule-R1.2"><i></i></a>
    <strong>R1.2</strong> 
    —
    Split a part into multiple pieces when no orientation is ideal for all loads.
  </p>
<p>As each piece gets printed individually, you are completely free to choose
the optimal orientation for each one.  Clever joints can be used to easily
assemble the whole part afterwards.  Just to mention one:  Dovetail joints have
proven themselves for 3d-printing, as they are well-printable in most orientations.</p>

<p><span>
  <div>
    <p><img alt="two parts with a dovetail joint" src="https://blog.rahix.de/design-for-3d-printing/Dovetail.png"></p><p>A dovetail joint is used to attach two halves of a part so that each piece
can be printed in an improved orientation.</p>
    
  </div>
</span></p><h4 id="to-infill-or-not-to-infill">To infill or not to infill<a href="#to-infill-or-not-to-infill"><i></i></a>
</h4>
<p>Contrary to the first rule, a topic that is not talked about enough in my eyes
is the role of infill in part strength.  There is a big misconception that you
can magically give your part ultimate strength by just using 100% infill.  While you
will certainly see an increase in strength, this method is not efficient at all
— most of the additional material does not contribute to part strength
and is just waste and an unnecessary increase of print time.</p>
<p>Why?  It all has to do with the distribution of force inside of a part.
Generally, most parts are not stressed in pure tension or compression.  Instead,
a lot of stress is applied in the form of bending moments and those distribute
the force unevenly.  The force will be greatest in places furthest away from
the center (neutral axis).  Thus, it is generally more effective for additional
material to be added at the surface, not the center of a part.</p>

<p><span>
  <div>
    <p><img alt="bending stress" src="https://blog.rahix.de/design-for-3d-printing/stress-concentration.png"></p><p>Stress is highest directly beneath the surface of the part.  Notice the blue
neutral axis in the middle, where stress is lowest.</p>
    
  </div>
</span></p><p>In 3d-printer speak, this means you should increase the number of
perimeters/shells rather than the infill percentage.  Stefan from CNC Kitchen
has done a thorough analysis of this (<a href="https://www.youtube.com/watch?v=AmEaNAwFSfI">YouTube</a>).</p>
<p>In general, his work is a great resource for in-depth information about the
mechanical behavior of 3d-printed parts (CNC Kitchen <a href="https://www.cnckitchen.com/">Blog/Website</a>
and <a href="https://www.youtube.com/@CNCKitchen">YouTube</a>).</p>
<p>
    <a href="#rule-R1.3"><i></i></a>
    <strong>R1.3</strong> 
    —
    Most strength comes from the part's surface, not the infill.
  </p>
<h4 id="the-flow-of-forces">The Flow of Forces<a href="#the-flow-of-forces"><i></i></a>
</h4>
<p>However, there is more to this.  We can actually influence the amount of stress
at the surface by changing the shape of the part.  This is often much more
effective than optimizing print settings.  A way to conceptualize the stress
inside of a part is to think about the forces "flowing" through it. The visual
representation of this is called a <em>force lines drawing</em>
(<a href="https://en.wikipedia.org/wiki/Force_lines">Wikipedia</a>).</p>

<p><span>
  <div>
    <p><img alt="force lines around a crack" src="https://blog.rahix.de/design-for-3d-printing/crack-force-lines.png"></p>
    
  </div>
</span></p><p>Stress will concentrate in places where the force lines are close together.
You can see how the sharp corners of the crack in the image above lead to
regions of particularly high stress and thus will probably lead to part failure
in these locations.</p>
<p>This isn't specific to 3d-printing but as it is such an important topic, I want
to include it here anyway.  The rules are generally the same for any
manufacturing method: We want to minimize stress and the best way to do it is
this:</p>
<p>
    <a href="#rule-R1.4"><i></i></a>
    <strong>R1.4</strong> 
    —
    Guide forces on the most direct path possible.
  </p>
<p>Or in other words, keep the force lines as short and straight as possible.</p>
<p>One example where this matters a lot are sharp corners.  They impact part
strength very negatively and are usually easy to avoid by simply adding a
fillet.  You can see how a fillet allows a much more direct path for the force
than a sharp corner:</p>

<p><span>
  <div>
    <p><img alt="force lines around corner and fillet" src="https://blog.rahix.de/design-for-3d-printing/force-lines-fillet.svg"></p><p>Force lines around a sharp corner vs. a corner with a fillet.</p>
    
  </div>
</span></p><h4 id="cross-sectional-considerations">Cross-sectional Considerations<a href="#cross-sectional-considerations"><i></i></a>
</h4>
<p>The inhomogeneous nature of 3d-printed parts with their shell and infill has
yet more implications for part strength that are worth talking about.  In
traditional design engineering, you are taught to reduce cross-sectional area
of a part as much as possible.  You should only keep material in places where
it has the highest effect on increasing part strength. The reason for this is
that reduced volume directly leads to reduced material usage and thus cost and
weight savings.</p>
<p>With 3d-printing, things work differently.  You can usually increase the
cross-sectional area quite a bit, without a noticeable increase in material
usage, as the infill is mostly empty space.  You should rather strive to reduce
the surface area of a part, because that is where most material is used.</p>
<p>This means that you can get away with much thicker shapes in 3d-printed parts,
which obviously benefits part strength a lot.  Don't try to work against this
artificially — if a design allows for it, go for the fattest shape
possible.</p>
<p>
    <a href="#rule-R1.5"><i></i></a>
    <strong>R1.5</strong> 
    —
    Use large cross sections.  Prefer thick shapes over thin shapes.
  </p>
<p>To give an example, think about the traditional I-beam
(<a href="https://en.wikipedia.org/wiki/I-beam">Wikipedia</a>) cross-section. The idea is that material is only kept in
the regions far away from the neutral axis of the bending moment, where it has
the highest impact on part strength.  This allows for significant weight
savings.</p>
<p>While the I-beam profile works wonders for homogeneous materials, it is not a
good idea for 3d-printing. A square cross-section of the same outer dimensions
will have comparable or better strength but usually does not lead to an
increase in material usage and print-time.  It might actually perform better in
those areas, as shown below.</p>

<p><span>
  <div>
    <p><img alt="i-beam vs square profile comparison" src="https://blog.rahix.de/design-for-3d-printing/Profile-Comparison.png"></p><p>Comparison of print times and material usage for I-beam profile and square profile.</p>
    
  </div>
</span></p><h4 id="simulation-struggles">Simulation Struggles<a href="#simulation-struggles"><i></i></a>
</h4>
<p>In traditional mechanical engineering, whenever a part needs to be evaluated
for its strength, the tool of choice is simulation.  Simulation allows making
accurate prediction of a part's behavior under load without needing to
manufacture it.  Especially for parts made from expensive materials or made using
expensive manufacturing methods, performing simulations beforehand is critical
for the project's budget.</p>
<p>Unfortunately, simulation quickly breaks down when trying to analyze 3d-printed
parts.  The problem is, once again, the inhomogeneous nature of 3d-printing.
While stress analysis may still be useful to find critical areas to give
attention to, extracting realistic values for limiting forces is a lost cause.</p>
<p>Luckily, there is an alternative to simulations that 3d-printing offers.  The
exceptionally low cost of manufacturing 3d-printed parts means that printing
prototypes for testing mechanical properties is often the most cost-effective
solution.  Need to find yield strength of a design?  Just print a few copies
and empirically determine it.</p>
<p>However, there is one things to be careful with: While I see test-prints as a
great way to determine mechanical properties, I advise to not rely on them for
determining dimensional accuracy.  I will go into more detail on my reasons for
this in the next chapter.</p>
<p>Finally, an implication of the above is that <em>topology optimization</em>
(<a href="https://en.wikipedia.org/wiki/Topology_optimization">Wikipedia</a>) is not really well suited for FFF 3d-printing.
While it is amazing for other additive manufacturing processes, current tools
are usually not able to produce designs that would truly be optimal for FFF
3d-printing.  Not to speak of the often less-than-ideal printability of such
parts.</p>
<h2 id="2-manufacturing-tolerance-and-part-finish">2. Manufacturing Tolerance and Part Finish<a href="#2-manufacturing-tolerance-and-part-finish"><i></i></a>
</h2>
<p>Next, I want to talk about optimizing your design to improve manufacturing
tolerances.  Certain shapes will generally print cleaner than others.
If you take this into consideration during design, you can achieve parts that
fit on first print and won't need endless tweaking of the slicer settings and
printer.  A welcome side-effect of improved tolerance is a cleaner part finish
as well.  Thus, the following chapter covers both: Manufacturing tolerance and
part finish.</p>
<h4 id="chamfers-vs-fillets">Chamfers vs. Fillets<a href="#chamfers-vs-fillets"><i></i></a>
</h4>
<p>To start, let's talk about edges.  Generally, mechanical parts should avoid
sharp edges because they are unpleasant to touch.  Any good design breaks edges
by either adding a round fillet or a 45° chamfer of appropriate size.</p>
<p>While the choice between the two is often made for stylistic reasons, there are
actually some rather important printability aspects to it.  There are two
situations that we have to look at, distinguished by the print orientation of
the edge:</p>
<p>Adding a fillet or chamfer to edges <em>parallel</em> to the print surface means the
feature will be built up from multiple layers, potentially with overhang.
Fillets are not well suited for this. They start with an incredibly steep
overhang that will not print well.  Often, such fillets will have large
surface deviations.  Even if no overhang is involved, when the
fillet sits on top, the changing curvature will make the layer steps very
visible, degrading the part finish.  Chamfers are much better suited in this
orientation.  They have a constant overhang angle, leading to a very consistent
layer stepping, which looks a lot more pleasing.</p>

<p><span>
  <div>
    <p><img alt="fillets and chamfers horizontal" src="https://blog.rahix.de/design-for-3d-printing/Chamfer-And-Fillet-Vertical.png"></p><p>Fillets and chamfers on edges parallel to the print surface.  The blue
perimeter on the bottom of the fillet marks an unsupported overhang.</p>
    
  </div>
</span></p><p>Contrary, edges <em>vertical</em> to the print surface are much better suited for
fillets.  A fillet means that the printer does not have to make sharp corners
while laying the perimeters.  This reduces print-head acceleration and thus
reduces surface artifacts (ringing, overshoot).  A chamfer on such edges will
have two sharp corners, which will never look as pretty and will never achieve
the same tolerances as a round fillet.</p>

<p><span>
  <div>
    <p><img alt="fillets and chamfers vertical" src="https://blog.rahix.de/design-for-3d-printing/Chamfer-And-Fillet-Horizontal.png"></p><p>Fillets and chamfers on edges vertical to the print surface.  The print speed
significantly decreases for the chamfer corners while the fillets can almost
keep the same velocity.</p>
    
  </div>
</span></p><p>Thus, unless stylistic choices dictate something else, the cleanest prints will
be achieved by following this rule:</p>
<p>
    <a href="#rule-R2.1"><i></i></a>
    <strong>R2.1</strong> 
    —
    Use chamfers on edges parallel to the print surface.  Use fillets on edges
vertical to the print surface.
  </p>
<h4 id="horizontal-holes">Horizontal Holes<a href="#horizontal-holes"><i></i></a>
</h4>
<p>With horizontal fillets, a major problem is the steep overhang.  There are
other design features where similar problems arise. For example, horizontally
oriented circular holes.  The larger they get, the bigger the problem will be.
The solution is to deviate from the perfect circle for a more optimized shape.
For smaller holes, a teardrop shape with a 90° angle works well.  Larger holes
can be extended to have a flat "roof".  Keep in mind that a slight amount of
additional clearance will be needed beneath the roof, due to the bridges
drooping slightly.</p>

<p><span>
  <div>
    <p><img alt="alternative horizontal holes" src="https://blog.rahix.de/design-for-3d-printing/Horizontal-Holes.png"></p><p>On the left: A ⌀4&nbsp;mm hole with a 90° teardrop.  On the right: A ⌀10&nbsp;mm hole
with a flat roof.  The roof sits 0.4&nbsp;mm above the theoretical circle so slight
drooping of the bridge won't affect the fit.</p>
    
  </div>
</span></p><p>
    <a href="#rule-R2.2"><i></i></a>
    <strong>R2.2</strong> 
    —
    Improve horizontal holes by using a teardrop shape or giving the hole a flat roof.
  </p>
<h4 id="seemingly-seamless">Seemingly Seamless<a href="#seemingly-seamless"><i></i></a>
</h4>
<p>A topic that is very important for both part finish and dimensional
tolerances is the placement of the perimeter seams.  Perimeter seams are the
point where a perimeter line starts and ends.  It is very hard to tune a
printer such that there won't be any seam artifacts.  Thus, let's put thought
into the placement of this seam so it does not interfere with any functionality
or aesthetics.</p>
<p>Usually, the seam placement is chosen automatically by the slicer.  For each
layer, it searches for the sharpest corner (concave or convex) and then places
the seam right into it.  This has proven to be a very reliable way to
produce good results without requiring any explicit user input.</p>
<p>However, this algorithm breaks down in two situations:</p>
<ol>
<li>On perfectly round perimeters, like those of a circular hole or outline.</li>
<li>When all corners have the same angle or when the sharpest corner requires
tight tolerances.</li>
</ol>
<p>Especially the first case is very relevant. This effect has severe consequences
for the tolerances you can achieve with a circular hole.  While the rest of the
perimeter may deviate by less than 0.1&nbsp;mm, the seam artifacts can easily
bump that up to 0.4&nbsp;mm.  And worse, it shifts the centerline of the hole
because the seam is only in one place.</p>
<p>The solution is simple: Add a corner where the seam can be placed without
interfering.  Instead of a perfect circle, shape the hole like a teardrop where
the corner has a 120° angle.  Note the difference to the horizontal holes
discussed previously.</p>

<p><span>
  <div>
    <p><img alt="teardrop for perimeter seam placement" src="https://blog.rahix.de/design-for-3d-printing/Teardrop-Vertical.png"></p><p>This teardrop shape allows the seam to not interfere with the circular hole
(white dots/line indicates seam placement).</p>
    
  </div>
</span></p><p>
    <a href="#rule-R2.3"><i></i></a>
    <strong>R2.3</strong> 
    —
    Use a teardrop shape for vertical holes to avoid inaccuracy due to perimeter
seams.
  </p>
<p>Similarly, if other corners must not be used for seam placement, add a small
notch somewhere that will serve as the "sharpest corner" and thus take the
seam automatically.  While you can of course also manually place the seam using
the slicer software, this trick means no slicer tweaking is needed.</p>

<p><span>
  <div>
    <p><img alt="aesthetic difference with optimized seam placement" src="https://blog.rahix.de/design-for-3d-printing/aesthetic-seam.jpg"></p><p>Difference of forcing the seam placement.  On the left, the seam destroys the
appearance of the groove in the center.  On the right, a small notch on the
back of the part takes the seam and the groove looks much better.</p>
    
  </div>
</span></p><p>To summarize:</p>
<p>
    <a href="#rule-R2.4"><i></i></a>
    <strong>R2.4</strong> 
    —
    Consider where the seam will be placed.  If tolerances are tight, provide a
sharp concave corner for the seam the hide in.
  </p>
<h4 id="expectable-tolerances-of-fff-fdm">Expectable Tolerances of FFF/FDM<a href="#expectable-tolerances-of-fff-fdm"><i></i></a>
</h4>
<p>A fundamental building block of any "design for manufacturing" is knowing the
accuracy limits of the manufacturing process.  As stated in the beginning, the
goal is to build <em>portable</em> designs, so we should assume conservative limits
instead of exhausting the capabilities of a particular printer.  Note that this
is backwards to the traditional workflow: Instead of <em>specifying</em> tolerances
that someone else then has to achieve during manufacturing, you <em>accept</em>
certain tolerances of the manufacturing process and design accordingly.</p>
<p>It also needs to be noted that a base assumption here is that the 3d-printers in
question are well-calibrated.  A machine that is out of calibration will have
an additional constant error on top of its tolerances.  For example, badly
configured <em>steps/mm</em> values will lead to a constant factor deviation of
measurements along a certain axis. Part design cannot and should not
accommodate for this — there wouldn't be much left if we tried.</p>

<p><span>
  <div>
    <p><img alt="visualization of tolerances and calibration error" src="https://blog.rahix.de/design-for-3d-printing/Tolerance-Calibration.png"></p><p>A miscalibrated printer will have a constant error on top of its general
manufacturing tolerances.</p>
    
  </div>
</span></p><p>Before we can talk about tolerance values, it is important to understand what
causes these deviations in the first place.  A tolerance value means nothing
without the context that it applies to.  In 3d-printing especially, tolerances
will vary widely depending on specific part geometry.  This topic is quite
far-reaching, as a very large number of factors play into it.  As such, this
chapter will only contain the most important things to consider.</p>
<p>Starting from the bottom, the lower bound on accuracy is due to the <em>step
resolution</em> of the printer's stepper motors and their drivers.  Fundamentally,
there is no way to get more accurate than this, as dimensions will always fall
somewhere between two step locations.  However, on state-of-the-art
3d-printers, the theoretical step resolution is around 0.01&nbsp;mm.  Whether
this value is actually achievable is debatable, but the takeaway is that step
resolution is well below the deviations caused by other effects.</p>
<p>Continuing on, there are more effects of the printer's motion system on
dimensional accuracy.  They all have to do with mechanical unrigidness of the
printer. Tuning the slicer settings and advanced motion control (input shaping)
can bring large improvements here.  However, an equally big role is played by
the geometry the print-head is printing.  Sharp corners are the worst offender
as they force the printer to maximum acceleration.  By optimizing a part for
"easy motion", tolerances of printed parts can often be improved a lot.</p>

<p><span>
  <div>
    <p><img alt="speeds around different corners" src="https://blog.rahix.de/design-for-3d-printing/Chamfer-And-Fillet-Horizontal.png"></p><p>Once again, fillets and chamfers show quite spectacularly how different the
print-head acceleration will be in different scenarios.  Sharper corners
lead to higher acceleration and will thus have worse tolerance.</p>
    
  </div>
</span></p><p>
    <a href="#rule-R2.5"><i></i></a>
    <strong>R2.5</strong> 
    —
    Design part geometry for easy motion paths while printing, to improve dimensional accuracy.
  </p>
<p>The next anti-accuracy component of a 3d-printer is the extruder and hotend.
Uneven extrusion makes the line width vary, which has an impact on the outer
dimensions of a part.  Additionally, the nozzle of the printer always slightly
drags the extruded line behind it.  This leads to circles always being slightly
undersized.  On inner circles (vertical holes), assume deviation to make the
hole smaller.  On outer circles, assume deviation to make the outer diameter
smaller, but to a lesser degree.</p>
<p>As a rule of thumb, for current popular FFF 3d-printers with an 0.4&nbsp;mm
nozzle at 0.2&nbsp;mm layer height, these effects lead to deviations in the
range of ±0.1&nbsp;mm for each surface. The deviations are shifted for circular
paths, as mentioned.  While many surfaces will be more accurate, this value is
a safe bet for surface deviation.</p>
<p>Unfortunately, while 3d-printers usually do not have a noteworthy growth in
tolerance for larger dimensions, other effects take over here: Especially
warping and shrinkage of the 3d-printed parts as they cool down.</p>
<p>Warping and shrinkage depend heavily on the choice of material, but there are
also part geometries that experience it more than others. In general, the more
voluminous a part is and the less sharp its edges are, the less warping can be
expected.  Think about the forces exerted when the material shrinks and how
the geometry can resist those forces.</p>
<p>
    <a href="#rule-R2.6"><i></i></a>
    <strong>R2.6</strong> 
    —
    Prevent warping by making parts voluminous and their surfaces smooth and rounded.  The ideal shape is a sphere.
  </p>
<h4 id="perfect-precision">Perfect Precision<a href="#perfect-precision"><i></i></a>
</h4>
<p>The previous chapter discussed what tolerances can be expected from the FFF
3d-printing process.  However, there are of course situations where more
accuracy is required.  One approach that is often used to deal with this is
experimentally determining a CAD dimension where the final part fits perfectly.
3d-printing makes this very easy because test prints are cheap and fast.
Often, 3d-printers have surprisingly good repeatability so a dimension that was
optimized using test prints can achieve quite incredible tolerances.  Some call
this the <em>goldilocks approach</em> (<a href="https://hackaday.com/2020/06/09/finding-perfect-part-fits-with-the-goldilocks-approach-and-openscad/">Hackaday</a>).</p>
<p>However, this empirical approach has a huge drawback:  It makes your design
depend heavily on the slicer settings that were used and the specific printer
that the part was printed on.  As discussed in the beginning, the goal is to
avoid such dependence as it makes the design less <em>portable</em>.</p>
<p>Luckily, there are other approaches that you can use to achieve the same goal,
but without requiring tight tolerances.  Of course, these approaches also have
their limitations.  So what is best in a given situation needs to be evaluated
on a case-by-case basis.</p>
<p>The first approach I want to discuss here is ingeniously simple.  It is best
summarized like this (and I wish I had a proper source for this quote):</p>
<p>
    <a href="#rule-R2.7"><i></i></a>
    <strong>R2.7</strong> 
    —
    If you can't make it precise, make it adjustable.
  </p>
<p>The idea is that in places where accuracy is needed, you should design your
part such that the dimension can be adjusted.  There is a huge variety of
adjustment mechanisms to choose from.  The following four examples should serve
to demonstrate the possibilities:</p>
<p><strong>Oblong holes for screws</strong> allow sliding a part into alignment.  This allows
for a large range of motion, but makes adjustments of less than 1&nbsp;mm very
difficult.  This method can also be used quite well to correct for angular
errors.</p>

<p><span>
  <div>
    <p><img alt="oblong holes for adjustment" src="https://blog.rahix.de/design-for-3d-printing/oblong-hole-adjustment.jpg"></p><p>An oblong hole is used to allow adjusting the position of a part.</p>
    
  </div>
</span></p><p><strong>Opposing screws for height adjustment</strong> allow fixing a dimension in place
quite permanently.  The natural reduction of the screws make tiny adjustments
easy to perform.  A downside is that access from both sides is needed, which is
not always easily achievable.  Adjusting such a mechanism is unfortunately a
bit tedious, because it is a constant back and forth between loosening one
screw a bit and then tightening the opposing screw again.</p>

<p><span>
  <div>
    <p><img alt="opposing screws holes for adjustment" src="https://blog.rahix.de/design-for-3d-printing/opposing-screws.png"></p><p>The two screws in the center oppose each other to finely adjust the height of the block between them.</p>
    
  </div>
</span></p><p><strong>(Grub)screws opposed by a spring or flexure</strong> are great for making adjustments
very easy.  The spring tension counteracting the screw means that turning the
screw either direction adjusts the dimension accordingly.  If required, a
second (grub)screw can be used to fix the part in place after adjusting.</p>

<p><span>
  <div>
    <p><img alt="screw with spring for adjustment" src="https://blog.rahix.de/design-for-3d-printing/spring-adjustment.jpg"></p><p>A height adjustment using a long M5 screw opposed by a spring.</p>
    
  </div>
</span></p><p><strong>Shimming with spring steel shims</strong> is the industry favorite for small
adjustments that only need to be changed very seldomly.  Premade shims in
various thicknesses are stacked to achieve the precise offset that is required.
Industrial shims are usually made from spring steel, but 3d-printed shims may be
conceivable for some applications.  However, especially for rather small
heights, small surface defects from the printing process could negatively
affect accuracy.</p>

<p><span>
  <div>
    <p><img alt="example of shimming" src="https://blog.rahix.de/design-for-3d-printing/shimming.jpg"></p><p>Crude example of a small 0.2&nbsp;mm shim being used for levelling a fixture.</p>
    
  </div>
</span></p><h4 id="engineering-fits">Engineering Fits<a href="#engineering-fits"><i></i></a>
</h4>
<p>In traditional mechanical engineering, when two parts need to fit each other
geometrically, a so-called <em>engineering fit</em> is used.  An engineering fit is a
systematic classification of the allowed dimensions for the two parts.
Usually, this system of fits is applied to holes and shafts, but it can be applied
to other shapes equally well.</p>
<p>The most important distinction is between</p>
<ul>
<li><strong>clearance fits</strong> where there is always some play between the parts, for any
of the allowed dimensions,</li>
<li><strong>transition fits</strong> where they might be play or interference,</li>
<li>and <strong>interference fits</strong> where the two parts will always interfere and thus
(significant) force is required to join them.  This is also referred to as a
<em>press fit</em>.</li>
</ul>

<p><span>
  <div>
    <p><img alt="illustrations of the different fit types" src="https://blog.rahix.de/design-for-3d-printing/engineering-fits.png"></p>
    
  </div>
</span></p><p>ISO 286 then goes on to define specific classes of fits with distinct tolerance
bands for the dimensions of the shaft and hole (<a href="https://en.wikipedia.org/wiki/Engineering_fit">Wikipedia</a>).
These are great for traditional manufacturing, but unfortunately, they are
mostly useless for 3d-printing.  We can't really print parts to a specific
tolerance — instead, the 3d-printer has an inherent tolerance that we have
to design for.</p>
<p>If tight tolerance engineering fits are required regardless, an option that
should not go unmentioned is post-processing of printed parts.  Especially for
printed holes, there is the possibility of reaming (<a href="https://en.wikipedia.org/wiki/Reamer">Wikipedia</a>)
them, to achieve, for example, an H6 fit.  That said, this option only makes
sense where absolutely required.  Otherwise, the additional effort is hard to
justify.</p>
<p>For any other cases, the choice is essentially between a clearance fit and an
interference fit.  Clearance fits are easy — a gap between the parts
that is larger than 2x the printer tolerance does the trick.  Interference fits,
on the other hand, are more difficult.  The problem is that large print
tolerances mean the forces in the interference fit vary wildly between
different parts.  In the worst case, this can even lead to joints breaking due
to excessive forces.</p>
<p>To counteract this, designs need to tolerate the large tolerances of
3d-printing.  There are a few tricks one can use here, which I will share
next.</p>
<h6 id="circles-considered-harmful">Circles Considered Harmful<a href="#circles-considered-harmful"><i></i></a>
</h6>
<p>The seemingly obvious solution for transition- or interference-fitting a
circular shape is a circular hole in your design.  However, I want to make the
point that this is rarely the best solution for 3d-printing.  Especially when
the aforementioned tolerance issues become relevant, circular holes should be
avoided.</p>
<p>Other shapes, like squares or hexagons can fit the mating part just as well but
are much more forgiving in regards to tolerance.  The reason for this can be
seen when considering how each shape deforms in interference situations.  A
circle can only widen by stretching along its circumference.  This quickly
leads to material failure when large amounts of interference need to be
compensated.  A square or hexagon, on the other hand, can bend to accommodate
the bigger diameter.  Little to no stretching of the material is present in
this case.</p>

<p><span>
  <div>
    <p><img alt="visualization of circle vs hexagon interference fit" src="https://blog.rahix.de/design-for-3d-printing/Interference-Visualization.png"></p><div>
      <p>red: oversized shaft / green: undersized circle or hexagon / grey hatching: interference regions</p>
<p>A circle has to stretch significantly while a hexagon just bends a little bit.</p>
    </div>
    
  </div>
</span></p><p>Another point in favor or the square or hexagon is the seam problem, as
discussed before.  A perfect circle will be heavily distorted by the seam while
a square can hide the seam neatly in one of its corners.</p>

<p><span>
  <div>
    <p><img alt="a hexagonal hole fits a cylinder pin" src="https://blog.rahix.de/design-for-3d-printing/Hex-Holes.png"></p><p>Hexagonal holes are used for a light press-fit with cylindrical stubs.  This
design also has two pockets to the sides of each stub that help with easy
removal.</p>
    
  </div>
</span></p><p>
    <a href="#rule-R2.8"><i></i></a>
    <strong>R2.8</strong> 
    —
    Do not use circular holes for interference fits.  Use hexagon or square holes instead.
  </p>
<p>This technique is mostly useful for smaller diameters.  The additional space
occupied by the corners can become problematic for lager holes and as such,
different design approaches should be preferred.</p>
<h6 id="crush-ribs">Crush Ribs<a href="#crush-ribs"><i></i></a>
</h6>
<p>For bigger diameters, there is another trick that can be borrowed from
injection molding. <em>Crush ribs</em> are features that are meant to be plastically
deformed during assembly. Usually, a ribbing structure is added to a hole where
the ribs can be crushed into the "valleys" between them.</p>
<p>In injection molding, these crush ribs are mainly used to work around the
required draft angle.  For 3d-printing, we can repurpose them to compensate
the printer tolerance, as described by Dan Royer (<a href="https://hackaday.com/2020/10/15/adding-crush-ribs-to-3d-printed-parts-for-a-better-press-fit/">Hackaday</a>).</p>

<p><span>
  <div>
    <p><img alt="example of crush ribs in a design" src="https://blog.rahix.de/design-for-3d-printing/Crush-Rib.png"></p><p>A blind bore with crush-ribs designed for a press-fit with a shaft.</p>
    
  </div>
</span></p><p>The ribs are dimensioned such that they still achieve the required interference
at either end of the tolerance band.  Because deforming the small ribs is much
easier than deforming an entire contact surface, the joining force is kept
within limits across the entire tolerance band.</p>

<p><span>
  <div>
    <p><img alt="tolerance considerations with crush ribs" src="https://blog.rahix.de/design-for-3d-printing/Crush-Rib-Tolerance.png"></p><p>To achieve a secure fit, the force of the minimum and maximum press-fit must be considered.</p>
    
  </div>
</span></p><p>When designing crush ribs, keep in mind that such small features are prone to
have much worse tolerance than larger features.  This has two effects:</p>
<ul>
<li>The tolerance of the crush rib shown in the drawing above will be larger than usual.</li>
<li>As the crush-rib is a convex shape, it is likely to be more undersized than oversized.</li>
</ul>
<p>To give some numbers, undersizing the crush ribs by 0.2&nbsp;mm and oversizing
the bore by 0.4&nbsp;mm lead to good results for me.</p>
<p>Something to keep in mind with crush ribs is that they should not be
reassembled more than once.  Their reliance on plastic deformation means that
the joint force will significantly decrease on reassembly.</p>
<p>
    <a href="#rule-R2.9"><i></i></a>
    <strong>R2.9</strong> 
    —
    Use crush ribs for press fits that are only assembled once.
  </p>
<h6 id="grip-fins">Grip Fins<a href="#grip-fins"><i></i></a>
</h6>
<p>Another interesting approach presented by Slant 3D is called <em>grip fins</em>
(<a href="https://www.youtube.com/watch?v=yzg_NXM-NRs">YouTube</a>).  Instead of plastic deformation, like with crush ribs,
grip fins only deform elastically.  The big advantage is of course, that such a
joint can be reassembled repeatedly, which crush ribs do not really allow for.</p>

<p><span>
  <div>
    <p><img alt="example of grip fins" src="https://blog.rahix.de/design-for-3d-printing/Grip-Fins.png"></p><p>Grip fins are an approach for tolerance compensation that use elastic deformation.</p>
    
  </div>
</span></p><p>
    <a href="#rule-R2.10"><i></i></a>
    <strong>R2.10</strong> 
    —
    Use grip fins for press fits that need to be reassembled more than once.
  </p>
<h2 id="3-process-optimization">3. Process Optimization<a href="#3-process-optimization"><i></i></a>
</h2>
<p>We have talked about designing for strength and designing for accuracy.  Let's
move closer to the manufacturing process itself.  The following chapter will
investigate ways to improve parts such that 3d-printing them gets easier,
faster, and to make parts less likely to result in failed prints.</p>
<h4 id="support-material">Support Material<a href="#support-material"><i></i></a>
</h4>
<p>The elephant in the room is certainly the topic of support material.  Support
material is used as scaffolding for features of a part that would otherwise
hang in thin air.  After printing, it is removed, to reveal the final
shape.</p>

<p><span>
  <div>
    <p><img alt="support material" src="https://blog.rahix.de/design-for-3d-printing/support-material.png"></p><p>Support material (shown in green) is used to help print an overhang that
would not be possible otherwise.</p>
    
  </div>
</span></p><p>Unfortunately, the use of support material comes with a number of drawbacks:</p>
<ul>
<li>It adds an additional post-processing step after printing.  The removal of
supports can be quite time-consuming.</li>
<li>It consumes additional material, which later turns to waste.</li>
<li>It leads to bad tolerances on supported surfaces, much worse than for the rest of the part.</li>
<li>It leaves a worse surface finish on supported surfaces.</li>
</ul>
<p>For the designer, the implication is simple:</p>
<p>
    <a href="#rule-R3.1"><i></i></a>
    <strong>R3.1</strong> 
    —
    Avoid the necessity of support material.
  </p>
<p>Yes, there are certainly many situations where using supports is the
appropriate choice.  But more often than not, slight design changes can
eliminate features that would otherwise require supports.</p>

<p><span>
  <div>
    <p><img alt="support material" src="https://blog.rahix.de/design-for-3d-printing/Support-Optimization.png"></p><p>Example of a feature that can be optimized to not require support material.</p>
    
  </div>
</span></p><p>A big role in this is played by the overall orientation of a part on the
print bed.  This was discussed previously, for part strength considerations, but
has equal effect on support requirement.  Balancing between these two is often
quite tricky.</p>

<p><span>
  <div>
    <p><img alt="showcase for the dilemma between support material and strength" src="https://blog.rahix.de/design-for-3d-printing/Support-Strength-Dilemma.png"></p><div>
      <p>The dilemma of optimizing for part strength or for avoiding supports:  The
orientation of the hook on the left part needs no supports but the force on
the hook can rip the layers apart.  The orientation on the right is ideal for
strength but needs support material.</p>
<p>In this case, the left option was chosen anyway because the forces are small
enough to not break the layers.</p>
    </div>
    
  </div>
</span></p><p>One design trick that is surprisingly powerful is the exploitation of diagonal
orientation.  Rather than keeping the rectangular shapes of a part aligned to
the printer axes, tilt the part upwards by 45°.</p>

<p><span>
  <div>
    <p><img alt="example of diagonal part orientation" src="https://blog.rahix.de/design-for-3d-printing/Diagonal-Orientation.png"></p><p>The left orientation leads to a large bridge which will leave inaccuracy and
visible artifacts.  Simply tilting the part so all the straight edges get
printed diagonally leads to all sides getting printed equally well.</p>
    
  </div>
</span></p><p>Not only does this method avoid supports or overly long bridges, it also leads to a
more uniform part finish as all faces now get printed with the same
orientation.  The easily discernible top and bottom surfaces are elegantly
avoided.</p>
<p>Of course print stability is affected by this.  You may need to include a brim
to keep such parts from falling over.  Slant 3D explores this concept of
diagonal orientation in more depth (<a href="https://www.youtube.com/watch?v=8TIhkxQNINY">YouTube</a>).</p>
<p>
    <a href="#rule-R3.2"><i></i></a>
    <strong>R3.2</strong> 
    —
    Clever part orientation on the print surface can eliminate the need for
supports.
  </p>
<h4 id="divide-and-conquer">Divide and Conquer<a href="#divide-and-conquer"><i></i></a>
</h4>
<p>Even with the best intentions, it is not always possible to make a part work
without requiring any support material.  While this may mean that it is time to
accept defeat and just print using supports, there is one last option that
should be evaluated.  It certainly isn't always the way to go, but it is worth
considering:</p>
<p>
    <a href="#rule-R3.3"><i></i></a>
    <strong>R3.3</strong> 
    —
    Split a part into multiple pieces when no orientation can avoid supports.
  </p>
<p>The trade-off here is between accepting the downsides of support material or
making assembly more complex.  Both solutions will require some amount of
post-print work, but which solution is better will always depend on the exact
circumstances.  Especially for one-off or low-volume designs, a slightly more
complex assembly is easy to justify.</p>

<p><span>
  <div>
    <p><img alt="a design where multiple parts were used to avoid supports" src="https://blog.rahix.de/design-for-3d-printing/multiple-parts.jpg"></p><p>The hooks were printed separately and then attached using screws.  This
allowed avoiding supports and also helped with optimizing print orientation
for better surface finish in critical locations (for example, the taper of
the hook).</p>
    
  </div>
</span></p><h4 id="sacrificial-layers">Sacrificial Layers<a href="#sacrificial-layers"><i></i></a>
</h4>
<p>A large number of situations where avoiding supports gets tricky are caused by
the need for printing counterbored holes upside-down.</p>

<p><span>
  <div>
    <p><img alt="counterbored hole upside-down" src="https://blog.rahix.de/design-for-3d-printing/Upside-Down-Counterbore.png"></p><p>An upside-down counterbore is problematic to print due to unsupported overhangs.</p>
    
  </div>
</span></p><p>Taking a cross-section, the problem is easily visible: The step of the
counterbore is unsupported.  Bridges won't work, because the hole in the middle
gets in the way. Using support material to help would be particularly nasty
here, because removing it from inside the hole can be very difficult.</p>
<p>One solution for this problem is the use of a <em>sacrificial layer</em>.  The idea is
simple:  Instead of an unsupported overhang, a bridge across the entire hole is
created.  This bridge is only a single layer high and only serves to
support the smaller diameter hole on top.</p>

<p><span>
  <div>
    <p><img alt="sacrificial layer" src="https://blog.rahix.de/design-for-3d-printing/Sacrificial-Layer.png"></p><p>A one layer thick bridge was added to the upside-down counterbore.  This
layer enables clean printing of the feature and can be removed easily after
printing.</p>
    
  </div>
</span></p><p>After printing, this thin layer is cut or drilled out of the part, leaving
behind the desired geometry.</p>

<p><span>
  <div>
    <p><img alt="sacrificial layer after printing" src="https://blog.rahix.de/design-for-3d-printing/sacrificial-layer-real.jpg"></p><p>This is what a sacrificial layer looks like right of the print bed.  The next
step is to cut it out using a knife or to remove it with a drill.</p>
    
  </div>
</span></p><p>Of course, upside-down counterbores aren't the only place where this trick can
be useful.  In fact, the next chapter will show an even better solution for
upside-down counterbores.  But to give an example, I have often used
sacrificial layers for the counterbore equivalent of an oblong hole.</p>
<p>
    <a href="#rule-R3.4"><i></i></a>
    <strong>R3.4</strong> 
    —
    Use sacrificial layers to avoid internal overhangs that would otherwise require supports.
  </p>
<h4 id="the-overhanging-counterbore-trick">The Overhanging Counterbore Trick<a href="#the-overhanging-counterbore-trick"><i></i></a>
</h4>
<p>Sacrificial layers are a useful tool, but they come with an additional
post-processing step.  There can also be problems with strands from the
bridging becoming loose while cutting out the sacrificial layer.  Lucky for us,
there is an even more clever way to deal with overhanging counterbores in
particular.</p>
<p>Instead of bridging over the entire hole and thus closing the through-hole on
top, bridges are only placed where they don't interfere.  Layer by layer, more
bridges are added to finally achieve the circular inner hole.</p>

<p><span>
  <p><img alt="counterbore trick stackup" src="https://blog.rahix.de/design-for-3d-printing/counterbore.png">
    
  </p>
</span></p><p>The layers of this design are as follows, from the bottom up:</p>
<ol>
<li>First, simply cut the circular counterbore to the desired depth.</li>
<li>Then, make a one layer thick cut (typically 0.2&nbsp;mm) such that only
material remains where the bridges do not run across the inner hole.  Two
strips of bridges will be left, running left and right of the inner hole.</li>
<li>On the next layer, make another one layer thick square cut with the
side length matching the inner hole diameter.  This will only leave bridges
running top and bottom of the inner hole, perpendicular to the bridges on
the layer before.</li>
<li>Finally, from the next layer on, the inner hole will be printed as a circle.
This works because the remaining bridges are small enough to not cause
issues.</li>
</ol>

<p><span>
  <div>
    <p><img alt="counterbore trick result" src="https://blog.rahix.de/design-for-3d-printing/counterbore-real.jpg"></p><p>This is what the overhanging counterbore trick looks like from below after
printing.  You can see the bridges crossing perpendicularly.</p>
    
  </div>
</span></p><p>This trick is particularly elegant because it does not require any
post-processing and it prints exceptionally cleanly.  Of course, there is a
limit to this, mostly with increasing diameters.  But unless there is a good
reason against it, this method should be your go-to for any overhanging
counterbores.</p>
<p>
    <a href="#rule-R3.5"><i></i></a>
    <strong>R3.5</strong> 
    —
    Use the overhanging counterbore trick.
  </p>
<p>For transparency, I tried finding an original source for this, but
unfortunately it isn't clear who first came up with it...</p>
<h4 id="layers-of-bridges">Layers of Bridges<a href="#layers-of-bridges"><i></i></a>
</h4>
<p>The concept from the overhanging counterbores can be generalized to an even
more powerful method.  The basic idea of bridges that support further bridges
can be used for quite advanced geometry that does not need any supports.
You will also see this called <em>sequential bridging</em> in some places.</p>
<p>An impressive example is the OpenFlexure microscope
(<a href="https://openflexure.org/projects/microscope/">Website</a>), where this design approach is used
extensively. The best way to understand how it works is to download their
models and view the layers after running the model through a slicer
(<a href="https://build.openflexure.org/openflexure-microscope/v7.0.0-beta1/models/main_body.html">STL Download</a>).</p>


<p>Especially for large bridges, it is a good idea to first build up the bridge
for a few layers before starting the next one. This gives the lower bridge more
strength and more time to cool before it gets loaded with the features on top.</p>
<p>
    <a href="#rule-R3.6"><i></i></a>
    <strong>R3.6</strong> 
    —
    Bridges on top of other bridges allow for advanced geometry, without
requiring additional support structure.
  </p>
<h4 id="well-meant-material-saving">Well Meant Material Saving<a href="#well-meant-material-saving"><i></i></a>
</h4>
<p>We have previously evaluated the I-beam shape in the context of 3d-printing.
The conclusion was that simply shaving off volume is a traditional design
approach that does not map well to 3d-printing.  There are more cases where
this topic pops up.  Cutouts, that are meant to save material, are commonly
added, while really not achieving their purpose in 3d-printed designs.  In fact,
they can lead to the opposite: More material usage and longer print times.</p>

<p><span>
  <div>
    <p><img alt="comparison of print time for a part with and without holes" src="https://blog.rahix.de/design-for-3d-printing/Adapter-Comparison.png"></p><p>A real world example of a part where holes were added with the intent of
saving material.  But after comparing the models it gets clear that the holes
actually increased the material usage and print time.</p>
    
  </div>
</span></p><p>It is necessary to shift the focus from volume to surface area, as that is
where material is most dense in 3d-printed parts.  You should strive to reduce
surface area by making shapes fat and voluminous.</p>
<p>
    <a href="#rule-R3.7"><i></i></a>
    <strong>R3.7</strong> 
    —
    Keep surface area minimal.  Design voluminous.  Do not make cutouts in an attempt to save material.
  </p>
<h4 id="optimizing-bed-adhesion">Optimizing Bed Adhesion<a href="#optimizing-bed-adhesion"><i></i></a>
</h4>
<p>Finally, a topic that mostly becomes relevant when many parts are to be
printed.  To optimize the removal of the final part from the print surface,
certain considerations can be made.</p>
<p>First of all, the surface area that is touching the bed should be carefully
balanced.  Too little will lead to parts toppling over during printing.  Too
much will make detaching the part more difficult.  Slant 3D has some nice tips
about this (<a href="https://www.youtube.com/watch?v=SZwXREFoWKA">YouTube</a>).</p>
<p>
    <a href="#rule-R3.8"><i></i></a>
    <strong>R3.8</strong> 
    —
    Reduce the surface area that is touching the print-bed when aiming for mass production.
  </p>
<h5 id="mouse-ears">Mouse Ears<a href="#mouse-ears"><i></i></a>
</h5>
<p>Related, if you are using brims to counteract parts coming loose, the mouse ear
approach presented by Slant 3D (<a href="https://www.youtube.com/watch?v=MCcFMDv_4eo">YouTube</a>) is a portable
alternative to consider that also reduces post-processing effort.</p>

<p><span>
  <div>
    <p><img alt="mouse ear as a brim alternative" src="https://blog.rahix.de/design-for-3d-printing/Mouse-Ears.png"></p><p>Two types of mouse ears: The right one is placed directly on the part.  The
left one has a small stub which makes removal even easier.</p>
    
  </div>
</span></p><p>Mouse ears are directly designed into the CAD shape of the part so they do not
need to be added later during slicing.</p>
<p>
    <a href="#rule-R3.9"><i></i></a>
    <strong>R3.9</strong> 
    —
    Add mouse ears to parts that have problems with bed adhesion.
  </p>
<h2 id="4-functional-integration">4. Functional Integration<a href="#4-functional-integration"><i></i></a>
</h2>
<h4 id="general-considerations">General Considerations<a href="#general-considerations"><i></i></a>
</h4>
<p>Functional integration is the art of designing such that each part performs as
many of the required functions as possible.  The goal is to reduce part count.
This speeds up assembly and thus often leads to decreased manufacturing cost.</p>
<p>A ubiquitous example of functional integration are clips in injection molded
plastic parts.  They replace the additional joining element that would be
required otherwise.  For this reason, they play a large role in making
products cheaper and easier to manufacture.  Clips will be discussed in more
detail later.</p>
<p>3d-printing is suited especially well for functional integration, because
additional geometric complexity does not really impact manufacturing cost.
This is in contrast to, for example, subtractive machining, where additional
geometric features generally drive up the cost.  So striving for maximum
functional integration is usually a good idea with 3d-printing.</p>
<p>However, there are some nuances to this.  Printing everything in a single part
limits the print orientation for some features. This can negatively affect
strength, tolerances, and geometric freedom.  Additionally, a highly integrated
part is more difficult to prototype because each iteration requires reprinting
a large model.  And later in the life cycle, repair of a broken part also
requires replacement of a bigger component than what would have been necessary
with a design of many smaller parts that are joined together.  More than once,
I have explicitly kept certain functions in separated parts to facilitate
easier prototyping or repair.</p>
<p>Nevertheless, the following chapters will demonstrate some design ideas that
allow integration of certain features.</p>
<h4 id="zip-tie-channels">Zip tie Channels<a href="#zip-tie-channels"><i></i></a>
</h4>
<p>The first idea that I will demonstrate is a rather simple trick.  You can add
small channels to surfaces of your part that allow fastening a zip tie to fix
some cables or wires in place.  There is a great video by Alan Reiner where I
learned about this (<a href="https://www.youtube.com/watch?v=qgt9P6jsyaY">YouTube</a>).  The <em>Zip Tie City</em> demo model
is available for download (<a href="https://www.printables.com/model/248291-zip-tie-city">Printables</a>).</p>

<p><span>
  <div>
    <p><img alt="zip tie channels on a real part" src="https://blog.rahix.de/design-for-3d-printing/ziptie-channels.jpg"></p><p>Zip tie channels are used to fix cables to a moving assembly.  You can spot
one unused zip tie channel to the left.</p>
    
  </div>
</span></p><p>The channel design is quite simply a section of a hollow cylinder.  Dimensions
that work well for smaller zip ties (100&nbsp;mm) can be taken from the drawing below.</p>

<p><span>
  <div>
    <p><img alt="zip tie channel dimensions" src="https://blog.rahix.de/design-for-3d-printing/ziptie-channels.png"></p><p>Zip tie channel dimensions for typical 100&nbsp;mm zip ties.</p>
    
  </div>
</span></p><p>An important note is that orientation of the zip tie channels should be
considered.  When the channel is printed such that no perimeters bridge across
it, it gets much weaker.  In this orientation, the bridge can easily get ripped
out which I have already seen happening in practice.</p>
<p>
    <a href="#rule-R4.1"><i></i></a>
    <strong>R4.1</strong> 
    —
    Use zip tie channels to fasten cables to a part.
  </p>
<h4 id="flexures">Flexures<a href="#flexures"><i></i></a>
</h4>
<p>Usually, mechanical parts are designed such that they can be considered
completely stiff in approximation.  Flexures are design features that
explicitly use elasticity of a part to integrate certain freedoms of
movement (<a href="https://en.wikipedia.org/wiki/Flexure">Wikipedia</a>).</p>
<p>Overall, 3d-printing is perfect for designs that incorporate flexures.  The
thin features required by flexures are easy to print, while complex to create
with other manufacturing methods.</p>
<p>
    <a href="#rule-R4.2"><i></i></a>
    <strong>R4.2</strong> 
    —
    Use flexures to integrate moving features into a part.
  </p>
<p>Two types of flexures that are very common will be discussed in more detail
below: Clips and so-called "living hinges".  But first, a few general points
about flexures in 3d-printing are presented.</p>
<p>The amount of movement allowed by a flexure is mostly limited by how much
deflection is possible without plastic deformation.  A thicker flexure will
start plastically deforming much earlier.  Thus, it is very important to keep
flexures as thin as possible when large movements are intended.  Force can be
increased by stacking multiple thin flexures, rather than creating a single
thick one.  Alternatively, the travel distance of a thicker flexure can be
increased by increasing its length — maybe even in a snaking pattern.</p>

<p><span>
  <div>
    <p><img alt="double flexure for more stiffness" src="https://blog.rahix.de/design-for-3d-printing/cable-comb-hooks.jpg"></p><p>The hooks on this cable comb are made from two parallel flexures.  This is
more stiff than a single flexure, but has the same travel in the elastic
region.</p>
    
  </div>
</span></p><p>
    <a href="#rule-R4.3"><i></i></a>
    <strong>R4.3</strong> 
    —
    Design flexures such that they only deform elastically when used.
  </p>
<p>To make flexures long-lived, it is important to ensure they cannot be moved too
far.  Features should be strategically placed as end-stops to a flexures
movement.  This way, the movement is always kept in the elastic region and the
flexure will live much longer.</p>
<p>
    <a href="#rule-R4.4"><i></i></a>
    <strong>R4.4</strong> 
    —
    Ensure that flexures have hard limits that prevent breaking them.
  </p>
<p>Another use for flexures is sidestepping tolerance issues.  This only works
when no significant force is exerted on the joint, but can be a nice tool to
cheaply solve accuracy issues.  An example of this are the previously
discussed <a href="https://blog.rahix.de/design-for-3d-printing/#grip-fins">"grip fins"</a>.</p>
<h4 id="clips">Clips<a href="#clips"><i></i></a>
</h4>
<p>Arguably the largest amount of flexures show up in the form of clips.  Clips
have revolutionized the mass-production industry because they provide a
fastener-free way of joining together parts.  Either permanently or in a way
that can be repeatedly opened and closed again.  For the same reason, clips are
popular for 3d-printed designs — avoiding additional fastening hardware
is often desirable.</p>
<p>However, care must exercised with design of clips.  The travel distance of the
clip, retention force, and feature size must be carefully balanced.  Another
significant factor is the orientation of the clip relative to the print
surface.  Especially clips built across layers are very fragile.  In many cases
it is better to keep travel minimal and instead work with a larger clip force
for retention.</p>
<p>
    <a href="#rule-R4.5"><i></i></a>
    <strong>R4.5</strong> 
    —
    Ensure clips won't break from use.  Optimize designs for minimal clip movement.
  </p>
<p>For clips that retain by form-locking the other part geometrically, it is a
great idea to include a way to undo the clip.  Usually, this can be achieved by
providing space to reach the clip with a tool like a screw-driver to push it
backwards.</p>
<p>
    <a href="#rule-R4.6"><i></i></a>
    <strong>R4.6</strong> 
    —
    Provide a way to undo form-locking clips.
  </p>
<h4 id="living-hinges">Living Hinges<a href="#living-hinges"><i></i></a>
</h4>
<p>One more type of flexure that has exploded in popularity as part of optimizing
for mass-production are "living hinges".  These are hinges which move by
plastic deformation rather than disjoint parts rotating on an axis.  Any cheap
plastic container with a lid probably uses one or two of these hinges, because
they are so much more cost-effective and simple than proper hinged joints.</p>
<p>We can also make use of living hinges in 3d-printed design, but there are a few
limitations:  The hinge absolutely needs to to lie flat on the print bed to
achieve the thin sheet of plastic that reliably deforms.  Experiments have been
done with living hinges made from bridging layers, but these do not perform
nearly as good.</p>

<p>Engineers Grow has discussed living hinges for 3d-printing
(<a href="https://www.youtube.com/watch?v=E2CqXgaQsyI">YouTube</a>) and so has Slant 3D (<a href="https://www.youtube.com/watch?v=TiEyFle6lTM">YouTube</a>).</p>
<h4 id="printed-bearings">Printed Bearings<a href="#printed-bearings"><i></i></a>
</h4>
<p>Another design element that's sometimes suited for integration are bearings.
Especially where large diameter bearings are required, it can be more effective
to integrate the bearing into the design of the mating parts instead of using
an off-the-shelf component.</p>
<p>To integrate a bearing, the races become features of the part design.  During
assembly, standard steel bearing balls are inserted.  Optionally, a printed
cage may be necessary to ensure correct spacing between the balls.</p>

<p><span>
  <div>
    <p><img alt="printed bearing" src="https://blog.rahix.de/design-for-3d-printing/printed-bearing.jpg"></p>
    
  </div>
</span></p><p>Positive Altitude has published a video where printed bearing design is
discussed in a lot more detail (<a href="https://youtu.be/0KhV6JW2sHg">YouTube</a>).</p>
<h4 id="print-in-place-mechanisms">Print-in-place Mechanisms<a href="#print-in-place-mechanisms"><i></i></a>
</h4>
<p>As the final type of functional integration that will be discussed,
<em>print-in-place mechanisms</em> are another unique possibility of 3d-printing.
Multiple parts of an interlocking mechanism can be printed at the same time.
No post-print assembly is required.</p>
<p>Print-in-place mechanisms can include joints that would be impossible to
assemble after printing.  The trick is that interlocking parts are printed in
their already joined position.  This can be an incredibly useful design tool to
make strong mechanisms without complicated assembly.</p>

<p><span>
  <div>
    <p><img alt="herringbone planetary gear set" src="https://blog.rahix.de/design-for-3d-printing/Herringbone-Gears.jpg"></p><div>
      <p>Standard example of a print-in-place mechanism is the herringbone planetary
gear set.  This model could not be assembled from its parts after printing.</p>
<p>Design by <a href="https://www.thingiverse.com/thing:53451">Emmett Lalish</a> and picture by <a href="https://www.printables.com/model/3119-herringbone-planetary-gear">Prusa Research</a>.</p>
    </div>
    
  </div>
</span></p><p>The big downside of print-in-place mechanisms is the additional design
complexity.  Part orientation is fixed, making avoiding supports and design for
strength difficult to achieve.  Especially where interlocking parts meet, it
can be difficult to adequately support floating geometry.  Sometimes, special
<em>break-away surfaces</em> are placed between parts.  These then have to be broken
with force after printing.</p>
<p>
    <a href="#rule-R4.7"><i></i></a>
    <strong>R4.7</strong> 
    —
    Use break-away surfaces to support floating geometry in print-in-place designs.
  </p>
<p>Another critical topic for print-in-place design is the necessary clearance
between interfacing parts.  Printing multiple surfaces very close to each other
can be difficult for some printers.  It seems that a clearance of 0.3&nbsp;mm
should allow most current printers to print a design.</p>
<p>
    <a href="#rule-R4.8"><i></i></a>
    <strong>R4.8</strong> 
    —
    Ensure sufficient clearance between features in print-in-place designs.
  </p>
<h2 id="5-beyond-plastic-machine-elements">5. Beyond plastic - Machine Elements<a href="#5-beyond-plastic-machine-elements"><i></i></a>
</h2>
<p>After discussing the peculiarities of 3d-printed parts for so long, it is time
to widen the horizon.  Most functional designs will not be entirely 3d-printed,
but also include other components.  The largest number of them are most likely
going to be <em>machine elements</em>.  Screws, nuts, bearings, and all the other
standard parts optimized for certain functions.  In the following chapter,
design considerations related to the use of such machine elements will be
evaluated.</p>
<h4 id="nuts-and-bolts">Nuts and bolts<a href="#nuts-and-bolts"><i></i></a>
</h4>
<p>Arguably the most ubiquitous machine elements are screws.  For the most part,
they are used to fasten different components to each other.  While it is easy
to just pick the next best screw and design your parts around it, there is more
to it if the joint is meant to last.  In traditional mechanical design,
selection of screws can be a science of itself. Especially, when the screw is
heavily loaded or safety critical.  With design for 3d-printing, it is
technically no different — but fortunately, you can usually get away with
ignoring a lot of the complexity involved.  Nonetheless, I think it is
important to at least know the limits and some general guidance on what is good
or bad for a screwed connection of 3d-printed parts.</p>
<h6 id="screw-preload">Screw Preload<a href="#screw-preload"><i></i></a>
</h6>
<p>In screw theory, one of the most important values is the preload force
(<a href="https://en.wikipedia.org/wiki/Bolted_joint#Theory">Wikipedia</a>).  That's the clamping force exerted by
the screw at rest after tightening.  For a screwed joint to work, the preload
force must be greater than any forces experienced by the joint during
operation.</p>

<p><span>
  <div>
    <p><img alt="spring analogy for a bolted joint" src="https://blog.rahix.de/design-for-3d-printing/Spring_Analogy_for_a_Bolted_Joint.jpg"></p><div>
      <p>A bolted or screwed joint can be modelled using springs.  Preload compresses
the parts and elongates the screw.  As long as the external load never
becomes greater than the preload, the joint is fixed in place.</p>
<p>Image by <a href="https://commons.wikimedia.org/wiki/File:Spring_Analogy_for_a_Bolted_Joint.jpg">Stephen Mckelvey</a>.</p>
    </div>
    
  </div>
</span></p><p>In metal to metal joints, this preload is achieved by elongation of the screw.
In 3d-printing and other thermoplastics applications, the joint members are much
less stiff than the screw, leading to the preload mostly coming from compression
of the plastic parts instead.  The implication is that any of the traditional
preload calculation methods are mostly useless here.</p>
<p>Attempting to reach the required preload force, one quickly reaches the yield
strength limit of the printed parts, leading to deformation or worse.  While
the achievable preload is usually enough to deal with static loads of an
application, it is often not possible to counteract the dynamic loads.
Vibrations or shocks are usually the worst offenders.</p>
<p>The solution is one that's also often used for additional security of metal
joints as well:  Use additional screw locking measures, like lock-nuts or a
threadlocking adhesive.</p>
<p>
    <a href="#rule-R5.1"><i></i></a>
    <strong>R5.1</strong> 
    —
    Protect dynamically loaded screws with additional locking measures like threadlocking adhesive.
  </p>
<h6 id="screw-length">Screw Length<a href="#screw-length"><i></i></a>
</h6>
<p>Another noteworthy parameter of screwed joints is the length of the screw, or
more precisely the length of the compressed section of the 3d-printed parts. As
a rule of thumb, you should design the joint such that the screw is as long as
possible.</p>
<p>There are two effects at play that make this a reasonable choice:</p>
<ol>
<li>A long screw ensures the part is preloaded entirely in compression rather
than tension.  When an external load is then applied, it counteracts the
compression.  With a short screw, the external load would just add more
tension which printed parts often cannot take well anyway.</li>
<li>A longer screw has to be tightened with more turns to compress the part past
its yield strength.  This means it becomes less likely to accidentally
overtighten the joint.</li>
</ol>
<p>
    <a href="#rule-R5.2"><i></i></a>
    <strong>R5.2</strong> 
    —
    Design screwed connections for maximum screw length.
  </p>
<h6 id="threads-in-printed-parts">Threads in Printed Parts<a href="#threads-in-printed-parts"><i></i></a>
</h6>
<p>With screws and thermoplastics, an important consideration is the thread for the
screw to fasten to.  In metal applications, the thread is often simply cut into
the mating part.  With 3d-printed parts, the same is possible, but threads made
from thermoplastics are a lot weaker.  This means the threads can be easily
stripped by overtightening the screw.</p>
<p>Nonetheless, plastic threads are an option.  They can be tapped into a part if
an adequately sized hole is printed and wall thickness is high enough.  For
bigger diameter threads (M8 and up), modelling the threads in CAD and printing
them is an option, although quality of such threads is not always good.  The
strength of thermoplastic threads is surprisingly good — however
they do not like repeated assembly and the aforementioned overtightening problem
should not be underestimated.</p>

<p><span>
  <div>
    <p><img alt="tapping a thread into a printed part" src="https://blog.rahix.de/design-for-3d-printing/thread-tapping.jpg"></p><p>Tapping a thread into a printed part.</p>
    
  </div>
</span></p><p>
    <a href="#rule-R5.3"><i></i></a>
    <strong>R5.3</strong> 
    —
    Cut threads into printed parts with a thread tap for quick design of low-reuse joints.
  </p>
<h6 id="rib-thread-forming">Rib Thread Forming<a href="#rib-thread-forming"><i></i></a>
</h6>
<p>An alternative to thread tapping is to combine the previously discussed concept
of <a href="https://blog.rahix.de/design-for-3d-printing/#crush-ribs">Crush Ribs</a> with a standard threaded screw.  The screw can
deform the crush ribs to create its own thread.  This rib thread forming works
surprisingly well, as demonstrated by Thomas Sanladerer
(<a href="https://www.youtube.com/watch?v=HgEEtk85rAY">YouTube</a>).</p>

<p><span>
  <div>
    <p><img alt="rib threads in a part" src="https://blog.rahix.de/design-for-3d-printing/rib-threads.png"></p><p>A part designed for rib thread forming, next to through holes with the
teardrop trick discussed earlier.  The rib threads start tapered to make it
easy to get the screw in.</p>
    
  </div>
</span></p><p>
    <a href="#rule-R5.4"><i></i></a>
    <strong>R5.4</strong> 
    —
    Use rib thread forming for no-postprocessing low-reuse threads in printed parts.
  </p>
<h6 id="threaded-inserts">Threaded Inserts<a href="#threaded-inserts"><i></i></a>
</h6>
<p>As the downsides of plastic threads are well known from injection molding,
alternative solutions have emerged.  Most popular are small metal threaded
inserts which are added to the part as a strong and long-lasting thread.
Usually, these inserts are heat-set into an appropriately dimensioned hole.</p>

<p><span>
  <div>
    <p><img alt="threaded inserts" src="https://blog.rahix.de/design-for-3d-printing/threaded-inserts.jpg"></p><p>M5 heat-set threaded inserts in a 3d-printed part.</p>
    
  </div>
</span></p><p>Threaded inserts are much stronger than plastic threads.  They are not easily
stripped by overtightening and they survive repeated reassembly many times
over.</p>
<p>Such threaded inserts have become very popular in the 3d-printing community.
Stefan from CNC Kitchen has a nice writeup about the use of heat-set inserts
(<a href="https://www.cnckitchen.com/blog/tipps-amp-tricks-fr-gewindeeinstze-im-3d-druck-3awey">CNC Kitchen</a>).</p>
<p>
    <a href="#rule-R5.5"><i></i></a>
    <strong>R5.5</strong> 
    —
    Use heat-set threaded inserts to add highly reusable and robust threads to a part.
  </p>
<p>A design limitation of threaded inserts is that they are not reliably usable
for screws inserted from the back side.  During insertion, heat-set inserts
often push some molten plastic into the hole beneath them, preventing easy
insertion of a screw from the back side.  While the hole diameter can be tuned
to prevent this, my experience is that this is not really portable to other
3d-printers.</p>
<h6 id="embedded-nuts">Embedded Nuts<a href="#embedded-nuts"><i></i></a>
</h6>
<p>A more cost effective alternative to threaded insert is embedding standard nuts
into 3d-printed parts.  Usually, this is done by preparing an appropriate
cutout into which the nut can be inserted after or during printing.</p>

<p><span>
  <div>
    <p><img alt="embedded nuts" src="https://blog.rahix.de/design-for-3d-printing/embedded-nut.jpg"></p><p>A standard M4 nut embedded into a part.  It was inserted through the sideface
cutout visible in the picture.</p>
    
  </div>
</span></p><p>Embedded nuts are not only cheap, they are also a great alternative for the
mentioned design limitation of threaded inserts.  A nut is predestined to be
placed on the far side of a part, making through-joints with long screws easy
to realize.  This also plays into the benefits of using long screws again.</p>
<p>Two kinds of cutouts have emerged in my designs:</p>
<ul>
<li>Sideface cutouts are rectangular cutouts reaching in from a side of the part to insert the nut horizontally.</li>
<li>Backface cutouts are hexagonal pockets on the exit of the screw through-hole.</li>
</ul>

<p><span>
  <div>
    <p><img alt="embedded nuts" src="https://blog.rahix.de/design-for-3d-printing/Nut-Slot-Options.png"></p><p>Cross-section view of different cutouts for embedded nuts.  On the left, the
cutouts are from the side.  On the right, the they are from the back of the
screw.</p>
    
  </div>
</span></p><p>
    <a href="#rule-R5.6"><i></i></a>
    <strong>R5.6</strong> 
    —
    Make cutouts to embed standard nuts into a part.
  </p>
<p>The big downside of the embedded nuts approach is that they often fall out of
their cutouts during assembly.  Tuning the cutout dimensions usually does not
work well — the only real solution is to "plug" the cutouts by inserting
the nuts halfway through the print.  This does, however, complicate the
printing and assembly complexity considerably, so it often is not a desirable
choice either.</p>
<h6 id="thread-strength">Thread Strength<a href="#thread-strength"><i></i></a>
</h6>
<p>With all these options for adding threads and the many design considerations
surrounding them, one last factor that does play a role is the strength of the
threads.  Lucky for us, all of the discussed options almost always have
strength beyond the design requirements, with the other factors being more
prevalent.</p>
<p>Again, Stefan from CNC Kitchen has a nice writeup on his experiments
on the topic (<a href="https://www.cnckitchen.com/blog/helicoils-threaded-insets-and-embedded-nuts-in-3d-prints-strength-amp-strength-assessment">CNC Kitchen</a>).</p>
<h4 id="dowel-pins">Dowel Pins<a href="#dowel-pins"><i></i></a>
</h4>
<p>Let's look at machine elements beyond screws next.  While the use of dowel pins
is quite rare in 3d-printed design, they should still get mentioned.  In
traditional mechanical design, they are the go-to solution for accurately and
repeatedly locating one part to another.</p>
<p>As the tolerances of 3d-printing often make such accuracy superfluous, the uses
of dowel pins here are limited.  Additionally, such pins need an engineering
fit which is difficult to achieve in 3d-printing.  The best solution are the
<a href="https://blog.rahix.de/design-for-3d-printing/#circles-considered-harmful">hexagonal holes</a> or <a href="https://blog.rahix.de/design-for-3d-printing/#crush-ribs">crush ribs</a>
mentioned before, or post processing of the printed parts.</p>

<p><span>
  <div>
    <p><img alt="embedded nuts" src="https://blog.rahix.de/design-for-3d-printing/engraving-jig.png"></p><p>A fixture for precisely aligning a part for laser engraving.  Dowel pins were
used as reliable locating features.</p>
    
  </div>
</span></p><h4 id="embedded-hardware">Embedded Hardware<a href="#embedded-hardware"><i></i></a>
</h4>
<p>With the <a href="https://blog.rahix.de/design-for-3d-printing/#embedded-nuts">embedded nuts</a>, we already discussed one element that
can be inserted into a part mid-print.  This concept can be used for many
other things as well — many designs have been published that use this
method in some way.</p>
<p>To quickly discuss the concept again, it works like this:</p>
<ol>
<li>In the design, a cavity is left for the component to occupy.</li>
<li>During slicing, a pause is configured right before the layer above the cavity.</li>
<li>When the printer reaches the pause, the hardware is inserted.</li>
<li>Then the print is resumed, enclosing the hardware and often permanently fusing the part with it.</li>
</ol>
<p>To get some inspiration, take a look at the window in this <a href="https://www.printables.com/model/894202-modern-gridfinity-case">Modern Gridfinity
Case</a> by Matthew.  It is made from a transparent
PETG sheet that gets inserted into the print.  The design even includes small
tabs to hold the sheet in place, right as it is inserted.  This makes for a very
robust design and the result is hard to achieve with any other methods.</p>
<div>


  <p><img alt="modern gridfinity case" src="https://blog.rahix.de/design-for-3d-printing/modern-gridfinity-case.jpg">
    
  </p>




  <p><img alt="inserting the window sheet" src="https://blog.rahix.de/design-for-3d-printing/insert-transparent-sheet.gif">
    
  </p>


</div>
<p>Of course, many other components can be inserted into 3d-prints.  To name a few more:</p>
<ul>
<li>"Trapped fasteners" are inserted such that they cannot fall out of the part, making it hard to loose them.</li>
<li>Magnets</li>
<li>Metal mesh</li>
</ul>
<p>Of course the big advantage of embedded hardware is that additional joining or
fastening means can be avoided.  Do, however, consider the downside that
embedded hardware can only be extracted destructively and replacement is
usually impossible.</p>
<p>
    <a href="#rule-R5.7"><i></i></a>
    <strong>R5.7</strong> 
    —
    Embed hardware into 3d-printed parts to avoid more complex fastening or joining methods.
  </p>
<h4 id="printing-on-fabric">Printing on Fabric<a href="#printing-on-fabric"><i></i></a>
</h4>
<p>A technique that is very similar to inserted hardware, but deserves attention on
its own, is incorporating fabric into a 3d-print.  This produces parts with
unique flexibility, especially interesting for wearable applications.  For this
reason, 3d-printing onto fabric is quite popular in the cosplay scene.</p>
<p>The gist is that a print consisting of many individual pieces is paused roughly
2 to 3 layers in.  Then a piece of tulle fabric is draped across the initial
layers.  When resuming the print, the new layers on top will fuse into the lower
ones, securing each part to the fabric.</p>

<p><span>
  <div>
    <p><img alt="Printing on Fabric" src="https://blog.rahix.de/design-for-3d-printing/print-on-fabric.jpg"></p><div>
      <p>Showcase of some cosplay parts printed onto fabric.</p>
<p>Image by <a href="https://www.youtube.com/@UncleJessy">Uncle Jessy</a>.</p>
    </div>
    
  </div>
</span></p><p>The flexibility of the resulting design can be finely controlled by the geometry
of the solid 3d-printed shapes.  Other people have already experimented with this
technique a lot, for example you can check out the video tutorial from UncleJessy
(<a href="https://www.youtube.com/watch?v=CjH5KGw3Psw">YouTube</a>).</p>
<h2 id="6-appearance">6. Appearance<a href="#6-appearance"><i></i></a>
</h2>
<h4 id="complex-shapes">Complex Shapes<a href="#complex-shapes"><i></i></a>
</h4>
<p>While appearance isn't the main concern of functional design, no parts should
be designed without consideration of their appearance.  3d-printing uniquely
allows complex surface shapes to be added "for free".  Suddenly, the
consideration is no longer about increased manufacturing cost, the only
remaining factor is the time it takes to design such shapes.</p>
<p>Of course, complex shapes should still follow the basic constraints of the
process — avoiding overhangs and not mandating support material.  But
beyond that, there is no reason to limit designs to the straight and
rectangular shapes of traditional functional parts.</p>
<p>There are even places where complex, maybe organic, shapes have a use beyond
appearance: For ergonomic design, such shapes are usually much better suited.</p>
<p>
    <a href="#rule-R6.1"><i></i></a>
    <strong>R6.1</strong> 
    —
    Complex shapes are often "for free" in 3d-printing.  Use them to improve
appearance or ergonomics.
  </p>
<h4 id="shadow-lines">Shadow Lines<a href="#shadow-lines"><i></i></a>
</h4>
<p>One trick we can borrow from traditional product design practices are <em>shadow
lines</em>.  They improve the appearance of seams between mating surfaces of
multiple joined parts.  When two surfaces are pressed directly against each
other, the result will never look great due to the imperfections on both sides
leading to an uneven seam.</p>
<p><em>Shadow lines</em> improve on this by leaving a controlled larger gap between the
surfaces and hiding the actual mechanical connection behind a small lip.  The
larger gap is wide enough such that the small surface imperfections are no
longer distinguishable.  This leaves the result with a very even look without
requiring extreme precision.</p>

<p><span>
  <div>
    <p><img alt="Example of a part with a shadow line" src="https://blog.rahix.de/design-for-3d-printing/shadow-line-valve.jpg"></p><p>The upper and lower half of the valve actuator body have an evenly sized gap
between them.  The real seam is actually hidden behind the shadow line.</p>
    
  </div>
</span></p><p>It may be enough to place a single lip on the shadow line to hide the real seam.
If dust protection is of concern, a second lip on the inner side can transform
the shadow line into a sort of labyrinth seal, providing for more than just
aesthetics.</p>
<p>Shadow lines can also be used on parts joined at an angle, like a lid placed
flat onto a box.  In this case, a constant offset is left between the size of
the lid and the surrounding box, with alignment tabs on the inside ensuring the
lid stays centered.</p>

<p><span>
  <div>
    <p><img alt="Shadow line on the lid of an electronics enclosure" src="https://blog.rahix.de/design-for-3d-printing/shadow-line-electronics.jpg"></p><p>The lid is surrounded by yet another evenly sized gap.  To ensure it stays
centered, alignment ribs on the inside locate it quite precisely.</p>
    
  </div>
</span></p><p>To learn even more about this concept, Teaching Tech has made a video about the
topic (<a href="https://youtu.be/8dhFhU7Nl_0">YouTube</a>).</p>
<p>
    <a href="#rule-R6.2"><i></i></a>
    <strong>R6.2</strong> 
    —
    Create shadow lines along the joining edge between two parts.
  </p>
<h4 id="surface-texture">Surface Texture<a href="#surface-texture"><i></i></a>
</h4>
<p>Another significant factor for appearance is surface texture.  With
3d-printing, we generally do not have much control about them.  Especially
vertical surfaces will always prominently display the layer lines inherent to
the process.</p>
<p>With special print beds, like textured steel sheets, the bottom surface of parts
can be refined.  This allows for a very clean texture, but is of course quite
limited because only one surface of each part can get this treatment.
Sometimes, it is actually desirable to avoid having a bed surface on a part, to
keep all sides looking more even.  One way to do this is rotating parts to a
diagonal angle relative to the print bed.</p>
<p>For vertical surfaces, where the layer lines are sometimes destroying the
aesthetics of a part, a trick has been developed to help hiding them.  <em>Fuzzy
skin</em> adds irregularities to each layer perimeter, building up to a surface that
appears more rough, but without prominent layer lines.</p>
<p>Fuzzy skin also has a pleasant feel to it, if tuned correctly, which has made it
a popular choice for 3d-printed grips and handles.</p>

<p><span>
  <div>
    <p><img alt="Part with fuzzy skin" src="https://blog.rahix.de/design-for-3d-printing/fuzzy_skin_preview.jpg"></p><div>
      <p>Fuzzy skin on a tool grip makes for a nice appearance and pleasant feeling while holding.</p>
<p>Image by <a href="https://help.prusa3d.com/article/fuzzy-skin_246186">Prusa Research</a></p>
    </div>
    
  </div>
</span></p><p>
    <a href="#rule-R6.3"><i></i></a>
    <strong>R6.3</strong> 
    —
    Use surface texture to make parts appear less 3d-printed.
  </p>
<h4 id="printed-text">Printed Text<a href="#printed-text"><i></i></a>
</h4>
<p>Another nice side-effect of the nature of 3d-printing is the fact that it is
laughably simple to add text or symbols to a part.  No need for stickers,
decals, or labor-intensive engravings.  There is no excuse to not add text to a
printed part.</p>
<p>A starting point is to make it a habit to add part numbers into each part of a
larger design.  This way it is easy to keep everything organized.  Especially
when iterating on a part a lot, having the revision index printed onto it can
reduce confusion quite a lot.</p>

<p><span>
  <div>
    <p><img alt="part with text printed on it" src="https://blog.rahix.de/design-for-3d-printing/engraved-text.jpg"></p><p>An enclosure with a label text engraved into it.</p>
    
  </div>
</span></p><p>There are two ways to add text or symbols:  Either the text gets cut into the
surface, which is called engraving.  Or the text can be raised over the
surface, called embossing.  For the most part, engraving leads to much cleaner
results with 3d-printing.  This is especially true as the font size gets
smaller.</p>
<p>
    <a href="#rule-R6.4"><i></i></a>
    <strong>R6.4</strong> 
    —
    Prefer engraving text over embossing.
  </p>
<p>Regarding orientation, the best results will be achieved by placing the text such
that it is vertical to the print surface.  This way it gets printed as part of
the perimeter lines where the printer can achieve the most detail.</p>
<p>
    <a href="#rule-R6.5"><i></i></a>
    <strong>R6.5</strong> 
    —
    Place engraved/embossed text vertical to the print surface.
  </p>
<p>Experience has shown that most printers can print a text with 0.6&nbsp;mm
minimum stroke width and engraved to 0.5&nbsp;mm depth without issues.</p>

<p>For the final chapter, I want to investigate a completely different design
approach for 3d-printing — vase mode design.  Inherently, the FFF
3d-printing process is not volumetric.  Volumetric parts are only achieved by
using lines of plastic, fused together such that a volume emerges.  But
whether this is the most efficient way to make use of the 3d-printing process
is a question worth asking.</p>
<p>Vase mode is an alternative approach where only a hollow shell of a part is
printed, using a single perimeter line.  Instead of individual layers, the Z
height slowly increases such that filament is laid down in a spiral. This has
a number of advantageous consequences:</p>
<ul>
<li>It is very efficient because 100% of the print time is spent extruding filament.</li>
<li>Due to the lack of individual layers, there is no seam in the surface.</li>
<li>As the printer never stops printing, no stringing issues can occur.</li>
<li>It is very fast because the least amount of plastic is used to represent a part's shape.</li>
<li>It can also leads to very light parts due to the decreased filament usage.</li>
</ul>
<p>The big downside is of course that parts become very unstable due to the
missing internal support.  For certain kinds of parts, especially artistic
ones, this may not be a problem.  Even for functional parts, like vases —
where this technique got its name — the strength might already be
sufficient.</p>

<p><span>
  <div>
    <p><img alt="Vase mode vase" src="https://blog.rahix.de/design-for-3d-printing/vase.png"></p>
    
  </div>
</span></p><p>With vase mode, the most important factor to part strength is surface geometry.
Surfaces that are flat or only bend in only one direction perform the worst.
Intricate shapes often show much more strength.  This can be seen especially
with the impressive strength some vase designs manage to achieve.</p>
<h4 id="beading-patterns">Beading Patterns<a href="#beading-patterns"><i></i></a>
</h4>
<p>What those vases achieve through style can be used for technical designs in
the form of <em>beading patterns</em>.  They describe features that are added to thin
sheet-material parts to give them additional strength.  CNC-Kitchen has made a
video about using beading patterns for 3d-printing where this concept is
explored in more detail (<a href="https://www.youtube.com/watch?v=3-ygdNQThAs">YouTube</a>).  There is also a german
web-book going into detail about beading patterns in general
(<a href="https://4ming.de/de/forming-handbuch/leichtbau-durch-sicken-fachbuch">4ming.de</a>).</p>

<p>
    <a href="#rule-R7.1"><i></i></a>
    <strong>R7.1</strong> 
    —
    Use beading patterns to make vase mode parts more stiff.
  </p>
<h4 id="unconventional-vase-mode">Unconventional Vase Mode<a href="#unconventional-vase-mode"><i></i></a>
</h4>
<p>Interestingly, it is possible to adapt vase mode for even more uses.  The trick
is in viewing vase mode as a way to have very tight control over the way that a
printer extrudes filament.  A part is designed such that its surface perimeters
describe a path for the printer which ultimately results in the intended
geometry.  An impressive example of this is the stackable tray designed by
FPacheco (<a href="https://www.printables.com/model/514142-vase-mode-stackable-hextray-60-mm-hexagons">Printables</a>).</p>

<p><span>
  <div>
    <p><img alt="vase mode stackable tray" src="https://blog.rahix.de/design-for-3d-printing/vase-mode-tray.jpg"></p><p>This tray looks nothing like a vase-mode part but is the result of very careful and clever design.</p>
    
  </div>
</span></p><p>While the resulting part for such designs looks nothing like a normal vase mode
part, it still gets the 100% print time benefit of vase mode.  Especially when
a part shall be produced in high quantity, this may have significant impact.</p>
<h2 id="conclusion">Conclusion<a href="#conclusion"><i></i></a>
</h2>
<p>Alright, you have made it to the end.  Below, you will find a final summary of
all the design rules discussed in this blog post.  Use them as a reference for
your designs, as "sane defaults" when there is no good reason to deviate.  But
at the same time, these are no laws — there are many situations where
doing things differently is the better choice.  It is your job as the design
engineer to make these decisions.</p>
<p>I have thought long about where I should publish this information.  For now,
placing it here on my blog felt like the easiest choice.  But such "rules" live
from discussion and I would love to hear your opinions and to get talking about
them.  If enough interest emerges, I would be interested in turning this into an
"open-source book" where others can contribute their own rules or suggest
changes to the existing ones.  My ultimate goal is finding the best approaches
for design for 3d-printing.  And such a feat is only achievable by the community
at large.</p>
<h2 id="checklist">Checklist<a href="#checklist"><i></i></a>
</h2>
<p>As promised, here is a summary of all the rules from the blog post.  With
checkboxes, in case you like them :)</p>
<!-- START RULES LIST -->
<ol>
<li><strong>Designing for Part Strength</strong>
<ul>
<li><label>
  
  <a href="#rule-R1.1"><strong>R1.1</strong></a> 
  —
  Tensile forces should be aligned parallel to the print surface.
</label>
</li>
<li><label>
  
  <a href="#rule-R1.2"><strong>R1.2</strong></a> 
  —
  Split a part into multiple pieces when no orientation is ideal for all loads.
</label>
</li>
<li><label>
  
  <a href="#rule-R1.3"><strong>R1.3</strong></a> 
  —
  Most strength comes from the part's surface, not the infill.
</label>
</li>
<li><label>
  
  <a href="#rule-R1.4"><strong>R1.4</strong></a> 
  —
  Guide forces on the most direct path possible.
</label>
</li>
<li><label>
  
  <a href="#rule-R1.5"><strong>R1.5</strong></a> 
  —
  Use large cross sections.  Prefer thick shapes over thin shapes.
</label>
</li>
</ul>
</li>
<li><strong>Manufacturing Tolerance and Part Finish</strong>
<ul>
<li><label>
  
  <a href="#rule-R2.1"><strong>R2.1</strong></a> 
  —
  Use chamfers on edges parallel to the print surface.  Use fillets on edges
vertical to the print surface.
</label>
</li>
<li><label>
  
  <a href="#rule-R2.2"><strong>R2.2</strong></a> 
  —
  Improve horizontal holes by using a teardrop shape or giving the hole a flat roof.
</label>
</li>
<li><label>
  
  <a href="#rule-R2.3"><strong>R2.3</strong></a> 
  —
  Use a teardrop shape for vertical holes to avoid inaccuracy due to perimeter
seams.
</label>
</li>
<li><label>
  
  <a href="#rule-R2.4"><strong>R2.4</strong></a> 
  —
  Consider where the seam will be placed.  If tolerances are tight, provide a
sharp concave corner for the seam the hide in.
</label>
</li>
<li><label>
  
  <a href="#rule-R2.5"><strong>R2.5</strong></a> 
  —
  Design part geometry for easy motion paths while printing, to improve dimensional accuracy.
</label>
</li>
<li><label>
  
  <a href="#rule-R2.6"><strong>R2.6</strong></a> 
  —
  Prevent warping by making parts voluminous and their surfaces smooth and rounded.  The ideal shape is a sphere.
</label>
</li>
<li><label>
  
  <a href="#rule-R2.7"><strong>R2.7</strong></a> 
  —
  If you can't make it precise, make it adjustable.
</label>
</li>
<li><label>
  
  <a href="#rule-R2.8"><strong>R2.8</strong></a> 
  —
  Do not use circular holes for interference fits.  Use hexagon or square holes instead.
</label>
</li>
<li><label>
  
  <a href="#rule-R2.9"><strong>R2.9</strong></a> 
  —
  Use crush ribs for press fits that are only assembled once.
</label>
</li>
<li><label>
  
  <a href="#rule-R2.10"><strong>R2.10</strong></a> 
  —
  Use grip fins for press fits that need to be reassembled more than once.
</label>
</li>
</ul>
</li>
<li><strong>Process Optimization</strong>
<ul>
<li><label>
  
  <a href="#rule-R3.1"><strong>R3.1</strong></a> 
  —
  Avoid the necessity of support material.
</label>
</li>
<li><label>
  
  <a href="#rule-R3.2"><strong>R3.2</strong></a> 
  —
  Clever part orientation on the print surface can eliminate the need for
supports.
</label>
</li>
<li><label>
  
  <a href="#rule-R3.3"><strong>R3.3</strong></a> 
  —
  Split a part into multiple pieces when no orientation can avoid supports.
</label>
</li>
<li><label>
  
  <a href="#rule-R3.4"><strong>R3.4</strong></a> 
  —
  Use sacrificial layers to avoid internal overhangs that would otherwise require supports.
</label>
</li>
<li><label>
  
  <a href="#rule-R3.5"><strong>R3.5</strong></a> 
  —
  Use the overhanging counterbore trick.
</label>
</li>
<li><label>
  
  <a href="#rule-R3.6"><strong>R3.6</strong></a> 
  —
  Bridges on top of other bridges allow for advanced geometry, without
requiring additional support structure.
</label>
</li>
<li><label>
  
  <a href="#rule-R3.7"><strong>R3.7</strong></a> 
  —
  Keep surface area minimal.  Design voluminous.  Do not make cutouts in an attempt to save material.
</label>
</li>
<li><label>
  
  <a href="#rule-R3.8"><strong>R3.8</strong></a> 
  —
  Reduce the surface area that is touching the print-bed when aiming for mass production.
</label>
</li>
<li><label>
  
  <a href="#rule-R3.9"><strong>R3.9</strong></a> 
  —
  Add mouse ears to parts that have problems with bed adhesion.
</label>
</li>
</ul>
</li>
<li><strong>Functional Integration</strong>
<ul>
<li><label>
  
  <a href="#rule-R4.1"><strong>R4.1</strong></a> 
  —
  Use zip tie channels to fasten cables to a part.
</label>
</li>
<li><label>
  
  <a href="#rule-R4.2"><strong>R4.2</strong></a> 
  —
  Use flexures to integrate moving features into a part.
</label>
</li>
<li><label>
  
  <a href="#rule-R4.3"><strong>R4.3</strong></a> 
  —
  Design flexures such that they only deform elastically when used.
</label>
</li>
<li><label>
  
  <a href="#rule-R4.4"><strong>R4.4</strong></a> 
  —
  Ensure that flexures have hard limits that prevent breaking them.
</label>
</li>
<li><label>
  
  <a href="#rule-R4.5"><strong>R4.5</strong></a> 
  —
  Ensure clips won't break from use.  Optimize designs for minimal clip movement.
</label>
</li>
<li><label>
  
  <a href="#rule-R4.6"><strong>R4.6</strong></a> 
  —
  Provide a way to undo form-locking clips.
</label>
</li>
<li><label>
  
  <a href="#rule-R4.7"><strong>R4.7</strong></a> 
  —
  Use break-away surfaces to support floating geometry in print-in-place designs.
</label>
</li>
<li><label>
  
  <a href="#rule-R4.8"><strong>R4.8</strong></a> 
  —
  Ensure sufficient clearance between features in print-in-place designs.
</label>
</li>
</ul>
</li>
<li><strong>Beyond plastic - Machine Elements</strong>
<ul>
<li><label>
  
  <a href="#rule-R5.1"><strong>R5.1</strong></a> 
  —
  Protect dynamically loaded screws with additional locking measures like threadlocking adhesive.
</label>
</li>
<li><label>
  
  <a href="#rule-R5.2"><strong>R5.2</strong></a> 
  —
  Design screwed connections for maximum screw length.
</label>
</li>
<li><label>
  
  <a href="#rule-R5.3"><strong>R5.3</strong></a> 
  —
  Cut threads into printed parts with a thread tap for quick design of low-reuse joints.
</label>
</li>
<li><label>
  
  <a href="#rule-R5.4"><strong>R5.4</strong></a> 
  —
  Use rib thread forming for no-postprocessing low-reuse threads in printed parts.
</label>
</li>
<li><label>
  
  <a href="#rule-R5.5"><strong>R5.5</strong></a> 
  —
  Use heat-set threaded inserts to add highly reusable and robust threads to a part.
</label>
</li>
<li><label>
  
  <a href="#rule-R5.6"><strong>R5.6</strong></a> 
  —
  Make cutouts to embed standard nuts into a part.
</label>
</li>
<li><label>
  
  <a href="#rule-R5.7"><strong>R5.7</strong></a> 
  —
  Embed hardware into 3d-printed parts to avoid more complex fastening or joining methods.
</label>
</li>
</ul>
</li>
<li><strong>Appearance</strong>
<ul>
<li><label>
  
  <a href="#rule-R6.1"><strong>R6.1</strong></a> 
  —
  Complex shapes are often "for free" in 3d-printing.  Use them to improve
appearance or ergonomics.
</label>
</li>
<li><label>
  
  <a href="#rule-R6.2"><strong>R6.2</strong></a> 
  —
  Create shadow lines along the joining edge between two parts.
</label>
</li>
<li><label>
  
  <a href="#rule-R6.3"><strong>R6.3</strong></a> 
  —
  Use surface texture to make parts appear less 3d-printed.
</label>
</li>
<li><label>
  
  <a href="#rule-R6.4"><strong>R6.4</strong></a> 
  —
  Prefer engraving text over embossing.
</label>
</li>
<li><label>
  
  <a href="#rule-R6.5"><strong>R6.5</strong></a> 
  —
  Place engraved/embossed text vertical to the print surface.
</label>
</li>
</ul>
</li>
<li><strong>Extra: Vase Mode Design</strong>
<ul>
<li><label>
  
  <a href="#rule-R7.1"><strong>R7.1</strong></a> 
  —
  Use beading patterns to make vase mode parts more stiff.
</label>
</li>
</ul>
</li>
</ol>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Typed Lisp, a Primer (101 pts)]]></title>
            <link>https://alhassy.com/TypedLisp.html</link>
            <guid>43887998</guid>
            <pubDate>Sun, 04 May 2025 17:17:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alhassy.com/TypedLisp.html">https://alhassy.com/TypedLisp.html</a>, See on <a href="https://news.ycombinator.com/item?id=43887998">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="text-typing-via-macros">
<p>
Checking the type of inputs is tedious and so I <a href="https://www.reddit.com/r/emacs/comments/cct5hp/functional_type_declarations_in_elisp/">guessed</a> it could be done using
macros and advice. Looking at <a href="https://docs.racket-lang.org/ts-guide/types.html">Typed Racket</a> for inspiration, the following
fictitious syntax would add advice to <code>f</code> that checks the optional arguments <code>xᵢ</code>
have type <code>σᵢ</code> and the mandatory positional arguments have type <code>τᵢ</code> according
to position, and the result of the computation is of type <code>τ</code>.
To the best of my knowledge, no one had done this for Emacs Lisp —I don't know why.
</p>
<div>
<pre>(declare-type 'f ((<span>:x₁</span> σ₁) … (<span>:xₘ</span> σₘ)) (τ₁ … τₙ τ))
</pre>
</div>

<p>
To modify a variable, or function, we may simply redefine it; but a much more elegant and powerful
approach is to <a href="https://www.gnu.org/software/emacs/manual/html_node/elisp/Advising-Functions.html">“advise”</a> the current entity with some new behaviour. In our case of interest, we will
<i>advise functions to check their arguments before executing their bodies</i>.
</p>

<p>
Below is my attempt: <a id="org84d2c90"><code>declare-type</code></a>. Before you get scared or think it's horrendous, be charitable and
note that about a third of the following is documentation and a third is local declarations.
</p>
<div>
<pre>(<span>cl-defmacro</span> <span>declare-type</span> (f key-types <span>&amp;rest</span> types)
  <span>"Attach the given list of types to the function ‘f’</span>
<span>   by advising the function to check its arguments’ types</span>
<span>   are equal to the list of given types.</span>

<span>   We name the advice ‘⟪f⟫-typing-advice’ so that further</span>
<span>   invocations to this macro overwrite the same advice function</span>
<span>   rather than introducing additional, unintended, constraints.</span>

<span>   Using type specifiers we accommodate for unions of types</span>
<span>   and subtypes, etc ♥‿♥.</span>

<span>   ‘</span><span>key-types</span><span>’ should be of the shape (:x₀ t₀ ⋯ :xₙ tₙ);</span>
<span>    when there are no optional types, use symbol “:”.</span>

<span>    E.g., (declare-type my-func (:z string :w integer) integer symbol string)</span>
<span>  "</span>

  <span>;; </span><span>Basic coherency checks. When there aren't optional types, key-types is the “:” symbol.</span>
  (should (<span>and</span> (<span>listp</span> types) (<span>or</span> (<span>listp</span> key-types) (<span>symbolp</span> key-types))))

  (<span>letf*</span> ((pairify (<span>lambda</span> (xs) (<span>loop</span> for i in xs by #'<span>cddr</span>         <span>;; </span><span>Turn a list of flattenned pairs</span>
                                      for j in (<span>cdr</span> xs) by #'<span>cddr</span>   <span>;; </span><span>into a list of explicit pairs.</span>
                                      collect (<span>cons</span> i j))))         <span>;; </span><span>MA: No Lisp method for this!?</span>
         (result-type  (<span>car</span> (<span>-take-last</span> 1 types)))
         (types        (<span>-drop-last</span> 1 types))
         (num-of-types (<span>length</span> types))
         (key-types-og (<span>unless</span> (<span>symbolp</span> key-types) key-types))
         (key-types    (<span>funcall</span> pairify key-types-og))
         (advice-name  (<span>intern</span> (<span>format</span> <span>"%s-typing-advice"</span> f)))
         (notify-user  (<span>format</span> <span>"%s now typed %s → %s → %s."</span>
                               `,f key-types-og types result-type)))

      `(<span>progn</span>
         (<span>defun</span> ,advice-name (orig-fun <span>&amp;rest</span> args)

           <span>;; </span><span>Split into positional and key args; optionals not yet considered.</span>
           (<span>letf*</span> ((all-args
                     (<span>-split-at</span>
                       (<span>or</span> (<span>--find-index</span> (<span>not</span> (<span>s-blank?</span> (<span>s-shared-start</span> <span>":"</span> (<span>format</span> <span>"%s"</span> it)))) args) ,num-of-types)
                        args)) <span>;; </span><span>The “or” is for when there are no keywords provided.</span>
                  (pos-args  (<span>car</span> all-args))
                  (key-args  (<span>funcall</span> ,pairify (<span>cadr</span> all-args)))
                  (fun-result nil)
                  ((<span>symbol-function</span> 'shucks)
                     (<span>lambda</span> (eτ e g)
                       (<span>unless</span> (<span>typep</span> g eτ)
                         (<span>error</span> <span>"%s: Type mismatch! Expected %s %s ≠ Given %s %s."</span>
                                (<span>function</span> ,f) eτ e (<span>type-of</span> g) (<span>prin1-to-string</span> g))))))

         <span>;; </span><span>Check the types of positional arguments.</span>
         (<span>unless</span> (<span>equal</span> ,num-of-types (<span>length</span> pos-args))
           (<span>error</span> <span>"%s: Insufficient number of arguments; given %s, %s, but %s are needed."</span>
                  (<span>function</span> ,f) (<span>length</span> pos-args) pos-args ,num-of-types))
         (<span>loop</span> for (ar ty pos) in (<span>-zip</span> pos-args (<span>quote</span> ,types) (<span>number-sequence</span> 0 ,num-of-types))
               <span>do</span> (shucks ty (<span>format</span> <span>"for argument %s"</span> pos) ar))

         <span>;; </span><span>Check the types of *present* keys.</span>
         (<span>loop</span> for (k . v) in key-args
               <span>do</span> (shucks (<span>cdr</span> (<span>assoc</span> k (<span>quote</span> ,key-types))) k v))

         <span>;; </span><span>Actually execute the orginal function on the provided arguments.</span>
         (<span>setq</span> fun-result (<span>apply</span> orig-fun args))
         (shucks (<span>quote</span> ,result-type) <span>"for the result type (!)"</span> fun-result)

         <span>;; </span><span>Return-value should be given to caller.</span>
         fun-result))

      <span>;; </span><span>Register the typing advice and notify user of what was added.</span>
      (<span>advice-add</span> (<span>function</span> ,f) <span>:around</span> (<span>function</span> ,advice-name))
      ,notify-user )))
</pre>
</div>

<pre>declare-type
</pre>


<p>
There are some notable shortcomings: Lack of support for type variables and, for now, no support for
optional arguments. Nonetheless, I like it —of course.
( Using <a href="https://www.gnu.org/software/emacs/manual/html_node/elisp/Watching-Variables.html#Watching-Variables">variable watchers</a> we could likely add support for type variables as well as
function-types. )
</p>

<p>
<b>We accidentally forgot to consider an argument.</b>
</p>
<div>
<pre>(declare-type f₁ (<span>:z</span> <span>string</span> <span>:w</span> <span>list</span>) integer symbol <span>string</span>)
<span>;; </span><span>⇒ f₁ now typed (:z string :w integer) → (integer symbol) → string.</span>

(<span>cl-defun</span> <span>f₁</span> (x y <span>&amp;key</span> z w) (<span>format</span> <span>"%s"</span> x))
<span>;; </span><span>⇒ f₁ now defined</span>

(f₁ 'x) <span>;; </span><span>⇒ f₁: Insufficient number of arguments; given 2, (x), but 3 are needed.</span>
</pre>
</div>
<p>
The type declaration said we needed 3 arguments, but we did not consider one of them.
</p>

<p>
<b>We accidentally returned the wrong value.</b>
</p>
<div>
<pre>(declare-type f₂ (<span>:z</span> <span>string</span> <span>:w</span> <span>list</span>) integer symbol <span>string</span>)
(<span>cl-defun</span> <span>f₂</span> (x y <span>&amp;key</span> z w) x)

(f₂ 144 'two)
<span>;; </span><span>⇒ f₂: Type mismatch! Expected string for the result type (!) ≠ Given integer 144.</span>
</pre>
</div>

<p>
<b>We accidentally forgot to supply an argument.</b>
</p>
<div>
<pre>(declare-type f₃ (<span>:z</span> <span>string</span> <span>:w</span> <span>list</span>) integer symbol <span>string</span>)
(<span>cl-defun</span> <span>f₃</span> (x y <span>&amp;key</span> z w) (<span>format</span> <span>"%s"</span> x))

(f₃ 144)
<span>;; </span><span>⇒ f₃: Insufficient number of arguments; given 1, (144), but 2 are needed.</span>
</pre>
</div>

<p>
<b>A positional argument is supplied of the wrong type.</b>
</p>
<div>
<pre>(f₃ 'one <span>"two"</span>)
<span>;; </span><span>⇒  f₃: Type mismatch! Expected integer for argument 0 ≠ Given symbol one.</span>

(f₃ 144 <span>"two"</span>)
<span>;; </span><span>⇒ f₃: Type mismatch! Expected symbol for argument 1 ≠ Given string "two".</span>
</pre>
</div>
<p>
Notice: When multiple positional arguments have type-errors, the errors are reported one at a time.
</p>

<p>
<b>A keyword argument is supplied of the wrong type.</b>
</p>
<div>
<pre>(f₃ 1 'two <span>:z</span> 'no₀ <span>:w</span> 'no₁)
<span>;; </span><span>⇒ f₃: Type mismatch! Expected string :z ≠ Given symbol no₀.</span>

(f₃ 1 'two <span>:z</span> <span>"ok"</span> <span>:w</span> 'no₁)
<span>;; </span><span>⇒ f₃: Type mismatch! Expected string :w ≠ Given symbol no₁.</span>

(f₃ 1 'two <span>:z</span> <span>"ok"</span> <span>:w</span> 23)
<span>;; </span><span>⇒ f₃: Type mismatch! Expected string :w ≠ Given integer 23.</span>

(f₃ 1 'two <span>:z</span> <span>"ok"</span> <span>:w</span> '(a b 1 2)) <span>;; </span><span>⇒ okay; no type-error.</span>
</pre>
</div>

<p>
<b>We have no optional arguments.</b>
</p>
<div>
<pre>(declare-type f₄ : integer symbol <span>string</span>)
(<span>cl-defun</span> <span>f₄</span> (x y <span>&amp;key</span> z w) (<span>format</span> <span>"%s"</span> x))

(f₄ 144 'two <span>:z</span> <span>"bye"</span>)
<span>;; </span><span>⇒  f₄: Type mismatch! Expected nil :z ≠ Given string "bye".</span>
<span>;; </span><span>( We shouldn't have any keyword :z according to the type declaration! )</span>

(f₄ 144 'two) <span>;; </span><span>⇒ "144"</span>
</pre>
</div>

<p>
<b>We can incorporate type specfiers such as unions!</b>
</p>
<div>
<pre>(declare-type f₅ : (<span>or</span> integer <span>string</span>) <span>string</span>)
(<span>cl-defun</span> <span>f₅</span> (x) (<span>format</span> <span>"%s"</span> x))

(f₅ 144)     <span>;; </span><span>⇒ "144"</span>
(f₅ <span>"neato"</span>) <span>;; </span><span>⇒ "neato"</span>

(f₅ 'shaka-when-the-walls-fell)
<span>;; </span><span>⇒ f₅: Type mismatch! Expected (or integer string) for argument 0</span>
<span>;;       </span><span>≠ Given symbol shaka-when-the-walls-fell.</span>
</pre>
</div>

<p>
<b>No positional arguments but a complex optional argument!</b>
</p>
<div>
<pre>(declare-type f₆ (<span>:z</span> (satisfies (<span>lambda</span> (it) (<span>and</span> (<span>integerp</span> it) (<span>=</span> 0 (<span>mod</span> it 5))))))
                 character)
(<span>cl-defun</span> <span>f₆</span> (<span>&amp;key</span> z) ?A)

(f₆ 'hi)     <span>;; </span><span>⇒  Keyword argument 144 not one of (:z)</span>
(f₆)         <span>;; </span><span>⇒ 65; i.e., the character ‘A’</span>
(f₆ <span>:z</span> 6)
<span>;; </span><span>⇒  f₆: Type mismatch!</span>
<span>;;    </span><span>Expected (satisfies (lambda (it) (and (integerp it) (= 0 (mod it 5))))) :z</span>
<span>;;    </span><span>≠ Given integer 6.</span>

(f₆ <span>:z</span> 10) <span>;; </span><span>⇒ 65; i.e., the expected output since 10 mod 5 ≈ 0 &amp; so 10 is valid input.</span>
</pre>
</div>

<p>
<b>Preconditions!</b> The previous example had a complex type on a keyword, but that was
essentially a pre-condition; we can do the same on positional arguments.
</p>
<div>
<pre>(declare-type f₇ : (satisfies (<span>lambda</span> (it) (<span>=</span> it 5)))
                   integer)
(<span>cl-defun</span> <span>f₇</span> (n) n)
<span>;; </span><span>The identity on 5 function; and undefined otherwise.</span>

(f₇ 4)
<span>;; </span><span>⇒ f₇: Type mismatch! Expected (satisfies (lambda (it) (= it 5))) for argument 0</span>
<span>;;       </span><span>≠ Given integer 4.</span>

(f₇ 5) <span>;; </span><span>⇒ 5</span>
</pre>
</div>

<p>
<b>Postconditions!</b>
Given an integer greater than 5, we present an integer greater than 2; i.e.,
this is a constructive proof that \(∀ n • n &gt; 5 ⇒ n &gt; 2\).
</p>
<div>
<pre>(declare-type f₈ : (satisfies (<span>lambda</span> (in)  (<span>&gt;</span> in 5)))
                   (satisfies (<span>lambda</span> (out) (<span>&gt;</span> out 2))))
(<span>cl-defun</span> <span>f₈</span> (n) n)
<span>;; </span><span>The identity on 5 function; and undefined otherwise.</span>

(f₈ 4)
<span>;; </span><span>⇒  f₈: Type mismatch! Expected (satisfies (lambda (in) (&gt; in 5))) for argument 0</span>
<span>;;        </span><span>≠ Given integer 4.</span>

(f₈ 72) <span>;; </span><span>⇒ 72; since indeed 72 &gt; 5 for the input, and clearly 72 &gt; 2 for the output.</span>
</pre>
</div>

<p>
As it currently stands we cannot make any explicit references between the inputs
and the output, but that's an easy fix: Simply add a local function <code>old</code> to the
<a href="#org84d2c90"><code>declare-type</code></a> macro which is intentionally exposed so that it can be used in the
type declarations to refer to the ‘old’, or initial, values provided to the function.
Additionally, one could also add keyword arguments <code>:requires</code> and <code>:ensures</code>
for a more sophisticated pre- and post-condition framework.
<a href="https://github.com/sellout/quid-pro-quo">Something</a> along these lines is implemented for Common Lisp.
</p>

<p>
Here's a fun exercise: Recast the <a href="https://liquid.kosmikus.org/01-intro.html#/what-is-liquid-haskell">Liquid Haskell</a> examples in Lisp using this
<a href="#org84d2c90"><code>declare-type</code></a> form.
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dummy's Guide to Modern LLM Sampling (194 pts)]]></title>
            <link>https://rentry.co/samplers</link>
            <guid>43887637</guid>
            <pubDate>Sun, 04 May 2025 16:26:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rentry.co/samplers">https://rentry.co/samplers</a>, See on <a href="https://news.ycombinator.com/item?id=43887637">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    
                    <article>
                        
                        <div><h2 id="dummys-guide-to-modern-llm-sampling">Dummy's Guide to Modern LLM Sampling<a href="#dummys-guide-to-modern-llm-sampling" title="Permanent link"> </a></h2>
<div>
<ol>
<li><a href="#intro-knowledge">Intro Knowledge</a><ol>
<li><a href="#short-glossary">Short Glossary</a></li>
<li><a href="#why-tokens">Why tokens?</a><ol>
<li><a href="#why-not-letters">Why not letters?</a></li>
<li><a href="#why-not-whole-words">Why not whole words?</a></li>
<li><a href="#how-are-the-sub-words-chosen">How are the sub-words chosen?</a></li>
</ol>
</li>
<li><a href="#how-does-the-model-generate-text">How does the model generate text?</a></li>
<li><a href="#from-tokens-to-text-how-llms-generate-content">From Tokens to Text: How LLMs Generate Content</a></li>
</ol>
</li>
<li><a href="#sampling">Sampling</a><ol>
<li><a href="#notes-on-algorithm-presentations">Notes on Algorithm Presentations</a><ol>
<li><a href="#notation-guide">Notation Guide</a></li>
<li><a href="#implementation-considerations">Implementation Considerations</a></li>
</ol>
</li>
<li><a href="#temperature">Temperature</a></li>
<li><a href="#presence-penalty">Presence Penalty</a></li>
<li><a href="#frequency-penalty">Frequency Penalty</a></li>
<li><a href="#repetition-penalty">Repetition Penalty</a></li>
<li><a href="#dry-dont-repeat-yourself">DRY (Don't Repeat Yourself)</a></li>
<li><a href="#top-k">Top-K</a></li>
<li><a href="#top-p">Top-P</a></li>
<li><a href="#min-p">Min-P</a></li>
<li><a href="#top-a">Top-A</a></li>
<li><a href="#xtc-exclude-top-choices">XTC (eXclude Top Choices)</a></li>
<li><a href="#top-n-sigma">Top-N-Sigma</a></li>
<li><a href="#tail-free-sampling">Tail-Free Sampling</a></li>
<li><a href="#eta-cutoff">Eta Cutoff</a></li>
<li><a href="#epsilon-cutoff">Epsilon Cutoff</a></li>
<li><a href="#locally-typical-sampling">Locally Typical Sampling</a></li>
<li><a href="#quadratic-sampling">Quadratic Sampling</a></li>
<li><a href="#mirostat-sampling">Mirostat Sampling</a></li>
<li><a href="#dynamic-temperature-sampling">Dynamic Temperature Sampling</a></li>
<li><a href="#beam-search">Beam Search</a></li>
<li><a href="#contrastive-search">Contrastive Search</a></li>
</ol>
</li>
<li><a href="#sampler-order">Sampler Order</a><ol>
<li><a href="#the-typical-sampling-pipeline">The Typical Sampling Pipeline</a></li>
<li><a href="#effects-and-interactions-of-samplers-with-each-other">Effects and Interactions of Samplers with Each Other</a><ol>
<li><a href="#how-samplers-transform-distributions">How Samplers Transform Distributions</a></li>
<li><a href="#critical-order-dependent-interactions">Critical Order-Dependent Interactions</a><ol>
<li><a href="#temperature-before-vs-after-filtering">Temperature Before vs. After Filtering</a></li>
<li><a href="#penalties-before-vs-after-other-samplers">Penalties Before vs. After Other Samplers</a></li>
<li><a href="#drys-position-matters">DRY's Position Matters</a></li>
</ol>
</li>
<li><a href="#synergies-and-conflicts">Synergies and Conflicts</a><ol>
<li><a href="#synergistic-combos">Synergistic Combos</a></li>
<li><a href="#conflicting-combos">Conflicting Combos</a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</div>
<h2 id="intro-knowledge">Intro Knowledge<a href="#intro-knowledge" title="Permanent link"> </a></h2>
<p>Large Language Models (LLMs) work by taking a piece of text (e.g. user prompt) and calculating the next word. In more technical terms, <em>tokens</em>. LLMs have a vocabulary, or a dictionary, of valid tokens, and will reference those in training and inference (the process of generating text). More on that below. You need to understand why we use tokens (sub-words) instead of words or letters first. But first, a short glossary of some technical terms that aren't explained in the sections below in-depth:</p>
<h3 id="short-glossary">Short Glossary<a href="#short-glossary" title="Permanent link"> </a></h3>
<p><strong>Logits</strong>: The raw, unnormalized scores output by the model for each token in its vocabulary. Higher logits indicate tokens the model considers more likely to come next.<br>
<strong>Softmax</strong>: A mathematical function that converts logits into a proper probability distribution - values between 0 and 1 that sum to 1.<br>
<strong>Entropy</strong>: A measure of uncertainty or randomness in a probability distribution. Higher entropy means the model is less certain about which token should come next.<br>
<strong>Perplexity</strong>: Related to entropy, perplexity measures how "surprised" the model is by the text. Lower perplexity indicates higher confidence.<br>
<strong>n-gram</strong>: A contiguous sequence of n tokens. For example, "once upon a" is a 3-gram.<br>
<strong>Context window</strong> (or sequence length): The maximum number of tokens an LLM can process at once, including both the prompt and generated output.<br>
<strong>Probability distribution</strong>: A function that assigns probabilities to all possible outcomes (tokens) such that they sum to 1. Think of it like percentages: if 1% was 0.01, 50% was 0.5, and 100% was 1.0.</p>
<h3 id="why-tokens">Why tokens?<a href="#why-tokens" title="Permanent link"> </a></h3>
<p>Your first instinct might be using a vocabulary of words or letters for an LLM. But instead, we use sub-words: some common words are preserved as whole in the vocabulary (e.g. <code>the</code>, or <code>apple</code> might be a single token due to how common they are in the English language), but others are fragmented into common sub-words (e.g. <code>bi-fur-cat-ion</code> Why is this? There are several, very good reasons:</p>
<h5 id="why-not-letters">Why not letters?<a href="#why-not-letters" title="Permanent link"> </a></h5>
<p>Many reasons. To name a few: LLMs have a limited context window (amount of tokens it can process at once). With character-level tokenization, even a moderate amount of text would lead to sequence length explosion (too many tokens for too little text). The word <code>tokenization</code> would be 12 tokens instead of, for example, 2 or 3 in a sub-word system. Longer sequences also require much more computation for <a href="https://arxiv.org/abs/1706.03762" rel="" target="_blank">self-attention</a>. But more importantly, the model would need to learn higher-level patterns spanning <em>a lot</em> more positions. Understanding that <code>t-h-e</code> represents a single concept requires connecting information across three positions instead of one. This may also lead to meaningful relationships becoming more distant. Related concepts might be dozens or hundreds of positions apart.</p>
<h5 id="why-not-whole-words">Why not whole words?<a href="#why-not-whole-words" title="Permanent link"> </a></h5>
<p>A pure world-level tokenization would need us to create a vocabulary spanning the entire possible word list in the English language, and if we're doing multiple languages, then many times that. This would make our embedding matrix unreasonably large and expensive. It would also struggle with <strong>new</strong> or <strong>rare</strong> words. When a model encounters a word not in its vocabulary, it typically replaces it with an "unknown" token, losing virtually all semantic information. Sub-word tokenization would represent new words by combining existing subword tokens. For example, if we invent a new word called "grompuficious", a sub-word tokenizer might represent it as <code>g-romp-u-ficious</code>, depending on the tokenizer. <br>
Another thing to mention is morphological awareness: many languages create words by combining morphemes (prefixes, roots, suffixes). E.g., as demonstrated earlier, <code>unhappiness</code> can be broken into <code>un-happi-ness</code>; sub-word tokenization can naturally capture these relationships. It also allows us to perform cross-lingual transfer. For languages with complex morphology or compounding (e.g. German or Finnish where words can be extremely long combinations). </p>
<h4 id="how-are-the-sub-words-chosen">How are the sub-words chosen?<a href="#how-are-the-sub-words-chosen" title="Permanent link"> </a></h4>
<p>If a language model uses a new tokenizer, the development team may decide to take a representative sample of their training data, and <em>train</em> a tokenizer to find the most commond sub-words in the dataset. They will set a vocabulary size beforehand, and then the tokenizer will try and find enough sub-words to fill up the list.</p>
<h3 id="how-does-the-model-generate-text">How does the model generate text?<a href="#how-does-the-model-generate-text" title="Permanent link"> </a></h3>
<p>During training, the model sees many terabytes worth of text and builds an internal probability map for tokens. For example, after it's seen the tokens for <code>How are</code> are usually followed by the tokens <code>you?</code>, it will learn that to be the most probable next set of tokens. Once this map has been built internally to a satisfactory degree, the training is stopped and a checkpoint is released to the public (or kept private and served from an API, e.g. OpenAI). During inference, the user will provide the LLM with a text, and the LLM, based on the probabilities it's learned through training, will decide what token comes next. <strong>However</strong>, it will not decide just one token: it will take into consideration <em>every possible token</em> that exists in its vocabulary, assigns a probability score to each, and (depending on your sampler) will only output the most probable token, i.e. the one with the highest score. This would make for a rather boring output (unless you need determinism), so this is where Sampling comes in.</p>
<h3 id="from-tokens-to-text-how-llms-generate-content">From Tokens to Text: How LLMs Generate Content<a href="#from-tokens-to-text-how-llms-generate-content" title="Permanent link"> </a></h3>
<p>Now that we understand how LLMs break down and represent text using tokens, let's explore how they actually generate content. The process of text generation in LLMs involve two key steps:</p>
<ol>
<li><strong>Prediction</strong>: For each position, the model calculates the probability distribution over all possible next tokens in its vocabulary.</li>
<li><strong>Selection</strong>: The model must choose one token from this distribution to add to the growing text.</li>
</ol>
<p>The first step is fixed - determined by the model's parameters after training. However, the second step - token selection - is where  <strong>sampling</strong> occurs. While we could simply always choose the most likely token (known as "greedy" sampling), this tends to produce repetitive, deterministic text. Sampling introduces controlled randomness to make outputs more varied.</p>
<h2 id="sampling">Sampling<a href="#sampling" title="Permanent link"> </a></h2>
<p>As explained above, LLMs will pick the most probable token to generate. Sampling is the practice of introducing <em>controlled randomness</em>. With pure "greedy" sampling, it would pick the #1 option every time, but that's boring! We use sampling methods like temperature, penalties, or truncation to allow for a bit of creative variation. This document will go through every popular sampling method, and explains how all of them word; both from a simple-to-understand and technical perspectives.</p>
<h3 id="notes-on-algorithm-presentations">Notes on Algorithm Presentations<a href="#notes-on-algorithm-presentations" title="Permanent link"> </a></h3>
<p>Throughout this document, algorithms are presented in pseudo-code format that combines mathematical notation with programming concepts. Here are some guidelines to help interpret these:</p>
<h5 id="notation-guide">Notation Guide<a href="#notation-guide" title="Permanent link"> </a></h5>
<ul>
<li><strong>L</strong>: the logits tensor (raw scores output by the model)</li>
<li><strong>P</strong>: probabilities (after applying softmax to logits)</li>
<li><strong>←</strong>: assignment operation (equal to <code>=</code> in programming)</li>
<li><strong>∑</strong>: summation</li>
<li><strong>|x|</strong>: either absolute value or length/size of <code>x</code>, depending on context</li>
<li><strong>x[i]</strong>: accessing the <code>i</code>-th element of <code>x</code></li>
<li><strong>v</strong>: logical OR operation</li>
<li><strong>¬</strong>: logical NOT operation</li>
<li><strong>∞</strong>: infinity (often used to mask out tokens by setting logits to negative infinity)</li>
<li><strong>argmax(x)</strong>: returns the index of the maximum value in <code>x</code></li>
<li><strong>∈</strong>: "element of" (e.g., <code>x ∈ X</code> means <code>x</code> is an element of set <code>X</code>)</li>
</ul>
<h5 id="implementation-considerations">Implementation Considerations<a href="#implementation-considerations" title="Permanent link"> </a></h5>
<p>The algorithms provided are written for clarity rather than optimization. Production implementations would typically:</p>
<ol>
<li>Vectorize operations where possible for efficiency</li>
<li>handle edge cases and numerical stability issues (though parts that need this have occasionally been highlighted in the algorithms below)</li>
<li>Incorporate batch processing for multiple sequences, if necessary for the framework</li>
<li>Cache intermediate results where beneficial</li>
</ol>
<h3 id="temperature">Temperature<a href="#temperature" title="Permanent link"> </a></h3>
<p>Think of this as the "creativity knob" on your LLM. At low temperatures (close to 0), the model becomes very cautious and predictable - it almost always picks the most likely next word. It's like ordering the same dish at your favourite restaurant every time because you know you'll like it (or maybe you don't know any better). At higher temperatures (like 0.7-1.0), the model gets very creative and willing to take chances. It may choose the 3rd or 4th most likely word instead of always the top choice. This makes text more varied and interesting, but also increases the chance of errors. Very high temperatures (above 1.0) make the model wild and unpredictable, unless you use it in conjunction with other sampling methods (e.g. min-p) to reign it in.</p>
<p><strong>Technical</strong>:<br>
Temperature works by directly manipulating the probability distribution over the vocabulary. The model produces logits (unnormalized scores) for each token in the vocabulary, which are then divided by the temperature value. When temperature is less than 1, this makes high logits relatively higher and low logits relatively lower, giving us a more peaked distribution where the highest-scoring tokens become even more likely. When temperature exceeds 1, this flattens the distribution, making the probability gap between high and low scoring tokens smaller, thus increasing randomness. After applying temperature, the modified logits are converted to a probability distribution (using softmax) and a token is randomly sampled from this distribution. The mathematical effect is that Temperature <code>T</code> transformers probabilities by essentially raising each probability to the power of <code>1/T</code> before renormalizing.</p>
<p><strong>Algorithm</strong><br>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-1-1"><a id="L-1-1" name="L-1-1"></a><span>Algorithm</span> <span>1</span> <span>Temperature</span> <span>Sampling</span>
</span><span id="L-1-2"><a id="L-1-2" name="L-1-2"></a><span>Required</span>: <span>Logits</span> <span>tensor</span> <span>L</span>, <span>temperature</span> <span>parameter</span> <span>T</span>
</span><span id="L-1-3"><a id="L-1-3" name="L-1-3"></a><span>Output</span>: <span>Modified</span> <span>logits</span> <span>with</span> <span>adjusted</span> <span>probability</span> <span>distribution</span>
</span><span id="L-1-4"><a id="L-1-4" name="L-1-4"></a><span>1</span>: <span>T</span> ← <span>max</span><span>(</span><span>T</span>, ε<span>)</span>  <span>//</span> <span>Prevent</span> <span>numerical</span> <span>issues</span> <span>with</span> <span>extremely</span> <span>low</span> <span>temperatures</span>
</span><span id="L-1-5"><a id="L-1-5" name="L-1-5"></a><span>2</span>: <span>if</span> <span>T</span> <span>&lt;</span> <span>0</span>.<span>1</span> <span>then</span>
</span><span id="L-1-6"><a id="L-1-6" name="L-1-6"></a><span>3</span>:     <span>L</span> ← <span>L</span> <span>-</span> <span>max</span><span>(</span><span>L</span><span>)</span> <span>+</span> <span>1</span>  <span>//</span> <span>Shift</span> <span>range</span> <span>to</span> [<span>-</span><span>inf</span>, <span>1</span>] <span>for</span> <span>numerical</span> <span>stability</span>
</span><span id="L-1-7"><a id="L-1-7" name="L-1-7"></a><span>4</span>: <span>end</span> <span>if</span>
</span><span id="L-1-8"><a id="L-1-8" name="L-1-8"></a><span>5</span>: <span>L</span> ← <span>L</span> <span>/</span> <span>T</span>  <span>//</span> <span>Apply</span> <span>temperature</span> <span>scaling</span>
</span><span id="L-1-9"><a id="L-1-9" name="L-1-9"></a><span>6</span>: <span>return</span> <span>L</span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="presence-penalty">Presence Penalty<a href="#presence-penalty" title="Permanent link"> </a></h3>
<p>This discourages the model from repeating any token that has appeared before, regardless of how many times it's been used. Think of it like a party host who wants to make sure everyone gets a turn to speak. If Tim has already spoken once, he gets slightly discouraged from speaking again, whether he spoke once or ten times before. This is generally not recommend, since better penalty strategies exist (see: DRY).</p>
<p><strong>Technical</strong>:<br>
Presence Penalty works by applying a fixed penalty to any token that has appeared in the generated text. We first identify which tokens have been used before using the output mask (which is True for tokens that appear <em>at least once</em>). It then subtracts the presence penalty value from the logits to those tokens. This makes previously used tokens less likely to be selected again, regardless of how frequently they've appeared. The penalty is applied uniformly to any token that has been used at least once.</p>
<p><strong>Algorithm</strong><br>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-2-1"><a id="L-2-1" name="L-2-1"></a><span>Algorithm</span> <span>2</span> <span>Presence</span> <span>Penalty</span>
</span><span id="L-2-2"><a id="L-2-2" name="L-2-2"></a><span>Required</span><span>:</span> <span>Logits</span> <span>tensor</span> <span>L</span><span>,</span> <span>output</span> <span>tokens</span> <span>O</span><span>,</span> <span>penalty</span> <span>weight</span> <span>λp</span>
</span><span id="L-2-3"><a id="L-2-3" name="L-2-3"></a><span>Output</span><span>:</span> <span>Modified</span> <span>logits</span> <span>with</span> <span>penalty</span> <span>applied</span> <span>for</span> <span>token</span> <span>presence</span>
</span><span id="L-2-4"><a id="L-2-4" name="L-2-4"></a><span>1</span><span>:</span> <span>Vsize</span> <span>←</span> <span>|</span><span>L</span><span>[</span><span>0</span><span>]</span><span>|</span>  <span>//</span> <span>Vocabulary</span> <span>size</span> <span>from</span> <span>logits</span> <span>dimension</span>
</span><span id="L-2-5"><a id="L-2-5" name="L-2-5"></a><span>2</span><span>:</span> <span>Moutput</span> <span>←</span> <span>BinaryMask</span><span>(</span><span>O</span><span>,</span> <span>Vsize</span><span>)</span>  <span>//</span> <span>Create</span> <span>binary</span> <span>mask</span> <span>where</span> <span>token</span> <span>has</span> <span>appeared</span> <span>at</span> <span>least</span> <span>once</span>
</span><span id="L-2-6"><a id="L-2-6" name="L-2-6"></a><span>3</span><span>:</span> <span>P</span> <span>←</span> <span>λp</span> <span>·</span> <span>Moutput</span>  <span>//</span> <span>Calculate</span> <span>penalty</span> <span>matrix</span>
</span><span id="L-2-7"><a id="L-2-7" name="L-2-7"></a><span>4</span><span>:</span> <span>L</span> <span>←</span> <span>L</span> <span>-</span> <span>P</span>  <span>//</span> <span>Apply</span> <span>presence</span> <span>penalty</span> <span>to</span> <span>logits</span>
</span><span id="L-2-8"><a id="L-2-8" name="L-2-8"></a><span>5</span><span>:</span> <span>return</span> <span>L</span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="frequency-penalty">Frequency Penalty<a href="#frequency-penalty" title="Permanent link"> </a></h3>
<p>Discourages tokens based on how many times they've already been used. This is simply Presence Penalty but with the number of occurrences being taken into account. The more frequently a word has appeared, the less likely it will appear again.</p>
<p><strong>Technical</strong>:<br>
The frequency penalty multiplies the count of each token's previous occurrences by the penalty value and subtracts this from that token's logit score. We track how many times each token has appeared in the generated output, and if it's appeared three times, its logit is reduced <code>3 x (frequency penalty)</code>. This creates a progressive penalty that increases with each repeated use of a token.</p>
<p><strong>Algorithm</strong><br>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-3-1"><a id="L-3-1" name="L-3-1"></a><span>Algorithm</span> <span>3</span> <span>Frequency</span> <span>Penalty</span>
</span><span id="L-3-2"><a id="L-3-2" name="L-3-2"></a><span>Required</span>: <span>Logits</span> <span>tensor</span> <span>L</span>, <span>output</span> <span>tokens</span> <span>O</span>, <span>penalty</span> <span>weight</span> λ<span>f</span>
</span><span id="L-3-3"><a id="L-3-3" name="L-3-3"></a><span>Output</span>: <span>Modified</span> <span>logits</span> <span>with</span> <span>penalty</span> <span>applied</span> <span>proportional</span> <span>to</span> <span>token</span> <span>frequency</span>
</span><span id="L-3-4"><a id="L-3-4" name="L-3-4"></a><span>1</span>: <span>Vsize</span> ← <span>|</span><span>L</span>[<span>0</span>]<span>|</span>  <span>//</span> <span>Vocabulary</span> <span>size</span> <span>from</span> <span>logits</span> <span>dimension</span>
</span><span id="L-3-5"><a id="L-3-5" name="L-3-5"></a><span>2</span>: <span>Coutput</span> ← <span>TokenCounts</span><span>(</span><span>O</span>, <span>Vsize</span><span>)</span>  <span>//</span> <span>Count</span> <span>occurrences</span> <span>of</span> <span>each</span> <span>token</span> <span>in</span> <span>output</span>
</span><span id="L-3-6"><a id="L-3-6" name="L-3-6"></a><span>3</span>: <span>P</span> ← λ<span>f</span> · <span>Coutput</span>  <span>//</span> <span>Calculate</span> <span>penalty</span> <span>matrix</span> <span>proportional</span> <span>to</span> <span>counts</span>
</span><span id="L-3-7"><a id="L-3-7" name="L-3-7"></a><span>4</span>: <span>L</span> ← <span>L</span> <span>-</span> <span>P</span>  <span>//</span> <span>Apply</span> <span>frequency</span> <span>penalty</span> <span>to</span> <span>logits</span>
</span><span id="L-3-8"><a id="L-3-8" name="L-3-8"></a><span>5</span>: <span>return</span> <span>L</span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="repetition-penalty">Repetition Penalty<a href="#repetition-penalty" title="Permanent link"> </a></h3>
<p>The repetition penalty works a bit differently from the other two: it penalizes both tokens from the prompt and generated output, affecting positive and negative logits differently. For positive scores, it divides the score by the penalty (making it smaller); for negative scores, it multiplies by the penalty (making it more negative). Useful for breaking out of loops, with the cost of coherency at more aggressive values.</p>
<p><strong>Technical</strong>:<br>
The penalty is applied to tokens that have appeared in either prompt or the generated text so far. We create a mask combining both prompt and output masks. for tokens in this combined mask, we apply the penalty by either dividing or multiplying the logits depending on whether they're positive or negative. This approach avoids over-penalizing already unlikely tokens while effectively reducing the probability of tokens with higher scores. The most distinctive feature is its different treatment of positive and negative logits, which helps maintain a more balanced probability distribution.</p>
<p><strong>Algorithm</strong><br>
</p><div>

<table><tbody><tr><td><div><pre><span><a href="#L-4-1"> 1</a></span>
<span><a href="#L-4-2"> 2</a></span>
<span><a href="#L-4-3"> 3</a></span>
<span><a href="#L-4-4"> 4</a></span>
<span><a href="#L-4-5"> 5</a></span>
<span><a href="#L-4-6"> 6</a></span>
<span><a href="#L-4-7"> 7</a></span>
<span><a href="#L-4-8"> 8</a></span>
<span><a href="#L-4-9"> 9</a></span>
<span><a href="#L-4-10">10</a></span>
<span><a href="#L-4-11">11</a></span>
<span><a href="#L-4-12">12</a></span>
<span><a href="#L-4-13">13</a></span>
<span><a href="#L-4-14">14</a></span>
<span><a href="#L-4-15">15</a></span>
<span><a href="#L-4-16">16</a></span>
<span><a href="#L-4-17">17</a></span></pre></div></td><td><div><pre><span></span><span id="L-4-1"><a id="L-4-1" name="L-4-1"></a><span>Algorithm</span> <span>4</span> <span>Repetition</span> <span>Penalty</span>
</span><span id="L-4-2"><a id="L-4-2" name="L-4-2"></a><span>Required</span>: <span>Logits</span> <span>tensor</span> <span>L</span>, <span>prompt</span> <span>tokens</span> <span>P</span>, <span>output</span> <span>tokens</span> <span>O</span>, <span>penalty</span> <span>factor</span> λ<span>r</span>
</span><span id="L-4-3"><a id="L-4-3" name="L-4-3"></a><span>Output</span>: <span>Modified</span> <span>logits</span> <span>with</span> <span>asymmetric</span> <span>penalty</span> <span>for</span> <span>repeated</span> <span>tokens</span>
</span><span id="L-4-4"><a id="L-4-4" name="L-4-4"></a><span>1</span>: <span>Vsize</span> ← <span>|</span><span>L</span>[<span>0</span>]<span>|</span>  <span>//</span> <span>Vocabulary</span> <span>size</span> <span>from</span> <span>logits</span> <span>dimension</span>
</span><span id="L-4-5"><a id="L-4-5" name="L-4-5"></a><span>2</span>: <span>Mprompt</span> ← <span>BinaryMask</span><span>(</span><span>P</span>, <span>Vsize</span><span>)</span>  <span>//</span> <span>Create</span> <span>binary</span> <span>mask</span> <span>for</span> <span>tokens</span> <span>in</span> <span>prompt</span>
</span><span id="L-4-6"><a id="L-4-6" name="L-4-6"></a><span>3</span>: <span>Moutput</span> ← <span>BinaryMask</span><span>(</span><span>O</span>, <span>Vsize</span><span>)</span>  <span>//</span> <span>Create</span> <span>binary</span> <span>mask</span> <span>for</span> <span>tokens</span> <span>in</span> <span>output</span>
</span><span id="L-4-7"><a id="L-4-7" name="L-4-7"></a><span>4</span>: <span>M</span> ← <span>Mprompt</span> ∨ <span>Moutput</span>  <span>//</span> <span>Combined</span> <span>mask</span> <span>for</span> <span>tokens</span> <span>in</span> <span>either</span> <span>prompt</span> <span>or</span> <span>output</span>
</span><span id="L-4-8"><a id="L-4-8" name="L-4-8"></a><span>5</span>: <span>R</span> ← <span>matrix</span> <span>of</span> <span>size</span> <span>|</span><span>L</span><span>|</span> × <span>Vsize</span> <span>filled</span> <span>with</span> λ<span>r</span>
</span><span id="L-4-9"><a id="L-4-9" name="L-4-9"></a><span>6</span>: <span>R</span>[¬<span>M</span>] ← <span>1</span>.<span>0</span>  <span>//</span> <span>No</span> <span>penalty</span> <span>for</span> <span>tokens</span> <span>not</span> <span>previously</span> <span>seen</span>
</span><span id="L-4-10"><a id="L-4-10" name="L-4-10"></a><span>7</span>: <span>for</span> <span>each</span> <span>position</span> <span>(</span><span>i</span>,<span>j</span><span>)</span> <span>in</span> <span>L</span> <span>do</span>
</span><span id="L-4-11"><a id="L-4-11" name="L-4-11"></a><span>8</span>:     <span>if</span> <span>L</span>[<span>i</span>,<span>j</span>] <span>&gt;</span> <span>0</span> <span>then</span>
</span><span id="L-4-12"><a id="L-4-12" name="L-4-12"></a><span>9</span>:         <span>L</span>[<span>i</span>,<span>j</span>] ← <span>L</span>[<span>i</span>,<span>j</span>] <span>/</span> <span>R</span>[<span>i</span>,<span>j</span>]  <span>//</span> <span>Divide</span> <span>positive</span> <span>logits</span>
</span><span id="L-4-13"><a id="L-4-13" name="L-4-13"></a><span>10</span>:     <span>else</span>
</span><span id="L-4-14"><a id="L-4-14" name="L-4-14"></a><span>11</span>:         <span>L</span>[<span>i</span>,<span>j</span>] ← <span>L</span>[<span>i</span>,<span>j</span>] <span>*</span> <span>R</span>[<span>i</span>,<span>j</span>]  <span>//</span> <span>Multiply</span> <span>negative</span> <span>logits</span>
</span><span id="L-4-15"><a id="L-4-15" name="L-4-15"></a><span>12</span>:     <span>end</span> <span>if</span>
</span><span id="L-4-16"><a id="L-4-16" name="L-4-16"></a><span>13</span>: <span>end</span> <span>for</span>
</span><span id="L-4-17"><a id="L-4-17" name="L-4-17"></a><span>14</span>: <span>return</span> <span>L</span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="dry-dont-repeat-yourself">DRY (Don't Repeat Yourself)<a href="#dry-dont-repeat-yourself" title="Permanent link"> </a></h3>
<p>DRY sampling is like having an editor who watches for repetitive patterns in your writing. Let's say you're writing a story and you've already used the phrase "once upon a time" - DRY will discourage you from using that exact same phrase again. But it's much smarter than simple word repetition prevention (like the 3 penalties above).</p>
<p>DRY looks for repeating patterns (called n-grams) in your text. If it notices you've written something like "the cat sat on the" before and you're about to repeat this pattern, it discourages the next word that would continue the repetition. The longer the repeating pattern, the stronger the discouragement. This prevents the text from falling into loops or recycling the same phrases and keeps the output fresh and varied. What makes DRY special is that it considers existing patterns. It's particularly useful for creative writing where repetitive text would sound unnatural.</p>
<p><strong>Technical</strong>:<br>
DRY sampling works by detecting n-gram repetitions and penalizing tokens that would continue these patterns. The algorithm examines the sequence of tokens generated so far and identifies repeating patterns that end with the most recently generated token. We track where the last token appears elsewhere in the text, then compares the contexts before these appearances. When it finds matching contexts (repeated n-grams), it identifies which tokens followed these patterns previously and penalizes them in the current logits distribution. Key parameters include the multiplier (strength of the penalty), base (how much stronger the penalty becomes for longer n-grams), minimum n-gram length to consider, and maximum n-gram length to check. The algorithm also respects "sequence breakers" (like punctuation) that reset pattern matching, and has range limits to only consider recent text for efficiency.</p>
<p>The penalty is applied exponentially based on the length of the matching pattern, with longer matches receiving stronger penalties. This creates a dynamic system that prevents repetition while still allowing natural text flow, making output text more diverse and human-like by avoiding the repetitive patterns that often plague language models.</p>
<p><strong>Algorithm</strong><br>
</p><div>

<table><tbody><tr><td><div><pre><span><a href="#L-5-1"> 1</a></span>
<span><a href="#L-5-2"> 2</a></span>
<span><a href="#L-5-3"> 3</a></span>
<span><a href="#L-5-4"> 4</a></span>
<span><a href="#L-5-5"> 5</a></span>
<span><a href="#L-5-6"> 6</a></span>
<span><a href="#L-5-7"> 7</a></span>
<span><a href="#L-5-8"> 8</a></span>
<span><a href="#L-5-9"> 9</a></span>
<span><a href="#L-5-10">10</a></span>
<span><a href="#L-5-11">11</a></span>
<span><a href="#L-5-12">12</a></span>
<span><a href="#L-5-13">13</a></span>
<span><a href="#L-5-14">14</a></span>
<span><a href="#L-5-15">15</a></span>
<span><a href="#L-5-16">16</a></span>
<span><a href="#L-5-17">17</a></span>
<span><a href="#L-5-18">18</a></span>
<span><a href="#L-5-19">19</a></span>
<span><a href="#L-5-20">20</a></span>
<span><a href="#L-5-21">21</a></span>
<span><a href="#L-5-22">22</a></span>
<span><a href="#L-5-23">23</a></span>
<span><a href="#L-5-24">24</a></span>
<span><a href="#L-5-25">25</a></span>
<span><a href="#L-5-26">26</a></span>
<span><a href="#L-5-27">27</a></span>
<span><a href="#L-5-28">28</a></span>
<span><a href="#L-5-29">29</a></span>
<span><a href="#L-5-30">30</a></span>
<span><a href="#L-5-31">31</a></span>
<span><a href="#L-5-32">32</a></span>
<span><a href="#L-5-33">33</a></span>
<span><a href="#L-5-34">34</a></span>
<span><a href="#L-5-35">35</a></span>
<span><a href="#L-5-36">36</a></span>
<span><a href="#L-5-37">37</a></span>
<span><a href="#L-5-38">38</a></span>
<span><a href="#L-5-39">39</a></span>
<span><a href="#L-5-40">40</a></span>
<span><a href="#L-5-41">41</a></span>
<span><a href="#L-5-42">42</a></span>
<span><a href="#L-5-43">43</a></span>
<span><a href="#L-5-44">44</a></span>
<span><a href="#L-5-45">45</a></span>
<span><a href="#L-5-46">46</a></span>
<span><a href="#L-5-47">47</a></span>
<span><a href="#L-5-48">48</a></span>
<span><a href="#L-5-49">49</a></span>
<span><a href="#L-5-50">50</a></span>
<span><a href="#L-5-51">51</a></span>
<span><a href="#L-5-52">52</a></span>
<span><a href="#L-5-53">53</a></span>
<span><a href="#L-5-54">54</a></span>
<span><a href="#L-5-55">55</a></span>
<span><a href="#L-5-56">56</a></span>
<span><a href="#L-5-57">57</a></span>
<span><a href="#L-5-58">58</a></span>
<span><a href="#L-5-59">59</a></span>
<span><a href="#L-5-60">60</a></span>
<span><a href="#L-5-61">61</a></span>
<span><a href="#L-5-62">62</a></span>
<span><a href="#L-5-63">63</a></span>
<span><a href="#L-5-64">64</a></span>
<span><a href="#L-5-65">65</a></span>
<span><a href="#L-5-66">66</a></span>
<span><a href="#L-5-67">67</a></span>
<span><a href="#L-5-68">68</a></span>
<span><a href="#L-5-69">69</a></span>
<span><a href="#L-5-70">70</a></span>
<span><a href="#L-5-71">71</a></span>
<span><a href="#L-5-72">72</a></span>
<span><a href="#L-5-73">73</a></span>
<span><a href="#L-5-74">74</a></span>
<span><a href="#L-5-75">75</a></span>
<span><a href="#L-5-76">76</a></span>
<span><a href="#L-5-77">77</a></span>
<span><a href="#L-5-78">78</a></span>
<span><a href="#L-5-79">79</a></span>
<span><a href="#L-5-80">80</a></span></pre></div></td><td><div><pre><span></span><span id="L-5-1"><a id="L-5-1" name="L-5-1"></a><span>Algorithm</span> <span>5</span> <span>DRY</span> <span>(</span><span>Don</span><span>'</span><span>t</span> <span>Repeat</span> <span>Yourself</span><span>)</span> <span>Sampling</span>
</span><span id="L-5-2"><a id="L-5-2" name="L-5-2"></a><span>Required</span><span>:</span> <span>Logits</span> <span>tensor</span> <span>L</span><span>,</span> <span>input</span> <span>tokens</span> <span>I</span><span>,</span> <span>output</span> <span>tokens</span> <span>O</span><span>,</span> <span>multiplier</span> <span>λ</span><span>,</span> <span>base</span> <span>b</span><span>,</span> 
</span><span id="L-5-3"><a id="L-5-3" name="L-5-3"></a>         <span>minimum</span> <span>n</span><span>-</span><span>gram</span> <span>length</span> <span>Nmin</span><span>,</span> <span>sequence</span> <span>breaker</span> <span>tokens</span> <span>B</span><span>,</span> <span>range</span> <span>limit</span> <span>r</span><span>,</span>
</span><span id="L-5-4"><a id="L-5-4" name="L-5-4"></a>         <span>maximum</span> <span>n</span><span>-</span><span>gram</span> <span>length</span> <span>Nmax</span><span>,</span> <span>maximum</span> <span>occurrences</span> <span>M</span><span>,</span> <span>early</span> <span>exit</span> <span>threshold</span> <span>E</span>
</span><span id="L-5-5"><a id="L-5-5" name="L-5-5"></a><span>Output</span><span>:</span> <span>Modified</span> <span>logits</span> <span>with</span> <span>penalties</span> <span>for</span> <span>repeating</span> <span>patterns</span>
</span><span id="L-5-6"><a id="L-5-6" name="L-5-6"></a>
</span><span id="L-5-7"><a id="L-5-7" name="L-5-7"></a><span>1</span><span>:</span> <span>for</span> <span>each</span> <span>sequence</span> <span>s</span> <span>where</span> <span>λ</span> <span>&gt;</span> <span>0</span> <span>do</span>
</span><span id="L-5-8"><a id="L-5-8" name="L-5-8"></a><span>2</span><span>:</span>     <span>//</span> <span>Prepare</span> <span>token</span> <span>sequence</span> <span>by</span> <span>concatenating</span> <span>prompt</span> <span>and</span> <span>generated</span> <span>tokens</span>
</span><span id="L-5-9"><a id="L-5-9" name="L-5-9"></a><span>3</span><span>:</span>     <span>p</span> <span>←</span> <span>Length</span> <span>of</span> <span>valid</span> <span>tokens</span> <span>in</span> <span>I</span><span>[</span><span>s</span><span>]</span>
</span><span id="L-5-10"><a id="L-5-10" name="L-5-10"></a><span>4</span><span>:</span>     <span>q</span> <span>←</span> <span>Length</span> <span>of</span> <span>valid</span> <span>tokens</span> <span>in</span> <span>O</span><span>[</span><span>s</span><span>]</span>
</span><span id="L-5-11"><a id="L-5-11" name="L-5-11"></a><span>5</span><span>:</span>     <span>T</span> <span>←</span> <span>Concatenate</span><span>(</span><span>I</span><span>[</span><span>s</span><span>][</span><span>1</span><span>:p</span><span>],</span> <span>O</span><span>[</span><span>s</span><span>][</span><span>1</span><span>:q</span><span>])</span>
</span><span id="L-5-12"><a id="L-5-12" name="L-5-12"></a><span>6</span><span>:</span>
</span><span id="L-5-13"><a id="L-5-13" name="L-5-13"></a><span>7</span><span>:</span>     <span>if</span> <span>r</span> <span>&gt;</span> <span>0</span> <span>then</span>
</span><span id="L-5-14"><a id="L-5-14" name="L-5-14"></a><span>8</span><span>:</span>         <span>T</span> <span>←</span> <span>T</span><span>[</span><span>-</span><span>r</span><span>:]</span>  <span>//</span> <span>Consider</span> <span>only</span> <span>the</span> <span>last</span> <span>r</span> <span>tokens</span> <span>if</span> <span>range</span> <span>limit</span> <span>is</span> <span>set</span>
</span><span id="L-5-15"><a id="L-5-15" name="L-5-15"></a><span>9</span><span>:</span>     <span>end</span> <span>if</span>
</span><span id="L-5-16"><a id="L-5-16" name="L-5-16"></a><span>10</span><span>:</span>
</span><span id="L-5-17"><a id="L-5-17" name="L-5-17"></a><span>11</span><span>:</span>     <span>if</span> <span>|</span><span>T</span><span>|</span> <span>&lt;</span> <span>2</span> <span>then</span> <span>continue</span> <span>end</span> <span>if</span>  <span>//</span> <span>Skip</span> <span>if</span> <span>sequence</span> <span>too</span> <span>short</span>
</span><span id="L-5-18"><a id="L-5-18" name="L-5-18"></a><span>12</span><span>:</span>
</span><span id="L-5-19"><a id="L-5-19" name="L-5-19"></a><span>13</span><span>:</span>     <span>last</span> <span>←</span> <span>T</span><span>[</span><span>-</span><span>1</span><span>]</span>  <span>//</span> <span>Last</span> <span>token</span> <span>in</span> <span>sequence</span>
</span><span id="L-5-20"><a id="L-5-20" name="L-5-20"></a><span>14</span><span>:</span>     <span>if</span> <span>last</span> <span>∈</span> <span>B</span> <span>then</span> <span>continue</span> <span>end</span> <span>if</span>  <span>//</span> <span>Skip</span> <span>if</span> <span>last</span> <span>token</span> <span>is</span> <span>a</span> <span>sequence</span> <span>breaker</span>
</span><span id="L-5-21"><a id="L-5-21" name="L-5-21"></a><span>15</span><span>:</span>
</span><span id="L-5-22"><a id="L-5-22" name="L-5-22"></a><span>16</span><span>:</span>     <span>//</span> <span>Create</span> <span>mask</span> <span>for</span> <span>sequence</span> <span>breaker</span> <span>positions</span>
</span><span id="L-5-23"><a id="L-5-23" name="L-5-23"></a><span>17</span><span>:</span>     <span>breakMask</span> <span>←</span> <span>ZeroVector</span><span>(|</span><span>T</span><span>|)</span>
</span><span id="L-5-24"><a id="L-5-24" name="L-5-24"></a><span>18</span><span>:</span>     <span>for</span> <span>each</span> <span>token</span> <span>id</span> <span>b</span> <span>∈</span> <span>B</span> <span>do</span>
</span><span id="L-5-25"><a id="L-5-25" name="L-5-25"></a><span>19</span><span>:</span>         <span>breakMask</span> <span>←</span> <span>breakMask</span> <span>∨</span> <span>(</span><span>T</span> <span>=</span> <span>b</span><span>)</span>
</span><span id="L-5-26"><a id="L-5-26" name="L-5-26"></a><span>20</span><span>:</span>     <span>end</span> <span>for</span>
</span><span id="L-5-27"><a id="L-5-27" name="L-5-27"></a><span>21</span><span>:</span>
</span><span id="L-5-28"><a id="L-5-28" name="L-5-28"></a><span>22</span><span>:</span>     <span>//</span> <span>Find</span> <span>maximum</span> <span>allowed</span> <span>n</span><span>-</span><span>gram</span> <span>length</span> <span>before</span> <span>hitting</span> <span>a</span> <span>sequence</span> <span>breaker</span>
</span><span id="L-5-29"><a id="L-5-29" name="L-5-29"></a><span>23</span><span>:</span>     <span>maxN</span> <span>←</span> <span>0</span>
</span><span id="L-5-30"><a id="L-5-30" name="L-5-30"></a><span>24</span><span>:</span>     <span>for</span> <span>i</span> <span>from</span> <span>1</span> <span>to</span> <span>min</span><span>(|</span><span>breakMask</span><span>|,</span> <span>Nmax</span><span>)</span> <span>do</span>
</span><span id="L-5-31"><a id="L-5-31" name="L-5-31"></a><span>25</span><span>:</span>         <span>if</span> <span>breakMask</span><span>[</span><span>-</span><span>i</span><span>]</span> <span>then</span> <span>break</span> <span>end</span> <span>if</span>
</span><span id="L-5-32"><a id="L-5-32" name="L-5-32"></a><span>26</span><span>:</span>         <span>maxN</span> <span>←</span> <span>i</span>
</span><span id="L-5-33"><a id="L-5-33" name="L-5-33"></a><span>27</span><span>:</span>     <span>end</span> <span>for</span>
</span><span id="L-5-34"><a id="L-5-34" name="L-5-34"></a><span>28</span><span>:</span>
</span><span id="L-5-35"><a id="L-5-35" name="L-5-35"></a><span>29</span><span>:</span>     <span>if</span> <span>maxN</span> <span>≤</span> <span>Nmin</span> <span>then</span> <span>continue</span> <span>end</span> <span>if</span>  <span>//</span> <span>Skip</span> <span>if</span> <span>maximum</span> <span>n</span><span>-</span><span>gram</span> <span>too</span> <span>short</span>
</span><span id="L-5-36"><a id="L-5-36" name="L-5-36"></a><span>30</span><span>:</span>
</span><span id="L-5-37"><a id="L-5-37" name="L-5-37"></a><span>31</span><span>:</span>     <span>//</span> <span>Initialize</span> <span>array</span> <span>to</span> <span>track</span> <span>longest</span> <span>matching</span> <span>n</span><span>-</span><span>gram</span> <span>for</span> <span>each</span> <span>token</span>
</span><span id="L-5-38"><a id="L-5-38" name="L-5-38"></a><span>32</span><span>:</span>     <span>ngramLengths</span> <span>←</span> <span>ZeroVector</span><span>(</span><span>VocabularySize</span><span>)</span>
</span><span id="L-5-39"><a id="L-5-39" name="L-5-39"></a><span>33</span><span>:</span>
</span><span id="L-5-40"><a id="L-5-40" name="L-5-40"></a><span>34</span><span>:</span>     <span>//</span> <span>Find</span> <span>all</span> <span>positions</span> <span>where</span> <span>the</span> <span>last</span> <span>token</span> <span>appears</span>
</span><span id="L-5-41"><a id="L-5-41" name="L-5-41"></a><span>35</span><span>:</span>     <span>endpoints</span> <span>←</span> <span>FindIndices</span><span>(</span><span>T</span> <span>=</span> <span>last</span><span>)</span>
</span><span id="L-5-42"><a id="L-5-42" name="L-5-42"></a><span>36</span><span>:</span>     <span>if</span> <span>|</span><span>endpoints</span><span>|</span> <span>&lt;</span> <span>2</span> <span>then</span> <span>continue</span> <span>end</span> <span>if</span>
</span><span id="L-5-43"><a id="L-5-43" name="L-5-43"></a><span>37</span><span>:</span>
</span><span id="L-5-44"><a id="L-5-44" name="L-5-44"></a><span>38</span><span>:</span>     <span>//</span> <span>Remove</span> <span>the</span> <span>last</span> <span>occurrence</span> <span>(</span><span>current</span> <span>position</span><span>)</span>
</span><span id="L-5-45"><a id="L-5-45" name="L-5-45"></a><span>39</span><span>:</span>     <span>endpoints</span> <span>←</span> <span>endpoints</span><span>[:-</span><span>1</span><span>]</span>
</span><span id="L-5-46"><a id="L-5-46" name="L-5-46"></a><span>40</span><span>:</span>
</span><span id="L-5-47"><a id="L-5-47" name="L-5-47"></a><span>41</span><span>:</span>     <span>//</span> <span>Limit</span> <span>number</span> <span>of</span> <span>previous</span> <span>occurrences</span> <span>to</span> <span>check</span>
</span><span id="L-5-48"><a id="L-5-48" name="L-5-48"></a><span>42</span><span>:</span>     <span>if</span> <span>|</span><span>endpoints</span><span>|</span> <span>&gt;</span> <span>M</span> <span>then</span>
</span><span id="L-5-49"><a id="L-5-49" name="L-5-49"></a><span>43</span><span>:</span>         <span>endpoints</span> <span>←</span> <span>endpoints</span><span>[</span><span>-</span><span>M</span><span>:</span><span>]</span>
</span><span id="L-5-50"><a id="L-5-50" name="L-5-50"></a><span>44</span><span>:</span>     <span>end</span> <span>if</span>
</span><span id="L-5-51"><a id="L-5-51" name="L-5-51"></a><span>45</span><span>:</span>
</span><span id="L-5-52"><a id="L-5-52" name="L-5-52"></a><span>46</span><span>:</span>     <span>//</span> <span>Check</span> <span>each</span> <span>previous</span> <span>occurrence</span> <span>of</span> <span>the</span> <span>last</span> <span>token</span> <span>for</span> <span>matching</span> <span>contexts</span>
</span><span id="L-5-53"><a id="L-5-53" name="L-5-53"></a><span>47</span><span>:</span>     <span>for</span> <span>each</span> <span>idx</span> <span>in</span> <span>Reverse</span><span>(</span><span>endpoints</span><span>)</span> <span>do</span>
</span><span id="L-5-54"><a id="L-5-54" name="L-5-54"></a><span>48</span><span>:</span>         <span>if</span> <span>idx</span> <span>=</span> <span>|</span><span>T</span><span>|</span> <span>-</span> <span>1</span> <span>then</span> <span>continue</span> <span>end</span> <span>if</span>
</span><span id="L-5-55"><a id="L-5-55" name="L-5-55"></a><span>49</span><span>:</span>
</span><span id="L-5-56"><a id="L-5-56" name="L-5-56"></a><span>50</span><span>:</span>         <span>matchLen</span> <span>←</span> <span>0</span>
</span><span id="L-5-57"><a id="L-5-57" name="L-5-57"></a><span>51</span><span>:</span>         <span>//</span> <span>Look</span> <span>backward</span> <span>to</span> <span>find</span> <span>matching</span> <span>context</span>
</span><span id="L-5-58"><a id="L-5-58" name="L-5-58"></a><span>52</span><span>:</span>         <span>for</span> <span>u</span> <span>from</span> <span>1</span> <span>to</span> <span>min</span><span>(</span><span>idx</span><span>,</span> <span>maxN</span><span>)</span> <span>do</span>
</span><span id="L-5-59"><a id="L-5-59" name="L-5-59"></a><span>53</span><span>:</span>             <span>if</span> <span>breakMask</span><span>[</span><span>idx</span> <span>-</span> <span>u</span><span>]</span> <span>then</span> <span>break</span> <span>end</span> <span>if</span>
</span><span id="L-5-60"><a id="L-5-60" name="L-5-60"></a><span>54</span><span>:</span>             <span>if</span> <span>T</span><span>[</span><span>idx</span> <span>-</span> <span>u</span><span>]</span> <span>≠</span> <span>T</span><span>[</span><span>-</span><span>u</span> <span>-</span> <span>1</span><span>]</span> <span>then</span> <span>break</span> <span>end</span> <span>if</span>
</span><span id="L-5-61"><a id="L-5-61" name="L-5-61"></a><span>55</span><span>:</span>             <span>matchLen</span> <span>←</span> <span>u</span>
</span><span id="L-5-62"><a id="L-5-62" name="L-5-62"></a><span>56</span><span>:</span>         <span>end</span> <span>for</span>
</span><span id="L-5-63"><a id="L-5-63" name="L-5-63"></a><span>57</span><span>:</span>
</span><span id="L-5-64"><a id="L-5-64" name="L-5-64"></a><span>58</span><span>:</span>         <span>if</span> <span>matchLen</span> <span>&gt;</span> <span>0</span> <span>then</span>
</span><span id="L-5-65"><a id="L-5-65" name="L-5-65"></a><span>59</span><span>:</span>             <span>nextToken</span> <span>←</span> <span>T</span><span>[</span><span>idx</span> <span>+</span> <span>1</span><span>]</span>  <span>//</span> <span>Token</span> <span>that</span> <span>followed</span> <span>this</span> <span>pattern</span> <span>before</span>
</span><span id="L-5-66"><a id="L-5-66" name="L-5-66"></a><span>60</span><span>:</span>             <span>newLen</span> <span>←</span> <span>matchLen</span> <span>+</span> <span>1</span>
</span><span id="L-5-67"><a id="L-5-67" name="L-5-67"></a><span>61</span><span>:</span>             <span>ngramLengths</span><span>[</span><span>nextToken</span><span>]</span> <span>←</span> <span>max</span><span>(</span><span>ngramLengths</span><span>[</span><span>nextToken</span><span>],</span> <span>newLen</span><span>)</span>
</span><span id="L-5-68"><a id="L-5-68" name="L-5-68"></a><span>62</span><span>:</span>             
</span><span id="L-5-69"><a id="L-5-69" name="L-5-69"></a><span>63</span><span>:</span>             <span>if</span> <span>newLen</span> <span>≥</span> <span>E</span> <span>then</span> <span>break</span> <span>end</span> <span>if</span>  <span>//</span> <span>Early</span> <span>exit</span> <span>if</span> <span>match</span> <span>is</span> <span>long</span> <span>enough</span>
</span><span id="L-5-70"><a id="L-5-70" name="L-5-70"></a><span>64</span><span>:</span>         <span>end</span> <span>if</span>
</span><span id="L-5-71"><a id="L-5-71" name="L-5-71"></a><span>65</span><span>:</span>     <span>end</span> <span>for</span>
</span><span id="L-5-72"><a id="L-5-72" name="L-5-72"></a><span>66</span><span>:</span>     
</span><span id="L-5-73"><a id="L-5-73" name="L-5-73"></a><span>67</span><span>:</span>     <span>//</span> <span>Apply</span> <span>penalties</span> <span>to</span> <span>tokens</span> <span>that</span> <span>would</span> <span>continue</span> <span>repeating</span> <span>patterns</span>
</span><span id="L-5-74"><a id="L-5-74" name="L-5-74"></a><span>68</span><span>:</span>     <span>penaltyMask</span> <span>←</span> <span>(</span><span>ngramLengths</span> <span>&gt;</span> <span>0</span><span>)</span>
</span><span id="L-5-75"><a id="L-5-75" name="L-5-75"></a><span>69</span><span>:</span>     <span>if</span> <span>any</span><span>(</span><span>penaltyMask</span><span>)</span> <span>then</span>
</span><span id="L-5-76"><a id="L-5-76" name="L-5-76"></a><span>70</span><span>:</span>         <span>scales</span> <span>←</span> <span>b</span> <span>^</span> <span>(</span><span>ngramLengths</span><span>[</span><span>penaltyMask</span><span>]</span> <span>-</span> <span>Nmin</span><span>)</span>  <span>//</span> <span>Exponential</span> <span>scaling</span> <span>by</span> <span>pattern</span> <span>length</span>
</span><span id="L-5-77"><a id="L-5-77" name="L-5-77"></a><span>71</span><span>:</span>         <span>L</span><span>[</span><span>s</span><span>][</span><span>penaltyMask</span><span>]</span> <span>←</span> <span>L</span><span>[</span><span>s</span><span>][</span><span>penaltyMask</span><span>]</span> <span>-</span> <span>λ</span> <span>*</span> <span>scales</span>
</span><span id="L-5-78"><a id="L-5-78" name="L-5-78"></a><span>72</span><span>:</span>     <span>end</span> <span>if</span>
</span><span id="L-5-79"><a id="L-5-79" name="L-5-79"></a><span>73</span><span>:</span> <span>end</span> <span>for</span>
</span><span id="L-5-80"><a id="L-5-80" name="L-5-80"></a><span>74</span><span>:</span> <span>return</span> <span>L</span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="top-k">Top-K<a href="#top-k" title="Permanent link"> </a></h3>
<p>Instead of considering all possible next words (which could be tens of thousands), the model narrows down to only the K most likely candidates. If K is 40, the model will only choose from the top 40 most likely next words. This approach prevents the model from selecting extremely unlikely words while still maintaining some randomness.</p>
<p><strong>Technical</strong>:<br>
This method works by sorting the logits for all possible next tokens and keeping only the K highest values while setting all others to negative infinity. We first sort the logits in ascending order and get their indices. Then, we identify the Kth highest value and create a mask for all values below this threshold. Any logit below this threshold is set to <code>-inf</code>, ensuring these tokens have effectively zero probability after applying softmax. The filtered logits are then returned to their original order using the saved indices.</p>
<p><strong>Algorithm</strong><br>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-6-1"><a id="L-6-1" name="L-6-1"></a><span>Algorithm</span><span> </span><span>6</span><span> </span><span>Top</span><span>-</span><span>K</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-6-2"><a id="L-6-2" name="L-6-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>parameter</span><span> </span><span>k</span><span></span>
</span><span id="L-6-3"><a id="L-6-3" name="L-6-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>all</span><span> </span><span>but</span><span> </span><span>top</span><span>-</span><span>k</span><span> </span><span>options</span><span> </span><span>filtered</span><span> </span><span>out</span><span></span>
</span><span id="L-6-4"><a id="L-6-4" name="L-6-4"></a><span>1</span><span>:</span><span> </span><span>Lsorted</span><span>,</span><span> </span><span>Lidx</span><span> </span><span>←</span><span> </span><span>Sort</span><span>(</span><span>L</span><span>,</span><span> </span><span>descending</span><span>=</span><span>False</span><span>)</span><span>  </span><span>//</span><span> </span><span>Sort</span><span> </span><span>logits</span><span> </span><span>in</span><span> </span><span>ascending</span><span> </span><span>order</span><span></span>
</span><span id="L-6-5"><a id="L-6-5" name="L-6-5"></a><span>2</span><span>:</span><span> </span><span>kth</span><span> </span><span>←</span><span> </span><span>Lsorted</span><span>[</span><span>|Lsorted| - k</span><span>]</span><span>  </span><span>//</span><span> </span><span>Find</span><span> </span><span>the</span><span> </span><span>kth</span><span> </span><span>largest</span><span> </span><span>logit</span><span> </span><span>value</span><span></span>
</span><span id="L-6-6"><a id="L-6-6" name="L-6-6"></a><span>3</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>Lsorted</span><span> </span><span>&lt;</span><span> </span><span>kth</span><span>  </span><span>//</span><span> </span><span>Create</span><span> </span><span>mask</span><span> </span><span>for</span><span> </span><span>values</span><span> </span><span>below</span><span> </span><span>the</span><span> </span><span>threshold</span><span></span>
</span><span id="L-6-7"><a id="L-6-7" name="L-6-7"></a><span>4</span><span>:</span><span> </span><span>Lsorted</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span>  </span><span>//</span><span> </span><span>Filter</span><span> </span><span>out</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-6-8"><a id="L-6-8" name="L-6-8"></a><span>5</span><span>:</span><span> </span><span>L</span><span> </span><span>←</span><span> </span><span>Unsort</span><span>(</span><span>Lsorted</span><span>,</span><span> </span><span>Lidx</span><span>)</span><span>  </span><span>//</span><span> </span><span>Restore</span><span> </span><span>original</span><span> </span><span>ordering</span><span></span>
</span><span id="L-6-9"><a id="L-6-9" name="L-6-9"></a><span>6</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="top-p">Top-P<a href="#top-p" title="Permanent link"> </a></h3>
<p>Instead of picking a fixed number of options like Top-K, Top-P selects the smallest set of words whose combined probability exceeds threshold <code>P</code>. It's like saying "I'll only consider dishes that make up 90% of all orders at this restaurant." If <code>P</code> is 0.9, the model includes just enough of the highest-probability words to reach 90% cumulative probability, whether that's 5 words or 500. In situations where the model is very confident, it might only need a few options, but when uncertainty is high, it can consider more possibilities.</p>
<p><strong>Technical</strong>:<br>
Top-P works by selecting the smallest set of tokens whose cumulative probability exceeds the threshold <code>P</code>. After applying Top-K filtering, we convert logits to probabilities using softmax and calculates their cumulative sum in ascending order. It then creates a mask identifying tokens where the cumulative probability is still below 1-P (i.e. these tokens aren't part of the "nucleus" that makes up <code>P</code> of the total probability mass). These tokens are masked out by setting their logits to <code>-inf</code>. We need to ensure that at least one token remains viable by always keeping the highest-probability token (setting its mask to False). Finally, the filtered logits are returned to their original order using the saved indices from the initial sorting.</p>
<p><strong>Algorithm</strong><br>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-7-1"><a id="L-7-1" name="L-7-1"></a><span>Algorithm</span><span> </span><span>7</span><span> </span><span>Top</span><span>-</span><span>P</span><span> </span><span>(</span><span>Nucleus</span><span>)</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-7-2"><a id="L-7-2" name="L-7-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>probability</span><span> </span><span>threshold</span><span> </span><span>p</span><span> </span><span>∈</span><span> </span><span>[</span><span>0,1</span><span>]</span><span></span>
</span><span id="L-7-3"><a id="L-7-3" name="L-7-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>smallest</span><span> </span><span>probability</span><span> </span><span>tokens</span><span> </span><span>filtered</span><span> </span><span>out</span><span></span>
</span><span id="L-7-4"><a id="L-7-4" name="L-7-4"></a><span>1</span><span>:</span><span> </span><span>Lsorted</span><span>,</span><span> </span><span>Lidx</span><span> </span><span>←</span><span> </span><span>Sort</span><span>(</span><span>L</span><span>,</span><span> </span><span>descending</span><span>=</span><span>False</span><span>)</span><span>  </span><span>//</span><span> </span><span>Sort</span><span> </span><span>logits</span><span> </span><span>in</span><span> </span><span>ascending</span><span> </span><span>order</span><span></span>
</span><span id="L-7-5"><a id="L-7-5" name="L-7-5"></a><span>2</span><span>:</span><span> </span><span>Psorted</span><span> </span><span>←</span><span> </span><span>Softmax</span><span>(</span><span>Lsorted</span><span>)</span><span>  </span><span>//</span><span> </span><span>Convert</span><span> </span><span>to</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-7-6"><a id="L-7-6" name="L-7-6"></a><span>3</span><span>:</span><span> </span><span>Pcum</span><span> </span><span>←</span><span> </span><span>CumulativeSum</span><span>(</span><span>Psorted</span><span>)</span><span>  </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>cumulative</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-7-7"><a id="L-7-7" name="L-7-7"></a><span>4</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>Pcum</span><span> </span><span>≤</span><span> </span><span>(</span><span>1</span><span> </span><span>-</span><span> </span><span>p</span><span>)</span><span>  </span><span>//</span><span> </span><span>Create</span><span> </span><span>mask</span><span> </span><span>for</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-7-8"><a id="L-7-8" name="L-7-8"></a><span>5</span><span>:</span><span> </span><span>mask</span><span>[</span><span>|mask| - 1</span><span>]</span><span> </span><span>←</span><span> </span><span>False</span><span>  </span><span>//</span><span> </span><span>Always</span><span> </span><span>keep</span><span> </span><span>at</span><span> </span><span>least</span><span> </span><span>one</span><span> </span><span>token</span><span></span>
</span><span id="L-7-9"><a id="L-7-9" name="L-7-9"></a><span>6</span><span>:</span><span> </span><span>Lsorted</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span>  </span><span>//</span><span> </span><span>Filter</span><span> </span><span>out</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-7-10"><a id="L-7-10" name="L-7-10"></a><span>7</span><span>:</span><span> </span><span>L</span><span> </span><span>←</span><span> </span><span>Unsort</span><span>(</span><span>Lsorted</span><span>,</span><span> </span><span>Lidx</span><span>)</span><span>  </span><span>//</span><span> </span><span>Restore</span><span> </span><span>original</span><span> </span><span>ordering</span><span></span>
</span><span id="L-7-11"><a id="L-7-11" name="L-7-11"></a><span>8</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="min-p">Min-P<a href="#min-p" title="Permanent link"> </a></h3>
<p>Min-P sets a quality threshold relative to these best option. Using the restaurant analogy again, imagine you're at a restaurant with your friend who always orders the most popular dish. You decide you'll only consider dishes that are at least 20% as popular as their top choice. If the most popular dish gets ordered 100 times a day, you set Min-P to 0.2, you'll only consider dishes ordered at least 20 times a day. When the model is very confident about the best choice, the threshold becomes higher, and fewer alternatives are considered. When the model is uncertain, more alternatives pass the threshold.</p>
<p>Min-P is usually used in conjunction with higher temperature values (1.0-1.2), and used in very low values (0.1).</p>
<p><strong>Technical</strong>:<br>
Min-P filters out tokens whose probabilities fall below a certain fraction of the highest probability token. We first convert logits to probabilities using softmax and identify the maximum probability value for each sequence. Then, create a threshold by multiplying this maximum probability by the Min-P value. For example, if the highest probability is 0.6 and Min-P is 0.1, the threshold would be 0.06.</p>
<p>Any token with a probability below this threshold is masked out by setting its logit to <code>-inf</code>. This creates a dynamic filtering mechanism where the cutoff adapts to the confidence level of the model for each prediction. Min-P doesn't require sorting the entire vocabulary like Top-K or Top-P, so it ends up being more efficient as well.</p>
<p><strong>Algorithm</strong><br>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-8-1"><a id="L-8-1" name="L-8-1"></a><span>Algorithm</span><span> </span><span>8</span><span> </span><span>Min</span><span>-</span><span>P</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-8-2"><a id="L-8-2" name="L-8-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>threshold</span><span> </span><span>fraction</span><span> </span><span>min_p</span><span> </span><span>∈</span><span> </span><span>[</span><span>0,1</span><span>]</span><span></span>
</span><span id="L-8-3"><a id="L-8-3" name="L-8-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>low</span><span> </span><span>probability</span><span> </span><span>tokens</span><span> </span><span>filtered</span><span> </span><span>out</span><span></span>
</span><span id="L-8-4"><a id="L-8-4" name="L-8-4"></a><span>1</span><span>:</span><span> </span><span>P</span><span> </span><span>←</span><span> </span><span>Softmax</span><span>(</span><span>L</span><span>)</span><span>  </span><span>//</span><span> </span><span>Convert</span><span> </span><span>logits</span><span> </span><span>to</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-8-5"><a id="L-8-5" name="L-8-5"></a><span>2</span><span>:</span><span> </span><span>Pmax</span><span> </span><span>←</span><span> </span><span>Max</span><span>(</span><span>P</span><span>)</span><span>  </span><span>//</span><span> </span><span>Find</span><span> </span><span>maximum</span><span> </span><span>probability</span><span></span>
</span><span id="L-8-6"><a id="L-8-6" name="L-8-6"></a><span>3</span><span>:</span><span> </span><span>threshold</span><span> </span><span>←</span><span> </span><span>min_p</span><span> </span><span>×</span><span> </span><span>Pmax</span><span>  </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>dynamic</span><span> </span><span>threshold</span><span></span>
</span><span id="L-8-7"><a id="L-8-7" name="L-8-7"></a><span>4</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>P</span><span> </span><span>&lt;</span><span> </span><span>threshold</span><span>  </span><span>//</span><span> </span><span>Create</span><span> </span><span>mask</span><span> </span><span>for</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-8-8"><a id="L-8-8" name="L-8-8"></a><span>5</span><span>:</span><span> </span><span>L</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span>  </span><span>//</span><span> </span><span>Filter</span><span> </span><span>out</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-8-9"><a id="L-8-9" name="L-8-9"></a><span>6</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="top-a">Top-A<a href="#top-a" title="Permanent link"> </a></h3>
<p>This one applies a filter that gets stricter when the model is more confident. Top-A creates a threshold that's proportional to the square of the highest probability token. When the model is very confident, the threshold becomes much higher due to the squaring effect, dramatically limiting options only to the very best alternatives. When the model is less confident, the threshold drops more rapidly.</p>
<p><strong>Technical</strong>:</p>
<p>Top-A is essentially if min-p was squared instead of linear. Note that Top-A predates Min-P.</p>
<p><strong>Algorithm</strong><br>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-9-1"><a id="L-9-1" name="L-9-1"></a><span>Algorithm</span><span> </span><span>9</span><span> </span><span>Top</span><span>-</span><span>A</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-9-2"><a id="L-9-2" name="L-9-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>parameter</span><span> </span><span>a</span><span></span>
</span><span id="L-9-3"><a id="L-9-3" name="L-9-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>options</span><span> </span><span>below</span><span> </span><span>squared</span><span> </span><span>threshold</span><span> </span><span>filtered</span><span> </span><span>out</span><span></span>
</span><span id="L-9-4"><a id="L-9-4" name="L-9-4"></a><span>1</span><span>:</span><span> </span><span>P</span><span> </span><span>←</span><span> </span><span>Softmax</span><span>(</span><span>L</span><span>)</span><span>  </span><span>//</span><span> </span><span>Convert</span><span> </span><span>logits</span><span> </span><span>to</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-9-5"><a id="L-9-5" name="L-9-5"></a><span>2</span><span>:</span><span> </span><span>Pmax</span><span> </span><span>←</span><span> </span><span>Max</span><span>(</span><span>P</span><span>)</span><span>  </span><span>//</span><span> </span><span>Find</span><span> </span><span>maximum</span><span> </span><span>probability</span><span></span>
</span><span id="L-9-6"><a id="L-9-6" name="L-9-6"></a><span>3</span><span>:</span><span> </span><span>threshold</span><span> </span><span>←</span><span> </span><span>Pmax²</span><span> </span><span>×</span><span> </span><span>a</span><span>  </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>squared</span><span> </span><span>threshold</span><span></span>
</span><span id="L-9-7"><a id="L-9-7" name="L-9-7"></a><span>4</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>P</span><span> </span><span>&lt;</span><span> </span><span>threshold</span><span>  </span><span>//</span><span> </span><span>Create</span><span> </span><span>mask</span><span> </span><span>for</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-9-8"><a id="L-9-8" name="L-9-8"></a><span>5</span><span>:</span><span> </span><span>L</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span>  </span><span>//</span><span> </span><span>Filter</span><span> </span><span>out</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-9-9"><a id="L-9-9" name="L-9-9"></a><span>6</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="xtc-exclude-top-choices">XTC (eXclude Top Choices)<a href="#xtc-exclude-top-choices" title="Permanent link"> </a></h3>
<p>XTC works by occasionally excluding the most likely option based on two parameters: a probability of activation, and a threshold for determining which choices to exclude. When XTC activates (based on random chance), it looks at all the top choices whose probabilities exceed the threshold and removes all but the lowest-scoring one among them.</p>
<p>This forces the model to occasionally "think outside the box" and select words it wouldn't normally. Unlike other samplers that just filter out unlikely options, XTC specifically targets the most predictable choices.</p>
<p><strong>Technical</strong>:<br>
XTC occasionally excludes the most likely token from consideration, keeping only the least likely token among those that exceed a specified threshold. We first determine whether to apply XTC for each sequence based on a random draw against the specified probability. For sequences where XTC activates, we convert logits to probs, sort them in descending order, and identify tokens whose probabilities exceed the threshold (skipping the very top choices initially). If tokens above the threshold are found, we count how many tokens qualify (including the top choice that was initially skipped). We then mask out all these high-prob tokens except for the last/lowest one in the sorted list. This effectively removes the most obvious choices. You can call this sampler a contrarian.</p>
<p><strong>Algorithm</strong><br>
</p><div>

<table><tbody><tr><td><div><pre><span><a href="#L-10-1"> 1</a></span>
<span><a href="#L-10-2"> 2</a></span>
<span><a href="#L-10-3"> 3</a></span>
<span><a href="#L-10-4"> 4</a></span>
<span><a href="#L-10-5"> 5</a></span>
<span><a href="#L-10-6"> 6</a></span>
<span><a href="#L-10-7"> 7</a></span>
<span><a href="#L-10-8"> 8</a></span>
<span><a href="#L-10-9"> 9</a></span>
<span><a href="#L-10-10">10</a></span>
<span><a href="#L-10-11">11</a></span>
<span><a href="#L-10-12">12</a></span>
<span><a href="#L-10-13">13</a></span>
<span><a href="#L-10-14">14</a></span>
<span><a href="#L-10-15">15</a></span>
<span><a href="#L-10-16">16</a></span>
<span><a href="#L-10-17">17</a></span>
<span><a href="#L-10-18">18</a></span>
<span><a href="#L-10-19">19</a></span>
<span><a href="#L-10-20">20</a></span>
<span><a href="#L-10-21">21</a></span>
<span><a href="#L-10-22">22</a></span>
<span><a href="#L-10-23">23</a></span>
<span><a href="#L-10-24">24</a></span>
<span><a href="#L-10-25">25</a></span>
<span><a href="#L-10-26">26</a></span></pre></div></td><td><div><pre><span></span><span id="L-10-1"><a id="L-10-1" name="L-10-1"></a><span>Algorithm</span><span> </span><span>10</span><span> </span><span>XTC</span><span> </span><span>(</span><span>eXclude</span><span> </span><span>Top</span><span> </span><span>Choices</span><span>)</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-10-2"><a id="L-10-2" name="L-10-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>threshold</span><span> </span><span>t</span><span>,</span><span> </span><span>activation</span><span> </span><span>probability</span><span> </span><span>p</span><span></span>
</span><span id="L-10-3"><a id="L-10-3" name="L-10-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>top</span><span> </span><span>choices</span><span> </span><span>filtered</span><span> </span><span>out</span><span> </span><span>except</span><span> </span><span>for</span><span> </span><span>the</span><span> </span><span>lowest</span><span>-</span><span>scoring</span><span> </span><span>one</span><span></span>
</span><span id="L-10-4"><a id="L-10-4" name="L-10-4"></a><span>1</span><span>:</span><span> </span><span>apply</span><span> </span><span>←</span><span> </span><span>RandomBernoulli</span><span>(</span><span>p</span><span>)</span><span>  </span><span>//</span><span> </span><span>Determine</span><span> </span><span>which</span><span> </span><span>sequences</span><span> </span><span>to</span><span> </span><span>apply</span><span> </span><span>XTC</span><span> </span><span>to</span><span></span>
</span><span id="L-10-5"><a id="L-10-5" name="L-10-5"></a><span>2</span><span>:</span><span> </span><span>if</span><span> </span><span>not</span><span> </span><span>any</span><span>(</span><span>apply</span><span>)</span><span> </span><span>then</span><span> </span><span>return</span><span> </span><span>L</span><span> </span><span>end</span><span> </span><span>if</span><span></span>
</span><span id="L-10-6"><a id="L-10-6" name="L-10-6"></a><span>3</span><span>:</span><span></span>
</span><span id="L-10-7"><a id="L-10-7" name="L-10-7"></a><span>4</span><span>:</span><span> </span><span>P</span><span> </span><span>←</span><span> </span><span>Softmax</span><span>(</span><span>L</span><span>)</span><span>  </span><span>//</span><span> </span><span>Convert</span><span> </span><span>logits</span><span> </span><span>to</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-10-8"><a id="L-10-8" name="L-10-8"></a><span>5</span><span>:</span><span> </span><span>Psorted</span><span>,</span><span> </span><span>Pidx</span><span> </span><span>←</span><span> </span><span>Sort</span><span>(</span><span>P</span><span>,</span><span> </span><span>descending</span><span>=</span><span>True</span><span>)</span><span>  </span><span>//</span><span> </span><span>Sort</span><span> </span><span>probabilities</span><span> </span><span>in</span><span> </span><span>descending</span><span> </span><span>order</span><span></span>
</span><span id="L-10-9"><a id="L-10-9" name="L-10-9"></a><span>6</span><span>:</span><span></span>
</span><span id="L-10-10"><a id="L-10-10" name="L-10-10"></a><span>7</span><span>:</span><span> </span><span>for</span><span> </span><span>each</span><span> </span><span>sequence</span><span> </span><span>i</span><span> </span><span>where</span><span> </span><span>apply</span><span>[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>True</span><span> </span><span>do</span><span></span>
</span><span id="L-10-11"><a id="L-10-11" name="L-10-11"></a><span>8</span><span>:</span><span>     </span><span>//</span><span> </span><span>Find</span><span> </span><span>tokens</span><span> </span><span>above</span><span> </span><span>threshold</span><span> </span><span>(</span><span>starting</span><span> </span><span>from</span><span> </span><span>second</span><span>-</span><span>highest</span><span>)</span><span></span>
</span><span id="L-10-12"><a id="L-10-12" name="L-10-12"></a><span>9</span><span>:</span><span>     </span><span>aboveThreshold</span><span> </span><span>←</span><span> </span><span>Psorted</span><span>[</span><span>i, 1:</span><span>]</span><span> </span><span>≥</span><span> </span><span>t</span><span></span>
</span><span id="L-10-13"><a id="L-10-13" name="L-10-13"></a><span>10</span><span>:</span><span></span>
</span><span id="L-10-14"><a id="L-10-14" name="L-10-14"></a><span>11</span><span>:</span><span>     </span><span>//</span><span> </span><span>Count</span><span> </span><span>how</span><span> </span><span>many</span><span> </span><span>tokens</span><span> </span><span>qualify</span><span> </span><span>(</span><span>including</span><span> </span><span>the</span><span> </span><span>top</span><span> </span><span>one</span><span>)</span><span></span>
</span><span id="L-10-15"><a id="L-10-15" name="L-10-15"></a><span>12</span><span>:</span><span>     </span><span>count</span><span> </span><span>←</span><span> </span><span>Sum</span><span>(</span><span>aboveThreshold</span><span>)</span><span> </span><span>+</span><span> </span><span>1</span><span></span>
</span><span id="L-10-16"><a id="L-10-16" name="L-10-16"></a><span>13</span><span>:</span><span></span>
</span><span id="L-10-17"><a id="L-10-17" name="L-10-17"></a><span>14</span><span>:</span><span>     </span><span>if</span><span> </span><span>count</span><span> </span><span>&gt;</span><span> </span><span>1</span><span> </span><span>then</span><span></span>
</span><span id="L-10-18"><a id="L-10-18" name="L-10-18"></a><span>15</span><span>:</span><span>         </span><span>//</span><span> </span><span>Get</span><span> </span><span>indices</span><span> </span><span>of</span><span> </span><span>the</span><span> </span><span>high</span><span>-</span><span>probability</span><span> </span><span>tokens</span><span> </span><span>to</span><span> </span><span>remove</span><span></span>
</span><span id="L-10-19"><a id="L-10-19" name="L-10-19"></a><span>16</span><span>:</span><span>         </span><span>tokensToRemove</span><span> </span><span>←</span><span> </span><span>Pidx</span><span>[</span><span>i, 0:count-1</span><span>]</span><span></span>
</span><span id="L-10-20"><a id="L-10-20" name="L-10-20"></a><span>17</span><span>:</span><span></span>
</span><span id="L-10-21"><a id="L-10-21" name="L-10-21"></a><span>18</span><span>:</span><span>         </span><span>//</span><span> </span><span>Mask</span><span> </span><span>out</span><span> </span><span>all</span><span> </span><span>high</span><span>-</span><span>probability</span><span> </span><span>tokens</span><span> </span><span>except</span><span> </span><span>the</span><span> </span><span>lowest</span><span>-</span><span>scoring</span><span> </span><span>one</span><span></span>
</span><span id="L-10-22"><a id="L-10-22" name="L-10-22"></a><span>19</span><span>:</span><span>         </span><span>L</span><span>[</span><span>i, tokensToRemove</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span></span>
</span><span id="L-10-23"><a id="L-10-23" name="L-10-23"></a><span>20</span><span>:</span><span>     </span><span>end</span><span> </span><span>if</span><span></span>
</span><span id="L-10-24"><a id="L-10-24" name="L-10-24"></a><span>21</span><span>:</span><span> </span><span>end</span><span> </span><span>for</span><span></span>
</span><span id="L-10-25"><a id="L-10-25" name="L-10-25"></a><span>22</span><span>:</span><span></span>
</span><span id="L-10-26"><a id="L-10-26" name="L-10-26"></a><span>23</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="top-n-sigma">Top-N-Sigma<a href="#top-n-sigma" title="Permanent link"> </a></h3>
<p>This one sets a statistical quality bar for word choices. Think of it like selecting players for a sports team and decide to take anyone who scores within 2 standard deviations of the top performer.</p>
<p>The sampler uses basic statistics to create a more adaptive threshold. It looks at how spread out (standard deviation) the scores for all possible next words are, then sets a cutoff at <code>N</code> standard deviations below the highest-scoring word. In situations where the model has a few very strong preferences and many mediocre options, the standard deviation will be small, creating a stricter threshold. But when there are many similarly good options, the standard deviation increases. Essentially, when the model is certain about a few good choices, it stays focused on those. When there are multiple reasonable options (like in creative writing), it allows for more variety without including truly poor choices.</p>
<p><strong>Technical</strong>:<br>
Top-N-Sigma creates a threshold based on statistical properties of the distribution of logits, specifically the maximum value and standard deviation. We calculate the standard deviation of logits across the vocabulary for each sequence. Then, set a threshold by subtracting <code>N</code> times this standard deviation from the maximum logit value. any logit below this threshold is set to <code>-inf</code>. The approach is mathematically elegant because it adapts to the shape of the distribution rather than just its absolute values. In distributions with high variance, the threshold will be lower, allowing more tokens in. It accounts for both the absolute best score and the overall distribution of scores.</p>
<p><strong>Algorithm</strong><br>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-11-1"><a id="L-11-1" name="L-11-1"></a><span>Algorithm</span><span> </span><span>11</span><span> </span><span>Top</span><span>-</span><span>N</span><span>-</span><span>Sigma</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-11-2"><a id="L-11-2" name="L-11-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>parameter</span><span> </span><span>n</span><span> </span><span>(</span><span>number</span><span> </span><span>of</span><span> </span><span>standard</span><span> </span><span>deviations</span><span>)</span><span></span>
</span><span id="L-11-3"><a id="L-11-3" name="L-11-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>values</span><span> </span><span>below</span><span> </span><span>statistical</span><span> </span><span>threshold</span><span> </span><span>filtered</span><span> </span><span>out</span><span></span>
</span><span id="L-11-4"><a id="L-11-4" name="L-11-4"></a><span>1</span><span>:</span><span> </span><span>σ</span><span> </span><span>←</span><span> </span><span>StandardDeviation</span><span>(</span><span>L</span><span>)</span><span>  </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>standard</span><span> </span><span>deviation</span><span> </span><span>of</span><span> </span><span>logits</span><span> </span><span>across</span><span> </span><span>vocabulary</span><span></span>
</span><span id="L-11-5"><a id="L-11-5" name="L-11-5"></a><span>2</span><span>:</span><span> </span><span>Lmax</span><span> </span><span>←</span><span> </span><span>Max</span><span>(</span><span>L</span><span>)</span><span>  </span><span>//</span><span> </span><span>Find</span><span> </span><span>maximum</span><span> </span><span>logit</span><span> </span><span>value</span><span></span>
</span><span id="L-11-6"><a id="L-11-6" name="L-11-6"></a><span>3</span><span>:</span><span> </span><span>threshold</span><span> </span><span>←</span><span> </span><span>Lmax</span><span> </span><span>-</span><span> </span><span>n</span><span> </span><span>×</span><span> </span><span>σ</span><span>  </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>threshold</span><span> </span><span>n</span><span> </span><span>standard</span><span> </span><span>deviations</span><span> </span><span>below</span><span> </span><span>maximum</span><span></span>
</span><span id="L-11-7"><a id="L-11-7" name="L-11-7"></a><span>4</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>L</span><span> </span><span>&lt;</span><span> </span><span>threshold</span><span>  </span><span>//</span><span> </span><span>Create</span><span> </span><span>mask</span><span> </span><span>for</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-11-8"><a id="L-11-8" name="L-11-8"></a><span>5</span><span>:</span><span> </span><span>L</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span>  </span><span>//</span><span> </span><span>Filter</span><span> </span><span>out</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-11-9"><a id="L-11-9" name="L-11-9"></a><span>6</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="tail-free-sampling">Tail-Free Sampling<a href="#tail-free-sampling" title="Permanent link"> </a></h3>
<p>TFS is like looking at the slope change in a graph of word probabilities and cutting it off where there's a significant drop. For example, you're lining up candidates for a job based on their scores - instead of using a fixed cutoff, you look for where there's a big gap between consecutive scores and say "everyone past this point is significantly worse."</p>
<p>TFS examines how the probability distribution changes by looking at second derivatives (the "curvature" or rate of change in the slope). It focues on points where the distribution starts to flatten out, marking the transition from "good" candidates to "the long tail of mediocre options." What makes it special is that it adapts to the natural structure of the distribution rather than imposing arbitrary thresholds. It finds the "elbow point" where good options end and the less relevant ones begin.</p>
<p><strong>Technical</strong>:<br>
TFS works by analyzing the curvature of the probability distribution to identify where the "tail" begins. We first sort the logits in descending order and convert them to probs. Then, calculate the absolute value of the second derivative (<code>diff().diff().abs()</code>), which captures how quickly the slope of the probability distribution changes. This identifies points where there are significant kinks in the curve - often indicating the transition between meaningful tokens and the long tail. The second derivatives are normalized to create a probability distribution of the curvature, and a cumulative distribution function (CDF) is calculated. The TFS parameter serves as a threshold on this CDF - when the cumulative curvature exceeds the threshold, tokens beyond that point are masked out. Of course, there needs to be boundary conditions to make sure at least one token always remains viable, and finally, return filtered logits to their original order.</p>
<p>TFS ultimately focues on the shape of the distribution rather than absolute values.</p>
<p><strong>Algorithm</strong><br>
</p><div>

<table><tbody><tr><td><div><pre><span><a href="#L-12-1"> 1</a></span>
<span><a href="#L-12-2"> 2</a></span>
<span><a href="#L-12-3"> 3</a></span>
<span><a href="#L-12-4"> 4</a></span>
<span><a href="#L-12-5"> 5</a></span>
<span><a href="#L-12-6"> 6</a></span>
<span><a href="#L-12-7"> 7</a></span>
<span><a href="#L-12-8"> 8</a></span>
<span><a href="#L-12-9"> 9</a></span>
<span><a href="#L-12-10">10</a></span>
<span><a href="#L-12-11">11</a></span>
<span><a href="#L-12-12">12</a></span>
<span><a href="#L-12-13">13</a></span>
<span><a href="#L-12-14">14</a></span>
<span><a href="#L-12-15">15</a></span></pre></div></td><td><div><pre><span></span><span id="L-12-1"><a id="L-12-1" name="L-12-1"></a><span>Algorithm</span><span> </span><span>12</span><span> </span><span>Tail</span><span>-</span><span>Free</span><span> </span><span>Sampling</span><span> </span><span>(</span><span>TFS</span><span>)</span><span></span>
</span><span id="L-12-2"><a id="L-12-2" name="L-12-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>threshold</span><span> </span><span>t</span><span> </span><span>∈</span><span> </span><span>[</span><span>0,1</span><span>]</span><span></span>
</span><span id="L-12-3"><a id="L-12-3" name="L-12-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>"tail"</span><span> </span><span>of</span><span> </span><span>distribution</span><span> </span><span>filtered</span><span> </span><span>out</span><span></span>
</span><span id="L-12-4"><a id="L-12-4" name="L-12-4"></a><span>1</span><span>:</span><span> </span><span>Lsorted</span><span>,</span><span> </span><span>Lidx</span><span> </span><span>←</span><span> </span><span>Sort</span><span>(</span><span>L</span><span>,</span><span> </span><span>descending</span><span>=</span><span>True</span><span>)</span><span>  </span><span>//</span><span> </span><span>Sort</span><span> </span><span>logits</span><span> </span><span>in</span><span> </span><span>descending</span><span> </span><span>order</span><span></span>
</span><span id="L-12-5"><a id="L-12-5" name="L-12-5"></a><span>2</span><span>:</span><span> </span><span>P</span><span> </span><span>←</span><span> </span><span>Softmax</span><span>(</span><span>Lsorted</span><span>)</span><span>  </span><span>//</span><span> </span><span>Convert</span><span> </span><span>to</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-12-6"><a id="L-12-6" name="L-12-6"></a><span>3</span><span>:</span><span> </span><span>D1</span><span> </span><span>←</span><span> </span><span>Diff</span><span>(</span><span>P</span><span>)</span><span>  </span><span>//</span><span> </span><span>First</span><span> </span><span>differences</span><span> </span><span>(</span><span>slope</span><span>)</span><span></span>
</span><span id="L-12-7"><a id="L-12-7" name="L-12-7"></a><span>4</span><span>:</span><span> </span><span>D2</span><span> </span><span>←</span><span> </span><span>|</span><span>Diff</span><span>(</span><span>D1</span><span>)</span><span>|</span><span>  </span><span>//</span><span> </span><span>Absolute</span><span> </span><span>second</span><span> </span><span>differences</span><span> </span><span>(</span><span>change</span><span> </span><span>in</span><span> </span><span>slope</span><span>)</span><span></span>
</span><span id="L-12-8"><a id="L-12-8" name="L-12-8"></a><span>5</span><span>:</span><span> </span><span>D2norm</span><span> </span><span>←</span><span> </span><span>D2</span><span> </span><span>/</span><span> </span><span>Sum</span><span>(</span><span>D2</span><span>)</span><span>  </span><span>//</span><span> </span><span>Normalize</span><span> </span><span>curvature</span><span></span>
</span><span id="L-12-9"><a id="L-12-9" name="L-12-9"></a><span>6</span><span>:</span><span> </span><span>CurvatureCDF</span><span> </span><span>←</span><span> </span><span>CumulativeSum</span><span>(</span><span>D2norm</span><span>)</span><span>  </span><span>//</span><span> </span><span>Cumulative</span><span> </span><span>distribution</span><span> </span><span>of</span><span> </span><span>curvature</span><span></span>
</span><span id="L-12-10"><a id="L-12-10" name="L-12-10"></a><span>7</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>CurvatureCDF</span><span> </span><span>&gt;</span><span> </span><span>t</span><span>  </span><span>//</span><span> </span><span>Create</span><span> </span><span>mask</span><span> </span><span>where</span><span> </span><span>curvature</span><span> </span><span>CDF</span><span> </span><span>exceeds</span><span> </span><span>threshold</span><span></span>
</span><span id="L-12-11"><a id="L-12-11" name="L-12-11"></a><span>8</span><span>:</span><span> </span><span>//</span><span> </span><span>Add</span><span> </span><span>boundary</span><span> </span><span>conditions</span><span> </span><span>(</span><span>keep</span><span> </span><span>top</span><span> </span><span>token</span><span>,</span><span> </span><span>mask</span><span> </span><span>after</span><span> </span><span>end</span><span> </span><span>of</span><span> </span><span>vocabulary</span><span>)</span><span></span>
</span><span id="L-12-12"><a id="L-12-12" name="L-12-12"></a><span>9</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>Concatenate</span><span>(</span><span>[</span><span>False</span><span>]</span><span>,</span><span> </span><span>mask</span><span>,</span><span> </span><span>[</span><span>True</span><span>]</span><span>)</span><span></span>
</span><span id="L-12-13"><a id="L-12-13" name="L-12-13"></a><span>10</span><span>:</span><span> </span><span>Lsorted</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span>  </span><span>//</span><span> </span><span>Filter</span><span> </span><span>out</span><span> </span><span>tokens</span><span> </span><span>in</span><span> </span><span>the</span><span> </span><span>"tail"</span><span></span>
</span><span id="L-12-14"><a id="L-12-14" name="L-12-14"></a><span>11</span><span>:</span><span> </span><span>L</span><span> </span><span>←</span><span> </span><span>Unsort</span><span>(</span><span>Lsorted</span><span>,</span><span> </span><span>Lidx</span><span>)</span><span>  </span><span>//</span><span> </span><span>Restore</span><span> </span><span>original</span><span> </span><span>ordering</span><span></span>
</span><span id="L-12-15"><a id="L-12-15" name="L-12-15"></a><span>12</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="eta-cutoff">Eta Cutoff<a href="#eta-cutoff" title="Permanent link"> </a></h3>
<p>Eta Cutoff adapts to how certain or uncertain the model is. Using the previous analogy again, let's say you have a stack of resumes. If there's one clearly outstanding candidate, you might be very selective about who else gets an interview. But if all candidates are similarly qualified, you might interview more of them. Eta cutoff works on this principle by looking at both the individual probability of each word and the overall entropy (uncertainty) of the distribution. When the model is very certain about what comes next (low entropy), the cutoff threshold becomes stricter, keeping only the most probable options. When the model is uncertain (high entropy), the threshold becomes more lenient.</p>
<p><strong>Technical</strong>:<br>
We create a dynamic probability threshold that considers both the <code>eta</code> parameter and the entropy of the distribution. We calculate logprobs and then compute the negative entropy (perplexity) of the distribution. Then, determine the threshold (<code>eps</code>) as the minimum between <code>eta</code> and a function of <code>eta</code> modified by entropy: <code>sqrt(eta) * exp(neg_entropy)</code>. This creates a dynamic threshold for us that decreases when entropy is high and vice versa.</p>
<p>Any token with a probability below this threshold is masked out.</p>
<p><strong>Algorithm</strong><br>
</p><div>

<table><tbody><tr><td><div><pre><span><a href="#L-13-1"> 1</a></span>
<span><a href="#L-13-2"> 2</a></span>
<span><a href="#L-13-3"> 3</a></span>
<span><a href="#L-13-4"> 4</a></span>
<span><a href="#L-13-5"> 5</a></span>
<span><a href="#L-13-6"> 6</a></span>
<span><a href="#L-13-7"> 7</a></span>
<span><a href="#L-13-8"> 8</a></span>
<span><a href="#L-13-9"> 9</a></span>
<span><a href="#L-13-10">10</a></span>
<span><a href="#L-13-11">11</a></span>
<span><a href="#L-13-12">12</a></span></pre></div></td><td><div><pre><span></span><span id="L-13-1"><a id="L-13-1" name="L-13-1"></a><span>Algorithm</span><span> </span><span>13</span><span> </span><span>Eta</span><span> </span><span>Cutoff</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-13-2"><a id="L-13-2" name="L-13-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>parameter</span><span> </span><span>η</span><span> </span><span>∈</span><span> </span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span><span></span>
</span><span id="L-13-3"><a id="L-13-3" name="L-13-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>dynamic</span><span> </span><span>low</span><span>-</span><span>probability</span><span> </span><span>filtering</span><span></span>
</span><span id="L-13-4"><a id="L-13-4" name="L-13-4"></a><span>1</span><span>:</span><span> </span><span>LN</span><span> </span><span>←</span><span> </span><span>LogSoftmax</span><span>(</span><span>L</span><span>)</span><span>  </span><span>//</span><span> </span><span>Log</span><span>-</span><span>normalized</span><span> </span><span>logits</span><span></span>
</span><span id="L-13-5"><a id="L-13-5" name="L-13-5"></a><span>2</span><span>:</span><span> </span><span>P</span><span> </span><span>←</span><span> </span><span>Exp</span><span>(</span><span>LN</span><span>)</span><span>  </span><span>//</span><span> </span><span>Convert</span><span> </span><span>to</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-13-6"><a id="L-13-6" name="L-13-6"></a><span>3</span><span>:</span><span> </span><span>H</span><span> </span><span>←</span><span> </span><span>-</span><span>Sum</span><span>(</span><span>P</span><span> </span><span>×</span><span> </span><span>LN</span><span>)</span><span>  </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>negative</span><span> </span><span>entropy</span><span> </span><span>(</span><span>higher</span><span> </span><span>means</span><span> </span><span>more</span><span> </span><span>certainty</span><span>)</span><span></span>
</span><span id="L-13-7"><a id="L-13-7" name="L-13-7"></a><span>4</span><span>:</span><span> </span><span>threshold</span><span> </span><span>←</span><span> </span><span>Min</span><span>(</span><span>η</span><span>,</span><span> </span><span>√</span><span>η</span><span> </span><span>×</span><span> </span><span>Exp</span><span>(</span><span>H</span><span>))</span><span>  </span><span>//</span><span> </span><span>Dynamic</span><span> </span><span>threshold</span><span> </span><span>based</span><span> </span><span>on</span><span> </span><span>certainty</span><span></span>
</span><span id="L-13-8"><a id="L-13-8" name="L-13-8"></a><span>5</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>P</span><span> </span><span>&lt;</span><span> </span><span>threshold</span><span>  </span><span>//</span><span> </span><span>Create</span><span> </span><span>mask</span><span> </span><span>for</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-13-9"><a id="L-13-9" name="L-13-9"></a><span>6</span><span>:</span><span> </span><span>topIdx</span><span> </span><span>←</span><span> </span><span>ArgMax</span><span>(</span><span>P</span><span>)</span><span>  </span><span>//</span><span> </span><span>Find</span><span> </span><span>index</span><span> </span><span>of</span><span> </span><span>highest</span><span> </span><span>probability</span><span> </span><span>token</span><span></span>
</span><span id="L-13-10"><a id="L-13-10" name="L-13-10"></a><span>7</span><span>:</span><span> </span><span>mask</span><span>[</span><span>topIdx</span><span>]</span><span> </span><span>←</span><span> </span><span>False</span><span>  </span><span>//</span><span> </span><span>Always</span><span> </span><span>keep</span><span> </span><span>highest</span><span> </span><span>probability</span><span> </span><span>token</span><span></span>
</span><span id="L-13-11"><a id="L-13-11" name="L-13-11"></a><span>8</span><span>:</span><span> </span><span>L</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span>  </span><span>//</span><span> </span><span>Filter</span><span> </span><span>out</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-13-12"><a id="L-13-12" name="L-13-12"></a><span>9</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="epsilon-cutoff">Epsilon Cutoff<a href="#epsilon-cutoff" title="Permanent link"> </a></h3>
<p>Like setting a minimum vote threshold in an election - any candidate who doesn't get at least a certain percentage of votes is eliminated. Basically a simpler version of filtering that removes all options below a fixed probability threshold.</p>
<p>It's a straightforward approach that removes extremely unlikely words without affecting the relative probabilities of the remaining options. It's useful for cleaning up the long tail of improbable tokens that might otherwise occasionally be selected due to random chance. Not as adaptive as other methods, but it's simple and predictable, and removes undesirables without dramatically changing the distribution's shape.</p>
<p><strong>Technical</strong>:<br>
A fixed probability threshold to eliminate unlikely tokens. Convert logits to probs using softmax, create a mask identifying tokens whose probabilities fall below the <code>epsilon</code> threshold. Any token below this threshold has its logits set to negative infinity.</p>
<p>its difference with eta cutoff is that epsilon uses a fixed threshold <em>regardless</em> of the distribution's properties. Both methods are ultimately a way to prune the vocabulary space without complex sorting operations, which may make inference slower.</p>
<p><strong>Algorithm</strong><br>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-14-1"><a id="L-14-1" name="L-14-1"></a><span>Algorithm</span><span> </span><span>14</span><span> </span><span>Epsilon</span><span> </span><span>Cutoff</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-14-2"><a id="L-14-2" name="L-14-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>threshold</span><span> </span><span>ε</span><span> </span><span>∈</span><span> </span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span><span></span>
</span><span id="L-14-3"><a id="L-14-3" name="L-14-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>fixed</span><span> </span><span>low</span><span>-</span><span>probability</span><span> </span><span>filtering</span><span></span>
</span><span id="L-14-4"><a id="L-14-4" name="L-14-4"></a><span>1</span><span>:</span><span> </span><span>P</span><span> </span><span>←</span><span> </span><span>Softmax</span><span>(</span><span>L</span><span>)</span><span>  </span><span>//</span><span> </span><span>Convert</span><span> </span><span>logits</span><span> </span><span>to</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-14-5"><a id="L-14-5" name="L-14-5"></a><span>2</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>P</span><span> </span><span>&lt;</span><span> </span><span>ε</span><span>  </span><span>//</span><span> </span><span>Create</span><span> </span><span>mask</span><span> </span><span>for</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>fixed</span><span> </span><span>threshold</span><span></span>
</span><span id="L-14-6"><a id="L-14-6" name="L-14-6"></a><span>3</span><span>:</span><span> </span><span>topIdx</span><span> </span><span>←</span><span> </span><span>ArgMax</span><span>(</span><span>P</span><span>)</span><span>  </span><span>//</span><span> </span><span>Find</span><span> </span><span>index</span><span> </span><span>of</span><span> </span><span>highest</span><span> </span><span>probability</span><span> </span><span>token</span><span></span>
</span><span id="L-14-7"><a id="L-14-7" name="L-14-7"></a><span>4</span><span>:</span><span> </span><span>mask</span><span>[</span><span>topIdx</span><span>]</span><span> </span><span>←</span><span> </span><span>False</span><span>  </span><span>//</span><span> </span><span>Always</span><span> </span><span>keep</span><span> </span><span>highest</span><span> </span><span>probability</span><span> </span><span>token</span><span></span>
</span><span id="L-14-8"><a id="L-14-8" name="L-14-8"></a><span>5</span><span>:</span><span> </span><span>L</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span>  </span><span>//</span><span> </span><span>Filter</span><span> </span><span>out</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-14-9"><a id="L-14-9" name="L-14-9"></a><span>6</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="locally-typical-sampling">Locally Typical Sampling<a href="#locally-typical-sampling" title="Permanent link"> </a></h3>
<p>This sampler is like focusing on the most "average" word choices rather than the most probable ones. Like you're writing a story and want it to sound natural but not predictable. Instead of always picking the most obvious words or completely random ones, you'd choose words that are neither too surprising nor too boring — words that feel "typically human."</p>
<p>The method itself works by measuring how much each potential word deviates from the average surprisal (unexpectedness) of all options. Words that are extremely predictable or extremely surprising are considered less "typical" than those with middle-of-the-road surprisal values. Typical sampling keeps tokens whose surprisal is close to the average, and might make the text feel more "balanced." It doesn't look at raw probabilities but at how predictable each word is within the context.</p>
<p><strong>Technical</strong>:<br>
Typical sampling selects tokens based on how close their "surprisal" (negative logprob) is to the average surprisal across all tokens. Calculate logprobs and then compute the negative entropy. For each token, calculate the absolute difference between its surprisal and this average — this is the "surprisal deviation" that measures how typical or atypical each token is. Tokens are then sorted by their surprisal deviation (most typical to least typical). The <code>typical-p</code> parameter determines what portion of the cumulative probability mass to keep.</p>
<p><strong>Algorithm</strong><br>
</p><div>

<table><tbody><tr><td><div><pre><span><a href="#L-15-1"> 1</a></span>
<span><a href="#L-15-2"> 2</a></span>
<span><a href="#L-15-3"> 3</a></span>
<span><a href="#L-15-4"> 4</a></span>
<span><a href="#L-15-5"> 5</a></span>
<span><a href="#L-15-6"> 6</a></span>
<span><a href="#L-15-7"> 7</a></span>
<span><a href="#L-15-8"> 8</a></span>
<span><a href="#L-15-9"> 9</a></span>
<span><a href="#L-15-10">10</a></span>
<span><a href="#L-15-11">11</a></span>
<span><a href="#L-15-12">12</a></span>
<span><a href="#L-15-13">13</a></span>
<span><a href="#L-15-14">14</a></span>
<span><a href="#L-15-15">15</a></span></pre></div></td><td><div><pre><span></span><span id="L-15-1"><a id="L-15-1" name="L-15-1"></a><span>Algorithm</span><span> </span><span>15</span><span> </span><span>Locally</span><span> </span><span>Typical</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-15-2"><a id="L-15-2" name="L-15-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>threshold</span><span> </span><span>p</span><span> </span><span>∈</span><span> </span><span>[</span><span>0,1</span><span>]</span><span></span>
</span><span id="L-15-3"><a id="L-15-3" name="L-15-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>tokens</span><span> </span><span>furthest</span><span> </span><span>from</span><span> </span><span>mean</span><span> </span><span>surprisal</span><span> </span><span>filtered</span><span> </span><span>out</span><span></span>
</span><span id="L-15-4"><a id="L-15-4" name="L-15-4"></a><span>1</span><span>:</span><span> </span><span>LN</span><span> </span><span>←</span><span> </span><span>LogSoftmax</span><span>(</span><span>L</span><span>)</span><span>  </span><span>//</span><span> </span><span>Log</span><span>-</span><span>normalized</span><span> </span><span>logits</span><span></span>
</span><span id="L-15-5"><a id="L-15-5" name="L-15-5"></a><span>2</span><span>:</span><span> </span><span>P</span><span> </span><span>←</span><span> </span><span>Exp</span><span>(</span><span>LN</span><span>)</span><span>  </span><span>//</span><span> </span><span>Convert</span><span> </span><span>to</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-15-6"><a id="L-15-6" name="L-15-6"></a><span>3</span><span>:</span><span> </span><span>H</span><span> </span><span>←</span><span> </span><span>Sum</span><span>(</span><span>P</span><span> </span><span>×</span><span> </span><span>LN</span><span>)</span><span>  </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>negative</span><span> </span><span>entropy</span><span> </span><span>(</span><span>expected</span><span> </span><span>surprisal</span><span>)</span><span></span>
</span><span id="L-15-7"><a id="L-15-7" name="L-15-7"></a><span>4</span><span>:</span><span> </span><span>S</span><span> </span><span>←</span><span> </span><span>|</span><span>H</span><span> </span><span>-</span><span> </span><span>LN</span><span>|</span><span>  </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>surprisal</span><span> </span><span>deviation</span><span> </span><span>for</span><span> </span><span>each</span><span> </span><span>token</span><span></span>
</span><span id="L-15-8"><a id="L-15-8" name="L-15-8"></a><span>5</span><span>:</span><span> </span><span>Sidx</span><span> </span><span>←</span><span> </span><span>ArgsortAscending</span><span>(</span><span>S</span><span>)</span><span>  </span><span>//</span><span> </span><span>Sort</span><span> </span><span>indices</span><span> </span><span>by</span><span> </span><span>surprisal</span><span> </span><span>deviation</span><span> </span><span>(</span><span>most</span><span> </span><span>typical</span><span> </span><span>first</span><span>)</span><span></span>
</span><span id="L-15-9"><a id="L-15-9" name="L-15-9"></a><span>6</span><span>:</span><span> </span><span>Preordered</span><span> </span><span>←</span><span> </span><span>GatherByIndices</span><span>(</span><span>P</span><span>,</span><span> </span><span>Sidx</span><span>)</span><span>  </span><span>//</span><span> </span><span>Reorder</span><span> </span><span>probabilities</span><span> </span><span>by</span><span> </span><span>typicality</span><span></span>
</span><span id="L-15-10"><a id="L-15-10" name="L-15-10"></a><span>7</span><span>:</span><span> </span><span>Pcum</span><span> </span><span>←</span><span> </span><span>CumulativeSum</span><span>(</span><span>Preordered</span><span>)</span><span>  </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>cumulative</span><span> </span><span>sum</span><span> </span><span>of</span><span> </span><span>reordered</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-15-11"><a id="L-15-11" name="L-15-11"></a><span>8</span><span>:</span><span> </span><span>maskSorted</span><span> </span><span>←</span><span> </span><span>Pcum</span><span> </span><span>≥</span><span> </span><span>p</span><span>  </span><span>//</span><span> </span><span>Create</span><span> </span><span>mask</span><span> </span><span>where</span><span> </span><span>cumulative</span><span> </span><span>typical</span><span> </span><span>probability</span><span> </span><span>exceeds</span><span> </span><span>threshold</span><span></span>
</span><span id="L-15-12"><a id="L-15-12" name="L-15-12"></a><span>9</span><span>:</span><span> </span><span>maskSorted</span><span>[</span><span>0:MinTokensToKeep</span><span>]</span><span> </span><span>←</span><span> </span><span>False</span><span>  </span><span>//</span><span> </span><span>Always</span><span> </span><span>keep</span><span> </span><span>at</span><span> </span><span>least</span><span> </span><span>MinTokensToKeep</span><span> </span><span>tokens</span><span></span>
</span><span id="L-15-13"><a id="L-15-13" name="L-15-13"></a><span>10</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>ScatterByIndices</span><span>(</span><span>maskSorted</span><span>,</span><span> </span><span>Sidx</span><span>)</span><span>  </span><span>//</span><span> </span><span>Restore</span><span> </span><span>original</span><span> </span><span>ordering</span><span> </span><span>of</span><span> </span><span>mask</span><span></span>
</span><span id="L-15-14"><a id="L-15-14" name="L-15-14"></a><span>11</span><span>:</span><span> </span><span>L</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span>  </span><span>//</span><span> </span><span>Filter</span><span> </span><span>out</span><span> </span><span>tokens</span><span> </span><span>that</span><span> </span><span>are</span><span> </span><span>least</span><span> </span><span>typical</span><span></span>
</span><span id="L-15-15"><a id="L-15-15" name="L-15-15"></a><span>12</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="quadratic-sampling">Quadratic Sampling<a href="#quadratic-sampling" title="Permanent link"> </a></h3>
<p>Whereas most sampling techniques either filter out tokens (like Top-K) or simply adjust randomness (like Temperature), Quadratic Sampling takes a more nuanced approach by reshaping the entire probability distribution using mathematical transformations.</p>
<p>Quadratic works by applying a mathematical transformation that adjusts the gap between high and low prob tokens using quadratic and cubic equations (it was actually called Cubic sampling before the quadratic function was added later). The transformation is centered around the highest-scoring token, with two key params controlling the effect: the smoothing factor determines the overall strength of the adjustment, while the smoothing curve controls the shape of the transformation. It can essentially make the highest probability tokens more prominent while gently supressing the lower ones, or it can flatten the distribution to give more tokens a chance.</p>
<p><strong>Technical</strong>:<br>
Quadratic transforms the logits using a combination of quadratic and cubic terms. We calculate two coefficients: <code>k</code> and <code>s</code>, derived from the smoothing factor and curve params. Those coefficients control the balance between quadratic and cubic terms in the transformation. For each affected sequence, we identify the maximum logit value and calculate the difference between each logit and this maximum. Then, apply a transformation that adjusts this based on the quadratic equation: <code>diff -= diff²(s·diff - k)</code>. This creates a non-linear adjustment that varies depending on how far each logit is from the maximum. Because of that, the highest probability token is unchanged while others are adjusted relative to it. When <code>s</code> is positive, the cubic term increases the contrast between high and low logits (making the distribution more peaked). When <code>k</code> is positive, the quadratic term works to flatten the distribution.</p>
<p><strong>Algorithm</strong><br>
</p><div>

<table><tbody><tr><td><div><pre><span><a href="#L-16-1"> 1</a></span>
<span><a href="#L-16-2"> 2</a></span>
<span><a href="#L-16-3"> 3</a></span>
<span><a href="#L-16-4"> 4</a></span>
<span><a href="#L-16-5"> 5</a></span>
<span><a href="#L-16-6"> 6</a></span>
<span><a href="#L-16-7"> 7</a></span>
<span><a href="#L-16-8"> 8</a></span>
<span><a href="#L-16-9"> 9</a></span>
<span><a href="#L-16-10">10</a></span>
<span><a href="#L-16-11">11</a></span>
<span><a href="#L-16-12">12</a></span>
<span><a href="#L-16-13">13</a></span>
<span><a href="#L-16-14">14</a></span>
<span><a href="#L-16-15">15</a></span>
<span><a href="#L-16-16">16</a></span>
<span><a href="#L-16-17">17</a></span>
<span><a href="#L-16-18">18</a></span>
<span><a href="#L-16-19">19</a></span>
<span><a href="#L-16-20">20</a></span>
<span><a href="#L-16-21">21</a></span>
<span><a href="#L-16-22">22</a></span>
<span><a href="#L-16-23">23</a></span>
<span><a href="#L-16-24">24</a></span>
<span><a href="#L-16-25">25</a></span>
<span><a href="#L-16-26">26</a></span>
<span><a href="#L-16-27">27</a></span></pre></div></td><td><div><pre><span></span><span id="L-16-1"><a id="L-16-1" name="L-16-1"></a><span>Algorithm</span><span> </span><span>16</span><span> </span><span>Quadratic</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-16-2"><a id="L-16-2" name="L-16-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>smoothing</span><span> </span><span>factor</span><span> </span><span>α</span><span>,</span><span> </span><span>smoothing</span><span> </span><span>curve</span><span> </span><span>β</span><span></span>
</span><span id="L-16-3"><a id="L-16-3" name="L-16-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>non</span><span>-</span><span>linear</span><span> </span><span>transformation</span><span> </span><span>applied</span><span></span>
</span><span id="L-16-4"><a id="L-16-4" name="L-16-4"></a><span>1</span><span>:</span><span> </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>coefficients</span><span> </span><span>for</span><span> </span><span>quadratic</span><span> </span><span>and</span><span> </span><span>cubic</span><span> </span><span>terms</span><span></span>
</span><span id="L-16-5"><a id="L-16-5" name="L-16-5"></a><span>2</span><span>:</span><span> </span><span>k</span><span> </span><span>←</span><span> </span><span>α</span><span> </span><span>×</span><span> </span><span>(</span><span>3</span><span> </span><span>-</span><span> </span><span>β</span><span>)</span><span> </span><span>/</span><span> </span><span>2</span><span>  </span><span>//</span><span> </span><span>Quadratic</span><span> </span><span>coefficient</span><span></span>
</span><span id="L-16-6"><a id="L-16-6" name="L-16-6"></a><span>3</span><span>:</span><span> </span><span>s</span><span> </span><span>←</span><span> </span><span>α</span><span> </span><span>×</span><span> </span><span>(</span><span>β</span><span> </span><span>-</span><span> </span><span>1</span><span>)</span><span> </span><span>/</span><span> </span><span>2</span><span>  </span><span>//</span><span> </span><span>Cubic</span><span> </span><span>coefficient</span><span></span>
</span><span id="L-16-7"><a id="L-16-7" name="L-16-7"></a><span>4</span><span>:</span><span></span>
</span><span id="L-16-8"><a id="L-16-8" name="L-16-8"></a><span>5</span><span>:</span><span> </span><span>//</span><span> </span><span>Apply</span><span> </span><span>transformation</span><span> </span><span>only</span><span> </span><span>where</span><span> </span><span>smoothing</span><span> </span><span>factor</span><span> </span><span>is</span><span> </span><span>non</span><span>-</span><span>zero</span><span></span>
</span><span id="L-16-9"><a id="L-16-9" name="L-16-9"></a><span>6</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>(</span><span>α</span><span> </span><span>≠</span><span> </span><span>0</span><span>)</span><span></span>
</span><span id="L-16-10"><a id="L-16-10" name="L-16-10"></a><span>7</span><span>:</span><span> </span><span>if</span><span> </span><span>not</span><span> </span><span>any</span><span>(</span><span>mask</span><span>)</span><span> </span><span>then</span><span> </span><span>return</span><span> </span><span>L</span><span> </span><span>end</span><span> </span><span>if</span><span></span>
</span><span id="L-16-11"><a id="L-16-11" name="L-16-11"></a><span>8</span><span>:</span><span></span>
</span><span id="L-16-12"><a id="L-16-12" name="L-16-12"></a><span>9</span><span>:</span><span> </span><span>Ltarget</span><span> </span><span>←</span><span> </span><span>L</span><span>[</span><span>mask</span><span>]</span><span>  </span><span>//</span><span> </span><span>Select</span><span> </span><span>logits</span><span> </span><span>to</span><span> </span><span>transform</span><span></span>
</span><span id="L-16-13"><a id="L-16-13" name="L-16-13"></a><span>10</span><span>:</span><span> </span><span>Lmax</span><span> </span><span>←</span><span> </span><span>Max</span><span>(</span><span>Ltarget</span><span>)</span><span>  </span><span>//</span><span> </span><span>Find</span><span> </span><span>maximum</span><span> </span><span>logit</span><span> </span><span>value</span><span></span>
</span><span id="L-16-14"><a id="L-16-14" name="L-16-14"></a><span>11</span><span>:</span><span> </span>
</span><span id="L-16-15"><a id="L-16-15" name="L-16-15"></a><span>12</span><span>:</span><span> </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>differences</span><span> </span><span>from</span><span> </span><span>maximum</span><span> </span><span>and</span><span> </span><span>apply</span><span> </span><span>transformation</span><span></span>
</span><span id="L-16-16"><a id="L-16-16" name="L-16-16"></a><span>13</span><span>:</span><span> </span><span>Δ</span><span> </span><span>←</span><span> </span><span>Ltarget</span><span> </span><span>-</span><span> </span><span>Lmax</span><span>  </span><span>//</span><span> </span><span>Distance</span><span> </span><span>from</span><span> </span><span>maximum</span><span> </span><span>logit</span><span></span>
</span><span id="L-16-17"><a id="L-16-17" name="L-16-17"></a><span>14</span><span>:</span><span></span>
</span><span id="L-16-18"><a id="L-16-18" name="L-16-18"></a><span>15</span><span>:</span><span> </span><span>//</span><span> </span><span>Apply</span><span> </span><span>quadratic</span><span> </span><span>transformation</span><span>:</span><span> </span><span>Δ</span><span> </span><span>-=</span><span> </span><span>Δ²</span><span>(</span><span>s</span><span>·</span><span>Δ</span><span> </span><span>-</span><span> </span><span>k</span><span>)</span><span></span>
</span><span id="L-16-19"><a id="L-16-19" name="L-16-19"></a><span>16</span><span>:</span><span> </span><span>Δnew</span><span> </span><span>←</span><span> </span><span>Δ</span><span> </span><span>-</span><span> </span><span>(</span><span>Δ²</span><span> </span><span>×</span><span> </span><span>(</span><span>s</span><span> </span><span>×</span><span> </span><span>Δ</span><span> </span><span>-</span><span> </span><span>k</span><span>))</span><span></span>
</span><span id="L-16-20"><a id="L-16-20" name="L-16-20"></a><span>17</span><span>:</span><span></span>
</span><span id="L-16-21"><a id="L-16-21" name="L-16-21"></a><span>18</span><span>:</span><span> </span><span>//</span><span> </span><span>Handle</span><span> </span><span>potential</span><span> </span><span>numerical</span><span> </span><span>issues</span><span></span>
</span><span id="L-16-22"><a id="L-16-22" name="L-16-22"></a><span>19</span><span>:</span><span> </span><span>Δnew</span><span>[</span><span>isNaN(Δnew)</span><span>]</span><span> </span><span>←</span><span> </span><span>0</span><span></span>
</span><span id="L-16-23"><a id="L-16-23" name="L-16-23"></a><span>20</span><span>:</span><span></span>
</span><span id="L-16-24"><a id="L-16-24" name="L-16-24"></a><span>21</span><span>:</span><span> </span><span>//</span><span> </span><span>Apply</span><span> </span><span>transformed</span><span> </span><span>differences</span><span></span>
</span><span id="L-16-25"><a id="L-16-25" name="L-16-25"></a><span>22</span><span>:</span><span> </span><span>L</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>Ltarget</span><span> </span><span>-</span><span> </span><span>Δnew</span><span></span>
</span><span id="L-16-26"><a id="L-16-26" name="L-16-26"></a><span>23</span><span>:</span><span></span>
</span><span id="L-16-27"><a id="L-16-27" name="L-16-27"></a><span>24</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="mirostat-sampling">Mirostat Sampling<a href="#mirostat-sampling" title="Permanent link"> </a></h3>
<p>Mirostat is like an adaptive thermostat for text generation that automatically adjusts to maintain a consistent level of "surprise" or unpredictability. Just as a thermostat keeps your room temperature stable by turning heating on and off, Mirostat keeps text generation at a consistent level of unpredictability by dynamically adjusting how conservative or creative the sampling is. It works by measuring the "surprisal" (how unexpected each token is) and comparing it to a target value. If recent text has been to predictable, Mirostat allows more surprising tokens to be selected. If it's been too chaotic, Mirostat tightens the constraints to focus on more predictable ones. This creates a feedback loop that maintains a consistent perplexity throughput the generated text. </p>
<p>Mirostat's main selling point is that it's self-regulating, adapting to different contexts without requiring manual parameter adjustment. </p>
<p><strong>Technical</strong>:<br>
Mirostat works as an adaptive token filtering method with a feedback control loop to maintain consistent perplexity.</p>
<p>The core algorithm has two main phases:</p>
<ol>
<li><strong>Filtering phase</strong>:<br>
</li>
</ol>
<ul>
<li>Calculate the "surprisal" (negative logprob, converted to bits) for each potential token.</li>
<li>Use an adaptive threshold ("mu") to filter out tokens that are too surprising.</li>
<li>Ensure at least one token (the most likely) remains available for selection.</li>
<li>Effectively create a dynamic Top-K filter where K varies based on the current <code>mu</code> value.</li>
</ul>
<ol start="2">
<li><strong>Update phase</strong>:<br>
</li>
</ol>
<ul>
<li>After a token is selected, calculate the actual surprisal of the chosen token.</li>
<li>Compare this surprisal to a target value ("tau").</li>
<li>Update the <code>mu</code> threshold based on the difference between actual and target surprisal.</li>
<li>An <code>eta</code> parameter controls how quickly <code>mu</code> adjusts (learning rate).</li>
</ul>
<p>The mathematical foundation is a control system using the equation <code>mu_{t+1} = mu_t - η × (surprisal_t - τ)</code></p>
<p>Where:</p>
<ul>
<li><code>mu</code> is the adaptively changing surprisal threshold</li>
<li><code>τ</code> (tau) is the target surprisal (perplexity)</li>
<li><code>η</code> (eta) is the learning rate</li>
<li><code>surprisal_t</code> is the surprisal of the chosen token</li>
</ul>
<p>This gives us a negative feedback loop that pushes generation towards the target surprisal level. If tokens are too surprising, <code>mu</code> decreases. If tokens are too precitable, <code>mu</code> increases. This sampling method is likely the most sophisticated and complex to exist.</p>
<p><strong>Algorithm</strong><br>
</p><div>

<table><tbody><tr><td><div><pre><span><a href="#L-17-1"> 1</a></span>
<span><a href="#L-17-2"> 2</a></span>
<span><a href="#L-17-3"> 3</a></span>
<span><a href="#L-17-4"> 4</a></span>
<span><a href="#L-17-5"> 5</a></span>
<span><a href="#L-17-6"> 6</a></span>
<span><a href="#L-17-7"> 7</a></span>
<span><a href="#L-17-8"> 8</a></span>
<span><a href="#L-17-9"> 9</a></span>
<span><a href="#L-17-10">10</a></span>
<span><a href="#L-17-11">11</a></span>
<span><a href="#L-17-12">12</a></span>
<span><a href="#L-17-13">13</a></span>
<span><a href="#L-17-14">14</a></span>
<span><a href="#L-17-15">15</a></span>
<span><a href="#L-17-16">16</a></span>
<span><a href="#L-17-17">17</a></span>
<span><a href="#L-17-18">18</a></span>
<span><a href="#L-17-19">19</a></span>
<span><a href="#L-17-20">20</a></span></pre></div></td><td><div><pre><span></span><span id="L-17-1"><a id="L-17-1" name="L-17-1"></a><span>Algorithm</span><span> </span><span>17</span><span> </span><span>Mirostat</span><span> </span><span>v2</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-17-2"><a id="L-17-2" name="L-17-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>mu</span><span> </span><span>parameter</span><span> </span><span>(</span><span>surprisal</span><span> </span><span>threshold</span><span>),</span><span> </span>
</span><span id="L-17-3"><a id="L-17-3" name="L-17-3"></a><span>          </span><span>tau</span><span> </span><span>parameter</span><span> </span><span>(</span><span>target</span><span> </span><span>surprisal</span><span>),</span><span> </span><span>eta</span><span> </span><span>parameter</span><span> </span><span>(</span><span>learning</span><span> </span><span>rate</span><span>)</span><span></span>
</span><span id="L-17-4"><a id="L-17-4" name="L-17-4"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>dynamic</span><span> </span><span>filtering</span><span> </span><span>based</span><span> </span><span>on</span><span> </span><span>surprisal</span><span> </span><span>control</span><span></span>
</span><span id="L-17-5"><a id="L-17-5" name="L-17-5"></a>
</span><span id="L-17-6"><a id="L-17-6" name="L-17-6"></a><span>//</span><span> </span><span>Phase</span><span> </span><span>1</span><span>:</span><span> </span><span>Filter</span><span> </span><span>tokens</span><span> </span><span>based</span><span> </span><span>on</span><span> </span><span>current</span><span> </span><span>surprisal</span><span> </span><span>threshold</span><span></span>
</span><span id="L-17-7"><a id="L-17-7" name="L-17-7"></a><span>1</span><span>:</span><span> </span><span>LN</span><span> </span><span>←</span><span> </span><span>LogSoftmax</span><span>(</span><span>L</span><span>)</span><span>  </span><span>//</span><span> </span><span>Log</span><span>-</span><span>normalized</span><span> </span><span>logits</span><span></span>
</span><span id="L-17-8"><a id="L-17-8" name="L-17-8"></a><span>2</span><span>:</span><span> </span><span>S</span><span> </span><span>←</span><span> </span><span>-</span><span>LN</span><span> </span><span>/</span><span> </span><span>log</span><span>(</span><span>2</span><span>)</span><span>  </span><span>//</span><span> </span><span>Convert</span><span> </span><span>to</span><span> </span><span>surprisal</span><span> </span><span>in</span><span> </span><span>bits</span><span></span>
</span><span id="L-17-9"><a id="L-17-9" name="L-17-9"></a><span>3</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>S</span><span> </span><span>&gt;</span><span> </span><span>μ</span><span>  </span><span>//</span><span> </span><span>Create</span><span> </span><span>mask</span><span> </span><span>for</span><span> </span><span>tokens</span><span> </span><span>with</span><span> </span><span>surprisal</span><span> </span><span>exceeding</span><span> </span><span>threshold</span><span></span>
</span><span id="L-17-10"><a id="L-17-10" name="L-17-10"></a><span>4</span><span>:</span><span> </span><span>topIdx</span><span> </span><span>←</span><span> </span><span>ArgMax</span><span>(</span><span>L</span><span>)</span><span>  </span><span>//</span><span> </span><span>Find</span><span> </span><span>index</span><span> </span><span>of</span><span> </span><span>highest</span><span> </span><span>probability</span><span> </span><span>token</span><span></span>
</span><span id="L-17-11"><a id="L-17-11" name="L-17-11"></a><span>5</span><span>:</span><span> </span><span>mask</span><span>[</span><span>topIdx</span><span>]</span><span> </span><span>←</span><span> </span><span>False</span><span>  </span><span>//</span><span> </span><span>Always</span><span> </span><span>keep</span><span> </span><span>highest</span><span> </span><span>probability</span><span> </span><span>token</span><span></span>
</span><span id="L-17-12"><a id="L-17-12" name="L-17-12"></a><span>6</span><span>:</span><span> </span><span>L</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span>  </span><span>//</span><span> </span><span>Filter</span><span> </span><span>out</span><span> </span><span>tokens</span><span> </span><span>above</span><span> </span><span>surprisal</span><span> </span><span>threshold</span><span></span>
</span><span id="L-17-13"><a id="L-17-13" name="L-17-13"></a><span>7</span><span>:</span><span> </span><span>//</span><span> </span><span>At</span><span> </span><span>this</span><span> </span><span>point</span><span>,</span><span> </span><span>token</span><span> </span><span>selection</span><span> </span><span>would</span><span> </span><span>occur</span><span> </span><span>using</span><span> </span><span>the</span><span> </span><span>filtered</span><span> </span><span>logits</span><span></span>
</span><span id="L-17-14"><a id="L-17-14" name="L-17-14"></a>
</span><span id="L-17-15"><a id="L-17-15" name="L-17-15"></a><span>//</span><span> </span><span>Phase</span><span> </span><span>2</span><span>:</span><span> </span><span>Update</span><span> </span><span>mu</span><span> </span><span>based</span><span> </span><span>on</span><span> </span><span>selected</span><span> </span><span>token</span><span> </span><span>(</span><span>performed</span><span> </span><span>after</span><span> </span><span>token</span><span> </span><span>selection</span><span>)</span><span></span>
</span><span id="L-17-16"><a id="L-17-16" name="L-17-16"></a><span>8</span><span>:</span><span> </span><span>t</span><span> </span><span>←</span><span> </span><span>SelectedToken</span><span>  </span><span>//</span><span> </span><span>Token</span><span> </span><span>chosen</span><span> </span><span>after</span><span> </span><span>filtering</span><span></span>
</span><span id="L-17-17"><a id="L-17-17" name="L-17-17"></a><span>9</span><span>:</span><span> </span><span>St</span><span> </span><span>←</span><span> </span><span>-</span><span>LogProb</span><span>(</span><span>t</span><span>)</span><span> </span><span>/</span><span> </span><span>log</span><span>(</span><span>2</span><span>)</span><span>  </span><span>//</span><span> </span><span>Surprisal</span><span> </span><span>of</span><span> </span><span>selected</span><span> </span><span>token</span><span> </span><span>in</span><span> </span><span>bits</span><span></span>
</span><span id="L-17-18"><a id="L-17-18" name="L-17-18"></a><span>10</span><span>:</span><span> </span><span>μnew</span><span> </span><span>←</span><span> </span><span>μ</span><span> </span><span>-</span><span> </span><span>η</span><span> </span><span>×</span><span> </span><span>(</span><span>St</span><span> </span><span>-</span><span> </span><span>τ</span><span>)</span><span>  </span><span>//</span><span> </span><span>Update</span><span> </span><span>mu</span><span> </span><span>using</span><span> </span><span>control</span><span> </span><span>equation</span><span></span>
</span><span id="L-17-19"><a id="L-17-19" name="L-17-19"></a><span>11</span><span>:</span><span> </span><span>StoreForNextStep</span><span>(</span><span>μnew</span><span>)</span><span>  </span><span>//</span><span> </span><span>Save</span><span> </span><span>updated</span><span> </span><span>mu</span><span> </span><span>for</span><span> </span><span>next</span><span> </span><span>token</span><span> </span><span>generation</span><span></span>
</span><span id="L-17-20"><a id="L-17-20" name="L-17-20"></a><span>12</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="dynamic-temperature-sampling">Dynamic Temperature Sampling<a href="#dynamic-temperature-sampling" title="Permanent link"> </a></h3>
<p>This method adjusts the temperature value based on the entropy (uncertainty) of the current token distribution. When the model is very certain about what comes next (low entropy), it uses a higher temperature to introduce more diversity. When the model is already uncertain (high entropy), it uses a lower temperature to keep the output more focused and coherent. You simply set a minimum and maximum temperature, then an exponent that controls how quickly it transitions between them based on the context's entropy, and the sampler will do the rest.</p>
<p><strong>Technical</strong>:<br>
Dynatemp automatically adjusts the temp param based on the normalized entropy of the prob distribution. We calculate the probability distribution and compute its entropy, which measures how spread out or concentrated the probabilities are.</p>
<p>This entropy is then normalized by dividing the maximum possible entropy (log of the number of tokens with finite logits). This gives us a value between 0 and 1, where 0 represents minimum entropy and 1 represents maximum entropy. The normalized entropy is raised to a power controlled by the exponent parameter, allowing for non-linear mapping between entropy and temperature. This transformed value is then used to interpolate between the min and max temp settings:</p>
<p><code>temperature = min_temp + (max_temp - min_temp) * (normalized_entropy ^ exponent)</code></p>
<p><strong>Algorithm</strong><br>
</p><div>

<table><tbody><tr><td><div><pre><span><a href="#L-18-1"> 1</a></span>
<span><a href="#L-18-2"> 2</a></span>
<span><a href="#L-18-3"> 3</a></span>
<span><a href="#L-18-4"> 4</a></span>
<span><a href="#L-18-5"> 5</a></span>
<span><a href="#L-18-6"> 6</a></span>
<span><a href="#L-18-7"> 7</a></span>
<span><a href="#L-18-8"> 8</a></span>
<span><a href="#L-18-9"> 9</a></span>
<span><a href="#L-18-10">10</a></span>
<span><a href="#L-18-11">11</a></span>
<span><a href="#L-18-12">12</a></span></pre></div></td><td><div><pre><span></span><span id="L-18-1"><a id="L-18-1" name="L-18-1"></a><span>Algorithm</span> <span>18</span> <span>Dynamic</span> <span>Temperature</span> <span>Sampling</span>
</span><span id="L-18-2"><a id="L-18-2" name="L-18-2"></a><span>Required</span>: <span>Logits</span> <span>tensor</span> <span>L</span>, <span>minimum</span> <span>temperature</span> <span>Tmin</span>, <span>maximum</span> <span>temperature</span> <span>Tmax</span>, <span>exponent</span> α
</span><span id="L-18-3"><a id="L-18-3" name="L-18-3"></a><span>Output</span>: <span>Temperature</span> <span>value</span> <span>T</span> <span>dynamically</span> <span>calculated</span> <span>based</span> <span>on</span> <span>distribution</span> <span>entropy</span>
</span><span id="L-18-4"><a id="L-18-4" name="L-18-4"></a><span>1</span>: <span>P</span> ← <span>softmax</span><span>(</span><span>L</span><span>)</span>  <span>//</span> <span>Calculate</span> <span>probability</span> <span>distribution</span>
</span><span id="L-18-5"><a id="L-18-5" name="L-18-5"></a><span>2</span>: <span>H</span> ← <span>-</span>∑<span>(</span><span>P</span> · <span>log</span><span>(</span><span>P</span><span>))</span>  <span>//</span> <span>Calculate</span> <span>entropy</span> <span>of</span> <span>the</span> <span>distribution</span>
</span><span id="L-18-6"><a id="L-18-6" name="L-18-6"></a><span>3</span>: <span>Vvalid</span> ← <span>|</span>{<span>l</span> ∈ <span>L</span> <span>|</span> <span>l</span> <span>&gt;</span> <span>-</span>∞}<span>|</span>  <span>//</span> <span>Count</span> <span>tokens</span> <span>with</span> <span>finite</span> <span>logits</span>
</span><span id="L-18-7"><a id="L-18-7" name="L-18-7"></a><span>4</span>: <span>Hmax</span> ← <span>log</span><span>(</span><span>Vvalid</span><span>)</span>  <span>//</span> <span>Maximum</span> <span>possible</span> <span>entropy</span>
</span><span id="L-18-8"><a id="L-18-8" name="L-18-8"></a><span>5</span>: <span>Hnorm</span> ← <span>H</span> <span>/</span> <span>Hmax</span>  <span>//</span> <span>Normalize</span> <span>entropy</span> <span>to</span> <span>range</span> [<span>0</span>,<span>1</span>]
</span><span id="L-18-9"><a id="L-18-9" name="L-18-9"></a><span>6</span>: <span>Htrans</span> ← <span>Hnorm</span><span>^</span>α  <span>//</span> <span>Apply</span> <span>non</span><span>-</span><span>linear</span> <span>transformation</span> <span>based</span> <span>on</span> <span>exponent</span>
</span><span id="L-18-10"><a id="L-18-10" name="L-18-10"></a><span>7</span>: <span>T</span> ← <span>Tmin</span> <span>+</span> <span>(</span><span>Tmax</span> <span>-</span> <span>Tmin</span><span>)</span> · <span>Htrans</span>  <span>//</span> <span>Interpolate</span> <span>between</span> <span>min</span> <span>and</span> <span>max</span> <span>temperature</span>
</span><span id="L-18-11"><a id="L-18-11" name="L-18-11"></a><span>8</span>: <span>T</span> ← <span>max</span><span>(</span><span>T</span>, ε<span>)</span>  <span>//</span> <span>Ensure</span> <span>temperature</span> <span>is</span> <span>within</span> <span>valid</span> <span>bounds</span>
</span><span id="L-18-12"><a id="L-18-12" name="L-18-12"></a><span>9</span>: <span>return</span> <span>T</span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="beam-search">Beam Search<a href="#beam-search" title="Permanent link"> </a></h3>
<p>Beam search is like exploring multiple paths on a map simultaneously to find the best overall route. Instead of making a single choice at each step and hoping it leads to a good outcome (as in greedy, i.e. 0 temperature, or random sampling, i.e. every other sampling method described here), beam search maintains multiple "parallel universes" of text generation and continuously evaluates which ones are most promising. Imagine you're writing a story and reach a point where you could continue in several different ways. Rather than committing to just one direction, beam search would explore, say, the top 5 continuations simultaneously. At the next decision point, it'd consider possible continuations for all 5 paths, then keep only the 5 best paths overall. This process repeats until the generation is complete.</p>
<p>Beam search isn't used much anymore, because it's expensive, and there are better sampling methods out there.</p>
<p><strong>Technical</strong>:<br>
Beam search maintains and expands a fixed number of candidate sequences (the beam width) at each decoding step, selecting the overall most probable sequences.<br>
We implement it in two phases: the <strong>prompt phase</strong>, and the <strong>generation phase</strong>. When processing the initial prompt, we simply select the top <code>2k</code> (twice the beam width) most likely tokens from the probability distribution. This creates our initial set of candidate sequences. Then in the next phase, during ongoing generation, we take each of the current beam sequences and their accumulated logprobs, add the logprobs of each possible next token to create combined scores, flatten these scores across all beam sequences and vocabs, select the top <code>2k</code> combinations of <code>(parent sequence, next token)</code>, and then finally extract the parent indices and next token IDs from these combinations. </p>
<p>Typically, we sample twice the beam width candidates at each step to ensure enough valid candidates remain after filtering out completed sequence. This approach was popularized by Google's Tensor2Tensor library and is now the standard. Mathematically, beam search chooses the <code>k</code> sequences with the highest scores, where the score for a sequence is the sum of logprobs of all tokens in that sequence. This maximizes the joint probability of the entire sequence rather than making locally optimal choices at each step, which is what greedy decoding does. </p>
<p>Unlike random sampling, beam search is deterministic and will always produce the same output given the same input (assuming ties are broken consistently).</p>
<p><strong>Algorithm</strong><br>
</p><div>

<table><tbody><tr><td><div><pre><span><a href="#L-19-1"> 1</a></span>
<span><a href="#L-19-2"> 2</a></span>
<span><a href="#L-19-3"> 3</a></span>
<span><a href="#L-19-4"> 4</a></span>
<span><a href="#L-19-5"> 5</a></span>
<span><a href="#L-19-6"> 6</a></span>
<span><a href="#L-19-7"> 7</a></span>
<span><a href="#L-19-8"> 8</a></span>
<span><a href="#L-19-9"> 9</a></span>
<span><a href="#L-19-10">10</a></span>
<span><a href="#L-19-11">11</a></span>
<span><a href="#L-19-12">12</a></span>
<span><a href="#L-19-13">13</a></span>
<span><a href="#L-19-14">14</a></span>
<span><a href="#L-19-15">15</a></span>
<span><a href="#L-19-16">16</a></span>
<span><a href="#L-19-17">17</a></span>
<span><a href="#L-19-18">18</a></span>
<span><a href="#L-19-19">19</a></span>
<span><a href="#L-19-20">20</a></span>
<span><a href="#L-19-21">21</a></span>
<span><a href="#L-19-22">22</a></span>
<span><a href="#L-19-23">23</a></span>
<span><a href="#L-19-24">24</a></span>
<span><a href="#L-19-25">25</a></span>
<span><a href="#L-19-26">26</a></span>
<span><a href="#L-19-27">27</a></span>
<span><a href="#L-19-28">28</a></span>
<span><a href="#L-19-29">29</a></span>
<span><a href="#L-19-30">30</a></span>
<span><a href="#L-19-31">31</a></span>
<span><a href="#L-19-32">32</a></span>
<span><a href="#L-19-33">33</a></span>
<span><a href="#L-19-34">34</a></span>
<span><a href="#L-19-35">35</a></span>
<span><a href="#L-19-36">36</a></span>
<span><a href="#L-19-37">37</a></span>
<span><a href="#L-19-38">38</a></span>
<span><a href="#L-19-39">39</a></span>
<span><a href="#L-19-40">40</a></span>
<span><a href="#L-19-41">41</a></span>
<span><a href="#L-19-42">42</a></span>
<span><a href="#L-19-43">43</a></span>
<span><a href="#L-19-44">44</a></span>
<span><a href="#L-19-45">45</a></span>
<span><a href="#L-19-46">46</a></span>
<span><a href="#L-19-47">47</a></span>
<span><a href="#L-19-48">48</a></span>
<span><a href="#L-19-49">49</a></span>
<span><a href="#L-19-50">50</a></span>
<span><a href="#L-19-51">51</a></span>
<span><a href="#L-19-52">52</a></span>
<span><a href="#L-19-53">53</a></span>
<span><a href="#L-19-54">54</a></span>
<span><a href="#L-19-55">55</a></span>
<span><a href="#L-19-56">56</a></span>
<span><a href="#L-19-57">57</a></span></pre></div></td><td><div><pre><span></span><span id="L-19-1"><a id="L-19-1" name="L-19-1"></a><span>Algorithm</span><span> </span><span>19</span><span> </span><span>Beam</span><span> </span><span>Search</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-19-2"><a id="L-19-2" name="L-19-2"></a><span>Required</span><span>:</span><span> </span><span>Input</span><span> </span><span>sequence</span><span>(</span><span>s</span><span>),</span><span> </span><span>beam</span><span> </span><span>width</span><span> </span><span>k</span><span>,</span><span> </span><span>vocabulary</span><span> </span><span>size</span><span> </span><span>V</span><span>,</span><span> </span><span>language</span><span> </span><span>model</span><span> </span><span>LM</span><span></span>
</span><span id="L-19-3"><a id="L-19-3" name="L-19-3"></a><span>Output</span><span>:</span><span> </span><span>Top</span><span> </span><span>k</span><span> </span><span>most</span><span> </span><span>likely</span><span> </span><span>sequence</span><span> </span><span>continuations</span><span></span>
</span><span id="L-19-4"><a id="L-19-4" name="L-19-4"></a>
</span><span id="L-19-5"><a id="L-19-5" name="L-19-5"></a><span>//</span><span> </span><span>Phase</span><span> </span><span>1</span><span>:</span><span> </span><span>Initialization</span><span></span>
</span><span id="L-19-6"><a id="L-19-6" name="L-19-6"></a><span>1</span><span>:</span><span> </span><span>Initialize</span><span> </span><span>empty</span><span> </span><span>beam</span><span> </span><span>B</span><span> </span><span>=</span><span> </span><span>[]</span><span></span>
</span><span id="L-19-7"><a id="L-19-7" name="L-19-7"></a><span>2</span><span>:</span><span> </span><span>Initialize</span><span> </span><span>first</span><span> </span><span>sequence</span><span> </span><span>with</span><span> </span><span>empty</span><span> </span><span>text</span><span> </span><span>and</span><span> </span><span>score</span><span> </span><span>0</span><span></span>
</span><span id="L-19-8"><a id="L-19-8" name="L-19-8"></a>
</span><span id="L-19-9"><a id="L-19-9" name="L-19-9"></a><span>//</span><span> </span><span>Phase</span><span> </span><span>2</span><span>:</span><span> </span><span>Prompt</span><span> </span><span>Processing</span><span></span>
</span><span id="L-19-10"><a id="L-19-10" name="L-19-10"></a><span>3</span><span>:</span><span> </span><span>logprobs</span><span> </span><span>←</span><span> </span><span>LM</span><span>(</span><span>prompt</span><span>)</span><span>  </span><span>//</span><span> </span><span>Get</span><span> </span><span>logprobs</span><span> </span><span>for</span><span> </span><span>next</span><span> </span><span>token</span><span> </span><span>given</span><span> </span><span>prompt</span><span></span>
</span><span id="L-19-11"><a id="L-19-11" name="L-19-11"></a><span>4</span><span>:</span><span> </span><span>candidates</span><span> </span><span>←</span><span> </span><span>[]</span><span></span>
</span><span id="L-19-12"><a id="L-19-12" name="L-19-12"></a><span>5</span><span>:</span><span> </span><span>//</span><span> </span><span>Select</span><span> </span><span>2</span><span>k</span><span> </span><span>tokens</span><span> </span><span>to</span><span> </span><span>ensure</span><span> </span><span>enough</span><span> </span><span>valid</span><span> </span><span>candidates</span><span> </span><span>remain</span><span></span>
</span><span id="L-19-13"><a id="L-19-13" name="L-19-13"></a><span>6</span><span>:</span><span> </span><span>tokens</span><span>,</span><span> </span><span>scores</span><span> </span><span>←</span><span> </span><span>TopK</span><span>(</span><span>logprobs</span><span>,</span><span> </span><span>2</span><span>k</span><span>)</span><span></span>
</span><span id="L-19-14"><a id="L-19-14" name="L-19-14"></a><span>7</span><span>:</span><span> </span><span>for</span><span> </span><span>each</span><span> </span><span>token</span><span> </span><span>t</span><span>,</span><span> </span><span>score</span><span> </span><span>s</span><span> </span><span>in</span><span> </span><span>(</span><span>tokens</span><span>,</span><span> </span><span>scores</span><span>)</span><span> </span><span>do</span><span></span>
</span><span id="L-19-15"><a id="L-19-15" name="L-19-15"></a><span>8</span><span>:</span><span>     </span><span>candidates</span><span>.</span><span>append</span><span>((</span><span>prompt</span><span> </span><span>+</span><span> </span><span>t</span><span>,</span><span> </span><span>s</span><span>,</span><span> </span><span>0</span><span>))</span><span>  </span><span>//</span><span> </span><span>(</span><span>sequence</span><span>,</span><span> </span><span>score</span><span>,</span><span> </span><span>parent_idx</span><span>)</span><span></span>
</span><span id="L-19-16"><a id="L-19-16" name="L-19-16"></a><span>9</span><span>:</span><span> </span><span>end</span><span> </span><span>for</span><span></span>
</span><span id="L-19-17"><a id="L-19-17" name="L-19-17"></a><span>10</span><span>:</span><span> </span><span>Sort</span><span> </span><span>candidates</span><span> </span><span>by</span><span> </span><span>score</span><span> </span><span>(</span><span>descending</span><span>)</span><span></span>
</span><span id="L-19-18"><a id="L-19-18" name="L-19-18"></a><span>11</span><span>:</span><span> </span><span>B</span><span> </span><span>←</span><span> </span><span>candidates</span><span>[</span><span>:k</span><span>]</span><span>  </span><span>//</span><span> </span><span>Keep</span><span> </span><span>top</span><span> </span><span>k</span><span> </span><span>candidates</span><span></span>
</span><span id="L-19-19"><a id="L-19-19" name="L-19-19"></a>
</span><span id="L-19-20"><a id="L-19-20" name="L-19-20"></a><span>//</span><span> </span><span>Phase</span><span> </span><span>3</span><span>:</span><span> </span><span>Generation</span><span> </span><span>Phase</span><span></span>
</span><span id="L-19-21"><a id="L-19-21" name="L-19-21"></a><span>12</span><span>:</span><span> </span><span>for</span><span> </span><span>each</span><span> </span><span>decoding</span><span> </span><span>step</span><span> </span><span>do</span><span></span>
</span><span id="L-19-22"><a id="L-19-22" name="L-19-22"></a><span>13</span><span>:</span><span>     </span><span>candidates</span><span> </span><span>←</span><span> </span><span>[]</span><span></span>
</span><span id="L-19-23"><a id="L-19-23" name="L-19-23"></a><span>14</span><span>:</span><span>     </span><span>//</span><span> </span><span>For</span><span> </span><span>each</span><span> </span><span>sequence</span><span> </span><span>in</span><span> </span><span>the</span><span> </span><span>beam</span><span></span>
</span><span id="L-19-24"><a id="L-19-24" name="L-19-24"></a><span>15</span><span>:</span><span>     </span><span>for</span><span> </span><span>i</span><span> </span><span>from</span><span> </span><span>0</span><span> </span><span>to</span><span> </span><span>|</span><span>B</span><span>|-</span><span>1</span><span> </span><span>do</span><span></span>
</span><span id="L-19-25"><a id="L-19-25" name="L-19-25"></a><span>16</span><span>:</span><span>         </span><span>seq</span><span>,</span><span> </span><span>cum_score</span><span>,</span><span> </span><span>_</span><span> </span><span>←</span><span> </span><span>B</span><span>[</span><span>i</span><span>]</span><span></span>
</span><span id="L-19-26"><a id="L-19-26" name="L-19-26"></a><span>17</span><span>:</span><span>         </span><span>logprobs</span><span> </span><span>←</span><span> </span><span>LM</span><span>(</span><span>seq</span><span>)</span><span>  </span><span>//</span><span> </span><span>Get</span><span> </span><span>logprobs</span><span> </span><span>for</span><span> </span><span>next</span><span> </span><span>token</span><span> </span><span>given</span><span> </span><span>sequence</span><span></span>
</span><span id="L-19-27"><a id="L-19-27" name="L-19-27"></a><span>18</span><span>:</span><span>         </span><span>//</span><span> </span><span>Add</span><span> </span><span>cumulative</span><span> </span><span>score</span><span> </span><span>to</span><span> </span><span>current</span><span> </span><span>logprobs</span><span></span>
</span><span id="L-19-28"><a id="L-19-28" name="L-19-28"></a><span>19</span><span>:</span><span>         </span><span>full_scores</span><span> </span><span>←</span><span> </span><span>logprobs</span><span> </span><span>+</span><span> </span><span>cum_score</span><span></span>
</span><span id="L-19-29"><a id="L-19-29" name="L-19-29"></a><span>20</span><span>:</span><span>         </span><span>//</span><span> </span><span>Flatten</span><span> </span><span>scores</span><span> </span><span>across</span><span> </span><span>all</span><span> </span><span>beam</span><span> </span><span>sequences</span><span> </span><span>and</span><span> </span><span>vocabulary</span><span></span>
</span><span id="L-19-30"><a id="L-19-30" name="L-19-30"></a><span>21</span><span>:</span><span>         </span><span>all_scores</span><span>.</span><span>append</span><span>(</span><span>full_scores</span><span>)</span><span></span>
</span><span id="L-19-31"><a id="L-19-31" name="L-19-31"></a><span>22</span><span>:</span><span>     </span><span>end</span><span> </span><span>for</span><span></span>
</span><span id="L-19-32"><a id="L-19-32" name="L-19-32"></a><span>23</span><span>:</span><span></span>
</span><span id="L-19-33"><a id="L-19-33" name="L-19-33"></a><span>24</span><span>:</span><span>     </span><span>//</span><span> </span><span>Select</span><span> </span><span>2</span><span>k</span><span> </span><span>highest</span><span> </span><span>scoring</span><span> </span><span>(</span><span>sequence</span><span>,</span><span> </span><span>next_token</span><span>)</span><span> </span><span>combinations</span><span></span>
</span><span id="L-19-34"><a id="L-19-34" name="L-19-34"></a><span>25</span><span>:</span><span>     </span><span>flat_scores</span><span> </span><span>←</span><span> </span><span>Flatten</span><span>(</span><span>all_scores</span><span>)</span><span>  </span><span>//</span><span> </span><span>Size</span><span>:</span><span> </span><span>(</span><span>beam_width</span><span> </span><span>×</span><span> </span><span>vocab_size</span><span>)</span><span></span>
</span><span id="L-19-35"><a id="L-19-35" name="L-19-35"></a><span>26</span><span>:</span><span>     </span><span>top_indices</span><span>,</span><span> </span><span>top_scores</span><span> </span><span>←</span><span> </span><span>TopK</span><span>(</span><span>flat_scores</span><span>,</span><span> </span><span>2</span><span>k</span><span>)</span><span></span>
</span><span id="L-19-36"><a id="L-19-36" name="L-19-36"></a><span>27</span><span>:</span><span></span>
</span><span id="L-19-37"><a id="L-19-37" name="L-19-37"></a><span>28</span><span>:</span><span>     </span><span>//</span><span> </span><span>Extract</span><span> </span><span>parent</span><span> </span><span>sequence</span><span> </span><span>index</span><span> </span><span>and</span><span> </span><span>token</span><span> </span><span>id</span><span> </span><span>from</span><span> </span><span>flat</span><span> </span><span>indices</span><span></span>
</span><span id="L-19-38"><a id="L-19-38" name="L-19-38"></a><span>29</span><span>:</span><span>     </span><span>for</span><span> </span><span>each</span><span> </span><span>idx</span><span>,</span><span> </span><span>score</span><span> </span><span>in</span><span> </span><span>(</span><span>top_indices</span><span>,</span><span> </span><span>top_scores</span><span>)</span><span> </span><span>do</span><span></span>
</span><span id="L-19-39"><a id="L-19-39" name="L-19-39"></a><span>30</span><span>:</span><span>         </span><span>parent_idx</span><span> </span><span>←</span><span> </span><span>idx</span><span> </span><span>÷</span><span> </span><span>V</span><span>  </span><span>//</span><span> </span><span>Integer</span><span> </span><span>division</span><span></span>
</span><span id="L-19-40"><a id="L-19-40" name="L-19-40"></a><span>31</span><span>:</span><span>         </span><span>token_id</span><span> </span><span>←</span><span> </span><span>idx</span><span> </span><span>mod</span><span> </span><span>V</span><span></span>
</span><span id="L-19-41"><a id="L-19-41" name="L-19-41"></a><span>32</span><span>:</span><span>         </span><span>parent_seq</span><span>,</span><span> </span><span>parent_score</span><span>,</span><span> </span><span>_</span><span> </span><span>←</span><span> </span><span>B</span><span>[</span><span>parent_idx</span><span>]</span><span></span>
</span><span id="L-19-42"><a id="L-19-42" name="L-19-42"></a><span>33</span><span>:</span><span>         </span><span>candidates</span><span>.</span><span>append</span><span>((</span><span>parent_seq</span><span> </span><span>+</span><span> </span><span>token_id</span><span>,</span><span> </span><span>score</span><span>,</span><span> </span><span>parent_idx</span><span>))</span><span></span>
</span><span id="L-19-43"><a id="L-19-43" name="L-19-43"></a><span>34</span><span>:</span><span>     </span><span>end</span><span> </span><span>for</span><span></span>
</span><span id="L-19-44"><a id="L-19-44" name="L-19-44"></a><span>35</span><span>:</span><span></span>
</span><span id="L-19-45"><a id="L-19-45" name="L-19-45"></a><span>36</span><span>:</span><span>     </span><span>//</span><span> </span><span>Handle</span><span> </span><span>completed</span><span> </span><span>sequences</span><span> </span><span>separately</span><span> </span><span>(</span><span>if</span><span> </span><span>needed</span><span>)</span><span></span>
</span><span id="L-19-46"><a id="L-19-46" name="L-19-46"></a><span>37</span><span>:</span><span>     </span><span>...</span><span> </span>
</span><span id="L-19-47"><a id="L-19-47" name="L-19-47"></a><span>38</span><span>:</span><span></span>
</span><span id="L-19-48"><a id="L-19-48" name="L-19-48"></a><span>39</span><span>:</span><span>     </span><span>//</span><span> </span><span>Update</span><span> </span><span>beam</span><span> </span><span>with</span><span> </span><span>top</span><span> </span><span>k</span><span> </span><span>candidates</span><span></span>
</span><span id="L-19-49"><a id="L-19-49" name="L-19-49"></a><span>40</span><span>:</span><span>     </span><span>Sort</span><span> </span><span>candidates</span><span> </span><span>by</span><span> </span><span>score</span><span> </span><span>(</span><span>descending</span><span>)</span><span></span>
</span><span id="L-19-50"><a id="L-19-50" name="L-19-50"></a><span>41</span><span>:</span><span>     </span><span>B</span><span> </span><span>←</span><span> </span><span>candidates</span><span>[</span><span>:k</span><span>]</span><span></span>
</span><span id="L-19-51"><a id="L-19-51" name="L-19-51"></a><span>42</span><span>:</span><span></span>
</span><span id="L-19-52"><a id="L-19-52" name="L-19-52"></a><span>43</span><span>:</span><span>     </span><span>if</span><span> </span><span>stopping_criteria_met</span><span> </span><span>then</span><span></span>
</span><span id="L-19-53"><a id="L-19-53" name="L-19-53"></a><span>44</span><span>:</span><span>         </span><span>break</span><span></span>
</span><span id="L-19-54"><a id="L-19-54" name="L-19-54"></a><span>45</span><span>:</span><span>     </span><span>end</span><span> </span><span>if</span><span></span>
</span><span id="L-19-55"><a id="L-19-55" name="L-19-55"></a><span>46</span><span>:</span><span> </span><span>end</span><span> </span><span>for</span><span></span>
</span><span id="L-19-56"><a id="L-19-56" name="L-19-56"></a>
</span><span id="L-19-57"><a id="L-19-57" name="L-19-57"></a><span>47</span><span>:</span><span> </span><span>return</span><span> </span><span>B</span><span>  </span><span>//</span><span> </span><span>Return</span><span> </span><span>top</span><span> </span><span>k</span><span> </span><span>sequences</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="contrastive-search">Contrastive Search<a href="#contrastive-search" title="Permanent link"> </a></h3>
<p>When choosing the next word, CS balances two competing goals: picking words that make sense in context (high probability) while avoiding repetivie patterns (low similarity to what's already been written). Similar to beam search, this sampling method is not widely used anymore.</p>
<p>CS was invented to address a fundamental limitation of likelihood-based sampling: the tendency to either be too repetitive (at low temps) or too incoherent (at high temps). Instead of just adjusting randomness, it explicitly rewards diversity.</p>
<p><strong>Technical</strong>:<br>
CS introduces a degeneration penalty based on the similarity between candidate tokens and the existing context. We first select top-k most probable tokens based on logits. Then, extract vector representations of the existing context and each candidate continuation from the model's hidden states. For each candidate token, calculate a score that balances: 1) its probability (how likely it is according to the model), 2) its degeneration penalty (how similar it is to the existing context). The alpha parameter controls the trade-off between likelihood and diversity.</p>
<p>Mathematically, it is:<br>
<code>score(x) = α * P(x) - (1-α) * sim(x, context)</code></p>
<p>Where:</p>
<ul>
<li><code>P(x)</code> is the probability of token <code>x</code></li>
<li><code>sim(x, context)</code> is the semantic similarity between <code>x</code> and the context</li>
<li><code>α</code> is the balancing coefficient.</li>
</ul>
<p><strong>Algorithm</strong><br>
</p><div>

<table><tbody><tr><td><div><pre><span><a href="#L-20-1"> 1</a></span>
<span><a href="#L-20-2"> 2</a></span>
<span><a href="#L-20-3"> 3</a></span>
<span><a href="#L-20-4"> 4</a></span>
<span><a href="#L-20-5"> 5</a></span>
<span><a href="#L-20-6"> 6</a></span>
<span><a href="#L-20-7"> 7</a></span>
<span><a href="#L-20-8"> 8</a></span>
<span><a href="#L-20-9"> 9</a></span>
<span><a href="#L-20-10">10</a></span>
<span><a href="#L-20-11">11</a></span>
<span><a href="#L-20-12">12</a></span>
<span><a href="#L-20-13">13</a></span>
<span><a href="#L-20-14">14</a></span>
<span><a href="#L-20-15">15</a></span>
<span><a href="#L-20-16">16</a></span>
<span><a href="#L-20-17">17</a></span>
<span><a href="#L-20-18">18</a></span>
<span><a href="#L-20-19">19</a></span>
<span><a href="#L-20-20">20</a></span>
<span><a href="#L-20-21">21</a></span>
<span><a href="#L-20-22">22</a></span>
<span><a href="#L-20-23">23</a></span>
<span><a href="#L-20-24">24</a></span>
<span><a href="#L-20-25">25</a></span>
<span><a href="#L-20-26">26</a></span>
<span><a href="#L-20-27">27</a></span>
<span><a href="#L-20-28">28</a></span>
<span><a href="#L-20-29">29</a></span>
<span><a href="#L-20-30">30</a></span>
<span><a href="#L-20-31">31</a></span>
<span><a href="#L-20-32">32</a></span>
<span><a href="#L-20-33">33</a></span>
<span><a href="#L-20-34">34</a></span>
<span><a href="#L-20-35">35</a></span>
<span><a href="#L-20-36">36</a></span>
<span><a href="#L-20-37">37</a></span>
<span><a href="#L-20-38">38</a></span>
<span><a href="#L-20-39">39</a></span>
<span><a href="#L-20-40">40</a></span>
<span><a href="#L-20-41">41</a></span>
<span><a href="#L-20-42">42</a></span>
<span><a href="#L-20-43">43</a></span>
<span><a href="#L-20-44">44</a></span>
<span><a href="#L-20-45">45</a></span>
<span><a href="#L-20-46">46</a></span>
<span><a href="#L-20-47">47</a></span>
<span><a href="#L-20-48">48</a></span>
<span><a href="#L-20-49">49</a></span>
<span><a href="#L-20-50">50</a></span></pre></div></td><td><div><pre><span></span><span id="L-20-1"><a id="L-20-1" name="L-20-1"></a><span>Algorithm</span><span> </span><span>20</span><span> </span><span>Contrastive</span><span> </span><span>Search</span><span></span>
</span><span id="L-20-2"><a id="L-20-2" name="L-20-2"></a><span>Required</span><span>:</span><span> </span><span>Input</span><span> </span><span>prompt</span><span>,</span><span> </span><span>model</span><span> </span><span>M</span><span>,</span><span> </span><span>top</span><span>-</span><span>k</span><span> </span><span>parameter</span><span> </span><span>k</span><span>,</span><span> </span><span>alpha</span><span> </span><span>parameter</span><span> </span><span>α</span><span> </span><span>∈</span><span> </span><span>[</span><span>0,1</span><span>]</span><span></span>
</span><span id="L-20-3"><a id="L-20-3" name="L-20-3"></a><span>Output</span><span>:</span><span> </span><span>Generated</span><span> </span><span>text</span><span> </span><span>that</span><span> </span><span>balances</span><span> </span><span>likelihood</span><span> </span><span>and</span><span> </span><span>diversity</span><span></span>
</span><span id="L-20-4"><a id="L-20-4" name="L-20-4"></a>
</span><span id="L-20-5"><a id="L-20-5" name="L-20-5"></a><span>1</span><span>:</span><span> </span><span>seq</span><span> </span><span>←</span><span> </span><span>prompt</span><span>  </span><span>//</span><span> </span><span>Initialize</span><span> </span><span>with</span><span> </span><span>input</span><span> </span><span>prompt</span><span></span>
</span><span id="L-20-6"><a id="L-20-6" name="L-20-6"></a><span>2</span><span>:</span><span></span>
</span><span id="L-20-7"><a id="L-20-7" name="L-20-7"></a><span>3</span><span>:</span><span> </span><span>for</span><span> </span><span>each</span><span> </span><span>decoding</span><span> </span><span>step</span><span> </span><span>do</span><span></span>
</span><span id="L-20-8"><a id="L-20-8" name="L-20-8"></a><span>4</span><span>:</span><span>     </span><span>//</span><span> </span><span>Get</span><span> </span><span>logits</span><span> </span><span>from</span><span> </span><span>the</span><span> </span><span>model</span><span> </span><span>for</span><span> </span><span>next</span><span> </span><span>token</span><span></span>
</span><span id="L-20-9"><a id="L-20-9" name="L-20-9"></a><span>5</span><span>:</span><span>     </span><span>logits</span><span> </span><span>←</span><span> </span><span>M</span><span>(</span><span>seq</span><span>)</span><span></span>
</span><span id="L-20-10"><a id="L-20-10" name="L-20-10"></a><span>6</span><span>:</span><span></span>
</span><span id="L-20-11"><a id="L-20-11" name="L-20-11"></a><span>7</span><span>:</span><span>     </span><span>//</span><span> </span><span>Get</span><span> </span><span>top</span><span>-</span><span>k</span><span> </span><span>most</span><span> </span><span>likely</span><span> </span><span>tokens</span><span></span>
</span><span id="L-20-12"><a id="L-20-12" name="L-20-12"></a><span>8</span><span>:</span><span>     </span><span>topk_logits</span><span>,</span><span> </span><span>topk_indices</span><span> </span><span>←</span><span> </span><span>TopK</span><span>(</span><span>logits</span><span>,</span><span> </span><span>k</span><span>)</span><span></span>
</span><span id="L-20-13"><a id="L-20-13" name="L-20-13"></a><span>9</span><span>:</span><span></span>
</span><span id="L-20-14"><a id="L-20-14" name="L-20-14"></a><span>10</span><span>:</span><span>     </span><span>//</span><span> </span><span>Convert</span><span> </span><span>to</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-20-15"><a id="L-20-15" name="L-20-15"></a><span>11</span><span>:</span><span>     </span><span>topk_probs</span><span> </span><span>←</span><span> </span><span>Softmax</span><span>(</span><span>topk_logits</span><span>)</span><span></span>
</span><span id="L-20-16"><a id="L-20-16" name="L-20-16"></a><span>12</span><span>:</span><span></span>
</span><span id="L-20-17"><a id="L-20-17" name="L-20-17"></a><span>13</span><span>:</span><span>     </span><span>//</span><span> </span><span>Get</span><span> </span><span>hidden</span><span> </span><span>representations</span><span> </span><span>from</span><span> </span><span>the</span><span> </span><span>model</span><span>'</span><span>s</span><span> </span><span>last</span><span> </span><span>layer</span><span></span>
</span><span id="L-20-18"><a id="L-20-18" name="L-20-18"></a><span>14</span><span>:</span><span>     </span><span>H</span><span> </span><span>←</span><span> </span><span>GetHiddenStates</span><span>(</span><span>M</span><span>,</span><span> </span><span>seq</span><span>)</span><span></span>
</span><span id="L-20-19"><a id="L-20-19" name="L-20-19"></a><span>15</span><span>:</span><span>     </span><span>h_last</span><span> </span><span>←</span><span> </span><span>H</span><span>[</span><span>-1</span><span>]</span><span>  </span><span>//</span><span> </span><span>Hidden</span><span> </span><span>state</span><span> </span><span>for</span><span> </span><span>the</span><span> </span><span>last</span><span> </span><span>token</span><span></span>
</span><span id="L-20-20"><a id="L-20-20" name="L-20-20"></a><span>16</span><span>:</span><span></span>
</span><span id="L-20-21"><a id="L-20-21" name="L-20-21"></a><span>17</span><span>:</span><span>     </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>degeneration</span><span> </span><span>penalty</span><span> </span><span>for</span><span> </span><span>each</span><span> </span><span>candidate</span><span> </span><span>token</span><span></span>
</span><span id="L-20-22"><a id="L-20-22" name="L-20-22"></a><span>18</span><span>:</span><span>     </span><span>penalties</span><span> </span><span>←</span><span> </span><span>[]</span><span></span>
</span><span id="L-20-23"><a id="L-20-23" name="L-20-23"></a><span>19</span><span>:</span><span>     </span><span>for</span><span> </span><span>each</span><span> </span><span>candidate</span><span> </span><span>token</span><span> </span><span>t</span><span> </span><span>in</span><span> </span><span>topk_indices</span><span> </span><span>do</span><span></span>
</span><span id="L-20-24"><a id="L-20-24" name="L-20-24"></a><span>20</span><span>:</span><span>         </span><span>//</span><span> </span><span>Get</span><span> </span><span>hidden</span><span> </span><span>representation</span><span> </span><span>if</span><span> </span><span>we</span><span> </span><span>were</span><span> </span><span>to</span><span> </span><span>append</span><span> </span><span>this</span><span> </span><span>token</span><span></span>
</span><span id="L-20-25"><a id="L-20-25" name="L-20-25"></a><span>21</span><span>:</span><span>         </span><span>h_candidate</span><span> </span><span>←</span><span> </span><span>GetHiddenState</span><span>(</span><span>M</span><span>,</span><span> </span><span>seq</span><span> </span><span>+</span><span> </span><span>t</span><span>)</span><span>[</span><span>-1</span><span>]</span><span></span>
</span><span id="L-20-26"><a id="L-20-26" name="L-20-26"></a><span>22</span><span>:</span><span></span>
</span><span id="L-20-27"><a id="L-20-27" name="L-20-27"></a><span>23</span><span>:</span><span>         </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>similarity</span><span> </span><span>between</span><span> </span><span>candidate</span><span> </span><span>and</span><span> </span><span>context</span><span></span>
</span><span id="L-20-28"><a id="L-20-28" name="L-20-28"></a><span>24</span><span>:</span><span>         </span><span>sim</span><span> </span><span>←</span><span> </span><span>CosineSimilarity</span><span>(</span><span>h_candidate</span><span>,</span><span> </span><span>h_last</span><span>)</span><span></span>
</span><span id="L-20-29"><a id="L-20-29" name="L-20-29"></a><span>25</span><span>:</span><span></span>
</span><span id="L-20-30"><a id="L-20-30" name="L-20-30"></a><span>26</span><span>:</span><span>         </span><span>//</span><span> </span><span>Store</span><span> </span><span>similarity</span><span> </span><span>as</span><span> </span><span>the</span><span> </span><span>degeneration</span><span> </span><span>penalty</span><span></span>
</span><span id="L-20-31"><a id="L-20-31" name="L-20-31"></a><span>27</span><span>:</span><span>         </span><span>penalties</span><span>.</span><span>append</span><span>(</span><span>sim</span><span>)</span><span></span>
</span><span id="L-20-32"><a id="L-20-32" name="L-20-32"></a><span>28</span><span>:</span><span>     </span><span>end</span><span> </span><span>for</span><span></span>
</span><span id="L-20-33"><a id="L-20-33" name="L-20-33"></a><span>29</span><span>:</span><span></span>
</span><span id="L-20-34"><a id="L-20-34" name="L-20-34"></a><span>30</span><span>:</span><span>     </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>final</span><span> </span><span>scores</span><span> </span><span>using</span><span> </span><span>weighted</span><span> </span><span>combination</span><span></span>
</span><span id="L-20-35"><a id="L-20-35" name="L-20-35"></a><span>31</span><span>:</span><span>     </span><span>//</span><span> </span><span>Higher</span><span> </span><span>alpha</span><span> </span><span>weights</span><span> </span><span>likelihood</span><span> </span><span>more</span><span>,</span><span> </span><span>lower</span><span> </span><span>alpha</span><span> </span><span>weights</span><span> </span><span>diversity</span><span> </span><span>more</span><span></span>
</span><span id="L-20-36"><a id="L-20-36" name="L-20-36"></a><span>32</span><span>:</span><span>     </span><span>scores</span><span> </span><span>←</span><span> </span><span>α</span><span> </span><span>*</span><span> </span><span>topk_probs</span><span> </span><span>-</span><span> </span><span>(</span><span>1</span><span>-</span><span>α</span><span>)</span><span> </span><span>*</span><span> </span><span>penalties</span><span></span>
</span><span id="L-20-37"><a id="L-20-37" name="L-20-37"></a><span>33</span><span>:</span><span></span>
</span><span id="L-20-38"><a id="L-20-38" name="L-20-38"></a><span>34</span><span>:</span><span>     </span><span>//</span><span> </span><span>Select</span><span> </span><span>token</span><span> </span><span>with</span><span> </span><span>highest</span><span> </span><span>combined</span><span> </span><span>score</span><span></span>
</span><span id="L-20-39"><a id="L-20-39" name="L-20-39"></a><span>35</span><span>:</span><span>     </span><span>best_idx</span><span> </span><span>←</span><span> </span><span>ArgMax</span><span>(</span><span>scores</span><span>)</span><span></span>
</span><span id="L-20-40"><a id="L-20-40" name="L-20-40"></a><span>36</span><span>:</span><span>     </span><span>next_token</span><span> </span><span>←</span><span> </span><span>topk_indices</span><span>[</span><span>best_idx</span><span>]</span><span></span>
</span><span id="L-20-41"><a id="L-20-41" name="L-20-41"></a><span>37</span><span>:</span><span></span>
</span><span id="L-20-42"><a id="L-20-42" name="L-20-42"></a><span>38</span><span>:</span><span>     </span><span>//</span><span> </span><span>Append</span><span> </span><span>to</span><span> </span><span>sequence</span><span></span>
</span><span id="L-20-43"><a id="L-20-43" name="L-20-43"></a><span>39</span><span>:</span><span>     </span><span>seq</span><span> </span><span>←</span><span> </span><span>seq</span><span> </span><span>+</span><span> </span><span>next_token</span><span></span>
</span><span id="L-20-44"><a id="L-20-44" name="L-20-44"></a><span>40</span><span>:</span><span></span>
</span><span id="L-20-45"><a id="L-20-45" name="L-20-45"></a><span>41</span><span>:</span><span>     </span><span>if</span><span> </span><span>stopping_criteria_met</span><span> </span><span>then</span><span></span>
</span><span id="L-20-46"><a id="L-20-46" name="L-20-46"></a><span>42</span><span>:</span><span>         </span><span>break</span><span></span>
</span><span id="L-20-47"><a id="L-20-47" name="L-20-47"></a><span>43</span><span>:</span><span>     </span><span>end</span><span> </span><span>if</span><span></span>
</span><span id="L-20-48"><a id="L-20-48" name="L-20-48"></a><span>44</span><span>:</span><span> </span><span>end</span><span> </span><span>for</span><span></span>
</span><span id="L-20-49"><a id="L-20-49" name="L-20-49"></a><span>45</span><span>:</span><span></span>
</span><span id="L-20-50"><a id="L-20-50" name="L-20-50"></a><span>46</span><span>:</span><span> </span><span>return</span><span> </span><span>seq</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h2 id="sampler-order">Sampler Order<a href="#sampler-order" title="Permanent link"> </a></h2>
<p>The document thus far has explained individual sampling methods, but in real-world LLM implementations, these techniques are often applied in a carefully orchestrated sequence. Some libraries allow for the user to customize the order of samplers per-request, but most do not.</p>
<h3 id="the-typical-sampling-pipeline">The Typical Sampling Pipeline<a href="#the-typical-sampling-pipeline" title="Permanent link"> </a></h3>
<p>A typical sampling pipeline in production LLM systems follow these sequential steps:</p>
<ol>
<li><strong>Generate Raw Logits</strong>: The model produces unnormalized logits for each token in the vocabulary.</li>
<li><strong>Apply Token Filtering/Banning</strong>: Remove tokens that shouldn't be considred at all.</li>
<li><strong>Apply Penalties</strong>: Apply repetition, frequency, and presence penalties to discourage repetitive outputs.</li>
<li><strong>Apply Pattern-Based Techniques</strong>: Methods like DRY (Don't Repeat Yourself) are applied to identify and penalize repetitive patterns.</li>
<li><strong>Apply Temperature Scaling</strong>: Temperature is either applied as the first or the last sampler (outside penalties and post-softmax samplers), depending on the implementation. For most tasks, temperature is applied first. For creative writing, it is usually last.</li>
<li><strong>Apply Distribution-Shaping Methods</strong>: Techniques like Top-K, Top-P, Min-P, etc. that filter or reshape the probability distribution.</li>
<li><strong>Sample from the Final Distribution</strong>: After all modifications, select a token from the resulting probability distribution.</li>
</ol>
<h3 id="effects-and-interactions-of-samplers-with-each-other">Effects and Interactions of Samplers with Each Other<a href="#effects-and-interactions-of-samplers-with-each-other" title="Permanent link"> </a></h3>
<p>The order in which sampling techniques are applied have serious implications for text generation. This is an oft-unexplored topic, and libraries usually follow a somewhat standardized order without any flexibility. Each sampler modifies the probability distribution, creating a transformed landscape that subsequent samplers work with. Understanding these is very crucial.</p>
<h4 id="how-samplers-transform-distributions">How Samplers Transform Distributions<a href="#how-samplers-transform-distributions" title="Permanent link"> </a></h4>
<p>To understand sampler interaction, let's first visualize it:</p>
<ol>
<li><strong>Original Distribution</strong>: A typical token distribution has a few high-probability tokens followed by a long tail of increasingly unlikely options.</li>
<li><strong>After Penalties</strong>: Penalties flatten peaks for previously used tokens, so they elevate alternatives that haven't appeared yet.</li>
<li><strong>After Temperature</strong>: Lower temperatures (&lt;1.0) sharpen the distribution, making peaks higher and valleys lower. Higher temperatures (&gt;1.0) flatten the distribution.</li>
<li><strong>After Filtering (Top-K/P/etc.)</strong>: These truncate the distribution, removing lowest-probability tokens and often renormalizing the remaining probs.</li>
</ol>
<p>Each transformation, as you may guess, fundamentally changes what subsequent samplers <strong>see</strong> and can dramatically influence the final output.</p>
<h4 id="critical-order-dependent-interactions">Critical Order-Dependent Interactions<a href="#critical-order-dependent-interactions" title="Permanent link"> </a></h4>
<h5 id="temperature-before-vs-after-filtering">Temperature Before vs. After Filtering<a href="#temperature-before-vs-after-filtering" title="Permanent link"> </a></h5>
<p>One of the most important ordering decisions is whether to apply temperature scaling before or after filtering methods:</p>
<p><strong>Temperature → Filtering</strong>:</p>
<ul>
<li>Temperature first reshapes the entire distribution</li>
<li>Low temperature concentrates probability mass on fewer tokens before filtering</li>
<li>High temperature spreads probability mass more evenly before filtering</li>
<li>Filtering then operates on this reshaped distribution</li>
</ul>
<p><strong>Filtering → Temperature</strong>:</p>
<ul>
<li>Filtering first truncates the distribution to only the most likely tokens</li>
<li>Temperature then only affects the relative probabilities among these filtered tokens</li>
<li>Can produce more constrained outputs even with high temperatures</li>
</ul>
<p>Example: With a Top-K of 40 and temperature of 1.5, applying temperature first might allow some tokens from outside the original top 40 to become likely enough to survive filtering. Applying filtering first ensures only the original top 40 tokens remain, regardless of temperature.</p>
<h5 id="penalties-before-vs-after-other-samplers">Penalties Before vs. After Other Samplers<a href="#penalties-before-vs-after-other-samplers" title="Permanent link"> </a></h5>
<p><strong>Penalties → Temperature</strong>:</p>
<ul>
<li>Penalties first reduce probabilities of repeated tokens</li>
<li>Temperature then amplifies or diminishes these adjustments</li>
<li>With high temperatures, penalties may be effectively erased</li>
<li>With low temperatures, penalties may be excessively amplified</li>
</ul>
<p><strong>Temperature → Penalties</strong>:</p>
<ul>
<li>Temperature first reshapes the entire distribution</li>
<li>Penalties then operate on this temperature-modified distribution</li>
<li>Can lead to more balanced and predictable penalty effects</li>
</ul>
<h5 id="drys-position-matters">DRY's Position Matters<a href="#drys-position-matters" title="Permanent link"> </a></h5>
<p>The DRY (Don't Repeat Yourself) sampler, which penalizes continuing n-gram patterns, is particularly sensitive to positioning:</p>
<p><strong>DRY Early in Pipeline</strong>:</p>
<ul>
<li>Applied to the raw or lightly modified distribution</li>
<li>Has strong effect on preventing repetition</li>
<li>Subsequent samplers may still introduce repetition by elevating penalized tokens</li>
</ul>
<p><strong>DRY Late in Pipeline</strong>:</p>
<ul>
<li>Applied after other samplers have modified the distribution</li>
<li>May have weaker effect if previous samplers have already eliminated some options</li>
<li>Final arbiter against repetition before token selection</li>
</ul>
<h4 id="synergies-and-conflicts">Synergies and Conflicts<a href="#synergies-and-conflicts" title="Permanent link"> </a></h4>
<h5 id="synergistic-combos">Synergistic Combos<a href="#synergistic-combos" title="Permanent link"> </a></h5>
<ol>
<li><strong>Top-K + Top-P</strong>: Top-K provides a hard limit on tokens considered, while Top-P adapts to the confidence of the model. Together they give us guardrails and enough flexibility.</li>
<li><strong>Temperature + Min-P</strong>: High temperature flattens the distribution, while Min-P establishes a quality floor relative to the best token. This combination increases creativity while filtering out truly poor options.</li>
</ol>
<h5 id="conflicting-combos">Conflicting Combos<a href="#conflicting-combos" title="Permanent link"> </a></h5>
<ol>
<li><strong>High Temperature + Low Top-K</strong>: High temperature tries to flatten the distribution, while low Top-K severely restricts options. The Top-K will essentially override much of the temperature's effect.</li>
<li><strong>Multiple Filtering Methods</strong>: Applying Top-K, Top-P, Min-P, and TFS together often results in the most restrictive one dominating, making the others redundant.</li>
<li><strong>XTC + Top-A</strong>: Both methods aim to exclude top choices but in different ways. Using both can overly restrict the sampling space.</li>
</ol></div>
                    </article>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TScale – distributed training on consumer GPUs (124 pts)]]></title>
            <link>https://github.com/Foreseerr/TScale</link>
            <guid>43886601</guid>
            <pubDate>Sun, 04 May 2025 13:29:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Foreseerr/TScale">https://github.com/Foreseerr/TScale</a>, See on <a href="https://news.ycombinator.com/item?id=43886601">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">TScale</h2><a id="user-content-tscale" aria-label="Permalink: TScale" href="#tscale"></a></p>
<p dir="auto">This repo contains transformer train and inference code written in C++ and CUDA.</p>
<p dir="auto">TScale is designed to run on consumer hardware. To achive best results it features</p>
<ul dir="auto">
<li>Optimized transformer architecture with faster convergence and ~2x reduced attention costs</li>
<li>Support for fp8 and int8 model weights and activations precision</li>
<li>Optimized for consumer nVidia GPUs including fast reduced precision training without sacrificing model quality</li>
<li>CPU offload reduces GPU memory requirements for training</li>
<li>Sync distributed training on several same config hosts</li>
<li>1-bit gradient compression allowing using regular ethernet links for interconnect</li>
<li>Async distributed training on arbitrary hosts with negligible network traffic. In this mode training can be run on geographically separated hosts</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Distributed training of 1.5B model on consumer GPU</h2><a id="user-content-distributed-training-of-15b-model-on-consumer-gpu" aria-label="Permalink: Distributed training of 1.5B model on consumer GPU" href="#distributed-training-of-15b-model-on-consumer-gpu"></a></p>
<p dir="auto">By using inexpensive GPUs and async distributed mode TScale trains LLMs fast and affordable. Log loss for the 1.5B model trained on fineweb-edu for 2 days and $500 on several spot instances with 4090:
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Foreseerr/TScale/blob/main/img/fed_hellaswag.png"><img src="https://github.com/Foreseerr/TScale/raw/main/img/fed_hellaswag.png" alt="Nice train loss graph"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Training your own 1T model at home</h2><a id="user-content-training-your-own-1t-model-at-home" aria-label="Permalink: Training your own 1T model at home" href="#training-your-own-1t-model-at-home"></a></p>
<p dir="auto">1T model size sounds beyond reach for most people and even organisations. However if we consider creative ways to count model size then there is nothing impossible. In this case we build a model with 1T index which we lookup for every token to make prediction with much smaller model. In terms of logloss/perplexity this construction easily achieves stellar results. Index for fineweb-edu occupies about 1T of disk space. Training run of 125M model with this ~1T index achieves <strong>x8</strong> perplexity reduction:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Perplexity</th>
</tr>
</thead>
<tbody>
<tr>
<td>125M</td>
<td>19.02</td>
</tr>
<tr>
<td>125M + 1T index</td>
<td>2.28</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Read more</h2><a id="user-content-read-more" aria-label="Permalink: Read more" href="#read-more"></a></p>
<p dir="auto"><a href="https://github.com/Foreseerr/TScale/blob/main/doc/125M_model.md">Training 125M model</a></p>
<p dir="auto"><a href="https://github.com/Foreseerr/TScale/blob/main/doc/1.5B_model.md">Training 1.5B model</a></p>
<p dir="auto"><a href="https://github.com/Foreseerr/TScale/blob/main/doc/1T_model.md">Training 1T (!) model in your kitchen</a></p>
<p dir="auto"><a href="https://github.com/Foreseerr/TScale/blob/main/doc/fed.md">Async distributed train</a></p>
<p dir="auto"><a href="https://github.com/Foreseerr/TScale/blob/main/doc/precision.md">Notes on model and compute precision</a></p>
<p dir="auto"><a href="https://github.com/Foreseerr/TScale/blob/main/doc/model.md">TScale transformer model</a></p>
<p dir="auto"><a href="https://github.com/Foreseerr/TScale/blob/main/doc/lm_search.md">Data indexing</a></p>
<p dir="auto"><a href="https://github.com/Foreseerr/TScale/blob/main/doc/tokenizer.md">Tokenizer</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Build</h2><a id="user-content-build" aria-label="Permalink: Build" href="#build"></a></p>
<p dir="auto">To build the the code CUDA v12.3 and C++ compiler are required, msvc for windows,  cmake+clang for Linux. To support cross platform build files generation this repo uses <a href="https://github.com/Foreseerr/TScale/blob/main/doc/fo.md">fo</a>, lightweight solution/build files generator. To generate build files you need to compile fo/fo.cpp and run it with two arguments. First argument is root of source tree, second argument is directory to store build files to.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Windows</h2><a id="user-content-windows" aria-label="Permalink: Windows" href="#windows"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="D:\TScale>fo.exe code sln"><pre>D:<span>\T</span>Scale<span>&gt;</span>fo.exe code sln</pre></div>
<p dir="auto">Then open code.sln from d:\TScale\sln\code.sln.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Linux</h2><a id="user-content-linux" aria-label="Permalink: Linux" href="#linux"></a></p>
<p dir="auto">To compile TScale for linux you need to compile fo.cpp, generate CMakeLists.txt file, run cmake, run make.</p>
<div dir="auto" data-snippet-clipboard-copy-content="~/TScale/fo$ clang++17 fo.cpp -o fo
~/TScale/fo$ cd ..
~/TScale$ ./fo/fo code make.dir
~/TScale$ cd make.dir
~/TScale/make.dir$ cmake -D CMAKE_BUILD_TYPE=RelWithDebInfo .
~/TScale/make.dir$ make"><pre><span>~</span>/TScale/fo$ clang++17 fo.cpp -o fo
<span>~</span>/TScale/fo$ <span>cd</span> ..
<span>~</span>/TScale$ ./fo/fo code make.dir
<span>~</span>/TScale$ <span>cd</span> make.dir
<span>~</span>/TScale/make.dir$ cmake -D CMAKE_BUILD_TYPE=RelWithDebInfo <span>.</span>
<span>~</span>/TScale/make.dir$ make</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Get train data</h2><a id="user-content-get-train-data" aria-label="Permalink: Get train data" href="#get-train-data"></a></p>
<p dir="auto">Examples in the code use <a href="https://mattmahoney.net/dc/textdata.html" rel="nofollow">enwik9</a> dataset and its truncacted version enwik8. Also Hugging Face hosted datasets openwebtext, ontocord/CulturaY, danasone/librusec are used in examples. To import them use <a href="https://github.com/Foreseerr/TScale/blob/main/pysrc/hf_import/import.py">hf_import</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Train model</h2><a id="user-content-train-model" aria-label="Permalink: Train model" href="#train-model"></a></p>
<p dir="auto"><a href="https://github.com/Foreseerr/TScale/blob/main/code/gpt/train">gpt_train</a> is used to train a model. It is controlled by the <a href="https://github.com/Foreseerr/TScale/blob/main/doc/train_script.md">train script</a> and <a href="https://github.com/Foreseerr/TScale/blob/main/doc/data_script.md">data script</a>. Default scripts are stored in <a href="https://github.com/Foreseerr/TScale/blob/main/code/gpt/train/main_gpt.cpp">main_gpt.cpp</a>. To load train script from file run gpt_train with '-d data_script.txt -s train_script.txt' arguments.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">quick run</h2><a id="user-content-quick-run" aria-label="Permalink: quick run" href="#quick-run"></a></p>
<p dir="auto">Compile gpt-train. Run it in the root directory:</p>
<div dir="auto" data-snippet-clipboard-copy-content="~/TScale$ ./make.dir/gpt-train"><pre><span>~</span>/TScale$ ./make.dir/gpt-train</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">sync distributed run</h2><a id="user-content-sync-distributed-run" aria-label="Permalink: sync distributed run" href="#sync-distributed-run"></a></p>
<p dir="auto">Currently training can be distributed only among pow2 number of worker hosts.</p>
<p dir="auto">To start a worker process run gpt_train with '-w 10000' argument. 10000 specifies port number to use.</p>
<p dir="auto">To run master process call net_train('worker.txt') function in train script. List worker IP addresses in the file provided to net_train().</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">multiple GPU</h2><a id="user-content-multiple-gpu" aria-label="Permalink: multiple GPU" href="#multiple-gpu"></a></p>
<p dir="auto">To use multiple GPU devices set DEVICE_COUNT variable in train script to number of GPUs to use. For distributed runs DEVICE_COUNT is applied on each worker, heterogeneous configurations are not supported.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">scripts</h2><a id="user-content-scripts" aria-label="Permalink: scripts" href="#scripts"></a></p>
<p dir="auto">Description of scripts used in training: <a href="https://github.com/Foreseerr/TScale/blob/main/doc/data_script.md">data script</a>, <a href="https://github.com/Foreseerr/TScale/blob/main/doc/train_script.md">train script</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Inference test</h2><a id="user-content-inference-test" aria-label="Permalink: Inference test" href="#inference-test"></a></p>
<p dir="auto">To try inferencing from the trained model you can use <a href="https://github.com/Foreseerr/TScale/blob/main/code/gpt/infer">gpt_infer</a>. It runs basic http server on 11311 port and allows sampling continuations from the model. Current implementation is slow and designed for demonstration purposes only.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Texan who built an empire of ecstasy (112 pts)]]></title>
            <link>https://www.texasmonthly.com/news-politics/ecstasy-starck-club-drugs-eighties-dallas/</link>
            <guid>43886552</guid>
            <pubDate>Sun, 04 May 2025 13:21:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.texasmonthly.com/news-politics/ecstasy-starck-club-drugs-eighties-dallas/">https://www.texasmonthly.com/news-politics/ecstasy-starck-club-drugs-eighties-dallas/</a>, See on <a href="https://news.ycombinator.com/item?id=43886552">Hacker News</a></p>
Couldn't get https://www.texasmonthly.com/news-politics/ecstasy-starck-club-drugs-eighties-dallas/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[The Alabama Landline That Keeps Ringing (290 pts)]]></title>
            <link>https://oxfordamerican.org/oa-now/the-alabama-landline-that-keeps-ringing</link>
            <guid>43886243</guid>
            <pubDate>Sun, 04 May 2025 12:02:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://oxfordamerican.org/oa-now/the-alabama-landline-that-keeps-ringing">https://oxfordamerican.org/oa-now/the-alabama-landline-that-keeps-ringing</a>, See on <a href="https://news.ycombinator.com/item?id=43886243">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>


          <div>


    <div>

      
      <h3>Auburn University’s help desk is still answering the public’s calls 70 years on</h3>

              
      
      <p>April 23, 2025</p>

    </div>

    <div>
              <p><img src="https://oxfordamerican.org/media/pages/oa-now/the-alabama-landline-that-keeps-ringing/099259767a-1744917496/foy-information-desk-at-auburn-university.jpg"></p><p>Foy Information Desk at Auburn University, Photo by Emily McCrary</p>
                          
    </div>


  </div>
    
    
                  <div>
    <p><span lang="EN"><sub><strong>I</strong></sub>f you sit at the James E. Foy Information Desk in the Melton Student Center at Auburn University, answering the phones on a Wednesday night, you might be responsible for answering a question like this: “If you died on the operating table and they declared you legally dead and wrote out a death certificate and everything, but then you came back to life, what are the legal ramifications? Do you technically no longer exist? Do you have to be declared undead by a judge?”</span></p>
<p><span lang="EN">A little later, the phone will ring again, and the caller might ask, “Who is the most famous person in the world?” Your next question: “How do you get the Super Serum in Call of Duty?” And finally, when you pick up the phone close to eleven o’clock, quitting time, you might hear someone blow a giant raspberry then hang up.</span></p>
<p><span lang="EN">I spent the better part of two days and nights listening to students answer questions at the Foy desk, where phones have been ringing since 1953, when James E. Foy, Auburn’s then dean of students, opened the line as a resource for students and then as a service to the public. For just as long, students who sit there have been answering any question asked of them—or at least tried their best.</span></p>  </div>
              <div>
      <h2><p>By nine o’clock, the student center is quiet. That’s when people like Beulah call.</p></h2>
      
    </div>
              <div>
    <p><span lang="EN"><sub><strong>T</strong></sub>he desk looks different today than it did seventy years ago. For starters, it’s in an expensive</span><span lang="EN">, modern-looking student center. The old Foy Hall still exists, and now houses a few small student-engagement offices; but it’s got low ceilings and could use an update. There used to be stacks of books at the desk—encyclopedias and dictionaries, reference texts, phone books, the <em>Farmers’ Almanac</em>, the <em>Guinness Book of World Records</em>, and Emily Post’s <em>Etiquette</em>—but they’ve been replaced by three desktop iMacs, the really nice ones, whose backs are blue and orange, like the school’s colors. As for the phone number, that’s still the same: (334) 844-4244.</span></p>
<p><span lang="EN">A day’s worth of calls to Foy would look a lot like someone’s browser history: What is cefuroxime prescribed for? What’s the average cost of an acre of land in Texas? What’s the cheapest property in New Jersey? How much is Elon Musk worth? What’s the customer service phone number for Costco? How much is a ticket to the Super Bowl? What is watercress? What is that weird smell? Are AirPods Pro 2 waterproof? What do I do if there’s a snake in my house? What are all the names for female genitals? (Foy student workers won’t respond to that last question, but that doesn’t stop callers from asking it.<a>)&nbsp;</a></span></p>
<p><span lang="EN">About <a>13 million people in the US </a><!-- [if !supportAnnotations]-->&nbsp;and <a>2.6 billion people globally </a><!-- [if !supportAnnotations]-->don’t use the internet, whether for reasons of availability, desire, cost, or religion. Some may have it but don’t feel confident using it. To their callers, who dial from all over the world, these students are the internet. And lucky for callers, these students are remarkably non-judgmental when it comes to the questions they’re asked. </span></p>
<p><span lang="EN">The employee manual for the Foy desk is ten sheets of 8.5x11, with instructions on how to clock in and out and pick up shifts. Training for the phones is minimal. More or less, the rules are these: Be as polite as possible, end the call if the question is offensive, don’t answer anything that sounds like a homework question, and if someone makes a threat, hang up and dial *57 (that helps the police trace the call), then tell your supervisor.</span></p>
<p><span lang="EN">During the day, the phones ring about ten to fifteen times an hour. Most of the calls are from the general public. Occasionally, an Auburn student calls to ask about basketball tickets or whether their brown jacket ended up in the lost and found. As classes wrap up for the afternoon and the sun sets, the big windows that filter sunlight turn into mirrors. Calls become less frequent, so students working at the desk settle into their homework. It’s a perk of the job. By nine o’clock, the student center is quiet. That’s when people like Beulah call.</span></p>  </div>
              <div>
      <h2><p>Foy student workers have exceptional manners, and they don’t go poking around in someone else’s business.</p></h2>
      
    </div>
              <div>
    <p><span lang="EN"><sub><strong>B</strong></sub>eulah has a lot of pets and an affinity for bearded dragons. She seems to attract wild animals too: There’s been a fox in her yard, a snake in her house, a spider on her curtains. She’s called to ask for the phone number for an exterminator more than once. While most callers are off the line in less than two minutes, Beulah’s calls last at least fifteen. But who she is, the students hardly know. Accents are sometimes a giveaway, but hers isn’t Southern, nor is it apparently from the Northeast, where her area code suggests she lives. Kamran Kimber, an overachieving junior studying business management, loves getting Beulah’s calls. “She sounds like—oh, my goodness—who was the lady who played in <em>White Lotus</em>?” He stops to think. “Jennifer Coolidge. She sounds like her.” </span></p>
<p><span lang="EN">Students glean information about their regular callers slowly. They’re not expressly forbidden to ask questions, but this is Alabama, and many of the students working at the desk were born and raised here or come from neighboring southern states. Even when drunk people from frat parties call, many student workers don’t hang up unless the partiers say something inappropriate. After all, being intoxicated doesn’t mean you don’t have a legitimate question. Foy student workers have exceptional manners, and they don’t go poking around in someone else’s business.</span></p>
<p><span lang="EN">With so little information, students are left to imagine who their callers might be. They stitch together an image informed by the caller’s voice, their accent, their area code, and their questions. In the 2010s, there was the Care Home Lady, who called every night. Matt Sheorn was picking up the phone back then, when he was an undergrad. He can’t remember the nature of Care Home Lady’s questions, just that he imagined her sitting alone in a nursing home, and that he was always happy to stay on the phone as long as she needed. “You can’t help but imagine what they’re like and in what setting,” he told me. “Is she just sitting in a rocking chair at night and just picks up that phone and keeps dialing Foy?” He wonders: Did she spend all day thinking about what question to ask?</span></p>  </div>
              <div>
              <p><img src="https://oxfordamerican.org/media/pages/oa-now/the-alabama-landline-that-keeps-ringing/028a8ef7cf-1744917494/photo-by-emily-mccrary-7.jpeg"></p><p>The Melton Student Center at Auburn University, home of the Foy Information Desk; Photo by Emily McCrary</p>
                          </div>


              <div>
      <h2><p><em>Why</em> someone calls doesn’t matter. It’s just their job to help.</p></h2>
      
    </div>
              <div>
    <p><span lang="EN"><sub><strong>E</strong></sub>ven though they don’t know much about the lives of their regulars, the students miss them if they don’t call. I asked some of the students about the last time they heard from Beulah, and no one could name a recent call. “I really hope she’s OK,” one of them told me, looking a little grave. </span></p>
<p><span lang="EN">Braxton Stacey is a senior studying industrial and systems engineering. He comes from Muscle Shoals, Alabama, a small town on the Tennessee River close to the borders of Mississippi and Tennessee. He has the pleasing and hard-to-identify southern accent of someone who comes from a border town. </span></p>
<p><span lang="EN">He’s a supervisor at the desk, which means he mostly <a>checks people into reserved conference rooms </a></span><span lang="EN"><!-- [if !supportAnnotations]--></span><span lang="EN">in the student center and locks up the building at the end of the night. He’s also backup for the three students answering calls. But before his promotion, he worked the phones.</span></p>
<p><span lang="EN">He told me that, one night, “this old woman called, and this is the only time I ever talked to her, but she talked for about an hour. She honestly wasn’t asking much of anything. But she told me—and I don’t know how true this is—she told me that one of her friends had recently passed away, and so she was just calling to talk. That felt good just to let her talk to me. I didn’t have to say much. She was just—she needed somebody.” She told Braxton about her grandkids and her house needing repairs and how she was getting them done. Ordinary things. “It wasn’t anything too specific,” he said. “Except for her friend.” </span></p>
<p><span lang="EN">Calls like these don’t come in as often as ones about, say, whether a certain type of rocking chair is on sale at Lowe’s or what the weather will be this weekend in Elkins Park, Pennsylvania. But every single one gets the same amount of respect.</span></p>
<p><span lang="EN">Call after call comes in during the hours that I sit at the desk. Someone calls to ask about the net worth of a real estate investor named David Lichtenstein. Another, who sounds like a tiny child, asks how far it is from Earth to Pluto. The questions are so varied, so unique, I ask the students, <a>“Don’t you wonder why these people call?” Sometimes, they tell me, but as far as they’re concerned, it’s none of their business. </a></span><span lang="EN">They admit that some people feel the need to explain why they call so often—that they live out in the sticks or can’t afford the internet. But to these students, <em>why</em> someone calls doesn’t matter. It’s just their job to help.</span></p>  </div>
              <div>
      <h2><p>Cora says she believes the calls she answers are the ones she’s meant to answer.</p></h2>
      
    </div>
              <div>
    <p><span lang="EN"><sub><strong>O</strong></sub>n my first night at the desk, a student supervisor comes out to shake my hand. It’s Cora Baldwin, a senior about to finish her degree in software engineering. Her voice is soft and gentle, maybe a little nervous, like she’s afraid she might be intruding. She shows me around the desk and introduces me to the students answering the phones. “I’m not really sure how to host you,” she says, “but I want to make you feel welcome. What can we do?”</span></p>
<p><span lang="EN">The students let me hover over their shoulders while they do their jobs. There’s a lot of “yes, ma’am” and “no, ma’am.” Students say they know which callers expect to be addressed that way. The thicker the southern accent, the greater the expectation.</span></p>
<p><span lang="EN">Hours later, the building is quiet, Foy is about to close for the night, and we’ve likely heard the last of the calls. The students and I are circled up in our rolling chairs, and they’re telling stories about their favorite calls. Many they’ve never shared before. Like late-night conversations in college tend to do, the mood becomes confessional. Cora says she believes the calls she answers are the ones she’s meant to answer. No coincidences. I want to know whether there have been any calls that the students just can’t get out of their heads. One of them talks about the hour they spent on the phone with a caller, helping her plan a trip from Arizona to Canada. Another talks about a kid who called to complain that he was bored, but clearly he was alone and very, very lonely. That was a long call.</span></p>
<p><span lang="EN">Then Cora tells us this: Two years ago, while working a day shift, she answered a call from an older gentleman. He had a list of celebrities and wanted her to look up their birthdays, month and day. “As I read out the dates, he would say something like ‘Mmmhmm, OK. OK, that makes sense.’” He told her that he could deduce things about people with no more information than that. They chatted about what Cora was studying, and what kind of career she had in mind. Another caller who needed someone to talk to, she figured. “Then he asked for my birthday.” When Cora told him, “He says, ‘Oh, you don’t want to be a software engineer, you want to work with people.’”</span></p>
<p><span lang="EN">He was right, though Cora had never said it out loud. “Really, I’ve always had an affinity toward broken people,” she says. “I’d like to set up a commune for runaway girls. Growing up in my small town, people really didn’t go into protective services. They just stayed in their bad situations. I’d like to do that.” This is news to her colleagues on the desk, who have always pictured her as a coder. But it makes sense, they tell me. Cora is exactly the kind of person who should be taking in people who need help. “But I probably won’t,” she says, and shrugs. “I need a job.”</span></p>
<p><span lang="EN">I say to her, “Still, when I asked you to name a call you can’t forget, that’s the one you brought up. It must have meant something to you.” It did, Cora admits. Like she said before, she doesn’t really believe in coincidences. But how that fits into her life, she can’t picture yet</span></p>  </div>
              </article><div>
        <h2><a href="https://oxfordamerican.org/authors/emily-mccrary">Emily McCrary</a></h2>
        <p>Emily McCrary is an independent journalist based in Richmond, VA, whose work has appeared in the <em>Economist</em>, the BBC, the <em>Washington Post</em>, Lit Hub, JSTOR Daily, and <em>Discover</em>, among many others. She is the recipient of a Virginia Press Association award for business and financial journalism.</p>      </div></div>]]></description>
        </item>
    </channel>
</rss>