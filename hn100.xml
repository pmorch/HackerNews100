<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 25 Jul 2023 14:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: PyCareer ‚Äì Exclusive Python Job Board (506 pts)]]></title>
            <link>https://www.pycareer.io/</link>
            <guid>36861908</guid>
            <pubDate>Tue, 25 Jul 2023 13:29:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pycareer.io/">https://www.pycareer.io/</a>, See on <a href="https://news.ycombinator.com/item?id=36861908">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>üêç PyCareer</h2><p>Python jobs from all over the web!</p><p>ü§ñ Collected with AI daily!</p><p>üöÄ 79 total job sources scraped daily!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What We Know About LLMs (105 pts)]]></title>
            <link>https://willthompson.name/what-we-know-about-llms-primer</link>
            <guid>36860992</guid>
            <pubDate>Tue, 25 Jul 2023 11:57:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://willthompson.name/what-we-know-about-llms-primer">https://willthompson.name/what-we-know-about-llms-primer</a>, See on <a href="https://news.ycombinator.com/item?id=36860992">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article id="block-what-we-know-about-llms-primer"><p><span><span>Will Thompson</span></span></p><p><span><span><span><span><span>@</span>July 23, 2023</span></span></span><span> </span></span></p><ul id="block-cf06de2e26d446f38d1b8f2444827acd"><li><a href="#13524c3f617649df8311bba44f426b26"><p><span><span>Intro</span></span></p></a></li><li><a href="#f707d2def7494b129f38a0f8cf72c3f5"><p><span><span>What We Mean By ‚ÄúLLM‚Äù</span></span></p></a></li><li><a href="#1e1db7bf46744ae9996a606fe76b8cdb"><p><span><span>What We Knew About LLMs (From Before)</span></span></p></a></li><li><a href="#89b981fe82344f2e9ca11e2c8b5b3958"><p><span><span>Generalization üß†</span></span></p></a></li><li><a href="#85b3ec4df96e4c878c1a6db6357269a2"><p><span><span>Power Laws in Performance üö®</span></span></p></a></li><li><a href="#eae32bda8e6f423da00cbd16d0ed325f"><p><span><span>Research Trends üìà</span></span></p></a></li><li><a href="#a72fc0eed97f4d01940b386423b0ff5c"><p><span><span>InstructGPT ü§ñ</span></span></p></a></li><li><a href="#c32b3388754146de815fc07ed8fbc2e1"><p><span><span>Steerability ‚õ∑Ô∏è &amp; Alignment üßòüèΩ‚Äç‚ôÄÔ∏è</span></span></p></a></li><li><a href="#63e48dd8cd8a4d46be4029ff8a031e47"><p><span><span>1. Instruction Fine-Tuning  üéõÔ∏è</span></span></p></a></li><li><a href="#44942fd81c604e8fa5b14272b008ba0b"><p><span><span>2. RLHF üíé</span></span></p></a></li><li><a href="#e66065b5c05343109388237954ef3311"><p><span><span>Conclusion: LLMs as Reasoning Agents ü§î</span></span></p></a></li><li><a href="#f1502945b85a48f684e5d7f6d842f07c"><p><span><span>Citation</span></span></p></a></li></ul><h2 id="block-13524c3f617649df8311bba44f426b26"><span id="13524c3f617649df8311bba44f426b26"></span><span><span>Intro</span></span></h2><div id="block-e6aa957fa7064aad95329c7de0339c27"><p><span><span></span><img alt="Crypto VCs &amp; ‚Äùbuilders‚Äù making a hard left into AI (Borrowed from ML Twitter)" sizes="100vw" srcset="https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F5b7f470c-e1fc-42a6-b868-8f685fe02778%2FUntitled.png&amp;w=640&amp;q=80 640w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F5b7f470c-e1fc-42a6-b868-8f685fe02778%2FUntitled.png&amp;w=750&amp;q=80 750w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F5b7f470c-e1fc-42a6-b868-8f685fe02778%2FUntitled.png&amp;w=828&amp;q=80 828w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F5b7f470c-e1fc-42a6-b868-8f685fe02778%2FUntitled.png&amp;w=1080&amp;q=80 1080w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F5b7f470c-e1fc-42a6-b868-8f685fe02778%2FUntitled.png&amp;w=1200&amp;q=80 1200w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F5b7f470c-e1fc-42a6-b868-8f685fe02778%2FUntitled.png&amp;w=1920&amp;q=80 1920w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F5b7f470c-e1fc-42a6-b868-8f685fe02778%2FUntitled.png&amp;w=2048&amp;q=80 2048w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F5b7f470c-e1fc-42a6-b868-8f685fe02778%2FUntitled.png&amp;w=3840&amp;q=80 3840w" src="https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F5b7f470c-e1fc-42a6-b868-8f685fe02778%2FUntitled.png&amp;w=3840&amp;q=80" decoding="async" data-nimg="responsive" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p><figcaption><span><span><em>Crypto VCs &amp; ‚Äùbuilders‚Äù making a hard left into AI (Borrowed from ML Twitter)</em></span></span></figcaption></div><p><span><span>We are in the midst of yet another AI Summer where the possible futures enumerated in the Press seem both equally amazing and terrifying. LLMs are predicted to both create immeasurable wealth for society as well as potentially compete with (or deprecate?) knowledge workers. While bond markets are trying to read the tea leaves on future Fed rate hikes, equity markets are bullish on all things AI. Many companies are rapidly adopting some form of AI play in order to appease shareholder FOMO. A large percentage of YC cohort members are, unsurprisingly, generative AI startups now. All the ‚ÄúMAANG‚Äùs (whatever they are called these days) seem to have some form of giant LLM they are building now. It‚Äôs as though crypto was forgotten overnight; the public imagination appears singularly captivated with what possibilities AI may usher forth. </span></span></p><p><span><span>The madness of crowds aside, it is worth reflecting on what we concretely know about LLMs at this point in time and how these insights sparked the latest AI fervor. This will help put into perspective the relevance of current research efforts and the possibilities that abound.</span></span></p><h2 id="block-f707d2def7494b129f38a0f8cf72c3f5"><span id="f707d2def7494b129f38a0f8cf72c3f5"></span><span><span>What We Mean By ‚ÄúLLM‚Äù</span></span></h2><p><span><span>When people say ‚ÄúLarge Language Models‚Äù, they typically are referring to a type of deep learning architecture called a Transformer. Transformers are models that work with sequence data (e.g. text, images, time series, etc) and are part of a larger family of models called  </span><span><a href="https://d2l.ai/chapter_recurrent-neural-networks/sequence.html" target="_blank" rel="noopener noreferrer">Sequence Models</a></span><span>. Many Sequence Models can also be thought of as Language Models, or models that learn a probability distribution of the next word/pixel/value in a sequence: </span><span><!--$--><span id=""><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>t</mi></msub><mi mathvariant="normal">‚à£</mi><msub><mi>w</mi><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>‚àí</mo><mn>2</mn></mrow></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(w_t|w_{t-1},w_{t-2},...)</annotation></semantics></math></span></span></span><!--/$--></span><span>. </span></span></p><div id="block-8ea137637f6c495898ef4406cd64c76b"><p><span><span></span><img alt="Figure 9.7.1. in this illustrated guide&nbsp;" sizes="100vw" srcset="https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F6ad86efb-513e-4e57-b6fd-0243da1034e7%2FUntitled.png&amp;w=640&amp;q=80 640w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F6ad86efb-513e-4e57-b6fd-0243da1034e7%2FUntitled.png&amp;w=750&amp;q=80 750w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F6ad86efb-513e-4e57-b6fd-0243da1034e7%2FUntitled.png&amp;w=828&amp;q=80 828w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F6ad86efb-513e-4e57-b6fd-0243da1034e7%2FUntitled.png&amp;w=1080&amp;q=80 1080w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F6ad86efb-513e-4e57-b6fd-0243da1034e7%2FUntitled.png&amp;w=1200&amp;q=80 1200w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F6ad86efb-513e-4e57-b6fd-0243da1034e7%2FUntitled.png&amp;w=1920&amp;q=80 1920w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F6ad86efb-513e-4e57-b6fd-0243da1034e7%2FUntitled.png&amp;w=2048&amp;q=80 2048w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F6ad86efb-513e-4e57-b6fd-0243da1034e7%2FUntitled.png&amp;w=3840&amp;q=80 3840w" src="https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F6ad86efb-513e-4e57-b6fd-0243da1034e7%2FUntitled.png&amp;w=3840&amp;q=80" decoding="async" data-nimg="responsive" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p><figcaption><span><span>Figure 9.7.1. in this illustrated guide&nbsp;</span><span><a href="https://d2l.ai/chapter_recurrent-modern/seq2seq.html" target="_blank" rel="noopener noreferrer">here</a></span><span>. This is a type of encoder-decoder RNN. Notice the left-to-right causal ordering. Each token is processed sequentially.</span></span></figcaption></div><p><span><span>What differentiates the Transformer from its predecessors is it‚Äôs ability to learn the contextual relationship of values within a sequence through a mechanism called (self-) </span><span><a href="https://lilianweng.github.io/posts/2018-06-24-attention/" target="_blank" rel="noopener noreferrer">Attention</a></span><span>. Unlike the Recurrent Neural Network (</span><span><a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks" target="_blank" rel="noopener noreferrer">RNN</a></span><span>), where the arrow of time is preserved by processing each time step serially within a sequence, Transformers can read the entire sequence at once and learn to ‚Äúpay attention to‚Äù only the values that came earlier in time (via ‚Äúmasking‚Äù). This allows for faster training times (i.e. the whole sequence in parallel) and larger model parameter sizes. Transformers were once considered ‚Äúlarge‚Äù when they were ~ 100MM+ parameters; today, published models are </span><span><strong>~500B-1T parameters</strong></span><span> in size. Anecdotally, several papers have reported a major inflection point in Transformer behavior around ~</span><span><strong>100B+</strong></span><span> parameters. (Note: these models are generally too large to fit into a single GPU and require the model to be broken apart and distributed across multiple nodes).</span></span></p><div id="block-fd7c5af78fd946f7b25873b190a09312"><p><span><span></span><img alt="From Jay Alammar‚Äôs popular " srcset="https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fb1fe8e95-631d-4c6a-9cde-edb751bfcf96%2FUntitled.png&amp;w=640&amp;q=80 1x, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fb1fe8e95-631d-4c6a-9cde-edb751bfcf96%2FUntitled.png&amp;w=1080&amp;q=80 2x" src="https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fb1fe8e95-631d-4c6a-9cde-edb751bfcf96%2FUntitled.png&amp;w=1080&amp;q=80" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p><figcaption><span><span>From Jay Alammar‚Äôs popular </span><span><a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer">‚ÄúThe Illustrated Transformer‚Äù</a></span><span> blog post.  A context window is used to read in K tokens simultaneously in order to understand the relative meaning of words in a sentence. This is opposed to the RNN (see above), which technically can be thought as having an infinite context window (an observation by Karpathy), although each word must be processed serially. RNNs have a history of requiring more clever architecture tweaks to overcome their innate inability to connect long-term dependencies in a sentence, which is actually where the original (hierarchical) attention mechanism was born out of necessity.</span></span></figcaption></div><p><span><span>Transformers can be generally categorized into one of three categories: ‚Äú</span><span><strong>encoder only</strong></span><span>‚Äù (a la </span><span><a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener noreferrer">BERT</a></span><span>); ‚Äú</span><span><strong>decoder only</strong></span><span>‚Äù (a la </span><span><a href="https://openai.com/gpt-4" target="_blank" rel="noopener noreferrer">GPT</a></span><span>); and having an ‚Äú</span><span><strong>encoder-decoder</strong></span><span>‚Äù architecture (a la </span><span><a href="https://arxiv.org/abs/1910.10683" target="_blank" rel="noopener noreferrer">T5</a></span><span>). Although all of these architectures can be rigged for a broad range of tasks (e.g. classification, translation, etc),  </span><span><a href="https://huggingface.co/learn/nlp-course/chapter1/5?fw=pt" target="_blank" rel="noopener noreferrer">encoders</a></span><span> are thought to be useful for tasks where the entire sequence needs to be understood (such as sentiment classification), whereas </span><span><a href="https://huggingface.co/learn/nlp-course/chapter1/6" target="_blank" rel="noopener noreferrer">decoders</a></span><span> are thought to be useful for tasks where text needs to be completed (such as completing a sentence). Encoder-decoder architectures can be applied to a variety of problems, but are most famously associated with language translation.</span></span></p><p><span><span><em>Decoder-only Transformers such as ChatGPT &amp; GPT-4 are the class of LLM that are ubiquitously referring to as ‚Äúgenerative AI‚Äù.</em></span></span></p><h2 id="block-1e1db7bf46744ae9996a606fe76b8cdb"><span id="1e1db7bf46744ae9996a606fe76b8cdb"></span><span><span>What We Knew About LLMs (From Before)</span></span></h2><p><span><span>Since the debut of the </span><span><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">OG Transformer paper</a></span><span> ~6 years ago, we‚Äôve gleamed a couple interesting properties about this class of models.</span></span></p><h2 id="block-89b981fe82344f2e9ca11e2c8b5b3958"><span id="89b981fe82344f2e9ca11e2c8b5b3958"></span><span><span>Generalization üß†</span></span></h2><p><span><span>We learned that the the same trained LLM could figure out how to complete many different tasks with only being shown a few examples for each task; that is, </span><span><a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener noreferrer">LLMs are few-shot learners</a></span><span>. This meant that whatever the LLM had learned about language in it‚Äôs (pre-)training task (which is usually predicting the next word in a sequence), it could translate to new tasks without needing to be trained from scratch to do said task (and with only a handful of examples). </span></span></p><p><span><span>That is, we discovered LLMs‚Äô capacity to </span><span><em>generalize</em></span><span>.</span></span></p><div id="block-15f5462f917748768b78dc4b30517aac"><p><span><span></span><img alt="Figure 1.2 in " sizes="100vw" srcset="https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F1e48b4d2-1a1d-4839-bc7d-5f8665056ab0%2FScreenshot_2023-06-13_at_6.02.22_PM.png&amp;w=640&amp;q=80 640w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F1e48b4d2-1a1d-4839-bc7d-5f8665056ab0%2FScreenshot_2023-06-13_at_6.02.22_PM.png&amp;w=750&amp;q=80 750w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F1e48b4d2-1a1d-4839-bc7d-5f8665056ab0%2FScreenshot_2023-06-13_at_6.02.22_PM.png&amp;w=828&amp;q=80 828w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F1e48b4d2-1a1d-4839-bc7d-5f8665056ab0%2FScreenshot_2023-06-13_at_6.02.22_PM.png&amp;w=1080&amp;q=80 1080w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F1e48b4d2-1a1d-4839-bc7d-5f8665056ab0%2FScreenshot_2023-06-13_at_6.02.22_PM.png&amp;w=1200&amp;q=80 1200w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F1e48b4d2-1a1d-4839-bc7d-5f8665056ab0%2FScreenshot_2023-06-13_at_6.02.22_PM.png&amp;w=1920&amp;q=80 1920w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F1e48b4d2-1a1d-4839-bc7d-5f8665056ab0%2FScreenshot_2023-06-13_at_6.02.22_PM.png&amp;w=2048&amp;q=80 2048w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F1e48b4d2-1a1d-4839-bc7d-5f8665056ab0%2FScreenshot_2023-06-13_at_6.02.22_PM.png&amp;w=3840&amp;q=80 3840w" src="https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F1e48b4d2-1a1d-4839-bc7d-5f8665056ab0%2FScreenshot_2023-06-13_at_6.02.22_PM.png&amp;w=3840&amp;q=80" decoding="async" data-nimg="responsive" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p><figcaption><span><span>Figure 1.2 in </span><span><a href="https://arxiv.org/pdf/2005.14165.pdf" target="_blank" rel="noopener noreferrer">the GPT-3 paper</a></span><span>. Shows that larger models are better at generalizing to new tasks (especially via ‚Äúin-context learning‚Äù).</span></span></figcaption></div><h2 id="block-85b3ec4df96e4c878c1a6db6357269a2"><span id="85b3ec4df96e4c878c1a6db6357269a2"></span><span><span>Power Laws in Performance üö®</span></span></h2><p><span><span>We also learned that LLMs had </span><span><a href="https://arxiv.org/pdf/2001.08361.pdf" target="_blank" rel="noopener noreferrer">predictable (power law) scaling behavior</a></span><span>. With larger training datasets, models could scale up in parameter size and become more data efficient, ultimately leading to better performance on benchmarks. </span></span></p><p><span><span>Given a dataset size and chosen model size, we can (seemingly magically) predict the performance of the model prior to (pre-)training it.</span></span></p><h2 id="block-eae32bda8e6f423da00cbd16d0ed325f"><span id="eae32bda8e6f423da00cbd16d0ed325f"></span><span><span>Research Trends üìà</span></span></h2><p><span><span>Given these observations, a large research trend in LLMs was</span><span><strong> training progressively larger and larger LLMs</strong></span><span> and measuring their performance on benchmarks (although, some papers such as the</span><span><a href="https://arxiv.org/abs/2103.00020" target="_blank" rel="noopener noreferrer"> CLIP paper</a></span><span> call into question whether benchmark performance actually reflected generalizability, part of a nuanced observation called ‚ÄúThe Cheating Hypothesis‚Äù). This required splitting these models across many GPUs/TPUs (i.e. </span><span><strong>model parallelism</strong></span><span>) due in large part to a model tweak provided from </span><span><a href="https://arxiv.org/abs/1909.08053" target="_blank" rel="noopener noreferrer">the Megatron paper</a></span><span>, </span><span><a href="https://lilianweng.github.io/posts/2021-09-25-train-large/" target="_blank" rel="noopener noreferrer">innovations in model/pipeline sharding</a></span><span>, and packages such as </span><span><a href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener noreferrer">DeepSpeed</a></span><span>. </span><span><a href="https://arxiv.org/abs/2209.05433" target="_blank" rel="noopener noreferrer">Quantization</a></span><span> also reduced the memory and computational footprint of these models. And since the traditional self-attention mechanism at the core of the Transformer is </span><span><!--$--><span id=""><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N^2)</annotation></semantics></math></span></span></span><!--/$--></span><span> in space and time complexity,  naturally research into faster mechanisms such as </span><span><a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener noreferrer">Flash Attention</a></span><span> was of considerable interest. Further, innovations like </span><span><a href="https://arxiv.org/pdf/2108.12409.pdf" target="_blank" rel="noopener noreferrer">Alibi</a></span><span> allowed for variable context windows. This opened the door to </span><span><strong>larger context windows </strong></span><span>and is the reason why today‚Äôs LLMs have as large as </span><span><a href="https://www.anthropic.com/index/100k-context-windows" target="_blank" rel="noopener noreferrer">100k token context windows</a></span><span>. </span></span></p><p><span><span>And given the size of these (very large) LLMs, there was interest in how to </span><span><strong>fine-tune them to problems more efficiently</strong></span><span>. Innovation in Parameter-Efficient Fine-Tuning (</span><span><strong>PEFT</strong></span><span>) such as </span><span><a href="https://arxiv.org/pdf/1902.00751.pdf" target="_blank" rel="noopener noreferrer">Adapters</a></span><span> and </span><span><a href="https://arxiv.org/pdf/2106.09685.pdf" target="_blank" rel="noopener noreferrer">LoRA</a></span><span> allowed for faster fine-tuning since there are fewer parameters to adjust in these paradigms. Combined with the advent of 4- and 8-bit </span><span><strong>quantization</strong></span><span>, it‚Äôs now even possible to fine-tune a model on CPU! (note: most models are trained using 16 or 32 bit floats)</span></span></p><p><span><span>[</span><span><strong>Note</strong></span><span>: This is not a comprehensive overview of research trends. For instance, there was considerable research into other areas such as LLMs‚Äô ability to </span><span><a href="https://arxiv.org/abs/2107.06499" target="_blank" rel="noopener noreferrer">regurgitate information</a></span><span>, </span><span><a href="https://arxiv.org/pdf/2104.13733.pdf" target="_blank" rel="noopener noreferrer">adversarial attacks</a></span><span>, and domain specific LLMs such as </span><span><a href="https://arxiv.org/pdf/2107.03374.pdf" target="_blank" rel="noopener noreferrer">Codex</a></span><span> (for writing code) as well as early-stage multimodal LLMs (i.e. Transformers that understand images, text, etc). Further, </span><span><a href="https://arxiv.org/pdf/2112.04426.pdf" target="_blank" rel="noopener noreferrer">RETRO</a></span><span> and </span><span><a href="https://arxiv.org/pdf/2112.09332.pdf" target="_blank" rel="noopener noreferrer">webGPT</a></span><span> showed that smaller LLMs could perform the same as larger models with efficient querying/ information retrieval].</span></span></p><p><span><span>[And some of these papers like Flash Attention and LoRA came chronologically after the papers discussed in the next few sections].</span></span></p><h2 id="block-a72fc0eed97f4d01940b386423b0ff5c"><span id="a72fc0eed97f4d01940b386423b0ff5c"></span><span><span>InstructGPT ü§ñ</span></span></h2><p><span><span>Yet, a major breakthrough in our understanding of LLM behavior was made with the release of </span><span><a href="https://arxiv.org/abs/2203.02155" target="_blank" rel="noopener noreferrer">the instructGPT paper</a></span><span>. </span></span></p><p><span><span>GPT-3 (particularly the large parameter kind) already demonstrated the ability to follow natural language instructions (i.e. ‚Äúprompts‚Äù), although these instructions typically needed to be </span><span><em>carefully worded </em></span><span>to get a desired output. </span></span></p><p><span><span>Yet, the output tended to be regurgitated language found deep in the dark corners of the Internet: </span><span><em>unfiltered</em></span><span>, likely </span><span><em>offensive</em></span><span>, </span><span><em>untruthful</em></span><span> and most probably </span><span><em>not</em></span><span> the response the user wanted. If the response wasn‚Äôt regurgitated, it could even be entirely made up (what are typically referred to as ‚Äú</span><span><strong>hallucinations</strong></span><span>‚Äù). </span></span></p><p><span><span>That is to say, the LLMs‚Äô capacity to follow natural language instructions could be quite </span><span><em>underwhelming</em></span><span>. </span></span></p><div id="block-b630ee614965424e992bdf837e20f0b4"><p><span><span></span><img alt="GPT-3 v. InstructGPT in response to a prompt. From OpenAI‚Äôs " sizes="100vw" srcset="https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fd18b685a-7738-4f28-87ea-e084dd49b259%2FScreenshot_2023-06-19_at_10.29.13_PM.png&amp;w=640&amp;q=80 640w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fd18b685a-7738-4f28-87ea-e084dd49b259%2FScreenshot_2023-06-19_at_10.29.13_PM.png&amp;w=750&amp;q=80 750w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fd18b685a-7738-4f28-87ea-e084dd49b259%2FScreenshot_2023-06-19_at_10.29.13_PM.png&amp;w=828&amp;q=80 828w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fd18b685a-7738-4f28-87ea-e084dd49b259%2FScreenshot_2023-06-19_at_10.29.13_PM.png&amp;w=1080&amp;q=80 1080w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fd18b685a-7738-4f28-87ea-e084dd49b259%2FScreenshot_2023-06-19_at_10.29.13_PM.png&amp;w=1200&amp;q=80 1200w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fd18b685a-7738-4f28-87ea-e084dd49b259%2FScreenshot_2023-06-19_at_10.29.13_PM.png&amp;w=1920&amp;q=80 1920w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fd18b685a-7738-4f28-87ea-e084dd49b259%2FScreenshot_2023-06-19_at_10.29.13_PM.png&amp;w=2048&amp;q=80 2048w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fd18b685a-7738-4f28-87ea-e084dd49b259%2FScreenshot_2023-06-19_at_10.29.13_PM.png&amp;w=3840&amp;q=80 3840w" src="https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fd18b685a-7738-4f28-87ea-e084dd49b259%2FScreenshot_2023-06-19_at_10.29.13_PM.png&amp;w=3840&amp;q=80" decoding="async" data-nimg="responsive" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p><figcaption><span><span>GPT-3 v. InstructGPT in response to a prompt. From OpenAI‚Äôs </span><span><a href="https://openai.com/research/instruction-following" target="_blank" rel="noopener noreferrer">blog post</a></span><span>.</span></span></figcaption></div><h2 id="block-c32b3388754146de815fc07ed8fbc2e1"><span id="c32b3388754146de815fc07ed8fbc2e1"></span><span><span>Steerability ‚õ∑Ô∏è &amp; Alignment üßòüèΩ‚Äç‚ôÄÔ∏è</span></span></h2><p><span><span>Training LLMs to predict the next token in a sentence has been surprisingly effective in teaching them to learn a generalizable representation of language. Train for this task over a large corpus (like the entire Internet), maybe add some other prediction tasks into the mix (translation, classification, etc - i.e. ‚Äú</span><span><em>mixture of objectives</em></span><span>‚Äù), and voila, you have yourself an LLM that, if large enough, can be easily taught to do other specific things with only a handful of examples (i.e. few-shot learners).</span></span></p><p><span><span>Yet, this training objective does not seem to translate into LLMs following ‚Äú</span><span><strong>user intent</strong></span><span>‚Äù.</span></span></p><p><span><span>LLMs might be able to answer multiple choice questions well or be fine-tuned to do some specific task; but left to their own devices, they aren‚Äôt shown to be particularly good at following user instructions </span><span><em>without significant guardrails </em></span><span>(particularly in a zero-shot setting). </span></span></p><p><span><span>In fact, many a time they tend to regurgitate an answer that they‚Äôve seen before, or ignore the instructions entirely and ramble, or give a confident sounding answer that was non-sense (i.e. hallucinations). This is the problem that the ML research community calls </span><span><strong>steerability</strong></span><span> - the ability to prompt an LLM to provide a desired result.</span></span></p><div id="block-c9c12af17d3149b58733da9f171b8844"><p><span><span>Compound this with the desire that an LLM output </span><span><strong>embody a set of values</strong></span><span> (e.g. lack racism or homophobia, or say</span><span><strong> </strong></span><span><strong><a href="https://arxiv.org/pdf/2204.05862.pdf" target="_blank" rel="noopener noreferrer">Anthropic‚Äôs HHH</a></strong></span><span>) or quality of output expected by the user and we begin to appreciate the surface of </span><span><a href="https://ai-alignment.com/?gi=903be471f45d" target="_blank" rel="noopener noreferrer"><strong>the alignment problem</strong></a></span><span>. What </span><span><a href="https://openai.com/blog/democratic-inputs-to-ai" target="_blank" rel="noopener noreferrer">values an LLM is aligned to</a></span><span>, how to evaluate for alignment, and whether </span><span><a href="https://openai.com/blog/our-approach-to-alignment-research" target="_blank" rel="noopener noreferrer">we can align systems more intelligent than ourselves</a></span><span> are all interesting open research threads.</span></span></p></div><p><span><span>Within the instructGPT paper, the authors developed a 2 part solution to trying to tackle this problem: </span></span></p><ol type="1"><li id="block-17622ca1f78c452fb85c8e3f4261c19a"><span><span>Supervised (Instruction) Fine-tuning  (SFT)</span></span></li><li id="block-310b159822f34ce59119be1db49ca2b5"><span><span>Reinforcement Learning via Human Feedback (RLHF)</span></span></li></ol><p><span><span>Using these 2 sequential training tasks, the authors are able to convert a GPT-3 into what they call </span><span><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>InstructGPT</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span><span>. </span></span></p><p><span><span>The InstructGPT paper‚Äôs results showed that simply creating larger models was an </span><span><em>insufficient condition</em></span><span> for developing a steerable and aligned model:</span><span><strong> in fact, the 175B GPT-3 ‚Äúprompted‚Äù model performed worse on average than the 1.3B parameter InstructGPT</strong></span><span>.</span></span></p><div id="block-fe0d3856caf04645b909099586af9e38"><p><span><span></span><img alt="Comparison of human assessments of the outputs of GPT-3, GPT-3-prompted, ‚ÄúSFT‚Äù, and InstructGPT (‚ÄùSFT‚Äù+‚ÄùRLHF‚Äù). Again from the OpenAI " sizes="100vw" srcset="https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F54317062-ef3b-482b-bf45-636d0c1e6d79%2FScreenshot_2023-07-23_at_2.33.16_PM.png&amp;w=640&amp;q=80 640w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F54317062-ef3b-482b-bf45-636d0c1e6d79%2FScreenshot_2023-07-23_at_2.33.16_PM.png&amp;w=750&amp;q=80 750w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F54317062-ef3b-482b-bf45-636d0c1e6d79%2FScreenshot_2023-07-23_at_2.33.16_PM.png&amp;w=828&amp;q=80 828w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F54317062-ef3b-482b-bf45-636d0c1e6d79%2FScreenshot_2023-07-23_at_2.33.16_PM.png&amp;w=1080&amp;q=80 1080w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F54317062-ef3b-482b-bf45-636d0c1e6d79%2FScreenshot_2023-07-23_at_2.33.16_PM.png&amp;w=1200&amp;q=80 1200w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F54317062-ef3b-482b-bf45-636d0c1e6d79%2FScreenshot_2023-07-23_at_2.33.16_PM.png&amp;w=1920&amp;q=80 1920w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F54317062-ef3b-482b-bf45-636d0c1e6d79%2FScreenshot_2023-07-23_at_2.33.16_PM.png&amp;w=2048&amp;q=80 2048w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F54317062-ef3b-482b-bf45-636d0c1e6d79%2FScreenshot_2023-07-23_at_2.33.16_PM.png&amp;w=3840&amp;q=80 3840w" src="https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F54317062-ef3b-482b-bf45-636d0c1e6d79%2FScreenshot_2023-07-23_at_2.33.16_PM.png&amp;w=3840&amp;q=80" decoding="async" data-nimg="responsive" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p><figcaption><span><span>Comparison of human assessments of the outputs of GPT-3, GPT-3-prompted, ‚ÄúSFT‚Äù, and InstructGPT (‚ÄùSFT‚Äù+‚ÄùRLHF‚Äù). Again from the OpenAI </span><span><a href="https://openai.com/research/instruction-following" target="_blank" rel="noopener noreferrer">blog post</a></span><span>. Using humans to ascertain the quality of a generative model‚Äôs output makes more sense than using a traditional set of benchmarks that need quantitative answers. Both the training and test labelers (completely different set of people - OpenAI did an incredible job designing an experimental design that minimizes label leakage and overfitting to one‚Äôs preferences) were selected using a rigorous evaluation criteria to make sure they were ‚Äúaligned‚Äù with OpenAI‚Äôs values they want to align their LLM to. These Likert scale measurements capture the steerability and alignment quality of these model outputs.</span></span></figcaption></div><p><span><span>That is to say,</span><span><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong> h</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span><span><em><strong>ow</strong></em></span><span> you trained your LLM can be </span><span><strong>equally as important</strong></span><span> a knob as </span><span><strong>model size</strong></span><span>.</span></span></p><p><span><span>And while you might not have heard of InstructGPT, this paper‚Äôs recipe was the basis for  </span><span><strong><a href="https://chat.openai.com/auth/login" target="_blank" rel="noopener noreferrer">chatGPT</a></strong></span><span><strong> </strong></span><span>as well as a plethora of open source models that one can find on HuggingFace (check out the </span><span><a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" target="_blank" rel="noopener noreferrer">ever changing ü§ó&nbsp;LLM Leaderboard</a></span><span>).</span></span></p><div id="block-92988009e1b140a3a04e3273bb41fcac"><p><span><span></span><img alt="What is a blog post without a meme?" srcset="https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F07ac2735-c209-4ef0-b888-61dd7000cf60%2FScreenshot_2023-07-23_at_2.44.14_PM.png&amp;w=640&amp;q=80 1x, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F07ac2735-c209-4ef0-b888-61dd7000cf60%2FScreenshot_2023-07-23_at_2.44.14_PM.png&amp;w=1080&amp;q=80 2x" src="https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F07ac2735-c209-4ef0-b888-61dd7000cf60%2FScreenshot_2023-07-23_at_2.44.14_PM.png&amp;w=1080&amp;q=80" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p><figcaption><span><span>What is a blog post without a meme?</span></span></figcaption></div><h2 id="block-63e48dd8cd8a4d46be4029ff8a031e47"><span id="63e48dd8cd8a4d46be4029ff8a031e47"></span><span><span>1. Instruction Fine-Tuning  üéõÔ∏è</span></span></h2><p><span><span>The first step to improving the output of a generative model logically seems to be teach it to follow instructions better. </span></span></p><p><span><span>To that end, the authors culminated a series of prompts via submissions to their playground API from real users (i.e. tackle the cold-start problem) as well as a mixture of prompting tasks of their own devise. Using a rigorous selection process,  labelers were hired to produce high-quality outputs that were aligned with </span><span><u>Anthropic‚Äôs 3 Laws</u></span><span>: helpful, honest and harmless</span><span><strong> (HHH</strong></span><span>, or maybe </span><span><a href="https://en.wikipedia.org/wiki/Triple_H" target="_blank" rel="noopener noreferrer">Triple H</a></span><span>?</span><span><strong>). </strong></span></span></p><p><span><span>Using these gold-standard labels, they fine-tuned a GPT model to learn these outputs. This is what they call ‚Äú</span><span><strong>Supervised Fine-tuning</strong></span><span>‚Äù (SFT).</span></span></p><h2 id="block-44942fd81c604e8fa5b14272b008ba0b"><span id="44942fd81c604e8fa5b14272b008ba0b"></span><span><span>2. RLHF üíé</span></span></h2><p><span><span>Now what about teaching an LLM to produce outputs that embody a set of values?</span></span></p><p><span><span>While the public perception of OpenAI is predominantly tied to GPT* models, a non-trivial percentage of their efforts has been focused on attacking </span><span><a href="https://openai.com/blog/our-approach-to-alignment-research" target="_blank" rel="noopener noreferrer">the alignment problem</a></span><span>. Their solution to this problem, thus far, has been what they call </span><span><strong>Reinforcement Learning via Human Feedback  (RLHF) </strong></span><span>(note: this is expected to not work in the case of </span><span><a href="https://openai.com/blog/introducing-superalignment" target="_blank" rel="noopener noreferrer">superalignment</a></span><span>, or teaching systems smarter than ourselves our values). </span></span></p><p><span><span>Teaching an LLM a set of human values is a challenging modeling task. Traditional loss functions usually measure the model‚Äôs internal belief about the probability the data belongs to the correct label (i.e. cross entropy). But human values are more complex; they are hard to encapsulate within a single label . Rather than </span><span><em>explicitly</em></span><span> labeling data, it might be easier for a human to read two or more LLM outputs and encode their preferences through </span><span><em>comparison</em></span><span>. An AI system can then </span><span><strong>implicitly</strong></span><span> learn these preferences by learning to correctly rank a set of options. </span></span></p><p><span><span>Along these lines, RLHF seeks to teach an AI system a set of preferences by learning a </span><span><strong>reward function/model </strong></span><span>through interacting with a human and gaining their input. </span></span></p><p><span><span>This reward model is then used to </span><span><strong>guide the LLM</strong></span><span> to produce higher-valued outputs through </span><span><strong>reinforcement learning. </strong></span><span> At each step, the reward model takes an input and output and returns a ‚Äúpreferability‚Äù number; the LLM seeks to narrow the difference between the ‚Äúoptimal‚Äù preference and it‚Äôs current output over many interactions. This is used to train the LLM to produce a set of outputs that are ‚Äúaligned‚Äù with the values of the labelers/modelers/etc (for a decent primer on RLHF, check out HuggingFace‚Äôs blog </span><span><a href="https://huggingface.co/blog/rlhf" target="_blank" rel="noopener noreferrer">post</a></span><span>).</span></span></p><div id="block-fa3ffdbc02a343628c2a55f38013926e"><p><span><span></span><img alt="Labeler output rankings. Figure 12 in the appendix of the " sizes="100vw" srcset="https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fa6d60548-2dc3-4165-bf28-d1e7da1702ed%2FScreenshot_2023-07-23_at_3.35.43_PM.png&amp;w=640&amp;q=80 640w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fa6d60548-2dc3-4165-bf28-d1e7da1702ed%2FScreenshot_2023-07-23_at_3.35.43_PM.png&amp;w=750&amp;q=80 750w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fa6d60548-2dc3-4165-bf28-d1e7da1702ed%2FScreenshot_2023-07-23_at_3.35.43_PM.png&amp;w=828&amp;q=80 828w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fa6d60548-2dc3-4165-bf28-d1e7da1702ed%2FScreenshot_2023-07-23_at_3.35.43_PM.png&amp;w=1080&amp;q=80 1080w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fa6d60548-2dc3-4165-bf28-d1e7da1702ed%2FScreenshot_2023-07-23_at_3.35.43_PM.png&amp;w=1200&amp;q=80 1200w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fa6d60548-2dc3-4165-bf28-d1e7da1702ed%2FScreenshot_2023-07-23_at_3.35.43_PM.png&amp;w=1920&amp;q=80 1920w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fa6d60548-2dc3-4165-bf28-d1e7da1702ed%2FScreenshot_2023-07-23_at_3.35.43_PM.png&amp;w=2048&amp;q=80 2048w, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fa6d60548-2dc3-4165-bf28-d1e7da1702ed%2FScreenshot_2023-07-23_at_3.35.43_PM.png&amp;w=3840&amp;q=80 3840w" src="https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2Fa6d60548-2dc3-4165-bf28-d1e7da1702ed%2FScreenshot_2023-07-23_at_3.35.43_PM.png&amp;w=3840&amp;q=80" decoding="async" data-nimg="responsive" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p><figcaption><span><span>Labeler output rankings. Figure 12 in the appendix of the </span><span><a href="https://arxiv.org/pdf/2203.02155.pdf" target="_blank" rel="noopener noreferrer">paper</a></span><span>. The labeler selection process and metrics are worth diving into.</span></span></figcaption></div><p><span><span>Among the theoretical benefits of using reinforcement learning to teach an LLM human preferences is </span><span><strong>the potential for reducing an LLM‚Äôs tendency towards regurgitation</strong></span><span> (as John Schulman and others have emphasized recently in lectures). Rather than overfitting/blindly learning a specific label distribution, RLHF is using rewards/penalties to guide the LLM outputs towards a more optimal answer through incentives without explicitly seeing a label (as in next token prediction).</span></span></p><p><span><span>InstructGPT was shown to produce outputs that were less toxic, more truthful, and more steerable.  In fact, this paper was a major catalyst&nbsp;in the </span><span><em>outpouring</em></span><span> of new innovations using LLMs. </span></span></p><h2 id="block-e66065b5c05343109388237954ef3311"><span id="e66065b5c05343109388237954ef3311"></span><span><span>Conclusion: LLMs as Reasoning Agents ü§î</span></span></h2><p><span><span>An emergent property of LLMs that we have gleamed in the wake of the InstructGPT paper is their capacity as </span><span><strong>reasoning agents</strong></span><span>. </span></span></p><div id="block-796a00d399c9419d83d6779999b13cc4"><p><span><span></span><img alt="An observation by Jan Leike about instructGPT." srcset="https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F8b9b7f9d-c0d5-4dc3-9ce6-7967efaaeb7b%2FScreenshot_2023-07-24_at_12.04.04_AM.png&amp;w=640&amp;q=80 1x, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F8b9b7f9d-c0d5-4dc3-9ce6-7967efaaeb7b%2FScreenshot_2023-07-24_at_12.04.04_AM.png&amp;w=1080&amp;q=80 2x" src="https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F8b9b7f9d-c0d5-4dc3-9ce6-7967efaaeb7b%2FScreenshot_2023-07-24_at_12.04.04_AM.png&amp;w=1080&amp;q=80" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p><figcaption><span><span>An observation by Jan Leike about instructGPT.</span></span></figcaption></div><p><span><span>Given a set of instructions, an instruction fine-tuned/aligned LLM is able (conditional on size and training quality) to</span><span><strong> reason through a set of steps</strong></span><span> to produce a desired output. This has lead to a flurry of research into different prompt writing techniques: </span><span><strong>self-ask</strong></span><span>, </span><span><strong>chain-of-thought,</strong></span><span> etc. are all different parlor tricks designed to guide an LLM towards deducing the right answer. This set of techniques is what the ML community calls </span><span><a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/" target="_blank" rel="noopener noreferrer"><strong>prompt engineering</strong></a></span><span>, which seems more an art form than a science these days. </span></span></p><div id="block-da7f0c9a17914c1b8b80cbe50aaafda6"><p><span><span></span><img alt="Great " srcset="https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F74512b9a-a50c-47c4-95b0-76ac8d0a9d77%2FScreenshot_2023-07-15_at_6.00.49_PM.png&amp;w=640&amp;q=80 1x, https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F74512b9a-a50c-47c4-95b0-76ac8d0a9d77%2FScreenshot_2023-07-15_at_6.00.49_PM.png&amp;w=1080&amp;q=80 2x" src="https://willthompson.name/_next/image?url=https%3A%2F%2Fassets.super.so%2Fd0376c53-81de-4b6c-b0fb-0a16f4ae9da5%2Fimages%2F74512b9a-a50c-47c4-95b0-76ac8d0a9d77%2FScreenshot_2023-07-15_at_6.00.49_PM.png&amp;w=1080&amp;q=80" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p><figcaption><span><span>Great </span><span><a href="https://twitter.com/karpathy/status/1617979122625712128?s=20" target="_blank" rel="noopener noreferrer">tweet</a></span><span> from Karpathy on prompting. Read the sub-tweets citing some interesting papers. </span></span></figcaption></div><p><span><span>As LLMs improve over time, it is possible that researchers/practitioners will not need to lean on these techniques as much, despite sensational claims in the media that you can make </span><span><a href="https://www.forbes.com/sites/jodiecook/2023/07/12/ai-prompt-engineers-earn-300k-salaries-heres-how-to-learn-the-skill-for-free/?sh=78287b69d4a1" target="_blank" rel="noopener noreferrer">$300k being a  prompt engineer</a></span><span>.</span></span></p><p><span><span>Along with research into prompt engineering, LLM-centric tech stacks (what many refer to as </span><span><strong><a href="https://lilianweng.github.io/posts/2023-06-23-agent/" target="_blank" rel="noopener noreferrer">autonomous agents</a></strong></span><span>) is an area of enthusiasm. These </span><span><strong>agents</strong></span><span> utilize LLMs as problem solvers; provision them with a set of tools at their disposal and these models can solve a surprising number of tasks. </span></span></p><p><span><span>Much of the open source efforts are focused on developing these tools- it‚Äôs an exciting time to see what possibilities LLMs will further unlock üòé.</span></span></p><h2 id="block-f1502945b85a48f684e5d7f6d842f07c"><span id="f1502945b85a48f684e5d7f6d842f07c"></span><span><span>Citation</span></span></h2><!--$--><div id="block-3bebe81e0adf4cdc9d013c38c04a4447"><pre><code>@article{
  title   = "What We Know About LLMs (Primer)",
  author  = "Thompson, Will",
  journal = "https://willthompson.name",
  year    = "2023",
  month   = "July",
  day   = "23",
  url     = "https://willthompson.name/what-we-know-about-llms-primer"
}</code></pre><figcaption><span></span></figcaption></div><!--/$--></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Invoice Dragon ‚Äì An open source app to create PDF invoices (124 pts)]]></title>
            <link>https://invoicedragon.com/</link>
            <guid>36860898</guid>
            <pubDate>Tue, 25 Jul 2023 11:45:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://invoicedragon.com/">https://invoicedragon.com/</a>, See on <a href="https://news.ycombinator.com/item?id=36860898">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><main><main><div><svg xmlns="http://www.w3.org/2000/svg" width="168" height="155" fill="#ffffff" viewBox="0 0 256 256"><path d="M216,40H40A16,16,0,0,0,24,56V208a8,8,0,0,0,11.58,7.15L64,200.94l28.42,14.21a8,8,0,0,0,7.16,0L128,200.94l28.42,14.21a8,8,0,0,0,7.16,0L192,200.94l28.42,14.21A8,8,0,0,0,232,208V56A16,16,0,0,0,216,40ZM176,144H80a8,8,0,0,1,0-16h96a8,8,0,0,1,0,16Zm0-32H80a8,8,0,0,1,0-16h96a8,8,0,0,1,0,16Z"></path></svg><h2>A fast and convenient solution for effortlessly creating Invoices and Receipts.<br>Absolutely Free !</h2></div></main></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Common Pitfalls in Go Benchmarking (1203 pts)]]></title>
            <link>https://eli.thegreenplace.net/2023/common-pitfalls-in-go-benchmarking/</link>
            <guid>36860065</guid>
            <pubDate>Tue, 25 Jul 2023 09:51:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eli.thegreenplace.net/2023/common-pitfalls-in-go-benchmarking/">https://eli.thegreenplace.net/2023/common-pitfalls-in-go-benchmarking/</a>, See on <a href="https://news.ycombinator.com/item?id=36860065">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                
                <p>Go programmers have the good fortune of excellent testing and benchmarking
tooling built into the standard library - in the <tt>testing</tt> package. However,
<em>benchmarking is hard</em>. This isn't Go specific; it's just one of those things
experienced developers learn over time.</p>
<p>This post lists some common benchmarking pitfalls Go programmers run into. It
assumes basic familiarity with writing Go benchmarks; consult the <a href="https://pkg.go.dev/testing">testing
package documentation</a> if needed. While these
pitfalls are presented in Go, they exist in any programming language or
environment, so the lessons learned here are widely applicable.</p>
<div id="benchmarking-the-wrong-thing">
<h2>Benchmarking the wrong thing</h2>
<p>Let's say we want to benchmark the new sorting functionality available in the
<tt>slices</tt> package starting with Go 1.21 <a href="#footnote-1" id="footnote-reference-1">[1]</a>. Consider the following
benchmark:</p>
<div><pre><span></span><span>const</span><span> </span><span>N</span><span> </span><span>=</span><span> </span><span>100</span><span>_000</span><span></span>

<span>func</span><span> </span><span>BenchmarkSortIntsWrong</span><span>(</span><span>b</span><span> </span><span>*</span><span>testing</span><span>.</span><span>B</span><span>)</span><span> </span><span>{</span><span></span>
<span>  </span><span>ints</span><span> </span><span>:=</span><span> </span><span>makeRandomInts</span><span>(</span><span>N</span><span>)</span><span></span>
<span>  </span><span>b</span><span>.</span><span>ResetTimer</span><span>()</span><span></span>

<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>b</span><span>.</span><span>N</span><span>;</span><span> </span><span>i</span><span>++</span><span> </span><span>{</span><span></span>
<span>    </span><span>slices</span><span>.</span><span>Sort</span><span>(</span><span>ints</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>
<span>}</span><span></span>

<span>func</span><span> </span><span>makeRandomInts</span><span>(</span><span>n</span><span> </span><span>int</span><span>)</span><span> </span><span>[]</span><span>int</span><span> </span><span>{</span><span></span>
<span>  </span><span>ints</span><span> </span><span>:=</span><span> </span><span>make</span><span>([]</span><span>int</span><span>,</span><span> </span><span>n</span><span>)</span><span></span>
<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>n</span><span>;</span><span> </span><span>i</span><span>++</span><span> </span><span>{</span><span></span>
<span>    </span><span>ints</span><span>[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>rand</span><span>.</span><span>Intn</span><span>(</span><span>n</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>
<span>  </span><span>return</span><span> </span><span>ints</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>Why is this benchmark wrong? Because it's not really testing what you think it's
testing. <tt>slices.Sort</tt> sorts a slice <em>in-place</em>. After the first iteration,
the <tt>ints</tt> slice is already sorted, so all subsequent iterations "sort"
a sorted slice. This is not what we want to measure <a href="#footnote-2" id="footnote-reference-2">[2]</a>. The
right measurement would create a new random slice for each iteration:</p>
<div><pre><span></span><span>func</span><span> </span><span>BenchmarkSortInts</span><span>(</span><span>b</span><span> </span><span>*</span><span>testing</span><span>.</span><span>B</span><span>)</span><span> </span><span>{</span><span></span>
<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>b</span><span>.</span><span>N</span><span>;</span><span> </span><span>i</span><span>++</span><span> </span><span>{</span><span></span>
<span>    </span><span>b</span><span>.</span><span>StopTimer</span><span>()</span><span></span>
<span>    </span><span>ints</span><span> </span><span>:=</span><span> </span><span>makeRandomInts</span><span>(</span><span>N</span><span>)</span><span></span>
<span>    </span><span>b</span><span>.</span><span>StartTimer</span><span>()</span><span></span>
<span>    </span><span>slices</span><span>.</span><span>Sort</span><span>(</span><span>ints</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>On my machine, the second benchmark is almost 100x slower; this makes sense,
because it actually sorts something on each iteration.</p>
</div>
<div id="forgetting-to-reset-the-timer">
<h2>Forgetting to reset the timer</h2>
<p>Note how the benchmarks mentioned above are careful to reset or stop/start
the benchmarking timer around certain operations. This is on purpose, because
the benchmark scaffold measures the run-time of the entire <tt>Benchmark*</tt>
function, executing it many times and dividing the total execution time by
<tt>b.N</tt> (much more details on this process later in the post).</p>
<p>Try it! Remove the <tt>b.StopTimer()</tt> and <tt>b.StartTimer()</tt> calls from the
benchmarking loop in <tt>BenchmarkSortInts</tt> and compare the results - you'll
notice the benchmark now reports noticeably slower per-op execution time
because creating the slices of random integers also happens <tt>b.N</tt> times.</p>
<p>This can throw results off significantly in some cases. Even if we use the
same technique to benchmark two approaches, the errors don't necessarily cancel
out. Due to <a href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahl's law</a> we
can easily report the wrong speed-up when not isolating the computation we
care about.</p>
</div>
<div id="fooled-by-the-compiler">
<h2>Fooled by the compiler</h2>
<p>The trickiest benchmarking pitfall is dealing with compiler optimizations
thwarting our results. The Go compiler doesn't give <tt>Benchmark*</tt> functions
any preferential treatment and will try to optimize them and their contents
just as it would any other Go code <a href="#footnote-3" id="footnote-reference-3">[3]</a>. This produces wrong results and it's
not even easy to know they are wrong! Consider this benchmark:</p>
<div><pre><span></span><span>func</span><span> </span><span>isCond</span><span>(</span><span>b</span><span> </span><span>byte</span><span>)</span><span> </span><span>bool</span><span> </span><span>{</span><span></span>
<span>  </span><span>if</span><span> </span><span>b</span><span>%</span><span>3</span><span> </span><span>==</span><span> </span><span>1</span><span> </span><span>&amp;&amp;</span><span> </span><span>b</span><span>%</span><span>7</span><span> </span><span>==</span><span> </span><span>2</span><span> </span><span>&amp;&amp;</span><span> </span><span>b</span><span>%</span><span>17</span><span> </span><span>==</span><span> </span><span>11</span><span> </span><span>&amp;&amp;</span><span> </span><span>b</span><span>%</span><span>31</span><span> </span><span>==</span><span> </span><span>9</span><span> </span><span>{</span><span></span>
<span>    </span><span>return</span><span> </span><span>true</span><span></span>
<span>  </span><span>}</span><span></span>
<span>  </span><span>return</span><span> </span><span>false</span><span></span>
<span>}</span><span></span>

<span>func</span><span> </span><span>BenchmarkIsCondWrong</span><span>(</span><span>b</span><span> </span><span>*</span><span>testing</span><span>.</span><span>B</span><span>)</span><span> </span><span>{</span><span></span>
<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>b</span><span>.</span><span>N</span><span>;</span><span> </span><span>i</span><span>++</span><span> </span><span>{</span><span></span>
<span>    </span><span>isCond</span><span>(</span><span>201</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>We're trying to benchmark the <tt>isCond</tt> function, but the compiler is
fooling us (at least in recent Go versions):</p>
<div><pre><span></span>BenchmarkIsCondWrong-8        1000000000     0.2401 ns/op
</pre></div>
<p>Sub-nanosecond operation times are always a bit suspect, although not entirely
out of the question given the simplicity of the <tt>isCond</tt> function. There are
two serious errors here, however:</p>
<ol>
<li>We use a constant input to <tt>isCond</tt>; since <tt>isCond</tt> is a simple function
that is likely to be inlined into the benchmark function, the compiler can
theoretically constant-propagate its input through its contents and replace
the code with the compile-time computed answer.</li>
<li>Even if we used a non-constant for the input, the result of <tt>isCond</tt> is
unused, so the compiler may optimize it away.</li>
</ol>
<p>And in fact, in this example the contents of the benchmark loop are optimized
away entirely; looking at the disassembly, we see this:</p>
<div><pre><span></span>    JMP     BenchmarkIsCondWrong_pc7
BenchmarkIsCondWrong_pc4:
    INCQ    CX
BenchmarkIsCondWrong_pc7:
    CMPQ    416(AX), CX
    JGT     BenchmarkIsCondWrong_pc4
    RET
</pre></div>
<p><tt>CX</tt> holds <tt>i</tt>, and <tt>416(AX)</tt> accesses <tt>b.N</tt>. This is just an empty
loop! Now the 0.24 nanoseconds per iteration make sense <a href="#footnote-4" id="footnote-reference-4">[4]</a>.</p>
<p>Here's an even more confounding manifestation of the same problem:</p>
<div><pre><span></span><span>func</span><span> </span><span>countCond</span><span>(</span><span>b</span><span> </span><span>[]</span><span>byte</span><span>)</span><span> </span><span>int</span><span> </span><span>{</span><span></span>
<span>  </span><span>result</span><span> </span><span>:=</span><span> </span><span>0</span><span></span>
<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>len</span><span>(</span><span>b</span><span>);</span><span> </span><span>i</span><span>++</span><span> </span><span>{</span><span></span>
<span>    </span><span>if</span><span> </span><span>isCond</span><span>(</span><span>b</span><span>[</span><span>i</span><span>])</span><span> </span><span>{</span><span></span>
<span>      </span><span>result</span><span>++</span><span></span>
<span>    </span><span>}</span><span></span>
<span>  </span><span>}</span><span></span>
<span>  </span><span>return</span><span> </span><span>result</span><span></span>
<span>}</span><span></span>

<span>func</span><span> </span><span>BenchmarkCountWrong</span><span>(</span><span>b</span><span> </span><span>*</span><span>testing</span><span>.</span><span>B</span><span>)</span><span> </span><span>{</span><span></span>
<span>  </span><span>inp</span><span> </span><span>:=</span><span> </span><span>getInputContents</span><span>()</span><span></span>
<span>  </span><span>b</span><span>.</span><span>ResetTimer</span><span>()</span><span></span>
<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>b</span><span>.</span><span>N</span><span>;</span><span> </span><span>i</span><span>++</span><span> </span><span>{</span><span></span>
<span>    </span><span>countCond</span><span>(</span><span>inp</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>
<span>}</span><span></span>

<span>func</span><span> </span><span>getInputContents</span><span>()</span><span> </span><span>[]</span><span>byte</span><span> </span><span>{</span><span></span>
<span>  </span><span>n</span><span> </span><span>:=</span><span> </span><span>400000</span><span></span>
<span>  </span><span>buf</span><span> </span><span>:=</span><span> </span><span>make</span><span>([]</span><span>byte</span><span>,</span><span> </span><span>n</span><span>)</span><span></span>
<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>n</span><span>;</span><span> </span><span>i</span><span>++</span><span> </span><span>{</span><span></span>
<span>    </span><span>buf</span><span>[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>byte</span><span>(</span><span>n</span><span> </span><span>%</span><span> </span><span>32</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>
<span>  </span><span>return</span><span> </span><span>buf</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>Now there are certainly no constant propagation issues, and the result of
<tt>isCond</tt> is clearly used inside <tt>countCond</tt>. But we've committed the same
error! While the result of <tt>isCond</tt> is used, the result of <tt>countCond</tt> is
not, and therefore the compiler will do something like:</p>
<ul>
<li>Inline <tt>isCond</tt> into <tt>countCond</tt></li>
<li>Inline <tt>countCond</tt> into the benchmark function</li>
<li>Realize that the loop body in <tt>coundCond</tt> isn't producing side effects or
any result used outside it</li>
<li>Hollow out the loop body in <tt>countCond</tt>, leaving only an empty loop</li>
</ul>
<p>This process results in confusing benchmark results because the loop
remains in place, and thus if we grow the input, the benchmarked execution
time will grow accordingly! One of the oldest tricks in the benchmarking book is
to change the input size and observe how the run-time is affected; if it's not
moving, something is wrong. If it's growing roughly in line with the asymptotic
complexity of the code under test, it's at least passing a simple smoke test.
But in this case the heuristic fails due to the specific optimizations
performed.</p>
<div id="keeping-compiler-optimizations-in-benchmarks-under-control">
<h3>Keeping compiler optimizations in benchmarks under control</h3>
<p>There are two main techniques in Go to thwart compiler optimizations where we
don't want them.</p>
<p>The first is using the <a href="https://pkg.go.dev/runtime#KeepAlive">runtime.KeepAlive</a>
function. Originally introduced for fine-grained control of finalizers, it's
also useful as a very-low-overhead way to tell the compiler "I really need this
value, even if you can prove I do not". Here's how we can fix our <tt>countCond</tt>
benchmark with its help:</p>
<div><pre><span></span><span>func</span><span> </span><span>BenchmarkCountKeepAlive</span><span>(</span><span>b</span><span> </span><span>*</span><span>testing</span><span>.</span><span>B</span><span>)</span><span> </span><span>{</span><span></span>
<span>  </span><span>inp</span><span> </span><span>:=</span><span> </span><span>getInputContents</span><span>()</span><span></span>
<span>  </span><span>b</span><span>.</span><span>ResetTimer</span><span>()</span><span></span>
<span>  </span><span>result</span><span> </span><span>:=</span><span> </span><span>0</span><span></span>
<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>b</span><span>.</span><span>N</span><span>;</span><span> </span><span>i</span><span>++</span><span> </span><span>{</span><span></span>
<span>    </span><span>result</span><span> </span><span>+=</span><span> </span><span>countCond</span><span>(</span><span>inp</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>
<span>  </span><span>runtime</span><span>.</span><span>KeepAlive</span><span>(</span><span>result</span><span>)</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>Running the benchmark makes it clear that there's more work going into each
iteration now:</p>
<div><pre><span></span>BenchmarkCountWrong-8                 12481       95911 ns/op
BenchmarkCountKeepAlive-8              4143      285527 ns/op
</pre></div>
<p>Another way is without needing a special function, but is slightly more
dangerous; we can use a global exported value to collect the result:</p>
<div><pre><span></span><span>var</span><span> </span><span>Sink</span><span> </span><span>int</span><span></span>

<span>func</span><span> </span><span>BenchmarkCountSink</span><span>(</span><span>b</span><span> </span><span>*</span><span>testing</span><span>.</span><span>B</span><span>)</span><span> </span><span>{</span><span></span>
<span>  </span><span>inp</span><span> </span><span>:=</span><span> </span><span>getInputContents</span><span>()</span><span></span>
<span>  </span><span>b</span><span>.</span><span>ResetTimer</span><span>()</span><span></span>
<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>b</span><span>.</span><span>N</span><span>;</span><span> </span><span>i</span><span>++</span><span> </span><span>{</span><span></span>
<span>    </span><span>Sink</span><span> </span><span>+=</span><span> </span><span>countCond</span><span>(</span><span>inp</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>Even though our benchmark doesn't use <tt>Sink</tt>, it's very difficult for the
compiler to eliminate because it's a package-level exported value that can be
used pretty much anywhere in the program. I did say it's slightly more
dangerous, though, because theoretically the compiler <em>could</em> prove it's
redundant using more sophisticated cross-package analysis.</p>
<p>Which brings us the to ultimate point: when in doubt, always double check the
assembly generated for the benchmark function, and ensure that the compiler
wasn't too clever.</p>
</div>
<div id="ongoing-work-in-the-go-standard-library">
<h3>Ongoing work in the Go standard library</h3>
<p>Addressing the issue of compiler optimizations in benchmarks is something the Go
team is interested in. There are currently two active proposals in discussion:</p>
<ul>
<li><a href="https://github.com/golang/go/issues/61179">Issue 61179</a>: adding a
low-overhead "keep this value" function in the <tt>testing</tt> package; this would
be similar to <tt>runtime.KeepAlive</tt>, but more targeted at benchmarks with
a better name and clearer guarantees.</li>
<li>A more ambitious proposal in <a href="https://github.com/golang/go/issues/61515">issue 61515</a> discusses changing the main
benchmarking API of the <tt>testing</tt> package to facilitate safer benchmarking.</li>
</ul>
</div>
</div>
<div id="misusing-b-n">
<h2>Misusing <tt>b.N</tt></h2>
<p>There are at least two common ways to misuse the benchmark repetition indicator
<tt>b.N</tt>. Here's the first, completely forgetting the loop:</p>
<div><pre><span></span><span>import</span><span> </span><span>(</span><span></span>
<span>  </span><span>"crypto/rand"</span><span></span>
<span>  </span><span>"testing"</span><span></span>
<span>)</span><span></span>

<span>func</span><span> </span><span>BenchmarkRandPrimeWrongNoLoop</span><span>(</span><span>b</span><span> </span><span>*</span><span>testing</span><span>.</span><span>B</span><span>)</span><span> </span><span>{</span><span></span>
<span>  </span><span>rand</span><span>.</span><span>Prime</span><span>(</span><span>rand</span><span>.</span><span>Reader</span><span>,</span><span> </span><span>200</span><span>)</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>From proper measurements, the <tt>crypto/rand.Prime</tt> invocation with length 200
should take around 1 ms on my machine, but this benchmark reports impossibly
low times - fractions of a nanosecond.</p>
<p>The second is trickier:</p>
<div><pre><span></span><span>func</span><span> </span><span>BenchmarkRandPrimeWrongUseI</span><span>(</span><span>b</span><span> </span><span>*</span><span>testing</span><span>.</span><span>B</span><span>)</span><span> </span><span>{</span><span></span>
<span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>b</span><span>.</span><span>N</span><span>;</span><span> </span><span>i</span><span>++</span><span> </span><span>{</span><span></span>
<span>    </span><span>rand</span><span>.</span><span>Prime</span><span>(</span><span>rand</span><span>.</span><span>Reader</span><span>,</span><span> </span><span>i</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>Note how it's using <tt>i</tt> in each iteration (and thus indirectly <tt>b.N</tt>). On my
machine, this benchmark <strong>doesn't terminate</strong>; what gives?</p>
<p>To understand what's going on here, we should first discuss how benchmarking in
Go works in a bit more detail. The benchmarking harness invokes our benchmark
functions several times. How many? This can be
determined by flags like <tt><span>-benchtime</span></tt>, but the default is for one second.
How does the harness know how many repetitions to pass to our benchmark function
in order to get 1s of execution time? By trying it with low repetitions first,
measuring how long it takes, and then running with increasingly high repetitions
until the intended duration is reached <a href="#footnote-5" id="footnote-reference-5">[5]</a>.</p>
<p>There are some nuances to the process: e.g. the number of repetitions doesn't
grow too fast between attempts (at most 100x, as of Go 1.21), and it is capped
at 1 billion. The key point is that it uses the execution time from the
previous attempt to determine how many repetitions to run next.</p>
<p>With these insights in mind, let's examine the two misuses shown above.</p>
<ol>
<li>Forgetting to loop until <tt>b.N</tt>. Each benchmark invocation only performs the
tested operations once. No matter how many times the benchmark harness
repeats the benchmark with increasing <tt>b.N</tt>, the function takes a
millisecond to run! Therefore, eventually the benchmark harness hits its
<tt>N</tt> limit of a billion invocations, and divides the execution time (1 ms)
by this <tt>N</tt> to get a nonsensical result.</li>
<li>Using the value of <tt>b.N</tt> for something other than "how many times to
repeat the benchmark". <tt>rand.Prime</tt> is very fast when its input length
is small, but gets pretty slow for large inputs. The harness starts by
running the function once to get its bearings, and then 100 times. For size
100 the run-time of <tt>rand.Prime</tt> is moderate, so the next time the harness
can increase <tt>b.N</tt> by another factor of 100. But for higher inputs,
<tt>rand.Prime</tt> also takes much longer. We end up with a quadratic run-time
explosion! Our benchmark function isn't literally hanging - it <em>will</em> finish
eventually, but it may take long minutes or hours.</li>
</ol>
<p>It's worth mentioning that the proposal in <a href="https://github.com/golang/go/issues/61515">issue 61515</a> would address this problem as
well, by exposing a benchmarking API that would be much harder to misuse in this
way. Another proposal that may affect this is <a href="https://github.com/golang/go/issues/61405">add range over int</a>, which will permit us to write
<tt>for range b.N</tt> to iterate exactly <tt>b.N</tt> times, without an explicit
iteration variable.</p>
<hr>
<table id="footnote-1">
<colgroup><col><col></colgroup>
<tbody>
<tr><td><a href="#footnote-reference-1">[1]</a></td><td><p>For background on this work and why the new generic sorting functions
are faster, see <a href="https://eli.thegreenplace.net/2022/faster-sorting-with-go-generics/">this post</a>.</p>
<p>If you're reading this before Go 1.21's final release (in Aug 2023),
you can download the
<a href="https://go.dev/blog/go1.21rc">release candidate</a> or use
<a href="https://pkg.go.dev/golang.org/dl/gotip">gotip</a>.</p>
</td></tr>
</tbody>
</table>
<table id="footnote-2">
<colgroup><col><col></colgroup>
<tbody>
<tr><td><a href="#footnote-reference-2">[2]</a></td><td>To be clear, it's also important to know how well sorting algorithms
perform on sorted or almost-sorted inputs, but here we're talking about
the more general case.</td></tr>
</tbody>
</table>
<table id="footnote-3">
<colgroup><col><col></colgroup>
<tbody>
<tr><td><a href="#footnote-reference-3">[3]</a></td><td>While it's possible to think of ways to treat <tt>Benchmark*</tt> specially,
it's pretty hard to do it correctly because we very much <em>want</em> the
compiler to optimize the functions we're benchmarking. After all, we
want to know what their real run-time is. Drawing the line
between "optimize here" and "don't touch there" is tricky!</td></tr>
</tbody>
</table>
<table id="footnote-4">
<colgroup><col><col></colgroup>
<tbody>
<tr><td><a href="#footnote-reference-4">[4]</a></td><td>The Go compiler doesn't currently optimize loops entirely away, unless
it can prove they terminate.</td></tr>
</tbody>
</table>
<table id="footnote-5">
<colgroup><col><col></colgroup>
<tbody>
<tr><td><a href="#footnote-reference-5">[5]</a></td><td>This also has the side effect of providing whatever <em>warming</em> the
benchmark needs w.r.t. caches (CPU caches, paged memory, file system
caches, DNS caches, etc).</td></tr>
</tbody>
</table>
</div>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[From Python to Elixir Machine Learning (159 pts)]]></title>
            <link>https://www.thestackcanary.com/from-python-pytorch-to-elixir-nx/</link>
            <guid>36859785</guid>
            <pubDate>Tue, 25 Jul 2023 09:04:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thestackcanary.com/from-python-pytorch-to-elixir-nx/">https://www.thestackcanary.com/from-python-pytorch-to-elixir-nx/</a>, See on <a href="https://news.ycombinator.com/item?id=36859785">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>As Elixir's Machine Learning (ML) ecosystem grows, many Elixir enthusiasts who wish to adopt the new machine learning libraries in their projects are stuck at a crossroads of wanting to move away from their existing ML stack (typically Python) while not having a clear path of how to do so. I would like to take some time to talk to WHY I believe now is a good time to start porting over Machine Learning code into Elixir, and HOW I went about doing just this for two libraries I wrote: <a href="https://github.com/acalejos/exgboost?ref=thestackcanary.com">EXGBoost</a> (from Python XGBoost) and <a href="https://github.com/acalejos/mockingjay?ref=thestackcanary.com">Mockingjay</a> (from Python Hummingbird).</p>
<h2 id="why-is-python-not-sufficient">Why is Python not Sufficient?</h2>
<p>There's a common saying in programming languages that no language is perfect, but that different languages are suited for different jobs. Languages such as C, Rust, and now even Zig are known for their targeting systems development, while languages such as C++, C#, and Java are more commonly used for application development, and obviously there are the web languages such as JavaScript/TypeScript, PHP, Ruby (on Rails), and more. There are gradations to these rules of course, but more often than not there are good reasons that languages tend to exist within the confines of particular use cases. </p>
<p> Languages such as Elixir and Go tend to be used in large distributed systems because they place an emphasis on having great support for common concurrency patterns, which can come at the cost of supporting other domains. Go, for example, has barely (if any?) support for machine learning libraries, but it's also not trying to cater to that as a target domain. For a long time,e the same could have been said about Elixir, but over the past two or so years, there has been a massive concerted push from the Elixir community to not only have support for machine learning, but to push the envelope with the maintaining state of the art libraries that are beginning to compete with the other dominant machine learning languages - namely Python. </p>
<p>Python has long been the gold standard in the realm of machine learning. The breadth of libraries and the low entry barrier makes Python a great language to work with, but it does create a bit of a bottleneck. Any application that wishes to integrate machine learning has historically had only a couple of options: have a Python component or reach into the underlying libraries that power much of the Python libraries directly. Despite all the good parts of Python I mentioned before, speed and support for concurrency are not on that list. Elixir-Nx is striving to give another option - an option that can take advantage of the native distributed support that Elixir and the BEAM VM have to offer. Nx's <code>Nx.Serving</code> construct is a drop-in solution for serving distributed machine-learning models. </p>
<h2 id="how-to-proceed">How to Proceed</h2>
<p>Sean Moriarity, the co-creator of Nx, creator of Axon, and author of <a href="https://pragprog.com/titles/smelixir/machine-learning-in-elixir/?ref=thestackcanary.com">Machine Learning in Elixir</a>, has talked many times about how the initial creation of Nx and Axon involved hours upon hours of reading source code from reference implementations of libraries in Python and C++, namely the Tensorflow source code. While I was writing <a href="https://github.com/acalejos/exgboost?ref=thestackcanary.com">EXGBoost</a> and <a href="https://github.com/acalejos/mockingjay?ref=thestackcanary.com">Mockingjay</a>, much of my time, especially towards the beginning, was spent referencing the Python and C++ implementations of the original libraries.  This builds a great fundamental understanding of the libraries as well as taught me how to identify patterns in Python and C++ and identify the Elixir pattern that could express the same ideas. This skill is invaluable, and the better I got at it the faster I could write. Below is a summary and key takeaways from my process of porting Python / PyTorch to Elixir / Nx. </p>
<h2 id="workflow-overview">Workflow Overview</h2>
<p>Before I get to the examples from the code bases, I would like to briefly explain the high-level cyclical workflow I established while working on this effort, and what I would recommend to anyone pursuing a similar endeavor. </p>
<h3 id="understand-the-macro-system">Understand the Macro System</h3>
<p> Much like how there's a common strategy to reading comprehension which involves reading through the entire document once to get a high-level understanding and then doing subsequent shorter reads to gain more in-depth understanding with the added context of the entire piece, you can consider doing the same when reading code. My first step was to follow the logical flow from the call of <code>hummingbird.ml.convert</code> to the final result. You can use tools such as function tracers and callgraph generators to accelerate this part of the process, or manually trace depending on the extent of the codebase. I felt in my case that it was manageable to trace myself. </p>
<h3 id="read-the-documentation">Read the Documentation</h3>
<p>Once you have a general understanding of the flow and process of the original system, you can start referring to the documentation for some additional context. In my case, this lead me to the academic paper <a href="https://scnakandala.github.io/papers/TR_2020_Hummingbird.pdf?ref=thestackcanary.com"><em>Taming Model Serving Complexity, Performance and Cost: A Compilation to Tensor Computations Approach</em></a><em>, </em>which was the underlying ground work and basis for their implementation. I could write a whole other blog post about the process of transcribing algorithms and code from academic papers and pseudocode, but for now just know that these are some of the most important pieces you can refer to while re-implementing or porting over a piece of source code. </p>
<h3 id="read-the-source-code-in-detail">Read the Source Code in Detail</h3>
<p>This is the point in which you want to disambiguate the higher-level ideas from the first step and really gain a fine, high-resolution understanding of what is happening. There might even be some points in which you need to deconflict the source code with its documentation and/or paper reference. In those cases, the source code almost always wins, and if not, then you likely have a bug report you can file. If you see things you don't fully understand, you don't necessarily need to address it here, but you should make note of it and keep it in mind while working in case new details help resolve it. </p>
<h3 id="implement-the-new-code">Implement the New Code</h3>
<p>At this point, you should feel comfortable enough to start implementing the code. I found this to be a very iterative process, meaning I would think I had a grasp on something, then would start working on implementing it, then would realize I did not understand it as well as I had thought and would work my way back through the previous steps. </p>
<h2 id="example">Example</h2>

<h2 id="class-vs-behaviour">Class vs. Behaviour</h2>
<p>As a result of the reading and comprehension I did of the Hummingbird code base, I realized fairly early on that my library was going to have some key differences. One of the main reasons for these differences was the fact that the Hummingbird code base was built as a retroactive library that needed to cater to existing APIs that existed throughout the Python ecosystem. They chose to only add support for converting decision trees according to the SKLearn API. I, conversely, chose to write Mockingjay in such a way that it would be incumbent upon the authors of decision tree libraries to implement a protocol to interface with Mockingjay's <code>convert</code> function. This difference meant that I could establish a <code>Mockingjay.Tree</code> data structure that I would use throughout my library, rather than having to reconstruct tree features from various other APIs as is done in Hummingbird. </p>
<p>Next, Hummingbird approaches its pipeline in a very-object oriented manner, as makes sense when using Python. Here' we are focusing on the implementation of the three decision tree conversion strategies: GEMM, Tree Traversal, and PErfect Tree Traversal.  It implements the following base class for tree conversions as well as Pytorch networks. </p>
<div><p>üí°</p><p dir="ltr"><span> Since they're inheriting from </span><code><span>torch.nn.model</span></code><span> they must also implement the </span><code><span>forward</span></code><span> method.</span></p></div>
<pre><code>class AbstracTreeImpl(PhysicalOperator):
    """
    Abstract class definig the basic structure for tree-base models.
    """

    def __init__(self, logical_operator, **kwargs):
        super().__init__(logical_operator, **kwargs)

    @abstractmethod
    def aggregation(self, x):
        """
        Method defining the aggregation operation to execute after the model is evaluated.

        Args:
            x: An input tensor

        Returns:
            The tensor result of the aggregation
        """
        pass

class AbstractPyTorchTreeImpl(AbstracTreeImpl, torch.nn.Module):
    """
    Abstract class definig the basic structure for tree-base models implemented in PyTorch.
    """

    def __init__(
        self, logical_operator, tree_parameters, n_features, classes, n_classes, decision_cond="&lt;=", extra_config={}, **kwargs
    ):
        """
        Args:
            tree_parameters: The parameters defining the tree structure
            n_features: The number of features input to the model
            classes: The classes used for classification. None if implementing a regression model
            n_classes: The total number of used classes
            decision_cond: The condition of the decision nodes in the x &lt;cond&gt; threshold order. Default '&lt;='. Values can be &lt;=, &lt;, &gt;=, &gt;
        """
        super(AbstractPyTorchTreeImpl, self).__init__(logical_operator, **kwargs)</code></pre>
<p>They then proceed to inherit from these base classes and have different classes for each of the three decision tree strategies as well as their gradient-boosted counterparts, leaving them with three classes for each strategies (1 base class per strategy, 1 for ensemble implementations, and 1 for normal impementations) and nine total classes. </p>
<p>I chose to approach this using a <code>behaviour</code></p>
<pre><code>defmodule Mockingjay.Strategy do
  @moduledoc false
  @type t :: Nx.Container.t()

  @callback init(data :: any(), opts :: Keyword.t()) :: term()
  @callback forward(x :: Nx.Container.t(), term()) :: Nx.Tensor.t()
  ...
end</code></pre>
<p><code>forward</code> will perform setup functionality depending on the strategy and return the parameters that will need to be passed to <code>forward</code> later on. This allows for a very simple top-level api. The whole top-level <code>mockingjay.ex</code> file can fit here:</p>
<pre><code>  def convert(data, opts \\ []) do
    {strategy, opts} = Keyword.pop(opts, :strategy, :auto)

    strategy =
      case strategy do
        :gemm -&gt;
          Mockingjay.Strategies.GEMM

        :tree_traversal -&gt;
          Mockingjay.Strategies.TreeTraversal

        :perfect_tree_traversal -&gt;
          Mockingjay.Strategies.PerfectTreeTraversal

        :auto -&gt;
          Mockingjay.Strategy.get_strategy(data, opts)

        _ -&gt;
          raise ArgumentError,
                "strategy must be one of :gemm, :tree_traversal, :perfect_tree_traversal, or :auto"
      end

    {post_transform, opts} = Keyword.pop(opts, :post_transform, nil)
    state = strategy.init(data, opts)

    fn data -&gt;
      result = strategy.forward(data, state)
      {_, n_trees, n_classes} = Nx.shape(result)

      result
      |&gt; aggregate(n_trees, n_classes)
      |&gt; post_transform(post_transform, n_classes)
    end
  end  </code></pre>
<p>As you can see, the use of a behaviour here allows a strategy-agnostic approach to generating a prediction pipeline. In the object-oriented implementation, each class implements <code>init</code>, <code>forward</code>, <code>aggregate</code>, and <code>post_transform</code>. We get the same result from a functional pipeline approach, where each step generates the needed information as input parameters for the next step. So, instead of storing intermediate results as object properties or values in an object's <strong><code>__dict__</code></strong>, we just pass them along in the pipeline. I would argue this creates a much simpler and easier to follow implementation (but I am also quite biased).</p>
<h2 id="pytorch-to-nx">PyTorch to Nx </h2>
<p>For these examples, we will be looking at porting the implementations of the <code>forward</code> function for the three conversion strategies from Python to Nx.</p>
<h4 id="gemm">GEMM</h4>
<p>Next, let's look at the <code>forward</code> function implementation for GEMM, one of the three conversion strategies. In Hummingbird, they implemented the <code>forward</code> step in the base class for each strategy. So given three GEMM classes with the signatures of <code>GEMMTreeImpl(AbstractPyTorchTreeImpl)</code>, <code>GEMMDecisionTreeImpl(GEMMTreeImpl)</code>, and <code>GEMMGBDTImpl(GEMMTreeImpl)</code>, the <code>forward</code> function is defined in the <code>GEMMTreeImpl</code> class, since both ensemble and non-ensemble decision tree models share the same forward step. </p>
<pre><code>def forward(self, x):
      x = x.t()
      x = self.decision_cond(torch.mm(self.weight_1, x), self.bias_1)
      x = x.view(self.n_trees, self.hidden_one_size, -1)
      x = x.float()

      x = torch.matmul(self.weight_2, x)

      x = x.view(self.n_trees * self.hidden_two_size, -1) == self.bias_2
      x = x.view(self.n_trees, self.hidden_two_size, -1)
      if self.tree_op_precision_dtype == "float32":
          x = x.float()
      else:
          x = x.double()

      x = torch.matmul(self.weight_3, x)
      x = x.view(self.n_trees, self.hidden_three_size, -1)</code></pre>
<p>Now, here is the Nx implementation:</p>
<pre><code>@impl true
  deftransform forward(x, {arg, opts}) do
    opts =
      Keyword.validate!(opts, [
        :condition,
        :n_trees,
        :n_classes,
        :max_decision_nodes,
        :max_leaf_nodes,
        :n_weak_learner_classes,
        :custom_forward
      ])

    _forward(x, arg, opts)
  end

  defnp _forward(x, arg, opts \\ []) do
    %{mat_A: mat_A, mat_B: mat_B, mat_C: mat_C, mat_D: mat_D, mat_E: mat_E} = arg

    condition = opts[:condition]
    n_trees = opts[:n_trees]
    n_classes = opts[:n_classes]
    max_decision_nodes = opts[:max_decision_nodes]
    max_leaf_nodes = opts[:max_leaf_nodes]
    n_weak_learner_classes = opts[:n_weak_learner_classes]

    mat_A
    |&gt; Nx.dot([1], x, [1])
    |&gt; condition.(mat_B)
    |&gt; Nx.reshape({n_trees, max_decision_nodes, :auto})
    |&gt; then(&amp;Nx.dot(mat_C, [2], [0], &amp;1, [1], [0]))
    |&gt; Nx.reshape({n_trees * max_leaf_nodes, :auto})
    |&gt; Nx.equal(mat_D)
    |&gt; Nx.reshape({n_trees, max_leaf_nodes, :auto})
    |&gt; then(&amp;Nx.dot(mat_E, [2], [0], &amp;1, [1], [0]))
    |&gt; Nx.reshape({n_trees, n_weak_learner_classes, :auto})
    |&gt; Nx.transpose()
    |&gt; Nx.reshape({:auto, n_trees, n_classes})
  end
</code></pre>
<p>Do not be distracted by the length of this code snippet, as much of the lines are taken up by validating arguments. Let's look at a more stripped-down version without that:</p>
<pre><code>@impl true
  deftransform forward(x, {arg, opts}) do
    _forward(x, arg, opts)
  end

  defnp _forward(x, arg, opts \\ []) do
    mat_A
    |&gt; Nx.dot([1], x, [1])
    |&gt; condition.(mat_B)
    |&gt; Nx.reshape({n_trees, max_decision_nodes, :auto})
    |&gt; then(&amp;Nx.dot(mat_C, [2], [0], &amp;1, [1], [0]))
    |&gt; Nx.reshape({n_trees * max_leaf_nodes, :auto})
    |&gt; Nx.equal(mat_D)
    |&gt; Nx.reshape({n_trees, max_leaf_nodes, :auto})
    |&gt; then(&amp;Nx.dot(mat_E, [2], [0], &amp;1, [1], [0]))
    |&gt; Nx.reshape({n_trees, n_weak_learner_classes, :auto})
    |&gt; Nx.transpose()
    |&gt; Nx.reshape({:auto, n_trees, n_classes})
  end
</code></pre>
<p>Let's take a look at some obvious difference:</p>
<ul>
<li>The <code>Nx</code> code does not have to transpose in the first step since <code>Nx.dot/4</code> allows you to specify the contracting axes.</li>
<li>You can use <code>Nx.dot/6</code> to get the same behavior as <code>torch.matmul</code>
<ul>
<li><code>torch.matmul</code> does a lot of wizardry with broadcasting to make this instance work</li>
</ul>
</li>
<li>We use functions such as <code>Nx.equal</code> to fit into the pipeline rather than using the <code>==</code> oeprator (which would work outside of a pipeline)</li>
<li><code>torch.view</code> is equivalent to <code>Nx.reshape</code></li>
<li><code>Nx</code> uses the <code>:auto</code> atom to where <code>torch</code> uses <code>-1</code> to reference infering the sie of an axis</li>
</ul>

<p>Outside of these differences, the code translates fairly easily. Let's take a look at a bit of a more complex instance.</p>
<h4 id="tree-traversal">Tree Traversal</h4>
<p>Here is the Python implementation:</p>
<pre><code>def _expand_indexes(self, batch_size):
        indexes = self.nodes_offset
        indexes = indexes.expand(batch_size, self.num_trees)
        return indexes.reshape(-1)

def forward(self, x):
        indexes = self.nodes_offset
        indexes = indexes.expand(batch_size, self.num_trees).reshape(-1)

        for _ in range(self.max_tree_depth):
            tree_nodes = indexes
            feature_nodes = torch.index_select(self.features, 0, tree_nodes).view(-1, self.num_trees)
            feature_values = torch.gather(x, 1, feature_nodes)

            thresholds = torch.index_select(self.thresholds, 0, indexes).view(-1, self.num_trees)
            lefts = torch.index_select(self.lefts, 0, indexes).view(-1, self.num_trees)
            rights = torch.index_select(self.rights, 0, indexes).view(-1, self.num_trees)

            indexes = torch.where(self.decision_cond(feature_values, thresholds), lefts, rights).long()
            indexes = indexes + self.nodes_offset
            indexes = indexes.view(-1)

        output = torch.index_select(self.values, 0, indexes).view(-1, self.num_trees, self.n_classes)</code></pre>
<p>And here is the Nx implementation:</p>
<pre><code>defn _forward(x, features, lefts, rights, thresholds, nodes_offset, values, opts \\ []) do
    max_tree_depth = opts[:max_tree_depth]
    num_trees = opts[:num_trees]
    n_classes = opts[:n_classes]
    condition = opts[:condition]
    unroll = opts[:unroll]

    batch_size = Nx.axis_size(x, 0)

    indices =
      nodes_offset
      |&gt; Nx.broadcast({batch_size, num_trees})
      |&gt; Nx.reshape({:auto})

    {indices, _} =
      while {tree_nodes = indices, {features, lefts, rights, thresholds, nodes_offset, x}},
            _ &lt;- 1..max_tree_depth,
            unroll: unroll do
        feature_nodes = Nx.take(features, tree_nodes) |&gt; Nx.reshape({:auto, num_trees})
        feature_values = Nx.take_along_axis(x, feature_nodes, axis: 1)
        local_thresholds = Nx.take(thresholds, tree_nodes) |&gt; Nx.reshape({:auto, num_trees})
        local_lefts = Nx.take(lefts, tree_nodes) |&gt; Nx.reshape({:auto, num_trees})
        local_rights = Nx.take(rights, tree_nodes) |&gt; Nx.reshape({:auto, num_trees})

        result =
          Nx.select(
            condition.(feature_values, local_thresholds),
            local_lefts,
            local_rights
          )
          |&gt; Nx.add(nodes_offset)
          |&gt; Nx.reshape({:auto})

        {result, {features, lefts, rights, thresholds, nodes_offset, x}}
      end

    values
    |&gt; Nx.take(indices)
    |&gt; Nx.reshape({:auto, num_trees, n_classes})
  end</code></pre>
<p>Here there are some much more striking differences, namely the use of <code>Nx</code>'s <code>while</code> expression compared to a <code>for</code> loop in Python. We use <code>while</code> in this case since it can achieve the same purpose as the Python <code>for</code> loop and it is supported by <code>Nx</code> within a <code>defn</code> expression. Otherwise, we might have to perform some of the calculations within a <code>deftransform</code>, as we will see in the next example.  Another obvious difference is that in the Nx implementation, we have to pass the required variables around throughout these operation, whereas Python can use stored class attributes. </p>
<p>Still, the conversion is quite straightforward. I hope you are beginning to see that this is not an impossible effort, and can be accomplished given you have a firm understanding of the source material.</p>
<h4 id="perfect-tree-traversal">Perfect Tree Traversal</h4>
<p>Lastly, let's look at the last conversion strategy. Yet again, this conversion is even slightly more complex, but hopefully seeing this example will help you in your case:</p>
<pre><code>def forward(self, x):
        prev_indices = (self.decision_cond(torch.index_select(x, 1, self.root_nodes), self.root_biases)).long()
        prev_indices = prev_indices + self.tree_indices
        prev_indices = prev_indices.view(-1)

        factor = 2
        for nodes, biases in zip(self.nodes, self.biases):
            gather_indices = torch.index_select(nodes, 0, prev_indices).view(-1, self.num_trees)
            features = torch.gather(x, 1, gather_indices).view(-1)
            prev_indices = (
                factor * prev_indices + self.decision_cond(features, torch.index_select(biases, 0, prev_indices)).long()
            )

        output = torch.index_select(self.leaf_nodes, 0, prev_indices).view(-1, self.num_trees, self.n_classes)
</code></pre>
<p>And the Elixir implementation:</p>
<pre><code>defnp _forward(
          x,
          root_features,
          root_thresholds,
          features,
          thresholds,
          values,
          indices,
          opts \\ []
        ) do
    prev_indices =
      x
      |&gt; Nx.take(root_features, axis: 1)
      |&gt; opts[:condition].(root_thresholds)
      |&gt; Nx.add(indices)
      |&gt; Nx.reshape({:auto})
      |&gt; forward_reduce_features(x, features, thresholds, opts)

    Nx.take(values, prev_indices)
    |&gt; Nx.reshape({:auto, opts[:num_trees], opts[:n_classes]})
  end

  deftransformp forward_reduce_features(prev_indices, x, features, thresholds, opts \\ []) do
    Enum.zip_reduce(
      Tuple.to_list(features),
      Tuple.to_list(thresholds),
      prev_indices,
      fn nodes, biases, acc -&gt;
        gather_indices = nodes |&gt; Nx.take(acc) |&gt; Nx.reshape({:auto, opts[:num_trees]})
        features = Nx.take_along_axis(x, gather_indices, axis: 1) |&gt; Nx.reshape({:auto})

        acc
        |&gt; Nx.multiply(@factor)
        |&gt; Nx.add(opts[:condition].(features, Nx.take(biases, acc)))
      end
    )
  end</code></pre>
<p>You can see that in this case, we have a function defined in a <code>deftransform</code> within our <code>forward</code> pipeline. Why is this so? Well, when writing definitions within <code>defn</code> you forfeit the use of the default Elixir kernel for the <code>Nx.Kernel</code> module. If you want full access to all of the normal Elixir modules, you need to use a <code>deftransform</code>. We needed to use <code>Enum.zip_reduce</code> in this instance (rather than <code>Nx</code>'s <code>while</code> like before) since the <code>features</code> and <code>thresholds</code> lists are not of uniform shape. Their shape represents the length of a given depth of a binary tree, so they will be a nested list of lengths <code>[1,2,4,8...]</code>. This is an optimization as opposed to normal <code>TreeTraversal</code>, but required a bit of a different approach as opposed to the Python implementation which took advantage of <code>torch.nn.ParameterList</code> to build out the same lists. You might also notice the use of <code>Tuple.to_list</code> on lines 25 and 26. This was required since we needed <code>features</code> and <code>thresholds</code> to be stored in <code>Nx.container</code>'s when passed into the <code>deftransform</code>, and <code>Tuple</code> implements the <code>Nx.Container</code> protocol, while lists do not. Even still, given that knowledge of the intricacies of <code>defn</code> and <code>deftransform</code>, the final ported solution is very similar to the reference solution.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this post, I tried to accomplish several things at once, and perhaps that lead to a cluttered article, but I felt the need to address all of these points at once. I do not mean to suggest that Machine Learning has no place in Python or that Python will not continue to be the most dominant player in Machine Learning, but that I think some healthy competition is a good thing, and that perhaps Python does have some shortcomings that might give other languages valid reasons to coexist in the space. </p>
<p>Next, I wanted to address some specifics as to what Elixir has to offer to the machine learning space. I think it is uniquely positioned to be quite competitive considering the large community push to support more and more libraries, as well as the large application development community that can benefit from an in-house solution.</p>
<p>Lastly, I wanted to share some practical tips for those looking to move on from Python to Elixir, but feeling somewhat helpless in the process. I think that Sean Moriarity's book that I mentioned at the beginning of this article is an invaluable resource and great step in the education of machine learning for Elixir developers, but it can nonetheless feel daunting to seemingly throw out existing working solutions for new-fangled, perhaps not as well respected solutions. I hope I showed how anybody can approach this problem, and any existing Elixir developer can be a machine learning developer going forward. The ground work has been laid, and the tools are available. Thank you for reading (especially if you made it to the end)!</p>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I've skimmed 66520 newsgroups trying to find some life on the Usenet (2020) (120 pts)]]></title>
            <link>https://mastodon.sdf.org/@cfenollosa/103469996345323076</link>
            <guid>36859510</guid>
            <pubDate>Tue, 25 Jul 2023 08:26:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.sdf.org/@cfenollosa/103469996345323076">https://mastodon.sdf.org/@cfenollosa/103469996345323076</a>, See on <a href="https://news.ycombinator.com/item?id=36859510">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The 1990s Amiga with Video Toaster has a VFX cool factor that endures today (134 pts)]]></title>
            <link>https://cdm.link/2023/07/amiga-video-toaster-cool-factor/</link>
            <guid>36857758</guid>
            <pubDate>Tue, 25 Jul 2023 04:07:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cdm.link/2023/07/amiga-video-toaster-cool-factor/">https://cdm.link/2023/07/amiga-video-toaster-cool-factor/</a>, See on <a href="https://news.ycombinator.com/item?id=36857758">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    	  
<p>Todd Rundgren, <em>Babylon 5</em>, <em>The X-Files</em>, <em>SeaQuest DSV</em>, and even those demo videos ‚Äì NewTek‚Äôs Video Toaster has plenty of 90s nostalgia behind it. But it also demonstrates workflow features in switching, titling, and animation that could easily inspire streamers today.</p>



<p>Hang on, I was about to start with an intro, but ‚Äì frankly, I can‚Äôt do any better than this Yorkshire Television-produced CITV show Bad Influence!, the answer to the question ‚Äúwhat if Computer Chronicles were <em>not</em> made by complete dorks who were apparently trapped in a low-budget PBS studio?‚Äù <em>Please</em> read the rest of this article imagining the voices of these two presenters.</p>



<p>‚ÄúBut first have a look at this!‚Äù ‚Äú<em>The AMIIIIGA??!‚Äù</em></p>



<figure><p>
<iframe title="How CGI was done on TV shows with low cost Amiga! (Babylon 5)" width="500" height="375" src="https://www.youtube.com/embed/vO5DYSjyQJA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>That video is doubly significant as it coincides with the native edition of LightWave3D. Also, way to not underestimate your audience or talk down. (Of course you recognize a Vorlon spacecraft already!)</p>



<p>So, a very brief history of the Video Toaster. NewTek founder Tim Jenison loved Vermeer and designed the first edition, with Brad Carvey (who also worked on <em>Men In Black</em> VFX) building the prototype. Announced in 1987, the first edition was a US$2399 Amiga add-on ‚Äì an insanely low price given six-digit costs of broadcast rigs at the time. (Even in 2023 dollars, that‚Äôs only $5,470.53, or roughly the price of a set of Apple Pro wheels and a video cable or whatever.) You would need an Amiga 2000, but even that was not a wildly expensive machine at the time.</p>



<p>Oh yeah, bonus ‚Äì the Amiga‚Äôs system clock ran conveniently at double the NTSC color carrier frequency for easy sync. And the Amiga‚Äôs innovative on-board graphics chipset helped, too.</p>



<p>The Video Toaster was a complete bundle: a full-sized card to add video connections, a real-time four-channel video switcher, a bundled LightWave 3D modeling / rendering / animation package, frame buffer-based effects, character generation, overlays, animated transitions ‚Äì the works. It was the 90s, so you‚Äôd need some video tape recorders and a controller since all this beauty would be going to video tape. But this more than any other invention democratized visual effects and made them mainstream not just for high-budget films, but for television, as well. (And keep in mind that Steve Jobs at the time was still trying to sell Pixar‚Äôs Image Computer, with complete rigs running more into the six-digit territory again.)</p>



<p>Eventually, you could buy complete ready-to-play rigs for around US$5000, the likes of NBC popularized usage, Wil Wheaton (yeah, <em>that</em> Wil Wheaton) signed on as a technology evangelist and consultant, and LightWave 3D had a life of its own as a standalone system.</p>



<p>Ars Technica‚Äôs Jeremy Reimer wrote a beautiful history of the Amiga and dedicated a chapter to the Video Toaster:</p>



<p><a href="https://arstechnica.com/gadgets/2016/03/a-history-of-the-amiga-part-9-the-video-toaster/">A history of the Amiga, part 9: The Video Toaster</a></p>



<p>Want some video artifacts of this age? Oh, does YouTube ever have you covered.</p>



<p>First, for peak 90s, you‚Äôve got to have Todd Rundgren‚Äôs ‚ÄúChange Myself‚Äù video, which definitely seems like it was just a Video Toaster demo more than something with a ‚Ä¶ concept. (Clocks. A stop light. Todd as a leaf and a crude boat sail. A chess board. Uh, sperm? Did someone just try to literally interpret every line?)</p>



<figure><p>
<iframe loading="lazy" title="NewTek - Video Toaster Demo - Todd Rundgren &quot;Change Myself&quot;" width="500" height="281" src="https://www.youtube.com/embed/Ee8MlDm01Fk?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>How did it come together? Well, actually, we can ask 1991 Todd Rundgren for the accurate answer to that at SIGGRAPH, which awesomely was hosted by Pasadena‚Äôs Jet Propulsion Laboratory.</p>



<figure><p>
<iframe loading="lazy" title="1991 - Todd Rundgren Details How He Created His Groundbreaking 'Change Myself' Music Video" width="500" height="375" src="https://www.youtube.com/embed/Jhs24mL8Lx0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>Amiga‚Äôs own promo videos are wonderful 90s ephemera on their own, even if they seem to have been directed by straight teenage boys. (‚ÄúI want jet planes, and some girls in swimsuits, and UFOs, and skateboards, and football!‚Äù) Wil Wheaton, Penn Jillette, and Tony Hawk all make the 1994 video.</p>



<figure><p>
<iframe loading="lazy" title="Video Toaster 2 0" width="500" height="375" src="https://www.youtube.com/embed/f3YfpCx1W7o?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<figure><p>
<iframe loading="lazy" title="Newtek Video Toaster 4000 1994 Video" width="500" height="375" src="https://www.youtube.com/embed/C_K8vnx2ZDc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>Then there‚Äôs this wonderful launch video, with a voice that is both somehow dramatically epic and ‚Ä¶ like way sexier than you‚Äôd expect. (<em>Toaster Paint</em>. <em>Oh yeah.</em>) It does do a great job of explaining the technological capabilities and their implications in simple terms. </p>



<figure><p>
<iframe loading="lazy" title="Revolution: the Video Toaster and the Amiga computer  (Part 1)" width="500" height="375" src="https://www.youtube.com/embed/seznQmDp2pU?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<figure><p>
<iframe loading="lazy" title="Revolution: the Video Toaster and the Amiga computer  (Part 2)" width="500" height="375" src="https://www.youtube.com/embed/3AN1Sk058GE?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>Honestly, jokes aside, how is this demo video so much better than the corporate snooze-fest of the new Apple, Google, and Microsoft presentations? It‚Äôs like everyone had their souls removed through their nose sometime in the early 2000s. Let‚Äôs fix this, people.</p>



<p>NewTek is also famous for their long-time spokeperson and technology evangelist, <a href="https://en.wikipedia.org/wiki/Kiki_Stockhammer">Kiki Stockhammer</a>. Kiki also had a driving role inside the company, and is legendary for her ability to deliver product pitches even off-script. She‚Äôs probably one of the greatest tech spokespeople of all time. She‚Äôs still doing this sort of work today (<a href="https://www.youtube.com/watch?v=6iIGXVnPMPE">recent video</a>), and basically <em>any</em> of us working tech ought to follow her and take some notes. Also ‚Äì here‚Äôs one thing Steve Jobs never did. It‚Äôs actually her silhouette in the Video Toaster‚Äôs transitions, as this <a href="https://www.youtube.com/watch?v=jUrj-h3GejA">behind-the-scenes shoot proves</a>. She‚Äôs the literal face (and shadow) of Video Toaster through its whole history.</p>



<p>She has enough of a fanbase that there‚Äôs even this extended-length documentary, with some, uh, kinda questionable camera work for the host (the dude who is <em>not</em> Stockhammer), but let‚Äôs do this!</p>



<figure><p>
<iframe loading="lazy" title="Kiki Stockhammer: The Original E-Girl" width="500" height="281" src="https://www.youtube.com/embed/Iufe0RmHIqQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>Here‚Äôs the story in her own words, in a 2022 talk:</p>



<figure><p>
<iframe loading="lazy" title="Stockhammer at the  Amiwest Show 2022" width="500" height="281" src="https://www.youtube.com/embed/6iIGXVnPMPE?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>But look ‚Äì the Video Toaster is good enough that people are using it now. Here‚Äôs a walkthrough from March 2020 (ooh, good timing, you‚Äôre about to be doing a lot of streaming).</p>



<figure><p>
<iframe loading="lazy" title="NewTek Video Toaster General Overview Demo on Commodore Amiga" width="500" height="281" src="https://www.youtube.com/embed/6yFdaCmRkBk?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>I think my favorite Video Toaster-produced video is this 1993 one hosted by Wil Wheaton himself:</p>



<figure><p>
<iframe loading="lazy" title="Beyond The Edge Hosted by Wil Wheaton. Video by NewTek for their Video Toaster for Commodore Amiga" width="500" height="375" src="https://www.youtube.com/embed/KGuuXGYFmY0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>And then there‚Äôs <em>Babylon 5</em>. Those effects looked a little crude, then really crude, and now‚Ä¶ kind of weirdly cool and artistic and ahead of their time. (The show‚Äôs approach to real-physics spacecraft maneuvers and modeling came to influence later TV like the 2000s reboot of <em>Battlestar Galactica</em>, which also used LightWave 3D. There is even a Babylon 5 station model tucked into an early scene in the escaping fleet.)</p>



<figure><p>
<iframe loading="lazy" title="The story of VFX, Commodore Amiga and Babylon 5" width="500" height="281" src="https://www.youtube.com/embed/Xn3UraChY0c?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>The Amiga didn‚Äôt last past the pilot (see intro here, complete with <em>very</em> different Stewart Copeland music), but LightWave 3D did. A 1997 interview with coproducer George Johnsen details their rig. Before I start a 90s-style Amiga-versus-Mac-versus-PC argument, yeah, they used basically every platform available at the time. </p>



<figure><p>
<iframe loading="lazy" title="babylon 5 &quot;the gathering&quot; intro (original)" width="500" height="375" src="https://www.youtube.com/embed/nc9KM9YQ-WQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<blockquote>
<p><em>Are Macintoshes used in the production?</em></p>



<p>Even though the Joe uses ‚Äúanother‚Äù platform, the show actually uses many! Macs have been essential to the creation of the show from the beginning. Even farther back, the Amiga and the Newtek Toaster were employed.</p>



<p>Currently we use Pentiums and Alphas for animation, Macs for Editing, Matte paintings and Compositing, and SGI‚Äôs for Compositing and titling. If that isn‚Äôt platform independent, I don‚Äôt know what is! The arsenal looks like the following-</p>



<p>Alphas for design stations serving 5 animators and one animation assistant (housekeeping and slate specialist). Most of these stations run Lightwave and a couple add Softimage. VERY plug-in hungry. PVR‚Äôs on every station, with calibrated component NTSC (darn it, I hates ntsc) right beside.</p>



<p>P6‚Äôs in quad enclosures for part of the renderstack, and Alphas for the rest, backed up 2x per day to an optical jukebox.Todd Rund</p>



<p>Completed shots output to a DDR post rendering and get integrated into the show.</p>



<p>Shots to composite go to the Macs running After Effects, or the SGI running Flint, depending on the type of comp being done, and then to the DDR (8 minutes capacity on the SGI).</p>



<p>Boy it sure sounds easy! The only problem is, we have a killer schedule and very picky producers, and ESPECIALLY picky viewers! üôÇ</p>



<p>It is, however, a bunch of fun!</p>
</blockquote>



<p>Actually using the tool was of course far less glamorous, as Computer Chronicles demonstrated in an early demo in 1988. But it‚Äôs still stunning, even there:</p>



<figure><p>
<iframe loading="lazy" title="Amiga Video Toaster - Computer Chronicles (1988) | Early Computer Video Editing &amp; Video Production" width="500" height="375" src="https://www.youtube.com/embed/7qCYr0fPqCI?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>But these easy switching, transition, and titling tools ‚Äì even if the aesthetic is dated now ‚Äì could sure come in handy in 2020s software. I do hope developers look back at some of the analog-era stuff. We‚Äôve lost a lot of the directness and immediacy of some of the hardware and software of the time. (I hated VHS for its never-ending degradation, but wow, was it easy to use a physical jog wheel on a two-deck controller ‚Äì for one example. And of course, people still use Edirol-now-Roland V4s.)</p>



<p>OBS, I‚Äôm looking at you.</p>



<figure><p>
<iframe loading="lazy" title="Video Toaster 4000 Demo" width="500" height="375" src="https://www.youtube.com/embed/TT-b08IVnr8?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>Tons of musicians are now using trackers and other Amiga-era music tools who never had access to them ‚Äì or weren‚Äôt even born yet ‚Äì in the 90s. So maybe it‚Äôs time for some of the visual ideas of the Amiga to get a reboot, too.</p>



<p>For more on that musical history, see the last CDM Amiga flashback:</p>



<figure></figure>



<p>And yeah, I never get tired of watching Andy Warhol and Debbie Harry at the 1985 debut of the platform:</p>



<figure><p>
<iframe loading="lazy" title="Launch Of Amiga in US 23.7.1985 - with Andy Warhol  and Debbie Harry" width="500" height="375" src="https://www.youtube.com/embed/OtPRv3VQ_Z0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>Oh, and one more bonus ‚Äì speaking of Wil Wheaton, here he is telling a little girl how to deal with being called a nerd. (I‚Äôm‚Ä¶ guessing if you‚Äôre at this point in <em>this</em> article, this is probably relevant.)</p>



<figure><p>
<iframe loading="lazy" title="Wil Wheatons response to a little girl on how to deal with being called a nerd" width="500" height="281" src="https://www.youtube.com/embed/04WJEEb33CY?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>

        
    	  
    	  <div><p>Tags: <a href="https://cdm.link/tag/3d/" rel="tag">3D</a>, <a href="https://cdm.link/tag/90s/" rel="tag">90s</a>, <a href="https://cdm.link/tag/animation/" rel="tag">animation</a>, <a href="https://cdm.link/tag/babylon-5/" rel="tag">Babylon 5</a>, <a href="https://cdm.link/tag/hardware/" rel="tag">Hardware</a>, <a href="https://cdm.link/tag/history/" rel="tag">history</a>, <a href="https://cdm.link/tag/i-love-the-90s-2/" rel="tag">I love the 90s</a>, <a href="https://cdm.link/tag/lightwave-3d/" rel="tag">Lightwave 3D</a>, <a href="https://cdm.link/tag/newtek/" rel="tag">newtek</a>, <a href="https://cdm.link/tag/retro/" rel="tag">retro</a>, <a href="https://cdm.link/tag/software/" rel="tag">Software</a>, <a href="https://cdm.link/tag/switchers/" rel="tag">switchers</a>, <a href="https://cdm.link/tag/todd-rundgren/" rel="tag">Todd Rundgren</a>, <a href="https://cdm.link/tag/video-switchers/" rel="tag">video switchers</a>, <a href="https://cdm.link/tag/video-toaster/" rel="tag">Video Toaster</a></p></div>
    	  
    		      		
    						
				
				
				        
                          
                  	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Parents, environmentalists to Google: stop Chromebooks from expiring this summer (193 pts)]]></title>
            <link>https://pirg.org/edfund/resources/chromebook-expiration-full-letter/</link>
            <guid>36857506</guid>
            <pubDate>Tue, 25 Jul 2023 03:27:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pirg.org/edfund/resources/chromebook-expiration-full-letter/">https://pirg.org/edfund/resources/chromebook-expiration-full-letter/</a>, See on <a href="https://news.ycombinator.com/item?id=36857506">Hacker News</a></p>
Couldn't get https://pirg.org/edfund/resources/chromebook-expiration-full-letter/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Got called to a professor‚Äôs office after a complaint his SPARC4 was running slow (554 pts)]]></title>
            <link>https://infosec.exchange/@paco/110772422266480371</link>
            <guid>36857314</guid>
            <pubDate>Tue, 25 Jul 2023 03:00:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://infosec.exchange/@paco/110772422266480371">https://infosec.exchange/@paco/110772422266480371</a>, See on <a href="https://news.ycombinator.com/item?id=36857314">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Mozilla Standards Positions Opposes Web Integrity API (661 pts)]]></title>
            <link>https://github.com/mozilla/standards-positions/issues/852</link>
            <guid>36857032</guid>
            <pubDate>Tue, 25 Jul 2023 02:14:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mozilla/standards-positions/issues/852">https://github.com/mozilla/standards-positions/issues/852</a>, See on <a href="https://news.ycombinator.com/item?id=36857032">Hacker News</a></p>
<div id="readability-page-1" class="page"><div disabled="" sortable="">
          <p dir="auto">Mozilla opposes this proposal because it contradicts our principles and vision for the Web.</p>
<p dir="auto">Any browser, server, or publisher that implements common standards is <a href="https://www.mozilla.org/en-US/about/webvision/full/#openness" rel="nofollow">automatically part of the Web</a>:</p>
<blockquote>
<p dir="auto">Standards themselves aim to avoid assumptions about the underlying hardware or software that might restrict where they can be deployed. This means that no single party decides which form-factors, devices, operating systems, and browsers may access the Web. It gives people more choices, and thus more avenues to overcome personal obstacles to access. Choices in assistive technology, localization, form-factor, and price, combined with thoughtful design of the standards themselves, all permit a wildly diverse group of people to reach the same Web.</p>
</blockquote>
<p dir="auto">Mechanisms that attempt to restrict these choices are harmful to the openness of the Web ecosystem and are not good for users.</p>
<p dir="auto">Additionally, the use cases listed depend on the ability to ‚Äúdetect non-human traffic‚Äù which as described would likely obstruct many existing uses of the Web such as assistive technologies, automatic testing, and archiving &amp; search engine spiders. These depend on tools being able to receive content intended for humans, and then transform, test, index, and summarize that content for humans. The safeguards in the proposal (e.g., ‚Äúholdback‚Äù, or randomly failing to produce an attestation) are unlikely to be effective, and are inadequate to address these concerns.</p>
<p dir="auto">Detecting fraud and invalid traffic is a challenging problem that we're interested in helping address. However this proposal does not explain how it will make practical progress on the listed use cases, and there are clear downsides to adopting it.</p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Signal walks the line between anarchism and pragmatism (157 pts)]]></title>
            <link>https://www.wired.com/story/signal-politics-software-criticism/</link>
            <guid>36856882</guid>
            <pubDate>Tue, 25 Jul 2023 01:53:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/signal-politics-software-criticism/">https://www.wired.com/story/signal-politics-software-criticism/</a>, See on <a href="https://news.ycombinator.com/item?id=36856882">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>For 20 years,</span> the only way to really communicate privately was to use a widely hated piece of software called Pretty Good Privacy. The software, known as PGP, aimed to make secure communication accessible to the lay user, but it was so poorly designed that even Edward Snowden messed up his first attempt to use PGP to email a friend of Laura Poitras. It also required its users to think like engineers, which included participating in exceptionally nerdy activities like attending real-life ‚Äúkey-signing parties‚Äù to verify your identity to other users. Though anyone could technically use PGP, the barrier to entry was so high that only about 50,000 people used it at its peak, meaning that privacy itself was out of reach for most.</p><p>These days, to talk to a friend securely, all you have to do is download a free app. For a certain set, that app will be Signal. Snowden and Elon Musk have recommended it; it‚Äôs been name-dropped on big-budget shows like <em>House of Cards, Mr. Robot,</em> and <em>Euphoria,</em> and its users include journalists, members of the White House, NBA players, Black Lives Matters activists, and celebrities trying to get their hands on Ozempic. Its founder has been profiled by <em>The New Yorker</em> and appeared on Joe Rogan‚Äôs podcast. A tiny organization with virtually no marketing budget has become synonymous with digital privacy in the public imagination.</p><p>Technology can be deeply shaped by the personal inclinations of a founder. Facebook‚Äôs light-fingeredness with user data is inseparable from its roots in Zuckerberg‚Äôs dorm room as an app for ranking women by their looks; Apple‚Äôs minimalist design was influenced by Jobs‚Äô time spent practicing Zen Buddhism. Signal is no different. During its formative years, the charismatic face of Signal was Moxie Marlinspike, a dreadlocked anarchist who spent his time sailing around the world, living in punk houses, and serving free food to the unhoused. He led every aspect of Signal‚Äôs development for almost a decade, <a data-offer-url="https://signal.org/blog/new-year-new-ceo/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://signal.org/blog/new-year-new-ceo/&quot;}" href="https://signal.org/blog/new-year-new-ceo/" rel="nofollow noopener" target="_blank">at one point complaining</a>, &nbsp;‚ÄúI was writing all the Android code, was writing all of the server code, was the only person on call for the service, was facilitating all product development, and was managing everyone. I couldn‚Äôt ever leave cell service.‚Äù</p><p>In the field of cryptography, Marlinspike is considered the driving force behind bringing end-to-end encryption‚Äîthe technology underlying Signal‚Äîto the real world. In 2017, Marlinspike and his collaborator, Trevor Perrin, received the Levchin Prize, a prominent prize for cryptographers, for their work on the Signal Protocol. Afterward, Dan Boneh, the Stanford professor who chaired the award committee, <a href="https://www.newyorker.com/magazine/2020/10/26/taking-back-our-privacy">commented</a> that he wasn‚Äôt sure that end-to-end encryption would have become widespread without Marlinspike‚Äôs work. At the very least, ‚Äúit would have taken many more decades,‚Äù he said.</p><p>The motivations that led to end-to-end encryption going mainstream lie far out on the political fringe. The original impetus for Marlinspike‚Äôs entry into cryptography, around 2007, was to challenge existing power structures, particularly the injustice of how (as he put it) ‚ÄúInternet insecurity is used by people I don‚Äôt like against people I do: the government against the people.‚Äù But sticking to anarchism would imply an almost certain defeat. As Marlinspike once <a data-offer-url="https://moxie.org/stories/promise-defeat/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://moxie.org/stories/promise-defeat/&quot;}" href="https://moxie.org/stories/promise-defeat/" rel="nofollow noopener" target="_blank">noted</a>, the ‚Äútrail of ideas that disappears into the horizon behind me is completely and utterly mined over with failures ‚Ä¶ Anarchists are best known for their failures.‚Äù</p><p>For an idealistic engineer to succeed, he will have to build something that is useful to many. So there has also been an unusually pragmatic bent to Signal‚Äôs approach. Indeed, in many interviews, Marlinspike has taken a mainstream stance, insisting that ‚ÄúSignal is just trying to bring normality to the internet.‚Äù Signal‚Äôs success depends on maintaining its principled anarchist commitments while finding a wide-ranging appeal to the masses, two goals that might seem at odds. Examining how the app navigates this tension can help us understand what might come next in Signal‚Äôs new quest to reach ‚Äú<a href="https://www.theverge.com/23409716/signal-encryption-messaging-sms-meredith-whittaker-imessage-whatsapp-china">everyone on the planet.</a>‚Äù</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>Released after WhatsApp</span> &nbsp;set the standards for messaging, Signal‚Äôs problem has always been how to keep up with its competition‚Äîa fine dance between mimicry (so as to seem familiar to new users) and innovation (to poach users from its competitors). Signal started off by copying WhatsApp's user experience, while at the same time pioneering end-to-end encryption, a feature that WhatsApp turned around and copied from Signal. Throughout this evolutionary dance, Signal has managed to maintain an unusual focus on the autonomy of the individual, a wariness of state authority, and an aversion to making money, characteristics that are recognizably anarchist.</p><p>Because a small fringe of cypherpunks, Marlinspike included, came to see cryptography as a way to remedy the imbalance of power between the individual and the state, Signal focused on getting end-to-end encryption on messages and calls absolutely right. With Signal, no one can read your messages. Amazon can‚Äôt, the US government can‚Äôt, Signal can‚Äôt. The same is true for voice calls and metadata: A user‚Äôs address book and group chat titles are just as safe. Signal knows basically nothing about you, other than your phone number (which is not mapped to your username), the time you created your account, and the time you last used the app. Your data can‚Äôt be sold to others or cause ads to follow you around on the internet. Using Signal is just like talking with your friend in the kitchen.</p><p>Because Signal is committed to retaining as little metadata as possible, that makes it hard for it to implement new features that are standard to other apps. Signal is essentially footing the cost of this commitment in engineer-hours, since implementing popular features like group chats, address books, and stickers all required doing novel research in cryptography. That Signal built them anyway is a testament to its desire for mass appeal.</p><p>Signal also pioneered features that gave individuals more autonomy over their information, such as disappearing messages (which WhatsApp later adopted) and a feature that let users blur faces in a photo (which it rapidly rolled out to support the Black Lives Matter protests). At the same time, Signal has garnered users' trust because its code is open source, so that security researchers can verify that its end-to-end encryption is as strong as the organization claims.</p><p>For the ordinary user, though, individual autonomy and privacy may not be as important. On WhatsApp, users accept that it will be very hard to figure out what exactly the app knows about you and who it might be shared with. Users‚Äô information is governed by an ever-shifting labyrinth of grudging caveats and clauses like ‚Äúwe will share your transaction data and IP address with Facebook‚Äù and ‚Äúwe can‚Äôt see your precise location, but we‚Äôll still try to estimate it as best as we can‚Äù and ‚Äúwe <em>will</em> find out if you click on a WhatsApp share button on the web.‚Äù WhatsApp is also closed-source, so its code can‚Äôt be audited. If using Signal is like talking in a friend‚Äôs kitchen, using WhatsApp is like meeting at a very loud bar‚Äîyour conversation is safe, but you‚Äôre exposed, and you‚Äôll have to pay for your place.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>If you‚Äôre not an anarchist, you may be less worried about a shadowy state and more worried about actual people you know. People in your community might be harassing you in a group chat, an abusive ex might be searching your chats for old photos to leak, or your child might have gotten access to your unlocked phone. WhatsApp‚Äôs features better support a threat model that is sensitive to interpersonal social dynamics: You can leave groups silently, block screenshots for view-once messages, and lock specific chats. WhatsApp can even view the text of end-to-end encrypted messages that have been reported by a user for moderation, whereas Signal has no moderation at all.</p><p>Idealists have called centralization one of the main ills of the internet because it locks users into walled gardens controlled by authoritarian companies. In a great stroke of pragmatism, Signal chose to be centralized anyway. Other encrypted-messaging apps like Matrix offer a federated model akin to email, in which users across different servers can still communicate through a shared protocol. (Someone on Gmail can still email someone on Yahoo, whereas someone on Facebook Messenger can‚Äôt contact someone on Signal.) This federated approach more closely mirrors anarchy; it could theoretically be better, because there would be no single point of failure and no single service provider for a government to pressure. But federated software creates a proliferation of different clients and servers for the same protocol, making it hard to upgrade. Users are already used to centralized apps that behave like Facebook or Twitter, and email has already become centralized into a few main service providers. It turns out that being authoritarian is important for maintaining a consistent user experience and a trusted brand, and for rolling out software updates quickly. Even anarchism has its limits.</p><p><span>What Signal has</span> accomplished so far is impressive. But users famously judge software not on how much it can do, but on how much it can‚Äôt. In that spirit, it‚Äôs time to complain.</p><p>Because of Signal‚Äôs small team, limited funding, and the challenges of implementing features under end-to-end encryption, the app bafflingly lacks a number of important features. It doesn‚Äôt have encrypted backups for iOS; messages can only be transferred between phones. If you lose your iPhone, you lose all your Signal chat history.</p><p>Signal also doesn‚Äôt do a good job serving some of its core users. Activists and organizers deal with huge amounts of messages that involve many people and threads, but Signal‚Äôs interface lacks ways to organize all this information. These power users‚Äô group chats become so unwieldy that they migrate to Slack, losing the end-to-end encryption that brought them to Signal in the first place. It‚Äôs common to try and make multiple group chats between the same people to manage all their threads. When users are hacking ‚Äúdesire paths‚Äù into your interface to create a new feature, or leaving because of the lack of the feature, that‚Äôs a strong hint that something is missing.</p><p>WhatsApp and Telegram, on the other hand, are leading the way on defining how group chats can scale up. WhatsApp ‚Äúcommunities‚Äù gather different private group chats in one place, better mimicking the organization of a neighborhood or school that may be discussing several things at once. Telegram‚Äôs social media ‚Äúchannel‚Äù features are better for broadcasting info en masse, though Telegram‚Äôs lack of moderation has been blamed for attracting the kind of fringe crowd that has been banned from all other platforms.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>It's no exaggeration</span> to say that small features in a chat app encode different visions of how society should be organized. If the first reacji in the palette was a thumbs down rather than a heart, maybe we would all be more negative, cautious people. What kind of social vision did Signal arise from?</p><p>‚ÄúLooking back, I and everyone I knew was looking for that secret world hidden in this one,‚Äù Marlinspike admitted in a 2016 <a href="https://www.wired.com/2016/07/meet-moxie-marlinspike-anarchist-bringing-encryption-us/">interview</a>. A key text in anarchist theory describes the idea of a ‚Äútemporary autonomous zone,‚Äù a place of short-term freedom where people can experiment with new ways to live together outside the confines of current social norms. Originally coined to describe ‚Äúpirate utopias‚Äù that may be apocryphal, the term has <a data-offer-url="https://www.documentjournal.com/2020/08/better-living-through-anarchy-tracking-the-rise-of-the-temporary-autonomous-zone/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.documentjournal.com/2020/08/better-living-through-anarchy-tracking-the-rise-of-the-temporary-autonomous-zone/&quot;}" href="https://www.documentjournal.com/2020/08/better-living-through-anarchy-tracking-the-rise-of-the-temporary-autonomous-zone/" rel="nofollow noopener" target="_blank">since been used</a> to understand the life and afterlife of real-world DIY spaces like communes, raves, seasteads, and protests. And Signal is, unmistakably, a temporary autonomous zone that Marlinspike has spent almost a decade building.</p><p>Because temporary autonomous zones create spaces for the radical urges that society represses, they keep life in the daytime more stable. They can sometimes make money in the way that nightclubs and festivals do. But temporary autonomous zones are temporary for a reason. Over and over, zone denizens make the same mistake: They can‚Äôt figure out how to interact productively with the wider society. The zone often runs out of money because it exists in a world where people need to pay rent. Success is elusive; when a temporary autonomous zone becomes compelling enough to threaten daytime stability, it may be violently repressed. Or the attractive freedoms offered by the zone may be taken up in a milder form by the wider society, and eventually the zone ceases to exist because its existence has pressured wider society to be a little more like it. What kind of end might Signal come to?</p><p>There are reasons to think that Signal may not be around for very long. The nonprofit‚Äôs blog, meant to convince us of the elite nature of its engineers, has the unintentional effect of conveying the incredible difficulty of building any new software feature under end-to-end encryption. Its team numbers roughly 40; Marlinspike has just <a data-offer-url="https://twitter.com/signalapp/status/1669374296785928193" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://twitter.com/signalapp/status/1669374296785928193&quot;}" href="https://twitter.com/signalapp/status/1669374296785928193" rel="nofollow noopener" target="_blank">left the organization</a>. Achieving impossible feats may be fun for a stunt hacker with something to prove, but competing with major tech companies‚Äô engineering teams may not be sustainable for a small nonprofit with Marlinspike no longer at the helm.</p><p>Fittingly for an organization formerly led by an anarchist, Signal lacks a sustainable business model, to the point where you might almost call it anti-capitalist. It has survived so far in ways that don‚Äôt seem replicable, and that may alienate some users. Signal is largely funded by a big loan from a WhatsApp founder, and that loan has already grown to $100 million. It has also accepted funding from the US government through the Open Technology Fund. Because Signal can‚Äôt sell its users‚Äô data, it has recently begun developing a business model based on directly providing services to users and encouraging them to donate to Signal in-app. But to get enough donations, the nonprofit must grow from 40 million users to 100 million. The company‚Äôs aggressive pursuit of growth, coupled with lack of moderation in the app, has already led Signal employees themselves to <a href="https://www.theverge.com/22249391/signal-app-abuse-messaging-employees-violence-misinformation">publicly question</a> whether growth might come from abusive users, such as far-right groups using Signal to organize.</p><p>But there are also reasons for hope. So far, the most effective change that Signal has created is arguably not the existence of the app itself, but making it easy for WhatsApp to bring Signal-style end-to-end encryption to billions of users. Since WhatsApp‚Äôs adoption, Facebook Messenger, Google‚Äôs Android Messages, and Microsoft‚Äôs Skype have all adopted the open source Signal Protocol, though in milder forms, as the history of temporary autonomous zones would have us guess. Perhaps the existence of the Signal Protocol, coupled with demand from increasingly privacy-conscious users, will encourage better-funded messaging apps to compete against each other to be as encrypted as possible. Then Signal would no longer need to exist. (In fact, this resembles Signal‚Äôs original theory of change, before they decided they would rather compete with mainstream tech companies.)</p><p>Now, as the era of the global watercooler ends, small private group chats are becoming the future of social life on the internet. Signal started out a renegade, a pirate utopia encircled by cryptography, but the mainstream has become‚Äîalarmingly quickly‚Äîmuch closer to the vision Signal sought. In one form or another, its utopia just might last.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Twitter‚Äôs Rebrand to X Could Be a Trademark Nightmare Thanks to Microsoft (242 pts)]]></title>
            <link>https://themessenger.com/tech/twitters-rebrand-to-x-could-be-a-trademark-nightmare-thanks-to-microsoft</link>
            <guid>36856433</guid>
            <pubDate>Tue, 25 Jul 2023 01:01:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://themessenger.com/tech/twitters-rebrand-to-x-could-be-a-trademark-nightmare-thanks-to-microsoft">https://themessenger.com/tech/twitters-rebrand-to-x-could-be-a-trademark-nightmare-thanks-to-microsoft</a>, See on <a href="https://news.ycombinator.com/item?id=36856433">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><p>Elon Musk‚Äôs sudden move to rebrand <a href="https://themessenger.com/tech/elon-musks-biographer-hints-why-the-tech-mogul-seems-obsessed-with-the-letter-x">Twitter as X</a> may run into the unyielding wall of trademark law ‚Äî&nbsp;but not because its logo looks suspiciously familiar to a variety of typeface fonts. Instead, the problem may be in the <a href="https://themessenger.com/business/musk-says-twitter-will-become-x-and-drop-all-the-birds">X brand</a> itself, legal experts told The Messenger.</p><p><strong>X: An Investigation</strong></p><p>While many users quickly noted that the X looks eerily similar to a letter from a typeface made by font producer Monotype, the company was quick to shut down speculation that Musk was using their intellectual property without attribution or payment.</p><p>Monotype Executive Creative Director Phil Garnham Executive told The Messenger in a statement that the company ‚Äúcan confirm that whilst it is similar, this is not the capital X glyph from Monotype‚Äôs ‚ÄúSpecial Alphabets 4.‚Äù</p><p>Others have pointed out that the logo is similar if not identical to a Unicode X.&nbsp;</p><p>And a musician, India-based EDM artist Kxlider, said they‚Äôre worried about what the new Twitter logo‚Äôs resemblance to his own logo will mean for their career.</p><p>However, several trademark lawyers told The Messenger that Musk likely won‚Äôt encounter any problems using his new X logo for these reasons.&nbsp;</p><p>‚ÄúIt doesn't matter if it's somebody else's typeface, what matters in the trademark world is whether you're infringing somebody else's preexisting brand,‚Äù said <a href="https://www.owe.com/attorneys/linda-joy-kattwinkel/" target="_blank" rel="noreferrer noopener">Linda Joy Kattwinkel</a>, a copyright, trademark and arts lawyer in California who is also a visual artist.&nbsp;</p><p>That, she says, really only comes into play if you are working in a related field. Kxlider is a musician, and X is a social media company.</p><p>‚ÄúThe question there would be, is Twitter stepping on his toes in terms of his business?‚Äù Kattwinkel said. ‚ÄúThe courts would look into: is Twitter doing anything in the music scene or anything that‚Äôs likely to encroach on what he's been doing all along? Probably not, but stranger things have happened in the courts.‚Äù</p><p><a href="https://www.google.com/search?q=joel+rothman+miami&amp;oq=joel+rothman+miami&amp;aqs=chrome..69i57j33i160l3.3018j0j7&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noreferrer noopener">Joel Rothman</a>, a Miami lawyer who specializes in intellectual property infringement, told The Messenger that font producers usually protect their creations not through any copyright laws but through terms of service that restrict what users can do with them based on the license agreement.&nbsp;</p><p>"The use of a single letter is typically not prohibited, necessarily, by those agreements. Rather, it's the distribution of the entire font family,‚Äù Rothman said. Whether or not using single letter as a logo would be a violation would depend on the exact terms of the agreement ‚Äî&nbsp;but again, that appears not to be a potential problem.</p><figure><p><span><img alt="A worker removes letters from the Twitter sign that is posted on the exterior of Twitter headquarters on July 24, 2023 in San Francisco, California." sizes="100vw" srcset="https://themessenger.com/_next/image?url=https%3A%2F%2Fcms.themessenger.com%2Fwp-content%2Fuploads%2F2023%2F07%2FGettyImages-1568024962-1024x683.jpg&amp;w=640&amp;q=75 640w, https://themessenger.com/_next/image?url=https%3A%2F%2Fcms.themessenger.com%2Fwp-content%2Fuploads%2F2023%2F07%2FGettyImages-1568024962-1024x683.jpg&amp;w=750&amp;q=75 750w, https://themessenger.com/_next/image?url=https%3A%2F%2Fcms.themessenger.com%2Fwp-content%2Fuploads%2F2023%2F07%2FGettyImages-1568024962-1024x683.jpg&amp;w=828&amp;q=75 828w, https://themessenger.com/_next/image?url=https%3A%2F%2Fcms.themessenger.com%2Fwp-content%2Fuploads%2F2023%2F07%2FGettyImages-1568024962-1024x683.jpg&amp;w=1080&amp;q=75 1080w, https://themessenger.com/_next/image?url=https%3A%2F%2Fcms.themessenger.com%2Fwp-content%2Fuploads%2F2023%2F07%2FGettyImages-1568024962-1024x683.jpg&amp;w=1200&amp;q=75 1200w, https://themessenger.com/_next/image?url=https%3A%2F%2Fcms.themessenger.com%2Fwp-content%2Fuploads%2F2023%2F07%2FGettyImages-1568024962-1024x683.jpg&amp;w=1920&amp;q=75 1920w, https://themessenger.com/_next/image?url=https%3A%2F%2Fcms.themessenger.com%2Fwp-content%2Fuploads%2F2023%2F07%2FGettyImages-1568024962-1024x683.jpg&amp;w=2048&amp;q=75 2048w, https://themessenger.com/_next/image?url=https%3A%2F%2Fcms.themessenger.com%2Fwp-content%2Fuploads%2F2023%2F07%2FGettyImages-1568024962-1024x683.jpg&amp;w=3840&amp;q=75 3840w" src="https://themessenger.com/_next/image?url=https%3A%2F%2Fcms.themessenger.com%2Fwp-content%2Fuploads%2F2023%2F07%2FGettyImages-1568024962-1024x683.jpg&amp;w=3840&amp;q=75" decoding="async" data-nimg="fill" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p><figcaption><span>A worker removes letters from the Twitter sign that is posted on the exterior of Twitter headquarters on July 24, 2023 in San Francisco, California.</span><span>Justin Sullivan/Getty Images</span></figcaption></figure><p><strong>A Brand Problem?</strong></p><p>Rather, the biggest obstacle in the way of Twitter's rebrand may be the fact that both Microsoft and Meta hold trademarks on the use of the letter X as a brand identity used for social media and communications purposes.&nbsp;</p><p>Microsoft‚Äôs trademark application on the brand mark 'X' was filed <a href="https://tsdr.uspto.gov/#caseNumber=76041368&amp;caseSearchType=US_APPLICATION&amp;caseType=DEFAULT&amp;searchType=statusSearch" target="_blank" rel="noreferrer noopener">all the way back in 2003</a> and last renewed on July 18, 2023.</p><p>In the application, Microsoft stated the X branding would be used for ‚Äúinteractive multiplayer game services for games played over computer networks and global communications networks; providing computer games and video games downloadable over computer global communications networks; providing information on the video game and computer game industries via the Internet; and providing information on computer games, video games, video game consoles and accessories therefor via the Internet.‚Äù</p><p>For context, a <a href="https://tsdr.uspto.gov/documentviewer?caseId=sn76041368&amp;docId=SPE20221228152814&amp;linkId=3#docIndex=2&amp;page=1" target="_blank" rel="noreferrer noopener">document</a> filed in December in support of the trademark branding was an advertisement for Xbox Community services.&nbsp;</p><p>Meta‚Äôs trademark <a href="https://tsdr.uspto.gov/#caseNumber=87980831&amp;caseType=SERIAL_NO&amp;searchType=statusSearch" target="_blank" rel="noreferrer noopener">use on 'X,'</a> which includes initial documentation from <a href="https://tsdr.uspto.gov/documentviewer?caseId=sn87980831&amp;docId=RFA20190508154130&amp;linkId=31#docIndex=30&amp;page=1" target="_blank" rel="noreferrer noopener">Microsoft filed in 2017</a>, is more encompassing.</p><p>The trademark 'X,' which is stylized as a white and blue symbol, is trademarked to brand a wide array of uses, including but not limited to streaming of video and audio content, ‚Äúproviding online forums for transmission of messages among computer users," and "providing internet chat rooms." These are also services offered by Twitter. The company also claimed the right to use X for audio broadcasting, which is similar to the Twitter Spaces feature.&nbsp;</p><p>It‚Äôs unclear when the trademark for the stylized X switched from Microsoft to Meta.</p><p>Copyright lawyer Katttwinkel said the Microsoft trademark filing could prevent Twitter from claiming their services are different enough that they could also successfully trademark 'X' for the brand. Twitter's parent company, X Corp., Kattwinkel said, would have to argue in court that the services their brand offers are actually different, despite both companies being to do with similar technology.</p><p>‚ÄúIn my opinion, [Twitter] won't win such an argument," she added.</p><p>And Ed Timberlake, a former examining attorney at the U.S. Patent &amp; Trademark Office and a practicing trademark lawyer, told The Messenger that Microsoft's claim might be the stronger one if both it and Meta separately decided to take Twitter to court.</p><p>"The strength of [Meta's] registration is it's primarily strong for just that stylization," said Timberlake, who was among the Twitter users who <a href="https://twitter.com/TimberlakeLaw/status/1683522597022236672" target="_blank" rel="noreferrer noopener">spotted</a> the Microsoft trademark registration.</p><p>"The rights are narrower because it's registered in that stylized form."</p><p>"Whereas Microsoft's X is essentially the X registered, without regard to what outfit it shows up in," Timberlake said.</p><p>"So it could show up in blue and yellow, it could show up in blue and white, it could show up as just a typed thing, it could show up in a different font ‚Äî and the registration rights would be just as powerful," he said.</p><p>Meta and Microsoft did not immediately respond to The Messenger's request for comment. Twitter's autoresponder replied "We'll get back to you soon."</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Byron Bay data breach victim told to pay Adidas, NBA $1.2M by US courts (184 pts)]]></title>
            <link>https://www.abc.net.au/news/2023-07-25/byron-bay-data-breach-victim-adidas-nab-us-court-action-damages/102575726</link>
            <guid>36855646</guid>
            <pubDate>Mon, 24 Jul 2023 23:29:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.abc.net.au/news/2023-07-25/byron-bay-data-breach-victim-adidas-nab-us-court-action-damages/102575726">https://www.abc.net.au/news/2023-07-25/byron-bay-data-breach-victim-adidas-nab-us-court-action-damages/102575726</a>, See on <a href="https://news.ycombinator.com/item?id=36855646">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Sarah Luke initially shrugged off a data breach that resulted in&nbsp;her personal details being released onto the dark web.</p><section role="contentinfo" aria-label="key points" data-component="KeyPoints" data-uri="coremedia://teaser/102575910"><h2 data-component="Heading">Key points:</h2><ul data-component="List" role="list"><li data-component="ListItem"><span></span>Hackers traded hundreds of counterfeit goods using Sarah Luke's PayPal account</li><li data-component="ListItem"><span></span>Adidas and the National Basketball Association filed charges relating to trademark infringement</li><li data-component="ListItem"><span></span>The US courts awarded damages against Ms Luke amounting to $US1.2 million ($1.8 million)</li></ul></section><p>But then&nbsp;she was charged in the United States with offences including trademark infringement, and was told to pay damages of $US1.2 million ($1.8 million).&nbsp;</p><p>The first inkling of her impending legal drama came via an email in December, referring to counterfeit Adidas items traded under her name.</p><p>"I thought it was a scam, another hoax, and I deleted the first email," she said.</p><p>"After subsequent emails, I realised, there's something in this, this is real.</p><p>"It was shocking, because this is big.&nbsp;</p><p>"The charges were cybersquatting, trademark infringement, IP infringement, things I don't know anything about."&nbsp;</p><p>Ms Luke said the nightmare began after her information was compromised in the <a href="https://www.abc.net.au/news/2022-12-01/medibank-data-leak-has-everything-been-released-now/101720028" data-component="ContentLink" data-uri="coremedia://article/101720028">Medibank data breach</a>.</p><p>She said this was the only breach of her information she was aware of.</p><p>Medibank released a statement to the ABC saying none of its customers' passwords were compromised in the breach, and it was therefore in no way connected to what unfolded for Ms Luke.</p><figure role="group" data-component="VerticalArticleFigure" aria-labelledby="102575902" data-uri="coremedia://imageproxy/102575902" attribution="[object Object]" figuretype="photo"><img alt="A printed legal document from the US district court in Florida, listing Adidas and others as plaintiffs." sizes="(max-width: 543px) 543px," srcset="https://live-production.wcms.abc-cdn.net.au/db002ef42b9d03fc725099844724dbac?impolicy=wcms_crop_resize&amp;cropH=1017&amp;cropW=1356&amp;xPos=221&amp;yPos=0&amp;width=862&amp;height=647 543w, https://live-production.wcms.abc-cdn.net.au/db002ef42b9d03fc725099844724dbac?impolicy=wcms_crop_resize&amp;cropH=1017&amp;cropW=1526&amp;xPos=136&amp;yPos=0&amp;width=862&amp;height=575" src="https://live-production.wcms.abc-cdn.net.au/db002ef42b9d03fc725099844724dbac?impolicy=wcms_crop_resize&amp;cropH=1017&amp;cropW=1526&amp;xPos=136&amp;yPos=0&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"><p><figcaption id="102575902" data-component="VerticalArticleFigure__figcaption" aria-live="polite"> <!-- -->Two companies&nbsp;took the action against Sarah Luke through the US court system.<!-- --> <!-- -->(<span data-component="Byline"><span data-component="Text"><span>ABC North Coast: Hannah Ross</span></span></span>)</figcaption></p></figure><p>Ms Luke said&nbsp;hackers took&nbsp;control of her PayPal account, in a credential stuffing attack that affected 35,000 PayPal customers in December.</p><p>Credential stuffing is where hackers access an account by using automation to try out username and password pairs sourced from data leaks on various websites.</p><p>Ms Luke said over the course of two days from December 6 to 8, her PayPal account was used to make hundreds of fraudulent transactions.&nbsp;</p><p>She was then served electronically with papers from the US District Court of Florida outlining Adidas' case against her.</p><p>Similar charges against her were also filed by the National Basketball Association in the District Court of Illinois.&nbsp;</p><p>In both cases, Adidas and the NBA were given leave by the courts to run the cases ex parte ‚Äî&nbsp;without a requirement for all parties in the case to be present.&nbsp;</p><p>In court documents seen by the ABC, default judgements were handed down by the US courts and damages were awarded against Ms Luke of $US200,000 ($293,000) in the NBA case and $US1million ($1.5 million)&nbsp;in the Adidas matter.</p><figure role="group" data-component="VerticalArticleFigure" aria-labelledby="102575904" data-uri="coremedia://imageproxy/102575904" attribution="[object Object]" figuretype="photo"><img alt="A woman with blonde hair sits at a table with a laptop and documents strewn about. She is reading something on a piece of paper." sizes="(max-width: 543px) 543px," srcset="https://live-production.wcms.abc-cdn.net.au/2b669ffe825a348b6695801a1676ec7c?impolicy=wcms_crop_resize&amp;cropH=1068&amp;cropW=1424&amp;xPos=108&amp;yPos=0&amp;width=862&amp;height=647 543w, https://live-production.wcms.abc-cdn.net.au/2b669ffe825a348b6695801a1676ec7c?impolicy=wcms_crop_resize&amp;cropH=1068&amp;cropW=1602&amp;xPos=19&amp;yPos=0&amp;width=862&amp;height=575" src="https://live-production.wcms.abc-cdn.net.au/2b669ffe825a348b6695801a1676ec7c?impolicy=wcms_crop_resize&amp;cropH=1068&amp;cropW=1602&amp;xPos=19&amp;yPos=0&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"><p><figcaption id="102575904" data-component="VerticalArticleFigure__figcaption" aria-live="polite"> <!-- -->Sarah Luke says the legal action is taking its toll emotionally and financially.<!-- --> <!-- -->(<span data-component="Byline"><span data-component="Text"><span>&nbsp;ABC North Coast: Hannah Ross</span></span></span>)</figcaption></p></figure><h2 data-component="Heading">Emotional toll</h2><p>Six months on, the Byron Bay woman said she was no closer to clearing her name.</p><p>She has taken the matter to the NSW Police, the Australian Consumer Complaints Authority, the Australian Financial Complaints Commission and the Australian Cyber Security Centre.</p><p>"I've come up against so many barriers trying to sort this out," Ms Luke said.</p><p>"I have felt unheard and unseen by so many organisations and parties.</p><p>"It just goes on and on and I don't know where to go now ‚Äî&nbsp;I don't know who to turn to."</p><figure role="group" data-component="VerticalArticleFigure" aria-labelledby="102625288" data-uri="coremedia://imageproxy/102625288" attribution="[object Object]" figuretype="photo"><img alt="Adidas shoe being held in woman's hands" sizes="(max-width: 543px) 543px," srcset="https://live-production.wcms.abc-cdn.net.au/670f4e78c7547a7031269806d45888d3?impolicy=wcms_crop_resize&amp;cropH=1115&amp;cropW=1487&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=647 543w, https://live-production.wcms.abc-cdn.net.au/670f4e78c7547a7031269806d45888d3?impolicy=wcms_crop_resize&amp;cropH=1041&amp;cropW=1562&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=575" src="https://live-production.wcms.abc-cdn.net.au/670f4e78c7547a7031269806d45888d3?impolicy=wcms_crop_resize&amp;cropH=1041&amp;cropW=1562&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"><p><figcaption id="102625288" data-component="VerticalArticleFigure__figcaption" aria-live="polite"> <!-- -->Adidas filed a civil suit against Sarah Luke in the District Court of Florida.<!-- --> <!-- -->(<span data-component="Byline"><span data-component="Text"><span>ABC North Coast: Hannah Ross</span></span></span>)</figcaption></p></figure><p>Ms Luke has engaged an intellectual property lawyer in the United States, with an initial engagement fee of $US10,000 ($14,800), in a bid to have the rulings overturned and the damages retracted. &nbsp;</p><p>The single mother of four said the situation was taking its toll.</p><p>"The anxiety that this causes, not knowing if they are going to come and take our house, can they freeze my assets, can they get access to my bank accounts?</p><p>"We just don't know and it really is a case of guilty until I can prove otherwise."</p><h2 data-component="Heading">Legal uncertainty</h2><p>William Gallagher, a professor of Law at the Golden Gate University in San Francisco and director of the Intellectual Property Law Program, said it was possible the companies behind the lawsuits were just flexing their corporate muscles.</p><p>"Maybe just getting the judgement is reward in its own right, it shows they are aggressive enforcers, the message gets out there," he said.</p><p>"There are&nbsp;some benefits to famous trademark owners, even if they never end up enforcing the monetary judgement."</p><p>He said these types of civil suits were relatively common in the US, as corporate entities worked to protect their brands.</p><p>But he said it was not automatic that a US judgement could be enforced in a different jurisdiction, such as Australia.</p><p>Ms Luke said it was not a risk she wanted to take.</p><p>"I've been unable to get any advice as to whether these lawsuits can stick or whether these companies are just trying to prove a point," she said.</p><p>Retired magistrate and dean of law at Southern Cross University, David Heilpern, said if a US company wanted to enforce a judgement in Australia, it would have to register it with the local courts.</p><figure role="group" data-component="VerticalArticleFigure" aria-labelledby="102625490" data-uri="coremedia://imageproxy/102625490" attribution="[object Object]" figuretype="photo"><img alt="A man with glasses places a thick legal book on a shelf in an office. He has a neutral expression on his face." sizes="(max-width: 543px) 543px," srcset="https://live-production.wcms.abc-cdn.net.au/23c301c237d0f93f5a3de9e300f50baa?impolicy=wcms_crop_resize&amp;cropH=1098&amp;cropW=1464&amp;xPos=471&amp;yPos=0&amp;width=862&amp;height=647 543w, https://live-production.wcms.abc-cdn.net.au/23c301c237d0f93f5a3de9e300f50baa?impolicy=wcms_crop_resize&amp;cropH=1098&amp;cropW=1647&amp;xPos=288&amp;yPos=0&amp;width=862&amp;height=575" src="https://live-production.wcms.abc-cdn.net.au/23c301c237d0f93f5a3de9e300f50baa?impolicy=wcms_crop_resize&amp;cropH=1098&amp;cropW=1647&amp;xPos=288&amp;yPos=0&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"><p><figcaption id="102625490" data-component="VerticalArticleFigure__figcaption" aria-live="polite"> <!-- -->David Heilpern says our digital identities are&nbsp;vulnerable to laws in other jurisdictions.<!-- --> <!-- -->(<span data-component="Byline"><span data-component="Text"><span>ABC North Coast: Hannah Ross</span></span></span>)</figcaption></p></figure><p>Mr Heilpern said even if&nbsp;Adidas and the NBA chose not to&nbsp;pursue the matter here,&nbsp;the judgement could affect Ms Luke's ability to travel to the US.</p><p>He said the case highlighted a problem for lawmakers dealing with the rise of cybercrime.</p><p>"It's really a sign of the times and technology being in advance of the law," he said.</p><p>"This situation would not have occurred 30 years ago before we had digital identities that so easily and seamlessly cross national boundaries.</p><p>"The Australian government and the legal mechanisms need to catch up to ensure that ‚Ä¶ Australians are protected."</p><p>The Australian Financial Complaints Authority said it was unable to comment on individual cases it was investigating.</p><p>The ABC has also sought comment from the federal Minister for Cyber Security, Clare O'Neil.</p><p>The Department of Home&nbsp;Affairs responded by offering to refer the case to&nbsp;the Australian Federal Police.</p></div><p><span data-component="Text">Posted<!-- -->&nbsp;</span><time data-component="ScreenReaderOnly" datetime="2023-07-24T22:17:24.000Z">6 hours ago</time><time data-component="Text">Mon 24 Jul 2023 at 10:17pm</time>, <span data-component="Text">updated<!-- -->&nbsp;</span><time data-component="ScreenReaderOnly" datetime="2023-07-25T02:55:38.000Z">2 hours ago</time><time data-component="Text">Tue 25 Jul 2023 at 2:55am</time></p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vscode.dev: Local Development with Cloud Tools (162 pts)]]></title>
            <link>https://vscode.dev/</link>
            <guid>36855517</guid>
            <pubDate>Mon, 24 Jul 2023 23:12:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vscode.dev/">https://vscode.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=36855517">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The Fall of Stack Overflow (513 pts)]]></title>
            <link>https://observablehq.com/@ayhanfuat/the-fall-of-stack-overflow</link>
            <guid>36855516</guid>
            <pubDate>Mon, 24 Jul 2023 23:11:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://observablehq.com/@ayhanfuat/the-fall-of-stack-overflow">https://observablehq.com/@ayhanfuat/the-fall-of-stack-overflow</a>, See on <a href="https://news.ycombinator.com/item?id=36855516">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next" data-reactroot=""><nav><div><a href="https://observablehq.com/"><svg role="img" viewBox="0 0 25 28" width="25" height="28" aria-label="Observable" fill="currentColor" style="width:22px"><path d="M12.5 22.6667C11.3458 22.6667 10.3458 22.4153 9.5 21.9127C8.65721 21.412 7.98339 20.7027 7.55521 19.8654C7.09997 18.9942 6.76672 18.0729 6.56354 17.1239C6.34796 16.0947 6.24294 15.0483 6.25 14C6.25 13.1699 6.30417 12.3764 6.41354 11.6176C6.52188 10.8598 6.72292 10.0894 7.01563 9.30748C7.30833 8.52555 7.68542 7.84763 8.14479 7.27274C8.62304 6.68378 9.24141 6.20438 9.95208 5.87163C10.6979 5.51244 11.5458 5.33333 12.5 5.33333C13.6542 5.33333 14.6542 5.58467 15.5 6.08733C16.3428 6.588 17.0166 7.29733 17.4448 8.13459C17.8969 8.99644 18.2271 9.9103 18.4365 10.8761C18.6448 11.841 18.75 12.883 18.75 14C18.75 14.8301 18.6958 15.6236 18.5865 16.3824C18.4699 17.1702 18.2639 17.9446 17.9719 18.6925C17.6698 19.4744 17.2948 20.1524 16.8427 20.7273C16.3906 21.3021 15.7927 21.7692 15.0479 22.1284C14.3031 22.4876 13.4542 22.6667 12.5 22.6667ZM14.7063 16.2945C15.304 15.6944 15.6365 14.864 15.625 14C15.625 13.1073 15.326 12.3425 14.7292 11.7055C14.1313 11.0685 13.3885 10.75 12.5 10.75C11.6115 10.75 10.8688 11.0685 10.2708 11.7055C9.68532 12.3123 9.36198 13.1405 9.375 14C9.375 14.8927 9.67396 15.6575 10.2708 16.2945C10.8688 16.9315 11.6115 17.25 12.5 17.25C13.3885 17.25 14.124 16.9315 14.7063 16.2945ZM12.5 27C19.4031 27 25 21.1792 25 14C25 6.82075 19.4031 1 12.5 1C5.59687 1 0 6.82075 0 14C0 21.1792 5.59687 27 12.5 27Z" fill="currentColor"></path></svg></a></div></nav><div><div><a href="https://observablehq.com/@ayhanfuat"><picture><source srcset="https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=60&amp;format=avif 1x, https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=120&amp;format=avif 2x, https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=180&amp;format=avif 3x" type="image/avif"><source srcset="https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=60&amp;format=webp 1x, https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=120&amp;format=webp 2x, https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=180&amp;format=webp 3x" type="image/webp"><source srcset="https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=60&amp;format=jpeg 1x, https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=120&amp;format=jpeg 2x, https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=180&amp;format=jpeg 3x" type="image/jpeg"><source srcset="https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=60&amp;format=png 1x, https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=120&amp;format=png 2x, https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=180&amp;format=png 3x" type="image/png"><img alt="@ayhanfuat" src="https://avatars1.githubusercontent.com/u/6108349?v=4&amp;s=60"></picture></a><div><p><span data-state="tooltip-hidden" data-reach-tooltip-trigger=""><a href="https://observablehq.com/@ayhanfuat">Ayhan Fuat √áelik</a></span></p></div></div></div><div data-cy="metadata-bar"><div data-cy="access-level" title="Anyone can see edits to this notebook."><svg viewBox="0 0 16 16" width="16" height="16" fill="none" stroke="currentColor" stroke-width="1.8"><circle cx="8" cy="8" r="7.1"></circle><path d="M1.14285 10.5286 L14.8571 10.5286 M1.14285 5.71429H14.8571 M8 1.14285C7.42857 2.09523 5.14285 4.21428 5.14285 8C5.14285 12.5714 8 14.8571 8 14.8571 M8 1.14285C8.57143 2.09523 10.8571 4.21428 10.8571 8C10.8571 12.5714 8 14.8571 8 14.8571"></path></svg><p>Public</p></div><div title=""><svg width="16" height="17" viewBox="0 0 16 17" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M1 14C1 13.4477 1.44772 13 2 13L4 13C4.55228 13 5 13.4477 5 14C5 14.5523 4.55228 15 4 15L2 15C1.44772 15 1 14.5523 1 14Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M10.884 3.65785L7.78484 10.3041L8.12765 11.9351L9.59745 11.1493L12.6967 4.50309L10.884 3.65785ZM10.823 1.42262C10.3224 1.18921 9.72744 1.40577 9.49404 1.90631L5.83135 9.76098C5.7399 9.95707 5.71453 10.1775 5.75904 10.3893L6.44467 13.6513C6.5818 14.3038 7.30684 14.6418 7.89476 14.3275L10.8344 12.7559C11.0252 12.6539 11.1778 12.4928 11.2692 12.2967L14.9319 4.44202C15.1653 3.94148 14.9487 3.3465 14.4482 3.11309L10.823 1.42262Z" fill="currentColor"></path></svg><p>Edited </p></div><div><svg width="16" height="16" viewBox="0 0 16 16" fill="currentColor" style="flex-shrink:0"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.5 1.75C3.80964 1.75 3.25 2.30964 3.25 3C3.25 3.69036 3.80964 4.25 4.5 4.25C5.19036 4.25 5.75 3.69036 5.75 3C5.75 2.30964 5.19036 1.75 4.5 1.75ZM1.75 3C1.75 1.48122 2.98122 0.25 4.5 0.25C6.01878 0.25 7.25 1.48122 7.25 3C7.25 4.16599 6.52434 5.1625 5.5 5.56253V7H8.5C9.4199 7 10.1947 6.37895 10.4281 5.53327C9.44188 5.11546 8.75 4.13853 8.75 3C8.75 1.48122 9.98122 0.25 11.5 0.25C13.0188 0.25 14.25 1.48122 14.25 3C14.25 4.18168 13.5047 5.18928 12.4585 5.57835C12.1782 7.51343 10.5127 9 8.5 9H5.5V10.4375C6.52434 10.8375 7.25 11.834 7.25 13C7.25 14.5188 6.01878 15.75 4.5 15.75C2.98122 15.75 1.75 14.5188 1.75 13C1.75 11.834 2.47566 10.8375 3.5 10.4375L3.5 9V7V5.56253C2.47566 5.1625 1.75 4.16599 1.75 3ZM4.5 11.75C3.80964 11.75 3.25 12.3096 3.25 13C3.25 13.6904 3.80964 14.25 4.5 14.25C5.19036 14.25 5.75 13.6904 5.75 13C5.75 12.3096 5.19036 11.75 4.5 11.75ZM10.25 3C10.25 2.30964 10.8096 1.75 11.5 1.75C12.1904 1.75 12.75 2.30964 12.75 3C12.75 3.69036 12.1904 4.25 11.5 4.25C10.8096 4.25 10.25 3.69036 10.25 3Z"></path></svg><p><span>1&nbsp;fork</span></p></div><div><svg viewBox="0 0 16 16" fill="var(--white)" stroke="var(--moon-gray)" stroke-width="1.8" stroke-linecap="round" stroke-linejoin="round" width="16" height="16" style="width:16px;height:16px;stroke-width:2"><path d="M13.075 3.925A3.157 3.157 0 0 0 10.842 3c-.838 0-1.641.478-2.233 1.07L8 4.68l-.609-.61c-1.233-1.233-3.233-1.378-4.466-.145a3.158 3.158 0 0 0 0 4.467L3.534 9 8 13.788 12.466 9l.609-.608a3.157 3.157 0 0 0 0-4.467z"></path></svg><p>2</p><!-- --><p>&nbsp;Like</p><!-- --><p>s</p></div></div><div><p>3</p></div><div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div><div><p><svg width="17" height="15" viewBox="-8.5 -7.5 17 15" style="overflow:visible" role="button" aria-label="Click to insert or merge cells" stroke="currentColor" stroke-width="2" stroke-linecap="round" fill="none" transform=""><title>Insert cell</title><path d="M0,-4.5V4.5M-4.5,0H4.5"></path></svg></p></div></div><div data-cy="welcome-mat-banner"><p><span>Create interactive documents like this one.</span></p></div><div><div data-cy="welcome-mat-footer"><p><span>Learn new data visualization techniques. Perform complex data analysis.<!-- --> <br> Publish your findings in a compelling document. All in the same tool.</span></p></div><div><div><p>More from Observable creators</p></div></div><div><div><a title="Home" href="https://observablehq.com/"><svg role="img" width="173" height="24" viewBox="0 0 173 24" aria-label="Observable" fill="currentColor"><path d="M40.8496 20.6083C39.7407 20.6083 38.6757 20.4234 37.6548 20.0538C36.6339 19.6842 35.7186 19.1209 34.9089 18.364C34.1168 17.607 33.4919 16.6565 33.0343 15.5124C32.5766 14.3683 32.3478 13.0218 32.3478 11.4727C32.3478 9.92375 32.5766 8.57718 33.0343 7.43305C33.5095 6.2889 34.1432 5.33839 34.9353 4.5815C35.7274 3.82462 36.6339 3.26135 37.6548 2.8917C38.6757 2.52205 39.7407 2.33723 40.8496 2.33723C41.9585 2.33723 43.0234 2.52205 44.0444 2.8917C45.0653 3.26135 45.9718 3.82462 46.7639 4.5815C47.556 5.33839 48.1809 6.2889 48.6386 7.43305C49.1138 8.57718 49.3514 9.92375 49.3514 11.4727C49.3514 13.0218 49.1225 14.3683 48.665 15.5124C48.2073 16.6565 47.5736 17.607 46.7639 18.364C45.9718 19.1209 45.0653 19.6842 44.0444 20.0538C43.0234 20.4234 41.9585 20.6083 40.8496 20.6083ZM40.8496 18.9977C41.6065 18.9977 42.2049 18.6896 42.645 18.0736C43.1027 17.4398 43.4283 16.5598 43.622 15.4332C43.8332 14.3067 43.9388 12.9865 43.9388 11.4727C43.9388 9.95894 43.8332 8.63879 43.622 7.51226C43.4283 6.38572 43.1027 5.50562 42.645 4.87193C42.2049 4.23826 41.6065 3.92142 40.8496 3.92142C40.1103 3.92142 39.5118 4.23826 39.0542 4.87193C38.5965 5.50562 38.2621 6.38572 38.0509 7.51226C37.8572 8.63879 37.7604 9.95894 37.7604 11.4727C37.7604 12.9865 37.8572 14.3067 38.0509 15.4332C38.2621 16.5598 38.5965 17.4398 39.0542 18.0736C39.5118 18.6896 40.1103 18.9977 40.8496 18.9977ZM50.5773 20.1066V18.76L51.7655 18.496C51.7831 17.8622 51.7919 17.1846 51.7919 16.4629C51.7919 15.7413 51.7919 15.1076 51.7919 14.5619V3.60459L50.3661 3.41977V2.23162L56.386 1.04348L56.8349 1.33392L56.7293 5.00396V8.33075C57.7678 7.30983 59.0175 6.79937 60.4785 6.79937C61.4466 6.79937 62.3267 7.0546 63.1188 7.56506C63.9109 8.05792 64.5358 8.80601 64.9934 9.80933C65.4687 10.7951 65.7063 12.0448 65.7063 13.5586C65.7063 15.002 65.4335 16.2429 64.8878 17.2814C64.3422 18.3199 63.6293 19.1209 62.7491 19.6842C61.8867 20.2298 60.9626 20.5026 59.9768 20.5026C59.22 20.5026 58.5599 20.3706 57.9966 20.1066C57.4333 19.8601 56.9405 19.4993 56.518 19.0241L55.99 20.5026L50.5773 20.1066ZM58.0494 9.41329C57.8206 9.41329 57.6006 9.44848 57.3893 9.5189C57.1957 9.5893 57.0021 9.68612 56.8085 9.80933V17.5982C57.1605 17.8447 57.5742 17.968 58.0494 17.968C58.7535 17.968 59.3168 17.607 59.7392 16.8853C60.1616 16.1637 60.3729 15.0548 60.3729 13.5586C60.3729 12.0448 60.1616 10.9799 59.7392 10.3638C59.3168 9.73012 58.7535 9.41329 58.0494 9.41329ZM71.7275 20.5026C69.8968 20.5026 68.2775 20.0714 66.8693 19.2089L67.0013 15.8292H69.404L69.8264 18.5752C70.1257 18.6984 70.4337 18.7952 70.7505 18.8657C71.0674 18.9185 71.3931 18.9449 71.7275 18.9449C72.3964 18.9449 72.9156 18.8393 73.2852 18.628C73.6549 18.4168 73.8397 18.0647 73.8397 17.5718C73.8397 17.2198 73.699 16.903 73.4173 16.6213C73.1532 16.3397 72.5812 16.0845 71.7011 15.8556L70.1961 15.4596C69.1575 15.178 68.3654 14.6851 67.8198 13.981C67.2742 13.2594 67.0013 12.3881 67.0013 11.3671C67.0013 10.047 67.503 8.95563 68.5063 8.09312C69.5272 7.23062 70.9882 6.79937 72.8892 6.79937C73.699 6.79937 74.447 6.88738 75.1335 7.06341C75.8376 7.23942 76.5505 7.50345 77.2721 7.8555L77.0609 10.839H74.6318L74.0509 8.48917C73.8749 8.45397 73.6813 8.42757 73.4701 8.40996C73.2765 8.37476 73.0389 8.35715 72.7572 8.35715C72.2644 8.35715 71.8419 8.47157 71.4898 8.70039C71.1554 8.91162 70.9882 9.24606 70.9882 9.70372C70.9882 9.98535 71.1115 10.2582 71.3578 10.5222C71.6043 10.7862 72.1851 11.0414 73.1004 11.2879L74.579 11.6839C75.8112 12.0184 76.6912 12.5377 77.2193 13.2417C77.765 13.9459 78.0379 14.8172 78.0379 15.8556C78.0379 17.4047 77.4658 18.5664 76.3216 19.3409C75.1951 20.1153 73.6637 20.5026 71.7275 20.5026ZM85.8767 8.22514C85.4718 8.22514 85.1286 8.52438 84.847 9.12285C84.5653 9.72132 84.4069 10.839 84.3717 12.476H85.6127C86.2464 12.476 86.6688 12.3528 86.88 12.1064C87.1088 11.8424 87.2233 11.3759 87.2233 10.707C87.2233 9.79173 87.0824 9.14926 86.8008 8.7796C86.5368 8.40996 86.2287 8.22514 85.8767 8.22514ZM85.9559 20.5026C84.6358 20.5026 83.4564 20.2298 82.4179 19.6842C81.3969 19.1384 80.5873 18.3552 79.9887 17.3342C79.3903 16.2957 79.091 15.0548 79.091 13.6114C79.091 12.4673 79.2935 11.4727 79.6983 10.6278C80.1032 9.78293 80.6488 9.07884 81.3353 8.51557C82.0219 7.93471 82.7876 7.50345 83.6325 7.22182C84.4774 6.94018 85.331 6.79937 86.1936 6.79937C87.4609 6.79937 88.5082 7.06341 89.3355 7.59147C90.1804 8.10193 90.8141 8.79721 91.2365 9.67732C91.659 10.5398 91.8702 11.5079 91.8702 12.5816C91.8702 12.8633 91.8614 13.1097 91.8438 13.3209C91.8262 13.5146 91.791 13.7346 91.7382 13.981H84.3981C84.5389 15.266 84.9174 16.1989 85.5335 16.7797C86.1672 17.3606 86.8712 17.651 87.6457 17.651C88.3145 17.651 88.8867 17.5367 89.3619 17.3078C89.8547 17.0614 90.286 16.7622 90.6557 16.4101L91.6854 17.4134C91.1221 18.4872 90.3477 19.2704 89.3619 19.7634C88.3937 20.2562 87.2584 20.5026 85.9559 20.5026ZM92.9624 20.0802V18.7336L94.2298 18.4432C94.2473 17.8094 94.2562 17.1406 94.2562 16.4365C94.2562 15.7149 94.2562 15.0812 94.2562 14.5355V12.8721C94.2562 12.3792 94.2473 11.9832 94.2298 11.6839C94.2298 11.3848 94.221 11.1119 94.2034 10.8654C94.2034 10.6014 94.1945 10.2934 94.177 9.94135L92.7512 9.70372V8.62118L98.5863 6.79937L99.0615 7.08981L99.2727 10.1526C99.6248 9.00844 100.127 8.16353 100.778 7.61787C101.447 7.0722 102.098 6.79937 102.732 6.79937C103.383 6.79937 103.929 6.99299 104.369 7.38024C104.826 7.74988 105.099 8.33956 105.187 9.14925C105.152 9.85333 104.941 10.4078 104.553 10.8126C104.166 11.1998 103.709 11.3935 103.18 11.3935C102.371 11.3935 101.631 10.9094 100.963 9.94135L100.831 9.75652C100.514 10.1086 100.197 10.5662 99.88 11.1295C99.5808 11.6928 99.3784 12.2736 99.2727 12.8721V14.5355C99.2727 15.046 99.2727 15.6444 99.2727 16.3309C99.2727 17.0173 99.2816 17.6687 99.2991 18.2848L101.359 18.7336V20.0802H92.9624ZM114.743 8.46277V7.22182H119.707V8.46277L118.123 8.7268L113.766 20.0802H111.39L106.69 8.7268L105.423 8.46277V7.22182H113.37V8.46277L111.971 8.7796L114.162 14.8523L116.195 8.7532L114.743 8.46277ZM129.974 20.5026C129.147 20.5026 128.478 20.3442 127.967 20.0274C127.475 19.6929 127.114 19.2265 126.885 18.628C126.374 19.1912 125.855 19.6489 125.327 20.001C124.817 20.3354 124.086 20.5026 123.136 20.5026C122.097 20.5026 121.261 20.2034 120.627 19.6049C120.011 19.0064 119.703 18.1703 119.703 17.0966C119.703 16.4101 119.853 15.8028 120.152 15.2747C120.469 14.7291 121.015 14.2363 121.789 13.7962C122.581 13.3386 123.69 12.9249 125.116 12.5552C125.327 12.5024 125.574 12.4409 125.855 12.3704C126.137 12.2825 126.427 12.2033 126.727 12.1328V11.1823C126.727 10.0558 126.594 9.28127 126.33 8.85881C126.084 8.43636 125.547 8.22514 124.72 8.22514C124.649 8.22514 124.579 8.22514 124.509 8.22514C124.456 8.22514 124.394 8.22514 124.324 8.22514V9.12285C124.324 10.1966 124.104 10.9622 123.664 11.4199C123.224 11.86 122.722 12.08 122.159 12.08C121.085 12.08 120.416 11.6047 120.152 10.6542C120.152 9.51009 120.689 8.58599 121.763 7.8819C122.854 7.16021 124.465 6.79937 126.594 6.79937C128.425 6.79937 129.71 7.20422 130.449 8.01391C131.206 8.80601 131.585 10.1086 131.585 11.9216V17.968C131.585 18.2671 131.734 18.4168 132.034 18.4168C132.139 18.4168 132.245 18.3816 132.35 18.3112C132.456 18.2232 132.588 18.0647 132.746 17.8358L133.512 18.2584C133.195 19.0681 132.755 19.6489 132.192 20.001C131.646 20.3354 130.907 20.5026 129.974 20.5026ZM124.245 16.2781C124.245 16.9294 124.377 17.4134 124.641 17.7302C124.905 18.0472 125.23 18.2056 125.618 18.2056C125.741 18.2056 125.873 18.1792 126.014 18.1264C126.172 18.0559 126.41 17.9152 126.727 17.7038V13.3737C126.462 13.4442 126.207 13.541 125.961 13.6642C125.591 13.8403 125.213 14.1394 124.825 14.5619C124.438 14.9843 124.245 15.5564 124.245 16.2781ZM134.041 20.1066V18.76L135.229 18.496C135.247 17.8622 135.256 17.1846 135.256 16.4629C135.256 15.7413 135.256 15.1076 135.256 14.5619V3.60459L133.83 3.41977V2.23162L139.85 1.04348L140.299 1.33392L140.193 5.00396V8.33075C141.232 7.30983 142.481 6.79937 143.942 6.79937C144.91 6.79937 145.791 7.0546 146.583 7.56506C147.375 8.05792 148 8.80601 148.457 9.80933C148.933 10.7951 149.17 12.0448 149.17 13.5586C149.17 15.002 148.898 16.2429 148.352 17.2814C147.806 18.3199 147.093 19.1209 146.213 19.6842C145.351 20.2298 144.426 20.5026 143.441 20.5026C142.684 20.5026 142.024 20.3706 141.461 20.1066C140.897 19.8601 140.405 19.4993 139.982 19.0241L139.454 20.5026L134.041 20.1066ZM141.513 9.41329C141.284 9.41329 141.064 9.44848 140.853 9.5189C140.659 9.5893 140.466 9.68612 140.272 9.80933V17.5982C140.625 17.8447 141.038 17.968 141.513 17.968C142.217 17.968 142.781 17.607 143.203 16.8853C143.626 16.1637 143.837 15.0548 143.837 13.5586C143.837 12.0448 143.626 10.9799 143.203 10.3638C142.781 9.73012 142.217 9.41329 141.513 9.41329ZM150.465 20.0802V18.7336L151.653 18.4696C151.671 17.8007 151.679 17.1406 151.679 16.4893C151.697 15.838 151.707 15.1868 151.707 14.5355V3.6838L150.28 3.41977V2.23162L156.379 1.04348L156.828 1.33392L156.723 5.00396V14.5355C156.723 15.1868 156.723 15.8469 156.723 16.5157C156.74 17.167 156.758 17.8271 156.776 18.496L157.963 18.7336V20.0802H150.465ZM165.938 8.22514C165.533 8.22514 165.189 8.52438 164.908 9.12285C164.626 9.72132 164.467 10.839 164.432 12.476H165.674C166.307 12.476 166.73 12.3528 166.94 12.1064C167.169 11.8424 167.284 11.3759 167.284 10.707C167.284 9.79173 167.143 9.14926 166.861 8.7796C166.597 8.40996 166.289 8.22514 165.938 8.22514ZM166.016 20.5026C164.696 20.5026 163.517 20.2298 162.478 19.6842C161.457 19.1384 160.647 18.3552 160.049 17.3342C159.45 16.2957 159.152 15.0548 159.152 13.6114C159.152 12.4673 159.354 11.4727 159.759 10.6278C160.164 9.78293 160.71 9.07884 161.395 8.51557C162.082 7.93471 162.848 7.50345 163.693 7.22182C164.537 6.94018 165.392 6.79937 166.254 6.79937C167.522 6.79937 168.568 7.06341 169.396 7.59147C170.241 8.10193 170.874 8.79721 171.297 9.67732C171.719 10.5398 171.93 11.5079 171.93 12.5816C171.93 12.8633 171.922 13.1097 171.904 13.3209C171.886 13.5146 171.851 13.7346 171.799 13.981H164.459C164.6 15.266 164.978 16.1989 165.594 16.7797C166.228 17.3606 166.932 17.651 167.706 17.651C168.375 17.651 168.947 17.5367 169.423 17.3078C169.915 17.0614 170.346 16.7622 170.716 16.4101L171.746 17.4134C171.182 18.4872 170.408 19.2704 169.423 19.7634C168.454 20.2562 167.319 20.5026 166.016 20.5026Z"></path><path d="M11.4413 19.7265C10.3849 19.7265 9.46964 19.4977 8.69544 19.04C7.92403 18.5841 7.30727 17.9383 6.91536 17.1761C6.49867 16.3829 6.19365 15.5442 6.00767 14.6801C5.81035 13.743 5.71422 12.7903 5.72068 11.8359C5.72068 11.0802 5.77027 10.3577 5.87038 9.66687C5.96953 8.97688 6.15355 8.27548 6.42147 7.56358C6.68939 6.85167 7.03454 6.23445 7.45501 5.71104C7.89276 5.17482 8.45875 4.73835 9.10924 4.4354C9.79191 4.10837 10.568 3.9453 11.4413 3.9453C12.4977 3.9453 13.4131 4.17413 14.1873 4.63178C14.9587 5.08762 15.5755 5.73343 15.9674 6.49571C16.3811 7.28039 16.6834 8.11242 16.875 8.99178C17.0658 9.87026 17.1621 10.8189 17.1621 11.8359C17.1621 12.5917 17.1125 13.3141 17.0123 14.0049C16.9057 14.7222 16.7171 15.4272 16.4498 16.1082C16.1733 16.8201 15.8301 17.4374 15.4162 17.9608C15.0025 18.4842 14.4552 18.9094 13.7735 19.2364C13.0918 19.5634 12.3147 19.7265 11.4413 19.7265ZM13.4608 13.9249C14.0079 13.3785 14.3122 12.6225 14.3017 11.8359C14.3017 11.0232 14.0281 10.3268 13.4817 9.74686C12.9344 9.1669 12.2547 8.87692 11.4413 8.87692C10.628 8.87692 9.94827 9.1669 9.40099 9.74686C8.86507 10.2993 8.56912 11.0534 8.58102 11.8359C8.58102 12.6486 8.85467 13.3449 9.40099 13.9249C9.94827 14.5049 10.628 14.7949 11.4413 14.7949C12.2547 14.7949 12.9278 14.5049 13.4608 13.9249ZM11.4413 23.6718C17.7599 23.6718 22.8828 18.3723 22.8828 11.8359C22.8828 5.29952 17.7599 0 11.4413 0C5.12287 0 0 5.29952 0 11.8359C0 18.3723 5.12287 23.6718 11.4413 23.6718Z"></path></svg></a><div><p><span><a href="https://www.linkedin.com/company/observable"><svg height="16" style="fill:currentColor" version="1.1" viewBox="0 0 512 512" width="16"><title>LinkedIn</title><path d="M473.305,-1.353c20.88,0 37.885,16.533 37.885,36.926l0,438.251c0,20.393 -17.005,36.954 -37.885,36.954l-436.459,0c-20.839,0 -37.773,-16.561 -37.773,-36.954l0,-438.251c0,-20.393 16.934,-36.926 37.773,-36.926l436.459,0Zm-37.829,436.389l0,-134.034c0,-65.822 -14.212,-116.427 -91.12,-116.427c-36.955,0 -61.739,20.263 -71.867,39.476l-1.04,0l0,-33.411l-72.811,0l0,244.396l75.866,0l0,-120.878c0,-31.883 6.031,-62.773 45.554,-62.773c38.981,0 39.468,36.461 39.468,64.802l0,118.849l75.95,0Zm-284.489,-244.396l-76.034,0l0,244.396l76.034,0l0,-244.396Zm-37.997,-121.489c-24.395,0 -44.066,19.735 -44.066,44.047c0,24.318 19.671,44.052 44.066,44.052c24.299,0 44.026,-19.734 44.026,-44.052c0,-24.312 -19.727,-44.047 -44.026,-44.047Z" style="fill-rule:nonzero"></path></svg></a></span><span><a href="https://twitter.com/observablehq"><svg viewBox="0 0 16 16" width="16" height="16" fill="currentColor"><title>Twitter</title><path d="M15 3.429c-.517.24-1.07.402-1.651.477a3.028 3.028 0 0 0 1.264-1.675 5.761 5.761 0 0 1-1.827.728 2.8 2.8 0 0 0-2.1-.959C9.1 2 7.812 3.355 7.812 5.025c0 .24.027.47.074.691-2.39-.119-4.509-1.327-5.926-3.153-.25.444-.39.96-.39 1.522 0 1.052.509 1.977 1.279 2.52a2.758 2.758 0 0 1-1.302-.379c.66 1.819 1.428 2.821 2.306 3.007a2.783 2.783 0 0 1-1.293.052c.37 1.202 1.43 2.078 2.691 2.102A5.588 5.588 0 0 1 1 12.641 7.886 7.886 0 0 0 5.417 14c5.291 0 8.181-4.611 8.181-8.604 0-.128 0-.258-.008-.387.374-.283.844-.81 1.41-1.58z"></path></svg></a></span><span><a href="https://github.com/observablehq/"><svg viewBox="0 0 16 16" width="16" height="16" fill="currentColor"><title>GitHub</title><path d="M14.0609 4.65755C13.435 3.58505 12.5859 2.73595 11.5135 2.11005C10.4409 1.48413 9.26999 1.17125 7.99989 1.17125C6.72994 1.17125 5.55864 1.48423 4.4863 2.11005C3.4138 2.73591 2.56476 3.58505 1.9388 4.65755C1.31295 5.73002 1 6.90116 1 8.17095C1 9.69625 1.44501 11.0678 2.33526 12.2861C3.22542 13.5044 4.37536 14.3474 5.78501 14.8153C5.94909 14.8457 6.07056 14.8243 6.14954 14.7516C6.22855 14.6787 6.26801 14.5875 6.26801 14.4782C6.26801 14.46 6.26644 14.296 6.26341 13.9861C6.26028 13.6761 6.25881 13.4057 6.25881 13.175L6.04917 13.2113C5.91551 13.2358 5.74689 13.2461 5.54331 13.2432C5.33983 13.2404 5.1286 13.219 4.90989 13.1794C4.69109 13.1401 4.48757 13.0489 4.29919 12.9062C4.11091 12.7634 3.97725 12.5764 3.89823 12.3457L3.80709 12.136C3.74634 11.9963 3.6507 11.8412 3.52004 11.6712C3.38937 11.501 3.25724 11.3856 3.12358 11.3249L3.05977 11.2792C3.01724 11.2488 2.97779 11.2122 2.9413 11.1697C2.90484 11.1273 2.87755 11.0847 2.85932 11.0421C2.84106 10.9995 2.85619 10.9646 2.90487 10.9371C2.95356 10.9097 3.04154 10.8964 3.1692 10.8964L3.35142 10.9236C3.47295 10.948 3.62328 11.0208 3.80259 11.1424C3.98181 11.2639 4.12914 11.4218 4.2446 11.6162C4.38443 11.8654 4.55289 12.0552 4.75046 12.1859C4.94788 12.3166 5.14692 12.3818 5.3474 12.3818C5.54788 12.3818 5.72103 12.3666 5.86692 12.3364C6.01265 12.306 6.14938 12.2603 6.27704 12.1996C6.33173 11.7923 6.48062 11.4794 6.72359 11.2607C6.37728 11.2243 6.06593 11.1695 5.78938 11.0966C5.51299 11.0236 5.22737 10.9052 4.93271 10.741C4.6379 10.577 4.39334 10.3733 4.19895 10.1304C4.00454 9.88734 3.84499 9.56824 3.72052 9.17337C3.59598 8.77835 3.5337 8.32268 3.5337 7.80622C3.5337 7.07086 3.77377 6.4451 4.2538 5.92858C4.02893 5.37573 4.05016 4.75597 4.31755 4.06936C4.49377 4.01461 4.75509 4.05569 5.1014 4.19236C5.44777 4.32909 5.70137 4.44621 5.86245 4.54332C6.02354 4.6404 6.15261 4.72267 6.24984 4.78939C6.81505 4.63147 7.39832 4.55249 7.99982 4.55249C8.60133 4.55249 9.18473 4.63147 9.74996 4.78939L10.0963 4.57075C10.3331 4.42486 10.6128 4.29116 10.9347 4.16963C11.2567 4.04816 11.503 4.0147 11.6732 4.06945C11.9465 4.75609 11.9709 5.37582 11.7459 5.92867C12.2259 6.4452 12.4661 7.07112 12.4661 7.80632C12.4661 8.32277 12.4036 8.77989 12.2793 9.17794C12.1548 9.57606 11.9938 9.89485 11.7964 10.135C11.5988 10.3751 11.3526 10.5771 11.058 10.7411C10.7633 10.9052 10.4776 11.0236 10.2012 11.0966C9.92465 11.1695 9.6133 11.2244 9.26699 11.2608C9.58284 11.5342 9.7408 11.9656 9.7408 12.555V14.478C9.7408 14.5872 9.77879 14.6784 9.85483 14.7513C9.93078 14.8241 10.0507 14.8455 10.2148 14.815C11.6246 14.3472 12.7746 13.5041 13.6647 12.2858C14.5547 11.0676 14.9999 9.69599 14.9999 8.17069C14.9996 6.90106 14.6865 5.73002 14.0609 4.65755Z"></path></svg></a></span><span><a href="https://www.youtube.com/c/Observablehq"><svg height="16" style="fill:currentColor" version="1.1" viewBox="0 0 512 512" width="16"><title>YouTube</title><path d="M501.303,132.765c-5.887,-22.03 -23.235,-39.377 -45.265,-45.265c-39.932,-10.7 -200.038,-10.7 -200.038,-10.7c0,0 -160.107,0 -200.039,10.7c-22.026,5.888 -39.377,23.235 -45.264,45.265c-10.697,39.928 -10.697,123.238 -10.697,123.238c0,0 0,83.308 10.697,123.232c5.887,22.03 23.238,39.382 45.264,45.269c39.932,10.696 200.039,10.696 200.039,10.696c0,0 160.106,0 200.038,-10.696c22.03,-5.887 39.378,-23.239 45.265,-45.269c10.696,-39.924 10.696,-123.232 10.696,-123.232c0,0 0,-83.31 -10.696,-123.238Zm-296.506,200.039l0,-153.603l133.019,76.802l-133.019,76.801Z" style="fill-rule:nonzero"></path></svg></a></span></p></div></div><div><div><div><p><a href="https://observablehq.com/product">Product</a></p></div><ul><li><a href="https://observablehq.com/data-integrations">Integrations</a></li><li><a href="https://observablehq.com/pricing">Pricing</a></li><li><a href="https://observablehq.com/enterprise">Enterprise</a></li></ul></div><div><div><p><a href="https://observablehq.com/explore">Resources</a></p></div><ul><li><a href="https://observablehq.com/customer-stories">Customer stories</a></li><li><a href="https://observablehq.com/community">Community</a></li><li><a href="https://observablehq.com/documentation">Documentation</a></li></ul></div><div><div><p><a href="https://observablehq.com/about">Company</a></p></div><ul><li><a href="https://observablehq.com/blog">Blog</a></li><li><a href="https://observablehq.com/about#jobs">Jobs</a></li><li><a href="https://observablehq.com/collection/@observablehq/newsletters/2">Newsletter</a></li></ul></div></div><div><p><span>¬© 2023 Observable, Inc.</span><span><a href="https://observablehq.com/privacy-policy">Privacy</a></span><span><a href="https://observablehq.com/terms-of-service">Terms of Service</a></span></p></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft Owns the Trademark for ‚ÄúX‚Äù (209 pts)]]></title>
            <link>https://tsdr.uspto.gov/#caseNumber=2693757&amp;caseSearchType=US_APPLICATION&amp;caseType=DEFAULT&amp;searchType=statusSearch</link>
            <guid>36855342</guid>
            <pubDate>Mon, 24 Jul 2023 22:50:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tsdr.uspto.gov/#caseNumber=2693757&#x26;caseSearchType=US_APPLICATION&#x26;caseType=DEFAULT&#x26;searchType=statusSearch">https://tsdr.uspto.gov/#caseNumber=2693757&#x26;caseSearchType=US_APPLICATION&#x26;caseType=DEFAULT&#x26;searchType=statusSearch</a>, See on <a href="https://news.ycombinator.com/item?id=36855342">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="contentBoxMain">
            <div id="announcementWrapper"><pre>&lt;b&gt;For assistance with TSDR&lt;/b&gt;, email &lt;a href="mailto:teas@uspto.gov"&gt;teas@uspto.gov&lt;/a&gt;  and include your serial number, the document you are looking for, and a screenshot of any error messages you have received.</pre></div>
            
            
            
            
            
            <div id="multiSearchContainer">
               <table id="multiSerReg">
                  <colgroup>
                     <col width="10%">
                     <col width="10%">
                     <col width="7%">
                     <col width="7%">
                     <col width="7%">
                     <col width="9%">
                     <col width="17%">
                     <col width="10%">
                     <col width="20%">
                  </colgroup>
                  <thead>
                     <tr>
                        <th>Image</th>
                        <th>Mark</th>
                        <th>Ser No</th>
                        <th>Reg No</th>
                        <th>Status</th>
                        <th>Filing</th>
                        <th>Owner</th>
                        <th>Class(es)</th>
                        <th>Goods and Services</th>
                     </tr>
                  </thead>
                  <tfoot>
                     <tr>
                        <th>Image</th>
                        <th>Mark</th>
                        <th>Ser No</th>
                        <th>Reg No</th>
                        <th>Status</th>
                        <th>Filing</th>
                        <th>Owner</th>
                        <th>Class(es)</th>
                        <th>Goods and Services</th>
                     </tr>
                  </tfoot>
               </table>
               <table id="multiIntlReg">
                  <colgroup>
                     <col width="10%">
                     <col width="10%">
                     <col width="7%">
                     <col width="7%">
                     <col width="7%">
                     <col width="9%">
                     <col width="17%">
                     <col width="10%">
                     <col width="20%">
                  </colgroup>
                  <thead>
                     <tr>
                        <th>Image</th>
                        <th>Mark</th>
                        <th>IR No</th>
                        <th>IR Date</th>
                        <th>Ser No</th>
                        <th>Ref No</th>
                        <th>Owner</th>
                        <th>Class(es)</th>
                        <th>Goods and Services</th>
                     </tr>
                  </thead>
                  <tfoot>
                     <tr>
                        <th>Image</th>
                        <th>Mark</th>
                        <th>IR No</th>
                        <th>IR Date</th>
                        <th>Ser No</th>
                        <th>Ref No</th>
                        <th>Owner</th>
                        <th>Class(es)</th>
                        <th>Goods and Services</th>
                     </tr>
                  </tfoot>
               </table>
               <table id="multiRef">
                  <colgroup>
                     <col width="15%">
                     <col width="15%">
                     <col width="15%">
                     <col width="15%">
                     <col width="40%">
                  </colgroup>
                  <thead>
                     <tr>
                        <th>Reference No.</th>
                        <th>Filing Date</th>
                        <th>Intl Reg No.</th>
                        <th>Intl Reg Date</th>
                        <th>Status</th>
                     </tr>
                  </thead>
                  <tfoot>
                     <tr>
                        <th>Reference No.</th>
                        <th>Filing Date</th>
                        <th>Intl Reg No.</th>
                        <th>Intl Reg Date</th>
                        <th>Status</th>
                     </tr>
                  </tfoot>
               </table>
               <table id="multiCerts">
                  <colgroup>
                     <col width="15%">
                     <col width="19%">
                     <col width="19%">
                     <col width="34%">
                     <col width="23%">
                  </colgroup>
                  <thead>
                     <tr>
                        <th><label for="certResultsTbody">Select All
                              </label></th>
                        <th>Number</th>
                        <th>Mail/Create Date</th>
                        <th>Description</th>
                        <th>Type</th>
                     </tr>
                  </thead>
                  <tfoot>
                     <tr>
                        <th>Select All</th>
                        <th>Number</th>
                        <th>Mail/Create Date</th>
                        <th>Description</th>
                        <th>Type</th>
                     </tr>
                  </tfoot>
               </table>
            </div>
         </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cephalopods use RNA editing to modify their nervous sytems (134 pts)]]></title>
            <link>https://pubmed.ncbi.nlm.nih.gov/28388405/</link>
            <guid>36855236</guid>
            <pubDate>Mon, 24 Jul 2023 22:38:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pubmed.ncbi.nlm.nih.gov/28388405/">https://pubmed.ncbi.nlm.nih.gov/28388405/</a>, See on <a href="https://news.ycombinator.com/item?id=36855236">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-page" data-article-pmid="28388405">
    
<!-- "Filters applied" shows only when page is redirected from search -->
<!-- because search found one result -->

    

    

    <main id="article-details">
  
  

  

  



  


<header id="heading">
  
    
      
      <div id="short-view-heading">
        

        

<h2>
  
    
    
    
    
      
  Trade-off between Transcriptome Plasticity and Genome Evolution in Cephalopods


    
  
</h2>

        

        <p><span>
    
      
        <span><span>Noa Liscovitch-Brauer</span><span>&nbsp;et al.</span></span>
      
    
  </span>
  
    
      <span>
        Cell<span>.</span>
      </span>
      
        <span>
          <time datetime="2017">2017</time><span>.</span>
        </span>
      
    
  
</p>

        
  <p><span>Free PMC article</span></p>
        
      </div>
    
  
</header>

  



  

  



  <div id="abstract">
    
      <h2>
        Abstract
        
      </h2>
      
        
          
            <p>
      
      RNA editing, a post-transcriptional process, allows the diversification of proteomes beyond the genomic blueprint; however it is infrequently used among animals for this purpose. Recent reports suggesting increased levels of RNA editing in squids thus raise the question of the nature and effects of these events. We here show that RNA editing is particularly common in behaviorally sophisticated coleoid cephalopods, with tens of thousands of evolutionarily conserved sites. Editing is enriched in the nervous system, affecting molecules pertinent for excitability and neuronal morphology. The genomic sequence flanking editing sites is highly conserved, suggesting that the process confers a selective advantage. Due to the large number of sites, the surrounding conservation greatly reduces the number of mutations and genomic polymorphisms in protein-coding regions. This trade-off between genome evolution and transcriptome plasticity highlights the importance of RNA recoding as a strategy for diversifying proteins, particularly those associated with neural function. PAPERCLIP.
    </p>
          
        
      

      
    

    

    
      


  

  
    <p>
      
        <strong>
          Keywords:
        </strong>
      
      ADAR; Epitranscriptome; RNA editing; RNA modifications; cephalopods; genome evolution; neural plasticity; proteome diversity.
    </p>
  


    

  </div>


  
  


  <p id="copyright">
    Copyright ¬© 2017 Elsevier Inc. All rights reserved.
  </p>


  
  <div id="conflict-of-interest">
    <h2>
      Conflict of interest statement
    </h2>

    <p xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:p1="http://pubmed.gov/pub-one">The authors declare no conflict of interest.</p>
  </div>


  
  
  
    
      <div id="figures">
        <h2>
          Figures
        </h2>

        <div id="slides-container" itemscope="" itemtype="http://schema.org/ImageGallery">
          
            <figure itemscope="" itemtype="http://schema.org/ImageObject" itemprop="associatedMedia" data-slide-index="0" data-label-slug="figure-1">
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f1.jpg" itemprop="contentUrl" aria-describedby="figure-caption-0" role="button" data-image-width="800" data-image-height="495" data-image-alt="Figure 1" data-pmc-id="PMC5499236" data-figure-id="F1">
                <img itemprop="thumbnail" id="article-image-0" src="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f1.gif" alt="Figure 1">
              </a>

              <meta itemprop="width" itemtype="http://schema.org/ImageObject" content="800">
              <meta itemprop="height" itemtype="http://schema.org/ImageObject" content="495">

              
                

                

                <figcaption id="figure-caption-0" itemtype="http://schema.org/ImageObject" itemprop="description">
                  
                  



  
    <strong>
      Figure 1. Extensive recoding is an invention of coleoid cephalopods
    </strong>
  

  
    <p><b>(A)</b> The species studied span the cephalopod evolutionary tree, as well as sea hare (<i>Aplysia californica</i>) as an outgroup (top). For comparison, a representative tree for vertebrates is shown (bottom), constructed based on divergence times estimated in (Hedges et al., 2006). <b>(B)</b> Tens-thousands of A-to-I editing sites (identified as A-to-G DNA-RNA mismatches) are detected in squid, sepia and the two octopus species (see Tables S1‚ÄìS4 for more details). The noise level (estimated by the number of G-to-A mismatches) is rather low. In contrast, in nautilus and sea hare no enrichment of A-to-G mismatches is observed (inset). <b>(C)</b> The nucleotides neighboring the detected editing sites, show a clear pattern consistent with known ADAR preference (Alon et al., 2015; Eggington et al., 2011; Kleinberger and Eisenberg, 2010) for the extensively recoded coleoid species ‚Äì squid, sepia, and the two octopus species ‚Äì but not in nautilus or sea hare. The motif is characterized by under-representation of G upstream to the editing site (relative location ‚àí1) and over-representation of G in the downstream base (The height of the entire stack of letters represents the information content in bits, the relative height of each letter represents its frequency).</p>
  



                </figcaption>
              

            </figure>
          
            <figure itemscope="" itemtype="http://schema.org/ImageObject" itemprop="associatedMedia" data-slide-index="1" data-label-slug="figure-2">
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f2.jpg" itemprop="contentUrl" aria-describedby="figure-caption-1" role="button" data-image-width="800" data-image-height="312" data-image-alt="Figure 2" data-pmc-id="PMC5499236" data-figure-id="F2">
                <img itemprop="thumbnail" id="article-image-1" src="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f2.gif" alt="Figure 2">
              </a>

              <meta itemprop="width" itemtype="http://schema.org/ImageObject" content="800">
              <meta itemprop="height" itemtype="http://schema.org/ImageObject" content="312">

              
                

                

                <figcaption id="figure-caption-1" itemtype="http://schema.org/ImageObject" itemprop="description">
                  
                  



  
    <strong>
      Figure 2. Proteomic validation of recoding by RNA editing
    </strong>
  

  
    <p>We analyzed peptides identified by mass spectrometry analysis of two squid tissues, looking for evidence of recoding. For each site covered by one or more peptides, we marked whether the edited, non-edited or both versions of the peptide are observed. The distribution is presented, binned by the predicted RNA editing level (as measured from RNA-seq data). In parentheses are the numbers of recoding sites analyzed in each editing-level bin. The proteomic recoding level follows closely the predicted RNA editing level. Altogether, this experiment validated protein recoding in 432 sites in two tissues: <b>(A)</b> Squid stellate ganglion, where 320 of the 3,204 single-site peptides (10.0%) were shown to be edited. <b>(B)</b> Squid giant axon (giant fiber lobe), where 283 of the 2,741 single-site peptides (10.3%) were shown to be edited.</p>
  



                </figcaption>
              

            </figure>
          
            <figure itemscope="" itemtype="http://schema.org/ImageObject" itemprop="associatedMedia" data-slide-index="2" data-label-slug="figure-3">
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f3.jpg" itemprop="contentUrl" aria-describedby="figure-caption-2" role="button" data-image-width="800" data-image-height="930" data-image-alt="Figure 3" data-pmc-id="PMC5499236" data-figure-id="F3">
                <img itemprop="thumbnail" id="article-image-2" src="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f3.gif" alt="Figure 3">
              </a>

              <meta itemprop="width" itemtype="http://schema.org/ImageObject" content="800">
              <meta itemprop="height" itemtype="http://schema.org/ImageObject" content="930">

              
                

                

                <figcaption id="figure-caption-2" itemtype="http://schema.org/ImageObject" itemprop="description">
                  
                  



  
    <strong>
      Figure 3. Editing in Octopus bimaculoides
    </strong>
  

  
    <p><b>(A)</b> A-to-I editing sites were found within coding sequences of <i>Octopus bimaculoides</i> using three methods: the genome-free method (alignment to de-novo transcriptome), the genome-dependent approach using REDItools (Picardi and Pesole, 2013), and identification of hyper-edited reads (Porath et al., 2014). Overall, the three methods identified 170,825 unique AG sites in <i>Octopus bimaculoides</i> coding sequences (38,066 hyper-editing sites do not overlap those found by the other methods). See <b>Methods</b> for analysis of the differences between the results of the first two methods. <b>(B)</b> RNA editing levels, measured across the whole transcriptome (see Table S5) by the editing index (weighted average of editing levels over all editing sites identified in the transcriptome, see <b>Methods</b>). Levels vary across tissues and are highest for neural tissues (see Table S6). Unlike mammals, a sizable fraction of editing events (11‚Äì13% in neural tissues) results in recoding events. Annotation of transcripts and repeats is based on (Albertin et al., 2015). (CNS= central nervous system; ANC=Axial nerve cord; OL=Optic Lobe; Sub=Subesophageal ganglia; Supra=Supraesophageal ganglia; PSG=posterior salivary gland; ST15=stage 15 embryo) <b>(C)</b> The number of editing sites in coding region is comparable to the number found in introns. <b>(D</b>) Unlike the case in mammals, editing is not exceptionally enriched in specific repeat families in <i>Octopus bimaculoides</i>, as measured by the editing index (here defined as the editing level averaged over all, edited and unedited, adenosines in each specific repeat family). <b>(E)</b> Protocadherins is a gene family known to be principally expressed in the brain, important for mediating combinatorial complexity in neuronal connections and are thought to play a role in diversifying neural circuitry (Chen and Maniatis, 2013). It was impressively expanded in <i>Octopus bimaculoides</i> (Albertin et al., 2015). A large number of protocadherins are found in the assembled transcriptomes for the four coleoid species (127‚Äì251 open reading frames), but not in nautilus (28 open reading frames). <b>(F‚ÄìG)</b> Protocadherins contain significantly higher numbers of AG sites <b>(F)</b> and are edited at higher levels (editing level summed over all sites and normalized by ORF length), in all four coleoid species but not in nautilus <b>(G)</b>.</p>
  



                </figcaption>
              

            </figure>
          
            <figure itemscope="" itemtype="http://schema.org/ImageObject" itemprop="associatedMedia" data-slide-index="3" data-label-slug="figure-4">
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f4.jpg" itemprop="contentUrl" aria-describedby="figure-caption-3" role="button" data-image-width="800" data-image-height="815" data-image-alt="Figure 4" data-pmc-id="PMC5499236" data-figure-id="F4">
                <img itemprop="thumbnail" id="article-image-3" src="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f4.gif" alt="Figure 4">
              </a>

              <meta itemprop="width" itemtype="http://schema.org/ImageObject" content="800">
              <meta itemprop="height" itemtype="http://schema.org/ImageObject" content="815">

              
                

                

                <figcaption id="figure-caption-3" itemtype="http://schema.org/ImageObject" itemprop="description">
                  
                  



  
    <strong>
      Figure 4. Extensive recoding is conserved across coleoid cephalopods
    </strong>
  

  
    <p><b>(A)</b> Tens-thousands sites are conserved across species (see Table S7). The closer the species are evolutionarily, the higher the number of conserved sites. <b>(B)</b> Virtually all (97.5‚Äì99%) mismatches conserved across species are A-to-G, resulting from A-to-I editing. Manual inspection of the few non-A-to-G mismatches appearing in multiple species suggests that they either result from systematic erroneous alignments, or they are actually editing sites that were mistakenly identified as G-to-A mismatches due to insufficient DNA coverage. <b>(C)</b> The majority of editing sites is conserved between the two octopus species, and even the most distant species share a sizable fraction of their sites. <b>(D)</b> In contrast, only 36 human recoding sites (1‚Äì2% of human recoding sites) are shared by mouse, and a similar number is shared between <i>Drosophila melanogaster</i> and <i>D. mojavensis</i> (Yu et al., 2016) (diverged at later times than squid-sepia). <b>(E)</b> Interestingly, 1146 AG modification sites (in 443 proteins) are conserved and shared by all four coleoid cephalopod species. Of these, 887 are recoding sites and 705 are highly edited (&gt;=10% editing) recoding sites (in 393 proteins). <b>(F)</b> Some proteins include multiple highly-edited recoding sites (see Table S8). Of note are Uromodulin, Œ± Spectrin (previously reported to harbor the highest number of recoding sites in squid (Alon et al., 2015)), and Calcium-dependent secretion activator 1 (CAPS1) with 14, 8 and 7 strong shared recoding sites, respectively. Recoding in CAPS1 was found to be conserved in vertebrate species from human to zebrafish (Li et al., 2009). <b>(G)</b> Not only are the locations of editing sites conserved, but their editing levels are correlated as well. Editing levels in 887 recoding sites shared by all species are highly, positively and significantly correlated in all pairs of coleoid cephalopod species (p&lt;1e‚Äì75 for all pairs; see Supp. Fig. 1 for three additional pairs). Correlation is higher the closer the species are to each other in evolutionary terms, with Pearson rho = 0.95 for the two octopus species.</p>
  



                </figcaption>
              

            </figure>
          
            <figure itemscope="" itemtype="http://schema.org/ImageObject" itemprop="associatedMedia" data-slide-index="4" data-label-slug="figure-5">
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f5.jpg" itemprop="contentUrl" aria-describedby="figure-caption-4" role="button" data-image-width="800" data-image-height="779" data-image-alt="Figure 5" data-pmc-id="PMC5499236" data-figure-id="F5">
                <img itemprop="thumbnail" id="article-image-4" src="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f5.gif" alt="Figure 5">
              </a>

              <meta itemprop="width" itemtype="http://schema.org/ImageObject" content="800">
              <meta itemprop="height" itemtype="http://schema.org/ImageObject" content="779">

              
                

                

                <figcaption id="figure-caption-4" itemtype="http://schema.org/ImageObject" itemprop="description">
                  
                  



  
    <strong>
      Figure 5. Signs for positive selection of recoding by editing
    </strong>
  

  
    <p><b>(A)</b> The fraction of recoding sites among all editing sites in coding region increases with editing levels (top), as well as the fraction of recoding sites among all conserved sites (bottom). Red horizontal dashed line represents the recoding fraction expected assuming neutrality. <b>(B)</b> Editing levels are higher in conserved recoding sites. Distributions of editing levels in four groups of putative A-to-I editing sites: recoding and conserved (Rec+, Cons+), recoding and non-conserved (Rec+, Cons‚àí), conserved sites that cause a synonymous change (Rec‚àí, Cons+), and non-conserved synonymous sites (Rec‚àí, Cons‚àí). Horizontal red lines mark the median level, and yellow diamonds mark the mean. Conservation and non-synonymity are both positively correlated with higher editing levels, as well as their interaction (ANOVA, p-value&lt;1.0e‚Äì162). Data presented here for squid (conserved sites are conserved in sepia), but the results are similar and significant for all species. <b>(C)</b> In contrast with the case in humans, highly edited sites tend to be more conserved: the fraction of conserved sites rises with the editing level for all species pairs, but more dramatically for the closely related octopuses and the sepia-squid pair. <b>(D)</b> Highly conserved regions of the transcriptome are enriched in editing sites, further attesting for positive selection of RNA editing. Density of editing sites (number of AG sites normalized by length) is higher for 112 recoding regions that are highly-conserved across the four species (&gt;95% identity; average length 1382bp), compared with all other, less conserved, regions (Wilcoxon p-value&lt;0.001 for all species). Error bars represent the S.E.M.</p>
  



                </figcaption>
              

            </figure>
          
            <figure itemscope="" itemtype="http://schema.org/ImageObject" itemprop="associatedMedia" data-slide-index="5" data-label-slug="figure-6">
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f6.jpg" itemprop="contentUrl" aria-describedby="figure-caption-5" role="button" data-image-width="800" data-image-height="979" data-image-alt="Figure 6" data-pmc-id="PMC5499236" data-figure-id="F6">
                <img itemprop="thumbnail" id="article-image-5" src="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f6.gif" alt="Figure 6">
              </a>

              <meta itemprop="width" itemtype="http://schema.org/ImageObject" content="800">
              <meta itemprop="height" itemtype="http://schema.org/ImageObject" content="979">

              
                

                

                <figcaption id="figure-caption-5" itemtype="http://schema.org/ImageObject" itemprop="description">
                  
                  



  
    <strong>
      Figure 6. Conserved and species-specific editing sites affect protein function
    </strong>
  

  
    <p>Unedited (wt) and singly-edited versions of the voltage-dependent K<sup>+</sup> channels of the K<sub>v</sub>2 subfamily were studied under voltage-clamp (see Table S9). <b>(A)</b> (i) Current traces resulting from a voltage step from ‚àí80 mV to 40 mV for the wt Sepia K<sub>v</sub>2.1 and the same construct containing the sepia-specific I529V edit, lying within the 4<sup>th</sup> transmembrane domain (green), showing that I529V accelerates the rate of slow inactivation. (ii) Time constants for slow inactivation determined by fitting single exponentials to traces similar to those in panel (i) at different activating voltages (Vm). <b>(B)</b> (i) Tail currents measured at a voltage (Vm) of ‚àí80mV, following an activating pulse of +20 mV for 25 ms. Traces are shown for the wt K<sub>v</sub>2.1 channels from squid, sepia and Octopus vulgaris. (ii) Tail currents for the same channels edited at the shared I-to-V site in the 6<sup>th</sup> transmembrane span, following the same voltage protocol. (iii) Time constants from single exponential fits to tail currents obtained at various negative voltages (Vm) (following an activating pulse to 20 mV for 25 ms) show that the unedited channels close at distinct rates, (iv) but the edited versions close at similar rates. N = 5 ¬± s.e.m. for all data plotted in this figure.</p>
  



                </figcaption>
              

            </figure>
          
            <figure itemscope="" itemtype="http://schema.org/ImageObject" itemprop="associatedMedia" data-slide-index="6" data-label-slug="figure-7">
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f7.jpg" itemprop="contentUrl" aria-describedby="figure-caption-6" role="button" data-image-width="800" data-image-height="735" data-image-alt="Figure 7" data-pmc-id="PMC5499236" data-figure-id="F7">
                <img itemprop="thumbnail" id="article-image-6" src="https://www.ncbi.nlm.nih.gov/pmc/articles/instance/5499236/bin/nihms863911f7.gif" alt="Figure 7">
              </a>

              <meta itemprop="width" itemtype="http://schema.org/ImageObject" content="800">
              <meta itemprop="height" itemtype="http://schema.org/ImageObject" content="735">

              
                

                

                <figcaption id="figure-caption-6" itemtype="http://schema.org/ImageObject" itemprop="description">
                  
                  



  
    <strong>
      Figure 7. RNA editing slows down cephalopod genome evolution
    </strong>
  

  
    <p><b>(A)</b> Inter-species mutations are purified from genome loci surrounding conserved recoding sites (data shown for sites shared by squid and sepia). Depletion of mutations extends up to ~100bp of shared recoding sites (left). As a control, we show the mutations density (mutations/bp) around random non-edited adenosines from the same transcripts (right). Yellow ‚Äì synonymous change; light green ‚Äì non-synonymous; dark green ‚Äì deletions. <b>(B)</b> Genomic polymorphisms are depleted near editing/recoding/conserved-recoding sites in squid, attesting to reduced genome plasticity. Effect is stronger for recoding sites, and even more so for the conserved recoding sites. <b>(C)</b> GC-content is elevated near editing sites in squid, allowing for more stable double-stranded RNA structures. The effect is even stronger in conserved sites. Dashed line represents the baseline GC level in the entire ORFome, and error bars represent the S.E.M. See Supp. Fig. 3 for analyses similar to those presented in panels A‚ÄìC in other species.</p>
  



                </figcaption>
              

            </figure>
          
        </div>

        
          
        

        <!-- Root element of PhotoSwipe. Must have class pswp. -->




      </div>
    
  


  
  
    
      <div id="linked-commentary">
        <h2>
          Comment in
        </h2>
        <ul>
          
            
              
              
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/28420879/" ref="article_id=28420879&amp;linksrc=comments_link" data-ga-category="comment_correction" data-ga-action="28420879" data-ga-label="linked-commentary">
      
        Evolutionary genetics: Fantastic beasts - cephalopod RNA recoding.
      
    </a></p><p><span>Starling S.</span>
        
      
    
    <span>Starling S.</span>
    <span>Nat Rev Genet. 2017 Jun;18(6):329. doi: 10.1038/nrg.2017.31. Epub 2017 Apr 19.</span>
    <span>Nat Rev Genet. 2017.</span>
  
  <span>PMID: <span>28420879</span></span>
  
  
  
  
  <span>No abstract available.</span>
</p>

  </div>
  
    </li>
  


            
          
            
              
              
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/28716332/" ref="article_id=28716332&amp;linksrc=comments_link" data-ga-category="comment_correction" data-ga-action="28716332" data-ga-label="linked-commentary">
      
        Shaping and Reshaping Transcriptome Plasticity during Evolution.
      
    </a></p><p><span>Hussain S.</span>
        
      
    
    <span>Hussain S.</span>
    <span>Trends Biochem Sci. 2017 Sep;42(9):682-684. doi: 10.1016/j.tibs.2017.06.009. Epub 2017 Jul 14.</span>
    <span>Trends Biochem Sci. 2017.</span>
  
  <span>PMID: <span>28716332</span></span>
  
  
  
  
  
</p>

  </div>
  
    </li>
  


            
          
        </ul>
      </div>
    
  


  
  
    <div id="similar">
      <h2>
        Similar articles
      </h2>
      
        <ul id="similar-articles-list">
          
  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/34022057/" ref="article_id=34022057&amp;linksrc=similar_articles_link&amp;ordinalpos=1" data-ga-category="similar_article" data-ga-action="34022057" data-ga-label="">
      
        Adaptive Proteome Diversification by Nonsynonymous A-to-I RNA Editing in Coleoid Cephalopods.
      
    </a></p><p><span>Shoshan Y, Liscovitch-Brauer N, Rosenthal JJC, Eisenberg E.</span>
        
      
    
    <span>Shoshan Y, et al.</span>
    <span>Mol Biol Evol. 2021 Aug 23;38(9):3775-3788. doi: 10.1093/molbev/msab154.</span>
    <span>Mol Biol Evol. 2021.</span>
  
  <span>PMID: <span>34022057</span></span>
  <span>Free PMC article.</span>
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/36790891/" ref="article_id=36790891&amp;linksrc=similar_articles_link&amp;ordinalpos=2" data-ga-category="similar_article" data-ga-action="36790891" data-ga-label="">
      
        Extensive Recoding of the Neural Proteome in Cephalopods by RNA Editing.
      
    </a></p><p><span>Rosenthal JJC, Eisenberg E.</span>
        
      
    
    <span>Rosenthal JJC, et al.</span>
    <span>Annu Rev Anim Biosci. 2023 Feb 15;11:57-75. doi: 10.1146/annurev-animal-060322-114534.</span>
    <span>Annu Rev Anim Biosci. 2023.</span>
  
  <span>PMID: <span>36790891</span></span>
  
  
  <span>Review.</span>
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/37295402/" ref="article_id=37295402&amp;linksrc=similar_articles_link&amp;ordinalpos=3" data-ga-category="similar_article" data-ga-action="37295402" data-ga-label="">
      
        Temperature-dependent RNA editing in octopus extensively recodes the neural proteome.
      
    </a></p><p><span>Birk MA, Liscovitch-Brauer N, Dominguez MJ, McNeme S, Yue Y, Hoff JD, Twersky I, Verhey KJ, Sutton RB, Eisenberg E, Rosenthal JJC.</span>
        
      
    
    <span>Birk MA, et al.</span>
    <span>Cell. 2023 Jun 8;186(12):2544-2555.e13. doi: 10.1016/j.cell.2023.05.004. Epub 2023 Jun 8.</span>
    <span>Cell. 2023.</span>
  
  <span>PMID: <span>37295402</span></span>
  
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/33265998/" ref="article_id=33265998&amp;linksrc=similar_articles_link&amp;ordinalpos=4" data-ga-category="similar_article" data-ga-action="33265998" data-ga-label="">
      
        The ADAR Family in Amphioxus: RNA Editing and Conserved Orthologous Site Predictions.
      
    </a></p><p><span>Zawisza-√Ålvarez M, P√©rez-Calles C, Gattoni G, Garcia-Fern√†ndez J, Benito-Guti√©rrez √à, Herrera-√öbeda C.</span>
        
      
    
    <span>Zawisza-√Ålvarez M, et al.</span>
    <span>Genes (Basel). 2020 Nov 30;11(12):1440. doi: 10.3390/genes11121440.</span>
    <span>Genes (Basel). 2020.</span>
  
  <span>PMID: <span>33265998</span></span>
  <span>Free PMC article.</span>
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/32729084/" ref="article_id=32729084&amp;linksrc=similar_articles_link&amp;ordinalpos=5" data-ga-category="similar_article" data-ga-action="32729084" data-ga-label="">
      
        Proteome Diversification by RNA Editing.
      
    </a></p><p><span>Eisenberg E.</span>
        
      
    
    <span>Eisenberg E.</span>
    <span>Methods Mol Biol. 2021;2181:229-251. doi: 10.1007/978-1-0716-0787-9_14.</span>
    <span>Methods Mol Biol. 2021.</span>
  
  <span>PMID: <span>32729084</span></span>
  
  
  <span>Review.</span>
  
  
</p>

  </div>
  
    </li>
  


    
  


        </ul>

        
          

        
      
    </div>
  


  


  
    <div id="citedby">
      <h2>
        Cited by
      </h2>
      
        <ul id="citedby-articles-list">
          
  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/37293227/" ref="article_id=37293227&amp;linksrc=citedby_articles_link&amp;ordinalpos=1" data-ga-category="cited_by" data-ga-action="37293227" data-ga-label="">
      
        Adaptation of A-to-I RNA editing in bacteria, fungi, and animals.
      
    </a></p><p><span>Duan Y, Li H, Cai W.</span>
        
      
    
    <span>Duan Y, et al.</span>
    <span>Front Microbiol. 2023 May 24;14:1204080. doi: 10.3389/fmicb.2023.1204080. eCollection 2023.</span>
    <span>Front Microbiol. 2023.</span>
  
  <span>PMID: <span>37293227</span></span>
  <span>Free PMC article.</span>
  
  
  
  <span>No abstract available.</span>
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/37266373/" ref="article_id=37266373&amp;linksrc=citedby_articles_link&amp;ordinalpos=2" data-ga-category="cited_by" data-ga-action="37266373" data-ga-label="">
      
        Transcriptome-wide selection and validation of a solid set of reference genes for gene expression studies in the cephalopod mollusk <em>Octopus vulgaris</em>.
      
    </a></p><p><span>Imperadore P, Cagnin S, Allegretti V, Millino C, Raffini F, Fiorito G, Ponte G.</span>
        
      
    
    <span>Imperadore P, et al.</span>
    <span>Front Mol Neurosci. 2023 May 17;16:1091305. doi: 10.3389/fnmol.2023.1091305. eCollection 2023.</span>
    <span>Front Mol Neurosci. 2023.</span>
  
  <span>PMID: <span>37266373</span></span>
  <span>Free PMC article.</span>
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/37199468/" ref="article_id=37199468&amp;linksrc=citedby_articles_link&amp;ordinalpos=3" data-ga-category="cited_by" data-ga-action="37199468" data-ga-label="">
      
        The continuing discovery on the evidence for RNA editing in SARS-CoV-2.
      
    </a></p><p><span>Pu X, Xu Q, Wang J, Liu B.</span>
        
      
    
    <span>Pu X, et al.</span>
    <span>RNA Biol. 2023 Jan;20(1):219-222. doi: 10.1080/15476286.2023.2214437.</span>
    <span>RNA Biol. 2023.</span>
  
  <span>PMID: <span>37199468</span></span>
  <span>Free PMC article.</span>
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/36877730/" ref="article_id=36877730&amp;linksrc=citedby_articles_link&amp;ordinalpos=4" data-ga-category="cited_by" data-ga-action="36877730" data-ga-label="">
      
        Identification of exceptionally potent adenosine deaminases RNA editors from high body temperature organisms.
      
    </a></p><p><span>Avram-Shperling A, Kopel E, Twersky I, Gabay O, Ben-David A, Karako-Lampert S, Rosenthal JJC, Levanon EY, Eisenberg E, Ben-Aroya S.</span>
        
      
    
    <span>Avram-Shperling A, et al.</span>
    <span>PLoS Genet. 2023 Mar 6;19(3):e1010661. doi: 10.1371/journal.pgen.1010661. eCollection 2023 Mar.</span>
    <span>PLoS Genet. 2023.</span>
  
  <span>PMID: <span>36877730</span></span>
  <span>Free PMC article.</span>
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/36799984/" ref="article_id=36799984&amp;linksrc=citedby_articles_link&amp;ordinalpos=5" data-ga-category="cited_by" data-ga-action="36799984" data-ga-label="">
      
        Evidence Supporting That C-to-U RNA Editing Is the Major Force That Drives SARS-CoV-2 Evolution.
      
    </a></p><p><span>Wang J, Wu L, Pu X, Liu B, Cao M.</span>
        
      
    
    <span>Wang J, et al.</span>
    <span>J Mol Evol. 2023 Apr;91(2):214-224. doi: 10.1007/s00239-023-10097-1. Epub 2023 Feb 17.</span>
    <span>J Mol Evol. 2023.</span>
  
  <span>PMID: <span>36799984</span></span>
  <span>Free PMC article.</span>
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  


        </ul>

        
          

        
      
    </div>
  


  
  
  
    <div id="publication-types">
      <h2>
        Publication types
      </h2>

      <ul><li></li><li></li><li></li></ul>
    </div>
  


  
  
    <div id="mesh-terms">
      <h2>
        MeSH terms
      </h2>

      <ul><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li></ul>
    </div>
  


  
  
    <div id="substances">
      <h2>
        Substances
      </h2>

      <ul><li></li><li></li></ul>
    </div>
  


  

  

  



  
  

  


  
    <div id="grants">
      <h2>
        Grant support
      </h2>
      <div><ul id="grants-head-list"><li><a title="All articles for grant 311257/ERC_/European Research Council/International" href="https://pubmed.ncbi.nlm.nih.gov/?term=311257%2FERC_%2FEuropean+Research+Council%2FInternational%5BGrant+Number%5D&amp;sort=date&amp;sort_order=desc" ref="linksrc=grant_link" data-ga-category="search" data-ga-action="grant" data-ga-label="311257">
          311257/ERC_/European Research Council/International
        </a></li><li><a title="All articles for grant R01 NS064259/NS/NINDS NIH HHS/United States" href="https://pubmed.ncbi.nlm.nih.gov/?term=R01+NS064259%2FNS%2FNINDS+NIH+HHS%2FUnited+States%5BGrant+Number%5D&amp;sort=date&amp;sort_order=desc" ref="linksrc=grant_link" data-ga-category="search" data-ga-action="grant" data-ga-label="R01 NS064259">
          R01 NS064259/NS/NINDS NIH HHS/United States
        </a></li><li><a title="All articles for grant R01 NS087726/NS/NINDS NIH HHS/United States" href="https://pubmed.ncbi.nlm.nih.gov/?term=R01+NS087726%2FNS%2FNINDS+NIH+HHS%2FUnited+States%5BGrant+Number%5D&amp;sort=date&amp;sort_order=desc" ref="linksrc=grant_link" data-ga-category="search" data-ga-action="grant" data-ga-label="R01 NS087726">
          R01 NS087726/NS/NINDS NIH HHS/United States
        </a></li></ul></div>
    </div>
  


  
  <div id="linkout">
    <h2>
      LinkOut - more resources
    </h2>

    <ul><li><h3>Full Text Sources</h3><ul><li><a href="https://linkinghub.elsevier.com/retrieve/pii/S0092-8674(17)30344-6" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=3048&amp;uid=28388405&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=0413066" data-ga-category="link_out" data-ga-action="Full Text Sources" data-ga-label="Elsevier Science">
                    Elsevier Science
                  </a></li><li><a href="https://europepmc.org/abstract/MED/28388405" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=6082&amp;uid=28388405&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=0413066" data-ga-category="link_out" data-ga-action="Full Text Sources" data-ga-label="Europe PubMed Central">
                    Europe PubMed Central
                  </a></li><li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/28388405/" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=3494&amp;uid=28388405&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=0413066" data-ga-category="link_out" data-ga-action="Full Text Sources" data-ga-label="PubMed Central">
                    PubMed Central
                  </a></li></ul></li><li><h3>Other Literature Sources</h3><ul><li><a href="https://facultyopinions.com/pubmed/28388405" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=3533&amp;uid=28388405&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=0413066" data-ga-category="link_out" data-ga-action="Other Literature Sources" data-ga-label="H1 Connect">
                    H1 Connect
                  </a></li><li><a href="https://www.lens.org/lens/search/patent/list?q=citation_id:28388405" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=8681&amp;uid=28388405&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=0413066" data-ga-category="link_out" data-ga-action="Other Literature Sources" data-ga-label="The Lens - Patent Citations">
                    The Lens - Patent Citations
                  </a></li><li><a href="https://scite.ai/reports/28388405" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=10307&amp;uid=28388405&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=0413066" data-ga-category="link_out" data-ga-action="Other Literature Sources" data-ga-label="scite Smart Citations">
                    scite Smart Citations
                  </a></li></ul></li></ul>
  </div>


</main>

    
  


    

    

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google‚Äôs nightmare ‚ÄúWeb Integrity API‚Äù wants a DRM gatekeeper for the web (931 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2023/07/googles-web-integrity-api-sounds-like-drm-for-the-web/</link>
            <guid>36854114</guid>
            <pubDate>Mon, 24 Jul 2023 20:59:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2023/07/googles-web-integrity-api-sounds-like-drm-for-the-web/">https://arstechnica.com/gadgets/2023/07/googles-web-integrity-api-sounds-like-drm-for-the-web/</a>, See on <a href="https://news.ycombinator.com/item?id=36854114">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2020/07/android-spying-800x450.jpg" alt="A man laughs at his smartphone while a cartoon characters peaks over his shoulder.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2020/07/android-spying.jpg" data-height="563" data-width="1000">Enlarge</a> <span>/</span> The little Android robot is watching everything you do.</p></figcaption>  </figure>

  




<!-- cache hit 800:single/related:615bdf60c76de8a9103dade2c9b14285 --><!-- empty -->
<p>Google's newest proposed web standard is... DRM? Over the weekend the Internet got wind of <a href="https://github.com/RupertBenWiser/Web-Environment-Integrity/tree/main">this proposal</a> for a "Web Environment Integrity API. " The <a href="https://github.com/RupertBenWiser/Web-Environment-Integrity/blob/main/explainer.md">explainer</a> is authored by four Googlers, including at least one person on Chrome's "<a href="https://arstechnica.com/gadgets/2023/02/googles-privacy-sandbox-advertising-system-arrives-on-android-in-beta/">Privacy Sandbox</a>" team, which is responding to the <a href="https://arstechnica.com/gadgets/2022/07/google-delays-death-of-tracking-cookies-again-wants-more-time-for-testing/">death of tracking cookies</a> by building a user-tracking ad platform right into the browser.</p>
<p>The intro to the Web Integrity API starts out: "Users often depend on websites trusting the client environment they run in. This trust may assume that the client environment is honest about certain aspects of itself, keeps user data and intellectual property secure, and is transparent about whether or not a human is using it."</p>
<p>The goal of the project is to learn more about the person on the other side of the web browser, ensuring they aren't a robot and that the browser hasn't been modified or tampered with in any unapproved ways. The intro says this data would be useful to advertisers to better count ad impressions, stop social network bots, enforce intellectual property rights, stop cheating in web games, and help financial transactions be more secure.</p>                                            
                                                        
<p>Perhaps the most telling line of the explainer is that it "takes inspiration from existing native attestation signals such as [Apple's] <a href="https://developer.apple.com/documentation/devicecheck/validating_apps_that_connect_to_your_server">App Attest</a> and the [Android] <a href="https://developer.android.com/google/play/integrity">Play Integrity API</a>." Play Integrity (formerly called "SafetyNet") is an Android API that lets apps find out if your device has been rooted. Root access allows you full control over the device that you purchased, and a lot of app developers don't like that. So if you root an Android phone and get flagged by the Android Integrity API, several types of apps will just refuse to run. You'll generally be locked out of banking apps, Google Wallet, online games, Snapchat, and some media apps like Netflix. You <em>could</em> be using root access to cheat at games or phish banking data, but you could also just want root to customize your device, remove crapware, or have a viable backup system. Play Integrity doesn't care and will lock you out of those apps either way. Google wants the same thing for the web.</p>
<p>Google's plan is that, during a webpage transaction, the web server could require you to pass an "environment attestation" test before you get any data. At this point your browser would contact a "third-party" attestation server, and you would need to pass some kind of test. If you passed, you would get a signed "IntegrityToken" that verifies your environment is unmodified and points to the content you wanted unlocked. You bring this back to the web server, and if the server trusts the attestation company, you get the content unlocked and finally get a response with the data you wanted.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[People in 1920s Berlin nightclubs flirted via pneumatic tubes (289 pts)]]></title>
            <link>https://www.atlasobscura.com/articles/pneumatic-tube-table-phone-flirting-berlin</link>
            <guid>36854061</guid>
            <pubDate>Mon, 24 Jul 2023 20:54:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.atlasobscura.com/articles/pneumatic-tube-table-phone-flirting-berlin">https://www.atlasobscura.com/articles/pneumatic-tube-table-phone-flirting-berlin</a>, See on <a href="https://news.ycombinator.com/item?id=36854061">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="article-body">
<p><span>You hear it often: dating </span>today doesn‚Äôt work like it used to. Or: apps like Tinder have made flirting more distant.</p>
<p>But the process of staring, judging, and messaging potential suitors from afar‚Äîhallmarks of modern dating apps‚Äîis not new. Beginning in the 1920s, nightclub-goers in Berlin who feared face-to-face encounters could communicate with beautiful strangers from across the room.</p>
<p>All they needed to do? Turn to the nearest pneumatic tube.</p>
<p>Two nightclubs in particular‚Äîthe Resi and the Femina‚Äîpioneered the trend. At the Resi (also called the Residenz-Casino), a large nightclub with a live band and a dance floor that held 1,000 people, an elaborate system of table phones and pneumatic tubes allowed for anonymous, late-night flirtation between complete strangers.</p>
<figure><img src="https://img.atlasobscura.com/LpS7oAs99gAjNXH7kMcvkohGLmNUem_T-asmE7IkKzU/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy80ZjE4ZDQzMGE4/NzAwZmI0M2RfUmVz/aV8xLnBuZw.png" alt="Resi nightclub tables awaiting enigmatic and flirtatious strangers." width="auto" data-kind="article-image" id="article-image-42702" data-src="https://img.atlasobscura.com/LpS7oAs99gAjNXH7kMcvkohGLmNUem_T-asmE7IkKzU/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy80ZjE4ZDQzMGE4/NzAwZmI0M2RfUmVz/aV8xLnBuZw.png"><figcaption>Resi nightclub tables awaiting enigmatic and flirtatious strangers. <a target="_blank" href="http://liquidfireworks.com/about-us/">Courtesy of Michael Przystawik</a></figcaption></figure>
<p>A <em><a href="http://archives.chicagotribune.com/1968/10/30/page/48/article/the-tribune-travelers-guide">Chicago Tribune</a></em> article describes the Resi‚Äôs ‚Äúnightly ‚Äòspectacular‚Äô‚Äî‚Äòa dancing water ballet‚Äô with jets of water rising and falling to a recorded symphony while colored lights flash.‚Äù The water-jet ballet, now known as a ‚Äú<a href="http://liquidfireworks.com/video-gallery/">Waltzing Water</a>,‚Äù began in 1928 and drew in many visitors.</p>
<p>But the <em>Tribune</em> article refers to the system of phones and pneumatic tubes at each table as the Resi‚Äôs ‚Äúbig lure.‚Äù</p>
<p>Phones were fixed to individual tables, and above many was a lighted number. Singles needed only to look around the room until a fetching stranger caught their eye, note the number, and then direct a message to that table. ‚ÄúLonesome Americans, and others, can call or send a note to equally lonesome women who look like they would enjoy company,‚Äù the article noted.</p>
<p>In 1931, during the heyday of this across-the-nightclub flirtation, <em><a href="https://books.google.com/books?id=nlpBDAAAQBAJ&amp;pg=PA164&amp;lpg=PA164&amp;dq=The+Residenz-Casino&amp;source=bl&amp;ots=v3JYahpG1R&amp;sig=BeO7Wv1zKw2FmCrhC0hUYqbZ5ZA&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiUor2wh5vUAhUo9YMKHVpMA38Q6AEIWTAJ#v=onepage&amp;q=The%20Residenz-Casino&amp;f=false">The Berliner Herold</a></em> described the process of receiving a call from an amorous stranger: ‚Äúthe tabletop telephones buzzed, and the acquaintance with the blonde, raven-haired or redheaded, monocle-wearing beauty was made, one was no longer alone, and had twice as much fun.‚Äù (At the Ballhaus Berlin, this numbered phone system still lives today‚Äîcheck out photos <a href="http://www.ballhaus-berlin.de/first-gallery/">here</a>.)</p>
<figure><img src="https://img.atlasobscura.com/vsmYxzi0VnQuRhaU1hq-j6i0wls1sSIsuRyRLenFV3k/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy9lMTFhZTA1ODkz/N2M0NDZkODVfUmVz/aS0yLmpwZWc.jpg" alt="The Resi floor plan." width="auto" data-kind="article-image" id="article-image-42689" data-src="https://img.atlasobscura.com/vsmYxzi0VnQuRhaU1hq-j6i0wls1sSIsuRyRLenFV3k/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy9lMTFhZTA1ODkz/N2M0NDZkODVfUmVz/aS0yLmpwZWc.jpg"><figcaption>The Resi floor plan. <a target="_blank" href="http://www.cabaret-berlin.com/?p=491">Courtesy of Brendan Nash </a></figcaption></figure>
<p>Similar systems thrived at the Femina, the larger of the two nightclubs, which <a href="https://books.google.com/books?id=nlpBDAAAQBAJ&amp;q=two+large+bars+and+a+smaller+one+in+the+vestibule#v=snippet&amp;q=two%20large%20bars%20and%20a%20smaller%20one%20in%20the%20vestibule&amp;f=false">boasted</a> more than 2,000 seats, ‚Äútwo large bars and a smaller one in the vestibule, in addition to three orchestras, a hydraulic dance floor,‚Äù and over 225 table telephones, which were accompanied by instructions in both German and English.</p>
<p>But for those who were too shy to pick up the phone, the pneumatic tubes offered a perfect alternative. The tubes were built into the handrails, and one was located at each table. The nightclub provided paper on which to scrawl notes. Patrons only had to specify where they wanted their missives sent. Like messaging on a dating app, but with‚Äîyou know‚Äîtubes.</p>
<p>At the Resi, many provocative notes were passed around, but eager flirters needed to be careful‚Äî‚Äú<a href="http://archives.chicagotribune.com/1968/10/30/page/48/article/the-tribune-travelers-guide">messages sent by tube [were] checked by female ‚Äòcensors‚Äô in the switchboard room</a>‚Äù in an early form of comment moderation.</p>
<figure><img src="https://img.atlasobscura.com/ryYDtpazWBELSiSJ2X06o4FRiUPMURueJ5JStWcJhAY/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy80ZjE4ZDQzMGE4/NzAwZmI0M2RfUmVz/aV8yLnBuZw.png" alt="The lavish Resi nightclub." width="auto" data-kind="article-image" id="article-image-42703" data-src="https://img.atlasobscura.com/ryYDtpazWBELSiSJ2X06o4FRiUPMURueJ5JStWcJhAY/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy80ZjE4ZDQzMGE4/NzAwZmI0M2RfUmVz/aV8yLnBuZw.png"><figcaption>The lavish Resi nightclub. <a target="_blank" href="http://liquidfireworks.com/about-us/">Courtesy of Michael Przystawik</a></figcaption></figure>
<p>The pneumatic tube system existed for decades, and Americans who visited Berlin after World War II <a href="http://www.cabaret-berlin.com/?p=491#comments">remember it fondly</a>.</p>
<p>Today, many fictionalized accounts memorialize it: <em>Then We Take Berlin</em> by John Lawton <a href="https://books.google.com/books?id=m6vGAgAAQBAJ&amp;pg=PA246&amp;lpg=PA246&amp;dq=pneumatic+tube+nightclub&amp;source=bl&amp;ots=7IP9C488gY&amp;sig=jUynGQMdbt4bihAh2QQ3NC_TmJ4&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjhq6vpwZ3UAhWBaxQKHRvcA-k4ChDoAQhMMAg#v=onepage&amp;q=pneumatic%20tube%20nightclub&amp;f=false">describes</a> how visitors could ‚Äúwrite a message, stick it in the snake‚Äôs head, yank on the handle and the pneumatic tube would whisk it up to the top gallery and they‚Äôd redirect it to the right table.‚Äù (<em>Cabaret</em>, meanwhile, tributes the table phone system in ‚Äú<a href="https://www.youtube.com/watch?v=X5SLGipH-cA">The Telephone Song</a>.‚Äù) Ian McEwan‚Äôs novel <em>The Innocent</em> also offers an evocative tribute. When his main character, Leonard, visits a fictionalized version of the Resi nightclub, the protagonist finds a pamphlet that boasts the establishment‚Äôs ‚ÄúModern Table-Phone-System‚Äù and ‚ÄúPneumatic-Table-Mail-Service,‚Äù which sends ‚Äúevery night thousands of letters or little presents from one visitor to another.‚Äù</p>
<p>This ‚Äútable mail service‚Äù was real, and allowed patrons to send more than just a handwritten note to that handsome stranger across the way. The Resi offered a long menu of gifts that visitors could dispatch via pneumatic tube‚Äîincluding <a href="https://books.google.com/books?id=81FjCwAAQBAJ&amp;q=pneumatic#v=snippet&amp;q=pneumatic&amp;f=false">perfume bottles, cigar cutters, travel plans</a>, and, <a href="https://www.roundabouttheatre.org/Roundabout/media/Roundabout/PDF/UPSTAGE/Cabaret_UpstageGuide_R3.pdf">according to one source, cocaine</a>.</p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wind and Solar Will Be 25% of Total U.S. Generating Capacity Within Three Years (193 pts)]]></title>
            <link>https://electricenergyonline.com/article/energy/category/climate-change/82/1033617/wind-solar-will-be-25-of-total-u-s-generating-capacity-within-three-years-as-they-grow-by-over-100-gw-.html</link>
            <guid>36853565</guid>
            <pubDate>Mon, 24 Jul 2023 20:11:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electricenergyonline.com/article/energy/category/climate-change/82/1033617/wind-solar-will-be-25-of-total-u-s-generating-capacity-within-three-years-as-they-grow-by-over-100-gw-.html">https://electricenergyonline.com/article/energy/category/climate-change/82/1033617/wind-solar-will-be-25-of-total-u-s-generating-capacity-within-three-years-as-they-grow-by-over-100-gw-.html</a>, See on <a href="https://news.ycombinator.com/item?id=36853565">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="description_news"><p>Based upon a review by the SUN DAY Campaign of data newly released by the Federal Energy Regulatory Commission (FERC), utility-scale solar and wind appear to be on track to provide a quarter of the nation's installed electrical generating capacity within three years.&nbsp;</p>



<p>In its latest monthly "Energy Infrastructure Update" report (with data through May 31, 2023) FERC notes that wind is now 11.63% of total installed generating capacity while utility-scale solar provides another 6.86%. &nbsp;</p>



<p>However, over the next three years (i.e., by May 2026), FERC anticipates "high probability additions" of solar to provide another 80,087-MW while wind is expected to expand by 19,816-MW. Assuming that materializes, in three years, wind would account for 12.43% of installed capacity while utility-scale solar would provide another 12.41%. And that does not include generating capacity provided by small-scale, distributed (e.g., rooftop) solar. [1]&nbsp;</p>



<p>Factoring in FERC's forecasts for hydropower, geothermal, and biomass, renewable energy sources would expand from today's 28.01% of installed generating capacity to 33.85% - i.e., over a third - by May 2026. &nbsp;&nbsp;</p>



<p>Solar and wind's share of U.S. generating capacity could actually be substantially higher if new capacity exceeds FERC's forecast of "high probability additions." The agency indicates that the amount of solar and wind in the three-year pipeline could be nearly three times higher than the total of the "high probability additions". Solar could add 214,022-MW while wind could grow by 66,065-MW.&nbsp;</p>



<p>Moreover, recent history suggests that solar and wind growth is outpacing FERC's predictions for "high probability additions." A year ago, FERC reported "high-probability additions" for wind and solar within three years of 18,711-MW and 62,835-MW respectively.[2] FERC's latest 3-year forecast for those sources is now 22.5% higher. &nbsp;&nbsp;</p>



<p>Signs of that prospective growth may already be evident. For the first five months of 2023, wind and solar accounted for more than half of the new capacity additions this year - 51.07% - comprised of 4,460-MW of solar and 2.645-MW of wind. New capacity provided by hydropower (254-MW), geothermal (37-MW), and biomass (29-MW) brought renewables' combined share of new capacity up to 53.38%. The balance (other than 2-MW from oil) was provided by natural gas. &nbsp;</p>



<p>Notwithstanding the recent natural gas additions, FERC foresees a net decline of 1,564-MW in natural gas generating capacity over the next three years in addition to a drop of 19,966-MW in coal capacity. Oil and nuclear capacity are also foreseen falling - by 569-MW and 123-MW respectively.&nbsp;</p>



<p>Should just FERC's "high probability" forecasts materialize, by May 2026, installed U.S. fossil fuels' share of total capacity will drop over the next three years: natural gas - 41.67% (from 44.37% in May 2023), coal - 14.05% (from 16.49%), and oil - 2.68% (from 2.89%). Nuclear power will also fall from 8.07% today to 7.60% in May 2026.&nbsp;</p>



<p>"Wind and solar are now poised to each provide an eighth of the nation's installed generating capacity within three years while all renewables combined will account for over a third," noted the SUN DAY Campaign's executive director Ken Bossong. "But in light of renewable energy growth rates of recent years, those numbers may very well prove to be an under-estimate."</p>

<p><b>Sources:&nbsp;</b></p>

<p>FERC's 7-page "Energy Infrastructure Update for May 2023" was released on July 17, 2023, and can be found at:&nbsp;<a href="https://cms.ferc.gov/media/energy-infrastructure-update-may-2023">https://cms.ferc.gov/media/energy-infrastructure-update-may-2023</a>. For the information cited in this update, see the tables entitled "New Generation In-Service (New Build and Expansion)," "Total Available Installed Generating Capacity," and "Generation Capacity Additions and Retirements." &nbsp;</p>



<p><b>Notes:</b></p>

<p>[1] FERC generally only reports data for utility-scale facilities (i.e., those rated 1-MW or greater) and therefore its data do not reflect the capacity of distributed renewables, notably rooftop solar PV which - according to the U.S. Energy Information Administration (EIA) - accounts for approximately 30% of the nation's electrical generation by solar. That would imply that the total of distributed and utility-scale solar capacity combined is significantly more than the solar capacity of 6.86% reported by FERC for the first five months of 2023 and is perhaps closer to 9.0% or 10.0%.&nbsp;</p>



<p>[2] FERC's 6-page "Energy Infrastructure Update for May 2022" was released on July 12, 2022, and can be found at:&nbsp;<a href="https://cms.ferc.gov/media/energy-infrastructure-update-may-2022">https://cms.ferc.gov/media/energy-infrastructure-update-may-2022</a>&nbsp;</p>



<p>The SUN DAY Campaign is a non-profit research and educational organization founded in 1992 to support a rapid transition to 100% reliance on sustainable energy technologies as a cost-effective alternative to nuclear power and fossil fuels and as a solution to climate change. Follow on&nbsp;Twitter: @SunDayCampaign&nbsp;&nbsp;</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Python: Overlooked core functionalities (217 pts)]]></title>
            <link>https://erikvandeven.medium.com/python-uncovering-the-overlooked-core-functionalities-54590420c225</link>
            <guid>36853495</guid>
            <pubDate>Mon, 24 Jul 2023 20:06:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://erikvandeven.medium.com/python-uncovering-the-overlooked-core-functionalities-54590420c225">https://erikvandeven.medium.com/python-uncovering-the-overlooked-core-functionalities-54590420c225</a>, See on <a href="https://news.ycombinator.com/item?id=36853495">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://erikvandeven.medium.com/?source=post_page-----54590420c225--------------------------------"><div aria-hidden="false"><p><img alt="Erik van de Ven" src="https://miro.medium.com/v2/resize:fill:88:88/1*fwRvhhQzroW7HLMQ2MJN5w.jpeg" width="44" height="44" loading="lazy"></p></div></a></div><figure><figcaption>Photo by <a href="https://unsplash.com/fr/@usinglight?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Stefan Steinbauer</a> on <a href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="1076">What Kyle Simpson mentions about JavaScript in his books, Luciano Ramalho mentions in his book ‚ÄòFluent Python‚Äô about Python. They basically address the same issue of both languages. To put it in my own words:</p><blockquote><p id="dccc">Because the language is so easy to learn, many practitioners only scratch the surface of its full potential, neglecting to delve into the more advanced and powerful aspects of the language which makes it so truly unique and powerful</p></blockquote><p id="abb4">So let‚Äôs discuss briefly all functionalities you might haven‚Äôt heard of, but you definitely want to know if you aim to become a truly seasoned Pythonista.</p><h2 id="e247">Evaluation of Default Arguments</h2><p id="2954">Python arguments are evaluated when the function definition is encountered. Good to remember that! This means that each time the <strong>fib_memo </strong>function (which is mentioned below) is called without explicitly providing a value for the memo argument, it will use the same dictionary object that was created when the function was defined.</p><pre><span id="d90b">def fib_memo(n, memo={0:0, 1:1}):<br>  """ <br>  n is the number nth number <br>  you would like to return in the sequence <br>  """<br>  if not n in memo:<br>    memo[n] = fib_memo(n-1) + fib_memo(n-2)<br>  return memo[n]<p># 6th Fibonacci (including 0 as first number)<br>fib_memo(6) # should return 8</p></span></pre><p id="6d25">So this code works, in Python. This also means that you could execute the <strong>fib_memo </strong>function in a single script multiple times, like in a for loop, with each execution increasing the fibonacci number to be computed, without hitting the ‚Äúmaximum recursion depth exceeded‚Äù limit, as the memo will keep expanding. <a href="https://medium.com/@erikvandeven/memoization-done-right-in-python-96d3527a43f6" rel="noopener">More information can be found in my other article</a>.</p><h2 id="34c6">Walrus Operator</h2><p id="08b5">The walrus operator (<code>:=</code> ), introduced in Python 3.8, allows you to assign a value to a variable within an expression. This way, you can assign the value to a variable and check its value in one expression:</p><pre><span id="ce90">import random<br>some_value = random.randint(0,100) # return a number between 0 and, including, 100<p>if((below_ten := some_value) &lt; 10):<br>    print(f"{below_ten} is smaller than 10")</p></span></pre><p id="d177">Obviously, it‚Äôs also easy to assign and check whether the returned value contains a truthy value or not:</p><pre><span id="fbac">if(result := some_method()): # If result is not Falsy<br>    print(result)</span></pre><h2 id="3b91">*args and **kwargs</h2><p id="2eeb">With the asterisk (<code>*</code> ) you can unpack the arguments or keyword arguments (with <code>**</code> ) before passing them to the function. For example, let‚Äôs consider the following code:</p><pre><span id="9ecb">my_numbers = [1,2]<p>def sum_numbers(first_number, second_number):<br>    return first_number + second_number</p><p># This will return a TypeError.<br># TypeError: sum() missing 1 required positional argument: 'second_number'<br>sum_numbers(my_numbers)</p><p># This will return the expected result, 3<br>sum_numbers(*my_numbers)</p></span></pre><p id="c430">When we call the <code>sum_numbers</code> function without unpacking <code>my_numbers</code>, it raises a <em>TypeError </em>because the function expects two separate arguments. However, by using the asterisk (*), we can unpack the values from <code>my_numbers</code> and pass them as individual arguments, resulting in the correct output.</p><p id="4790">This unpacking technique works not only with tuples and lists, but also with dictionaries (though it will pass the keys as arguments). But what about keyword arguments? For that, we can utilize the double asterisk (**). Take the following code as an example:</p><pre><span id="70c5">def greet_person(last_name, first_name):<br>  print(f"Hello {first_name} {last_name}")<p>data = {"first_name": "John", "last_name": "Doe"}<br>greet_person(**data)</p></span></pre><p id="275b">Besides unpacking a sequence to pass them as arguments to a function, you could also use it to create a new sequence, for example:</p><pre><span id="ec87">numbers = [1, 2, 3, 4, 5]<br>new_list_numbers = [*numbers]</span></pre><p id="8f8f">The original numbers list remains unaffected, and you have a <code>new_list_numbers</code> variable which contains a copy of the same list. Be careful with links containing objects, though:</p><pre><span id="16bb">numbers = [[1, 2], [3, 4], [5, 6]]<br>packed_numbers = [*numbers]<p>numbers[0].append(10)  # Modify the nested list within the original list</p><p>print(numbers)          # Output: [[1, 2, 10], [3, 4], [5, 6]]<br>print(packed_numbers)   # Output: [[1, 2, 10], [3, 4], [5, 6]]</p></span></pre><h2 id="ffdb">any and all</h2><p id="bae3"><code>any</code> and <code>all</code> are built-in functions that operate on iterable objects (such as lists, tuples, or sets) and return a Boolean value based on the elements in the iterable. An example:</p><pre><span id="26e3">some_booleans = [True, False, False, False]<p>any(some_booleans) # returns True<br>all(some_booleans) # returns</p></span></pre><p id="cec9">You could use the <code>all</code> and <code>any</code> functions in combination with list comprehensions, which return an iterable and pass it as argument to the <code>all</code> functions:</p><pre><span id="e6d1">numbers = [5, 10, 3, 8, -2]<br>all_positive = all(num &gt; 0 for num in numbers)</span></pre><p id="499e">‚Ä¶ or any functions:</p><pre><span id="c007">fruits = ['apple', 'banana', 'cherry', 'durian']<p># Check if all fruits start with 'a'<br>result = all(fruit.startswith('a') for fruit in fruits)<br>print(result)  # Output: False</p></span></pre><p id="5547">A table which shows the differences of outputs depending on the values in the iterable, is shown below.</p><figure></figure><h2 id="648b">Swapping Variables</h2><p id="95bd">You can combine tuple packing (what is happening on the right of the equal (=) sign) and unpacking (what is happening on the left of the equal(=) sign) and leverage this functionality for swapping variables:</p><pre><span id="9ac7">a = 10<br>b = 5<p># Swap the values of b and a by packing and unpacking<br>a, b = b, a</p><p>print(a) # 5<br>print(b) # 10</p></span></pre><h2 id="0cd5">str vs repr</h2><p id="12d6">We are used to converting some variable or value to a string, using<code>str(some_value)</code>, so we can print it for debugging purposes. I would like to make you aware of <code>repr(some_value)</code>. The main difference is that <code>repr</code> tries to return a printable representation of the object, while <code>str</code> just tries to return a string representation. A better example is shown below:</p><pre><span id="e33c">import datetime<p>today = datetime.datetime.now()</p><p>print(str(today))   # Output: 2023-07-20 15:30:00.123456<br>print(repr(today))  # Output: datetime.datetime(2023, 7, 20, 15, 30, 0, 123456)</p></span></pre><p id="2e4c">As you can see, <code>str()</code> simply returns the <em>datetime </em>as a string representation. If you want to determine whether the variable <strong>today </strong>contains a string or a <em>datetime </em>object, you wouldn‚Äôt be able to discern that information from this alone. On the other hand, ‚Å£<code>repr()</code> provides information about the actual object that the variable holds. This information is significantly more valuable during debugging.</p><h2 id="4869">Extended Iterable Unpacking</h2><p id="22ad">We can keep this simple: if you would like to get the first and last value of a sequence in a single command:</p><pre><span id="4ede">first, *middle, last = [1, 2, 3, 4, 5]<p>print(first)  # 1<br>print(middle) # [2, 3, 4]<br>print(last)   # 5</p></span></pre><p id="50aa">But this works as well</p><pre><span id="f1df">*the_first_three, second_last, last = [1, 2, 3, 4, 5]<p>print(the_first_three) # [1, 2, 3]<br>print(second_last)     # 4<br>print(last)            # 5</p></span></pre><p id="516a">Or other combinations.</p><h2 id="326c">Multiple Context Managers</h2><p id="d74c">We are used to using one context manager at a time, like opening a file:</p><pre><span id="734d">with open('file.txt', 'r') as file:<br>    # Code that uses the file<br>    # The file will be automatically closed at the end of the block<br>    # even if an exception occurs<p>    # Example: reading lines from the file<br>    for line in file:<br>        print(line.strip())</p><p>with open('file_2.txt', 'r') as other_file:<br>    # Second context manager </p><p>    for line in other_file:<br>        print(line.strip())</p></span></pre><p id="a93d">But we could easily open multiple files in a single statement. Easy if you would like to write lines to the other file for example:</p><pre><span id="e21f">with open('file1.txt') as file1, open('file2.txt') as file2:<br>    # Code that uses both file1 and file2<br>    # The files will be automatically closed at the end of the block<br>    # even if an exception occurs<p>    # Example: reading lines from file1 and writing them to file2<br>    for line in file1:<br>        file2.write(line)</p></span></pre><h2 id="6f7b">The Python Debugger</h2><p id="aedc">We could just print a ton of variables in our file, for debugging purposes, or we could simply use the Python Debugger (pdb), which helps us to set breakpoints which makes it so much easier:</p><pre><span id="2d39">import pdb<p># Set this breakpoint somewhere in your code<br>pdb.set_trace()</p></span></pre><p id="e97d">What makes this so much more valuable is that the program will stop at the breakpoint at which you could print any variable to check its value or existence at that specific breakpoint. Try it! These are several commands you could use when the program hits a breakpoint:</p><ul><li id="6187"><code>n</code> or <code>next</code>: Execute the next line.</li><li id="cfb5"><code>s</code> or <code>step</code>: Step into a function call.</li><li id="0f52"><code>c</code> or <code>continue</code>: Continue execution until the next breakpoint.</li><li id="b58f"><code>l</code> or <code>list</code>: Show the current code context.</li><li id="1c34"><code>p &lt;expression&gt;</code> or <code>pp &lt;expression&gt;</code>: Print the value of an expression.</li><li id="48c2"><code>b &lt;line&gt;</code> or <code>break &lt;line&gt;</code>: Set a new breakpoint at the specified line.</li><li id="c763"><code>h</code> or <code>help</code>: Get help on using <code>pdb</code>.</li><li id="455f"><code>q</code> or <code>quit</code>: Quit the debugger and terminate the program.</li></ul><h2 id="fffc">collections.Counter</h2><p id="3fd8">The <code>Counter</code> class from the <code>collections</code> module provides a convenient way to count elements in an iterable:</p><pre><span id="7d70">from collections import Counter<p>my_list = [1, 2, 3, 1, 2, 1, 3, 4, 5]<br>counts = Counter(my_list)<br>print(counts)  # Output: Counter({1: 3, 2: 2, 3: 2, 4: 1, 5: 1})</p></span></pre><h2 id="ed15">Combinations Using Itertools</h2><p id="f5b8">We could combine different for loops to create permutations, combinations or a cartesian product, or we could simply use the built in itertools.</p><h2 id="0cdd">Permutations</h2><pre><span id="0079">import itertools<p># Generating permutations<br>perms = itertools.permutations([1, 2, 3], 2)<br>print(list(perms))  # Output: [(1, 2), (1, 3), (2, 1), (2, 3), (3, 1), (3, 2)]</p></span></pre><h2 id="0c28">Combinations</h2><pre><span id="6457">import itertools<p># Generating combinations<br>combs = itertools.combinations('ABC', 2)<br>print(list(combs))  # Output: [('A', 'B'), ('A', 'C'), ('B', 'C')]</p></span></pre><h2 id="380c">Cartesian product</h2><pre><span id="992e">import itertools<p># Generating Cartesian product<br>cartesian = itertools.product('AB', [1, 2])<br>print(list(cartesian))  # Output: [('A', 1), ('A', 2), ('B', 1), ('B', 2)]</p></span></pre><h2 id="662b">Two Ways of Using Underscore</h2><p id="823c">These are two ways to use the underscore in Python: as a separater for large numbers or as a throwaway variable.</p><h2 id="0db3">Throwaway Variable</h2><p id="7870">The underscore <code>_</code> can be used as a throwaway variable to discard unwanted values:</p><pre><span id="0672"># Ignoring the first return value of a function<br>_, result = some_function()<p># Looping without using the loop variable<br>for _ in range(5):<br>    do_something()</p><p># You just need the first and the last<br>first, *_, last = [1, 2, 3, 4, 5]</p></span></pre><h2 id="a32f">Separater for Large Numbers</h2><p id="81aa">You can use underscores (<code>_</code>) as visual separators to enhance readability when working with large numeric values. This feature was introduced in Python 3.6 and is known as "underscore literals."</p><pre><span id="6f99">population = 7_900_000_000<br>revenue = 3_249_576_382.50<p>print(population)  # Output: 7900000000<br>print(revenue)  # Output: 3249576382.5</p></span></pre></div><div><p id="ac5b">Liked this article?</p><p id="135d">A few <strong>claps üëè </strong>is very much appreciated (Just hold that clap button and never let go üòú)! And if you want to read more, make sure to give me a <strong>follow. üôè</strong></p><p id="803d">Thanks so much for reading!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Algorithmic Beauty of Plants [pdf] (127 pts)]]></title>
            <link>http://algorithmicbotany.org/papers/abop/abop.pdf</link>
            <guid>36853206</guid>
            <pubDate>Mon, 24 Jul 2023 19:39:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://algorithmicbotany.org/papers/abop/abop.pdf">http://algorithmicbotany.org/papers/abop/abop.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=36853206">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Advanced Performance Extensions (APX) (136 pts)]]></title>
            <link>https://www.intel.com/content/www/us/en/developer/articles/technical/advanced-performance-extensions-apx.html</link>
            <guid>36853166</guid>
            <pubDate>Mon, 24 Jul 2023 19:35:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.intel.com/content/www/us/en/developer/articles/technical/advanced-performance-extensions-apx.html">https://www.intel.com/content/www/us/en/developer/articles/technical/advanced-performance-extensions-apx.html</a>, See on <a href="https://news.ycombinator.com/item?id=36853166">Hacker News</a></p>
<div id="readability-page-1" class="page">


















    
    
    
        















    
    
        
        
        
        
        
        
        
    




    
    
        
    




    
    
    
    
        
    


    
    
        
    


    
    
        
    


    
    
        
    






















    
    
    
    
    



    


    

































    
        <div data-component="global-nav-redesign" data-component-id="1">
            <header role="banner">
                <nav role="navigation" aria-label="main navigation" data-igm="">
                    <!-- Brand and toggle get grouped for better mobile display -->
                    <div>

                        <div>
                            <a href="https://www.intel.com/content/www/us/en/homepage.html" alt="Intel homepage">
                                    
                                <img src="https://www.intel.com/content/dam/logos/intel-header-logo.svg" height="300" width="118" alt="Intel logo - Return to the home page">
                            </a>
                        </div>

                        

                        <!-- START MOBLE TOGGLE buttons -->
                        <div>


                            <!-- START: NON-signed in panel -->
                            <span id="not-logged-in-scenario">

    
        </span>


                            


















































<span id="logged-in-scenario">






</span>

                            
















    




<div id="panel-language-selector" aria-expanded="false" aria-selected="false">
                <h2>
                    
                        
                            Select Your Language
                        
                        
                    
                </h2>
            </div>


                            <!-- END: NON-sign in panel -->
                            

























    

    



    








    











    
    
    
        
    


    
    <div aria-live="off" id="simplify-search" data-component="wa_skip_track" data-igm-search-content="" aria-expanded="false" aria-selected="false" document-height="true">
                             
                                 
                            
                            <!-- Search Result Typeahead -->
                            <div id="igm-search-result" data-igm-search-results="">
                                    
                                        Sign In to access restricted content
                                </div>
                            <!-- Recent Searches: 1) display default search info if no search terms is available  -->
                            <!-- Recent Searches: 2) display recenter terms when available and hide default search info  -->
                            <div data-igm-search-related="">
                                <div>
                                    <!-- default search info -->
                                    <div>
                                        <h3>Using Intel.com Search</h3>
                                        <p>You can easily search the entire Intel.com site in several ways.</p>
                                        <ul>
                                            <li>
                                                Brand Name:
                                                <strong>
                                                    Core i9
                                                </strong>
                                            </li>
                                            <li>
                                                Document Number:
                                                <strong>
                                                    123456
                                                </strong>
                                            </li>
                                            <li>
                                                Code Name:
                                                <strong>
                                                    Alder Lake
                                                </strong>
                                            </li>
                                            <li>
                                                Special Operators:
                                                <strong>
                                                    ‚ÄúIce Lake‚Äù, Ice AND Lake, Ice OR Lake, Ice*
                                                </strong>
                                            </li>
                                        </ul>
                                    </div>
                                    <!-- quick links is always visible on the recents overlay -->
                                    <div>
                                        <h3>Quick Links</h3>
                                        <p>You can also try the quick links below to see results for most popular searches.</p>
                                        <ul>
                                            <!--<li>
                                                <a class="quick-link" rel="noopener noreferrer"
                                                   href=https://ark.intel.com?wapkw=quicklink:product-specifications>
                                                    Product Specifications
                                                </a>
                                            </li>-->
                                            <li>
                                                <a rel="noopener noreferrer" href="https://www.intel.com/content/www/us/en/products/overview.html?wapkw=quicklink:products">
                                                    Product Information
                                                </a>
                                            </li>
                                            <li><a rel="noopener noreferrer" href="https://www.intel.com/content/www/us/en/support.html?wapkw=quicklink:support">
                                                Support
                                            </a>
                                            </li>
                                            <li>
                                                <a rel="noopener noreferrer" href="https://downloadcenter.intel.com/?wapkw=quicklink:download-center">
                                                    Drivers &amp; Software
                                                </a>
                                            </li>
                                        </ul>
                                    </div>
                                    <!-- recent search terms -->
                                    <div data-component="wa_skip_track" data-component-id="1">
                                            <h3>Recent Searches</h3>
                                        </div>
                                </div>
                                <div>
                                    
                                        Sign In to access restricted content
                                </div>
                            </div>
                            
                                 <div data-igm-advanced-search="">
											<div data-component="wa_skip_track" data-component-id="1">
													<h3>Advanced Search</h3>
													<div>
															<h3>Only search in</h3>
															<div aria-label="Only Search In">
																<label for="search_title">
																	
																	Title</label>

																	<label for="search_description">
																	
																Description</label>

																	<label for="search_id">
																	Content ID</label>
															</div>

															
														</div>
												</div>
											<div>
												Sign in to access
												restricted content.
											</div>
										</div>
                                    
                        </div>


                        </div>
                        <!-- END MOBILE TOGGLE buttons -->
                    </div>
                </nav>
            </header>
        </div>

    
    


<div id="alertMsg" data-scroll-track="false">
                        <p>The browser version you are using is not recommended for this site.<br>Please consider upgrading to the latest version of your browser by clicking one of the following links.</p>
                        <div>
                            <ul>
                                
                                    <li><a href="https://support.apple.com/downloads/safari">Safari</a></li>
                                
                                    <li><a href="https://support.google.com/chrome/answer/95346?hl=en">Chrome</a></li>
                                
                                    <li><a href="https://www.microsoft.com/en-us/edge">Edge</a></li>
                                
                                    <li><a href="https://www.mozilla.org/en-US/firefox/new/">Firefox</a></li>
                                
                            </ul>
                        </div>
                    </div>


<main id="primary-content">


























    
        
        
        <main id="primary-content-main">
            

            <div id="articleHero-1" data-component="articleHero" data-component-id="1">
                                        <h2>Introducing Intel¬Æ Advanced Performance Extensions (Intel¬Æ APX)</h2>
                                    </div>

            
            <article data-component-id="1" data-component="articletemplate">
                <div>
                            <div>
                                        
                                        <img src="" alt="author-image">
                                    </div>

                            <div>
                                            <!-- BEGIN NEW BUILD IN - Article Actions - DESKTOP -->
                                            
                                            <div id="articleintro" data-component="articleintro" data-component-id="1">
                    <span>Sebastian Winkel </span>
                    <span></span>
                
                    <span>Jason Agron</span>
                    <span></span>
                </div>

                                            <div id="articleparagraph-1" data-component="articleparagraph" data-component-id="1">
                    
                    


<commons:articleparagraphtag>


    <div>
                                
                                
                                    
                                    
                                        <p>Intel¬Æ architecture powers datacenters and personal computers around the world. Since its introduction by Intel¬Æ in 1978, the architecture has continuously evolved to take advantage of emerging workloads and the relentless pace of Moore‚Äôs law. The original instruction set defined only eight 16-bit general-purpose registers, which over the years were doubled in number and quadrupled in size. A large set of vector registers was added, and most recently Intel¬Æ AMX introduced two-dimensional matrix registers, providing a big jump in AI performance.<a href="https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/overview.html"><sup>1</sup></a></p>

<p>Today, we are introducing the next major step in the evolution of Intel¬Æ architecture. Intel¬Æ Advanced Performance Extensions (Intel¬Æ APX) expands the entire x86 instruction set with access to more registers and adds various new features that improve general-purpose performance. The extensions are designed to provide efficient performance gains across a variety of workloads ‚Äì without significantly increasing silicon area or power consumption of the core.</p>

<p>Intel¬Æ APX doubles the number of general-purpose registers (GPRs) from 16 to 32. This allows the compiler to keep more values in registers; as a result, APX-compiled code contains 10% fewer loads and more than 20% fewer stores than the same code compiled for an Intel¬Æ 64 baseline.<sup>2</sup>&nbsp;Register accesses are not only faster, but they also consume significantly less dynamic power than complex load and store operations.</p>

<p>Compiler enabling is straightforward ‚Äì a new REX2 prefix provides uniform access to the new registers across the legacy integer instruction set. Intel¬Æ AVX instructions gain access via new bits defined in the existing EVEX prefix. In addition, legacy integer instructions now can also use EVEX to encode a dedicated destination register operand ‚Äì turning them into three-operand instructions and reducing the need for extra register move instructions. While the new prefixes increase average instruction length, there are 10% fewer instructions in APX-compiled code<sup>2</sup>, resulting in similar code density as before.</p>

<p>The new GPRs are XSAVE-enabled, which means that they can be automatically saved and restored by XSAVE/XRSTOR sequences during context switches. They do not change the size and layout of the XSAVE area as they take up the space left behind by the deprecated Intel¬Æ MPX registers.</p>

<p>We propose to define the new GPRs as caller-saved (volatile) state in application binary interfaces (ABIs), facilitating interoperability with legacy binaries. Optimized calling conventions can be introduced where legacy compatibility requirements are relaxed. Generally, more register state will need to be managed at function boundaries. In order to reduce the associated overhead, we are adding PUSH2/POP2 instructions that transfer two register values within a single memory operation. The processor tracks these new instructions internally and fast-forwards register data between matching PUSH2 and POP2 instructions without going through memory.</p>

<p>The performance features introduced so far will have limited impact in workloads that suffer from a large number of conditional branch mispredictions. As out-of-order CPUs continue to become deeper and wider, the cost of mispredictions increasingly dominates performance of such workloads. Branch predictor improvements can mitigate this to a limited extent only as data-dependent branches are fundamentally hard to predict.</p>

<p>To address this growing performance issue, we significantly expand the conditional instruction set of x86, which was first introduced with the Intel¬Æ Pentium¬Æ Pro in the form of CMOV/SET instructions. These instructions are used quite extensively by today‚Äôs compilers, but they are too limited for broader use of if-conversion (a compiler optimization that replaces branches with conditional instructions).</p>

<p>Intel¬Æ APX adds conditional forms of load, store, and compare/test instructions, and it also adds an option for the compiler to suppress the status flags writes of common instructions. These enhancements expand the applicability of if-conversion to much larger code regions, cutting down on the number of branches that may incur misprediction penalties. All these conditional ISA improvements are implemented via EVEX prefix extensions of existing legacy instructions.</p>

<p>Application developers can take advantage of Intel¬Æ APX by simple recompilation ‚Äì source code changes are not expected to be needed. Workloads written in dynamic languages will automatically benefit as soon as the underlying runtime system has been enabled.</p>

<p>Intel¬Æ APX demonstrates the advantage of the variable-length instruction encodings of x86 ‚Äì new features enhancing the entire instruction set can be defined with only incremental changes to the instruction-decode hardware. This flexibility has allowed Intel¬Æ architecture to adapt and flourish over four decades of rapid advances in computing ‚Äì and it enables the innovations that will keep it thriving into the future.</p>

<h2>References</h2>

<ul>
	<li><a href="http://cdrdv2.intel.com/v1/dl/getContent/784266">Intel¬Æ Advanced Performance Extensions (Intel¬Æ APX) Architecture Specification</a>&nbsp;- specifies the Intel¬Æ APX extension of the encodings and the semantics of Intel architecture.</li>
	<li><a href="https://cdrdv2.intel.com/v1/dl/getContent/784265">Intel¬Æ Advanced Performance Extensions (Intel¬Æ APX) Software Enabling Introduction</a>&nbsp;-&nbsp;outlines the changes needed to enable Intel¬Æ APX in compilers, ABIs, operating systems, and hypervisors.</li>
	<li><a href="https://cdrdv2.intel.com/v1/dl/getContent/784267">Intel¬Æ Advanced Vector Extensions 10 (Intel¬Æ AVX10) Architecture Specification</a>&nbsp;- describes the Intel¬Æ Advanced Vector Extensions 10 Instruction Set Architecture.</li>
	<li><a href="https://cdrdv2.intel.com/v1/dl/getContent/784343">The Converged Vector ISA: Intel¬Æ Advanced Vector Extensions 10 Technical Paper</a>&nbsp;- provides introductory information regarding the converged vector ISA: Intel¬Æ Advanced Vector Extensions 10.</li>
</ul>

<h2>Footnotes</h2>

<ol>
	<li><a href="https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/overview.html">intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/overview.html</a></li>
	<li>This projection is based on a prototype simulation of the SPEC CPU¬Æ 2017 Integer benchmark suite.</li>
</ol>

                                    
                                
                            </div>

                    



                </commons:articleparagraphtag></div>


                                        </div>
                        </div>

                
                    
                

                
                
                
                
                    
                        
                    
                

            </article>
        </main>
    
    





    <div>
                    <h4>Product and Performance Information</h4>
                    
                    
                    
                        
                            
                        
                    
                    
                    
                </div>




    


</main>












































    







</div>]]></description>
        </item>
        <item>
            <title><![CDATA[A simple guide to fine-tuning Llama 2 (256 pts)]]></title>
            <link>https://brev.dev/blog/fine-tuning-llama-2</link>
            <guid>36852971</guid>
            <pubDate>Mon, 24 Jul 2023 19:18:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brev.dev/blog/fine-tuning-llama-2">https://brev.dev/blog/fine-tuning-llama-2</a>, See on <a href="https://news.ycombinator.com/item?id=36852971">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><strong>In this guide, I show how you can fine-tune Llama 2 to be a dialog summarizer!</strong></p><p>Last weekend, I wanted to finetune Llama 2 (which now reigns supreme in the <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">Open LLM leaderboard</a>) on a dataset of my own collection of Google Keep notes; each one of my notes has both a title and a body so I wanted to train Llama to generate a body from a given title.</p><p>This first part of the tutorial covers finetuning Llama 2 on the <a href="https://huggingface.co/datasets/samsum">samsum dialog summarization dataset</a> using Huggingface libraries. I tend to find that while Huggingface has built a superb library in transformers, their guides tend to overcomplicate things for the average joe. The second part, fine-tuning on custom data, is coming at the end of the week!</p><p>To get started, get yourself either an A10, A10G, A100 (or any GPU with &gt;24GB GPU memory). If you're not sure where to start, the <a href="https://console.brev.dev/environment/new?instance=gpu_1x_a10">Brev Cloud</a> makes it easy to access each of these GPUs!</p><h2 id="1-download-the-model">1. Download the model</h2><p>Clone Meta's Llama inference repo (which contains the download script):</p><pre><code><span>git</span><span> clone https://github.com/facebookresearch/llama.git</span>
</code></pre><p>Then run the download script:</p><pre><code><span>bash</span><span> download.sh</span>
</code></pre><p>It'll prompt you to enter the URL you got sent by Meta in an email. If you haven't signed up, do it <a href="https://ai.meta.com/llama/">here</a>. They are surprisingly quick at sending you the email!</p><p>For this guide, you only need to download the 7B model.</p><h2 id="2-convert-model-to-hugging-face-format">2. Convert model to Hugging Face format</h2><pre><code><span>pip </span><span>install</span><span> git+https://github.com/huggingface/transformers</span>
<span></span><span>cd</span><span> transformers</span>
<!-- -->
<span>python convert_llama_weights_to_hf.py </span><span>\</span><span></span>
<span>    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir models_hf/7B</span>
</code></pre><p>This now gives us a Hugging Face model that we can fine-tune leveraging Huggingface libraries!</p><h2 id="3-run-the-fine-tuning-notebook">3. Run the fine-tuning notebook:</h2><p>Clone the Llama-recipies repo:</p><pre><code><span>git</span><span> clone https://github.com/facebookresearch/llama-recipes.git</span>
</code></pre><p>Then open the <strong>quickstart.ipynb</strong> file in your preferred notebook interface:</p><p>(I use Jupyter lab like so):</p><pre><code><span>pip </span><span>install</span><span> jupyterlab</span>
<span>jupyter lab </span><span># in the repo you want to work in</span>
</code></pre><p>Then just run the whole notebook.</p><p>Make sure you change the line:</p><pre><code><span>model_id</span><span>=</span><span>"./models_hf/7B"</span>
</code></pre><p>to your actual model path that you converted. And that's that! You will end up with a Lora fine-tuned.</p><h2 id="4-run-inference-on-your-fine-tuned-model">4. Run inference on your fine-tuned model</h2><p>The issue here is that Huggingface only saves the adapter weights and not the full model. So we need to load the adapter weights into the full model. I struggled for a bit finding the right documentation to do this...But eventually worked it out!</p><p>Import libraries:</p><pre><code><span>import</span><span> torch</span>
<span></span><span>from</span><span> transformers </span><span>import</span><span> LlamaForCausalLM</span><span>,</span><span> LlamaTokenizer</span>
<span></span><span>from</span><span> peft </span><span>import</span><span> PeftModel</span><span>,</span><span> PeftConfig</span>
</code></pre><p>Load the tokenizer and model:</p><pre><code><span>model_id</span><span>=</span><span>"./models_hf/7B"</span><span></span>
<span>tokenizer </span><span>=</span><span> LlamaTokenizer</span><span>.</span><span>from_pretrained</span><span>(</span><span>model_id</span><span>)</span><span></span>
<span>model </span><span>=</span><span>LlamaForCausalLM</span><span>.</span><span>from_pretrained</span><span>(</span><span>model_id</span><span>,</span><span> load_in_8bit</span><span>=</span><span>True</span><span>,</span><span> device_map</span><span>=</span><span>'auto'</span><span>,</span><span> torch_dtype</span><span>=</span><span>torch</span><span>.</span><span>float16</span><span>)</span>
</code></pre><p>Load the adapter from where you saved it post-train:</p><pre><code><span>model </span><span>=</span><span> PeftModel</span><span>.</span><span>from_pretrained</span><span>(</span><span>model</span><span>,</span><span> </span><span>"/root/llama-recipes/samsungsumarizercheckpoint"</span><span>)</span>
</code></pre><p>Run inference:</p><pre><code><span>eval_prompt </span><span>=</span><span> </span><span>"""</span>
<span>Summarize this dialog:</span>
<span>A: Hi Tom, are you busy tomorrow‚Äôs afternoon?</span>
<span>B: I‚Äôm pretty sure I am. What‚Äôs up?</span>
<span>A: Can you go with me to the animal shelter?.</span>
<span>B: What do you want to do?</span>
<span>A: I want to get a puppy for my son.</span>
<span>B: That will make him so happy.</span>
<span>A: Yeah, we‚Äôve discussed it many times. I think he‚Äôs ready now.</span>
<span>B: That‚Äôs good. Raising a dog is a tough issue. Like having a baby ;-)</span>
<span>A: I'll get him one of those little dogs.</span>
<span>B: One that won't grow up too big;-)</span>
<span>A: And eat too much;-))</span>
<span>B: Do you know which one he would like?</span>
<span>A: Oh, yes, I took him there last Monday. He showed me one that he really liked.</span>
<span>B: I bet you had to drag him away.</span>
<span>A: He wanted to take it home right away ;-).</span>
<span>B: I wonder what he'll name it.</span>
<span>A: He said he‚Äôd name it after his dead hamster ‚Äì Lemmy  - he's  a great Motorhead fan :-)))</span>
<span>---</span>
<span>Summary:</span>
<span>"""</span><span></span>
<!-- -->
<span>model_input </span><span>=</span><span> tokenizer</span><span>(</span><span>eval_prompt</span><span>,</span><span> return_tensors</span><span>=</span><span>"pt"</span><span>)</span><span>.</span><span>to</span><span>(</span><span>"cuda"</span><span>)</span><span></span>
<!-- -->
<span>model</span><span>.</span><span>eval</span><span>(</span><span>)</span><span></span>
<span></span><span>with</span><span> torch</span><span>.</span><span>no_grad</span><span>(</span><span>)</span><span>:</span><span></span>
<span>    </span><span>print</span><span>(</span><span>tokenizer</span><span>.</span><span>decode</span><span>(</span><span>model</span><span>.</span><span>generate</span><span>(</span><span>**</span><span>model_input</span><span>,</span><span> max_new_tokens</span><span>=</span><span>100</span><span>)</span><span>[</span><span>0</span><span>]</span><span>,</span><span> skip_special_tokens</span><span>=</span><span>True</span><span>)</span><span>)</span>
</code></pre><p>Next in this series, I'll show you how you can format your own dataset to train Llama 2 on a custom task! Message me on <a href="https://twitter.com/samlhuillier_">Twitter</a> if you want to get me to hurry up on this!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Elixir is still safe (222 pts)]]></title>
            <link>https://paraxial.io/blog/still-safe</link>
            <guid>36852231</guid>
            <pubDate>Mon, 24 Jul 2023 18:22:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://paraxial.io/blog/still-safe">https://paraxial.io/blog/still-safe</a>, See on <a href="https://news.ycombinator.com/item?id=36852231">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img src="https://paraxial.io/blog/cheetah.jpeg" width="700"></p>
<p>
In March 2021 Nathan Long published <a href="https://dockyard.com/blog/2021/03/30/elixir-is-safe">Elixir is Safe</a>, a post about the security benefits of using Elixir, which focused on memory and thread safety. It is an excellent article for programmers and executives about the security benefits of Elixir. In July 2023 I googled ‚ÄúElixir is Safe‚Äù, and the first result was a snippet from the paper, ‚ÄúVision for a Secure Elixir Ecosystem: An Empirical Study of Vulnerabilities in Elixir Programs‚Äù, which was <a href="https://dl.acm.org/doi/10.1145/3476883.3520204">published by the ACM in April 2022.</a></p>
<p><img src="https://paraxial.io/blog/google.png" width="550"></p><p>
Several of the paper‚Äôs authors are students, and this post is not meant to criticize them as researchers. The paper is written as a reaction to Nathan‚Äôs article, which accurately describes the security benefits of Elixir. The paper‚Äôs claims are misleading, and the fact that it currently ranks above the original article on Google, and was published in a reputable journal, warrants a response.</p>
<p>
<em>‚ÄúPractitioners perceive Elixir to be a ‚Äòsafe‚Äô language, as the language allows practitioners to write fast software programs without introducing vulnerabilities, unlike other languages, such as C[16]. Positive perception of practitioners about the safety and security Elixir programs is subject to empirical validation. Practitioner perceptions are formed through personal experience, and not based on empirical evidence [9]. A systematic empirical investigation that quantifies reported security vulnerabilities can shed light on the state of security of Elixir programs. Such empirical investigation can also yield recommendations for practitioners and researchers on how to securely develop Elixir programs.‚Äù</em></p>
<p>
<em>[16] Nathan Long. 2021. Elixir is Safe. <a href="https://dockyard.com/blog/2021/03/30/elixir-is-safe">https://dockyard.com/blog/2021/03/30/elixir-is-safe</a></em></p>
<p>
Nobody thinks Elixir ‚Äúallows practitioners to write fast software programs without introducing vulnerabilities‚Äù. The article ‚ÄúElixir is Safe‚Äù never makes that claim, rather it mentions that Elixir is a memory safe language, which C is not, and discusses the thread safety benefits of Elixir. These are both important topics, for example Elixir completely eliminates <a href="https://paraxial.io/blog/data-race">security issues due to data races</a>, a severe vulnerability that can lead to users being logged into the wrong account, a disaster for banking and medical portals. </p>
<p>
An empirical study showing that Elixir programs contain vulnerabilities is not surprising, because software written in every language has them. Imagine Elixir as a type of asphalt which reduces the rate of potholes in roads, and Nathan‚Äôs article is explaining this benefit. Then a paper is published saying, ‚Äúroads paved with Elixir still have potholes‚Äù, ignoring that it results in fewer potholes. The paper does not mention memory or thread safety at all, which are the specific security benefits Nathan discusses. Rather, it surveys popular open-source Elixir projects for vulnerability related commits. </p>
<p><img src="https://paraxial.io/blog/repo_set.png" width="500"></p><p>
Which repos were selected for the dataset? The paper does not mention them, below is a list pulled from the <a href="https://figshare.com/articles/dataset/Elixir_Vulnerability_Dataset/15078573/1">dataset</a>. The unix command <code>awk -F "," '{print $1}' ELIXIR_FINAL_SECURITY_BUG_DATASET.csv | sort | uniq</code> was used to get this list.</p>
<pre><code>1. 30-days-of-elixir
2. absinthe
3. analytics
4. awesome-elixir
5. credo
6. curlconverter
7. distillery
8. edeliver
9. elixir
10. elixir-koans
11. guardian
12. httpoison
13. papercups
14. phoenix
15. phoenix-trello
16. poison
17. quantum-core
18. realtime
19. rustler</code></pre>
<p>
It is surprising to only see 19 repos, when the paper says 25. The inclusion of <code>30-days-of-elixir</code>, <code>awesome-elixir</code>, and <code>elixir-koans</code> is debatable, considering these repos do not store projects intended to be run in production. The counts for ‚ÄúElixir Files‚Äù and ‚ÄúElixir-related Commits‚Äù in the dataset do match the paper. </p>
<p><img src="https://paraxial.io/blog/table2.png" width="500"></p><p>
These numbers do match the dataset, but the definition of ‚Äúprogram‚Äù is misleading. For example, commit <code>2d68e9c</code> maps to two files in the dataset:</p>
<pre><code>absinthe/test/absinthe/phase/document/complexity_test.exs
absinthe/lib/absinthe/phase/document/complexity/result.ex</code></pre>
<p>
<a href="https://github.com/absinthe-graphql/absinthe/pull/800/commits/2d68e9c4d6540a200cd134661c23400d26631f75">View this commit on Github.</a> Both of these files are considered a ‚Äúprogram‚Äù in the paper. It is misleading to say the dataset has ‚Äú319 programs with vulnerability related commits‚Äù, the paper came to that number by counting files per commit classified as vulnerable. </p>
<p>
In the paper‚Äôs dataset, the repo for <a href="https://plausible.io/">Plausible Analytics</a> is included, with commit <code>f7b37fe</code>. You can <a href="https://github.com/plausible/analytics/commit/f7b37fe9eac54336f7a61a6b4d2378a381637b5b">view this commit on Github</a>, but it is for a PR with 32 changed files, so the commit alone is not enough information to determine what vulnerability the dataset is referring to. The paper mentions that commits were found via a keyword search for terms often used to describe vulnerabilities, one of which is ‚Äúsql‚Äù.</p>
<pre><code>lib/plausible_web/controllers/api/external_controller.ex</code></pre>
<pre><code><span>postgres_health</span><span> </span><span>=</span><span>
  </span><span>case</span><span> </span><span>Ecto.Adapters.SQL</span><span>.</span><span>query</span><span data-group-id="7394294953-1">(</span><span>Plausible.Repo</span><span>,</span><span> </span><span>"SELECT 1"</span><span>,</span><span> </span><span data-group-id="7394294953-2">[</span><span data-group-id="7394294953-2">]</span><span data-group-id="7394294953-1">)</span><span> </span><span data-group-id="7394294953-3">do</span><span>
    </span><span data-group-id="7394294953-4">{</span><span>:ok</span><span>,</span><span> </span><span>_</span><span data-group-id="7394294953-4">}</span><span> </span><span>-&gt;</span><span> </span><span>"ok"</span><span>
    </span><span>e</span><span> </span><span>-&gt;</span><span> </span><span>"error: </span><span data-group-id="7394294953-5">#{</span><span>inspect</span><span data-group-id="7394294953-6">(</span><span>e</span><span data-group-id="7394294953-6">)</span><span data-group-id="7394294953-5">}</span><span>"</span><span>
  </span><span data-group-id="7394294953-3">end</span></code></pre>
<p>
Sobelow will flag this code as vulnerable, even though it is not, because no user input is being passed to the SQL query. </p>
<pre><code>lib/plausible_release.ex</code></pre>
<pre><code><span>  </span><span>defp</span><span> </span><span>do_create_ch_db</span><span data-group-id="8989898227-1">(</span><span data-group-id="8989898227-1">)</span><span> </span><span data-group-id="8989898227-2">do</span><span>
    </span><span>db_to_create</span><span> </span><span>=</span><span> </span><span>Keyword</span><span>.</span><span>get</span><span data-group-id="8989898227-3">(</span><span>Application</span><span>.</span><span>get_env</span><span data-group-id="8989898227-4">(</span><span>:plausible</span><span>,</span><span> </span><span>:clickhouse</span><span data-group-id="8989898227-4">)</span><span>,</span><span> </span><span>:database</span><span data-group-id="8989898227-3">)</span><span>
    </span><span>IO</span><span>.</span><span>puts</span><span data-group-id="8989898227-5">(</span><span>"create </span><span data-group-id="8989898227-6">#{</span><span>inspect</span><span data-group-id="8989898227-7">(</span><span>db_to_create</span><span data-group-id="8989898227-7">)</span><span data-group-id="8989898227-6">}</span><span> clickhouse database/tables if it doesn't exist"</span><span data-group-id="8989898227-5">)</span><span>
    </span><span>Clickhousex</span><span>.</span><span>query</span><span data-group-id="8989898227-8">(</span><span>:clickhouse</span><span>,</span><span> </span><span>"CREATE DATABASE IF NOT EXISTS </span><span data-group-id="8989898227-9">#{</span><span>db_to_create</span><span data-group-id="8989898227-9">}</span><span>"</span><span>,</span><span> </span><span data-group-id="8989898227-10">[</span><span data-group-id="8989898227-10">]</span><span data-group-id="8989898227-8">)</span><span>
  </span><span data-group-id="8989898227-2">end</span></code></pre>
<p>
The string interpolation in a Clickhouse DB query looks suspicious, but the <code>db_to_create</code> value is not set by user input, so there is no vulnerability. The keywords ‚Äúsecurity‚Äù and ‚Äúvulnerability‚Äù also appear in the PR. Link to sub-commits:</p>
<p>
<a href="https://github.com/plausible/analytics/pull/209/commits/b3f783edb698c273cfa13271be3a74c9f226d65a"><code>b3f783e</code></a> - Adds a container security tool, not related to Elixir. </p>
<p>
<a href="https://github.com/plausible/analytics/pull/209/commits/57171f893aa762d02eeea5130f6e84f739f447a3"><code>57171f8</code></a> - The text of this comment says, ‚Äúfixing some vulnerabilities identified by the scanning tools‚Äù, however it involved a Debian upgrade, not related to Elixir.</p>
<p>
The paper counts <code>f7b37fe</code> as ‚Äú28 programs with vulnerability related commits‚Äù, because it involves 28 Elixir source code files, which is not correct. The lack of clarity around the vulnerable code in the dataset is concerning, making it impossible to truly replicate the paper‚Äôs findings, because you have to guess which specific lines the dataset is referring to as vulnerable. The dataset contains no labels for the type of security issue found (DoS, XSS, CSRF, etc), no information on the severity of each issue, and no discussion on how an attacker would use each vulnerability. This limits its utility in discussing the security of Elixir applications.</p>
<p>
It is highly unlikely that the dataset from this paper includes vulnerabilities related to thread or memory safety, which is the main point Nathan correctly makes in his article <a href="https://dockyard.com/blog/2021/03/30/elixir-is-safe">Elixir is Safe</a>.</p>
<hr>
<p>
<em><a href="https://paraxial.io/">Paraxial.io</a> secures Elixir and Phoenix applications. Professional services, including Elixir developer security training and penetration testing, are also available. <a href="https://calendly.com/paraxial/paraxial-io">Schedule a call today</a>.</em></p>
<meta property="og:image" content="https://paraxial.io/blog/cheetah.jpeg">
<meta name="twitter:image" content="https://paraxial.io/blog/cheetah.jpeg">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="Elixir is (Still) Safe">
<meta name="twitter:description" content="Examining the misleading claims in a published paper about Elixir security">
<meta name="twitter:site" content="@paraxialio">

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Have Attention Spans Been Declining? ‚Äì Yes, 65% (563 pts)]]></title>
            <link>https://slimemoldtimemold.com/2023/07/24/your-mystery-have-attention-spans-been-declining/</link>
            <guid>36851644</guid>
            <pubDate>Mon, 24 Jul 2023 17:43:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://slimemoldtimemold.com/2023/07/24/your-mystery-have-attention-spans-been-declining/">https://slimemoldtimemold.com/2023/07/24/your-mystery-have-attention-spans-been-declining/</a>, See on <a href="https://news.ycombinator.com/item?id=36851644">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
		<main id="main" role="main">

		
			
<article id="post-3310">
	<!-- .entry-header -->

	<div>
		
<p>[<em>This is one of the finalists in the SMTM Mysteries Contest, by a reader writing under the pseudonym&nbsp;</em>Cennfaeladh<em>. We‚Äôll be posting about one of these a week until we have gotten through all the finalists. At the end, we‚Äôll ask you to vote for a favorite, so remember which ones you liked</em>.]</p>







<p><strong>I investigate whether the attention span of individual humans has been falling over the last two decades (prompted by curiosity about whether the introduction of the internet may be harmful to cognitive performance). I find little direct work on the topic, despite its wide appeal. Reviewing related research indicates that individual attention spans might indeed have been declining<sub>65%</sub>.</strong></p>



<p>In what might be just the age-old regular<a href="https://en.wikipedia.org/wiki/Ephebiphobia"> ephebiphobia</a>, claims have been raised that individual attention spans<a href="https://docs.google.com/document/d/1HgV-GYVzALKj3ykZAqZD1HGWUKLgUopFMfOPHx-t0uI/edit#heading=h.pmepesu94ht5"> have been declining</a>‚Äînot just among adolescents, but among the general population. If so, this would be quite worrying: Much of the economy in industrialized societies is comprised of knowledge work, and knowledge work depends on attention to the task at hand: switching between tasks too often might prevent progress on complicated and difficult problems.</p>



<p>I became interested in the topic after<a href="https://edition.cnn.com/2023/01/11/health/short-attention-span-wellness/index.html"> seeing</a><a href="https://eu.usatoday.com/story/life/health-wellness/2021/12/22/covid-attention-span-exhaustion/8926439002/"> several</a><a href="https://twitter.com/amix011/status/1603927882459672576"> claims</a> that e.g.<a href="https://en.wikipedia.org/wiki/Generation_Z"> Generation Z</a> allegedly has lower attention spans, observing myself and how I struggled to get any work done when connected to the internet, and hearing reports from others online and in person having the same problem.</p>



<p>The exact question being asked is:</p>



<p><strong>‚ÄúHave the attention spans of individuals on neutral tasks (that is, tasks that are not specifically intended to be stimulating) declined from 2000 to the present?‚Äù</strong></p>



<p>(One might also formulate it as ‚ÄúIs there an equivalent of the ‚Äú<a href="https://en.wikipedia.org/wiki/Flynn_Effect#Possible_end_of_progression">Reversed Flynn Effect</a>‚Äù for attention span?‚Äù) I am not particularly wedded to the specific timeframe, though the worries mentioned above assert that this has become most stark during the last decade or so, attributing the change to widespread social media/smartphone/internet usage. Data from before 2000 or just the<a href="https://en.wikipedia.org/wiki/Aughts"> aughts</a> would be less interesting. The near-global<a href="https://en.wikipedia.org/wiki/COVID-19_lockdowns"> COVID-19 lockdows</a> could provide an especially enlightening<a href="https://en.wikipedia.org/wiki/Natural_experiment"> natural experiment</a>: Did social media usage increase (my guess: yes<sub>90%</sub>), and if so, did attention spans decrease at the same time (or with a lag) (my guess: also yes<sub>75%</sub>), but I don‚Äôt think anyone has the data on that <em>and</em> wants to share it.</p>



<p>Ideally want to have experiments from ~2000 up to 2019: close enough to the present to see whether there is a downward trend (a bit more than a decade after the introduction of the<a href="https://en.wikipedia.org/wiki/iPhone_(1st_generation)"> iPhone in 2007</a>), but before the<a href="https://en.wikipedia.org/wiki/COVID-19_pandemic"> COVID-19 pandemic</a> which might be a huge confounder, or just have accelerated existing trends (which we can probably check in another 2 years).</p>



<p>I am mostly interested in the attention span of individual humans and not groups:<a href="https://www.nature.com/articles/s41467-019-09311-w"> Lorenz-Spreen et al. 2019</a> investigate the development of a construct they call ‚Äúcollective attention‚Äù (and indeed find a decline), but that seems less economically relevant than individual attention span. I am also far less interested in self-perception of attention span, give me data from a proper<a href="http://www.careergym.com/psychometric_glossary/power_test"> power-</a> or<a href="http://www.careergym.com/psychometric_glossary/speed_test"> speed-test</a>, cowards!</p>



<p>So the question I am asking is not any of the following:</p>



<ul>
<li>‚ÄúDoes more social media/internet usage cause decreased attention spans?‚Äù</li>



<li>‚ÄúDoes more social media/internet usage correlate with decreased attention spans?‚Äù</li>



<li>‚ÄúDoes more social media/internet usage correlate with people reporting having shorter attention spans?‚Äù</li>



<li>‚ÄúDid collective attention spans decrease?‚Äù</li>



<li>‚ÄúAre people on average spending less time on webpages than they used to?‚Äù</li>
</ul>







<h2><strong>How Is Attention Span Defined?</strong></h2>



<p><a href="https://en.wikipedia.org/wiki/Attention">Attention</a> is generally divided into three distinct categories: <strong>sustained attention</strong>, which is the consistent focus on a specific task or piece of information over time (Wikipedia states that the span for sustained attention has a<a href="https://www.gwern.net/Leprechauns"> leprechaun</a> figure of 10 minutes floating around, elaborated on in<a href="https://psycnet.apa.org/record/2007-09934-002"> Wilson &amp; Korn 2007</a>); <strong>selective attention</strong>, which is the ability to resist distractions while focusing on important information while performing on a task (the thing trained during<a href="https://en.wikipedia.org/wiki/Mindfulness"> mindfulness meditation</a>); and <strong>alternating</strong> or <strong>divided attention</strong>, also known as the ability to<a href="https://en.wikipedia.org/wiki/Human_multitasking"> multitask</a>.</p>



<p>When asking the question ‚Äúhave attention spans been declining‚Äù, we‚Äôd ideally want the same test measuring all those three aspects of attention (and not just asking people about their<a href="https://guzey.com/statistics/dont-believe-self-reported-data/"> perception via surveys</a>), performed annually on large random samples of humans over decades, ideally with additional information such as age, sex, intelligence (or alternatively educational attainment), occupation etc. I‚Äôm personally most interested in the development of sustained attention, and less so in the development of selective attention. But I have not been able to find such research, and in fact there is apparently no agreed upon test for measuring attention span in the first place:</p>



<blockquote>
<p>She studies attention in drivers and witnesses to crime and says the idea of an ‚Äúaverage attention span‚Äù is pretty meaningless. ‚ÄúIt‚Äôs very much task-dependent. How much attention we apply to a task will vary depending on what the task demand is.‚Äù</p>




<cite><em>‚Äî Simon Maybin quoting Dr. Gemma Briggs,</em><a href="https://www.bbc.com/news/health-38896790"><em> ‚ÄúBusting the attention span myth‚Äù</em></a><em>, 2017</em></cite></blockquote>



<p>So,<a href="https://slatestarcodex.com/2014/08/11/does-the-glasgow-coma-scale-exist-do-comas/"> similar to comas</a>, attention span doesn‚Äôt exist‚Ä¶sure,<a href="https://unremediatedgender.space/2019/Dec/on-the-argumentative-form-super-proton-things-tend-to-come-in-varieties/index.html"> super-proton things come in varieties</a>, but <strong><em>which varieties</em></strong>?? And how??? Goddamn, psychologists, do your job and don‚Äôt just<a href="https://www.lesswrong.com/rationality/explain-worship-ignore"> worship</a> complexity.</p>



<p>Perhaps I should soften my tone, as this perspective appears elsewhere:</p>



<blockquote>
<p>[‚Ä¶] Gould suggests the metaphor of a dense bush whose branches are periodically pruned by nature. This allows for parallel evolutionary sequences, some of which are adaptive and others not ‚Äî at any moment in time only the tips of aseledaptive branches are in evidence, the pruned ones cannot be seen. Thus rather than being direct descendants of primitive hominids, for example, huankind would have evolved along a separate but parallel line from other primates.</p>



<p>Might the ontogeny of selective attention recapitulate this theme? That is, rather than selective attention comprising a single construct with a fixed ontogenic plan, might it be better conceptualized as a multidimensional construct with separat, parallel developmental trajectories for different components. To carry the analogy still further, might the specific developmental progression for a particular component of selective attention be determined by the adaptive fit of that component with the individual‚Äôs ‚Äòenvironmental press‚Äô? Although such a conjecture rekindles the tened of <em>ontogeny recapitulates phylogney</em> long since abandoned in physiological development (e.g., Dixon and Lerner, 1985), we suggest that it may nonetheless provide an overarching framework within which to cast life-span research and theory on the development of selective attention.</p>




<cite><em>‚Äî Plude et al.,</em><a href="https://pubmed.ncbi.nlm.nih.gov/7976468/"><em> ‚ÄúThe development of selective attention: A life-span overview‚Äù</em></a><em> p. 31, 1994</em></cite></blockquote>







<h2><strong>How Do We Measure Attention Span?</strong></h2>



<p>One of my hopes was that there is a canonical and well-established (and therefore, ah, <em>tested</em>) test for attention span (or just attention) √† la the IQ test for <em>g</em>: If so, I would be able to laboriously go through the literature on attention, extract the individual measurements (and maybe even acquire some datasets) and perform a meta-analysis.</p>



<h3><strong>Continuous Performance Tests</strong></h3>



<p>For measuring sustained and selective attention, I found the family of<a href="https://en.wikipedia.org/wiki/Continuous_performance_task"> continuous performance tests</a>, including the Visual and Auditory CPT (IVA-2), the<a href="https://en.wikipedia.org/wiki/Test_of_variables_of_Attention"> Test of Variables of Attention</a> (T.O.V.A.), <a href="https://storefront.mhs.com/collections/conners-cpt-3">Conners‚Äô CPT-III</a>, the <a href="https://pubmed.ncbi.nlm.nih.gov/23299180/">gradCPT</a> and the <a href="https://www.qbtech.com/adhd-tests/">QbTest</a>, some of which are described<a href="https://en.wikipedia.org/wiki/Continuous_performance_task#Test_administration"> here</a>. These tests usually contain two parts: a part with low stimulation and rare changes of stimuli, which tests for lack of attention, and a part with high stimulation and numerous changes of stimuli, which tests for impulsivity/self control.</p>



<p>Those tests<a href="https://en.wikipedia.org/wik/Continuous_performance_task#Test_scoring"> usually report four different scores</a>:</p>



<ol>
<li><strong>Correct detection:</strong> This indicates the number of times the client responded to the target stimulus. Higher rates of correct detections indicate better attentional capacity.</li>



<li><a href="https://en.wikipedia.org/wiki/Reaction_time"><strong>Reaction times</strong></a><strong>:</strong> This measures the amount of time between the presentation of the stimulus and the client‚Äôs response.</li>



<li><strong>Omission errors:</strong> This indicates the number of times the target was presented, but the client did not respond/click the mouse. High omission rates indicate that the subject is either not paying attention (distractibility) to stimuli or has a sluggish response.</li>



<li><strong>Commission errors:</strong> This score indicates the number of times the client responded but no target was presented. A fast reaction time and high commission error rate points to difficulties with impulsivity. A slow reaction time with high commission and omission errors, indicates inattention in general.</li>
</ol>



<p>I‚Äôm currently unsure about two crucial points:</p>



<ul>
<li>How much does any CPT measure the concept we naively call attention span? The papers I‚Äôve read don‚Äôt refer to attention span per se, but a general capability of sustained and selective attention.</li>



<li>Are there any time-series analyses or longitudinal studies using a CPT, or alternatively meta-analyses using data collected from existing studies? I have not been able to find any.</li>
</ul>



<h3><strong>Other Heterogenous Metrics</strong></h3>



<p>I also attempted to find a survey or review paper on attention span, but was unsuccessful in my quest, so I fell back to collecting metrics for attention span from different papers:</p>



<ul>
<li><a href="https://dl.motamem.org/microsoft-attention-spans-research-report.pdf">Gausby 2015</a>
<ul>
<li>Three online tests (probably devised by the authors (?), since no source is given) (n‚âà2000 Canadians). Very little information about the exact nature of the tests.
<ul>
<li>Sustained attention span: ‚ÄúCounting the number of times responds correctly identified an X occurring after an A.‚Äù</li>



<li>Selective attention span: ‚ÄúCounting the number of times respondents correctly identified a change in the orientation of the rectangles‚Äù</li>



<li>Alternating attention span: ‚ÄúCalculating the difference in the time lapsed to perform a series of consecutive number or letter classification, compared to a mixture of number and letter classifications.‚Äù</li>
</ul>
</li>



<li>Neurological research: The same games/tests as above with the participants being measured with an<a href="https://en.wikipedia.org/wiki/Electroencephalography"> EEG</a> (‚ÄúResults were reported as ACE (Attention, Connectivity, Encoding) scores, as well as the number of attention bursts‚Äù) (n=112 Canadians)</li>
</ul>
</li>



<li><a href="https://www.proquest.com/openview/4d7e41eb019558871327638cf5234990/1?pq-origsite=gscholar&amp;cbl=716332">Carstens et al. 2018</a> (n=209 American respondents to a survey)
<ul>
<li>Questionnaire developed by the authors based on Conners 2004 (reliability: Œ±=0.786)</li>
</ul>
</li>



<li><a href="https://psycnet.apa.org/record/2007-09934-002">Wilson &amp; Korn 2007</a> report several different measures of attention span during lectures: the amount of notes taken over time, observation of the students by an author of one study or two independent observers in another study, retention of material after the lecture, self-report in 5-minute intervals during the lecture, and heart rate. They also note that ‚ÄúResearchers use behaviors such as fidgeting, doodling, yawning, and looking around as indicators of inattentiveness (e.g., Frost, 1965; Johnstone &amp; Percival, 1976).‚Äù</li>



<li><a href="https://pubmed.ncbi.nlm.nih.gov/7976468/">Plude et al. 1994</a> review how selective attention develops during a human life. For measuring attention, they mainly focus on studies using reaction time as a metric‚Äîthe speed at which an action occurs as a result of a changing stimulus: eye movement patterns of infants, simple tests such as pressing a button on a changing (often visual) stimulus, the influence of irrelevant visual stimuli at the periphery on a task performed at the centre of the visual field, judging similarity of stimuli at various distances in the visual field, responding to a target stimulus surrounded by interfering distractor stimuli, and determining whether a visual target item is present or absent. They also mention skin conductance (measuring arousal).
<ul>
<li>They also mention studies investigating the time required for attentional switching in acoustic contexts: ‚ÄúPearson and Lane (1991a) studied the time course of the attention-shifting process between lists and also found large age-related improvements between 8 and 11 years. Whereas 8-year-olds required more than 3.5 s to completely switch from monitoring one list to another, 11-year-olds and adults appeared to complete the switch in less than 2.5 seconds.‚Äù</li>
</ul>
</li>



<li><a href="https://www.digitalinformationworld.com/2020/02/report-shows-that-attention-spans-are-shortening.html">Muhammad 2020</a>
<ul>
<li>Time spent on websites on average.
<ul>
<li>This is not an adequate metric, I believe: It would also decline if people would become better at prioritising on which websites are more worthy of their attention.</li>
</ul>
</li>
</ul>
</li>



<li><a href="https://www.nature.com/articles/s41467-019-09311-w">Lorenz-Spreen et al. 2019</a>
<ul>
<li>Time that specific pieces of information (hashtags/n-grams/Reddit submissions &amp;c) were popular</li>
</ul>
</li>
</ul>



<hr>



<p>As it stands, I think there‚Äôs a decent chance<sub>60%</sub> that one or several tests from the CPT family can be used as tests for attention span without much of a problem.</p>



<p>I don‚Äôt think a separate dedicated test for attention span exists<sub>45%</sub>: The set of listed measures I found (apart from the CPT) appears to be too heterogenous, idiosyncratic, mostly not quantitative enough and measuring slightly different things to be robustly useful for a meta-analysis.</p>







<h2><strong>What Are the Existing Investigations?</strong></h2>



<blockquote>
<p>A lack of long-term studies means we can‚Äôt tell whether attention spans have actually declined.</p>




<cite><em>‚ÄîBobby Duffy &amp; Marion Thain,</em><a href="https://www.kcl.ac.uk/policy-institute/assets/how-people-focus-and-live-in-the-modern-information-environment.pdf"><em> ‚ÄúDo we have your attention‚Äù</em></a><em> p. 5, 2022</em></cite></blockquote>







<ul>
<li><a href="https://dl.motamem.org/microsoft-attention-spans-research-report.pdf">Gausby 2015</a>
<ul>
<li>Questions answered:
<ul>
<li>Sustained attention:
<ul>
<li><em>Do younger people perform worse on the sustained attention span test?</em>, Yes (31% high sustained attention for group aged 18-34, 34% for group aged 35-54, and 35% group aged 55+) (the methodology is wholly unclear here, though: how do we determine the group that has ‚Äúhigh sustained attention span‚Äù? Did they perform any statisitical tests? If yes, which?).</li>



<li><em>Do people who report more technology usage (web browsing/multi-screen usage while online/social media usage/tech adoption) perform worse on the sustained attention span test?</em>, Yes. Light:medium:heavy usage for web browsing has 39%:33%:27% users with high sustained attention span, 36%:33%:27% for light:medium:heavy multi-screen usage, 36%:29%:23% for light:medium:heavy social media usage and 35%:31%:25% for light:medium:heavy tech adoption (though these numbers are basically not elaborated on).</li>
</ul>
</li>



<li>Selective attention:
<ul>
<li><em>Do younger people perform worse on the selective attention span test?</em> No (34% high selective attention for group aged 18-34, 30% for group aged 35-54, and 35% group aged 55+).</li>



<li><em>Do people with high selective attention use fewer devices at the same time?</em> Yes (details p. 31).</li>
</ul>
</li>



<li>Alternating attention:
<ul>
<li><em>Do younger people perform worse on the alternating attention span test?</em> No (36% high selective attention for group aged 18-34, 28% for group aged 35-54, and 36% group aged 55+).</li>



<li><em>Do people who report more technology usage (tech adoption/web browsing/multi-screen usage while online) perform worse on the alternating attention span test?</em> No, they seem to perform better: Light:medium:heavy tech adoption corresponds to 31%:39%:40% having high alternating attention spans, light:medium:heavy web browsing to 29%:34%:37% and multi-screening while online to 27%:32%:37%.</li>



<li><em>Do people who use social media more have higher Attention/Connection/Encoding scores on EEG measurements?</em>, Not quite: ‚ÄúModerate users of social media are better at multi-tasking than lower users. But, when crossing into the top quartile of social media usage, scores plummet.‚Äù</li>
</ul>
</li>
</ul>
</li>



<li>This is a marketing statement wearing the skinsuit of a previously great paper, it would be awesome if they released their exact methodology (tests performed, data collected, exact calculations &amp; code written). I can smell that they actually put effort into the research: Creating an actual test instead of just asking respondents about their attention spans, doing EEG measurements of over 100 people, for 3 different types of attention‚Ä¶come on! Just put out there what you did!</li>
</ul>
</li>



<li><a href="https://www.proquest.com/openview/4d7e41eb019558871327638cf5234990/1?pq-origsite=gscholar&amp;cbl=716332">Carstens et al. 2018</a> (n=209 American respondents to a survey)
<ul>
<li>Questions answered:
<ul>
<li><em>Is self-reported attention span related to the number of social media accounts?</em>, No, not statistically significant (F(2, 206)=0.1223, p&gt;0.05) (via a one-way<a href="https://en.wikipedia.org/wiki/ANOVA"> ANOVA</a>)</li>



<li><em>Is self-reported attention span related to whether a respondent mainly uses a mobile phone or a computer?</em>, No, not statistically significant (P(2,713)=0.923, p&gt;0.05) (via a one-way ANOVA)</li>
</ul>
</li>



<li>I do <strong>not</strong> trust this paper: Calling (what I think is) Generation Z ‚ÄúGeneration D‚Äù (without source for the term), being clearly written in Word, and confusing grammar (I <em>think</em> the authors are all Americans, so no excuse here):</li>
</ul>
</li>
</ul>



<blockquote>
<p>Users that are older such as late adolescents and emerging adults average approximately 30-minutes daily for just Facebook that does not calculate the time spent on all social media networks</p>




<cite><em>‚ÄîCarstens et al.,</em><a href="https://www.proquest.com/openview/4d7e41eb019558871327638cf5234990/1?pq-origsite=gscholar&amp;cbl=716332"><em> ‚ÄúSocial Media Impact on Attention Span‚Äù</em></a><em> p. 2, 2018</em></cite></blockquote>







<blockquote>
<p>Bakardjieva and Gaden (2012) examined the field of social interaction in general to the everyday chatter of unstructured and spontaneous interactions among individuals to highly structured and regulated interaction consisting of the military or the stock exchange.</p>




<cite><em>‚ÄîCarstens et al.,</em><a href="https://www.proquest.com/openview/4d7e41eb019558871327638cf5234990/1?pq-origsite=gscholar&amp;cbl=716332"><em> ‚ÄúSocial Media Impact on Attention Span‚Äù</em></a><em> p. 3, 2018</em></cite></blockquote>







<ul>
<li><a href="https://www.digitalinformationworld.com/2020/02/report-shows-that-attention-spans-are-shortening.html">Muhammad 2020</a>
<ul>
<li>Question answered: <em>How much time do people spend on a website, on average?</em>, ‚Äúif you look at the trend for mobile browsing between the years 2017 and 2019 you would see that there is a drop of about 11 seconds in the average time spent on a website.‚Äù and ‚ÄúThe data suggests that the average amount of time spent on websites before navigating away for all devices has gone down by 49 seconds which is a pretty huge reduction all things considered.‚Äù</li>



<li>The data is from the right timeframe (up to but not including 2020), but the linked<a href="https://www.similarweb.com/corp/reports/2020-digital-trends-lp/"> SimilarWeb report</a> is behind a paywall, so I can‚Äôt confirm the numbers. Furthermore, the time spent on websites is a weak proxy: Perhaps people simply have become better at prioritising information sources.</li>
</ul>
</li>



<li><a href="https://www.nature.com/articles/s41467-019-09311-w">Lorenz-Spreen et al. 2019</a>
<ul>
<li>Questions answered:
<ul>
<li><em>How long does any particular hashtag stay in the group of the top 50 most used hashtags? Specifically, how has that number developed from 2013 to 2016?</em>, ‚Äúin 2013 a hashtag stayed within the top 50 for 17.5 hours on average, a number which gradually decreases to 11.9 hours in 2016‚Äù, and ‚ÄúThe average maximum popularity ‚ü®L(tpeak)‚ü©</li>
</ul>
</li>
</ul>
</li>
</ul>



<p>on one day tpeak stays relatively constant, while the average gradients ‚ü®ŒîL‚ü©</p>



<ul>
<li>in positive and negative direction become steeper over the years.‚Äù</li>



<li><em>Do things become more popular faster over time? That is, when e.g. a movie is gaining popularity, did it take longer to become popular in 1985 than it did in 2018?</em>, Broadly yes (the trends holds for popularity of hashtags in tweets (2013-2016)/<a href="https://en.wikipedia.org/wiki/n-gram">n-grams</a> in books (1900-2004)/number of theaters that movies were screened in (1985-2018)/topics for search queries on Google (2010-2017)/Reddit comments on posts (2010-2015)/citations of publications (1990-2015)/daily traffic for Wikipedia articles (2012-2017)). Again the length of the time at the peak mostly didn‚Äôt change (except in the case of Wikipedia articles, where the time at the peak <em>shrunk</em>)</li>



<li>While it investigates a question different from the one I have, this paper seems good and trustworthy to me, while supporting a suspicion I‚Äôve had (observing that the lifecycle of e.g. memes has apparently sped up significantly). I‚Äôd be interested in seeing whether the same process holds for internet communities I‚Äôm part of (for example on votes<a href="https://www.lesswrong.com/"> LessWrong</a> and the<a href="https://forum.effectivealtruism.org/"> EA Forum</a> or forecasts on<a href="https://www.metaculus.com/"> Metaculus</a>).</li>
</ul>


<div>
<figure><img data-attachment-id="3324" data-permalink="https://slimemoldtimemold.com/2023/07/24/your-mystery-have-attention-spans-been-declining/image-8-3/" data-orig-file="https://slimemoldtimemold.files.wordpress.com/2023/07/image-8.png" data-orig-size="841,559" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-8" data-image-description="" data-image-caption="" data-medium-file="https://slimemoldtimemold.files.wordpress.com/2023/07/image-8.png?w=300" data-large-file="https://slimemoldtimemold.files.wordpress.com/2023/07/image-8.png?w=660" src="https://slimemoldtimemold.files.wordpress.com/2023/07/image-8.png" alt="Chart indicating how the speed at which hashtags become popular changed over the years. Four plots (yellow, green, blue and purple) which form a peak in the middle and fall off at the sides. The yellow line is highest around the peak, the green one is lower, blue even lower and purple the lowest." title="Chart indicating how the speed at which hashtags become popular changed over the years. Four plots \(yellow, green, blue and purple\) which form a peak in the middle and fall off at the sides. The yellow line is highest around the peak, the green one is lower, blue even lower and purple the lowest."></figure></div>






<p><a href="https://www.goodreads.com/en/book/show/60795084">Mark 2023</a> is a recent book about attention spans, which I was excited to read and find the important studies I‚Äôd missed. Unfortunately, it is quite thin on talking about the development of attention span over time. It states that</p>



<blockquote>
<p>My own research, as well as those of others, has shown that over the last fifteen years, our attention spans have declined in duration when we use our devices. Our attention spans while on our computers and smartphones have become short‚Äîcrazily short‚Äîas we now spend about forty-seven seconds on any screen on average.</p>




<cite><em>‚ÄîGloria Mark, ‚ÄúAttention Span‚Äù p. 13/14, 2023</em></cite></blockquote>







<p>which is not quite strong enough a measurement for me.</p>



<blockquote>
<p>In 2004, in our earliest study, we found that people averaged about one hundred fifty seconds (two and a half minutes) on a computer screen before switching their attention to another screen; in 2012, the average went down to seventy-five seconds before switching. In later years, from 2016 to 2021, the average amount of time on any screen before switching was found to be relatively consistent between forty-four and fifty seconds. Others replicated our results, also with computer logging. seconds. Others replicated our results, also with computer logging. Andr√© Meyer and colleagues at Microsoft Research found the average attention span of twenty software developers over eleven workdays to be fifty seconds.‚Åπ For her dissertation, my student Fatema Akbar found the average attention span of fifty office workers in various jobs over a period of three to four weeks to be a mere forty-four seconds.¬π‚Å∞ In other words, in the last several years, every day and all day in the workplace, people switch their attention on computer screens about every forty-seven seconds on average. In fact, in 2016 we found the median (i.e., midpoint) for length of attention duration to be forty seconds.¬π¬π This means that half the observations of attention length on any screen were shorter than forty seconds. </p>




<cite><em>‚ÄîGloria Mark, ‚ÄúAttention Span‚Äù p. 74/75, 2023</em></cite></blockquote>



<p><img width="555" height="356" src="https://lh5.googleusercontent.com/bdlZsd-IZ76TjJJ776QzZQ0Ftu2GLD2KxQWz-cXycXNPvdEJ-3PM25Cukktv8clkkVva7-s2vJGktWKQxh53AYLobCOVJ4DyZgXUFDrATpJ9GRPc-4OMa5mm_GcPGDo-AjBTqz-Nmh-y7yK0_d9uoaA"></p>







<p>She doesn‚Äôt mention the hypothesis that this could be the symptom of a higher ability to prioritize tasks, although she is adamant that multi-tasking is bad.</p>



<p>Furthermore, this behavior displays only a decrease in the <em>propensity</em> of attention, but not necessarily one of <em>capacity</em>: Perhaps people could concentrate more, if they wanted to/were incentivized to, but they don‚Äôt, because there is no strong intent to or reward for doing so. Admittedly, this is less of an argument in the workplace where these studies were conducted, but perhaps people just care not as much about their jobs (or so I‚Äôve heard).</p>



<blockquote>
<p>when email was cut off, people‚Äôs attention spans were significantly longer while working on their computers‚Äîin other words, they switched their attention less frequently.</p>




<cite><em>‚ÄîGloria Mark, ‚ÄúAttention Span‚Äù p. 97, 2023</em></cite></blockquote>







<p>She gives some useful statistics about time spent on screens:</p>



<blockquote>
<p>Nielsen reports that Americans spend on average five hours and thirty minutes daily of screen time on their computers, tablets and phones<a href="https://www.nielsen.com/us/en/insights/report/2021/total-audience-advertising-across-todays-media/"><sup>8</sup></a>. [‚Ä¶] But what is really astonishing is that when we add in the time watching other media like TV and films to this, then we see that our attention is fixated on some form of screen, in some type of mediated environment, nearly ten hours a day<a href="https://www.nielsen.com/us/en/insights/report/2021/total-audience-advertising-across-todays-media/"><sup>8</sup></a>.</p>




<cite><em>‚ÄîGloria Mark, ‚ÄúAttention Span‚Äù p. 180, 2023</em></cite></blockquote>







<p>She connects attention span to shot-length in movies:</p>



<blockquote>
<p>The type of motion within shots has been changing. According to film scholar James Cutting and his colleagues at Cornell, shots containing the onset of motion (like a standing person who then runs) have increased because filmmakers believe that it will better attract viewers‚Äô attention. [‚Ä¶] The average film shot length in 1930 was twelve seconds, but then began to shorten, reaching an average of less than four seconds after the year 2010, as measured by James Cutting and colleagues.<sup>12</sup> Interestingly, the shot length for film sequels also decreased. For example, the shot length of the first Iron Man film averaged about 3.7 seconds; for Iron Man 2, 3.0 seconds; and for Iron Man 3, about 2.4 seconds.<sup>13</sup></p>




<cite><em>‚ÄîGloria Mark, ‚ÄúAttention Span‚Äù p. 180/181, 2023</em></cite></blockquote>






<div>
<figure><img data-attachment-id="3322" data-permalink="https://slimemoldtimemold.com/2023/07/24/your-mystery-have-attention-spans-been-declining/image-7-3/" data-orig-file="https://slimemoldtimemold.files.wordpress.com/2023/07/image-7.png" data-orig-size="839,448" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-7" data-image-description="" data-image-caption="" data-medium-file="https://slimemoldtimemold.files.wordpress.com/2023/07/image-7.png?w=300" data-large-file="https://slimemoldtimemold.files.wordpress.com/2023/07/image-7.png?w=660" src="https://slimemoldtimemold.files.wordpress.com/2023/07/image-7.png" alt=""></figure></div>






<blockquote>
<p>Like in TV and film, shot lengths in television commercials also shortened over time. The average shot length of commercials in 1978 was 3.8 seconds, dropping down to an average of 2.3 seconds in 1991. [‚Ä¶] It‚Äôs not just the shot lengths, though, that are short‚Äîthe overall length of advertisements on TV has also decreased. The majority of ads started out as sixty seconds in length in the 1950s,<sup>26</sup> but that length comprised only 5 percent of ads shown in 2017. In the 1980s, advertisers started experimenting with showing fifteen-second ads instead of thirty-second ads. They discovered that fifteen seconds was even more persuasive than thirty seconds, especially when the ads used elements expressing cuteness and humor.<sup>27</sup> In 2014, 61 percent of ads were thirty seconds in length, but three years later, that percentage decreased to 49 percent.<sup>28</sup></p>




<cite><em>‚ÄîGloria Mark, ‚ÄúAttention Span‚Äù p. 189, 2023</em></cite></blockquote>











<h2><strong>Do People Believe Attention Spans Have Declined?</strong></h2>



<blockquote>
<p>Half of the public feel their attention span is shorter than it used to be, compared with around a quarter (23%) who believe they are just attentive [sic] as they‚Äôve always been.</p>



<p>Again, the feeling of is not just reported by the young ‚Äî it‚Äôs also the dominant feeling among the middle aged too, with 56% of 35- to 54-year-olds thinking their attention spans have worsened.</p>




<cite><em>‚ÄîBobby Duffy &amp; Marion Thain,</em><a href="https://www.kcl.ac.uk/policy-institute/assets/how-people-focus-and-live-in-the-modern-information-environment.pdf"><em> ‚ÄúDo we have your attention‚Äù</em></a><em> p. 6, 2022</em></cite></blockquote>







<blockquote>
<p>Even more widespread is the belief that young people‚Äôs attention spans in particular are worse than they were in the past‚Äîtwo-thirds of people think this is the case (66%).</p>



<p>Perhaps unsurprisingly, this belief is most common among the oldest age group surveyed, of those aged 55 or over ‚Äî however, young people themselves also feel this way, with a majority of 18- 34-year-olds holding this view.</p>




<cite><em>‚ÄîBobby Duffy &amp; Marion Thain,</em><a href="https://www.kcl.ac.uk/policy-institute/assets/how-people-focus-and-live-in-the-modern-information-environment.pdf"><em> ‚ÄúDo we have your attention‚Äù</em></a><em> p. 7, 2022</em></cite></blockquote>







<p>Note that<a href="https://pubmed.ncbi.nlm.nih.gov/7976468/"> selective attention mostly improves with age</a>, so the older age-groups might be comparing themselves now to the younger age groups now (as opposed to remembering back at their own attention spans).</p>



<blockquote>
<p>The absence of long-term research means it remains unknown whether technology has caused a deterioration in the country‚Äôs ability to concentrate ‚Äî but comparisons with survey data from previous decades indicate that, on some measures the public feel more pressured than they did in the past.</p>




<cite><em>‚ÄîBobby Duffy &amp; Marion Thain,</em><a href="https://www.kcl.ac.uk/policy-institute/assets/how-people-focus-and-live-in-the-modern-information-environment.pdf"><em> ‚ÄúDo we have your attention‚Äù</em></a><em> p. 18, 2022</em></cite></blockquote>







<p>In response to the questions (n=2093 UK adults aged 18+ in 2021):</p>



<ul>
<li>‚ÄúTo what extent do you agree or disagree with the following statement? <strong>The pace of life is too much for me these days</strong>‚Äù (1983: 30% agree, 2021: 41% agree)</li>



<li>‚ÄúTo what extent do you agree or disagree with the following statement? <strong>I wish I could slow down the pace of my life</strong>‚Äù (1997: 47% agree, 1999: 51% agree, 2008: 45% agree, 2021: 54% agree)</li>
</ul>







<h2><strong>What About Rates of ADHD?</strong></h2>



<p><a href="https://www.cdc.gov/ncbddd/adhd/timeline.html">Data from the CDC</a> shows a clear increase in the percentage of children with a parent-reported ADHD diagnosis:</p>


<div>
<figure><img data-attachment-id="3323" data-permalink="https://slimemoldtimemold.com/2023/07/24/your-mystery-have-attention-spans-been-declining/image-27/" data-orig-file="https://slimemoldtimemold.files.wordpress.com/2023/07/image.jpeg" data-orig-size="900,450" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://slimemoldtimemold.files.wordpress.com/2023/07/image.jpeg?w=300" data-large-file="https://slimemoldtimemold.files.wordpress.com/2023/07/image.jpeg?w=660" src="https://slimemoldtimemold.files.wordpress.com/2023/07/image.jpeg" alt=""></figure></div>






<p>There has been a similar increase in the diagnosis of<a href="https://www.psychologytoday.com/us/blog/balanced/202302/why-is-the-prevalence-of-adhd-increasing"> ADHD among adults</a>, ‚Äúfrom 0.43 to 0.96 percent‚Äù between 2007 and 2016.</p>



<p>However, this does not necessarily mean that the <em>rate</em> of ADHD has increased, if e.g. awareness of ADHD has increased and therefore leads to more diagnoses.</p>







<h2><strong>What Could A Study Look Like?</strong></h2>



<p>Compared to other feats that psychology is accomplishing, finding out whether individual attention spans are declining appears to be of medium difficulty, so I‚Äôll try to outline how this could be accomplished in three different ways:</p>



<ol>
<li>Develop a good instrument for measuring attention span (optionally just use a<a href="https://en.wikipedia.org/wiki/Continuous_performance_task"> continuous performance test</a>). Once one has a suitable instrument for measuring attention span, one can every year (or every second year) for a couple of years pick a random sample from the population (not of the same set of people, though, since attention span<a href="https://www.kcl.ac.uk/policy-institute/assets/how-people-focus-and-live-in-the-modern-information-environment.pdf"> increases with age</a>), e.g. via the internet if the test can be done online. One could then apply a<a href="https://en.wikipedia.org/wiki/Linear_trend_estimation"> linear trend estimation</a> or a fancier statistical technique I don‚Äôt know to find out whether attention spans have declined between the measurements.
<ol>
<li>This could be done relatively cheaply: Let‚Äôs say we collect 50 datapoints a year, from Mechanical Turk workers at $10/hr. A conservative estimate is that the test takes ~30 minutes to complete, so for three years the cost of the data would be 50‚ãÖ3‚ãÖ10$/h‚ãÖ0.5h=$750. It looks like there <a href="https://gitlab.pavlovia.org/demos/continuous_performance_test">are</a> <a href="https://github.com/search?q=continuous+performance+task">open-source implementations</a> <a href="https://github.com/NeuroanatomyAndConnectivity/ConjunctiveContinuousPerformanceTask">of the test</a> available (Conners‚Äô CPT 3 costs <a href="https://storefront.mhs.com/collections/conners-cpt-3">$1.5k</a>), so the additional cost is for the researcher setting up the test and recruiting the participants, which could take ~30 hours, and another ~30 hours for analysing the data. So the total cost of the experiment would be, at an hourly wage of $15 for the researcher (come on, we can let a grad student do it), $750+60h‚ãÖ15$/h=$1650</li>
</ol>
</li>



<li>. Fudging upwards by taking the<a href="https://en.wikipedia.org/wik/Planning_fallacy"> planning fallacy</a> into account gives $2k for the experiment.</li>
</ol>



<ol start="2">
<li>Find someone who <a href="https://www.braintrain.com/cpt/">has been collecting data on attention span</a>, ask them for it nicely, and analyse that data.</li>



<li>Use the<a href="https://en.wikipedia.org/wiki/Control_group"> control groups</a> from studies testing the effect of interventions on attention as data and then perform a<a href="https://en.wikipedia.org/wiki/Meta-analysis"> meta-analysis</a>. A lot of studies use some variant of the CPT, I started collecting such studies in Appendix B.</li>
</ol>







<h2><strong>Conclusion</strong></h2>



<p>Given the amount of interest the question about shrinking attention spans has received, I was surprised to not find a knockdown study of the type I was looking for, and instead many different investigations that were either not <em>quite</em> answering the question I was asking or too shoddy (or murky) to be trusted. It seems likely to me that individual attention spans have declined (I‚Äôd give it ~70%), but I wouldn‚Äôt be surprised if the decline was relatively small, noisy &amp; dependent on specific tests.</p>



<p>So‚Äîwhy hasn‚Äôt anyone investigated this question to satisfaction yet? After all, it doesn‚Äôt seem to me to be extremely difficult to do (compared to<a href="https://en.wikipedia.org/wiki/Large_Hadron_Collider"> other</a><a href="https://en.wikipedia.org/wiki/Smallpox#Eradication"> things</a><a href="https://en.wikipedia.org/wiki/Apollo_11"> science</a><a href="https://en.wikipedia.org/wiki/Dolly_(sheep)"> has</a><a href="https://en.wikipedia.org/wiki/AlphaFold"> accomplished</a>), there is pretty clearly a lot of media attention on the question (so much so that<a href="https://www.bbc.com/news/health-38896790"> a likely incorrect number proliferates</a> far &amp; wide), it appears economically and strategically relevant to me (especially sustained attention is <em>probably</em> an important factor in knowledge work, I‚Äôd guess?) and it slots more or less into cognitive psychology.</p>



<p>I‚Äôm not sure why this hasn‚Äôt happened yet (and consider this text evidence for a partial violation of<a href="https://marginalrevolution.com/marginalrevolution/2015/04/tyler-cowens-three-laws.html"> Cowen‚Äôs 2nd law</a>‚Äîalthough, to be fair, the law doesn‚Äôt specify there needs to be a <em>good</em> literature on everything‚Ä¶). The reasons I can think of is that one would need to first develop a good test for determining attention span, which is some work in itself (or use the CPT); be relatively patient (since the test would need to be re-run at least twice with a &gt;1 year pause, for which the best grant structure might not exist); there are many partial investigations into the topic, making it appear like it‚Äôs solved; and perhaps there just aren‚Äôt enough cognitive psychologists around to investigate all the interesting questions that come up.</p>



<p>So I want to end with a call to action: If you have the capacity to study this problem, there is room for improvement in the existing literature! Attention spans could be important, it‚Äôs probably not hard to measure them, and many people claim that they‚Äôre declining, but are way too confident about it given the state of the evidence. False numbers are widely circulated, meaning that correct numbers might be cited even more widely. And it‚Äôs probably not even (that) hard!</p>



<p>Consider your incentives :-).</p>







<h2><strong>Appendix A: Claims That Attention Spans Have Been Declining</strong></h2>



<p>Most of these are either unsourced or cite<a href="https://dl.motamem.org/microsoft-attention-spans-research-report.pdf"> Gausby 2015</a><a href="https://www.bbc.com/news/health-38896790"> fallaciously</a> (which<a href="https://www.archemedx.com/learning-resources/learning-science/bradbury-attention-span-during-lectures-8-seconds-10-minutes-or-more/"> Bradbury 2016</a> conjectures to be the number of seconds spent on websites on average).</p>



<blockquote>
<p>Today, individuals are constantly on an information overload from both the quantity of information available and the speed of which information gets into the hands of individuals through advertising and multimedia. Attention deficits tend to be increasing as it is challenging to attract individuals and hold their attention long enough for people to read or watch messages such as work memos, advertisements, etc.</p>
<cite><em>‚ÄîCarstens et al.,</em><a href="https://www.proquest.com/openview/4d7e41eb019558871327638cf5234990/1?pq-origsite=gscholar&amp;cbl=716332"><em> ‚ÄúSocial Media Impact on Attention Span‚Äù</em></a><em> p. 2, 2018</em></cite></blockquote>







<blockquote>
<p>Big data plays an important role in the development of microlearning. In the age of big data, human‚Äôs attention span is decreasing. As per Hebert (1971), ‚Äúwhat information consumes is rather obvious: it consumes the attention of its recipients. Hence a wealth of information creates a poverty of attention and a need to allocate that attention efficiently among the overabundance of information sources that might consume it‚Äù (p. 41). An example of short attention span in the age of big data can be found in the music industry, as per (Gauvin, 2017), the average time that passed before the audience would hear the vocals on any radio song was 23 s, today the average intro is just 5 s long. Wertz (2017) also suggested that 40% of users are likely to abandon a website if it does not load within three seconds or less. Furthermore, a survey (Gausby, 2015) conducted by Microsoft indicated that the average attention span of a human dropped from 12 to eight seconds, which means shorter than a goldfish. Given the average human attention span is decreasing, microlearning becomes more and more important because it emphasises short learning duration.</p>
<cite><em>‚ÄîLeong et al., </em><a href="https://www.emerald.com/insight/content/doi/10.1108/JWAM-10-2020-0044/full/html"><em>‚ÄúA review of the trend of microlearning‚Äù</em></a><em> p. 2, 2020</em></cite></blockquote>







<blockquote>
<p>Unfortunately, all too many of us are having ‚Äúsquirrel‚Äù days, according to Dr. Gloria Mark, a professor of informatics at the University of California, Irvine, who studies how digital media impacts our lives. In her new book,<a href="https://www.amazon.com/Attention-Span-Finding-Fighting-Distraction/dp/1335449418"> ‚ÄúAttention Span: A Groundbreaking Way to Restore Balance, Happiness and Productivity,‚Äù</a> Mark explained how decades of research has tracked the decline of the ability to focus.</p>



<p>‚ÄúIn 2004, we measured the average attention on a screen to be 2¬Ω minutes,‚Äù Mark said. ‚ÄúSome years later, we found attention spans to be about 75 seconds. Now we find people can only pay attention to one screen for an average of 47 seconds.‚Äù</p>



<p>Not only do people concentrate for less than a minute on any one screen, Mark said, but when attention is diverted from an active work project, it also takes about 25 minutes to refocus on that task.</p>
<cite><em>‚ÄîSandee LaMotte,</em><a href="https://edition.cnn.com/2023/01/11/health/short-attention-span-wellness/index.html"><em> ‚ÄúYour attention span is shrinking, studies say. Here‚Äôs how to stay focused‚Äù</em></a><em>, 2023</em></cite></blockquote>







<blockquote>
<p>Tech-savvy users often say that the way the modern internet works has made it so that people‚Äôs attention spans are getting shorter every single day but the truth behind this story is rather tough to ascertain. However, recent data from<a href="https://www.similarweb.com/corp/reports/2020-digital-trends-lp/"> SimilarWeb</a> indicates that people definitely are suffering from shorter attention spans, and what‚Äôs more is that these attention spans are shortening at a pretty rapid pace when you take into account the numerous factors that are coming into play, all of which serve some kind of purpose in this trend.</p>



<p>If you look at the data for how long users spend on websites before navigating away, for the most part the trend has been that these times are remaining more or less stable on web based browsing, but if you look at the trend for mobile browsing between the years 2017 and 2019 you would see that there is a drop of about 11 seconds in the average time spent on a website. When you take into account the fact that mobile browsing is starting to become a lot more popular and in many ways has become the preferred form of browsing for people on the internet, the change is a lot more drastic.</p>
<cite><em>‚ÄîZia Muhammad,</em><a href="https://www.digitalinformationworld.com/2020/02/report-shows-that-attention-spans-are-shortening.html"><em> ‚ÄúResearch Indicates That Attention Spans Are Shortening‚Äù</em></a><em>, 2020</em></cite></blockquote>







<blockquote>
<p>However, as much as technology can be used as an effective learning tool inside and outside the classroom, there‚Äôs no denying that one of the biggest challenges faced by educators today is the distraction posed by social media. <strong>Students are distracted by their phones during class, and even without that distraction, the time they spend on social media outside the classroom has an impact on their attention spans</strong>.</p>
<cite><em>‚ÄîEU Business School,</em><a href="https://www.euruni.edu/blog/the-truth-about-decreasing-attention-spans-in-university-students/"><em> ‚ÄúThe Truth about Decreasing Attention Spans in University Students‚Äù</em></a><em>, 2022</em></cite></blockquote>







<p>(No link given.)</p>



<blockquote>
<p>In 2015, a study commissioned by Microsoft and discussed in Time magazine found that the average attention span was in fact only 8 s. If indeed this is the case, then even participating in a 15-min lecture would be positively heroic. To place this in perspective, it was reported in the same Time article, that goldfish, of the piscine rather than snack variety, have an attention span of 9 s, one whole second greater than humans! It is perhaps rather premature to opt for an 8-s lecture format, as there are many caveats to the Time article, not the least of which is that no one knows how to actually measure a goldfish‚Äôs attention span. What has been measured is goldfish memory, which, according to researchers in the School of Psychology at the University of Plymouth, is actually quite good (7). Similarly the 8-s attention span for humans actually reflects the average time a person will spend on a web page before looking somewhere else.</p>
<cite><em>‚ÄîNeil A. Bradbury,</em><a href="https://www.archemedx.com/learning-resources/learning-science/bradbury-attention-span-during-lectures-8-seconds-10-minutes-or-more/"><em> ‚ÄúAttention span during lectures: 8 seconds, 10 minutes, or more?‚Äù</em></a><em>, 2016</em></cite></blockquote>











<h2><strong>Appendix B: Studies for a Meta-Analysis</strong></h2>



<p>I‚Äôll list the closest thing those studies have to a control group, list sorted by year.</p>



<h3><strong>Studies Using the CPT</strong></h3>



<ul>
<li><a href="https://academic.oup.com/schizophreniabulletin/article-pdf/22/4/643/5462174/22-4-643.pdf">What Determines Continuous Performance Task Performance?</a> (Maria J. O. van Asma/Ren√© P. Rombouts/Robert J. van den Bosch, 1996): Patient controls n=19 (mean age 29.5¬±8.1 years), normal controls n=20 (mean age 32.2¬±8.3 years). Tested with an unspecified digit related <em>CPT</em>.</li>



<li><a href="https://academic.oup.com/schizophreniabulletin/article-pdf/24/1/163/5309286/24-1-163.pdf">Performance of the Continuous Performance Test Among Community Samples</a> (Chuhsing K. Hsiao/Hai-Gwo Hwu/Li-Ling Hsiao/Wei J. Chen, 1998): n=115 students (aged 14¬±0.8 years) and n=345 adults (aged 41.3¬±13 years), randomly selected, from the<a href="https://en.wikipedia.org/wiki/Jinshan_District,_New_Taipei"> Jinshan District</a>. Tested using the <em>CPT 1-9</em>, both degraded and undegraded.</li>



<li><a href="https://www.academia.edu/download/47902451/s0924-9338_2899_2980711-120160808-9487-4azvq5.pdf">Decreased frontal activation in schizophrenics during stimulation with the Continuous Performance Test ‚Äî a functional magnetic resonance imaging study</a> (H.-P. Volz/C. Gaser/F. H√§ger/R. Rzanny/J. P√∂nisch/H.-J. Mentzel/W.A. Kaiser/H. Sauer, 1999): n=20 German volunteers (28.2¬±5.7 years), tested using the <em>CPT-double-T-version</em>.</li>



<li><a href="https://www.researchgate.net/profile/John-Klaric/publication/304425329_Continuous_Performance_Test_Performance_in_a_Normative_Epidemiological_Sample/links/598261b9a6fdcc8b56f57e5c/Continuous-Performance-Test-Performance-in-a-Normative-Epidemiological-Sample.pdf">Continuous Performance Test Performance in a Normative Epidemiological</a> Sample (C. Keith Conners/Jeffery N. Epstein/Adrian Angold/John Klaric, 2002): n=816 children from North Carolina (9-17 years), tested using <em>Conners‚Äô CPT</em>.</li>



<li><a href="https://www.researchgate.net/profile/Elizabeth-Costello/publication/9049493_Relations_Between_Continuous_Performance_Test_Performance_Measures_and_ADHD_Behaviors/links/551ac1520cf2fdce84373c26/Relations-Between-Continuous-Performance-Test-Performance-Measures-and-ADHD-Behaviors.pdf">Relations Between Continuous Performance Test Performance Measures and ADHD Behaviors</a> (Jeffery N. Epstein/Alaatin Erkanli/C. Keith Conners/John Klaric/Jane E. Costello/Adrian Angold, 2003): Epidemiological sample n=817 North Carolina children. Administered the Conners‚Äô CPT.</li>



<li><a href="https://www.academia.edu/download/54248709/j.ics.2004.04.03220170825-21101-1dvtxr7.pdf">Longitudinal change of ERP during cued continuous performance test in child with attention-deficit/hyperactivity disorder</a> (Hisaki Ozaki/Hisao Maekawa/Satoshi Futakami/Shinji Okazaki, 2004): n=1 (yes, really), male child with ADHD, measurements at 9.6/10.6/11.6 years old with and without medication, tested using the <em>CPT-AX</em>.</li>



<li><a href="https://labs.psych.ucsb.edu/schooler/jonathan/sites/labs.psych.ucsb.edu.schooler.jonathan/files/pubs/stream_of_consciousness.pdf">Segmenting the stream of consciousness: The psychological correlates of temporal structures in the time series data of a continuous performance task</a> (Jonathan Smallwood/Merrill McSpadden/Bryan Luus/Jonathan Schooler, 2007): n=23 Canadian undergraduate students (aged 19-22). Tested using an unspecified <em>CPT</em> with the task of recognising specific digits as target stimuli. Probably not useful.</li>



<li><a href="https://www.pnas.org/content/106/37/15583.full/">Cognitive control in media multitaskers</a> (Anthony D. Wagner/Clifford Nass/Eyal Ophir, 2009): n=28 (?), tested with the <em>CPT-AX</em>.</li>



<li><a href="https://journals.sagepub.com/doi/pdf/10.1177/1087054708323019?casa_token=oYYJVOVBfmwAAAAA:QUTP5af1t1LHYdCmJoIvsIc_yDTX8l8L5QnL1-x9miVWEbwM02W-4fgpq4yGofQlF2zl5sh4-YgpXek">Measuring Several Aspects of Attention in One Test</a> (Iwona Kovalik-Gran/Jens Egeland, 2010): n=376 Norwegian patients, aged 14-77 (mean 32.9 years, standard deviation 13.8), either referred to Egeland or to the Vestfold Mental Health Care Trust, with various psychological disorders (57 without such disorders). Tested using <em>Conners‚Äô CPT</em>. Scores for normal group not reported independently.</li>



<li><a href="https://bktimes.net/data/board_notice/1340624308-42.pdf">Association between urine cotinine levels, continuous performance test variables, and attention deficit hyperactivity disorder and learning disability symptoms in school-aged children</a> (B.-N. Kim/E.-J. Park/H.-J.Yoo/I.-H. Cho/J.-H. Lee/J. Hur/J.-W. Kim/M.-H. Park/M.-S. Shin/S.-B. Hong/S.-C. Cho/S.-K. Han/S. Park/S.-Y. Bhang/Y.-C. Hong, 2012): n=989 Korean children (mean age 9.1¬±0.7 years), tested with an unspecified <em>CPT</em>.</li>



<li><a href="https://link.springer.com/article/10.3758/s13414-012-0413-x">Sustaining visual attention in the face of distraction: a novel gradual-onset continuous performance task</a> (Joseph DeGutis/Michael Esterman/Monica Rosenberg/Sarah Noonan, 2013): n=29 healthy US-American participants (18-26 years old), tested with the therein developed <em>gradCPT</em>.</li>



<li><a href="https://journals.sagepub.com/doi/pdf/10.1177/0956797615594896?casa_token=_fGih5qwpdYAAAAA:PogTmgY1Kg2L-glPuM8RXl_4VNhfoESvGBYDWhWO0OznTktyHaFEpUqkwYawh8JotSdChvFdhFrHHKQ">Sustained attention across the lifespan in a sample of 10,000: Dissociating ability and strategy</a> (Francesca C. Fortenbaugh/Jeremy Wilmer/Joseph DeGutes/Kathryn Russo/Laura Germine/Mallory Grosso/Michael Esterman, 2015): n=10430 random internet sample, aged 10 to 70 (mean 26.07¬±11.77 years). Tested using the <em>gradCPT</em>.</li>



<li><a href="https://documentserver.uhasselt.be/bitstream/1942/22633/1/main-article-cognac.pdf">Recent <em>versus</em> chronic exposure to particulate matter air pollution in association with neurobehavioral performance in a panel study of primary schoolchildren</a> (Charlotte Vanpoucke/Eline B. Provost/Harry A. Roels/Karen Vrijens/Mineke K. Viaene/Nelly D. Saenen/Tim S. Nawrot/Wouter Lefebvre, 2016): n=310 Belgian school children (aged 10.2¬±1.3 years), tested using an unspecified <em>CPT</em> (reporting 593¬±51.2 msec as the outcome of the test (?)).</li>



<li><a href="https://link.springer.com/article/10.1186/s12888-016-0993-4">Linear and non-linear analyses of Conner‚Äôs Continuous Performance Test-II discriminate adult patients with attention deficit hyperactivity disorder from patients with mood and anxiety disorders</a> (Anita L. Hansen/Jan √òystein Berle/Ketil J. Oedegaard/Kristin Mjeldheim/Ole Bernt Fasmer/Vigdis Elin Gi√¶ver Syrstad/Wenche F√∏rland, 2016): n=99 adult (aged 18-65 years) Norwegian patients in need of a diagnostic evaluation of ADHD, mood or anxiety disorders, tested with <em>Conners‚Äô CPT II</em>.</li>



<li><a href="https://www.researchgate.net/profile/Esteban-Vaucheret-Paz/publication/323374989_Continuous_performance_test_in_children_with_intellectual_disability_and_attention_deficit_hyperactivity_disorder/links/61a50d90ee3e086e3d3a72a8/Continuous-performance-test-in-children-with-intellectual-disability-and-attention-deficit-hyperactivity-disorder.pdf">Continuous performance test in children with intellectual disability and attention deficit hyperactivity disorder</a> (Puga Mar√≠a Celeste/Vaucheret Paz Esteban/Leist Mariana/Garc√≠a Basalo Mar√≠a Jos√©/Baliarda Florencia/Ekonen Christy/Lascombes Mar√≠a IVsabel/Agostau Guillermo, 2018): n=122 Argentinian children (mean age 10 years) with ADHD and IQ&gt;80, tested with the <em>CPT II</em>.</li>



<li><a href="https://www.tandfonline.com/doi/abs/10.1080/23279095.2019.1570199">Reliability and validity of the Conners‚Äô Continuous Performance Test</a> (Danielle Shaked/Lauren M. D. Faulkner/Kathryn Tolle/Carrington R. Wendell/Shari R. Waldstein/Robert J. Spencer, 2019): n=91 undergraduate psychology students (20.01¬±1.68 years), tested using <em>Conners‚Äô CPT II</em>. Page 3 lists more retest studies for Conners‚Äô CPT II.</li>



<li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8244585/">Poor Sleep Hygiene is Associated with Decreased Discrimination and Inattention on Continuous Performance Task in Doctor of Physical Therapy Students: A Cross-sectional Study</a> (Catherine F Siengsukon/Stacy Coffyn, 2020): n=50 Kansas students of Physical Therapy (age 24.18¬±1.6 years), tested with <em>Conners CPT 3</em>. (Raw CPT results not reported as far as I can tell, therefore probably useless :-|. But perhaps the authors can be contacted for the raw data‚Ä¶)</li>



<li><a href="https://link.springer.com/article/10.1186/s41235-021-00303-3">Do concerns about COVID-19 impair sustained attention</a>? (Jihyang Jun/Yi Ni Toh/Caitlin A. Sisk/Roger W. Remington/Vanessa G. Lee, 2021): n=161 participants (23¬±5.2 years), recruited online. No control group, but instead a correlational study. Tested using the <em>scene CPT</em>.</li>



<li><a href="https://journals.sagepub.com/doi/pdf/10.1177/1087054719829822?casa_token=hEKiWfMs9QkAAAAA:9u3lUp2R-Pu7v9pKLVHzkXpDiZRegEYVvUQMdBl8ATuVPAlQPoNUWRDYnbJ5bZfID1k5V6rzEpX_rjc">Individual Variability in Reaction Time and Prediction of Clinical Response to Methyphenidate in Adult ADHD: A Prospective Open Label Study Using Conners‚Äô Continuous Performance Test II</a> (Jan Haavik/Jens Egeladn/Mats Fredriksen/Ole Bernt Fasmer, 2021): n=123 Norwegian participants with ADHD, tested using <em>Conners‚Äô CPT II</em>.</li>
</ul>



<p>Furthermore, ‚Äú<a href="https://acamh.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-7610.1993.tb01784.x?casa_token=2lrnDpDqclUAAAAA:OYObW0UahohP9yeg0XI8QDDWGO64t7oZEbPdynEkA_8J1HgD2XwWETWpFppQzyuBGRx5LXok7WZDn0xfFA">Is the Continuous Performance Task a Valuable Research Tool for use with Children with Attention-Deficit-Hyperactivity Disorder</a>‚Äù (Linda S. Siegel/Penny V. Corkum, 1993) p. 8-9 contains references to several studies from before 1993 using the CPT on children with ADHD.</p>







<h2><strong>Appendix C: How I Believed One Might Measure Attention Span Before I Found Out About The CPT</strong></h2>



<p>Before I found out about the Continuous Performance Test, I speculated about how to measure attention span:</p>



<p>(Note that I‚Äôm not a psychometrician, but I like speculating about things, so the ideas below might contain subtle and glaring mistakes. Noting them down here anyway because I might want to implement them at some point.)</p>



<p>It seems relatively easy to measure attention span with a power- or speed-test, via one of three methods:</p>



<ol>
<li>Present a stimulus, change the stimulus and let the test subject report the change in stimulus; this results in two numbers: the time between the stimulus being initially being presented and the time it was changed (let‚Äôs call this value t_change), and the time between the change of the stimulus and the reporting of the change (calling this value t_report). Performing this test with different value of t_change should result in different values of t_report. There is a t_change for which t_report falls over a threshold value, that t_change can be called the attention span.
<ol>
<li>This method has some disadvantages:
<ol>
<li>It needs a change in stimulus that requires selective attention to notice, but changing e.g. visual stimuli involves motion, which direct attention. (Idea: have a colored stimulus continuously changing color, and a reference color, once the stimulus has the reference color, the subject is supposed to report; avoiding sudden changes in visual stimuli.)</li>



<li>The method would require many samples to find the t_change for which t_report falls over the threshold value.</li>



<li>Performing the test multiple times in a row might induce mental fatigue, decreasing attention span</li>
</ol>
</li>
</ol>
</li>



<li>Let the test subject engage in a mentally draining exercise like the<a href="https://en.wikipedia.org/wiki/Stroop_test"> Stroop test</a> with some performance measure. I would the performance to decline over time, and one could define a threshold value at which the subject ‚Äúis no longer paying attention‚Äù.</li>



<li>Let the subject observe a neutral stimulus while measuring some indicator of attention (such as arousal via skin conductivity or the<a href="https://en.wikipedia.org/wiki/Default_mode_network"> default mode network</a> being inactive), when the measured value falls under/over a threshold the subject has ‚Äúlost attention‚Äù.
<ol>
<li>This method has the major disadvantage that it requires special equipment to perform.</li>
</ol>
</li>
</ol>



<p>Such an instrument would of course need to have different forms of<a href="https://en.wikipedia.org/wiki/Reliability_(psychometrics)"> reliability</a> and<a href="https://en.wikipedia.org/wiki/Validity_(psychometrics)"> validity</a>, and I think it would probably work best as a power test or a speed test.</p>



<p>I‚Äôm not sure how such a test would relate to standard<a href="https://en.wikipedia.org/wiki/Intelligence_quotient"> IQ tests</a>: would it simply measure a subpart of <em>g</em>, completely independent or just partially related to it?</p>
			</div><!-- .entry-content -->

</article><!-- #post-## -->

				<!-- .navigation -->
	
			
<!-- #comments -->

		
		</main><!-- #main -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Vision Pro developer kit (158 pts)]]></title>
            <link>https://developer.apple.com/visionos/developer-kit/</link>
            <guid>36851535</guid>
            <pubDate>Mon, 24 Jul 2023 17:36:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.apple.com/visionos/developer-kit/">https://developer.apple.com/visionos/developer-kit/</a>, See on <a href="https://news.ycombinator.com/item?id=36851535">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main" role="main">

		<section>
			<div>

						<h2>Apple&nbsp;Vision&nbsp;Pro developer&nbsp;kit</h2>
						<p>Have an innovative idea for an app or game for visionOS that requires building and testing on Vision&nbsp;Pro? Apply for a Vision&nbsp;Pro developer kit. This kit will help you deliver amazing spatial experiences by letting you quickly build, iterate, and&nbsp;test&nbsp;on&nbsp;Vision&nbsp;Pro.</p>
						
					</div>
			<div>
					<picture>
						<source srcset="https://developer.apple.com/visionos/developer-kit/images/vision-side_2x.webp" type="image/webp">
						<img src="https://developer.apple.com/visionos/developer-kit/images/vision-side_2x.png" width="100%" alt="">
					</picture>
				</div>
		</section>

		<div>

						<h3>How it works</h3>
						<p>We‚Äôll loan you a Vision&nbsp;Pro developer kit to prepare your app for the launch of the new App&nbsp;Store on Vision&nbsp;Pro.</p>
						<p>You‚Äôll also receive:</p>
						<ul>
							<li>Help setting up the device and onboarding.</li>
							<li>Check-ins with Apple experts for UI design and development guidance, and help refining your app.</li>
							<li>Two additional code-level support requests, so you can troubleshoot any issues with your code.</li>
						</ul>
						<p>This Apple-owned development device needs to be returned upon request.</p>

						<h3>How to apply</h3>
						<p>Submit a brief application for a Vision&nbsp;Pro developer kit. You‚Äôll need to be an Account&nbsp;Holder in the Apple&nbsp;Developer Program, provide details about your team‚Äôs development skills and existing apps, and agree to the terms and conditions. Applications will be reviewed and priority will be given to applicants creating an app that takes advantage of visionOS features and&nbsp;capabilities.</p>
						
						
					</div>
		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Transformer Attention is off by one (851 pts)]]></title>
            <link>https://www.evanmiller.org/attention-is-off-by-one.html</link>
            <guid>36851494</guid>
            <pubDate>Mon, 24 Jul 2023 17:33:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.evanmiller.org/attention-is-off-by-one.html">https://www.evanmiller.org/attention-is-off-by-one.html</a>, See on <a href="https://news.ycombinator.com/item?id=36851494">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">

<p>By <a href="https://www.evanmiller.org/">Evan Miller</a></p>

<p><em>July 24, 2023</em></p>





<blockquote>
    <hr>
<p>
About which one cannot speak, one must pass over in silence. ‚ÄìWittgenstein
</p>

    <hr>
</blockquote>

<p>
Do you see the off-by-one error in this formula?
</p><p>

\[
\textrm{Attention}(Q, K, V) = \textrm{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
\]

</p><p>
The attention formula is the central equation of modern AI,
but there‚Äôs a bug in it that has been driving me nuts the last week. I tried
writing a serious-looking research paper about the bug and my proposed fix, but
I lost a series of pitched battles against Pytorch and <code>biblatex</code>,
so I figured I‚Äôd just write a blog post instead. (History is written by the
winners; blogs are written by‚Ä¶)
</p>

<p>
In this post I‚Äôll explain how (IMHO) the current generation of AI models have
an off-by-one error in a crucial place, and it‚Äôs making everyone‚Äôs Transformer
models needlessly difficult to compress and deploy. Consider this an opinion
piece, but if anyone out there on the Internet wants to run some experiments and
prove me right, we can collaborate and call ourselves scientists.
</p>

<h2>
It‚Äôs All About Outliers
</h2>

<p>
First let‚Äôs talk about why this off-by-one error matters. <em>ChatGPT works
great, what‚Äôs your problem bro?</em> I first caught the scent of something
amiss whilst minding my business and perusing research papers on quantization,
the technique by which LLM Edgers <a href="https://justine.lol/mmap/">cram</a>
big models onto Mac Minis and <a href="https://github.com/ggerganov/llama.cpp/issues/2164">Rasberry Pis</a> and
jailbroken home thermostats. 
    Everybody in AI is limited by
    RAM, so the less RAM you use the more cool stuff you can do, both <a href="https://arxiv.org/abs/2211.05102">Up
        There In The Cloud</a> and down here on the edge. LLMs have billions of weights,
and if we can make these weights tighten their haunches and suck in their
paunches, we can compose better sonnets or plagiarize superior essays or
accelerate the end of days, whatever your personal motivation for using
language may be.
</p>

<p>
RAM stores information. This sounds like a tautology, but hang with me.
<a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Information is negative log-probability</a>,
and is how many bits we need to store things. If a stream of numbers is highly
predictable, for example is always contained in a limited range, we need fewer
bits to store them. If a stream of numbers is not predictable, like once in a
blue moon a mega-number shows up, we need more binary digits to encode the
Colossus.
</p>

<p>
This is what‚Äôs been happening in LLMs ‚Äì for reasons that are only partially
understood, Transformer models contain these outlier weights and are
emitting Black Swan mega-activations that are much, much, much larger, like
orders of magnitude larger, than their peers. <em>But no one can get rid of them</em>;
the megalodons seem to be critical to the operation of these models, and their
existence is contrary to everything we thought we knew about neural networks
prior to building ones that worked so well. <a href="https://arxiv.org/abs/2105.06990">Many</a> <a href="https://arxiv.org/abs/2109.12948">papers</a> have been written about
these outlier values, and people have been <a href="https://arxiv.org/abs/1909.05840">cooking</a> <a href="https://arxiv.org/abs/2101.01321">up</a> <a href="https://arxiv.org/abs/2208.07339">all</a> <a href="https://arxiv.org/abs/2211.10438">kinds</a> <a href="https://arxiv.org/abs/1910.06188">of</a> bit-burning
schemes to encode them with fewer ones and zeroes, because right now we get
pretty gnarly performance degradation with vanilla scale-and-bias integer
quantization. <a href="https://github.com/ggerganov">Georgi</a> can tell you more; I‚Äôm getting ahead of myself.
</p>

<p>
The best analysis of all of this comes from a team at Qualcomm AI Research in this paper:
<strong><a href="https://arxiv.org/abs/2306.12929">Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing</a></strong>. The authors traced the existence of these outlier values to the
attention mechanism‚Äôs 
    softmax function,
the seemingly innocent exponentiator that no one thought capable of such
kurtotic barbarities. The researchers came <em>this close</em> to finding the
off-by-one error, like killer-in-the-closet close, but they must all be on
summer vacation in Italy as none of them are responding to my email overtures, and so I
must appeal to the international community of scholars the old-fashioned way.
</p>

<p>
If you read the linked paper, just ignore their proposals. Sounds harsh, but
hear me out. The clipped softmax comes with a wheel-spinning zero gradient, and
their gated attention proposal, while workable, introduces millions of new
parameters to solve what is really just a failure to increment. There‚Äôs a
simple and hindsight-obvious solution here that, from all of my reading, no one
has thought to try.
</p>

<p>
All right, I‚Äôve talked up this silly error enough. I‚Äôve alluded to its
location. Let‚Äôs talk about the 
    <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax function</a>
and why it‚Äôs
not-quite-the-right tool for the job when it comes to attention.
</p>


<h2>
The Trouble With Softmax
</h2>

<p>
Now to explain the bug you really have to understand what the attention
mechanism is trying to do. Most numerical bugs are programmers implementing
equations incorrectly. However, when you‚Äôre dealing not with bad code, but with
bad math, you need to understand where that equation comes from and what it‚Äôs
supposed to be doing before you have any hope of fixing it.
</p>

<p>
I had to read like 50 arXiV papers to understand all this, and probably I
should have taken a Udemy course or binge-watched Andrej Karpathy‚Äôs YouTube
<a href="https://www.youtube.com/c/AndrejKarpathy">channel</a> instead, but
let‚Äôs start with what is called the input embedding, which is a floating-point
vector that represents a word in the input string.
</p>

<p>
This vector seems to get taller every model year, for example the recent <a href="https://ai.meta.com/llama/">LLaMA
    2 model from Meta</a> uses an embedding vector of length 3,204, which works out to
6KB+ in half-precision floating-point, just to represent <em>one word</em> in
the vocabulary, which typically contains 30,000 - 50,000 entries.
</p>

<p>
Now if you‚Äôre a memory-miserly C programmer <a href="https://www.evanmiller.org/you-cant-dig-upwards.html">like me</a>, you might wonder, why in
the world are these AI goobers using 6KB to represent something that ought to
take, like 2 bytes tops? If their vocabulary is less than \(2^{16}\)=65,384, we
only need 16 bits to represent an entry, yeah?
</p>

<p>
Well, here is what the Transformer is actually doing: it <em>transforms</em>
(eh?) that input vector to an output vector <em>of the same size</em>, and that
final 6KB output vector needs to encode <em>absolutely everything needed to
predict the token after the current one</em>. The job of each layer of the
Transformer is quite literally adding information to the original, single-word
vector. This is where the residual (n√©e skip) connections come in: all of the
attention machinery is just adding supplementary material to that original two
bytes‚Äô worth of information, analyzing the larger context to indicate, for
instance, that the word <em>pupil</em> is referring to a student, and not to the
hole in your eye. Repeat the attention mechanism a few dozen times and you‚Äôve mastered
the English language and all its vasty contents.
</p>

<p>
Now the final step of Transformer is to multiply this output vector by a
rectangular matrix, and cram the resulting vocabulary-length vector into a
softmax, treating those exponentiated outputs as next-token probabilities. This
is reasonable, but everyone knows it‚Äôs not quite right, as no sane and
respected model out there regards those output probabilities as correct, but
instead every implementation and its sister uses a <a href="https://arxiv.org/abs/2007.14966">sampling mechanism</a> to hide
the fact that softmax is over-representing low probabilities.
</p>

<p>
That‚Äôs all fine and workable; softmax in the output step gives us a gradient
for every word in the vocabulary, and it‚Äôs a reasonable choice until something
better comes along.
</p>

<p>
But what I want to argue is that sauce for the goose shouldn‚Äôt be slathered on
the gander; the Transformer‚Äôs <em>output</em> softmax serves a very different
purpose from the attention mechanism‚Äôs <em>internal</em> softmax, and we‚Äôd all
do well to get rid of the latter, or at least prop up its denominator with something
handy ‚õ±Ô∏è.
</p>

<h3>An exponential peg‚Ä¶</h3>

<p>
So what is softmax? It started out in statistical mechanics as a way to
predict the distribution of states based on their energy levels:
</p><p>

\[
p_i \propto \exp\left(-\frac{\varepsilon_i}{kT}\right)
\]

</p><p>
Then the economists got a hold of it and realized that if the noise term in
people‚Äôs otherwise linear utility functions happened to follow a <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Gumbel
distribution</a> (doesn‚Äôt yours?), then the probability someone will choose an
item will be proportional to the exponent of the utility inputs:
</p><p>

\[
\Pr(Y_i=k)=\frac{\exp(X_i \beta_k)}{\sum_j \exp(X_i \beta_j)}
\]

</p><p>
This gave softmax a life in multinomial logistic functions; this is where I got
to know the old chap, as in I hand-derived the Hessian of this sucker in grad
school and coded it up to run on GPUs using linear fixed effects, which to my
knowledge no one else has been foolish enough to attempt before or since. I
just mention this because I know the softmax function better than I know many
of my humanoid friends, and I can recognize when it‚Äôs being used for things it
oughtn‚Äôt.
</p>

<p>
Softmax is a kind of cheat code to map real-valued numbers to probabilities
that sum to one. In physics, it works quite well; in economics, it‚Äôs a little
bit of a lie, but by the time it got to machine learning, it just became a
thing that seemed to work whenever a discrete choice was involved. Softmax
the vector and pick something, all right?
</p><p>

\[
(\textrm{softmax(x)})_i =\frac{\exp(x_i)}{\sum_j \exp(x_j)}
\]

</p><p>
Here‚Äôs the core mechanic of softmax: <em>it forces a choice among competing
alternatives</em>, whether it‚Äôs particles picking an energy state or consumers
choosing a car. That is, if a softmax mechanism <em>doesn‚Äôt want to choose
anything at all</em>, softmax will require modification, or else we would
expect the softmax to produce distortions once it encounters actual data.
</p>

<p>
In the context of LLMs, one of those distortions is to heavily weight
non-semantic tokens (commas and such), and those beefy weights become those
difficult-to-compress outlier values that make Life on the Edge harder
than it should be. <em>The Qualcomm AI researchers found that 97%+ of outlier
activations in LLMs occur in whitespace and punctuation positions.</em>
Something‚Äôs fishy around here‚Ä¶ and I thought I ordered the chicken.
</p>

<h3>
    ‚Ä¶in a linear hole
</h3>

<p>
Let‚Äôs dive in to how softmax is used inside of attention and see exactly where
it goes wrong. Here‚Äôs the equation again:
</p><p>

\[
\textrm{Attention}(Q, K, V) = \textrm{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
\]

</p><p>
Breaking it down: in decoder-only models (i.e., everything since ChatGPT), \(Q\),
\(K\), and \(V\) all originate from the same input sequence. They‚Äôre not identical,
as they‚Äôve been projected in different ways along the way, but in each layer
they all start with the same annotated (added-to) embedding vector.
</p>

<p>
Now: \(QK^T\) is looking for correlations between token (embedding) vectors in
different positions, essentially building up a square matrix of correlation
(dot-product scaled by \(1/\sqrt{d}\)) values, where each column and row corresponds to a token
position. Then each row of this square matrix is softmaxed, with the
resulting probabilities used as a mixing function for the value vectors in the
\(V\) matrix. The probability-mixed \(V\) gets added to the input vector, with the
sum passed on down the neural network for further processing.
</p>

<p>
Multi-head attention goes through this process several times, in parallel, per
layer. Essentially it divides up the embedding vector, and each head uses
information in the entire vector to annotate one (non-overlapping) segment of
the output vector. If you were confused by the Concatenation operation in the
<a href="https://arxiv.org/abs/1706.03762">original Transformer paper</a>, that‚Äôs all that‚Äôs happening: Head 1 adds
information to Segment 1, Head 2 adds information to Segment 2, and so on.
</p>

<p>
<em>The problem with using softmax is that it forces each attention head to
make an annotation, even if it has no information to add to the output
vector.</em> Using softmax to choose among discrete alternatives is great;
using it for optional annotation (i.e. as input into addition) is, like,
not cool, man. The problem here is exacerbated with multi-head attention, as a
specialized head is more likely to want to ‚Äúpass‚Äù than a general-purpose one.
These attention heads are needlessly noisy, a deafening democracy where
abstention is disallowed.
</p>

<p>
Now it‚Äôs possible that softmax should be replaced wholesale, but it‚Äôs worked
pretty well for the most part, except for this one wee little bug that prevents
attention heads from saying nothing. So I propose a very small tweak on which I
am willing to stake all future Internet claims to being correct. The tweak
is so small, yet so obvious, and it‚Äôs been sitting here under everyone‚Äôs noses
ever since attention was <a href="https://arxiv.org/abs/1409.0473">invented</a> (2014).
</p><p>
Are you ready?
</p>

<p>
Are you <em>ready-ready</em>?
</p>

<p>
I can‚Äôt hear you.
</p>


<h2>
Softmax One and Quiet Attention
</h2>

<p>
All right, here it is and you are welcome, the long-awaited Softmax Super-Mod that
is soon to set the LLM hacker channels aflame:
</p><p>

\[
(\textrm{softmax}_1(x))_i = \frac{\exp(x_i)}{1+\sum_j \exp(x_j)}
\]

</p><p>
Bit of a let-down, eh? All I did was added one to the denominator. This lets
the vector as a whole tend to zero if it wants, but otherwise just shrinks the
values by a small amount, a shrinkage which will be made up for during
normalization, which happens right after attention.
</p>

<p>
The key difference is in the negative limit, when entries in \(x\) are significantly
less than zero and the model is trying to avoid an annotation altogether.
Compare the limiting behavior of the original softmax
</p><p>

\[
\lim_{x_1 \to -\infty} \ldots
\lim_{x_k \to -\infty} 
(\textrm{softmax}(x))_i = \frac{1}{k} \gt 0
\]

</p><p>
to that of the new and improved softmax<sub>1</sub>:
</p><p>

\[
\lim_{x_1 \to -\infty} \ldots
\lim_{x_k \to -\infty} 
(\textrm{softmax}_1(x))_i = 0
\]

</p><p>
Vanilla softmax will always emit the same total weight; softmax<sub>1</sub>
mostly looks the same, but comes with an escape hatch in the negative orthant.
To be clear: the core issue here is <em>mathematical</em> and not
<em>numerical</em> in nature. Extra precision won‚Äôt save the softmax; all
Transformers are affected.
</p>

<p>
You‚Äôll notice a couple of other things about softmax<sub>1</sub>. The
derivative is positive, so we always have a non-zero gradient, and its sum is
between zero and one, so the output isn‚Äôt out of control. The function maintains the property
</p><p>

\[
\frac{(\textrm{softmax}_1(x))_i}{(\textrm{softmax}_1(x))_j} 
= \frac{(\textrm{softmax}(x))_i}{(\textrm{softmax}(x))_j} 
= \frac{\exp(x_i)}{\exp(x_j)} \quad \forall \ i, j
\]

</p><p>i.e. relative values in the output vector are unchanged.</p>

<p>
Originally I wanted to call this
function <em>ghostmax</em>, as you can think of there being an extra
zero-valued entry in \(x\) (as \(\exp(0)=1\)), as well as a zero vector in the
\(V\) matrix that attenuates the result. But I didn‚Äôt want to scare anyone.
</p>

<p>
Even though softmax<sub>1</sub> is facially quite boring, I‚Äôm 99.44% sure that it will
resolve the outlier feedback loop that‚Äôs making quantization the subject of
cascades of research. If you want to run some experiments and prove me right,
<a href="https://twitter.com/EvMill">DM me on Twitter</a> and we‚Äôll get a
paper going.
</p>

<p>
We can call the improved mechanism <em>QuietAttention</em>, as it allows
attention heads to keep their yaps shut. So may I propose to a receptive public
</p><p>

\[
\textrm{QuietAttention}(Q, K, V) := \textrm{softmax}_1 \left(\frac{QK^T}{\sqrt{d}}\right)V
\]

</p><p>
I think a test could be hacked together fairly quickly: if you prefix every
input context with a zero vector, and ensure that your neural network of choice
doesn‚Äôt add any bias (including with the positional encoding), then the zero
should pass through unaltered and will have the effect of adding unity to every
subsequent softmax denominator; that way you won‚Äôt lose your mind mucking with 
gradient code. I <em>think</em> this could be done with a LLaMA model that uses
fixed embeddings and a special prefix token, but 
I‚Äôd very much like to get
back to my non-existent hobbies, and
I‚Äôve already spent more time
on this problem than my therapist needs to know about.
</p>

<p>
You‚Äôd still have to re-train the model, so don‚Äôt try this on an RPi just yet.
But do let me know how those weight kurtoses and activation infinity norms are
looking after a few runs. I‚Äôm thinking those numbers will make for a handsome
table in a soon-to-be influential arXiV paper, either when those Qualcomm AI
researchers step off the plane from Italy, or someone in an LLM hacker channel
figures out <code>biblatex</code>, whichever happens first.
</p>



<hr>

<p><em>You‚Äôre reading <a href="https://www.evanmiller.org/">evanmiller.org</a>, a random collection of math, tech, and musings. If you liked this you might also enjoy:
    </em></p><ul><em>
        <li><a href="https://www.evanmiller.org/how-not-to-sort-by-average-rating.html">How Not To Sort By Average Rating</a>
        </li><li><a href="https://www.evanmiller.org/winkel-tripel-warping-trouble.html">Winkel Tripel Warping Trouble</a>
        </li><li><a href="https://www.evanmiller.org/you-cant-spell-cuped-without-frisch-waugh-lovell.html">You Can‚Äôt Spell CUPED Without Frisch-Waugh-Lovell</a>
        </li><li><a href="https://www.evanmiller.org/evaluating-splatoons-ranking-system.html">Evaluating Splatoon‚Äôs Ranking System</a>
    </li></em></ul>

<hr>
<p><em>Get new articles as they‚Äôre published, via <a href="https://www.linkedin.com/in/evanmmiller/">LinkedIn</a>, <a href="https://twitter.com/EvMill">Twitter</a>, or <a href="https://www.evanmiller.org/news.xml">RSS</a>.</em></p>

<hr>

<p><em>Want to look for statistical patterns in your MySQL, PostgreSQL, or SQLite database? My desktop statistics software <strong><a href="https://www.wizardmac.com/">Wizard</a></strong> can help you analyze <strong>more data in less time</strong> and <strong>communicate discoveries visually</strong> without spending days struggling with pointless command syntax. Check it out!</em></p>
<p><a href="https://www.wizardmac.com/"><img height="128" width="128" src="https://www.evanmiller.org/images/index/wizard2.png"></a><br>
<strong><a href="https://www.wizardmac.com/">Wizard</a></strong><br><span>Statistics the Mac way</span>
</p>


<hr>

<p><a href="https://www.evanmiller.org/">Back to Evan Miller‚Äôs home page</a> 
‚Äì <a href="https://www.evanmiller.org/news.xml">Subscribe to RSS</a>
‚Äì <a href="https://www.linkedin.com/in/evanmmiller/">LinkedIn</a>
‚Äì <a href="https://twitter.com/EvMill">Twitter</a> 
</p>

<hr>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[40 years ago yesterday Air Canada Flight 143 ran out of fuel mid-flight (613 pts)]]></title>
            <link>https://www.damninteresting.com/the-gimli-glider/</link>
            <guid>36850111</guid>
            <pubDate>Mon, 24 Jul 2023 16:03:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.damninteresting.com/the-gimli-glider/">https://www.damninteresting.com/the-gimli-glider/</a>, See on <a href="https://news.ycombinator.com/item?id=36850111">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

                    

                    <p><span>

                                                	<span>Long-Form:</span>
                            <span>When a botched imperial-to-metric conversion left a commercial jet with insufficient fuel, pilots had to improvise.<br></span>
                            <span>Written by <a href="https://www.damninteresting.com/contributors/alan-bellows/">Alan Bellows</a></span>
                        
                                                	‚Ä¢
                        	<span>Non-Fiction</span>
                        
		        					        		‚Ä¢
			        		<span>November 2007</span>
		        		                    </span>


                </p></div><div>

		            				<article>
											<p>
			¬© 2007 All Rights Reserved. Do not distribute or repurpose this work without written permission from the copyright holder(s).
	</p>
<p>
	Printed from https://www.damninteresting.com/the-gimli-glider/<br>
</p>
												
		                
						
						
											
					<p>‚ÄúHoly shit.‚Äù</p>
<p>Inside the cockpit of the cruising airliner, Captain Bob Pearson was understandably alarmed at the out-of-the-ordinary beeps that were chiming from his flight computer.  On the control panel, an amber <i>low fuel pressure</i> warning lamp lit up to punctuate the audio alarm.  </p>
<p>First Officer Maurice Quintal, copilot of Air Canada Flight 143, checked the indicator light to determine the cause of the computer‚Äôs complaints.  ‚ÄúSomething‚Äôs wrong with the fuel pump,‚Äù he reported.</p>
<p>The mustachioed Captain Pearson pulled out the trusty Boeing handbook, his fingers dashing through the pages to find the specifics of the warning.  To his relief, the troubleshooting chart indicated that the situation was not as perilous as it might seem: the fuel pump in the left wing tank was signaling a problem, a minor issue considering that gravity would continue to feed the engines even if the pump failed.  </p>
<p>‚ÄúYou know,‚Äù he commented to Copilot Quintal, ‚ÄúI would not take this air‚Ä¶‚Äù  He trailed off as the computer blurted out another four beeps, and the indicator panel lit up like a Christmas tree decorated with bad news. ‚ÄúOh <i>fuck</i>,‚Äù Pearson lamented, ‚Äúwe‚Äôve got to go to Winnipeg.‚Äù
</p>
<p>
The date was 23 July 1983, and although the fuel pressure warnings were not the flight‚Äôs first mechanical frustrations, they were certainly the most distressing so far.  When pilots Pearson and Quintal had arrived for their shift earlier that day, they had been notified that the plane‚Äôs fuel gauges were non-functional due to a fault in the Fuel Quantity Indicator System (FQIS).  Even worse, the component required to repair it could not be delivered until later that evening.  </p>
<p>Rather than canceling the flight, Captain Pearson instructed the engineers to check the fuel level manually.  The four-month-old 767 was a state-of-the-art machine with state-of-the-art glitches, and FQIS issues were becoming a common complaint.  Several independent dripstick checks later, the fuel hosers were satisfied that sufficient fuel was loaded, and they advised Air Canada Flight 143 to take off.  The airliner departed from Montreal at 5:48pm eastern time with their sixty-one passengers. At 6:58pm they made a brief scheduled stopover in Ottawa, where engineers once again checked the fuel dripsticks‚Å†‚Äî just to be safe.  </p>
<p>It was just after 8:00pm central time that the cockpit computer began its string of inexplicable beeps and warning lights.  As the jumbo jet crossed the Canadian countryside at 41,000 feet, Copilot Quintal thumbed through the 767 handbook to ascertain the nature of the airplane‚Äôs problem.  ‚ÄúThey don‚Äôt say anything if you‚Äôve got more than one though, main tank, eh?‚Äù he said to Captain Pearson, as well as the flight engineer who had joined them.  ‚ÄúLike there‚Äôs two pumps, they don‚Äôt say anything about only one, eh?‚Äù  According to the computer‚Äôs calculations there should have been plenty of fuel remaining, but multiple fuel pumps were indicating pressure problems.  The flummoxed flight crew decided to divert to the nearby Winnipeg airport as a precaution, and alerted Air Traffic Control (ATC) of their intent.</p>
<figure><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.66451612903226" data-lazy-load-src="//damn-8791.kxcdn.com/wp-content/uploads/2007/11/767_cockpit.jpg" alt="The cockpit of a typical Boeing 767" title="The cockpit of a typical Boeing 767"></p><figcaption>The cockpit of a typical Boeing 767</figcaption></figure>
<p>‚ÄúAir Canada 143 cleared present position direct Winnipeg,‚Äù the tower responded.  ‚ÄúWe‚Äôre landing runway 31.  You‚Äôre cleared to maintain six thousand descent your discretion.‚Äù  Pearson and Quintal updated their flight computer with the new heading and destination.  ‚ÄúAir Canada 143 did you want any <i>assistance</i>?‚Äù the traffic controller inquired, where ‚Äúassistance‚Äù is an aeronautic euphemism for a reception from the fire brigade.</p>
<p>‚ÄúFor the moment we won‚Äôt require any assistance,‚Äù Pearson responded. </p>
<p>The flight engineer struggled to assess the situation.  ‚ÄúYou‚Äôve got nothing in the center tanks, eh?‚Äù he inquired of the captain.</p>
<p>‚ÄúNo, we ran the pumps,‚Äù the captain replied, referring to an earlier attempt to transfer fuel from another tank.  ‚ÄúUh, let‚Äôs put them back on again.‚Äù  Within moments, several more warning lights snapped on in quick succession.  ‚ÄúHoly shit.‚Äù </p>
<p>‚ÄúGod damn,‚Äù Quintal remarked, ‚Äúthey‚Äôre all going out, eh?  How about uh‚Ä¶‚Äù</p>
<p>‚ÄúAll the lights are on,‚Äù Pearson observed soberly, as the array of low fuel pressure indicators glowed with incandescent urgency.  The captain summoned the in-charge flight attendant to the cockpit and apprised him of the situation, but his summary was outdated mere moments later.  The flight computer bellowed out a flamboyant <i>BONG!</i> which none of the men present could recall having heard before.  </p>
<p>‚ÄúOkay,‚Äù the captain observed upon examining the instruments, ‚ÄúWe‚Äôve lost the left engine.‚Äù</p>
<p>‚ÄúOkay, what‚Ä¶will we do?‚Äù Quintal replied.  ‚ÄúWant the checklist now?‚Äù </p>
<p>‚ÄúChecklist, yeah.‚Äù</p>
<p>The pilots began preparations for a delicate-but-very-doable single-engine landing, and Copilot Quintal contacted Winnipeg tower to request the previously offered ‚Äúassistance‚Äù.  It was becoming increasingly clear that the plane‚Äôs problems lay not in its machinery, but in its fuel.  The men, however, were unsure of exactly what was amiss.</p>
<p>Following two minutes of uneventful descent, the ever-present vibrations in the deck were disrupted by an almost imperceptible shudder, and  the white-noise hum of the remaining jet engine faded away with a long and melancholy mechanical sigh.  The gauges and monitors of the control panel‚Å†‚Äî which had been so animated with anxiety mere moments before‚Å†‚Äî fell dark.  Absent the usual murmur of the twin turbofans, an unsettling silence hung heavy in the air.</p>
<p>‚ÄúHow come I have no instruments?‚Äù Captain Pearson wondered aloud, though the answer lingered mockingly in the cockpit‚Äôs uncharacteristic quiet.  The airliner‚Äôs generators and hydraulic systems required at least a single functioning engine in order to operate, without which there was no electricity for the computer, and no power to manipulate the ailerons, rudder, and elevator.  In effect, the highly advanced flying machine had roughly the maneuverability of a flying brick, with barely enough instrumentation to monitor its slow dive towards the Earth.  After a few ponderous moments, however, the automatic emergency systems twitched into action.  Onboard batteries revived a few of the most critical instruments, and a door popped open on the plane‚Äôs underbelly to expose a ram-air turbine (RAT) designed to provide limited emergency hydraulic support.</p>
<p>‚Äú143,‚Äù the radio crackled, ‚ÄúWe have lost your transponder return right now.‚Äù</p>
<figure><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="1.4622641509434" data-lazy-load-src="//damn-8791.kxcdn.com/wp-content/uploads/2007/11/bob_pearson.jpg" alt="Captain Bob Pearson" title="Captain Bob Pearson"></p><figcaption>Captain Bob Pearson</figcaption></figure>
<p>Captain Pearson was beginning to grasp the true gravity of the situation.  ‚ÄúCenter, one-four-three, this is a mayday and we require a vector onto the closest available runway.  We are out of 22,000 feet on‚Ä¶ both engines have failed due to looks like fuel starvation and we are on emergency instruments and can only give you limited headings.  Information‚Å†‚Äî we are heading two five zero now, please give us a vector to the nearest runway.‚Äù</p>
<p>‚Äú143 we copy all that okay.  We have lost your transponder return and attempting to pick up your target now‚Ä¶ we have it now, just stand by on the two fifty heading.‚Äù</p>
<p>‚ÄúAh, roger.‚Äù</p>
<p>After repeated unsuccessful attempts to restart the stalled engines, Pearson and Quintal once again consulted the 767 emergency manual, this time for advice on an unpowered landing.  Much to their dismay, no such section existed, presumably because a simultaneous engine failure had been too ridiculous for Boeing engineers to contemplate.  The pilots sat anxiously in their darkened cockpit and monitored the plane‚Äôs slow and silent descent using a handful of analog instruments based on pre-WW2 technology: a magnetic compass, an artificial horizon, an airspeed indicator, and an altimeter.  </p>
<p>The traffic controller in the tower at Winnipeg advised the flight officers of their options.  ‚Äú143 we show you at sixty-five miles from Winnipeg and approximately forty-five miles from Gimli.‚Äù</p>
<p>‚ÄúOkay,‚Äù Pearson responded, ‚Äúis there emergency equipment at Gimli?</p>
<p>‚ÄúNegative emergency equipment at all.  Just one runway available I believe and no control and no information on it.‚Äù</p>
<p>‚ÄúWe‚Äôd prefer Winnipeg then.‚Äù  </p>
<p>In a stroke of profound luck, Captain Pearson was an accomplished glider pilot, a skill which afforded him with some sense of the vehicle‚Äôs glide capabilities.  He applied his expertise to estimate the plane‚Äôs best glide ratio speed, but having neither a vertical speed indicator nor a view of the landscape through the clouds, he was unaware that Winnipeg was well beyond the reach of their gravity-gripped flight equipment.</p>
<p>Back in the passenger compartment, the in-charge flight attendant radiated counterfeit calm as he informed the plane‚Äôs sixty-one passengers of the situation, and instructed them in the subtle art of not freaking out during an in-flight emergency.  In the meantime, crew members directed able-bodied men to move into the rows alongside the exit doors, then solemnly buckled into their own seats.  Many of the crew members were keenly aware that jumbo jets such as theirs were not designed for dead-stick flight‚Å†‚Äî let alone dead-stick landings.  In all probability, their inevitable confrontation with the Earth would not be an improvement on their current situation.</p>
<p>As the impromptu glider emerged from the ceiling of clouds and obtained a view of the landscape, the pilots quickly realized that the plane was shedding altitude far too quickly to have any chance of reaching Winnipeg.  Copilot Quintal confirmed this conclusion using radar data from Air Traffic Control.  </p>
<p>‚ÄúHow far are we from Gimli?‚Äù Pearson inquired of the Winnipeg tower.</p>
<figure><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.75806451612903" data-lazy-load-src="//damn-8791.kxcdn.com/wp-content/uploads/2007/11/gimli_glider_map.jpg" alt="" title=""></p></figure>
<p>‚ÄúYou are approximately twelve miles from Gimli right now.‚Äù</p>
<p>Air Traffic Control had no specific data on the remote airstrip, but in another stroke of luck, First Officer Quintal had been stationed there during his time in the Royal Canadian Air Force.  Lacking any feasible alternative, the copilot recommended they drop in on his old friends from the service.  He was not aware, however, that the facility had since been converted into a public airport; nor did any of the men know that one of its two runways had been decommissioned and carved up for use as a racetrack.</p>
<p>As Flight 143 fell below the Air Traffic Control radar range, the tower grimly requested a count of the souls on board.  As Pearson began his long final approach, he scraped up a bit of optimism as he updated Winnipeg tower on their status.  ‚ÄúWe have the field in sight,‚Äù he reported, ‚Äúand we feel we‚Äôre in good shape.‚Äù  </p>
<p>On the ground at Gimli, it was Family Day at the local racetrack.  Sports Racers buzzed along the decommissioned runway as spectators cheered from the sidelines.  A collection of campers at the end of the airstrip soaked up the summer Saturday evening as their dinners sizzled on assorted barbecues.  Without the jet engines to announce the airliner‚Äôs approach, the people were oblivious of the 132-ton Boeing behemoth which was bearing down on them.</p>
<p>In the cockpit, Copilot Quintal activated the manual landing gear controls, and the two main gears lowered and locked.  The nose gear, however, dangled limply from its housing.  For Captain Pearson, the flight controls were becoming increasingly difficult to operate.  The effectiveness of the emergency RAT was governed by the speed of the wind slipping around the fuselage, so as the plane gradually slowed, the hydraulic assistance was diminishing.  Nevertheless, Pearson needed to sharply reduce the speed and altitude of his approach, otherwise the 767 would overshoot the tarmac; and without engines there would be no opportunity for a second try.  Ordinarily an airline pilot would apply some combination of flaps and aerobrakes, but none of these systems were functioning on Pearson‚Äôs crippled craft.  </p>
<p>Lacking a more orthodox option, Captain Pearson cranked the control wheel to the right and gave the left rudder pedal a firm stomp.  The criss-crossed controls tilted the deck to the right as one wing dipped toward the ground, providing the passengers with a lovely view of the golf course on one side, and nothing but blue sky on the other.  The fuselage also rotated its heading to the left, becoming diagonal relative to its direction of travel.  Such <i>forward-slip</i> maneuvers were sometimes used on small planes and gliders, but the curve-ballish air acrobatics were unheard of with a jumbo jet.  The airplane indeed decelerated, but the reduced airspeed robbed the controls of even more precious hydraulic pressure, requiring Pearson to apply monumental force to try to straighten the slip.  </p>
<figure data-embiggen="true"><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.8" data-lazy-load-src="https://damn-8791.kxcdn.com/wp-content/uploads/2007/11/forward_slip1.jpg" alt="Orientation of the aircraft during forward slip maneuver" title="Orientation of the aircraft during forward slip maneuver"></p><figcaption>Orientation of the aircraft during forward slip maneuver</figcaption></figure>
<p>At the opposite end of the runway, the Family Day campers and spectators had finally spotted the silent and oddly-angled incoming aircraft, and they were scrambling from its path with appropriate levels of panic.  First Officer Quintal caught sight of the fleeing families, but it was far too late to revise their landing plans, so he opted not to distract the captain with the unsettling discovery.</p>
<p>Forty feet above the ground‚Å†‚Äî mere seconds before contact‚Å†‚Äî Captain Pearson managed to wrestle Flight 143 back to a straight and level approach.  At 8:38pm central time, the rear landing gears grabbed the tarmac at Gimli airport, and Bob Pearson stood on the brake pedals as the airplane skidded towards the scattering bystanders.  A few of the loudly protesting tires finally succumbed to the abuse and blew out with adequate force to shimmy the fuselage.  As some of the weight shifted forward, the unsecured front landing gear buckled, dumping the nose section onto the pavement and spraying a three-hundred foot shower of sparks.  </p>
<p>After sledding across the asphalt for 2,900 feet, Air Canada Flight 143 ground to a halt just a few hundred yards from the shocked onlookers.  There was a moment of stupefied contemplation within the passenger cabin, followed by an eruption of cheering and applause.  Meanwhile several astute racetrack workers dashed to the nose of Flight 143 and doused a small friction-induced fire using hand-held extinguishers.  Within a few minutes the inflatable rubber escape chutes plopped from the sides of the plane, and the sixty-nine frazzled occupants disembarked.  </p>
<p>A crew of engineers from Winnipeg airport clambered into a van and headed for Gimli to assess the damage.  During transit, however, their vehicle unexpectedly ran out of fuel, nearly ripping a hole in the delicate space-irony continuum.  When airline mechanics finally arrived at the landing site, they found all three of the 767‚Äôs fuel tanks completely dry, with no evidence of a fuel leak.  A review of the day‚Äôs events traced the problem back to the manual dripstick checks in Montreal and Ottawa.  In order to maintain awareness of the overall weight of the aircraft, flight crews kept track of fuel quantity based on kilograms rather than the fuel company‚Äôs liter-based measurements.  Pearson and Quintal had determined the fuel weight by multiplying the the number of dripsticked liters by 1.77, as indicated by the documentation.  However, unbeknownst to the pilots and the fuel crew, this multiplier provided the weight in imperial pounds;  the new, all-metric 767 was based on kilograms, and required a multiplier of 0.8.  As a consequence of this documentation disconnect, Flight 143 had left Montreal with roughly half the necessary fuel.</p>
<p>Because the rear escape slides were excessively steep due to the buckled front gear, a few bumps and bruises were sustained on egress; but no one was seriously injured in the Gimli incident.  Had it not been for Pearson‚Äôs capable captaining and glider experience, as well as Quintal‚Äôs cucumber-cool support, the outcome of the metric mixup might have been considerably less pleasant.  In addition, had it not been for the drag created by the collapsed front gear, the powerless plane would have plunged into the crowd of spectators, sowing destruction and death in its wake.  All told, the soon-to-be-dubbed ‚ÄúGimli Glider‚Äù was a nearly perfect demonstration of dead-stick flying, accompanied by an extra-large portion of good fortune.</p>
<figure data-embiggen="true"><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.70967741935484" data-lazy-load-src="https://damn-8791.kxcdn.com/wp-content/uploads/2007/11/gimlix.jpg" alt="The Gimli after landing (photo by Wayne Glowacki, Winnipeg Free Press)" title="The Gimli after landing (photo by Wayne Glowacki, Winnipeg Free Press)"></p><figcaption>The Gimli after landing (photo by Wayne Glowacki, Winnipeg Free Press)</figcaption></figure>
<p>With just two days of mechanical jiggering, Captain Pearson‚Äôs wounded 767 was patched up sufficiently to fly it out for repairs elsewhere.  The Gimli Glider officially rejoined the Air Canada fleet after a bit of body work, a new front gear, a new wiring harness, a repaired Fuel Quantity Indicator System, and a full load of jet fuel.  The internal investigation into the incident laid the blame partially upon Captain Bob Pearson and First Officer Maurice Quintal, who should have observed the Minimum Equipment List (MEL) and grounded the aircraft since it lacked functioning fuel gauges.  Some of the responsibility was also assigned to the maintenance workers, and to ‚Äúcorporate deficiencies.‚Äù  As a consequence Pearson was briefly demoted, and Quintal was suspended for two weeks.  Nonetheless both pilots continued to work for Air Canada, and in 1985 they received the well-deserved <i>F√©d√©ration A√©ronautique Internationale</i> Diploma for Outstanding Airmanship for their handling of the unusual landing.</p>
<p>As for the Gimli Glider herself, the twenty-four year old 767 remains an active part of the Air Canada fleet to this very day.  Some grizzled old pilots swear that sometimes, when the wind is just right on a quiet night, you can just about make out the double-engine-failure <i>BONG!</i> as the old girl is flying by; and if you‚Äôre very lucky, you might catch the faint odor of damp pilots in the air.</p>
				

											<p><b>Update:</b> The Gimli Glider‚ÄìAir Canada Flight 143‚Äìwas retired from service on 24 January 2008 in a ceremony involving Captain Robert Pearson, First Officer Maurice Quintal, and three of the six flight attendants who had been aboard Flight 143 during its unscheduled glide and rough landing.</p>

										
										
						<p>
			¬© 2007 All Rights Reserved. Do not distribute or repurpose this work without written permission from the copyright holder(s).
	</p>
<p>
	Printed from https://www.damninteresting.com/the-gimli-glider/<br>
</p>
						<p>
							<i>Since you enjoyed our work enough to print it out, and read it clear to the end, would you consider donating a few dollars at https://www.damninteresting.com/donate</i> ?
						</p>
									</article>
			
		            	            		
	            	
		            			

				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zenbleed (308 pts)]]></title>
            <link>https://lock.cmpxchg8b.com/zenbleed.html</link>
            <guid>36848680</guid>
            <pubDate>Mon, 24 Jul 2023 14:34:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lock.cmpxchg8b.com/zenbleed.html">https://lock.cmpxchg8b.com/zenbleed.html</a>, See on <a href="https://news.ycombinator.com/item?id=36848680">Hacker News</a></p>
Couldn't get https://lock.cmpxchg8b.com/zenbleed.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
    </channel>
</rss>