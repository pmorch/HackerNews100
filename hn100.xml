<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 15 Oct 2024 16:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: I built the most over-engineered Deal With It emoji generator (256 pts)]]></title>
            <link>https://emoji.build/deal-with-it-generator/</link>
            <guid>41848150</guid>
            <pubDate>Tue, 15 Oct 2024 13:05:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://emoji.build/deal-with-it-generator/">https://emoji.build/deal-with-it-generator/</a>, See on <a href="https://news.ycombinator.com/item?id=41848150">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Web Browser Engineering (446 pts)]]></title>
            <link>https://browser.engineering/index.html</link>
            <guid>41846780</guid>
            <pubDate>Tue, 15 Oct 2024 09:42:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://browser.engineering/index.html">https://browser.engineering/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=41846780">Hacker News</a></p>
<div id="readability-page-1" class="page">


<header>


<a href="https://twitter.com/browserbook">Twitter</a> ·
<a href="https://browserbook.substack.com/">Blog</a> ·
<a href="https://patreon.com/browserengineering">Patreon</a> ·
<a href="https://github.com/browserengineering/book/discussions">Discussions</a>
</header>




<nav id="toc">
<ul>
<li><a href="#introduction" id="toc-introduction">Introduction</a></li>
<li><a href="#part-1-loading-pages" id="toc-part-1-loading-pages">Part
1: Loading Pages</a></li>
<li><a href="#part-2-viewing-documents" id="toc-part-2-viewing-documents">Part 2: Viewing Documents</a></li>
<li><a href="#part-3-running-applications" id="toc-part-3-running-applications">Part 3: Running
Applications</a></li>
<li><a href="#part-4-modern-browsers" id="toc-part-4-modern-browsers">Part 4: Modern Browsers</a></li>
</ul>
</nav>

<p>Web browsers are ubiquitous, but how do they work? This book
explains, building a basic but complete web browser, from networking to
JavaScript, in a couple thousand lines of Python.</p>

<div>
<figure>
<img src="https://browser.engineering/im/cover.jpg" alt="The cover for Web Browser Engineering, from Oxford University Press">

</figure>
<section id="pre-order-web-browser-engineering">
<h2>Pre-order <em>Web Browser Engineering</em></h2>
<p><em>Web Browser Engineering</em> will be published by Oxford
University Press before the end of the year. To get it as soon as it’s
out, <a href="https://global.oup.com/academic/product/web-browser-engineering-9780198913863">pre-order
now!</a></p>
</section>
</div>
<p>Follow this book’s <a href="https://browserbook.substack.com/archive">blog</a> or <a href="https://twitter.com/browserbook">Twitter</a> for updates. You can
also talk about the book with others in our <a href="https://github.com/browserengineering/book/discussions">discussion
forum</a>.</p>
<p>If you are enjoying the book, consider supporting us on <a href="https://patreon.com/browserengineering">Patreon</a>.</p>
<p>Or just <a href="mailto:author@browser.engineering">send us an
email</a>!</p>
<section id="introduction">
<h2>Introduction</h2>
<ol type="1">
<li><a href="https://browser.engineering/preface.html">Preface</a></li>
<li><a href="https://browser.engineering/intro.html">Browsers and the Web</a></li>
<li><a href="https://browser.engineering/history.html">History of the Web</a></li>
</ol>
</section>
<h2 id="part-1-loading-pages">Part 1: Loading Pages</h2>
<ol type="1">
<li><a href="https://browser.engineering/http.html">Downloading Web Pages</a><br>
URLs and HTTP requests</li>
<li><a href="https://browser.engineering/graphics.html">Drawing to the Screen</a><br>
Creating windows and drawing to a canvas</li>
<li><a href="https://browser.engineering/text.html">Formatting Text</a><br>
Word wrapping and line spacing</li>
</ol>
<h2 id="part-2-viewing-documents">Part 2: Viewing Documents</h2>
<ol start="4" type="1">
<li><a href="https://browser.engineering/html.html">Constructing an HTML Tree</a><br>
Parsing and fixing HTML</li>
<li><a href="https://browser.engineering/layout.html">Laying Out Pages</a><br>
Inline and block layout</li>
<li><a href="https://browser.engineering/styles.html">Applying Author Styles</a><br>
Parsing and applying CSS</li>
<li><a href="https://browser.engineering/chrome.html">Handling Buttons and Links</a><br>
Hyperlinks and browser chrome</li>
</ol>
<h2 id="part-3-running-applications">Part 3: Running Applications</h2>
<ol start="8" type="1">
<li><a href="https://browser.engineering/forms.html">Sending Information to Servers</a><br>
Form submission and web servers</li>
<li><a href="https://browser.engineering/scripts.html">Running Interactive Scripts</a><br>
Changing the DOM and reacting to events</li>
<li><a href="https://browser.engineering/security.html">Keeping Data Private</a><br>
Cookies and logins, XSS and CSRF</li>
</ol>
<h2 id="part-4-modern-browsers">Part 4: Modern Browsers</h2>
<ol start="11" type="1">
<li><a href="https://browser.engineering/visual-effects.html">Adding Visual Effects</a><br>
Blending, clipping, and compositing</li>
<li><a href="https://browser.engineering/scheduling.html">Scheduling Tasks and Threads</a><br>
The event loop and the rendering pipeline</li>
<li><a href="https://browser.engineering/animations.html">Animating and Compositing</a><br>
Smooth animations using the GPU</li>
<li><a href="https://browser.engineering/accessibility.html">Making Content Accessible</a><br>
Keyboard input, zooming, and the accessibility tree</li>
<li><a href="https://browser.engineering/embeds.html">Supporting Embedded Content</a><br>
Images, iframes, and scripting</li>
<li><a href="https://browser.engineering/invalidation.html">Reusing Previous Computation</a><br>
Invalidation, editing, and correctness</li>
</ol>













</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Pumpkin – A Modern Minecraft server written in Rust (138 pts)]]></title>
            <link>https://github.com/Snowiiii/Pumpkin</link>
            <guid>41846636</guid>
            <pubDate>Tue, 15 Oct 2024 09:18:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Snowiiii/Pumpkin">https://github.com/Snowiiii/Pumpkin</a>, See on <a href="https://news.ycombinator.com/item?id=41846636">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<p dir="auto"><h2 tabindex="-1" dir="auto">Pumpkin</h2><a id="user-content-pumpkin" aria-label="Permalink: Pumpkin" href="#pumpkin"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Snowiiii/Pumpkin/actions/workflows/rust.yml/badge.svg"><img src="https://github.com/Snowiiii/Pumpkin/actions/workflows/rust.yml/badge.svg" alt="CI"></a>
<a href="https://discord.gg/wT8XjrjKkf" rel="nofollow"><img src="https://camo.githubusercontent.com/044987ec7a0aa910e2c5480a2fce200ff9fa8269a4a2533a2a5c87e06ba3a866/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f313236383539323333373434353937383139332e7376673f6c6162656c3d266c6f676f3d646973636f7264266c6f676f436f6c6f723d66666666666626636f6c6f723d373338394438266c6162656c436f6c6f723d364137454332" alt="Discord" data-canonical-src="https://img.shields.io/discord/1268592337445978193.svg?label=&amp;logo=discord&amp;logoColor=ffffff&amp;color=7389D8&amp;labelColor=6A7EC2"></a>
<a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/6cd0120cc4c5ac11d28b2c60f76033b52db98dac641de3b2644bb054b449d60c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/dc0c67e5ec43ed955dd30f906d640e079aecd82f8be64ef1d7153ca8e6afc894/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f63757272656e745f76657273696f6e2d312e32312e312d626c7565"><img src="https://camo.githubusercontent.com/dc0c67e5ec43ed955dd30f906d640e079aecd82f8be64ef1d7153ca8e6afc894/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f63757272656e745f76657273696f6e2d312e32312e312d626c7565" alt="Current version)" data-canonical-src="https://img.shields.io/badge/current_version-1.21.1-blue"></a></p>
</div>
<p dir="auto"><a href="https://snowiiii.github.io/Pumpkin/" rel="nofollow">Pumpkin</a> is a Minecraft server built entirely in Rust, offering a fast, efficient,
and customizable experience. It prioritizes performance and player enjoyment while adhering to the core mechanics of the game.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/71594357/357392192-7e2e865e-b150-4675-a2d5-b52f9900378e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjkwMDY1MDIsIm5iZiI6MTcyOTAwNjIwMiwicGF0aCI6Ii83MTU5NDM1Ny8zNTczOTIxOTItN2UyZTg2NWUtYjE1MC00Njc1LWEyZDUtYjUyZjk5MDAzNzhlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDE1VDE1MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI0Nzc3NzBlYzI3NzQwMjg0MzJlZjY2YmM4YzBmOGFkZDY0YjExZTFiYzNhMDRjNThkMzg3MGMyOWUzODlmMDQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.mLiSktRWZz3v9ppfunfxn0kkOzFN9jmIRh5VoXxUJs4"><img src="https://private-user-images.githubusercontent.com/71594357/357392192-7e2e865e-b150-4675-a2d5-b52f9900378e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjkwMDY1MDIsIm5iZiI6MTcyOTAwNjIwMiwicGF0aCI6Ii83MTU5NDM1Ny8zNTczOTIxOTItN2UyZTg2NWUtYjE1MC00Njc1LWEyZDUtYjUyZjk5MDAzNzhlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDE1VDE1MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI0Nzc3NzBlYzI3NzQwMjg0MzJlZjY2YmM4YzBmOGFkZDY0YjExZTFiYzNhMDRjNThkMzg3MGMyOWUzODlmMDQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.mLiSktRWZz3v9ppfunfxn0kkOzFN9jmIRh5VoXxUJs4" alt="image"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What Pumpkin wants to achieve</h2><a id="user-content-what-pumpkin-wants-to-achieve" aria-label="Permalink: What Pumpkin wants to achieve" href="#what-pumpkin-wants-to-achieve"></a></p>
<ul dir="auto">
<li><strong>Performance</strong>: Leveraging multi-threading for maximum speed and efficiency.</li>
<li><strong>Compatibility</strong>: Supports the latest Minecraft server version and adheres to vanilla game mechanics.</li>
<li><strong>Security</strong>: Prioritizes security by preventing known exploits.</li>
<li><strong>Flexibility</strong>: Highly configurable with the ability to disable unnecessary features.</li>
<li><strong>Extensibility</strong>: Provides a foundation for plugin development.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">What Pumpkin will not</h2><a id="user-content-what-pumpkin-will-not" aria-label="Permalink: What Pumpkin will not" href="#what-pumpkin-will-not"></a></p>
<ul dir="auto">
<li>Be a drop-in replacement for vanilla or other servers</li>
<li>Be compatible with plugins or mods for other servers</li>
<li>Function as a framework for building a server from scratch.</li>
</ul>
<div dir="auto"><p dir="auto">Important</p><p dir="auto">Pumpkin is currently under heavy development.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features (WIP)</h2><a id="user-content-features-wip" aria-label="Permalink: Features (WIP)" href="#features-wip"></a></p>
<ul>
<li> Configuration (toml)</li>
<li> Server Status/Ping</li>
<li> Login</li>
<li>Player Configuration
<ul>
<li> Registries (biome types, paintings, dimensions)</li>
<li> Server Brand</li>
<li> Server Links</li>
<li> Set Resource Pack</li>
<li> Cookies</li>
</ul>
</li>
<li>World
<ul>
<li> World Joining</li>
<li> Player Tab-list</li>
<li> World Loading</li>
<li> Entity Spawning</li>
<li> Chunk Loading</li>
<li> World Generation</li>
<li> Chunk Generation</li>
<li> World Borders</li>
<li> World Saving</li>
</ul>
</li>
<li>Player
<ul>
<li> Player Skins</li>
<li> Player Client brand</li>
<li> Player Teleport</li>
<li> Player Movement</li>
<li> Player Animation</li>
<li> Player Inventory</li>
<li> Player Combat</li>
</ul>
</li>
<li>Server
<ul>
<li> Plugins</li>
<li> Query</li>
<li> RCON</li>
<li> Inventories</li>
<li> Particles</li>
<li> Chat</li>
<li> Commands</li>
</ul>
</li>
<li>Proxy
<ul>
<li> Velocity</li>
</ul>
</li>
</ul>
<p dir="auto">Check out our <a href="https://github.com/users/Snowiiii/projects/12/views/3">Github Project</a> to see current progress</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to run</h2><a id="user-content-how-to-run" aria-label="Permalink: How to run" href="#how-to-run"></a></p>
<p dir="auto">See <a href="https://snowiiii.github.io/Pumpkin/about/quick-start.html" rel="nofollow">https://snowiiii.github.io/Pumpkin/about/quick-start.html</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributions</h2><a id="user-content-contributions" aria-label="Permalink: Contributions" href="#contributions"></a></p>
<p dir="auto">Contributions are welcome! See <a href="https://github.com/Snowiiii/Pumpkin/blob/master/CONTRIBUTING.md">CONTRIBUTING.md</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Docs</h2><a id="user-content-docs" aria-label="Permalink: Docs" href="#docs"></a></p>
<p dir="auto">The Documentation of Pumpkin can be found at <a href="https://snowiiii.github.io/Pumpkin/" rel="nofollow">https://snowiiii.github.io/Pumpkin/</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Communication</h2><a id="user-content-communication" aria-label="Permalink: Communication" href="#communication"></a></p>
<p dir="auto">Consider joining our <a href="https://discord.gg/wT8XjrjKkf" rel="nofollow">discord</a> to stay up-to-date on events, updates, and connect with other members.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Funding</h2><a id="user-content-funding" aria-label="Permalink: Funding" href="#funding"></a></p>
<p dir="auto">If you want to fund me and help the project, Check out my <a href="https://github.com/sponsors/Snowiiii">GitHub sponsors</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Thanks</h2><a id="user-content-thanks" aria-label="Permalink: Thanks" href="#thanks"></a></p>
<p dir="auto">A big thanks to <a href="https://wiki.vg/" rel="nofollow">wiki.vg</a> for providing valuable information used in the development of this project.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: MynaUI Icons – 1180 Beautifully Crafted Open Source Icons (102 pts)]]></title>
            <link>https://mynaui.com/icons</link>
            <guid>41846539</guid>
            <pubDate>Tue, 15 Oct 2024 09:03:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mynaui.com/icons">https://mynaui.com/icons</a>, See on <a href="https://news.ycombinator.com/item?id=41846539">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Not affiliated with Figma, TailwindCSS or shadcn/ui.</p><nav><a href="https://mynaui.com/legal">Legal</a><a target="_blank" rel="noreferrer noopener" title="X (Twitter)" href="https://twitter.com/praveenjuge/"><svg width="24" height="24" fill="none" stroke="currentColor" stroke-width="1.5" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg"><path d="m19 4-5.93 6.93M5 20l5.93-6.93m0 0 5.795 6.587c.19.216.483.343.794.343h1.474c.836 0 1.307-.85.793-1.435L13.07 10.93m-2.14 2.14L4.214 5.435C3.7 4.85 4.17 4 5.007 4h1.474c.31 0 .604.127.794.343l5.795 6.587"></path></svg></a><a target="_blank" rel="noreferrer noopener" title="Dribbble" href="https://dribbble.com/praveenjuge/"><svg width="24" height="24" fill="none" stroke="currentColor" stroke-width="1.5" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg"><path d="M12 21a9 9 0 1 0 0-18 9 9 0 0 0 0 18"></path><path d="M3.07 10.875c1.7.102 6.2.195 9.08-1.035s5.358-3.492 6.208-4.21"></path><path d="M8.625 3.654c1.409 1.3 4.482 4.61 5.625 7.896 1.143 3.286 1.566 7.326 1.827 8.476"></path><path d="M21 12c-1.313 0-4.936-.495-8.178.928-3.522 1.547-6.072 3.946-7.184 5.438"></path></svg></a></nav></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Retreat to Muskworld (127 pts)]]></title>
            <link>https://niedermeyer.io/2024/10/11/the-retreat-to-muskworld/</link>
            <guid>41845596</guid>
            <pubDate>Tue, 15 Oct 2024 06:34:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://niedermeyer.io/2024/10/11/the-retreat-to-muskworld/">https://niedermeyer.io/2024/10/11/the-retreat-to-muskworld/</a>, See on <a href="https://news.ycombinator.com/item?id=41845596">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<figure><img loading="lazy" data-attachment-id="277" data-permalink="https://niedermeyer.io/2024/10/11/the-retreat-to-muskworld/screenshot-2024-10-11-174447/" data-orig-file="https://niedermeyer.io/wp-content/uploads/2024/10/screenshot-2024-10-11-174447.png" data-orig-size="847,644" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2024-10-11 174447" data-image-description="" data-image-caption="" data-medium-file="https://niedermeyer.io/wp-content/uploads/2024/10/screenshot-2024-10-11-174447.png?w=300" data-large-file="https://niedermeyer.io/wp-content/uploads/2024/10/screenshot-2024-10-11-174447.png?w=847" tabindex="0" role="button" width="847" height="644" src="https://niedermeyer.io/wp-content/uploads/2024/10/screenshot-2024-10-11-174447.png?w=847" alt="A Tesla Cybercab driving past a small town movie set, during Tesla's We, Robot event"><figcaption><em>A totally real robotaxi, driving in an extremely normal town</em></figcaption></figure>



<p>Almost eight years ago, Elon Musk announced that every Tesla made from that moment forward would be capable of Level 5 autonomous driving with nothing more than a software update. It was a pivotal moment in Tesla’s history, committing the company to not just succeed as an electric automaker, but solve one of the most ambitious AI and robotics challenges possible. To create confidence in that staggering aspiration, Tesla released a video of a Model X driving around Palo Alto autonomously to the Rolling Stones’ “Paint it Black,” claiming that the driver behind the wheel was only there “for legal purposes.”</p>



<p>Eight long and hype-filled years later, Tesla is still looking for ways to build confidence in its ability to deliver a “general solution to self-driving” through hype and spectacle, even as companies like Waymo deliver <a href="https://techcrunch.com/2024/08/20/waymo-is-now-giving-100000-robotaxi-rides-week/">the reality of 100,000 driverless taxi rides per week</a>. Rather than meeting the competitive challenge from Waymo with real driverless rides on real public streets, Tesla’s latest ploy for credibility sees the firm retreating ever deeper into fantasy, building what can only be described as a temporary theme park on a movie studio lot for its first ever “driverless” demonstration.</p>



<p>This contrast is instructive. The “Paint It Black” video of eight years ago was no more “real” or “fake” than yesterday’s “We, Robot” demonstration, but at least it had the pretense of reality: it depicted a real car on real roads. Tesla’s latest spectacle likely cost orders of magnitude more to produce, but it didn’t even purport to show any actual real-world capability. The entire thing was pure fantasy, in a contained fantasy world, built on a movie theater lot that exists for the sole purpose of producing such spectacles.</p>



<p>This trajectory, from simulating future capability on public roads to creating a fantasy world for fantasy cars to show off fantasy capabilities, should worry Tesla’s supporters. We can already see Musk retreating into a misinformation-fueled fantasy world every day on Twitter, and the jarring divisiveness of the Cybertruck suggests that his runaway ego is already making Tesla’s products less palatable. If Musk’s retreat into a self-soothing fantasy bubble is also making his hype game less effective, and the 8% drop in Tesla’s stock price suggests that it is, his most important skill set is on the line.</p>



<p>Of course, with Wall Street analysts almost universally declaring themselves “underwhelmed,” it has to be asked: what else could they have possibly expected? Did they really believe that now, after eight years of empty hype, fake statistics, and blown deadlines, Tesla would actually start providing credible evidence to back the litany of bullshit? Having made no effort to explain his chronic inability to meet (let alone stop making) his self-imposed deadlines, and facing no real consequences for nearly a decade of what is either unprecedented public delusion or deception, why on earth would Elon Musk make a serious play for credibility now?</p>



<p>Having drawn a poor hand eight years ago, <a href="https://niedermeyer.io/2024/04/22/no-more-rebuys-mr-musk/">Elon Musk is playing poker the only way he knows how: going all-in on every hand</a>. This strategy has created a confidence game of unprecedented proportions in our financial markets, as every gambler in the casino wants to put a chip on the guy going all-in and winning every time, and only the $13 billion of hung debt for the Twitter deal suggests his hot streak might ever end. All Musk has to do to keep the music playing is to project confidence, which is infinitely easier to do in a studio lot setpiece than out on public roads.</p>



<p>For everyone not locked into this financial-cognitive nightmare, it’s hard to imagine anyone seriously believing that a night of delusional Disney Adult cringe might actually inflate Tesla’s stock beyond the current ~$680 billion valuation. Given that Tesla’s “Full Self-Driving” is already the subject of a multi-year <a href="https://apnews.com/article/tesla-investigations-justice-department-musk-self-driving-29a68864f75c9fabbd04f7a87d169444">federal investigation</a> into <a href="https://www.reuters.com/business/autos-transportation/tesla-autopilot-probe-us-prosecutors-focus-securities-wire-fraud-2024-05-08/">securities and wire fraud</a>, We, Robot’s blatant fantasy-mongering is downright shocking. If anything, showing a low-speed, closed-course theme park ride in order to build confidence around Tesla’s progress toward actual real-world driverless capability is almost too childish to call a fraud. </p>



<p>Ultimately, Musk’s increasingly-degenerate gambling run is slouching toward one last big coinflip: the 2024 presidential election. With <a href="https://www.nytimes.com/2024/10/11/us/politics/elon-musk-donald-trump-pennsylvania.html">Musk going “all-in” on Donald Trump</a>, and <a href="https://nypost.com/2024/10/07/us-news/elon-musk-suggests-hell-be-thrown-in-prison-if-harris-beats-trump-if-he-loses-im-fed/">musing that he will end up in a prison cell if Kamala Harris is elected</a>, it’s clear that his main political issue is his freedom to keep rolling over his endless confidence game without legal consequences. If Trump wins and delivers Musk the impunity he craves, the line between amusement park fantasy and $700 billion self-driving juggernaut will all but disappear, and we will all find ourselves living in Muskworld’s house of mirrors.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Superstitious Users and the FreeBSD Logo (105 pts)]]></title>
            <link>https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006642.html</link>
            <guid>41845427</guid>
            <pubDate>Tue, 15 Oct 2024 06:10:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006642.html">https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006642.html</a>, See on <a href="https://news.ycombinator.com/item?id=41845427">Hacker News</a></p>
<div id="readability-page-1" class="page">
   
<!--htdig_noindex-->
    <b>Brett Glass</b> 
    <a href="mailto:freebsd-chat%40freebsd.org?Subject=Superstitious%20users%20and%20the%20FreeBSD%20logo&amp;In-Reply-To=" title="Superstitious users and the FreeBSD logo">brett at lariat.net
       </a><br>
    <i>Wed Nov 30 02:30:04 UTC 2011</i>
    <ul>
        <li>Previous message: <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006641.html">Query about about freebsd-chat digestt, Vol 398, Issue 1
</a></li>
        <li>Next message: <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006643.html">Superstitious users and the FreeBSD logo
</a></li>
         <li> <b>Messages sorted by:</b> 
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/date.html#6642">[ date ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/thread.html#6642">[ thread ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/subject.html#6642">[ subject ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/author.html#6642">[ author ]</a>
         </li>
       </ul>
    <hr>  
<!--/htdig_noindex-->
<!--beginarticle-->
<pre>Everyone:

I just got a call from the owner of a hotel for which we provide 
hotspot service. She says that a guest spotted the "Powered by 
FreeBSD" logo at the bottom of the login page, and was offended; 
the guest was convinced that either we or the hotel management 
"worshipped the Devil" and refused to stay at the hotel unless the 
logo was removed. The owner could make no headway by explaining 
that the besneakered mascot was a cartoon character and was a 
daemon, not the Devil. And she feared upsetting the guest even more 
if she said that large portions of the same software are inside 
every Mac and iPad. The hotel stands to lose more than $1000 if the 
guest, who had originally planned to stay for a long period, moves out.

One of our tech support people also got a call directly from the 
hotel guest, who claimed that having the logo on the page 
constituted "abuse." The guest also claimed to be "losing money" 
because she wouldn't use the hotspot if there was a "devil" on the 
splash page. He didn't even realize what she was talking about at 
first.... He couldn't imagine why on Earth this person was calling 
him and going on about devils.

Attempts at misguided religious censorship notwithstanding, I don't 
want to see one of my  ISP's customers lose business. And I'd like 
to keep a FreeBSD logo on our hotspot page. Is there artwork that 
doesn't include horned creatures that might offend the ignorant or 
superstitious?

--Brett Glass

</pre>


<!--endarticle-->
<!--htdig_noindex-->
    <hr>
    <ul>
        <!--threads-->
	<li>Previous message: <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006641.html">Query about about freebsd-chat digestt, Vol 398, Issue 1
</a></li>
	<li>Next message: <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006643.html">Superstitious users and the FreeBSD logo
</a></li>
         <li> <b>Messages sorted by:</b> 
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/date.html#6642">[ date ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/thread.html#6642">[ thread ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/subject.html#6642">[ subject ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/author.html#6642">[ author ]</a>
         </li>
       </ul>

<hr>
<a href="http://lists.freebsd.org/mailman/listinfo/freebsd-chat">More information about the freebsd-chat
mailing list</a><br>
<!--/htdig_noindex-->

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cheating alleged after men's world conker champion found with steel chestnut (355 pts)]]></title>
            <link>https://www.theguardian.com/sport/2024/oct/14/cheating-alleged-after-mens-world-conker-champion-found-with-steel-chestnut</link>
            <guid>41844545</guid>
            <pubDate>Tue, 15 Oct 2024 03:12:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/sport/2024/oct/14/cheating-alleged-after-mens-world-conker-champion-found-with-steel-chestnut">https://www.theguardian.com/sport/2024/oct/14/cheating-alleged-after-mens-world-conker-champion-found-with-steel-chestnut</a>, See on <a href="https://news.ycombinator.com/item?id=41844545">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>The World Conker Championships is investigating cheating allegations after the men’s winner was found to have a steel chestnut in his pocket.</p><p>David Jakins won the annual title in Southwick, <a href="https://www.theguardian.com/uk-news/northamptonshire" data-link-name="in body link" data-component="auto-linked-tag">Northamptonshire</a>, on Sunday for the first time after competing since 1977.</p><p>But the 82-year-old was found to have a metal replica in his pocket when he was searched by organisers after his victory.</p><p>The retired engineer has denied using the metal variety in the tournament.</p><p>Jakins was responsible for drilling and inserting strings into other competitors’ chestnuts as the competition’s top judge, known as the “King Conker”.</p><p>Alastair Johnson-Ferguson, who lost in the men’s final against Jakins, said he suspected “foul play”, the Telegraph reported.</p><p>The 23-year-old said: “My conker disintegrated in one hit, and that just doesn’t happen … I’m suspicious of foul play and have expressed my surprise to organisers.”</p><p>Kelci Banschbach, 34, from Indianapolis, defeated the men’s champion in the grand final to become the first American to win the competition. More than 200 people took part.</p><p>Jakins said: “I was found with the steel conker in my pocket, but I only carry [it] around with me for humour value and I did not use it during the event.</p><p>“Yes, I did help prepare the conkers before the tournament. But this isn’t cheating or a fix, and I didn’t mark the strings.”</p><p>St John Burkett, a spokesperson for the World Conker Championships, said the cheating claims were being investigated.</p><p>“Allegations of foul play have been received that somehow King Conker swapped his real conker for the metal one later found in his pocket.</p><p>“Players select conkers from a sack before each round.</p><p>“There are also suggestions that King Conker had marked the strings of harder nuts. We can confirm he was involved in drilling and lacing the nuts before the event.</p><p>“We are investigating.”</p><p>More than 2,000 conkers had been prepared prior to the event.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zamba2-7B (272 pts)]]></title>
            <link>https://www.zyphra.com/post/zamba2-7b</link>
            <guid>41842975</guid>
            <pubDate>Mon, 14 Oct 2024 22:45:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.zyphra.com/post/zamba2-7b">https://www.zyphra.com/post/zamba2-7b</a>, See on <a href="https://news.ycombinator.com/item?id=41842975">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-animation="default" data-collapse="tiny" data-duration="400" data-easing="ease" data-easing2="ease" role="banner"><p><a href="https://www.zyphra.com/"><img width="123" sizes="(max-width: 479px) 123px, (max-width: 767px) 22vw, 123px" alt="" src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch.png" loading="lazy" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/66c3951edd1c45b91e0e0079_LogoStretch.png 1505w"></a></p></div><div><p><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash.jpg" loading="lazy" width="726" height="Auto" alt="" sizes="(max-width: 479px) 90vw, (max-width: 1279px) 80vw, 90vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-500.jpg 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-800.jpg 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-1080.jpg 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-1600.jpg 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-2000.jpg 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-2600.jpg 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash-p-3200.jpg 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6706d880c339bfc1098f384a_shubham-dhage-l1xJB5sN1ew-unsplash.jpg 5760w"></p><p><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670700157cd6aceff5b383ed_zamba27b.png" loading="lazy" width="648" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670700157cd6aceff5b383ed_zamba27b-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670700157cd6aceff5b383ed_zamba27b-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670700157cd6aceff5b383ed_zamba27b.png 1470w"></p></div><div><p>October 14, 2024</p><p>PALO ALTO, CALIFORNIA</p><p>Zyphra is excited to release Zamba2-7B, a state-of-the-art small language model. At the 7B scale, we outperform the leading models of Mistral, Google’s Gemma and Meta’s Llama3 series in both quality and performance. We believe Zamba2-7B is the leading model for running on-device and on consumer GPUs as well as for many enterprise applications which require a powerful but compact and efficient model for natural-language tasks.</p><p>Authors</p><p>Zyphra Team</p><p>Collaborators</p><p>Daniel A Roberts (Sequoia Capital &amp; MIT), Andrey Gromov (Meta FAIR), Kushal Tirumala (Meta FAIR) and Hassan Shapourian (Cisco)</p></div><div id="introduction"><div><div id="zamba1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="zamba2"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="Quality vs. Inference Speed" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="zamba3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div></section></div><div><div id="zyda1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="zyda2"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="zyda3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div></section><section id="zyda4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section></div><div><section id="small1"><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><div><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div></section><section id="small2"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="small3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section><section id="small4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a></section><section id="small5"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><div><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div></section></div><div><div id="rag1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="rag2"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="rag3"><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section><section id="rag4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a></section><div id="rag5"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></div><div><div id="tree1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="tree2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div></section><section id="tree3"><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" width="674" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" width="586" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section></div><div><div id="layer1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="layer2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" width="619" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" width="658" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" width="680" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div></section><div id="layer3"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></div><div><section id="edge1"><div><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" width="455" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><div id="edge2"><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><section id="edge3"><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section><div id="edge4"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><div id="edge5"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></div><div><div id="hop1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="hop2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="hop3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" width="581" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div></section><section id="hop4"><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section><div id="hop5"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><section id="hop6"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" width="367" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section><section id="hop7"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" width="479" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a></section><section id="hop8"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" width="458" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w"></a></section><section id="hop9"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png" loading="lazy" width="360" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png 1389w"></a></section></div><div><div target="_blank" id="cook1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="cook2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="cook3"><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a></section><div id="cook4"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><div id="cook5"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div><section id="cook7"><p>What is Annealing?</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a></section><section id="cook8"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a></section><section id="hop9"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png 1389w"></a></section></div><div><div target="_blank" id="mini1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="mini2"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="mini3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a></section><section id="mini4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a></section><section id="mini5"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png" loading="lazy" width="384" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w"></a><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div></section></div><div><section id="noc1"><div target="_blank"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" width="429" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="noc2"><div><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" width="267" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" width="319" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a><div><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section><section id="noc3"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" width="400" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a></section></div><div><div target="_blank" id="longrag1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="longrag2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><div target="_blank" id="longrag3"><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><section id="longrag4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><div target="_blank"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p></section><section id="longrag5"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div target="_blank"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section></div><div><div target="_blank" id="zamba2_1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="zamba2_2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" width="570" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" width="570" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div target="_blank"><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" width="570" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" width="319" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a></section><section id="zamba2_3"><div target="_blank"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" width="571" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png" loading="lazy" width="570" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png" loading="lazy" width="571" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png 1389w"></a><div target="_blank"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section></div><div><div target="_blank" id="zyda2_1"><ul role="list"><li>Zamba2-7B achieves SOTA evaluation benchmark performance and superior inference efficiency compared to currently leading 7B models such as Mistral-7B, Gemma-7B, and Llama3-8B</li><li>Zamba2-7B is extremely inference-efficient, achieving 25% faster time to first token, a 20% improvement in tokens per second, and a significant reduction in memory usage compared to models such as Llama3-8B.</li><li>Architectural improvements over <a href="https://arxiv.org/abs/2405.16712" target="_blank">Zamba1-7B</a>:<ul role="list"><li><a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba1</a> blocks have been replaced with <a href="https://arxiv.org/abs/2405.21060" target="_blank">Mamba2</a> blocks</li><li>Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.</li><li>We apply a LoRA projector to each shared MLP block, which allows the network to specialize the MLPs at each invocation of the shared layer across depth.</li></ul></li><li>We release the model weights open-source (Apache 2.0)</li></ul></div><section id="zyda2_2"><p>Zamba2 performs exceptionally well on standard language modeling evaluation sets, especially given its latency and generation speed. Among small language models (&lt;9B), we lead the pack in both quality and performance.</p></section><section id="zyda2_3"><div target="_blank"><p>Our model outperforms existing state-of-the-art models for the following reasons:</p><ol start="1" role="list"><li>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</li><li>Our 3 trillion token pre-training dataset, which is composed of a combination of <a href="https://arxiv.org/abs/2406.01981" target="_blank">Zyda</a> and openly-available datasets that are aggressively filtered and deduplicated and achieves a state-of-the-art quality in ablations vs the existing top open-source pretraining datasets.</li><li>We have a separate "annealing" pre-training phase, which rapidly decays the learning rate over 100B high-quality tokens. Our annealing set is carefully curated for quality and collated from varied high-quality sources.</li></ol><p>Due to the exceptional quality of our pretraining and annealing datasets, Zamba2-7B performs extremely well on a per-training-token basis, sitting comfortably above the curve traced out by competitor models.</p></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu.png 3456w"></a><p>Zamba2-7B utilizes and extends our original Zamba hybrid SSM-attention architecture. The core Zamba architecture consists of a backbone of Mamba layers interleaved with one or more shared attention layers (one shared attention in Zamba1, two in Zamba2). This attention has shared weights to minimize the parameter cost of the model. We find that concatenating the original model embeddings of the input to this attention block improves performance, likely due to better maintenance of information across depth. The Zamba2 architecture also applies LoRA projection matrices to the shared MLP to gain some additional expressivity in each block and allow each shared block to specialize slightly to its own unique position while keeping the additional parameter overhead small.</p><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-1600.png 1600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2000.png 2000w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-2600.png 2600w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new-p-3200.png 3200w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707df1fe10f98f8b7269326_evals_new.png 3456w"></a><div target="_blank"><p>We achieve state-of-the-art inference efficiency, including latency, throughput and memory usage because:</p><ol start="1" role="list"><li>Mamba2 blocks are extremely efficient, and have roughly 4 times the throughput of an equal-parameter transformer block.</li><li>Mamba blocks only have small hidden states to store and don't require a KV-cache, so we only need to store KV states for the invocations of the shared attention block.</li><li>We choose model sizings that are very amenable to parallelization on modern hardware (i.e. multiple streaming multiprocessors on GPUs, multiple cores on CPUs).</li></ol></div><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de16d0e11b5bf55867a0_tokens_mmlu.png 929w"></a><div target="_blank"><p>Zamba2-7B was trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM. Zamba2-7B thus demonstrates that at the 7B scale the frontier is still reachable and surpassable with a small team and moderate budget.</p><p>Zamba2-7B will be released under an open source license, allowing researchers, developers, and companies to leverage its capabilities. We invite the broader AI community to explore Zamba's unique architecture and continue pushing the boundaries of efficient foundation models. A Huggingface integration is available <a href="https://github.com/Zyphra/transformers_zamba2" target="_blank">here</a>, and a pure-pytorch implementation is available <a href="https://github.com/Zyphra/Zamba2" target="_blank">here</a>.</p><p>Zyphra's team is committed to democratizing advanced AI systems, exploring novel architectures on the frontier of performance, and advancing the scientific study and understanding of powerful models. We look forward to collaborating with others who share our vision.</p></div></section><section id="zyda2_4"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6708068af80b909c9424ad52_Zamba2_7B_DarkMode.png 1134w"></a></section><section id="zyda2_5"><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de29d70a1d7c77b74e06_ttft.png 1190w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de31937ef3a27a78df29_throughput.png 1189w"></a><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png" loading="lazy" alt="" sizes="(max-width: 479px) 100vw, (max-width: 991px) 80vw, (max-width: 1279px) 78vw, 79vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/6707de35353887430253284d_memory.png 1389w"></a></section><section id="zyda2_7"><h3>Analysis of Global Duplicates</h3><p>We present histograms depicting distribution of cluster sizes in all the datasets (see Fig. 7-11). Please, note that all the figures are in log-log scale. We see a significant drop in the number of clusters starting from the size of around 100. This drop is present both in DCLM and FineWeb-Edu2 (see Fig. 8 and 9 respectively), and most likely is explained by a combination of&nbsp; the deduplication strategy and quality when creating both datasets: DCLM deduplication was done individually within 10 shards, while FineWeb-Edu2 was deduplicated within every Common Crawl snapshot. We find that large clusters usually contain low quality material (repeated advertisements, license agreements templates, etc), so it’s not surprising that such documents were removed. Notably, DCLM still contained one cluster with the size close to 1 million documents, containing low quality documents seemingly coming from the advertisements (see Appendix).We find both Zyda-1and Dolma-CC contain a small amount of duplicates, which is expected, since both datasets were deduplicated globally by their authors. Remaining duplicates are likely false negatives from the initial deduplication procedure. Note, that distribution of duplicates clusters sizes of these two datasets (Fig. 10 and 11) don’t contain any sharp drops, but rather hyper exponentially decreases with cluster size.&nbsp;</p><a href="#"><img src="https://cdn.prod.website-files.com/img/placeholder-thumb.svg" loading="lazy" alt=""></a><h5><em>Figure 7: Distribution of cluster sizes of duplicates in global dataset (log-log scale).</em></h5><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9.png" loading="lazy" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce32dec6f79f17080a313_zyda2_9.png 1600w" alt=""></a><h5><em>Figure 8: Distribution of cluster sizes of duplicates in DCLM (log-log scale).</em></h5><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10.png" loading="lazy" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce3935947824a1a901e83_zyda2_10.png 1600w" alt=""></a><h5><em>Figure 9: Distribution of cluster sizes of duplicates in FineWeb-Edu2 (log-log scale).</em></h5><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11.png" loading="lazy" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce40649e136be23e5d07c_zyda2_11.png 1600w" alt=""></a><h5><em>Figure 10: Distribution of cluster sizes of duplicates in Zyda-1 (log-log scale).</em></h5><a href="#"><img src="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12.png" loading="lazy" sizes="100vw" srcset="https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12-p-500.png 500w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12-p-800.png 800w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12-p-1080.png 1080w, https://cdn.prod.website-files.com/669abfd62b0d2313e8024c99/670ce44b1a7ffa2b4d4f141f_zyda2_12.png 1600w" alt=""></a><h5><em>Figure 11: Distribution of cluster sizes of duplicates in Dolma-CC (log-log scale).</em></h5><h3>Largest cluster in DCLM</h3><p>Below is an example of the document from the largest cluster (~1M documents) of duplicates in DCLM (quality score 0.482627):<br>‍<em>Is safe? Is scam?<br>Is safe for your PC?<br>Is safe or is it scam?<br>Domain is SafeSafe score: 1</em>‍<br>‍<em>The higher the number, the more dangerous the website.Any number higher than 1 means DANGER.</em>‍<br>‍<em>Positive votes:<br>Negative votes:<br>Vote Up Vote Down review</em>‍<br>‍<em>Have you had bad experience with Warn us, please!</em></p><h3>Examples of varying quality score in DCLM in a cluster</h3><p>Below one will find a few documents with different quality scores from DCLM coming from the same duplicates cluster. Quality score varies from ~0.2 to ~0.04.</p></section></div></div>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Physics of Magic Windows (2021) (155 pts)]]></title>
            <link>https://mattferraro.dev/posts/caustics-engineering</link>
            <guid>41842775</guid>
            <pubDate>Mon, 14 Oct 2024 22:25:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mattferraro.dev/posts/caustics-engineering">https://mattferraro.dev/posts/caustics-engineering</a>, See on <a href="https://news.ycombinator.com/item?id=41842775">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><p><time datetime="2021-08-18">August 18, 2021</time></p><p>I recently made a physical object that defies all intuition. It's a square of acrylic, smooth on both sides, totally transparent. A tiny window.</p><div><p><img alt="Clear Acrylic" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fwindow_clear.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fwindow_clear.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fwindow_clear.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy"></p></div><p>But it has the magic property that if you shine a flashlight on it, it forms an image:</p><div><p><img alt="2D Image of Cat" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2F2D_image.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2F2D_image.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2F2D_image.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy"></p></div><p>And if you take it out in the sun, it produces this 3D hologram:</p><video width="100%" height="auto" autoplay="" muted="" controls="" loop=""><source src="https://mattferraro.dev/images/caustics-engineering/3dcat.mp4" type="video/mp4"></video><p>This post describes the math that went into making the object, and how you can create your own.</p><h2 id="but-first-how-is-this-even-possible">But first: How is this even possible?</h2><p>Let's focus on the 2D image before talking about the hologram.</p><p>The physical phenomenon we're looking at is called a <em>caustic</em>.</p><div><p><img alt="Example Caustic" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fglass_caustic_pd.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fglass_caustic_pd.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fglass_caustic_pd.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy"></p></div><p>Caustics are the bright patches of light we see when illuminating a transparent object. All the photons that don't pass directly through the object are what form the object's shadow. All those photons still have to go somewhere; they contribute to the caustic pattern.</p><p>The most interesting aspect of caustics is that they arise from even the tiniest of variations in surface flatness. Even the gentlest waves on the surface of a pool form powerful lenses that cast intense caustics on the floor below.</p><div><p><img alt="Water Caustics" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fwater_caustics_cc.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fwater_caustics_cc.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fwater_caustics_cc.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy"></p></div><p>The reason my acrylic square can form an image is because I've distributed just the right amount of concavity and convexity into the surface so that the refracted light forms a caustic image.</p><p>To gain some intuition for how it is done, consider a traditional convex lens:</p><div><p><img alt="Parabolic Lens" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ftraditional_lens_2.svg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ftraditional_lens_2.svg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ftraditional_lens_2.svg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>This lens forms the simplest possible caustic. If all the incoming light is from a single, very distant light source like the Sun, this lens focuses all of its incoming light into a single point. The caustic image from this lens is dark everywhere with one very bright spot in the center.</p><p>Zooming in on one small section of the lens we notice a few properties:</p><ol><li>The overall thickness of the lens does not have a direct impact on the outgoing ray angle. We could add material to the left side of this lens and nothing would change. The first transition, from air to glass, can be entirely ignored.</li><li>The angle between the incoming light rays and the glass-air boundary has a strong effect on the refracted ray angle.</li><li>Whether two rays converge or diverge is controlled by how <em>curved</em> the lens is where the glass meets the air</li></ol><p>In other words, the height of the glass <span>h(x)</span> is not on its own important. But the slope of the glass, <span>\frac{\mathrm{d}h}{\mathrm{d}x}</span>, gives us the outgoing ray angle via Snell's law. Where rays converge the image is brighter than the light source. Where rays diverge the image is darker. Therefore the brightness of the image (at that point, where the rays fall) is related to <span>\frac{\mathrm{d}^2h}{\mathrm{d}x^2}</span>.</p><p>The thickness of my acrylic slab varies across the entire <span>xy</span> plane, so I'll call it <span>h(x,y)</span> and we'll think of it as a <strong>heightmap</strong>.</p><p>By controlling <span>\nabla h = (\frac{\partial h}{\partial x}, \frac{\partial h}{\partial y}</span>), and <span>\nabla ^2 h = (\frac{\partial ^2 h}{\partial x^2} + \frac{\partial ^2 h}{\partial y^2})</span>, we can steer all of our incoming light to the correct locations in the image, while contributing the right brightness to make it recognizable. By making some simplifying assumptions we can guarantee that the resulting heightmap will be smooth and continuous.</p><p>For the Magic Window shown above, the total height variation over the <span>10cm \times 10cm</span> surface is about <span>2.0mm</span>.</p><div><p><img alt="Slight Refraction" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fslight_refraction.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fslight_refraction.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fslight_refraction.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>See how the slight variations in surface height distort the straight line of the floor moulding? Our Magic Window works like any other lens—by bending light.</p><h2 id="table-of-contents">Table of Contents</h2><ul><li><a href="#formulating-the-problem">Formulating the Problem</a></li><li><a href="#steps-to-a-solution">Steps to a Solution</a></li><li><a href="#morphing-the-cells">Morphing the Cells</a><ul><li><a href="#computing-the-loss">Computing the Loss</a></li><li><a href="#stepping-to-reduce-loss">Stepping to Reduce Loss</a></li></ul></li><li><a href="#snells-law-and-normal-vectors">Snell's Law and Normal Vectors</a></li><li><a href="#finding-the-heightmap">Finding the Heightmap</a></li><li><a href="#manufacturing">Manufacturing</a></li><li><a href="#acknowledgements">Acknowledgements</a></li><li><a href="#my-code">My Code</a></li><li><a href="#licensing">Licensing</a></li><li><a href="#contact-me">Contact me</a></li><li><a href="#one-last-thing">One Last Thing</a></li></ul><h2 id="formulating-the-problem">Formulating the Problem</h2><p>We want to find a heightmap <span>h(x,y)</span> whose caustic image has brightness <span>b(u,v)</span>, equal to some input image. To achieve this we can imagine a grid of cells, akin to pixels, on the surface of the acrylic lens. Here each "pixel" on the lens corresponds to a pixel in the image. Image pixels and their corresponding lens-space "pixels" are labeled with shared <span>(i, j)</span> coordinates.</p><div><p><img alt="Diagram" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fdiagram.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fdiagram.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fdiagram.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Remember that <span>(i, j)</span> are integers labeling the column and row of the pixel, whereas <span>(x, y)</span> and <span>(u, v)</span> are real numbers measured in something like meters or inches.</p><h2 id="steps-to-a-solution">Steps to a Solution</h2><p><strong>Step 1:</strong> We morph the cells on the lens, making them bigger or smaller, so that the area of lens cell <span>(i, j)</span> is proportional to the brightness of image cell <span>(i, j)</span>. The resulting lens grid is no longer square—lots of warping and skew have to be introduced to maintain continuity. This step is by far the hardest part and must be solved iteratively.</p><p><strong>Step 2:</strong> For each cell <span>(i, j)</span> we need to find the angle from the lens cell to image cell and use Snell's law to find the required surface normal. This step is straightforward geometry.</p><p><strong>Step 3:</strong> Integrate all the surface normals to find a continuous heightmap <span>h(x,y)</span>. We're back to iterative methods here, but if we apply certain contraints to how we solve step 1, this step is actually fast and easy.</p><h2 id="morphing-the-cells">Morphing the Cells</h2><p>For an image with <span>n \times n</span> pixels, the lens grid will need <span>(n+1) \times (n+1)</span> points, so that each cell in the lens grid is defined by four points. Technically we should adopt yet another coordinate system to label the <em>points</em> in the lens grid since they are distinct from the <em>cells</em> in the lens grid, but I think it's easier to just reuse <span>(i, j)</span> and we can say that for grid cell <span>(i, j)</span>, the point in the upper left is defined as grid point <span>(i, j)</span>.</p><div><p><img alt="Diagram 2" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fdiagram2.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fdiagram2.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fdiagram2.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>This leaves us with one row and one column of extra grid points along the bottom and right edges, but that will be trivial to deal with when it comes up.</p><p>Each <em>point</em> in the lens grid <span>(i,j)</span> has an <span>(x, y)</span> coordinate. A point's <span>(i, j)</span> coordinates never change but the <span>(x, y)</span> coordinates will change as we morph the cells more and more.</p><h2 id="computing-the-loss">Computing the Loss</h2><p>Given the <span>(x, y)</span> locations of all the lens grid points, simple geometry lets us calculate the area of each lens grid cell. Of course at first every cell has the same area, but that will change as soon as we start morphing things.</p><p>The condition we want is that every lens grid <em>cell</em> <span>(i, j)</span> has an <em>area</em> which scales with the <em>brightness</em> of image pixel <span>b(i, j)</span>.</p><p>Area and brightness are not compatible units so it is helpful to normalize cell area by the full window area, and pixel brightness by total image brightness, so that each is measured in a unitless "percentage".</p><p>\tag{1.0}
\frac{A_{ij}}{\Sigma A} = \frac{b_{ij}}{\Sigma b}</p><p>Intuitively, this means:</p><blockquote><p>If a single pixel contributes <span>x\%</span> of the brightness of the entire image, the corresponding window cell should take up <span>x\%</span> of the area of the entire window.</p></blockquote><p>Equation <span>(1.0)</span> is the goal, but it will not be not be true until after we've morphed the window grid. Until we've done that, we need to compute a loss function which tells us how badly we're missing our target. Something like:</p><p>\tag{1.1}
L = \frac{b_{ij}}{\Sigma b} - \frac{A_{ij}}{\Sigma A}</p><p>In code:</p><pre><code><span># In Julia-flavored psuedocode</span>
img <span>=</span> read_image<span>(</span><span>"cat.png"</span><span>)</span>
brightness <span>=</span> convert_to_grayscale<span>(</span>img<span>)</span>
total_brightness <span>=</span> sum<span>(</span>brightness<span>)</span>
brightness <span>=</span> brightness <span>.</span><span>/</span> total_brightness

w <span>=</span> <span>.1</span> <span># meters</span>
h <span>=</span> <span>.1</span> <span># meters</span>
area_total <span>=</span> w <span>*</span> h
loss <span>=</span> compute_pixel_area<span>(</span>grid<span>)</span> <span>.</span><span>/</span> area_total <span>-</span> brightness
</code></pre><div><p><img alt="Image and Loss Function" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fimage_and_loss.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fimage_and_loss.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fimage_and_loss.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Where I've colorized the loss function so that red areas indicate regions where our grid cells need to grow and blue regions indicate where our grid cells need to shrink.</p><p>This image is the loss function <span>L</span> and I'll refer to it a lot. </p><h2 id="stepping-to-reduce-loss">Stepping to Reduce Loss</h2><p>The loss image can be thought of as a scalar field <span>L(x, y)</span>. The gradient of a scalar field yields a vector field, which we could call <span>\nabla L(x,y)</span>. We can step each grid point slowly in the direction of the gradient field, and in doing so the cells that are too small will get bigger and the cells that are too big will get smaller. Our loss will shrink, and we'll create our image!</p><p>The first thing to do is compute <span>\nabla L</span> and look at the vector field:</p><div><p><img alt="Gradient of L as a vector field" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fgrad_L.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fgrad_L.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fgrad_L.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Crap.</p><p><span>\nabla L</span> is a very poorly behaved vector field. It is noisy, discontinuous, and in many places equal to zero.</p><p>Almost everywhere, neighboring points need to step in drastically different directions. This creates a situation where improving one cell's loss will necessarily worsen its neighbor's losses, which means that in practice this method can never converge. It's a dead end.</p><hr><p>Instead let's draw an analogy to Computational Fluid Dynamics. We need to dilate certain cells and shrink others according to a brightness function. This is similar to modeling compressible air flow where each cell has pressure defined as a pressure function.</p><p>If every cell in a 2D grid has some initial pressure, how does the system relax over time? The regions with high pressure expand and the regions of low pressure contract, with regions of middling pressure getting shoved around in a sort of global tug-of-war. Clearly, our problem is analogous.</p><p>So, how is this problem solved in CFD simulations? A standard approach is to define a <strong>Velocity Potential</strong> called <span>\Phi</span> (read: <em>phi</em>). The Velocity Potential <span>\Phi</span> is a scalar field defined at each cell. Its units are <span>meters^2 / second</span> which at first glance is not very easy to interpret. But the reason <span>\Phi</span> is convenient is that its spatial derivatives are measured in <span>meters/second</span>. In other words, the gradient of <span>\Phi</span> gives a vector whose units are velocity:</p><p>\tag{1.2}
\nabla \Phi = \left( \frac{\partial{\Phi}}{\partial{x}}, \frac{\partial{\Phi}}{\partial{y}} \right) = \vec{v}</p><div><p><img alt="Phi" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_phi.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_phi.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_phi.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Here is an example <span>\Phi</span>. It is just some scalar field best viewed as a heightmap.</p><div><p><img alt="Gradient of Phi" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_grad_phi.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_grad_phi.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_grad_phi.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>And here is the gradient of that same <span>\Phi</span>. These vectors are velocity vectors that point uphill. If we were performing Computational Fluid Dynamics, these vectors would indicate how fluid might flow from regions of high pressure to regions of low pressure. </p><p>Notice how well behaved this vector field is! There is gentle variation across the field but any two neighbors are very similar to each other. None of the arrows pierce the boundary.</p><p>In our case we don't have fluid pressure, we have light pressure. Regions in our image which are too bright have high light pressure, which is quantified in our loss function <span>L</span>.</p><p>If we can somehow use <span>L</span> to find a <span>\Phi</span> that describes our light pressure distribution, all we need to do is calculate <span>\vec{v} = \nabla \Phi</span> and we'll be able to morph all of our lens grid points according to <span>\vec{v}</span> to decrease our loss!</p><p>So how do we find a suitable <span>\Phi</span>? Well, the property we know about each cell is its loss, which encodes how much that cell needs to grow or shrink. </p><blockquote><p>This property, how much a cell grows or shrinks over time as it moves with a velocity field, is called the <strong>divergence</strong> of that field.</p></blockquote><p>Divergence is written as <span>\nabla \cdot</span>, so in our case, we know that we need to find a velocity field <span>\vec{v}</span> whose divergence equals the loss:</p><p>\tag{1.3}
\nabla \cdot \vec{v} = L(x, y)</p><p>Unfortunately there is no "inverse divergence" operator so we cannot easily invert this equation to find <span>\vec{v}</span> directly. But we <em>can</em> plug equation <span>(1.2)</span> in to equation <span>(1.3)</span> to yield:</p><p>\tag{1.4}
\nabla \cdot \nabla \Phi = L(x, y)</p><p>Which we read as <em>The divergence of the gradient of the potential field <span>\Phi</span> equals the loss</em>.</p><p>This equation comes up surprisingly frequently in many branches of physics and math. It is usually written in a more convenient shorthand:</p><p>\tag{1.5}
\nabla ^2 \Phi = L</p><p>Which you may recognize as <a href="https://mattferraro.dev/posts/poissons-equation">Poisson's Equation</a>!</p><p>This is fantastic news because Poisson's equation is <a href="https://mattferraro.dev/posts/poissons-equation#how-do-i-solve-it">extremely easy</a> to solve! If you aren't familiar with it, just think of this step like inverting a big matrix, or numerically integrating an ODE, or finding the square root of a real number. It's an intricate, tedious task that would be painful to do with a paper and pencil, but it's the kind of thing computers are <em>really</em> good at.</p><p>Now that we've written down the problem as Poisson's Equation, it is as good as solved. We can use any off the shelf solver, plug in our known <span>L(x, y)</span> using Neumann boundary conditions and boom, and out pops <span>\Phi(x,y)</span> as if by magic.</p><div><p><img alt="Phi" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fclearly_cat_phi.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fclearly_cat_phi.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fclearly_cat_phi.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Can you figure out why the cat appears so clearly in this 3D rendering of <span>\Phi</span>? What controls the brightness of each pixel in a render like this?</p><p>We plug <span>\Phi</span> in to Equation <span>(1.2)</span> to find <span>\vec{v}</span> and we take a look at the vector field:</p><div><p><img alt="Gradient of Phi" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_grad_phi.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_grad_phi.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fexample_grad_phi.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Disappointingly, it does not look like a kitty to me.</p><p>And technically we need to march our points in the direction of <em>negative</em> <span>\nabla L</span> if we want to <em>decrease</em> <span>L</span>. Here's <span>-\nabla L</span>:</p><div><p><img alt="Negative Gradient of Phi" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fnegative_grad_phi.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fnegative_grad_phi.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fnegative_grad_phi.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>But the good news is that this vector field is smooth and well-behaved. We simply march the grid points along this vector field and we'll get exactly what we need. </p><p>If you squint you can almost see how the bright background will expand and the cat's dark fur will shrink.</p><div><p><img alt="Image and Vector Field" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fside_by_side.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fside_by_side.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fside_by_side.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>We step all the lens grid points forward some small amount in the direction of <span>-\vec{v}</span>. After morphing the grid a tiny amount we recompute the loss function <span>L</span>, find a new <span>\Phi</span> and new <span>-\vec{v}</span>, and take another small step.</p><pre><code><span># In Julia-flavored psuedocode</span>
image <span>=</span> read_image<span>(</span><span>"cat.png"</span><span>)</span>
gray <span>=</span> convert_to_grayscale<span>(</span>image<span>)</span>
grid <span>=</span> create_initial_grid<span>(</span>gray<span>.</span>size <span>+</span> <span>1</span><span>)</span>

L <span>=</span> compute_loss<span>(</span>gray<span>,</span> grid<span>)</span>

<span>while</span> max<span>(</span>L<span>)</span> <span>&gt;</span> <span>0.01</span>
    ϕ <span>=</span> poisson_solver<span>(</span>L<span>,</span> <span>"neumann"</span><span>,</span> <span>0</span><span>)</span>
    v <span>=</span> compute_gradient<span>(</span>ϕ<span>)</span>
    grid <span>=</span> step_grid<span>(</span>grid<span>,</span> <span>-</span>v<span>)</span>
    L <span>=</span> compute_loss<span>(</span>gray<span>,</span> grid<span>)</span>
<span>end</span>
</code></pre><p>After three or four iterations the loss gets very small and we've got our morphed cells!</p><div><p><img alt="Grid After Warping" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fside_by_side_warped.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fside_by_side_warped.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fside_by_side_warped.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Look at how this cat's chin ballooned out but her nose and forehead shrunk. Her left ear is noticably longer and thinner because the bright background had to grow to take up more light. Her pupils went from oblong to sharp.</p><p>Note that image on the right is just a screenshot of Fusion360's default mesh rendering with the wireframe turned on:</p><div><p><img alt="Screenshot of Fusion360" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2FFusion360.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2FFusion360.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2FFusion360.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>The reason it is darker in some areas is because the mesh is more tightly packed in those areas. Let's zoom in on the eye:</p><div><p><img alt="Zoom in on the Eye" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fzoom_eye.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fzoom_eye.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fzoom_eye.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Look at how detailed that is! We've managed to capture even the bright reflections in her eyes. Zooming in further to just the pupil:</p><div><p><img alt="Zoom in on the Pupil" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fzoom_pupil.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fzoom_pupil.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fzoom_pupil.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>We can see the fine structure of the grid cells. Our formulation of the problem is only concerned with cells as quadralaterals. The triangles you see are just an artifact of converting our quadralateral grid into a triangle mesh more suitable for other software to deal with.</p><p>So again, in summary:</p><div><p><img alt="Overall Flow" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Foverall_flow.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Foverall_flow.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Foverall_flow.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>If we follow these steps we will successfully morph our grid points. Now we've got to do some geometry!</p><h2 id="snells-law-and-normal-vectors">Snell's Law and Normal Vectors</h2><p>Snell's law tells us how light bends when passing from one material to another. </p><div><p><img alt="Snell's Law" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fsnells_law_2.svg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fsnells_law_2.svg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fsnells_law_2.svg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>\tag{2.0}
\frac{\sin(\theta_2)}{\sin(\theta_1)} = \frac{n_1}{n_2}</p><p>Where <span>n_1 = 1.49</span> is the <a href="https://en.wikipedia.org/wiki/Refractive_index">Refractive Index</a> of acrylic and <span>n_2 = 1</span> is the refractive index of air. If we know <span>\theta_2</span>, Snell's Law gives us <span>\theta_1</span>.</p><p>Snell's law is not some arbitrary axiom of physics. It is a direct consequence of Fermat's <a href="https://en.wikipedia.org/wiki/Fermat%27s_principle">Principle of Least Time</a>, which is a fascinating and critical link between ray optics and wave optics. But that's a topic for another day.</p><p>In our case, each lens cell <span>(i, j)</span> has migrated to position <span>(x, y)</span>, and it needs to send its light to the image plane at <span>(u, v)</span>, which sits some distance away <span>d</span>.</p><p>We start by defining a 3D normal vector <span>\vec{N}(x, y)</span> which everywhere points normal to our heightmap <span>h(x, y)</span>.</p><div><p><img alt="Example Surface Normals" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fsurface_normals.svg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fsurface_normals.svg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fsurface_normals.svg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Normal vectors always point perpendicular to the surface they start on. They generally encode meaning in their direction, not their length, so we're free to scale them to any length that is convenient for our purposes. Very often people choose to make their Normal vectors of length <span>1</span>.</p><p>But if we normalize <span>\vec{N}</span> so that its <span>z</span> coordinate is <span>-1</span>, we can write it:</p><p>\tag{2.1}
\vec{N} = (\frac{\partial{h}}{\partial{x}}, \frac{\partial{h}}{\partial{y}}, -1)</p><p>If you consider just the <span>x</span> and <span>y</span> components, we recognize that</p><p>\tag{2.2}
\vec{N}_{xy} = \nabla h</p><p>Which is a property often used in computer graphics applications, as well as geospatial applications involving <a href="https://en.wikipedia.org/wiki/Digital_elevation_model">Digital Elevation Models</a>.</p><p>Using Snell's Law, a small angle approximation, and a lot of tedious geometry, we find the <span>x</span> and <span>y</span> components of the normal vector <span>\vec{N}</span>:</p><p>\tag{2.3}
N_x(i, j) = \tan \frac{\tan^{-1} \left( \frac{u - x} {d} \right)} {(n_1 - n_2)}</p><p>\tag{2.4}
N_y(i, j) = \tan \frac{\tan^{-1} \left( \frac{v - y} {d} \right)} {(n_1 - n_2)}</p><p>There is nothing interesting about this derivation so I've skipped it here.</p><h2 id="finding-the-heightmap">Finding the Heightmap</h2><p>At this point we have our morphed grid cells and we've found all our surface normals. All we have to do is find a heightmap <span>h(x,y)</span> that has the required surface normals.</p><p>Unfortunately, this is not a problem that is solvable in the general case.</p><p>We could try to integrate the normals manually, starting at one corner and working our way down the grid, but this method does not usually result in a physically realizable object. </p><p>If the integral of the normals running left to right pulls your surface up, but the integral of the normals running top to bottom pulls your surface down, there is just no solution that results in a solid, unbroken surface.</p><p>A much better approach is to reach back to equation <span>(2.2)</span>, repeated here:</p><p>\tag{2.2}
\vec{N}_{xy} = \nabla h</p><p>And to take the divergence of both sides:</p><p>\tag{2.5}
\nabla \cdot \vec{N}_{xy} = \nabla \cdot \nabla h</p><p>Do you recognize the form of this equation? Adopting shorthand and swapping sides:</p><p>\tag{2.6}
\nabla ^2 h = \nabla \cdot \vec{N}_{xy}</p><p>We arrive at yet another instance of <a href="https://mattferraro.dev/posts/poissons-equation">Poisson's Equation</a>! We found <span>\vec{N}_{xy}</span> in the previous section, and calculating the divergence of a known vector field is easy:</p><p>\tag{2.7}
\nabla \cdot \vec{N}_{xy} = \left( \frac{\partial}{\partial{x}}, \frac{\partial}{\partial{y}} \right) \cdot (\vec{N}_x, \vec{N}_y) = \frac{\partial{\vec{N}_x}}{\partial{x}} + \frac{\partial{\vec{N}_y}}{\partial{y}}</p><p>In code it looks like:</p><pre><code>δx <span>=</span> <span>(</span>Nx<span>[</span>i<span>+</span><span>1</span><span>,</span> j<span>]</span> <span>-</span> Nx<span>[</span>i<span>,</span> j<span>]</span><span>)</span>
δy <span>=</span> <span>(</span>Ny<span>[</span>i<span>,</span> j<span>+</span><span>1</span><span>]</span> <span>-</span> Ny<span>[</span>i<span>,</span> j<span>]</span><span>)</span>
divergence<span>[</span>i<span>,</span> j<span>]</span> <span>=</span> δx <span>+</span> δy
</code></pre><p>All that's left is to plug our known <span>\nabla \cdot \vec{N}_{xy}</span> in to a Poisson solver with Neumann boundary conditions and out pops <span>h(x, y)</span>, ready to use!</p><p>Well, there's one thing left to improve. By modifying the height of each point we've actually changed the distance from each lens point to the image, so the lens-image distance is no longer a constant <span>d</span> it is actually a function <span>D(x,y)</span>. With our heightmap in hand we can easily calculate:</p><p>\tag{2.8}
D(x,y) = d - h(x,y)</p><p>And repeat the process by calculating new normals using <span>D(x,y)</span> instead of <span>d</span>, which lets us create a new heightmap.</p><p>We can loop this process and measure changes to ensure convergence, but in practice just 2 or 3 iterations is all you need:</p><pre><code><span># In Julia-flavored psuedocode</span>
d <span>=</span> <span>.2</span> <span># meters</span>
D <span>=</span> d <span>.</span><span>*</span> array_of_ones<span>(</span>n<span>,</span> n<span>)</span>

<span>for</span> i <span>in</span> <span>1</span><span>:</span><span>3</span>
    Nx<span>,</span> Ny <span>=</span> compute_normals<span>(</span>grid<span>,</span> D<span>)</span>
    divergence <span>=</span> compute_divergence<span>(</span>Nx<span>,</span> Ny<span>)</span>
    h <span>=</span> poisson_solver<span>(</span>divergence<span>,</span> <span>"neumann"</span><span>,</span> <span>0</span><span>)</span>
    D <span>=</span> copy<span>(</span>h<span>)</span>
<span>end</span>
</code></pre><p>The resulting heightmap can be converted to a solid object by adopting a triangular grid and closing off the back surface.</p><div><p><img alt="Final Object" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinal_object_1.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinal_object_1.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinal_object_1.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Note that the image looks mirrored when looking at it head on. That's because the heightmap forms the <em>back</em> surface of the Magic Window. The front surface is factory flat.</p><div><p><img alt="Final Object" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinal_object_2.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinal_object_2.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinal_object_2.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>The height differences are subtle but certainly enough to get the job done.</p><div><p><img alt="Finished Product" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinished_product.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinished_product.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Ffinished_product.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><h2 id="manufacturing">Manufacturing</h2><p>The process of manufacturing our Magic Window is identical to carving any other 2.5D object.</p><p>We bring our object into Fusion360 or any other CAM software. We set up a roughing toolpath left to right, and a finishing toolpath top to bottom just like you find in most tutorials.</p><p>Any old CNC router or mill will work. I designed and built my own router last year. If you want to do the same I recommend you start <a href="https://mattferraro.dev/posts/cnc-router">here</a>.</p><p>I used a <span>\frac{1}{4}</span> inch diameter, ball-nosed, carbide bit for both roughing and finishing passes, which took 10 minutes and 90 minutes respectively.</p><div><p><img alt="On the Router" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fon_the_router.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fon_the_router.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fon_the_router.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>After carving the surface finish is rough and transluscent. We need to wet sand using <span>200, 400, 600, 1000</span> and <span>1500</span> grit sandpapers, then finish with a soft rag and some automotive polish. Sanding and polishing takes about half an hour for a <span>10 cm \times 10 cm</span> Magic Window.</p><div><p><img alt="After Sanding" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fafter_sanding.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fafter_sanding.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcaustics-engineering%2Fafter_sanding.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><h2 id="acknowledgements">Acknowledgements</h2><p>All of the math for this post came from <a href="http://nishitalab.org/user/egaku/tog14/yue-continuous-caustics-lens.pdf">Poisson-Based Continuous Surface Generation for Goal-Based Caustics</a>, a phenomenal 2014 paper by Yue et al. If you continue this work in some way, please cite them.</p><h2 id="my-code">My Code</h2><p>My source code is available <a href="https://github.com/MattFerraro/causticsEngineering">here</a>. I am a novice at programming in Julia so if you have suggestions for how to improve this code, please reach out or make a pull request!</p><p><strong>Caveats</strong>: There are a lot of issues with my code. I confuse <span>x</span> and <span>y</span> in several places. I have extra negative signs that I inserted that make the code work but I don't know why. My units and notation are inconsistent throughout. The original paper suggests a better way of calculating loss but I didn't implement it because the naive way was easier, yet I rolled my own mesh utilities and Poisson solver because I enjoyed the challenge.</p><p>In short: To me this code is a fun side project. If you want to build a business off of this code you should probably hire someone who knows how to program professionally in Julia.</p><h2 id="licensing">Licensing</h2><p>I've posted all my code under the MIT license. Please feel free to use this code for anything you want, including hobbyist, educational, and commercial uses. I only ask that if you make something, please show me!</p><p>Except where otherwise attributed, all images in this blog post and the blog post itself are my own work that I license as <a href="https://creativecommons.org/licenses/by/2.0/">CC-BY</a>.</p><p>The cat in this post is named Mitski and she approves of you using her image as the new standard reference image for image processing papers. It's time to let Lenna <a href="https://en.wikipedia.org/wiki/Lenna#Criticism">retire</a>.</p><p>If you use my code to make your own Magic Windows, I'd love to see them! I'm on Twitter at <a href="https://twitter.com/mferraro89">@mferraro89</a>. Email me at <a href="mailto:mattferraro.dev@gmail.com">mattferraro.dev@gmail.com</a> and I will gladly help if you get stuck!</p><h2 id="one-last-thing">One Last Thing</h2><p>I know what you're thinking. <em>What about the hologram?!</em></p><p>Does the math above imply that a hologram will always be created, or is this one cat hologram just an incredible coincidence?</p><p>Well you see, I've discovered a truly marvelous proof of this, which this website's margin is unfortunately too narrow to contain :)</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Routine dental X-rays are not backed by evidence–experts want it to stop (279 pts)]]></title>
            <link>https://arstechnica.com/health/2024/10/do-you-really-need-those-routine-dental-x-rays-probably-not/</link>
            <guid>41842294</guid>
            <pubDate>Mon, 14 Oct 2024 21:37:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/health/2024/10/do-you-really-need-those-routine-dental-x-rays-probably-not/">https://arstechnica.com/health/2024/10/do-you-really-need-those-routine-dental-x-rays-probably-not/</a>, See on <a href="https://news.ycombinator.com/item?id=41842294">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
            <article data-id="2056298">
  
  <header>
  <div>
    <div>
      

      

      <p>
        The actual recommendations might surprise you—along with the state of modern dentistry.
      </p>

      
    </div>

          <div>
        <p><img width="1000" height="1000" src="https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-907343114-1000x1000.jpg" alt="" loading="eager" decoding="async" fetchpriority="high" srcset="https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-907343114-1000x1000.jpg 1000w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-907343114-150x150.jpg 150w, https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-907343114-500x500.jpg 500w" sizes="(max-width: 1000px) 100vw, 1000px">
        </p>
        <div>
    
    <p>
      An expert looking at a dental X-ray and saying "look at that unnecessary X-ray," probably.

              <span>
          Credit:

                      <a href="https://www.gettyimages.com/detail/photo/dentist-showing-a-patient-her-x-ray-royalty-free-image/907343114?phrase=dental+x+rays&amp;adppopup=true">
          
          Getty | MilanEXPO

                      </a>
                  </span>
          </p>
  </div>
      </div>
      </div>
</header>

  

  
      
    
    <div>
                      
                      
          
<p>Has your dentist ever told you that it's recommended to get routine dental X-rays every year? My (former) dentist's office did this year—in writing, even. And they claimed that the recommendation came from the American Dental Association.</p>
<p>It's a common refrain from dentists, but it's false. The American Dental Association <em>does not</em> recommend annual routine X-rays. And this is not new; it's been that way for well over a decade.</p>
<p><a href="https://www.ada.org/-/media/project/ada-organization/ada/ada-org/files/resources/library/oral-health-topics/dental_radiographic_examinations_2012.pdf?rev=fd33893f4d634cbaab92733c2313c354&amp;hash=45F728CEF900B5B654539635A9147AA9">The association's guidelines from 2012</a> recommended that adults who don't have an increased risk of dental caries (myself included) need only bitewing X-rays of the back teeth every two to three years. Even people with a higher risk of caries can go as long as 18 months between bitewings. The guidelines also note that X-rays should not be preemptively used to look for problems: "Radiographic screening for the purpose of detecting disease before clinical examination should not be performed," the guidelines read. In other words, dentists are supposed to examine your teeth <em>before</em> they take any X-rays.</p>
<p>But, of course, the 2012 guidelines are outdated—the latest ones go further. In <a href="https://jada.ada.org/article/S0002-8177(23)00734-1/fulltext">updated guidance published in April</a>, the ADA doesn't recommend any specific time window for X-rays at all. Rather, it emphasizes that patient exposure to X-rays should be minimized, and any X-rays should be clinically justified.</p>
<p>There's a good chance you're surprised. Dentistry's overuse of X-rays is a problem dentists do not appear eager to discuss—and would likely prefer to skirt. My former dentist declined to comment for this article, for example. And other dentists have been doing that for years. Nevertheless, the problem is well-established. A New York Times article from 2016, titled "<a href="https://www.nytimes.com/2016/07/26/upshot/you-probably-dont-need-dental-x-rays-every-year.html">You Probably Don’t Need Dental X-Rays Every Year</a>," quoted a dental expert noting the exact problem:</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>"Many patients of all ages receive bitewing X-rays far more frequently than necessary or recommended. And adults in good dental health can go a decade between full-mouth X-rays."</p>
<h2>Data is lacking</h2>
<p>The problem has bubbled up again in a series of commentary pieces published in JAMA Internal Medicine today. The pieces were all sparked by a viewpoint that Ars reported on in May, in which three dental and health experts highlighted that <a href="https://arstechnica.com/science/2024/05/do-you-need-a-dentist-visit-every-6-months-that-filling-the-data-is-weak/">many routine aspects of dentistry, including biannual cleanings, are not evidence-based</a> and that the industry is rife with overdiagnosis and overtreatment. That viewpoint, titled "<a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2818193">Too Much Dentistry</a>," also appeared in JAMA Internal Medicine.</p>
<p>The new pieces take a more specific aim at dental radiography. But, as in the May viewpoint, experts also blasted dentistry more generally for being out of step with modern medicine in its lack of data to support its practices—practices that continue amid financial incentives to overtreat and little oversight to stop it, they note.</p>
<p>In a piece titled "<a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2825067">Too Much Dental Radiography</a>," Sheila Feit, a retired medical expert based in New York, pointed out that using X-rays for dental screenings is not backed by evidence. "Data are lacking about outcomes," she wrote. If anything, the weak data we have makes it look ineffective. For instance, <a href="https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD014545/full">a 2021 systemic review</a> of 77 studies that included data on a total of 15,518 tooth sites or surfaces found that using X-rays to detect early tooth decay led to a high degree of false-negative results. In other words, it led to missed cases.</p>
<p>Feit called for gold-standard randomized clinical trials to evaluate the risks and benefits of X-ray screenings for patients, particularly adults at low risk of caries. "Financial aspects of dental radiography also deserve further study," Feit added. Overall, Feit called the May viewpoint "a timely call for evidence to support or refute common clinical dental practices."</p>

          
                  </div>
                    
        
          
    
    <div>
          
          

<h2>Dentistry without oversight</h2>
<p><a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2825065">In a response</a> published simultaneously in JAMA Internal Medicine, oral medicine expert Yehuda Zadik championed Feit's point, calling it "an essential discussion about the necessity and risks of routine dental radiography, emphasizing once again the need for evidence-based dental care."</p>
<p>Zadik, a professor of dental medicine at The Hebrew University of Jerusalem, noted that the overuse of radiography in dentistry is a global problem, one aided by dentistry's unique delivery:</p>
<p>"Dentistry is among the few remaining health care professions where clinical examination, diagnostic testing including radiographs, diagnosis, treatment planning, and treatment are all performed in place, often by the same care practitioner" Zadik wrote. "This model of care delivery prevents external oversight of the entire process."</p>
<p>While routine X-rays continue at short intervals, Zadik notes that current data "favor the reduction of patient exposure to diagnostic radiation in dentistry," while advancements in dentistry dictate that X-rays should be used at "longer intervals and based on clinical suspicion."</p>
<p>Though the digital dental X-rays often used today provide smaller doses of radiation than the film X-rays used in the past, radiation's harms are cumulative. Zadik emphasizes that with the primary tenet of medicine being "First, do no harm," any unnecessary X-ray is an unnecessary harm. Further, other technology can sometimes be used instead of radiography, including <a href="https://en.wikipedia.org/wiki/Electronic_apex_locator">electronic apex locators</a> for root canal procedures.</p>
<p>"Just as it is now unimaginable that, in the past, <a href="https://wi101.wisc.edu/the-rise-and-fall-of-shoe-fitting-fluroscopes/">shoe fittings for children were conducted using X-rays</a>, in the future it will be equally astonishing to learn that the fit of dental crowns was assessed using radiographic imaging," Zadik wrote.</p>

          
                  </div>
                    
        
          
    
    <div>
        
        
        
        <div>
          
          
<h2>X-rays do more harm than good in children</h2>
<p>Feit's commentary also prompted a reply from the three authors of the original May viewpoint: Paulo Nadanovsky, Ana Paula Pires dos Santos, and David Nunan. The three followed up on Feit's point that data is weak on whether X-rays are useful for detecting early decay, specifically white spot lesions. The experts raise the damning point that even if dental X-rays were shown to be good at doing that, there's still no evidence that that's good for patients.</p>
<p>"[T]here is no evidence that detecting white spot lesions, with or without radiographs, benefits patients," the researchers wrote. "Most of these lesions do not progress into dentine cavities," and there's no evidence that early treatments make a difference in the long run.</p>
<p>To bolster the point, the three note that data from children suggest that X-ray screening does more harm than good. <a href="https://bmcoralhealth.biomedcentral.com/articles/10.1186/s12903-021-01528-w">In a randomized clinical trial published in 2021</a>, 216 preschool children were split into two groups: one that received only a visual-tactile dental exam, while the others received both a visual-tactile exam and X-rays. The study found that adding X-rays caused more harm than benefit because the X-rays led to false positives and overdiagnosis of cavitated caries needing restorative treatment. The authors of the trial concluded that "visual inspection should be conducted alone in regular clinical practice."</p>
<p>Like Zadik, the three researchers note that screenings for decay and cavities are not the only questionable use of X-rays in dental practice. Other common dental and orthodontic treatments involving radiography—practices often used in children and teens—might also be unnecessary harms. They raise the argument against the <a href="https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD003879.pub5/full">preventive removal of wisdom teeth</a>, which is also <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1963310/">not backed by evidence</a>.</p>
<p>Like Feit, the three researchers reiterate the call for well-designed trials to back up or refute common dental practices.</p>


          
                  </div>

                  
          <div>
  <div>
          <p><a href="https://arstechnica.com/author/beth/"><img src="https://arstechnica.com/wp-content/uploads/2016/05/b.mole-45957.jpg" alt="Photo of Beth Mole"></a></p>
  </div>

  <div>
    

    <p>
      Beth is Ars Technica’s Senior Health Reporter. Beth has a Ph.D. in microbiology from the University of North Carolina at Chapel Hill and attended the Science Communication program at the University of California, Santa Cruz. She specializes in covering infectious diseases, public health, and microbes.
    </p>
  </div>
</div>


  


  


  
              </div>
  </article>


<div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/10/starship_flight5_catch1-768x432.jpg" alt="Listing image for first story in Most Read: SpaceX catches returning rocket in mid-air, turning a fanciful idea into reality" decoding="async" loading="lazy">
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>


  

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla Optimus Bots Were Remotely Operated at Cybercab Event (212 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-10-14/tesla-s-optimus-robots-were-remotely-operated-at-cybercab-event</link>
            <guid>41842060</guid>
            <pubDate>Mon, 14 Oct 2024 21:10:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-10-14/tesla-s-optimus-robots-were-remotely-operated-at-cybercab-event">https://www.bloomberg.com/news/articles/2024-10-14/tesla-s-optimus-robots-were-remotely-operated-at-cybercab-event</a>, See on <a href="https://news.ycombinator.com/item?id=41842060">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Project Euler (159 pts)]]></title>
            <link>https://projecteuler.net/problem=912</link>
            <guid>41841581</guid>
            <pubDate>Mon, 14 Oct 2024 20:27:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://projecteuler.net/problem=912">https://projecteuler.net/problem=912</a>, See on <a href="https://news.ycombinator.com/item?id=41841581">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="problem">
<p>
Let $s_n$ be the $n$-th positive integer that does not contain three consecutive ones in its binary representation.<br>
For example, $s_1 = 1$ and $s_7 = 8$.
</p>
<p>
Define $F(N)$ to be the sum of $n^2$ for all $n\leq N$ where $s_n$ is odd. You are given $F(10)=199$.
</p>
<p>
Find $F(10^{16})$ giving your answer modulo $10^9+7$.
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Splitting engineering teams into defense and offense (157 pts)]]></title>
            <link>https://www.greptile.com/blog/how-we-engineer</link>
            <guid>41841366</guid>
            <pubDate>Mon, 14 Oct 2024 20:07:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.greptile.com/blog/how-we-engineer">https://www.greptile.com/blog/how-we-engineer</a>, See on <a href="https://news.ycombinator.com/item?id=41841366">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>October 14, 2024<!-- --> <span>(<!-- -->1<!-- -->d ago)</span></span></p><p>Author: <!-- -->Daksh Gupta</p></div><article><p>I’m Daksh, one of the co-founders of <u><a target="_blank" rel="noopener noreferrer" href="https://www.greptile.com/">Greptile</a></u>. We build AI that understands large codebases, which you can query via an API. Large software teams use it for things like AI-powered code reviews and diagnosing root causes for outages.</p>
<p>We are a team of 4 engineers. Our customers are often surprised to learn this, since they assume we must be much larger given the breadth of our product. While this is flattering, the truth is that our product is covered in warts, and our “lean” team is more a product of our inability to identify and hire great engineers, rather than an insistence on superhuman efficiency.</p>
<p>The result is that our product breaks more often than we’d like. The core functionality may remain largely intact but the periphery is often buggy, something we expect will improve only as our engineering headcount catches up to our product scope. Nevertheless, the reason we get anything done at all in these circumstances has to do with a specific way in which we structure our engineering team.</p>
<h2 id="event-driven-vs-long-running-processes"><a href="#event-driven-vs-long-running-processes"></a>Event-driven vs. long-running processes</h2>
<p>15 years ago, Paul Graham wrote about the “<u><a target="_blank" rel="noopener noreferrer" href="https://www.paulgraham.com/makersschedule.html">maker vs. manager schedule</a></u>”, the idea that makers, such as software developers, were different from managers in that they need long, uninterrupted hours to build great things. This essay resonated with engineers around the world who had been trying to squeeze their work in between endless mandatory meetings, and probably led to some sweeping changes at least at software-driven companies in favor of creating a “maker-schedule” for engineers.</p>
<p>Small startups don’t suffer from an excess of meetings and instead have a whole different problem. Customers!</p>
<p>Without dedicated technical support teams and usually with immature products, engineers take on a lot of support work - from hotfixes to building small features for large customers, to just helping customers navigate their products. With enough customers, there is very little time to build new features and make ambitious, complex changes to the codebase.</p>
<p>The engineering work that comes from customers, whether it is general support, bug fixes, or small modifications can be considered “event-driven” engineering.</p>
<p>The engineering work that includes longer-term (more than a week), ambitious projects, can be considered “long-running” engineering.</p>
<p>These two are at odds.</p>
<h2 id="the-fortress"><a href="#the-fortress"></a>The fortress</h2>
<p>Our solution to this problem has been simple, but so far, effective. This is not meant to be prescriptive. Every engineering team is different.</p>
<p>We instruct half the team (2 engineers) at a given point to work on long-running tasks in 2-4 week blocks. This could be refactors, big features, etc. During this time, they don’t have to deal with any support tickets or bugs. Their only job is to focus on getting their big PR out.</p>
<p>The other half of engineers must simply protect the first two from any support work, bugs, etc. Their job is to maintain a fortress around the long-running processes, by catching all the event-driven engineering work. At the end of the cycle, we swap.</p>
<h2 id="why-this-works"><a href="#why-this-works"></a>Why this works</h2>
<p>Remarkable things happen when you take distractions away from a craftsperson. They can spend more time in flow and keep a large amount of context on the “client-side” of their brains.</p>
<p>Critically, it takes only 1-2 short interruptions to dramatically reduce the amount of work an engineer can do in a day. This chart sums it up well.</p>
<p><img src="https://www.greptile.com/5-min.png" alt="Productivity chart"></p>
<div><p>Impact of interruptions on developer productivity</p></div>
<p>It follows then that it’s far more useful to isolate interruptions to a few people rather than disperse them to “keep everyone productive”. If you’re spending some amount of time on support, incrementally more time spent on support will not affect your productivity much.</p>


<h2 id="defenseoffense-engineering"><a href="#defenseoffense-engineering"></a>Defense/Offense engineering</h2>
<p>A mental model that I have found useful is to view event-driven engineering as “defensive” and long-running processes as “offensive”. This tracks nicely to the effect that each has.</p>
<p>Defensive engineering exists to maintain your product, whereas offensive engineering exists to expand it.</p>
<p>Defensive engineering more strongly correlates with your retention and customer satisfaction, whereas offensive engineering arguably correlates a little more strongly with your ability to acquire new customers.</p>
<h2 id="disclaimer"><a href="#disclaimer"></a>Disclaimer</h2>
<p>Not only am I not a professional engineering manager, this is also a very specific and usually ephemeral situation - a small team running a disproportionately fast growing product in a hyper-competitive and fast-evolving space. This is not advice, but rather an observation about how we run our engineering team.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bike Manufacturers Are Making Bikes Less Repairable (196 pts)]]></title>
            <link>https://www.ifixit.com/News/101675/bike-manufacturers-are-making-bikes-less-repairable</link>
            <guid>41840971</guid>
            <pubDate>Mon, 14 Oct 2024 19:27:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ifixit.com/News/101675/bike-manufacturers-are-making-bikes-less-repairable">https://www.ifixit.com/News/101675/bike-manufacturers-are-making-bikes-less-repairable</a>, See on <a href="https://news.ycombinator.com/item?id=41840971">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
<p>The bicycle is probably the canonical example of something that anyone can fix. Spares from all brands are mostly interchangeable, and you can do most repairs with wrenches, screwdrivers, and Allen keys, or some fairly standard tools for bottom brackets and chainrings. But that’s all changing.</p>



<p>Just like cars, tractors, computers, and seemingly every other product category, bikes—and especially e-bikes—are going all black box on us. Instead of using standard parts that can easily be swapped or upgraded, bike makers are using more and more proprietary parts. At the same time,<a href="https://www.vice.com/en/article/mechanics-ask-walmart-major-bike-manufacturers-to-stop-making-and-selling-built-to-fail-bikes/"> cheap bikes are getting worse</a> and are designed to fail, or rather, they are not designed to last, which is pretty much the same thing.</p>


  <div>
      <p><span>Featured Guide</span></p><div>
          <div>
            <h3>How to Replace a Bike Chain With a Master Link</h3>
            <p>Bicycle chains sometimes need to be removed for…</p>
            <p><a href="https://www.ifixit.com/Guide/How+to+Replace+a+Bike+Chain+With+a+Master+Link/140441">Follow this Guide</a></p>
          </div>
          <p><a href="https://www.ifixit.com/Guide/How+to+Replace+a+Bike+Chain+With+a+Master+Link/140441"><img decoding="async" loading="lazy" src="https://guide-images.cdn.ifixit.com/igi/DdhLc1N4sRZfdMOW.medium" alt="How to Replace a Bike Chain With a Master Link"></a>
          </p>
        </div>
          </div>



<h3>Riding Away From Standardization</h3>



<p>For example, the bottom bracket—the tubular bearing assembly at the bottom of the frame that the pedal axle threads through—has long been a fairly standard part.</p>



<p>Over the years, and on different continents, there may have been a few thread sizes, but a cyclist could easily buy the right part for a surprisingly reasonable price. Just as important, you’ve been able to remove the bottom bracket with one of a few simple tools. Now, though, a bike shop has to keep 20+ tools on hand to deal with all the proprietary fittings.</p>



<figure><img fetchpriority="high" loading="lazy" decoding="async" width="4000" height="2672" src="https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12.jpg 4000w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-1536x1026.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-2048x1368.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-1347x900.jpg 1347w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-450x300.jpg 450w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013530/2010-05-30-12-768x512.jpg 768w" sizes="(max-width: 4000px) 100vw, 4000px"><figcaption>Standard parts make modding and repair easier. Photo Charlie Sorrel.</figcaption></figure>



<p>On electric bikes, things are even worse. Batteries are as non-standard as they are on cell phones. Instead of creating a standard, a kind of giant li-ion AA-equivalent for all bikes, you’re stuck buying non-standard sizes that you won’t be able to use on a new bike. This creates its own kind of lock-in, like the batteries on power tools, perhaps making you more likely to stick with the same brand within a family.</p>



<p>Then there are the apps. If you’re considering an e-bike that requires an app to function, or to change settings, do not buy that bike. When (not if) that app is abandoned, the bike will become at best a hobbled version of itself.</p>



<p>The result is that possibly the greenest, most efficient form of transport is turning into yet another source of landfill and e-waste. But why?</p>



<p>The cynical—and probably correct—take is that it boosts sales. By using proprietary parts, a bike manufacturer guarantees you have to go back to them for spares. And if those spares are not readily available, or are too expensive, then maybe you’ll just give up and buy a new bike instead.</p>



<p>Couple this with the explosion in new bike tech in recent years, which is itself designed to drive the desire to “upgrade” a perfectly good bike by replacing it with a new one, and you can see the attraction for the bean counters. Electronic, wireless gear shifters. Carbon-fiber seat posts. Active, self-adjusting suspension. Proprietary apps for changing key features like the power mode. All of these are superfluous for most riders, and add complexity to what is essentially a very simple, and pretty much perfect, machine.</p>



<h3>Buy Cheap, Buy Twice</h3>



<p>Bikes are getting ever more popular, in large part thanks to e-bikes, which make riding easy for people who would not otherwise consider cycling. That’s good news! Alas, to service this popularity, cheap and crappy bikes have proliferated.</p>



<p>“Budget bikes from ‘big box’ stores […] cost little ($150 to $250) because manufacturers cut corners. These bikes are built to fail: badly engineered, constructed from low-grade materials and fabricated in countries with inhumane labor standards,” writes cycling advocate <a href="https://nyc.streetsblog.org/2021/03/24/opinion-how-the-budget-bike-trap-creates-inequality-in-nyc">Josh Bicker on StreetsBlog NYC</a>.</p>



<p>These bikes are often broken out of the box. That’s bad if the buyer is a bike shop, and possibly deadly if the buyer is an inexperienced rider buying off the internet. Buying a used bike is a much better way to get a well-made machine for a good price. The downside of that is that you need to know what to look for, and how to bring that bike up to correct working order so that it is reliable and safe.</p>



<figure><img loading="lazy" decoding="async" width="4256" height="2832" src="https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733.jpg 4256w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-1536x1022.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-2048x1363.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-1353x900.jpg 1353w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-450x300.jpg 450w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013636/2009-09-18-4733-768x512.jpg 768w" sizes="(max-width: 4256px) 100vw, 4256px"></figure>



<p>Fortunately, that’s possible with local bike kitchens and co-ops, or by asking your bike shop to look over the bike for you. This works best if that bike isn’t using a bunch of proprietary parts. You can’t reach into the spare parts bin for a brake caliper if your bike uses a proprietary disk-brake design, whether that’s a super-high-end model, or a closed unit with more plastic than metal.</p>



<p>Ideally all bikes would continue to draw on a pool of standard part-types, but manufacturers seem set on the opposite. This makes it all the more important that we have legislation to force them to make proprietary parts available for riders to buy themselves, not just selling to repair shops (if at all). And with the increase of technology in bikes, public repair information is also essential. You’re definitely not going to find powered-hub servicing guides on <a href="https://sheldonbrown.com/">Sheldon Brown</a>. Fingers crossed, but that legislation may indeed be on the way.</p>


  <div>
      <p><span>Featured Guide</span>

              <a href="https://www.ifixit.com/Guide/How+to+Repair+Electric+Bikes/75971"><img decoding="async" loading="lazy" src="https://guide-images.cdn.ifixit.com/igi/Od2deYTwnAFriE1g.medium" alt="How to Repair Electric Bikes"></a></p><h3>How to Repair Electric Bikes</h3>
        <p>What to inspect and do when looking to resolve…</p>
        <p><a href="https://www.ifixit.com/Guide/How+to+Repair+Electric+Bikes/75971">Follow this Guide</a></p>
          </div>



<h3>Batteries Should Be Interoperable</h3>



<p>Let’s get to batteries. After tires, tubes, cables and chains, the one thing on an electric bike that will 100% wear out and need replacing is the battery. Unlike most laptops, you can easily remove the battery from the bike. But forget about ordering up a standard replacement, because there isn’t one. The batteries are often shaped to fit the bike, but even those that clip into a section below the rear rack, or are otherwise independently-mounted vary in capacity, voltage, and current delivery.</p>



<p>That keeps replacement costs higher, but it also means that you are stuck if the manufacturer discontinues your battery. A bike that is otherwise in perfect working order might end up prematurely useless, or you will end up in the world of shonky spares from Amazon or another unreliable source.</p>



<figure><img loading="lazy" decoding="async" width="4000" height="3000" src="https://valkyrie.cdn.ifixit.com/media/2024/10/11013742/2010-09-26-4.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/10/11013742/2010-09-26-4.jpg 4000w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013742/2010-09-26-4-1536x1152.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013742/2010-09-26-4-2048x1536.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/10/11013742/2010-09-26-4-1200x900.jpg 1200w" sizes="(max-width: 4000px) 100vw, 4000px"><figcaption>Sure, why not? Photo Charlie Sorrel.</figcaption></figure>



<p>The standard excuses apply. Original parts are designed to work safely together. Using non-official parts can be dangerous, etc. That may be true, but if so, it’s only because the parts were designed that way. The blame is with the manufacturer. It’s totally possible to design around standard batteries. Ask anyone who’s ever made a device that runs on AA batteries, or swapped a new 12-Volt lead-acid battery into a car.</p>



<p>We are 100% against this trend. A bike is an almost perfect machine, and e-bikes combined with public transit are probably the best way to get cars out of cities, and to make personal transport sustainable.</p>



<p>“There’s no machine known that is more efficient than a human on a bicycle,” Bill Nye, the science guy,<a href="https://bigthink.com/articles/bill-nye-the-city-of-the-future/"> told Big Think</a>. “Bowl of oatmeal, 30 miles — you can’t come close to that.”</p>



<p>And yet all that is being ruined in an effort to make us buy a new bike every few years, instead of repairing the ones we have. Newer, more exotic specs and components encourage us to “upgrade,” just like with smartphones, laptops, and cameras, and they also turn the perfect machine into an unknowable black box that is often not worth the cost of repair.</p>



<figure><img loading="lazy" decoding="async" width="695" height="460" src="https://valkyrie.cdn.ifixit.com/media/2024/10/11084604/zcqutape5vm2kvphcxlu.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/10/11084604/zcqutape5vm2kvphcxlu.jpg 695w, https://valkyrie.cdn.ifixit.com/media/2024/10/11084604/zcqutape5vm2kvphcxlu-300x200.jpg 300w" sizes="(max-width: 695px) 100vw, 695px"><figcaption>The Infinite Battery is endlessly repairable, and even looks cool.</figcaption></figure>



<p>One ray of hope here is the Infinite Battery by Gouach, <a href="https://www.indiegogo.com/projects/infinite-the-repairable-universal-ebike-battery#/">currently seeking development funding via Indiegogo</a>. It’s compatible with all major brands’ setups, and offers the usual power capacity and safety features, but it is totally user serviceable. All parts can be swapped out individually, and when the cells inside start to wear out, you can replace them individually, almost as if your bike ran on around 30 AA cells</p>



<p>If you can repair a bike, and use standard spares, either new or harvested from dead bikes, then a bike can essentially live forever. If the growing anti-repair practices of the bike industry are allowed to threaten that, then we no longer own our machines. We are essentially renting disposable gadgets instead.</p>
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How I Experience Web Today (393 pts)]]></title>
            <link>https://how-i-experience-web-today.com</link>
            <guid>41840931</guid>
            <pubDate>Mon, 14 Oct 2024 19:22:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://how-i-experience-web-today.com">https://how-i-experience-web-today.com</a>, See on <a href="https://news.ycombinator.com/item?id=41840931">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <p>
                    https://example.com
                </p>
                <p><a href="https://how-i-experience-web-today.com/detail">
                    Then it shows me something
                </a></p><p>
                    Example Domain. This domain is for use in illustrative examples in documents. You may use this domain in literature without prior coordination or asking for ...
                </p>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Play 3.0 mini – A lightweight, reliable, cost-efficient Multilingual TTS model (227 pts)]]></title>
            <link>https://play.ht/news/introducing-play-3-0-mini/</link>
            <guid>41840872</guid>
            <pubDate>Mon, 14 Oct 2024 19:16:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://play.ht/news/introducing-play-3-0-mini/">https://play.ht/news/introducing-play-3-0-mini/</a>, See on <a href="https://news.ycombinator.com/item?id=41840872">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Today we’re releasing our most capable and conversational voice model that can speak in 30+ languages using any voice or accent, with industry leading speed and accuracy. We’re also releasing 50+ new conversational AI voices across languages.</p>
<p>Our mission is to make voice AI accessible, personal and capable for all. Part of that mission is to advance the current state of interactive voice technology in conversational AI and elevate user experience.</p>
<p>When you’re building real time applications using TTS, a few things really matter – latency, reliability, quality and naturalness of speech. While we’ve been leading on latency and naturalness of speech with our previous generation models, Play 3.0 mini makes significant improvements to reliability and audio quality while still being the fastest and most conversational voice model.</p>
<p>Play3.0 mini is the first in a series of efficient multi-lingual AI text-to-speech models we plan to release over the coming months. Our goal is to make the models smaller and cost-efficient so they can be run on devices and at scale.</p>
<h2>Play 3.0 mini is our fastest, most conversational speech model yet</h2>
<p>3.0 mini achieves a mean latency of 189 milliseconds for TTFB, making it our fastest AI Text to Speech model. It supports text-in streaming from LLMs and audio-out streaming, and can be used via our HTTP REST API, websockets API or SDKs. 3.0 mini is also more efficient than Play 2.0, and runs inference 28% faster.</p>
<figure><p>
<iframe title="Introducing Play 3.0 mini - a new compact Text to Speech model for realtime Voice AI" width="640" height="360" src="https://www.youtube.com/embed/DusTj5NLC9w?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p></figure>
<h2>Play 3.0 mini supports 30+ languages across any voice</h2>
<p>Play 3.0 mini now supports more than 30+ languages, many with multiple male and female voice options out of the box.&nbsp; Our English, Japanese, Hindi, Arabic, Spanish, Italian, German, French, and Portuguese voices are available now for production use cases, and are available through our <a href="https://docs.play.ht/reference/api-getting-started">API</a> and on our <a href="https://www.play.ht/playground">playground</a>.&nbsp; Additionally, Afrikaans, Bulgarian, Croatian, Czech, Hebrew, Hungarian, Indonesian, Malay, Mandarin, Polish, Serbian, Swedish, Tagalog, Thai, Turkish, Ukrainian, Urdu, and Xhosa are available for testing.</p>
<h2>Play 3.0 mini is more accurate</h2>
<p>Our goal with Play 3.0 mini was to build the best TTS model for conversational AI. To achieve this, the model had to outperform competitor models in latency and accuracy while generating speech in the most conversational tone.</p>
<p>LLMs hallucinate and voice LLMs are no different. Hallucinations in voice LLMs can be in the form of extra or missed words or numbers in the output audio not part of the input text. Sometimes they can just be random sounds in the audio. This makes it difficult to use generative voice models reliably.</p>
<p>Here are some challenging text prompts that most TTS models struggle to get right –</p>
<blockquote>
<p><em>“Okay, so your flight UA2390 from San Francisco to Las Vegas on November 3rd is confirmed. And, your ticket number is F X 2, 3 9 A, 7 R T. The flight is scheduled to depart at 2:45 p.m. Is there anything else I can assist you with?”</em></p>
</blockquote>
<figure><audio controls="" src="https://playtht-website-assets.s3.amazonaws.com/voice-samples/t1.wav"></audio></figure>
<blockquote>
<p><em>“Now, when people RSVP, they can call the event coordinator at&nbsp;<strong>555 342 1234</strong>, but if they need more details, they can also call the backup number, which is&nbsp;<strong>416 789 0123</strong>.”</em></p>
</blockquote>
<figure><audio controls="" src="https://playtht-website-assets.s3.amazonaws.com/voice-samples/t2+(1).mp3"></audio></figure>
<blockquote>
<p>“<em>I’ve successfully processed your order and I’d like to confirm your product ID. It is A as in Alpha, 1, 2, 3, B as in Bravo, 5, 6, 7,&nbsp; Z as in Zulu, 8, 9, 0,&nbsp; X as in X-ray.</em>“</p>
</blockquote>
<figure><audio controls="" src="https://playtht-website-assets.s3.amazonaws.com/voice-samples/t3.mp3"></audio></figure>
<p>3.0 mini was finetuned specifically on a diverse dataset of alpha-numeric phrases to make it reliable for critical use cases where important information such as phone numbers, passport numbers, dates, currencies, etc. can’t be misread.</p>
<h2>Play 3.0 mini reads alphanumeric sequences more naturally</h2>
<p>We’ve trained the model to read numbers and acronyms just like humans do. The model adjusts its pace and slows down any alpha-numeric characters. Phone numbers for instance are read out with more natural pacing, and similarly all acronyms and abbreviations. This makes the overall conversational experience more natural.</p>
<blockquote>
<p><em>“Alright, let’s troubleshoot your laptop issue. First, let’s confirm your device’s ID so we’re on the same page. The I D is 894-d94-774-496-438-9b0-d2. Did I get that right?</em>“</p>
</blockquote>
<figure><audio controls="" src="https://playtht-website-assets.s3.amazonaws.com/voice-samples/t4.mp3"></audio></figure>
<h2>Play 3.0 mini achieves the best voice similarity for voice cloning</h2>
<p>When cloning voices, close often isn’t good enough.&nbsp; Play 3.0 voice cloning achieves state-of-the-art performance when cloning voices, ensuring accurate reproduction of accent, tone, and inflection of cloned voices.&nbsp; In benchmarking using a popular open source embedding model, we lead competitor models by a wide margin for similarity to the original voice.&nbsp; Try it for yourself by cloning your own voice, and talking to yourself on <a href="https://play.ai/" target="_blank" rel="noopener">https://play.ai</a>&nbsp;</p>
<h2>Websockets API Support</h2>
<p>3.0 mini’s API now supports websockets, which significantly reduces the overhead of opening and closing HTTP connections, and makes it easier than ever to enable text-in streaming from LLMs or other sources.</p>
<h2>Play 3.0 mini is a cost efficient model</h2>
<p>We’re happy to announce reduced pricing for our higher volume Startup and Growth tiers, and have now introduced a new Pro tier at $49 a month for businesses with more modest requirements.&nbsp; Check out our new pricing table <a href="https://play.ht/pricing/?planType=api">here</a>.</p>
<p>We look forward to seeing what you build with us!&nbsp; If you’ve custom, high volume requirements, feel free to <a href="https://play.ht/contact-us/">contact our sales team</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Funding Construction of Seven U.S. Nuclear Reactors (547 pts)]]></title>
            <link>https://www.wsj.com/business/energy-oil/google-nuclear-power-artificial-intelligence-87966624</link>
            <guid>41840769</guid>
            <pubDate>Mon, 14 Oct 2024 19:06:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/business/energy-oil/google-nuclear-power-artificial-intelligence-87966624">https://www.wsj.com/business/energy-oil/google-nuclear-power-artificial-intelligence-87966624</a>, See on <a href="https://news.ycombinator.com/item?id=41840769">Hacker News</a></p>
Couldn't get https://www.wsj.com/business/energy-oil/google-nuclear-power-artificial-intelligence-87966624: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[The three-page paper that shook philosophy: Gettiers in software engineering (270 pts)]]></title>
            <link>https://jsomers.net/blog/gettiers</link>
            <guid>41840390</guid>
            <pubDate>Mon, 14 Oct 2024 18:28:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jsomers.net/blog/gettiers">https://jsomers.net/blog/gettiers</a>, See on <a href="https://news.ycombinator.com/item?id=41840390">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
              <p>In 1963, the philosopher Edmund Gettier published a three-page <a href="http://www-bcf.usc.edu/~kleinsch/Gettier.pdf">paper</a> in the journal <em>Analysis</em> that quickly became a classic in the field. Epistemologists going back to the Greeks had debated what it meant to know something, and in the Enlightenment, a definition was settled upon: to know something is to have a <em>justified true belief</em> about it:</p>

<ul>
<li><strong>justified</strong> in the sense of deriving from evidence</li>
<li><strong>true</strong>, because it doesn't make sense to "know" a falsehoood</li>
<li><strong>belief</strong>, i.e., a proposition in your head</li>
</ul>

<p>Gettier, in his tiny paper, upended the consensus. He asked "Is Justified True Belief Knowledge?" and offered three cases---soon to be known as "the Gettier cases"---that suggested you could have a JTB about something and yet still we would want to say you didn't <em>know</em> it. For that, he earned lasting fame, and his paper generated a literature <a href="https://en.wikipedia.org/wiki/Gettier_problem">all its own</a>.</p>

<h2>A Gettier case</h2>

<p>Supppose you're standing in a field and off in the distance you see a cow. But suppose that what you're actually looking at isn't a cow, it's just a convincingly lifelike model of a cow made out of papier-mâché. You're not seeing a cow, you're seeing the model. But then finally suppose that right behind the papier-mâché cow is a real cow!</p>

<p>On the one hand, you have a justified true belief that "there is a cow in the field": (1) you believe there's a cow in the field; (2) that belief didn't come from nowhere, but is justified by your seeing something that looks exactly like a cow; (3) and there is, in fact, a cow in the field. Still, we wouldn't want to say that you <em>know</em> there's a cow in the field, because in a sense you got lucky: by a strange coincidence, there happened to be a real cow there---a cow you knew nothing about.</p>

<h2>In software engineering</h2>

<p>At my old company, <a href="http://genius.com/">Genius</a>, the CTO---who'd studied philosophy as an undergrad---was obsessed with these Gettier cases. He called them "gettiers" for short. So we used to talk about gettiers all the time, no doubt in part just because it felt clever to talk about them, but also because when you're a programmer, you run into things that feel like Gettier cases with unusual frequency. And once you have a name for them, you start seeing them everywhere.</p>

<p>Here's a recent example. I was working on a web application that used a client-side framework that had been developed in-house. My app was a little search engine, and in my latest pull request, I'd made it so that when you hit Enter in the search field, the field lost focus, so that folks who like to browse the web via their keyboard wouldn't have to manually escape from the input box.</p>

<p>When I released the new version, I noticed that I'd broken the autofocusing of the search field that was supposed to happen on pageload. I started poking around, only to discover that I couldn't seem to get the correct behavior back. No matter what code I changed, which lines I commented out, how many times I hard-refreshed the browser, etc., I couldn't get the autofocus to work.</p>

<p>What had actually happened is that a coworker of mine had made a change to the framework itself, which changed how certain events were bound to the root DOM element, and as a result broke the "autofocus" attribute. At some point, I did a routine rebase on top of this change (and many other unrelated changes). Which meant that when I deployed my little pull request, I was <em>also</em> deploying a bug I had nothing to do with---one that ended up breaking autofocus. It only appeared as though my changes caused the problem, because I'd edited some code having to do with focus in the search field.</p>

<p>Note that I had a justified belief that "the pull request I just deployed broke autofocus on the production site," and in fact my change <em>did</em> break it---making the belief true. But the break actually happened for a completely different reason!</p>

<p>(Yes, I should have caught the bug in testing, and in fact I did notice some odd behavior. But making software is hard!)</p>

<p>Here's another example. (This one's from a long time ago, so the details might be a bit off.) A user once reported that on-site messages were no longer generating email notifications, and I was asked to investigate. Soon, I discovered that someone had recently pushed a change to the code that handled emails in our web app; the change seemed to introduce a bug that was responsible for the broken behavior. But---gettier!---the email service that the code relied on had itself gone down, at almost the exact same time that the change was released. I could have had a JTB that the code change had caused the emails to stop delivering, but still we wouldn't want to say I "knew" this was the cause, because it was actually the service outage that was directly responsible.</p>

<h2>A new term of art</h2>

<p>A philosopher might say that these aren't bona fide Gettier cases. True gettiers are rare. But it's still a useful idea, and it became something of a term of art at Genius---and has stuck with me since---because it's a good name for one of the trickiest situations you can get into as a programmer: a problem has multiple potential causes, and you have every reason to believe in one of them, even though another is secretly responsible.</p>

<p>Having a term for these tricky cases allows you, I think, to be ever-so-slightly more alert to them. You can be a better developer this way. As I've spent more time writing software, I've gotten better at sensing when my assumptions are probably wrong---when something gettieresque might be going on: have I forgotten to clear the cache? Am I working off the wrong branch? Am I even hitting this code path?</p>

<p>Software is a complex and ephemeral business. More than most people, developers are daily faced with bizarre epistemological problems. It helps to be able to distinguish a cow in the field from, well, a gettier.</p>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Commonly used arm positions can overestimate blood pressure readings: study (303 pts)]]></title>
            <link>https://medicalxpress.com/news/2024-10-commonly-arm-positions-substantially-overestimate.html</link>
            <guid>41840023</guid>
            <pubDate>Mon, 14 Oct 2024 17:57:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medicalxpress.com/news/2024-10-commonly-arm-positions-substantially-overestimate.html">https://medicalxpress.com/news/2024-10-commonly-arm-positions-substantially-overestimate.html</a>, See on <a href="https://news.ycombinator.com/item?id=41840023">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									    
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2024/johns-hopkins-medicine-1.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2024/johns-hopkins-medicine-1.jpg" data-sub-html="Researchers say their study findings underscore the importance of adhering to clinical guidelines calling for firm arm support on a desk or other surface when measuring blood pressure. Credit: Tammy Brady">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2024/johns-hopkins-medicine-1.jpg" alt="Johns Hopkins Medicine study finds commonly used arm positions can substantially overestimate blood pressure readings" title="Researchers say their study findings underscore the importance of adhering to clinical guidelines calling for firm arm support on a desk or other surface when measuring blood pressure. Credit: Tammy Brady" width="800" height="529">
             <figcaption>
                Researchers say their study findings underscore the importance of adhering to clinical guidelines calling for firm arm support on a desk or other surface when measuring blood pressure. Credit: Tammy Brady
            </figcaption>        </figure>
    </div><p>A study led by Johns Hopkins Medicine researchers concludes that commonly used ways of positioning the patient's arm during blood pressure (BP) screenings can substantially overestimate test results and may lead to a misdiagnosis of hypertension.</p>

                                        
                                                                                  
                                         

                                                                                                                                    <p>In a report on the <a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/10.1001/jamainternmed.2024.5213" target="_blank">study</a>, which was published Oct. 7 in <i>JAMA Internal Medicine</i>, investigators examined the effects of three different arm positions: an arm supported on a desk, arm supported on a lap, and an unsupported arm hanging at the patient's side.</p>
<p>Researchers found that lap support overestimated <a href="https://medicalxpress.com/tags/systolic+pressure/" rel="tag">systolic pressure</a> (the top number in a BP reading) by nearly 4 mmHg, and an unsupported arm hanging at the side overestimated systolic pressure by nearly 7 mmHg.</p>
<p>The findings confirm that arm position makes a "huge difference" when it comes to an accurate <a href="https://medicalxpress.com/tags/blood+pressure/" rel="tag">blood pressure</a> measurement, says Tammy Brady, M.D., Ph.D., vice chair for <a href="https://medicalxpress.com/tags/clinical+research/" rel="tag">clinical research</a> in the Department of Pediatrics at the Johns Hopkins University School of Medicine, medical director of the pediatric hypertension program at Johns Hopkins Children's Center, deputy director of the Welch Center for Prevention, Epidemiology, and Clinical Research and senior author of the study.</p>
<p>And they underscore the importance of adhering to clinical guidelines calling for firm support on a desk or other surface when measuring blood pressure, the investigators add.</p>

                                                                                                                                                         
                                                                                                                                                                                                <p>According to the American Heart Association, nearly half of U.S. adults have elevated blood pressure, a diagnosis made when the measured force of blood flowing through blood vessels is higher than what is generally considered normal, on average 120/80.*</p>
<p>Untreated, <a href="https://medicalxpress.com/tags/high+blood+pressure/" rel="tag">high blood pressure</a> increases the risk of stroke, heart attack and other serious cardiovascular conditions. Because hypertension may cause minimal or no symptoms, early and frequent screening during routine checkups is considered the cornerstone of hypertension management.</p>
<p>In most cases, lifestyle changes such as weight loss, healthy diets and exercise, as well as therapy with any of a variety of medications, can keep BP under control.</p>
<p>The latest clinical practice guidelines from the American Heart Association emphasize several key steps for an <a href="https://medicalxpress.com/tags/accurate+measurement/" rel="tag">accurate measurement</a>—including <a href="https://clinicalconnection.hopkinsmedicine.org/videos/cuff-size-matters-for-blood-pressure-monitoring-and-device-accuracy" target="_blank">appropriate cuff size</a>, back support, feet flat on the floor with legs uncrossed, and an appropriate arm position, in which the middle of an adjustable BP cuff is positioned at mid-heart level on an arm supported on a desk or table.</p>
<p>Despite these recommendations, the researchers say BP is too often measured with patients seated on an exam table without any, or inadequate, arm support. In some cases, a clinician holds the arm, or the patient holds an arm in their lap.</p>

                                                                                                                                            <p>In the new Johns Hopkins study, the researchers recruited 133 adult participants (78% Black, 52% female) between Aug. 9, 2022, and June 1, 2023. Study participants, who ranged from age 18 to 80, were sorted at random into one of six possible groups that differed by order of the three seated arm positions.</p>
<p>Measurements were taken during a single visit between 9 a.m. and 6 p.m. Before BP measures were taken, all participants first emptied their bladders and then walked for two minutes to mimic a typical clinical scenario in which people walk into a clinic or office before screening takes place. They then underwent a five-minute, seated rest period with their backs and feet supported.</p>
<p>Each person, wearing an upper arm BP cuff selected and sized based on their upper arm size, had three sets of triplicate measurements taken with a digital blood pressure device 30 seconds apart.</p>
<p>Upon completion of each set of three measurements, the cuff was removed, participants walked for two minutes and rested for five minutes. On the same visit, they then underwent a fourth set of triplicate measurements with their arm supported on a desk, a set used to account for well-known variations in BP readings. All of the measurements were conducted in a quiet and private space, and participants were asked not to talk to researchers or use their phones during the screening.</p>
<p>Researchers found that BP measurements obtained with arm positions frequently used in clinical practice—an arm on the lap or unsupported at the side—were markedly higher than those obtained when the arm was supported on a desk, the standard, recommended arm position.</p>
<p>Supporting the arm on the lap overestimated systolic BP—the top number of a reading, or the force of blood flow when pumped out of the heart, by 3.9 mmHg and diastolic blood pressure—the bottom number, or the pressure in the arteries when the heart rests between beats, by 4.0 mmHg. An unsupported arm at the side overestimated systolic by 6.5 mmHg and diastolic by 4.4 mmHg.</p>
<p>"If you are consistently measuring blood pressure with an unsupported arm, and that gives you an overestimated BP of 6.5 mmHg, that's a potential difference between a systolic BP of 123 and 130, or 133 and 140—which is considered stage 2 hypertension," says Sherry Liu, M.H.S., an epidemiology research coordinator at the Welch Center for Prevention, Epidemiology, and Clinical Research, Department of Epidemiology, at Johns Hopkins Bloomberg School of Public Health and study author.</p>
<p>Investigators caution that their study results may only apply during screenings with automated BP devices, and may not apply to readings done with other BP devices.</p>
<p>However, Brady says, the findings suggest that clinicians need to pay better attention to best practice guidelines, and that patients "must advocate for themselves in the clinical setting and when measuring their BP at home."</p>
<h2>*Current blood pressure guidelines and variability</h2>
<p>According to the 2017 guidelines from the American Heart Association, normal blood pressure is less than 120/80 mmHg. Blood pressure readings between 120-129/80 mmHg are classified as elevated, with hypertension being diagnosed at 130/80 mmHg or higher.</p>
<p>It's important to note that blood pressure can vary due to factors such as stress, diet, caffeine intake, and smoking. Therefore, to obtain an accurate reading, it is crucial to measure blood pressure under consistent conditions, following clinical guidelines.</p>

                                                                                                                                                                            
                                        											<div>
												                                                    <p><strong>More information:</strong>
                                                    Hairong Liu et al, Arm Position and Blood Pressure Readings, <i>JAMA Internal Medicine</i> (2024). DOI: 10.1001/jamainternmed.2024.5213 , <a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/10.1001/jamainternmed.2024.5213" target="_blank">jamanetwork.com/journals/jamai … ainternmed.2024.5213</a>
																								
																								</p>
																							</div>
                                        											
																					
                                                                                                                        
                                        <!-- print only -->
                                        <div>
                                            <p><strong>Citation</strong>:
                                                 Commonly used arm positions can substantially overestimate blood pressure readings, study finds (2024, October 7)
                                                 retrieved 14 October 2024
                                                 from https://medicalxpress.com/news/2024-10-commonly-arm-positions-substantially-overestimate.html
                                            </p>
                                            <p>
                                            This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
                                            part may be reproduced without the written permission. The content is provided for information purposes only.
                                            </p>
                                        </div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PayloadCMS: Open-Source, Fullstack Next.js Framework (192 pts)]]></title>
            <link>https://github.com/payloadcms/payload</link>
            <guid>41839994</guid>
            <pubDate>Mon, 14 Oct 2024 17:55:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/payloadcms/payload">https://github.com/payloadcms/payload</a>, See on <a href="https://news.ycombinator.com/item?id=41839994">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><p><a href="https://payloadcms.com/" rel="nofollow"><img width="100%" src="https://github.com/payloadcms/payload/raw/main/packages/payload/src/admin/assets/images/github-banner-alt.jpg?raw=true" alt="Payload headless CMS Admin panel built with React"></a></p></div>
<p dir="auto">
  <a href="https://github.com/payloadcms/payload/actions"><img alt="GitHub Workflow Status" src="https://camo.githubusercontent.com/aa40d8228c4f55ab9abc8872111aa72bfafd26727ae1d051c244bc243029e109/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f7061796c6f6164636d732f7061796c6f61642f6d61696e2e796d6c3f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/github/actions/workflow/status/payloadcms/payload/main.yml?style=flat-square"></a>
  &nbsp;
  <a href="https://discord.gg/payload" rel="nofollow"><img alt="Discord" src="https://camo.githubusercontent.com/e9df3d6912b718b05305520e56871d24ffcbc53fd43df018b74822e0c48f0be4/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3936373039373538323732313537323933343f6c6162656c3d446973636f726426636f6c6f723d373238396461267374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/discord/967097582721572934?label=Discord&amp;color=7289da&amp;style=flat-square"></a>
  &nbsp;
  <a href="https://www.npmjs.com/package/payload" rel="nofollow"><img alt="npm" src="https://camo.githubusercontent.com/aa42c043a19ec19a1ffd1579b6ec06392ffc7e5929490f8310af0178218c867f/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f7061796c6f61643f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/npm/v/payload?style=flat-square"></a>
  &nbsp;
  <a href="https://twitter.com/payloadcms" rel="nofollow"><img src="https://camo.githubusercontent.com/ef66e825f06cba28732876c654ca76f0f2f59ac6b97ab8fab70229394244a5f1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f666f6c6c6f772d7061796c6f6164636d732d3144413146323f6c6f676f3d74776974746572267374796c653d666c61742d737175617265" alt="Payload Twitter" data-canonical-src="https://img.shields.io/badge/follow-payloadcms-1DA1F2?logo=twitter&amp;style=flat-square"></a>
</p>
<hr>
<p dir="auto"><h4 tabindex="-1" dir="auto">
<a href="https://payloadcms.com/docs/getting-started/what-is-payload" rel="nofollow"><strong>Explore the Docs</strong></a>&nbsp;·&nbsp;<a href="https://payloadcms.com/community-help" rel="nofollow"><strong>Community Help</strong></a>&nbsp;·&nbsp;<a href="https://demo.payloadcms.com/" rel="nofollow"><strong>Try Live Demo</strong></a>&nbsp;·&nbsp;<a href="https://github.com/payloadcms/payload/discussions/1539" data-hovercard-type="discussion" data-hovercard-url="/payloadcms/payload/discussions/1539/hovercard"><strong>Roadmap</strong></a>&nbsp;·&nbsp;<a href="https://www.g2.com/products/payload-cms/reviews#reviews" rel="nofollow"><strong>View G2 Reviews</strong></a>
</h4><a id="user-content-explore-the-docscommunity-helptry-live-demoroadmapview-g2-reviews" aria-label="Permalink: Explore the Docs&nbsp;·&nbsp;Community Help&nbsp;·&nbsp;Try Live Demo&nbsp;·&nbsp;Roadmap&nbsp;·&nbsp;View G2 Reviews" href="#explore-the-docscommunity-helptry-live-demoroadmapview-g2-reviews"></a></p>
<hr>
<div dir="auto"><p dir="auto">Important</p><p dir="auto">🎉 <strong>Payload 3.0 beta released!</strong> You can now deploy Payload fully in any Next.js app folder. Read more in the <a href="https://payloadcms.com/blog/30-beta-install-payload-into-any-nextjs-app-with-one-line" rel="nofollow"><strong>announcement post</strong></a>.</p>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Benefits over a regular CMS</h3><a id="user-content-benefits-over-a-regular-cms" aria-label="Permalink: Benefits over a regular CMS" href="#benefits-over-a-regular-cms"></a></p>
<ul dir="auto">
  <li>Don’t hit some third-party SaaS API, hit your own API</li>
  <li>Use your own database and own your data</li>
  <li>It's just Express - do what you want outside of Payload</li>
  <li>No need to learn how Payload works - if you know JS, you know Payload</li>
  <li>No vendor lock-in</li>
  <li>Avoid microservices hell - get everything (even auth) in one place</li>
  <li>Never touch ancient WP code again</li>
  <li>Build faster, never hit a roadblock</li>
  <li>Both admin and backend are 100% extensible</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">☁️ Deploy instantly with Payload Cloud.</h2><a id="user-content-️-deploy-instantly-with-payload-cloud" aria-label="Permalink: ☁️ Deploy instantly with Payload Cloud." href="#️-deploy-instantly-with-payload-cloud"></a></p>
<p dir="auto">Create a cloud account, connect your GitHub, and <a href="https://payloadcms.com/new" rel="nofollow">deploy in minutes</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Get started by self-hosting completely free, forever.</h2><a id="user-content--get-started-by-self-hosting-completely-free-forever" aria-label="Permalink: 🚀 Get started by self-hosting completely free, forever." href="#-get-started-by-self-hosting-completely-free-forever"></a></p>
<p dir="auto">Before beginning to work with Payload, make sure you have all of the <a href="https://payloadcms.com/docs/getting-started/installation" rel="nofollow">required software</a>.</p>
<div data-snippet-clipboard-copy-content="npx create-payload-app@latest"><pre lang="text"><code>npx create-payload-app@latest
</code></pre></div>
<p dir="auto">Alternatively, it only takes about five minutes to <a href="https://payloadcms.com/docs/getting-started/installation#from-scratch" rel="nofollow">create an app from scratch</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🖱️ One-click templates</h2><a id="user-content-️-one-click-templates" aria-label="Permalink: 🖱️ One-click templates" href="#️-one-click-templates"></a></p>
<p dir="auto">Jumpstart your next project by starting with a pre-made template. These are production-ready, end-to-end solutions designed to get you to market as fast as possible.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/payloadcms/payload/tree/main/templates/ecommerce">🛒 E-Commerce</a></h3><a id="user-content--e-commerce" aria-label="Permalink: 🛒 E-Commerce" href="#-e-commerce"></a></p>
<p dir="auto">Eliminate the need to combine Shopify and a CMS, and instead do it all with Payload + Stripe. Comes with a beautiful, fully functional front-end complete with shopping cart, checkout, orders, and much more.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/payloadcms/payload/tree/main/templates/website">🌐 Website</a></h3><a id="user-content--website" aria-label="Permalink: 🌐 Website" href="#-website"></a></p>
<p dir="auto">Build any kind of website, blog, or portfolio from small to enterprise. Comes with a beautiful, fully functional front-end complete with posts, projects, comments, and much more.</p>
<p dir="auto">We're constantly adding more templates to our <a href="https://github.com/payloadcms/payload/tree/main/templates">Templates Directory</a>. If you maintain your own template, consider adding the <code>payload-template</code> topic to your GitHub repository for others to find.</p>
<ul dir="auto">
<li><a href="https://github.com/payloadcms/payload/tree/main/templates">Official Templates</a></li>
<li><a href="https://github.com/topics/payload-template">Community Templates</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">✨ Features</h2><a id="user-content--features" aria-label="Permalink: ✨ Features" href="#-features"></a></p>
<ul dir="auto">
<li>Completely free and open-source</li>
<li><a href="https://payloadcms.com/docs/graphql/overview" rel="nofollow">GraphQL</a>, <a href="https://payloadcms.com/docs/rest-api/overview" rel="nofollow">REST</a>, and <a href="https://payloadcms.com/docs/local-api/overview" rel="nofollow">Local</a> APIs</li>
<li><a href="https://payloadcms.com/docs/admin/overview" rel="nofollow">Easily customizable ReactJS Admin</a></li>
<li><a href="https://payloadcms.com/docs/production/deployment" rel="nofollow">Fully self-hosted</a></li>
<li><a href="https://payloadcms.com/docs/authentication/overview" rel="nofollow">Extensible Authentication</a></li>
<li><a href="https://payloadcms.com/docs/upload/overview" rel="nofollow">Local file storage &amp; upload</a></li>
<li><a href="https://payloadcms.com/docs/versions/overview" rel="nofollow">Version History and Drafts</a></li>
<li><a href="https://payloadcms.com/docs/configuration/localization" rel="nofollow">Field-based Localization</a></li>
<li><a href="https://payloadcms.com/docs/fields/blocks" rel="nofollow">Block-based Layout Builder</a></li>
<li><a href="https://payloadcms.com/docs/fields/rich-text" rel="nofollow">Extensible SlateJS rich text editor</a></li>
<li><a href="https://payloadcms.com/docs/fields/array" rel="nofollow">Array field type</a></li>
<li><a href="https://payloadcms.com/docs/fields/overview#conditional-logic" rel="nofollow">Field conditional logic</a></li>
<li>Extremely granular <a href="https://payloadcms.com/docs/access-control/overview" rel="nofollow">Access Control</a></li>
<li><a href="https://payloadcms.com/docs/hooks/overview" rel="nofollow">Document and field-level hooks</a> for every action Payload provides</li>
<li>Built with Typescript &amp; very Typescript-friendly</li>
<li>Intensely fast API</li>
<li>Highly secure thanks to HTTP-only cookies, CSRF protection, and more</li>
</ul>
<p dir="auto"><a href="https://github.com/payloadcms/payload/discussions"><strong>Request Feature</strong></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🗒️ Documentation</h2><a id="user-content-️-documentation" aria-label="Permalink: 🗒️ Documentation" href="#️-documentation"></a></p>
<p dir="auto">Check out the <a href="https://payloadcms.com/docs/getting-started/what-is-payload" rel="nofollow">Payload website</a> to find in-depth documentation for everything that Payload offers.</p>
<p dir="auto">Migrating from v1 to v2? Check out the <a href="https://github.com/payloadcms/payload/releases/tag/v2.0.0">2.0 Release Notes</a> on how to do it.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🙋 Contributing</h2><a id="user-content--contributing" aria-label="Permalink: 🙋 Contributing" href="#-contributing"></a></p>
<p dir="auto">If you want to add contributions to this repository, please follow the instructions in <a href="https://github.com/payloadcms/payload/blob/main/CONTRIBUTING.md">contributing.md</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">📚 Examples</h2><a id="user-content--examples" aria-label="Permalink: 📚 Examples" href="#-examples"></a></p>
<p dir="auto">The <a href="https://github.com/payloadcms/payload/blob/main/examples">Examples Directory</a> is a great resource for learning how to setup Payload in a variety of different ways, but you can also find great examples in our blog and throughout our social media.</p>
<p dir="auto">If you'd like to run the examples, you can either copy them to a folder outside this repo or run them directly by (1) navigating to the example's subfolder (<code>cd examples/your-example-folder</code>) and (2) using the <code>--ignore-workspace</code> flag to bypass workspace restrictions (e.g., <code>pnpm --ignore-workspace install</code> or <code>pnpm --ignore-workspace dev</code>).</p>
<p dir="auto">You can see more examples at:</p>
<ul dir="auto">
<li><a href="https://github.com/payloadcms/payload/blob/main/examples">Examples Directory</a></li>
<li><a href="https://payloadcms.com/blog" rel="nofollow">Payload Blog</a></li>
<li><a href="https://www.youtube.com/@payloadcms" rel="nofollow">Payload YouTube</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔌 Plugins</h2><a id="user-content--plugins" aria-label="Permalink: 🔌 Plugins" href="#-plugins"></a></p>
<p dir="auto">Payload is highly extensible and allows you to install or distribute plugins that add or remove functionality. There are both officially-supported and community-supported plugins available. If you maintain your own plugin, consider adding the <code>payload-plugin</code> topic to your GitHub repository for others to find.</p>
<ul dir="auto">
<li><a href="https://github.com/orgs/payloadcms/repositories?q=topic%3Apayload-plugin">Official Plugins</a></li>
<li><a href="https://github.com/topics/payload-plugin">Community Plugins</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚨 Need help?</h2><a id="user-content--need-help" aria-label="Permalink: 🚨 Need help?" href="#-need-help"></a></p>
<p dir="auto">There are lots of good conversations and resources in our Github Discussions board and our Discord Server. If you're struggling with something, chances are, someone's already solved what you're up against. 👇</p>
<ul dir="auto">
<li><a href="https://github.com/payloadcms/payload/discussions">GitHub Discussions</a></li>
<li><a href="https://github.com/payloadcms/payload/issues">GitHub Issues</a></li>
<li><a href="https://t.co/30APlsQUPB" rel="nofollow">Discord</a></li>
<li><a href="https://payloadcms.com/community-help" rel="nofollow">Community Help</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">⭐ Like what we're doing? Give us a star</h2><a id="user-content--like-what-were-doing-give-us-a-star" aria-label="Permalink: ⭐ Like what we're doing? Give us a star" href="#-like-what-were-doing-give-us-a-star"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/38d5e6db344c163aaaae2a559865ff61aca0b9832f4e83bb54012434be974127/68747470733a2f2f636d732e7061796c6f6164636d732e636f6d2f6d656469612f7061796c6f61642d6769746875622d737461722e676966"><img src="https://camo.githubusercontent.com/38d5e6db344c163aaaae2a559865ff61aca0b9832f4e83bb54012434be974127/68747470733a2f2f636d732e7061796c6f6164636d732e636f6d2f6d656469612f7061796c6f61642d6769746875622d737461722e676966" alt="payload-github-star" data-animated-image="" data-canonical-src="https://cms.payloadcms.com/media/payload-github-star.gif"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">👏 Thanks to all our contributors</h2><a id="user-content--thanks-to-all-our-contributors" aria-label="Permalink: 👏 Thanks to all our contributors" href="#-thanks-to-all-our-contributors"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f296954b37ac59ab7f397c85901c87a419bba668dbcef88e624ceb7f8218d7a8/68747470733a2f2f636f6e7472696275746f72732d696d672e7765622e6170702f696d6167653f7265706f3d7061796c6f6164636d732f7061796c6f6164"><img src="https://camo.githubusercontent.com/f296954b37ac59ab7f397c85901c87a419bba668dbcef88e624ceb7f8218d7a8/68747470733a2f2f636f6e7472696275746f72732d696d672e7765622e6170702f696d6167653f7265706f3d7061796c6f6164636d732f7061796c6f6164" data-canonical-src="https://contributors-img.web.app/image?repo=payloadcms/payload"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Response to DHH (160 pts)]]></title>
            <link>https://ma.tt/2024/10/on-dhh/</link>
            <guid>41839864</guid>
            <pubDate>Mon, 14 Oct 2024 17:42:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ma.tt/2024/10/on-dhh/">https://ma.tt/2024/10/on-dhh/</a>, See on <a href="https://news.ycombinator.com/item?id=41839864">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page" role="main">

			
				
<article id="post-128140">
	<!-- .entry-header -->

		<div>
		
<p>I’ll just remind everyone at the start that this is a respectful debate, and DHH and I tried to get on a call but couldn’t because we were both traveling.</p>



<p>However, “<a href="https://world.hey.com/dhh/automattic-is-doing-open-source-dirty-b95cf128">Automattic is doing open source dirty</a>” is an abomination of a headline, and David’s second post <a href="https://world.hey.com/dhh/open-source-royalty-and-mad-kings-a8f79d16">Open source royalty and mad kings</a>, is just sloppy. So I’m forced to reply publicly:</p>



<p>DHH claims to be an expert on open source, but his toxic personality and inability to scale teams means that although he has invented about half a trillion dollars worth of good ideas, most of the value has been captured by others. Let’s look at 37signals portfolio:</p>



<ul>
<li><a href="https://www.hey.com/">Hey</a>, proprietary, some sort of email / calendar / blogging thing that almost no one uses. It’s trying to be Gmail/Workspace and <a href="https://medium.com/">Medium</a> at the same time. And you can arbitrarily cut off anyone publishing with Hey, they have no open source rights.</li>



<li><a href="https://once.com/campfire">Campfire</a>, proprietary, you invented <a href="https://slack.com/">Slack</a> but they took the idea and built a $900M/ARR business with it, while you are trying to make shrinkwrap licensing a “thing” with <a href="https://once.com/">Once</a>.</li>



<li><a href="https://once.com/writebook">Writebook</a>, proprietary. Pretty cool.</li>



<li><a href="https://basecamp.com/">Basecamp</a>, proprietary. Great software. You invented the ideas <a href="https://www.atlassian.com/">Atlassian</a> ran with and built a $4.4B/revenue and growing business.</li>



<li><a href="https://rubyonrails.org/">Rails</a>, finally some open source! Looks like ~943k lines of code, 143k from Basecamp org. Automattic publishes 6.58M lines of open source code, 6.9x more than you. Yet, we’re “doing open source dirty”? <a href="https://www.shopify.com/">Shopify</a> used Rails to build a $7B/revenue and growing business, why didn’t you?</li>
</ul>



<p>David, perhaps it would be good to explore with a therapist or coach why you keep having these great ideas but cannot scale them beyond a handful of niche customers. I will give full credit and respect. 37signals inspired tons of what Automattic does! We’re now half a billion in revenue. Why are you still so small?</p>



<p>I was surprised someone as smart as DHH would fall for WP Engine’s lame deferral to make this about “GPL code” or forking, rather than trademarks. We have no problem with their use of GPL code, our beef is with their trademark abuse.</p>



<p>Let’s talk about trademarks! I don’t own the WordPress trademark personally, it belongs to a foundation on which I’m one of three votes. Rails? </p>



<blockquote>
<p><strong>“Rails”</strong>,&nbsp;<strong>“Ruby on Rails”</strong>, and the&nbsp;<strong>Rails logo</strong>&nbsp;are registered trademarks of&nbsp;<strong>David Heinemeier Hansson</strong>, but are under exclusive license to&nbsp;<a href="https://rubyonrails.org/foundation">The Rails Foundation</a>, which is responsible for administering their use and permission. You may not use these trademarks in a commercial setting to imply that your product or service is endorsed or associated with Ruby on Rails without permission. You may use these marks to refer to Ruby on Rails in a way where it’s clear that you’re simply referring to the project, not claiming endorsement or association.</p>
</blockquote>



<p>Huh, sounds like if I wanted to start RailsEngine I would need a trademark license. You are  ignoring WP Engine’s trademark abuse while retaining the same for your Rails trademark. The same as Drupal, where “Drupal is a registered trademark of Dries Buytaert, who retains sole ownership and control of this policy and any trademark licensing.” (<a href="https://dri.es/solving-the-maker-taker-problem">Dries has also decided to drop in on this debate</a>.)</p>



<p>Dries or David could arbitrarily withdraw their trademarks from the foundations / etc. at any time and for any reason or no reason. If they die, it’s not clear what happens to the trademarks. Their communities should look into that and consider a different name or taking over the trademark into a Foundation with multiple board members. </p>



<p>David, perhaps instead of spending <a href="https://www.autoblog.com/news/pagani-zonda-hh-commissioner-revealed-as-30-year-old-chicago-sof">$2M on a race car</a>, you should do some <a href="https://ma.tt/2024/09/charitable-contributions/">philanthropy</a>.</p>



<p>Instead of <a href="https://world.hey.com/dhh/how-it-started-how-it-s-going-baefaf09">bragging about your beautiful office in the clouds</a>, you should question why you can’t scale teams.</p>



<p>When you did a (less generous) buy-out offer <a href="https://www.theverge.com/2021/4/30/22412714/basecamp-employees-memo-policy-hansson-fried-controversy">33% of your team left</a>, vs <a href="https://ma.tt/2024/10/alignment/">8.4% of mine</a>.</p>



<p>I’m unsure why you felt you had to insert yourself into this fight with Silver Lake / WP Engine and take their side, but here we are.</p>



<p>Respectfully,<br>Matt</p>

	</div><!-- .entry-content -->
	
	<!-- .entry-meta -->
</article><!-- #post -->
						<nav>
		<h2>
			Post navigation		</h2>
		<!-- .nav-links -->
	</nav><!-- .navigation -->
						
<!-- #comments -->
			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scale Ruins Everything (126 pts)]]></title>
            <link>https://coldwaters.substack.com/p/scale-ruins-everything</link>
            <guid>41839776</guid>
            <pubDate>Mon, 14 Oct 2024 17:34:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://coldwaters.substack.com/p/scale-ruins-everything">https://coldwaters.substack.com/p/scale-ruins-everything</a>, See on <a href="https://news.ycombinator.com/item?id=41839776">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>During the </span><a href="https://coldwaters.substack.com/p/the-mystery-box-is-out-of-outs" rel="nofollow ugc noopener">Product Bubble</a><span>, venture capital investments grew from $45 billion to $620 billion. How did speculation in companies that </span><em>might </em><span>exist nearly catch up to $1.1 trillion of private equity speculation in companies that </span><em>do</em><span> exist? And why did that torrent of money ruin so much? </span></p><p><span>As we’ve </span><a href="https://coldwaters.substack.com/p/the-mystery-box-is-out-of-outs" rel="nofollow ugc noopener">previously discussed</a><span>, wealth flocked to corporate equity to chase stories that could still come true - what poker players call </span><em>outs</em><span>. VC funds created even more outs, bundling otherwise unlikely startups and promising that only a few had to come true. </span></p><p><span>Massive unicorns were needed to carry all that deadweight, and it turns out that even fluffy and harmless things are absolutely </span><em>terrifying</em><span> if you make them big enough.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08a8a929-6327-4335-bc40-2aabe432bf79_2400x1008.jpeg" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08a8a929-6327-4335-bc40-2aabe432bf79_2400x1008.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08a8a929-6327-4335-bc40-2aabe432bf79_2400x1008.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08a8a929-6327-4335-bc40-2aabe432bf79_2400x1008.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08a8a929-6327-4335-bc40-2aabe432bf79_2400x1008.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08a8a929-6327-4335-bc40-2aabe432bf79_2400x1008.jpeg" width="1456" height="612" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/08a8a929-6327-4335-bc40-2aabe432bf79_2400x1008.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:612,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Ghostbusters Fx Stay Puft 2&quot;,&quot;title&quot;:&quot;Ghostbusters Fx Stay Puft 2&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Ghostbusters Fx Stay Puft 2" title="Ghostbusters Fx Stay Puft 2" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08a8a929-6327-4335-bc40-2aabe432bf79_2400x1008.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08a8a929-6327-4335-bc40-2aabe432bf79_2400x1008.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08a8a929-6327-4335-bc40-2aabe432bf79_2400x1008.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08a8a929-6327-4335-bc40-2aabe432bf79_2400x1008.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>It’s the Stay Puft Short Term Rental App</figcaption></figure></div><p>VC funds knew they were putting $300 billion a year into worthless companies at the bubble’s peak, because a typical fund looks like this:</p><ul><li><p>🦄 10% of companies are home runs that provide nearly all the fund’s returns</p></li><li><p>😑 30% of companies pay for themselves, and maybe give some modest return </p></li><li><p>💩 60% of companies are complete failures that go to zero</p></li></ul><p><span>On top of this power distribution, funds had to get an absolutely </span><em>massive</em><span> pile of cash into play. They never know exactly who will win or lose, but they chased more and more marginal deal flow to get everything on the table. VC funds now needed to find a hundred unicorns </span><em>per year</em><span> to keep their aggregate returns above a Fidelity IRA. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102aeac4-0630-41fa-8e4a-42bd3e2cc777_2000x713.jpeg" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102aeac4-0630-41fa-8e4a-42bd3e2cc777_2000x713.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102aeac4-0630-41fa-8e4a-42bd3e2cc777_2000x713.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102aeac4-0630-41fa-8e4a-42bd3e2cc777_2000x713.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102aeac4-0630-41fa-8e4a-42bd3e2cc777_2000x713.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102aeac4-0630-41fa-8e4a-42bd3e2cc777_2000x713.jpeg" width="728" height="259.532" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/102aeac4-0630-41fa-8e4a-42bd3e2cc777_2000x713.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:713,&quot;width&quot;:2000,&quot;resizeWidth&quot;:728,&quot;bytes&quot;:321279,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102aeac4-0630-41fa-8e4a-42bd3e2cc777_2000x713.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102aeac4-0630-41fa-8e4a-42bd3e2cc777_2000x713.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102aeac4-0630-41fa-8e4a-42bd3e2cc777_2000x713.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102aeac4-0630-41fa-8e4a-42bd3e2cc777_2000x713.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Follow the Unicorn</figcaption></figure></div><p>Off the top of your head, you can probably think of a few ways of making a billion dollar company. For example:</p><ul><li><p>A B2B SaaS Analytics company with $70 million in growing ARR. Doable with a decent product, a great inside sales team, and a few years to ramp up. </p></li><li><p>A Government Contractor with $80 million in annual EBITDA. Doable with govie relationships across multiple programs, access to primes, and a decade or so to ramp up past performance. A war helps. </p></li><li><p>A Social Network with 5 million daily users. Doable if you find the right niche, get some sticky viral growth going, and target countries with good GDP/capita. </p></li></ul><p><span>These aren’t that weird. But the funds need 100 winners, </span><a href="https://amplitude.com/" rel="nofollow ugc noopener">and</a><span> </span><a href="https://www.fivetran.com/" rel="nofollow ugc noopener">they</a><span> </span><a href="https://mode.com/" rel="nofollow ugc noopener">can’t</a><span> </span><em><a href="https://www.snowflake.com/en/" rel="nofollow ugc noopener">all</a></em><span> </span><a href="https://www.sigmacomputing.com/" rel="nofollow ugc noopener">be</a><span> </span><a href="https://www.thoughtspot.com/" rel="nofollow ugc noopener">B2B</a><span> </span><a href="https://cloud.google.com/looker/" rel="nofollow ugc noopener">SaaS</a><span> </span><a href="https://www.databricks.com/" rel="nofollow ugc noopener">Analytics</a><span> </span><a href="https://hi.firebolt.io/" rel="nofollow ugc noopener">Platforms</a><span>. Every VC company needs a path to a billion dollar valuation. Ideally multi-billion. </span></p><p>So what does a billion dollar dog food company look like in real life? Sure, you have an Excel model… but what does it look like to feed a million dogs? Where are you going to get a half million pounds of meat every single day? What if you keep growing?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39b189dc-a8d8-4b3a-9626-6d9d290f558a_1994x380.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39b189dc-a8d8-4b3a-9626-6d9d290f558a_1994x380.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39b189dc-a8d8-4b3a-9626-6d9d290f558a_1994x380.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39b189dc-a8d8-4b3a-9626-6d9d290f558a_1994x380.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39b189dc-a8d8-4b3a-9626-6d9d290f558a_1994x380.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39b189dc-a8d8-4b3a-9626-6d9d290f558a_1994x380.png" width="1994" height="380" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/39b189dc-a8d8-4b3a-9626-6d9d290f558a_1994x380.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:380,&quot;width&quot;:1994,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1210531,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39b189dc-a8d8-4b3a-9626-6d9d290f558a_1994x380.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39b189dc-a8d8-4b3a-9626-6d9d290f558a_1994x380.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39b189dc-a8d8-4b3a-9626-6d9d290f558a_1994x380.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39b189dc-a8d8-4b3a-9626-6d9d290f558a_1994x380.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Twenty more herds like this and you’ve got one day’s breakfast!</figcaption></figure></div><p><a href="https://www.youtube.com/watch?v=2zhDfUAQSbs" rel="nofollow ugc noopener">Ghostbusters (1983)</a><span> isn’t just a top five action-comedy; it’s a prescient look into the weirdness of a unicorn-dominated world. When the Ghostbusters were forced to choose the form of the Destroyer of New York, they tried to think of the most harmless thing. Something that could never destroy us. Oops. </span></p><div><p><strong>Ray</strong><span>: I tried to think of the most harmless thing… something that could never destroy us… </span><br><span>Mr. Stay Puft.</span></p><p><strong>Venkman</strong><span>: Nice thinking, Ray</span></p><p><strong>Ray</strong><span>: We used to roast Stay Puft Marshmallows. By the fire. At Camp Waconda…</span></p><p><strong>Venkman</strong><span>: Ray has gone by-by Egon… what have you got left?</span></p><p><strong>Egon</strong><span>: Sorry Venkman, I’m terrified beyond the capacity for rational thought. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d03240f-42f2-4e68-aa7b-5a6cd06f6206_850x351.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d03240f-42f2-4e68-aa7b-5a6cd06f6206_850x351.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d03240f-42f2-4e68-aa7b-5a6cd06f6206_850x351.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d03240f-42f2-4e68-aa7b-5a6cd06f6206_850x351.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d03240f-42f2-4e68-aa7b-5a6cd06f6206_850x351.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d03240f-42f2-4e68-aa7b-5a6cd06f6206_850x351.png" width="850" height="351" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9d03240f-42f2-4e68-aa7b-5a6cd06f6206_850x351.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:351,&quot;width&quot;:850,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:200410,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d03240f-42f2-4e68-aa7b-5a6cd06f6206_850x351.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d03240f-42f2-4e68-aa7b-5a6cd06f6206_850x351.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d03240f-42f2-4e68-aa7b-5a6cd06f6206_850x351.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d03240f-42f2-4e68-aa7b-5a6cd06f6206_850x351.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Ray just needs some seed funding</figcaption></figure></div></div><p>The Ghostbusters aren’t alone. </p><p>Tony, Evan, Andy, and Stanley tried to think of the most harmless thing: a universal food delivery service, expanding the options from just Chinese food and pizza. </p><p>Brian, Nathan, and Joe tried to think of the most harmless thing: people making extra money but renting out their spare room for the night. </p><p>Travis and Garrett tried to think of the most harmless thing: people making extra money by giving people car rides. </p><p>Mark tried to think of the most harmless thing: keeping track of your friends, and organizing group photos. With comments! </p><p>Those were the innocuous founding ideas behind DoorDash, AirBnB, Uber, and Facebook. At scale, they would ruin communities, put restaurants out of business, destroy the dream of home ownership, and eventually undermine democracy itself. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F638478b6-bdf8-449d-ac6f-31c049e55d17_1080x537.jpeg" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F638478b6-bdf8-449d-ac6f-31c049e55d17_1080x537.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F638478b6-bdf8-449d-ac6f-31c049e55d17_1080x537.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F638478b6-bdf8-449d-ac6f-31c049e55d17_1080x537.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F638478b6-bdf8-449d-ac6f-31c049e55d17_1080x537.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F638478b6-bdf8-449d-ac6f-31c049e55d17_1080x537.jpeg" width="728" height="361.97777777777776" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/638478b6-bdf8-449d-ac6f-31c049e55d17_1080x537.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:537,&quot;width&quot;:1080,&quot;resizeWidth&quot;:728,&quot;bytes&quot;:86471,&quot;alt&quot;:&quot;red and yellow no smoking sign&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="red and yellow no smoking sign" title="red and yellow no smoking sign" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F638478b6-bdf8-449d-ac6f-31c049e55d17_1080x537.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F638478b6-bdf8-449d-ac6f-31c049e55d17_1080x537.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F638478b6-bdf8-449d-ac6f-31c049e55d17_1080x537.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F638478b6-bdf8-449d-ac6f-31c049e55d17_1080x537.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Your fries are soggy, your dasher hates you, the restaurant is going out of business, and DoorDash loses a billion dollars a year. </figcaption></figure></div><p><span>Now that they know those could do so much damage, founders looking for VC funding might think harder about their own ideas. What’s the society-destroying version of </span><a href="https://colossal.com/" rel="nofollow ugc noopener">bringing back extinct species through gene editing</a><span>? Not the current business model, or the next one. The one at massive scale that makes them a unicorn. It’s probably not the problem </span><a href="https://www.youtube.com/watch?v=h58lRIVHhGc" rel="nofollow ugc noopener">we’re all thinking of</a><span>. </span></p><p>Uber and AirBnB grew almost in lock-step. Both sets of founders thought that they were on the road to improving society... based on their original idea. Both were unlocking stagnant supply, reducing waste and helping people make money. And they were right at first, as they helped users monetize extra beds and cars for extra cash.</p><p>But there simply wasn’t enough extra stuff to hit the scale they needed. They tried encouraging some users users to buy more beds and cars to turn into full-time businesses. Then those users grew to be most of their users. Eventually, they induced so much demand for beds and cars amongst entrepreneurs that they made them less affordable for consumers. </p><p>Each company is now valued around $100b. And rents in tourist areas have become unaffordable. Would their younger selves have gone down this path if they knew where it led? Maybe, maybe not.</p><p>Maybe the VC funds themselves will consider whether they actually work at this scale. </p><p><span>Too much wealth flowing into VC exploded assets under management, and caused inefficient investment (not </span><em>your</em><span> fund, of course, I mean the other ones). Unicorns had to chase society-bending hyper growth to drag the resulting deadweight ahead of the S&amp;P 500, and that was </span><em>before</em><span> the half trillion dollars thrown in during 2021.</span></p><p><span>The bump in interest rates has cooled things down. Funds, LPs, and founders have some breathing room to consider whether this all makes sense. Is there a billion-dollar version of </span><em>every</em><span> idea? Do they </span><em>need</em><span> to maximize AUM? Does most funds even beat the market? </span></p><p><a href="https://unu.edu/press-release/un-study-reveals-hidden-environmental-impacts-bitcoin-carbon-not-only-harmful-product" rel="nofollow ugc noopener">Maybe they’ll just put it all in bitcoin.</a></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Vortex – a high-performance columnar file format (200 pts)]]></title>
            <link>https://github.com/spiraldb/vortex</link>
            <guid>41839773</guid>
            <pubDate>Mon, 14 Oct 2024 17:34:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/spiraldb/vortex">https://github.com/spiraldb/vortex</a>, See on <a href="https://news.ycombinator.com/item?id=41839773">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Vortex</h2><a id="user-content-vortex" aria-label="Permalink: Vortex" href="#vortex"></a></p>
<p dir="auto"><a href="https://github.com/spiraldb/vortex/actions"><img src="https://github.com/fulcrum-so/vortex/actions/workflows/ci.yml/badge.svg" alt="Build Status"></a>
<a href="https://crates.io/crates/vortex-array" rel="nofollow"><img src="https://camo.githubusercontent.com/5e4c7b2bac2564f0b28006fdc5832127a05ecfe4c324a299deca48c113f2539a/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f766f727465782d61727261792e737667" alt="Crates.io" data-canonical-src="https://img.shields.io/crates/v/vortex-array.svg"></a>
<a href="https://docs.rs/vortex-array" rel="nofollow"><img src="https://camo.githubusercontent.com/bb82ef82b46a0d868cb2eaa855fb1724ffd3d9d614d5ff26be508aec779e9792/68747470733a2f2f646f63732e72732f766f727465782d61727261792f62616467652e737667" alt="Documentation" data-canonical-src="https://docs.rs/vortex-array/badge.svg"></a>
<a href="https://pypi.org/project/vortex-array/" rel="nofollow"><img src="https://camo.githubusercontent.com/506d8e3db54d4420bdf98ef197df095cb9bf0b76bf29d032704d00cb6e4f4198/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f766f727465782d6172726179" alt="PyPI - Python Version" data-canonical-src="https://img.shields.io/pypi/pyversions/vortex-array"></a></p>
<p dir="auto">Vortex is a toolkit for working with compressed Apache Arrow arrays in-memory, on-disk, and over-the-wire.</p>
<p dir="auto">Vortex is designed to be to columnar file formats what Apache DataFusion is to query engines (or, analogously,
what LLVM + Clang are to compilers): a highly extensible &amp; extremely fast <em>framework</em> for building a modern
columnar file format, with a state-of-the-art, "batteries included" reference implementation.</p>
<p dir="auto">Vortex is an aspiring successor to Apache Parquet, with dramatically faster random access reads (100-200x faster)
and scans (2-10x faster), while preserving approximately the same compression ratio and write throughput. It will also support very wide
tables (at least 10s of thousands of columns) and (eventually) on-device decompression on GPUs.</p>
<div dir="auto"><p dir="auto">Caution</p><p dir="auto">This library is still under rapid development and is a work in progress!</p>
<p dir="auto">Some key features are not yet implemented, both the API and the serialized format are likely to change in breaking ways,
and we cannot yet guarantee correctness in all cases.</p>
</div>
<p dir="auto">The major features of Vortex are:</p>
<ul dir="auto">
<li><strong>Logical Types</strong> - a schema definition that makes no assertions about physical layout.</li>
<li><strong>Zero-Copy to Arrow</strong> - "canonicalized" (i.e., fully decompressed) Vortex arrays can be zero-copy converted to/from Apache Arrow arrays.</li>
<li><strong>Extensible Encodings</strong> - a pluggable set of physical layouts. In addition to the builtin set of Arrow-compatible encodings,
the Vortex repository includes a number of state-of-the-art encodings (e.g., FastLanes, ALP, FSST, etc.) that are implemented
as extensions. While arbitrary encodings can be implemented as extensions, we have intentionally chosen a small set
of encodings that are highly data-parallel, which in turn allows for efficient vectorized decoding, random access reads,
and (in the future) decompression on GPUs.</li>
<li><strong>Cascading Compression</strong> - data can be recursively compressed with multiple nested encodings.</li>
<li><strong>Pluggable Compression Strategies</strong> - the built-in Compressor is based on BtrBlocks, but other strategies can trivially be used instead.</li>
<li><strong>Compute</strong> - basic compute kernels that can operate over encoded data (e.g., for filter pushdown).</li>
<li><strong>Statistics</strong> - each array carries around lazily computed summary statistics, optionally populated at read-time.
These are available to compute kernels as well as to the compressor.</li>
<li><strong>Serialization</strong> - Zero-copy serialization of arrays, both for IPC and for file formats.</li>
<li><strong>Columnar File Format (in progress)</strong> - A modern file format that uses the Vortex serde library to store compressed array data.
Optimized for random access reads and extremely fast scans; an aspiring successor to Apache Parquet.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview: Logical vs Physical</h2><a id="user-content-overview-logical-vs-physical" aria-label="Permalink: Overview: Logical vs Physical" href="#overview-logical-vs-physical"></a></p>
<p dir="auto">One of the core design principles in Vortex is strict separation of logical and physical concerns.</p>
<p dir="auto">For example, a Vortex array is defined by a logical data type (i.e., the type of scalar elements) as well as a physical encoding
(the type of the array itself). Vortex ships with several built-in encodings, as well as several extension encodings.</p>
<p dir="auto">The built-in encodings are primarily designed to model the Apache Arrow in-memory format, enabling us to construct
Vortex arrays with zero-copy from Arrow arrays. There are also several built-in encodings (e.g., <code>sparse</code> and
<code>chunked</code>) that are useful building blocks for other encodings. The included extension encodings are mostly designed
to model compressed in-memory arrays, such as run-length or dictionary encoding.</p>
<p dir="auto">Analogously, <code>vortex-serde</code> is designed to handle the low-level physical details of reading and writing Vortex arrays. Choices
about which encodings to use or how to logically chunk data are left up to the <code>Compressor</code> implementation.</p>
<p dir="auto">One of the unique attributes of the (in-progress) Vortex file format is that it encodes the physical layout of the data within the
file's footer. This allows the file format to be effectively self-describing and to evolve without breaking changes to
the file format specification.</p>
<p dir="auto">In fact, the format is designed to support forward compatibility by optionally embedding WASM decoders directly into the files
themselves. This should help avoid the rapid calcification that has plagued other columnar file formats.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Components</h2><a id="user-content-components" aria-label="Permalink: Components" href="#components"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Logical Types</h3><a id="user-content-logical-types" aria-label="Permalink: Logical Types" href="#logical-types"></a></p>
<p dir="auto">The Vortex type-system is still in flux. The current set of logical types is:</p>
<ul dir="auto">
<li>Null</li>
<li>Bool</li>
<li>Integer(8, 16, 32, 64)</li>
<li>Float(16, b16, 32, 64)</li>
<li>Binary</li>
<li>UTF8</li>
<li>Struct</li>
<li>List (partially implemented)</li>
<li>Date/Time/DateTime/Duration (implemented as an extension type)</li>
<li>Decimal: TODO</li>
<li>FixedList: TODO</li>
<li>Tensor: TODO</li>
<li>Union: TODO</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Canonical/Flat Encodings</h3><a id="user-content-canonicalflat-encodings" aria-label="Permalink: Canonical/Flat Encodings" href="#canonicalflat-encodings"></a></p>
<p dir="auto">Vortex includes a base set of "flat" encodings that are designed to be zero-copy with Apache Arrow. These are the
canonical representations of each of the logical data types. The canonical encodings currently supported are:</p>
<ul dir="auto">
<li>Null</li>
<li>Bool</li>
<li>Primitive (Integer, Float)</li>
<li>Struct</li>
<li>VarBin (Binary, UTF8)</li>
<li>VarBinView (Binary, UTF8)</li>
<li>Extension</li>
<li>...with more to come</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Compressed Encodings</h3><a id="user-content-compressed-encodings" aria-label="Permalink: Compressed Encodings" href="#compressed-encodings"></a></p>
<p dir="auto">Vortex includes a set of highly data-parallel, vectorized encodings. These encodings each correspond to a compressed
in-memory array implementation, allowing us to defer decompression. Currently, these are:</p>
<ul dir="auto">
<li>Adaptive Lossless Floating Point (ALP)</li>
<li>BitPacked (FastLanes)</li>
<li>Constant</li>
<li>Chunked</li>
<li>Delta (FastLanes)</li>
<li>Dictionary</li>
<li>Fast Static Symbol Table (FSST)</li>
<li>Frame-of-Reference</li>
<li>Run-end Encoding</li>
<li>RoaringUInt</li>
<li>RoaringBool</li>
<li>Sparse</li>
<li>ZigZag</li>
<li>...with more to come</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Compression</h3><a id="user-content-compression" aria-label="Permalink: Compression" href="#compression"></a></p>
<p dir="auto">Vortex's default compression strategy is based on the
<a href="https://www.cs.cit.tum.de/fileadmin/w00cfj/dis/papers/btrblocks.pdf" rel="nofollow">BtrBlocks</a> paper.</p>
<p dir="auto">Roughly, for each chunk of data, a sample of at least ~1% of the data is taken. Compression is then attempted (
recursively) with a set of lightweight encodings. The best-performing combination of encodings is then chosen to encode
the entire chunk. This sounds like it would be very expensive, but given basic statistics about a chunk, it is
possible to cheaply prune many encodings and ensure the search space does not explode in size.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Compute</h3><a id="user-content-compute" aria-label="Permalink: Compute" href="#compute"></a></p>
<p dir="auto">Vortex provides the ability for each encoding to specialize the implementation of a compute function to avoid
decompressing where possible. For example, filtering a dictionary-encoded UTF8 array can be more cheaply performed by
filtering the dictionary first.</p>
<p dir="auto">Note--as mentioned above--that Vortex does not intend to become a full-fledged compute engine, but rather to implement
basic compute operations as may be required for efficient scanning &amp; pushdown.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Statistics</h3><a id="user-content-statistics" aria-label="Permalink: Statistics" href="#statistics"></a></p>
<p dir="auto">Vortex arrays carry lazily-computed summary statistics. Unlike other array libraries, these statistics can be populated
from disk formats such as Parquet and preserved all the way into a compute engine. Statistics are available to compute
kernels as well as to the compressor.</p>
<p dir="auto">The current statistics are:</p>
<ul dir="auto">
<li>BitWidthFreq</li>
<li>TrailingZeroFreq</li>
<li>IsConstant</li>
<li>IsSorted</li>
<li>IsStrictSorted</li>
<li>Max</li>
<li>Min</li>
<li>RunCount</li>
<li>TrueCount</li>
<li>NullCount</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Serialization / Deserialization (Serde)</h3><a id="user-content-serialization--deserialization-serde" aria-label="Permalink: Serialization / Deserialization (Serde)" href="#serialization--deserialization-serde"></a></p>
<p dir="auto">The goals of the <code>vortex-serde</code> implementation are:</p>
<ul dir="auto">
<li>Support scanning (column projection + row filter) with zero-copy and zero heap allocation.</li>
<li>Support random access in constant or near-constant time.</li>
<li>Forward statistical information (such as sortedness) to consumers.</li>
<li>Provide IPC format for sending arrays between processes.</li>
<li>Provide an extensible, best-in-class file format for storing columnar data on disk or in object storage.</li>
</ul>
<p dir="auto">TODO: insert diagram here</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Integration with Apache Arrow</h2><a id="user-content-integration-with-apache-arrow" aria-label="Permalink: Integration with Apache Arrow" href="#integration-with-apache-arrow"></a></p>
<p dir="auto">Apache Arrow is the de facto standard for interoperating on columnar array data. Naturally, Vortex is designed to
be maximally compatible with Apache Arrow. All Arrow arrays can be converted into Vortex arrays with zero-copy,
and a Vortex array constructed from an Arrow array can be converted back to Arrow, again with zero-copy.</p>
<p dir="auto">It is important to note that Vortex and Arrow have different--albeit complementary--goals.</p>
<p dir="auto">Vortex explicitly separates logical types from physical encodings, distinguishing it from Arrow. This allows
Vortex to model more complex arrays while still exposing a logical interface. For example, Vortex can model a UTF8
<code>ChunkedArray</code> where the first chunk is run-length encoded and the second chunk is dictionary encoded.
In Arrow, <code>RunLengthArray</code> and <code>DictionaryArray</code> are separate incompatible types, and so cannot be combined in this way.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage</h3><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">For best performance we recommend using <a href="https://github.com/microsoft/mimalloc">MiMalloc</a> as the application's
allocator.</p>
<div dir="auto" data-snippet-clipboard-copy-content="#[global_allocator]
static GLOBAL_ALLOC: MiMalloc = MiMalloc;"><pre><span>#<span>[</span>global_allocator<span>]</span></span>
<span>static</span> <span>GLOBAL_ALLOC</span><span>:</span> <span>MiMalloc</span> = <span>MiMalloc</span><span>;</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Please see <a href="https://github.com/spiraldb/vortex/blob/develop/CONTRIBUTING.md">CONTRIBUTING.md</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<p dir="auto">In order to build vortex, you may also need to install the flatbuffer compiler (flatc):</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mac</h3><a id="user-content-mac" aria-label="Permalink: Mac" href="#mac"></a></p>

<p dir="auto">This repo uses rye to manage the combined Rust/Python monorepo build. First, make sure to run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install Rye from https://rye-up.com, and setup the virtualenv
rye sync"><pre><span><span>#</span> Install Rye from https://rye-up.com, and setup the virtualenv</span>
rye sync</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Licensed under the Apache License, Version 2.0 (the "License").</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Governance</h2><a id="user-content-governance" aria-label="Permalink: Governance" href="#governance"></a></p>
<p dir="auto">Vortex is and will remain an open-source project. Our intent is to model its governance structure after the
<a href="https://substrait.io/governance/" rel="nofollow">Substrait project</a>, which in turn is based on the model of the Apache Software Foundation.
Expect more details on this in Q4 2024.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgments 🏆</h2><a id="user-content-acknowledgments-" aria-label="Permalink: Acknowledgments 🏆" href="#acknowledgments-"></a></p>
<p dir="auto">This project is inspired by and--in some cases--directly based upon the existing, excellent work of many researchers
and OSS developers.</p>
<p dir="auto">In particular, the following academic papers greatly influenced the development:</p>
<ul dir="auto">
<li>Maximilian Kuschewski, David Sauerwein, Adnan Alhomssi, and Viktor Leis.
2023. <a href="https://www.cs.cit.tum.de/fileadmin/w00cfj/dis/papers/btrblocks.pdf" rel="nofollow">BtrBlocks: Efficient Columnar Compression
for Data Lakes</a>. Proc. ACM Manag. Data 1,
2,
Article 118 (June 2023), 14 pages. <a href="https://doi.org/10.1145/3589263" rel="nofollow">https://doi.org/10.1145/3589263</a></li>
<li>Azim Afroozeh and Peter
Boncz. <a href="https://www.vldb.org/pvldb/vol16/p2132-afroozeh.pdf" rel="nofollow">The FastLanes Compression Layout: Decoding &gt;100 Billion Integers per Second with Scalar
Code</a>. PVLDB, 16(9): 2132 - 2144, 2023.</li>
<li>Peter Boncz, Thomas Neumann, and Viktor Leis. <a href="https://www.vldb.org/pvldb/vol13/p2649-boncz.pdf" rel="nofollow">FSST: Fast Random Access String
Compression</a>.
PVLDB, 13(11): 2649-2661, 2020.</li>
<li>Azim Afroozeh, Leonardo X. Kuffo, and Peter Boncz. 2023. <a href="https://ir.cwi.nl/pub/33334/33334.pdf" rel="nofollow">ALP: Adaptive Lossless floating-Point
Compression</a>. Proc. ACM
Manag. Data 1, 4 (SIGMOD), Article 230 (December 2023), 26 pages. <a href="https://doi.org/10.1145/3626717" rel="nofollow">https://doi.org/10.1145/3626717</a></li>
</ul>
<p dir="auto">Additionally, we benefited greatly from:</p>
<ul dir="auto">
<li>the existence, ideas, &amp; implementation of <a href="https://arrow.apache.org/" rel="nofollow">Apache Arrow</a>.</li>
<li>likewise for the excellent <a href="https://github.com/apache/datafusion">Apache DataFusion</a> project.</li>
<li>the <a href="https://github.com/jorgecarleitao/parquet2">parquet2</a> project by <a href="https://github.com/jorgecarleitao">Jorge Leitao</a>.</li>
<li>the public discussions around choices of compression codecs, as well as the C++ implementations thereof,
from <a href="https://github.com/duckdb/duckdb">duckdb</a>.</li>
<li>the <a href="https://github.com/facebookincubator/velox">Velox</a> and <a href="https://github.com/facebookincubator/nimble">Nimble</a> projects,
and discussions with their maintainers.</li>
</ul>
<p dir="auto">Thanks to all of the aforementioned for sharing their work and knowledge with the world! 🚀</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Furilabs Linux Phone (114 pts)]]></title>
            <link>https://furilabs.com/shop/flx1/</link>
            <guid>41839326</guid>
            <pubDate>Mon, 14 Oct 2024 16:52:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://furilabs.com/shop/flx1/">https://furilabs.com/shop/flx1/</a>, See on <a href="https://news.ycombinator.com/item?id=41839326">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-elementor-type="product" data-elementor-id="2206" data-elementor-post-type="elementor_library">
					<div data-id="44bb64ec" data-element_type="section" data-settings="{&quot;stretch_section&quot;:&quot;section-stretched&quot;}">
					<div data-id="2a270102" data-element_type="column" data-widget_type="shortcode.default">
					<p><a id="flx1" data-magic360-options="columns:73;rows:1; images: https://furilabs.com/wp-content/uploads/2024/03/flx1_001.png https://furilabs.com/wp-content/uploads/2024/03/flx1_011.png https://furilabs.com/wp-content/uploads/2024/03/flx1_021.png https://furilabs.com/wp-content/uploads/2024/03/flx1_031.png https://furilabs.com/wp-content/uploads/2024/03/flx1_041.png https://furilabs.com/wp-content/uploads/2024/03/flx1_051.png https://furilabs.com/wp-content/uploads/2024/03/flx1_061.png https://furilabs.com/wp-content/uploads/2024/03/flx1_071.png https://furilabs.com/wp-content/uploads/2024/03/flx1_081.png https://furilabs.com/wp-content/uploads/2024/03/flx1_091.png https://furilabs.com/wp-content/uploads/2024/03/flx1_101.png https://furilabs.com/wp-content/uploads/2024/03/flx1_111.png https://furilabs.com/wp-content/uploads/2024/03/flx1_121.png https://furilabs.com/wp-content/uploads/2024/03/flx1_131.png https://furilabs.com/wp-content/uploads/2024/03/flx1_141.png https://furilabs.com/wp-content/uploads/2024/03/flx1_151.png https://furilabs.com/wp-content/uploads/2024/03/flx1_161.png https://furilabs.com/wp-content/uploads/2024/03/flx1_171.png https://furilabs.com/wp-content/uploads/2024/03/flx1_181.png https://furilabs.com/wp-content/uploads/2024/03/flx1_191.png https://furilabs.com/wp-content/uploads/2024/03/flx1_201.png https://furilabs.com/wp-content/uploads/2024/03/flx1_211.png https://furilabs.com/wp-content/uploads/2024/03/flx1_221.png https://furilabs.com/wp-content/uploads/2024/03/flx1_231.png https://furilabs.com/wp-content/uploads/2024/03/flx1_241.png https://furilabs.com/wp-content/uploads/2024/03/flx1_251.png https://furilabs.com/wp-content/uploads/2024/03/flx1_261.png https://furilabs.com/wp-content/uploads/2024/03/flx1_271.png https://furilabs.com/wp-content/uploads/2024/03/flx1_281.png https://furilabs.com/wp-content/uploads/2024/03/flx1_291.png https://furilabs.com/wp-content/uploads/2024/03/flx1_301.png https://furilabs.com/wp-content/uploads/2024/03/flx1_311.png https://furilabs.com/wp-content/uploads/2024/03/flx1_321.png https://furilabs.com/wp-content/uploads/2024/03/flx1_331.png https://furilabs.com/wp-content/uploads/2024/03/flx1_341.png https://furilabs.com/wp-content/uploads/2024/03/flx1_351.png https://furilabs.com/wp-content/uploads/2024/03/flx1_361.png https://furilabs.com/wp-content/uploads/2024/03/flx1_371.png https://furilabs.com/wp-content/uploads/2024/03/flx1_381.png https://furilabs.com/wp-content/uploads/2024/03/flx1_391.png https://furilabs.com/wp-content/uploads/2024/03/flx1_401.png https://furilabs.com/wp-content/uploads/2024/03/flx1_411.png https://furilabs.com/wp-content/uploads/2024/03/flx1_421.png https://furilabs.com/wp-content/uploads/2024/03/flx1_431.png https://furilabs.com/wp-content/uploads/2024/03/flx1_441.png https://furilabs.com/wp-content/uploads/2024/03/flx1_451.png https://furilabs.com/wp-content/uploads/2024/03/flx1_461.png https://furilabs.com/wp-content/uploads/2024/03/flx1_471.png https://furilabs.com/wp-content/uploads/2024/03/flx1_481.png https://furilabs.com/wp-content/uploads/2024/03/flx1_491.png https://furilabs.com/wp-content/uploads/2024/03/flx1_501.png https://furilabs.com/wp-content/uploads/2024/03/flx1_511.png https://furilabs.com/wp-content/uploads/2024/03/flx1_521.png https://furilabs.com/wp-content/uploads/2024/03/flx1_531.png https://furilabs.com/wp-content/uploads/2024/03/flx1_541.png https://furilabs.com/wp-content/uploads/2024/03/flx1_551.png https://furilabs.com/wp-content/uploads/2024/03/flx1_561.png https://furilabs.com/wp-content/uploads/2024/03/flx1_571.png https://furilabs.com/wp-content/uploads/2024/03/flx1_581.png https://furilabs.com/wp-content/uploads/2024/03/flx1_591.png https://furilabs.com/wp-content/uploads/2024/03/flx1_601.png https://furilabs.com/wp-content/uploads/2024/03/flx1_611.png https://furilabs.com/wp-content/uploads/2024/03/flx1_621.png https://furilabs.com/wp-content/uploads/2024/03/flx1_631.png https://furilabs.com/wp-content/uploads/2024/03/flx1_641.png https://furilabs.com/wp-content/uploads/2024/03/flx1_651.png https://furilabs.com/wp-content/uploads/2024/03/flx1_661.png https://furilabs.com/wp-content/uploads/2024/03/flx1_671.png https://furilabs.com/wp-content/uploads/2024/03/flx1_681.png https://furilabs.com/wp-content/uploads/2024/03/flx1_691.png https://furilabs.com/wp-content/uploads/2024/03/flx1_701.png https://furilabs.com/wp-content/uploads/2024/03/flx1_711.png https://furilabs.com/wp-content/uploads/2024/03/flx1_721.png; large-images: https://furilabs.com/wp-content/uploads/2024/03/flx1_001.png https://furilabs.com/wp-content/uploads/2024/03/flx1_011.png https://furilabs.com/wp-content/uploads/2024/03/flx1_021.png https://furilabs.com/wp-content/uploads/2024/03/flx1_031.png https://furilabs.com/wp-content/uploads/2024/03/flx1_041.png https://furilabs.com/wp-content/uploads/2024/03/flx1_051.png https://furilabs.com/wp-content/uploads/2024/03/flx1_061.png https://furilabs.com/wp-content/uploads/2024/03/flx1_071.png https://furilabs.com/wp-content/uploads/2024/03/flx1_081.png https://furilabs.com/wp-content/uploads/2024/03/flx1_091.png https://furilabs.com/wp-content/uploads/2024/03/flx1_101.png https://furilabs.com/wp-content/uploads/2024/03/flx1_111.png https://furilabs.com/wp-content/uploads/2024/03/flx1_121.png https://furilabs.com/wp-content/uploads/2024/03/flx1_131.png https://furilabs.com/wp-content/uploads/2024/03/flx1_141.png https://furilabs.com/wp-content/uploads/2024/03/flx1_151.png https://furilabs.com/wp-content/uploads/2024/03/flx1_161.png https://furilabs.com/wp-content/uploads/2024/03/flx1_171.png https://furilabs.com/wp-content/uploads/2024/03/flx1_181.png https://furilabs.com/wp-content/uploads/2024/03/flx1_191.png https://furilabs.com/wp-content/uploads/2024/03/flx1_201.png https://furilabs.com/wp-content/uploads/2024/03/flx1_211.png https://furilabs.com/wp-content/uploads/2024/03/flx1_221.png https://furilabs.com/wp-content/uploads/2024/03/flx1_231.png https://furilabs.com/wp-content/uploads/2024/03/flx1_241.png https://furilabs.com/wp-content/uploads/2024/03/flx1_251.png https://furilabs.com/wp-content/uploads/2024/03/flx1_261.png https://furilabs.com/wp-content/uploads/2024/03/flx1_271.png https://furilabs.com/wp-content/uploads/2024/03/flx1_281.png https://furilabs.com/wp-content/uploads/2024/03/flx1_291.png https://furilabs.com/wp-content/uploads/2024/03/flx1_301.png https://furilabs.com/wp-content/uploads/2024/03/flx1_311.png https://furilabs.com/wp-content/uploads/2024/03/flx1_321.png https://furilabs.com/wp-content/uploads/2024/03/flx1_331.png https://furilabs.com/wp-content/uploads/2024/03/flx1_341.png https://furilabs.com/wp-content/uploads/2024/03/flx1_351.png https://furilabs.com/wp-content/uploads/2024/03/flx1_361.png https://furilabs.com/wp-content/uploads/2024/03/flx1_371.png https://furilabs.com/wp-content/uploads/2024/03/flx1_381.png https://furilabs.com/wp-content/uploads/2024/03/flx1_391.png https://furilabs.com/wp-content/uploads/2024/03/flx1_401.png https://furilabs.com/wp-content/uploads/2024/03/flx1_411.png https://furilabs.com/wp-content/uploads/2024/03/flx1_421.png https://furilabs.com/wp-content/uploads/2024/03/flx1_431.png https://furilabs.com/wp-content/uploads/2024/03/flx1_441.png https://furilabs.com/wp-content/uploads/2024/03/flx1_451.png https://furilabs.com/wp-content/uploads/2024/03/flx1_461.png https://furilabs.com/wp-content/uploads/2024/03/flx1_471.png https://furilabs.com/wp-content/uploads/2024/03/flx1_481.png https://furilabs.com/wp-content/uploads/2024/03/flx1_491.png https://furilabs.com/wp-content/uploads/2024/03/flx1_501.png https://furilabs.com/wp-content/uploads/2024/03/flx1_511.png https://furilabs.com/wp-content/uploads/2024/03/flx1_521.png https://furilabs.com/wp-content/uploads/2024/03/flx1_531.png https://furilabs.com/wp-content/uploads/2024/03/flx1_541.png https://furilabs.com/wp-content/uploads/2024/03/flx1_551.png https://furilabs.com/wp-content/uploads/2024/03/flx1_561.png https://furilabs.com/wp-content/uploads/2024/03/flx1_571.png https://furilabs.com/wp-content/uploads/2024/03/flx1_581.png https://furilabs.com/wp-content/uploads/2024/03/flx1_591.png https://furilabs.com/wp-content/uploads/2024/03/flx1_601.png https://furilabs.com/wp-content/uploads/2024/03/flx1_611.png https://furilabs.com/wp-content/uploads/2024/03/flx1_621.png https://furilabs.com/wp-content/uploads/2024/03/flx1_631.png https://furilabs.com/wp-content/uploads/2024/03/flx1_641.png https://furilabs.com/wp-content/uploads/2024/03/flx1_651.png https://furilabs.com/wp-content/uploads/2024/03/flx1_661.png https://furilabs.com/wp-content/uploads/2024/03/flx1_671.png https://furilabs.com/wp-content/uploads/2024/03/flx1_681.png https://furilabs.com/wp-content/uploads/2024/03/flx1_691.png https://furilabs.com/wp-content/uploads/2024/03/flx1_701.png https://furilabs.com/wp-content/uploads/2024/03/flx1_711.png https://furilabs.com/wp-content/uploads/2024/03/flx1_721.png;" href="https://bunny-wp-pullzone-nrxogzjcpx.b-cdn.net/wp-content/uploads/2024/03/flx1_011.png"><img width="425" height="800" alt="" src="https://bunny-wp-pullzone-nrxogzjcpx.b-cdn.net/wp-content/uploads/2024/03/flx1_011.png"></a></p>
				</div>
				<div data-id="29071f2" data-element_type="column">
						
				<div data-id="531cddd" data-element_type="widget" data-widget_type="woocommerce-product-etheme_title.default">
				<p>
			<h2>FLX1</h2>		</p>
				</div>
				<div data-id="46830f0" data-element_type="widget" data-widget_type="woocommerce-product-etheme_price.default">
				<p> <span>Original price was: $550.00.</span><span>Current price is: $499.00.</span></p>
				</div>
				<div data-id="41a954a" data-element_type="widget" data-widget_type="woocommerce-product-etheme_short_description.default">
	<p>Fast, performant and cheap. You wanted all 3? Now you got it! The FLX1 from Furi Labs runs a fully optimized system called Furi OS, packing a lightning fast user interface, tons of storage, and a privacy centric approach like no other.</p>
<p>FuriLabs is ready to protect your data and keep you connected and secure at all times. Long term support, removable battery, IP68 and an unmatched price point. Get your hands on one today. FuriLabs: planned permanence!</p>
<p>Dimensions &amp; Weights:<br>
Phone: 171mm x 82mm x 12mm : 280g<br>
Box: 180mm x 90mm x 28mm : 76g<br>
Total: 180mm x 90mm x 28mm : 356g</p>
<p>See our <a href="https://furilabs.com/shipping/">shipping details</a>.</p>
<p>Read some reviews in the below tabs or <a href="https://furilabs.com/furilabs-in-the-media/">some external reviews</a>.</p>

</div>
				
				
				
				
				<div data-id="74b5f00" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h2>Share:</h2>		</p>
				</div>
				
					</div>
					</div>
				<div data-title="Tabs" data-id="32646211" data-element_type="section" data-widget_type="woocommerce-product-etheme_tabs.default" data-settings="{&quot;stretch_section&quot;:&quot;section-stretched&quot;}">
		
		
		
			<div data-id="ff3df52" data-element_type="container" data-widget_type="text-editor.default" data-elementor-type="product-post" data-elementor-id="77" data-elementor-post-type="product" id="tab-description" role="tabpanel" aria-labelledby="tab-title-description">
							<p>The FLX1 runs FuriOS, which is an operating system based on Debian, designed and oriented for mobile use without any artificial limitations.</p><p>Custom camera app with high quality video recording, picture capabilities and native QR code reading and Wifi hotspot joing via QR code.</p><p>VoLTE, MMS, SMS and the ability to use Android apps in a customized container are some of the additions we have built into FuriOS.</p>						</div>

		
			<div id="tab-et_custom_tab_01" role="tabpanel" aria-labelledby="tab-title-et_custom_tab_01"><table>
<tbody>
<tr>
<td colspan="2"><strong>Motherboard</strong></td>
</tr>
<tr>
<td>Chipset</td>
<td>Mediatek Dimensity 900</td>
</tr>
<tr>
<td>Memory</td>
<td>6GB LPDDR4X</td>
</tr>
<tr>
<td>Storage</td>
<td>128GB UFS2.1</td>
</tr>
<tr>
<td>CPU</td>
<td>2x Cortex-A78 2.4Ghz &amp;&amp; 6x Cortex A55 2.0Ghz</td>
</tr>
<tr>
<td>GPU</td>
<td>Mali G68 MC4</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td colspan="2"><strong>Camera</strong></td>
</tr>
<tr>
<td>Front Camera</td>
<td>16MP</td>
</tr>
<tr>
<td>Back Camera</td>
<td>50MP with Optical Image stabilization</td>
</tr>
<tr>
<td>Macro Camera</td>
<td>2MP</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td colspan="2"><strong>Battery</strong></td>
</tr>
<tr>
<td>Charging</td>
<td>Wired/Wireless and NFC combo</td>
</tr>
<tr>
<td>Battery Type</td>
<td>Li-Po Removable battery</td>
</tr>
<tr>
<td>Battery capacity</td>
<td>5000mAh</td>
</tr>
<tr>
<td>USB</td>
<td>Type C 3.0 waterproof</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td colspan="2"><strong>Connectivity</strong></td>
</tr>
<tr>
<td>Modem</td>
<td>2G/3G/4G/5G/5G ENDC</td>
</tr>
<tr>
<td>SIM Slots</td>
<td>Dual</td>
</tr>
<tr>
<td>WiFi</td>
<td>WiFi 6.0 (a/b/g/n/ac/ax)</td>
</tr>
<tr>
<td>Bluetooth</td>
<td>5.2, A2DP, LE</td>
</tr>
<tr>
<td>ESIM</td>
<td>N/a</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td colspan="2"><strong>Bands</strong></td>
</tr>
<tr>
<td>GSM</td>
<td>2/3/5/8</td>
</tr>
<tr>
<td>UMTS</td>
<td>B1/8</td>
</tr>
<tr>
<td>TD-LTE</td>
<td>B38/40</td>
</tr>
<tr>
<td>FDD-LTE</td>
<td>B1/3/5/7/8/20/28A/28B</td>
</tr>
<tr>
<td>5G</td>
<td>NR N1, 2, 3, 4, 5, 7, 8, 12, 20, 28, 38, 40, 41, 60, 66, 77, 78 SA/NSA</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td colspan="2"><strong>Screen</strong></td>
</tr>
<tr>
<td>Resolution</td>
<td>6.59" FHD+ IPS Display 10 point multi touch</td>
</tr>
<tr>
<td>Refresh rate</td>
<td>60Hz/120Hz panel</td>
</tr>
<tr>
<td>Glass type</td>
<td>Gorilla Glass 5</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td colspan="2"><strong>Fingerprint</strong></td>
</tr>
<tr>
<td>Fingerprint</td>
<td>Side Mounted, on power button</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td colspan="2"><strong>Peripherals</strong></td>
</tr>
<tr>
<td>Micro SD</td>
<td>Up to 1TB</td>
</tr>
<tr>
<td>Headphone jack</td>
<td>3.5mm waterproof</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td colspan="2"><strong>Material</strong></td>
</tr>
<tr>
<td>Back cover</td>
<td>Polycarbonate</td>
</tr>
<tr>
<td>Mid frame</td>
<td>Polycarbonate and TPU</td>
</tr>
<tr>
<td>Keys</td>
<td>Metal</td>
</tr>
<tr>
<td>Water proof</td>
<td>IP68</td>
</tr>
</tbody>
</table></div>

		
			<div id="tab-additional_information" role="tabpanel" aria-labelledby="tab-title-additional_information">
						

<table aria-label="Product Details">
			<tbody><tr>
			<th scope="row">Weight</th>
			<td>0.356 kg</td>
		</tr>
			<tr>
			<th scope="row">Dimensions</th>
			<td>18 × 9 × 2.80 cm</td>
		</tr>
	</tbody></table>
					</div>

		
			<div id="tab-reviews" role="tabpanel" aria-labelledby="tab-title-reviews">

    
    <div id="comments">

        
        <h2>
            3 reviews for <span>FLX1</span>        </h2>

                    <ol>
                <li id="li-comment-304">

	<div id="comment-304">

		<p><img alt="" src="https://secure.gravatar.com/avatar/45e1d2b07a0aee59c602933bcb7d5b78?s=80&amp;d=mm&amp;r=g" srcset="https://secure.gravatar.com/avatar/45e1d2b07a0aee59c602933bcb7d5b78?s=160&amp;d=mm&amp;r=g 2x" height="80" width="80" decoding="async" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2080%2080'%3E%3C/svg%3E"></p><div>

			<p><span>Rated <strong>5</strong> out of 5</span></p>
	<p>
		<strong>alaraajavamma </strong>
		<em>(verified owner)</em> 		<span>–</span> <time datetime="2024-07-26T13:47:28+10:00">July 26, 2024</time>
	</p>

	<div><p>By an incredible margin, the best linux phone I’ve had – and I’ve had practically all of them (Pinephones, Librem 5, Vollas etc.). For the first time this truly competes in the same world as the billion dollar Android / ios devices. </p>
<p>Imagine a full-blooded Phosh desktop environment with fully working camera &amp; gps and a device that is truly lightning fast and has battery protection according to today’s phone standards (without suspend 24 hours)</p>
</div>
		</div>
	</div>
</li><!-- #comment-## -->
<li id="li-comment-369">

	<div id="comment-369">

		<p><img alt="" src="https://secure.gravatar.com/avatar/71644a78a55e90778258c566472776a6?s=80&amp;d=mm&amp;r=g" srcset="https://secure.gravatar.com/avatar/71644a78a55e90778258c566472776a6?s=160&amp;d=mm&amp;r=g 2x" height="80" width="80" decoding="async" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2080%2080'%3E%3C/svg%3E"></p><div>

			<p><span>Rated <strong>5</strong> out of 5</span></p>
	<p>
		<strong>Brian Aberts </strong>
		<em>(verified owner)</em> 		<span>–</span> <time datetime="2024-09-06T09:54:17+10:00">September 6, 2024</time>
	</p>

	<div><p>This phone isn’t ready YET to hand to grandma and say “Use this!”. But I think it’s quickly heading in that direction. The amount of progress the dev team has made on it in the past month and a half has been an incredible journey. I bought this not knowing if the phone would work or not, as I had never heard of Furilabs… But I took the chance and ordered one on July 19, 2024.</p>
<p>When I first got it, I found out that the phone didn’t work at all in the USA. It would display a 2G or 2.75G symbol, but wouldn’t make calls or browse the web, as it didn’t have support for US cellular bands, only Europe and the rest of the world. I offered to give SSH access to my device to one of the developers, and have been letting him test modem builds on my device with a Ting MVNO Sim. At first it seemed kind of unbelievable that it would work and I’ll be honest, I also wasn’t confident in the developers to get it working. I’m glad I did though, because as of today the latest modem build the developer has put on the device has allowed me to place and receive calls, and browse the web on 4G LTE. 5G doesn’t work yet, and only band 4 of 4G LTE is enabled currently on the test build I’m running, but I can use the phone now!</p>
<p>I will continue to offer my device to run test builds, and I’m excited to see the progress. The phone’s software is extremely liberating compared to android. It’s fast, it runs a familiar debian-based Linux environment, and it already has some really unique features I have been wanting in a phone! For example, there is a button on the left side of the phone that you can program from within the settings to run shell scripts, take screenshots, open the camera or take pictures, and toggle the flashlight. This is the first Linux phone I’ve had with such a feature, and I think it’s a fantastic addition.</p>
<p>With my thoughts on the camera, it takes great quality pictures, and even records video! It might sound odd I am counting recording video as a feature, but other Linux phones like the PinePhone and Librem 5 do not support video recording with the camera, unless you use hacky scripts. Aside from that it’s fully water resistant, has a removable microSD slot, dual sim capability (at least in hardware, as currently that’s not enabled in software), a LED indicator for notifications, a fully working fingerprint sensor (as in you can enroll each finger on your hand and use it to login to the phone – at least after you login once from a cold boot with your pin or password), full disk encryption, Android app support with Waydroid (but amped up as it supports the phone’s sensors and GPS and runs android apps as if they’re native apps), and volume toggle in the accesibility settings menu that lets you set the speaker volume to 150%.</p>
<p>You might be wondering why you wouldn’t just buy a PinePhone, Librem 5, or a phone that can run Ubuntu Touch instead of this device. My answer to that is that the FLX1 while yes, it does use Halium, has far better support than a typical Ubuntu Touch or Droidian capable device. The developers involved with the project have been involved with the Droidian project, and are the one of very people who did a lot of the work to get good Waydroid integration on those systems originally. With the FLX1 they’re amping it up even more, with further improvements, fixes, and features that other devices haven’t seen and probably won’t. A big one for me being VoLTE and VoNR support, something even the mature Ubuntu Touch project still doesn’t have publicly available. To my knowledge, this is the first Linux phone that has 5G fully working, and notably 4G VoLTE working globally with hopefully soon to follow global 5G VoNR support. As I’ve said, the US band support for the rest of the 4G range and 5G is still something that they’re working on, and I hope to soon report progress on that front. Very exciting!</p>
<p>There’s tons of additional settings to dive into, such as USB settings for MTP, USB state, CD-ROM settings, a NFC support toggle, GPS SUPL server setting you can set to a custom server URL, a printer setting panel for adding and managing printers, and a slew of accesibility and privacy/security settings such as controlling screen lock, location access, file history, camera access, and more. Some of those things have already existed in phosh, but a number of them have been enabled or modified, offering things you don’t usually get on mobile Linux. A quick note on the Waydroid settings, it has a full panel for controlling starting/stopping of android apps, clearing app data, as well as controlling NFC access to Waydroid and enabling a shared folder. It also gives you the IP of the Android container and gives you information on the Android version running in the container. The developers have mentioned to me that they have big plans to rework phosh to be a bit cleaner, add more fine grained controls and settings, and add features such as RCS support for messaging and context-aware text suggestions in the phosh keyboard. </p>
<p>With that said, the new features like RCS likely won’t arrive for a bit as they have the cellular stuff to sort, as well as some bug fixes and other features they want to polish first. The device does have some bugs in Firefox and with video acceleration in video players such as MPV, and I’ve personally noticed that compass bearing in Waydroid is currently bugged although GPS itself does work otherwise. That alongside the fact that 4G LTE band support is currently limited to Band 4 in the USA means that I realize this phone isn’t currently for everyone, but I think it’s important to note that the developers are incredibly active on telegram and respond immediately to problems you might have. There has only been one device so far, to the best of my knowledge, that was defective from the manufacturer they use, but Furilabs was quick to replace it and apologize to the person for the inconvience. I believe they even overnighted it to him before taking a look at the device to see what hardware had failed.</p>
<p>The device has a fantastic custom recovery system. You can SSH in to copy files or try to fix a software issue, and it has a factory reset system to easily roll back to the original software, or you could even reflash the phone completely using a computer. Another big thing to note is that the developers also are on a roll at releasing bug fixes and offering workarounds for issues. Infact, they’re currently testing a new software release in their beta-tester telegram channel with a couple of volunteers to see if there’s any bugs that may have slipped through before release. Currently they aim to release a new update every month, and they release the full changelog on their website for everyone to see. When I asked about automated testing, one of the developers mentioned they would like to get to implementing that when they get a chance. </p>
<p>I highly recommend you take a look at the changelogs, web forum, and telegram as there’s tons of awesome information available. Not just to mention the fact that the phone is open source, with source code avaialable on github for everything except the modem and some of the recovery I believe (Which has intulectual property bits that mediatek wouldn’t be happy to have public). It’s not perfect right now, but nothing is perfect, let’s be real! What it is however, is incredibly promising. I’ve never had more hope in a Linux phone than this one. I’ve gotten up at 6AM some days to excitedly work with the developer to test out new modem firmware because it’s been a steady march of improvements each day, and that’s just the modem! I am highly confident within the next month, mauybe two, that this device is something I will be able to give to my mother to use.</p>
<p>With the open source stack, and the GTK framework powering applications, Android app support in a pinch, a powerful Debian-based stack with a proper Linux terminal and libraries, bash scripting, and powerful hardware, it has a lot to offer. I will update this review as it progresses to add more information and expand on where the software stands as it improves, but while this phone isn’t for everyone just year, I hope some of you reading this might be willing to pull the trigger and support the project by grabbing one of these, and my reasoning is simple: In order to survive and continue to improve on the device, Furilabs is going to need money coming in from sales. This is a small company, and every little thing will count. I’ve never had more hope for a Linux phone project than this one, and I’ve tussled with a lot over the years. And I’m sure if you’re interested you can reach out on the Telegram to maybe test out the latest modem builds for USA support and get it polished up quicker!</p>
<p>Posted 9/5/24</p>
</div>
		</div>
	</div>
</li><!-- #comment-## -->
<li id="li-comment-370">

	<div id="comment-370">

		<p><img alt="" src="https://secure.gravatar.com/avatar/3913dac94a176be1fd1a0fefe07cb7c6?s=80&amp;d=mm&amp;r=g" srcset="https://secure.gravatar.com/avatar/3913dac94a176be1fd1a0fefe07cb7c6?s=160&amp;d=mm&amp;r=g 2x" height="80" width="80" decoding="async" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2080%2080'%3E%3C/svg%3E"></p><div>

			<p><span>Rated <strong>5</strong> out of 5</span></p>
	<p>
		<strong>Hman </strong>
		<em>(verified owner)</em> 		<span>–</span> <time datetime="2024-09-06T10:35:03+10:00">September 6, 2024</time>
	</p>

	<div><p>From iPhone to Furiphone</p>
<p>I bought this phone because I care about my data and personal privacy. I didn’t realize that such a phone would radically alter my relationship to my phone: they could have just called it a “myPhone” because the phone (not to mention your data) belong to YOU. It feels like having something unique and that is YOURS in your pocket, which is an experience that I didn’t even realize was possible until getting it. You can also tell that the development team has made this a labor of LOVE. It is always a pleasure being able to talk to them about the phone and feel like you’re helping to develop a piece of linux history. (The first fully functional linux phone!)</p>
<p>You can install all the linux apps you want, while having access to all traditional apps through the excellent waydroid service installed on the phone. The phone is accessible through terminal on my computer and it has been great using a familiar interface to install apps and manage files. The latest updates are always something to look forward to, making the phone better and better with each iteration. The camera app runs buttery smooth and takes excellent pictures, and hard as it is to believe there will only be more improvements!</p>
<p>I was an iPhone user before and find myself missing the iOS ecosystem very little with this phone. It has all the apps and features that I need, they run well, and delivers a personal experience with great technical support staff available to talk about any problems that you have. It does a lot of things better than my iPhone, like provide instant access to all my folders and files on other devices through setting up a single app like Syncthing.</p>
<p>If you love Linux, having your own experience with technology and a great community attached to it, then you will love this phone. It’s worth it even beyond the added privacy and peace of mind that only linux can afford. So what are you waiting for? Go out and BUY your own now!</p>
</div>
		</div>
	</div>
</li><!-- #comment-## -->
            </ol>

                        </div>

            <p>Only logged in customers who have purchased this product may leave a review.</p>
    
    
</div>

					</div>
				</div></div>]]></description>
        </item>
    </channel>
</rss>