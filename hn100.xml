<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 30 Nov 2024 01:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Brits are scrolling away from X and aren't that interested in AI (120 pts)]]></title>
            <link>https://www.theregister.com/2024/11/29/ofcom_online_nation/</link>
            <guid>42277089</guid>
            <pubDate>Fri, 29 Nov 2024 21:03:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2024/11/29/ofcom_online_nation/">https://www.theregister.com/2024/11/29/ofcom_online_nation/</a>, See on <a href="https://news.ycombinator.com/item?id=42277089">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>Usage of Elon Musk's X social media platform is declining in the UK, and adult Brits aren't particularly interested in generative AI tools.</p>
<p>That's according to Ofcom's <a target="_blank" rel="nofollow" href="https://www.ofcom.org.uk/media-use-and-attitudes/online-habits/online-nation/">Online Nation</a> report, an annual publication looking at what UK citizens do during their hours online and how much time they spend glued to gadgets.</p>
<p>According to the comms regulator, adults spent an average of four hours 20 minutes a day online in May 2024 across tablets, smartphones, and computers.</p>

    

<p>The report also shows that the total UK adult reach in a month of X (formerly Twitter) continues to decline. For May 2022, Ofcom measured X's adult reach at 26.8 million. In 2023, it was 24 million. By May 2024, it had fallen to 22.1 million, a year-on-year decline of 8 percent.</p>

        


        

<p>X suffered the most significant fall in total adult use of all social media sites, which also resulted in it sliding down the rankings to sixth, behind Reddit, which registered the largest year-on-year growth – 47 percent – taking May's figure to 22.9 million.</p>
<p><a target="_blank" rel="nofollow" href="https://www.theregister.com/2024/11/20/x_marks_the_spot_for/">Several changes</a> have been made at X in recent months, but the platform has continued on a downward trajectory in Britain and Northern Ireland.</p>

        

<p>Ofcom's figures align with other research on X. While still hugely popular, the service has been shedding users over the last two years. UK-based <a target="_blank" rel="nofollow" href="https://soax.com/research/twitter-active-users">SOAX reported</a> an 8.83 percent decrease in monthly active users since 2022 and 5.14 percent since 2023. This is despite global growth in social media users, according to <a target="_blank" rel="nofollow" href="https://www.statista.com/statistics/278414/number-of-worldwide-social-network-users/">Statista</a>.</p>
<p>The decline co-incides with a change of ownership after Musk bought the business for <a target="_blank" href="https://www.theregister.com/2022/10/27/musk_sink_twitter/">$44 billion in October 2022</a>, and it became a doyen of free speech - whether that includes more hateful or more honest content depends on the users' perspective.</p>
<h3>AI – huh – what is good for? Perhaps a bit of searching?</h3>
<p>Ofcom also found that Google's search engine dominance in the UK also slipped slightly over the year, with 83 percent of online adults visiting in May 2024 compared to 86 percent the year before. Microsoft's Bing fell further, down to 39 percent from 46 percent.</p>
<p>Microsoft and Google have invested heavily in AI, with generative AI content turning up in the search results from their respective services. However, ChatGPT remains the most popular GenAI tool. Microsoft's Copilot came second, with 15 percent of UK internet users aged 16 and over having used it. Despite being only recently introduced, Google's Gemini was ranked fourth, with 10 percent of users.</p>
<ul>

<li><a href="https://www.theregister.com/2024/11/21/online_safety_act/">Now Online Safety Act is law, UK has 'priorities' – but still won't explain 'spy clause'</a></li>

<li><a href="https://www.theregister.com/2024/11/13/ofcom_mmwave_spectrum_auction/">Brit telcos to clash in high-speed mmWave spectrum showdown next year</a></li>

<li><a href="https://www.theregister.com/2024/11/12/cma_vodafone_three_remedies/">Watchdog reluctantly blesses Vodafone-Three merger – with strings attached</a></li>

<li><a href="https://www.theregister.com/2024/11/11/australia_social_media_ban/">Australia tells tots: No TikTok till you're 16... or X, Instagram and Facebook</a></li>
</ul>
<p>The figures also show sluggish AI adoption. More than half of adults in the survey had yet to use GenAI, with 38 percent declaring they were "not interested" and 35 percent saying they "did not need to."</p>
<p>Forty-eight percent of adults had used the technology, but only "for fun." Forty-three percent had used one for work, and the most popular activity was finding content. However, less than one in five (18 percent) trusted the output.</p>
<p>The numbers are slightly different for the under-16s. Fifty-four percent said they had used a GenAI tool, with more than half (53 percent) of those saying they had used it for schoolwork. Sixty-three percent reported using a GenAI tool "for fun."</p>

        

<p>The distribution across age groups shows that AI is making more significant inroads into younger demographics than older. However, it appears that investors may have to wait a little longer before AI bets start paying off. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Breaking the 4Chan CAPTCHA (170 pts)]]></title>
            <link>https://www.nullpt.rs/breaking-the-4chan-captcha</link>
            <guid>42276865</guid>
            <pubDate>Fri, 29 Nov 2024 20:32:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nullpt.rs/breaking-the-4chan-captcha">https://www.nullpt.rs/breaking-the-4chan-captcha</a>, See on <a href="https://news.ycombinator.com/item?id=42276865">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>Breaking the 4Chan CAPTCHA</span></p><h2 id="introduction">Introduction</h2>
<p>This project was entered into as a learning experience, to enhance my knowledge of machine learning, as well as TensorFlow specifically. At the end, I wanted to have a trained machine learning model that runs in the browser to reliably (at least 80% accuracy, &gt;90% preferred) solve the 4Chan CAPTCHA. These goals were achieved - let's talk about how I got there!</p>
<p>If you'd like to follow along with the code references, I have made the code public on GitHub <a href="https://github.com/AppleDash/4chan-captcha-playground">here</a>.</p>
<h2 id="terminology">Terminology</h2>
<ul>
<li><strong>CAPTCHA</strong>: A challenge-response test to determine whether or not a computer or website user is a human. The acronym stands for Completely Automated Public Turing test to tell Computers and Humans Apart.</li>
<li><strong>4Chan</strong>: A public, anonymous imageboard website with discussion boards on various topics. These boards are used for posting images and text discussions. Filling out a CAPTCHA is required before every post or reply.</li>
<li><strong>Normal CAPTCHA</strong>: The simplest form of the 4Chan CAPTCHA, that consists of an image with 5 or 6 alphanumeric characters. The user must read and correctly enter all of the characters in a field in order to make a post on 4Chan.</li>
<li><strong>Slider CAPTCHA</strong>: A more complex form of the 4Chan CAPTCHA, that consists of a background image with random-looking character fragments, and a foreground image with transparent "holes" or "windows" in it. A slider in the browser CAPTCHA form must be moved to correctly align the two images in order to see the CAPTCHA text.</li>
</ul>
<h2 id="getting-the-data">Getting the Data</h2>
<p>I've heard many times that the hardest part of any machine learning problem is getting the data to train your model. This assertion was definitely pertinent here, for several reasons. There's two parts to this problem: Getting the CAPTCHAs, and getting solutions to those CAPTCHAs.</p>
<h3 id="scraping-captchas-from-4chan">Scraping CAPTCHAs from 4Chan</h3>
<p>After looking at the HTTP requests in the browser console when requesting a new CAPTCHA, I found that it makes a request to <code>https://sys.4chan.org/captcha?framed=1&amp;board={board}</code>, where <code>{board}</code> is the name of the board we're trying to post on. The response is an HTML document that contains a script tag with a <code>window.parent.postMessage()</code> call with some JSON. On a hunch, I tried to remove the <code>framed=1</code> parameter, and found that this causes it to just spit out the raw JSON. That should be easier to work with. The JSON looks like this:</p>
<pre><code><span>{</span>
    <span>"challenge"</span><span>:</span> <span>"[some long and random string here]"</span><span>,</span>
    <span>"ttl"</span><span>:</span> <span>120</span><span>,</span>
    <span>"cd"</span><span>:</span> <span>5</span><span>,</span>
    <span>"img"</span><span>:</span> <span>"[a base64 string here]"</span><span>,</span>
    <span>"img_width"</span><span>:</span> <span>300</span><span>,</span>
    <span>"img_height"</span><span>:</span> <span>80</span><span>,</span>
    <span>"bg"</span><span>:</span> <span>"[a base64 string here]"</span><span>,</span>
    <span>"bg_width"</span><span>:</span> <span>349</span>
<span>}</span>
</code></pre>
<p>Some of these keys are pretty obvious. <code>ttl</code> and <code>cd</code> are the least obvious to me. I know from experience that the 4Chan CAPTCHA only displays for about 2 minutes before it expires and you have to request a new one, so that's what <code>ttl</code> must be. But what about <code>cd</code>? Let's make another request, shortly after the first one:</p>
<pre><code><span>{</span>
    <span>"error"</span><span>:</span> <span>"You have to wait a while before doing this again"</span><span>,</span>
    <span>"cd"</span><span>:</span> <span>23</span>
<span>}</span>
</code></pre>
<p>If I keep making the same request, the <code>cd</code> parameter steadily decreases, at a rate of about 1 per second. Alright, so this is how long you have to wait before requesting a new CAPTCHA. <code>cd</code> likely stands for "cooldown".</p>
<p>If I wait the 23 seconds, and then make another request, I get a successful response, but this time, the <code>cd</code> is 32. We have to wait longer every time. After some experimentation with a script, it looks like the first few requests can be made every 5 seconds, then it increments to 8, and then continues to roughly double until it's capped at 280 seconds, and stays there.</p>
<p>Additionally, once you've hit the 280 second timers, the CAPTCHA gets somewhat harder. It looks like this:</p>
<figure><img alt="Difficult 4Chan CAPTCHA with several horizontal lines and an oval obscuring the text, in addition to general noise all over the image" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-hard.png&amp;w=640&amp;q=75 640w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-hard.png&amp;w=750&amp;q=75 750w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-hard.png&amp;w=828&amp;q=75 828w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-hard.png&amp;w=1080&amp;q=75 1080w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-hard.png&amp;w=1200&amp;q=75 1200w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-hard.png&amp;w=1920&amp;q=75 1920w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-hard.png&amp;w=2048&amp;q=75 2048w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-hard.png&amp;w=3840&amp;q=75 3840w" src="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-hard.png&amp;w=3840&amp;q=75"><figcaption>Difficult 4Chan CAPTCHA with several horizontal lines and an oval obscuring the text, in addition to general noise all over the image</figcaption></figure>
<p>instead of this:</p>
<figure><img alt="Easier 4Chan CAPTCHA with one curved line over the text, in addition to general noise all over the image" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-easy.png&amp;w=640&amp;q=75 640w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-easy.png&amp;w=750&amp;q=75 750w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-easy.png&amp;w=828&amp;q=75 828w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-easy.png&amp;w=1080&amp;q=75 1080w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-easy.png&amp;w=1200&amp;q=75 1200w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-easy.png&amp;w=1920&amp;q=75 1920w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-easy.png&amp;w=2048&amp;q=75 2048w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-easy.png&amp;w=3840&amp;q=75 3840w" src="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-easy.png&amp;w=3840&amp;q=75"><figcaption>Easier 4Chan CAPTCHA with one curved line over the text, in addition to general noise all over the image</figcaption></figure>
<p>So, there's some throttling in place. The data also gets of lower (but still usable) quality if you make too many requests.</p>
<p>Something I will briefly touch on is that the user has to pass a Cloudflare Turnstile challenge to even request a CAPTCHA in the first place. As a result, simply using many proxies with a naive script is not realistic, without first passing the Cloudflare Turnstile and saving the relevant cookies. When I was scraping CAPTCHAs with the script I wrote for this, I simply copied the Cloudflare cookies from my browser, and manually replaced them whenever they expired.</p>
<p>I scraped several hundred CAPTCHAs in this manner - not enough to train the model, but it's at least a start. This still leaves us with a problem, though. We have all these CAPTCHAs, but we don't have the solutions. I could fill them out manually, but instead, let's try something else.</p>
<h3 id="getting-the-solutions">Getting the Solutions</h3>
<p>Or, <em>Humans are bad at solving the 4Chan CAPTCHA</em>.</p>
<p>A recurring theme in this project has been "this is easy for a computer to do, but hard for a human to do." Many users find the "slider" style CAPTCHAs incredibly frustrating, but I've had a 100% success rate in aligning them with the heuristic script I made (<code>trainer/captcha_aligner.py</code>). The 4Chan CAPTCHAs in general are widely considered by users of the site to be frustrating to solve. But, surely, for people who solve CAPTCHAs <em>for a living</em>, it shouldn't be an insurmountable challenge, right?</p>
<p>I coded a quick script (seen in the project under <code>trainer/labeler.py</code>) to send the CAPTCHAs to <a href="https://anti-captcha.com/">a commercial CAPTCHA solving service</a>, where real humans would solve the CAPTCHAs for me for a nominal fee. Writing the script was simple, but actually employing it was an exercise in frustration. I sent a couple dozen CAPTCHAs to the service, and nearly all of them came back with one or more characters incorrectly solved.</p>
<p>The service has a feature called "100% Recognition", which allowed me to specify that all my requests be first sent to <code>n</code> workers, and if <code>x</code> of those workers don't return the same solution, then send them to up to <code>y</code> more workers. It would only return an error after sending the CAPTCHAs to <code>n + y</code> workers and not getting at least <code>x</code> solutions the same. I configured my account with the values <code>n = 2</code>, <code>x = 2</code>, and <code>y = 3</code> - that is, initially send the CAPTCHA to 2 workers, and if they don't both agree, then send them to up to 3 additional workers until two of them agree, or none of them agree.</p>
<p>This improved the situation somewhat. About 80% of the CAPTCHAs were now being successfully solved, and after reviewing the results, 90% of those were correct, but about 10% had errors in them, which indicated that multiple workers were making the same mistakes. This was still less than ideal.</p>
<p><strong>A quick aside: What if I just ask someone I know to be reliable to do it for me, or even do it myself?</strong> I explored both of these approaches. I wrote a quick user script that saved the CAPTCHA image and the solution text, and just sat there requesting and solving CAPTCHAs in my free time. I also asked a good friend of mine to do the same. This yielded several hundred images, which I did add to the training set, but in the end this approach was abandoned because we still ran into the throttling problem, and the problem of the CAPTCHAs getting harder (and eventually, near-impossible) if you request too many of them.</p>
<p>I begun to wonder if there was a different way to look at this altogether. What if we didn't need to scrape CAPTCHAs and have them solved by humans?</p>
<h3 id="generating-synthetic-data">Generating Synthetic Data</h3>
<p>What if we could generate our own 4Chan CAPTCHAs? 4Chan, and the CAPTCHA it uses, are not open source, so I couldn't literally run the same code locally. But I could definitely approximate it.</p>
<p>The 4Chan CAPTCHA can be dissected into two main parts. The background, which looks like this:</p>
<figure><img alt="4Chan CAPTCHA background with the characters removed, leaving only general noise over the image" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-background.png&amp;w=640&amp;q=75 640w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-background.png&amp;w=750&amp;q=75 750w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-background.png&amp;w=828&amp;q=75 828w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-background.png&amp;w=1080&amp;q=75 1080w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-background.png&amp;w=1200&amp;q=75 1200w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-background.png&amp;w=1920&amp;q=75 1920w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-background.png&amp;w=2048&amp;q=75 2048w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-background.png&amp;w=3840&amp;q=75 3840w" src="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-background.png&amp;w=3840&amp;q=75"><figcaption>4Chan CAPTCHA background with the characters removed, leaving only general noise over the image</figcaption></figure>
<p>and the characters, which look like this:</p>
<figure><img alt="4Chan CAPTCHA character set, the characters 0, 2, 4, A, D, G, H, K, M, N, P, S, X, and Y with general noise and distortion" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-letters.png&amp;w=640&amp;q=75 640w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-letters.png&amp;w=750&amp;q=75 750w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-letters.png&amp;w=828&amp;q=75 828w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-letters.png&amp;w=1080&amp;q=75 1080w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-letters.png&amp;w=1200&amp;q=75 1200w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-letters.png&amp;w=1920&amp;q=75 1920w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-letters.png&amp;w=2048&amp;q=75 2048w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-letters.png&amp;w=3840&amp;q=75 3840w" src="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-letters.png&amp;w=3840&amp;q=75"><figcaption>4Chan CAPTCHA character set, the characters 0, 2, 4, A, D, G, H, K, M, N, P, S, X, and Y with general noise and distortion</figcaption></figure>
<p>We don't need to generate our own backgrounds from scratch. It's a relatively simple computer vision problem to take an image like the 4Chan CAPTCHA, and find all of the large <a href="https://docs.opencv.org/3.4/d4/d73/tutorial_py_contours_begin.html">contours</a> in the image, which represent the characters, and remove them. This leaves only the noisy background, as seen in the image above, which was generated using this algorithm.</p>
<p>Next, we need to isolate a decent number of characters, and label them with their values. If this was trivial to do with an algorithmic script, well, we wouldn't be here, because solving the CAPTCHA would also be trivial to do with an algorithmic script :) It's pretty easy to do this by hand, though, and that's what I settled on doing. It was annoying. I tagged the characters with <a href="https://github.com/microsoft/VoTT">VoTT</a> and then extracted them with a quick and dirty script, which also postprocessed them to make sure it was only the characters in the images. I ended up with 50-150 isolated images of each character. It was during this stage of the project that I realized the 4Chan CAPTCHA incoudes only the characters 0, 2, 4, A, D, G, H, K, M, N, P, S, X, and Y - likely done to avoid ambiguity.</p>
<p>Now we just have to put it all together. When extracting the digits, I observed a few patterns in how the characters are usually clustered or spread apart, and so I wrote my script to assemble images with backgrounds according to these formulas. And, of course, since the input character images are labelled, I can easily label the generated synthetic CAPTCHAs with their solutions.</p>
<h2 id="creating-the-model">Creating the Model</h2>
<p>Now we've got the data, it's time to train the model. I assembled a model architecture based on some research, after reading several different articles on CAPTCHA solving using neural networks.
I settled on an <abbr title="Long Short-Term Memory">LSTM</abbr> <abbr title="Convolutional Neural Network">CNN</abbr> architecture with 3 convolutional/max-pooling layers and 2 LSTM layers.
A fourth convolutional layer was also tested, but it did not improve performance.
CTC encoding of the CAPTCHA text was used, because the output was of variable length (either 5 or 6 characters).
I built the model using <a href="https://keras.io/">Keras</a> on top of <a href="https://www.tensorflow.org/">TensorFlow</a>.</p>
<h3 id="processing-the-data">Processing the data</h3>
<p>The input to the model was a mix of pre-aligned slider CAPTCHAs, normal CAPTCHAs, and synthetic CAPTCHAs.
The training script took care of ensuring they were 300x80 pixels and converted to pure black-and-white.</p>
<h4 id="always-read-the-docs">Always read the docs</h4>
<p><em>The arguments might not be in the order you expect.</em></p>
<p>One of the important steps in my data processing pipeline was making sure that all the CAPTCHA images are exactly 300x80 pixels.
Some images from the dataset, namely the older aligned "slider" CAPTCHAs, don't match this resolution / aspect ratio.
I could just fix the training data, but it's better in the end to make the training script able to cope with any data I throw at it.</p>
<p>I used <code>tf.image.resize()</code> for this. <a href="https://www.tensorflow.org/api_docs/python/tf/image/resize">The docs</a> on this are pretty simple,
for my use case I just need to pass the input image tensor, and the size, which is probably just a tuple of <code>(width, height)</code>, right?
Well, I made that assumption, and the code ran fine, so I didn't really give it a second thought.</p>
<p>Until... My model's performance was absolutely abysmal! Even after training for 32+ epochs,
the model barely performed at all on images it had seen before, and it really couldn't make anything at all of brand new CAPTCHA images,
yielding seemingly random predictions. What the heck was going on?</p>
<p>I decided to actually visualize the images I was feeding into the model, and see what they looked like
- maybe my black/white thresholding was going wrong?
I took a random image from the input data after processing, and visualized it, and I got... this:</p>
<figure><img alt="4Chan CAPTCHA that is vertically stretched to 80x300 pixels and completely unreadable" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-sideways.png&amp;w=640&amp;q=75 640w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-sideways.png&amp;w=750&amp;q=75 750w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-sideways.png&amp;w=828&amp;q=75 828w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-sideways.png&amp;w=1080&amp;q=75 1080w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-sideways.png&amp;w=1200&amp;q=75 1200w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-sideways.png&amp;w=1920&amp;q=75 1920w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-sideways.png&amp;w=2048&amp;q=75 2048w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-sideways.png&amp;w=3840&amp;q=75 3840w" src="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-sideways.png&amp;w=3840&amp;q=75"><figcaption>4Chan CAPTCHA that is vertically stretched to 80x300 pixels and completely unreadable</figcaption></figure>
<p>Yeah, you probably saw that coming as soon as I said "probably just a tuple of <code>(width, height)</code>". Turns out it's not. It is, in fact, a tuple of <code>(height, width)</code>! Had I taken the time to read the whole documentation page, I would have found this near the bottom of the page, where it provides more detail on the expected argument. This is definitely a lesson learned - read the documentation thoroughly when working with libraries you're unfamiliar with, even if you think you know how it works, and especially if things aren't working how you'd expect.</p>
<p>After fixing this bug, the training performance looked a lot more promising.</p>
<h3 id="training-the-model">Training the model</h3>
<p>The final dataset consisted of approximately 500 hand-solved images, and approximately 50,000 synthetically-generated images.
The synthetic images were generated based on random samples from approximately 2,500 background images and 50-150 images of each character.
This dataset was randomly shuffled, and then segmented 90/10 into training and evaluation sets.
Training took approximately 45 seconds per epoch on my NVIDIA RTX A4000 Laptop GPU.</p>
<p>At the end of the first epoch, the loss did not look very promising - it's still all the way up at 19. During the evaluation callback phase, predictions were nowhere near correct, yielding only 1-2 predicted characters that didn't match any of the characters in the image. This is to be expected during the early stages of training.</p>
<p>Later epochs greatly improved performance. By the end of the fourth epoch, loss decreased to 0.55, and the predictions were already looking good, with 5/5 of the random test predictions at this stage yielding correct results. Loss steadily decreased throughout the rest of the training epochs.</p>
<p>After experimenting with different numbers of epochs, 8-16 epochs was found to be a good trade-off between time and final model performance.
Loss stabilized by the 8th epoch, and increasing the epoch count beyond 16 yielded greatly diminishing returns.</p>
<figure><img alt="Graph of model loss versus epochs, showing initial large decrease followed by steady decline and levelling out" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-loss.png&amp;w=640&amp;q=75 640w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-loss.png&amp;w=750&amp;q=75 750w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-loss.png&amp;w=828&amp;q=75 828w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-loss.png&amp;w=1080&amp;q=75 1080w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-loss.png&amp;w=1200&amp;q=75 1200w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-loss.png&amp;w=1920&amp;q=75 1920w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-loss.png&amp;w=2048&amp;q=75 2048w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-loss.png&amp;w=3840&amp;q=75 3840w" src="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2F4chan-captcha-loss.png&amp;w=3840&amp;q=75"><figcaption>Graph of model loss versus epochs, showing initial large decrease followed by steady decline and levelling out</figcaption></figure>
<p>I wrote a quick test script (<code>trainer/infer.py</code>) to infer CAPTCHA solutions in Python. Results were promising on images the model had not seen before, yielding correct solutions in the limited number of test cases I tried.</p>
<h2 id="using-the-model-in-tensorflowjs">Using the Model in TensorFlow.js</h2>
<p>Writing the <a href="https://www.tensorflow.org/js">TensorFlow.js</a> code for the user script was quite straightforward. I chose TypeScript for this task. I re-implemented the CAPTCHA alignment algorithm from the Python code, as well as the image preprocessing code. All of this code is located in the <code>user-scripts/</code> directory in the repository.</p>
<p>The Python TensorFlow/Keras model formats aren't compatible with the model format expected by TensorFlow.js. There's an <a href="https://www.tensorflow.org/js/guide/conversion">official conversion script</a>, with instructions on how to use it. This should be easy, right?</p>
<h3 id="the-converter-doesnt-work-on-python-312">The converter doesn't work on Python 3.12</h3>
<p>This was a pretty simple problem that took awhile to figure out. The official TensorFlow-to-TFJS model converter doesn't work on Python 3.12. This doesn't seem to really be documented, and the error messages thrown when you try to use it on Python 3.12 are non-obvious. I tried an older version of Python (3.10) on a hunch, using PyEnv, and it worked like a charm.</p>
<h3 id="tensorflowjs-doesnt-support-keras-3">TensorFlow.js doesn't support Keras 3</h3>
<p>New problem: The conversion script supports converting Keras 3 models to TensorFlow.js format. The only problem? TensorFlow.js doesn't support actually reading those converted models. I found this out from a <a href="https://discuss.ai.google.dev/t/corrupted-configuration-and-batch-input-shape-loading-pre-trained-layers-model-in-tensorflow-js/24454">forum post</a>, after I spent a bit of time puzzling out why TFJS wouldn't read the model that the official conversion script output.</p>
<p>Luckily, the solution was easy: Use Keras 2. We can do this by training the model with the environment variable <code>TF_USE_LEGACY_KERAS=1</code> set, after installing the legacy <code>tf_keras</code> package. This may require some code changes. In my case, I only had to trivially modify one line. We also have to export the model using the legacy <code>.h5</code> model format, and specify that as the input format when running the conversion script.</p>
<h2 id="real-world-performance">Real-World Performance</h2>
<p>We've seen the performance on the training dataset, which consists mainly of synthetic images. But it doesn't matter if it can solve synthetic CAPTCHAs - we care about solving the real ones.</p>
<p>Good news: It works great on the real 4Chan CAPTCHA. Solving is fast, taking about 1 second to load the model the first time, and then being imperceptibly quick on subsequent executions. In my experience over hundreds of real CAPTCHAs solved in the browser, the model exhibits a greater than 90% successful solve rate. It rarely gets characters wrong - when it is innacurate, it typically omits a single character entirely. I believe this could be improved with greater training on actual data, or possibly tweaking the CAPTCHA layouts in the synthetic dataset generator.</p>
<figure><img alt="Animation of requesting a CAPTCHA in the 4Chan post form, and it being automatically solved by the user script." loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2Fdemo.gif&amp;w=640&amp;q=75 640w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2Fdemo.gif&amp;w=750&amp;q=75 750w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2Fdemo.gif&amp;w=828&amp;q=75 828w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2Fdemo.gif&amp;w=1080&amp;q=75 1080w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2Fdemo.gif&amp;w=1200&amp;q=75 1200w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2Fdemo.gif&amp;w=1920&amp;q=75 1920w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2Fdemo.gif&amp;w=2048&amp;q=75 2048w, https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2Fdemo.gif&amp;w=3840&amp;q=75 3840w" src="https://www.nullpt.rs/_next/image?url=%2Fposts%2Fbreaking-the-4chan-captcha%2Fdemo.gif&amp;w=3840&amp;q=75"><figcaption>Animation of requesting a CAPTCHA in the 4Chan post form, and it being automatically solved by the user script.</figcaption></figure>
<p>Small fun fact: This model has far better accuracy than the human-powered CAPTCHA solving service I described above.</p>
<h3 id="4-character-captchas">4-character CAPTCHAs</h3>
<p>While I was writing and editing this article after completing the project,
I noticed that 4Chan begun sometimes serving CAPTCHAs with only 4 characters, rather than the usual 5 and 6-character CAPTCHAs.
Despite this model only having been trained on 5 and 6-character CAPTCHAs,
performance on the 4-character CAPTCHAs is the same as for the 5 and 6-character CAPTCHAs.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I enjoyed this project a lot. It had a few challenges to overcome, and I learned a ton about machine learning and computer vision in the process. There are surely improvements that can be made, but for now, I'm pleased with the results, because I achieved what I set out to do from the start.</p>
<p>I hope you enjoyed reading this write-up as much as I enjoyed writing it, and I hope you learned something too!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What does this button do? – My new car has a mysterious and undocumented switch (244 pts)]]></title>
            <link>https://blog.koenvh.nl/what-does-this-button-do-cm42u2oi7000a09l42f54g2pr</link>
            <guid>42276620</guid>
            <pubDate>Fri, 29 Nov 2024 19:59:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.koenvh.nl/what-does-this-button-do-cm42u2oi7000a09l42f54g2pr">https://blog.koenvh.nl/what-does-this-button-do-cm42u2oi7000a09l42f54g2pr</a>, See on <a href="https://news.ycombinator.com/item?id=42276620">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><h2>My new car has a mysterious and undocumented switch</h2></p></div><div id="post-content-parent"><p>Last week I bought a car. After twelve years of service, my old trusty Peugeot 107 in blue has had its best. Expensive repairs were coming <em>at some point</em>, and I did not feel like waiting around for them to come. Plus the existing list of faults (like the high oil usage of about a litre per month, a brake that sometimes blocked without reason, or the smell of exhaust fumes that sometimes came into the car when the fan was on high) also started getting longer and longer.</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1732888980123/91643929-8f55-468a-8568-97d1a64862da.jpeg?auto=compress,format&amp;format=webp" alt=""></p>
<p>Anyway, new car time! After a lot of research I ended up with an Opel Corsa from 2020. To be precise, it’s an Opel Corsa Edition with 101 HP, and most importantly, it’s mine.</p>
<p>Unlike the Peugeot, the Opel has gadgets - quite a few of them. Of course it being my car, I want to know what all buttons do, so I read the entire manual (which is very annoying to read, as they make one manual for every version of the car, so half of it does not apply to this car, but I digress). One of those buttons was the following below the lighting controls.</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1732889068247/f39bda1d-77c1-4186-862b-c566cce517bb.jpeg?auto=compress,format&amp;format=webp" alt=""></p>
<p>Those do not appear in the manual, or the website, or anywhere. What could they be? Just flipping the switch does nothing, apart from turning off the light on the switch. So let’s look where the button goes. I can see that part of it is wired to the back of the OBD2 port (a port that retrieves data from the onboard computer about the car, such as pedal position, temperature, lights, rev count, speed - basically if you can see it on your dashboard it can be read using the OBD2 port), so it is getting information from the car, but apart from that the wires go to places that I can’t see without taking the car further apart.</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1732889143592/00372c30-63b8-4807-9ae3-4b15727304e5.jpeg?auto=compress,format&amp;format=webp" alt=""></p>
<p>Something else that I noticed before is that I heard the typical inference noise coming from that area of the car when putting the ignition on. You know, <a target="_blank" href="https://youtu.be/FYjs7vsaSEw">this</a> sound. <em>Ti-ti ta, ti-ti ta, ti-ti ta, ti-ti ta, ti-ti ta, ti-ti ta, ti-ti ta, TAAAAAAAAAAA</em>. That gave me the ominous feeling there might be something sending data in there. Sure, my car does that too so I can see in the app where I parked it (seriously, it does that), but at least I gave permission for that.</p>
<p>I asked the wisdom of the crowd. A lot of ideas came up: nitrous (would have been fun), LPG switch (it’s only petrol), flame kit (I wish), front parking sensors (those are standard on the car). No avail.</p>
<p>I called my dealership and asked. They guessed it might have been an immobiliser - bit strange on a car this young and type, and also strange for it to be a switch like this. I called the dealership that previously maintained the vehicle based on the phone number in the service history. They did not install it, and guessed it might be a black box. They did tell me who it previously belonged to (a large company), so I called their headquarters. They told me they don’t do their own cars any more, but that they outsourced it, and gave me a phone number. I called them, they told me they don’t do that, but he speculated it might be a GPS tracker.</p>
<p>Some more searching, and I figured I would just drive to my dealership. More speculating with the salespeople, who told me to make an appointment with the mechanics. I did, they had a quick look at it and I now have an answer:</p>
<p>The metal part is something to hold a magnet next to. It registers who's driving to a fleet tracker via a device also mounted in the car, which sends it to a fleet manager via the internet. That way it can be tracked which employee did what (and potentially who to send the fine to). That would also explain the mobile phone interference noises I've heard from that area of the car. So it’s a black box, a GPS tracker, and maybe also an immobiliser? I am not sure about the last one (and why it has a switch).</p>
<p>I'm getting it removed because now I'm basically driving around with a foreign GPS tracker. Some lease company somewhere is getting data on wherever I go. Kind of spooky if you think of it, especially as I assume I am one of the few actually looking into what this is. Most people would have probably driven around for years with a foreign GPS tracker.</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1732889803408/1353bcc6-c25c-482b-9a75-67686582c479.jpeg?auto=compress,format&amp;format=webp" alt=""></p>
<p>And that’s how the search comes to an end. After a bit of perseverance I figured out what it is. I now know my car is being tracked still, and that they know I did try out what the car’s acceleration is like at full throttle.</p>
<p>There are more interesting angles to this, like “can I request my data from the fleet manager thing that has been tracking my whereabouts under the GDPR?”, and “can I get free data from the SIM card embedded in the device that I now technically own?” but I will leave those for another day.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US house prices in 1950 vs. 2024, accounting for inflation (123 pts)]]></title>
            <link>https://brilliantmaps.com/us-houses-prices-1950-2024/</link>
            <guid>42276155</guid>
            <pubDate>Fri, 29 Nov 2024 19:03:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brilliantmaps.com/us-houses-prices-1950-2024/">https://brilliantmaps.com/us-houses-prices-1950-2024/</a>, See on <a href="https://news.ycombinator.com/item?id=42276155">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
			<p><img decoding="async" src="https://brilliantmaps.com/wp-content/uploads/1950-vs-2024.png" alt="How Much US Houses Cost In 1950 vs How Much The Cost In 2024" width="1640" height="647" srcset="https://brilliantmaps.com/wp-content/uploads/1950-vs-2024.png 1640w, https://brilliantmaps.com/wp-content/uploads/1950-vs-2024-300x118.png 300w, https://brilliantmaps.com/wp-content/uploads/1950-vs-2024-1024x404.png 1024w, https://brilliantmaps.com/wp-content/uploads/1950-vs-2024-768x303.png 768w, https://brilliantmaps.com/wp-content/uploads/1950-vs-2024-1536x606.png 1536w" sizes="(max-width: 1640px) 100vw, 1640px"></p>
<p>The map above shows how much houses cost in 1950 in each US state in 2024 inflation adjusted US dollars, compared to what the average cost actually is in 2024.  </p>
<p>In every single US state the increase was at at least double the rate of inflation from a low in Ohio of just 107% above inflation to a high in <a href="https://brilliantmaps.com/us-maps/alaska-map/" data-internallinksmanager029f6b8e52c="99" title="Map of Alaska The Government Doesn’t Want You To See">Alaska</a> of 675% above inflation. </p>
<p>The following map shows the percentage increase above inflation:</p>

<p><img decoding="async" src="https://brilliantmaps.com/wp-content/uploads/median-home-value-increase-1950-2024-above-inflation.png" alt="House price increases above inflation 1950-2024" width="1640" height="1294" srcset="https://brilliantmaps.com/wp-content/uploads/median-home-value-increase-1950-2024-above-inflation.png 1640w, https://brilliantmaps.com/wp-content/uploads/median-home-value-increase-1950-2024-above-inflation-300x237.png 300w, https://brilliantmaps.com/wp-content/uploads/median-home-value-increase-1950-2024-above-inflation-1024x808.png 1024w, https://brilliantmaps.com/wp-content/uploads/median-home-value-increase-1950-2024-above-inflation-768x606.png 768w, https://brilliantmaps.com/wp-content/uploads/median-home-value-increase-1950-2024-above-inflation-1536x1212.png 1536w" sizes="(max-width: 1640px) 100vw, 1640px"></p>
<p>And here are 3 maps showing Median Home Value in 1950 (in non-inflation adjusted terms), and bigger versions of the two maps at the top.</p>
<p>Finally, you can see all the data at the bottom:</p>
<h2>Median home value by state in 1950 in non-inflation adjusted dollars</h2>
<p><img loading="lazy" decoding="async" src="https://brilliantmaps.com/wp-content/uploads/median-home-value-by-state-in-1950-in-non-inflation-adjusted-dollars.png" alt="median home value by state in 1950 in non-inflation adjusted dollars" width="1640" height="1294" srcset="https://brilliantmaps.com/wp-content/uploads/median-home-value-by-state-in-1950-in-non-inflation-adjusted-dollars.png 1640w, https://brilliantmaps.com/wp-content/uploads/median-home-value-by-state-in-1950-in-non-inflation-adjusted-dollars-300x237.png 300w, https://brilliantmaps.com/wp-content/uploads/median-home-value-by-state-in-1950-in-non-inflation-adjusted-dollars-1024x808.png 1024w, https://brilliantmaps.com/wp-content/uploads/median-home-value-by-state-in-1950-in-non-inflation-adjusted-dollars-768x606.png 768w, https://brilliantmaps.com/wp-content/uploads/median-home-value-by-state-in-1950-in-non-inflation-adjusted-dollars-1536x1212.png 1536w" sizes="auto, (max-width: 1640px) 100vw, 1640px"></p>
<h2>Median home value by state in 1950 in inflation adjusted dollars</h2>
<p><img loading="lazy" decoding="async" src="https://brilliantmaps.com/wp-content/uploads/median-home-value-by-state-in-1950-in-inflation-adjusted-dollars.png" alt="Median home value by state in 1950 in inflation adjusted dollars" width="1640" height="1294" srcset="https://brilliantmaps.com/wp-content/uploads/median-home-value-by-state-in-1950-in-inflation-adjusted-dollars.png 1640w, https://brilliantmaps.com/wp-content/uploads/median-home-value-by-state-in-1950-in-inflation-adjusted-dollars-300x237.png 300w, https://brilliantmaps.com/wp-content/uploads/median-home-value-by-state-in-1950-in-inflation-adjusted-dollars-1024x808.png 1024w, https://brilliantmaps.com/wp-content/uploads/median-home-value-by-state-in-1950-in-inflation-adjusted-dollars-768x606.png 768w, https://brilliantmaps.com/wp-content/uploads/median-home-value-by-state-in-1950-in-inflation-adjusted-dollars-1536x1212.png 1536w" sizes="auto, (max-width: 1640px) 100vw, 1640px"></p>
<h2>Actual Median home value by state in 2024</h2>
<p><img loading="lazy" decoding="async" src="https://brilliantmaps.com/wp-content/uploads/median-home-value-by-state-in-2024.png" alt="Median home value by state in 2024" width="1640" height="1294" srcset="https://brilliantmaps.com/wp-content/uploads/median-home-value-by-state-in-2024.png 1640w, https://brilliantmaps.com/wp-content/uploads/median-home-value-by-state-in-2024-300x237.png 300w, https://brilliantmaps.com/wp-content/uploads/median-home-value-by-state-in-2024-1024x808.png 1024w, https://brilliantmaps.com/wp-content/uploads/median-home-value-by-state-in-2024-768x606.png 768w, https://brilliantmaps.com/wp-content/uploads/median-home-value-by-state-in-2024-1536x1212.png 1536w" sizes="auto, (max-width: 1640px) 100vw, 1640px"></p>
<p>And here’s all the data on the changes:</p>

<table id="tablepress-223">
<thead>
<tr>
	<th>State</th><th>Median Home Value (1950)</th><th>Inflation Adjusted Median Home Value (1950)</th><th>Median Home Value (2024)</th><th>Absolute $ Increase Above Inflation</th><th>Percent Increase Above Inflation</th>
</tr>
</thead>
<tbody>
<tr>
	<td>Alabama</td><td>$4,473</td><td>$60,072</td><td>$227,508</td><td>$167,436</td><td>279%</td>
</tr>
<tr>
	<td>Alaska</td><td>$3,477</td><td>$46,696</td><td>$362,098</td><td>$315,402</td><td>675%</td>
</tr>
<tr>
	<td>Arizona</td><td>$5,935</td><td>$79,707</td><td>$428,711</td><td>$349,004</td><td>438%</td>
</tr>
<tr>
	<td>Arkansas</td><td>$4,087</td><td>$54,888</td><td>$208,078</td><td>$153,190</td><td>279%</td>
</tr>
<tr>
	<td>California</td><td>$9,564</td><td>$128,445</td><td>$771,057</td><td>$642,612</td><td>500%</td>
</tr>
<tr>
	<td>Colorado</td><td>$7,151</td><td>$96,038</td><td>$541,072</td><td>$445,034</td><td>463%</td>
</tr>
<tr>
	<td>Connecticut</td><td>$11,862</td><td>$159,307</td><td>$405,595</td><td>$246,289</td><td>155%</td>
</tr>
<tr>
	<td>Delaware</td><td>$9,079</td><td>$121,931</td><td>$388,654</td><td>$266,723</td><td>219%</td>
</tr>
<tr>
	<td>District of Columbia</td><td>$14,498</td><td>$194,708</td><td>$602,769</td><td>$408,060</td><td>210%</td>
</tr>
<tr>
	<td>Florida</td><td>$6,612</td><td>$88,799</td><td>$392,176</td><td>$303,376</td><td>342%</td>
</tr>
<tr>
	<td>Georgia</td><td>$5,235</td><td>$70,306</td><td>$326,617</td><td>$256,311</td><td>365%</td>
</tr>
<tr>
	<td>Hawaii</td><td>$12,283</td><td>$164,961</td><td>$845,946</td><td>$680,985</td><td>413%</td>
</tr>
<tr>
	<td>Idaho</td><td>$5,852</td><td>$78,592</td><td>$451,520</td><td>$372,928</td><td>475%</td>
</tr>
<tr>
	<td>Illinois</td><td>$8,646</td><td>$116,116</td><td>$266,706</td><td>$150,590</td><td>130%</td>
</tr>
<tr>
	<td>Indiana</td><td>$6,226</td><td>$83,615</td><td>$242,113</td><td>$158,498</td><td>190%</td>
</tr>
<tr>
	<td>Iowa</td><td>$6,320</td><td>$84,878</td><td>$220,277</td><td>$135,400</td><td>160%</td>
</tr>
<tr>
	<td>Kansas</td><td>$5,462</td><td>$73,355</td><td>$229,012</td><td>$155,658</td><td>212%</td>
</tr>
<tr>
	<td>Kentucky</td><td>$5,283</td><td>$70,951</td><td>$212,088</td><td>$141,137</td><td>199%</td>
</tr>
<tr>
	<td>Louisiana</td><td>$5,141</td><td>$69,044</td><td>$201,519</td><td>$132,476</td><td>192%</td>
</tr>
<tr>
	<td>Maine</td><td>$4,856</td><td>$65,216</td><td>$401,297</td><td>$336,081</td><td>515%</td>
</tr>
<tr>
	<td>Maryland</td><td>$8,033</td><td>$107,883</td><td>$418,438</td><td>$310,555</td><td>288%</td>
</tr>
<tr>
	<td>Massachusetts</td><td>$9,144</td><td>$122,804</td><td>$623,131</td><td>$500,327</td><td>407%</td>
</tr>
<tr>
	<td>Michigan</td><td>$7,496</td><td>$100,671</td><td>$245,716</td><td>$145,044</td><td>144%</td>
</tr>
<tr>
	<td>Minnesota</td><td>$7,806</td><td>$104,835</td><td>$334,119</td><td>$229,285</td><td>219%</td>
</tr>
<tr>
	<td>Mississippi</td><td>$4,159</td><td>$55,855</td><td>$181,313</td><td>$125,457</td><td>225%</td>
</tr>
<tr>
	<td>Missouri</td><td>$6,399</td><td>$85,939</td><td>$248,328</td><td>$162,389</td><td>189%</td>
</tr>
<tr>
	<td>Montana</td><td>$5,797</td><td>$77,854</td><td>$462,631</td><td>$384,777</td><td>494%</td>
</tr>
<tr>
	<td>Nebraska</td><td>$5,918</td><td>$79,479</td><td>$259,443</td><td>$179,964</td><td>226%</td>
</tr>
<tr>
	<td>Nevada</td><td>$8,859</td><td>$118,976</td><td>$442,185</td><td>$323,209</td><td>272%</td>
</tr>
<tr>
	<td>New Hampshire</td><td>$6,199</td><td>$83,253</td><td>$478,955</td><td>$395,703</td><td>475%</td>
</tr>
<tr>
	<td>New Jersey</td><td>$10,408</td><td>$139,779</td><td>$534,773</td><td>$394,994</td><td>283%</td>
</tr>
<tr>
	<td>New Mexico</td><td>$5,697</td><td>$76,511</td><td>$303,910</td><td>$227,399</td><td>297%</td>
</tr>
<tr>
	<td>New York</td><td>$10,152</td><td>$136,341</td><td>$482,742</td><td>$346,400</td><td>254%</td>
</tr>
<tr>
	<td>North Carolina</td><td>$4,901</td><td>$65,820</td><td>$328,715</td><td>$262,895</td><td>399%</td>
</tr>
<tr>
	<td>North Dakota</td><td>$5,396</td><td>$72,468</td><td>$263,410</td><td>$190,942</td><td>263%</td>
</tr>
<tr>
	<td>Ohio</td><td>$8,304</td><td>$111,523</td><td>$230,798</td><td>$119,275</td><td>107%</td>
</tr>
<tr>
	<td>Oklahoma</td><td>$5,228</td><td>$70,212</td><td>$205,968</td><td>$135,756</td><td>193%</td>
</tr>
<tr>
	<td>Oregon</td><td>$6,846</td><td>$91,942</td><td>$492,683</td><td>$400,742</td><td>436%</td>
</tr>
<tr>
	<td>Pennsylvania</td><td>$6,992</td><td>$93,903</td><td>$268,824</td><td>$174,921</td><td>186%</td>
</tr>
<tr>
	<td>Rhode Island</td><td>$9,767</td><td>$131,171</td><td>$467,485</td><td>$336,314</td><td>256%</td>
</tr>
<tr>
	<td>South Carolina</td><td>$5,112</td><td>$68,654</td><td>$295,769</td><td>$227,115</td><td>331%</td>
</tr>
<tr>
	<td>South Dakota</td><td>$5,410</td><td>$72,656</td><td>$306,944</td><td>$234,287</td><td>322%</td>
</tr>
<tr>
	<td>Tennessee</td><td>$5,268</td><td>$70,749</td><td>$319,208</td><td>$248,458</td><td>351%</td>
</tr>
<tr>
	<td>Texas</td><td>$5,805</td><td>$77,961</td><td>$300,267</td><td>$222,306</td><td>285%</td>
</tr>
<tr>
	<td>Utah</td><td>$7,409</td><td>$99,503</td><td>$517,020</td><td>$417,517</td><td>420%</td>
</tr>
<tr>
	<td>Vermont</td><td>$6,277</td><td>$84,300</td><td>$390,132</td><td>$305,832</td><td>363%</td>
</tr>
<tr>
	<td>Virginia</td><td>$6,581</td><td>$88,383</td><td>$392,682</td><td>$304,299</td><td>344%</td>
</tr>
<tr>
	<td>Washington</td><td>$7,169</td><td>$96,280</td><td>$588,856</td><td>$492,576</td><td>512%</td>
</tr>
<tr>
	<td>West Virginia</td><td>$5,473</td><td>$73,502</td><td>$168,172</td><td>$94,670</td><td>129%</td>
</tr>
<tr>
	<td>Wisconsin</td><td>$7,927</td><td>$106,460</td><td>$306,566</td><td>$200,106</td><td>188%</td>
</tr>
<tr>
	<td>Wyoming</td><td>$6,811</td><td>$91,472</td><td>$354,108</td><td>$262,636</td><td>287%</td>
</tr>
</tbody>
</table>
<!-- #tablepress-223 from cache -->
<p>Data for 1950 US house prices came from the <a href="https://www.census.gov/data/tables/time-series/dec/coh-values.html" target="_blank" rel="noopener">US Census</a>, inflation data for the <a href="https://data.bls.gov/cgi-bin/cpicalc.pl" target="_blank" rel="noopener">Bureau of Labour Statistics</a> and the 2024 house prices from <a href="https://www.zillow.com/research/data/" target="_blank" rel="noopener">Zillow</a>. Maps created using <a href="https://www.datawrapper.de/" target="_blank" rel="noopener">Datawrapper</a>.    </p>
<p>Why do you think home prices have outpaced inflation since the 1950s?</p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chinese pebble-bed nuclear reactor passes "meltdown" test (108 pts)]]></title>
            <link>https://www.ans.org/news/article-6241/china-pebblebed-reactor-passes-meltdown-test/</link>
            <guid>42275834</guid>
            <pubDate>Fri, 29 Nov 2024 18:21:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ans.org/news/article-6241/china-pebblebed-reactor-passes-meltdown-test/">https://www.ans.org/news/article-6241/china-pebblebed-reactor-passes-meltdown-test/</a>, See on <a href="https://news.ycombinator.com/item?id=42275834">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="expand_6241"><p>The <a href="https://www.tsinghua.edu.cn/en/info/1399/10915.htm">Shidaowan plant</a>, a demonstration high-temperature, gas-cooled reactor with a pebble-bed module (HTR-PM), went into commercial operation last December. </p><p>Shidaowan’s twin 100-MW units house tiny uranium capsules encased in graphite shells about the size of billiard balls (dubbed “pebbles”), which make the energy density of the fuel much lower than in a traditional nuclear reactor with fuel rods. In the pebble design, the nuclear fission reaction occurs more slowly than in conventional reactors, but the fuel can withstand higher temperatures for longer and the heat resulting from the fission reaction is dispersed, enabling a passive cooling process.</p><p>The reactor doesn’t rely on large volumes of water in the cooling process—instead, a small amount of helium gas, which can withstand much higher temperatures than water, is piped through the system to naturally cool it down. If the reactor starts to get too hot, its components automatically slow down the nuclear reaction and the system cools. This setup makes such a reactor “meltdown proof,” in concept.</p><p><strong>The study:</strong> Researchers at Tsinghua University in China performed two safety tests on the Shidaowan plant’s reactor modules by shutting off active power supply to see if the decay heat could be removed passively. The responses of temperatures and nuclear power in each unit confirmed that they can be cooled down naturally, without active intervention.</p><p>“The results of the tests manifest the existence of commercial-scale inherent safety for the first time,” according to findings published in the journal <em><a href="https://www.cell.com/joule/abstract/S2542-4351(24)00290-3?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS2542435124002903%3Fshowall%3Dtrue">Joule</a></em>.</p><p>The Shidaowan project is a collaboration involving Tsinghua University as a technical leader, responsible for research and development and main components and systems design; China Huaneng Group as owner and operator of the plant; and China National Nuclear Corporation as overseer of engineering and procurement and fuel manufacturing.</p><p><strong>Other pebble beds:</strong> The pebble bed technology and design has previously been used in prototype reactors in China and Germany, but not a larger-scale plant like Shidaowan.</p><p>In the U.S., X-energy is working to deploy its pebble-fueled <a href="https://x-energy.com/reactors/xe-100">Xe-100</a> design—an 80-MWe high-temperature, gas-cooled reactor that can be scaled into a four-pack to make a 320-MWe power plant. X-energy’s license request is <a href="https://www.nrc.gov/reactors/new-reactors/advanced/who-were-working-with/licensing-activities/pre-application-activities/xe-100.html">under review</a> by the Nuclear Regulatory Commission.</p><p>X-energy was <a href="https://x-energy.com/media/news-releases/x-energy-awarded-80-million-department-of-energy-advanced-reactor-demonstration-program-ardp">one of two chosen</a> for the Department of Energy’s Advanced Reactor Demonstration Program in 2020. With $160 million in initial funding, ARDP set out to help bring two advanced reactors to commercial operation within seven years. (TerraPower’s Natrium reactor is the other.)</p><p>X-energy has a joint development agreement with Dow to develop its first Xe-100 plant at the chemical company’s plant in Seadrift, Texas. X-energy is also working with Energy Northwest under a joint development agreement to bring up to 12 of the Xe-100 small modular reactors to Washington state. That project is expected to be developed at a site adjacent to Energy Northwest’s Columbia nuclear power plant.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Buy Nothing Day (154 pts)]]></title>
            <link>https://buynothingday.co.uk/</link>
            <guid>42275745</guid>
            <pubDate>Fri, 29 Nov 2024 18:10:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://buynothingday.co.uk/">https://buynothingday.co.uk/</a>, See on <a href="https://news.ycombinator.com/item?id=42275745">Hacker News</a></p>
Couldn't get https://buynothingday.co.uk/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Why pipes sometimes get "stuck": buffering (275 pts)]]></title>
            <link>https://jvns.ca/blog/2024/11/29/why-pipes-get-stuck-buffering/</link>
            <guid>42275033</guid>
            <pubDate>Fri, 29 Nov 2024 16:43:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jvns.ca/blog/2024/11/29/why-pipes-get-stuck-buffering/">https://jvns.ca/blog/2024/11/29/why-pipes-get-stuck-buffering/</a>, See on <a href="https://news.ycombinator.com/item?id=42275033">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
     <p>Here’s a niche terminal problem that has bothered me for years but that I never
really understood until a few weeks ago. Let’s say you’re running this command
to watch for some specific output in a log file:</p>
<pre><code>tail -f /some/log/file | grep thing1 | grep thing2
</code></pre>
<p>If log lines are being added to the file relatively slowly, the result I’d see
is… nothing! It doesn’t matter if there were matches in the log file or not,
there just wouldn’t be any output.</p>
<p>I internalized this as “uh, I guess pipes just get stuck sometimes and don’t
show me the output, that’s weird”, and I’d handle it by just
running <code>grep thing1 /some/log/file | grep thing2</code> instead, which would work.</p>
<p>So as I’ve been doing a terminal deep dive over the last few months I was
really excited to finally learn exactly why this happens.</p>
<h3 id="why-this-happens-buffering">why this happens: buffering</h3>
<p>The reason why “pipes get stuck” sometimes is that it’s VERY common for
programs to buffer their output before writing it to a pipe or file. So the
pipe is working fine, the problem is that the program never even wrote the data
to the pipe!</p>
<p>This is for performance reasons: writing all output immediately as soon as you
can uses more system calls, so it’s more efficient to save up data until you
have 8KB or so of data to write (or until the program exits) and THEN write it
to the pipe.</p>
<p>In this example:</p>
<pre><code>tail -f /some/log/file | grep thing1 | grep thing2
</code></pre>
<p>the problem is that <code>grep thing1</code> is saving up all of its matches until it has
8KB of data to write, which might literally never happen.</p>
<h3 id="programs-don-t-buffer-when-writing-to-a-terminal">programs don’t buffer when writing to a terminal</h3>
<p>Part of why I found this so disorienting is that <code>tail -f file | grep thing</code>
will work totally fine, but then when you add the second <code>grep</code>, it stops
working!! The reason for this is that the way <code>grep</code> handles buffering depends
on whether it’s writing to a terminal or not.</p>
<p>Here’s how <code>grep</code> (and many other programs) decides to buffer its output:</p>
<ul>
<li>Check if stdout is a terminal or not using the <code>isatty</code> function
<ul>
<li>If it’s a terminal, use line buffering (print every line immediately as soon as you have it)</li>
<li>Otherwise, use “block buffering” – only print data if you have at least 8KB or so of data to print</li>
</ul>
</li>
</ul>
<p>So if <code>grep</code> is writing directly to your terminal then you’ll see the line as
soon as it’s printed, but if it’s writing to a pipe, you won’t.</p>
<p>Of course the buffer size isn’t always 8KB for every program, it depends on the implementation. For <code>grep</code> the buffering is handled by libc, and libc’s buffer size is
defined in the <code>BUFSIZ</code> variable. <a href="https://github.com/bminor/glibc/blob/c69e8cccaff8f2d89cee43202623b33e6ef5d24a/libio/stdio.h#L100">Here’s where that’s defined in glibc</a>.</p>
<p>(as an aside: “programs do not use 8KB output buffers when writing to a
terminal” isn’t, like, a law of terminal physics, a program COULD use an 8KB
buffer when writing output to a terminal if it wanted, it would just be
extremely weird if it did that, I can’t think of any program that behaves that
way)</p>
<h3 id="commands-that-buffer-commands-that-don-t">commands that buffer &amp; commands that don’t</h3>
<p>One annoying thing about this buffering behaviour is that you kind of need to
remember which commands buffer their output when writing to a pipe.</p>
<p>Some commands that <strong>don’t</strong> buffer their output:</p>
<ul>
<li>tail</li>
<li>cat</li>
<li>tee</li>
</ul>
<p>I think almost everything else will buffer output, especially if it’s a command
where you’re likely to be using it for batch processing. Here’s a list of some
common commands that buffer their output when writing to a pipe, along with the
flag that disables block buffering.</p>
<ul>
<li>grep (<code>--line-buffered</code>)</li>
<li>sed (<code>-u</code>)</li>
<li>awk (there’s a <code>fflush()</code> function)</li>
<li>tcpdump (<code>-l</code>)</li>
<li>jq (<code>-u</code>)</li>
<li>tr (<code>-u</code>)</li>
<li>cut (can’t disable buffering)</li>
</ul>
<p>Those are all the ones I can think of, lots of unix commands (like <code>sort</code>) may
or may not buffer their output but it doesn’t matter because <code>sort</code> can’t do
anything until it finishes receiving input anyway.</p>
<p>Also I did my best to test both the Mac OS and GNU versions of these but there
are a lot of variations and I might have made some mistakes.</p>
<h3 id="programming-languages-where-the-default-print-statement-buffers">programming languages where the default “print” statement buffers</h3>
<p>Also, here are a few programming language where the default print statement
will buffer output when writing to a pipe, and some ways to disable buffering
if you want:</p>
<ul>
<li>C (disable with <code>setvbuf</code>)</li>
<li>Python (disable with <code>python -u</code>, or <code>PYTHON_UNBUFFERED=1</code>, or <code>sys.stdout.reconfigure(line_buffering=False)</code>, or <code>print(x, flush=True)</code>)</li>
<li>Ruby (disable with <code>STDOUT.sync = true</code>)</li>
<li>Perl (disable with <code>$| = 1</code>)</li>
</ul>
<p>I assume that these languages are designed this way so that the default print
function will be fast when you’re doing batch processing.</p>
<p>Also whether output is buffered or not might depend on what print function you
use, for example in Rust <code>print!</code> buffers when writing to a pipe but <code>println!</code>
will flush its output.</p>
<h3 id="when-you-press-ctrl-c-on-a-pipe-the-contents-of-the-buffer-are-lost">when you press <code>Ctrl-C</code> on a pipe, the contents of the buffer are lost</h3>
<p>Let’s say you’re running this command as a hacky way to watch for DNS requests
to <code>example.com</code>, and you forgot to pass <code>-l</code> to tcpdump:</p>
<pre><code>sudo tcpdump -ni any port 53 | grep example.com
</code></pre>
<p>When you press <code>Ctrl-C</code>, what happens? In a magical perfect world, what I would
<em>want</em> to happen is for <code>tcpdump</code> to flush its buffer, <code>grep</code> would search for
<code>example.com</code>, and I would see all the output I missed.</p>
<p>But in the real world, what happens is that all the programs get killed and the
output in <code>tcpdump</code>’s buffer is lost.</p>
<p>I think this problem is probably unavoidable – I spent a little time with
<code>strace</code> to see how this works and <code>grep</code> receives the <code>SIGINT</code> before
<code>tcpdump</code> anyway so even if <code>tcpdump</code> tried to flush its buffer <code>grep</code> would
already be dead.</p>
<h3 id="redirecting-to-a-file-also-buffers">redirecting to a file also buffers</h3>
<p>It’s not just pipes, this will also buffer:</p>
<pre><code>sudo tcpdump -ni any port 53 &gt; output.txt
</code></pre>
<p>Redirecting to a file doesn’t have the same “<code>Ctrl-C</code> will totally destroy the
contents of the buffer” problem though – in my experience it usually behaves
more like you’d want, where the contents of the buffer get written to the file
before the program exits. I’m not 100% sure whether this is something you can
always rely on or not.</p>
<h3 id="a-bunch-of-potential-ways-to-avoid-buffering">a bunch of potential ways to avoid buffering</h3>
<p>Okay, let’s talk solutions. Let’s say you’ve run this command or s</p>
<pre><code>tail -f /some/log/file | grep thing1 | grep thing2
</code></pre>
<p>I asked people on Mastodon how they would solve this in practice and there were
5 basic approaches. Here they are:</p>
<h4 id="solution-1-run-a-program-that-finishes-quickly">solution 1: run a program that finishes quickly</h4>
<p>Historically my solution to this has been to just avoid the “command writing to
pipe slowly” situation completely and instead run a program that will finish quickly
like this:</p>
<pre><code>cat /some/log/file | grep thing1 | grep thing2 | tail
</code></pre>
<p>This doesn’t do the same thing as the original command but it does mean that
you get to avoid thinking about these weird buffering issues.</p>
<p>(you could also do <code>grep thing1 /some/log/file</code> but I often prefer to use an
“unnecessary” <code>cat</code>)</p>
<h4 id="solution-2-remember-the-line-buffer-flag-to-grep">solution 2: remember the “line buffer” flag to grep</h4>
<p>You could remember that grep has a flag to avoid buffering and pass it like this:</p>
<pre><code>tail -f /some/log/file | grep --line-buffered thing1 | grep thing2
</code></pre>
<h4 id="solution-3-use-awk">solution 3: use awk</h4>
<p>Some people said that if they’re specifically dealing with a multiple greps
situation, they’ll rewrite it to use a single <code>awk</code> instead, like this:</p>
<pre><code>tail -f /some/log/file |  awk '/thing1/ &amp;&amp; /thing2/'
</code></pre>
<p>Or you would write a more complicated <code>grep</code>, like this:</p>
<pre><code>tail -f /some/log/file |  grep -E 'thing1.*thing2'
</code></pre>
<p>(<code>awk</code> also buffers, so for this to work you’ll want <code>awk</code> to be the last command in the pipeline)</p>
<h4 id="solution-4-use-stdbuf">solution 4: use <code>stdbuf</code></h4>
<p><code>stdbuf</code> uses LD_PRELOAD to turn off libc’s buffering, and you can use it to turn off output buffering like this:</p>
<pre><code>tail -f /some/log/file | stdbuf -o0 grep thing1 | grep thing2
</code></pre>
<p>Like any <code>LD_PRELOAD</code> solution it’s a bit unreliable – it doesn’t work on
static binaries, I think won’t work if the program isn’t using libc’s
buffering, and doesn’t always work on Mac OS. Harry Marr has a really nice <a href="https://hmarr.com/blog/how-stdbuf-works/">How stdbuf works</a> post.</p>
<h4 id="solution-5-use-unbuffer">solution 5: use <code>unbuffer</code></h4>
<p><code>unbuffer program</code> will force the program’s output to be a TTY, which means
that it’ll behave the way it normally would on a TTY (less buffering, colour
output, etc). You could use it in this example like this:</p>
<pre><code>tail -f /some/log/file | unbuffer grep thing1 | grep thing2
</code></pre>
<p>Unlike <code>stdbuf</code> it will always work, though it might have unwanted side
effects, for example <code>grep thing1</code>’s will also colour matches.</p>
<p>If you want to install unbuffer, it’s in the <code>expect</code> package.</p>
<h3 id="that-s-all-the-solutions-i-know-about">that’s all the solutions I know about!</h3>
<p>It’s a bit hard for me to say which one is “best”, I think personally I’m
mostly likely to use <code>unbuffer</code> because I know it’s always going to work.</p>
<p>If I learn about more solutions I’ll try to add them to this post.</p>
<h3 id="i-m-not-really-sure-how-often-this-comes-up">I’m not really sure how often this comes up</h3>
<p>I think it’s not very common for me to have a program that slowly trickles data
into a pipe like this, normally if I’m using a pipe a bunch of data gets
written very quickly, processed by everything in the pipeline, and then
everything exits. The only examples I can come up with right now are:</p>
<ul>
<li>tcpdump</li>
<li><code>tail -f</code></li>
<li>watching log files in a different way like with <code>kubectl logs</code></li>
<li>the output of a slow computation</li>
</ul>
<h3 id="what-if-there-were-an-environment-variable-to-disable-buffering">what if there were an environment variable to disable buffering?</h3>
<p>I think it would be cool if there were a standard environment variable to turn
off buffering, like <code>PYTHON_UNBUFFERED</code> in Python. I got this idea from a
<a href="https://blog.plover.com/Unix/stdio-buffering.html">couple</a> of <a href="https://blog.plover.com/Unix/stdio-buffering-2.html">blog posts</a> by Mark Dominus
in 2018. Maybe <code>NO_BUFFER</code> like <a href="https://no-color.org/">NO_COLOR</a>?</p>
<p>The design seems tricky to get right; Mark points out that NETBSD has <a href="https://man.netbsd.org/setbuf.3">environment variables called <code>STDBUF</code>, <code>STDBUF1</code>, etc</a> which gives you a
ton of control over buffering but I imagine most developers don’t want to
implement many different environment variables to handle a relatively minor
edge case.</p>
<p>I’m also curious about whether there are any programs that just
automatically flush their output buffers after some period of time (like 1
second). It feels like it would be a nice solution but I can’t think of any
program that does that, maybe it’s not easy to implement?</p>
<h3 id="stuff-i-left-out">stuff I left out</h3>
<p>Some things I didn’t talk about in this post since these posts have been
getting pretty long recently and seriously does anyone REALLY want to read 3000
words about buffering?</p>
<ul>
<li>the difference between line buffering and having totally unbuffered output</li>
<li>how buffering to stderr is different from buffering to stdout</li>
<li>this post is only about buffering that happens <strong>inside the program</strong>, your
operating system’s TTY driver also does a little bit of buffering sometimes</li>
<li>other reasons you might need to flush your output other than “you’re writing
to a pipe”</li>
</ul>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Prometheus 3.0 (183 pts)]]></title>
            <link>https://prometheus.io/blog/2024/11/14/prometheus-3-0/</link>
            <guid>42274660</guid>
            <pubDate>Fri, 29 Nov 2024 15:52:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://prometheus.io/blog/2024/11/14/prometheus-3-0/">https://prometheus.io/blog/2024/11/14/prometheus-3-0/</a>, See on <a href="https://news.ycombinator.com/item?id=42274660">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      

<p>Following the recent release of <a href="https://prometheus.io/blog/2024/09/11/prometheus-3-beta/">Prometheus 3.0 beta</a> at PromCon in Berlin, the Prometheus Team
is excited to announce the immediate availability of Prometheus Version 3.0!</p>

<p>This latest version marks a significant milestone as it is the first major release in 7 years. Prometheus has come a long way in that time, 
evolving from a project for early adopters to becoming a standard part of the cloud native monitoring stack. Prometheus 3.0 aims to 
continue that journey by adding some exciting new features while largely maintaining stability and compatibility with previous versions.</p>

<p>The full 3.0 release adds some new features on top of the beta and also introduces a few additional breaking changes that we will describe in this article.</p>


<div><ul>
<li><a href="#new-ui">New UI
</a></li>
<li><a href="#remote-write-2-0">Remote Write 2.0
</a></li>
<li><a href="#utf-8-support">UTF-8 Support
</a></li>
<li><a href="#otlp-support">OTLP Support
</a></li>
<ul>
<li><a href="#otlp-ingestion">OTLP Ingestion
</a></li>
<li><a href="#utf-8-normalization">UTF-8 Normalization
</a></li>
</ul>
<li><a href="#native-histograms">Native Histograms
</a></li>
<li><a href="#breaking-changes">Breaking Changes
</a></li>
</ul></div>

<p>Here is a summary of the exciting changes that have been released as part of the beta version, as well as what has been added since:</p>

<h2 id="new-ui">New UI<a href="#new-ui" name="new-ui"></a>
</h2>

<p>One of the highlights in Prometheus 3.0 is its brand-new UI that is enabled by default:</p>

<p><img src="https://prometheus.io/assets/blog/2024-11-14/blog_post_screenshot_tree_view-s.png" alt="New UI query page"></p>

<p>The UI has been completely rewritten with less clutter, a more modern look and feel, new features like a <a href="https://promlens.com/"><strong>PromLens</strong></a>-style tree view,
and will make future maintenance easier by using a more modern technical stack.</p>

<p>Learn more about the new UI in general in <a href="https://promlabs.com/blog/2024/09/11/a-look-at-the-new-prometheus-3-0-ui/">Julius' detailed article on the PromLabs blog</a>.
Users can temporarily enable the old UI by using the <code>old-ui</code> feature flag.</p>

<p>Since the new UI is not battle-tested yet, it is also very possible that there are still bugs. If you find any, please 
<a href="https://github.com/prometheus/prometheus/issues/new?assignees=&amp;labels=&amp;projects=&amp;template=bug_report.yml">report them on GitHub</a>.</p>

<p>Since the beta, the user interface has been updated to support UTF-8 metric and label names.</p>

<p><img src="https://prometheus.io/assets/blog/2024-11-14/utf8_ui.png" alt="New UTF-8 UI"></p>

<h2 id="remote-write-2-0">Remote Write 2.0<a href="#remote-write-2-0" name="remote-write-2-0"></a>
</h2>

<p>Remote-Write 2.0 iterates on the previous protocol version by adding native support for a host of new elements including metadata, exemplars,
created timestamp and native histograms. It also uses string interning to reduce payload size and CPU usage when compressing and decompressing. 
There is better handling for partial writes to provide more details to clients when this occurs. More details can be found
<a href="https://prometheus.io/docs/specs/remote_write_spec_2_0/">here</a>.</p>

<h2 id="utf-8-support">UTF-8 Support<a href="#utf-8-support" name="utf-8-support"></a>
</h2>

<p>Prometheus now allows all valid UTF-8 characters to be used in metric and label names by default, as well as label values,
as has been true in version 2.x.</p>

<p>Users will need to make sure their metrics producers are configured to pass UTF-8 names, and if either side does not support UTF-8,
metric names will be escaped using the traditional underscore-replacement method. PromQL queries can be written with the new quoting syntax
in order to retrieve UTF-8 metrics, or users can specify the <code>__name__</code>  label name manually.</p>

<p>Currently only the Go client library has been updated to support UTF-8, but support for other languages will be added soon.</p>

<h2 id="otlp-support">OTLP Support<a href="#otlp-support" name="otlp-support"></a>
</h2>

<p>In alignment with <a href="https://prometheus.io/blog/2024/03/14/commitment-to-opentelemetry/">our commitment to OpenTelemetry</a>, Prometheus 3.0 features 
several new features to improve interoperability with OpenTelemetry. </p>

<h3 id="otlp-ingestion">OTLP Ingestion<a href="#otlp-ingestion" name="otlp-ingestion"></a>
</h3>

<p>Prometheus can be configured as a native receiver for the OTLP Metrics protocol, receiving OTLP metrics on the <code>/api/v1/otlp/v1/metrics</code> endpoint.</p>

<p>See our <a href="https://prometheus.io/docs/guides/opentelemetry">guide</a> on best practices for consuming OTLP metric traffic into Prometheus.</p>

<h3 id="utf-8-normalization">UTF-8 Normalization<a href="#utf-8-normalization" name="utf-8-normalization"></a>
</h3>

<p>With Prometheus 3.0, thanks to <a href="#utf-8-support">UTF-8 support</a>, users can store and query OpenTelemetry metrics without annoying changes to metric and label names like <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/pkg/translator/prometheus">changing dots to underscores</a>.</p>

<p>Notably this allows <strong>less confusion</strong> for users and tooling in terms of the discrepancy between what’s defined in OpenTelemetry semantic convention or SDK and what’s actually queryable.</p>

<p>To achieve this for OTLP ingestion, Prometheus 3.0 has experimental support for different translation strategies. See <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#:%7E:text=Settings%20related%20to%20the%20OTLP%20receiver%20feature">otlp section in the Prometheus configuration</a> for details.</p>

<blockquote>

</blockquote>

<h2 id="native-histograms">Native Histograms<a href="#native-histograms" name="native-histograms"></a>
</h2>

<p>Native histograms are a Prometheus metric type that offer a higher efficiency and lower cost alternative to Classic Histograms. Rather than having to choose (and potentially have to update) bucket boundaries based on the data set, native histograms have pre-set bucket boundaries based on exponential growth.</p>

<p>Native Histograms are still experimental and not yet enabled by default, and can be turned on by passing <code>--enable-feature=native-histograms</code>. Some aspects of Native Histograms, like the text format and accessor functions / operators are still under active design.</p>

<h2 id="breaking-changes">Breaking Changes<a href="#breaking-changes" name="breaking-changes"></a>
</h2>

<p>The Prometheus community strives to <a href="https://prometheus.io/docs/prometheus/latest/stability/">not break existing features within a major release</a>. With a new major release we took the opportunity to clean up a few, but small, long-standing issues. In other words, Prometheus 3.0 contains a few breaking changes. This includes changes to feature flags, configuration files, PromQL, and scrape protocols.</p>

<p>Please read the <a href="https://prometheus.io/docs/prometheus/3.0/migration/">migration guide</a> to find out if your setup is affected and what actions to take.</p>



<p>It’s impressive to see what we have accomplished in the community since Prometheus 2.0. We all love numbers, so let’s celebrate the efficiency improvements we made for both CPU and memory use for the TSDB mode. Below you can see performance numbers between 3 Prometheus versions on the node with 8 CPU and 49 GB allocatable memory.</p>

<ul>
<li>2.0.0 (7 years ago)</li>
<li>2.18.0 (4 years ago)</li>
<li>3.0.0 (now)</li>
</ul>

<p><img src="https://prometheus.io/assets/blog/2024-11-14/memory_bytes_ui.png" alt="Memory bytes"></p>

<p><img src="https://prometheus.io/assets/blog/2024-11-14/cpu_seconds_ui.png" alt="CPU seconds"></p>

<p>It’s furthermore impressive that those numbers were taken using our <a href="https://github.com/prometheus/prometheus/pull/15366">prombench macrobenchmark</a> 
that uses the same PromQL queries, configuration and environment–highlighting backward compatibility and stability for the core features, even with 3.0.</p>



<p>There are still tons of exciting features and improvements we can make in Prometheus and the ecosystem. Here is a non-exhaustive list to get you excited and… 
hopefully motivate you to contribute and join us!</p>

<ul>
<li>New, more inclusive <strong>governance</strong>
</li>
<li>More <strong>OpenTelemetry</strong> compatibility and features</li>
<li>OpenMetrics 2.0, now under Prometheus governance!</li>
<li>Native Histograms stability (and with custom buckets!)</li>
<li>More optimizations!</li>
<li>UTF-8 support coverage in more SDKs and tools</li>
</ul>



<p>You can try out Prometheus 3.0 by downloading it from our <a href="https://prometheus.io/download/#prometheus">official binaries</a> and <a href="https://quay.io/repository/prometheus/prometheus?tab=tags">container images</a>.</p>

<p>If you are upgrading from Prometheus 2.x, check out the migration guide for more information on any adjustments you will have to make.
Please note that we strongly recommend upgrading to v2.55 before upgrading to v3.0. Rollback is possible from v3.0 to v2.55, but not to earlier versions.</p>

<p>As always, we welcome feedback and contributions from the community!</p>


    <article>

    
    
    
  </article></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Llama.cpp guide – Running LLMs locally on any hardware, from scratch (256 pts)]]></title>
            <link>https://steelph0enix.github.io/posts/llama-cpp-guide/</link>
            <guid>42274489</guid>
            <pubDate>Fri, 29 Nov 2024 15:28:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://steelph0enix.github.io/posts/llama-cpp-guide/">https://steelph0enix.github.io/posts/llama-cpp-guide/</a>, See on <a href="https://news.ycombinator.com/item?id=42274489">Hacker News</a></p>
Couldn't get https://steelph0enix.github.io/posts/llama-cpp-guide/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Core copyright violation moves ahead in The Intercept's lawsuit against OpenAI (183 pts)]]></title>
            <link>https://www.niemanlab.org/2024/11/copyright-claim-moves-ahead-in-the-intercepts-lawsuit-against-openai/</link>
            <guid>42273817</guid>
            <pubDate>Fri, 29 Nov 2024 13:48:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.niemanlab.org/2024/11/copyright-claim-moves-ahead-in-the-intercepts-lawsuit-against-openai/">https://www.niemanlab.org/2024/11/copyright-claim-moves-ahead-in-the-intercepts-lawsuit-against-openai/</a>, See on <a href="https://news.ycombinator.com/item?id=42273817">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Nov.  27, 2024, 11:19 a.m.</p><p>The ruling comes after a judge dismissed similar claims filed by Raw Story and AlterNet earlier this month.</p><div id="content_div-233168">


<p>Last week, a <a href="https://storage.courtlistener.com/recap/gov.uscourts.nysd.616536/gov.uscourts.nysd.616536.122.0.pdf">New York federal judge ruled</a> a key copyright violation claim by The Intercept against OpenAI would move ahead in court. The ruling is the latest in a series of major legal decisions involving the AI developer this month, after OpenAI sought to dismiss lawsuits from several digital news publishers.</p>
<p>Judge Jed Rakoff said he’d hear the claim that OpenAI <a href="https://www.law.cornell.edu/uscode/text/17/1202#:~:text=(b)Removal,management%20information%2C">removed authorship information</a> when it allegedly fed The Intercept’s articles into the training data sets it used to build ChatGPT. Doing so could be a violation of the <a href="https://www.copyright.gov/dmca/">Digital Millennium Copyright Act (DMCA)</a>, a 1998 law that, among other protections, makes it illegal to remove the author name, usage terms, or title from a digital work.</p>
<p>The judge dismissed The Intercept’s claim that OpenAI had <a href="https://www.law.cornell.edu/uscode/text/17/1202#:~:text=(3)distribute%2C%20import%20for%20distribution%2C%20or%20publicly%20perform%20works%2C%20copies%20of%20works%2C%20or%20phonorecords%2C%20knowing%20that%20copyright%20management%20information%20has%20been%20removed%20or%20altered%20without%20authority%20of%20the%20copyright%20owner%20or%20the%20law%2C">knowingly distributed copies</a> of its articles after removing the DMCA-protected information. The judge also dismissed all The Intercept’s claims against Microsoft, which has a <a href="https://www.cnbc.com/2024/10/30/microsoft-cfo-says-openai-investment-will-cut-into-profit-this-quarter.html">multibillion-dollar</a> investment in OpenAI and was named in the initial filing. An opinion from the judge, laying out his reasoning for the dismissals, will be published in the coming weeks.</p>
<p>“The decision allows for a DMCA claim on behalf of digital publishers who do not have copyright registrations to proceed against OpenAI,” said <a href="https://www.loevy.com/attorneys/matt-topic/">Matt Topic</a>, a partner at Loevy &amp; Loevy, who is representing The Intercept. “We’re obviously disappointed to lose the claims against Microsoft, but the core claim is the DMCA claim against OpenAI, and we’re very happy to see that that will be going forward.”</p>
<p>“Our models are trained on publicly available data, grounded in fair use and related principles that we view as fair for creators,” OpenAI spokesperson Jason Deutrom said in a statement.</p>
<p>Earlier this year <a href="https://www.niemanlab.org/2024/03/the-intercept-charts-a-new-legal-strategy-for-digital-publishers-suing-openai/">I reported</a> that The Intercept’s case was carving out a new legal strategy for digital news publishers to sue OpenAI.</p>
<p>The New York Times’ lawsuit against OpenAI, and similar suits filed by <a href="https://www.nydailynews.com/2024/04/30/daily-news-sues-microsoft-open-ai-artifcial-intelligence-federal-court/">The New York Daily News</a> and <a href="https://revealnews.org/press/cir-sues-openai/">Mother Jones</a>, lead with claims of copyright infringement. Infringement suits require that relevant works were first registered with the U.S. Copyright Office (USCO). But most digital news publishers don’t have their article archives registered. For many, The Intercept included, filing all of their published work on the internet with the USCO is too costly or burdensome.</p>
<p>Until this summer, the government body required each individual website article page be filed and charged separately. In August, though, the USCO <a href="https://www.govinfo.gov/content/pkg/FR-2024-07-22/pdf/2024-15880.pdf">added a rule</a> that allows “news websites” to file articles in bulk. Among other reasons, the decision cited concerns about unchecked infringement of online news content and a hope for copyright registrations to stay “adaptive to technological changes.” But for most digital news publishers seeking legal action against OpenAI, particularly for its use of their work to train ChatGPT, the new rule came too late.</p>
<p>For now, The Intercept case is the only litigation by a news publisher, that is not tied to copyright infringement, to move past the motion-to-dismiss stage.</p>
<p>Earlier this month, the DMCA-focused legal strategy <a href="https://www.wired.com/story/opena-alternet-raw-story-copyright-lawsuit-dmca-standing/">took a major hit</a> when another New York federal judge dismissed all DMCA claims against OpenAI filed by Raw Story and AlterNet. The progressive digital news sites are jointly represented by Loevy &amp; Loevy.</p>
<p>“Let us be clear about what is really at stake here. The alleged injury for which Plaintiffs truly seek redress is not the exclusion of [content management information] from Defendants’ training sets, but rather Defendants’ use of Plaintiffs’ articles to develop ChatGPT without compensation,” wrote Judge Colleen MacMahon <a href="https://www.bloomberglaw.com/public/desktop/document/RawStoryMediaIncetalvOpenAIIncetalDocketNo124cv01514SDNYFeb282024/4?doc_id=X18AGVQ2BEU9DCBFDQULD4SNTVV">in that decision</a>.</p>
<p>Despite the setback, the judge said she would consider an amended complaint against OpenAI that took into account her concerns. A proposed amended complaint by Raw Story and AlterNet was filed by Loevy &amp; Loevy last week, just before The Intercept ruling was announced.</p>
<p>“When they populated their training sets with works of journalism, Defendants had a choice: they could train ChatGPT using works of journalism with the copyright management information protected by the DMCA intact, or they could strip it away. Defendants chose the latter,” reads the <a href="https://www.courtlistener.com/docket/68290709/119/1/raw-story-media-inc-v-openai-inc/">proposed amended complaint</a>. “In the process, [OpenAI] trained ChatGPT not to acknowledge or respect copyright, not to notify ChatGPT users when the responses they received were protected by journalists’ copyrights, and not to provide attribution when using the works of human journalists.”</p>
<p>Like The Intercept, Raw Story and AlterNet are asking for $2,500 in damages for each instance that OpenAI allegedly removed DMCA-protected information in its training data sets. If damages are calculated based on each individual article allegedly used to train ChatGPT, it could quickly balloon to tens of thousands of violations.</p>
<p>“The proposed amended complaint would match up and probably even go beyond the allegations that survived in The Intercept case,” said Topic. “Different judges could come out differently on the same question, but we feel optimistic that we will have the opportunity to proceed with an amended claim.”</p>
<p>It is unclear if the Intercept ruling will embolden other publications to consider DMCA litigation; few publications have followed in their footsteps so far. As time goes on, there is concern that new suits against OpenAI would be vulnerable to statute of limitations restrictions, particularly if news publishers want to cite the training data sets underlying ChatGPT. But the ruling is one signal that Loevy &amp; Loevy is narrowing in on a specific DMCA claim that can actually stand up in court.</p>
<p>“We do think that the claim that has survived for The Intercept is a claim that most digital publishers would also be able to bring,” said Topic.</p>
<p>Unsplash</p>







<p><a href="https://www.niemanlab.org/author/adeck">Andrew Deck</a> is a generative AI staff writer at Nieman Lab. Have tips about how AI is being used in your newsroom? You can reach Andrew via <a href="mailto:andrew_deck@harvard.edu">email</a> (andrew_deck@harvard.edu), <a href="https://twitter.com/decka227">Twitter</a> (@decka227), or Signal (+1 203-841-6241).</p>





















</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Alibaba releases an 'open' challenger to OpenAI's O1 reasoning model (117 pts)]]></title>
            <link>https://techcrunch.com/2024/11/27/alibaba-releases-an-open-challenger-to-openais-o1-reasoning-model/</link>
            <guid>42273780</guid>
            <pubDate>Fri, 29 Nov 2024 13:40:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/11/27/alibaba-releases-an-open-challenger-to-openais-o1-reasoning-model/">https://techcrunch.com/2024/11/27/alibaba-releases-an-open-challenger-to-openais-o1-reasoning-model/</a>, See on <a href="https://news.ycombinator.com/item?id=42273780">Hacker News</a></p>
Couldn't get https://techcrunch.com/2024/11/27/alibaba-releases-an-open-challenger-to-openais-o1-reasoning-model/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Don't Fuck with Scroll (340 pts)]]></title>
            <link>https://dontfuckwithscroll.com</link>
            <guid>42273505</guid>
            <pubDate>Fri, 29 Nov 2024 12:52:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dontfuckwithscroll.com">https://dontfuckwithscroll.com</a>, See on <a href="https://news.ycombinator.com/item?id=42273505">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Momentum (aka. smooth or interia) scrolling plugins (example <a href="https://dontfuckwithscroll.com/smooth.html">here</a>), while marketed as enhancements, are a plague upon the internet. They disrupt the natural, efficient, and predictable web browsing experience in countless ways, by they often degrading usability, accessibility, and performance. Here are ten reasons why they ruin the web for everyone.</p><div>
    <li>
      <h2>Violates User Expectations</h2>
      <p>Users know how scrolling works. It's been the same since the dawn of the web: you scroll, content moves. Momentum scrolling plugins hijack this fundamental behavior. Instead of instant and predictable movement, users get some bizarrely animated experience that feels more like playing a bad video game. Muscle memory? Tossed out the window. It's like giving people a steering wheel in a car and making it turn the opposite direction—just for kicks.</p>
      <p><strong>Impact:</strong> This disrupts muscle memory and established habits, which users rely on for efficient navigation. The experience feels artificial and clunky, making users feel like they're fighting against the page instead of interacting with it naturally.</p>
    </li>

    <li>
      <h2>Causes Motion Sickness</h2>
      <p>Not everyone wants a theme-park ride when reading an article. Momentum scrolling plugins introduce floaty, swoopy animations that feel like watching a shaky cam on repeat. Users prone to motion sickness or vertigo (and there are more than you think) can't handle these unnecessary flourishes. What's worse, many sites don't even offer an option to turn it off, leaving people stuck feeling queasy just trying to read your blog post.</p>
      <p><strong>Impact:</strong> Users prone to motion sickness or vertigo may find the website literally nauseating to use. Is your content so unimportant that you'd rather users leave than scroll down? Probably not.</p>
    </li>

    <li>
      <h2>Reduces Accessibility for Disabled Users</h2>
      <p>Web accessibility is a right, not an optional feature. Momentum scrolling plugins laugh in the face of inclusivity. They disrupt assistive technologies like screen readers and keyboard navigation by introducing timing delays. For users with disabilities, such as motor impairments or visual limitations, these delays can render a site unusable. A feature that excludes millions of users isn't a feature—it's a flaw.</p>
      <p><strong>Impact:</strong> Users with disabilities may struggle to interact with the site. Do you really want to alienate an entire demographic just to make scrolling look a little fancier? Accessibility isn't optional—it's a baseline requirement.</p>
    </li>

    <li>
      <h2>Inconsistent Performance Across Devices</h2>
      <p>Momentum scrolling plugins don't care if you're on a state-of-the-art gaming rig or a five-year-old budget phone. They load JavaScript that can lag, stutter, or outright break on older or lower-end devices. Why make your website cater only to people with perfect hardware? The web is supposed to work for everyone, not just the ones who can afford shiny new gadgets every year.</p>
      <p><strong>Impact:</strong> Instead of feeling "Momentum," the page feels broken on weaker devices. Great job—now you've ensured your site only works properly for people with high-end tech. Good UX means working for everyone, not just the rich.</p>
    </li>

    <li>
      <h2>Impairs Usability for Power Users</h2>
      <p>Power users exist. They're the ones who zoom through documentation, rapidly scroll through pages, and want precision. Momentum scrolling spits in their faces. It slows down their workflow by forcing them to endure molasses-like animations. They're not here for your slow, cutesy effects—they're here to get things done. Stop standing in their way. It's like making a racecar driver go 20 mph because it "looks prettier."</p>
      <p><strong>Impact:</strong> These users will hate your site. And let's be clear, power users are often the ones most likely to recommend or share content. Do you really want to alienate your most enthusiastic audience?</p>
    </li>

    <li>
      <h2>Increases Page Load Times</h2>
      <p>Momentum scrolling plugins add bloated JavaScript libraries, extra dependencies, and more CPU cycles to render animations. Guess what? That comes at a cost.</p>
      <p><strong>Impact:</strong> Slower load times annoy everyone. On mobile networks, especially in areas with poor connectivity, your fancy scrolling makes the page slower and less accessible. Congrats, you've sacrificed performance for aesthetics.</p>
    </li>

    <li>
      <h2>Breaks Native Browser Features</h2>
      <p>Modern browsers already include Momentum scrolling settings for users who want them. Adding third-party plugins often overrides or conflicts with these native features, breaking custom scroll gestures or momentum scrolling.</p>
      <p><strong>Impact:</strong> Your site ends up less functional than the browser it's displayed in. Users who expect their preferences to work (e.g., system-wide reduced motion settings) will feel betrayed. You just took something that worked and made it worse.</p>
    </li>

    <li>
      <h2>Makes Scroll Position Unclear</h2>
      <p>Momentum scrolling animations introduce a delay between action and result, making it hard to tell exactly where you are on the page. Combine this with long pages, and users are left feeling lost.</p>
      <p><strong>Impact:</strong> Users now spend extra time figuring out their position. Nobody likes a website that feels like a guessing game. Quick navigation becomes a chore instead of a feature. Great job, you just turned scrolling into a problem.</p>
    </li>

    <li>
      <h2>Adds Maintenance Overhead</h2>
      <p>Momentum scrolling plugins aren't "set it and forget it." They need regular updates to stay compatible with modern browsers, operating systems, and devices. And every update risks introducing new bugs.</p>
      <p><strong>Impact:</strong> You've now created extra work for your development team, all for a feature nobody really asked for. That's time and money that could've been spent making your site faster, more secure, or better optimized. But sure, keep chasing that "Momentum" scroll.</p>
    </li>

    <li>
      <h2>Disrespects the User's Control</h2>
      <p>Users come to your website for content, not for an overly choreographed scrolling experience. Overriding default scrolling assumes your vision of scrolling is better than the user's preferences or needs.</p>
      <p><strong>Impact:</strong> By enforcing Momentum scrolling, you're essentially telling users: "We know better than you." That arrogance doesn't go unnoticed. Respect your users' autonomy—don't dictate how they interact with your site.</p>
    </li>
  </div><p>Momentum scrolling plugins are the web equivalent of turning a functional bike into a unicycle because it "looks cool." It adds unnecessary complexity, degrades usability, and frustrates users. Instead of reinventing scrolling, stick to what works: native, predictable, fast scrolling behavior.</p><p>Don't make scrolling a thing. Just let people scroll.</p><p>And if you're still not convinced, go to the <a href="https://dontfuckwithscroll.com/smooth.html">smooth version</a> and try to tell me it's better.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gimp 3.0 – a milestone for open-source image editing (250 pts)]]></title>
            <link>https://lwn.net/SubscriberLink/998793/6c8d00bd1b2a7948/</link>
            <guid>42272927</guid>
            <pubDate>Fri, 29 Nov 2024 10:50:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/SubscriberLink/998793/6c8d00bd1b2a7948/">https://lwn.net/SubscriberLink/998793/6c8d00bd1b2a7948/</a>, See on <a href="https://news.ycombinator.com/item?id=42272927">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<blockquote>
<div>
<h3>Welcome to LWN.net</h3>
<p>
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider <a href="https://lwn.net/subscribe/">subscribing to LWN</a>.  Thank you
for visiting LWN.net!
</p></div>
</blockquote>

<p>
The long-awaited release of the <a href="https://gimp.org/">GNU Image
Manipulation Program</a> (GIMP)&nbsp;3.0 is on the way, marking the first
major update since version&nbsp;2.10 <a href="https://www.gimp.org/news/2018/04/27/gimp-2-10-0-released/">was
released</a> in April&nbsp;2018. It now features a <a href="https://docs.gtk.org/gtk3/">GTK&nbsp;3</a> user interface and GIMP&nbsp;3.0
introduces significant changes to the core platform and plugins. This
release also brings performance and usability improvements, as well as more
compatibility with Wayland and complex input sources.
</p>

<h4>Modernized interface</h4>

<p>
GIMP&nbsp;3.0 is the first release <a href="https://www.gimp.org/news/2023/07/09/gimp-2-99-16-released/#gtk3-port-officially-done">to
use GTK&nbsp;3</a>, a more modern foundation than the GTK&nbsp;2 base of
prior releases. <a href="https://docs.gtk.org/gtk4/">GTK&nbsp;4</a> has
been available for a few years now, and is <a href="https://gitlab.gnome.org/GNOME/gimp/-/issues/6440">on the project's
radar</a>, but the <a href="https://www.gimp.org/news/2020/12/25/gimp-2-99-4-released/#whats-next">plan</a>
was always to finish the GTK&nbsp;3 work first.
Moving to GTK&nbsp;3 brings initial Wayland compatibility
and HiDPI scaling. In addition, this allows for GIMP users to take
advantage of multi-touch input, bringing pinch-to-zoom gestures to the
program, and offering a better experience when working with complex
peripherals, such as advanced drawing tablets. These features were not
previously possible due to the limitations of GTK&nbsp;2.
</p>

<p><a href="https://lwn.net/Articles/999429/">
<img src="https://static.lwn.net/images/2024/gimp-welcome-sm.png" alt="[Welcome screen]" title="Welcome screen" width="300" height="316">
</a></p><p>
A secondary result of the transition to GTK&nbsp;3 is a refreshed user
interface (UI), now with support for CSS themes included. In this release,
four themes are available by default, including light, dark, and gray
themes, along with a high-contrast theme for users with visual
impairments. Additionally, this release has transitioned to using <a href="https://developer.gnome.org/hig/patterns/containers/header-bars.html">GTK's
header bar</a> component, typically used to combine an application's
toolbar and title bar into one unit. To maintain familiarity with previous
releases, however, GIMP&nbsp;3.0 still supports the traditional menu interface.
</p>

<p>
When GIMP 3.0 is launched, users will be greeted with a
new welcome screen, seen on the right. This dialog presents various useful
links for the GIMP project, such as tutorials, documentation, and donation
options. It also provides users with quick functions for setting up GIMP as
needed before starting their work. This includes the option to choose GTK
and icon themes, and for placing the menu in the title bar, saving vertical
space. In the "Create" tab of the dialog, users can create a new image,
select from their recent work, or open an existing image from the
filesystem.  The welcome screen can be disabled if desired.
</p>

<h4>Better workflow, performance, and color management</h4>

<p>
A major focus of this release is greater integration with the <a href="https://www.gegl.org/">Generic Graphics Library</a> (GEGL), first
introduced in 2000 for the purpose of improving GIMP's image-processing
capabilities through a <a href="https://glasnost.itcarlow.ie/~powerk/GeneralGraphicsNotes/scenegraph/scenegraph.htm">scene-graph-based</a>
architecture. As part of this effort, there have been numerous
optimizations to GIMP's core and to its standard plugins. In tandem with
memory management and multi-threading improvements, these changes should
bring significant speed boosts when applying filters and effects, even on
larger images.
</p>

<!-- middle-ad -->

<p>
GEGL allows image-processing operations to be chained together in such a
way that the original image data is preserved, along with a record of every
edit. This is referred to as non-destructive editing, and GIMP&nbsp;3.0 is
the first stable release of the project to make this workflow available,
though there is still more work to be done. Users can apply filters and effects to any layer without altering the original image. As a result, effect parameters can be changed even after they've been applied. This change removes the need to perform an undo any time a filter or effect does not have the desired result. Filters, and any plugins that use GEGL operations, now offer real-time previews.
</p>

<p><a href="https://lwn.net/Articles/999599/">
<img src="https://static.lwn.net/images/2024/gimp-split-preview-sm.png" alt="[Split preview]" title="Split preview" width="300" height="196">
</a></p><p>
GIMP&nbsp;3.0 continues efforts to improve color management under the
initiative referred to as "<a href="https://www.patreon.com/posts/20264674">space invasion</a>", and
delivers significant <a href="https://www.gimp.org/news/2024/02/21/gimp-2-99-18-released/#color-space-invasion">color-correctness
results</a>. This will improve the consistency of color reproduction across different devices and workflows. The foundation of these improvements is the <a href="https://gegl.org/babl/">babl library</a>, responsible for handling pixel-format conversions and color-space management. Color profiles are now automatically managed when opening files with an included color profile. In the 2.10.x series, manual intervention was required when loading these files.
</p>

<p>
GIMP&nbsp;3.0 supports palettes outside of the "Standard Red Green Blue"
(<a href="https://techterms.com/definition/srgb">sRGB</a>) range, such as
"Cyan Magenta Yellow Key" (<a href="https://techterms.com/definition/cmyk">CMYK</a>) and (<a href="https://en.wikipedia.org/wiki/CIELAB_color_space">CIELAB</a>). This
expanded color support, especially for CMYK, is essential to those who work
with print and desktop publishing. However, GIMP continues to use sRGB,
grayscale, and indexed colors for storing color information internally for
now. Conversion to other color spaces is done on output, where necessary.
</p>

<p>
Version&nbsp;3.0 delivers CMYK output for some file formats, specifically
JPEG, TIFF, and Photoshop (PSD). When working with these color formats, users often
need to use soft-proofing, a digital preview of how colors will look when
printed, helping ensure color accuracy in the final output. GIMP's&nbsp;3.0
soft-proofing workflow has been improved, and it saves this information <a href="https://www.gimp.org/news/2022/08/27/gimp-2-99-12-released/#simulation-data-is-now-image-data">in
its XCF format</a>, preserving settings between sessions.
</p>

<h4>Layers and file-format improvements</h4>

<p>
The layer workflow has been upgraded in GIMP&nbsp;3.0, with new features
that bring greater parity with other advanced photo editors such as Adobe
Photoshop and Affinity Photo. Layer operations can now be applied in bulk,
and multiple layers can be selected and grouped. Layers can even be moved,
reordered, duplicated, merged, and deleted <i>en masse</i>, whether or not they
are contiguous within the layer dialog. Each layer now displays a distinct
"Fx" icon to represent when filters and effects are in use. Filters and
effects can be managed from a popover, which is opened up by clicking on
this icon.
</p>

<p><a href="https://lwn.net/Articles/999430/">
<img src="https://static.lwn.net/images/2024/gimp-layers-sm.png" alt="[Layers dialog]" title="Layers dialog" width="300" height="208">
</a></p><p>
The developers intend to further improve the layer-effects workflow with tighter integration of these features and a <a href="https://developer.gimp.org/core/specifications/layer-effects-ui/">better UI</a>. In future releases, GIMP's non-destructive functionality will be extended to <a href="https://www.gimp.org/tutorials/Layer_Masks/">layer masks</a>, and <a href="https://docs.gimp.org/2.4/en/glossary.html#glossary-c:~:text=C-,Channels,-A%20Channel%20is">layer channels</a> as well. For users who wish to use a destructive workflow (where filters are applied to the original image data), there is a "Merge Filter" option available in the filter dialog.
</p>

<p>
Another new feature in GIMP&nbsp;3.0's layer workflow is <a href="https://www.gimp.org/news/2024/02/21/gimp-2-99-18-released/#auto-expanding-layers">auto-expanding
layers</a>. Layers may need to extend outside of the canvas in order to
preserve parts of an image that need to later be moved or transformed, or
within animations, where part of the animation needs to be made invisible
temporarily. An auto-expanding layer will extend beyond its borders when
painting outside of the canvas, though the canvas itself will retain its
size. To enable this feature, users will need to check the  "Expand Layers"
option on any brush tool. Layer masks can be set to expand with the layer;
the "Align And Distribute" tool has been completely reworked to recognize the layer's contents and not just its boundaries.
Improvements to guides, snapping, and the text editor also help make
GIMP's layer workflows more appealing in a way that will be familiar to
users of other tools in this space.
</p>

<p>
This update brings improved support for many
image formats. PSD images now preserve their layer order on import, and
Paintshop Pro images (PSP) preserve several features, including <a href="https://en.wikipedia.org/wiki/ICC_profile">International Color
Consortium (ICC) profiles</a>, <a href="https://docs.gimp.org/2.4/en/gimp-concepts-image-grid.html">grids,
and guides</a>. In a crucial bug fix for working with GIFs, GIMP&nbsp;3.0
detects whether these files contain animations, and correctly saves
animated GIFs when overwriting existing files.
</p>

<p>
New <a href="https://www.gimp.org/news/2024/02/21/gimp-2-99-18-released/#new-image-format-supports-farbfeld-esm-software-pix-hej2">image formats</a> have been added, with import and export supported for each format. The highly versatile <a href="https://www.selapa.net/swatchbooker/">SwatchBooker</a> palette format, SBZ, <a href="https://www.gimp.org/news/2024/02/21/gimp-2-99-18-released/#new-palette-format-support-swatchbooker">is now supported</a>, bringing parity with <a href="https://www.scribus.net/">Scribus</a> and <a href="https://krita.org/">Krita</a>. Unlike most other palette formats, SBZ files can save complex details such as layouts, textures, gradients, named colors, color-space information, and even multiple palettes in a single file. For processing raw images (formats that preserve unprocessed image data), GIMP has long relied on external applications such as <a href="https://www.darktable.org/">darktable</a> and <a href="https://www.rawtherapee.com/">RawTherapee</a>. However, recent changes in darktable's API <a href="https://github.com/darktable-org/darktable/issues/15968">broke this integration</a>, causing GIMP to be unable to detect and recognize if it was installed. To rectify this issue, dedicated darktable integration is <a href="https://www.gimp.org/news/2024/10/05/development-update/#darktable-integration">now available</a> thanks to a <a href="https://gitlab.gnome.org/GNOME/gimp/-/issues/10572">collaborative effort</a> between the two projects.
</p>

<p>
This update introduces a new extension system and file format, GEX,
allowing for easier distribution of plugins, themes, brushes, and other
means of extending and supplementing the core application. This extension
system even allows for multiple features, such as plugins, brushes, and
themes, to be packaged and managed simultaneously. For example, a project
such as <a href="https://github.com/Diolinux/PhotoGIMP">PhotoGIMP</a> could
use this system to completely transform GIMP's standard functionality with
a single package.
</p>

<p>
In addition, an <a href="https://www.gimp.org/news/2018/08/19/gimp-2-10-6-released/#gimp-extensions">extension manager</a> has been developed and is available in this update. It's not immediately exposed to users, however, because the backend infrastructure for distributing extensions is still under development. To access it, users will need to use GIMP's command search, which can be found in the menu under "Help&gt;Search and Run a Command". Once extension support has matured, it will allow managing other features on the fly.
</p>

<h4>Future developments</h4>

<p>
Apart from the myriad user-visible features, several under-the-hood changes
will either land in this release or <a href="https://developer.gimp.org/core/roadmap/#post-gimp-300">come to
fruition</a> in near future releases. GIMP&nbsp;3.0 contains the
infrastructure for <a href="https://developer.gimp.org/core/roadmap/#non-destructive-layer-types">non-destructive
layer types</a> (such as <a href="https://gitlab.gnome.org/GNOME/gimp/-/merge_requests/773">vector
layers</a> and <a href="https://gitlab.gnome.org/GNOME/gimp/-/merge_requests/823">link
layers</a>), <a href="https://developer.gimp.org/core/roadmap/#animation-and-multi-page-support">animations</a>
in the core application, multi-page files, and native support for working
in color spaces outside of the RGB range.
</p>

<p>
The changes in  GIMP&nbsp;3.0 have necessitated an <a href="https://developer.gimp.org/resource/script-fu/overview-v3/">API
break</a> so it is no longer compatible with plugins from older
releases. To lessen the impact, the developers have decided <a href="https://www.gimp.org/news/2020/11/06/gimp-2-99-2-released">to
couple</a> this shift with the incompatible changes of switching to
GTK&nbsp;3. Because of this, the development of the <a href="https://developer.gimp.org/core/roadmap/#gimp-210x-stable-branch-roadmap">2.10.x
branch</a> has continued, with <a href="https://www.gimp.org/news/2024/05/05/gimp-2-10-38-released/#backports-of-other-gtk3-features">some
features</a> being backported from GIMP&nbsp;3.0 during this development
cycle. Future releases within the 2.10.x branch are still possible, but
unlikely, with the foundation of the 3.x series reaching maturity. Do note,
however, that most of GIMP's existing plugins will need to be updated to
support the new API, which is expected to provide developers <a href="https://www.gimp.org/news/2022/02/25/gimp-2-99-10-released/#improved-api-for-plug-ins">with the benefit</a> of increased functionality and improved performance.
</p>

<p>
These are just some of the changes coming to GIMP in
version&nbsp;3.0. Exactly when GIMP&nbsp;3.0 will be released remains to be
seen, but users interested in trying the new features before release can <a href="https://www.gimp.org/downloads/devel/">download development
builds</a>; updates are <a href="https://www.gimp.org/news/">regularly
posted</a> on the GIMP web site. The first release candidate has <a href="https://www.gimp.org/news/2024/11/06/gimp-3-0-RC1-released/">already
landed</a> on November&nbsp;6, following the project <a href="https://discourse.gnome.org/t/gimps-master-branch-string-freeze/22895">reaching
string freeze</a> on the main branch. So long as there are no major bugs or
regressions, the final release will be built from the release candidate. However, additional
candidates may be necessary if any major problems are found. The
developers have <a href="https://www.gimp.org/news/2024/11/06/gimp-3-0-RC1-released/#future-changes-to-release-process">already
announced</a> that subsequent releases should land faster, with smaller,
feature-focused releases providing more stability. Overall, GIMP&nbsp;3.0
represents a big step forward for the free photo and graphics editor, and sets the stage for even greater improvements to come.
</p><br clear="all"><table>
           <tbody><tr><th colspan="2">Index entries for this article</th></tr>
           <tr><td><a href="https://lwn.net/Archives/GuestIndex/">GuestArticles</a></td><td><a href="https://lwn.net/Archives/GuestIndex/#Taylor_Roland">Taylor, Roland</a></td></tr>
            </tbody></table><br clear="all">

               <br clear="all">
               <hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using Pandoc and Typst to Produce PDFs (104 pts)]]></title>
            <link>https://imaginarytext.ca/posts/2024/pandoc-typst-tutorial/</link>
            <guid>42271078</guid>
            <pubDate>Fri, 29 Nov 2024 05:25:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://imaginarytext.ca/posts/2024/pandoc-typst-tutorial/">https://imaginarytext.ca/posts/2024/pandoc-typst-tutorial/</a>, See on <a href="https://news.ycombinator.com/item?id=42271078">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
<article>





<p>I recently responded to someone on Mastodon who asked about producing decent-looking PDFs from markdown. I replied eagerly with “OMG Typst!” and linked to my earlier blogpost about developing an <a href="https://imaginarytext.ca/posts/2024/more-typst">entire book layout template for Pandoc and Typst</a>. The response I then received was this was “far beyond” what they need – and on reflection I had to admit that my blog post was a bit, well, <em>niche</em>.</p>
<p>I recalled that a decade ago, when Pandoc was new (or at least I was new to Pandoc), I produced a set of tutorials for producing high-quality EPUBs from Pandoc, and that these were quite well received. These days I find EPUB pretty uninteresting,<sup><a href="#fn1" id="fnref1">[1]</a></sup> but I’m still using Pandoc for a variety of purposes, and especially using it to drive PDF (that is, “print”) production via Typst. Typst emerged as an output option for Pandoc sometime last year and since then has gotten even smarter and smoother.</p>
<p>So here’s an attempt at a basic tutorial for using Typst with Pandoc to produce beautiful – and easily customizable – PDFs and indeed printed pages.</p>
<p>Now… you know how when you google a recipe online, and you get to a food blog, and you have to scroll through 37 screens of narrative and photographs before they finally give you the recipe? Let’s cut to the chase: here’s the recipe. Now you can read the rest of this post to learn what it means.</p>
<blockquote>
<p><a href="https://imaginarytext.ca/posts/2024/img/simplePandocTypst.template">Download simplePandocTypst.template</a></p>
</blockquote>
<h2>Some Background</h2>
<p><strong>Typst</strong> is a new (since 2023) open-source tool for typesetting and page layout. It was designed to replace LaTeX, which has been the standard way of producing scientific publications <em>for decades</em> (in the niche where InDesign and Desktop Publishing apps fall down because of complex equation formatting). LaTeX has been used – and more significantly, <em>learned</em> – by generations of grad students in the hard sciences, as a way of formatting theses, articles, reports, and so on. But where LaTeX is weird, old, and arcane, Typst was designed as a modern, streamlined tool to produce excellent typography through templated pages.</p>
<p>Now, <em><a href="https://typst.app/">Typst.app</a></em> is an interactive website which invites you to compose Typst documents and see the typeset results on the fly. This is presumably aimed at those chemistry and math grad students who would otherwise write LaTeX code, often in an interactive web interface just like this. But Typst is <em>also</em>, significantly, a <a href="https://github.com/typst/typst">downloadable piece of open-source software</a>. Technically, what you’re downloading is the ‘typst compiler’ – you feed it a text file written in Typst sourcecode (specifying document properties, formatting, and so on), and it outputs a PDF file to spec.</p>
<p>That’s all very well but I’d just as soon not have to learn to write Typst source code at all (as I don’t deal with complex mathematical equations in my life). This is where Pandoc comes in.</p>
<p><strong><a href="https://pandoc.org/">Pandoc</a></strong>, which bills itself as a “universal document converter” is an open-source tool that will take just about any structured text format as input, and produce just about any other structured text format as output. But in practice, what it does exceptionally well is producing various outputs from <em><strong>markdown</strong></em> formatted text files. Which is to say <em>minimally formatted</em> text files, since the point of markdown is to be the simplest possible document formatting system. This is markdown’s great virtue: the formatting information is there, and is explicit and unambiguous, but it doesn’t get in the way of you seeing the text itself.</p>
<h3>Using Pandoc with Typst</h3>
<p>Now, when I say that Typst is an available output for Pandoc, I mean that we can pass a markdown-formatted text file to Pandoc, and it will convert it to Typst code, which we can then give to Typst to make a PDF. Actually, it’s easier than that, because Pandoc will both convert to Typst code <em>and</em> ask Typst to render that as a PDF in one step.</p>
<pre><code>pandoc mymarkdownfile.txt --pdf-engine=typst -o myPDF.pdf
</code></pre>
<p>You can try that, and you’ll get a nice-looking PDF. Pandoc uses a default <em>template</em> for Typst that defines page size, margins, and some basic typography. Typst right out of the box already does a lot of the work – hyphenation and justification and page numbering and so on. But we can have some fine-tuned control over that by tweaking the template that Pandoc uses. That said, Pandoc’s default Typst-formatted PDF may be good enough for basic use.</p>
<h2>Basic document metadata</h2>
<p>Typst expects to see some basic metadata about the document you’re producing: the title, the author, and the date, at minimum (though more is possible). In a markdown document, this is typically written as a metadata block at the top of the file like so:</p>
<pre><code>---
title: "Using Pandoc and Typst to produce PDFs"
author: JMax
date: 2024-07-10
---
</code></pre>
<p>Pandoc recognizes this, and so Typst will, by default, format them in a centered header, as if it were an ACM-style journal article or conference paper. You don’t <em>have to</em> include those elements, but if you do, Typst will style them as frontmatter in your PDF.</p>
<p>We can change how it treats those items by tweaking the template, just as we can change pretty much anything else: page size, orientation, columns, margins, typography, and so on.</p>
<h3>Customizing the template</h3>
<p>Pandoc keeps a hidden store of all the default templates it uses for different outputs, and you can ask Pandoc to show you its default template for any given format. We can ask for the Typst default template like so:</p>
<pre><code>pandoc -D typst
</code></pre>
<p>You <em>could</em> save that into a file, and modify it as you like, and you’d have a new ‘custom’ template you can use. You can specify your own template instead of the default one by telling Pandoc:</p>
<pre><code>pandoc inputfile.txt --pdf-engine=typst --template=myTypst.template -o output.pdf
</code></pre>
<h2>A Starter Custom Template</h2>
<p>But here I’m going to short-circuit things, and offer you a starter kit for a custom template. This is one that I’ve developed, bit by bit, over the past few months. It’s a simpler version of the book-layout template I blogged about earlier, but the bones are the same.</p>
<p>You can think of Typst templates as the equivalent of CSS stylesheets: they define the specs for all the various things in your document. Typst templates are more complicated than CSS, but that’s because there’s a lot more going on here: for instance, you could tell it to start page numbering only after the third right-hand page. There is actually a ton of programmable stuff that you could put into these templates; the one I’m offering here is pretty straightforward so you can go in and muck with it as you like.</p>
<p>For instance, I’ve set up the basic font as Times New Roman, because it’s available on almost every computer; you can change this to any font you have, though. I’ve set the default paper size to US Letter, but if you’re in Europe, you can set this to A4 or whatever you like. My paragraph styling is to indent the first line of all subsequent paragraphs, but I’ve included an optional instruction for flush-left paragraphs with space between them if you prefer that.</p>
<blockquote>
<p>This post as a PDF: <a href="https://imaginarytext.ca/posts/2024/img/UsingPandocTypsttoProducePDFs.pdf">UsingPandocTypsttoProducePDFs.pdf</a></p>
</blockquote>
<p>My template tries to cover a lot of formatting bases: images, footnotes, header and footer, code snippets, etc. You should, of course, consult the extensive (if not always crystal-clear) documentation at <a href="https://typst.app/docs/">https://typst.app/docs/</a> for details and more possibilities.</p>
<hr>
<p>Download, try it out, modify it. Let me know what you think.</p>
<blockquote>
<p><a href="https://imaginarytext.ca/posts/2024/img/simplePandocTypst.template">Download simplePandocTypst.template</a></p>
</blockquote>
<hr>
<section>
<ol>
<li id="fn1"><p>Check out <a href="https://pandoc.org/MANUAL.html">Pandoc’s excellent documentation</a>, and you’ll see loads of EPUB production details. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>


</article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Australian Parliament bans social media for under-16s (186 pts)]]></title>
            <link>https://apnews.com/article/australia-social-media-children-ban-safeguarding-harm-accounts-d0cde2603bdbc7167801da1d00ecd056</link>
            <guid>42270966</guid>
            <pubDate>Fri, 29 Nov 2024 05:01:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/australia-social-media-children-ban-safeguarding-harm-accounts-d0cde2603bdbc7167801da1d00ecd056">https://apnews.com/article/australia-social-media-children-ban-safeguarding-harm-accounts-d0cde2603bdbc7167801da1d00ecd056</a>, See on <a href="https://news.ycombinator.com/item?id=42270966">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>MELBOURNE, Australia (AP) — A <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/australia-social-media-ban-children-1abadf5445418c8c14f5f68cf76b38d0">social media ban for children</a></span> under 16 passed the Australian Parliament on Friday in a world-first law.</p><p>The law will make platforms including TikTok, Facebook, Snapchat, Reddit, X and Instagram liable for fines of up to 50 million Australian dollars ($33 million) for systemic failures to prevent children younger than 16 from holding accounts.</p><p>The Senate passed the bill on Thursday 34 votes to 19. The House of Representatives on Wednesday <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/australia-social-media-young-children-bf0ca2aedaf61b71fe335421240e94c4">overwhelmingly approved</a></span> the legislation by 102 votes to 13.</p><p>The House on Friday endorsed opposition amendments made in the Senate, making the bill law. </p><p>Prime Minister Anthony Albanese said the law supported parents concerned by online harms to their children.</p><p>“Platforms now have a social responsibility to ensure the safety of our kids is a priority for them,” Albanese told reporters.</p>
    

<p>The platforms have one year to work out how they could implement the ban before penalties are enforced.</p><p>Meta Platforms, which owns Facebook and Instagram, said the legislation had been “rushed.”</p><p>Digital Industry Group Inc., an advocate for the platforms in Australia, said questions remain about the law’s impact on children, its technical foundations and scope.</p>



<p>“The social media ban legislation has been released and passed within a week and, as a result, no one can confidently explain how it will work in practice – the community and platforms are in the dark about what exactly is required of them,” DIGI managing director Sunita Bose said.</p>
    
    
    
<p>The amendments passed on Friday bolster privacy protections. Platforms would not be allowed to compel users to provide government-issued identity documents including passports or driver’s licenses, nor could they demand digital identification through a government system.</p><p>Critics of the legislation fear that banning young children from social media will impact the privacy of all users who must establish they are older than 16.</p>
    

<p>While the major parties support the ban, many child welfare and mental health advocates are concerned about unintended consequences.</p><p>Sen. David Shoebridge, from the minority Greens party, said mental health experts agreed that the ban could dangerously isolate many children who used social media to find support.</p><p>“This policy will hurt vulnerable young people the most, especially in regional communities and especially the LGBTQI community, by cutting them off,” Shoebridge told the Senate.</p><p>Exemptions will apply for health and education services including YouTube, Messenger Kids, WhatsApp, Kids Helpline and Google Classroom.</p><p>Opposition Sen. Maria Kovacic said the bill was not radical but necessary. “The core focus of this legislation is simple: It demands that social media companies take reasonable steps to identify and remove underage users from their platforms,” Kovacic told the Senate.</p><p>“This is a responsibility these companies should have been fulfilling long ago, but for too long they have shirked these responsibilities in favor of profit,” she added.</p><p>Online safety campaigner Sonya Ryan, whose 15-year-old daughter Carly was murdered by a 50-year-old pedophile who pretended to be a teenager online, described the Senate vote as a “monumental moment in protecting our children from horrendous harms online.”</p>
    

<p>“It’s too late for my daughter, Carly, and the many other children who have suffered terribly and those who have lost their lives in Australia, but let us stand together on their behalf and embrace this together,” she said.</p><p>Wayne Holdsworth, whose teenage son Mac took his own life after falling victim to an online sextortion scam, had advocated for the age restriction and took pride in its passage. </p><p>“I have always been a proud Australian, but for me subsequent to today’s Senate decision, I am bursting with pride,” Holdsworth said.</p><p>Christopher Stone, executive director of Suicide Prevention Australia, the governing body for the suicide prevention sector, said the legislation failed to consider positive aspects of social media in supporting young people’s mental health and sense of connection.</p>
    

<p>“The government is running blindfolded into a brick wall by rushing this legislation. Young Australians deserve evidence-based policies, not decisions made in haste,” Stone said.</p><p>The platforms had complained that the law would be unworkable and had urged the Senate to delay the vote until at least June 2025 when a government-commissioned evaluation of age assurance technologies will report on how young children could be excluded.</p><p>“Naturally, we respect the laws decided by the Australian Parliament,” Facebook and Instagram owner Meta Platforms said. “However, we are concerned about the process which rushed the legislation through while failing to properly consider the evidence, what industry already does to ensure age-appropriate experiences, and the voices of young people.”</p>
    

<p>Snapchat said it was also concerned by the law and would cooperate with the government regulator, the eSafety Commissioner.</p><p>“While there are many unanswered questions about how this law will be implemented in practice, we will engage closely with the Government and the eSafety Commissioner during the 12-month implementation period to help develop an approach that balances privacy, safety and practicality. As always, Snap will comply with any applicable laws and regulations in Australia,” Snapchat said in a statement.</p><p>Critics argue the government is attempting to convince parents it is protecting their children ahead of a general election due by May. The government hopes that voters will reward it for responding to parents’ concerns about their children’s addiction to social media. Some argue the legislation could cause more harm than it prevents.</p><p>Criticisms include that the legislation was rushed through Parliament without adequate scrutiny, is ineffective, poses privacy risks for all users, and undermines the authority of parents to make decisions for their children.</p><p>Opponents also argue the ban would isolate children, deprive them of the positive aspects of social media, drive them to the dark web, discourage children too young for social media to report harm, and reduce incentives for platforms to improve online safety.</p><h2>____</h2><p>AP Business Writer Kelvin Chan in London contributed to this report.</p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[BYD launches sodium-ion grid-scale BESS product (131 pts)]]></title>
            <link>https://www.energy-storage.news/byd-launches-sodium-ion-grid-scale-bess-product/</link>
            <guid>42270610</guid>
            <pubDate>Fri, 29 Nov 2024 03:44:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.energy-storage.news/byd-launches-sodium-ion-grid-scale-bess-product/">https://www.energy-storage.news/byd-launches-sodium-ion-grid-scale-bess-product/</a>, See on <a href="https://news.ycombinator.com/item?id=42270610">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



<p>He said it uses the company’s Long Blade Battery, has a ‘CTS super integrated design’, and is the world’s first high-performance sodium-ion battery energy storage system (BESS). He claimed it has ultra high energy density, exceptional safety standards and flexible module design. </p>



<p>The BESS has an energy storage capacity of 2.3MWh and a nominal voltage of 1200V, with a voltage range from 800V-1400V.</p>



<p><em>Energy-Storage.news </em>has asked BYD’s press team for more information and will update this article or follow up in due course. </p>



<p>In response to questions left as comments on <a href="https://www.linkedin.com/posts/activity-7262096404557250560-TlM4/?utm_source=share&amp;utm_medium=member_desktop" target="_blank" rel="noreferrer noopener">his post</a>, Wang said, of the advantages of sodium-ion: “The first advantage is its natural abundance, which theoretically results in lower manufacturing costs compared to lithium-ion batteries. Furthermore, sodium-ion batteries offer a higher charge/discharge rate, a broader operating temperature range, a longer cycle life, and improved safety.”</p>



<p>Natural abundance of sodium and better fire safety features are the two main reasons many are pinning their hopes on sodium-ion as an alternative to lithium-ion, with the latter’s supply chain shocks of 2021 and 2022 and relatively rare but high-profile fire incidents causing major challenges for the energy storage industry to-date. </p>



<p>Most agree that, thanks to its cheaper materials, sodium-ion cells should also be cheaper than lithium-ion once manufacturing scales up. </p>



<p>Sodium-ion cells are however much less energy dense, as illustrated by BYD’s new product only packing 2.3MWh per 20-foot container, much less than the <a href="https://www.energy-storage.news/cost-shipping-energy-density-driven-convergence-to-5mwh-bess-form-factor-cea/" target="_blank" rel="noreferrer noopener">5MWh and more than is now standard in the lithium-ion BESS space</a>. That is less of an issue in the BESS segment than for EVs, however, though there are EVs in China being sold with sodium-ion batteries too. </p>



<p>Chinese companies are investing a lot into the sodium-ion technology space, and the world’s largest BESS system using sodium-ion technology is there, <a href="https://www.energy-storage.news/first-half-world-largest-200mwh-sodium-ion-project-comes-online-china/" target="_blank" rel="noreferrer noopener">a 100MW/200MWh system, half of which came online in summer</a>. One of the technology providers on that project was China-based company called HiNa Battery.</p>



<p>In January, BYD began construction of 30GWh sodium-ion battery plant in Xuzhou City, China. BYD is the largest EV company in the world by sales, and has also expanded into lithium-ion battery cells and BESS production over the years, <a href="https://www.energy-storage.news/huawei-and-byd-among-global-top-five-system-integrators-of-2022-amidst-china-price-war/" target="_blank" rel="noreferrer noopener">growing to be one of the largest in that space too</a>.</p>



<p>The US is also making a push into sodium-ion technology. The US Department of Energy (DOE) last week (21 November) awarded US$50 million to establish the ‘Low-cost Earth-abundant Na-ion Storage (LENS) Consortium’, which aims to develop high-energy, long-lasting sodium-ion battery technology.</p>



<p>The consortium will be led by Argonne National Laboratory, and will also involve Brookhaven National Laboratory, Lawrence Berkeley National Laboratory, Pacific Northwest National Laboratory, Sandia National Laboratories and SLAC National Accelerator Laboratory.</p>



<p>US-based sodium-ion BESS startup Peak Energy recently raised US$55 million in a Series A led by TDK Ventures, following which the <a href="https://www.energy-storage.news/density-cycle-life-now-where-they-need-to-be-sodium-ion-bess-investor-tdk-on-technologys-state-of-play/">TDK partner who led the deal spoke to <em>Energy-Storage.news </em>about the sodium-ion BESS space (Premium access article)</a>. </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Vince – A self hosted alternative to Google Analytics (224 pts)]]></title>
            <link>https://github.com/vinceanalytics/vince</link>
            <guid>42270389</guid>
            <pubDate>Fri, 29 Nov 2024 02:54:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/vinceanalytics/vince">https://github.com/vinceanalytics/vince</a>, See on <a href="https://news.ycombinator.com/item?id=42270389">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <themed-picture data-catalyst-inline="true"><picture width="640" height="250">
    <source media="(prefers-color-scheme: dark)" srcset="https://github.com/vinceanalytics/vince/raw/main/app/images/logo-darkmode.svg">
    <source media="(prefers-color-scheme: light)" srcset="https://github.com/vinceanalytics/vince/raw/main/app/images/mark.svg">
    <img alt="esbuild: An extremely fast JavaScript bundler" src="https://github.com/vinceanalytics/vince/raw/main/logo.svg">
  </picture></themed-picture>
  <br>
  <a href="https://vinceanalytics.com/" rel="nofollow">Website</a> |
  <a href="https://vinceanalytics.com/blog/deploy-local/" rel="nofollow">Getting started</a> |
  <a href="https://vinceanalytics.com/tags/api/" rel="nofollow">API</a>  |
  <a href="https://demo.vinceanalytics.com/v1/share/vinceanalytics.com?auth=Ls9tV4pzqOn7BJ7-&amp;demo=true" rel="nofollow">Demo</a> 
</p>
<p dir="auto"><strong>Vince</strong> is a self hosted alternative to Google Analytics.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><strong>Automatic TLS</strong> native support for let's encrypt.</li>
<li><strong>Drop in replacement for plausible</strong> you can use existing plausible  scripts and just point them to the vince instance (note that vince is lean and only covers features for a single entity self hosting, so it is not our goal to be feature parity with plausible).</li>
<li><strong>Outbounds links tracking</strong></li>
<li><strong>File download tracking</strong></li>
<li><strong>404 page tracking</strong></li>
<li><strong>Custom event tracking</strong></li>
<li><strong>Time period comparison</strong></li>
<li><strong>Public dashboards</strong> allow access to the dashoard to anyone(by default all dashboards are private).</li>
<li><strong>Unique shared access</strong> generate unique links to dahboards that can be password protected.</li>
<li><strong>Zero Dependency</strong>: Ships a single binary with everything in it. No runtime dependency.</li>
<li><strong>Easy to operate</strong>: One line commandline flags with env variables is all you need.</li>
<li><strong>Unlimited sites</strong>: There is no limit on how many sites you can manage.</li>
<li><strong>Unlimited events</strong>: scale according to availbale resources.</li>
<li><strong>Privacy friendly</strong>: No cookies and fully compliant with GDPR, CCPA and PECR.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Vince ships a single executable without any dependencies.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installing</h2><a id="user-content-installing" aria-label="Permalink: Installing" href="#installing"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">MacOS and Linux</h3><a id="user-content-macos-and-linux" aria-label="Permalink: MacOS and Linux" href="#macos-and-linux"></a></p>
<div data-snippet-clipboard-copy-content="curl -fsSL https://vinceanalytics.com/install.sh | bash"><pre><code>curl -fsSL https://vinceanalytics.com/install.sh | bash
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Docker</h3><a id="user-content-docker" aria-label="Permalink: Docker" href="#docker"></a></p>
<div data-snippet-clipboard-copy-content="docker pull ghcr.io/vinceanalytics/vince"><pre><code>docker pull ghcr.io/vinceanalytics/vince
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Helm</h3><a id="user-content-helm" aria-label="Permalink: Helm" href="#helm"></a></p>
<div data-snippet-clipboard-copy-content="❯ helm repo add vince http://vinceanalytics.com/charts
❯ helm install vince vince/vince"><pre><code>❯ helm repo add vince http://vinceanalytics.com/charts
❯ helm install vince vince/vince
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Download</h3><a id="user-content-download" aria-label="Permalink: Download" href="#download"></a></p>
<p dir="auto"><a href="https://github.com/vinceanalytics/vince/releases">see release page</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Checking installation</h2><a id="user-content-checking-installation" aria-label="Permalink: Checking installation" href="#checking-installation"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Start vince</h2><a id="user-content-start-vince" aria-label="Permalink: Start vince" href="#start-vince"></a></p>
<p dir="auto"><em><strong>create admin</strong></em></p>
<div data-snippet-clipboard-copy-content="❯ vince admin --name acme --password 1234"><pre><code>❯ vince admin --name acme --password 1234
</code></pre></div>
<p dir="auto"><em><strong>start server</strong></em></p>
<div data-snippet-clipboard-copy-content="❯ vince serve                            
2024/10/23 15:32:08 [JOB 1] WAL file vince-data/pebble/000002.log with log number 000002 stopped reading at offset: 124; replayed 1 keys in 1 batches
2024/10/23 15:32:08 INFO starting event processing loop
2024/10/23 15:32:08 INFO starting server addr=:8080"><pre><code>❯ vince serve                            
2024/10/23 15:32:08 [JOB 1] WAL file vince-data/pebble/000002.log with log number 000002 stopped reading at offset: 124; replayed 1 keys in 1 batches
2024/10/23 15:32:08 INFO starting event processing loop
2024/10/23 15:32:08 INFO starting server addr=:8080
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Comparison with Plausible Analytics</h2><a id="user-content-comparison-with-plausible-analytics" aria-label="Permalink: Comparison with Plausible Analytics" href="#comparison-with-plausible-analytics"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>feature</th>
<th>vince</th>
<th>plausible</th>
</tr>
</thead>
<tbody>
<tr>
<td>Entrerprise features</td>
<td>❌</td>
<td>✅</td>
</tr>
<tr>
<td>Hosted offering</td>
<td>❌</td>
<td>✅</td>
</tr>
<tr>
<td>Multi tenant</td>
<td>❌</td>
<td>✅</td>
</tr>
<tr>
<td>Funnels</td>
<td>❌</td>
<td>✅</td>
</tr>
<tr>
<td>Goals Conversion</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Unique visitors</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Total visits</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Page views</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Views per visit</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Visit duration</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Breakdown by <strong>Cities</strong>, <strong>Sources</strong>, <strong>Pages</strong> and <strong>Devices</strong></td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Self Hosted</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>&lt;1KB script</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>No Cookies(GDPR, PECR compliant)</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>100% data ownershiip</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Unique shared access to stats</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Outbound links tracking</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>File download tracking</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>404 page tracking</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Time period comparisons</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Unlimited sites</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr>
<td>Unlimited events</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr>
<td>Zero dependency</td>
<td>✅</td>
<td>❌ (needs elixir, clickhouse, postgresql ...etc)</td>
</tr>
<tr>
<td>Automatic TLS</td>
<td>✅</td>
<td>❌</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Credit</h2><a id="user-content-credit" aria-label="Permalink: Credit" href="#credit"></a></p>
<p dir="auto"><a href="https://github.com/plausible/analytics">Plausible Analytics</a> : <code>vince</code> started as a Go port of plausible with a focus on self hosting.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How much memory do you need in 2024 to run 1M concurrent tasks? (251 pts)]]></title>
            <link>https://hez2010.github.io/async-runtimes-benchmarks-2024/</link>
            <guid>42270378</guid>
            <pubDate>Fri, 29 Nov 2024 02:50:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hez2010.github.io/async-runtimes-benchmarks-2024/">https://hez2010.github.io/async-runtimes-benchmarks-2024/</a>, See on <a href="https://news.ycombinator.com/item?id=42270378">Hacker News</a></p>
Couldn't get https://hez2010.github.io/async-runtimes-benchmarks-2024/: Error: timeout of 10000ms exceeded]]></description>
        </item>
    </channel>
</rss>