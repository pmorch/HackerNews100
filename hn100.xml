<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 07 Jun 2025 11:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Low-Level Optimization with Zig (105 pts)]]></title>
            <link>https://alloc.dev/2025/06/07/zig_optimization</link>
            <guid>44208060</guid>
            <pubDate>Sat, 07 Jun 2025 07:26:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alloc.dev/2025/06/07/zig_optimization">https://alloc.dev/2025/06/07/zig_optimization</a>, See on <a href="https://news.ycombinator.com/item?id=44208060">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <div>
            <h2> 2025-06-07 # Optimizations with Zig </h2>
            <p>
                "Beware of the <a href="https://en.wikipedia.org/wiki/Turing_tarpit" target="_blank">Turing tar-pit</a>
                in which everything is possibile, but nothing of interest is easy." - Alan Perlis 1982
            </p>
            <p>
                What is of interest to you? Many things, I am certain. One such topic that I am constantly intrigued by
                is program optimization. Whether you are looking to <a href="https://youtu.be/KzT9I1d-LlQ" target="_blank">compute the largest fibonacci number</a> in one second, or creating the <a href="https://tigerbeetle.com/" target="_blank">fastest financial transaction database</a> ever
                written, or even just <a href="https://gaultier.github.io/blog/lessons_learned_from_a_successful_rust_rewrite.html" target="_blank">rewriting something in rust</a>, you likely know how rewarding optimization can be.
            </p>
            <p>
                Optimization separates the weak from the fast. Optimization isn't a thing of the past. Advances in
                technology shape the form of our programs, but don't negate the need to optimize. Well optimized
                programs save money, enable higher-tier scaling opportunities, and preserve system simplicity. Would you
                rather spend thousands to run shoddy code on autoscaling cloud infrastructure, or write better code,
                letting you use a handful of mid-tier servers at reduced latency and cost?
            </p>
            <p>
                In this article I aim to explain the concept of low-level optimization, and why Zig is particularly well
                suited for it. If you enjoy what you read, please consider <a href="https://www.paypal.com/donate/?business=2Z3H3UQA37LML&amp;no_recurring=0&amp;item_name=Like+what+you+see?+A+dollar+or+two+will+help+me+learn+and+publish+more+-+Thanks!&amp;currency_code=USD" target="_blank">supporting me</a> :)
            </p>
            <p><img src="https://alloc.dev/2025/06/07/servers.webp" width="100%">
        </p></div>

        <div id="trust_compiler">
            <h2> Trust the compiler? </h2>
            <p>
                Some people would say "trust the compiler, it knows best." It sure does, for most low-level situations!
                Optimizing compilers have come a long ways. Increased system resources and <a href="https://blog.vortan.dev/ematching" target="_blank">advances in IR transformations</a> have
                enabled compiler backends like <a href="https://github.com/llvm/llvm-project" target="_blank">LLVM</a>
                to deliver <a href="https://blog.matthieud.me/2020/exploring-clang-llvm-optimization-on-programming-horror/" target="_blank">impressive results</a>.
            </p>
            <p>
                Compilers are complicated beasts. The best optimizing backends will still <a href="https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:zig,selection:(endColumn:2,endLineNumber:7,positionColumn:2,positionLineNumber:7,selectionStartColumn:2,selectionStartLineNumber:7,startColumn:2,startLineNumber:7),source:'const+std+%3D+@import(%22std%22)%3B%0A%0Aexport+fn+makeArray()+%5B*%5Du8+%7B%0A++++var+list+%3D+std.ArrayListUnmanaged(u8).initCapacity(std.heap.c_allocator,+1)+catch+unreachable%3B%0A++++list.appendAssumeCapacity(123)%3B%0A++++return+list.items.ptr%3B%0A%7D'),l:'5',n:'0',o:'Zig+source+%231',t:'0')),k:50.12184508268059,l:'4',m:100,n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:z0141,filters:(b:'0',binary:'1',binaryObject:'1',commentOnly:'0',debugCalls:'1',demangle:'0',directives:'0',execute:'1',intel:'0',libraryCode:'0',trim:'1',verboseDemangling:'0'),flagsViewOpen:'1',fontScale:14,fontUsePx:'0',j:1,lang:zig,libs:!(),options:'-O+ReleaseFast+-fomit-frame-pointer+-mcpu+x86_64_v3+-lc',overrides:!(),selection:(endColumn:1,endLineNumber:1,positionColumn:1,positionLineNumber:1,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:1),l:'5',n:'0',o:'+zig+0.14.1+(Editor+%231)',t:'0')),k:49.878154917319414,l:'4',m:100,n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4" target="_blank">generate sub</a>-<a href="https://godbolt.org/#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1AAvPMFJL6yAngGVG6AMKpaAVxYMJAdlIOAMngMmABy7gBGmMR6AA6oCoS2DM5uHt6ksfE2AgFBoSwRUVwWmFZZDEIETMQEye6eXD6WmNaJFVUEOSHhkXoKldW1qQ0W/R2BXfk9XACUFqiuxMjsHAD0KwDUAOoIAJ7reArrhuuYqtG0eMiE6zQMhJjrfUwED0yuRCzPAuuo0TYseGMmHQ6wiBBexFBez8fgAagBZACkAGYAGKIjQAQQxmNOsWqNwY63oTGiAE1MFVMRAdpSoutXMiAEzTUGoFzrRFeABCOPW/JuCiYIEeAHdCMgEOsIAABJgKCCMpmkdYaaasrm8rECnWqznIgAiKOwnJ5fN1FrwVGltKqnKZAFZ1pJ9UbDaqNWbtRaffy0IJAq4HiAqEL1lwUVrMb6dVyDSdaEpTVGYz7/TYGEH1iGw5JI%2BbU3GC7q46RizquK7jcnyz6rTa6fanVwNBoq%2B61TXvamdenA8HQ0x1kz892Y3GE0nNbXU33MwOw8jR9Ge0Wx7GvAay%2BuBUz2ybpzvLdaaY3EY7na32/HO4eVz2BXOszmh0vkSnV5vJw87w/e7J%2B2zQdnWXB813vEtN23CCdWRfd1mITACAWIkCGIINoN9F0UTdE1EOQ4giSoMQlEwn0SinQ1q1cBhEKYSUmDCegyP5cC2KxPESAIQliUpck6W5U8qhFJVWTCdlaC7CD6yEyFzydbCqI7T0P2PBs7Xk8MrxwnCPSkh8ZNtOSL0kK8wDAXTby9GCe3wlCbhIzBQM/I0j1NVybP5OzCPWNCg2c1jN2LbyiMc5c4w4WZaE4B1eE8DgtFIVBOAALTMR55kWH8mWRHhSAITRItmABrEBJAATgAOkkAA2DRkQ0JkvHK8qauRZFJGkaKOEkOLCqSzheAUEANHywrZjgWAkDQFhojoSJyEoGa5voKJjA0KQihoWgIWGiAwn6sJAiqHZODyo7mGIHYAHkwm0ZoCu4XgZrYQRroYWhToS3gsDCVxgEcMREzOn7ME%2BIxxG%2B0h8EQloADdMGGqHTmad5ljywIXm6xKLjCYgTucLB%2BrQvAWBB0gEeIcSlANMHDGAC4jHGvgDGABRYTwTBRWu6JGHJ/hBBEMR2CkGRBEUFR1Ch3QigMJnTHMXHhsgWZfjKJGAFproAJRKSklCET5aEkjX%2BABAhTfxtgNdiTHIl4VBKeIPAsGViBZiaFo7AgBxBnqXwGHQTo8gKGI4gSAQ/bDzJEmD7pCmKUpWlGKOik9so2mqOPJgTp4BhcOpelGbPQ5mOYFiWCQopivqoeSjh1gV1VKqkFvpVwQgSHtXLpl4R6tHVUhSodDRKodZFWvKrwmVbZEapqh0fG63rSHixL66Gkaxu%2BwfuqZWv18G7eB9mSn4jsSQgA%3D" target="_blank">par code</a> in some cases. In fact, even
                state-of-art compilers will <a href="https://godbolt.org/#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1AB9U8lJL6yAngGVG6AMKpaAVxYMQAJlIOAMngMmABy7gBGmMQSPgAOqAqEtgzObh7epHEJNgIBQaEsEVFcPpaY1klCBEzEBCnuniWYVtkMldUEuSHhkdEWVTV1aSX9HYFdBT3FAJQWqK7EyOwcAKReAMyByG5YANTLa44KBPiCAHQI%2B9jLGgCC1zcAbqh46Du0qKgxAOKoAELEAOIEFcgQIay8xgIO1UUz2AHZfvcdsidiDBODITs8Hs1gARaH7RG3FE7ADuCDomB2EGx%2B0c%2B3xMPhRJuJJJAHp2TsAJJUaFYhQ7YDETBMAiRHYEBCGHYaVRUBWKqgAVlIOyY6BOwB2XA0ktQWLJc1orzEpKYAE9BSKFK5aAQkWzkZysQx1TsFCwxPRiDsHmJXFT0IH9TtUA9IlR3qSdgBaHY2u02Iyu9VuwI0BiEKnvT6nR1Ol2OMAcQUKGJlPA0UQtdW0aOC8OR6Nht2uBgJYBBV6gzDASIKNUKA1bQzavAKAtsl0PWwGZPaqVUxweivIHYISKYfPEp2G1aIvE6jSEqfLOG4%2B7n3EcGa0TjK3ieDhaUioTgr4fzRZ7dY8UgEJot4zJuGo9BAMwANYgMqGj6JwkhPkBb6cLwCggHBgEvrepBwLAMCICgqAsDElJkBQEBoCRZEoMAXDKlwfB0OKxDoRAYTIWEgTVBanD/lxzDEBaADyYTaGUWH/lRbCCMJDC0Lx2GkFgYSuMAjjeuh3C8FgXpGOISn4CK5QRlpr6YKoZSuOKfG8L295KbQeBhMQPHOFgyEEMQeAsLZpARsQYTxJguKYHpwBOUYQEzFGTDAAoABqeCYKSwkVs%2B/78IIIhiOwUgyIIigqOoSm6IxBhRaYxjmE5YToZAMyfC0WmxsJay8E2AIvJg9UQRYTQSUk9gME4Lj1Ho/hjPkhR6JkiQCIMniMXNLSdNNkz9c0FQjIteilOUAhtDUa3dEUfTtLtjFHO0J0TEUMxfgseV3g%2BSFKe%2BHDQgAHAAbLGP2SEKyDrvRpxcNSuCECQv5rFwUy8FhWhTCBopYFEfXQbB8EcIhpC%2BVjz6vh9aEYQB0W4QREBIFRpE%2BuQlA0zRwMVcAXgaLqTH2gOlAcUpAk8X5/NCaJ4nWH50mMAQckKchKlqRp9Zaf%2BuljgZr5GYNpnIRZVk2dp5CCE0yG1a5QnuUsr5eT5fkBUFSiheFkWgNhMUGPFSUpWljB%2BVlwiiOI%2BW%2B0VajIboPgsyAVU1c5vWNTEzWcK17VvgF3lYLHm2DXYEAOJdvgjbdM3LfE83JGNaTF1kSSFxt%2B0tEdtTl0tmcHa0Iw12d10DE3e3t1Np0SA9cxPYP2OPqQhMdZw31/QDOyjimbNg6ceoQJDRC%2BqssPw2TLso2B6MvTjvD43Bk8oRwJOYdFUEwXBDnJ%2BfxO70jR9eG9ROoS/wH%2BQOQ2SEAA" target="_blank">break language
                    specifications</a> (<a href="https://github.com/llvm/llvm-project/issues/143046" target="_blank">Clang assumes</a> that all loops without side effects will terminate). It is up to
                us to give our compilers as much information as possible, and verify that the compiler is functioning
                correctly. In the case of most low-level languages, you can generally massage your code until the
                compiler realizes it can apply a certain transform. In other situations, it's not so easy.
            </p>
            <p>
                Why are low-level languages generally more performant? You might think the reason is that high level
                languages are doing a lot of extra work, such as garbage collection, string interning, interpreting
                code, etc. While you are right, this isn't *entirely* complete. High level languages lack something that
                low level languages have in great adundance - intent.
            </p>
            <p>
                The verbosity of low level programming languages enable us to produce code that the compiler can reason
                about very well. As an example, consider the following JavaScript code:
            </p>
            <pre><code>function maxArray(x, y) {
    for (let i = 0; i &lt; 65536; i++) {
        x[i] = y[i] &gt; x[i] ? y[i] : x[i];
    }
}</code></pre>
            <p>
                As humans, we can interpret this code as setting the values in <code>x</code> as the maximum of
                <code>x</code> and <code>y</code>. The <a href="https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:javascript,selection:(endColumn:51,endLineNumber:15,positionColumn:51,positionLineNumber:15,selectionStartColumn:51,selectionStartLineNumber:15,startColumn:51,startLineNumber:15),source:'function+maxArray(x,+y)+%7B%0A++++for+(let+i+%3D+0%3B+i+%3C+65536%3B+i%2B%2B)+%7B%0A++++++++x%5Bi%5D+%3D+y%5Bi%5D+%3E+x%5Bi%5D+%3F+y%5Bi%5D+:+x%5Bi%5D%3B%0A++++%7D%0A%7D%0A%0Alet+arrA+%3D+new+Float64Array(65536).fill(1)%3B+//+all+1s%0Alet+arrB+%3D+new+Float64Array(65536).fill(2)%3B+//+all+2s%0A%0A//+Prepare+and+warm+up+the+function%0A%25PrepareFunctionForOptimization(maxArray)%3B%0AmaxArray(arrA,+arrB)%3B++++++//+Fill+type+feedback%0AmaxArray(arrA,+arrB)%3B++++++//+Stabilize+into+monomorphic%0A%25OptimizeFunctionOnNextCall(maxArray)%3B%0AmaxArray(arrA,+arrB)%3B++++++//+Trigger+optimization'),l:'5',n:'0',o:'Javascript+source+%231',t:'0')),k:49.48081530907177,l:'4',m:100,n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:v8trunk,filters:(b:'0',binary:'1',binaryObject:'1',commentOnly:'0',debugCalls:'1',demangle:'0',directives:'0',execute:'1',intel:'0',libraryCode:'0',trim:'1',verboseDemangling:'0'),flagsViewOpen:'1',fontScale:14,fontUsePx:'0',j:1,lang:javascript,libs:!(),options:'',overrides:!(),selection:(endColumn:1,endLineNumber:1,positionColumn:1,positionLineNumber:1,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:1),l:'5',n:'0',o:'+v8+(trunk)+(Editor+%231)',t:'0')),header:(),k:50.51918469092824,l:'4',m:100,n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4" target="_blank">generated bytecode</a> for this JavaScript (under V8) is pretty bloated. As a
                comparison, here is what the function might look like if it were written in Zig:
            </p>
            <pre><code>fn maxArray(
    noalias x: *align(64) [65536]f64,
    y: *align(64) const [65536]f64,
) void {
    for (x, y, 0..) |a, b, i| {
        x[i] = if (b &gt; a) b else a;
    }
}</code></pre>
            <p>
                In this more verbose language, we can tell the compiler additional information about our code. For
                example, the optimizer now knows about alignment, aliasing requirements, array sizes, and array element
                types - all at compile-time. Using this information, the compiler generates <a href="https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:zig,selection:(endColumn:2,endLineNumber:8,positionColumn:2,positionLineNumber:8,selectionStartColumn:2,selectionStartLineNumber:8,startColumn:2,startLineNumber:8),source:'export+fn+maxArray(%0A++++noalias+x:+*align(64)+%5B65536%5Df64,%0A++++y:+*align(64)+const+%5B65536%5Df64,%0A)+void+%7B%0A++++for+(x,+y,+0..)+%7Ca,+b,+i%7C+%7B%0A++++++++x%5Bi%5D+%3D+if+(b+%3E+a)+b+else+a%3B%0A++++%7D%0A%7D'),l:'5',n:'0',o:'Zig+source+%231',t:'0')),k:54.464285714285715,l:'4',m:100,n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:z0141,filters:(b:'0',binary:'1',binaryObject:'1',commentOnly:'0',debugCalls:'1',demangle:'0',directives:'0',execute:'1',intel:'0',libraryCode:'0',trim:'1',verboseDemangling:'0'),flagsViewOpen:'1',fontScale:14,fontUsePx:'0',j:1,lang:zig,libs:!(),options:'-OReleaseFast+-mcpu%3Dznver5+-fomit-frame-pointer',overrides:!(),selection:(endColumn:1,endLineNumber:1,positionColumn:1,positionLineNumber:1,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:1),l:'5',n:'0',o:'+zig+0.14.1+(Editor+%231)',t:'0')),header:(),k:45.5357142857143,l:'4',m:100,n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4" target="_blank">far superior, even vectorized</a> code.
                If you were wondering, <a href="https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:rust,selection:(endColumn:17,endLineNumber:10,positionColumn:17,positionLineNumber:10,selectionStartColumn:17,selectionStartLineNumber:10,startColumn:17,startLineNumber:10),source:'%23%5Brepr(align(64))%5D%0Apub+struct+Aligned%3CT:+%3FSized%3E(T)%3B%0A%0Apub+fn+max_array(x:+%26mut+Aligned%3C%5Bf64%3B+65536%5D%3E,+y:+%26Aligned%3C%5Bf64%3B+65536%5D%3E)+%7B%0A++++for+(x,+y)+in+x.0.iter_mut().zip(y.0.iter())+%7B%0A++++++++*x+%3D+if+*y+%3E+*x+%7B+*y+%7D+else+%7B+*x+%7D%3B%0A++++%7D%0A%7D%0A%0Apub+fn+main()+%7B%7D'),l:'5',n:'0',o:'Rust+source+%231',t:'0')),k:54.464285714285715,l:'4',m:100,n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:r1870,filters:(b:'0',binary:'1',binaryObject:'1',commentOnly:'0',debugCalls:'1',demangle:'0',directives:'0',execute:'1',intel:'0',libraryCode:'0',trim:'1',verboseDemangling:'0'),flagsViewOpen:'1',fontScale:14,fontUsePx:'0',j:1,lang:rust,libs:!(),options:'-C+opt-level%3D3+-C+target-cpu%3Dznver5',overrides:!(),selection:(endColumn:1,endLineNumber:1,positionColumn:1,positionLineNumber:1,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:1),l:'5',n:'0',o:'+rustc+1.87.0+(Editor+%231)',t:'0')),header:(),k:45.5357142857143,l:'4',m:100,n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4" target="_blank">equivalent Rust
                    code</a> generates near identical assembly.
            </p>
            <p>
                So can we *really* trust our compilers? It depends. Are you looking to uncover transformations to triple
                the throughput of your program's performance bottleneck? You should probably look at what the compiler
                is doing, and figure out if there are better ways to express *your intent* to the compiler. You may need
                to tweak the code to get the transforms you want. In the worst cases, you may discover that the compiler
                isn't applying optimal transforms to your code. In these cases, you may need to write inline assembly to
                squeeze out that last drop.
            </p>
            <p>
                But what about high level code? Except for <a href="https://spiral.net/" target="_blank">niche
                    cases</a>, the most we can do is <a href="https://github.com/llvm/llvm-project/tree/main/polly" target="_blank">reason about loops</a>. In other words, compilers cannot change our algorithms and
                optimize our paradigms. Their scope is relatively narrow.
            </p>
        </div>

        <div id="what_about_zig">
            <h2> So where does Zig fall? </h2>
            <p>
                I love Zig for it's verbosity. Due to this verbosity, it's easier to write more performant programs than
                with most other languages. With Zig's <a href="https://ziglang.org/documentation/master/#Builtin-Functions" target="_blank">builtin
                    functions</a>, non-optional <a href="https://ziglang.org/documentation/master/#Pointers" target="_blank">pointers</a>, <a href="https://ziglang.org/documentation/master/#unreachable" target="_blank"><code>unreachable</code> keyword</a>, well chosen <a href="https://ziglang.org/documentation/master/#Illegal-Behavior" target="_blank">illegal
                    behavior</a>, Zig's excellent <a href="https://ziglang.org/documentation/master/#comptime" target="_blank">comptime</a>... LLVM is practically spoon-fed information about our code.
            </p>
            <p>
                It isn't all rainbows though, there are tradeoffs. For example, Rust's memory model allows the compiler
                to always assume that function arguments never alias. You must manually specify this in Zig. If the
                compiler can't tell that your Zig function is always called with non-aliasing arguments, Rust
                functions will outperform the non-annotated Zig functions.
            </p>
            <p>
                If we take in well-annotated LLVM IR as the *only* metric of a language's optimization capability, then
                Zig does well. This is not all that Zig has up it's sleeves. Zig's true optimization superpower lies in
                compile-time execution.
            </p>
            <p><img src="https://alloc.dev/2025/06/07/gotta_go_fast.webp" alt="speedily (poorly) drawn lizard with a space helmet" width="100%">
        </p></div>

        <div id="what_is_comptime">
            <h2> What is <code>comptime</code>? </h2>
            <p>
                <a href="https://ziglang.org/documentation/master/#comptime" target="_blank">Zig's
                    <code>comptime</code></a> is all about code generation. Do you want to use a constant in your code?
                You can generate it at compile-time and the value will be embedded in the produced binary. Do you want
                to avoid writing the same <a href="https://en.wikipedia.org/wiki/Hash_table" target="_blank">hashmap
                    structure</a> for each type of data it can store? <code>comptime</code> has your back. Do you have
                data which is known at compile-time, and want the optimizer to elide code using this data? Yes, <a href="https://www.scottredig.com/blog/bonkers_comptime/" target="_blank">you can do this</a> with
                <code>comptime</code>. Zig's <code>comptime</code> is an example of <a href="https://en.wikipedia.org/wiki/Metaprogramming">metaprogramming</a>.
            </p>
            <p>
                So how is this different from macros? It's pretty nuanced. The purpose of <code>comptime</code> is
                essentially the same as macros. Some macros will modify the raw text of your code, and others can modify
                your program's <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree" target="_blank">AST</a>
                directly. This allows macros to inline code specific to the types and values of data in your program. In
                Zig, <code>comptime</code> code is just regular code running at compile-time. This code cannot have
                side-effects - such as network IO - and the emulated machine will match the compilation target.
            </p>
            <p>
                The reason that Zig's <code>comptime</code> can match up so well to macros is twofold. Firstly, almost
                all Zig code can be run at compile-time, using <code>comptime</code>. Secondly, at compile-time, all
                types can be inspected, reflected, and generated. This is <a href="https://ziglang.org/documentation/0.14.1/#Generic-Data-Structures" target="_blank">how
                    generics are implemented</a> in Zig.
            </p>
            <p>
                The flexibility of Zig's <code>comptime</code> has resulted in some rather nice improvements in other
                programming languages. For example, Rust has the <a href="https://docs.rs/crabtime/latest/crabtime/" target="_blank">"crabtime"</a> crate, which provides more flexibility and power than standard Rust
                macros. I believe a benefit of <code>comptime</code> over current alternatives lies in how seamlessly
                <code>comptime</code> fits into the Zig language. Unlike C++'s <code>constexpr</code>, you can use
                <code>comptime</code> without needing to learn a new "language" of sorts. C++ <a href="https://youtu.be/bIc5ZxFL198" target="_blank">is improving</a>, but it has a long ways to go
                if it hopes to compete with Zig in this domain.
            </p>
            <p>
                So can Zig's <code>comptime</code> do *everything* macros can? Nope. Token-pasting macros don't have a
                mirror in Zig's comptime. Zig is designed to be easy to read, and macros which modify or create
                variables in a unrelated scopes just don't cut it. Macros can define other macros, macros can alter the
                AST, and macros can implement mini-languages, or DSLs. Zig's <code>comptime</code> can't directly alter
                the AST. If you really want to, you can implement a DSL in Zig. For example, Zig's print function <a href="https://ziglang.org/documentation/master/#Case-Study-print-in-Zig" target="_blank">relies on
                    <code>comptime</code></a> to parse the format string. The print function's format string is a DSL of
                sorts. Based on the format string, Zig's <code>comptime</code> will construct a graph of functions to
                serialize your data. Here are some other examples of <code>comptime</code> DSLs in the wild: the <a href="https://github.com/tigerbeetle/tigerbeetle/blob/main/src/state_machine.zig#L5840" target="_blank">TigerBeetle account testing DSL</a>, <a href="https://github.com/InKryption/comath" target="_blank">comath: comptime math</a>, and <a href="https://github.com/ymndoseijin/zilliam" target="_blank">zilliam</a>, a Geometric Algebra library.
            </p>
            <p>
                Here are more resources for learning about Zig's <code>comptime</code>:
            </p>
            <ul>
                <li> <a href="https://andrewkelley.me/post/string-matching-comptime-perfect-hashing-zig.html" target="_blank">
                        String Matching based on Compile Time Perfect Hashing in Zig - Andrew Kelley </a> </li>
                <li> <a href="https://kristoff.it/blog/what-is-zig-comptime/" target="_blank">
                        What is Zig's Comptime? - Loris Cro </a> </li>
                <li> <a href="https://matklad.github.io/2025/04/19/things-zig-comptime-wont-do.html" target="_blank">
                        Things Zig comptime Won’t Do - matklad </a> </li>
                <li> <a href="https://zig.news/edyu/wtf-is-zig-comptime-and-inline-257b" target="_blank">
                        Zig Comptime - WTF is Comptime (and Inline) - Ed Yu </a> </li>
                <li> <a href="https://ziglang.org/documentation/master/#comptime" target="_blank">
                        Comptime - Zig Language Reference </a> </li>
                <li> <a href="https://zig.guide/language-basics/comptime/" target="_blank">
                        Comptime - zig.guide </a> </li>
            </ul>
        </div>

        <div id="string_comparison">
            <h2> String Comparison with <code>comptime</code>: </h2>
            <p>
                How do you compare two strings? Here's an approach that works in any language:
            </p>
            <pre><code>function stringsAreEqual(a, b) {
    if (a.length !== b.length) return false;
    for (let i = 0; i &lt; a.length; i++)
        if (a[i] !== b[i]) return false;
    return true;
}</code></pre>
            <p>
                We know that two strings aren't equal if their lengths aren't equal. We also know that if one byte isn't
                equal, then the strings as a whole aren't equal. Pretty simple, right? Yes, and the <a href="https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:zig,selection:(endColumn:2,endLineNumber:14,positionColumn:2,positionLineNumber:14,selectionStartColumn:2,selectionStartLineNumber:14,startColumn:2,startLineNumber:14),source:'const+std+%3D+@import(%22std%22)%3B%0A%0Aexport+fn+isHelloWorldA(str:+%5B*%5Dconst+u8,+len:+usize)+bool+%7B%0A++++const+hello+%3D+%22Hello,+world!!%5Cn%22%3B%0A++++if+(len+!!%3D+hello.len)+return+false%3B%0A++++return+staticEqlA(hello.len,+hello.*,+str)%3B%0A%7D%0A%0Afn+staticEqlA(comptime+len:+usize,+comptime+a:+%5Blen%5Du8,+b:+%5B*%5Dconst+u8)+bool+%7B%0A++++for+(0..len)+%7Cidx%7C+%7B%0A++++++++if+(a%5Bidx%5D+!!%3D+b%5Bidx%5D)+return+false%3B%0A++++%7D%0A++++return+true%3B%0A%7D'),l:'5',n:'0',o:'Zig+source+%231',t:'0')),header:(),k:50,l:'4',m:100,n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:z0141,filters:(b:'0',binary:'1',binaryObject:'1',commentOnly:'0',debugCalls:'1',demangle:'0',directives:'0',execute:'1',intel:'0',libraryCode:'0',trim:'1',verboseDemangling:'0'),flagsViewOpen:'1',fontScale:14,fontUsePx:'0',j:1,lang:zig,libs:!(),options:'-OReleaseFast+-fomit-frame-pointer+-mcpu%3Dznver5+-target+x86_64-linux',overrides:!(),selection:(endColumn:28,endLineNumber:8,positionColumn:28,positionLineNumber:8,selectionStartColumn:28,selectionStartLineNumber:8,startColumn:28,startLineNumber:8),source:1),l:'5',n:'0',o:'+zig+0.14.1+(Editor+%231)',t:'0')),header:(),k:50,l:'4',n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4" target="_blank">generated assembly</a> for this function
                reflects that. There's a slight issue though. We need to load individual bytes from both strings,
                comparing them individually. It would be nice if there was some way to optimize this. We could use SIMD
                here, chunking the input strings and comparing them block by block, but we would still be loading from
                two separate strings. In most cases, we already know one of the strings at compile-time. Can we do
                better? Yes:
            </p>
            <pre><code>fn staticEql(comptime a: []const u8, b: []const u8) bool {
    if (a.len != b.len) return false;
    for (0..a.len) |idx| {
        if (a[idx] != b[idx]) return false;
    }
    return true;
}</code></pre>
            <p>
                The difference here is that one of the strings is required to be known at compile-time. The compiler can
                use this new information to <a href="https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:zig,selection:(endColumn:1,endLineNumber:4,positionColumn:1,positionLineNumber:4,selectionStartColumn:1,selectionStartLineNumber:4,startColumn:1,startLineNumber:4),source:'export+fn+isHello(str:+%5B*%5Dconst+u8,+len:+usize)+bool+%7B%0A++++return+staticEql(%22Hello!!%5Cn%22,+str%5B0..len%5D)%3B%0A%7D%0A%0Afn+staticEql(comptime+a:+%5B%5Dconst+u8,+b:+%5B%5Dconst+u8)+bool+%7B%0A++++if+(a.len+!!%3D+b.len)+return+false%3B%0A++++for+(0..a.len)+%7Cidx%7C+%7B%0A++++++++if+(a%5Bidx%5D+!!%3D+b%5Bidx%5D)+return+false%3B%0A++++%7D%0A++++return+true%3B%0A%7D'),l:'5',n:'0',o:'Zig+source+%231',t:'0')),header:(),k:48.0168776371308,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:z0141,filters:(b:'0',binary:'1',binaryObject:'1',commentOnly:'0',debugCalls:'1',demangle:'0',directives:'0',execute:'1',intel:'0',libraryCode:'0',trim:'1',verboseDemangling:'0'),flagsViewOpen:'1',fontScale:14,fontUsePx:'0',j:1,lang:zig,libs:!(),options:'-OReleaseFast+-fomit-frame-pointer+-mcpu%3Dznver5+-target+x86_64-linux',overrides:!(),selection:(endColumn:1,endLineNumber:1,positionColumn:1,positionLineNumber:1,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:1),l:'5',n:'0',o:'+zig+0.14.1+(Editor+%231)',t:'0'),(h:output,i:(compilerName:'zig+trunk',editorid:1,fontScale:14,fontUsePx:'0',j:1,wrap:'1'),l:'5',n:'0',o:'Output+of+zig+0.14.1+(Compiler+%231)',t:'0')),header:(),k:51.983122362869196,l:'4',m:100,n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4" target="_blank">produce
                    improved</a> assembly code:
            </p>
            <pre><code>isHello:
        cmp     rsi, 7
        jne     .LBB0_8
        cmp     byte ptr [rdi], 72
        jne     .LBB0_8
        cmp     byte ptr [rdi + 1], 101
        jne     .LBB0_8
        cmp     byte ptr [rdi + 2], 108
        jne     .LBB0_8
        cmp     byte ptr [rdi + 3], 108
        jne     .LBB0_8
        cmp     byte ptr [rdi + 4], 111
        jne     .LBB0_8
        cmp     byte ptr [rdi + 5], 33
        jne     .LBB0_8
        cmp     byte ptr [rdi + 6], 10
        sete    al
        ret
.LBB0_8:
        xor     eax, eax
        ret</code></pre>
            <p>
                Is this not amazing? We just used <code>comptime</code> to make a function which compares a string
                against <code>"Hello!\n"</code>, and the assembly will run much faster than the naive comparison
                function. It's unfortunately still not perfect. Because we know the length of the expected string at
                compile-time, we can compare much larger sections of text at a time, instead of just byte-by-byte:
            </p>
            <pre><code>const std = @import("std");

fn staticEql(comptime a: []const u8, b: []const u8) bool {
    const block_len = std.simd.suggestVectorLength(u8) orelse @sizeOf(usize);

    // Exit early if the string lengths don't match up
    if (a.len != b.len) return false;

    // Find out how many large "blocks" we can compare at a time
    const block_count = a.len / block_len;
    // Find out how many extra bytes we need to compare
    const rem_count = a.len % block_len;

    // Compare "block_len" bytes of text at a time
    for (0..block_count) |idx| {
        const Chunk = std.meta.Int(.unsigned, block_len * 8);
        const a_chunk: Chunk = @bitCast(a[idx * block_len ..][0..block_len].*);
        const b_chunk: Chunk = @bitCast(b[idx * block_len ..][0..block_len].*);
        if (a_chunk != b_chunk) return false;
    }

    // Compare the remainder of bytes in both strings
    const Rem = std.meta.Int(.unsigned, rem_count * 8);
    const a_rem: Rem = @bitCast(a[block_count * block_len ..][0..rem_count].*);
    const b_rem: Rem = @bitCast(b[block_count * block_len ..][0..rem_count].*);
    return a_rem == b_rem;
}</code></pre>
            <p>
                Ok, so it's a bit more complex than the first example. Is it worth it though? Yep. The <a href="https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:zig,selection:(endColumn:1,endLineNumber:6,positionColumn:1,positionLineNumber:6,selectionStartColumn:1,selectionStartLineNumber:6,startColumn:1,startLineNumber:6),source:'const+std+%3D+@import(%22std%22)%3B%0A%0Aexport+fn+isHelloWorld(str:+%5B*%5Dconst+u8,+len:+usize)+bool+%7B%0A++++return+staticEql(%22Hello,+World!!%5Cn%22,+str%5B0..len%5D)%3B%0A%7D%0A%0Afn+staticEql(comptime+a:+%5B%5Dconst+u8,+b:+%5B%5Dconst+u8)+bool+%7B%0A++++const+block_len+%3D+std.simd.suggestVectorLength(u8)+orelse+@sizeOf(usize)%3B%0A%0A++++//+Exit+early+if+the+string+lengths+don!'t+match+up%0A++++if+(a.len+!!%3D+b.len)+return+false%3B%0A%0A++++//+Find+out+how+many+large+%22blocks%22+we+can+compare+at+a+time%0A++++const+block_count+%3D+a.len+/+block_len%3B%0A++++//+Find+out+how+many+extra+bytes+we+need+to+compare%0A++++const+rem_count+%3D+a.len+%25+block_len%3B%0A%0A++++//+Compare+%22block_len%22+bytes+of+text+at+a+time%0A++++for+(0..block_count)+%7Cidx%7C+%7B%0A++++++++const+Chunk+%3D+std.meta.Int(.unsigned,+block_len+*+8)%3B%0A++++++++const+a_chunk:+Chunk+%3D+@bitCast(a%5Bidx+*+block_len+..%5D%5B0..block_len%5D.*)%3B%0A++++++++const+b_chunk:+Chunk+%3D+@bitCast(b%5Bidx+*+block_len+..%5D%5B0..block_len%5D.*)%3B%0A++++++++if+(a_chunk+!!%3D+b_chunk)+return+false%3B%0A++++%7D%0A%0A++++//+Compare+the+remainder+of+bytes+in+both+strings%0A++++const+Rem+%3D+std.meta.Int(.unsigned,+rem_count+*+8)%3B%0A++++const+a_rem:+Rem+%3D+@bitCast(a%5Bblock_count+*+block_len+..%5D%5B0..rem_count%5D.*)%3B%0A++++const+b_rem:+Rem+%3D+@bitCast(b%5Bblock_count+*+block_len+..%5D%5B0..rem_count%5D.*)%3B%0A++++return+a_rem+%3D%3D+b_rem%3B%0A%7D'),l:'5',n:'0',o:'Zig+source+%231',t:'0')),header:(),k:56.046814044213264,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:z0141,filters:(b:'0',binary:'1',binaryObject:'1',commentOnly:'0',debugCalls:'1',demangle:'0',directives:'0',execute:'1',intel:'0',libraryCode:'0',trim:'1',verboseDemangling:'0'),flagsViewOpen:'1',fontScale:14,fontUsePx:'0',j:1,lang:zig,libs:!(),options:'-OReleaseFast+-fomit-frame-pointer+-mcpu%3Dznver5+-target+x86_64-linux',overrides:!(),selection:(endColumn:1,endLineNumber:1,positionColumn:1,positionLineNumber:1,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:1),l:'5',n:'0',o:'+zig+0.14.1+(Editor+%231)',t:'0'),(h:output,i:(compilerName:'zig+trunk',editorid:1,fontScale:14,fontUsePx:'0',j:1,wrap:'1'),l:'5',n:'0',o:'Output+of+zig+0.14.1+(Compiler+%231)',t:'0')),header:(),k:43.953185955786736,l:'4',m:100,n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4" target="_blank">generated assembly</a> is much more optimal. Comparing larger chunks utilizes
                larger registers, and reduces the number of conditional branches in our code:
            </p>
            <pre><code>isHelloWorld:
        cmp     rsi, 14 ; The length of "Hello, World!\n"
        jne     .LBB0_1
        movzx   ecx, word ptr [rdi + 12]
        mov     eax, dword ptr [rdi + 8]
        movabs  rdx, 11138535027311
        shl     rcx, 32 ; Don't compare out-of-bounds data
        or      rcx, rax
        movabs  rax, 6278066737626506568
        xor     rax, qword ptr [rdi]
        xor     rdx, rcx
        or      rdx, rax ; Both chunks must match
        sete    al
        ret
.LBB0_1:
        xor     eax, eax
        ret</code></pre>
            <p>
                If you try to compare much larger strings, you'll notice that this more advanced function will generate
                assembly which uses the larger SIMD registers. Just testing against <code>"Hello, World!\n"</code>
                though, you can tell that we <a href="https://gist.github.com/RetroDev256/660824008ff5526ea785d8b3659c29f2" target="_blank">significantly improved</a> the runtime performance of this function. (<code>a</code>
                was the runtime-only function, <code>b</code> was the same function where one argument was known at
                compile-time, and <code>c</code> was the more advanced function).
            </p>
            <p><img src="https://alloc.dev/2025/06/07/string_bench.webp" width="100%" alt="basic comptime was 45% faster while advanced comptime was 70% faster">
        </p></div>

        <div id="runtime_at_comptime">
            <h2> Runtime-known data at <code>comptime</code>: </h2>
            <p>
                Zig's <code>comptime</code> powers aren't limited to compile-time. You can generate some number of
                procedures at compile-time for simple cases, and dynamically dispatch to the right procedure,
                falling-back to a fully runtime implementation if you don't want to bloat your binary:
            </p>
            <pre><code>fn dispatchFn(runtime_val: u32) void {
    switch (runtime_val) {
        inline 0...100 =&gt; |comptime_val| {
            staticFn(comptime_val);
        },
        else =&gt; runtimeFn(runtime_val),
    }
}

fn staticFn(comptime val: u32) void {
    _ = val; // ...
}

fn runtimeFn(runtime_val: u32) void {
    _ = runtime_val; // ...
}</code></pre>
        </div>

        <div id="conclusion">
            <h2> Conclusion: </h2>
            <p>
                Is <code>comptime</code> useful? I would say so. I use it every time I write Zig code. It fits into the
                language really well, and removes the need for templates, macros, generics, and manual code generation.
                Yes, you can do all of this with other languages, but it isn't nearly as clean. Whenever I use Zig, I
                feel it's easier to write performant code for *actually useful* scenarios. In other words, Zig is not
                the "Turing tar-pit".
            </p>
            <p>
                The possibilities are <a href="https://github.com/RetroDev256/comptime_suffix_automaton" target="_blank">only limited to your imagination</a>. If you are required to use a language without
                good generics, code generation, templates, macros, or comptime at your workplace, I feel sorry for you.
            </p>
            <p>
                Hopefully you enjoyed this article. If you did, please consider <a href="https://www.paypal.com/donate/?business=2Z3H3UQA37LML&amp;no_recurring=0&amp;item_name=Like+what+you+see?+A+dollar+or+two+will+help+me+learn+and+publish+more+-+Thanks!&amp;currency_code=USD" target="_blank">supporting me</a>. On a final note, I think it's time for the language wars to end.
                Turing completeness is all that we need, and the details fade away when we look for the bigger picture.
                Does this mean we can't have favorite languages? No, it does not. People will still mistakenly say "C is
                faster than Python", when the language isn't what they are benchmarking. On that note, enjoy this Zig
                propaganda:
            </p>
            <iframe width="100%" src="https://www.youtube.com/embed/0Ahr2XWymPk" title="Zig Go Brrrrrr" frameborder="0" allowfullscreen=""></iframe>
        </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The FAIR Package Manager: Decentralized WordPress infrastructure (105 pts)]]></title>
            <link>https://joost.blog/path-forward-for-wordpress/</link>
            <guid>44207503</guid>
            <pubDate>Sat, 07 Jun 2025 04:45:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://joost.blog/path-forward-for-wordpress/">https://joost.blog/path-forward-for-wordpress/</a>, See on <a href="https://news.ycombinator.com/item?id=44207503">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Back in December, I wrote about the <a href="https://joost.blog/wordpress-leadership/">state of leadership in the WordPress ecosystem</a>. I shared how too much power rests with one person, and how the lack of clear governance puts contributors and businesses alike in difficult positions. That post ended with a call: <em>we need to lead</em>. That wasn’t rhetorical. It was a pivot.</p>



<p>Since then, a lot has happened. Today, I want to share what came next and where it’s going. This is the story of <strong>FAIR</strong>, a project I first named in that same blog post: <strong>Federated and Independent Repositories</strong>. At the time, it was just a placeholder for an idea. Now, it’s real, and it’s running.</p>



<h3 id="h-the-moment-the-ecosystem-shifted">The moment the ecosystem shifted</h3>



<p>In October, Sarah Savage <a href="https://aspirepress.org/a-vision-for-aspirepress-and-a-community-run-org-mirror/">published a post introducing AspirePress</a>, a community-run mirror of the WordPress.org plugin and theme repositories. That post was a spark. It showed what was technically possible, and voiced what many of us had been saying in smaller rooms for years: the ecosystem needed options.</p>



<p>Then, in early December, twenty core contributors <a href="https://www.therepository.email/wordpress-contributors-and-community-leaders-call-for-governance-reform-in-rare-open-letter">wrote an open letter</a> calling for governance reform in WordPress. These weren’t newcomers. They were committers, team leads, people who had spent years helping build the platform. Their message added weight to the concerns many had been feeling in private.</p>



<p>Shortly after, <a href="https://marucchi.com/wordpress-leadership-continued/">Karim Marucchi</a> and I published our thoughts. We proposed a dual-track forward: one that addressed technical issues like centralization, update bottlenecks, and discoverability, and one that introduced a different approach to governance, governance that would be transparent, accountable, and neutral.</p>



<p>That combination turned out to be a catalyst. Even more so after Matt <a href="https://wordpress.org/news/2025/01/jkpress/">blogged about it</a> with a sarcastic post on WordPress.org. People began reaching out, and conversations happening in parallel started to overlap.</p>



<h3 id="h-from-parallel-threads-to-shared-momentum">From parallel threads to shared momentum</h3>



<p>A number of different groups had been working on related problems, each bringing different perspectives, needs, and capabilities. Rather than announce a new umbrella, we focused on connecting the dots between these parallel efforts. We’re not naming everyone in that alignment publicly yet, but it’s safe to say: this <em>is</em> a group of groups, and that’s its strength.</p>



<p>What followed were weeks of collaboration. We mapped out which parts of the ecosystem needed reinforcement. Some things were urgent: plugin update services, the plugin and theme directories, static assets like emojis and avatars, and the dashboard feeds. We began with mirrors and drop-in replacements, but the plan was always broader than that.</p>



<p>We designed a system that could serve as a full distribution layer for WordPress. It would work with the core as it exists today, stay compatible while removing the bottlenecks created by centralization, and be governed independently of .org.</p>



<h3 id="h-fair-federated-independent-and-live">FAIR: federated, independent, and live</h3>



<p>FAIR is now a technical project under the Linux Foundation. Its technology is governed by a community-led Technical Steering Committee (TSC) and built by contributors from across the WordPress ecosystem. The TSC chose three amazing community leaders as their chairs: Carrie Dils, Mika Epstein and Ryan McCue. All together, they built, in a relatively short amount of time, a decentralized package management system, federation-ready mirrors, support for commercial plugins, cryptographic signing, and more.</p>



<p>The goal of FAIR is <strong>not</strong> to fork WordPress. We’re still using the same core software. We’re not building a separate platform. What we are doing is adding a new distribution layer and putting our own governance on top of it. It’s a new path within WordPress, not outside it.</p>



<p>You can still install WordPress from WordPress.org, and that won’t change. But if you want more control over how plugins are delivered, or a system that supports decentralization, FAIR gives you that choice.</p>



<p>This work builds on tools like Composer and package managers from the Linux world, but with a clear focus on usability for real WordPress users. Most people won’t even need to know how it works under the hood. They’ll just know it works.</p>



<h3 id="h-why-this-matters">Why this matters</h3>



<p>When I wrote about WordPress leadership, I meant it. Change in open source doesn’t happen from the outside. It happens when people show up, do the work, and offer a better option. That’s what we’ve done.</p>



<p>FAIR is not a protest. It’s not a fork. It is a <strong>contribution</strong>. It reflects our belief that WordPress deserves better infrastructure and more accountable governance, and that we can build that ourselves, together.</p>



<p>This project is the result of months of collaboration across companies, countries, and communities. Dozens and dozens of people have already worked on it, and more are joining every day.</p>



<p>You can learn more at <strong><a href="https://fair.pm/">fair.pm</a></strong>. There’s plenty of work still ahead, but the foundations are in place, and the path is open.</p>



<p>If you believe in the open web, in WordPress as shared infrastructure, and in a future where the people who contribute get a say in how the platform evolves: join us!</p>



<p>Other people we’ve been collaborating with have blogged as well:</p>



<ul>
<li><a href="https://marucchi.com/introducing-the-fair-package-manager-for-wordpress/">Karim Marucchi</a></li>



<li><a href="https://journal.rmccue.io/488/">Ryan McCue</a></li>



<li><a href="https://siobhanmckeown.com/a-way-forward-with-fair/">Siobhan McKeown</a></li>
</ul>



<p>Plus the <a href="https://www.linuxfoundation.org/press/linux-foundation-announces-the-fair-package-manager-project-for-open-source-content-management-system-stability">press release from the Linux Foundation</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Getting Past Procrastination (123 pts)]]></title>
            <link>https://spectrum.ieee.org/getting-past-procastination</link>
            <guid>44207095</guid>
            <pubDate>Sat, 07 Jun 2025 03:06:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/getting-past-procastination">https://spectrum.ieee.org/getting-past-procastination</a>, See on <a href="https://news.ycombinator.com/item?id=44207095">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="Getting Past Procastination"><p><em>This article is crossposted from </em><a href="https://spectrum.ieee.org/zaporizhzhia-nuclear-power-plant" target="_self">IEEE Spectrum</a><em>’s careers newsletter. <a href="https://engage.ieee.org/Career-Alert-Sign-Up.html" rel="noopener noreferrer" target="_blank"><em>Sign up now</em></a><em> to get insider tips, expert advice, and practical strategies, <em><em>written i<em>n partnership with tech career development company <a href="https://jointaro.com/" rel="noopener noreferrer" target="_blank">Taro</a> and </em></em></em>delivered to your inbox for free!</em></em></p><p>Across a decade working at hypergrowth tech companies like <a href="https://spectrum.ieee.org/tag/meta">Meta</a> and <a href="https://spectrum.ieee.org/tag/pinterest">Pinterest</a>, I constantly struggled with procrastination. I’d be assigned an important project, but I simply couldn’t get myself to get started. The source of my distraction varied—I would constantly check my email, read random documentation, or even scroll through my social feeds. But the result was the same: I felt a deep sense of dread that I was not making progress on the things that mattered.</p><p>At the end of the day, time is the only resource that matters. With every minute, you are making a decision about how to spend your life. Most of the ways people spend their time are ineffective. Especially in the tech world, our tasks and tools are constantly changing, so we must be able to adapt. What separates the best engineers from the rest of the pack is that they create systems that allow them to be consistently productive.</p><p>Here’s the core idea that changed my perspective on productivity: <strong>Action leads to motivation</strong>, not the other way around. You should not check your email or scroll <a href="https://spectrum.ieee.org/tag/instagram">Instagram</a> while you wait for motivation to “hit you.” Instead, just start doing something, anything, that makes progress toward your goal, and you’ll find that motivation will follow.</p><p>For example, if I have a high-priority, complex bug-fixing challenge at work, my approach is to decompose the problem into something much simpler. <em>Could I simply</em> add a log statement that prints the value of a relevant variable? My goal at this point is not to solve the bug, it’s simply to take a tiny step forward.</p><p>This creates a powerful <a href="https://spectrum.ieee.org/tag/flywheel">flywheel</a>: you’re productive → you feel good → you’re more productive.</p><p>Unfortunately, many engineers are stuck in the opposite flywheel, a downward spiral of procrastination: you’re unproductive → you feel bad → you’re unproductive.</p><p>The idea that motivation follows naturally from progress lets us lower the activation energy needed to enter the upward spiral. Author and motivational speaker Tony Robbins talks about a related concept that “motion creates emotion.” The actions we take, and even the way we move our body, affect how we feel. Once you realize that you can control your motivation, you can achieve stress-free productivity.</p><p>—Rahul</p><h2><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXRxzqsX3H7CzoAP8tlH0fjowYKuaOpFAGfQg611FSGQCPicCY_Zu77pP38Bq7HyWSAs=" target="_blank">Overcoming Tech Workforce Shortages With IEEE Microcredentials</a></h2><p><span>A shortage of technical workers is coming. Currently, most of these roles require university degrees, but specialized training through focused, skills-based microcredential courses could provide an alternative and expand the workforce. IEEE’s microcredentials program offers credentials that focus on the skills needed to become a technician, electrician, or programmer, regardless of educational background.</span></p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXRxzqsX3H7CzoAP8tlH0fjowYKuaOpFAGfQg611FSGQCPicCY_Zu77pP38Bq7HyWSAs=" target="_blank" title="Read more here.">Read more here.</a></p><h2><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXexwH67TVCzP_FrLeWBMKMZkkLIpLauUNIFTyCH7znsUKaiGLKVc-0hYeTdBLeD5zds=" target="_blank" title="How Software Engineers Actually Use AI">How Software Engineers Actually Use AI</a></h2><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXRxzqsX3H7CzoAP8tlH0fjowYKuaOpFAGfQg611FSGQCPicCY_Zu77pP38Bq7HyWSAs=" target="_blank" title="Read more here."></a><span>Amidst conflicting accounts of how <a href="https://spectrum.ieee.org/tag/programmers">programmers</a> use AI on the job, Wired surveyed 730 coders to get more clarity—then used <a href="https://spectrum.ieee.org/tag/chatgpt">ChatGPT</a> to comb through the data, with plenty of help from human editors and fact-checkers. The survey asked coders how much they use AI, their outlook on the technology, and how it has changed their jobs, among other questions.</span></p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXexwH67TVCzP_FrLeWBMKMZkkLIpLauUNIFTyCH7znsUKaiGLKVc-0hYeTdBLeD5zds=" target="_blank" title="Read more here.">Read more here.</a></p><h2><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXn-WPi12NZwyS58w9WNKrtyQ406RJlXcxcYHA9l4y1kUGMCWFQCXUYSqJI-5igxkdCY=" rel="noopener noreferrer" target="_blank" title="Profile: A Knee Injury Launched This VR Pioneer’s Career">Profile: A Knee Injury Launched This VR Pioneer’s Career</a></h2><p>Unlike many engineers, Carolina Cruz-Neira had little interest in technology as a child. Instead, she dreamed of becoming a professional ballerina. But when an injury forced her to pivot, Cruz-Neira found success in computer science, eventually blending her interests in art and science as a pioneer in <a href="https://spectrum.ieee.org/tag/virtual-reality">virtual reality</a>. </p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXn-WPi12NZwyS58w9WNKrtyQ406RJlXcxcYHA9l4y1kUGMCWFQCXUYSqJI-5igxkdCY=" rel="noopener noreferrer" target="_blank" title="Read more here.">Read more here.</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Read All of Cloudflare's Claude-Generated Commits (131 pts)]]></title>
            <link>https://www.maxemitchell.com/writings/i-read-all-of-cloudflares-claude-generated-commits/</link>
            <guid>44205697</guid>
            <pubDate>Fri, 06 Jun 2025 22:35:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.maxemitchell.com/writings/i-read-all-of-cloudflares-claude-generated-commits/">https://www.maxemitchell.com/writings/i-read-all-of-cloudflares-claude-generated-commits/</a>, See on <a href="https://news.ycombinator.com/item?id=44205697">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>A few days ago, my CTO <a href="https://www.linkedin.com/in/christopher-ingebrigtsen" target="_blank" rel="noreferrer">Chris</a> shared Cloudflare's open-sourced <a href="https://github.com/cloudflare/workers-oauth-provider" target="_blank" rel="noreferrer">OAuth 2.1 library</a> that was almost entirely written by Claude. </p><p>What caught my attention wasn't just the technical achievement, but that they'd documented their entire creative process. Every prompt, every iteration, every moment of human intervention was preserved in git commit messages—creating what felt like an archaeological record of human-AI collaboration. Reading through their development history was like watching a real-time conversation (and sometimes struggle) between human intuition and artificial intelligence.</p><p>The lead engineer, <a href="https://github.com/kentonv" target="_blank" rel="noreferrer">@kentonv</a>, started as an AI skeptic. <i>"I was trying to validate my skepticism. I ended up proving myself wrong."</i> Two months later, Claude had generated nearly all of the code in what became a production-ready authentication library.</p><p>Kenton included the prompt used to generate code in every commit, which made this exploration possible. As we begin to lean more heavily on AI tools within development, this practice will become increasingly important. Sometimes, the original prompt is more valuable (and easier) to review than the resulting code—especially when an engineer declares an incorrect assumption that the model blindly follows.</p><p>This transparency transforms git history from a record of changes into a record of intent, creating a new form of documentation that bridges human reasoning and machine implementation.
</p><p>Reading through roughly 50 commits revealed some interesting patterns about how this collaboration actually unfolded:</p><p><b>Prompt by example.</b> Their initial prompt was a substantial code block showing exactly how the library would be used by a worker implementing it. This approach eliminated ambiguity about method signatures while revealing practical considerations that abstract specifications often miss. It's the difference between describing a dance and demonstrating the choreography.</p><p><b>"You did X, but we should do Y. pls fix."</b> The most effective prompts followed a consistent pattern: clear context about the current state, explanation of why change was needed, and specific direction forward. No elaborate instructions—just contextual feedback that felt remarkably like correcting a colleague.</p><p><i>Personal note: In my own experience with Cursor, I've found it helpful to include direct links to documentation. I can easily reference `@&lt;pasted_docs_link&gt;` in prompts, though I haven't used Claude Code enough to know an equivalent workflow there.</i></p><p><b>Documentation becomes effortless.</b> A single sentence prompt generated comprehensive schema documentation. I've noticed models excel at these documentation generation tasks—transforming what used to be tedious housekeeping into natural byproducts of development.
</p><p>About 20 commits in, I noticed Kenton had to step in manually. Claude couldn't properly move a class declaration and needed a follow-up commit just to fix the positioning. Later, Claude resorted to using `grep` and `sed` bash commands because its search-and-replace function couldn't handle duplicate code blocks.</p><p>One comment resonated with me: <i>"I really could have done this one faster by hand, oh well."</i></p><p>Around the 40-commit mark, manual commits became frequent—styling, removing unused methods, the kind of housekeeping that coding models still struggle with. It's clear that AI generated &gt;95% of the code, but human oversight was essential throughout.</p><p>If you're working with AI coding tools, a few observations from their approach:</p><p>- <b>Focus on the deliverable.</b> For a backend service, define the public endpoints and their expected behavior. For a CLI tool, show example usage. For a library, demonstrate the integration.</p><p>
- <b>Treat prompts as version-controlled assets.</b> Including prompts in commit messages creates valuable context for future maintenance and debugging.</p><p>
- <b>Expect multi-shot prompting.</b> Almost every feature required multiple iterations and refinements. This isn't a limitation—it's how the collaboration works.</p><p>
- <b>Don't be afraid to get your hands dirty.</b> Some bugs and styling issues are faster to fix manually than to prompt through. Knowing when to intervene is part of the craft.
</p><p>Reading through these commits sparked an idea: what if we treated prompts as the actual source code? Imagine version control systems where you commit the prompts used to generate features rather than the resulting implementation. When models inevitably improve, you could connect the latest version and regenerate the entire codebase with enhanced capability.</p><p>This approach would make business logic inherently self-documenting—anyone who reads English could understand when features were added and why. The prompts would become like genetic instructions, containing the essential information needed to grow the application.</p><p>Of course, this assumes models can achieve strict prompt adherence and requires a <b>very</b> high level of trust in the models. In practice, generated code still needs deployment, testing, and maintenance. But it raises a fascinating question: if all business logic could be contained within self-documenting prompts, might we eventually reach a point where this history of prompt commits itself becomes the "application," running directly in a model? Could we eliminate the intermediary code generation step entirely?</p><p>Anyways, back to the the current state of reality.
</p><p>This OAuth library represents something larger than a technical milestone—it's evidence of a new creative dynamic emerging. One where artificial intelligence handles mechanical implementation while humans provide direction, context, and judgment.</p><p>It's clear there was substantial human involvement throughout this process. We're still far from AI independently implementing libraries of this scope. Almost every feature required multi-shot prompting, some bugs resisted all attempts at automated fixes, certain features would have been completed faster manually, and a human had to create every prompt and provide strategic direction.</p><p>Despite these limitations, AI generated the vast majority of functional code in this library. Claude Code was publicly launched just two weeks ago, and it's already enabling this level of collaboration.</p><p>For now, it's just another tool. But unlike a hammer, this tool is improving itself, learning from every interaction, becoming more capable with each iteration.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Falsehoods programmers believe about aviation (280 pts)]]></title>
            <link>https://flightaware.engineering/falsehoods-programmers-believe-about-aviation/</link>
            <guid>44205590</guid>
            <pubDate>Fri, 06 Jun 2025 22:20:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://flightaware.engineering/falsehoods-programmers-believe-about-aviation/">https://flightaware.engineering/falsehoods-programmers-believe-about-aviation/</a>, See on <a href="https://news.ycombinator.com/item?id=44205590">Hacker News</a></p>
<div id="readability-page-1" class="page">





	<section>

		





<main role="main">
	<article>
		<div>

			<section>
				<p>At FlightAware, our software needs to gracefully handle all sorts of weird and wonderful situations. While we as engineers might hope for aviation data to be clean and well-standardized, the real world is messy.</p><p>There are a lot of assumptions one could make when designing data types and schemas for aviation data that turn out to be inaccurate. In the spirit of <a href="https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/?ref=flightaware.engineering">Patrick McKenzie’s classic piece on names</a>, here are some false assumptions one might make about aviation. While many of these are simply common misconceptions, some of these assumptions have bitten our customers at various points, and others have caused issues in our own systems over the years.</p><p>Together they are illustrative of the situations that Hyperfeed, our flight tracking engine, is responsible for correctly interpreting in order to provide a clean and consistent data feed for our website, apps, and APIs.</p><h2 id="flights">Flights</h2><ul><li>Flights depart from a gate</li><li>Flights that depart from a gate <a href="https://www.flightaware.com/live/flight/AFR1/history/20250514/2040Z/KJFK/LFPG?ref=flightaware.engineering">only leave their gate once</a></li><li>Flights depart within a few hours of the time they were scheduled to</li><li>Flights depart <a href="https://www.flightaware.com/live/flight/PDT5965/history/20250508/2224Z/KCHO/KCLT?ref=flightaware.engineering">within a day</a> of the time they were scheduled to</li><li>Flights have schedules</li><li>Flights take off and land <a href="https://www.flightaware.com/live/flight/N144NE/history/20250518/1747Z/KPSM/L%2042.98589%20-71.12891?ref=flightaware.engineering">at airports</a></li><li>Airplanes (excluding helicopters) take off and land at airports</li><li>Flights are at most <a href="https://www.flightaware.com/live/flight/SIA21/history/20250516/1345Z/KEWR/WSSS?ref=flightaware.engineering">a dozen or so hours long</a></li><li>Okay, they’re at most <a href="https://www.flightaware.com/live/flight/HBAL812/history/20190717/1738Z?ref=flightaware.engineering">a few days long</a></li><li>Flights are identified by a flight number consisting of an airline’s code plus some numbers, like UAL1234</li><li>Flights <a href="https://www.flightaware.com/live/flight/C6031/history/20250521/1752Z/KBID/KFMH?ref=flightaware.engineering">are identified by either</a> an airline flight number like UAL1234, or the aircraft’s registration like N12345, B6459, or FHUVL</li><li>A flight identifier like B6459 is unambiguously either a registration (<a href="https://www.flightaware.com/live/flight/B6459?ref=flightaware.engineering" rel="noreferrer">B–6459</a>), an airline flight number (<a href="https://www.flightaware.com/live/flight/JBU459?ref=flightaware.engineering" rel="noreferrer">B6 459</a>), or something else</li><li>Flights don’t have <a href="https://en.wikipedia.org/wiki/Change_of_gauge_(aviation)?ref=flightaware.engineering">multiple flight numbers</a></li><li>Flights with multiple flight numbers unambiguously have one “main” flight number</li><li>A particular trip’s flight number(s) <a href="https://web.archive.org/web/20230328124705/https://community.southwest.com/t5/Blog/The-Science-behind-Flight-Numbers/ba-p/42760">never change</a></li><li>The flight number shown on your ticket <a href="https://www.eurocontrol.int/service/call-sign-similarity-service?ref=flightaware.engineering">is what the pilots and air traffic control are using</a></li><li>Flights don’t use the code of some entirely unrelated airline in their flight identifier</li><li>No flights use the same flight number within a day</li><li>Surely at least no flights use the same flight number at the same time?</li><li>Okay fine, separate flights from the same major passenger airline that depart within a few minutes of each other would not <a href="https://www.flightaware.com/live/flight/AAL2586/history/20250509/1935Z/TBPB/KCLT?ref=flightaware.engineering">both</a> have the <a href="https://www.flightaware.com/live/flight/AAL2586/history/20250508/1935Z/TBPB/KCLT?ref=flightaware.engineering">same</a> flight number… right?</li></ul><h2 id="airports">Airports</h2><ul><li>Airports <a href="https://en.wikipedia.org/wiki/Atat%C3%BCrk_Airport?ref=flightaware.engineering#Closure">never move</a></li><li>Terminal and gate numbers have a consistent naming scheme</li><li>Each runway is <a href="https://en.wikipedia.org/wiki/Hickam_Air_Force_Base?ref=flightaware.engineering">only used by one</a> airport </li><li>Airports always have two unique identifiers: a 4-letter Civil Aviation Organization (ICAO) code and a 3-letter International Air Transport Association (IATA) code</li><li>Airports always have three unique identifiers: an ICAO, an IATA, and a regionally-administered location code</li><li>The U.S. Department of Transportation <a href="https://www.bts.gov/topics/airlines-and-airports/world-airport-codes?ref=flightaware.engineering">assigns one</a> canonical code to <a href="https://www.faa.gov/air_traffic/flight_info/aeronav/aero_data/loc_id_search/Encodes_Decodes/?ref=flightaware.engineering">each airport</a> it oversees</li><li>No airports have <a href="https://en.wikipedia.org/wiki/EuroAirport_Basel_Mulhouse_Freiburg?ref=flightaware.engineering">multiple IATA codes</a></li><li>The ICAO code for airports in the U.S. <a href="https://www.flightaware.com/live/airport/PANC?ref=flightaware.engineering">always starts with the letter K</a></li><li>For U.S. airports whose ICAO code starts with K, the <a href="https://en.wikipedia.org/wiki/McClellan%E2%80%93Palomar_Airport?ref=flightaware.engineering">last three letters</a> are its IATA code</li><li>You can tell which <a href="https://www.flightaware.com/live/airport/NZIR?ref=flightaware.engineering">geographic region</a> an airport is in from its ICAO code</li><li>Everything that has an IATA code <a href="https://en.wikipedia.org/wiki/List_of_IATA-indexed_railway_stations,_bus_stations_and_ferry_terminals?ref=flightaware.engineering">is an airport</a></li><li>Everything that has an ICAO code <a href="https://en.wikipedia.org/wiki/Jezero_(crater)?ref=flightaware.engineering">is on Earth</a></li><li>Airports have at least one well-known identifier of some sort</li></ul><h2 id="airlines">Airlines</h2><ul><li>No <a href="https://en.wikipedia.org/wiki/SkyJet_Airlines?ref=flightaware.engineering">two</a> airlines <a href="https://en.wikipedia.org/wiki/Euroavia_Airlines?ref=flightaware.engineering">share</a> the same <a href="https://en.wikipedia.org/wiki/Airline_codes?ref=flightaware.engineering#IATA_airline_designator">IATA code</a></li><li>No <a href="https://en.wikipedia.org/wiki/EasyJet_UK?ref=flightaware.engineering">airlines</a> use <a href="https://en.wikipedia.org/wiki/EasyJet_Europe?ref=flightaware.engineering">multiple</a> IATA or ICAO <a href="https://en.wikipedia.org/wiki/EasyJet_Switzerland?ref=flightaware.engineering">codes</a></li><li>You can tell <a href="https://en.wikipedia.org/wiki/Aircraft_lease?ref=flightaware.engineering#Wet_lease">what airline is operating a flight</a> by looking at the physical aircraft</li><li>Airlines assign flight numbers to specific routes</li><li>Airlines only assign flight numbers to <a href="https://en.wikipedia.org/wiki/Codeshare_agreement?ref=flightaware.engineering">flights they operate</a></li><li>Airlines only assign flight numbers to <a href="https://www.flyertalk.com/forum/air-france-frequence-plus/1325488-how-fly-mlh-bsl.html?ref=flightaware.engineering">flights</a></li></ul><h2 id="navigation">Navigation</h2><ul><li>Waypoint names are unique</li><li>There is one <a href="https://en.wikipedia.org/wiki/Altitude?ref=flightaware.engineering#In_aviation">agreed-upon definition of altitude</a></li><li>Flight information from Air Navigation Service Providers is accurate</li><li>Okay, <em>pretty</em> accurate; they wouldn’t indicate that a flight had departed unless it really had</li><li>If they indicate that a flight plan has been cancelled, then that flight definitely isn’t going to operate — it wouldn’t simply be due to someone editing the flight plan</li><li>At least their radar data accurately identifies each aircraft</li><li>Radars with overlapping coverage areas agree on the location of a target they can both see</li><li>If they send us a flight plan with the ICAO identifier of a known airport as the destination, then there must have been some intention of arriving there</li><li>If an aircraft diverts to another destination, it won’t <a href="https://www.flightaware.com/live/flight/AAL1372/history/20250516/1410Z/KMIA/KRIC?ref=flightaware.engineering">divert again</a></li></ul><h2 id="transponders-and-ads-b">Transponders and ADS-B</h2><ul><li>ADS-B messages only come from aircraft</li><li>ADS-B messages only come from aircraft and airport service vehicles</li><li>ADS-B messages only come from vehicles of some kind</li><li>The GPS position in ADS-B messages <a href="https://en.wikipedia.org/wiki/Dilution_of_precision_(navigation)?ref=flightaware.engineering">is accurate</a></li><li>The GPS position in ADS-B messages is accurate <a href="https://en.wikipedia.org/wiki/Spoofing_attack?ref=flightaware.engineering#Global_navigation_satellite_system_spoofing">within some known uncertainty</a> radius</li><li>ADS-B messages always include the correct flight identification</li><li>Transponders are correctly programmed to indicate the aircraft type (helicopter, airplane, balloon, etc)</li><li>You can always determine a aircraft’s registration number from its ADS-B messages</li><li>Transponders are programmed with the correct Mode S address</li><li>All of the transponders on a single aircraft are programmed with the same Mode S address</li><li>Nobody will ever set their flight identification to weird things like NULL</li><li>People will remember to update the transponder when the aircraft’s registration changes</li><li>ADS-B messages are always received exactly as they were transmitted</li><li>No one ever transmits false ADS-B messages</li><li>Transponders never break and rodents never chew through cables</li></ul><hr><p>Thanks to my colleagues who contributed to or reviewed this collection of falsehoods: Mark Duell, Paul Durandt, Karina Elizondo, Matt Higgins, Thomas Kyanko, Nathan Reed, and Amy Szczepanski.</p>
			</section>
			
			

			


			


		<section>
			<a href="https://flightaware.engineering/">Back to home</a>
		</section>

			


		</div>
	</article>
</main>




		

		

	</section>

	

	


	



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Researchers develop ‘transparent paper’ as alternative to plastics (284 pts)]]></title>
            <link>https://japannews.yomiuri.co.jp/science-nature/technology/20250605-259501/</link>
            <guid>44205282</guid>
            <pubDate>Fri, 06 Jun 2025 21:43:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://japannews.yomiuri.co.jp/science-nature/technology/20250605-259501/">https://japannews.yomiuri.co.jp/science-nature/technology/20250605-259501/</a>, See on <a href="https://news.ycombinator.com/item?id=44205282">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="p-article-block" data-state="">


<figure>
<a href="https://japannews.yomiuri.co.jp/attachment/259499/" rel="attachment wp-att-259499"><img decoding="async" src="https://japannews.yomiuri.co.jp/wp-content/uploads/2025/06/transparent-paper1.jpg" alt=""></a>
<figcaption><span>Photos courtesy of JAMSTEC</span><br>Scenery 100 meters away is seen through a sheet of transparent paper which is 0.7 millimeters thick.</figcaption></figure>


<p><i></i>14:47 JST, June 5, 2025</p>

<p>A team of researchers with the Japan Agency for Marine-Earth Science and Technology (JAMSTEC) and other entities have developed thick sheets of transparent paper using cellulose, a material made from plant biomass.</p>
<p>The transparent paper sheets can be broken down by microbes into water and carbon dioxide. Also, they can be used to make containers because they are thicker than conventional cellulose-based materials. The new material is expected to replace plastics for this purpose, as plastics are a source of ocean pollution.</p>
<p>To make the paper sheets, the team used cellulose powder made from fibers found on the surfaces of cotton seeds. They dissolved the powder in a lithium bromide-water solution, mixing it in as they raised it to a high temperature until it became a gel, at which point the material was shaped and dried.</p>
<p>When the researchers shaped the material into cups and straws, they found that it was about as strong as polycarbonate, a type of plastic.</p>
<p>The paper sheets become transparent because they are packed tightly with nanometer-scale (one 1-billionth of a meter) fibers. The concentration of these fibers allows light to pass straight through the sheets without experiencing diffusion.</p>
<p>Even a sheet of the transparent paper that is 0.7 millimeters thick remains flexible, and scenery 100 meters away can be clearly seen through it.</p>
<p>On the assumption that the paper sheets will be washed into the ocean, the team investigated its biodegradability. The researchers sank the paper sheets into the ocean and checked whether microbes there were able to dissolve them.</p>
<p>The results showed that the deeper the place under the sea was, the slower the dissolution progressed, because there are fewer microbes in the deep ocean. But the paper sheets were mostly dissolved within four months even at 757 meters below the surface.</p>
<figure>
<a href="https://japannews.yomiuri.co.jp/attachment/259500/" rel="attachment wp-att-259500"><img decoding="async" src="https://japannews.yomiuri.co.jp/wp-content/uploads/2025/06/transparent-paper2.jpg" alt=""></a>
<figcaption><span>Photos courtesy of JAMSTEC</span><br>A cup and a straw made of the newly developed transparent paper</figcaption></figure>
<p>So far, paper packs have been the most common alternatives to plastic containers.</p>
<p>But business experts have pointed out that consumers are less willing to buy goods in paper packs because they cannot see the contents.</p>
<p>Transparent paper could overcome this problem, but bringing the material to market will require factories with the technology to mass-produce it.</p>
<p>Noriyuki Isobe, a deputy chief researcher for JAMSTEC, said, “If a plant for demonstration experiments of the technology is built, we estimate that the cost to produce the material will be about three times that of ordinary paper, while the volume of CO2 emissions can be kept to about half that of the plastic making process.”</p>
<p>Prof. Masaya Nogi of the University of Osaka, an expert on wooden materials, said, “There have been other types of transparent paper in the past, but the advantage of this over those is that it has been proven to be biodegradable in the deep sea.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Supreme Court allows DOGE to access social security data (129 pts)]]></title>
            <link>https://www.nbcnews.com/politics/supreme-court/supreme-court-trump-doge-social-security-data-access-elon-musk-rcna206515</link>
            <guid>44205060</guid>
            <pubDate>Fri, 06 Jun 2025 21:15:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nbcnews.com/politics/supreme-court/supreme-court-trump-doge-social-security-data-access-elon-musk-rcna206515">https://www.nbcnews.com/politics/supreme-court/supreme-court-trump-doge-social-security-data-access-elon-musk-rcna206515</a>, See on <a href="https://news.ycombinator.com/item?id=44205060">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>WASHINGTON — The <a href="https://www.nbcnews.com/politics/supreme-court" target="_blank">Supreme Court</a> on Friday allowed members of the Trump administration's Department of Government Efficiency to access Social Security Administration data.</p><p>The conservative-majority court, with its three liberal justices objecting, granted an emergency application filed by the Trump administration asking the justices to lift an injunction issued by a federal judge in Maryland.</p><p>The <a href="https://www.supremecourt.gov/opinions/24pdf/24a1063_6j37.pdf" target="_blank">unsigned order</a> said that members of the DOGE team assigned to the Social Security Administration should have "access to the agency records in question in order for those members to do their work."</p><p>The lawsuit challenging DOGE’s actions was filed by progressive group Democracy Forward on behalf of two unions — the American Federation of State, County and Municipal Employees, and the American Federation of Teachers — as well as the Alliance for Retired Americans.</p><p>"This is a sad day for our democracy and a scary day for millions of people," the groups said in a statement. "This ruling will enable President Trump and DOGE’s affiliates to steal Americans’ private and personal data."</p><p>The White House praised the ruling.</p><p>"The Supreme Court allowing the Trump Administration to carry out commonsense efforts to eliminate waste, fraud, and abuse and modernize government information systems is a huge victory for the rule of law," White House spokesperson Liz Huston said in a statement.</p><p><a href="https://www.nbcnews.com/politics/trump-administration/live-blog/trump-administration-elon-musk-travel-ban-tariffs-live-updates-rcna210179" target="_blank"><em>Follow live politics coverage here</em></a></p><p>Liberal Justice Ketanji Brown Jackson wrote a dissenting opinion questioning the need for the court to intervene on an emergency basis.</p><p>"In essence, the 'urgency' underlying the government’s stay application is the mere fact that it cannot be bothered to wait for the litigation process to play out before proceeding as it wishes," she added.</p><p>DOGE, set up by billionaire Elon Musk before his falling out with President Donald Trump,<strong> </strong>says it wants to modernize systems and detect waste and fraud at the agency. The data it seeks includes Social Security numbers, medical records, and tax and banking information.</p><p>“These teams have a business need to access the data at their assigned agency and subject the government’s records to much-needed scrutiny,” Solicitor General D. John Sauer wrote in court papers.</p><p>The lawsuit alleged that allowing broader access to the personal information would violate a federal law called the Privacy Act as well as the Administrative Procedure Act.</p><p>"The agency is obligated by the Privacy Act and its own regulations, practices, and procedures to keep that information secure — and not to share it beyond the circle of those who truly need it," the challengers' lawyers wrote in court papers.</p><p>U.S. District Judge Ellen Hollander had ruled that DOGE had no need to access the specific data at issue. The 4th U.S. Circuit Court of Appeals, based in Richmond, Virginia, declined to block Hollander's decision, leading to the Trump administration to file its emergency request at the Supreme Court.</p><p>Social Security Administration Commissioner Frank Bisignano welcomed Friday's ruling by the high court.</p><p>“The Supreme Court’s ruling is a major victory for American taxpayers. The Social Security Administration will continue driving forward modernization efforts, streamlining government systems, and ensuring improved service and outcomes for our beneficiaries,” Bisignano said in a statement.</p><p>In a <a href="https://www.supremecourt.gov/orders/courtorders/060625zr_5426.pdf" target="_blank">separate order</a> issued Friday in another case involving DOGE, the Supreme Court granted another request filed by the Trump administration.</p><p>That decision allows the Trump administration to, for now, shield DOGE from freedom of information requests seeking thousands of pages of material.</p><p>The move <a href="https://www.nbcnews.com/politics/supreme-court/supreme-court-temporarily-allows-trump-administration-shield-doge-docu-rcna208628" target="_blank">formalizes a decision</a> issued by Chief Justice John Roberts on May 23 that temporarily put lower court decisions on hold while the Supreme Court considered what next steps to take. The court also told lower courts to limit the scope of what material could be disclosed.</p><p>It means the government will not have to respond to requests for documents and allow for the deposition of the DOGE administrator, Amy Gleason, as a lower court had ruled, while litigation continues.</p><p>The three liberal justices noted their disagreement with that decision, too.</p><p>A spokesman for Citizens for Responsibility and Ethics in Washington, which filed the lawsuit, said the group was "obviously disappointed" with the decision but "pleased that the court allowed discovery to proceed."</p><p>A Justice Department spokesman did not immediately respond to a request for comment on the order.</p></div><div><div data-activity-map="expanded-byline-article-bottom"><div data-testid="byline-thumbnail"><a href="https://www.nbcnews.com/author/lawrence-hurley-ncpn1298564" tabindex="-1"><picture data-testid="picture"><source media="(min-width: 320px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_avif,q_auto:eco,dpr_2/newscms/2023_08/3595831/lawrence-hurley-byline-jm-1.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2023_08/3595831/lawrence-hurley-byline-jm-1.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2023_08/3595831/lawrence-hurley-byline-jm-1.jpg" alt="" height="48" width="48"></picture></a></div><p><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/lawrence-hurley-ncpn1298564">Lawrence Hurley</a></span><span><a href="https://x.com/lawrencehurley" target="_blank" rel="noopener noreferrer"><span></span></a><a href="mailto:Lawrence.Hurley@nbcuni.com" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Lawrence Hurley is a senior Supreme Court reporter for NBC News. </p></div><div data-activity-map="expanded-byline-article-bottom"><div data-testid="byline-thumbnail"><a href="https://www.nbcnews.com/author/gary-grumbach-ncpn752571" tabindex="-1"><picture data-testid="picture"><source media="(min-width: 320px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_avif,q_auto:eco,dpr_2/newscms/2024_16/3645263/240415-gary-grumbach-mn-1145.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2024_16/3645263/240415-gary-grumbach-mn-1145.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2024_16/3645263/240415-gary-grumbach-mn-1145.jpg" alt="" height="48" width="48"></picture></a></div><p><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/gary-grumbach-ncpn752571">Gary Grumbach</a></span><span><a href="https://x.com/GaryGrumbach" target="_blank" rel="noopener noreferrer"><span></span></a><a href="mailto:Gary.Grumbach@nbcuni.com" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Gary Grumbach is a NBC News Legal Affairs Reporter, based in Washington, D.C.</p></div><div><p>Garrett Haake</p><!-- --><p> and </p><!-- --><p>Julia Jester</p><!-- --> <!-- --><p>contributed</p><!-- --><p>.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Supreme Court Gives Doge Access to Social Security Data (121 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2025-06-06/supreme-court-gives-doge-access-to-social-security-data</link>
            <guid>44204767</guid>
            <pubDate>Fri, 06 Jun 2025 20:42:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2025-06-06/supreme-court-gives-doge-access-to-social-security-data">https://www.bloomberg.com/news/articles/2025-06-06/supreme-court-gives-doge-access-to-social-security-data</a>, See on <a href="https://news.ycombinator.com/item?id=44204767">Hacker News</a></p>
Couldn't get https://www.bloomberg.com/news/articles/2025-06-06/supreme-court-gives-doge-access-to-social-security-data: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Online sports betting: As you do well, they cut you off (121 pts)]]></title>
            <link>https://doc.searls.com/2025/05/21/online-sports-betting-is-for-losers/</link>
            <guid>44204603</guid>
            <pubDate>Fri, 06 Jun 2025 20:21:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://doc.searls.com/2025/05/21/online-sports-betting-is-for-losers/">https://doc.searls.com/2025/05/21/online-sports-betting-is-for-losers/</a>, See on <a href="https://news.ycombinator.com/item?id=44204603">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://flickr.com/photos/docsearls/81417674/in/album-1739217"><img decoding="async" src="https://150108457.v2.pressablecdn.com/wp-content/uploads/2025/05/vegas_020.jpg" alt="" width="70%" height="image" srcset="https://150108457.v2.pressablecdn.com/wp-content/uploads/2025/05/vegas_020.jpg 2560w , https://150108457.v2.pressablecdn.com/wp-content/uploads/2025/05/vegas_020-300x225.jpg 300w , https://150108457.v2.pressablecdn.com/wp-content/uploads/2025/05/vegas_020-1024x768.jpg 1024w , https://150108457.v2.pressablecdn.com/wp-content/uploads/2025/05/vegas_020-768x576.jpg 768w , https://150108457.v2.pressablecdn.com/wp-content/uploads/2025/05/vegas_020-1536x1152.jpg 1536w , https://150108457.v2.pressablecdn.com/wp-content/uploads/2025/05/vegas_020-2048x1536.jpg 2048w " sizes="(max-width: 2560px) 100vw, 2560px"></a></p>
<p>A few decades back my teenage son and I approached Las Vegas at night while traveling south on Interstate 15. When the skyline of the city began sparkling into view, the kid said, “Wow. Think of all the money people have made there!” This was a perfect tease for my response: “Dude, everything you see there was paid for by losers.”</p>
<p>The same is true for online sports betting, only more so. How? Soon as you do well, they cut you off. Sources:</p>
<ul>
<li><a href="https://www.espn.com/sports-betting/story/_/id/24425026/gambling-bookmakers-growing-us-legal-betting-market-allowed-ban-bettors">Won and done? Sportsbooks banning the smart money</a>. By <a href="http://davidpurdum/">@DavidPurdum</a> in ESPN</li>
<li><a href="https://www.wsj.com/business/media/sports-betting-companies-limit-winners-f06ea822">Sports Betting Companies Weed Out Winners. Gamblers Want to Know Why</a>. By <a href="https://www.wsj.com/news/author/katherine-sayre">Katherine Sayre</a> in&nbsp;<a href="https://www.wsj.com/"><em>The Wall Street Journal</em></a></li>
<li><a href="https://nypost.com/2024/07/18/sports/sportsbooks-seem-to-be-the-worst-losers-of-them-all/">Sportsbooks seem to be the worst losers of them all</a>. By <a href="https://nypost.com/author/phil-mushnick/">Phil Mushnick</a> in&nbsp;<a href="https://nypost.com/"><em>The New York Post</em></a></li>
<li><a href="https://www.nepm.org/regional-news/2024-05-22/sports-betting-companies-balk-at-discussing-limits-on-bettors">‘Very disappointing to me’: Sports betting companies balk at discussing limits on bettors</a><br>
By Colin A. Young in New England Public Media</li>
<li>In <a href="https://www.pushkin.fm/podcasts/against-the-rules/episode-5-the-mule">Episode 5</a> and <a href="https://www.pushkin.fm/podcasts/against-the-rules/episode-6-vip">Episode 6</a> of <a href="https://en.wikipedia.org/wiki/Michael_Lewis">Michael Lewis</a>‘ <a href="https://www.pushkin.fm/podcasts/against-the-rules">Against the Rules podcast series on sports gambling</a>, producer <a href="https://www.lidiajeankott.com/">Lydia Jean Kott</a> made a bunch of bad bets that won her the favor of sportsbooks (FanDuel and DraftKings), and then found herself cut off after she started winning (as a “mule” for a skilled sports gambler).</li>
</ul>
<p>Also interesting: While the online sportsbook algorithms are good at spotting gamblers who are smarter than the house, they aren’t so good at spotting and getting help for problem gamblers, which the sportsbook apps are all but (or actually) designed to create—especially out of young people who are quick to rationalize the game of losing money as a form of fun.</p>
<p>In <a href="https://outlier.bet/sports-betting-strategy/positive-ev-betting/why-sportsbooks-limit-your-bets/">Why Sportsbooks Limit Your Bets (And How to Avoid It)</a>, Outlier excuses the practice:</p>
<blockquote><p>Sportsbooks are businesses, and like any business, their primary goal is to make a profit. When a bettor consistently wins, they are essentially taking money from the sportsbook’s bottom line. To mitigate this risk, sportsbooks implement limits on the accounts of profitable bettors. The aim is to encourage more casual, recreational bettors, who are less likely to have an edge and more likely to contribute to the sportsbook’s revenue.</p></blockquote>
<p>Translation: They want people who throw their money away.</p>
<p>I could go on, but instead suggest you dig those two episodes of Against the Rules.</p>
<p>Oh, and here’s a bet: a generation or few from now (though hopefully sooner), we’ll look back on sports gambling everywhere the same way we look back today on smoking and drunk driving everywhere.</p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[United States Digital Service Origins (147 pts)]]></title>
            <link>https://usdigitalserviceorigins.org/</link>
            <guid>44204277</guid>
            <pubDate>Fri, 06 Jun 2025 19:43:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://usdigitalserviceorigins.org/">https://usdigitalserviceorigins.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44204277">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<p>June 6, 2025</p>
<p><b>Launching an Oral History of the Origins of the United States Digital Service</b></p>
<p><span>Opinions about technology’s role in government are everywhere right now. As founding members of the United States Digital Service, we could easily add our voices alone to the discourse — especially after it was reorganized and renamed to the United States DOGE Service on January 20, 2025.&nbsp;</span></p>
<p><span>While there’s certainly a need to think strategically about the future, it is also critical that we understand and learn from the past, because hard-won lessons shouldn’t be overshadowed by controversy or politics. After all, having a government that functions and delivers on its promises to the public shouldn’t be divisive.</span></p>
<p><span>That’s why over the past several years we have been collecting stories and experiences from our colleagues — the people who laid the foundation for an organization that brought over 700 technologists into government across three presidential administrations. Today we are proud to share the outcome of this work: </span><i><span>an oral history documenting how the United States Digital Service came to exist, and the initial days of building its foundation.&nbsp;&nbsp;</span></i></p>
<p><a href="https://usdigitalserviceorigins.org/"><span>The United States Digital Service Origins</span></a><span> features nearly 50 interviews, spanning 2009 – 2015, with those involved in the founding of the United States Digital Service, as well as some of the first leaders of agency teams and Communities of Practice. Each interview stands on its own, but in the aggregate the interviews piece together a larger story and vision of technology in government, and the realities of creating something new in an environment that is often paralyzed by inertia.&nbsp;</span></p>
<p><span>When we looked across interviews, we were able to identify </span><a href="https://usdigitalserviceorigins.org/insights/"><span>key themes and lessons</span></a><span>, and create a </span><a href="https://usdigitalserviceorigins.org/timeline/"><span>chronology of events</span></a><span> that tell the story of how the U.S. Digital Service was conceptualized and how it came to life. We also identified </span><a href="https://usdigitalserviceorigins.org/quotes/"><span>key quotes</span></a><span> that together provide a sense of the experience itself — what the work was actually like, as well as differing views and reflections on that time.&nbsp;&nbsp;</span></p>
<p><span>This is just one part of a larger story that led to the U.S. Digital Service; it’s one part of a 20+ year civic tech movement that transcends organizations, disciplines, and borders, and is built on the belief that people with experience designing and delivering digital products, combined with institutional knowledge and positioned at the right level of government, can transform public services — and that if we do this right, maybe we can make life even just a little bit better for people.</span></p>
<p><span>That story and movement doesn’t halt when one program — or presidency — comes to an end. Because it’s a body of work that’s bigger than any agency or group of people.&nbsp;</span></p>
<p><span>Whatever form government technology teams take in the coming years, the U.S. Digital Service’s legacy and learnings are relevant. It was not perfect, but it demonstrated what was possible, and those who had an opportunity to serve were forever changed.</span></p>
<p><em><span>– Kathy Pham and Emily Tavoulareas</span></em></p>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A year of funded FreeBSD development (279 pts)]]></title>
            <link>https://www.daemonology.net/blog/2025-06-06-A-year-of-funded-FreeBSD.html</link>
            <guid>44204224</guid>
            <pubDate>Fri, 06 Jun 2025 19:35:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.daemonology.net/blog/2025-06-06-A-year-of-funded-FreeBSD.html">https://www.daemonology.net/blog/2025-06-06-A-year-of-funded-FreeBSD.html</a>, See on <a href="https://news.ycombinator.com/item?id=44204224">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>
I've been maintaining
<a href="https://www.freebsd.org/">FreeBSD</a> on the
<a href="https://aws.amazon.com/ec2/">Amazon EC2</a> platform ever since I
first got it booting in 2010, but in
<a href="https://www.freebsd.org/news/newsflash/#2023-11-17:1">November
2023</a> I added to my responsibilities the role of FreeBSD release
engineering lead — just in time to announce the availability of
FreeBSD 14.0, although Glen Barber did all the release engineering work for
that release.  While I receive a small amount of funding from
<a href="https://antithesis.com/">Antithesis</a> and from my
<a href="https://www.patreon.com/c/cperciva">FreeBSD/EC2 Patreon</a>, it
rapidly became clear that my release engineering duties were competing
with — in fact, out-competing — FreeBSD/EC2 for my available
FreeBSD volunteer hours: In addition to my long list of "features to
implement" stagnating, I had increasingly been saying "huh that's weird...
oh well, no time to investigate that now".  In short, by early 2024 I was
becoming increasingly concerned that I was not in a position to be a good
"owner" of the FreeBSD/EC2 platform.
</p><p>
For several years leading up to this point I had been talking to Amazonians
on and off about the possibility of Amazon sponsoring my FreeBSD/EC2 work;
rather predictably, most of those conversation ended up with my
contacts at Amazon rhyming with "Amazon should definitely sponsor the work
you're doing... but I don't have any money available in <em>my</em> budget
for this".  Finally in April 2024 I found someone with a budget, and after
some discussions around timeline, scope, and process, it was determined
that Amazon would support me for a year via
<a href="https://github.com/sponsors/cperciva">GitHub Sponsors</a>.  I'm
not entirely sure if the year in question was June through May or July
through June — money had to move within Amazon, from Amazon to GitHub,
from GitHub to Stripe, and finally from Stripe into my bank account, so when
I received money doesn't necessarily reflect when Amazon <i>intended</i> to
give me money — but either way the sponsorship either has come to an end
or is coming to an end soon, so I figured now was a good time to write about
what I've done.
</p><p>
Amazon was nominally sponsoring me for 40 hours/month of work on FreeBSD
release engineering and FreeBSD/EC2 development — I made it clear to
them that sponsoring one and not the other wasn't really feasible, especially
given dependencies between the two — and asked me to track how much
time I was spending on things.  In the end, I spent roughly 50 hours/month
on this work, averaging 20 hours/month spent on EC2-specific issues, 20
hours/month making FreeBSD releases happen, and 10 hours/month on other
release engineering related work — although the exact breakdown
varied dramatically from month to month.
</p><p>
Following FreeBSD's
<a href="https://lists.freebsd.org/archives/freebsd-announce/2024-July/000143.html">quarterly
release schedule</a> (which I announced in July 2024, but put together and
presented at the FreeBSD developer summit at
<a href="https://www.bsdcan.org/">BSDCan</a> in May 2024), I managed four
FreeBSD releases during the past year: FreeBSD 13.4, in September 2024;
FreeBSD 14.2, in December 2024; FreeBSD 13.5, in March 2025; and FreeBSD
14.3, currently scheduled for release on June 10th.  The work involved in
manging each of these releases — nagging developers to get their
code into the tree in time, approving (or disapproving!) merge requests,
coordinating with other teams, building and testing images (usually three
Betas, one Release Candidate, and the final Release), writing announcement
text, and fixing any release-building breakage which arose along the way
— mostly happened in the month prior to the release (I refer to the
second month of each calendar quarter as "Beta Month") and ranged from a
low of 33.5 hours (for FreeBSD 13.5) to a high of 79 hours (for FreeBSD
14.2).  As one might imagine, the later in a stable branch you get, the
fewer the number of things there are breaking and the lower the amount of
work required for doing a release; while I wasn't tracking hours when I
managed FreeBSD 14.1, I suspect it took close to 100 hours of release
engineering time, and FreeBSD 15.0 is very likely to be well over that.
</p><p>
On the FreeBSD/EC2 side of things, there were two major features which
Amazon encouraged me to prioritize: The "power driver" for AWS Graviton
instances (aka "how the EC2 API tells the OS to shut down" — without
this, FreeBSD ignores the shutdown signal and a few minutes later EC2
times out and yanks the virtual power cable), and device hotplug on AWS
Graviton instances.  The first of these was straightforward: On Graviton
systems, the "power button" is a GPIO pin, the details of which are
specified via an ACPI _AEI object.  I added code to find those in ACPI and
pass the appropriate configuration through to the driver for the PL061 GPIO
controller; when the GPIO pin is asserted, the controller generates an
interrupt which causes the ACPI "power button" event to be triggered, which
in turn now shuts down the system.  There was one minor hiccup: The ACPI
tables provided by EC2 specify that the GPIO pin in question should be
configured as a "Pull Up" pin, but the PL061 controller in fact doesn't
have any pullup/pulldown resistors; this didn't cause problems on Linux
because Linux silently ignores GPIO configuration failures, but on FreeBSD
we disabled the device after failing to configure it.  I believe this EC2
bug will be fixed in future Graviton systems; but in the mean time I ship
FreeBSD/EC2 AMIs with a new "quirk": <tt>ACPI_Q_AEI_NOPULL</tt>, aka
"Ignore the PullUp flag on GPIO pin specifications in _AEI objects".
</p><p>
Getting hotplug working — or more specifically, getting hot
<i>unplug</i> working, since that's where most of the problems arose
— took considerably more work, largely because there were several
different problems, each presenting on a subset of EC2 instance types:
</p><ul><li>
On some Graviton systems, we leaked a (virtual) IRQ reservation during
PCI attach; this is harmless in most cases, but after attaching and
detaching an EBS volume 67 times we would run out of IRQs and the FreeBSD
kernel would panic.  This IRQ leakage was happening from some "legacy"
PCI interrupt routing code, and in the end I simply added a boot loader
setting to turn that code off in EC2.
</li><li>
On some Graviton systems, the firmware uses PCI device power state as an
indication that the OS has finished using a device and is ready for it
to be "ejected".  This is 100% a bug in EC2, and I believe it will be
fixed in due course; in the mean time, FreeBSD/EC2 AMIs have an ACPI
quirk <tt>ACPI_Q_CLEAR_PME_ON_DETACH</tt> which instructs them to flip
some bits in the PCI power management register before ejecting a
device.
</li><li>
On the newest generation of EC2 instances (both x86 and Graviton),
FreeBSD's nvme driver would panic after a PCIe unplug.  This bug I
didn't need to fix myself, beyond pointing it out to our nvme driver
maintainer.
</li><li>
On some EC2 instances (both x86 and Graviton), we would see a "ghost"
device on the PCI bus after it was ejected; attempts to access the
device would fail, but the "ghost" would block any attempt to attach
new devices.  This turned out to be another EC2 bug: The Nitro firmware
managing the PCI bus operated asynchronously from the firmware managing
the PCI <i>devices</i>, so there was a window of a few ms where a device
had been unplugged but the PCI bus still reported it as being present.
On Linux this is (almost always) not a problem since Linux scans buses
periodically and typically loses the race; but on FreeBSD we immediately
re-scan the PCI bus after detaching a device, so we usually won the race
against the Nitro firmware, causing us to "see" the device which was no
longer present.  My understanding is that this is being fixed
in Nitro to ensure that the PCI bus "knows" about a device detach before
the detach is acknowledged to the operating system; but in the mean time,
FreeBSD/EC2 AMIs have an ACPI quirk <tt>ACPI_Q_DELAY_BEFORE_EJECT_RESCAN</tt>
which adds a 10 ms delay between signalling that a device should be ejected
and rescanning the PCI bus.
</li></ul><p>
In addition to these functionality fixes, I made one "quality of
life" improvement to FreeBSD's hotplug handling: PCIe (used in the latest
generation of EC2 instances) mandates that after the "attention" button
is pressed to request a device eject, there is a 5 second delay —
in case a human standing in front of a machine says "oh no that's the
wrong disk" — and if the button is pressed a second time, the eject
request is cancelled.  This delay is entirely pointless in EC2, where there
is no human physically pressing a button (and no mechanism for pressing a
virtual button a second time) — so I added a boot loader tunable to
adjust that timeout and set it to zero in EC2.  Finally, I put together a
test script: I can now launch an EC2 instance and repeatedly plug and
unplug an EBS volume via the EC2 API, and confirm that FreeBSD sucessfully
attaches and detaches it 300 times in a row.  With luck, this will allow
me to ensure that hotplug is fully operational on future EC2 instance
types — at least, assuming I get access to them before they launch.
</p><p>
While those two were Amazon's top priorities for FreeBSD/EC2 work, they
were by no means the only things I worked on; in fact they only took up
about half of the time I spent on EC2-specific issues.  I did a lot of
work in 2021 and 2022 to speed up the FreeBSD boot process, but among the
"that's weird but I don't have time to investigate right now" issues I
had noticed in late 2023 and early 2024 was that FreeBSD/EC2 instances
sometimes took a surprisingly long time to boot.  I hadn't <i>measured</i>
how long they took, mind you; but as part of the FreeBSD weekly snapshot
process I ran test boots of a few EC2 instance types, and I had needed to
increase the sleep time between launching instances and trying to SSH
into them.
</p><p>
Well, the first thing to do with any sort of performance issues is to
collect data; so I benchmarked boot time on weekly EC2 AMI builds dating
back to 2018 — spinning up over ten thousand EC2 instances in the
process — and started generating FreeBSD
<a href="https://www.daemonology.net/freebsd-ec2-boot-performance/">boot
performance plots</a>.  Collecting new data and updating those plots is
now part of my weekly snapshot testing process; but even without drawing
plots, I could immediately see some issues.  I got to work:
</p><ul><li>
Starting in the first week of 2024, the FreeBSD boot process suddenly got
about 3x slower.  I started bisecting commits, and tracked it down to...
a commit which increased the root disk size from 5 GB to 6 GB.  Why?  Well,
I reached out to some of my friends at Amazon, and it turned out that the
answer was somewhere between "magic" and "you really don't want to know";
but the important part for me was that increasing the root disk size to 8
GB restored performance to earlier levels.
</li><li>
FreeBSD was also taking a long time to boot on Graviton 2 (aka <tt>c6g</tt>
and similar) instances, and after some investigation I tracked this down to
a problem with kernel entropy seeding: If the FreeBSD kernel doesn't have
enough entropy to generate secure random numbers, the boot process stalls
until it collects more entropy — and in a VM, that can take a while.
Now, we have code to obtain entropy via the EFI boot loader — which
effectively means asking the Nitro firmware to give us a secure seed —
but that ran into two problems: First, it wasn't actually running in EC2,
and second, when it did run, it was absurdly slow on the Graviton 2.
<br>
The first problem was easily solved: The entropy seeding request was being
made from the "boot menu" lua code, and (ironically in order to improve
boot performance) we bypass that menu in EC2; moving that to the right place
in the boot loader lua code ensured that it ran regardless of whether the
menu was enabled.  The second problem turned out to be a Graviton 2 issue:
It could provide a small amount of entropy quickly, but took a long time to
provide the 2048 bytes which the FreeBSD boot loader was requesting.  This
large request was due to the way that FreeBSD seeded its entropy system;
32 pools each needed 64 bytes of entropy.  Since this was in no way
<i>cryptographically</i> necessary — the multiple pools exist only as a
protection in case a pool state is leaked — I rewrote the code to make
it possible to take 64 bytes from EFI and use PBKDF2 as an "entropy spreader"
to turn that into the 2048 bytes our entropy-feeding API needed.  This took
the boot time of FreeBSD arm64/base/UFS from ~25 seconds down to ~8
seconds.
</li><li>
I also noticed that ZFS images were taking quite a bit longer to boot than
UFS images — and interestingly, this delta varied depending on the
amount of data on the disk (but not the disk size itself).  I traced this
down to a weird interaction of our filesystem-building code (makefs) and
what happens when you attach a ZFS pool: ZFS performs some filesystem
verification steps which involve traversing the most recent transaction
group, but makefs puts the everything into a single transaction group
— so when EC2 ZFS images booted, they had to read and process metadata
for every single file on disk (hence the number of files on disk affecting
the time taken).  Once I tracked down the issue, I was able to report it
to FreeBSD's makefs guru (Mark Johnston), who solved the problem simply by
recording a higher transaction group on the filesystem — that way,
the single transaction group was not "recent enough" to prompt the ZFS
transaction group verification logic.  ZFS images promptly dropped from ~22
seconds down to ~11 seconds of boot time.
</li><li>
Finally — and this issue was one I caught promptly as a result of
including boot performance in my weekly testing — in December 2024 I
updated the <tt>net/aws-ec2-imdsv2-get</tt> port to support IPv6.  This port
provides a command-line interface to the EC2 Instance MetaData Service, and
is necessary because when Amazon launched "IMDSv2" to paper over (but not
properly fix) the security problem inherent in exposing IAM credentials over
HTTP, they made it impossible to use FreeBSD <tt>fetch(1)</tt> to access the
IMDS.  Unfortunately when IPv6 support was added to
<tt>aws-ec2-imdsv2-get</tt>, two mistakes were made: First, it attempted to
connect on IPv6 first (even though IPv4-only is the default IMDS instance
configuration); and second, it kept the default TCP timeout (75 seconds).
Thanks to my testing, I got this fixed promptly, to attempt IPv4 first and
reduce the timeout to 100 ms — considering that IMDS requests are
serviced without ever leaving the physical system, waiting more than 100 ms
seemed unnecessary!
</li></ul>
<p>
One thing which had long been on my "features to implement" list for
FreeBSD/EC2 but I hadn't found time for earlier was adding more AMI
"flavours": A year ago, we had <tt>base</tt> (the
FreeBSD base system, with minimal additional code installed from the ports
tree to make it "act like an EC2 AMI") and <tt>cloud-init</tt> (as the name
suggests, FreeBSD with Cloud-init installed).  I added two more flavours
of FreeBSD AMI to the roster: <tt>small</tt> AMIs, which are like <tt>base</tt>
except without debug symbols, the LLDB debugger, 32-bit libraries, FreeBSD tests,
or the Amazon SSM Agent or AWS CLI — which collectively reduces the
disk space usage from ~5 GB to ~1 GB while not removing anything which most
people will use — and <tt>builder</tt> AMIs, which are FreeBSD
<a href="https://www.daemonology.net/blog/2015-11-21-FreeBSD-AMI-builder-AMI.html">AMI Builder AMIs</a>,
providing an easy path for users to create customized FreeBSD AMIs.
</p><p>
Of course, with 4 flavours of FreeBSD AMIs — and two filesystems (UFS
and ZFS), two architectures (amd64 and arm64), and three versions of
FreeBSD (13-STABLE, 14-STABLE, and 15-CURRENT) — all of the weekly
snapshot builds were starting to add up; so in May I finally got around to
cleaning up old images (and their associated EBS snapshots).  While I don't
pay for these images — the FreeBSD release engineering AWS account is
sponsored by Amazon — it was still costing <i>someone</i> money; so
when I realized I could get rid of 336 TB of EBS snapshots, I figured it
was worth spending a few hours writing shell scripts.
</p><p>
While most of my time was spent on managing release cycles and maintaining
the FreeBSD/EC2 platform, I did also spend some time on broader release
engineering issues — in fact, part of the design of the "quarterly"
release schedule is that it leaves a few weeks between finishing one
release and starting the next to allow for release engineering work which
can't effectively be done in the middle of a release cycle.  The first
issue I tackled here was parallelizing release building: With a large
number of EC2 AMIs being built, a large proportion of the release build
time was being spent not <i>building</i> but rather <i>installing</i>
FreeBSD into VM images.  I reworked the release code to parallelize this,
but found that it caused sporadic build failures — which were very
hard to isolate, since they only showed up with a complete release build
(which took close to 24 hours) and not with any subset of the build.
After many hours of work I finally tracked the problem down to a single
missing Makefile line: We weren't specifying that a directory should be
created before files were installed into it.  With that fix, I was able
to reduce the release build from ~22 hours down to ~13 hours, and also
"unlock" the ability to add more EC2 AMI flavours (which I couldn't do
earlier since it would have increased the build time too much).
</p><p>
Another general release engineering issue I started tackling was the
problem of build reproducibility — aided by the fact that I had
EC2 to draw upon.  As part of my weekly testing of snapshot images, I
now spin up EC2 instances and have them build their own AMIs — and
then use <a href="https://diffoscope.org/">diffoscope</a> to compare
the disk images they built against the ones they were launched from.
This has already found several issues — including some which
appeared partway through the year and were identified quickly thanks to the
regular testing — of which I've fixed a few and some others I've
passed on to other developers to tackle.
</p><p>
Of course, in addition to the big projects there's also a plethora of smaller
issues to tackle.  Build breakage (weekly snapshot builds are good at finding
this!); reviewing patches to the ENA driver; helping Dave Cottlehuber add
support for building OCI Containers and uploading them to repositories;
teaching my <tt>bsdec2-image-upload</tt> tool to gracefully handle internal
AWS errors; reporting an AWS security issue I stumbled across... some days
everything falls under the umbrella of "other stuff which needs to get done",
but a lot of it is just as important as the larger projects.
</p><p>
So what's next?  Well, I'm still the FreeBSD release engineering lead and the
maintainer of the FreeBSD/EC2 platform — just with rather less time to
devote to this work.  FreeBSD releases will continue to happen — 15.0
should land in December, followed in 2026 by 14.4, 15.1, 14.5, and 15.2
— but I probably won't have time to jump in and fix things as much, so
late-landing features are more likely to get removed from a release rather
than fixed in time for the release; we were
only able to ship OCI Containers starting in FreeBSD 14.2 because I had
funded hours to make sure all the pieces landed intact, and that sort of
involvement won't be possible.  On the EC2 side, now that I have regression
testing of boot performance set up, I'll probably catch any issues which need
to be fixed there; but the rest of my "features to implement" list —
automatically growing filesystems when EBS volumes expand, better automatic
configuration with multiple network interfaces (and network interface hot
plug), rolling "pre-patched" AMIs (right now FreeBSD instances update
themselves when they first boot), putting together a website to help users
generate EC2 <tt>user-data</tt> files (e.g., for installing packages and
launching daemons), returning to my work on FreeBSD/Firecracker and making
it a supported FreeBSD platform, etc. — is likely to stagnate unless
I find more time.
</p><p>
I've been incredibly lucky to get this sponsorship from Amazon; it's far more
than <a href="https://xkcd.com/2347/">most open source developers</a> ever
get.  I wish it wasn't ending; but I'm proud of the work I've done and I'll
always be grateful to Amazon for giving me this opportunity. 
</p>



<p><a href="https://disqus.com/">blog comments powered by <span>Disqus</span></a>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Highly efficient matrix transpose in Mojo (106 pts)]]></title>
            <link>https://veitner.bearblog.dev/highly-efficient-matrix-transpose-in-mojo/</link>
            <guid>44204155</guid>
            <pubDate>Fri, 06 Jun 2025 19:28:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://veitner.bearblog.dev/highly-efficient-matrix-transpose-in-mojo/">https://veitner.bearblog.dev/highly-efficient-matrix-transpose-in-mojo/</a>, See on <a href="https://news.ycombinator.com/item?id=44204155">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2025-06-06T17:46Z">
                    06 Jun, 2025
                </time>
            </i>
        </p>
    

    <p>In this blogpost I will step by step show you how to implement a highly efficient transpose kernel for the <code>Hopper</code> architecture using Mojo.
The best kernel archives a bandwidth of <code>2775.49 GB/s</code>, i.e. <code>84.1056%</code>.  The optimisations are the same that I applied to archive a bandwidth of <code>2771.35 GB/s</code> using pure <code>CUDA</code> on the same <code>H100</code> that I use here. That shows that Mojo can archive <code>CUDA</code> like performance on exactly the same task. You may compare the kernels with the previous <a href="https://github.com/simveit/effective_transpose/tree/main">kernels</a> I wrote and read my other <a href="https://veitner.bearblog.dev/making-matrix-transpose-really-fast-on-hopper-gpus/">blogpost</a> as well where I explain the concepts in detail. Here I will only briefly review them and instead focus on the implementation details. For readers without knowledge how to use <code>TMA</code> in Mojo I refer you to my previous <a href="https://veitner.bearblog.dev/use-tma-without-cuda/">blogpost</a> on this topic.</p>
<h2 id="naive-approach">Naive approach</h2><p>Before calling the kernel we need to initialise two <code>TMA descriptors</code>, this concept is similar to <code>cuTensorMapEncodeTiled</code> we can use in <code>CUDA</code>.</p>
<div><pre><span></span><span>var</span> <span>descriptor</span> <span>=</span> <span>create_tma_descriptor</span><span>[</span><span>DType</span><span>.</span><span>float32</span><span>,</span> <span>2</span><span>](</span>
	<span>gmem_dev</span><span>,</span>
	<span>(</span><span>GMEM_HEIGHT</span><span>,</span> <span>GMEM_WIDTH</span><span>),</span>
	<span>(</span><span>GMEM_WIDTH</span><span>,</span> <span>1</span><span>),</span>
	<span>(</span><span>SMEM_HEIGHT</span><span>,</span> <span>SMEM_WIDTH</span><span>),</span>
<span>)</span>
<span>var</span> <span>descriptor_tr</span> <span>=</span> <span>create_tma_descriptor</span><span>[</span><span>DType</span><span>.</span><span>float32</span><span>,</span> <span>2</span><span>](</span>
	<span>gmem_tr_dev</span><span>,</span>
	<span>(</span><span>GMEM_WIDTH</span><span>,</span> <span>GMEM_HEIGHT</span><span>),</span>
	<span>(</span><span>GMEM_HEIGHT</span><span>,</span> <span>1</span><span>),</span>
	<span>(</span><span>SMEM_WIDTH</span><span>,</span> <span>SMEM_HEIGHT</span><span>),</span>
<span>)</span>
</pre></div>
<p>We have two descriptors. Both in row major format, the one the transpose of the other. The corresponding <code>smems</code> in relation of transpose as well.
As a quick reminder he is the algorithm we are going to implement.
We take a tile, perform transpose inside the tile and put it at the opposite position in the matrix, i.e. at the transposed position</p>
<p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/veitner/36.webp" alt="Screenshot 2025-06-06 at 19"></p>
<p>Below is the code that archives that.</p>
<h3 id="load-to-shared-memory">Load to shared memory</h3><div><pre><span></span><span>@__llvm_arg_metadata</span><span>(</span><span>descriptor</span><span>,</span> <span>`</span><span>nvvm</span><span>.</span><span>grid_constant</span><span>`</span><span>)</span>
<span>@__llvm_arg_metadata</span><span>(</span><span>descriptor_tr</span><span>,</span> <span>`</span><span>nvvm</span><span>.</span><span>grid_constant</span><span>`</span><span>)</span>
<span>fn</span> <span>transpose_kernel_naive</span><span>[</span>
    <span>block_size</span><span>:</span> <span>Int</span>
<span>](</span><span>descriptor</span><span>:</span> <span>TMADescriptor</span><span>,</span> <span>descriptor_tr</span><span>:</span> <span>TMADescriptor</span><span>):</span>
    <span>var</span> <span>shmem</span> <span>=</span> <span>stack_allocation</span><span>[</span>
        <span>block_size</span> <span>*</span> <span>block_size</span><span>,</span>
        <span>DType</span><span>.</span><span>float32</span><span>,</span>
        <span>alignment</span><span>=</span><span>1024</span><span>,</span>
        <span>address_space</span> <span>=</span> <span>_GPUAddressSpace</span><span>.</span><span>SHARED</span><span>,</span>
    <span>]()</span>
    <span>var</span> <span>shmem_tr</span> <span>=</span> <span>stack_allocation</span><span>[</span>
        <span>block_size</span> <span>*</span> <span>block_size</span><span>,</span>
        <span>DType</span><span>.</span><span>float32</span><span>,</span>
        <span>alignment</span><span>=</span><span>1024</span><span>,</span>
        <span>address_space</span> <span>=</span> <span>_GPUAddressSpace</span><span>.</span><span>SHARED</span><span>,</span>
    <span>]()</span>
    <span>var</span> <span>mbar</span> <span>=</span> <span>stack_allocation</span><span>[</span>
        <span>1</span><span>,</span> <span>Int64</span><span>,</span> <span>address_space</span> <span>=</span> <span>_GPUAddressSpace</span><span>.</span><span>SHARED</span>
    <span>]()</span>
    <span>var</span> <span>descriptor_ptr</span> <span>=</span> <span>UnsafePointer</span><span>(</span><span>to</span><span>=</span><span>descriptor</span><span>)</span><span>.</span><span>bitcast</span><span>[</span><span>NoneType</span><span>]()</span>
    <span>var</span> <span>descriptor_tr_ptr</span> <span>=</span> <span>UnsafePointer</span><span>(</span><span>to</span><span>=</span><span>descriptor_tr</span><span>)</span><span>.</span><span>bitcast</span><span>[</span><span>NoneType</span><span>]()</span>

    <span>x</span> <span>=</span> <span>block_idx</span><span>.</span><span>x</span> <span>*</span> <span>block_size</span>
    <span>y</span> <span>=</span> <span>block_idx</span><span>.</span><span>y</span> <span>*</span> <span>block_size</span>

    <span>col</span> <span>=</span> <span>thread_idx</span><span>.</span><span>x</span> <span>%</span> <span>block_size</span>
    <span>row</span> <span>=</span> <span>thread_idx</span><span>.</span><span>x</span> <span>//</span> <span>block_size</span>

    <span># LOAD</span>
    <span>if</span> <span>thread_idx</span><span>.</span><span>x</span> <span>==</span> <span>0</span><span>:</span>
        <span>mbarrier_init</span><span>(</span><span>mbar</span><span>,</span> <span>1</span><span>)</span>
        <span>mbarrier_arrive_expect_tx_shared</span><span>(</span><span>mbar</span><span>,</span> <span>block_size</span> <span>*</span> <span>block_size</span> <span>*</span> <span>4</span><span>)</span>
        <span>cp_async_bulk_tensor_shared_cluster_global</span><span>(</span>
            <span>shmem</span><span>,</span> <span>descriptor_ptr</span><span>,</span> <span>mbar</span><span>,</span> <span>Index</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>)</span>
        <span>)</span>
    <span>barrier</span><span>()</span>
    <span>mbarrier_try_wait_parity_shared</span><span>(</span><span>mbar</span><span>,</span> <span>0</span><span>,</span> <span>10000000</span><span>)</span>
</pre></div>
<p>We annotate the descriptors with <code>nvvm.grid_constant</code> similar to what we would do in <code>CUDA</code>.
After allocating the shared memories we define the upper left coordinate of the tile using <code>x</code> and <code>y</code> and get <code>row</code> and <code>column</code> the current thread is responsible fore.
We'll than copy over the tile to the shared memory array.
This kernel archives a bandwidth of <code>1056.08 GB/s</code> which is faster than the <code>875.46 GB/s</code> we archived using <code>CUDA</code>. I believe that to be the reason because we use the <code>PTX</code> api for <code>TMA</code> transfers in Mojo. You can read about the difference between these in the <code>CUDA</code> api in <a href="https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog">this excellent blogpost</a>.</p>
<h3 id="compute-transpose-in-shared-memory">Compute transpose in shared memory</h3><div><pre><span></span><span># COMPUTE</span>
<span>shmem_tr</span><span>[</span><span>col</span> <span>*</span> <span>block_size</span> <span>+</span> <span>row</span><span>]</span> <span>=</span> <span>shmem</span><span>[</span><span>row</span> <span>*</span> <span>block_size</span> <span>+</span> <span>col</span><span>]</span>

<span># FENCE</span>
<span>barrier</span><span>()</span>
<span>tma_store_fence</span><span>()</span>
</pre></div>
<p>We compute the transpose using our two arrays. We'll than create a <code>fence</code> to let the <code>TMA</code> know we are finished with computation.</p>
<h3 id="store-to-gmem">Store to gmem</h3><div><pre><span></span><span># STORE</span>
<span>if</span> <span>thread_idx</span><span>.</span><span>x</span> <span>==</span> <span>0</span><span>:</span>
	<span>cp_async_bulk_tensor_global_shared_cta</span><span>(</span>
		<span>shmem_tr</span><span>,</span> <span>descriptor_tr_ptr</span><span>,</span> <span>Index</span><span>(</span><span>y</span><span>,</span> <span>x</span><span>)</span>
	<span>)</span>
	<span>cp_async_bulk_commit_group</span><span>()</span>

<span>cp_async_bulk_wait_group</span><span>[</span><span>0</span><span>]()</span>
</pre></div>
<p>We store the transposed result to the GMEM using the transposed TMA descriptor.</p>
<h2 id="swizzling">Swizzling</h2><p>For a more detailed explanation of what swizzling is and how it works please in my previous <a href="https://veitner.bearblog.dev/making-matrix-transpose-really-fast-on-hopper-gpus/">blogpost on matrix transpose</a> the concept is the same for Mojo. In the repo I link to at the end there is also one program which you can use to understand swizzling yourself.
Only two things need to be adjusted to make swizzling work:</p>
<ul>
<li>The descriptors need to be provided with the appropriate swizzling mode</li>
<li>Inside the kernel we need to use swizzled indices</li>
</ul>
<p>This can be implemented as follows</p>
<div><pre><span></span><span>var</span> <span>descriptor</span> <span>=</span> <span>create_tma_descriptor</span><span>[</span>
	<span>DType</span><span>.</span><span>float32</span><span>,</span> <span>2</span><span>,</span> <span>TensorMapSwizzle</span><span>.</span><span>SWIZZLE_128B</span>
<span>](</span>
	<span>gmem_dev</span><span>,</span>
	<span>(</span><span>GMEM_HEIGHT</span><span>,</span> <span>GMEM_WIDTH</span><span>),</span>
	<span>(</span><span>GMEM_WIDTH</span><span>,</span> <span>1</span><span>),</span>
	<span>(</span><span>SMEM_HEIGHT</span><span>,</span> <span>SMEM_WIDTH</span><span>),</span>
<span>)</span>
<span>var</span> <span>descriptor_tr</span> <span>=</span> <span>create_tma_descriptor</span><span>[</span>
	<span>DType</span><span>.</span><span>float32</span><span>,</span> <span>2</span><span>,</span> <span>TensorMapSwizzle</span><span>.</span><span>SWIZZLE_128B</span>
<span>](</span>
	<span>gmem_tr_dev</span><span>,</span>
	<span>(</span><span>GMEM_WIDTH</span><span>,</span> <span>GMEM_HEIGHT</span><span>),</span>
	<span>(</span><span>GMEM_HEIGHT</span><span>,</span> <span>1</span><span>),</span>
	<span>(</span><span>SMEM_WIDTH</span><span>,</span> <span>SMEM_HEIGHT</span><span>),</span>
<span>)</span>
</pre></div>
<p>We can compute swizzled indices like this:</p>
<div><pre><span></span><span>fn</span> <span>calculate_row_swizzle</span><span>[</span><span>block_size</span><span>:</span> <span>Int</span><span>](</span><span>col</span><span>:</span> <span>Int</span><span>,</span> <span>row</span><span>:</span> <span>Int</span><span>)</span> <span>-&gt;</span> <span>Int</span><span>:</span>
    <span>i16_tr</span> <span>=</span> <span>(</span><span>col</span> <span>*</span> <span>BLOCK_SIZE</span> <span>+</span> <span>row</span><span>)</span> <span>*</span> <span>4</span> <span>&gt;&gt;</span> <span>4</span>
    <span>y16_tr</span> <span>=</span> <span>i16_tr</span> <span>&gt;&gt;</span> <span>3</span>
    <span>x16_tr</span> <span>=</span> <span>i16_tr</span> <span>&amp;</span> <span>7</span>
    <span>x16_swz_tr</span> <span>=</span> <span>y16_tr</span> <span>^</span> <span>x16_tr</span>
    <span>return</span> <span>((</span><span>x16_swz_tr</span> <span>*</span> <span>4</span><span>)</span> <span>&amp;</span> <span>(</span><span>BLOCK_SIZE</span> <span>-</span> <span>1</span><span>))</span> <span>+</span> <span>(</span><span>row</span> <span>&amp;</span> <span>3</span><span>)</span>


<span>fn</span> <span>calculate_col_swizzle</span><span>[</span><span>block_size</span><span>:</span> <span>Int</span><span>](</span><span>col</span><span>:</span> <span>Int</span><span>,</span> <span>row</span><span>:</span> <span>Int</span><span>)</span> <span>-&gt;</span> <span>Int</span><span>:</span>
    <span>i16</span> <span>=</span> <span>(</span><span>row</span> <span>*</span> <span>BLOCK_SIZE</span> <span>+</span> <span>col</span><span>)</span> <span>*</span> <span>4</span> <span>&gt;&gt;</span> <span>4</span>
    <span>y16</span> <span>=</span> <span>i16</span> <span>&gt;&gt;</span> <span>3</span>
    <span>x16</span> <span>=</span> <span>i16</span> <span>&amp;</span> <span>7</span>
    <span>x16_swz</span> <span>=</span> <span>y16</span> <span>^</span> <span>x16</span>
    <span>return</span> <span>((</span><span>x16_swz</span> <span>*</span> <span>4</span><span>)</span> <span>&amp;</span> <span>(</span><span>block_size</span> <span>-</span> <span>1</span><span>))</span> <span>+</span> <span>(</span><span>col</span> <span>&amp;</span> <span>3</span><span>)</span>
</pre></div>
<p>and than use the swizzled indices inside our kernel like so:</p>
<div><pre><span></span><span>col_swizzle</span> <span>=</span> <span>calculate_col_swizzle</span><span>[</span><span>block_size</span><span>](</span><span>col</span><span>,</span> <span>row</span><span>)</span>
<span>row_swizzle</span> <span>=</span> <span>calculate_row_swizzle</span><span>[</span><span>block_size</span><span>](</span><span>col</span><span>,</span> <span>row</span><span>)</span>
<span>...</span>
<span># COMPUTE</span>
<span>shmem_tr</span><span>[</span><span>col</span> <span>*</span> <span>block_size</span> <span>+</span> <span>row_swizzle</span><span>]</span> <span>=</span> <span>shmem</span><span>[</span>
	<span>row</span> <span>*</span> <span>block_size</span> <span>+</span> <span>col_swizzle</span>
<span>]</span>
</pre></div>
<p>Everything else is exactly the same.</p>
<p>This kernel archives <code>1437.55 GB/s</code> compared to the <code>1251.76 GB/s</code> we get in <code>CUDA</code>.</p>
<h2 id="processes-a-batch-of-columns-per-thread">Processes a batch of columns per thread</h2><p>An important and common optimisation one can apply in memory bound kernels is thread coarsening which is essentially putting more work on each thread.
We can modify the previous kernel as follows to do that:</p>
<div><pre><span></span>    <span># COMPUTE</span>
    <span>@parameter</span>
    <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>batch_size</span><span>):</span>
        <span>col_</span> <span>=</span> <span>col</span> <span>+</span> <span>i</span>
        <span>row_</span> <span>=</span> <span>row</span>
        <span>col_swizzle</span> <span>=</span> <span>calculate_col_swizzle</span><span>[</span><span>block_size</span><span>](</span><span>col_</span><span>,</span> <span>row_</span><span>)</span>
        <span>row_swizzle</span> <span>=</span> <span>calculate_row_swizzle</span><span>[</span><span>block_size</span><span>](</span><span>col_</span><span>,</span> <span>row_</span><span>)</span>
        <span>shmem_tr</span><span>[</span><span>col</span> <span>*</span> <span>block_size</span> <span>+</span> <span>row_swizzle</span><span>]</span> <span>=</span> <span>shmem</span><span>[</span>
            <span>row</span> <span>*</span> <span>block_size</span> <span>+</span> <span>col_swizzle</span>
        <span>]</span>
</pre></div>
<p>Note that we launch less threads with this approach (we divide by a factor of <code>batch_size</code>) to account for the fact we are processing multiple columns per thread now.</p>
<p>This kernel archives a bandwidth of <code>2775.49 GB/s</code> compared to the <code>2771.35 GB/s</code> we archived in the equivalent <code>CUDA</code> kernel.</p>
<h2 id="conclusion">Conclusion</h2><p>I hope this blogpost showed you how to archive high performance on a common task in GPU computing using Mojo.
Feel free to contact me on <a href="https://www.linkedin.com/in/simon-veitner-174a681b6/">Linkedin</a> to chat about GPU programming or other topics related to MLSys.</p>
<p>The full code for the blogpost can be find on my <a href="https://github.com/simveit/efficient_transpose_mojo">Github</a>.</p>


    

    
        

        
            


        
    


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Illusion of Thinking: Understanding the Limitations of Reasoning LLMs [pdf] (205 pts)]]></title>
            <link>https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf</link>
            <guid>44203562</guid>
            <pubDate>Fri, 06 Jun 2025 18:18:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf">https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=44203562">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[SaaS is just vendor lock-in with better branding (184 pts)]]></title>
            <link>https://rwsdk.com/blog/saas-is-just-vendor-lock-in-with-better-branding</link>
            <guid>44203494</guid>
            <pubDate>Fri, 06 Jun 2025 18:10:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rwsdk.com/blog/saas-is-just-vendor-lock-in-with-better-branding">https://rwsdk.com/blog/saas-is-just-vendor-lock-in-with-better-branding</a>, See on <a href="https://news.ycombinator.com/item?id=44203494">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Developers are told "to focus on the product" and let SaaS vendors handle the rest, but integrating third-party services, whether it's auth, queuing, file storage, or image optimization, comes at a cost. Not just in dollars but in time, friction, and mental overhead.</p>
<p>There are five hidden taxes you pay every time you integrate a SaaS into your stack.</p>
<h2>1. The Discovery Tax</h2>
<p>Before you can integrate anything, you first have to figure out what they're actually selling?</p>
<ol>
<li>What problems are they solving?</li>
<li>Is it compatible with your stack?</li>
<li>Is their price sane at your scale? a</li>
<li>Are their docs clear and do they reveal any implementation weirdness?</li>
</ol>
<p>This unpaid research work is usually non-transferable. What you learn about "Uploady" or "MegaQueue" doesn't help you next time when you're evaluating something else. It's also subjective. It's marketing, and does the marketing message resonate with you?</p>
<h2>2. The Sign-Up Tax</h2>
<p>You've decided on a service, and this is the moment when you hand over your email and credit card.</p>
<ol>
<li>Do they support usage-based pricing or only lock-in tiers?</li>
<li>Can your team members access the dashboard, or do you have to pay more for that functionality? Despite only using the service the same amount!</li>
<li>Can you even test the product without hitting a paywall?</li>
</ol>
<p>You're now on the hook, even if you haven't written a single line of code.</p>
<h2>3. The Integration Tax</h2>
<p>Now the real work begins.</p>
<ol>
<li>You read the docs.</li>
<li>You install the libraries</li>
<li>You wire it into your framework.</li>
<li>And figure out the edge cases that the docs don't mention, because docs are marketing!</li>
</ol>
<p>Often you're left fighting your own tooling. They're aiming for the lowest common denominator, and you're bleeding edge. Or the other way around!</p>
<h2>4. The Local Development Tax</h2>
<p>You need the SaaS service to work locally. Does it even offer a local emulator? Can you stub it out in tests? Do you need to tunnel to the cloud just to test one feature?</p>
<p>Now you've got branching configuration logic, one for production, one for staging, one for local… If you're lucky.</p>
<h2>5. The Production Tax</h2>
<p>This is the part where you're "done," except you're not.</p>
<ol>
<li>Can you use this in your staging environment? What about pull request previews?</li>
<li>You need to securely manage the API keys.</li>
<li>Monitoring, logging, and alerting</li>
<li>Wondering why something worked in your laptop but fails in production?</li>
</ol>
<p>You've integrated the service, but now you're on the hook for its reliability in production.</p>
<h2>Conclusion</h2>
<p>The pitch of modern SaaS is "don't reinvent the wheel." But every wheel you bolt on comes with some friction. It's not just a service: It's a contract. It's a dependency. It's a subtle architectural shift, and it comes with taxes.</p>
<p>No matter what choice you make, it's always going to be vendor-locked in. Switching out something, even if it's open source and self-hosted, means that you're rewriting a lot of code.</p>
<p>So, my argument is, don't make those decisions. Just pick a platform. The thing that matters is the software that you want to write, not the framework or the services that it runs on.</p>
<p>Platforms like Cloudflare or Supabase shine. Where your database, queue, image service, and storage all live within the same platform and speak the same language. You avoid paying these taxes repeatedly. You simply pick the product that's already there.</p>
<ol>
<li>No context switching between vendors.</li>
<li>No API key wrangling.</li>
<li>No compatibility hacks or configuration forks.</li>
<li>Just fast, local feeling integrations that work the same in dev and production.</li>
</ol>
<p>It feels like everything is running on the same machine, and in a way it kind of is. That's the hidden superpower of integrated platforms. They collapse the distance between your code and your services. And in doing so, they give you back the one thing no SaaS vendor can sell you: "Flow."</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Breakthrough in search for HIV cure leaves researchers 'overwhelmed' (198 pts)]]></title>
            <link>https://www.theguardian.com/global-development/2025/jun/05/breakthrough-in-search-for-hiv-cure-leaves-researchers-overwhelmed</link>
            <guid>44202664</guid>
            <pubDate>Fri, 06 Jun 2025 16:44:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/global-development/2025/jun/05/breakthrough-in-search-for-hiv-cure-leaves-researchers-overwhelmed">https://www.theguardian.com/global-development/2025/jun/05/breakthrough-in-search-for-hiv-cure-leaves-researchers-overwhelmed</a>, See on <a href="https://news.ycombinator.com/item?id=44202664">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>A cure for HIV could be a step closer after researchers found a new way to force the virus out of hiding inside human cells.</p><p>The virus’s ability to conceal itself inside certain white blood cells has been one of the main challenges for scientists looking for a cure. It means there is a reservoir of the HIV in the body, capable of reactivation, that neither the immune system nor drugs can tackle.</p><p>Now researchers from the Peter Doherty Institute for Infection and Immunity in Melbourne, have demonstrated a way to make the virus visible, paving the way to fully clear it from the body.</p><p>It is based on mRNA technology, which came to prominence during the Covid-19 pandemic <a href="https://www.theguardian.com/science/2021/nov/01/flu-cancer-hiv-after-covid-success-what-next-for-mrna-vaccines" data-link-name="in body link">when it was used in vaccines</a> made by Moderna and Pfizer/BioNTech.</p><p>In a <a href="https://www.nature.com/articles/s41467-025-60001-2" data-link-name="in body link">paper published in Nature Communications</a>, the researchers have shown for the first time that mRNA can be delivered into the cells where HIV is hiding, by encasing it in a tiny, specially formulated fat bubble. The mRNA then instructs the cells to reveal the virus.</p><p>Globally, there are <a href="https://www.unaids.org/en/resources/fact-sheet" data-link-name="in body link">almost 40 million people</a> living with HIV, who must take medication for the rest of their lives in order to suppress the virus and ensure they do not develop symptoms or transmit it. For many it remains deadly, with UNAids figures suggesting one person died of HIV every minute in 2023.</p><p>It was “previously thought impossible” to deliver mRNA to the type of white blood cell that is home to HIV, said Dr Paula Cevaal, research fellow at the Doherty Institute and co-first author of the study, because those cells did not take up the fat bubbles, or lipid nanoparticles (LNPs), used to carry it.</p><p>The team have developed a new type of LNP that those cells will accept, known as LNP X. She said: “Our hope is that this new nanoparticle design could be a new pathway to an HIV cure.”</p><p>When a colleague first presented test results at the lab’s weekly meeting, Cevaal said, they seemed too good to be true.</p><p>“We sent her back into the lab to repeat it, and she came back the next week with results that were equally good. So we had to believe it. And of course, since then, we’ve repeated it many, many, many more times.</p><p>“We were overwhelmed by how [much of a] night and day difference it was – from not working before, and then all of a sudden it was working. And all of us were just sitting gasping like, ‘wow’.”</p><p>Further research will be needed to determine whether revealing the virus is enough to allow the body’s immune system to deal with it, or whether the technology will need to be combined with other therapies to eliminate HIV from the body.</p><p>The study is laboratory based and was carried out in cells donated by HIV patients. The path to using the technology as part of a cure for patients is long, and would require successful tests in animals followed by safety trials in humans, likely to take years, before efficacy trials could even begin.</p><p>“In the field of biomedicine, many things eventually don’t make it into the clinic – that is the unfortunate truth; I don’t want to paint a prettier picture than what is the reality,” stressed Cevaal. “But in terms of specifically the field of HIV cure, we have never seen anything close to as good as what we are seeing, in terms of how well we are able to reveal this virus.</p><p>“So from that point of view, we’re very hopeful that we are also able to see this type of response in an animal, and that we could eventually do this in humans.”</p><p>Dr Michael Roche of the University of Melbourne and co-senior author of the research, said the discovery could have broader implications beyond HIV, with the relevant white blood cells also involved in other diseases including cancers.</p><p>Dr Jonathan Stoye, a retrovirologist and emeritus scientist at the Francis Crick Institute, who was not involved in the study, said the approach taken by the Melbourne team appeared be a major advance on existing strategies to force the virus out of hiding, but further studies would be needed to determine how best to kill it after that.</p><p>He added: “Ultimately, one big unknown remains. Do you need to eliminate the entire reservoir for success or just the major part? If just 10% of the latent reservoir survives will that be sufficient to seed new infection? Only time will tell.</p><p>“However, that does not detract from the significance of the current study, which represents a major potential advance in delivery of mRNA for therapeutic purposes to blood cells.”</p><p>Prof Tomáš Hanke of the Jenner Institute, University of Oxford, disputed the idea that getting RNA into white blood cells had been a significant challenge. He said the hope that all cells in the body where HIV was hiding could be reached in this way was “merely a dream”.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Decreasing Gitlab repo backup times from 48 hours to 41 minutes (437 pts)]]></title>
            <link>https://about.gitlab.com/blog/2025/06/05/how-we-decreased-gitlab-repo-backup-times-from-48-hours-to-41-minutes/</link>
            <guid>44201975</guid>
            <pubDate>Fri, 06 Jun 2025 15:43:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://about.gitlab.com/blog/2025/06/05/how-we-decreased-gitlab-repo-backup-times-from-48-hours-to-41-minutes/">https://about.gitlab.com/blog/2025/06/05/how-we-decreased-gitlab-repo-backup-times-from-48-hours-to-41-minutes/</a>, See on <a href="https://news.ycombinator.com/item?id=44201975">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-67d56f21="" data-v-53094866="" data-v-74bd29c6=""><p>Repository backups are a critical component of any robust disaster recovery strategy. However, as repositories grow in size, the process of creating reliable backups becomes increasingly challenging.  Our own <a href="https://gitlab.com/gitlab-org/gitlab">Rails repository</a> was taking 48 hours to back up — forcing impossible choices between backup frequency and system performance. We wanted to tackle this issue for our customers and for our own users internally.</p>
<p>Ultimately, we traced the issue to a 15-year-old Git function with O(N²) complexity and fixed it with an algorithmic change, <strong>reducing backup times exponentially</strong>. The result: lower costs, reduced risk, and backup strategies that actually scale with your codebase.</p>
<p>This turned out to be a Git scalability issue that affects anyone with large repositories. Here's how we tracked it down and fixed it.</p>
<h2 id="backup-at-scale" tabindex="-1">Backup at scale <a href="#backup-at-scale"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h2>
<p>First, let's look at the problem. As organizations scale their repositories and backups grow more complex, here are some of the challenges they can face:</p>
<ul>
<li><strong>Time-prohibitive backups:</strong> For very large repositories, creating a repository backup could take several hours, which can hinder the ability to schedule regular backups.</li>
<li><strong>Resource intensity:</strong> Extended backup processes can consume substantial server resources, potentially impacting other operations.</li>
<li><strong>Backup windows:</strong> Finding adequate maintenance windows for such lengthy processes can be difficult for teams running 24/7 operations.</li>
<li><strong>Increased failure risk:</strong> Long-running processes are more susceptible to interruptions from network issues, server restarts, and system errors, which can force teams to restart the entire very long backup process from scratch.</li>
<li><strong>Race conditions:</strong> Because it takes a long time to create a backup, the repository might have changed a lot during the process, potentially creating an invalid backup or interrupting the backup because objects are no longer available.</li>
</ul>
<p>These challenges can lead to compromising on backup frequency or completeness – an unacceptable trade-off when it comes to data protection. Extended backup windows can force customers into workarounds. Some might adopt external tooling, while others might reduce backup frequency, resulting in potential inconsistent data protection strategies across organizations.</p>
<p>Now, let's dig into how we identified a performance bottleneck, found a resolution, and deployed it to help cut backup times.</p>
<h2 id="the-technical-challenge" tabindex="-1">The technical challenge <a href="#the-technical-challenge"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h2>
<p>GitLab's repository backup functionality relies on the <a href="https://git-scm.com/docs/git-bundle"><code>git bundle create</code></a> command, which captures a complete snapshot of a repository, including all objects and references like branches and tags. This bundle serves as a restoration point for recreating the repository in its exact state.</p>
<p>However, the implementation of the command suffered from poor scalability related to reference count, creating a performance bottleneck. As repositories accumulated more references, processing time increased exponentially. In our largest repositories containing millions of references, backup operations could extend beyond 48 hours.</p>
<h3 id="root-cause-analysis" tabindex="-1">Root cause analysis <a href="#root-cause-analysis"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h3>
<p>To identify the root cause of this performance bottleneck, we analyzed a flame graph of the command during execution.</p>
<p><img src="https://images.ctfassets.net/r9o86ar0p03f/7IS0wTEb44v1DTFKdklt1s/f8c724b1578ef0c8f550d9237f03dbeb/image1.jpg" alt="Flame graph showing command during execution"></p>
<p>A flame graph displays the execution path of a command through its stack trace. Each bar corresponds to a function in the code, with the bar's width indicating how much time the command spent executing within that particular function.</p>
<p>When examining the flame graph of <code>git bundle create</code> running on a repository with 10,000 references, approximately 80% of the execution time is consumed by the <code>object_array_remove_duplicates()</code> function. This function was introduced to Git in the <a href="https://gitlab.com/gitlab-org/git/-/commit/b2a6d1c686">commit b2a6d1c686</a> (bundle: allow the same ref to be given more than once, 2009-01-17).</p>
<p>To understand this change, it's important to know that <code>git bundle create</code> allows users to specify which references to include in the bundle. For complete repository bundles, the <code>--all</code> flag packages all references.</p>
<p>The commit addressed a problem where users providing duplicate references through the command line – such as <code>git bundle create main.bundle main main</code> - would create a bundle without properly handling the duplicated main reference. Unbundling this bundle in a Git repository would break, because it tries to write the same ref twice. The code to avoid duplication uses nested <code>for</code> loops that iterate through all references to identify duplicates. This O(N²) algorithm becomes a significant performance bottleneck in repositories with large reference counts, consuming substantial processing time.</p>
<h3 id="the-fix-from-o(n%C2%B2)-to-efficient-mapping" tabindex="-1">The fix: From O(N²) to efficient mapping <a href="#the-fix-from-o(n%C2%B2)-to-efficient-mapping"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h3>
<p>To resolve this performance issue, we contributed an upstream fix to Git that replaces the nested loops with a map data structure. Each reference is added to the map, which automatically ensures only a single copy of each reference is retained for processing.</p>
<p>This change dramatically enhances the performance of <code>git bundle create</code> and enables much better scalability in repositories with large reference counts. Benchmark testing on a repository with 10,000 references demonstrates a 6x performance improvement.</p>
<pre><code>Benchmark 1: bundle (refcount = 100000, revision = master)
  Time (mean ± σ): 	14.653 s ±  0.203 s	[User: 13.940 s, System: 0.762 s]
  Range (min … max):   14.237 s … 14.920 s	10 runs

Benchmark 2: bundle (refcount = 100000, revision = HEAD)
  Time (mean ± σ):  	2.394 s ±  0.023 s	[User: 1.684 s, System: 0.798 s]
  Range (min … max):	2.364 s …  2.425 s	10 runs

Summary
  bundle (refcount = 100000, revision = HEAD) ran
	6.12 ± 0.10 times faster than bundle (refcount = 100000, revision = master)
</code></pre>
<p>The patch was accepted and <a href="https://gitlab.com/gitlab-org/git/-/commit/bb74c0abbc31da35be52999569ea481ebd149d1d">merged</a> into upstream Git. At GitLab, we backported this fix to ensure our customers could benefit immediately, without waiting for the next Git release.</p>
<h2 id="the-result-dramatically-decreased-backup-times" tabindex="-1">The result: Dramatically decreased backup times <a href="#the-result-dramatically-decreased-backup-times"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h2>
<p>The performance gains from this improvement have been nothing short of transformative:</p>
<ul>
<li><strong>From 48 hours to 41 minutes:</strong> Creating a backup of our largest repository (<code>gitlab-org/gitlab</code>) now takes just 1.4% of the original time.</li>
<li><strong>Consistent performance:</strong> The improvement scales reliably across repository sizes.</li>
<li><strong>Resource efficiency:</strong> We significantly reduced server load during backup operations.</li>
<li><strong>Broader applicability:</strong> While backup creation sees the most dramatic improvement, all bundle-based operations that operate on many references benefit.</li>
</ul>
<h2 id="what-this-means-for-gitlab-customers" tabindex="-1">What this means for GitLab customers <a href="#what-this-means-for-gitlab-customers"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h2>
<p>For GitLab customers, this enhancement delivers immediate and tangible benefits on how organizations approach repository backup and disaster recovery planning:</p>
<ul>
<li><strong>Transformed backup strategies</strong>
<ul>
<li>Enterprise teams can establish comprehensive nightly schedules without impacting development workflows or requiring extensive backup windows.</li>
<li>Backups can now run seamlessly in the background during nightly schedules, instead of needing to be dedicated and lengthy.</li>
</ul>
</li>
<li><strong>Enhanced business continuity</strong>
<ul>
<li>With backup times reduced from days to minutes, organizations significantly minimize their recovery point objectives (RPO). This translates to reduced business risk – in a disaster scenario, you're potentially recovering hours of work instead of days.</li>
</ul>
</li>
<li><strong>Reduced operational overhead</strong>
<ul>
<li>Less server resource consumption and shorter maintenance windows.</li>
<li>Shorter backup windows mean reduced compute costs, especially in cloud environments, where extended processing time translates directly to higher bills.</li>
</ul>
</li>
<li><strong>Future-proofed infrastructure</strong>
<ul>
<li>Growing repositories no longer force difficult choices between backup frequency and system performance.</li>
<li>As your codebase expands, your backup strategy can scale seamlessly alongside it</li>
</ul>
</li>
</ul>
<p>Organizations can now implement more robust backup strategies without compromising on performance or completeness. What was once a challenging trade-off has become a straightforward operational practice.</p>
<p>Starting with the <a href="https://about.gitlab.com/releases/2025/05/15/gitlab-18-0-released/">GitLab 18.0</a> release, all GitLab customers regardless of their license tier can already fully take advantage of these improvements for their <a href="https://docs.gitlab.com/administration/backup_restore/backup_gitlab/">backup</a> strategy and execution. There is no further change in configuration required.</p>
<h2 id="what's-next" tabindex="-1">What's next <a href="#what's-next"><svg width="24" height="24" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.2426 3.75736C11.4615 2.97631 10.1952 2.97631 9.41416 3.75736L7.99995 5.17157C7.60942 5.56209 6.97626 5.56209 6.58573 5.17157C6.19521 4.78105 6.19521 4.14788 6.58573 3.75736L7.99995 2.34314C9.56205 0.781046 12.0947 0.781046 13.6568 2.34314C15.2189 3.90524 15.2189 6.4379 13.6568 8L12.2426 9.41421C11.8521 9.80473 11.2189 9.80473 10.8284 9.41421C10.4379 9.02369 10.4379 8.39052 10.8284 8L12.2426 6.58578C13.0236 5.80473 13.0236 4.5384 12.2426 3.75736Z" fill="#333333"></path><path d="M10.5355 5.4645C10.926 5.85502 10.926 6.48819 10.5355 6.87871L6.87863 10.5356C6.4881 10.9261 5.85494 10.9261 5.46441 10.5356C5.07389 10.145 5.07389 9.51188 5.46441 9.12135L9.12127 5.4645C9.51179 5.07397 10.145 5.07397 10.5355 5.4645Z" fill="#333333"></path><path d="M3.75742 9.41422C2.97637 10.1953 2.97637 11.4616 3.75742 12.2426C4.53847 13.0237 5.8048 13.0237 6.58584 12.2426L8.00006 10.8284C8.39058 10.4379 9.02375 10.4379 9.41427 10.8284C9.8048 11.219 9.8048 11.8521 9.41427 12.2426L8.00006 13.6569C6.43796 15.219 3.9053 15.219 2.3432 13.6569C0.781107 12.0948 0.781107 9.56211 2.3432 8.00001L3.75742 6.5858C4.14794 6.19527 4.78111 6.19527 5.17163 6.5858C5.56216 6.97632 5.56215 7.60948 5.17163 8.00001L3.75742 9.41422Z" fill="#333333"></path></svg></a></h2>
<p>This breakthrough is part of our ongoing commitment to scalable, enterprise-grade Git infrastructure. While the improvement of 48 hours to 41 minutes for backup creation time represents a significant milestone, we continue to identify and address performance bottlenecks throughout our stack.</p>
<p>We're particularly proud that this enhancement was contributed upstream to the Git project, benefiting not just GitLab users but the broader Git community. This collaborative approach to development ensures that improvements are thoroughly reviewed, widely tested, and available to all.</p>
<blockquote>
<p>Deep infrastructure work like this is how we approach performance at GitLab. Join the GitLab 18 virtual launch event to see what other fundamental improvements we're shipping. <a href="https://about.gitlab.com/eighteen/">Register today!</a></p>
</blockquote>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[4-7-8 Breathing (231 pts)]]></title>
            <link>https://www.breathbelly.com/exercises/4-7-8-breathing</link>
            <guid>44201901</guid>
            <pubDate>Fri, 06 Jun 2025 15:36:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.breathbelly.com/exercises/4-7-8-breathing">https://www.breathbelly.com/exercises/4-7-8-breathing</a>, See on <a href="https://news.ycombinator.com/item?id=44201901">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Meta: Shut Down Your Invasive AI Discover Feed. Now (486 pts)]]></title>
            <link>https://www.mozillafoundation.org/en/campaigns/meta-shut-down-your-invasive-ai-discover-feed-now/</link>
            <guid>44201872</guid>
            <pubDate>Fri, 06 Jun 2025 15:33:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mozillafoundation.org/en/campaigns/meta-shut-down-your-invasive-ai-discover-feed-now/">https://www.mozillafoundation.org/en/campaigns/meta-shut-down-your-invasive-ai-discover-feed-now/</a>, See on <a href="https://news.ycombinator.com/item?id=44201872">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
    <p data-block-key="ox7g0"><b>Meta is quietly turning private AI chats into public content — and too many people don’t realize it’s happening.</b></p><p data-block-key="a6hs9">That’s why the Mozilla community is demanding that Meta:</p><ol><li data-block-key="f4j6n"><b>Shut down the Discover feed</b> until real privacy protections are in place.</li><li data-block-key="3q7uu"><b>Make all AI interactions private by default</b> with no public sharing option unless explicitly enabled through informed consent.</li><li data-block-key="ddf8c"><b>Provide full transparency</b> about how many users have unknowingly shared private information.</li><li data-block-key="cfiao"><b>Create a universal, easy-to-use opt-out system</b> for all Meta platforms that prevents user data from being used for AI training.</li><li data-block-key="8j7da"><b>Notify all users whose conversations may have been made public</b>, and allow them to delete their content permanently.</li></ol><p data-block-key="3i8bo">Meta is blurring the line between private and public — and it’s happening at the cost of our privacy. People have the right to know when they’re speaking in public, especially when they believe they’re speaking in private.</p><p data-block-key="9cait">If you agree, add your name to demand Meta shut down its invasive AI feed — and guarantee that no private conversations are made public without clear, explicit, and informed opt-in consent.</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VPN providers in France ordered to block pirate sports IPTV (108 pts)]]></title>
            <link>https://torrentfreak.com/major-vpn-providers-ordered-to-block-pirate-sports-streaming-sites-250516/</link>
            <guid>44201857</guid>
            <pubDate>Fri, 06 Jun 2025 15:31:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://torrentfreak.com/major-vpn-providers-ordered-to-block-pirate-sports-streaming-sites-250516/">https://torrentfreak.com/major-vpn-providers-ordered-to-block-pirate-sports-streaming-sites-250516/</a>, See on <a href="https://news.ycombinator.com/item?id=44201857">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://torrentfreak.com/images/vpn-divertx1.png.webp">
<img loading="lazy" decoding="async" src="https://torrentfreak.com/images/vpn-divertx1.png" alt="vpn-divertx1" width="250" height="238">
</picture>
Article L. 333-10 of the French Sport Code enables rightsholders to request blocking measures against named pirate sites if they can demonstrate “serious and repeated infringement” of their exploitation rights. </p>
<p>To prevent pirate sites from being accessed on French soil, rightsholders may request that “all proportionate measures” are implemented by <em>any</em> online entity in a position to help. The scope of <a href="https://www.legifrance.gouv.fr/codes/article_lc/LEGIARTI000044247629/">Article L. 333-10</a> was always meant to be broad. </p>
<p>The first logical targets were local ISPs which easily fell within scope. Then, when inevitable circumvention raised its head, utilizing infrastructure beyond the reach of regular ISPs, Article L. 333-10 already had that covered. </p>
<p>Following a Canal+ complaint, use of third party DNS resolvers at Cloudflare and Google <a href="https://torrentfreak.com/google-cloudflare-cisco-will-poison-dns-to-stop-piracy-block-circumvention-240613/">headed to court in 2024</a>. Both were labeled intermediaries by the Court, and under the still unstressed scope of Article L. 333-10, both were considered capable of contributing to the suppression of piracy.</p>
<h2>Canal+ / LFP Target Major VPN Providers</h2>
<p>Having effectively added public DNS resolvers to the French blocking machine, attention turned to the next targets. In February 2025, it emerged that Groupe Canal + and Société d’édition de Canal Plus (SECP) had <a href="https://torrentfreak.com/rightsholders-target-vpn-providers-in-french-court-to-block-piracy-250207/">filed a case</a> in November 2024 against NordVPN, CyberGhost, ProtonVPN, ExpressVPN, and Surfshark.</p>
<p>The Canal companies alleged that “numerous” websites, accessible from within France, illegally streamed matches from various sporting competitions to which they hold the rights. </p>
<p>Since the VPNs’ subscribers were among those viewing the infringing streams, the Canal companies asked the court to compel the providers to implement “all measures likely to prevent access [to the illegal streams] from French territory,” including in all French overseas territories, “by any effective means”.</p>
<p>The VPN providers objected to the application on various grounds. Nord and Surfshark requested a declaration that the Canal companies lacked standing to act; as such, their application should be declared inadmissible. Proton sought a similar declaration while noting that the company lacked the ability to defend the requested blocking measures. The same assertion was made by CyberGhost and Express; all argued that Article L. 333-10 does not apply to VPN providers.</p>
<p>Other objections concerned jurisdiction and whether French law is compatible with EU legislation. CyberGhost and Express suggested a stay of proceedings pending a response from the Court of Justice of the European Union (CJEU) in the Dutch case, <a href="https://uitspraken.rechtspraak.nl/details?id=ECLI:NL:HR:2024:1603">AFS et al</a>.</p>
<h2>Court Rejects VPN Providers’ Objections</h2>
<p>In its decision published Thursday, the Court found that matters concerning the CJEU would have no impact on the current case. Calls for the Canal companies’ application to be declared inadmissible due to a lack of standing, were also dismissed, while a review of the allocation of rights pertaining to the various sports competitions raised no concerns.</p>
<p>Arguments that the VPN providers had no standing to defend themselves, due to Article L. 333-10 of the French Sports Code being inapplicable to VPN providers, fared no better.</p>
<p>The court notes that Article L.333-10 does not impose any restrictions on the targeted entity, adding that VPN providers are expressly covered under the Digital Services Act.</p>
<p>“Blocking such a service for certain domain names means that the provider of this service prevents its users from accessing the disputed domain names when using their VPN tool. Internet users using these virtual private networks would therefore no longer be able to access the disputed sites through this intermediary,” the decision reads.</p>
<p>“Consequently, the defendant companies, in their capacity as providers of virtual private network services, are technical intermediaries capable of contributing to remedying the harm that Groupe Canal+, SECP and Canal+ Rights claim to have suffered.”</p>
<h2>Blocking Order Issued</h2>
<p>The Court’s instructions and the full list of domains can be found below.</p>
<p><em>Update: the French football league LFP obtained a similar order on the same day, we have added a statement at the bottom of this article.</em></p>
<blockquote><p><small><em>[The Court] orders the companies Cyberghost LLC, Cyberghost Srl, Expressco services, Express technologies, Nordvpn (Netherlands), Nordvpn (Republic of Panama), Surfshark Bv, Surfshark Ltd and Proton to implement, at the latest within three days following notification of this decision, all measures necessary to prevent, until the date of the last match of the championship of the Premier League for the 2024/2025 season, currently set for May 25, 2025, access to the websites and IPTV services identified [below] as well as to the IPTV sites and services not yet identified on the date of this decision, from French territory, including in the overseas communities, departments and regions, and/or by their users based on a contract taken out in this territory, by any effective means, and in particular by blocking the following domain names and associated subdomains…..</em></small></p></blockquote>
<p>The cost of blocking will be shared between the parties, with the details to be agreed at a later date. A request by the plaintiffs to compel the VPN providers to publish details of the case on their homepages for publicity purposes, was described as “inappropriate” and rejected by the Court.</p>
<p>The decision reveals that many of the domain names submitted by Canal for blocking, are already subject to blocking measures by French ISPs following notification by telecoms regulator ARCOM. Familiar brands include Footy Bite, Cric HD, Buffstreams, Futbollibre, Rojadirecta, and Crackstreams, among dozens of others. In these cases widespread piracy has already been established, but it appears that in-depth proof of infringement may not be a hard requirement.</p>
<p>“Since the burden of proof should not be unnecessarily complex and costly, the court cannot require the claimants to demonstrate access to the disputed IPTV sites and services by using each of the defendants’ virtual private networks, just as it does not request findings by using each of the internet service providers’ networks when a blocking is requested of them on the basis of Article L. 333-10 of the Sports Code,” the court notes.</p>
<p>Had comprehensive checks been carried out, questions may have been raised over the need to block pirate domains previously seized by the Alliance for Creativity and Entertainment. </p>
<center><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://torrentfreak.com/images/ace-sus.png.webp 1237w, https://torrentfreak.com/images/ace-sus-300x149.png.webp 300w" sizes="auto, (max-width: 670px) 100vw, 670px">
<img loading="lazy" decoding="async" src="https://torrentfreak.com/images/ace-sus.png" alt="ace-sus" width="670" height="333" srcset="https://torrentfreak.com/images/ace-sus.png 1237w, https://torrentfreak.com/images/ace-sus-300x149.png 300w, https://torrentfreak.com/images/ace-sus-600x298.png 600w, https://torrentfreak.com/images/ace-sus-150x74.png 150w" sizes="auto, (max-width: 670px) 100vw, 670px">
</picture>
</center>
<p>Other domains from the Canal+ list below were seized by ACE during the last few days, so blocking those domains will be unnecessary too. </p>
<p><em>The decision of the Court of Paris is available <a href="https://torrentfreak.com/images/Canal-Plus-v-VPN-Providers-judgment-250515-FR.pdf">here</a> (pdf, French)</em></p>
<p><em>Update: French football league LFP obtained a similar order on the same day and released the following <a href="https://www.lfp.fr/article/la-lfp-et-lfp-media-obtiennent-des-vp-ns-de-bloquer-les-sites-pirates">statement</a>.</em></p><p><em>“The LFP and LFP Media welcome this decision by the Paris Judicial Court, which is a first in France, if not in the world, and constitutes a major step forward in the fight against piracy by ordering VPN services to implement measures to block pirate sites.”</em></p>
<p><em>aliezstream.pro<br>
antenasport.shop<br>
antenasports.ru<br>
antenasports.shop<br>
antenatv.online<br>
antenatv.store<br>
antennasport.ru<br>
asportv.shop<br>
livetv802.me<br>
toparena.store<br>
emb.ap1357.me<br>
embx224539.ap1357.me<br>
1.qwebplay.xyz<br>
livetv807.me<br>
cdn.livetv807.me<br>
boxtv60.com<br>
infinity-ott.com<br>
vbnl23.com<br>
footy-bite.com<br>
freesportstime.com<br>
livestreamlinks.cc<br>
crichdplayer.com<br>
crichd.sc<br>
crichd.mobi<br>
crichd.sx<br>
crichd.tv<br>
me.crichd.tv<br>
nbabite.to<br>
reddit.nbabite.to<br>
s2watch.link<br>
soccerinhd.com<br>
sportlemons.to<br>
sports-prime.com<br>
topevents.us<br>
buffsports.me<br>
buffstreams.sx<br>
vipbox.sx<br>
vipbox.bz<br>
freestreams-live.top<br>
alkooratv.onlinekora-tv.com<br>
onlinekora-tv.com<br>
futbollibre.ws<br>
kooracity.cc<br>
ok.tvkora-online.com<br>
tvkora-online.com<br>
pirlotv.football<br>
ramsportl .com<br>
roja.football<br>
rojadirectaenvivo.life<br>
rojadirectaenvivo.site<br>
rojadirectatv.at<br>
rojadirectatv.la<br>
rojadirectaz.top<br>
rojadiretta.me<br>
soccerlive.app<br>
thesport.live<br>
futbollibre.ws<br>
hesgoal.watch<br>
hesgoall .net<br>
librefutbol.su<br>
myp2p.tv<br>
myp2ptv.org<br>
myp2px.xyz<br>
partidosenvivo.gratis<br>
pirlotvenhd.org<br>
rojadirecta.org.pl<br>
rojadirectaenhd.live<br>
rojadirectaenhd.net<br>
rojadirectaenvivo.com<br>
rojadirectaenvivo.watch<br>
rojadirecta.tv<br>
rojadirectas.org<br>
rojadirectatv.at<br>
rojadirectatv.top<br>
rojadirectatvhd.site<br>
rojadirectatvs.com<br>
soccerstreams.cc<br>
soccerstreams.unblockedstream.online<br>
streameast.soccer<br>
tarjetarojaonline.org<br>
tarjetarojatv.run<br>
totalsportek.one<br>
totalsportk.org<br>
viperplay.net<br>
viperplay.online<br>
viperplayhd.com<br>
vipleague.app<br>
vipleagues.org<br>
vip-league.net<br>
livetv806.me<br>
rojadirectahdenvivo.com<br>
streamsthunder.tv<br>
rojadirectenvivo.me<br>
methstreams.me<br>
antenasports.ru<br>
asportv.shop<br>
toparena.store<br>
Ishunter.net<br>
tv1337.buzz<br>
livetv.sx<br>
sporttuna.pro<br>
livetv807.me<br>
embx224539.ap1366.me<br>
cdn.livetv807.me<br>
locatedinfain.com<br>
tvhd.tutvlive.info<br>
stream-24.net<br>
speci41eagle.com<br>
vl.methstreams.me<br>
klubsports.fun<br>
weblivehdplay.ru<br>
buddycenters.shop<br>
olalivehdplay.ru<br>
1 qwebplay.xyz<br>
sporttvls.com<br>
eur02024direct.ru<br>
librarywhispering.com<br>
cdn.livetv808.me<br>
watch.sporttuna.pro<br>
sporttuna.sx<br>
sporttuna.online<br>
lewblivehdplay.ru<br>
viwlivehdplay.ru<br>
r365.city<br>
fmytv.com<br>
yalla-shootv.live<br>
sportlemo.net<br>
sportlemon.be<br>
antenasport.site<br>
sportlemons.tv<br>
directatvhd.me<br>
sportlemon.info<br>
koora365.io<br>
sport365.live<br>
live.esportivos.one<br>
sportlemonx.com<br>
mip2p.top<br>
bein-live.tv<br>
yalla-shoot.fun<br>
sportlemone.top<br>
sportlemont.org<br>
thedaddy.to<br>
abbasport.site<br>
h5.365streams.world<br>
stad.yallashoot.vip<br>
crickfree.org<br>
drakulatv.eu<br>
sportlemon.net<br>
footdirect.ru<br>
hdmatch.link<br>
livehd7i.live<br>
noblockaabbdd-xcktb.xyz<br>
sportp2p.com<br>
crichd.info<br>
crichd-player.top<br>
crichd. to<br>
telerium.lol<br>
rojatv.tv<br>
mundialqatar2022tv.tv<br>
hesgoal.website<br>
redditsoccerstreams.one<br>
redditsoccerstreams.watch<br>
soccerbite.net<br>
hesgoal.one<br>
cricfree.be<br>
cricfree.me<br>
cricfree.pw<br>
crickfree.net<br>
cricfrees.com<br>
crackstreams.dev<br>
ronald07.me<br>
draculastream.org<br>
drakulastream.tv<br>
drakulastream.org<br>
drakulastream.xyz<br>
drakula.top<br>
drakula.stream<br>
elitegoltv.run<br>
firstrowl.xyz<br>
thesportsl.org<br>
livetv813.me<br>
sportp2p.com<br>
directatvhd.me<br>
Ishunter.net<br>
antenasport.shop<br>
antenasports.ru<br>
antenasports.shop<br>
ilovetoplay.xyz<br>
hoca2.com<br>
livetv814.me<br>
cdn.livetv814.me<br>
streamingon.org<br>
emb.ap1357.me<br>
livetv815.me<br>
cdn.livetv815.me<br>
noblockaabbdd-xcktb.xyz<br>
embx222304.ap1357.me<br>
tutvlive.info<br>
sporttvls.com<br>
quest4play.xyz<br>
antenasport.online<br>
wfzrbhp.luxevpn.xyz<br>
smart.lionsmart.cc</em></p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sandia turns on brain-like storage-free supercomputer (177 pts)]]></title>
            <link>https://blocksandfiles.com/2025/06/06/sandia-turns-on-brain-like-storage-free-supercomputer/</link>
            <guid>44201812</guid>
            <pubDate>Fri, 06 Jun 2025 15:24:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blocksandfiles.com/2025/06/06/sandia-turns-on-brain-like-storage-free-supercomputer/">https://blocksandfiles.com/2025/06/06/sandia-turns-on-brain-like-storage-free-supercomputer/</a>, See on <a href="https://news.ycombinator.com/item?id=44201812">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <!-- image --><div><figure><a href="https://blocksandfiles.com/wp-content/uploads/2025/06/CFA8505la-1.jpg" data-caption="Brad Theilman facing camera, and Felix Wang, background, unpack a new shipment of the SpiNNaker2 computing core.  It is a collaboration between Sandia and SpiNNcloud and will be the first in world commercial product from the cooperation. 

Funded through NNSA’s Advanced Simulation and Computing (ASC) program, this work will explore how neuromorphic computing can be leveraged for the nation’s nuclear deterrence missions.

SpiNNaker is a contraction of ‘Spiking Neural Network Architecture,’ which is a brain inspired neuromorphic computer for large-scale, real-time modeling of brain-like applications. This technology can simulate large brain-like networks to enhance researchers’ understanding of the brain, as well as provide a framework to test the boundaries of current computing capabilities.

Photo by Craig Fritz"><img width="696" height="464" src="https://blocksandfiles.com/wp-content/uploads/2025/06/CFA8505la-1-696x464.jpg" srcset="https://blocksandfiles.com/wp-content/uploads/2025/06/CFA8505la-1-696x464.jpg 696w, https://blocksandfiles.com/wp-content/uploads/2025/06/CFA8505la-1-300x200.jpg 300w, https://blocksandfiles.com/wp-content/uploads/2025/06/CFA8505la-1-1024x683.jpg 1024w, https://blocksandfiles.com/wp-content/uploads/2025/06/CFA8505la-1-768x512.jpg 768w, https://blocksandfiles.com/wp-content/uploads/2025/06/CFA8505la-1-1068x712.jpg 1068w, https://blocksandfiles.com/wp-content/uploads/2025/06/CFA8505la-1-630x420.jpg 630w, https://blocksandfiles.com/wp-content/uploads/2025/06/CFA8505la-1.jpg 1500w" sizes="(max-width: 696px) 100vw, 696px" alt="" title="SpiNNaker Computer"></a><figcaption>Brad Theilman facing camera, and Felix Wang, background, unpack a new shipment of the SpiNNaker2 computing core.  It is a collaboration between Sandia and SpiNNcloud and will be the first in world commercial product from the cooperation. 

Funded through NNSA’s Advanced Simulation and Computing (ASC) program, this work will explore how neuromorphic computing can be leveraged for the nation’s nuclear deterrence missions.

SpiNNaker is a contraction of ‘Spiking Neural Network Architecture,’ which is a brain inspired neuromorphic computer for large-scale, real-time modeling of brain-like applications. This technology can simulate large brain-like networks to enhance researchers’ understanding of the brain, as well as provide a framework to test the boundaries of current computing capabilities.

Photo by Craig Fritz</figcaption></figure></div>
            <!-- content -->
<p>Sandia National Labs has flipped the switch on its SpiNNaker 2 “brain-inspired” supercomputer that eschews both GPUs and internal storage.</p>



<p>The system, supplied by Germany-based SpiNNcloud, will rank amongst the top five “brain inspired” platforms, mimicking between 150 and 180 million neurons.</p>



<p>The architecture was initially <a href="https://www.theregister.com/2017/10/19/steve_furber_arm_brain_interview/">developed </a>by Arm pioneer Steve Furber, though it still falls somewhat short of the human brain’s 100 billion neurons.</p>



<p>As SpiNNcloud explains it, the SpiNNaker 2’s highly parallel architecture has 48 SpiNNaker 2 chips per server board, each of which in turn carries 152 based cores and specialized accelerators.</p>



<p>Each of the 48 chips packs 20 MB of SRAM, with each board carrying 96 GB of external LPDDR4 external memory. So, with 90 boards, that amounts to 8640 GB of DRAM, while a 1440 board system carries 69,120 chips and 138240 TB of DRAM.</p>



<p>Needless to say, the system uses high-speed chip to chip communication. And this, SpiNNcloud says, eliminates the need for centralized storage. That, and the vast amount of memory.</p>



<h2>Speeding on DRAM</h2>



<p>In Sandia’s case, it has taken delivery of a 24 board, 175,000 core system. At <a href="https://blocksandfiles.com/2022/09/05/doe-data-management-research/">Sandia</a>, according to <a href="https://spinncloud.com/">SpiNNcloud</a>, “The Supercomputer is hooked in to existing HPC systems and does not contain any OS or disks. The speed is generated by keeping data in the SRAM and DRAM.”</p>



<p>Standard parallel ethernet ports are “sufficient for loading/saving the data.” &nbsp;The “current maximum system” is over 10.5m cores, which SpiNNcloud says means it can maintain biological real time.</p>



<p>Moreover, it allows complex event-driven compute and simulations with more power efficiency compared to GPU systems.</p>



<p>Hector A. Gonzalez, co-founder&nbsp;and CEO of SpiNNcloud, said the system would be targeted at problems in “next generation defense and beyond. The SpiNNaker2’s efficiency gains make it particularly well-suited for the demanding computational needs of national security applications.”</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI is retaining all ChatGPT logs "indefinitely." Here's who's affected (114 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2025/06/openai-confronts-user-panic-over-court-ordered-retention-of-chatgpt-logs/</link>
            <guid>44201785</guid>
            <pubDate>Fri, 06 Jun 2025 15:21:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2025/06/openai-confronts-user-panic-over-court-ordered-retention-of-chatgpt-logs/">https://arstechnica.com/tech-policy/2025/06/openai-confronts-user-panic-over-court-ordered-retention-of-chatgpt-logs/</a>, See on <a href="https://news.ycombinator.com/item?id=44201785">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<p>In the copyright fight, Magistrate Judge Ona Wang granted the order within one day of the NYT's request. She agreed with news plaintiffs that it seemed likely that ChatGPT users may be spooked by the lawsuit and possibly set their chats to delete when using the chatbot to skirt NYT paywalls. Because OpenAI wasn't sharing deleted chat logs, the news plaintiffs had no way of proving that, she suggested.</p>
<p>Now, OpenAI is not only asking Wang to reconsider but has "also appealed this order with the District Court Judge," the Thursday statement said.</p>
<p>"We strongly believe this is an overreach by the New York Times," Lightcap said. "We’re continuing to appeal this order so we can keep putting your trust and privacy first."</p>
<h2>Who can access deleted chats?</h2>
<p>To protect users, OpenAI provides an <a href="https://openai.com/index/response-to-nyt-data-demands/">FAQ</a> that clearly explains why their data is being retained and how it could be exposed.</p>
<p>For example, the statement noted that the order doesn’t impact OpenAI API business customers under Zero Data Retention agreements because their data is never stored.</p>
<p>And for users whose data is affected, OpenAI noted that their deleted chats could be accessed, but they won't "automatically" be shared with The New York Times. Instead, the retained data will be "stored separately in a secure system" and "protected under legal hold, meaning it can’t be accessed or used for purposes other than meeting legal obligations," OpenAI explained.</p>
<p>Of course, with the court battle ongoing, the FAQ did not have all the answers.</p>
<p>Nobody knows how long OpenAI may be required to retain the deleted chats. Likely seeking to reassure users—some of which appeared to be considering switching to a rival service until the order lifts—OpenAI noted that "only a small, audited OpenAI legal and security team would be able to access this data as necessary to comply with our legal obligations."</p>


          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Too Many Open Files (120 pts)]]></title>
            <link>https://mattrighetti.com/2025/06/04/too-many-files-open</link>
            <guid>44201762</guid>
            <pubDate>Fri, 06 Jun 2025 15:18:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mattrighetti.com/2025/06/04/too-many-files-open">https://mattrighetti.com/2025/06/04/too-many-files-open</a>, See on <a href="https://news.ycombinator.com/item?id=44201762">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>
  
  <p>
    Jun 4, 2025
    
  </p>
  
  <p>Recently I’ve been working on a pretty big rust project and to my surprise I
couldn’t get tests to work properly.</p>
<p>Running <code>cargo test</code> would start running all the tests in the repo and after a
couple of milliseconds every single test would start to fail because of an error
that I’m not very familiar with</p>
<div>
<pre><code data-lang="rust">Io(Os { code: 24, kind: Other, message: "Too many open files" })</code></pre>
</div>
<p>Fortunately, the error is pretty explicit and straightfoward so I was able to
understand what was going on in a reasonable time. I’ve started digging a bit
and learned some stuff along the way.</p>
<p>Ever wondered how your programs juggle multiple tasks - reading files, sending
data over the network, or even just displaying text on your screen - all at
once? File descriptors are what make this all possible (in Unix systems).</p>
<p>At its core, a file descriptor (often abbreviated as fd) is simply a positive
integer used by the operating system kernel to identify an open file. In Unix,
<a href="https://en.wikipedia.org/wiki/Everything_is_a_file">"everything is a file."</a> Contrary to what the word says,
a file descriptor doesn’t just refer to regular files on your disk. It can
represent:</p>
<div>
<ul>
<li>
<p><strong>Regular files</strong>: The documents, images, and code files you interact with daily.</p>
</li>
<li>
<p><strong>Directories</strong>: Yes, even directories are treated like files to some extent,
allowing programs to list their contents.</p>
</li>
<li>
<p><strong>Pipes</strong>: Used for inter-process communication, allowing one program’s output to
become another’s input.</p>
</li>
<li>
<p><strong>Sockets</strong>: The endpoints for network communication, whether it’s talking to a
web server or another application on your local machine.</p>
</li>
<li>
<p><strong>Devices</strong>: Hardware devices like your keyboard, mouse, and printer are also
accessed via file descriptors.</p>
</li>
</ul>
</div>
<p>When a program wants to interact with any of these resources, it first asks the
kernel to "open" it. If successful, the kernel returns a file descriptor, which
the program then uses for all subsequent operations (reading, writing, closing,
etc.).</p>
<p>By convention, every Unix process starts with at least three standard file
descriptors automatically opened:</p>
<div>
<ul>
<li>
<p><code>0</code>: Standard Input (stdin) - Typically connected to your keyboard for user
input.</p>
</li>
<li>
<p><code>1</code>: Standard Output (stdout) - Usually connected to your terminal for
displaying normal program output.</p>
</li>
<li>
<p><code>2</code>: Standard Error (stderr) - Also usually connected to your terminal, but
specifically for displaying error messages.</p>
</li>
</ul>
</div>
<p>On macOS we can quickly check this, open your favorite terminal and run <code>ls
/dev/fd</code>.</p>
<div>
<pre><code data-lang="console">$ ls -lah /dev/fd
Permissions Size User         Date Modified Name
crw--w----  16,2 mattrighetti  4 Jun 00:44  0
crw--w----  16,2 mattrighetti  4 Jun 00:44  1
crw--w----  16,2 mattrighetti  4 Jun 00:44  2
dr--r--r--     - root         24 May 08:23  3</code></pre>
</div>
<p>On Linux we can do something similar but the repository is different and usually
follows the current pattern <code>/proc/&lt;pid&gt;/fd</code>. Running the same on Linux gives me this:</p>
<div>
<pre><code data-lang="console">$ echo $$ // prints the current process id
2806524

$ sudo ls -lah /proc/2806524/fd
total 0
dr-x------ 2 root root 11 Jun  4 00:40 .
dr-xr-xr-x 9 pi   pi    0 Jun  4 00:39 ..
lrwx------ 1 root root 64 Jun  4 00:40 0 -&gt; /dev/null
lrwx------ 1 root root 64 Jun  4 00:40 1 -&gt; /dev/null
lrwx------ 1 root root 64 Jun  4 00:40 10 -&gt; /dev/ptmx
lrwx------ 1 root root 64 Jun  4 00:40 11 -&gt; /dev/ptmx
lrwx------ 1 root root 64 Jun  4 00:40 2 -&gt; /dev/null
lrwx------ 1 root root 64 Jun  4 00:40 3 -&gt; 'socket:[14023056]'
lrwx------ 1 root root 64 Jun  4 00:40 4 -&gt; 'socket:[14023019]'
lrwx------ 1 root root 64 Jun  4 00:40 5 -&gt; 'socket:[14022300]'
lrwx------ 1 root root 64 Jun  4 00:40 6 -&gt; 'socket:[14023037]'
lrwx------ 1 root root 64 Jun  4 00:40 7 -&gt; /dev/ptmx
l-wx------ 1 root root 64 Jun  4 00:40 8 -&gt; /run/systemd/sessions/1501.ref</code></pre>
</div>
<p>As you can see, we have <code>0</code>, <code>1</code> and <code>2</code> as expected, but we also have a bunch of
other file descriptors.</p>
<p>Another useful command to check for open file descriptors is <code>lsof</code>, which
stands for "list open files".</p>
<div>
<pre><code data-lang="console">$ lsof -p $(echo $$)
COMMAND   PID         USER   FD   TYPE DEVICE SIZE/OFF                NODE NAME
zsh     39367 mattrighetti  cwd    DIR   1,17     2496              250127 /Users/mattrighetti
zsh     39367 mattrighetti  txt    REG   1,17  1361200 1152921500312522433 /bin/zsh
zsh     39367 mattrighetti  txt    REG   1,17    81288 1152921500312535786 /usr/share/locale/en_US.UTF-8/LC_COLLATE
zsh     39367 mattrighetti  txt    REG   1,17   170960 1152921500312525313 /usr/lib/zsh/5.9/zsh/zutil.so
zsh     39367 mattrighetti  txt    REG   1,17   118896 1152921500312525297 /usr/lib/zsh/5.9/zsh/terminfo.so
zsh     39367 mattrighetti  txt    REG   1,17   171344 1152921500312525281 /usr/lib/zsh/5.9/zsh/parameter.so
zsh     39367 mattrighetti  txt    REG   1,17   135696 1152921500312525255 /usr/lib/zsh/5.9/zsh/datetime.so
zsh     39367 mattrighetti  txt    REG   1,17   135568 1152921500312525291 /usr/lib/zsh/5.9/zsh/stat.so
zsh     39367 mattrighetti  txt    REG   1,17   338592 1152921500312525247 /usr/lib/zsh/5.9/zsh/complete.so
zsh     39367 mattrighetti  txt    REG   1,17   136880 1152921500312525293 /usr/lib/zsh/5.9/zsh/system.so
zsh     39367 mattrighetti  txt    REG   1,17   593088 1152921500312525303 /usr/lib/zsh/5.9/zsh/zle.so
zsh     39367 mattrighetti  txt    REG   1,17   134928 1152921500312525287 /usr/lib/zsh/5.9/zsh/rlimits.so
zsh     39367 mattrighetti  txt    REG   1,17   117920 1152921500312525263 /usr/lib/zsh/5.9/zsh/langinfo.so
zsh     39367 mattrighetti  txt    REG   1,17  2289328 1152921500312524246 /usr/lib/dyld
zsh     39367 mattrighetti  txt    REG   1,17   208128 1152921500312525249 /usr/lib/zsh/5.9/zsh/complist.so
zsh     39367 mattrighetti  txt    REG   1,17   118688 1152921500312525285 /usr/lib/zsh/5.9/zsh/regex.so
zsh     39367 mattrighetti  txt    REG   1,17   118288 1152921500312525305 /usr/lib/zsh/5.9/zsh/zleparameter.so
zsh     39367 mattrighetti    0u   CHR   16,1  0t17672                1643 /dev/ttys001
zsh     39367 mattrighetti    1u   CHR   16,1  0t17672                1643 /dev/ttys001
zsh     39367 mattrighetti    2u   CHR   16,1  0t17672                1643 /dev/ttys001
zsh     39367 mattrighetti   10u   CHR   16,1   0t5549                1643 /dev/ttys001</code></pre>
</div>
<p>According to the lsof documentation:</p>
<div>
<ul>
<li>
<p><code>cwd</code>: The current working directory of the process.</p>
</li>
<li>
<p><code>txt</code>: Executable files or shared libraries loaded into memory (e.g.,
/bin/zsh, modules like zutil.so, or system libraries like /usr/lib/dyld).</p>
</li>
<li>
<p><code>0u</code>, <code>1u</code>, <code>2u</code>: Standard input (0), output (1), and error (2) streams,
respectively. The <code>u</code> means the descriptor is open for both reading and writing.
These are tied to <code>/dev/ttys001</code> (my current terminal device).</p>
</li>
<li>
<p><code>10u</code>: Another file descriptor (also tied to <code>/dev/ttys001`</code>), likely used for
additional terminal interactions.</p>
</li>
</ul>
</div>
        <div title="Matt">
          
          <p>We now know that file descriptors are a way for the operating system to keep track of open files and other resources, nice!</p>
        </div>
<p>Have you ever wondered how many file descriptors can be open at the same time?
The most common answer in software engineering applies here too: It depends.</p>
<p>Each operating system has its own limits on the number of file descriptors a
process can open simultaneously. These limits are in place to prevent a single
misbehaving program from hogging all available resources and crashing the
system.</p>
<p>On macOS, we can easily inspect these limits using the <code>sysctl</code> and
<a href="https://linuxcommand.org/lc3_man_pages/ulimith.html"><code>ulimit</code></a> commands in your terminal.</p>
<div>
<pre><code data-lang="console">$ sysctl kern.maxfiles
kern.maxfiles: 245760

$ sysctl kern.maxfilesperproc
kern.maxfilesperproc: 122880

$ ulimit -n
256</code></pre>
</div>
<div>
<ul>
<li>
<p><code>kern.maxfiles</code> represents the absolute maximum number of file descriptors that
can be open across the entire macOS system at any given moment. It’s a global
governor, preventing the system from running out of file descriptor resources,
even if many different applications are running.</p>
</li>
<li>
<p><code>kern.maxfilesperproc</code> is the hard limit on the number of file descriptors
that a single process can have open. Think of it as the ultimate ceiling for an
individual application. No matter what, a process cannot open more files than
this hard limit set by the kernel.</p>
</li>
<li>
<p><code>ulimit -n</code> is your shell’s "soft" limit for the number of open file
descriptors. If a process tries to open more files than its soft limit, the
operating system will typically return an error (e.g., "Too many open files").
The good news is that a process can raise its own soft limit, but only up to its
hard limit.</p>
</li>
</ul>
</div>
<p>Enough with the theory, let’s get back to the problem I was having with my rust
tests.  My assumption was that since <code>cargo test</code> gets executed in my terminal,
it inevitably reaches a point where it tries to open more files than the soft
limit set by my shell, which is 256 in this case. When that happens, the
operating system screams at <code>cargo</code> and tells it that it can’t open any more
files, <code>cargo</code> then propagates that error to the tests and they all fail.</p>
<p>I wanted to confirm this hypothesis, so I created this monitoring script that
watches for <code>cargo test</code> PID and prints the number of open file descriptors at
different intervals.</p>
<div>
<pre><code data-lang="bash">#!/bin/bash

# This function exits the script gracefully
function cleanup() {
    echo -e "\nstopping."
    exit 0
}

# This function encapsulates the logic for formatting and printing the monitoring output.
# Arguments:
#   $1: Initial PID
#   $2: Total number of open files
print_status() {
    local initial_pid="$1"
    local total_open_files="$2"
    echo "$(date '+%H:%M:%S') - Main PID ($initial_pid) - open: ${total_open_files}"
}

PROCESS_NAME="cargo"
COMMAND_ARGS="test"

echo "press ctrl+c to stop."

# Find the Process ID (PID) of the initial command.
INITIAL_PID=$(pgrep -f "$PROCESS_NAME.*$COMMAND_ARGS" | head -n 1)

if [ -z "$INITIAL_PID" ]; then
    echo "waiting for '$PROCESS_NAME $COMMAND_ARGS' to start..."
    # If the process isn't found immediately, loop and wait for it.
    sleep 0.01
    while [ -z "$INITIAL_PID" ]; do
        INITIAL_PID=$(pgrep -f "$PROCESS_NAME.*$COMMAND_ARGS" | head -n 1)
    done
fi

echo "Found '$PROCESS_NAME $COMMAND_ARGS' with PID: $INITIAL_PID"

# trap command catches the INT signal (triggered by Ctrl+C)
# and calls the cleanup function to exit gracefully.
trap cleanup INT

while true; do
    # check if the main process (INITIAL_PID) is still running.
    if ! ps -p "$INITIAL_PID" &gt; /dev/null; then
        echo "PID $INITIAL_PID no longer running. bye!"
        break
    fi

    # `sudo lsof -p "$INITIAL_PID"` lists all open files for this specific PID.
    # `2&gt;/dev/null` redirects stderr (errors like "process not found") to null.
    # `grep -v " txt "` filters out loaded executable code and libraries for a more relevant count.
    # `wc -l` counts the lines, effectively the number of open files.
    # `tr -d ' '` removes any leading/trailing spaces for clean arithmetic.
    OPEN_FILES_COUNT=$(sudo lsof -p "$INITIAL_PID" 2&gt;/dev/null | grep -v " txt " | wc -l | tr -d ' ')

    # Ensure COUNT is not empty (it might be if lsof returns nothing)
    if [ -z "$OPEN_FILES_COUNT" ]; then
        OPEN_FILES_COUNT=0
    fi

    print_status "$INITIAL_PID" "$TOTAL_OPEN_FILES"
done</code></pre>
</div>
        <div title="Matt">
          
          <p>Note that usually to get an accurate count of open file descriptors you would also need to consider the entire process tree, not just the main process. This is because child processes can also open files, and their file descriptors contribute to the total count.  In my case, there was only one process (`cargo test`) running, so I didn't.</p>
        </div>
<p>I can now run this script in one terminal and run <code>cargo test</code> in another
terminal. I actually had to do this a couple of times to get a good sample of
data, rust runs pretty fast once the code is compiled and this monitor script
does not run fast enough to catch all the open file descriptors changes.</p>
<div>
<pre><code data-lang="console">$ sudo ./monitor.sh
press ctrl+c to stop.
waiting for 'cargo test' to start...
Found 'cargo test' with PID: 44152
01:46:21 - Main PID (44152) - open: 14
01:46:21 - Main PID (44152) - open: 32
01:46:21 - Main PID (44152) - open: 78
01:46:21 - Main PID (44152) - open: 155
01:46:21 - Main PID (44152) - open: 201
01:46:21 - Main PID (44152) - open: 228
01:46:21 - Main PID (44152) - open: 231
01:46:21 - Main PID (44152) - open: 237 # errors started happening here
01:46:21 - Main PID (44152) - open: 219
01:46:21 - Main PID (44152) - open: 205
01:46:21 - Main PID (44152) - open: 180
01:46:21 - Main PID (44152) - open: 110
01:46:21 - Main PID (44152) - open: 55
01:46:21 - Main PID (44152) - open: 28
01:46:21 - Main PID (44152) - open: 15
01:46:21 - Main PID (44152) - open: 0
PID 44152 no longer running. bye!</code></pre>
</div>
<p>I couldn’t get the script to catch the exact moment the process reached the soft
limits, but I can clearly see that the tests starts failing when the number of
open file descriptors reaches 237, which is pretty close to the soft limit of
256.</p>
<p>Time to fix this! This is a bit underwhelming, but the solution is to just bump
the soft limit of open file descriptors in my shell. I can do this by using the
<code>ulimit</code> command again.</p>
<div>
<pre><code data-lang="console">$ ulimit -n 8192
$ ulimit -n
8192</code></pre>
</div>
<p>Running <code>cargo test</code> now works as expected and no "Too many open files" error is thrown.</p>
<div id="chart">
    <h4>Open File Descriptors Over Time</h4>
    
</div>

<p>The above chart shows the number of open file descriptors with the new soft
limit. As you can see the max value reached is around 1600, which is way above
the previous limit of 256.</p>
<p>All in all, this was a fun exercise that taught me a lot about file descriptors
and how they work in Unix-like systems. Now you know how to troubleshoot this
error that might pop up in your own projects!</p>
</article>






    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An Interactive Guide to Rate Limiting (130 pts)]]></title>
            <link>https://blog.sagyamthapa.com.np/interactive-guide-to-rate-limiting</link>
            <guid>44201583</guid>
            <pubDate>Fri, 06 Jun 2025 14:58:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.sagyamthapa.com.np/interactive-guide-to-rate-limiting">https://blog.sagyamthapa.com.np/interactive-guide-to-rate-limiting</a>, See on <a href="https://news.ycombinator.com/item?id=44201583">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content-parent"><h2 id="heading-introduction">Introduction</h2>
<p>Rate limiting is a must have strategy in every back-end app. It prevent one user from overusing a resource and degrading the quality of service for other users. Here are some benefits of rate limiting</p>
<ul>
<li><p>It presents resource starvation</p>
</li>
<li><p>Reduces server hosting cost</p>
</li>
<li><p>Provides basic protection against <a target="_blank" href="https://en.wikipedia.org/wiki/Denial-of-service_attack">DDoS</a></p>
</li>
</ul>
<p>I have made four interactive app that let’s you play around with common rate limiting algorithms.</p>
<h2 id="heading-token-bucket">Token bucket</h2>
<h3 id="heading-working">Working:</h3>
<ul>
<li><p>A bucket holds fixed number tokens</p>
</li>
<li><p>Tokens are added to bucket at fixed rate</p>
</li>
<li><p>When a request comes in:</p>
<ul>
<li><p>If a token is available, it’s removed from the bucket and the request is allowed.</p>
</li>
<li><p>If no tokens are available, the request is rejected or delayed.</p>
</li>
</ul>
</li>
<li><p>Allows for occasional short burst if tokens are available</p>
</li>
</ul>
<p>I have created an <a target="_blank" href="https://tools.sagyamthapa.com.np/token-bucket">app</a> that let’s you play with leaky bucket algorithm.</p>

<h2 id="heading-leaky-bucket">Leaky bucket</h2>
<h3 id="heading-working-1">Working</h3>
<ul>
<li><p>Think of it as a bucket leaking at a fixed rate</p>
</li>
<li><p>Incoming requests are added to the bucket</p>
</li>
<li><p>Requests are processed (or "leak") at a <strong>constant rate</strong></p>
</li>
<li><p>If the bucket is full when a new request arrives, the request is dropped</p>
</li>
<li><p>Smooths out bursts; outputs requests at a steady rate</p>
<p>  I have made an <a target="_blank" href="https://tools.sagyamthapa.com.np/leaky-bucket">app</a> that let’s you play with leaky bucket algorithm.</p>
</li>
</ul>

<h2 id="heading-fixed-window-counter">Fixed window counter</h2>
<h3 id="heading-working-2">Working:</h3>
<ul>
<li><p>Time is divided into fixed size windows (e.g., 1 minute)</p>
</li>
<li><p>A counter tracks the number of requests per client/IP in the current window</p>
</li>
<li><p>If the count exceeds the limit, further requests are rejected until the next window</p>
</li>
<li><p>Simple and efficient, but allows burst traffic spike at end/start</p>
<p>  I have created an <a target="_blank" href="https://tools.sagyamthapa.com.np/fixed-window">app</a> that let’s you play with fixed bucket algorithm.</p>
</li>
</ul>

<h2 id="heading-sliding-window-counter">Sliding window counter</h2>
<h3 id="heading-working-3">Working:</h3>
<ul>
<li><p>Keeps a timestamped log of each request</p>
</li>
<li><p>When a request comes in, logs are checked to count how many requests were made in the last <code>X</code> seconds</p>
</li>
<li><p>If under the limit, the request is allowed and logged; otherwise, it’s rejected</p>
<p>  I have created an <a target="_blank" href="https://tools.sagyamthapa.com.np/sliding-window">app</a> that let’s you play with sliding bucket algorithm.</p>
</li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Top researchers leave Intel to build startup with 'the biggest, baddest CPU' (145 pts)]]></title>
            <link>https://www.oregonlive.com/silicon-forest/2025/06/top-researchers-leave-intel-to-build-startup-with-the-biggest-baddest-cpu.html</link>
            <guid>44201072</guid>
            <pubDate>Fri, 06 Jun 2025 14:07:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.oregonlive.com/silicon-forest/2025/06/top-researchers-leave-intel-to-build-startup-with-the-biggest-baddest-cpu.html">https://www.oregonlive.com/silicon-forest/2025/06/top-researchers-leave-intel-to-build-startup-with-the-biggest-baddest-cpu.html</a>, See on <a href="https://news.ycombinator.com/item?id=44201072">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p id="SZALMQPN25DQXLA42EDYATXDTU">Together, the four founders of Beaverton startup AheadComputing spent nearly a century at Intel.</p><p id="7DQCWW5M6FCYHGBBFPKHQXK4JY">They were among Intel’s top chip architects, working years in advance to develop new generations of microprocessors to power the computers of the future. </p><p id="LZDFRYI46NAPVL4BRWZ7BECEZY">Now they’re on their own, flying without a net, building a new class of microprocessor on an entirely different architecture from Intel’s. Founded a year ago, AheadComputing is trying to prove there’s a better way to design computer chips.</p><p id="KZAFSZ7L4NGABHBUFUJRIIPDSY">“AheadComputing is doing the biggest, baddest CPU in the world,” said Debbie Marr, the company’s CEO.</p><p id="3BBPK4LLVBD45LBEIE7XIP4ZRQ">A CPU is a central processing unit, the brain inside a computer. Intel has dominated the CPU market for decades, selling processors based on an architecture the chipmaker owns, called x86.</p><p id="OH4FNLKKOVGHPJV4QDYNBHFTNQ">Today, the computing ecosystem is fragmenting as new standards emerge. That’s one of the reasons Intel is struggling, as computing mainstays like Apple and Google use rival architectures to design their own chips for PCs, smartphones and data centers. </p><p id="HPECJENTQBAWRB4AWMGKF3NCLQ">AheadComputing is betting on an open architecture called RISC-V — RISC stands for “reduced instruction set computer.” The idea is to craft a streamlined microprocessor that works more efficiently by doing fewer things, and doing them better than conventional processors. </p><p id="MJOKIYKAMBFJBH6L2G6WGXJKH4">For AheadComputing’s founders and 80 employees, many of them also Intel alumni, it’s a major break from the kind of work they’ve been doing all their careers. They’ve left a company with more than 100,000 workers to start a business with fewer than 100.</p><p id="ILTV5O7QBJBOTJEOFGZQE3MZLI">“Every person in this room,” Marr said, looking across a conference table at her colleagues, “we could have stayed at Intel. We could have continued to do very exciting things at Intel.” </p><p id="Q5CNYQ7T5VFRDPL4DIWETUUI4M">They decided they had a better chance at leading a revolution in semiconductor technology at a startup than at a big, established company like Intel. And AheadComputing could be at the forefront of renewal in Oregon’s semiconductor ecosystem.</p><p id="WQTZVEPSNZECLK6KTHTKXCF3RY">“We see this opportunity, this light,” Marr said. “We took our chances.”</p><p id="N7SOASVO45EHPERYNSS4BFW6X4"><b>Big players lose clout</b></p><p id="HCKLZMD2FVHVTETV7IRCDQSE3M">Intel has been operating in Oregon since the 1970s and has long been Oregon’s largest corporate employer. It has more than 20,000 employees in Washington County, including some of the world’s top semiconductor researchers. </p><p id="ONOR6PP4RNAJRLZVXPC2KIXG5A">Very rarely, though, have any of those thousands of workers left to start their own chip business.</p><p id="4AHCXCVNAJGNJAVTERAAIN3KUY">That’s partly because it’s prohibitively expensive for a new company to build a semiconductor factory, and partly because Intel and other tech giants controlled the essential pieces around the CPU. It was hard for newcomers to break in.</p><p id="RISUTA4HUJDQHKDW72ZCA6E6JE">Today’s chip startups, however, don’t need their own factories. So-called fabless semiconductor designers like AheadComputing can outsource manufacturing to contractors like Taiwan Semiconductor Manufacturing Co. The graphics and AI chip designer Nvidia has become a $3 trillion company — the world’s most valuable business — doing exactly that.</p><p id="53UVBLCX3ZFM3G3SYL74CD7C44">And AheadComputing cofounder Jonathan Pearce says the reign of the chip titans is fading as the systems that run computers break up into a series of “chiplets” from various companies, optimized for specific tasks.</p><p id="GBQR6PXGGJF6FFG2EICXU2T7YU">“You get the opportunity for a company like AheadComputing to provide that piece of the overall system,” Pearce said. “As opposed to the past 20 years where it was just one tech giant.”</p><p id="D3PIRDSVKZA7ZJXRSTPT27KJ3U">RISC-V, pronounced “Risk-five,” is especially inviting to startups because it’s an open technology, overseen by a consortium of tech firms. Unlike processors based on x86 or ARM designs, there’s no licensing fee to use RISC-V.</p><p id="BY7NDKNCC5FPFDKSSA3VVISY2I">AheadComputing maintains that open standards are the future for microprocessors, giving technology companies more opportunity to customize chip designs to suit their specific needs. </p><p id="2BXOY3ZH3VCDVJ647GT3UJ2UQQ">It will be years before AheadComputing’s designs are on the market, but it envisions its chips will someday power PCs, laptops and data centers. Possible clients could include Google, Amazon, Samsung or other large computing companies. </p><figure><div><picture><source media="(max-width: 500px)" srcset="https://www.oregonlive.com/resizer/v2/TWYNPIP45ZH7NID424ARQ37OEU.jpg?auth=04f5165a951bcd601cae3bc30cdb1cd3c55fb4ded8b6b6b2ffd23140ec990dcd&amp;width=500&amp;quality=90 500w"><source media="(max-width: 800px)" srcset="https://www.oregonlive.com/resizer/v2/TWYNPIP45ZH7NID424ARQ37OEU.jpg?auth=04f5165a951bcd601cae3bc30cdb1cd3c55fb4ded8b6b6b2ffd23140ec990dcd&amp;width=800&amp;quality=90 800w"><source srcset="https://www.oregonlive.com/resizer/v2/TWYNPIP45ZH7NID424ARQ37OEU.jpg?auth=04f5165a951bcd601cae3bc30cdb1cd3c55fb4ded8b6b6b2ffd23140ec990dcd&amp;width=1280&amp;quality=90 1280w"><img src="https://www.oregonlive.com/resizer/v2/TWYNPIP45ZH7NID424ARQ37OEU.jpg?auth=04f5165a951bcd601cae3bc30cdb1cd3c55fb4ded8b6b6b2ffd23140ec990dcd&amp;width=500&amp;quality=90" sizes="50vw" fetchpriority="low" loading="lazy" alt="AheadComputing" decoding="async" height="600" width="300"></picture><figcaption><p>Members of AheadComputing's executive team. From left to right: Mark Dechene (co-founder), Alon Mahl (vice president), Jonathan Pearce (co-founder), Srikanth Srinivasan (co-founder), Debbie Marr (co-founder, CEO)<span>Mike Rogoway/The Oregonian</span></p></figcaption></div></figure><p id="FW5OQ4UJ7JCXZAERWP22SQOC6E">That vision, and the pedigree of the Intel alumni leading the Beaverton startup, has attracted a lot of attention within the industry. AheadComputing <a href="https://www.oregonlive.com/silicon-forest/2025/02/intel-alumni-raise-215-million-for-portland-computing-startup.html" target="_blank" rel="">raised $22 million in venture capital</a> in February. Prominent semiconductor engineer Jim Keller — a superstar in the rarified field of chip architecture, with stints at Intel, AMD and Apple — turned heads when he joined AheadComputing’s board a month later.</p><p id="FZUCJEBS3RFLDKHPRXWZ2GDBS4">The semiconductor industry is undergoing a transformation as new chips emerge for artificial intelligence and chipmakers test the laws of physics with radical innovations like quantum computing. </p><p id="BOTIYCVVIFDGDP7FPS4M3RO56U">“This is where the future is. It’s not about x86. It’s not traditional computer architecture. That’s pretty much a dead field,” said Christof Teuscher, a Portland State University professor who teaches microprocessor design and computer architecture. </p><p id="L6Q3Z2UAB5GGJA24C6HRUXAACM">AheadComputing is arriving at the right moment, Teuscher said, positioning itself to capitalize on upheaval in the industry — and the ongoing exodus of Intel employees. At this point, though, he said no one can know which new chip architectures will prevail.</p><p id="CX3FWJ5PJBCJROADK7DMOEYOIE">Traditionally, Teuscher said RISC-V has been used for embedded systems and academic research. He said he’s skeptical that it can prevail as the industry moves toward high-performance designs for artificial intelligence and supercomputing.</p><p id="JXX3ZXH345HYVKF7MRJTHOYWSE">“There’s high risk, but potentially high payoff,” Teuscher said. “And that’s definitely what I would describe their approach.”</p><p id="OWDPDKTWMJEHREWQITVDAOATPQ"><b>Silicon Forest’s new growth</b></p><p id="C3PWNDVWVJALJGH4LLGTACPP3I">At Intel, AheadComputing’s founders were accustomed to working with cutting-edge manufacturing tools and enormous research labs. Now they’re doing everything themselves — setting up their Wi-Fi network, adding memory to desktop computers and ordering snacks and coffee for the break room. </p><p id="TWQTCO3V3JHFFEU5H3UYENS224">Alon Mahl left Intel to be AheadComputing’s vice president of design verification, a key role ensuring the company’s chips work as expected. But one of his first tasks was to find and lease an office. He picked a spot on the top floor of The Round development near downtown Beaverton. </p><p id="GNVICN7BLJFB3H5OXPYXH4PNJQ">“That’s what’s exciting, is that you’re not doing only one thing. You’re doing everything,” Mahl said. “And each one is doing whatever he can to make this company a success. So that’s why I feel energized.”</p><p id="UFBBQY76EJAPPDSEJNXVSQH4F4">Intel provided the energy and innovation that fueled several generations of new chip technologies. It spent tens of billions to stay ahead of rivals, growing into Oregon’s largest corporate employer with more than 20,000 workers in Washington County.</p><p id="ZRXZQQNSQBHRVLJTTVSTBV6NUE">The cutting edge became increasingly tenuous, though, as the features on computer chips approached the atomic scale. Intel suffered a series of manufacturing setbacks, the result of bad bets on production technology and an utter failure to break into new sectors, like smartphones and artificial intelligence. </p><p id="ER5XEJUQLJCG7D4NVUUU64D7YE">Intel’s stumbles risk destabilizing Oregon’s semiconductor industry, one of the state’s economic pillars. The company <a href="https://www.oregonlive.com/silicon-forest/2025/01/intel-shed-3000-oregon-jobs-last-year-through-layoffs-buyouts-and-attrition.html" target="_blank" rel="">shed 3,000 jobs last year</a>, and <a href="https://www.oregonlive.com/silicon-forest/2025/04/intel-says-it-will-cut-costs-jobs-and-warns-sales-will-keep-falling.html" target="_blank" rel="">more layoffs are expected</a> in the coming weeks. </p><p id="ZIFJ3I5LDNDBNPVFQQD3T6K3GU">There’s no replacing the outsized impact of Intel’s factories. But new players could help keep Oregon in the game as the semiconductor industry evolves.</p><p id="LCDPDFPZPBHFHDSVFQYJ4XTOUI">Ampere Computing, founded by former Intel President Renée James, sold to Japanese investment bank SoftBank in March for $6.5 billion. It’s <a href="https://www.oregonlive.com/silicon-forest/2025/03/ampere-chip-designer-with-big-portland-office-sells-for-65-billion.html" target="_blank" rel="">retaining its offices in Portland’s Pearl District</a>, where it designs chips used in data centers.</p><p id="L2FPJZCR2FDCLJXDXB7MY6PEUU">Other Intel alumni have started a handful of other Oregon chip businesses in recent months. AheadComputing says it has already outgrown its offices at The Round and is looking for more space somewhere in Washington County.</p><p id="I4IXMNW4ZNG45KIGLH5HFC6SAE">Big companies are designed to meet many corporate needs, in the same way conventional CPUs are built to handle all computing workloads. AheadComputing wants to be more like its processors, moving faster because it’s dedicated to specific tasks. </p><p id="R76FH6I4CVDIVIWI7I4OWWVFQQ">“It’s very hard to disrupt an industry from inside,” said Mark Dechene, who spent 16 years as an Intel CPU architect before co-founding AheadComputing. </p><p id="ZSTGW2CHIFDSVOO4SZLMHR27HQ">At Intel, he said, researchers would consistently underestimate how long a project would take. He said AheadComputing’s engineers guess wrong about the timeframe, too, but now it’s because they’re moving much more quickly than they anticipated. </p><p id="BLIT55W24BGZBK6TIIZN5Q3KKY">“With a small, focused team of very capable people,” Dechene said, “you can get stuff done incredibly fast.” </p></div><p>If you purchase a product or register for an account through a link on our site, we may receive compensation.<span> By using this site, you consent to our <a href="https://www.advancelocal.com/advancelocalUserAgreement/user-agreement.html" target="_blank" rel="noopener noreferrer">User Agreement</a> and agree that your clicks, interactions, and personal information may be collected, recorded, and/or stored by us and social media and other third-party partners in accordance with our <a href="https://www.advancelocal.com/advancelocalUserAgreement/privacy-policy.html" target="_blank" rel="noopener noreferrer">Privacy Policy.</a></span></p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A masochist's guide to web development (201 pts)]]></title>
            <link>https://sebastiano.tronto.net/blog/2025-06-06-webdev/</link>
            <guid>44200895</guid>
            <pubDate>Fri, 06 Jun 2025 13:48:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sebastiano.tronto.net/blog/2025-06-06-webdev/">https://sebastiano.tronto.net/blog/2025-06-06-webdev/</a>, See on <a href="https://news.ycombinator.com/item?id=44200895">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<h2 id="table-of-contents">Table of contents</h2>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#setting-things-up">Setting things up</a></li>
<li><a href="#hello-world">Hello world</a></li>
<li><a href="#intermezzo-i-what-is-webassembly">Intermezzo I: What is WebAssembly?</a></li>
<li><a href="#building-a-library">Building a library</a></li>
<li><a href="#intermezzo-ii-javascript-and-the-dom">Intermezzo II: JavaScript and the DOM</a></li>
<li><a href="#loading-the-library-and-making-it-a-module">Loading the library and making it a module</a></li>
<li><a href="#multithreading">Multithreading</a></li>
<li><a href="#intermezzo-iii-web-workers-and-spectre">Intermezzo III: Web Workers and Spectre</a></li>
<li><a href="#dont-block-the-main-thread">Don’t block the main thread!</a></li>
<li><a href="#callback-functions">Callback functions</a></li>
<li><a href="#persistent-storage">Persistent storage</a></li>
<li><a href="#closing-thoughts">Closing thoughts</a></li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>I have recently worked on making a web application out of
<a href="https://git.tronto.net/nissy-core/file/README.md.html">my latest Rubik’s cube optimal solver</a>.
This involved building a rather complex C code base (with
multithreading, SIMD, callback functions and whatnot) to
<a href="https://en.wikipedia.org/wiki/WebAssembly">WebAssembly</a> via
<a href="https://emscripten.org/">Emscripten</a>, and writing a minimal amount of
JavaScript and HTML for the frontend.</p>
<p>This whole process was complex, tiring and at times frustrating -
but eventually <a href="https://tronto.net:48/">it was a success</a>! Not only
I accomplished my goal, but I have learnt a lot along the way. After
finishing the work, I decided to write down all that I have learnt and
share it with the world with this post.</p>
<p>You may be wondering why one should do such a thing instead of either
rewriting their code base in a more web-friendly language, or distributing
their app using a native GUI framework. The main reason to use WebAssembly
is that it can provide near-native performance (or so they claim) while
running inside a web browser; this gives you all the portability of a
web app without too much of a performance drawback, something that would
not be possible with an interpreted language such as JavaScript.</p>
<p>So, what is this blog post? A tutorial for web development? I am not sure
about this, but if it is, it is definitely not a normal one. As the title
suggests, you should not start from this guide unless you just <em>love</em>
banging your head against the wall.  If you are looking for a <em>sane</em>
guide to web development, I strongly advise you head on to the
<a href="https://developer.mozilla.org/en-US/docs/MDN/Tutorials">Mozilla Developer Network tutorials page</a>
and start from there.</p>
<p>But if you are a C or C++ developer looking to port a program or library
to the web, then you are in the right place. With this post I am going
to walk you through the process of building an increasingly complex
library that can run in a web browser.  Make sure you are
sitting comfortably and be ready to sweat, because I am not going to
shy away from the hard stuff and the complicated details.</p>
<p>To follow this tutorial you won’t need much experience with web
development, but some familiarity with HTML and an idea of what JavaScript
will be useful. It will also help to know that you can access your
browser’s JavaScript console and other developer tools by pressing F12,
at least on Firefox or Chrome - but I guess I have literally just taught
you that, if you did not already know it. For all the rest, I’ll make
sure to add many hyperlinks throughout the text, so you can follow them
if something is new to you.</p>
<p>A little disclaimer: although I am a somewhat experienced C developer,
I had very little web development experience before embarking in
this adventure.  If you are a web developer, you may find errors in
this post that are going to make you laugh at my ignorance. If you do,
I’d appreciate it if you could report them to me by sending an email to
<code>sebastiano@tronto.net</code>!</p>
<p>With this out of the way, let’s get started!</p>
<h2 id="setting-things-up">Setting things up</h2>
<p>The examples used in this tutorial are all contained in a git repository,
which you can find either on
<a href="https://git.tronto.net/emscripten-tutorial/file/README.md.html">my git page</a> or
<a href="https://github.com/sebastianotronto/emscripten-tutorial">on github</a>.</p>
<p>In order to follow them you are going to need:</p>
<ul>
<li>A working installation of <a href="https://emscripten.org/">Emscripten</a>
(which also includes Node.js). Refer to the official website for
installation instructions.</li>
<li>A web server such <a href="https://github.com/emikulic/darkhttpd">darkhttpd</a>
or the Python <code>http.server</code> package; the examples will use darkhttpd.</li>
</ul>
<p>I have only tested all of this on Linux, but everything should work
exactly the same on any UNIX system. If you are a Windows user, you can
either run everything inside
<a href="https://learn.microsoft.com/en-us/windows/wsl/">WSL</a>, or you can try and
adjust the examples to your system - if you choose this second option,
I’ll happily accept patches or pull requests :)</p>
<h2 id="hello-world">Hello world</h2>
<p>Let’s start with the classic Hello World program:</p>
<pre><code>#include &lt;stdio.h&gt;

int main() {
    printf("Hello, web!\n");
}
</code></pre>
<p>You can compile the code above with</p>
<pre><code>emcc -o index.html hello.c
</code></pre>
<p>And if you now start a web server in the current folder, for example with
<code>darkhttpd .</code> (the dot at the end is important), and open a web browser to
<a href="http://localhost:8080/">localhost:8080</a> (or whatever port your web server
uses), you should see something like this:</p>
<p><img src="https://sebastiano.tronto.net/blog/2025-06-06-webdev/hello.png" alt="Hello world in a browser"></p>
<p>As you can see, the compiler generated a bunch of extra stuff around
you print statement. You may or may not want this, but for now we can
take it as a convenient way to check that our program works as expected.</p>
<p>There are other ways to run this compiled code. With the command above,
the compiler should have generated for you 3 files:</p>
<ul>
<li><code>index.html</code> - the web page in the screenshot above.</li>
<li><code>index.wasm</code> - the actual compiled code of your program; this file contains
WebAssembly bytecode.</li>
<li><code>index.js</code> - some JavaScript <em>glue code</em> to make it possible for <code>index.wasm</code>
to actually run in a browser.</li>
</ul>
<p>If you don’t specify <code>-o index.html</code>, or if your specify <code>-o</code> followed
by a filename ending in <code>.js</code>, the <code>.html</code> page is not going to be
generated. In this case (but also if you <em>do</em> generate the html page),
you can run the JavaScript code in your terminal with:</p>
<pre><code>node index.js
</code></pre>
<p>In later examples, the same code may not work seamlessly in both a web
browser and in Node.js - for example, when dealing with persistent data
storage. But until then, we can generate all three files with a single
command and run our code in either way.</p>
<p>It is also possible to ask Emscripten to generate only the <code>.wasm</code> file,
in case you want to write the JavaScript glue code by yourself. To do
this, you can pass the <code>-sSTANDALONE_WASM</code> option to <code>emcc</code>. However,
in some cases the <code>.js</code> file is going to be generated even when this
option is used, for example when building a source file without a <code>main()</code>
entry point. Since this is something we’ll do soon, we can forget about
this option and just take it as a fact that the <code>.wasm</code> files generated
by emscripten require some glue JavaScript code to actually run,
but in case you are interested you can check out
<a href="https://emscripten.org/docs/tools_reference/settings_reference.html#standalone-wasm">the official documentation</a>.</p>
<p>You can find the code for this example, as well as scripts to
build it and run the web server, in the directory <code>00_hello_world</code>
of the git repository
(<a href="https://git.tronto.net/emscripten-tutorial/file/README.md.html">git.tronto.net</a>,
<a href="https://github.com/sebastianotronto/emscripten-tutorial">github</a>).</p>
<p>Anyway, now we can build our C code to run in a web page.  But this is
probably not the way we want to run it. First of all, we don’t want to
use the HTML template provided by Emscripten; but more importantly, we
probably don’t want to write a program that just prints stuff to standard
output. More likely, we want to write some kind of library of functions
that can be called from the front-end, so that the user can interact with
our program via an HTML + JavaScript web page.  Before going into that,
let’s take a break to discuss what we are actually compiling our code to.</p>
<h2 id="intermezzo-i-what-is-webassembly">Intermezzo I: What is WebAssembly?</h2>
<p><img src="https://sebastiano.tronto.net/blog/2025-06-06-webdev/wasm.png" alt="The logo of WebAssembly"></p>
<p><a href="https://en.wikipedia.org/wiki/WebAssembly">WebAssembly</a> is a low-level
language meant to run in a virtual machine inside a web browser. The main
motivation behind it is running higher-performance web applications compared
to JavaScript; this is made possible, by its
compact bytecode and its stack-based virtual machine.</p>
<p>WebAssembly (or WASM for short) is supported by all major browsers
since around 2017.  Interestingly, Emscripten, the compiler we are
using to translate our C code to WASM, first appeared in 2011,
predating WASM by a few years. Early on, Emscripten would compile
C and C++ code into JavaScript, or rather a subset thereof called
<a href="https://en.wikipedia.org/wiki/Asm.js">asm.js</a>.</p>
<p>Just like regular
<a href="https://en.wikipedia.org/wiki/Assembly_language">assembly</a>, WASM
also has a text-based representation. This means that one could write
WASM code directly, assemble it to bytecode, and then run it.  We are
not going to do it, but if you are curious here is a simple example
(computing the factorial of a number, taken from Wikipedia):</p>
<pre><code>(func (param i64) (result i64)
    local.get 0
    i64.eqz
    if (result i64)
        i64.const 1
    else
        local.get 0
        local.get 0
        i64.const 1
        i64.sub
        call 0
        i64.mul
    end)
</code></pre>
<p>As you can see, it looks like a strange mix of assembly and
<a href="https://en.wikipedia.org/wiki/Lisp_(programming_language)">Lisp</a>.
If you want to try and run WASM locally, outside of a web browser,
you could use something like <a href="https://wasmtime.dev/">Wasmtime</a>.</p>
<p>Until early 2025, the WASM “architecture” was 32-bit only. One big
limitation that this brings is that you cannot use more that 4GB
(2<sup>32</sup> bytes) of memory, because pointers are only 32 bits
long; moreover, your C / C++ code may need some adjustments if it
relied on the assumption that e.g. <code>sizeof(size_t) == 8</code>. At the
time writing a new standard that enables 64 bit pointers, called
WASM64, is supported on Firefox and Chrome, but not on Webkit-based
browsers such as Safari yet. Depending on when you are reading this,
this may have changed - you can check the status of WASM64 support
<a href="https://webassembly.org/features/">here</a>.</p>
<h2 id="building-a-library">Building a library</h2>
<p>Back to the main topic. Where were we? Oh yes, we wanted to build
a C <em>library</em> to WASM and call it from JavaScript. Our complex,
high-performance, math-heavy library probably looks something like this:</p>
<p>library.h (actually, we are not going to need this):</p>
<pre><code>int multiply(int, int);
</code></pre>
<p>library.c:</p>
<pre><code>int multiply(int a, int b) {
    return a * b;
}
</code></pre>
<p>Or maybe it is a bit more complicated than that. But we said we are
going to build up in complexity, and this is just the beginning, so
let’s stick to <code>multiply()</code>.</p>
<p>To build this library you can use:</p>
<pre><code>emcc -o library.js library.c
</code></pre>
<p>As we saw before, this is going to generate both a <code>library.js</code> and a
<code>library.wasm</code> file. Now we would like to call our library function
with something like this</p>
<p>program.js:</p>
<pre><code>var library = require("./library.js");
const result = library.multiply(6, 7);
console.log("The answer is " + result);
</code></pre>
<p><em>(The <code>require()</code> syntax above is valid when running this code in Node.js,
but not, for example when running in a browser. We’ll see in the next
session what to do in that case, but for now let’s stick to this.)</em></p>
<p>Unfortunately, this will not work for a couple of reasons. The reason
first is that Emscripten is going to add an underscore <code>_</code> to all our
function names; so we’ll have to call <code>library._multiply()</code>. But this
still won’t work, because by default the compiler does not <em>export</em> all
the functions in your code - that is, it does not make them visible to
the outside. To specify which functions you want to
export, you can use the <code>-sEXPORTED_FUNCTIONS</code> flag, like so:</p>
<pre><code>emcc -sEXPORTED_FUNCTION=_multiply -o library.js library.c
</code></pre>
<p>And now we finally have access to our <code>multiply()</code> function…</p>
<pre><code>$ node program.js
Aborted(Assertion failed: native function `multiply` called before runtime initialization)
</code></pre>
<p>…or maybe not. If you are new to JavaScript like I was a few weeks
ago, you may find this error message surprising. Some runtime must be
initialized, but can’t it just, like… initialize <em>before</em> trying to
run the next instruction?</p>
<p>Things are not that simple. A lot of things in JavaScript happen
<em>asynchronously</em>, and in these situations you’ll have to either use
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/await"><code>await</code></a>
or a
<a href="https://developer.mozilla.org/en-US/docs/Glossary/Callback_function"><em>callback function</em></a>.
So we’ll have to do something like this:</p>
<pre><code>var library = require("./build/library.js");

library.onRuntimeInitialized = () =&gt; {
    const result = library._multiply(6, 7);
    console.log("The answer is " + result);
};
</code></pre>
<p>And now we can finally run our program:</p>
<pre><code>$ node program.js 
The answer is 42
</code></pre>
<p>The code for this example can be found in the <code>01_library</code> folder in
the git repository
(<a href="https://git.tronto.net/emscripten-tutorial/file/README.md.html">git.tronto.net</a>,
<a href="https://github.com/sebastianotronto/emscripten-tutorial">github</a>).</p>
<h2 id="intermezzo-ii-javascript-and-the-dom">Intermezzo II: JavaScript and the DOM</h2>
<p><img src="https://sebastiano.tronto.net/blog/2025-06-06-webdev/logos.png" alt="The logos of HTML, CSS and JavaScript"></p>
<p>If we want to build an interactive web page using JavaScript, we’ll
need a way for our script to communicate with the page, i.e. a way
to access the HTML structure from JavaScript code. What we are looking
for is called
<em><a href="https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model">Document Object Model</a></em>,
or DOM for short.</p>
<p>For example, if you have a paragraph with some text in your HTML:</p>
<pre><code>&lt;p id="myParagraph"&gt;Hello!&lt;/p&gt;
</code></pre>
<p>you can access this text from JavaScript like this:</p>
<pre><code>var paragraph = document.getElementById("myParagraph");
paragraph.innerText = "New text!";
</code></pre>
<p>Here we are selecting the paragraph HTML element using its ID, and we
are changing its text via its <code>innerText</code> property, all from JavaScript.</p>
<p>Let’s see a more complex example:</p>
<p>HTML:</p>
<pre><code>&lt;button id="theButton"&gt;Press me!&lt;/button&gt;
</code></pre>
<p>JS:</p>
<pre><code>var button = document.getElementById("theButton");
var counter = 0;

button.addEventListener("click", () =&gt; {
    counter++;
    button.innerText = "I have been pressed " + counter + " times!";
});
</code></pre>
<p>In the example above we add an
<em><a href="https://developer.mozilla.org/en-US/docs/Web/API/EventTarget/addEventListener">event listener</a></em>
to a button: the (anonymous) function we defined is going to be called
every time the button is clicked. And since this is a web page, I guess
I can show you what this actually looks like.</p>
<p>Behold, the dynamic button:</p>


<p>If you are completely new to web development, you may be wondering
where you should write this JavaScript code. One option is to write it
in the same HTML file as the rest of the page, inside a <code>&lt;script&gt;</code> tag;
this is how I did it in the example above, as you can check by viewing
the source of this page (press Ctrl+U, or right-click and select
“view source”, or prepend <code>view-source:</code> to this page’s URL; hopefully
at least one of these methods should work in your browser).</p>
<p>However, if the script gets too large you may want to split it off in
a separate file, which we’ll demonstrate in this next example.</p>
<p>Let’s now make a template web page for using our powerful library. Let’s
start with the HTML, which is in large part boilerplate:</p>
<p>index.html:</p>
<pre><code>&lt;!doctype html&gt;
&lt;html lang="en-US"&gt;
&lt;head&gt;
    &lt;meta charset="utf-8" /&gt;
    &lt;meta name="viewport" content="width=device-width" /&gt;
    &lt;title&gt;Multiply two numbers&lt;/title&gt;
    &lt;script src="./script.js" defer&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;body&gt;
    &lt;p&gt;
    &lt;input id="aInput" /&gt; x &lt;input id="bInput" /&gt;
    &lt;button id="goButton"&gt;=&lt;/button&gt;
    &lt;span id="resultText"&gt;&lt;/span&gt;
    &lt;/p&gt;
&lt;/body&gt;

&lt;/html&gt;
</code></pre>
<p>Besides the <code>&lt;body&gt;</code> element, the only important line for us is line
7, which loads the script from a file. Notice that we use the <code>defer</code>
keyword here: this is telling the browser to wait until the whole page
has been loaded before executing the script. If we did not do this, we
could run in the situation where we <code>document.getElementById()</code> returns
<code>null</code>, because the element we are trying to get is not loaded yet (yes,
this happened to me while I was writing this post). If you want to know
more, check out this
<a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/script#defer">MDN page</a>.</p>
<p>Now to the JavaScript code. For now we are going to use the built-in
<code>*</code> operator to multiply the two numbers, but in the next section we
are going to replace it with our own library.</p>
<p>script.js (in the same folder as index.html):</p>
<pre><code>var aInput = document.getElementById("aInput");
var bInput = document.getElementById("bInput");
var button = document.getElementById("goButton");
var resultText = document.getElementById("resultText");

button.addEventListener("click", () =&gt; {
    var a = Number(aInput.value);
    var b = Number(bInput.value);
    resultText.innerText = a * b;
});
</code></pre>
<p>The final result will look something like this:</p>
<p>
 x 

<span id="resultText"></span>
</p>

<p>In a real-world scenario you would probably want to check that the text
provided in the input fields is actually a number, or perhaps use the
<a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/input/number"><code>type="number"</code></a>
attribute for the input fields. But we’ll ignore these issues here -
we are going to have more serious problems to deal with.</p>
<h2 id="loading-the-library-and-making-it-a-module">Loading the library and making it a module</h2>
<p>With what we have learned in the previous intermezzo (you are not skipping
those, right?) we can finally run our library code in a real web page. The
code is pretty much the same as above; we just need to include both the
library and the script file in the HTML:</p>
<pre><code>    &lt;script src="./library.js" defer&gt;&lt;/script&gt;
    &lt;script src="./script.js" defer&gt;&lt;/script&gt;
</code></pre>
<p>and of course we have to change the line where we perform the multiplication:</p>
<pre><code>    resultText.innerText = Module._multiply(a, b);
</code></pre>
<p>Here <code>Module</code> is the default name given to our library by
Emscripten. Apart from being too generic a name, this leads to another
problem: we can’t include more than one Emscripten-built library in our
page in this way - otherwise, both are going to be called <code>Module</code>.</p>
<p>Luckily, there is another way: we can build a
<a href="https://emscripten.org/docs/compiling/Modularized-Output.html">modularized</a>
library, i.e. obtain a
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Modules">JavaScript Module</a>.
This may sound a bit strange, because the name <code>Module</code> kind of implies
there is already a module. The way I understand it is that by default
Emscripten produces a <em>script</em> that <em>contains</em> a module named <code>Module</code>;
when building a modularized library, the whole resulting file is a module.</p>
<p>Modularizing our build is not necessary right now, but
there are a couple of other advantages to it:</p>
<ul>
<li>As mentioned above, we can change the name of our module and include
more than one Emscripten-built library, if we want.</li>
<li>We will be able to use the module in the same way in Node.js and in
our web page script. This way we can minimize the differences between
the two versions of our code, which can be useful for testing.</li>
<li>In case we want to build a more complex layer of JavaScript between
our library and our web page, with a modularized build we can easily
include the module in another file, which can then be included in the
main script.</li>
</ul>
<p>So let’s go ahead and build our library like so:</p>
<pre><code>emcc -sEXPORTED_FUNCTION=_multiply -sMODULARIZE -sEXPORT_NAME=MyLibrary \
     -o library.mjs library.c
</code></pre>
<p>Notice I have changed the extension from <code>.js</code> to <code>.mjs</code>. Don’t worry,
either extension can be used. And you are going to run into issues with
either choice:</p>
<ul>
<li>If you run your code in Node.js, it will understand that the library
file is a module only if you use the <code>.mjs</code> extension. Alternatively,
you can change some settings in a local configuration file to
enforce this.</li>
<li>If you run your code in a web page, your web server may not be
configured to serve <code>.mjs</code> files as JavaScript files. This can
easily be changed by adding a configuration line somewhere.</li>
</ul>
<p>In my examples I chose to use the <code>.mjs</code> extensions to make Node.js
happy, and I changed the configuration of my web servers as needed. For
example, for darkhttpd I added a file called <code>mime.txt</code> with a single
line <code>text/javascript mjs</code>, and launched the server with the
<code>--mimetypes mime.txt</code> option.</p>
<p>Now we have to make a couple of changes. Our <code>program.js</code>, for running
in node, becomes:</p>
<pre><code>import MyLibrary from "./library.mjs"

var myLibraryInstance = await MyLibrary();

const result = myLibraryInstance(6, 7);
console.log("The answer is " + result);
</code></pre>
<p>By the way, I have renamed this file to <code>program.mjs</code>. This is because
only modules can use the
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/import">static <code>import</code></a>
statement; alternatively, I could have used the
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/import">dynamic <code>import()</code></a>
and kept the <code>.js</code> extension.</p>
<p>Similary, we have to update our <code>script.js</code> (or <code>script.mjs</code>) to import
the module and create an instance. Moreover, we have to specify in the
HTML that the script is now a module:</p>
<pre><code>    &lt;script src="./script.mjs" type="module" defer&gt;&lt;/script&gt;
</code></pre>
<p>And we can get rid of the other <code>&lt;script&gt;</code> tag, since now the library
is included directly in <code>script.mjs</code>.</p>
<p>You can find the full the code for this example the folder
<code>02_library_modularized</code> in the git repository
(<a href="https://git.tronto.net/emscripten-tutorial/file/README.md.html">git.tronto.net</a>,
<a href="https://github.com/sebastianotronto/emscripten-tutorial">github</a>).</p>
<h2 id="multithreading">Multithreading</h2>
<p><img src="https://sebastiano.tronto.net/blog/2025-06-06-webdev/threads.jpg" alt="&quot;Gotta go fast&quot; meme"></p>
<p>Let’s move on to a more interesting example. If one of the goals of
WebAssembly is performance, there is no point in using only 1/16th of
your CPU - let’s port a multithreaded application to the web!</p>
<p>As a more complicated example, let’s write a function that counts how
many prime numbers there are in a given range. This function takes two
integers as input and returns a single integer as output, but it does
a non-trivial amount of work under the hood. A simple implementation of
this routine would be something like this:</p>
<pre><code>bool isprime(int n) {
    if (n &lt; 2)
        return false;

    for (int i = 2; i*i &lt;= n; i++)
        if (n % i == 0)
            return false;
    return true;
}

int primes_in_range(int low, int high) {
    if (low &lt; 0 || high &lt; low)
        return 0;

    int count = 0;
    for (int i = low; i &lt; high; i++)
        if (isprime(i))
            count++;

    return count;
}
</code></pre>
<p>This algorithm is
<a href="https://en.wikipedia.org/wiki/Embarrassingly_parallel">embarassingly parallelizable</a>:
we can split the interval <code>[low, high)</code> into smaller sub-intervals and
process each one of them in a separate thread; then we just need to add
up the results of the sub-intervals.</p>
<p>For the actual implementation, we are going to use
<a href="https://en.wikipedia.org/wiki/Pthreads">pthreads</a>, for the simple reason
that it is
<a href="https://emscripten.org/docs/porting/pthreads.html">supported by Emscripten</a>.
In practice, assuming we are working on a UNIX platform, we could also
use C11’s <a href="https://en.cppreference.com/w/c/header/threads">threads.h</a> or
C++’s <a href="https://en.cppreference.com/w/cpp/thread/thread.html">std::thread</a>,
but only because they happen to be wrappers around pthreads. On other
platforms, or in other implementations of the C and C++ standard library,
this may not be the case; so we’ll stick to old-school pthreads.</p>
<p>This is my parallel version of <code>primes_in_range()</code>:</p>
<p>primes.c:</p>
<pre><code>#include &lt;stdbool.h&gt;
#include &lt;pthread.h&gt;

#define NTHREADS 16

bool isprime(int);
void *pthread_routine(void *);

struct interval { int low; int high; int count; };

int primes_in_range(int low, int high) {
    pthread_t threads[NTHREADS];
    struct interval args[NTHREADS];

    if (low &lt; 0 || high &lt; low)
        return 0;

    int interval_size = (high-low)/NTHREADS + 1;
    for (int i = 0; i &lt; NTHREADS; i++) {
        args[i].low = low + i*interval_size;
        args[i].high = args[i].low + interval_size;
        pthread_create(&amp;threads[i], NULL, pthread_routine, &amp;args[i]);
    }

    int result = 0;
    for (int i = 0; i &lt; NTHREADS; i++) {
        pthread_join(threads[i], NULL);
        result += args[i].count;
    }

    return result;
}

bool isprime(int n) {
    if (n &lt; 2)
        return false;

    for (int i = 2; i*i &lt;= n; i++)
        if (n % i == 0)
            return false;
    return true;
}

void *pthread_routine(void *arg) {
    struct interval *interval = arg;

    interval-&gt;count = 0;
    for (int i = interval-&gt;low; i &lt; interval-&gt;high; i++)
        if (isprime(i))
            interval-&gt;count++;

    return NULL;
}
</code></pre>
<p><em>(Pro tip: if you take the number of threads as an extra parameter for
your function, you can pass to it the value
<a href="https://developer.mozilla.org/en-US/docs/Web/API/Navigator/hardwareConcurrency"><code>navigator.hardwareConcurrency</code></a>
from the JavaScript front-end and use exactly the maximum number of
threads that can run in parallel on the host platform.)</em></p>
<p>To build this with Emscripten we’ll have to pass the <code>-pthread</code> option and,
optionally, a suitable value for
<a href="https://emscripten.org/docs/tools_reference/settings_reference.html#pthread-pool-size"><code>-sPTHREAD_POOL_SIZE</code></a>.</p>
<p>If we want to run our multithreaded code in an actual browser, we’ll
have to scratch our head a bit harder. The code we are supposed to
write is exactly what we expect, but once again we have to tinker with
our web server configuration. For technical reasons that we’ll cover in
the next intermezzo, in order to run multithreaded code in a browser
we must add a couple of HTTP headers:</p>
<pre><code>Cross-Origin-Opener-Policy: same-origin
Cross-Origin-Embedder-Policy: require-corp
</code></pre>
<p>These headers are part of the response your browser will receive when
it requests any web page from the server. The way you set these depends on
the server you are using; with darkhttpd you can use the <code>--header</code> option.</p>
<p>With your server correctly set up, you can enjoy a multithreaded program
running in your browser!  As always, you can check out this example from
the <code>03_threads</code> folder of the git repository
(<a href="https://git.tronto.net/emscripten-tutorial/file/README.md.html">git.tronto.net</a>,
<a href="https://github.com/sebastianotronto/emscripten-tutorial">github</a>).</p>
<h2 id="intermezzo-iii-web-workers-and-spectre">Intermezzo III: Web Workers and Spectre</h2>
<p><img src="https://sebastiano.tronto.net/blog/2025-06-06-webdev/spectre.png" alt="The logo of the Spectre vulnerability"></p>
<p>On a low level, threads are implemented by Emscripten using
<a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API">web workers</a>,
which are processes separated from the main web page process and
communicate with it and with each other by
<a href="https://developer.mozilla.org/en-US/docs/Web/API/Worker/postMessage">passing messages</a>.
Web workers are commonly used to run slow operations in the background
without blocking the UI threads, so the web page remains responsive
while these operations run - we’ll do this in the next section.</p>
<p>Web workers do not have regular access to the same memory as the main
process, and this is something that will give us some issues in later
sections. However, there are ways around this limitation. One of these
ways is provided by
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/SharedArrayBuffer">SharedArrayBuffer</a>,
which we won’t use directly in this tutorial, but is used by
Emscripten under the hood.</p>
<p>And this is why we had to set the <code>Cross-Origin-*</code> headers. In 2018, a
CPU vulnerability called <a href="https://spectreattack.com/">Spectre</a> was found,
and it was shown that an attacker could take advantage of shared memory
between the main browser thread and web workers to
<a href="https://en.wikipedia.org/wiki/Spectre_(security_vulnerability)#Remote_exploitation">execute code remotely</a>.
As a counter-measure, most browsers now require your app to be in a
<a href="https://developer.mozilla.org/en-US/docs/Web/Security/Secure_Contexts">secure context</a>
and
<a href="https://developer.mozilla.org/en-US/docs/Web/API/Window/crossOriginIsolated">cross-origin isolated</a>
to allow using <code>SharedArrayBuffer</code>s.</p>
<p>Even if you do not plan to use web workers directly, it is still good to
have a rough idea of how they work, because of the
<a href="https://en.wikipedia.org/wiki/Leaky_abstraction">law of leaky abstractions</a>:
<em>all abstractions are leaky</em>.
The fact that we had to mess around with our <code>Cross-Origin-*</code> headers
despite not caring at all about <code>SharedArrayBuffer</code>s is a blatant example
of this.</p>
<h2 id="dont-block-the-main-thread">Don’t block the main thread!</h2>
<p>If you have run the previous example, may have noticed a scary warning
like this in your browser’s console:</p>
<p><img src="https://sebastiano.tronto.net/blog/2025-06-06-webdev/blocking.png" alt="A warning saying &quot;Blocking on the main thread is very dangerous, see [link]&quot;"></p>
<p><em>The link points to
<a href="https://emscripten.org/docs/porting/pthreads.html#blocking-on-the-main-browser-thread">this page</a>
in Emscripten’s documentation.</em></p>
<p>The issue here is that our heavy computation is not running “in the
background”, but its main thread (the one spawning the other threads)
coincides with the browser’s main thread, the one that is responsible
for drawing the UI and handling user interaction. So if our computation
really takes long, the browser is going to freeze - and after a few
seconds it will ask us if we want to kill this long-running script.</p>
<p>As we anticipated in the previous intermezzo, we are going to solve this
with a web worker. We will structure this solution as follows:</p>
<ul>
<li>The main script will be responsible for reading the user input, sending
a message to the worker to ask it to compute the result, and handling
the result that the worker is going to send back once it is done. No
slow operation is performed by this script, so that it won’t block
the main thread.</li>
<li>The worker will be responsible for receiving mesages from the main
script, handling them by calling the library, and sending a message
with the response back once it is done computing.</li>
</ul>
<p>In practice, this will look like this:</p>
<p>script.mjs:</p>
<pre><code>var aInput = document.getElementById("aInput");
var bInput = document.getElementById("bInput");
var button = document.getElementById("goButton");
var resultText = document.getElementById("resultText");

var worker = new Worker("./worker.mjs", { type: "module" });

button.addEventListener("click", () =&gt; worker.postMessage({
    a: Number(aInput.value),
    b: Number(bInput.value)
}));

worker.onmessage = (e) =&gt; resultText.innerText = "There are " +
    e.data.result + " primes between " + e.data.a + " and " + e.data.b;
</code></pre>
<p>worker.mjs:</p>
<pre><code>import Primes from "./build/primes.mjs";

var primes = await Primes();

onmessage = (e) =&gt; {
    const count = primes._primes_in_range(e.data.a, e.data.b);
    postMessage({ result: count, a: e.data.a, b: e.data.b });
};
</code></pre>
<p>More complicated than before, but nothing crazy. Notice how we are using
<a href="https://developer.mozilla.org/en-US/docs/Web/API/Worker/postMessage"><code>postMessage()</code></a>
and
<a href="https://developer.mozilla.org/en-US/docs/Web/API/Worker/message_event"><code>onmessage()</code></a>
to pass events back and forth. The argument of <code>postMessage()</code> is the
actual data we want to send in
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON">JSON</a>
format, while the argument of <code>onmessage()</code> is an
<a href="https://developer.mozilla.org/en-US/docs/Web/API/Event">event</a>
whose <code>data</code> property contains the object that was sent with <code>postMessage()</code>.</p>
<p>You can check out this example in the directory <code>04_no_block</code> in the
repository
(<a href="https://git.tronto.net/emscripten-tutorial/file/README.md.html">git.tronto.net</a>,
<a href="https://github.com/sebastianotronto/emscripten-tutorial">github</a>).
Try also large numbers, in the range of millions or tens of millions, and
compare it with the previous example - but not don’t go too large, we
only support 32-bit integers for now.  Notice how, with this new setup,
the browser remains responsive while it is loading the response.</p>
<p>Oh and by the way, a nice exercise for you now could be making the
script show some kind of <code>"Loading result..."</code> message while the worker
is working. This is not hard to do, but a huge improvement in user
experience!</p>
<h2 id="callback-functions">Callback functions</h2>
<p><img src="https://sebastiano.tronto.net/blog/2025-06-06-webdev/callback.jpg" alt="The hand of a person using a phone"></p>
<p>For one reason or another, your library function may take as parameter
another function. For example, you may use this other function to print
log messages regardless of where your library code is run: a command-line
tool may pass <code>printf()</code> to log to console, while a GUI application
may want to show these messages to some text area in a window, and it
will pass the appropriate function pointer parameter. This is the use case
that we are going to take as an example here, but it is not the only one.</p>
<p>Implementing this was probably the step that took me the longest in my
endeavor to port my Rubik’s cube solver to the web. Luckily for you,
when writing this post I found a simpler method, so you won’t have to
endure the same pain.</p>
<p>First, we’ll have to adapt our library function like this:</p>
<pre><code>int primes_in_range(int low, int high, void (*log)(const char *)) {
    /* The old code, with calls to log() whenever we want */
};
</code></pre>
<p><em>Tip: when using callback functions like this, it is good practice
to have them accept an extra <code>void *</code> parameter, and the library
function should also accept an extra <code>void *</code> parameter that it then
passes on to the callback. So our function would look something like
this: <code>int primes_int_range(int low, int high, void (*log)(const char *, void *), void *log_data)</code>.
This makes the setup extremely flexible, and allows passing callback
functions in situation where this may be tricky. For example, this
way you could pass a C++ member function by passing an object as
<code>log_data</code> and a function that call <code>log_data</code>’s member function
as <code>log</code>. Since we are not going to use this in this example, I’ll stick
to the simpler setup.</em></p>
<p>Now, to call our function from the JavaScript side we would like
to do something like this:</p>
<pre><code>int result = primes_in_range(a, b, console.log); // Logging to console
</code></pre>
<p>Unfortunately, this will not work, because <code>console.log</code>, a JavaScript
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Function">function object</a>,
does not get automatically converted to a function <em>pointer</em>, which is
what C expects. So we’ll have to do something slightly more complicated:</p>
<pre><code>import Primes from "./build/primes.mjs"

var primes = await Primes();
const logPtr = primes.addFunction((cstr) =&gt; {
    console.log(primes.UTF8ToString(cstr));
}, "vp");

const count = primes._primes_in_range(1, 100, logPtr);
</code></pre>
<p>Here <code>addFunction()</code> is a function generated by Emscripten.  Notice also
that we are wrapping our <code>console.log()</code> in a call to <code>UTF8ToString()</code>,
an Emscripten utility to convert C strings to JavaScript strings, and
that we are passing the function’s signature <code>"vp"</code> (returns <code>void</code>,
takes a <code>pointer</code>) as an argument; see
<a href="https://emscripten.org/docs/porting/connecting_cpp_and_javascript/Interacting-with-code.html#function-signatures">here</a>
for more information.</p>
<p>Other than that, you just need to add a couple of compiler flags:</p>
<ul>
<li><code>-sEXPORTED_RUNTIME_METHODS=addFunction,UTF8ToString</code> to tell the
compiler to make these two methods available.</li>
<li><code>-sALLOW_TABLE_GROWTH</code> to make it possible to add functions to
out module at runtime with <code>addFunction()</code>.</li>
</ul>
<p>And as you can check by running the example <code>05_callback</code> from the repo
(<a href="https://git.tronto.net/emscripten-tutorial/file/README.md.html">git.tronto.net</a>,
<a href="https://github.com/sebastianotronto/emscripten-tutorial">github</a>),
everything works as expected, both in Node.js and in a web page.  To make
the examples more interesting, the web page one is not only not logging the
messages to console, but it also shows them as text in the web page.</p>
<p><em>Note: you must be careful where you call this callback function from.
If you try to call it from outside the main thread - for example, in one
of the threads that are spawned to count the primes in the sub-intervals
- you’ll get a horrible crash. This is because web workers do not have
access to the functions that reside in another worker’s memory.</em></p>
<h2 id="persistent-storage">Persistent storage</h2>
<p><img src="https://sebastiano.tronto.net/blog/2025-06-06-webdev/storage.png" alt="3D rendering of a spinning hard drive"></p>
<p>Our multithreaded implementation of <code>primes_in_range()</code> is not slow, but
it could be faster. One possible way to speed it up is to use a look-up
table to make <code>is_prime()</code> run in constant time; for this we’ll need to
memorize which numbers below 2<sup>31</sup> (the maximum value of 32-bit
signed integer) are prime. This will require 2<sup>31</sup> bits of data,
or 256MB.  It would be nice if we could store this data persistently in
the user’s browser, so that if they use our app again in the future we
won’t need to repeat expensive calculations or re-download large files.</p>
<p>Putting aside the question of whether any of the above is a good idea,
and assuming you know how to generate such a table, in C you would
read and store the data like this:</p>
<pre><code>#include &lt;stdio.h&gt;

#define FILENAME "./build/primes_table"

void read_table(unsigned char *table) {
    FILE *f = fopen(FILENAME, "rb");
    fread(table, TABLESIZE, 1, f);
    fclose(f);
}

void store_table(const unsigned char *table) {
    FILE *f = fopen(FILENAME, "wb");
    fwrite(table, TABLESIZE, 1, f);
    fclose(f);
}
</code></pre>
<p><em>Note: the code snippet above is extremely simplified, you probably want
to add some error-handling code if you implement something like this.</em></p>
<p>The good news is that we can use the same code when building with
Emscripten! The bad news is that… well, it’s a bit more complicated
than that.</p>
<p>First of all, it is important to know that
<a href="https://emscripten.org/docs/api_reference/Filesystem-API.html">Emscripten’s File System API</a>
supports different “backends”, by which I mean ways of translating the
C / C++ file operations to WASM / JavaScript. I am not going to discuss
all of them here, but I want to highlight a few key points:</p>
<ul>
<li>The default backend is called <code>MEMFS</code>. It is a virtual file system
that resides in RAM, and all data written to it is lost when the
application is closed.</li>
<li>Only one of these backends (<code>NODERAWFS</code>) gives access to the actual
local file system, and it is only usable when running your app with
Node.js. Browsers are <em>sandboxed</em>, and the filesystem is not normally
accessible to them. There are ways, such as the
<a href="https://developer.mozilla.org/en-US/docs/Web/API/File_System_API">File System API</a>,
to access files, but as far as I understand each file you want to
access requires explicit actions from the user. We would like to manage
our data automatically, so we are not going to use this API.</li>
<li>The backend we are going to use is called <code>IDBFS</code>. It provides access
to the <a href="https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API">IndexedDB API</a>,
which allows to persistently store large quantities of data in the
browser’s cache. The data is only removed if the user asks for it,
for example by cleaning it from the browser’s settings page.</li>
</ul>
<p>To activate the <code>IDBFS</code> backend, we are going to add <code>--lidbfs.js</code>
to our compiler options.  The Indexed DB is not the only way to store
data persistently in the browser. For an overview of all the options,
you can take a look at
<a href="https://developer.mozilla.org/en-US/docs/Learn_web_development/Extensions/Client-side_APIs/Client-side_storage">this page on MDN</a>.</p>
<p>The compiler flag is not enough, however. We also need to:</p>
<ol>
<li>Create a directory (for the virtual file system) where our data file
is going to be stored. We are going to call this directory <code>assets</code>,
but you can pick any other name; it does not have to coincide with the
name of a directory that exists on your local file system.</li>
<li>Mount the directory we have just created in the indexed DB.</li>
<li>Synchronize the virtual file system, so that our script is able to
read pre-existing files.</li>
</ol>
<p>All of the above has to be done from JavaScript, which makes things a
little bit complicated, because we are reading our files from C code.
We have a couple of ways to work around this issue:</p>
<ul>
<li>Using
<a href="https://emscripten.org/docs/porting/connecting_cpp_and_javascript/Interacting-with-code.html#interacting-with-code-call-javascript-from-native">inline JavaScript</a>
in our C code with the <code>EM_JS()</code> or <code>EM_ASYNC_JS()</code> Emscripten macros.</li>
<li>Setting up the indexed DB file system when the module loads using
the <code>--pre-js</code> compiler option.</li>
</ul>
<p>Here we are going to use the second solution, but the first option is
good to keep in mind, because it allows us to call JavaScript code at
any point rather than just at startup.</p>
<p><em>Note: if you do end up using <code>EM_ASYNC_JS()</code> to make asynchronous
JavasScript functions callable from C, keep in mind that any C
function that, directly or indirectly, calls an async JavaScript
function, will now return a
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise">promise</a>
when called from JavaScript. But wether an async function is called is
determined at runtime, so you C function may return a value one time
and a promise another time, depending on how exactly it runs!</em></p>
<p>So we are going to add <code>--pre-js init_idbfs.js</code> to our compiler options,
with <code>init_idbfs.js</code> containing the following:</p>
<pre><code>Module['preRun'] = [
    async () =&gt; {
        const dir = "/assets";

        FS.mkdir(dir);
        FS.mount(IDBFS, { autoPersist: true }, dir);

        Module.fileSystemLoaded = new Promise((resolve, reject) =&gt; {
            FS.syncfs(true, (err) =&gt; {
                if (err) reject(err);
                else resolve(true);
            });
        });

    }
];
</code></pre>
<p>As you can see, the syncing operation is more complicated, the main
reason being that it is an
<a href="https://developer.mozilla.org/en-US/docs/Learn_web_development/Extensions/Async_JS">asynchronous operation</a>.
For this reason, we are wrapping it in a
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise">Promise</a>,
so we can detect when this operation is done and react accordingly.
We are going to do so from our worker script, which will send a message to
the main script to communicate that the file system is ready to go:</p>
<pre><code>primes.fileSystemLoaded.then(() =&gt; {
    postMessage({ type: "readySignal" });
});
</code></pre>
<p>The main script can then handle this signal as it prefers, for example by
enabling the <code>Compute</code> button, if it was previously marked as <code>disabled</code>.</p>
<p>One last thing: since we are now using a large amount of memory and
loading the virtual file system at the start, the compiler will complain
that we are not reserving enough memory for our application. Adding a
<code>-sINITIAL_MEMORY=272629760</code> compiler flag will do the trick (watch out:
the number you provide must be a multiple of 2<sup>16</sup>). I am not
entirely sure why this is the case, since we are not loading the file in
memory statically, but only at runtime, and only when the
<code>primes_in_range()</code> function is called. I would expect that using
<a href="https://emscripten.org/docs/tools_reference/settings_reference.html#allow-memory-growth"><code>-sALLOW_MEMORY_GROWTH</code></a>
would be enough - and indeed this is the case if we use the <code>EM_ASYNC_JS()</code>
macro to load the file system on-demand.</p>
<p>And with all this, we are ready to run our optimized version of the
<code>primes_in_range()</code> algorithm, all from within our browser! As always,
you can check out the complete code in the folder <code>06_storage</code> of
the repository
(<a href="https://git.tronto.net/emscripten-tutorial/file/README.md.html">git.tronto.net</a>,
<a href="https://github.com/sebastianotronto/emscripten-tutorial">github</a>).</p>
<p>If generating this data on the user’s side seems redundant, you can
also have it downloaded from the server. I won’t explain how to it here,
since there are many possible ways to achieve this - after all, the indexed
DB is also accessible from JavaScript. If you want to experiment more
with Emscripten you can try to use the
<a href="https://emscripten.org/docs/api_reference/fetch.html">Fetch API</a>; in my
project I was not able to make its synchronous version work together with
<code>-sMODULARIZE</code>, so I ended up using
<a href="https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API"><code>fetch()</code></a>
directly from within an <code>EM_ASYNC_JS()</code> function. This tutorial is already
too long, so I am going to leave this as an exercise for the reader.</p>
<h2 id="closing-thoughts">Closing thoughts</h2>
<p>I have discussed almost everything that I have learned about building a
webapp in C / C++ with Emscripten. I ended up using C, not C++, for all
of my example, so I did not have a chance to discuss some neat C++-specific
features such as
<a href="https://emscripten.org/docs/porting/connecting_cpp_and_javascript/embind.html"><code>EMBIND()</code></a>
and
<a href="https://emscripten.org/docs/api_reference/val.h.html"><code>emscripten::val</code></a>
- do check them out if you plan to use C++ for your web app!</p>
<p>Even if this page is structured like a tutorial, this is probably better
described as a collection of personal notes, a “brain dump” that I wrote
for myself as is the case with many of my blog posts. Writing this piece
was a great occasion for me to review the work that I have done and the
things I have learned. And while reflecting on all of this I was able to
isolate a specific impression that I had while working on this,
and I summarized it in on sentence:</p>
<center><strong><i>
It’s leaky abstractions all the way down.
</i></strong></center>
<p>If you have not encountered this term before (but you should, I have already
used it in this post), <em>leaky abstraction</em> is a term used to describe the
failure of an abstraction to hide the low-level details it is abstracting.
The so-called
<a href="https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/">law of leaky abstractions</a>
says that all abtractions are leaky. But, in my opinion not all
abstractions leak in the same way - some leak way more than others.</p>
<p>Emscripten is a great project that tries to abstract away all the web
(JavaScript, WASM, web workers, local storage…) so that you can build
and run your C / C++ code in a web browser. Frankly, this is mind-blowing,
and I have mad respect for the Emscripten developers.</p>
<p>But as soon as the complexity of your codebase bumps up a notch, you
immediately find out that the abstractions don’t hold anymore. If yor
app is multithreaded, you have to learn what a web worker is. If you
want to read some data from a file, welcome to the world of client-side
storage. You need 64-bit memory support because you are processing more
than 2GB of data? Sure, but first make sure that your users are not
using Safari.</p>
<p>But I am not complaining about this. A browser is a very different beast
from a bare-metal operating system, and it is to be expected that you
have to know something about the system you are deploying to. I am happy
that I could learn about all of this, and I believe this knowledge is
going to give me an extra edge whenever I’ll work on the web again.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Curate your shell history (103 pts)]]></title>
            <link>https://esham.io/2025/05/shell-history</link>
            <guid>44200870</guid>
            <pubDate>Fri, 06 Jun 2025 13:46:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://esham.io/2025/05/shell-history">https://esham.io/2025/05/shell-history</a>, See on <a href="https://news.ycombinator.com/item?id=44200870">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Simon Tatham wrote an article recently called <a href="https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/transience/">“Policy of transience”</a>, explaining (among other things) why you might want to disable your shell history file. Simon writes:</p>
<blockquote>
<p>My unusual habit is: <strong>turn off the history file completely,</strong> by putting the command ‘<code>unset HISTFILE</code>’ in my <code>.bashrc</code>. I still get history within a single instance of the shell, so I can edit my last command ten times until it works properly; but history isn’t shared between my terminal windows, or preserved when I log out and log in again. All the shell history I allow myself is localised and short-term.</p>
<p>[…]</p>
<p>If I type a shell command that’s <em>valuable</em> – one that did something useful enough that I might want it again in future, and long and complicated enough that I’d be annoyed to have to figure it out a second time from scratch – then I can’t rely on it just happening to be in my <code>.bash_history</code>. So instead I put it somewhere else: maybe a shell function in my <code>.bashrc</code>, or maybe a shell script in my directory of random useful scriptlets. Or maybe just in a file of notes jotted down to myself about useful shell runes to remember.</p>
<p>I find this a more useful way to remember shell commands. Firstly, this procedure separates the <em>working</em> version of the command from all the failed attempts just before it. Even within the context of one instance of <code>bash</code> I’ll sometimes accidentally recall a <em>wrong</em> version of a command when I was aiming for the corrected one two commands later; the idea of having a <em>year’s worth</em> of my own false starts available for accidental recall seems horrifying! Instead, I deliberately save <em>just</em> the working version, and let all the failed attempts go in the trash when I close the shell.</p>
</blockquote>
<p>For me, this idea feels uncomfortable! If anything, I’m a shell history maximalist; I have zsh configured to save my last 9,800 commands.<a href="#shell-history_fn1" id="shell-history_fnref1" role="doc-noteref"><sup>1</sup></a> I rely heavily on my shell history to remember how I did things before. I suspect my most-used form of zsh completion is to complete the current command line from a history entry.</p>
<p>But I also think Simon makes a good point about how useless it is to record all of your false starts. Why save <code>cd ~/Dekstop</code>, which I’m never going to want to run again?<a href="#shell-history_fn2" id="shell-history_fnref2" role="doc-noteref"><sup>2</sup></a> Why save <code>vim /etc/rc.conf</code> when <code>sudo vim /etc/rc.conf</code> is what I meant 100% of the time? At best, these dead-end commands are taking up unnecessary space in my history file. At worst, they’ll trip me up <em>again</em> the next time I need to do the same thing and naively run the first command from my history that looks right.</p>
<hr>
<p>For myself, I don’t want my shell history to be opt-in. But I <em>can</em> make it easier to remove the typos and dead ends. I came up with this zsh function to facilitate that:</p>
<div id="shell-history_cb1"><pre><code><span id="shell-history_cb1-1"><span>function</span> <span>smite</span><span>()</span> <span>{</span></span>
<span id="shell-history_cb1-2">    <span>setopt</span> LOCAL_OPTIONS ERR_RETURN PIPE_FAIL</span>
<span id="shell-history_cb1-3"></span>
<span id="shell-history_cb1-4">    <span>local</span> <span>opts</span><span>=</span><span>(</span> -I <span>)</span></span>
<span id="shell-history_cb1-5">    <span>if</span> <span>[[</span> <span>$1</span> <span>==</span> <span>'-a'</span> <span>]];</span> <span>then</span></span>
<span id="shell-history_cb1-6">        <span>opts</span><span>=</span><span>()</span></span>
<span id="shell-history_cb1-7">    <span>elif</span> <span>[[</span> <span>-n</span> <span>$1</span> <span>]];</span> <span>then</span></span>
<span id="shell-history_cb1-8">        <span>print</span> <span>&gt;&amp;</span><span>2</span> <span>'usage: smite [-a]'</span></span>
<span id="shell-history_cb1-9">        <span>return</span> <span>1</span></span>
<span id="shell-history_cb1-10">    <span>fi</span></span>
<span id="shell-history_cb1-11"></span>
<span id="shell-history_cb1-12">    <span>fc</span> <span>-l</span> <span>-n</span> <span>$opts</span> 1 <span>|</span> <span>\</span></span>
<span id="shell-history_cb1-13">        <span>fzf</span> <span>--no-sort</span> <span>--tac</span> <span>--multi</span> <span>|</span> <span>\</span></span>
<span id="shell-history_cb1-14">        <span>while</span> <span>IFS</span><span>=</span><span>''</span> <span>read</span> <span>-r</span> <span>command_to_delete</span><span>;</span> <span>do</span></span>
<span id="shell-history_cb1-15">            <span>printf</span> <span>'Removing history entry "%s"\n'</span> <span>$command_to_delete</span></span>
<span id="shell-history_cb1-16">            <span>local</span> <span>HISTORY_IGNORE</span><span>=</span><span>"</span><span>${(</span><span>b</span><span>)command_to_delete}</span><span>"</span></span>
<span id="shell-history_cb1-17">            <span>fc</span> <span>-W</span></span>
<span id="shell-history_cb1-18">            <span>fc</span> <span>-p</span> <span>$HISTFILE</span> <span>$HISTSIZE</span> <span>$SAVEHIST</span></span>
<span id="shell-history_cb1-19">        <span>done</span></span>
<span id="shell-history_cb1-20"><span>}</span></span></code></pre></div>
<p>Running <code>smite</code> opens an <a href="https://github.com/junegunn/fzf">fzf</a>-powered browser with your shell history in it. (So you’ll need fzf installed, but that’s the only dependency.) By default, you’ll only see your history from the current session, but running <code>smite -a</code> shows all of your history.</p>
<p>When you navigate to an entry and press <kbd>Return</kbd> or <kbd>Enter</kbd>, zsh will delete all instances of that command from your history. If you want to delete multiple commands at once, you can select them by pressing <kbd>Tab</kbd> on each one and then typing <kbd>Return</kbd> to commit your changes.</p>
<p>Multiline commands are not handled correctly, for now.</p>
<p>The function prints back all of the commands it’s deleting, just in case you select the wrong one.</p>
<p>Here’s a video:</p>
<video controls="controls" playsinline="playsinline" width="702" height="528">
<source src="https://esham.io/media/2025/shell-history/smite.mp4" type="video/mp4">
</video>
<p>The approach is taken from <a href="https://stackoverflow.com/a/63494771/371228">this Stack Overflow answer</a> by Marlon Richert, who later wrote a zsh plugin called <a href="https://github.com/marlonrichert/zsh-hist">zsh-hist</a> to make the history system easier to deal with. (I haven’t tried the plugin, but it seems to make it possible to delete the history entry with a given number, which is the omission in zsh that prevents multi-line commands from being easy to delete.)</p>
<hr>
<p>Even if my zsh code isn’t relevant to you, I hope you’ve taken a moment to consider what you want out of your shell history and if there’s anything you could tweak to get closer to that.</p>
<p>Since adding this function to my zshrc, I’ve been paying more attention to which commands are misfires, and pruning the ones that are. If my history file is never quite going to be a beautiful garden, I can at least put more effort into removing the weeds.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Odyc.js – A tiny JavaScript library for narrative games (191 pts)]]></title>
            <link>https://odyc.dev</link>
            <guid>44200866</guid>
            <pubDate>Fri, 06 Jun 2025 13:46:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://odyc.dev">https://odyc.dev</a>, See on <a href="https://news.ycombinator.com/item?id=44200866">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p>Play to create Games</p> <p>A small javascript library that lets you code video games even without programming experience.</p> <div><p><a href="https://odyc.dev/playground/">Create a game</a> <a href="https://odyc.dev/doc/getting-started/intro">Learn</a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Small Programs and Languages (106 pts)]]></title>
            <link>https://ratfactor.com/cards/pl-small</link>
            <guid>44200797</guid>
            <pubDate>Fri, 06 Jun 2025 13:38:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ratfactor.com/cards/pl-small">https://ratfactor.com/cards/pl-small</a>, See on <a href="https://news.ycombinator.com/item?id=44200797">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        <header>
            
                
            
            
            
            
            
            
                <p>Page started: <span>2025-06-02</span></p>
            
            
                <p>Page published: <span>2025-06-04</span></p>
            
            
        </header>

        
        <p>My <a href="https://ratfactor.com/forth/implementing">Implementing a Forth</a> article got some
great feedback, particularly around the subject of tiny Forth implementations.
(And it was an excuse to list some of the tiniest Forths I’ve seen.)</p>
<p>I know I’m not alone in seeing the appeal of tiny Forths, tiny languages, tiny
programs, and just small stuff in general. What’s up with that, anyway?</p>
<div>
<h2 id="_the_appeal_of_tiny_programs">The appeal of tiny programs</h2>
<div>
<p>Tiny programs are approachable.</p>
<p>If I see that something is 200 lines of code, I’m <em>much</em> more likely to
read it than if it’s 2,000 or 20,000 lines.</p>
<p>What’s more, if it’s a program that does something interesting in 20 lines when
I was <em>expecting</em>  2,000, then I’m going to be very, very interested to read
those 20 lines!</p>
<p>When I was researching DOM diffing libraries
prior to creating <a href="https://ratfactor.com/retrov/">RetroV</a>, one of
the repos I looked at was <strong>ijk</strong> by Luke Jackson.</p>
<p>I was pretty used to looking at fat JavaScript
libraries at that point, so when I cracked open
the source file for ijk, I was really surprised.
Here’s the source: <a href="https://raw.githubusercontent.com/lukejacksonn/ijk/refs/heads/master/index.js">index.js</a> (githubusercontent.com).</p>
<p><strong>Just 25 lines!</strong> I was instantly intrigued.</p>
<p>More recently, one of the responses to my aforementioned Forth article was
Philippe Brochard with a 46 byte "Forth".</p>
<p><strong>46 bytes!</strong></p>
<p>It’s <em>so</em> small, I can just put the hexdump right here:</p>
<div>
<pre>50b8 8e00 31d8 e8ff 0017 003c 0575 00ea
5000 3c00 7401 eb02 e8ee 0005 0588 eb47
b8e6 0200 d231 14cd e480 7580 c3f4</pre>
</div>

<p>Sure, very few people jump at a chance to read raw machine code. But that’s
exactly my point.  Something <em>this</em> small doesn’t feel so scary, right? I mean,
46 bytes. If you put the effort into it, you <em>know</em> you can figure out how it
works eventually.</p>
<p>This next one almost serves as a counter-example because the code is so dense
and frightening, but I can’t help mentioning it anyway.</p>
<p>One of my favorite software stories is Roger K.W. Hui’s Appendix A "Incunabulum"
from <em>An Implementation of J</em> in which he describes Arthur Whitney writing
an array-based language interpreter,</p>
<div>
<blockquote>
<p>"…​on <strong>one page</strong> and in one afternoon — an interpreter fragment
on the AT&amp;T 3B1 computer. I studied this interpreter for about a week for its
organization and programming style…​"</p>
</blockquote>
</div>
<p>Hui can list the entire interpreter in the appendix because it’s just that
single page, <strong>122 lines</strong> of incredibly cryptic C, which you can view here:</p>

<p>I haven’t personally tackled that puzzle yet, but it’s in my unwritten list of
dense computer things that I would print out and take with me if I suddenly
found myself cast out of society and forced to go live alone up on a mountain
or something.</p>
<p>(By the way, J is in the family of array languages which include APL.
If you enjoy brevity, cryptic programming puzzles, and high performance
numerical computing, these languages have got you covered!)</p>
<p>Something else that would certainly need to be in the collection I take to that
lonely mountain would have to be <em>The Most Beautiful Program Ever Written</em>,
according to the title of a talk by William Byrd.  Spoiler alert: It’s this
Lisp interpreter in Lisp:</p>
<div>
<pre>(define eval-expr
  (lambda (expr env)
    (pmatch expr
      [`,x (guard (symbol? x))
        (env x)]
      [`(lambda (,x) ,body)
        (lambda (arg)
          (eval-expr body (lambda (y)
                            (if (eq? x y)
                                arg
                                (env y)))))]
      [`(,rator ,rand)
       ((eval-expr rator env)
        (eval-expr rand env))])))</pre>
</div>

</div>
</div>
<div>
<h2 id="_recreation_or_fundamental_truths">Recreation or fundamental truths?</h2>
<div>
<p>Now, we all know that
<a href="https://en.wikipedia.org/wiki/Code_golf">code golfing</a> (wikipedia.org) is
a programmer’s recreational game, and just for fun, right?</p>
<p>But is any of this actually useful?</p>
<p>Actually, I’ll go one step further and say that these examples are
more than just useful. They are <em>meaningful</em>.</p>
<p>When you see a
<a href="https://fabiensanglard.net/rayTracing_back_of_business_card/">ray tracer that fits on a business card</a> (fabiensanglard.net), it makes you realize that a ray tracer is
<em>something that can fit on a business card</em>!</p>
<p>That’s an actual <em>fact</em> about the minimum complexity of ray tracers.</p>

<div>
<blockquote>
<p>"…​the length of a shortest computer program (in a predetermined programming language) that produces the object as output."</p>
</blockquote>
</div>
<p>(I learned about Kolmogorov complexity in a book about Kurt Gödel and it was my
favorite thing in what was otherwise a pretty dry and depressing book.)</p>
<p>The meaning I get from this is that code golfers, for the benefit of the whole
human race, produce versions of programs that represent the minimum
<em>complexity</em> of those programs. Way to go code golfers!</p>
<p>What do we know about the fundamental complexity of the Lisp programming language?</p>

<p>That’s a <em>fact</em> about Lisps with garbage collection. When you strip everything
else away, you can get the essentials down to <strong>436 bytes!</strong></p>
<p>I take comfort in this because that means the concept can be boiled down to
this hand-held size. And <em>that</em> means I can probably fit it in my head.</p>
<p>Thanks to Frank Force, we know that a 3D browser game can fit
in 2Kb of (insanely compressed) JavaScript source:
<a href="https://frankforce.com/how-i-made-a-3d-game-in-only-2k-of-javascript/">A 3D game in 2Kb of JS</a> (frankforce.com)</p>

<p>And we haven’t even scraped the <em>surface</em> of what the demoscene has to offer
for computers going back to the 1980s.
Here’s a good place to start: <a href="https://demoscene.assembly.org/">https://demoscene.assembly.org/</a></p>
<p>Mini-things are fun <em>and</em> useful <em>and</em> meaningful.</p>
<p>My point here is that you don’t have to actually study the intricate
clockwork puzzle of a miniaturized/golfed program to benefit from it.  The
miniature simply serves to prove that the concept <em>can</em> fit on a business card
and it <em>can</em> fit in your head.  Go ahead and find a readable version to study
instead. (The miniature will likely have <em>many</em> things to teach, but some of
them will be about how to make miniature programs.)</p>
</div>
</div>
<div>
<h2 id="_small_languages">Small languages</h2>
<div>
<p>Small programs are interesting. But I’m also interested in small programming
languages.</p>
<p><em>Generally</em> speaking, the smaller the language, the less expressive it is.</p>
<p>One of the most well-known, least expressive languages is assembly.</p>
<p>Assembly languages are <em>very</em> syntactically and conceptually simple.  There’s
not much language at all: it’s mostly a rigid sequence of opcode mnemonics.
Even with macros, writing in assembly is mostly about understanding the puzzle
of a particular CPU’s instruction set architecture, or ISA. (You can prove this
by comparing the syntax of assembly for CISC and RISC processors. The puzzle is
totally different, while the language remains practically unchanged.)</p>
<p>An honorable mention must be made here for <a href="https://ratfactor.com/snobol/">SNOBOL</a>, which is
kind of like assembly language both for the rigid syntactic reasons listed above and
<em>also</em> because it presents a similar control flow challenge. Like a CPU, it
only understands jumps and call/return. (In every other way, SNOBOL is
wildly different, being a string matching and replacing language. It’s weird
and fun and highly effective.)</p>
<p>Moving on to syntactically tiny "high-level" languages, these three are
brilliant:</p>
<div>
<ul>
<li>
<p><strong>Forths</strong> win just about any "smallness" contest. The only syntactical construct
is the space character. Forths are mind-bendingly flexible and powerful in a way that is downright disturbing.</p>
</li>
<li>
<p><strong>Lisps</strong>, (especially Scheme?), have small core languages with very simple syntax and enormous expressive power and flexibility.</p>
</li>
<li>
<p><strong>Tcl</strong> deserves to be mentioned with Forth and Lisp. Tcl is wild because <em>everything</em> is a string in a way that is hard to appreciate at first. You can construct your own language in Tcl just as you can with Forth or Lisp.</p>
</li>
</ul>
</div>
<p>Forth, Lisp, and Tcl only require you to understand a few core ideas and give
you limitless power in return. You could fit the syntactic core of these
languages on an index card or less.</p>
<p>And yet, to wield these tools effectively requires a massive shift in thinking
if you’re used to "normal" Algol-like procedural languages.</p>
<p>So how about something that is both small and simple and doesn’t require a
blood sacrifice?</p>
<p>I think the Lua programming language fits the bill. Check out how small the
core language is:</p>

<p>I’ve got the printed book version of this document and it’s a thin book! The
Lua language is covered in the beginning and it is just 27 pages. <strong>27
pages!</strong> I have game manuals longer than that.</p>
<p>The C language is pretty small. I don’t think it’s super easy to program
effectively and safely with C, but there’s no denying it’s compact and
pretty expressive for its size.</p>
<p>Newcomer Zig is C-like. But I think it’s actually quite large and you need to
know a lot of it to use it effectively.  (I’m also a fan of Zig and think it’s
<a href="https://ratfactor.com/zig/hard">totally worth it</a>. The size is there for a reason.)</p>
<p>JavaScript has a core language that is quite small. Probably on par with Lua,
actually. Functions, arrays, and objects (used as dictionaries) are pretty much
all the JS I use and I like it that way!</p>
<p>As with the natural human languages, the trade-off with a bigger programming
language is, hopefully, greater expressivity.</p>
<p>Small or expressive. Which way should you lean?</p>
<p>In a wonderful talk by
<a href="https://en.wikipedia.org/wiki/David_Ungar">David Ungar</a> (wikipedia.org)
titled <em>Self and Self: Whys and Wherefores</em>, he laments the mistake
of Self’s multiple inheritance:</p>
<div>
<blockquote>
<p>"It’s far better to make a system a little too simple so a person has to write extra stuff, even write it twice with a comment that says, 'Look over here,' than to have a system that acts in strange ways and nobody can understand why. So, simplicity trumps expressiveness."</p>
</blockquote>
</div>
<p>Got that? Here’s the bullet from the slide to help you remember:</p>
<div>
<p><img src="https://ratfactor.com/cards/images/david_ungar_self_and_self2.jpg" alt="snippet of a slide that reads Simplicity trumps expressiveness">
</p>
</div>
<p>I think small languages can <em>also</em> be fairly expressive, but only with very,
very careful design. Recall Forth, Lisp, and Tcl.</p>
<p><strong>Microworlds:</strong> The idea of small, self-contained, learnable programming
environments <em>also</em> has a David Ungar connection I discovered while watching
his talk.  See "ARK" on my <a href="https://ratfactor.com/cards/microworlds">Microworlds</a> card. (Smith and Ungar
co-created the <a href="https://selflanguage.org/">Self language</a> (selflanguage.org),
which is a notable milestone in programming and still under active development
to this day.)</p>
<p><strong>Libraries:</strong>
Another easily overlooked aspect of a programming language’s apparent size is
its "standard library" of functions, data structures, and methods.</p>
<p>I think function libraries are subject to the exact same size versus
expressiveness challenge.</p>
<p>For example, learning the
<a href="https://ramdajs.com/">Ramda</a> (ramdajs.com)
functional programming library for JavaScript was very much like learning a new
programming language and I never managed to learn all of it. The composability
of Ramda and the consistency of its curried functions was extremely eye-opening
to me. <strong>A little bit of Ramda goes a long way!</strong></p>
<p><strong>The feels:</strong> Concision in our tools has a certain quality that goes beyond
utility. Little things just <em>spark joy</em>, you know?</p>
<p>Why are small things so delightful?</p>
</div>
</div>
<div>
<h2 id="_the_appeal_of_tiny_things_in_general">The appeal of tiny things in general</h2>
<div>
<p>I’ve always had a fascination with miniatures, dioramas, and tiny scale models.</p>
<p>I know I’m not alone!</p>
<p><strong>Stephanie M. Langin-Hooper</strong>, author of <em>Figurines in Hellenistic Babylonia</em>
writes,</p>
<div>
<blockquote>
<p>"…​the act of shrinking a life-size thing to a smaller scale has an influential effect. The object is now cute, more accessible, more delicate, more 'squee'. It is also more helpless, and thus controllable – comforting and completely non-threatening due to its diminutive size.</p>
</blockquote>
</div>
<p>Which is exactly the reaction I have when I see a tiny
bit of code that does something useful.</p>
<p>(Also, I’m super okay with "controllable" in the context of a computer, but the
feeling of that paragraph changes a bit if we’re talking about, say, dolls.)</p>
<div>
<blockquote>
<p>"What might be threatening to think about in the real-scale world can be more manageable and more conceivable in miniature."</p>
</blockquote>
</div>
<p>Small things, "allow low-stakes experimentation," which
is why we often build tiny models of things before we scale them up.</p>

<p><strong>Simon Garfield</strong>, author of <em>In Miniature: How Small Things Illuminate the World</em>
has many of the same answers. He writes,</p>
<div>
<blockquote>
<p>"The answer lies in our desire for mastery and elucidation. The ability to enhance a life by bringing scaled-down order and illumination to an otherwise chaotic world – a world over which we may otherwise feel we have little control – cannot be overvalued. […​] At its simplest, the miniature shows us how to see, learn and appreciate more with less."</p>
</blockquote>
</div>
<p>I think this is a <strong>big</strong> part of what I get out of
<a href="https://ratfactor.com/assembly-nights">Assembly Nights</a> with the small scale and orderly logic
and internal consistency. Unlike the vast, fathomless Real World, an offline
computer is a fantasy place where I can believe I could understand <em>everything</em>
if I just tried hard enough for long enough.</p>
<div>
<blockquote>
<p>"The satisfaction of observing small things becomes a desire to make small things, and both stages address the human need for comprehension and order. We live in a huge and doomy universe, and controlling just a tiny scaled-down part of it restores our sense of order and worth."</p>
</blockquote>
</div>
<p>I’ve been studying computer history for a little while and I’ve only
scratched the surface. But one of the things I’ve been finding is that
a lot of the "big" stuff in our field are just accumulations of lots of
"little" victories as individuals have solved "little" problems in new and
different ways.</p>
<div>
<blockquote>
<p>"We create small universes in which we may bury ourselves to the exclusion of all else. Blocking out real life for a while – always the prerequisite of the dedicated domestic hobby, from doll’s house modeller to jigsaw enthusiast to adult book colourist – may be contemplative, meditative, blinkered and essential. The people crouching over tiny details as if the world depended on it are only doing it because their world does depend on it."</p>
</blockquote>
</div>


</div>
</div>

        
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dystopian tales of that time when I sold out to Google (212 pts)]]></title>
            <link>https://wordsmith.social/elilla/deep-in-mordor-where-the-shadows-lie-dystopian-stories-of-my-time-as-a-googler</link>
            <guid>44200773</guid>
            <pubDate>Fri, 06 Jun 2025 13:36:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wordsmith.social/elilla/deep-in-mordor-where-the-shadows-lie-dystopian-stories-of-my-time-as-a-googler">https://wordsmith.social/elilla/deep-in-mordor-where-the-shadows-lie-dystopian-stories-of-my-time-as-a-googler</a>, See on <a href="https://news.ycombinator.com/item?id=44200773">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I will do something I normally never do here, and make my first ever blog post on the topic of, long sigh: tech. I’ve already talked about Google a number of times on Mastodon which is, blessedly, by design, not discoverable; but I’ve decided to commit the full story to print.  Hopefully this won't come back to bite me in the ass but eh it’s the apocalypse, who cares at this point. At least Wordsmith Dot Social is half-abandoned and has no comment system, so I won’t have to deal with techbros batting for billionaires or preaching the power of Open Source (™&nbsp;Open&nbsp;Source&nbsp;Initiativeⓡ).</p>

<p>But if you clicked this, Dear Reader, then you wanted the tea; and I am nothing if not forthcoming with tea-spilling. The fact that Google fired me with shut-up money only makes it more fun to do it. So go get some chamomile and sit comfortably, for this is an old woman reminiscing; let’s talk about capitalism and anarchism, about the precariat and surveillance, plush dolls and churrascarias and gay argots; let us go back in time and space, and journey to tropical Brazil in the distant time of 2007…</p>

<p><img src="https://files.transmom.love/google/camera.jpeg" alt="Black-white photo of a camera installed on an architectural detail like a geometric series of parallel slats. Taken 2008."></p>

<h2 id="1-treason">1. Treason</h2>

<p><img src="https://files.transmom.love/google/paranoia.jpeg" alt="Cover to the Brazilian edition of the pen-and-paper RPG called Paranoia, second edition from the nineties. In a comedic cartoon style, it shows a man in a red suit walking down a corridor that's heavily monitored, and about to be ambushed by an assortment of robots, agents, clones of himself, and random weapons coming out of the walls, including one cartoonish round bomb on one hand, being lit by a different hand coming from another hole of the futuristic wall."></p>

<p>2007 wasn’t a good year to start a new career, as it turned out.</p>

<p>Google back then prided itself on broadcasting its Best Place To Work award, won year after year after year.  Younger people will have trouble picturing this, but Google used to nurture an image of being the “good one” among megacorps; they championed open standards (except when they didn’t), supported open source projects (until they backstabbed them), and used language that corporate wasn’t supposed to use, like “don’t be evil” (until they, infamously and in a true dark comedy move, retracted that motto). The work environment was all colourful, nerdy cool, not a single necktie in sight—this was seen as brave and refreshing rather than cringe and tired, you see.  And they made a big deal out of something called “20% time”: Every engineer was promised 1/5 of their work time for themselves, to do anything they want. (Google owners will still own whatever you create during your 20% time, natürlich).  Famously, Gmail came out of someone exploring their interests during 20% time.</p>

<p>I don’t think much of anything else came out of it, though.</p>

<p>I found out I was always overworked on drudgery; my main job was to fix boring bugs on the Ruby on Rails internal user accounting system that someone else had developed.  When I complained that this was a far cry from the academia-like, exciting research environment I had been promised, and asked to be assigned to a more challenging project, I was told the following rationale against it: “no”.   Moreover the deadlines and expectations were such that even if I worked (unpaid) overtime every day, I was still was at risk of a performance review.  Making actual use of the “20% time” felt like a pipe dream.</p>

<p>And all that with wages well below even the local market in our crumbling Third World economy. With no exciting research positions nor self-managed time nor compensation, what was the advantage over a high-paying job at Microsoft or IBM? A bright blue vinyl floor and WarioWare Wii in the cafeteria? Well we were the hip tech vanguard, we were all geniuses, we were paid in prestige and promises and ego massages.  Perform good enough and you might be awarded a smattering of shares at some point, get some crumbs from the bountiful capitalists' table.</p>

<p>Like most employees I blamed myself for not working hard enough to get good compensation—or to have time to exercise my right of 20% free time… Until I saw in the “Googlegeist” statistics that some 95% of employees never use their “20% time” at all, being trapped under the same pressures as I was.</p>

<p>I started a discussion about how the recruiter's promise of “20% free time” could maybe be this little thing that the forgotten priestesses of ancient Samarkand called a “lie”.  There was an internal Blogger system, only available for other employees and bosses; and I wrote a post arguing that if no one feels able to use their 20% time, then it’s not much of a perk, is it.</p>

<p>The result of this was my boss having a fit over me “backstabbing” him. See, me complaining about the unfulfilled recruiter promises marked me as an Unhappy Googler. And Google, if you remember, was the Best Place To Work.  It was very important that every promising young engineer thought of Google as the dream job where everyone is happy.  Unhappiness isn't allowed.  My manager was severely scolded by <em>his</em> manager for having <em>dissatisfaction</em> (gasp) within his team.</p>

<p>I said, “But the issue is real and not my fault, don’t you agree? I just used the data to bring it to attention.  Didn't you say we operate under 'radical transparency'?” (I was young and believed in this kind of slogan. Yes, I was a sitting duck and didn’t stand a chance.)</p>

<p>Boss replied,</p>

<blockquote><p>Radical transparency doesn't mean you get to say negative things.</p></blockquote>

<p>Exact quote, I remember every word in that backroom in Phoenix, AZ.</p>

<hr>

<p>Ever heard of the 1984 dark comedy RPG «Paranoia»? It takes place in a dystopian future society called the Alpha Complex. The Complex is ruled by the Computer, which is perfect and makes no mistakes, guaranteeing the best possible life for all humans.  If someone commits a crime, for example, the Computer will always know—every wall has cameras—and instantly disintegrate the offender—every wall is packed with death lasers.</p>

<p>Of course, if you suggested that this ever happens, you would be implying that the Computer can raise a criminal human. It would logically follow that you’re holding the Computer to be less than perfect.  That accusation is a crime of treason.  Treason is punished with death.  Nobody ever complains about life in the Alpha Complex, which goes to show how perfect the Computer is.  In fact, everyone in the Alpha Complex is perfectly happy with the Computer's rule.  To feel unhappy is equivalent to accusing the Computer of making mistakes, and it therefore constitutes treason.  Happiness is mandatory.  Are you happy, citizen?</p>

<h2 id="2-the-google-precariat-part-i-dictbot">2. The Google Precariat, Part I: <code>dictbot</code></h2>

<p><img src="https://files.transmom.love/google/balaclava.jpeg" alt="Photo of the anti-racism protests in reaction to the murder of the Black teenager Nahel in France. While most people are dressed in normal riot gear, with black hoodies and COVID masks, the photo focus on a protester who is wearing a red net over their had that covers their entire face, eyes and nose and all.  Underneath they wear a striped yellow-white T-shirt and large, hip-hop style chains.">
<em>(Photo by Stephane DUPRAT / Hans Lucas.)</em></p>

<p>When you joined Google, you were quickly overwhelmed by massive amounts of corporate jargon—a hundred opaque project names, TLAs for everything etc.  To help new Googlers settle in, the Intranet had an online glossary.</p>

<p>Now in the spirit of “20% time”, we were encouraged to tinker with pet projects, or so they told me.  And we used to hang out in IRC chatrooms back then.  So I made a little IRC bot that would fetch definitions from the glossary.  Very basic stuff, if someone said “wtf is Chrome” in the channel, the bot would dump the summary paragraph, “Project Chrome is an initiative to develop a Google web browser, based on KHTML…”</p>

<p>I then got scolded for it, because I was leaking private information into a space that could be accessed by “temps, part-timers, and contractors”—Google's sprawling precariat (put a pin on that, more on that later).  As I alluded to, we Googlers were pampered with prestige; but the “temps, part-timers and contractors”—no fun name for them, they were always called “temps, part-timers, and contractors”—were second-class in Google Nation, had to be constantly put in their place in a myriad ways.  How else would the Engineers feel like geniuses, if there wasn’t a “normie” class to be treated worse than them?</p>

<p>One of the barbed-wire fences around temps, part-timers and contractors is that they should not have access to inside info, e.g. what is Project Chrome. My bot was, allegedly, violating that norm.  I pointed out that all that my bot did was to fetch info from the glossary page, and that anyone with access to the IRC channels already had access to the glossary page.</p>

<p>Dear Reader, this is how I became responsible for provoking the Computer into fencing away the glossary website from temps, part-timers, and contractors.</p>

<h2 id="3-a-lament-from-project-android">3. A lament from Project Android</h2>

<p><img src="https://files.transmom.love/google/nightingale.jpeg" alt="A colourful illustration of a clockwork bird, made of gold and jewels, singing. From a Russian edition of Andersen's &quot;Nightingale&quot; story.">
<em>Illustration: Джона Пейшенса, ISBN:  5-232-00383-6.</em></p>

<p>The Reader might well imagine how I had become persona non grata to my boss after the “backstabbing” episode.  When I wrote that blog post, I had gotten a number of emails from employees thanking me for talking about it, saying they’re glad someone is finally taking a stand,  praising me for my bravery.</p>

<p>Now my posture back then will feel very natural for those of you who only met me post-transition, and knew me from the start as this like, badass nazi-punching antifa thug with no filter and no sense of consequences.  But you have to understand: back then I was a shy little nerd terrified of everything. I wasn't brave; I was incredibly, <em>magnificently</em> naïve.  I was maybe the only person in the world who believed Google’s corporate kool-aid; I bit it hook, line and sinker, I really did believe we were some sort of new, dynamic academia, we didn't work in offices we worked in “campi”, the company was a way to fund exciting new research and we were there to improve the world by organising its information.  At least I thought I was.</p>

<blockquote><p>Interviewer: What attracted you to Google?
me: I agree with the Ten Principles of the company.
Interviewer: The what now?
Me: The Ten Principles? Google's Principles? In the 'about' page?
Interviewer: Uuh, sure…</p></blockquote>

<p>It did not even <em>occur</em> to me that it was all a scam, that everyone else knew it was all a scam and the actual point was to get rich.  In retrospect I should have read the undertones in early Paul Graham essays; I was a literary girl, I'm good at undertones; but I only read what I wanted to be true.</p>

<hr>

<p>Not long after my post in the Intranet Blogger, there was a post by some engineer I didn't know; a core programmer from the secret Project Android.  Out of the three big ones I had to stay quiet about, Project Android and Project Chrome got finished and became highly successful—only to immediately turn into world-wrecking monstrosities that we, low-level grunts, would never have imagined.  The third project, a physical-layer broadcast technology for the Internet—Youtube with HD quality if you logged in at showtime—never went forward.</p>

<p>But this insider, they were venting about how disappointed they had become with the directions that Project Android was taking.  They were losing their motivation, this is not what they thought the “Linux phone” would be about, this wasn’t what they signed up for.  The blogger was silent on any tech details, or what exactly was so disappointing; but with the benefit of hindsight it's easy to imagine.</p>

<p>A few days later, the same person posted something like “haha disregard that, I was having personal mental health issues and wrote a ill-conceived rant but it's all my fault really, of course there's a always few bumps but Project Android is amazing actually!! Y'all are going to love this, it's going to change everything!! We're organising the world's information and making a difference”, etc. etc.</p>

<p>Like, conspicuously back-to-back, the two posts.</p>

<p>I’m an airheaded bimbo but at some point the lesson will penetrate even my smooth silly brain.  This time, I was observant.</p>

<h2 id="4-mona-entendida-odara-elza">4. Mona, entendida, odara… 🤔 elza</h2>

<p><img src="https://files.transmom.love/google/travestis2.jpeg" alt="Police photo of a group of Brazilian travestis—a local transfeminine culture—detained in skimpy clothing, their photos blurred."></p>

<p>I wasn't out as trans yet, but I was already proud to be queer.  Showed up first day with neon orange hair, unicorn T-shirt, the works.  That made of me a Gaygler™, and Google Belo Horizonte was always happy to have me on team photos to add some colour and progressiveness to the image.</p>

<p>Now even though Google is fundamentally a spyware advertising company (some 80% of its revenue is advertising; the proportion was even higher back then), we Engineers were kept carefully away from that reality, as much as meat eaters are kept away from videos of the meat industry: don't think about it, just enjoy your steak.  If you think about it it will stop being enjoyable, so we just churned along, pretending to work for an engineering company rather than for a giant machine with the sole goal of manipulating people into buying cruft.  The ads and business teams were on different floors, and we never talked to them.</p>

<p>One day one of the AdSense people asked me for a little meeting.  They sat right by my desk, all sleek and confident, and said that they had heard I was a Gaygler™ and were wondering if I could help with one of their clients.  “Can you tell me some words that the Brazilian gay community uses? like slang, popular media you like, names of parties, that kind of thing?”</p>

<p>Caught off-guard and unsure how to react, I struggled to think of gay-coded speech, and I was expertly mined for pajubá terms to be fed into the machine.  Whole interaction took maybe ten minutes.  The AdSense goon left, never to be seen again, leaving me feeling violated in ways I couldn't articulate.</p>

<p>Google supported its queer employees.</p>

<hr>

<p>After I got marked as a troublemaker and put into the inevitable performance review, one of the items raised against me was that my company profile page was 'too personal'.  the extent of personal information in my profile was this sentence: “I am a nerd, a bisexual polyamorist, and a parent.”¹</p>

<h2 id="5-the-google-precariat-part-ii-a-water-purifier-s-salary">5. The Google Precariat, Part II: A water purifier’s salary</h2>

<p><img src="https://files.transmom.love/google/2klassenpunsch_karl_berge.jpeg" alt="Cartoon of a homeless person being rejected at the communal kitchen for lacking documents.">
<em>“Sure, we give aid to the poor! We’ll only need your registration forms, bank statement, and certificate of good conduct!” Cartoon by Karl Berger for Augustin.</em></p>

<p>You might have noticed, Dear Reader, that I have made somewhat contradictory claims: 1) that we Engineers were pampered, and 2) that we Engineers were underpaid, pressured to do unpaid overtime at salaries low even for the Brazilian market.  Such was the carrot and the stick.  We all were told that if we performed just a bit better we would get higher pay, shares, positions at cool projects, and the biggest carrot of all: a relocation to the magical Global North where human rights are real.  A way through the wall.</p>

<p>We trudged on, with little more than promises and hope.  But we trudged on with <em>style.</em>  The offices were all gaudy in Google colours with vinyl flooring, full of fridges with free snacks; the break room had the latest Playstation with brand-new high-tech Rock Band controllers; when you joined in you got a small bonus to buy toys for your desk (most Engineers got legos, I got a pink Kirby plushie I would dress up).  This was unheard of; companies at the time were all Microsoft, all performative professionalism, Google was fun! Google gave you Perks, gods, so so many Perks. the Lumon motivation baubles from “Severance” gave me heavy Google flashbacks.  We were periodically treated to dinner with the managers at the most expensive churrascarias.  Master let us eat right there with him, inside the big house.</p>

<p>I will be honest and say that most of my fellow programmers ate that shit up, we had all been gold-star kids and here was the hottest company in the world constantly massaging our egos, telling us we were better than everyone for being geniuses. I would have loved to feel the same, I <em>tried</em> to feel the same, but I came from poverty and I could not stop noticing the precariat: temps, part-timers, and contractors, an entire layer of the company who did the brunt of work without being Googlers.  No toy budget for kitchen staff.</p>

<p>It's the little things that bugged me, how people would eat the free candy or have a bowl of cereal and just leave trash and dirty dishes everywhere for the cleaning ladies (contractors) to deal with; more than that the way nobody looked at them or said “thank you”.  We Brazilians have a social class for that, a social code underlying that studied invisibility, I knew what this was: these were maids.²  Servants.  The women in my family, my friends at school.  The “campus” was pretty open and my then-wife visited it a few times; it creeped the&nbsp;Fuck out of her, the distinction between people and non-people.</p>

<hr>

<p>We had those expensive, high-tech water purifiers, several on each floor. One day there was a discussion on the topic of cost savings, and I suggested the traditional Brazilian solution—the well-known ceramic filters in terracotta jars; they're consistently rated among the safest, need no electricity, make the water cool even in summer without spending any energy, cost little and last a long time before you need to replace the charcoal element, which is anyway inexpensive.  The idea was dismissed out of hand.  Too low-tech, I suppose.³</p>

<p>The fancy water purifiers weren't owned by Google; they were leased, at a high cost.  Somehow it bothered me a lot that each of those excessively technological water monsters got more money per month than any temp, part-timer, or contractor.</p>

<p>The water purifiers were never fired.</p>

<h2 id="6-cathy-don-t-send-that-email-today">6. Cathy don't send that email today</h2>

<p><img src="https://files.transmom.love/google/cathy.jpeg" alt="A still from the music video for &quot;Cathy don't go to the supermarket today&quot; (1985), by the extremely abusive sex cult Family International. The song is about how paying with barcodes and cards was an implementation of the Mark of the Beast and will cost your soul.  In the still, a large, creepy, Terminator-like enforcer in a trenchcoat is stopping a woman from paying groceries in cash. The number &quot;666&quot; is partly visible on the walls."></p>

<p>Google was my first taste of smartphones, back when that meant a Blackberry (delightful, sturdy little corporate toys with pleasantly clicky, full-QWERTY thumb-keyboards). Mobile data plans were prohibitively expensive for anyone on wage labour, but I was graciously allowed to use my company phone for private purposes; and I delighted in the novelty of not getting lost for once, walking up and down the hills of Belo Horizonte with futuristic, always-on Google Maps under, whoa, unlimited data.</p>

<p>Which is to say, Google was my first taste of the surveillance society that has now become the new normal.</p>

<p>The Reader will remember our big carrot; all of us at Google Brazil worked hard to get the job because it meant a ticket to the Global North (potentially). Now I had been a weeb from an early age, and back then I was already like, intermediate to advanced in Japanese.  So of course my dream was to move to Japan.  But when I talked about it with my boss—a disembodied face from Phoenix to whom I would report under a giant monitor; this too felt very new, very high-tech, and very dystopic at the time—he dismissed the idea out of hand, saying my Japanese wasn't fluent and that this would make me a poor fit.</p>

<p>I talked to my colleagues about it and someone said, wtf girl no, most international engineers brought to Shibuya cannot even say konnichiwa, if anything your language ability and cultural experience with the diaspora make you the ideal candidate.  We had a relevant contact in Google Sweden, and my mates said I should talk to them about contacting Shibuya directly regarding relocation.</p>

<p>And there I was after putting a target on my back as a troublemaker, about to directly contradict my boss and look for a way into Japan behind his back.  My colleagues <em>sternly</em> advised me to <em>never</em> mention any of this by email, and also not call from my desk. “You really think they would do that? Just go on my email inbox and breach privacy? :O” International calls were very expensive those days and I didn’t have a landline, so I ended up calling Sweden from a company line inside a little cleaning closet, between brooms and bottles of disinfectant, in the dark, after everyone was gone from the office.  Sorry, “campus”.</p>

<p>The Sweden contact told me they knew people in Tōkyō and were sure they would be happy to have me.  A couple weeks after that, I was fired.  (Mid-economic crisis, in the 3rd world, with one 2-year-old kid and another about to be born.)</p>

<p>And it was <em>so</em> weird and surreal to be in that little locker room, afraid of every whisper, aware that every communication was being spied on.  And when I tell this story to my now adult children, I struggle to convey how weird it was.  I realised belatedly that they never <em>experienced</em> existing with technology without it being the default expectation that it's hostile to you and it's spying on you all the time.  For them this has been the case <em>all of their lives</em>.</p>

<p>Today, the concept of “spyware” has been obsoleted because every software is spyware.  Google's “organising the information of the world” turned out to be indexing which Gaza families to bomb, children and all; “making money in the free market to invest in social change” was about bankrolling literal, textbook fascism.  Today, for us Latinx to even briefly step in the USA, if we don't have an always-on handheld device with spyware “social media”, its absence is taken as proof of criminality.  I will never visit Arizona again, and my kids will never know a world that's not like this; but for me I saw this world being forged up close and personal, deep in Mordor where the shadows lie.</p>

<h2 id="7-the-google-precariat-part-iii-without-a-heart-to-guide-them-the-other-powers-are-useless">7. The Google Precariat, Part III: Without a Heart to Guide them, the Other Powers are Useless</h2>

<p><img src="https://files.transmom.love/google/captain_planet.jpeg" alt="Fanart of the Eco-Villains from children's cartoon Captain Planet, coloured by hand. Villains include Hoggish Greedly, a rich man in a short mohawk and green suit; Looten PLunder, a sleazy-looking hunter in noveau-rich furs; Sly Sludge, looking less like an oil magnate and more like an operative in working overalls; Duke Nukem, a radioactive monster who looks like a stony yellow humanoid; Dr. Blight, a sexy mad scientist grinning evily, and her AI, MAL, shown as a digital green face; and Verminous Skumm, a rat-human hybrid.">
<em><a href="https://www.deviantart.com/vultureclaw/art/Captain-Planet-Eco-Villains-303582092" rel="nofollow">Art: Vultureclaw</a></em></p>

<p>I was always an anarchist, abstractly, but in many ways Google was my political awakening.</p>

<p>We had an office party every Friday evening.  Every single Friday.  It was called TGIF, “thanks God it's Friday”, and involved fancy finger food, drinks, and more of those dystopic heads on monitors talking to us of all the great things Google was doing to revolutionise the world.  Thanks to TGIFs, we all could leave work early on Friday afternoons.</p>

<p>I was such a sucker for things like this, I was so entranced by the food variety and the socialisation and the festive atmosphere, that it took me a long time to think of Bretch's question (“All those feasts—who did the dishes?”).⁴  Belatedly I realised that none of the dishwashers would think of Friday afternoons like, “graças a Deus é sexta-feira”.  My privilege of working less and partying every week was paid by them staying <em>late</em> every Friday, dealing with the aftermath of our juvenile entitlement.  Most of these women will never step inside a Fogo de Chão restaurant in their lives; while we were taken on fancy dinners at a whim by the bosses, when they wanted to reassure us of our specialness.</p>

<hr>

<p>One day, shortly before I was fired, the 2008 crisis had hit full force, the Phoenix office that managed us got shut, and Google had fired 70% of the South American precariat, in one fell swoop.  Then, during one of my last TGIFs, I accidentally listened to two high-level managers talking about it, two white male gringos in expensive business-casual. They were commenting on how the company was still doing perfectly fine without all that weight.</p>

<p>And that's not what stuck with me, the arguments, no.  I understood the incentives to do layoffs, and the human need to rationalise them.  What stuck with me was their happy smiling faces.  Their <em>laughing</em>.</p>

<p>Yes they laughed about it.  Out loud.</p>

<p>I had full awareness of what it meant for Third World people to be fired under the crisis, what it was about to be like for the Argentinians, for our families—but <em>so did they</em>, they were down here, they knew the reality.  They talked to us every day, they had their spreadsheets handled by temps and were now here eating food prepared by contractors.  Yet here they were, in tailored clothes that cost more than a cafeteria lady's living expenses, partying happily without even bothering to <em>pretend</em> to be sad about all those families.  Not caring enough about us to <em>even bullshit.</em></p>

<p>Any sympathies I might have had about the simplistic logic of free-market liberalism evaporated under that laugh.</p>

<p>As a little girl I used to despise cartoons like Captain Planet, whose devilish, paper-thin villains destroyed the world with manic laughs for nothing but the thrill of power, polluting for the sake of polluting.  I thought that was deeply unrealistic, and condescending too; I felt talked down to.  I cherished nuanced villains like Lady Eboshi from Mononoke-hime, the leader of Irontown who was destroying the ancient forest—but with the goal of liberating women from violent patriarchy and poverty; Irontown was a refuge for outcasts, its mining economy a ticket out of male domination, and Lady Eboshi would give her own life for her girls.  Complexity! Humanity!</p>

<p>It was at Google that I learned that no, capitalists are actually literally the same as Captain Planet villains.  We are not blessed enough to live in Ghibli reality, capital owners built us a 90s trashy USA cartoon reality. What is crypto mining if not a textbook Captain Planet villain scheme—to kill and raze and destroy for nothing but imaginary tokens proving that you did lots of killing and razing and destroying? What is GenAI if not stealing energy and water and even art itself, only to syphon it all into a grinder, producing no benefit but the hoarding of even more money away from the poors—when you already have more money than a human being could possibly ever spend? What is this all-encompassing addiction to “number go up” if not Sly Sludge, dripping happily with pollutants, going “Aloha suckers! I'll miss this profit paradise but I have a souvenir to remember it by”, as he picks a briefcase full of money and leaves the island to explode?</p>

<hr>

<p>My experience at Google drove me to want to understand capitalism, and I would eventually find in Malatesta the answer as to why capital owners cannot help but be cruel, revel in cruelty, performatively broadcast cruelty; why the cruelty is indeed almost a side effect, a corollary to what it means to <em>do</em> capitalism.  A mould that grows inevitable on material that’s inherently rotten. Every action you take has consequences not just for the world but for your psyche; you cannot avoid being affected by your decisions, anymore than you can avoid the third law of motion when you punch a wall.  You cannot make people work for you and hoard all the profits while they are stuck with fixed salaries, without in the process developing strong feelings on why you're entitled to do that and how they deserve it actually.</p>

<p>But before I got into political theory, it was Google who <em>demonstrated</em> to me what is capitalism, firsthand up close. I wouldn't say that this was worth working there, but I benefited from the lived experience; from that part of it, and nothing else.</p>


<ol><li><p>That was me in egg state beating around the bush; I am now fully out as a jock, a lesbian relationship-anarchist, and a mother.  I added this footnote so that men stop hitting on me because I wrote the b-word once in this text about capitalism.</p></li>

<li><p>Anyone interested in the Latina “maid” as a social class is encouraged to watch <em>Que horas ela volta? (2015)</em> (English title: <em>The Second Mother</em>). It’s an engaging and heartwrenching film but keep in mind: everything it portrays about the social othering of maids is factually true, and happening today.</p></li>

<li><p>If this kind of thing appeals to you <em>and</em> you haven’t heard of it yet, I am pleased to introduce you to <a href="https://solar.lowtechmagazine.com/" rel="nofollow">the low tech magazine</a>.</p></li>

<li></li></ol>

<blockquote><p>Who built Thebes of the Seven Gates?
All articles name the names of kings; I gather the kings
brought those boulders on their royal backs?
And great Babylon, who fell and fell again,
Who put'er back together, every time? In which flats
of gold-paved Lima lived the road-pavers?
The night the Great Wall of China was finished, where did
the construction crew hang out? Awesome Rome
is full of triumphal archs.  Who arched them up? Also—
who did the Caesars triumph over? We sing the palaces
    of Byzantium—
the whole thing was just palaces? Even Atlantis of tall tales
shouted, choking, as the seas swallowed it whole—</p>

<p>for its slaves.</p>

<p>Young Alexander conquered India.
All by himself then?
Caesar defeated the Gauls.
Did he bring along a cook at least?
Felipe de España, el Prudente, cried when his Armada
sunk into the sea.  And nobody else cried that day?
In the Seven Years' War, Federico Secondo grasped
victory.  Who else grasped it with him?</p>

<p>All these pages, all these conquests.
All those feasts—who did the dishes?
Every ten years a new Great Man.
Who covered the budget?</p>

<p>So many headlines.
So many questions.</p></blockquote>

<p><img src="https://files.transmom.love/google/zapatista.jpeg" alt="Art from the Zapatista revolutionaries. It shows a crowd of women and children in colourful indigenous clothes, all wearing red bandanas or black balaclavas, armed wtih sticks—children inclusive—and staring at the viewer between curly yellow-green leaves."></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Being fat is a trap (147 pts)]]></title>
            <link>https://federicopereiro.com/fat-trap/</link>
            <guid>44200199</guid>
            <pubDate>Fri, 06 Jun 2025 12:32:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://federicopereiro.com/fat-trap/">https://federicopereiro.com/fat-trap/</a>, See on <a href="https://news.ycombinator.com/item?id=44200199">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

			
<p>This has been a difficult article to write. Partly because this is a painful topic for many, and partly because I have to be completely open and vulnerable to write about it. I only write this in the hope it can help others in similar situations. Nothing in this article is meant to judge you. If someone judges you for being or feeling fat, they should go f…figure out themselves. Judging people inside the fat trap just intensifies their misery and reduces the odds they can get out of it.</p>



<p>Let’s start with my story. I have been overweight (occasionally bordering on obese) from about age 8 to 23. At 23 I found out I was pre-diabetic and went on a strict three month diet that put me back on a healthy weight. I’ve maintained a healthy weight since then till the present (I’m 40 now). However, I’ve spent most of that time focused (sometimes obsessed) with not becoming fat again and generally struggling with food. It is only in the past couple of years that I am finally starting to trust myself with food and giving up the feelings of worry and insecurity about how my body looks and feels.</p>



<p>The notion of two traps, a physical trap and a mental trap, comes from Allen Carr’s fantastic <a href="https://en.wikipedia.org/wiki/The_Easy_Way_to_Stop_Smoking">Easy Way To Stop Smoking</a>. He states, quite convincingly, that there is a physical aspect to smoking (nicotine addiction) and a mental aspect to smoking (feeling that you need it).</p>



<p>I believe that the fat trap is also physical and mental. Let’s start with the physical, which is the easiest to understand. There is a range of body fat percentage that is healthy. If you are outside (probably above) this percentage, you are damaging your health. The science is unequivocal on this. What the range is is a bit harder to ascertain, but, from what I’ve read, it is about 10-20% for men and 15-25% for women.</p>



<p>The physical trap of being fat is being physically laden with just too much fat. Besides the overall damage to your health, it is less fun to be in an overweight body: you have less energy; you have more aches; it is harder to move, your sleep is worse. As painful and sad this is, there is no denying it.</p>



<p>Now, I believe the body positivity movement is a great step forward. Body positivity is about accepting others’ bodies, as well as your own, without regard to size, shape and gender. For those inside the fat trap, this brings tremendous relief. Being judged for being fat, or for being obsessed about fat, is almost always extremely counterproductive. It is harmful. If you are a non-fat person that goes around judging fat people, it might astonish you to find out that <strong>most fat people are painfully and constantly aware that they are fat</strong>, as well as the fact that that’s bad for them and they should make a change. There is nothing to be gained and everything to lose by judging someone for being fat. Eating disorders have extremely high mortality rates among psychiatric disorders: <em>if you’re making someone feel bad about their weight, you may well be increasing the likelihood they will die sooner.</em></p>



<p>And while self-acceptance is essential to get out of the mental fat trap, I don’t think you should accept that you are going to be overweight or even obese for the rest of your life. I might be wrong, but I believe you have a choice. It might be a very difficult choice, and it may take years, but you have the choice to step out of the fat trap. Both of them.</p>



<p>Getting out of the physical fat trap, strictly speaking, is simple:</p>



<ul>
<li>Develop basic sleep habits so that you get decent sleep.</li>



<li>Exercise daily or almost daily.</li>



<li>Walk 7500 steps a day.</li>



<li>Reduce refined carbs, unhealthy fats and alcohol from your diet. Focus on getting enough vegetables, fruits, complex carbs and healthy fats.</li>



<li>In some cases, you’ll also need medical advice, particularly if you have hormonal issues.</li>
</ul>



<p>If you stick to the above, you’ll be out of the physical fat trap in a few months — if you are morbidly obese, it might take a couple of years. But it doesn’t really matter how long it will take. What matters is that you start the journey.</p>



<p>Simple, however, is not easy. And what really makes it hard to stick with a healthy lifestyle is the mental fat trap.</p>



<p>The mental fat trap is the most difficult one to escape. For me, it manifests in the following thoughts:</p>



<ul>
<li>I want food but I can’t have it.</li>



<li>If I have what I want, I will be fat again.</li>



<li>I cannot trust myself with food.</li>



<li>I don’t look good enough to feel good about myself.</li>



<li>When I manage to achieve X target weight, <em>then</em> I will feel good.</li>



<li>Counting the days or weeks until you reach “your goal”.</li>



<li>I’m too old not to be fat.</li>



<li>I don’t have the genes not to be fat.</li>



<li>I don’t have the genes to eat healthily.</li>



<li>Let’s read about this diet.</li>
</ul>



<p>These thoughts are the walls of the mental fat trap. It is an obsession with the problem of being or feeling fat, even if you’re objectively not overweight. A typical pattern (at least for me) is to fixate on “when will I get there”. The answer is: you are there now. It is only through daily acceptance and work that you can be free. This point comes straight from Carr’s insight: that the mental trap is a waiting-for-nothing. When you focus on the present, the trap dissolves.</p>



<p>If getting out of the physical trap is to switch to a healthy lifestyle and stick with it, getting out of the mental trap is to leave behind the thoughts that keep you obsessed with food and dieting. Getting out is learning to identify as someone that no longer has issues with food, and that is healing every day from the wounds of being in the fat trap.</p>



<p>How to get out of the mental fat trap? I know three ways only.</p>



<ul>
<li>Therapy.</li>



<li>Meditation.</li>



<li>Read books on the subject.</li>
</ul>



<p>Therapy is going to reveal painful, uncomfortable truths that you’ve been avoiding — or rather, confronting indirectly through your obsession with food. Therapy can also give you tools to replace your harmful thoughts with better alternatives.</p>



<p>Meditation will give you space to detach from your thoughts, so that you can identify them and be able to start letting some of them go.</p>



<p>Reading is probably the easiest starting point. I can highly recommend <a href="https://federicopereiro.com/notes-roth-breaking-free/">Geneen Roth’s Breaking Free from Emotional Eating</a>. This book taught me about how some of us use food as a way to comfort ourselves, numb ourselves, and generally avoid or withstand painful aspects of our lives. Another great recommendation is <a href="https://www.amazon.com/Let-Go-story-about-weight/dp/9081958437">Andrew Dasselaar’s Let Go</a>. Both books are based on harrowing personal trials by the authors. Geneen has been helping people out of the fat trap for decades.</p>



<p>I’m still not out of the mental fat trap, at least not yet. I’m mostly out, and very grateful to have made it this far, but I still worry about food every other day or so. For me, getting out of this trap is a process of letting go of those thoughts that make food the main problem of my life, as well as the go-to solution for any uncomfortable feelings that arise.</p>



<p>I hope to have helped you, and not hurt you. If you think any of this is wrong or counterproductive, please let me know (fpereiro@gmail.com) and I will carefully consider your opinion.</p>

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Doge Developed Error-Prone AI Tool to "Munch" Veterans Affairs Contracts (139 pts)]]></title>
            <link>https://www.propublica.org/article/trump-doge-veterans-affairs-ai-contracts-health-care</link>
            <guid>44199887</guid>
            <pubDate>Fri, 06 Jun 2025 11:49:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.propublica.org/article/trump-doge-veterans-affairs-ai-contracts-health-care">https://www.propublica.org/article/trump-doge-veterans-affairs-ai-contracts-health-care</a>, See on <a href="https://news.ycombinator.com/item?id=44199887">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-pp-location="article body">

        
                    <div data-pp-location="top-note">
                

                                                
            <p>ProPublica is a nonprofit newsroom that investigates abuses of power. Sign up to receive <a href="https://www.propublica.org/newsletters/the-big-story?source=www.propublica.org&amp;placement=top-note&amp;region=national">our biggest stories</a> as soon as they’re published.</p>

                

            </div><!-- end .article-body__top-notes -->
        
        
        




                    

<figure data-pp-id="1" data-pp-blocktype="embed">

    


                        
            
    
<figcaption>
    
    
    
    </figcaption>


</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="2.0">As the Trump administration prepared to cancel contracts at the Department of Veteran Affairs this year, officials turned to a software engineer with no health care or government experience to guide them.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="3.0">The engineer, working for the Department of Government Efficiency, quickly built an artificial intelligence tool to identify which services from private companies were not essential. He labeled those contracts “MUNCHABLE.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="4.0">The code, using outdated and inexpensive AI models, produced results with glaring mistakes. For instance, it hallucinated the size of contracts, frequently misreading them and inflating their value. It concluded more than a thousand were each worth $34 million, when in fact some were for as little as $35,000.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="6.0">The DOGE AI tool flagged more than 2,000 contracts for “munching.” It’s unclear how many have been or are on track to be canceled — the Trump administration’s decisions on VA contracts have largely been a black box. The VA uses contractors for many reasons, including to support hospitals, research and other services aimed at caring for ailing veterans.</p>
        
    
                        
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="8.0">VA officials have said they’ve killed nearly 600 contracts overall. Congressional Democrats have been pressing VA leaders for specific details of what’s been canceled without success.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="9.0">We identified at least two dozen on the DOGE list that have been canceled so far. Among the canceled contracts was one to maintain a gene sequencing device used to develop better cancer treatments. Another was for blood sample analysis in support of a VA research project. Another was to provide additional tools to measure and improve the care nurses provide.</p>
        
    
                    
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="11.0">ProPublica obtained the code and the contracts it flagged from a source and shared them with a half dozen AI and procurement experts. All said the script was flawed. Many criticized the concept of using AI to guide budgetary cuts at the VA, with one calling it “deeply problematic.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="12.0">Cary Coglianese, professor of law and of political science at the University of Pennsylvania who studies the governmental use and regulation of artificial intelligence, said he was troubled by the use of these general-purpose large language models, or LLMs. “I don’t think off-the-shelf LLMs have a great deal of reliability for something as complex and involved as this,” he said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="13.0">Sahil Lavingia, the programmer enlisted by DOGE, which was then run by Elon Musk, acknowledged flaws in the code.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="14.0">“I think that mistakes were made,” said Lavingia, who worked at DOGE for nearly two months. “I’m sure mistakes were made. Mistakes are always made. I would never recommend someone run my code and do what it says. It’s like that ‘Office’ episode where Steve Carell drives into the lake because Google Maps says drive into the lake. Do not drive into the lake.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="15.0">Though Lavingia has talked about his time at DOGE previously, this is the first time his work has been examined in detail and the first time he’s publicly explained his process, down to specific lines of code.</p>
        
    
                        
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="17.0">Lavingia has nearly 15 years of experience as a software engineer and entrepreneur but no formal training in AI. He briefly worked at Pinterest before starting Gumroad, a small e-commerce company that nearly collapsed in 2015. “I laid off 75% of my company — including many of my best friends. It really sucked,” he said. Lavingia kept the company afloat by “replacing every manual process with an automated one,” according to <a href="https://sahillavingia.com/god">a post on his personal blog</a>.</p>
        
    
                    

<figure data-pp-id="18" data-pp-blocktype="image">

    


                    
    


    <img alt="A man wearing khakis, a blue jacket and white sneakers sits in a leather armchair in an office in a renovated loft, decorated with deflated pink balloons, some paper streamers and a potted artificial tree." width="3000" height="2001" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/sahil_brs4154_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=534&amp;q=75&amp;w=800&amp;s=684cbd8f717d254a1cd5ab606983bf89" srcset="https://img.assets-d.propublica.org/v5/images/sahil_brs4154_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=267&amp;q=75&amp;w=400&amp;s=cabcfd67565f1f031df5d6a0650bb5e2 400w, https://img.assets-d.propublica.org/v5/images/sahil_brs4154_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=534&amp;q=75&amp;w=800&amp;s=684cbd8f717d254a1cd5ab606983bf89 800w, https://img.assets-d.propublica.org/v5/images/sahil_brs4154_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=800&amp;q=75&amp;w=1200&amp;s=626caeef3003d72b1fee76ad7c13101b 1200w, https://img.assets-d.propublica.org/v5/images/sahil_brs4154_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=867&amp;q=75&amp;w=1300&amp;s=5233281dd1fbc9d33f09311e437983fa 1300w, https://img.assets-d.propublica.org/v5/images/sahil_brs4154_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=967&amp;q=75&amp;w=1450&amp;s=ad8135856b060cdba0760a73b537742a 1450w, https://img.assets-d.propublica.org/v5/images/sahil_brs4154_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1067&amp;q=75&amp;w=1600&amp;s=a43a48c8965cafb5a7b3028827c8564a 1600w, https://img.assets-d.propublica.org/v5/images/sahil_brs4154_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1334&amp;q=75&amp;w=2000&amp;s=74c8ff0acf368e50b4cd2d992c7b3f8a 2000w">

            
    
<figcaption>
        <span>Sahil Lavingia at his office in Brooklyn</span>
    
        <span>
        <span>Credit: </span>
        Ben Sklar for ProPublica
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="19.0">Lavingia did not have much time to immerse himself in how the VA handles veterans’ care between starting on March 17 and writing the tool on the following day. Yet his experience with his own company aligned with the direction of the Trump administration, which has embraced the use of AI across government to streamline operations and save money.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="20.0">Lavingia said the quick timeline of <a href="https://www.whitehouse.gov/presidential-actions/2025/02/implementing-the-presidents-department-of-government-efficiency-cost-efficiency-initiative/">Trump’s February executive order</a>, which gave agencies 30 days to complete a review of contracts and grants, was too short to do the job manually. “That’s not possible — you have 90,000 contracts,” he said. “Unless you write some code. But even then it’s not really possible.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="21.0">Under a time crunch, Lavingia said he finished the first version of his contract-munching tool on his second day on the job — using AI to help write the code for him. He told ProPublica he then spent his first week downloading VA contracts to his laptop and analyzing them.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="22.0">VA press secretary Pete Kasperowicz lauded DOGE’s work on vetting contracts in a statement to ProPublica. “As far as we know, this sort of review has never been done before, but we are happy to set this commonsense precedent,” he said.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="23.0">The VA is reviewing all of its 76,000 contracts to ensure each of them benefits veterans and is a good use of taxpayer money, he said. Decisions to cancel or reduce the size of contracts are made after multiple reviews by VA employees, including agency contracting experts and senior staff, he wrote.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="24.0">Kasperowicz said that the VA will not cancel contracts for work that provides services to veterans or that the agency cannot do itself without a contingency plan in place. He added that contracts that are “wasteful, duplicative or involve services VA has the ability to perform itself” will typically be terminated.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="25.0">Trump officials have said they are working toward a <a href="https://www.nytimes.com/2025/05/06/us/politics/doug-collins-veterans-affairs-job-cuts.html">“goal” of cutting</a> around 80,000 people from the VA’s workforce of nearly 500,000. Most employees work in one of the VA’s 170 hospitals and nearly 1,200 clinics.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="26.0">The VA has said it would avoid cutting contracts that directly impact care out of fear that it would cause harm to veterans. ProPublica recently reported that relatively small cuts at the agency have already <a href="https://www.propublica.org/article/trump-veterans-affairs-budget-staff-cuts-jeopardize-cancer-research">been jeopardizing veterans’ care</a>.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="27.0">The VA has not explained how it plans to simultaneously move services in-house, as Lavingia’s code suggested was the plan, while also slashing staff.</p>
        
    
                                  
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="29.0">Many inside the VA told ProPublica the process for reviewing contracts was so opaque they couldn’t even see who made the ultimate decisions to kill specific contracts. Once the “munching” script had selected a list of contracts, Lavingia said he would pass it off to others who would decide what to cancel and what to keep. No contracts, he said, were terminated “without human review.”</p>
        
    
                        
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="31.0">“I just delivered the [list of contracts] to the VA employees,” he said. “I basically put munchable at the top and then the others below.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="32.0">VA staffers told ProPublica that when DOGE identified contracts to be canceled early this year — before Lavingia was brought on — employees sometimes were given little time to justify retaining the service. One recalled being given just a few hours. The staffers asked not to be named because they feared losing their jobs for talking to reporters.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="33.0">According to one internal email that predated Lavingia’s AI analysis, staff members had to respond in 255 characters or fewer — just shy of the 280 character limit on Musk’s X social media platform.</p>
        
    
                    

<figure data-pp-id="34" data-pp-blocktype="image">

    


                    
    


    <img alt="A portion of an email to VA staffers reads: “I was just informed subject is on a list that they are considering directing us to terminate. I am advising you, if this is not acceptable, please go up your chain to submit an appropriate justification. On previous requests we have been informed the one sentence justification/statement should be no more than 255 characters in length. The statement should explain why the contract is essential for VHA to fulfill its statutory purposes including statute support, healthcare support, or any critical mission requirement, where applicable.”" width="3000" height="644" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/20250604_doge-va-ai_255-characters_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=172&amp;q=75&amp;w=800&amp;s=49192739da6cdf7af94cd841344ccc19" srcset="https://img.assets-d.propublica.org/v5/images/20250604_doge-va-ai_255-characters_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=86&amp;q=75&amp;w=400&amp;s=3f4e3e2eb9d041b6b5e579b1ac3563fe 400w, https://img.assets-d.propublica.org/v5/images/20250604_doge-va-ai_255-characters_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=172&amp;q=75&amp;w=800&amp;s=49192739da6cdf7af94cd841344ccc19 800w, https://img.assets-d.propublica.org/v5/images/20250604_doge-va-ai_255-characters_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=258&amp;q=75&amp;w=1200&amp;s=9caafad40e8eef41e482dc69458dda95 1200w, https://img.assets-d.propublica.org/v5/images/20250604_doge-va-ai_255-characters_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=279&amp;q=75&amp;w=1300&amp;s=934a4aa534e9a89f2b5355118cc51e74 1300w, https://img.assets-d.propublica.org/v5/images/20250604_doge-va-ai_255-characters_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=311&amp;q=75&amp;w=1450&amp;s=3df53eb353b97c524e95f08232287b0c 1450w, https://img.assets-d.propublica.org/v5/images/20250604_doge-va-ai_255-characters_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=343&amp;q=75&amp;w=1600&amp;s=62d7b2434ad6fc9b1b8bdf8171f0f664 1600w, https://img.assets-d.propublica.org/v5/images/20250604_doge-va-ai_255-characters_maxWidth_3000_maxHeight_3000_ppi_72_quality_95_embedColorProfile_true.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=429&amp;q=75&amp;w=2000&amp;s=78f1e620bc43260b9add32cf76c0594e 2000w">

            
    
<figcaption>
        <span>A VA email tells staffers that the justification of contracts targeted by DOGE must be limited to 255 characters.</span>
    
        <span>
        <span>Credit: </span>
        Obtained by ProPublica
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="35.0">Once he started on DOGE’s contract analysis, Lavingia said he was confronted with technological limitations. At least some of the errors produced by his code can be traced to using older versions of OpenAI models available through the VA — models not capable of solving complex tasks, according to the experts consulted by ProPublica.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="36.0">Moreover, the tool’s underlying instructions were deeply flawed. Records show Lavingia programmed the AI system to make intricate judgments based on the first few pages of each contract — about the first 2,500 words — which contain only sparse summary information.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="37.0">“AI is absolutely the wrong tool for this,” said Waldo Jaquith, a former Obama appointee who oversaw IT contracting at the Treasury Department. “AI gives convincing looking answers that are frequently wrong. There needs to be humans whose job it is to do this work.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="38.0">Lavingia’s prompts did not include context about how the VA operates, what contracts are essential or which ones are required by federal law. This led AI to determine a core piece of the agency’s own contract procurement system was “munchable.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="39.0">At the core of Lavingia’s prompt is the direction to spare contracts involved in “direct patient care.”</p>
        
    
                    
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="41.0">Such an approach, experts said, doesn’t grapple with the reality that the work done by doctors and nurses to care for veterans in hospitals is only possible with significant support around them.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="42.0">Lavingia’s system also used AI to extract details like the contract number and “total contract value.” This led to avoidable errors, where AI returned the wrong dollar value when multiple were found in a contract. Experts said the correct information was readily available from public databases.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="43.0">Lavingia acknowledged that errors resulted from this approach but said those errors were later corrected by VA staff.</p>
        
    
                        
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="45.0">In late March, Lavingia published a version of the “munchable” script <a href="https://github.com/slavingia/va">on his GitHub account</a> to invite others to use and improve it, he told ProPublica. “It would have been cool if the entire federal government used this script and anyone in the public could see that this is how the VA is thinking about cutting contracts.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="46.0">According <a href="https://sahillavingia.com/doge">to a post on his blog</a>, this was done with the approval of Musk before he left DOGE. “When he asked the room about improving DOGE’s public perception, I asked if I could open-source the code I’d been writing,” Lavingia said. “He said yes — it aligned with DOGE’s goal of maximum transparency.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="47.0">That openness may have eventually led to Lavingia’s dismissal. Lavingia confirmed he was <a href="https://sahillavingia.com/doge">terminated from DOGE</a> after giving <a href="https://www.fastcompany.com/91330297/doge-sahil-lavignia-gumroad">an interview to Fast Company magazine</a> about his work with the department. A VA spokesperson declined to comment on Lavingia’s dismissal.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="48.0">VA officials have declined to say whether they will continue to use the “munchable” tool moving forward. But the administration may deploy AI to help the agency replace employees. Documents previously obtained by ProPublica show DOGE officials proposed in March consolidating the benefits claims department by relying more on AI.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="49.0">And the government’s contractors are paying attention. After Lavingia posted his code, he said he heard from people trying to understand how to keep the money flowing.</p>

<p data-pp-blocktype="copy" data-pp-id="49.1">“I got a couple DMs from VA contractors who had questions when they saw this code,” he said. “They were trying to make sure that their contracts don’t get cut. Or learn why they got cut.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="50.0">“At the end of the day, humans are the ones terminating the contracts, but it is helpful for them to see how DOGE or Trump or the agency heads are thinking about what contracts they are going to munch. Transparency is a good thing.”</p>
        
    
                    <div data-pp-location="bottom-note" data-pp-view="" data-pp-category="get involved">
                        <p>If you have any information about the misuse or abuse of AI within government agencies, Brandon Roberts is an investigative journalist on the news applications team and has a wealth of experience using and dissecting artificial intelligence. He can be reached on Signal @brandonrobertz.01 or by email <a href="https://www.propublica.org/cdn-cgi/l/email-protection#88eafae9e6ece7e6a6fae7eaedfafcfbc8f8fae7f8fdeae4e1ebe9a6e7faef"><span data-cfemail="6d0f1f0c03090203431f020f081f191e2d1d1f021d180f01040e0c43021f0a">[email&nbsp;protected]</span></a>.</p>
<p>If you have information about the VA that we should know about, contact reporter Vernal Coleman on Signal, vcoleman91.99, or via email, <a href="https://www.propublica.org/cdn-cgi/l/email-protection#1f697a6d717e73317c70737a727e715f6f6d706f6a7d73767c7e31706d78"><span data-cfemail="86f0e3f4e8e7eaa8e5e9eae3ebe7e8c6f6f4e9f6f3e4eaefe5e7a8e9f4e1">[email&nbsp;protected]</span></a>, and Eric Umansky on Signal, Ericumansky.04, or via email, <a href="https://www.propublica.org/cdn-cgi/l/email-protection#6b0e190208451e060a051800122b1b19041b1e090702080a4504190c"><span data-cfemail="21445348420f544c404f524a586151534e5154434d4842400f4e5346">[email&nbsp;protected]</span></a>.</p>

        </div><!-- end .article-body__bottom-notes -->
        
    </div></div>]]></description>
        </item>
    </channel>
</rss>