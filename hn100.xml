<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 05 Sep 2024 22:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[My job is to watch dreams die (2011) (154 pts)]]></title>
            <link>https://old.reddit.com/r/reddit.com/comments/k3zrz/by_request_from_the_jobs_thread_why_my_job_is_to/</link>
            <guid>41459365</guid>
            <pubDate>Thu, 05 Sep 2024 18:43:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/reddit.com/comments/k3zrz/by_request_from_the_jobs_thread_why_my_job_is_to/">https://old.reddit.com/r/reddit.com/comments/k3zrz/by_request_from_the_jobs_thread_why_my_job_is_to/</a>, See on <a href="https://news.ycombinator.com/item?id=41459365">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="http://www.reddit.com/r/AskReddit/comments/k3ifx/whats_the_most_fucked_up_thing_youve_had_to_do_at/c2ha0il">Original post here</a>.</p>

<p>I work at a real estate office.  We primarily sell houses that were foreclosed on by lenders.  We aren't involved in the actual foreclosures or evictions - anonymous lawyers in the cloud somewhere is tasked with the paperwork - we are the boots on the ground that interacts with the actual walls, roofs and occasional bomb threat.</p>

<p>When the lender forecloses - or is thinking of foreclosing - on a property one of the first things that happens is they send somebody out to see if there is actually a house there and if there is anybody living there who needs to be evicted.  Lawyers are expensive so they send a real estate agent or a property preservation company out to check.  There is the occasional discovery of fraud where there was never a house on the parcel to begin with, but such instances are rare.  Sometimes this initial visit results in discovering a house that has burned down or demolished, is abandoned or occupied by somebody who has absolutely no connection with the homeowner.  Sometimes the houses are discovered to be crack dens or meth labs, sometimes the sites of cock or dog fighting operations, or you might even find a back yard filled with a pot cultivation that can't be traced back to anybody because it was planted in yet another vacant house in a blighted neighborhood.  The house could be worth less than zero - blighted to the point where you can't even give it away (this is a literal statement, I have tried to give away many houses or even vacant lots with no takers over the years) or it could be a waterfront mansion in a gated golf community worth well over seven figures that does not include the number "one".    Sometimes they are found to have been seized by the IRS, the local tax authority, the DEA or the US Marshal.  Variety is the rule.  The end results are the law.</p>

<p>If the house is occupied my job is to make contact and determine who they are: there are laws that establish what happens to a borrower as opposed to a tenant and the servicemember relief act adds an additional set of questions that must be answered. Some of the people have an idea of why I am there.  Some claim they never knew they were foreclosed on, or tell me that they have worked something out with their lender, some won't tell me a thing and some threaten me to never return in the name of the police, their lawyer, or the occasional "or else/if I were you".  During one initial visit the sight of 50-60 motorcycles parked on the lawn suggested that we try again the next day.  At a couple the police had cordoned off the area and at one they were in the process of dredging the lake searching for the body of a depressed former homeowner.</p>

<p>If nobody is home I have to determine if they are at work, on vacation, in the army, wintering/summering at their other home, in jail, in a nursing home, dead or if they moved away.  It isn't easy.  Utilities can be left on for months.  Neighbors can be staging the yard and house to appear occupied to prevent blight in their neighborhood.  By the same token people will stop cutting the lawn for months, let trash and old phone books pile up on their porch, lose gas and electric service and continue to live in properties that have not only physically unsafe to approach but are so filthy that when it comes time to clean them out the crews have to wear hazmat suits.  One house had a gallon pickle jar filled with dead roaches on the porch.  Somebody lived in that house and thought that was a logical thing to do.  People like me are tasked with first contact.</p>

<p>Evictions are expensive and time-consuming.  Ultimately once the process gets that far there isn't much that can be done to prevent it.  You didn't pay your mortgage, the lender gets the house back.  There are an infinite number of reasons why the mortgage couldn't be paid, some are more sympathetic than others, but in the end you will be leaving the property willingly or not.  The lawyers handle the evictions - they churn through the paperwork in the background, ten thousand properties at a time.  They have it down to rote function based on templates, personal experience with the various judges and intimate knowledge of the federal, state and municipal laws, along with dealing with the occasional sheriff who refuses to evict somebody, the informal policies established by the local judges and a myriad of other problems that can arise.  As a business decision many lenders have determined that it is cheaper to settle with the occupants - instead of going through the formal eviction they will offer cash.  In exchange for surrendering a property in reasonably clean condition with the furnace still hooked up, the kitchen not stripped and the basement not intentionally flooded the lender will cut the occupants a check.  It costs much less than an eviction, provides reasonable hope that the plumbing won't freeze and can take a fraction of the time to obtain possession.  This is where the personal element becomes real.</p>

<p>(Continued in comments)</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[UE5 Nanite in WebGPU (206 pts)]]></title>
            <link>https://github.com/Scthe/nanite-webgpu</link>
            <guid>41458987</guid>
            <pubDate>Thu, 05 Sep 2024 17:55:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Scthe/nanite-webgpu">https://github.com/Scthe/nanite-webgpu</a>, See on <a href="https://news.ycombinator.com/item?id=41458987">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Nanite WebGPU</h2><a id="user-content-nanite-webgpu" aria-label="Permalink: Nanite WebGPU" href="#nanite-webgpu"></a></p>
<blockquote>
<p dir="auto">TL;DR: <a href="https://scthe.github.io/nanite-webgpu/?scene_file=jinxCombined&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.1" rel="nofollow">Demo scene Jinx</a> (640m triangles). <a href="https://scthe.github.io/nanite-webgpu/?scene_file=manyObjects2&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=1.0" rel="nofollow">Sample scene with many objects</a> (1.7b triangles). White triangles in the Jinx scene are software-rasterized. <strong>WebGPU is only available on Chrome!</strong></p>
</blockquote>
<p dir="auto">This project contains a <a href="https://youtu.be/qC5KtatMcUw?si=IOWaVk0sQNra_R6O&amp;t=97" rel="nofollow">Nanite</a> implementation in a web browser using WebGPU. This includes the meshlet LOD hierarchy, software rasterizer (at least as far as possible given the current state of WGSL), and billboard impostors. Culling on both per-instance and per-meshlet basis (frustum and occlusion culling in both cases). Supports textures and per-vertex normals. Possibly every statistic you can think of. There is a slider or a checkbox for every setting imaginable. Also works offline using Deno.</p>
<p dir="auto">First, we will see some screenshots, then there is (not even complete) list of features. Afterward, I will link you to a couple of <strong>demo scenes</strong> you can play with. In the FAQ section, you can read <strong>my thoughts about Nanite</strong>. Since this file got a bit long, I've moved usability-oriented stuff (stats/GUI explanation, build process, and unit test setup) into a separate <a href="https://github.com/Scthe/nanite-webgpu/blob/master/USAGE.md">USAGE.md</a>.</p>
<blockquote>
<p dir="auto">EDIT 16-08-2024: I've rewritten significant parts of this README once I had more time to look through it. And I've written <a href="https://github.com/Scthe/frostbitten-hair-webgpu">Frostbitten hair WebGPU</a> meantime #self-promo.</p>
</blockquote>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/9325337/348849467-ef4c8476-bf30-4241-96d0-a354efa0dea1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTQ2Ny1lZjRjODQ3Ni1iZjMwLTQyNDEtOTZkMC1hMzU0ZWZhMGRlYTEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZWExMWU3NDVkN2JlZjA2MDU0YWFhMDEzNWY4MWFhMWRmNjhkNTBmZmY0ODE0MDlhMTI5ZjAwMDFmMDg3YjhhZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.G4mQjdsP12YTNSLhVg3WVUyvLgm6bQxd27pe0lwi1lY"><img src="https://private-user-images.githubusercontent.com/9325337/348849467-ef4c8476-bf30-4241-96d0-a354efa0dea1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTQ2Ny1lZjRjODQ3Ni1iZjMwLTQyNDEtOTZkMC1hMzU0ZWZhMGRlYTEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZWExMWU3NDVkN2JlZjA2MDU0YWFhMDEzNWY4MWFhMWRmNjhkNTBmZmY0ODE0MDlhMTI5ZjAwMDFmMDg3YjhhZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.G4mQjdsP12YTNSLhVg3WVUyvLgm6bQxd27pe0lwi1lY" alt="scene-multiobject"></a></p>
<p dir="auto"><em><a href="https://scthe.github.io/nanite-webgpu/?scene_file=manyObjects2&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=1.0" rel="nofollow">Sample scene</a> containing 1.7b triangles. Nearly 98% of the triangles are software rasterized, as it's much faster than hardware.</em></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/9325337/348849421-4eef5e85-03dd-43f3-9a99-afbce59407f0.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTQyMS00ZWVmNWU4NS0wM2RkLTQzZjMtOWE5OS1hZmJjZTU5NDA3ZjAucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTRhY2I1NDQyZjNhMjBkYjAxMGFiNGQ1MmQ5MjcyMzY2ZmVlMmQxNjc3ZThhMGUyNzQ0ZjNjNzRiOTg3NDg0ZSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.xmvp4efV5JdJ_c1xPLWh7WbcIwRYrfRG5BNjHhOoCCo"><img src="https://private-user-images.githubusercontent.com/9325337/348849421-4eef5e85-03dd-43f3-9a99-afbce59407f0.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTQyMS00ZWVmNWU4NS0wM2RkLTQzZjMtOWE5OS1hZmJjZTU5NDA3ZjAucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTRhY2I1NDQyZjNhMjBkYjAxMGFiNGQ1MmQ5MjcyMzY2ZmVlMmQxNjc3ZThhMGUyNzQ0ZjNjNzRiOTg3NDg0ZSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.xmvp4efV5JdJ_c1xPLWh7WbcIwRYrfRG5BNjHhOoCCo" alt="scene-jinx"></a></p>
<p dir="auto"><em>My <a href="https://scthe.github.io/nanite-webgpu/?scene_file=jinxCombined&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.1" rel="nofollow">primary test scene</a>. <a href="https://sketchfab.com/3d-models/arcane-jinx-b74f25a5ee6e43efbe9766b9fbebc705" rel="nofollow">Arcane - Jinx</a> 3D model by sketchfab user <a href="https://sketchfab.com/rizky08" rel="nofollow">Craft Tama</a>. Unfortunately, the best simplification we get is from 44k to 3k triangles. The white triangles are software-rasterized triangles (between hardware-rasterized ones and the impostors in the far back). WebGPU does not support <code>atomic&lt;u64&gt;</code>, so I had to compress the data to fit into 32 bits (u16 for depth, 2*u8 for octahedron-encoded normals). It's a painful limitation, but at least you can see the entire system is working.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Nanite
<ul dir="auto">
<li><strong>Meshlet LOD hierarchy.</strong>
<ul dir="auto">
<li>Mesh preprocessing executes in the browser, using WebAssembly for <a href="https://github.com/zeux/meshoptimizer">meshoptimizer</a> and <a href="http://glaros.dtc.umn.edu/gkhome/metis/metis/overview" rel="nofollow">METIS</a>. While it might raise eyebrows, this was one of the goals.</li>
<li>There is a file exporter too, if you don't like to wait between page refreshes.</li>
</ul>
</li>
<li><strong>Software rasterizer.</strong>
<ul dir="auto">
<li>WebGPU does not have the <code>atomic&lt;u64&gt;</code> needed to implement this feature efficiently. Currently, I'm packing depth (<code>u16</code>) and octahedron-encoded normals (<code>2 * u8</code>) into 32 bits. It's enough to show that the rasterizer works.</li>
<li>With only 32 bits, we are butchering the precision. My only concern here is to show that the rasterization works. If you see the software rasterized bunny model in the background it will be white and it will have <em>reasonable</em> shading. Reprojecting depth and "compressing" normals is enough to get something.. not offending.</li>
<li>This also affects the depth pyramid used for occlusion culling.</li>
<li>There are other algorithms to do this. PPLL, or something with tiles, or double rasterization (1st pass writes depth, 2nd does compareExchange). But the 32-bit limitation is only in WebGPU, so I choose to stick to UE5's solution instead.</li>
</ul>
</li>
<li><strong>Billboard impostors.</strong> 12 images around the UP-axis, blended (with dithering) based on the camera position. Does not handle up/down views. Contains both diffuse and normals, so we can do nice shading at a runtime. UE5 <a href="https://advances.realtimerendering.com/s2021/Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf#page=97" rel="nofollow">uses</a> a more advanced version integrated with a visibility buffer.
<ul dir="auto">
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=jinxCombined&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.1&amp;impostors_forceonlybillboards&amp;impostors_texturesize=512" rel="nofollow">Impostors preview demo scene</a>. For this demo, I've increased the impostor texture size. This way you can see more details.</li>
</ul>
</li>
</ul>
</li>
<li>Culling:
<ul dir="auto">
<li><strong>Per-instance:</strong> frustum and occlusion culling.</li>
<li><strong>Per-meshlet:</strong> frustum and occlusion culling.</li>
<li><strong>Per-triangle:</strong> hardware backface culling and ofc. z-buffer. WebGPU does not have early-z.
<ul dir="auto">
<li>I have no idea how early-z works in WebGPU (it does not).</li>
</ul>
</li>
<li>I've also tried per-meshlet backface cone culling. It worked fine, but I cut it from the final release. See FAQ below for more details.</li>
<li>Occlusion culling is just a depth pyramid from the previous frame's depth buffer. No reprojection and no two-pass. The current implementation is enough to cull a lot of triangles (<strong>A LOT!</strong>) and to judge the performance impact (big improvement!). I expect someone will want to read the code, and they will be grateful this feature was not added.</li>
</ul>
</li>
<li>Switch between <strong>GPU-driven rendering</strong> and a <strong>naive CPU implementation</strong>. I have not spent much time optimizing the CPU version. It works, you can step through it with the debugger.</li>
<li>Supports <strong>textured models</strong> and <strong>many different objects</strong> at the same time.</li>
<li>Controls to <strong>change parameters at runtime</strong>. Debug views. "Freeze culling" allows the camera to move and inspect only what was drawn last frame.</li>
<li>A lot of <strong>stats</strong>. Memory, geometry. Total scene meshlets, triangles. Drawn meshlets, triangles (split between hardware and software rasterizer). Impostor count. Dedicated profiler button to get the timings.</li>
<li><strong>Custom file format</strong> so you don't have to preprocess the mesh every time. This is optional, you <strong>can also use an OBJ file</strong>.</li>
<li>Vertex <strong>position quantization</strong> (vec2u), <strong>octahedron encoded normals</strong> (vec2f).
<ul dir="auto">
<li>Position quantization is off by default. Toggle <code>CONFIG.useVertexQuantization</code> to enable. There are <em>funny</em> things happening to the numbers there, but everything <em>should</em> be handled correctly.</li>
</ul>
</li>
<li>Handles window resize. It's a web browser after all.</li>
<li>The whole app also <strong>runs offline in <a href="https://deno.com/" rel="nofollow">Deno</a></strong>. I've written shader unit tests this way.</li>
<li>Tons of WebGPU and WGSL code that you can copy to your own project. If you want to do something, I've either attempted to do it or discovered that it does not work.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Goals</h3><a id="user-content-goals" aria-label="Permalink: Goals" href="#goals"></a></p>
<p dir="auto">There were 2 primary goals for this project:</p>
<ol dir="auto">
<li><strong>Simplicity.</strong> We start with an OBJ file and everything is done in the app. No magic pre-processing steps, Blender exports, etc. You set the breakpoint at <code>loadObjFile()</code> and F10 your way till the first frame finishes.</li>
<li><strong>Experimentation.</strong> I could have built this with Vulkan and Rust. None would touch it. Instead, it's a webpage. You click the link, uncheck the checkbox and the FPS tanks 40%. And you think to yourself: "OK, that was an important checkbox. But what about this slider?". Or: "How does scene X affect memory allocation?". Right now I know that a lot of code can be optimized. Yet it would not matter till the simplification problem is solved.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo Scenes</h2><a id="user-content-demo-scenes" aria-label="Permalink: Demo Scenes" href="#demo-scenes"></a></p>
<ul dir="auto">
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=jinxCombined&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.1" rel="nofollow">Jinx</a> (120*120 instances, 640m triangles). A single Jinx is 44k triangles simplified to 3k at 59 root meshlets. Uses an OBJ file.</li>
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=manyObjects2&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=1.0" rel="nofollow">Lucy and dragons</a> (both objects at 70*70 instances, 1.7b triangles). See below for per-object details.</li>
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=lucy1b&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.5" rel="nofollow">Lucy</a> (110*110 instances, 1.2b triangles). A single Lucy statue is 100k triangles simplified to 86 at a single root meshlet. Uses binary file.</li>
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=dragonJson&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.5" rel="nofollow">Dragons</a> (70*70 instances, 1.2b triangles). A single dragon is 250k triangles simplified to 102 at a single root meshlet. Uses binary file.</li>
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=bunny1b&amp;impostors_threshold=1000&amp;softwarerasterizer_threshold=2400" rel="nofollow">Bunnies</a> (500*500 instances, 1.2b triangles). A single bunny is 5k triangles simplified to 96 at a single root meshlet. Uses an OBJ file. Bunnies are so small most are frustum culled.</li>
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=jinxCombined&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.1&amp;impostors_forceonlybillboards&amp;impostors_texturesize=512" rel="nofollow">Impostors preview</a>. For this demo, I've increased the impostor texture size. This way you can see more details.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">You can find details in <a href="https://github.com/Scthe/nanite-webgpu/blob/master/USAGE.md">USAGE.md</a>. Short version:</p>
<ul dir="auto">
<li>Use the <code>[W, S, A, D]</code> keys to move and <code>[Z, SPACEBAR]</code> to fly up or down. <code>[Shift]</code> to move faster.</li>
<li>If there is something weird, toggle culling options on/off. There are some minor bugs in the implementation.</li>
<li>The white triangles are software-rasterized triangles (between hardware-rasterized ones and the impostors in the far back). WebGPU does not support <code>atomic&lt;u64&gt;</code>, so I had to compress data to fit into 32 bits (u16 for depth, 2*u8 for octahedron encoded normals).
<ul dir="auto">
<li>16-bit depth is.. not a great idea. It produces <strong>tons</strong> of artifacts like z-fighting or leaks. Turn the software rasterizer off to easier inspect raw Nanite meshlets. Be prepared for a major performance hit!</li>
</ul>
</li>
<li>FPS might fluctuate due to the browser's enforced VSync. Use the "Profile" button instead.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What are the major differences compared to UE5's Nanite?</h3><a id="user-content-what-are-the-major-differences-compared-to-ue5s-nanite" aria-label="Permalink: What are the major differences compared to UE5's Nanite?" href="#what-are-the-major-differences-compared-to-ue5s-nanite"></a></p>
<ul dir="auto">
<li>Error metric is just a simple projected simplification error (read below).</li>
<li>Meshlet simplification is.. simplistic.</li>
<li>No two-pass occlusion culling.
<ul dir="auto">
<li>This would not be complicated to add, just tedious to debug. Unfortunately, it also has some interactions with the GUI settings. ATM some parts of the code are riddled with <code>ifs</code> for certain user settings. For example, you could press "Freeze culling" to stop updating the list of drawn meshlets. This includes software rasterized meshlets. Move the camera in this mode and all 10+ million 1 px-sized software rasterized triangles might become fullscreen. Adding two-pass occlusion culling might expose more such interactions. It would also make the code harder to read, which goes against my goals.</li>
</ul>
</li>
<li>No work queue in shaders. For meshlet culling and LOD selection, I dispatch thread per-meshlet.</li>
<li>No VRAM eviction of unused LODs and streaming.
<ul dir="auto">
<li>Theoretically, to load new meshlet data, you would write requested <code>meshletIds</code> into a separate GPUBuffer. Download it to RAM and load the content. Keep LRU (timestamp per-meshlet, visible from CPU) to manage evictions. In practice, I suspect you might also want to add a priority system.</li>
</ul>
</li>
<li>No visibility buffer. It's not possible with the <code>atomic&lt;u64&gt;</code> limitation that I have.
<ul dir="auto">
<li>BTW if you render material data into a GBuffer, you get Nanite integration with your material system for free.</li>
</ul>
</li>
<li>No built-in shadows/multiview.</li>
<li>My implementation focuses on using a predictable amount of memory for demo cases. This means it's not scalable if you have many <strong>different</strong> objects (not instances). You would have to know the upper bound of the drawn meshlets to preallocate buffers that hold data between the stages. The naive solutions like <code>bottomLevelMeshletsCount * instanceCount</code> easily end up in GBs of VRAM!</li>
<li>No BVH for instances (or any other hierarchical implementation). I just take all instances and frustum + occlusion cull them.</li>
<li>I don't have a GPU profiler on the web/Deno. Or a debugger, or printf for that matter.
<ul dir="auto">
<li>ITWouldGenerate_DX_CODE_THATIWOULDHAVE_TO_READ_ANYWAY_SONOiGUESS.</li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Does xxx billions of triangles mean anything?</h3><a id="user-content-does-xxx-billions-of-triangles-mean-anything" aria-label="Permalink: Does xxx billions of triangles mean anything?" href="#does-xxx-billions-of-triangles-mean-anything"></a></p>
<p dir="auto">There was a video on YouTube showing how Nanite handles 120 billion triangles. Yet most of them were frustum culled? Performance depends on a lot of factors.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Dense meshes</h4><a id="user-content-dense-meshes" aria-label="Permalink: Dense meshes" href="#dense-meshes"></a></p>
<p dir="auto">Having a lot of dense meshes up close could have a negative performance impact. Unless you are so close to them that they cover 50% of the screen. Then, the occlusion culling kicks in. Dense geometry also means that meshlets are small. 128 triangles in a 20,000,000 triangle mesh? They do not take much space on the screen and are easily occlusion/cone culled.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Instance count</h4><a id="user-content-instance-count" aria-label="Permalink: Instance count" href="#instance-count"></a></p>
<p dir="auto">What about millions of instances? Each has its own mat4x3 transform matrix. This consumes VRAM. Obligatory link to <a href="https://pharr.org/matt/blog/2018/07/09/moana-island-pbrt-2" rel="nofollow">swallowing the elephant (part 2)</a>. During the frame, you also need to store a list of things to render. In the worst-case scenario, each instance will render its most dense meshlets. In my implementation, this allocates <code>instanceCount * bottomLevelMeshletsCount * sizeof(vec2u)</code> bytes. A 5k triangle bunny might have only 56 fine-level meshlets (out of 159 total), but what if I want to render 100,000 of them? This is not a scalable memory allocation. In Chrome, WebGPU has a 128MB limit for storage buffers (can be raised if needed). You might notice that the demo scenes above were tuned to reflect that.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Scene arrangement</h4><a id="user-content-scene-arrangement" aria-label="Permalink: Scene arrangement" href="#scene-arrangement"></a></p>
<p dir="auto">The scenes in my app have objects arranged in a square. For far objects, only a small part will be visible. But they will use coarse meshlet LOD, that contains more than just a visible part. The visible part passes occlusion culling and leads to a lot of overdraw for the rest of the meshlet. This is not an optimal scene arrangement. You would also think that a dense grid placement (objects close to each other) is bad. It certainly renders more triangles close to the camera. But it also means that there are no huge differences in depth between them. This is a dream for occlusion culling. You could build a wall from high-poly meshes and it's actually one of the most performant scenarios. Objects far from each other mean that a random distant pixel pollutes the depth pyramid (the <a href="https://www.youtube.com/@MentourPilot/videos" rel="nofollow">Swiss cheese theory</a>). Does your scene have a floor? Can you <a href="https://advances.realtimerendering.com/s2021/Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf#page=96" rel="nofollow">merge</a> far objects into one?</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">In practice</h4><a id="user-content-in-practice" aria-label="Permalink: In practice" href="#in-practice"></a></p>
<p dir="auto">This leads to the <strong>Jinx test scene</strong>. The character is skinny. Looking down each row/column of the grid you can see the gaps. There is space between her arm and torso. This kills occlusion culling. The model does not simplify well. 3k triangles in the most coarse LOD (see below for more details). It's death by thousands of 1-pixel triangles. Software rasterizer helps a lot. Yet given the scene arrangement, most of the instances are rendered as impostors. Up close, the hardware rasterizer takes over. All 3 systems have different strengths.</p>
<blockquote>
<p dir="auto">With UE5's simplification algorithm, the balance is probably shifted. Much more software rasterizer, and less hardware one. And I wager a bet they don't have to rely on impostors as much. Their coarse LOD would be less than 3k tris (again, see below).</p>
</blockquote>
<p dir="auto">Basically, there are a lot of use cases. If you want a <strong>stable</strong> Nanite implementation, you have to test each one. But if you want a big triangle count, there are ways to cheat that too.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What surprised you about Nanite?</h3><a id="user-content-what-surprised-you-about-nanite" aria-label="Permalink: What surprised you about Nanite?" href="#what-surprised-you-about-nanite"></a></p>
<ol dir="auto">
<li>The goal of the DAG is not to "use fewer triangles for far objects". The goal is to have a consistent 1 pixel == 1 triangle across the entire screen. A triangle is our "unit of data". The artist imports a sculpture from ZBrush. We need to need a way (through an error metric) to display it no matter if it's 1m or 500m from the camera. This is not possible with discrete LOD meshes (each LOD level is a separate geometry). Sometimes you would want an LOD between 2 levels. You need continuous LODs. This is the reason for the meshlet hierarchy. It allows you to "sample" geometry at any detail level you choose.</li>
<li>You spend more time working on culling and meshlets instead of Nanite itself. You <strong>WILL</strong> reimplement both <a href="https://ubm-twvideo01.s3.amazonaws.com/o1/vault/gdc2016/Presentations/Wihlidal_Graham_OptimizingTheGraphics.pdf" rel="nofollow">"Optimizing the Graphics Pipeline with Compute"</a> and <a href="https://advances.realtimerendering.com/s2015/aaltonenhaar_siggraph2015_combined_final_footer_220dpi.pdf" rel="nofollow">"GPU-Driven Rendering Pipelines"</a>.</li>
<li>Meshlet LOD hierarchy is quite easy to get working. Praise <a href="https://github.com/zeux/meshoptimizer">meshoptimizer</a> and <a href="http://glaros.dtc.umn.edu/gkhome/metis/metis/overview" rel="nofollow">METIS</a>! But if you want to do it efficiently, <a href="https://advances.realtimerendering.com/s2021/Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf#page=50" rel="nofollow">it will be a pain</a>. See next question for full story. I just went with the simplest option.</li>
<li>If your mesh does not simplify cleanly, you end up with e.g. ~3000 triangles that cover a single pixel (Jinx scene). The efficiency scales with your mesh simplification code. And if you want pixel-sized triangles (the main selling point for most people), you <strong>need</strong> a software rasterizer. The billboard impostors are also a good stability-oriented fallback. As mentioned above, the whole system should work cohesively.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">What about mesh simplification?</h3><a id="user-content-what-about-mesh-simplification" aria-label="Permalink: What about mesh simplification?" href="#what-about-mesh-simplification"></a></p>
<blockquote>
<p dir="auto">Remember, we are not doing a simple "take a mesh and return something that has X% of the triangles". We are doing the simplification in the context of meshlets and METIS.</p>
</blockquote>
<p dir="auto">UE5 has its own mesh simplification code. It's the first thing that happens in the asset pipeline. Thus, everything saved here will have avalanche-like benefits for the rest of the system. It was also a problem with the Jinx model. On <a href="https://advances.realtimerendering.com/s2021/Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf#page=95" rel="nofollow">slide 95</a> Brian Karis states that <strong>all</strong> their LOD graphs end at a <strong>single</strong> root cluster. So no matter the model you provide, they can simplify it to 128 triangles. It makes you less reliant on the impostors. In my app, I could e.g. increase meshoptimizer's <code>target_error</code> parameter. But consider the following story:</p>
<ol dir="auto">
<li>My first test model was a bunny with 5k triangles. Easy to debug (check for holes, etc.). It simplified into a single 128 tris meshlet. Nice!</li>
<li>I've tried to load the Jinx model. At some point, the simplification stopped. You gave it X triangles and received the same X triangles. This crashed my app on an assertion.</li>
<li>OK, so if the model does not simplify beyond some level, I will allow the DAG to have many roots. If you failed to remove at least 6+% of the triangles, stop the algorithm for this part of the mesh.</li>
<li>The Jinx model now works correctly. It stops simplifying beyond 7-9 LOD levels, but this only means there are many hierarchy roots.</li>
<li>I load the bunny again and it no longer simplifies to a single root meshlet. Turns out, <strong>a lot of the meshlets did not reduce triangles that much</strong>. But with enough iterations, for such a simple model, we were able to reduce it to only 128 triangles. The whole time we were getting the &lt;6% simplification for some meshlets (so 94% of triangles were left untouched). We just did not know about it. And a lot of meshlets were also not "full". They contained less than 128 triangles.</li>
</ol>
<blockquote>
<p dir="auto">To reproduce, use <code>const SCENE_FILE: SceneName = 'singleBunny';</code> and set <code>CONFIG.nanite.preprocess.simplificationFactorRequirement: 0.94</code>. This option requires triangle reduction by at least 6%. We end up with 512 triangles. Then, set <code>simplificationFactorRequirement: 0.97</code> (require reducing triangle count by at least 3%, which is much more lenient). You end up with a single root that has 116 tris.</p>
</blockquote>
<p dir="auto">It was my first time using meshoptimizer, so you can probably tune it better. In the offline setting, it's possible to retry simplification with a bigger <code>target_error</code>. Or increase <code>target_error</code> for more coarse meshlet levels? From my experiments, both of these changes do not matter. You could also allow the hierarchy to have the bottom children on different levels (probably? there are some issues with this approach e.g. non-uniform mesh density). Maybe generate conservative (with a bigger triangle count than usual), discrete LOD levels in an old way and then use them if the algorithm gets stuck? This makes the error metric and the entire hierarchy pointless. Introduce new custom vertices? Merge more meshlets than 4? Smaller meshlets? Replace meshoptimizer? UE5 also has special weights for METIS partitioning. <strong>Most important, can your (METIS-enchanced) simplification, guarantee that splitting 256 triangles into 128 triangles, will ALWAYS result in 128 triangles?</strong> I think that once you have this guarantee, the simplification (while still not trivial), is significantly easier. With it, you no longer have to think about the concept of triangles in your meshlet hierarchy. You can start thinking only about DAG and nodes. This highlights the need for goor bottom-level meshlets.</p>
<p dir="auto">You may need someone to dedicate their time only to simplification. Personally, I just got it to work and moved on.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/9325337/348849351-157866eb-88e9-4896-895a-9400915478a4.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTM1MS0xNTc4NjZlYi04OGU5LTQ4OTYtODk1YS05NDAwOTE1NDc4YTQucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NmY5NjI3ZTVmNjQwMWNmMmVkNDQ5MGYzOWQxMzU5ZmFjN2RlODMwODg1ZDAxOTA5N2E3YWJjMzhmYWUwMjg2NSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.DRI0B0vfRRt63fIazOdKQ7-fYXvIZLESQLA9VzGgkNk"><img src="https://private-user-images.githubusercontent.com/9325337/348849351-157866eb-88e9-4896-895a-9400915478a4.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTM1MS0xNTc4NjZlYi04OGU5LTQ4OTYtODk1YS05NDAwOTE1NDc4YTQucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NmY5NjI3ZTVmNjQwMWNmMmVkNDQ5MGYzOWQxMzU5ZmFjN2RlODMwODg1ZDAxOTA5N2E3YWJjMzhmYWUwMjg2NSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.DRI0B0vfRRt63fIazOdKQ7-fYXvIZLESQLA9VzGgkNk" alt="simplification"></a></p>
<p dir="auto"><em>Trying to Nanite-simplify <a href="https://sketchfab.com/3d-models/modular-mecha-doll-neon-mask-1e0dcf3e016f4bc897d4b39819220732" rel="nofollow">Modular Mecha Doll Neon Mask</a> (910k tris) 3D model by Sketchfab user <a href="https://sketchfab.com/chambersu1996" rel="nofollow">Chambersu1996</a>. After the 5th hierarchy level, the simplification stops with 180k triangles left. This would be inefficient to render, but still manageable if we switched to impostors <strong>quickly</strong>. A better solution would be to actually spend X hours investigating the simplification process.</em></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What about error metric?</h3><a id="user-content-what-about-error-metric" aria-label="Permalink: What about error metric?" href="#what-about-error-metric"></a></p>
<p dir="auto">Assume you have a mesh that has 20,000,000 triangles. With meshlet hierarchy, you can render it at any triangle count you would have wanted (with a minimum of 128 triangles - 1 meshlet). How do you choose the right meshlets? What does the <em>right meshlet</em> mean? At the end of the day, <strong>THIS</strong> is exactly what Nanite is. Everything else (simplification, meshlet DAG, software rasterizer, etc.) is just a prerequisite to actually start working on this problem. I admit, as the author of this repo, it's a bit disheartening.</p>
<p dir="auto">A few days ago, SIGGRAPH 2024 presentations were published. In <a href="https://advances.realtimerendering.com/s2024/content/Cao-NanoMesh/AdavanceRealtimeRendering_NanoMesh0810.pdf" rel="nofollow">"Seamless rendering on mobile"</a>, Shun Cao from Tencent Games provided the following metric (slide 12):</p>
<div dir="auto" data-snippet-clipboard-copy-content="device_factor = device_power * device_level
// from the slightly blurred graph image it seems to be:
// decay_factor(x) = 1 / (1 + exp(-(x-5000) / 1000)) from 0 to 9000
decay_factor = 1 / (1 + exp(distance_to_view / decay_distance))
threshold = projected_area * device_factor * decay_factor"><pre><span>device_factor</span> <span>=</span> <span>device_power</span> <span>*</span> <span>device_level</span>
<span>// from the slightly blurred graph image it seems to be:</span>
<span>// decay_factor(x) = 1 / (1 + exp(-(x-5000) / 1000)) from 0 to 9000</span>
<span>decay_factor</span> <span>=</span> <span>1</span> <span>/</span> <span>(</span><span>1</span> <span>+</span> <span>exp</span><span>(</span><span>distance_to_view</span> <span>/</span> <span>decay_distance</span><span>)</span><span>)</span>
<span>threshold</span> <span>=</span> <span>projected_area</span> <span>*</span> <span>device_factor</span> <span>*</span> <span>decay_factor</span></pre></div>
<p dir="auto"><em><a href="https://www.wolframalpha.com/input?i=f%28x%29+%3D+1+%2F+%281+%2B+exp%28-%28x-5000%29+%2F+1000%29%29+from+0+to+9000" rel="nofollow">Wolfram alpha for decay_factor</a> as far as I was able to decipher from the function image.</em></p>
<p dir="auto">I have used projected simplification error (as provided by meshoptimizer). It's not a great metric for Nanite. I think that other vertex attributes have to be part of this function too. You should be able to assign different weights on a per-attribute basis. Normals on Jinx's face were a huge problem. In my app, I could just move the LOD error threshold slider to the left. I can say that this approach has an educational value. You will have to find something better.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Should you write your own implementation of Nanite?</h3><a id="user-content-should-you-write-your-own-implementation-of-nanite" aria-label="Permalink: Should you write your own implementation of Nanite?" href="#should-you-write-your-own-implementation-of-nanite"></a></p>
<p dir="auto">Depends. The simplest answer is to just use UE5. You will not beat UE5 in its own game. Looking at Steam's front page, most of the games are simple enough to not need it. It's interesting that (at the time of the writing) the 2 most known Nanite titles are Fortnite and Senua's Saga: Hellblade II. Both have opposite objectives and tones. I recommend the Digital Foundry's <a href="https://www.youtube.com/watch?v=u-zmFVzUmPc" rel="nofollow">"Inside Senua's Saga: Hellblade 2 - An Unreal Engine 5 Masterpiece - The Ninja Theory Breakdown"</a>. E.g. they've mentioned a separate Houdini pipeline to extract transparency from static meshes. And while both games are different, both were developed by excellent engineering and visual teams.</p>
<p dir="auto">If you want to write your own implementation as a side project, then don't let me stop you. But unless you tackle simplification and error metric problems, you will end up with code similar to mine. You will still learn a lot.</p>
<p dir="auto">If you want to add this tech to the existing engine, I'm not a person you should be asking (I don't work in the industry). In my opinion, you should start by implementing <a href="https://ubm-twvideo01.s3.amazonaws.com/o1/vault/gdc2016/Presentations/Wihlidal_Graham_OptimizingTheGraphics.pdf" rel="nofollow">"Optimizing the Graphics Pipeline with Compute"</a> and <a href="https://advances.realtimerendering.com/s2015/aaltonenhaar_siggraph2015_combined_final_footer_220dpi.pdf" rel="nofollow">"GPU-Driven Rendering Pipelines"</a> first. This is already quite a complex task. Multi-step culling is tricky. You have to handle scene and world chunk management. Animated meshes. And that's just the beginning. But with these incremental steps, you will have something that works and can be tested at every step of the transition. Once this is stable, you can try a software rasterizer. Even if you don't end up shipping it, there is a lot to learn. Depending on the codebase, it can be surprisingly easy to add. Only after you have done the above steps you should try adding Nanite-like tech. As the various sliders in my app can tell you, they are all required for Nanite to be performant. The basic meshlet hierarchy for a toy renderer is a weekend project. Real implementation will have to deal with simplification and error metric issues.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Is per-meshlet backface cone culling worth it?</h3><a id="user-content-is-per-meshlet-backface-cone-culling-worth-it" aria-label="Permalink: Is per-meshlet backface cone culling worth it?" href="#is-per-meshlet-backface-cone-culling-worth-it"></a></p>
<p dir="auto">I've implemented the basics, but the gains are limited. Check the comment in <a href="https://github.com/Scthe/nanite-webgpu/blob/8c15e85b32d8b890ef573f58f1fbb782544f972c/src/constants.ts#L160">constants.ts</a> for implementation details.</p>
<ol dir="auto">
<li>It works best if you have a dense mesh where all triangles in a cluster have similar normals. Dense meshes are something that Nanite was designed for. Yet coarse LOD levels will have normals pointing in different directions. Arseny Kapoulkine had <a href="https://zeux.io/2023/04/28/triangle-backface-culling/#estimating-culling-efficiency" rel="nofollow">similar observations</a>.</li>
<li>There is some duplication with occlusion culling. Backfaces are behind front faces in the z-buffer.</li>
<li>Computing the cone is done on a per-meshlet level. For me, this means a WebAssembly call every time. This took 30% of the whole preprocessing step. Preprocessing all models offline would solve this problem. Yet it goes against my goals for this project. I want you to take the simplest possible 3D object format and see that my program works. That's why this app is a webpage and not Rust+Vulkan. No one would have cloned the repo to run the code. But everyone has clicked the demo links above (right?).</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Honourable mentions</h2><a id="user-content-honourable-mentions" aria-label="Permalink: Honourable mentions" href="#honourable-mentions"></a></p>
<ul dir="auto">
<li>Arseny Kapoulkine. This app is only possible due to <a href="https://github.com/zeux/meshoptimizer">meshoptimizer</a>. I've also watched a few of niagara videos and read its source code. And read his blog.
<ul dir="auto">
<li><a href="https://github.com/zeux/meshoptimizer/pull/704" data-hovercard-type="pull_request" data-hovercard-url="/zeux/meshoptimizer/pull/704/hovercard">Newer meshoptimizer versions</a> have <code>meshopt_SimplifySparse</code> specifically for Nanite clones. I've not updated to this version as I am moving on from this app. I want to leave it in a state that I've tested during development.</li>
</ul>
</li>
<li>Folks from Epic Games. Not only for creating Nanite but also for being open on how it works under the hood.</li>
<li><a href="https://ubm-twvideo01.s3.amazonaws.com/o1/vault/gdc2016/Presentations/Wihlidal_Graham_OptimizingTheGraphics.pdf" rel="nofollow">"Optimizing the Graphics Pipeline with Compute"</a> by Graham Wihlidal.</li>
<li><a href="https://advances.realtimerendering.com/s2015/aaltonenhaar_siggraph2015_combined_final_footer_220dpi.pdf" rel="nofollow">"GPU-Driven Rendering Pipelines"</a> by Ulrich Haar and Sebastian Aaltonen.</li>
<li><a href="http://glaros.dtc.umn.edu/gkhome/metis/metis/overview" rel="nofollow">METIS</a>.</li>
<li><a href="https://emscripten.org/" rel="nofollow">Emscripten</a>. Used to run both meshoptimizer and METIS in the browser.
<ul dir="auto">
<li><a href="https://marcoselvatici.github.io/WASM_tutorial/" rel="nofollow">Webassembly Tutorial</a> by Marco Selvatici.</li>
</ul>
</li>
<li><a href="https://sketchfab.com/3d-models/arcane-jinx-b74f25a5ee6e43efbe9766b9fbebc705" rel="nofollow">Arcane - Jinx</a> 3D model by sketchfab user <a href="https://sketchfab.com/rizky08" rel="nofollow">Craft Tama</a>. Used under <a href="https://creativecommons.org/licenses/by/4.0/" rel="nofollow">CC Attribution</a> license.
<ul dir="auto">
<li>I've merged the textures, adjusted UVs, and removed the weapon.</li>
</ul>
</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Phind-405B and faster, high quality AI answers for everyone (134 pts)]]></title>
            <link>https://www.phind.com/blog/introducing-phind-405b-and-better-faster-searches</link>
            <guid>41458083</guid>
            <pubDate>Thu, 05 Sep 2024 16:22:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.phind.com/blog/introducing-phind-405b-and-better-faster-searches">https://www.phind.com/blog/introducing-phind-405b-and-better-faster-searches</a>, See on <a href="https://news.ycombinator.com/item?id=41458083">Hacker News</a></p>
Couldn't get https://www.phind.com/blog/introducing-phind-405b-and-better-faster-searches: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: AnythingLLM – Open-Source, All-in-One Desktop AI Assistant (168 pts)]]></title>
            <link>https://github.com/Mintplex-Labs/anything-llm</link>
            <guid>41457633</guid>
            <pubDate>Thu, 05 Sep 2024 15:40:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Mintplex-Labs/anything-llm">https://github.com/Mintplex-Labs/anything-llm</a>, See on <a href="https://news.ycombinator.com/item?id=41457633">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">
  <a href="https://anythingllm.com/" rel="nofollow"><img src="https://github.com/Mintplex-Labs/anything-llm/raw/master/images/wordmark.png?raw=true" alt="AnythingLLM logo"></a>
</p>
<p><a href="https://trendshift.io/repositories/2415" rel="nofollow"><img src="https://camo.githubusercontent.com/13a0218d5bc383ce61ac2154a48e40d6ce0c079fe6c930524732238d033b7a72/68747470733a2f2f7472656e6473686966742e696f2f6170692f62616467652f7265706f7369746f726965732f32343135" alt="Mintplex-Labs%2Fanything-llm | Trendshift" width="250" height="55" data-canonical-src="https://trendshift.io/api/badge/repositories/2415"></a>
</p>
<p dir="auto">
    <b>AnythingLLM:</b> The all-in-one AI app you were looking for.<br>
    Chat with your docs, use AI Agents, hyper-configurable, multi-user, &amp; no frustrating set up required.
</p>
<p dir="auto">
  <a href="https://discord.gg/6UyHPeGZAC" rel="nofollow">
      <img src="https://camo.githubusercontent.com/3b816ca02eba9f9b0e63fe98bdfff52b93212792c74d87757fabdf67c42e9e36/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d6d696e74706c65785f6c6162732d626c75652e7376673f7374796c653d666c6174266c6f676f3d646174613a696d6167652f706e673b6261736536342c6956424f5277304b47676f414141414e5355684555674141414341414141416743414d4141414245704972474141414149474e49556b304141486f6d41414341684141412b6741414149446f414142314d414141366d41414144715941414158634a79365554774141414831554578555251414141502f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f72362b75626e352b3775372f332b2f7633392f656e7136757271362f76372b393766333972623236656f715431425130704f54342b526b757a7337636e4b796b5a4b53304e4853486c3866647a6433656a6f365578505555424452647a63335277674968386a4a53416b4a6d3578637648783861616e714234694a46425456657a7437563568596c4a5656754c6a343370396669496d4b434d6e4b5a4b556c61616f7153456c4a32317763665430394f337537757672367a45304e72362f774355704b3571636e66372b2f6e68376645644b5448782b66307450554f546c3561697071696f754d477475627a3543524451344f7354477875666e35313568593761337548312f6758427964494f46686c5659577658323971616f7143516f4b7337507a2f507a38372f417755744f554e665932644852306d6872624f7672374535525579387a4e585232642f6633392b586c35555a4a53783068497a51334f647261322f7a382f476c736261476a7045524853657a73374c2f42775363724c5451344f646e61327a4d334f626d377533782f674b536d70396a5a3254314151752f7637317064586b56495372322b767967734c69496e4b54673750614f6c706973764d635847787a6b38506c646158504c793875377537726d36753753317473444277766a342b4d5045786265347565586d35732f51304b7966376577414141416f64464a4f5577414142436c73724e6a782f514d326c392f376c686d49366a54422f6b413147674b4a4e2b6e65613676792f4d4c5a515965564b4b3372564135744141414141574a4c523051422f774974336741414141643053553146422b634b4241416d4d5a42486a584941414149535355524256446a4c59324341416b596d5a685a574e6e594f446e593256685a6d4a6b5947564d44497963584e77367342426277386646796379456f5947666b4642445651674b414150794d6a516c35495745514444596749433846554d444b4b736d6c67415779694542574d6a474a5935594571784d4171474d57464e58414159584767416b594a5351326351464b436b59465253687133416d6b705267594a62676862553074624230547236756b626747684449313067795366427743774455574273596d706d447151744c4b327362545130624f3373485941384757594757576a34575473364f627534616d69344f546d3765786871654870352b344443564a5a42446d7164723775666e332b41726b5a676b4a2b6655334349526d67595746694f41525947766f354f5155486845554146546b462b6b564852734c42676b496579596d4c6a776f4f6334684d536b354a546e494e53303644433867776345455a3652715a476c704f6663335a4f626c352b675a2b5452324552574679425151464d4635656b6c6d7155705162352b52655536315a554f766b465656585851425341726169747132396f3147694b63664c7a63323975306d6a78427a71307451306b777735785a48744855476558686b5a6864784259675a3464304c49366334676a7764377369515172614f703141697651364375414b5a43444242525151514e516755622f4247663363714343695a4f636e4365335151494b484e5254706b36624467705a6a526b7a673370425154427264744363755a43676c7541443076506d4c316749647653697855755767714e7332594a2b4455686b4559787567676b476d4f515563636b72696f50544a434f58456e5a354a533559736c62476e75795645526c44444676474555504f5776777161483652566b484b6575444d4b36534b6e486c5668546778386a65546d71793645696a374b366e4c71694779507743687361314d55726e713177414141435630525668305a4746305a54706a636d5668644755414d6a41794d7930784d4330774e4651774d446f7a4f446f304f5373774d446f774d423956306138414141416c6445565964475268644755366257396b61575a35414449774d6a4d744d5441744d4452554d4441364d7a67364e446b724d4441364d44427543476b54414141414b4852465748526b5958526c4f6e52706257567a64474674634141794d44497a4c5445774c544130564441774f6a4d344f6a51354b7a41774f6a41774f5231497a4141414141424a52553545726b4a6767673d3d" alt="Discord" data-canonical-src="https://img.shields.io/badge/chat-mintplex_labs-blue.svg?style=flat&amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAMAAABEpIrGAAAAIGNIUk0AAHomAACAhAAA+gAAAIDoAAB1MAAA6mAAADqYAAAXcJy6UTwAAAH1UExURQAAAP////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////r6+ubn5+7u7/3+/v39/enq6urq6/v7+97f39rb26eoqT1BQ0pOT4+Rkuzs7cnKykZKS0NHSHl8fdzd3ejo6UxPUUBDRdzc3RwgIh8jJSAkJm5xcvHx8aanqB4iJFBTVezt7V5hYlJVVuLj43p9fiImKCMnKZKUlaaoqSElJ21wcfT09O3u7uvr6zE0Nr6/wCUpK5qcnf7+/nh7fEdKTHx+f0tPUOTl5aipqiouMGtubz5CRDQ4OsTGxufn515hY7a3uH1/gXBydIOFhlVYWvX29qaoqCQoKs7Pz/Pz87/AwUtOUNfY2dHR0mhrbOvr7E5RUy8zNXR2d/f39+Xl5UZJSx0hIzQ3Odra2/z8/GlsbaGjpERHSezs7L/BwScrLTQ4Odna2zM3Obm7u3x/gKSmp9jZ2T1AQu/v71pdXkVISr2+vygsLiInKTg7PaOlpisvMcXGxzk8PldaXPLy8u7u7rm6u7S1tsDBwvj4+MPExbe4ueXm5s/Q0Kyf7ewAAAAodFJOUwAABClsrNjx/QM2l9/7lhmI6jTB/kA1GgKJN+nea6vy/MLZQYeVKK3rVA5tAAAAAWJLR0QB/wIt3gAAAAd0SU1FB+cKBAAmMZBHjXIAAAISSURBVDjLY2CAAkYmZhZWNnYODnY2VhZmJkYGVMDIycXNw6sBBbw8fFycyEoYGfkFBDVQgKAAPyMjQl5IWEQDDYgIC8FUMDKKsmlgAWyiEBWMjGJY5YEqxMAqGMWFNXAAYXGgAkYJSQ2cQFKCkYFRShq3AmkpRgYJbghbU0tbB0Tr6ukbgGhDI10gySfBwCwDUWBsYmpmDqQtLK2sbTQ0bO3sHYA8GWYGWWj4WTs6Obu4ami4OTm7exhqeHp5+4DCVJZBDmqdr7ufn3+ArkZgkJ+fU3CIRmgYWFiOARYGvo5OQUHhEUAFTkF+kVHRsLBgkIeyYmLjwoOc4hMSk5JTnINS06DC8gwcEEZ6RqZGlpOfc3ZObl5+gZ+TR2ERWFyBQQFMF5eklmqUpQb5+ReU61ZUOvkFVVXXQBSAraitq29o1GiKcfLzc29u0mjxBzq0tQ0kww5xZHtHUGeXhkZhdxBYgZ4d0LI6c4gjwd7siQQraOp1AivQ6CuAKZCDBBRQQQNQgUb/BGf3cqCCiZOcnCe3QQIKHNRTpk6bDgpZjRkzg3pBQTBrdtCcuZCgluAD0vPmL1gIdvSixUuWgqNs2YJ+DUhkEYxuggkGmOQUcckrioPTJCOXEnZ5JS5YslbGnuyVERlDDFvGEUPOWvwqaH6RVkHKeuDMK6SKnHlVhTgx8jeTmqy6Eij7K6nLqiGyPwChsa1MUrnq1wAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMy0xMC0wNFQwMDozODo0OSswMDowMB9V0a8AAAAldEVYdGRhdGU6bW9kaWZ5ADIwMjMtMTAtMDRUMDA6Mzg6NDkrMDA6MDBuCGkTAAAAKHRFWHRkYXRlOnRpbWVzdGFtcAAyMDIzLTEwLTA0VDAwOjM4OjQ5KzAwOjAwOR1IzAAAAABJRU5ErkJggg==">
  </a> |
  <a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/LICENSE">
      <img src="https://camo.githubusercontent.com/b4357f651d9e266e4b3854471ec55091f13a9f067f8b0144037ef6ab46239a66/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d6c6963656e7365266d6573736167653d4d495426636f6c6f723d7768697465" alt="License" data-canonical-src="https://img.shields.io/static/v1?label=license&amp;message=MIT&amp;color=white">
  </a> |
  <a href="https://docs.anythingllm.com/" rel="nofollow">
    Docs
  </a> |
   <a href="https://my.mintplexlabs.com/aio-checkout?product=anythingllm" rel="nofollow">
    Hosted Instance
  </a>
</p>
<p dir="auto">
  <b>English</b> · <a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/locales/README.zh-CN.md">简体中文</a> · <a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/locales/README.ja-JP.md">日本語</a>
</p>
<p dir="auto">
👉 AnythingLLM for desktop (Mac, Windows, &amp; Linux)! <a href="https://anythingllm.com/download" rel="nofollow"> Download Now</a>
</p>
<p dir="auto">A full-stack application that enables you to turn any document, resource, or piece of content into context that any LLM can use as references during chatting. This application allows you to pick and choose which LLM or Vector Database you want to use as well as supporting multi-user management and permissions.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/16845892/294273127-cfc5f47c-bd91-4067-986c-f3f49621a859.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDIsIm5iZiI6MTcyNTU2NDYwMiwicGF0aCI6Ii8xNjg0NTg5Mi8yOTQyNzMxMjctY2ZjNWY0N2MtYmQ5MS00MDY3LTk4NmMtZjNmNDk2MjFhODU5LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDE5MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTYyOTRlZDk0MGM3NThmMTFjMGQwZjY3NjFkYzQyZTQzZjY1ZjM5ODY2ZmRjMGRkMzk2ZTNlNjIwOTU5MTg4MjkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.702OhOJH1eZoGn3wHkT3cpc8NgWnuhjAtLNyV5-8ztA"><img src="https://private-user-images.githubusercontent.com/16845892/294273127-cfc5f47c-bd91-4067-986c-f3f49621a859.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDIsIm5iZiI6MTcyNTU2NDYwMiwicGF0aCI6Ii8xNjg0NTg5Mi8yOTQyNzMxMjctY2ZjNWY0N2MtYmQ5MS00MDY3LTk4NmMtZjNmNDk2MjFhODU5LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDE5MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTYyOTRlZDk0MGM3NThmMTFjMGQwZjY3NjFkYzQyZTQzZjY1ZjM5ODY2ZmRjMGRkMzk2ZTNlNjIwOTU5MTg4MjkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.702OhOJH1eZoGn3wHkT3cpc8NgWnuhjAtLNyV5-8ztA" alt="Chatting" data-animated-image=""></a></p>
<details>
<summary><kbd>Watch the demo!</kbd></summary>
<p dir="auto"><a href="https://youtu.be/f95rGD9trL0" rel="nofollow"><img src="https://github.com/Mintplex-Labs/anything-llm/raw/master/images/youtube.png" alt="Watch the video"></a></p>
</details>
<p dir="auto"><h3 tabindex="-1" dir="auto">Product Overview</h3><a id="user-content-product-overview" aria-label="Permalink: Product Overview" href="#product-overview"></a></p>
<p dir="auto">AnythingLLM is a full-stack application where you can use commercial off-the-shelf LLMs or popular open source LLMs and vectorDB solutions to build a private ChatGPT with no compromises that you can run locally as well as host remotely and be able to chat intelligently with any documents you provide it.</p>
<p dir="auto">AnythingLLM divides your documents into objects called <code>workspaces</code>. A Workspace functions a lot like a thread, but with the addition of containerization of your documents. Workspaces can share documents, but they do not talk to each other so you can keep your context for each workspace clean.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cool features of AnythingLLM</h2><a id="user-content-cool-features-of-anythingllm" aria-label="Permalink: Cool features of AnythingLLM" href="#cool-features-of-anythingllm"></a></p>
<ul dir="auto">
<li>🆕 <strong>Multi-modal support (both closed and open-source LLMs!)</strong></li>
<li>👤 Multi-user instance support and permissioning <em>Docker version only</em></li>
<li>🦾 Agents inside your workspace (browse the web, run code, etc)</li>
<li>💬 <a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/embed/README.md">Custom Embeddable Chat widget for your website</a> <em>Docker version only</em></li>
<li>📖 Multiple document type support (PDF, TXT, DOCX, etc)</li>
<li>Simple chat UI with Drag-n-Drop funcitonality and clear citations.</li>
<li>100% Cloud deployment ready.</li>
<li>Works with all popular <a href="#supported-llms-embedder-models-speech-models-and-vector-databases">closed and open-source LLM providers</a>.</li>
<li>Built-in cost &amp; time-saving measures for managing very large documents compared to any other chat UI.</li>
<li>Full Developer API for custom integrations!</li>
<li>Much more...install and find out!</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Supported LLMs, Embedder Models, Speech models, and Vector Databases</h3><a id="user-content-supported-llms-embedder-models-speech-models-and-vector-databases" aria-label="Permalink: Supported LLMs, Embedder Models, Speech models, and Vector Databases" href="#supported-llms-embedder-models-speech-models-and-vector-databases"></a></p>
<p dir="auto"><strong>Large Language Models (LLMs):</strong></p>
<ul dir="auto">
<li><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/server/storage/models/README.md#text-generation-llm-selection">Any open-source llama.cpp compatible model</a></li>
<li><a href="https://openai.com/" rel="nofollow">OpenAI</a></li>
<li><a href="https://openai.com/" rel="nofollow">OpenAI (Generic)</a></li>
<li><a href="https://azure.microsoft.com/en-us/products/ai-services/openai-service" rel="nofollow">Azure OpenAI</a></li>
<li><a href="https://aws.amazon.com/bedrock/" rel="nofollow">AWS Bedrock</a></li>
<li><a href="https://www.anthropic.com/" rel="nofollow">Anthropic</a></li>
<li><a href="https://ai.google.dev/" rel="nofollow">Google Gemini Pro</a></li>
<li><a href="https://huggingface.co/" rel="nofollow">Hugging Face (chat models)</a></li>
<li><a href="https://ollama.ai/" rel="nofollow">Ollama (chat models)</a></li>
<li><a href="https://lmstudio.ai/" rel="nofollow">LM Studio (all models)</a></li>
<li><a href="https://localai.io/" rel="nofollow">LocalAi (all models)</a></li>
<li><a href="https://www.together.ai/" rel="nofollow">Together AI (chat models)</a></li>
<li><a href="https://www.perplexity.ai/" rel="nofollow">Perplexity (chat models)</a></li>
<li><a href="https://openrouter.ai/" rel="nofollow">OpenRouter (chat models)</a></li>
<li><a href="https://mistral.ai/" rel="nofollow">Mistral</a></li>
<li><a href="https://groq.com/" rel="nofollow">Groq</a></li>
<li><a href="https://cohere.com/" rel="nofollow">Cohere</a></li>
<li><a href="https://github.com/LostRuins/koboldcpp">KoboldCPP</a></li>
<li><a href="https://github.com/BerriAI/litellm">LiteLLM</a></li>
<li><a href="https://github.com/oobabooga/text-generation-webui">Text Generation Web UI</a></li>
</ul>
<p dir="auto"><strong>Embedder models:</strong></p>
<ul dir="auto">
<li><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/server/storage/models/README.md">AnythingLLM Native Embedder</a> (default)</li>
<li><a href="https://openai.com/" rel="nofollow">OpenAI</a></li>
<li><a href="https://azure.microsoft.com/en-us/products/ai-services/openai-service" rel="nofollow">Azure OpenAI</a></li>
<li><a href="https://localai.io/" rel="nofollow">LocalAi (all)</a></li>
<li><a href="https://ollama.ai/" rel="nofollow">Ollama (all)</a></li>
<li><a href="https://lmstudio.ai/" rel="nofollow">LM Studio (all)</a></li>
<li><a href="https://cohere.com/" rel="nofollow">Cohere</a></li>
</ul>
<p dir="auto"><strong>Audio Transcription models:</strong></p>
<ul dir="auto">
<li><a href="https://github.com/Mintplex-Labs/anything-llm/tree/master/server/storage/models#audiovideo-transcription">AnythingLLM Built-in</a> (default)</li>
<li><a href="https://openai.com/" rel="nofollow">OpenAI</a></li>
</ul>
<p dir="auto"><strong>TTS (text-to-speech) support:</strong></p>
<ul dir="auto">
<li>Native Browser Built-in (default)</li>
<li><a href="https://github.com/rhasspy/piper">PiperTTSLocal - runs in browser</a></li>
<li><a href="https://platform.openai.com/docs/guides/text-to-speech/voice-options" rel="nofollow">OpenAI TTS</a></li>
<li><a href="https://elevenlabs.io/" rel="nofollow">ElevenLabs</a></li>
</ul>
<p dir="auto"><strong>STT (speech-to-text) support:</strong></p>
<ul dir="auto">
<li>Native Browser Built-in (default)</li>
</ul>
<p dir="auto"><strong>Vector Databases:</strong></p>
<ul dir="auto">
<li><a href="https://github.com/lancedb/lancedb">LanceDB</a> (default)</li>
<li><a href="https://www.datastax.com/products/datastax-astra" rel="nofollow">Astra DB</a></li>
<li><a href="https://pinecone.io/" rel="nofollow">Pinecone</a></li>
<li><a href="https://trychroma.com/" rel="nofollow">Chroma</a></li>
<li><a href="https://weaviate.io/" rel="nofollow">Weaviate</a></li>
<li><a href="https://qdrant.tech/" rel="nofollow">Qdrant</a></li>
<li><a href="https://milvus.io/" rel="nofollow">Milvus</a></li>
<li><a href="https://zilliz.com/" rel="nofollow">Zilliz</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Technical Overview</h3><a id="user-content-technical-overview" aria-label="Permalink: Technical Overview" href="#technical-overview"></a></p>
<p dir="auto">This monorepo consists of three main sections:</p>
<ul dir="auto">
<li><code>frontend</code>: A viteJS + React frontend that you can run to easily create and manage all your content the LLM can use.</li>
<li><code>server</code>: A NodeJS express server to handle all the interactions and do all the vectorDB management and LLM interactions.</li>
<li><code>collector</code>: NodeJS express server that process and parses documents from the UI.</li>
<li><code>docker</code>: Docker instructions and build process + information for building from source.</li>
<li><code>embed</code>: Submodule for generation &amp; creation of the <a href="https://github.com/Mintplex-Labs/anythingllm-embed">web embed widget</a>.</li>
<li><code>browser-extension</code>: Submodule for the <a href="https://github.com/Mintplex-Labs/anythingllm-extension">chrome browser extension</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🛳 Self Hosting</h2><a id="user-content--self-hosting" aria-label="Permalink: 🛳 Self Hosting" href="#-self-hosting"></a></p>
<p dir="auto">Mintplex Labs &amp; the community maintain a number of deployment methods, scripts, and templates that you can use to run AnythingLLM locally. Refer to the table below to read how to deploy on your preferred environment or to automatically deploy.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Docker</th>
<th>AWS</th>
<th>GCP</th>
<th>Digital Ocean</th>
<th>Render.com</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/docker/HOW_TO_USE_DOCKER.md"><img src="https://github.com/Mintplex-Labs/anything-llm/raw/master/images/deployBtns/docker.png" alt="Deploy on Docker"></a></td>
<td><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/cloud-deployments/aws/cloudformation/DEPLOY.md"><img src="https://github.com/Mintplex-Labs/anything-llm/raw/master/images/deployBtns/aws.png" alt="Deploy on AWS"></a></td>
<td><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/cloud-deployments/gcp/deployment/DEPLOY.md"><img src="https://camo.githubusercontent.com/8e9acf4df0af19c7eb27d48d2bd9966f7311d8ae1fe4900f504b8d4eabb8d769/68747470733a2f2f6465706c6f792e636c6f75642e72756e2f627574746f6e2e737667" alt="Deploy on GCP" data-canonical-src="https://deploy.cloud.run/button.svg"></a></td>
<td><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/cloud-deployments/digitalocean/terraform/DEPLOY.md"><img src="https://camo.githubusercontent.com/e093a0ed531124a715aad44362848ca2cff28c3182c2c0ca4a70b2564b681f59/68747470733a2f2f7777772e6465706c6f79746f646f2e636f6d2f646f2d62746e2d626c75652e737667" alt="Deploy on DigitalOcean" data-canonical-src="https://www.deploytodo.com/do-btn-blue.svg"></a></td>
<td><a href="https://render.com/deploy?repo=https://github.com/Mintplex-Labs/anything-llm&amp;branch=render" rel="nofollow"><img src="https://camo.githubusercontent.com/a103822afe1d58c7da6beafbc0c65bb7b8d622dd193dded1b45b3c0ad6466d82/68747470733a2f2f72656e6465722e636f6d2f696d616765732f6465706c6f792d746f2d72656e6465722d627574746f6e2e737667" alt="Deploy on Render.com" data-canonical-src="https://render.com/images/deploy-to-render-button.svg"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Railway</th>
<th>RepoCloud</th>
<th>Elestio</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://railway.app/template/HNSCS1?referralCode=WFgJkn" rel="nofollow"><img src="https://camo.githubusercontent.com/e4002051668809c220b10ad92ddd6fb87f365d8cd4ff470e0aeca3bc5b05450e/68747470733a2f2f7261696c7761792e6170702f627574746f6e2e737667" alt="Deploy on Railway" data-canonical-src="https://railway.app/button.svg"></a></td>
<td><a href="https://repocloud.io/details/?app_id=276" rel="nofollow"><img src="https://camo.githubusercontent.com/ac294f4b769f6436dadb17205434b234b32e4d88831f182c522fd36b4534a7a3/68747470733a2f2f64313674307063343834367835322e636c6f756466726f6e742e6e65742f6465706c6f796c6f62652e737667" alt="Deploy on RepoCloud" data-canonical-src="https://d16t0pc4846x52.cloudfront.net/deploylobe.svg"></a></td>
<td><a href="https://elest.io/open-source/anythingllm" rel="nofollow"><img src="https://camo.githubusercontent.com/76131e33a65d9a0728265791dcf78030580a9932466d139e5ae38f87f926c5b5/68747470733a2f2f656c6573742e696f2f696d616765732f6c6f676f732f6465706c6f792d746f2d656c657374696f2d62746e2e706e67" alt="Deploy on Elestio" data-canonical-src="https://elest.io/images/logos/deploy-to-elestio-btn.png"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/BARE_METAL.md">or set up a production AnythingLLM instance without Docker →</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to setup for development</h2><a id="user-content-how-to-setup-for-development" aria-label="Permalink: How to setup for development" href="#how-to-setup-for-development"></a></p>
<ul dir="auto">
<li><code>yarn setup</code> To fill in the required <code>.env</code> files you'll need in each of the application sections (from root of repo).
<ul dir="auto">
<li>Go fill those out before proceeding. Ensure <code>server/.env.development</code> is filled or else things won't work right.</li>
</ul>
</li>
<li><code>yarn dev:server</code> To boot the server locally (from root of repo).</li>
<li><code>yarn dev:frontend</code> To boot the frontend locally (from root of repo).</li>
<li><code>yarn dev:collector</code> To then run the document collector (from root of repo).</li>
</ul>
<p dir="auto"><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/server/storage/documents/DOCUMENTS.md">Learn about documents</a></p>
<p dir="auto"><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/server/storage/vector-cache/VECTOR_CACHE.md">Learn about vector caching</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Telemetry &amp; Privacy</h2><a id="user-content-telemetry--privacy" aria-label="Permalink: Telemetry &amp; Privacy" href="#telemetry--privacy"></a></p>
<p dir="auto">AnythingLLM by Mintplex Labs Inc contains a telemetry feature that collects anonymous usage information.</p>
<details>
<summary><kbd>More about Telemetry &amp; Privacy for AnythingLLM</kbd></summary>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why?</h3><a id="user-content-why" aria-label="Permalink: Why?" href="#why"></a></p>
<p dir="auto">We use this information to help us understand how AnythingLLM is used, to help us prioritize work on new features and bug fixes, and to help us improve AnythingLLM's performance and stability.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Opting out</h3><a id="user-content-opting-out" aria-label="Permalink: Opting out" href="#opting-out"></a></p>
<p dir="auto">Set <code>DISABLE_TELEMETRY</code> in your server or docker .env settings to "true" to opt out of telemetry. You can also do this in-app by going to the sidebar &gt; <code>Privacy</code> and disabling telemetry.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What do you explicitly track?</h3><a id="user-content-what-do-you-explicitly-track" aria-label="Permalink: What do you explicitly track?" href="#what-do-you-explicitly-track"></a></p>
<p dir="auto">We will only track usage details that help us make product and roadmap decisions, specifically:</p>
<ul dir="auto">
<li>Typ of your installation (Docker or Desktop)</li>
<li>When a document is added or removed. No information <em>about</em> the document. Just that the event occurred. This gives us an idea of use.</li>
<li>Type of vector database in use. Let's us know which vector database provider is the most used to prioritize changes when updates arrive for that provider.</li>
<li>Type of LLM in use. Let's us know the most popular choice and prioritize changes when updates arrive for that provider.</li>
<li>Chat is sent. This is the most regular "event" and gives us an idea of the daily-activity of this project across all installations. Again, only the event is sent - we have no information on the nature or content of the chat itself.</li>
</ul>
<p dir="auto">You can verify these claims by finding all locations <code>Telemetry.sendTelemetry</code> is called. Additionally these events are written to the output log so you can also see the specific data which was sent - if enabled. No IP or other identifying information is collected. The Telemetry provider is <a href="https://posthog.com/" rel="nofollow">PostHog</a> - an open-source telemetry collection service.</p>
<p dir="auto"><a href="https://github.com/search?q=repo%3AMintplex-Labs%2Fanything-llm%20.sendTelemetry(&amp;type=code">View all telemetry events in source code</a></p>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">👋 Contributing</h2><a id="user-content--contributing" aria-label="Permalink: 👋 Contributing" href="#-contributing"></a></p>
<ul dir="auto">
<li>create issue</li>
<li>create PR with branch name format of <code>&lt;issue number&gt;-&lt;short name&gt;</code></li>
<li>LGTM from core-team</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🌟 Contributors</h2><a id="user-content--contributors" aria-label="Permalink: 🌟 Contributors" href="#-contributors"></a></p>
<p dir="auto"><a href="https://github.com/mintplex-labs/anything-llm/graphs/contributors"><img src="https://camo.githubusercontent.com/216243cb397375babf4a9b21f6a6968b7589f578ff1daa0db6ae56a33ca4997a/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d6d696e74706c65782d6c6162732f616e797468696e672d6c6c6d" alt="anythingllm contributors" data-canonical-src="https://contrib.rocks/image?repo=mintplex-labs/anything-llm"></a></p>
<p dir="auto"><a href="https://star-history.com/#mintplex-labs/anything-llm&amp;Date" rel="nofollow"><img src="https://camo.githubusercontent.com/455256132c080bc5bbf5419d1c14fa4d1a5727d75c41c4df0bc8e8fe7dbccb63/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6d696e74706c65782d6c6162732f616e797468696e672d6c6c6d26747970653d54696d656c696e65" alt="Star History Chart" data-canonical-src="https://api.star-history.com/svg?repos=mintplex-labs/anything-llm&amp;type=Timeline"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔗 More Products</h2><a id="user-content--more-products" aria-label="Permalink: 🔗 More Products" href="#-more-products"></a></p>
<ul dir="auto">
<li><strong><a href="https://github.com/mintplex-labs/vector-admin">VectorAdmin</a>:</strong> An all-in-one GUI &amp; tool-suite for managing vector databases.</li>
<li><strong><a href="https://github.com/Mintplex-Labs/openai-assistant-swarm">OpenAI Assistant Swarm</a>:</strong> Turn your entire library of OpenAI assistants into one single army commanded from a single agent.</li>
</ul>
<p dir="auto"><a href="#readme-top"><img src="https://camo.githubusercontent.com/d658b6c3935e61bd4aab9a571190fc3c48cbafc89fe6b16250c0cba28ed73234/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d4241434b5f544f5f544f502d3232323632383f7374796c653d666c61742d737175617265" alt="" data-canonical-src="https://img.shields.io/badge/-BACK_TO_TOP-222628?style=flat-square"></a></p>
<hr>
<p dir="auto">Copyright © 2024 <a href="https://github.com/mintplex-labs">Mintplex Labs</a>. <br>
This project is <a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/LICENSE">MIT</a> licensed.</p>

</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AlphaProteo generates novel proteins for biology and health research (198 pts)]]></title>
            <link>https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/</link>
            <guid>41457331</guid>
            <pubDate>Thu, 05 Sep 2024 15:05:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/">https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/</a>, See on <a href="https://news.ycombinator.com/item?id=41457331">Hacker News</a></p>
Couldn't get https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Maitai (YC S24) – Self-Optimizing LLM Platform (104 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=41456552</link>
            <guid>41456552</guid>
            <pubDate>Thu, 05 Sep 2024 13:42:43 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=41456552">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="41456552">
      <td><span></span></td>      <td><center><a id="up_41456552" href="https://news.ycombinator.com/vote?id=41456552&amp;how=up&amp;goto=item%3Fid%3D41456552"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=41456552">Launch HN: Maitai (YC S24) – Self-Optimizing LLM Platform</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_41456552">104 points</span> by <a href="https://news.ycombinator.com/user?id=cmdalsanto">cmdalsanto</a> <span title="2024-09-05T13:42:43.000000Z"><a href="https://news.ycombinator.com/item?id=41456552">8 hours ago</a></span> <span id="unv_41456552"></span> | <a href="https://news.ycombinator.com/hide?id=41456552&amp;goto=item%3Fid%3D41456552">hide</a> | <a href="https://hn.algolia.com/?query=Launch%20HN%3A%20Maitai%20%28YC%20S24%29%20%E2%80%93%20Self-Optimizing%20LLM%20Platform&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=41456552&amp;auth=5b3d028aa07cccfc7151d9d6f9e80c752e36b1c3">favorite</a> | <a href="https://news.ycombinator.com/item?id=41456552">50&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>Hey HN - this is Christian and Ian from Maitai (<a href="https://trymaitai.ai/">https://trymaitai.ai</a>). We're building an LLM platform that optimizes request routing, autocorrects bad responses, and automatically fine-tunes new application-specific models with incremental improvements. Here’s a demo video:  <a href="https://www.loom.com/share/a2cd9192359840cab5274ccba399bd87?sid=7097fd84-ea85-42cd-9616-84abc1087a56" rel="nofollow">https://www.loom.com/share/a2cd9192359840cab5274ccba399bd87?...</a>.</p><p>If you want to try it out, we built a game (<a href="https://maitaistreasure.com/" rel="nofollow">https://maitaistreasure.com</a>) to show how our real-time autocorrections work with mission-critical expectations (like never giving financial advice). Try and coax the bot to give you the secret phrase in its system prompt. If you're the first to crack it, you can email us the phrase and win a bounty. Maitai is used to make sure the bot always adheres to our expectations, and thus never gives up the secret phrase.</p><p>We built Maitai because getting an LLM app into production and maintaining it is a slog. Teams spend most of their time on LLM reliability rather than their main product. We experienced this ourselves at our previous jobs deploying AI-enabled applications for Presto—the vast majority of time was making sure the model did what we wanted it to do.</p><p>For example, one of our customers builds AI ordering agents for restaurants. It's crucial that their LLMs return results in a predictable, consistent manner throughout the conversation. If not, it leads to a poor guest experience and a staff member may intervene. At the end of the order conversation, they need to ensure that the order cart matches what the customer requested before it's submitted to the Point of Sale system. It's common for a human-in-the-loop to review critical pieces of information like this, but it’s costly to set up such a pipeline and it’s difficult to scale. When it's time to send out a receipt and payment link, they must first get the customer's consent to receive text messages, else they risk fines for violating the Telephone Consumer Protection Act. To boot, getting from 0 to 1 usually relies on inefficient general-purpose models that aren't viable at any sort of scale beyond proof of concept.</p><p>Since reliability is the #1 thing hindering the adoption of LLMs in production, we decided to help change that. Here's how it works:</p><p>1. Maitai sits between the client and the LLMs as a super lightweight proxy, analyzing traffic to automatically build a robust set of expectations for how the LLM should respond.</p><p>2. The application sends a request to Maitai, and Maitai forwards it to the appropriate LLM (user specified, but we'll preemptively fallback to a similar model if we notice issues with the primary model).</p><p>3. We intercept the response from the LLM, and evaluate it against the expectations we had previously built.</p><p>4. If we notice that an expectation was not met, we surface a fault (Slack, webhook) and can, optionally, substitute the faulty response with a clean response to be sent back to the client. This check and correction adds about 250ms on average right now, and we're working on making it faster.</p><p>5. We use all of the data from evaluating model responses to fine-tune application-specific models. We're working on automating this step for passive incremental improvements. We'd like to get it to a point where our user's inference step just gets better, faster, and cheaper over time without them having to do anything.</p><p>Our hope is that we take on the reliability and resiliency problems of the LLMs for our customers, and make it so they can focus on domain specific problems instead.</p><p>We're self-serve (<a href="https://portal.trymaitai.ai/">https://portal.trymaitai.ai</a>), and have both Python and Node SDKs that mock OpenAI's for quick integration. Users can set their preferences for primary and secondary (fallback) models in our Portal, or in code. Right now, the expectations we use for real-time evaluations are automatically generated, but we manually go through and do some pruning before enabling them. Fine-tuning is all done manually for now.</p><p>We charge for platform usage, plus a monthly application fee. Customers can bring their own LLM provider API keys, or use ours and pay at-cost for what they use. We have contracts with most of our current customers, so we are still trying to figure out what's right for our pay-as-you-go plan.</p><p>We securely store requests and responses that go through Maitai, as well as derivative data such as evaluation results. This information is used for fine-tuning models, accessible only by the organization the data belongs to. Data is never shared between our users.  API keys we manage on behalf of our customers are only injected before sending to the LLM provider, and never leave our servers otherwise. We're working on SOC2 and HIPAA compliance, as well as a self-hosted solution for companies with extremely sensitive data privacy requirements.</p><p>We’d love to get your feedback on what we’re building, or hear about your experience building around LLMs!</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Hacker League – Open-Source Rocket League on Linux (166 pts)]]></title>
            <link>https://github.com/moritztng/hacker-league</link>
            <guid>41456411</guid>
            <pubDate>Thu, 05 Sep 2024 13:24:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/moritztng/hacker-league">https://github.com/moritztng/hacker-league</a>, See on <a href="https://news.ycombinator.com/item?id=41456411">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><details open="">
  <summary>
    
    <span aria-label="Video description hacker-league.mp4">hacker-league.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19519902/364779763-3a630d46-ec17-4da8-8879-76320ea563fe.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NTA1MDEsIm5iZiI6MTcyNTU1MDIwMSwicGF0aCI6Ii8xOTUxOTkwMi8zNjQ3Nzk3NjMtM2E2MzBkNDYtZWMxNy00ZGE4LTg4NzktNzYzMjBlYTU2M2ZlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDE1MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTNkMDg3MjdiZmFkMTZiODk4YTJiYzBkYzc4MTk4ZGNmOTJjZjYxMjY3NzI1MGMzOTFjNTg1Y2YxNWVkOWE1OWYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.C7QQg92wm3q0awLIIoj6I8rQuGAJsOeWGMyqE9soFcQ" data-canonical-src="https://private-user-images.githubusercontent.com/19519902/364779763-3a630d46-ec17-4da8-8879-76320ea563fe.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NTA1MDEsIm5iZiI6MTcyNTU1MDIwMSwicGF0aCI6Ii8xOTUxOTkwMi8zNjQ3Nzk3NjMtM2E2MzBkNDYtZWMxNy00ZGE4LTg4NzktNzYzMjBlYTU2M2ZlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDE1MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTNkMDg3MjdiZmFkMTZiODk4YTJiYzBkYzc4MTk4ZGNmOTJjZjYxMjY3NzI1MGMzOTFjNTg1Y2YxNWVkOWE1OWYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.C7QQg92wm3q0awLIIoj6I8rQuGAJsOeWGMyqE9soFcQ" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Install</h2><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<p dir="auto">Currently only debian based distros with x86_64. Please help me build it on other platforms. If you have an external GPU, make sure the drivers are installed</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt install curl &amp;&amp; curl -sL https://raw.githubusercontent.com/moritztng/hacker-league/main/install.sh | bash"><pre>sudo apt install curl <span>&amp;&amp;</span> curl -sL https://raw.githubusercontent.com/moritztng/hacker-league/main/install.sh <span>|</span> bash</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Play</h2><a id="user-content-play" aria-label="Permalink: Play" href="#play"></a></p>
<p dir="auto">Use a gamepad for maximum fun</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd hacker-league
./hacker-league"><pre><span>cd</span> hacker-league
./hacker-league</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Build from source</h2><a id="user-content-build-from-source" aria-label="Permalink: Build from source" href="#build-from-source"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/moritztng/hacker-league.git
cd hacker-league
sudo apt install libvulkan-dev vulkan-validationlayers-dev spirv-tools libglfw3-dev libglm-dev libeigen3-dev vim-common xxd g++ make
curl -L -o ./shaders/glslc https://github.com/moritztng/hacker-league/releases/download/glslc/glslc
chmod +x ./shaders/glslc
make debug
curl -L -o &quot;gamepad.txt&quot; https://raw.githubusercontent.com/mdqinc/SDL_GameControllerDB/master/gamecontrollerdb.txt"><pre>git clone https://github.com/moritztng/hacker-league.git
<span>cd</span> hacker-league
sudo apt install libvulkan-dev vulkan-validationlayers-dev spirv-tools libglfw3-dev libglm-dev libeigen3-dev vim-common xxd g++ make
curl -L -o ./shaders/glslc https://github.com/moritztng/hacker-league/releases/download/glslc/glslc
chmod +x ./shaders/glslc
make debug
curl -L -o <span><span>"</span>gamepad.txt<span>"</span></span> https://raw.githubusercontent.com/mdqinc/SDL_GameControllerDB/master/gamecontrollerdb.txt</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Community</h2><a id="user-content-community" aria-label="Permalink: Community" href="#community"></a></p>
<ul dir="auto">
<li>Discord Server: <a href="https://discord.gg/BbNH27st" rel="nofollow">https://discord.gg/BbNH27st</a></li>
<li>I build in public on X: <a href="https://x.com/moritzthuening" rel="nofollow">https://x.com/moritzthuening</a></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Porting systemd to musl Libc-powered Linux (174 pts)]]></title>
            <link>https://catfox.life/2024/09/05/porting-systemd-to-musl-libc-powered-linux/</link>
            <guid>41454779</guid>
            <pubDate>Thu, 05 Sep 2024 08:44:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://catfox.life/2024/09/05/porting-systemd-to-musl-libc-powered-linux/">https://catfox.life/2024/09/05/porting-systemd-to-musl-libc-powered-linux/</a>, See on <a href="https://news.ycombinator.com/item?id=41454779">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-449">
	<!-- .entry-header -->

	
	
	<div>
		
<p>I have completed an <a href="https://code.atwilcox.tech/sphen/scaly/systemd/-/commits/adelie-v256">initial new port</a> of systemd to musl.  This patch set does not share much in common with the existing OpenEmbedded patchset.  I wanted to make a fully updated patch series targeting more current releases of systemd and musl, taking advantage of the latest features and updates in both.  I also took a focus on writing patches that could be sent for consideration of inclusion upstream.</p>



<p>The final result is a system that appears to be surprisingly reliable considering the newness of the port, and very fast to boot.</p>



<h2>Why?</h2>



<p>I have wanted to do this work for almost a decade.  In fact, a mention of multiple service manager options – including systemd – is present on the <a href="https://web.archive.org/web/20160109133511/http://adelielinux.org/">original Adélie Web site from 2015</a>.  Other initiatives have always taken priority, until someone contacted us at <a href="https://www.wilcoxti.com/">Wilcox Technologies Inc. (WTI)</a> interested in paying on a contract basis to see this effort completed.</p>



<p>I want to be clear that I did not do this for money.  I believe strongly that there is genuine value in having multiple service managers available.  User freedom and user choice matter.  There are cases where this support would have been useful to me and to many others in the community.  I am excited to see this work nearing public release and honoured to be a part of creating more choice in the Linux world.</p>



<h2>How?</h2>



<p>I started with the latest release tag, v256.5.  I wanted a version closely aligned to upstream’s current progress, yet not too far away from the present “stable” 255 release.  I also wanted to make sure that the fallout from upstream’s removal of split-/usr support would be felt to its maximum, since reverting that decision is a high priority.</p>



<p>I fixed build errors as they happened until I finally had a built systemd.  During this phase, I consulted the original OE patchset twice: once for usage of <code>GLOB_BRACE</code>, and the other for usage of <code>malloc_info</code> and <code>malloc_trim</code>.  Otherwise, the patchset was authored entirely originally, mostly through the day (and into the night) of August 16th, 2024.</p>



<p>Many of the issues seen were related to inclusion of headers, and I am already working on <a href="https://github.com/systemd/systemd/pull/34064">bringing</a> those fixes <a href="https://github.com/systemd/systemd/pull/34066">upstream</a>.  It was then time to run the test suite.</p>



<h2>Tests!</h2>



<p>The test suite started with 27 failures.  Most of them were simple fixes, but one that gave me a lot of trouble was the <code>time-util</code> test.  The <a href="https://git.musl-libc.org/cgit/musl/tree/src/time/strptime.c"><code>strptime</code> implementation in musl</a> does not support the <code>%z</code> format specifier (for time zones), which the systemd test relies on.  I could have disabled those tests, but I felt like this would be taking away a lot of functionality.  I considered things like important journals from other systems – they would likely have timestamps with <code>%z</code> formats.  I wrote a <code>%z</code> translation for systemd and saw the tests passing.</p>



<p>Other test failures were simple <a href="https://github.com/systemd/systemd/pull/34065">C portability fixes</a>, which are also in the process of being sent upstream.</p>



<p>The test suite for <code>systemd-sysusers</code> was the next sticky one.  It really exercises the POSIX library functions <code>getgrent</code> and <code>getpwent</code>.  The musl implementations of these are fine, but they don’t cope well with the old NIS compatibility shims from the glibc world.  They also <a href="https://www.openwall.com/lists/musl/2021/10/11/1">can’t handle “incomplete” lines</a>.  The fix for incomplete line handling is pending, so in the meantime I made the test have no incomplete lines.  I added a shim for the NIS compatibility entries in systemd’s <code>putgrent_sane</code> function, making it a little less “sane” but fixing the support perfectly.</p>



<p>Then it was time for the final failing test: <code>test-recurse-dir</code>, which was receiving an <code>EFAULT</code> error code from <code>getdents64</code>.  Discussing this with my friends on the Gentoo IRC, we began to wonder if this was an architecture-specific bug.  I was doing my port work on my Talos II, a 64-bit PowerPC system.  I copied the code over to an Intel Skylake and found the test suite passed.  That was both good, in that the tests were all passing, but also bad, because it meant I was dealing with a PPC64-specific bug.  I wasn’t sure if this was a kernel bug, a musl bug, or a systemd bug.</p>



<p>Digging into it further, I realised that the pointer math being done would be invalid when cast to a pointer-to-structure on PPC64 due to object alignment guarantees in the ABI.  I changed it to use a temporary variable for the pointer math and casting that temporary, and it passed!</p>



<p>And that is how I became the first person alive to see systemd passing its entire test suite on a big-endian 64-bit PowerPC musl libc system.</p>



<h2>The moment of truth</h2>



<p>I created a small disk image and ran a very strange command: <code>apk add adelie-base-posix dash-binsh systemd</code>.  I booted it up as a KVM VM in Qemu and saw “Welcome to Adélie Linux 1.0 Beta 5” before a rather ungraceful – and due to Qemu framebuffer endian issues, colour-swapped – segmentation fault:</p>



<figure><img data-attachment-id="453" data-permalink="https://catfox.life/2024/09/05/porting-systemd-to-musl-libc-powered-linux/the-dawn-of-a-new-error/" data-orig-file="https://catfox.life/wp-content/uploads/2024/09/the-dawn-of-a-new-error.png" data-orig-size="1736,1392" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="the-dawn-of-a-new-error" data-image-description="" data-image-caption="" data-medium-file="https://catfox.life/wp-content/uploads/2024/09/the-dawn-of-a-new-error.png?w=300" data-large-file="https://catfox.life/wp-content/uploads/2024/09/the-dawn-of-a-new-error.png?w=840" tabindex="0" role="button" width="1024" height="821" src="https://catfox.life/wp-content/uploads/2024/09/the-dawn-of-a-new-error.png?w=1024" alt=""><figcaption>Welcome to an endian-swapped systemd core dump!</figcaption></figure>



<p>Debugging this was an experience in early systems debugging that I haven’t had in years.  There’s a great summary on this methodology at <a href="https://linus.schreibt.jetzt/posts/debugging-pid1.html">Linus’s blog</a>.</p>



<p>It turned out that I had disabled a test from build-util as I incorrectly assumed that was only used when debugging in the build root.  Since I did not want to spend time digging around how it manually parses ELF files to find their RPATH entries for a feature we are unlikely to use, I stubbed that functionality out entirely.  We can always fix it later.</p>



<p>Recreating the disk image and booting it up, I was greeted by an Adélie “rescue” environment booted by systemd.  It was frankly bizarre, but also really cool.</p>



<figure><img data-attachment-id="454" data-permalink="https://catfox.life/2024/09/05/porting-systemd-to-musl-libc-powered-linux/habbening/" data-orig-file="https://catfox.life/wp-content/uploads/2024/09/habbening.png" data-orig-size="2064,1470" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="habbening" data-image-description="" data-image-caption="" data-medium-file="https://catfox.life/wp-content/uploads/2024/09/habbening.png?w=300" data-large-file="https://catfox.life/wp-content/uploads/2024/09/habbening.png?w=840" tabindex="0" role="button" width="1024" height="729" src="https://catfox.life/wp-content/uploads/2024/09/habbening.png?w=1024" alt=""><figcaption>The first time systemd ever booted an Adélie Linux system.</figcaption></figure>



<h2>From walking to flying</h2>



<p>Next, I built test packages on the Skylake builder we are using for x86_64 development.  I have a 2012 MacBook Pro that I keep around for testing various experiments, and this felt like a good system for the ultimate experiment.  The goal: swapping init systems with a single command.</p>



<p>It turns out that D-Bus and PolicyKit require systemd support to be enabled or disabled at build-time.  There is no way to build them in a way that allows them to operate on both types of init system.  This is an area I would like to work on more in the future.</p>



<p>I wrote package recipes for both that are built against systemd and “replace” the non-systemd versions.  I also marked them to <code>install_if</code> the system wanted systemd.</p>



<p>Next up were some more configuration and dependency fixes.  I found out via this experiment that some of the Adélie system packages do not place their pkg-config files in the proper place.  I also decided that if I’m already testing this far, I’d use networkd to bring up the laptop in question.</p>



<p>I ran the fateful command <code>apk del openrc; apk add systemd</code> and rebooted.  To my surprise, it all worked!  The system booted up perfectly with systemd.  The oddest sight was my utmps units running:</p>



<figure><img data-attachment-id="455" data-permalink="https://catfox.life/2024/09/05/porting-systemd-to-musl-libc-powered-linux/need-more-fromage-2/" data-orig-file="https://catfox.life/wp-content/uploads/2024/09/need-more-fromage-2.png" data-orig-size="1280,800" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="need-more-fromage-2" data-image-description="" data-image-caption="" data-medium-file="https://catfox.life/wp-content/uploads/2024/09/need-more-fromage-2.png?w=300" data-large-file="https://catfox.life/wp-content/uploads/2024/09/need-more-fromage-2.png?w=840" tabindex="0" role="button" width="1024" height="640" src="https://catfox.life/wp-content/uploads/2024/09/need-more-fromage-2.png?w=1024" alt=""><figcaption>systemd running s6-ipcserver.  The irony is not lost on me.</figcaption></figure>



<h2>Still needed: polish…</h2>



<p>While the system works really well, and boots in 1/3rd the time of OpenRC on the same system, it isn’t ready for prime time just yet.</p>



<p>Rebooting from a KDE session causes the compositor to freeze.  I can reboot manually from a command line, or even from a Konsole inside the session, but not using Plasma’s built-in power buttons.  This may be a PolicyKit issue – I haven’t debugged it properly yet.</p>



<p>There aren’t any service unit files written or packaged yet, other than OpenSSH and utmps.  We are working with our sponsor on an effort to add -systemd split packages to any of the packages with -openrc splits.  We should be able to rely on upstream units where present, and lean on Gentoo and Fedora’s systemd experts to have good base files to reference when needed.  I’ve already landed <a href="https://git.adelielinux.org/adelie/abuild/-/merge_requests/16">support for this in abuild</a>.</p>



<h2>…and You!</h2>



<p>This project could not have happened without the generous sponsors of Wilcox Technologies Inc (WTI) making it possible, nor without the generous sponsors of Adélie Linux keeping the distro running.  Please consider supporting both <a href="https://www.adelielinux.org/contribute/">Adélie Linux</a> and <a href="https://www.patreon.com/WilcoxTech">WTI</a> if you have the means.  Together, we are creating the future of Linux systems – a future where users have the choice and freedom to use the tooling they desire.</p>



<p>If you want to help test this new system out, please reach out to me on IRC (awilfox on Interlinked or Libera), or the Adéliegram Telegram channel.  It will be a little while before a public beta will be available, as more review and discussion with other projects is needed.  We are working with systemd, musl, and other projects to make this as smooth as possible.  We want to ensure that what we provide for testing is up to our highest standards of quality.</p>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building a WoW (World of Warcraft) Server in Elixir (173 pts)]]></title>
            <link>https://pikdum.dev/posts/thistle-tea/</link>
            <guid>41454741</guid>
            <pubDate>Thu, 05 Sep 2024 08:36:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pikdum.dev/posts/thistle-tea/">https://pikdum.dev/posts/thistle-tea/</a>, See on <a href="https://news.ycombinator.com/item?id=41454741">Hacker News</a></p>
Couldn't get https://pikdum.dev/posts/thistle-tea/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Desed: Demystify and debug your sed scripts (141 pts)]]></title>
            <link>https://github.com/SoptikHa2/desed</link>
            <guid>41453557</guid>
            <pubDate>Thu, 05 Sep 2024 04:46:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/SoptikHa2/desed">https://github.com/SoptikHa2/desed</a>, See on <a href="https://news.ycombinator.com/item?id=41453557">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Desed</h2><a id="user-content-desed" aria-label="Permalink: Desed" href="#desed"></a></p>
<p dir="auto">Demystify and debug your sed scripts, from comfort of your terminal.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/SoptikHa2/desed/blob/master/img/desed.gif"><img src="https://github.com/SoptikHa2/desed/raw/master/img/desed.gif" alt="desed usage example" data-animated-image=""></a></p>
<p dir="auto">Desed is a command line tool with beautiful TUI that provides users with comfortable interface and practical debugger, used to step through complex sed scripts.</p>
<p dir="auto">Some of the notable features include:</p>
<ul dir="auto">
<li>Preview variable values, both of them!</li>
<li>See how will a substitute command affect pattern space before it runs</li>
<li>Step through sed script - both forward and backwards!</li>
<li>Place breakpoints and examine program state</li>
<li>Hot reload and see what changes as you edit source code</li>
<li>Its name is a palindrome</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install</h2><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Alpine Linux</h3><a id="user-content-alpine-linux" aria-label="Permalink: Alpine Linux" href="#alpine-linux"></a></p>
<p dir="auto"><code>aports/testing/desed</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Arch Linux</h3><a id="user-content-arch-linux" aria-label="Permalink: Arch Linux" href="#arch-linux"></a></p>
<p dir="auto">Via AUR: <a href="https://aur.archlinux.org/packages/desed-git/" rel="nofollow">desed-git</a> or <a href="https://aur.archlinux.org/packages/desed/" rel="nofollow">desed</a> as stable version.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DragonFly BSD</h3><a id="user-content-dragonfly-bsd" aria-label="Permalink: DragonFly BSD" href="#dragonfly-bsd"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Fedora</h3><a id="user-content-fedora" aria-label="Permalink: Fedora" href="#fedora"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">FreeBSD</h3><a id="user-content-freebsd" aria-label="Permalink: FreeBSD" href="#freebsd"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Void Linux</h3><a id="user-content-void-linux" aria-label="Permalink: Void Linux" href="#void-linux"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Source</h3><a id="user-content-source" aria-label="Permalink: Source" href="#source"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/soptikha2/desed
cd desed
cargo install --path .
cp &quot;desed.1&quot; &quot;$(manpath | cut -d':' -f1)/man1&quot;"><pre>git clone https://github.com/soptikha2/desed
<span>cd</span> desed
cargo install --path <span>.</span>
cp <span><span>"</span>desed.1<span>"</span></span> <span><span>"</span><span><span>$(</span>manpath <span>|</span> cut -d<span><span>'</span>:<span>'</span></span> -f1<span>)</span></span>/man1<span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Cargo</h3><a id="user-content-cargo" aria-label="Permalink: Cargo" href="#cargo"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Precompiled binaries</h3><a id="user-content-precompiled-binaries" aria-label="Permalink: Precompiled binaries" href="#precompiled-binaries"></a></p>
<p dir="auto">See <a href="https://github.com/SoptikHa2/desed/releases">releases</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Dependencies:</h3><a id="user-content-dependencies" aria-label="Permalink: Dependencies:" href="#dependencies"></a></p>
<p dir="auto">Development: <code>rust</code>, <code>cargo</code> (&gt;= 1.38.0)</p>
<p dir="auto">Runtime: <code>sed</code> (GNU version, &gt;= 4.6) (desed works on BSD if you installed <code>gsed</code>)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Controls</h2><a id="user-content-controls" aria-label="Permalink: Controls" href="#controls"></a></p>
<ul dir="auto">
<li>Mouse scroll to scroll through source code, click on line to toggle breakpoint</li>
<li><code>j</code>, <code>k</code>, <code>g</code>, <code>G</code>, just as in Vim. Prefixing with numbers works too.</li>
<li><code>b</code> to toggle breakpoint (prefix with number to toggle breakpoint on target line)</li>
<li><code>s</code> to step forward, <code>a</code> to step backwards</li>
<li><code>r</code> to run to next breakpoint or end of script, <code>R</code> to do the same but backwards</li>
<li><code>l</code> to instantly reload code and continue debugging in the exactly same place as before</li>
<li><code>q</code> to <a href="https://github.com/hakluke/how-to-exit-vim">quit</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How does it work?</h2><a id="user-content-how-does-it-work" aria-label="Permalink: How does it work?" href="#how-does-it-work"></a></p>
<p dir="auto">GNU sed actually provides pretty useful debugging interface, try it yourself with <code>--debug</code> flag. However the interface is not interactive and I wanted something closer to traditional debugger. <a href="https://soptik.tech/articles/building-desed-the-sed-debugger.html" rel="nofollow">I've written something here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Does it really work?</h2><a id="user-content-does-it-really-work" aria-label="Permalink: Does it really work?" href="#does-it-really-work"></a></p>
<p dir="auto">Depends. Sed actually doesn't tell me which line number is it currently executing, so I have to emulate parts of sed to guess that. Which might not be bulletproof. But it certainly worked good enough to debug tetris without issues.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why sed??</h2><a id="user-content-why-sed" aria-label="Permalink: Why sed??" href="#why-sed"></a></p>
<p dir="auto">Sed is the perfect programming language, <a href="https://tildes.net/~comp/b2k/programming_challenge_find_path_from_city_a_to_city_b_with_least_traffic_controls_inbetween#comment-2run" rel="nofollow">especially for graph problems</a>. It's plain and simple and doesn't clutter your screen with useless identifiers like <code>if</code>, <code>for</code>, <code>while</code>, or <code>int</code>. Furthermore since it doesn't have things like numbers, it's very simple to use.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">But why?</h2><a id="user-content-but-why" aria-label="Permalink: But why?" href="#but-why"></a></p>
<p dir="auto">I wanted to program in sed but it lacked good tooling up to this point, so I had to do something about it.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why?</h2><a id="user-content-why" aria-label="Permalink: Why?" href="#why"></a></p>
<p dir="auto">Because it's the standard stream editor for filtering and transforming text. And someone wrote <a href="https://github.com/uuner/sedtris">tetris</a> in it!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What is the roadmap for future updates?</h2><a id="user-content-what-is-the-roadmap-for-future-updates" aria-label="Permalink: What is the roadmap for future updates?" href="#what-is-the-roadmap-for-future-updates"></a></p>
<p dir="auto">I would like to introduce syntax highlighting and add this tool to standard repositories of all major distributions.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Is this a joke?</h2><a id="user-content-is-this-a-joke" aria-label="Permalink: Is this a joke?" href="#is-this-a-joke"></a></p>
<p dir="auto">I thought it was. But apparently it's actually useful for some people.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Other projects</h2><a id="user-content-other-projects" aria-label="Permalink: Other projects" href="#other-projects"></a></p>
<ul dir="auto">
<li><a href="https://github.com/soptikha2/video-summarizer">video summarizer</a>, a tool and browser extensions that determines if people in video are currently talking or not, and speeds up the video accordingly. Great for long lecture videos for skipping time spent writing on a whiteboard.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kids who use ChatGPT as a study assistant do worse on tests (163 pts)]]></title>
            <link>https://hechingerreport.org/kids-chatgpt-worse-on-tests/</link>
            <guid>41453300</guid>
            <pubDate>Thu, 05 Sep 2024 03:50:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hechingerreport.org/kids-chatgpt-worse-on-tests/">https://hechingerreport.org/kids-chatgpt-worse-on-tests/</a>, See on <a href="https://news.ycombinator.com/item?id=41453300">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="content">
		<main id="main">

								

				
				<div>

					<section id="block-2"><p><em>The Hechinger Report is a national nonprofit newsroom that reports on one topic: education. Sign up for our&nbsp;<a href="https://hechingerreport.org/newsletters" target="_blank" rel="noreferrer noopener">weekly newsletters</a>&nbsp;to get stories like this delivered directly to your inbox.&nbsp;Consider supporting our stories and becoming&nbsp;<a href="https://hechingerreport.fundjournalism.org/?campaign=701VK000003ezHZYAY" target="_blank" rel="noreferrer noopener">a member</a>&nbsp;today.</em></p></section>

<article id="post-103317">
	<div>

		
		
					<p>Does AI actually help students learn? A recent experiment in a high school provides a cautionary tale.&nbsp;</p><p>Researchers at the University of Pennsylvania found that Turkish high school students who had access to ChatGPT while doing practice math problems did worse on a math test compared with students who didn’t have access to ChatGPT. Those with ChatGPT solved 48 percent more of the practice problems correctly, but they ultimately scored 17 percent worse on a test of the topic that the students were learning.&nbsp;</p><p>A third group of students had access to a revised version of ChatGPT that functioned more like a tutor. This chatbot was programmed to provide hints without directly divulging the answer. The students who used it did spectacularly better on the practice problems, solving 127 percent more of them correctly compared with students who did their practice work without any high-tech aids. But on a test afterwards, these AI-tutored students did no better. Students who just did their practice problems the old fashioned way — on their own — matched their test scores.</p><p>The researchers titled their paper, “Generative AI Can Harm Learning,” to make clear to parents and educators that the current crop of freely available AI chatbots can “substantially inhibit learning.” Even a fine-tuned version of ChatGPT designed to mimic a tutor doesn’t necessarily help.</p><p>The researchers believe the problem is that students are using the chatbot as a “crutch.” When they analyzed the questions that students typed into ChatGPT, students often simply asked for the answer. Students were not building the skills that come from solving the problems themselves.&nbsp;</p><p>ChatGPT’s errors also may have been a contributing factor. The chatbot only answered the math problems correctly half of the time. Its arithmetic computations were wrong 8 percent of the time, but the bigger problem was that its step-by-step approach for how to solve a problem was wrong 42 percent of the time. The tutoring version of ChatGPT was directly fed the correct solutions and these errors were minimized.</p><p>A <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4895486">draft paper about the experiment</a> was posted on the website of SSRN, formerly known as the Social Science Research Network, in July 2024. The paper has not yet been published in a peer-reviewed journal and could still be revised.&nbsp;</p><p>This is just one experiment in another country, and more studies will be needed to confirm its findings. But this experiment was a large one, involving nearly a thousand students in grades nine through 11 during the fall of 2023. Teachers first reviewed a previously taught lesson with the whole classroom, and then their classrooms were randomly assigned to practice the math in one of three ways: with access to ChatGPT, with access to an AI tutor powered by ChatGPT or with no high-tech aids at all. Students in each grade were assigned the same practice problems with or without AI. Afterwards, they took a test to see how well they learned the concept. Researchers conducted four cycles of this, giving students four 90-minute sessions of practice time in four different math topics to understand whether AI tends to help, harm or do nothing.</p><p>ChatGPT also seems to produce overconfidence. In surveys that accompanied the experiment, students said they did not think that ChatGPT caused them to learn less even though they had. Students with the AI tutor thought they had done significantly better on the test even though they did not. (It’s also another good reminder to all of us that our <a href="https://hechingerreport.org/proof-points-college-students-often-dont-know-when-theyre-learning/">perceptions of how much we’ve learned are often wrong</a>.)</p><p>The authors likened the problem of learning with ChatGPT to autopilot. They recounted how an overreliance on autopilot led the Federal Aviation Administration to recommend that pilots minimize their use of this technology. Regulators wanted to make sure that pilots still know how to fly when autopilot fails to function correctly.&nbsp;</p><p>ChatGPT is not the first technology to present a tradeoff in education. Typewriters and computers reduce the need for handwriting. Calculators reduce the need for arithmetic. When students have access to ChatGPT, they might answer more problems correctly, but learn less. Getting the right result to one problem won’t help them with the next one.</p><p><em>This story about&nbsp;using <a href="https://hechingerreport.org/kids-chatgpt-worse-on-tests/">ChatGPT to practice math</a>&nbsp;was written by Jill Barshay and produced by&nbsp;<a href="https://hechingerreport.org/special-reports/higher-education/" target="_blank" rel="noreferrer noopener">The Hechinger Report</a>, a nonprofit, independent news organization focused on inequality and innovation in education. Sign up for&nbsp;<a href="https://hechingerreport.org/proofpoints/" target="_blank" rel="noreferrer noopener"><em>Proof Points</em></a>&nbsp;and other&nbsp;<a href="https://hechingerreport.org/newsletters/" target="_blank" rel="noreferrer noopener"><em>Hechinger newsletters</em></a>.</em></p>
<div id="custom_html-3">
	
<p>The Hechinger Report provides in-depth, fact-based, unbiased reporting on education that is free to all readers. But that doesn't mean it's free to produce. Our work keeps educators and the public informed about pressing issues at schools and on campuses throughout the country. We tell the whole story, even when the details are inconvenient. Help us keep doing that.</p>

<p><a href="https://checkout.fundjournalism.org/memberform?amount=15&amp;installmentPeriod=monthly&amp;org_id=hechingerreport&amp;campaign=701f4000000dsvy">Join us today.</a></p>
</div>	</div><!-- .entry-content -->

	<!-- .entry-footer -->

	
			<div>
															<p><a href="https://hechingerreport.org/author/jill-barshay/" rel="author">
											<img alt="Avatar photo" src="https://hechingerreport.org/wp-content/uploads/2015/01/Barshay-80x80.jpg" srcset="https://i0.wp.com/hechingerreport.org/wp-content/uploads/2015/01/Barshay.jpg?fit=154%2C150&amp;ssl=1 2x" height="80" width="80">											</a></p><!-- .author-bio-text -->

			</div><!-- .author-bio -->
			
</article><!-- #post-${ID} -->

<!-- #comments -->
				</div><!-- .main-content -->

			
		</main><!-- #main -->
	</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Yi-Coder: A Small but Mighty LLM for Code (232 pts)]]></title>
            <link>https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md</link>
            <guid>41453237</guid>
            <pubDate>Thu, 05 Sep 2024 03:38:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md">https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md</a>, See on <a href="https://news.ycombinator.com/item?id=41453237">Hacker News</a></p>
Couldn't get https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Accelerando (2005) (169 pts)]]></title>
            <link>https://www.antipope.org/charlie/blog-static/fiction/accelerando/accelerando.html</link>
            <guid>41452962</guid>
            <pubDate>Thu, 05 Sep 2024 02:33:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.antipope.org/charlie/blog-static/fiction/accelerando/accelerando.html">https://www.antipope.org/charlie/blog-static/fiction/accelerando/accelerando.html</a>, See on <a href="https://news.ycombinator.com/item?id=41452962">Hacker News</a></p>
Couldn't get https://www.antipope.org/charlie/blog-static/fiction/accelerando/accelerando.html: Error: unable to verify the first certificate]]></description>
        </item>
        <item>
            <title><![CDATA[Canadian mega landlord using AI 'pricing scheme' as it hikes rents (133 pts)]]></title>
            <link>https://breachmedia.ca/canadian-mega-landlord-ai-pricing-scheme-hikes-rents/</link>
            <guid>41452781</guid>
            <pubDate>Thu, 05 Sep 2024 01:59:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://breachmedia.ca/canadian-mega-landlord-ai-pricing-scheme-hikes-rents/">https://breachmedia.ca/canadian-mega-landlord-ai-pricing-scheme-hikes-rents/</a>, See on <a href="https://news.ycombinator.com/item?id=41452781">Hacker News</a></p>
Couldn't get https://breachmedia.ca/canadian-mega-landlord-ai-pricing-scheme-hikes-rents/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Tinystatus: A tiny status page generated by a Python script (180 pts)]]></title>
            <link>https://github.com/harsxv/tinystatus</link>
            <guid>41452339</guid>
            <pubDate>Thu, 05 Sep 2024 00:40:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/harsxv/tinystatus">https://github.com/harsxv/tinystatus</a>, See on <a href="https://news.ycombinator.com/item?id=41452339">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">TinyStatus</h2><a id="user-content-tinystatus" aria-label="Permalink: TinyStatus" href="#tinystatus"></a></p>
<p dir="auto">TinyStatus is a simple, customizable status page generator that allows you to monitor the status of various services and display them on a clean, responsive web page. <a href="https://status.harry.id/" rel="nofollow">Check out an online demo.</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/32115753/364659939-28227221-d1e1-442e-89a4-2a0a09615514.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1MzYxMDIsIm5iZiI6MTcyNTUzNTgwMiwicGF0aCI6Ii8zMjExNTc1My8zNjQ2NTk5MzktMjgyMjcyMjEtZDFlMS00NDJlLTg5YTQtMmEwYTA5NjE1NTE0LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDExMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA2Zjc0OGQxMGYxYzRjYmI0MmU0MGVkMzczMmIxMGZjNDQ4YzdkYWEyY2VmMTgxYWQ2YTMyYzA5MzFiZWJjYTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.QCCL41W8KUg1xVUkqRXKeIjf9PYclCeikbDKVyKaZK4"><img src="https://private-user-images.githubusercontent.com/32115753/364659939-28227221-d1e1-442e-89a4-2a0a09615514.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1MzYxMDIsIm5iZiI6MTcyNTUzNTgwMiwicGF0aCI6Ii8zMjExNTc1My8zNjQ2NTk5MzktMjgyMjcyMjEtZDFlMS00NDJlLTg5YTQtMmEwYTA5NjE1NTE0LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDExMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA2Zjc0OGQxMGYxYzRjYmI0MmU0MGVkMzczMmIxMGZjNDQ4YzdkYWEyY2VmMTgxYWQ2YTMyYzA5MzFiZWJjYTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.QCCL41W8KUg1xVUkqRXKeIjf9PYclCeikbDKVyKaZK4" alt="image"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Monitor HTTP endpoints, ping hosts, and check open ports</li>
<li>Responsive design for both status page and history page</li>
<li>Customizable service checks via YAML configuration</li>
<li>Incident history tracking</li>
<li>Automatic status updates at configurable intervals</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Prerequisites</h2><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<ul dir="auto">
<li>Python 3.7 or higher</li>
<li>pip (Python package manager)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Clone the repository or download the source code:</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/yourusername/tinystatus.git
cd tinystatus"><pre><code>git clone https://github.com/yourusername/tinystatus.git
cd tinystatus
</code></pre></div>
</li>
<li>
<p dir="auto">Install the required dependencies:</p>
<div data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre><code>pip install -r requirements.txt
</code></pre></div>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Create a <code>.env</code> file in the project root and customize the variables:</p>
<div data-snippet-clipboard-copy-content="CHECK_INTERVAL=30
MAX_HISTORY_ENTRIES=100
LOG_LEVEL=INFO
CHECKS_FILE=checks.yaml
INCIDENTS_FILE=incidents.md
TEMPLATE_FILE=index.html.theme
HISTORY_TEMPLATE_FILE=history.html.theme
STATUS_HISTORY_FILE=history.json"><pre><code>CHECK_INTERVAL=30
MAX_HISTORY_ENTRIES=100
LOG_LEVEL=INFO
CHECKS_FILE=checks.yaml
INCIDENTS_FILE=incidents.md
TEMPLATE_FILE=index.html.theme
HISTORY_TEMPLATE_FILE=history.html.theme
STATUS_HISTORY_FILE=history.json
</code></pre></div>
</li>
<li>
<p dir="auto">Edit the <code>checks.yaml</code> file to add or modify the services you want to monitor. Example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="- name: GitHub Home
  type: http
  host: https://github.com
  expected_code: 200

- name: Google DNS
  type: ping
  host: 8.8.8.8

- name: Database
  type: port
  host: db.example.com
  port: 5432"><pre>- <span>name</span>: <span>GitHub Home</span>
  <span>type</span>: <span>http</span>
  <span>host</span>: <span>https://github.com</span>
  <span>expected_code</span>: <span>200</span>

- <span>name</span>: <span>Google DNS</span>
  <span>type</span>: <span>ping</span>
  <span>host</span>: <span>8.8.8.8</span>

- <span>name</span>: <span>Database</span>
  <span>type</span>: <span>port</span>
  <span>host</span>: <span>db.example.com</span>
  <span>port</span>: <span>5432</span></pre></div>
</li>
<li>
<p dir="auto">(Optional) Customize the <code>incidents.md</code> file to add any known incidents or maintenance schedules.</p>
</li>
<li>
<p dir="auto">(Optional) Modify the <code>index.html.theme</code> and <code>history.html.theme</code> files to customize the look and feel of your status pages.</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Run the TinyStatus script:</p>

</li>
<li>
<p dir="auto">The script will generate two HTML files:</p>
<ul dir="auto">
<li><code>index.html</code>: The main status page</li>
<li><code>history.html</code>: The status history page</li>
</ul>
</li>
<li>
<p dir="auto">To keep the status page continuously updated, you can run the script in the background:</p>
<ul dir="auto">
<li>On Unix-like systems (Linux, macOS):
<div data-snippet-clipboard-copy-content="nohup python tinystatus.py &amp;"><pre><code>nohup python tinystatus.py &amp;
</code></pre></div>
</li>
<li>On Windows, you can use the Task Scheduler to run the script at startup.</li>
</ul>
</li>
<li>
<p dir="auto">Serve the generated HTML files using your preferred web server (e.g., Apache, Nginx, or a simple Python HTTP server for testing).</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Customization</h2><a id="user-content-customization" aria-label="Permalink: Customization" href="#customization"></a></p>
<ul dir="auto">
<li>Adjust the configuration variables in the <code>.env</code> file to customize the behavior of TinyStatus.</li>
<li>Customize the appearance of the status page by editing the CSS in <code>index.html.theme</code> and <code>history.html.theme</code>.</li>
<li>Add or remove services by modifying the <code>checks.yaml</code> file.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome! Please feel free to submit a Pull Request.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is open source and available under the <a href="https://github.com/harsxv/tinystatus/blob/master/LICENSE">MIT License</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Laminar – Open-Source DataDog + PostHog for LLM Apps, Built in Rust (189 pts)]]></title>
            <link>https://github.com/lmnr-ai/lmnr</link>
            <guid>41451698</guid>
            <pubDate>Wed, 04 Sep 2024 22:52:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/lmnr-ai/lmnr">https://github.com/lmnr-ai/lmnr</a>, See on <a href="https://news.ycombinator.com/item?id=41451698">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a href="https://www.ycombinator.com/companies/laminar-ai" rel="nofollow"><img src="https://camo.githubusercontent.com/3bf938994198a5b1d850adab39ba81e5675045b3b2b496b9856ce7b833eae93a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f59253230436f6d62696e61746f722d5332342d6f72616e6765" alt="Static Badge" data-canonical-src="https://img.shields.io/badge/Y%20Combinator-S24-orange"></a>
<a href="https://x.com/lmnrai" rel="nofollow"><img src="https://camo.githubusercontent.com/7217689996b50018699ee10564c16fa62744b484f8ab968cfcd2b4b992212b15/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f6c6d6e726169" alt="X (formerly Twitter) Follow" data-canonical-src="https://img.shields.io/twitter/follow/lmnrai"></a>
<a href="https://discord.gg/nNFUUDAKub" rel="nofollow"> <img src="https://camo.githubusercontent.com/3c567c02e658b3bbc878a42c214883dc7706c5206ceea744dd66138c5b9e348f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4a6f696e5f446973636f72642d3436343634363f266c6f676f3d646973636f7264266c6f676f436f6c6f723d353836354632" alt="Static Badge" data-canonical-src="https://img.shields.io/badge/Join_Discord-464646?&amp;logo=discord&amp;logoColor=5865F2"> </a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Laminar - Open-Source observability, analytics, evals and prompt chains for complex LLM apps.</h2><a id="user-content-laminar---open-source-observability-analytics-evals-and-prompt-chains-for-complex-llm-apps" aria-label="Permalink: Laminar - Open-Source observability, analytics, evals and prompt chains for complex LLM apps." href="#laminar---open-source-observability-analytics-evals-and-prompt-chains-for-complex-llm-apps"></a></p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/14181915/364260202-88e1f801-1dbf-4e5b-af71-1a3923661cd1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1MTgxMDIsIm5iZiI6MTcyNTUxNzgwMiwicGF0aCI6Ii8xNDE4MTkxNS8zNjQyNjAyMDItODhlMWY4MDEtMWRiZi00ZTViLWFmNzEtMWEzOTIzNjYxY2QxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDA2MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTYyYTBmYjliYjg1YjAzYWJmY2IyZTBhMGIzNDRhMGFjNTdmZmMxODQ1MDliZGRkOWE2ZDdjOWY4YzQxMzQzMmYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.7I9SrNoGGdke9Vnw-RjlOeneRQgabGyiRu8uDYQadi4"><img width="1439" alt="traces" src="https://private-user-images.githubusercontent.com/14181915/364260202-88e1f801-1dbf-4e5b-af71-1a3923661cd1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1MTgxMDIsIm5iZiI6MTcyNTUxNzgwMiwicGF0aCI6Ii8xNDE4MTkxNS8zNjQyNjAyMDItODhlMWY4MDEtMWRiZi00ZTViLWFmNzEtMWEzOTIzNjYxY2QxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDA2MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTYyYTBmYjliYjg1YjAzYWJmY2IyZTBhMGIzNDRhMGFjNTdmZmMxODQ1MDliZGRkOWE2ZDdjOWY4YzQxMzQzMmYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.7I9SrNoGGdke9Vnw-RjlOeneRQgabGyiRu8uDYQadi4"></a>
<p dir="auto">Think of it as DataDog + PostHog for LLM apps.</p>
<ul dir="auto">
<li>OpenTelemetry-based instrumentation: automatic for LLM / vector DB calls with just 2 lines of code + decorators to track functions (powered by an amazing <a href="https://github.com/traceloop/openllmetry">OpenLLMetry</a> open-source package by TraceLoop).</li>
<li>Semantic events-based analytics. Laminar hosts background job queues of LLM pipelines. Outputs of those pipelines are turned into metrics. For example, you can design a pipeline which extracts "my AI drive-through agent made an upsell" data, and track this metric in Laminar.</li>
<li>Built for scale with a modern stack: written in Rust, RabbitMQ for message queue, Postgres for data, Clickhouse for analytics</li>
<li>Insightful, fast dashboards for traces / spans / events</li>
</ul>
<p dir="auto">Read the <a href="https://docs.lmnr.ai/" rel="nofollow">docs</a>.</p>
<p dir="auto">This is a work in progress repo and it will be frequently updated.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting started" href="#getting-started"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Laminar Cloud</h3><a id="user-content-laminar-cloud" aria-label="Permalink: Laminar Cloud" href="#laminar-cloud"></a></p>
<p dir="auto">The easiest way to get started is with a generous free tier on our managed platform -&gt; <a href="https://www.lmnr.ai/" rel="nofollow">lmnr.ai</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Self-hosting with Docker compose</h3><a id="user-content-self-hosting-with-docker-compose" aria-label="Permalink: Self-hosting with Docker compose" href="#self-hosting-with-docker-compose"></a></p>
<p dir="auto">Start local version with docker compose.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:lmnr-ai/lmnr
cd lmnr
docker compose up"><pre>git clone git@github.com:lmnr-ai/lmnr
<span>cd</span> lmnr
docker compose up</pre></div>
<p dir="auto">This will spin up the following containers:</p>
<ul dir="auto">
<li>app-server – the core app logic, backend, and the LLM proxies</li>
<li>rabbitmq – message queue for sending the traces and observations reliably</li>
<li>qdrant – vector database</li>
<li>semantic-search-service – service for interacting with qdrant and embeddings</li>
<li>frontend – the visual front-end dashboard for interacting with traces</li>
<li>postgres – the database for all the application data</li>
<li>clickhouse – columnar OLAP database for more efficient event and trace analytics</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Instrumenting Python code</h3><a id="user-content-instrumenting-python-code" aria-label="Permalink: Instrumenting Python code" href="#instrumenting-python-code"></a></p>
<p dir="auto">First, create a project and generate a Project API Key. Then,</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install lmnr
echo &quot;LMNR_PROJECT_API_KEY=<YOUR_PROJECT_API_KEY>&quot; >> .env"><pre>pip install lmnr
<span>echo</span> <span><span>"</span>LMNR_PROJECT_API_KEY=&lt;YOUR_PROJECT_API_KEY&gt;<span>"</span></span> <span>&gt;&gt;</span> .env</pre></div>
<p dir="auto">To automatically instrument LLM calls of popular frameworks and LLM provider libraries just add</p>
<div dir="auto" data-snippet-clipboard-copy-content="from lmnr import Laminar as L
L.initialize(project_api_key=&quot;<LMNR_PROJECT_API_KEY>&quot;)"><pre><span>from</span> <span>lmnr</span> <span>import</span> <span>Laminar</span> <span>as</span> <span>L</span>
<span>L</span>.<span>initialize</span>(<span>project_api_key</span><span>=</span><span>"&lt;LMNR_PROJECT_API_KEY&gt;"</span>)</pre></div>
<p dir="auto">In addition to automatic instrumentation, we provide a simple <code>@observe()</code> decorator, if you want to trace inputs / outputs of functions</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Example</h4><a id="user-content-example" aria-label="Permalink: Example" href="#example"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import os
from openai import OpenAI

from lmnr import observe, Laminar as L
L.initialize(project_api_key=&quot;<LMNR_PROJECT_API_KEY>&quot;)

client = OpenAI(api_key=os.environ[&quot;OPENAI_API_KEY&quot;])

@observe()  # annotate all functions you want to trace
def poem_writer(topic=&quot;turbulence&quot;):
    prompt = f&quot;write a poem about {topic}&quot;
    response = client.chat.completions.create(
        model=&quot;gpt-4o&quot;,
        messages=[
            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt},
        ],
    )
    poem = response.choices[0].message.content
    return poem

if __name__ == &quot;__main__&quot;:
    print(poem_writer(topic=&quot;laminar flow&quot;))"><pre><span>import</span> <span>os</span>
<span>from</span> <span>openai</span> <span>import</span> <span>OpenAI</span>

<span>from</span> <span>lmnr</span> <span>import</span> <span>observe</span>, <span>Laminar</span> <span>as</span> <span>L</span>
<span>L</span>.<span>initialize</span>(<span>project_api_key</span><span>=</span><span>"&lt;LMNR_PROJECT_API_KEY&gt;"</span>)

<span>client</span> <span>=</span> <span>OpenAI</span>(<span>api_key</span><span>=</span><span>os</span>.<span>environ</span>[<span>"OPENAI_API_KEY"</span>])

<span>@<span>observe</span>()  <span># annotate all functions you want to trace</span></span>
<span>def</span> <span>poem_writer</span>(<span>topic</span><span>=</span><span>"turbulence"</span>):
    <span>prompt</span> <span>=</span> <span>f"write a poem about <span><span>{</span><span>topic</span><span>}</span></span>"</span>
    <span>response</span> <span>=</span> <span>client</span>.<span>chat</span>.<span>completions</span>.<span>create</span>(
        <span>model</span><span>=</span><span>"gpt-4o"</span>,
        <span>messages</span><span>=</span>[
            {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>"You are a helpful assistant."</span>},
            {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>prompt</span>},
        ],
    )
    <span>poem</span> <span>=</span> <span>response</span>.<span>choices</span>[<span>0</span>].<span>message</span>.<span>content</span>
    <span>return</span> <span>poem</span>

<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:
    <span>print</span>(<span>poem_writer</span>(<span>topic</span><span>=</span><span>"laminar flow"</span>))</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Sending events</h4><a id="user-content-sending-events" aria-label="Permalink: Sending events" href="#sending-events"></a></p>
<p dir="auto">You can send events in two ways:</p>
<ul dir="auto">
<li><code>.event(name, value)</code> – instant event with a value.</li>
<li><code>.evaluate_event(name, evaluator, data)</code> –  event that is evaluated by evaluator pipeline based on the data.</li>
</ul>
<p dir="auto">Note that to run an evaluate event, you need to crate an evaluator pipeline and create a target version for it.</p>
<p dir="auto">Laminar processes background job queues of pipeline processes and records outputs of pipelines as events.</p>
<p dir="auto">Read our <a href="https://docs.lmnr.ai/" rel="nofollow">docs</a> to learn more about event types and how they are created and evaluated.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from lmnr import Laminar as L
# ...
poem = response.choices[0].message.content

# this will register True or False value with Laminar
L.event(&quot;topic alignment&quot;, topic in poem)

# this will run the pipeline `check_wordy` with `poem` set as the value
# of `text_input` node, and write the result as an event with name
# &quot;excessive_wordiness&quot;
L.evaluate_event(&quot;excessive_wordiness&quot;, &quot;check_wordy&quot;, {&quot;text_input&quot;: poem})"><pre><span>from</span> <span>lmnr</span> <span>import</span> <span>Laminar</span> <span>as</span> <span>L</span>
<span># ...</span>
<span>poem</span> <span>=</span> <span>response</span>.<span>choices</span>[<span>0</span>].<span>message</span>.<span>content</span>

<span># this will register True or False value with Laminar</span>
<span>L</span>.<span>event</span>(<span>"topic alignment"</span>, <span>topic</span> <span>in</span> <span>poem</span>)

<span># this will run the pipeline `check_wordy` with `poem` set as the value</span>
<span># of `text_input` node, and write the result as an event with name</span>
<span># "excessive_wordiness"</span>
<span>L</span>.<span>evaluate_event</span>(<span>"excessive_wordiness"</span>, <span>"check_wordy"</span>, {<span>"text_input"</span>: <span>poem</span>})</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Laminar pipelines as prompt chain managers</h4><a id="user-content-laminar-pipelines-as-prompt-chain-managers" aria-label="Permalink: Laminar pipelines as prompt chain managers" href="#laminar-pipelines-as-prompt-chain-managers"></a></p>
<p dir="auto">You can create Laminar pipelines in the UI and manage chains of LLM calls there.</p>
<p dir="auto">After you are ready to use your pipeline in your code, deploy it in Laminar by selecting the target version for the pipeline.</p>
<p dir="auto">Once your pipeline target is set, you can call it from Python in just a few lines.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from lmnr import Laminar as L

L.initialize('<YOUR_PROJECT_API_KEY>')

result = l.run(
    pipeline = 'my_pipeline_name',
    inputs = {'input_node_name': 'some_value'},
    # all environment variables
    env = {'OPENAI_API_KEY': 'sk-some-key'},
)"><pre><span>from</span> <span>lmnr</span> <span>import</span> <span>Laminar</span> <span>as</span> <span>L</span>

<span>L</span>.<span>initialize</span>(<span>'&lt;YOUR_PROJECT_API_KEY&gt;'</span>)

<span>result</span> <span>=</span> <span>l</span>.<span>run</span>(
    <span>pipeline</span> <span>=</span> <span>'my_pipeline_name'</span>,
    <span>inputs</span> <span>=</span> {<span>'input_node_name'</span>: <span>'some_value'</span>},
    <span># all environment variables</span>
    <span>env</span> <span>=</span> {<span>'OPENAI_API_KEY'</span>: <span>'sk-some-key'</span>},
)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Learn more</h2><a id="user-content-learn-more" aria-label="Permalink: Learn more" href="#learn-more"></a></p>
<p dir="auto">To learn more about instrumenting your code, check out our client libraries:</p>
<p dir="auto"><a href="https://www.npmjs.com/package/@lmnr-ai/lmnr" rel="nofollow"> <img src="https://camo.githubusercontent.com/6b3081997512b3addac3266d3dcaa06d9fe0cfdc9d34a7b64f56c68ee0e10398/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f2534306c6d6e722d61692532466c6d6e723f6c6162656c3d6c6d6e72266c6f676f3d6e706d266c6f676f436f6c6f723d434233383337" alt="NPM Version" data-canonical-src="https://img.shields.io/npm/v/%40lmnr-ai%2Flmnr?label=lmnr&amp;logo=npm&amp;logoColor=CB3837"> </a>
<a href="https://pypi.org/project/lmnr/" rel="nofollow"> <img src="https://camo.githubusercontent.com/667df376d1224a1681f52ee5b358b52d73094911caecd6bcfa5fd55e6622df40/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6c6d6e723f6c6162656c3d6c6d6e72266c6f676f3d70797069266c6f676f436f6c6f723d333737354139" alt="PyPI - Version" data-canonical-src="https://img.shields.io/pypi/v/lmnr?label=lmnr&amp;logo=pypi&amp;logoColor=3775A9"> </a></p>
<p dir="auto">To get deeper understanding of the concepts, follow on to the <a href="https://docs.lmnr.ai/" rel="nofollow">docs</a> and <a href="https://docs.lmnr.ai/tutorials" rel="nofollow">tutorials</a>.</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>