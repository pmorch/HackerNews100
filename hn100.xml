<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 11 Jan 2024 17:00:12 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Outlook is Microsoft's new data collection service (115 pts)]]></title>
            <link>https://proton.me/blog/outlook-is-microsofts-new-data-collection-service</link>
            <guid>38953618</guid>
            <pubDate>Thu, 11 Jan 2024 15:36:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://proton.me/blog/outlook-is-microsofts-new-data-collection-service">https://proton.me/blog/outlook-is-microsofts-new-data-collection-service</a>, See on <a href="https://news.ycombinator.com/item?id=38953618">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><time datetime="2024-01-05T14:56:11">Published on January 5, 2024</time></p>
<p>With Microsoft’s <a rel="noreferrer noopener" target="_blank" href="https://www.windowscentral.com/software-apps/windows-11/whats-new-with-the-outlook-app-for-windows-11">rollout of the new Outlook for Windows<span>(new window)</span></a>, it appears the company has transformed its email app into a <a rel="" target="_self" href="https://proton.me/blog/ban-surveillance-advertising">surveillance<span>(new window)</span></a> tool for targeted advertising.</p>



<p>Everyone talks about the <a rel="" target="_self" href="https://proton.me/blog/google-privacy-sandbox">privacy-washing<span>(new window)</span></a> campaigns of Google and Apple as they mine your online data to generate advertising revenue. But now it looks like Outlook is no longer <a rel="" target="_self" href="https://proton.me/blog/how-to-send-secure-email-outlook">simply an email service<span>(new window)</span></a>; it’s a data collection mechanism for Microsoft’s 772 external partners and an ad delivery system for Microsoft itself.</p>



<p>Here’s how and why.</p>



<h2><strong>Microsoft shares your data with 772 third parties&nbsp;</strong></h2>



<p>Some European users who download the new Outlook for Windows will encounter a modal with a troubling disclosure about how Microsoft and several hundred third parties process their data:&nbsp;</p>


<div>
<figure><img width="1478" height="1020" data-public-id="wp/Outlook.png" loading="lazy" decoding="async" src="https://res.cloudinary.com/dbulfrlrz/images/w_1478,h_1020,c_scale/f_auto,q_auto/v1704390849/wp/Outlook/Outlook.png?_i=AA" alt="" data-format="png" data-transformations="f_auto,q_auto" data-version="1704390849"></figure></div>


<p>The window informs users that Microsoft and those 772 third parties use their data for a number of purposes, including to:&nbsp;</p>



<ul>
<li>Store and/or access information on the user’s device</li>



<li>Develop and improve products</li>



<li>Personalize ads and content</li>



<li>Measure ads and content</li>



<li>Derive audience insights</li>



<li>Obtain precise geolocation data</li>



<li>Identify users through device scanning</li>
</ul>



<p>This latest version of Outlook confirms that more of Big Tech’s profit margins are becoming ever more dependent on the collection of your personal data. Outlook also prompts you to choose how ads look on your screen, making it clear that advertising is a key part of the deal.</p>


<div>
<figure><img width="467" height="531" data-public-id="wp/outlook-ad-layout.png" loading="lazy" decoding="async" src="https://res.cloudinary.com/dbulfrlrz/images/w_467,h_531,c_scale/f_auto,q_auto/v1704390976/wp/outlook-ad-layout/outlook-ad-layout.png?_i=AA" alt="" data-format="png" data-transformations="f_auto,q_auto" data-version="1704390976"></figure></div>


<p>Mac users logged into the new Outlook will even encounter ads that appear as inbox messages. Some ads are for Microsoft applications while others come from third-parties selling products.&nbsp;</p>


<div>
<figure><img width="1234" height="778" data-public-id="wp/Microsoft-Ads.png" loading="lazy" decoding="async" src="https://res.cloudinary.com/dbulfrlrz/images/w_1234,h_778,c_scale/f_auto,q_auto/v1704391048/wp/Microsoft-Ads/Microsoft-Ads.png?_i=AA" alt="" data-format="png" data-transformations="f_auto,q_auto" data-version="1704391048"></figure></div>


<h2><strong>Microsoft’s “Advertising Partners”</strong></h2>



<p>Thanks to the EU’s General Data Protection Regulation, Europeans are at least informed that a small village of third parties will be able to look at their data. Americans, thanks to their government’s refusal to pass privacy legislation, are never even informed this is happening.</p>



<p>In Outlook settings, UK users can explore a “List of Advertising Partners,” which shows the disturbing number of ad companies working with Microsoft. These third-party companies – called partners – carry names such as “ADMAX” and “ADSOCY.” This is unavailable in the settings for users in the US and Switzerland.</p>


<div>
<figure><img width="990" height="669" data-public-id="wp/outlook-advertising-preferences-2.png" loading="lazy" decoding="async" src="https://res.cloudinary.com/dbulfrlrz/images/w_990,h_669,c_scale/f_auto,q_auto/v1704391132/wp/outlook-advertising-preferences-2/outlook-advertising-preferences-2.png?_i=AA" alt="" data-format="png" data-transformations="f_auto,q_auto" data-version="1704391132"></figure></div>


<p>To some extent, the new Outlook lets you choose how your data is used, but it’s not as simple as clicking a single toggle. </p>



<p>“Depending on the type of data they collect, use, and process and other factors including privacy by design, certain partners rely on your consent while others require you to opt-out,” reads the preferences page for users in the UK. “Click on each advertising company listed below to view their privacy policy and exercise your choices.”</p>


<div>
<figure><img width="987" height="669" data-public-id="wp/outlook-advertising-preferences-1.png" loading="lazy" decoding="async" src="https://res.cloudinary.com/dbulfrlrz/images/w_987,h_669,c_scale/f_auto,q_auto/v1704391214/wp/outlook-advertising-preferences-1/outlook-advertising-preferences-1.png?_i=AA" alt="" data-format="png" data-transformations="f_auto,q_auto" data-version="1704391214"></figure></div>


<p>Not every partner has the same rules. Users can read each individual privacy policy before deciding, but reading is not required.</p>



<p>Such policies are usually <a rel="" target="_self" href="https://proton.me/blog/how-to-read-privacy-policy">long, rambling, and notoriously difficult to understand<span>(new window)</span></a>. But for many companies, <a rel="" target="_self" href="https://proton.me/blog/how-to-read-privacy-policy">that’s the idea<span>(new window)</span></a>. Such policies are intentionally written this way to give companies the maximum freedom to do what they want with your data. That often means selling your personal details to third-party advertisers and data brokers while making it difficult for you to opt out.</p>



<p>With the new Outlook, Microsoft forces users to enter maze-like privacy statements to seize back some control of their data. Of course, Microsoft knows that almost no one <a rel="noreferrer noopener" target="_blank" href="https://www.pewresearch.org/internet/2019/11/15/americans-attitudes-and-experiences-with-privacy-policies-and-laws">reads privacy policies<span>(new window)</span></a>. If everyone understood those policies, revenue would be jeopardized.&nbsp;</p>



<h2><strong>New Outlook steals your email password</strong></h2>



<p>Microsoft’s integration of Outlook with cloud services has <a rel="noreferrer noopener" target="_blank" href="https://news.ycombinator.com/item?id=38219568">raised privacy alarm bells<span>(new window)</span></a>.</p>



<p>When you <a rel="noreferrer noopener" target="_blank" href="https://support.microsoft.com/en-us/office/sync-your-account-in-outlook-to-the-microsoft-cloud-985f9e19-d308-4e85-9d1d-0c6f32f8e981#OfficeVersion=New_Outlook_for_Windows">sync third-party email accounts from services like Yahoo or Gmail<span>(new window)</span></a> with the new Outlook, you risk granting Microsoft <a rel="noreferrer noopener" target="_blank" href="https://www.heise.de/news/Microsoft-krallt-sich-Zugangsdaten-Achtung-vorm-neuen-Outlook-9357691.html?wt_mc=rss.red.ho.ho.atom.beitrag.beitrag">access<span>(new window)</span></a> to the <a rel="" target="_self" href="https://proton.me/blog/smtp-imap-pop3#imap">IMAP<span>(new window)</span></a> and <a rel="" target="_self" href="https://proton.me/blog/smtp-imap-pop3#smtp">SMTP<span>(new window)</span></a> credentials, emails, contacts, and events associated with those accounts, according to the German IT blog <a rel="noreferrer noopener" target="_blank" href="https://www.heise.de/news/Microsoft-krallt-sich-Zugangsdaten-Achtung-vorm-neuen-Outlook-9357691.html?wt_mc=rss.red.ho.ho.atom.beitrag.beitrag">Heise Online<span>(new window)</span></a>.</p>



<p>“Although Microsoft explains that it is possible to switch back to the previous apps at any time, the data will already be stored by the company,” Heise reported. “This allows Microsoft to read the emails.”</p>



<p>You can’t use the new Outlook without syncing all this information with Microsoft Cloud — there is only the option to cancel, <a rel="noreferrer noopener" target="_blank" href="https://www.xda-developers.com/privacy-implications-new-microsoft-outlook">according to the developers’ forum XDA<span>(new window)</span></a>. It is also configured to send login details – including usernames and passwords – directly to Microsoft servers.&nbsp;</p>



<p>Although this transfer is secured with Transport Layer Security (TLS), according to Heise Online, your IMAP and SMTP username and password are transmitted to Microsoft in plain text. <a rel="noreferrer noopener" target="_blank" href="https://www.xda-developers.com/privacy-implications-new-microsoft-outlook">XDA<span>(new window)</span></a> was able to show their test credentials for a third-party email service provider on Microsoft’s servers.&nbsp;</p>


<div>
<figure><img width="1222" height="424" data-public-id="wp/XDA-Credentials-Screenshot.png" loading="lazy" decoding="async" src="https://res.cloudinary.com/dbulfrlrz/images/w_1222,h_424,c_scale/f_auto,q_auto/v1704391494/wp/XDA-Credentials-Screenshot/XDA-Credentials-Screenshot.png?_i=AA" alt="" data-format="png" data-transformations="f_auto,q_auto" data-version="1704391494"></figure></div>


<p>Microsoft is enabling itself to access your email account at any time without your knowledge, allowing it to scan and analyze your emails — and share them with third parties.</p>



<p>To users unaware of the privacy implications, using the new Outlook may seem harmless. But what it could mean is welcoming Microsoft into your data vault and giving them complete freedom to potentially use it however they want.</p>



<p>Professor Ulrich Kelber, the Federal Commissioner for Data Protection and Freedom of Information of Germany, expressed concern about the data capabilities of the new Outlook. He <a rel="noreferrer noopener" target="_blank" href="https://social.bund.de/@bfdi/111381793883035665">announced on Mastodon<span>(new window)</span></a> his intention to request a report from the Irish Data Protection Commissioner, which is responsible for ensuring companies like Microsoft uphold data protection and privacy standards.</p>



<p>Microsoft has not issued a public response to criticisms about its latest data grab. But the software giant has been upfront about its push to use targeted advertising to reach new revenue heights. In 2021, Microsoft Advertising earned $10 billion. But <a rel="noreferrer noopener" target="_blank" href="https://www.businessinsider.com/how-microsoft-advertising-plans-to-grow-a-20-billion-business-2022-10?r=US&amp;IR=T">Microsoft wants to double that total<span>(new window)</span></a>.</p>



<h2><strong>What kind of data does Microsoft collect?&nbsp;</strong></h2>



<p>Per its advertising policy, Microsoft does not use personal data from emails, chats, or documents to target ads. But the ads that pop up may be selected based on other data that gave the company insight about you – such as “<a rel="noreferrer noopener" target="_blank" href="https://privacy.microsoft.com/en-us/privacystatement#mainadvertisingmodule">your interests and favorites, your location, your transactions, how you use our products, your search queries, or the content you view<span>(new window)</span></a>.”</p>



<p>A deeper dive into Microsoft’s privacy policy shows what <a rel="noreferrer noopener" target="_blank" href="https://privacy.microsoft.com/en-US/privacystatement#mainpersonaldatawecollectmodule">personal data it may extract<span>(new window)</span></a>:</p>



<ul>
<li>Name and contact data</li>



<li>Passwords</li>



<li>Demographic data</li>



<li>Payment data</li>



<li>Subscription and licensing data</li>



<li>Search queries</li>



<li>Device and usage data</li>



<li>Error reports and performance data</li>



<li>Voice data</li>



<li>Text, inking, and typing data</li>



<li>Images</li>



<li>Location data</li>



<li>Content</li>



<li>Feedback and ratings</li>



<li>Traffic data</li>
</ul>



<p>The policy offers a glimpse of <a rel="noreferrer noopener" target="_blank" href="https://privacy.microsoft.com/en-US/privacystatement#mainpersonaldatawecollectmodule">where your data might end up<span>(new window)</span></a>:</p>



<ul>
<li>Service providers</li>



<li>User-directed entities</li>



<li>Payment processing providers</li>



<li>Third parties that perform online advertising services for Microsoft</li>
</ul>



<h2><strong>Microsoft steers toward data dollars</strong></h2>



<p>When Google rolled out a <a rel="noreferrer noopener" target="_blank" href="https://googleblog.blogspot.com/2012/01/updating-our-privacy-policies-and-terms.html">privacy policy expanding its powers to collect data<span>(new window)</span></a>, the company drew criticisms from regulators and rivals, including Microsoft, which took out <a rel="noreferrer noopener" target="_blank" href="https://cdn.geekwire.com/wp-content/uploads/2012/02/MICUS0004299_NYT_v2-2.pdf">full-page newspaper ads<span>(new window)</span></a> telling Google users that Google did not respect their privacy.</p>



<p>A short time later, however, <a rel="noreferrer noopener" target="_blank" href="https://www.nytimes.com/2012/10/20/technology/microsoft-expands-gathering-and-use-of-data-from-web-products.html">Microsoft unveiled a privacy policy<span>(new window)</span></a> allowing it to use personal information to sell targeted advertising, moving aggressively in a direction it once decried.</p>



<p>Microsoft has since made significant moves toward surveillance revenue, following in the footsteps of Google, Facebook, and, most recently, <a rel="" target="_self" href="https://proton.me/blog/apple-ad-company">Apple<span>(new window)</span></a>. Like other Big Tech companies, Microsoft recognized a chance to generate large revenue streams by collecting and analyzing user data. This data-centric mindset has been part of a larger trend of established companies <a rel="" target="_self" href="https://proton.me/blog/google-privacy-washing">vying for a slice of the surveillance cash pie<span>(new window)</span></a>.</p>



<p>The appointment of Satya Nadella as CEO in 2014 marked a turning point for Microsoft, which faced scrutiny that same year after admitting to <a rel="noreferrer noopener" target="_blank" href="https://www.theguardian.com/technology/2014/mar/21/microsoft-tightens-privacy-policy-journalists-emails">reading emails from a journalist’s Hotmail account<span>(new window)</span></a>, forcing the company to <a rel="noreferrer noopener" target="_blank" href="https://www.pcworld.com/article/444479/microsoft-tweaks-privacy-policies-after-email-spying-backlash.html">tighten its privacy policy<span>(new window)</span></a>.</p>



<p>Within three months of taking the job, Nadella released a study from a market intelligence firm that concluded “companies taking advantage of their data have the potential to raise an additional $1.6 trillion in revenues over companies that don’t,” wrote author Shoshana Zuboff in her book, <a rel="noreferrer noopener" target="_blank" href="https://www.theguardian.com/books/2019/oct/04/shoshana-zuboff-surveillance-capitalism-assault-human-automomy-digital-privacy"><em>The Age of Surveillance Capitalism</em><span>(new window)</span></a><em>.</em></p>



<p>Key developments that followed included the Bing search engine and the digital assistant Cortana, both designed to capture and analyze user data. The release of Windows 10 in 2015 further underscored Microsoft’s commitment to this new direction. Scrutiny from the privacy community was swift.</p>



<p>Windows 10 “is currently a privacy morass in dire need of reform,” <a rel="noreferrer noopener" target="_blank" href="https://slate.com/technology/2015/08/windows-10-privacy-problems-heres-how-bad-they-are-and-how-to-plug-them.html">wrote software engineer David Auerbach in <em>Slate</em><span>(new window)</span></a>, describing how the new operating system, “gives itself the right to pass loads of your data to Microsoft’s servers, use your bandwidth for Microsoft’s own purposes, and profile your Windows usage.”</p>



<p>Microsoft’s pivot toward advertising continued <a rel="noreferrer noopener" target="_blank" href="https://www.adexchanger.com/online-advertising/xandr-formerly-appnexus-is-now-formerly-att-after-its-acquisition-by-microsoft">with its 2021 purchase of Xandr<span>(new window)</span></a>, but then it decided that it wanted to capitalize on the captive user base its walled garden created and shifted its focus to <a rel="noreferrer noopener" target="_blank" href="https://www.adexchanger.com/platforms/microsoft-is-deprioritizing-third-party-ad-tech-amid-reorgs-and-layoffs">show first-party ads in its services<span>(new window)</span></a>.&nbsp;</p>



<p>Given this direction, Outlook’s new form makes a certain sort of sense.</p>



<p>In <a rel="noreferrer noopener" target="_blank" href="https://www.businessinsider.com/how-microsoft-advertising-plans-to-grow-a-20-billion-business-2022-10?r=US&amp;IR=T">a 2022 interview with Business Insider<span>(new window)</span></a>, Rob Wilk, Microsoft’s head of advertising, talked of opportunities with properties like Xbox, which includes a console business as well as logged-in accounts – “just one of the areas we’re going to play in,” he said.</p>



<p>“Imagine a world, not too far off, where all of these pieces are stitched together to make a cleaner, clearer offering for our advertisers,” Wilk said. “And, don’t forget, we’ve also got browsing information and data across gaming and the Microsoft Windows business with billions of users – this gives us a unique advantage to understand intent.”</p>



<p>Wilk dubbed Microsoft’s advertising push a “<a rel="noreferrer noopener" target="_blank" href="https://www.adexchanger.com/online-advertising/microsoft-ads-chief-rob-wilk-on-why-advertising-is-the-companys-newfound-religion">newfound religion<span>(new window)</span></a>.”</p>



<h2><strong>Surveillance in the name of profit&nbsp;</strong></h2>



<p>Microsoft claims that collecting your data is “<a rel="noreferrer noopener" target="_blank" href="https://privacy.microsoft.com/en-us/privacystatement">to provide you rich, interactive experiences.<span>(new window)</span></a>”</p>



<p>Yet in the realm of Big Tech, advertising and ad revenue have become ends in themselves, justifying a business model based on the surveillance of your private data in the name of profit.</p>



<p>With this rollout of the new Outlook as a data collection and ad delivery service, Microsoft has revealed itself to be no different than the Googles and Metas of the world. For those companies to make privacy the default would mean losing the revenue they have become addicted to.&nbsp;</p>



<p>There are other business models out there deployed by companies that focus first and foremost on online security and privacy.</p>



<p>Proton is one of them.&nbsp;</p>



<h2><strong>Switch to real privacy</strong></h2>



<p>Proton uses end-to-end encryption to protect your <a href="https://proton.me/mail">emails</a>, <a href="https://proton.me/calendar">calendar</a>, <a href="https://proton.me/drive">files stored in the cloud</a>, <a href="https://proton.me/pass">passwords and login credentials</a>, and your <a rel="noopener noreferrer" target="_blank" href="https://protonvpn.com/">internet connection<span>(new window)</span></a>. Our security architecture is designed to keep your data invisible even to us, as our business model gives you more privacy, not less.</p>



<p>Proton provides free and open-source technology to expand access to privacy, security and freedom online. But you can always upgrade to paid plans to access extra features, allowing you to pay with money rather than sensitive data.</p>



<p>And Proton makes it <a href="https://proton.me/easyswitch">easy to switch</a> to our platform. In a few easy steps, you can migrate to an email service you can trust.</p>



<p>We believe in building an internet that works for people and not just for profit. The privacy washing companies like Microsoft and Google routinely perform in the name of revenue is just one more obstacle to a better internet where privacy is the default.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Internet Is Full of AI Dogshit (362 pts)]]></title>
            <link>https://aftermath.site/the-internet-is-full-of-ai-dogshit</link>
            <guid>38952526</guid>
            <pubDate>Thu, 11 Jan 2024 14:23:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aftermath.site/the-internet-is-full-of-ai-dogshit">https://aftermath.site/the-internet-is-full-of-ai-dogshit</a>, See on <a href="https://news.ycombinator.com/item?id=38952526">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The Internet used to be so simple to use that people collectively coined the term “<a href="https://letmegooglethat.com/" target="_blank" rel="noreferrer noopener">let me Google that for you</a>” to make fun of people who had the audacity of asking other people questions online. In the future I fear that people will have no other choice but to ask people for information from the Internet, because right now it’s all full of AI dogshit.</p><p>In September of this year, Google users discovered that the search engine was incorrectly <a href="https://arstechnica.com/information-technology/2023/09/can-you-melt-eggs-quoras-ai-says-yes-and-google-is-sharing-the-result/" target="_blank" rel="noreferrer noopener">telling people that eggs can melt</a>. Why? Because instead of surfacing websites, Google now grabs snapshots of pages in a drop down menu, allowing users to read search results without clicking on anything. This practice often grabs incorrect information, like an AI-generated answer from Quora that insisted that eggs can melt when they definitely cannot.</p><p>In yet another example of the increasing uselessness of Google search: Literally today I was able to recreate <a href="https://bsky.app/profile/lubchansky.bsky.social/post/3kf6a5p3wdq2e" target="_blank" rel="noreferrer noopener">comic artist Mattie Lubchanskey</a>’s Google search for sinus inflammation that returned results for having an inflamed penis.</p><figure><img alt="A screenshot of a google search for &quot;sinus inflammation&quot; that instead served information about penis inflammation" src="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/11/Screen-Shot-2023-11-28-at-1.39.27-PM.png?w=710" srcset="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/11/Screen-Shot-2023-11-28-at-1.39.27-PM.png?w=425&amp;quality=75 425w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/11/Screen-Shot-2023-11-28-at-1.39.27-PM.png?w=850&amp;quality=75 850w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/11/Screen-Shot-2023-11-28-at-1.39.27-PM.png?w=585&amp;quality=75 585w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/11/Screen-Shot-2023-11-28-at-1.39.27-PM.png?w=1170&amp;quality=75 1170w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/11/Screen-Shot-2023-11-28-at-1.39.27-PM.png?w=710&amp;quality=75 710w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2023/11/Screen-Shot-2023-11-28-at-1.39.27-PM.png?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"></figure><p>Yesterday, an unhinged maniac on Twitter claimed to have performed an Internet traffic “heist.” How did he do this? He looked at the site index for the website he wanted to beat in Google search results, exported the article URLS, and then used AI to quickly write articles based on them. In the Twitter thread about this, he described manually editing the titles for these web pages as “optional.”</p><p>Also yesterday, the once venerable outlet <em>Sports Illustrated</em> was caught with its pants down. According to a story from <em>Futurism</em>, it appears that multiple different articles on the site were not only written by AI, but claimed to be authored by <a href="https://futurism.com/sports-illustrated-ai-generated-writers" target="_blank" rel="noreferrer noopener">people who were also completely made up</a>. When <em>Futurism</em> reached out for comment, this content was deleted.&nbsp;</p><p><em>Sports Illustrated</em> later released a statement saying that the articles were authored by a third party, were not AI generated, and that the writers were writing under pseudonyms. This does not explain why the articles were removed after press reached out for comment. Nor does it explain why <em>Sports Illustrated</em> <a href="https://www.wsj.com/articles/sports-illustrated-publisher-taps-ai-to-generate-articles-story-ideas-11675428443" target="_blank" rel="noreferrer noopener">publicly stated they would be using AI to generate content</a> for them in an article in the <em>Wall Street Journal</em> from February of this year.</p><div><figure><div><blockquote data-width="550" data-dnt="true"><div lang="en" dir="ltr"><p>Today, an article was published alleging that Sports Illustrated published AI-generated articles. According to our initial investigation, this is not accurate.</p><p>The articles in question were product reviews and were licensed content from an external, third-party company, AdVon…</p></div>— Sports Illustrated (@SInow) <a href="https://twitter.com/SInow/status/1729275460922622374?ref_src=twsrc%5Etfw">November 27, 2023</a></blockquote></div></figure></div><p><br>The internet has been broken in a fundamental way. It is no longer a repository of people communicating with people; increasingly, it is just a series of machines communicating with machines. </p><p>The once ubiquitous phrase “let me Google that for you” is now meaningless. You are as likely to return incorrect information as you are complete fabrications, and the people who put this content on the Internet do not care. The people who hold the purse strings for <em>Sports Illustrated</em> are more interested in gaming Google search results and the resultant ad revenue from that practice than actually serving their readers. If that means the rest of us get information about inflamed penises when we’re trying to know how long sinus inflammation is supposed to last, well, I guess we’re shit out of luck.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is htmx Just Another JavaScript Framework? (144 pts)]]></title>
            <link>https://htmx.org/essays/is-htmx-another-javascript-framework/</link>
            <guid>38952214</guid>
            <pubDate>Thu, 11 Jan 2024 13:58:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://htmx.org/essays/is-htmx-another-javascript-framework/">https://htmx.org/essays/is-htmx-another-javascript-framework/</a>, See on <a href="https://news.ycombinator.com/item?id=38952214">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

  
  
    <address>Alexander Petros</address>
    <p><time>January 10, 2024</time></p><p>One of the most common criticisms of htmx, usually from people hearing about it for the first time, goes like this:</p>
<blockquote>
<p>You’re complaining about the complexity of modern frontend frameworks, but your solution is just another complex frontend framework.</p>
</blockquote>
<p>This is an excellent objection! It’s the right to question to ask about <em>any</em> third-party (3P) code that you introduce into your project. Even though you aren’t writing the 3P code yourself, by including it in your project you are committed to understanding it—and refreshing that understanding if you want to upgrade it. That’s a big commitment.</p>
<p>Let’s break this criticism down into its constituent parts, and determine exactly how much htmx indulges in the harms it claims to solve.</p>
<h2 id="the-difference-between-a-library-and-a-framework"><a href="#the-difference-between-a-library-and-a-framework" aria-label="Anchor link for: the-difference-between-a-library-and-a-framework">#</a>The difference between a library and a framework</h2>
<p>Some htmx defenders jump to our aid with: “htmx isn’t a framework, it’s a library.” This is probably incorrect.</p>
<p>“Framework” is a colloquial term—there’s no hard rule for the point at which some third-party code evolves from a “library” into a “framework”—but we should still try to define it. In this context:</p>
<ul>
<li><strong>Library</strong> - 3P code whose API does not significantly influence the rest of the application</li>
<li><strong>Framework</strong> - 3P code whose API dictates the overall structure of the application</li>
</ul>
<p>If you prefer metaphors: a library is a cog that you add to your machine, a framework is a pre-built machine that you control by customizing its cogs.</p>
<p>This distinction, fuzzy though it may be, is important because it describes how easily some third-party code can be replaced. For example, a JavaScript service that uses a CSV parsing library can probably swap in a different CSV parsing library without too much trouble; a JavaScript service that uses the NextJS framework, however, is probably going to depend on NextJS for its entire useful life, since an enormous chunk of the code is written with the assumption that it is interacting with NextJS constructs.</p>
<p>Therefore, if your service is built atop a framework, its useful lifespan is tied to the useful lifespan of that framework. If that framework is abandoned, or despised, or otherwise undesirable to work on, the difficulty of modifying your project will steadily increase until you give up modifying it, and eventually, mothball it altogether.</p>
<p>That’s what people are worried about when they ask is “is htmx just another JavaScript framework?” They want to be sure that they’re not committing to a system that will be obsolete soon, like so many of the past web development frameworks.</p>
<p>So: is htmx a framework? And is it going to be fast made obsolete, leaving a trail of un-maintainable websites in the wake of its meteoric demise?</p>
<h2 id="htmx-is-usually-a-framework"><a href="#htmx-is-usually-a-framework" aria-label="Anchor link for: htmx-is-usually-a-framework">#</a>htmx is (usually) a framework</h2>
<p>With apologies to our community’s ongoing debate about this question—I think htmx is pretty clearly a framework, at least in the majority use-case. But it does depend on how you use it.</p>
<p>Wherever you make use of htmx in your project, you’re including htmx attributes in your HTML (i.e. <code>hx-post</code>, <code>hx-target</code>), writing endpoints that are called with htmx-formatted data (with certain request headers), and returning data from those endpoints that is formatted in ways that htmx expects (HTML with <code>hx-*</code> controls). All of these attributes and headers and endpoints interact with each other to create a system by which elements enter and exit the DOM via network request.</p>
<p>If you use htmx to handle a non-trivial number of your website’s network requests, then the inclusion of htmx in your application has significant implications for the project’s structure, from the way you structure your frontend markup, to the database queries your endpoints make. That is framework-like behavior, and in that scenario, htmx cannot be trivially replaced.</p>
<p>You can definitely use htmx in a library-like manner, to add dynamic functionality to just a few sections of your web page. But you can write <a rel="noopener" target="_blank" href="https://www.patterns.dev/vanilla/islands-architecture">React in this library-like manner too</a> and nobody argues that React isn’t a framework. Suffice to say that many people who use htmx in their applications are doing so in a way that bends to the demands of htmx, as a framework for building hypermedia applications.</p>
<p>As they should! Building with htmx works a lot better if you play to its strengths. You can send JSON-formatted form bodies, <a href="https://htmx.org/extensions/json-enc/">if you really insist</a>. But you shouldn’t! It’s simpler to just use <code>application/x-www-form-urlencoded</code> bodies, and write an endpoint that accepts them. You can write an endpoint that is re-used across multiple different clients, <a href="https://htmx.org/essays/why-tend-not-to-use-content-negotiation/">if you really insist</a>. But you shouldn’t!  It’s simpler to <a href="https://htmx.org/essays/splitting-your-apis/">split your data and your hypermedia APIs into separate URLs</a>. Yes, htmx can be used as a library, but maybe let it be your framework too.</p>
<p>That does not mean, however, that htmx is Just Another JavaScript Framework, because htmx has a huge advantage that the other frameworks do not: HTML.</p>
<h2 id="htmx-is-for-writing-html"><a href="#htmx-is-for-writing-html" aria-label="Anchor link for: htmx-is-for-writing-html">#</a>htmx is for writing HTML</h2>
<p>Let’s say you’re using htmx as a framework—is it a <em>JavaScript</em> framework? In one obvious sense, yes: htmx is implemented with ~4k lines of JS. But in another, much more important sense, it is not: React, Svelte, Solid, and so on have you write JS(X) that the framework converts into HTML; htmx just has you write HTML. This removes entire categories of maintenance that might make you abandon other frameworks with time.</p>
<p>Codebases tend to get stuck when you want to upgrade or change some dependency, but the framework you use is incompatible with that change. Java is the most notorious offender here—there are untold millions of lines of Java in production that will never leave Java 8 because upgrading Spring is too hard—but the npm package ecosystem is a close second. When you use the htmx “framework” you will never have this problem, because htmx is a <a href="https://htmx.org/essays/no-build-step/">zero-dependency, client-loaded JavaScript file</a>, so it is guaranteed to never conflict with whatever build process or dependency chain your server <em>does</em> depend on.</p>
<p>Browsers render HTML, so no compiler or transpiler is ever necessary to work with htmx. While many htmx users happily render API responses with JSX, htmx works very well with <a rel="noopener" target="_blank" href="https://jinja.palletsprojects.com/">classic</a> <a rel="noopener" target="_blank" href="https://ejs.co/">template</a> <a rel="noopener" target="_blank" href="https://docs.ruby-lang.org/en/2.3.0/ERB.html">engines</a>, making it portable to <a href="https://htmx.org/essays/hypermedia-on-whatever-youd-like/">whatever language you like</a>. Say what you will about Django and Rails, but they were relevant in 2008 and they’re relevant today—htmx integrates seamlessly with them both. This is a recurring theme with htmx-driven development: htmx works well with development tools old and new, because the common denominator in all these tools is HTML, and htmx is for writing HTML.</p>
<p><img width="500" src="https://htmx.org/img/memes/htmxanddjango.png" alt="A monkey labeled 'HTMX' protecting a cute dog named 'Django' from 'all that compilated JS noise'">
</p>
<p>Pushing the user to define the behavior of their application primarily in HTML, rather than JS, has too many advantages to cover in this essay, so I’ll stick to the one people hate most about JavaScript fameworks: churn. Depending on when you wrote your React application, you might have written your form with <a rel="noopener" target="_blank" href="https://legacy.reactjs.org/docs/forms.html">controlled class components</a>, or <a rel="noopener" target="_blank" href="https://blog.logrocket.com/react-hook-form-complete-guide/">react hooks</a>, or this <a rel="noopener" target="_blank" href="https://react.dev/reference/react-dom/components/form">experimental <code>&lt;form&gt;</code> extension</a>. This is genuinely maddening, especially if you—like me—first learned how to make a web form with class components.</p>
<p>No matter when you wrote your htmx application, however, the behavior of an htmx form has always been defined in largely the same way a regular HTML form is: with <code>&lt;form&gt;</code>. With htmx adding additional network functionality, you can finally use <code>PUT</code> requests and control where the response goes, but in all other respects—validation, inputs, labels, autocomplete—you have default <code>&lt;form&gt;</code> element behavior.</p>
<p>Finally, because htmx simply extends HTML in a very narrow domain (network requests and DOM replacements), most of the “htmx” you write is just plain old HTML. When you have access to complex state management mechanisms, it’s incredibly easy to implement a custom collapsable div; when you don’t, you might stop long enough to search up the <a rel="noopener" target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/details"><code>&lt;details&gt;</code></a> element. Whenever a problem can be solved by native HTML elements, the longevity of the code improves tremendously as a result. This is a much less alienating way to learn web development, because the bulk of your knowledge will remain relevant as long as HTML does.</p>
<p>In this respect, htmx is much more like JQuery than React (htmx’s predecessor, <a rel="noopener" target="_blank" href="https://intercoolerjs.org/">intercooler.js</a>, was a JQuery extension), but it improves on JQuery by using a declarative, HTML-based interface: where JQuery made you go to the <code>&lt;script&gt;</code> tag to specify AJAX behavior, htmx requires only a simple <code>hx-post</code> attribute.</p>
<p>In short, while htmx can be used as a framework, it’s a framework that <a rel="noopener" target="_blank" href="https://unplannedobsolescence.com/blog/custom-html-has-levels">deviates far less from the web’s semantics</a> than the JavaScript frameworks do, and will benefit from improvements in those semantics with no additional work from the user, thanks to the web’s <a rel="noopener" target="_blank" href="https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/The_web_and_web_standards#dont_break_the_web">excellent backwards compatibility guarantees</a>. If you want to build a website that lasts for a long time, these qualities make htmx a substantially better bet than many of its contemporaries.</p>
<p><em>NOTE: Despite agreeing with this analysis, finding no logical flaws in the essay, and allowing me to publish it on his website, Carson continues to insist that htmx is a library.</em></p>
<p><img width="500" src="https://htmx.org/img/memes/istudiedhtml.png" alt="A man holding a sword. He says: 'When you wrote class components, I studied HTML. When you were converting classes to hooks, I mastered the HTML. While you wasted time moving all your client-side logic to server components, I cultivated inner HTML. And now that the browser won't hydrate your thick client JSON API you have the audactiy to come to me for help?'">
</p>

  <p>
    &lt;/&gt;
  </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Timeline to remove DSA support in OpenSSH (109 pts)]]></title>
            <link>https://lists.mindrot.org/pipermail/openssh-unix-announce/2024-January/000156.html</link>
            <guid>38951847</guid>
            <pubDate>Thu, 11 Jan 2024 13:20:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lists.mindrot.org/pipermail/openssh-unix-announce/2024-January/000156.html">https://lists.mindrot.org/pipermail/openssh-unix-announce/2024-January/000156.html</a>, See on <a href="https://news.ycombinator.com/item?id=38951847">Hacker News</a></p>
<div id="readability-page-1" class="page">
   
    <b>djm at openssh.com</b> 
    <a href="mailto:openssh-unix-announce%40mindrot.org?Subject=Re%3A%20%5Bopenssh-unix-announce%5D%20Announce%3A%20timeline%20to%20remove%20DSA%20support%20in%0A%20OpenSSH&amp;In-Reply-To=%3Cfac472a8f9544382%40cvs.openbsd.org%3E" title="[openssh-unix-announce] Announce: timeline to remove DSA support in OpenSSH">djm at openssh.com
       </a><br>
    <i>Thu Jan 11 15:31:18 AEDT 2024</i>
    <ul>
        
        
         <li> <b>Messages sorted by:</b> 
              <a href="https://lists.mindrot.org/pipermail/openssh-unix-announce/2024-January/date.html#156">[ date ]</a>
              <a href="https://lists.mindrot.org/pipermail/openssh-unix-announce/2024-January/thread.html#156">[ thread ]</a>
              <a href="https://lists.mindrot.org/pipermail/openssh-unix-announce/2024-January/subject.html#156">[ subject ]</a>
              <a href="https://lists.mindrot.org/pipermail/openssh-unix-announce/2024-January/author.html#156">[ author ]</a>
         </li>
       </ul>
    <hr>  
<!--beginarticle-->
<pre>Hi,

OpenSSH plans to remove support for DSA keys in the near future. This
message describes our rationale, process and proposed timeline.

Rationale
---------

DSA, as specified in the SSHv2 protocol, is inherently weak - being
limited to a 160 bit private key and use of the SHA1 digest. Its
estimated security level is &lt;=80 bits symmetric equivalent[1][2].

OpenSSH has disabled DSA keys by default since 2015 but has retained
optional support for them. DSA is the only mandatory-to-implement
algorithm in the SSHv2 RFCs[3], mostly because alternative algorithms
were encumbered by patents when the SSHv2 protocol was designed and
specified.

Since then, the world has moved on. RSA is unencumbered and support
for it is ubiquitous. ECDSA offers significant performance and
security benefits over modp DSA, and EdDSA overs further performance
and security improvements over both again.

The only remaining use of DSA at this point should be deeply legacy
devices. As such, we no longer consider the costs of maintaining DSA
in OpenSSH to be justified. Moreover, we hope that OpenSSH's final
removal of this insecure algorithm accelerates its deprecation in
other SSH implementations and allows maintainers of cryptography
libraries to remove it too.

Process and timeline
--------------------

The next release of OpenSSH (due around 2024/03) will make DSA
optional at compile time, but still enable it by default. Users and
downstream distributors of OpenSSH may use this option to explore the
impact of DSA removal in their environments, or to hard-deprecate it
early if they desire.

Around 2024/06, a release of OpenSSH will change this compile-time
default to disable DSA. It may still be enabled by users/distributors
if needed.

Finally, in the first OpenSSH release after 2025/01/01 the DSA code
will be removed entirely.

In summary:

2024/01 - this announcement
2024/03 (estimated) - DSA compile-time optional, enabled by default
2024/06 (estimated) - DSA compile-time optional, *disabled* by default
2025/01 (estimated) - DSA is removed from OpenSSH

Questions
---------

 * What if I have devices that only support DSA?

Removing DSA from OpenSSH will not remove endpoints that require DSA
from the world and users may still need to connect to them. Although
new releases of OpenSSH will no longer support DSA, past releases and
alternate SSH implementations will continue to do so.

We recommend that users with an ongoing need to connect to DSA-only
endpoints maintain a legacy release of an OpenSSH client for this
purpose, similar to what was recommended when support for the SSHv1
protocol was removed.

For example, Debian maintains a "openssh-client-ssh1" package built
from OpenSSH 7.5 for the purpose of connecting to SSHv1 endpoints.
This package or something similar is likely to be sufficient for
DSA-only endpoints too.

 * Doesn't this make OpenSSH non-compliant with RFC4253?

Practically, no more than we've been since 2015 when we stopped
offering DSA by default.

 * Why make this change now? Why not earlier/later?

We feel like enough time has passed since DSA was disabled by default
for the overwhelming majority of users to have abandoned use of the
algorithm. We are also likely to start exploring a post-quantum
signature algorithm soon and are mindful of the overall size and
complexity of the key/signature code.

 * I want to discuss this change further

The <a href="https://lists.mindrot.org/mailman/listinfo/openssh-unix-dev">https://lists.mindrot.org/mailman/listinfo/openssh-unix-dev</a>
mailing list is the best place to discuss this. Alternately you can
email the OpenSSH developers at <a href="https://lists.mindrot.org/mailman/listinfo/openssh-unix-announce">openssh at openssh.com.</a>

Thanks,
Damien Miller, on behalf of the OpenSSH project

[1] <a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf">https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf</a>
[2] <a href="https://www.rfc-editor.org/rfc/rfc9142.html#section-1.1">https://www.rfc-editor.org/rfc/rfc9142.html#section-1.1</a>
[3] <a href="https://www.rfc-editor.org/rfc/rfc4253.html#section-6.6">https://www.rfc-editor.org/rfc/rfc4253.html#section-6.6</a>
</pre>

<!--endarticle-->
    <hr>
    <ul>
        <!--threads-->
	
	
         <li> <b>Messages sorted by:</b> 
              <a href="https://lists.mindrot.org/pipermail/openssh-unix-announce/2024-January/date.html#156">[ date ]</a>
              <a href="https://lists.mindrot.org/pipermail/openssh-unix-announce/2024-January/thread.html#156">[ thread ]</a>
              <a href="https://lists.mindrot.org/pipermail/openssh-unix-announce/2024-January/subject.html#156">[ subject ]</a>
              <a href="https://lists.mindrot.org/pipermail/openssh-unix-announce/2024-January/author.html#156">[ author ]</a>
         </li>
       </ul>

<hr>
<a href="https://lists.mindrot.org/mailman/listinfo/openssh-unix-announce">More information about the openssh-unix-announce
mailing list</a><br>

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[A TV Show Forced Britain's Devastating Post Office Scandal into the Light (117 pts)]]></title>
            <link>https://www.nytimes.com/2024/01/10/world/europe/uk-itv-mr-bates-vs-post-office.html</link>
            <guid>38951802</guid>
            <pubDate>Thu, 11 Jan 2024 13:16:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/01/10/world/europe/uk-itv-mr-bates-vs-post-office.html">https://www.nytimes.com/2024/01/10/world/europe/uk-itv-mr-bates-vs-post-office.html</a>, See on <a href="https://news.ycombinator.com/item?id=38951802">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/01/10/world/europe/uk-itv-mr-bates-vs-post-office.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: GodotOS: A Fake Operating System Interface Made in the Godot Engine (263 pts)]]></title>
            <link>https://github.com/popcar2/GodotOS</link>
            <guid>38951172</guid>
            <pubDate>Thu, 11 Jan 2024 12:09:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/popcar2/GodotOS">https://github.com/popcar2/GodotOS</a>, See on <a href="https://news.ycombinator.com/item?id=38951172">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:popcar2/GodotOS" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="u6_vHEwWiRrWWjuQU2BmKaOH8VKtw05UZF0bJ6YaMNTbAW8ckmG_UL7p0Fgnexpg-jE8ehpWsZSd6cWDMOBC0g" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="popcar2/GodotOS" data-current-org="" data-current-owner="popcar2" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=tEHAl%2BSSzHsYD%2Bemfa%2BtOQ3KmonEik9gA3xdfzxcnZe8mKyTC7S2a%2FzZ4IStk90%2FxpNHdWUnEG%2BT2tR%2BLtAAQO1YReHzwvyo4VjdeNRQaX4nqWyU%2Bng90MQNJq%2BlvOAwNis9kYc39gZ3ibrOAYVU43i6cTnEV5uP6blyWBFX2EhagPX8iBOXip%2F1DX8d9tTQiO6dnKqvk%2Fp127PrTfHBaUoVAEQ2ikFrBl%2F5yK75ragW84TrYUbd%2B619N1aNbToypbTgD41u5OISMdAD%2B59DDWQiUxRD1KpjAqHWH%2FLMPoUgzpdsj5HjjbvShMpzceI79nxu3j3AuasrwthC5%2BgcNUNFPIiIbKTDNlmal06D02r9AOUGLMqQS9NUxDExrHO2YPCSKx5Rz62ASEdBoI6L1hqzQFTBlYek5DWHk1O%2FyUd8L95ls3VklijevqFQnRwOrVO6ImSuIkv2hJZ6A%2FCSHsQFISi8iXOF8oxDS7%2FlvIVXgvlPdoRJDA%2BMyqYCziywP6kUWN6AwtONZQ%3D%3D--AuitZ%2BFPr7dDyiZ2--5w6uhdp23nz4K8TggV1rWg%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=popcar2%2FGodotOS" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/popcar2/GodotOS&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="d57be7798187a2abc9a8c85f23a4ba7c03e638ffede3a9986e1060fa892ec613" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Tell HN: Bash.org Is No More (277 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=38950721</link>
            <guid>38950721</guid>
            <pubDate>Thu, 11 Jan 2024 11:17:08 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=38950721">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="38951803"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38951803" href="https://news.ycombinator.com/vote?id=38951803&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><p><span>Oh man, that is so sad to hear.<p>bash.org has given me endless laughs. It always cheered me up.</p><p>Its too bad the the top 100/200 hadn't changed in years. I guess that's because IRC has been mostly dead for a while now (no more new submission) and that the voting algorithm favored a self-feeding feedback loop. Nonetheless, it was fun to come back once every few years and re-read the top quotes.</p><p>Hopefully someone revives the site. Hopefully it's just that the server needs some love or something. Do we have any idea who is behind it?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="38951992"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_38951992" href="https://news.ycombinator.com/vote?id=38951992&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><p><span>&gt; Its too bad the the top 100/200 hadn't changed in years.<p>I checked it every couple years or so when I remembered some part of some top quote and wanted to get the full thing. I always saw the top quotes never changed, so I just assumed the entire site wasn’t really updated.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="38951271"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38951271" href="https://news.ycombinator.com/vote?id=38951271&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><p><span>It was down for a couple of months already. However, the IP and server seems to be there. Maybe the person who keeps that up will restart the daemons when they remember they operate one of the nostalgia cornerstones of better part of the internet.<p>Maybe the server's password is hunter2. Let's see whether can I access it.</p><p>Edit: Nope. Seems firewalled.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="38951296"><td></td></tr>
                <tr id="38951319"><td></td></tr>
                <tr id="38951553"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_38951553" href="https://news.ycombinator.com/vote?id=38951553&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><p><span>That's because of the HN security. It prints all passwords as stars.<p>You can try putting your HN password in a comment, it would be visible only by you, and the others will not see it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="38951756"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_38951756" href="https://news.ycombinator.com/vote?id=38951756&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><p><span>I guess you're right. My HN password is *****.<p>When I edit, I can see it, but when I save the comment, it becomes starred. It even randomizes the length every time I save.</p><p>Brilliant!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="38951622"><td></td></tr>
                <tr id="38951974"><td></td></tr>
                  <tr id="38951611"><td></td></tr>
                <tr id="38951724"><td></td></tr>
                <tr id="38951904"><td></td></tr>
                <tr id="38952042"><td></td></tr>
                                                <tr id="38951867"><td></td></tr>
                <tr id="38952192"><td></td></tr>
            <tr id="38951910"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_38951910" href="https://news.ycombinator.com/vote?id=38951910&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><p><span>Host resolves, packets are dropped (ICMP timeouts, but nothing is "unreachable"). My sysadmin gut says that the server is there, behind a firewall, and the webserver is down/stopped, or the firewall is killing everything.<p>The IP is not shared. It reverse-resolves, too.</p><p>So, it's not dismantled and thrown to side.</p><p>Looks like the hosting provider, Idologic, got bought by Stablepoint. Maybe they have somehow blocked the site during the merger?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="38952039"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_38952039" href="https://news.ycombinator.com/vote?id=38952039&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><br><div>
                  <p><span>One of the most famous quotes was about a server that is online and pings, but the sysadmin doesn't know anymore where it physically is.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="38952184"><td></td></tr>
            <tr id="38952088"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_38952088" href="https://news.ycombinator.com/vote?id=38952088&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><br><div>
                  <p><span>Ah, I probably missed it because we had the following dialogue <i>at the office</i>.<pre><code>    &lt;senior-sysadm&gt; Hey bayindirh, is the log server up?
    &lt;bayindirh&gt; *SSHs to server* Yes, it's up and running nicely.
    &lt;senior-sysadm&gt; Where's that thing in the system room?
    &lt;bayindirh&gt; *Scratches head* Umm, I don't know?
    &lt;senior-sysadm&gt; Go find it, we'll upgrade it to newer HW.
    &lt;bayindirh&gt; Uh, OK. *leaves desk to dig the system room*.
</code></pre>
P.S.: I'm the one who installed that server physically and configured it in the first place. :D</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="38952000"><td></td></tr>
                              <tr id="38951809"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38951809" href="https://news.ycombinator.com/vote?id=38951809&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><br><div>
                  <p><span>I just had an argument over IRC with a stranger on the internet last week.  We're still out there.<p>I thought the humor on this site hadn't aged well, but this one got me:</p><pre><code>    &lt; pronto&gt; :(
    &lt; GiftdKook&gt; Turn that frown upside down!
    &lt; korozion&gt; ):</code></pre></span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="38951888"><td></td></tr>
                <tr id="38951960"><td></td></tr>
                <tr id="38952158"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_38952158" href="https://news.ycombinator.com/vote?id=38952158&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><p><span>That reminds me about the time when I used to run a similar website but focused on quotes from the IRC run by Flashback Forum (one of the larger/the largest? discussion forums in the Nordics).<p>Apparently I put the source for the site on GitHub (<a href="https://github.com/victorb/Flashback-Citat">https://github.com/victorb/Flashback-Citat</a> [12 year old PHP code!]) but I cannot find any actual archive of any of the quotes nor the running website, sadly :/
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="38951916"><td></td></tr>
            <tr id="38952018"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_38952018" href="https://news.ycombinator.com/vote?id=38952018&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><br><div>
                  <p><span>bash.im, the Russian version, was replaced with a "NO WAR" message for a while, and now it's gone.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="38951356"><td></td></tr>
                <tr id="38952006"><td></td></tr>
                  <tr id="38951864"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38951864" href="https://news.ycombinator.com/vote?id=38951864&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><br><div>
                  <p><span>This is what happens when you don't use a proper capital-S Stack. Probably they weren't even using Kubernetes and a separate multi-cloud management DB for monitoring their data pipeline ingest.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="38950831"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38950831" href="https://news.ycombinator.com/vote?id=38950831&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><p><span>Thanks to the folk who put it together and ran it nearly 20 years. I wonder how much the whole thing cost?<p>Kind of thing that gets put together as a "hey this is cool" project, it runs and people use it, so we'll just leave it up...  months later its <i>more</i> popular, the original authors of the system moved on, but no one can just pull the plug now. this is a <i>public resource</i>, so we'll just keep feeding it.</p><p>Years later, someone may go through and fix the design problems; or not. It might be no one figured out how to resolve the dependency on PHP2 or Python1 the original code may have had.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="38951438"><td></td></tr>
                <tr id="38951678"><td></td></tr>
                <tr id="38951851"><td></td></tr>
                  <tr id="38951976"><td></td></tr>
                  <tr id="38951679"><td></td></tr>
                  <tr id="38951507"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38951507" href="https://news.ycombinator.com/vote?id=38951507&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><br><div>
                  <p><span>Always surprised when some of these sites shut down. The operating cost seems low and putting on a few ads (ethical, non-intrusive, etc.) can net you passive $100+/mo.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="38951739"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_38951739" href="https://news.ycombinator.com/vote?id=38951739&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><p><span>It's more than money, they still require care and feeding.<p>At some age (and I'm getting to that), you just tire of being a sysadm, esp. for "home"/hobby stuff.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="38951956"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_38951956" href="https://news.ycombinator.com/vote?id=38951956&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><p><span>Even if it's a static HTML, you need to patch your webserver, OS, and migrate the whole stack to newer versions.<p>This is why I'm scaling down my home infrastructure to SBCs and run everything on Debian with stock package repositories. It reduces tons of burden to something very manageable.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="38952002"><td></td></tr>
                <tr id="38952060"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_38952060" href="https://news.ycombinator.com/vote?id=38952060&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><p><span>A webserver like Apache and NGINX are way more complex than they look. It's sometimes possible to exploit bugs with benign/simple requests, even if you don't run advanced stacks on them. See [0] for example.<p>If you're not running strict firewall rules to limit your SSH access and if you expose other services outside, they also need constant patching against newer attacks.</p><p>Lastly, security standards evolve. Your SSH and SSL layers need to be kept up to date to patch holes and add newer algorithms while deprecating others, further reducing the attack surface.</p><p>[0]: <a href="https://www.exploit-db.com/exploits/50383" rel="nofollow">https://www.exploit-db.com/exploits/50383</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="38952065"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_38952065" href="https://news.ycombinator.com/vote?id=38952065&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><br><div>
                  <p><span>Because no software is perfect, which means every lock has weaknesses that sooner or later get found out. Chances are that, say, a Linux 2.x server that was considered "very secure" in 2005 would now be pwned in a few hours.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="38952069"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_38952069" href="https://news.ycombinator.com/vote?id=38952069&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><br><div>
                  <p><span>Because both the Linux kernel and whatever SSL and web server stack you use regularly have their remotely exploitable vulnerabilities.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="38951900"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_38951900" href="https://news.ycombinator.com/vote?id=38951900&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><br><div>
                  <p><span>A historical archive of something should allow for robust, non-interactive ways to persist. Maybe there should be standards for this. In the mean time we can find gratitude to archive.org and similar services</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="38951385"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38951385" href="https://news.ycombinator.com/vote?id=38951385&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><p><span>&gt;old internet<p>Reminds me of Gigablast disappearing, a search engine that was in the spotlight in the early 00's, a sole developer competing amongst AlltheWeb and Google.</p><p>When their site disappeared there was barely a mention.</p><p>I guess since the mass of geocities was uprooted it's become the norm, the churn of the web and generally accepted. archive.org is great, but it does seem strange how transient information has become on the web. HN and archive.org have good memories.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="38951562"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38951562" href="https://news.ycombinator.com/vote?id=38951562&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><p><span>Wasn't it already dead for ages?<p>I remember back when it got popular it seemed to stop accepting submissions after a short time.</p><p>And the hunter2 stuff got stuck on the top list forever, probably because the mechanism is self-reinforcing by making it easiest to vote for the stuff already on the top.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="38951836"><td></td></tr>
                <tr id="38951979"><td></td></tr>
                  <tr id="38950912"><td></td></tr>
                <tr id="38951746"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_38951746" href="https://news.ycombinator.com/vote?id=38951746&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><p><span>Would make for a nice webservice, like those Pokemon/StarWars/etc apis.<p>Ironically, those quotes will likely survive a lot of more modern content. Even viral stuff, these days, will disappear incredibly quickly - bat an eyelid and the imgur link is broken, the twitter post is paywalled, the reddit thread is taken down... And any private service like Discord or Slack will happily burn everything after a few months.</p><p>"The internet does not forget" is such a massive lie.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="38951774"><td></td></tr>
                        <tr id="38951523"><td></td></tr>
            <tr id="38951743"><td></td></tr>
                <tr id="38951854"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_38951854" href="https://news.ycombinator.com/vote?id=38951854&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><p><span>Maybe this helps?<p>&gt; Managers: Ninety</p><p>&gt; Moderators: Amanda, vx0, kastein</p><p>I'm guessing they might be around on Libera or Freenode
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="38951879"><td></td></tr>
            <tr id="38952003"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38952003" href="https://news.ycombinator.com/vote?id=38952003&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><p><span>I remember first being pointed to the site for having said: <i>Wanting a man who doesn't smell is like wanting a woman who doesn't talk.</i><p>Its importance was immediately obvious.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="38951413"><td></td></tr>
                <tr id="38951714"><td></td></tr>
                <tr id="38951886"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_38951886" href="https://news.ycombinator.com/vote?id=38951886&amp;how=up&amp;goto=item%3Fid%3D38950721"></a></center>    </td><td><p><span>Is it? It was its own private entity and now it's offline probably forever.<p>Edit: I think it'd be far better off on Wikipedia than Twitter
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="38951966"><td></td></tr>
            <tr id="38952047"><td></td></tr>
                              </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta unlawfully ignores the users' right to easily withdraw consent (234 pts)]]></title>
            <link>https://noyb.eu/en/meta-ignores-users-right-easily-withdraw-consent</link>
            <guid>38949856</guid>
            <pubDate>Thu, 11 Jan 2024 09:33:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://noyb.eu/en/meta-ignores-users-right-easily-withdraw-consent">https://noyb.eu/en/meta-ignores-users-right-easily-withdraw-consent</a>, See on <a href="https://news.ycombinator.com/item?id=38949856">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
                    <p><strong>Since the beginning of November, Instagram and Facebook users who don’t want to be tracked have to pay a “privacy fee” of up to € 251.88 per year. While one (free) click is enough to consent to being tracked, users can only withdraw their consent by going through the complicated process of switching to a paid subscription. This is illegal, as the GDPR clearly states that withdrawing your consent must be “as easy as” giving it. In addition to a </strong><a href="https://noyb.eu/en/noyb-files-gdpr-complaint-against-meta-over-pay-or-okay"><strong>previous </strong><em><strong>noyb </strong></em><strong>complaint</strong></a><strong> relating to the consent phase of the “pay or okay” system, </strong><em><strong>noyb </strong></em><strong>has filed an additional complaint with the Austrian data protection authority today to take into account the withdrawal situation.</strong></p><ul><li><a href="https://noyb.eu/sites/default/files/2024-01/Meta_Withdrawal_Complaint_REDACTED_EN.pdf">Complaint against Meta on the withdrawal issue</a> (EN)</li><li><a href="https://noyb.eu/sites/default/files/2023-11/Complaint%20-%20Meta%20Pay%20or%20Okay%20-%20REDACTED.pdf">Previous complaint relating to the consent phase</a> (EN – Machine translation)</li></ul><p><strong>Meta’s latest attempt to circumvent EU privacy laws.</strong> It has merely been six months since the European Court of Justice (CJEU)<a href="https://noyb.eu/en/cjeu-declares-metafacebooks-gdpr-approach-largely-illegal"> ruled that Meta’s handling of user data was illegal</a>. Yet the social media giant has launched its third attempt to circumvent European privacy laws. Instead of asking users for their consent, Meta is now charging people for choosing a privacy-friendly setting. As of the beginning of November 2023, it costs up to €251.88 a year to maintain your fundamental right to data protection on Facebook and Instagram. Users who don’t want to pay will have to accept being tracked for targeted advertising. <em>noyb </em><a href="https://noyb.eu/en/noyb-files-gdpr-complaint-against-meta-over-pay-or-okay">has already filed a complaint</a> against this approach in November 2023.</p><p><strong>Expensive withdrawal of consent.</strong> But Meta’s seriously flawed approach to free consent isn’t the only issue at hand. Once users have consented to being tracked, there’s no easy way to withdraw it at a later date. This is illegal. Despite Article 7 of the GDPR clearly stating that “it shall be as easy to withdraw as to give consent”, the only option to “withdraw” the (one-click) consent, is to buy a € 251.88 subscription. In addition, the complainant had to navigate through several windows and banners to find the page where he could actually revoke consent.</p><p>Massimiliano Gelmi, data protection lawyer at <em>noyb</em>: <em>“The law is clear, withdrawing consent must be as easy as giving it in the first place. It is painfully obvious that paying € 251,88 per year to withdraw consent is not as easy as clicking an “Okay” button to accept the tracking.”</em></p><p><strong>Clear violation.</strong> The European Data Protection Board (EDPB) even mention monetary costs as an example of a burden that is incompatible with the principle of Article 7 GDPR in its guidelines, making it clear that Meta is making the withdrawal of consent not nearly as easy as to give consent.</p><p><strong>Complaint filed in Austria.</strong> <em>noyb </em>has now filed a complaint with the Austrian data protection authority (DSB) on behalf of one complainant. The authority should order Meta to bring its processing operations in compliance with European data protection law and to provide users with an easy way to withdraw their consent – without having to pay a fee. In addition, <em>noyb </em>suggests that the authorities should impose a fine to prevent further violations of the GDPR. The case will likely be forwarded by the Austrian DSB to the Irish DPC, who is the “lead authority” for Meta in the EU.</p>
            
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building a baseline JIT for Lua automatically (2023) (104 pts)]]></title>
            <link>https://sillycross.github.io/2023/05/12/2023-05-12/</link>
            <guid>38949231</guid>
            <pubDate>Thu, 11 Jan 2024 08:11:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sillycross.github.io/2023/05/12/2023-05-12/">https://sillycross.github.io/2023/05/12/2023-05-12/</a>, See on <a href="https://news.ycombinator.com/item?id=38949231">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
      
        <blockquote>
<p><text>This is the Part 2 of a series. Feel free to read the prequel for more context: <a href="https://sillycross.github.io/2022/11/22/2022-11-22/">Building the fastest Lua interpreter automatically</a></text></p>
</blockquote>
<p>Building a good VM for a dynamic language takes a ton of engineering. The best-performing VMs (e.g., <a href="https://github.com/WebKit/WebKit/tree/main/Source/JavaScriptCore" target="_blank" rel="noopener">JavaScriptCore</a>, <a href="https://v8.dev/" target="_blank" rel="noopener">V8</a>, <a href="https://firefox-source-docs.mozilla.org/js/index.html" target="_blank" rel="noopener">SpiderMonkey</a>) employ at least 3 VM tiers (interpreter, baseline JIT<sup><a href="#fn1" id="fnref1">[1]</a></sup> and optimizing JIT), and pervasively use hand-coded assembly in every VM tier<sup><a href="#fn2" id="fnref2">[2]</a></sup>. Optimizations such as inline caching and type speculation are required to get high performance, but they require high expertise and introduce additional engineering complexity.</p>
<!--The best interpreters are hand-coded in assembly. The baseline JIT compiler[^1] is more assembly, plus an assembler to assemble them to machine code at runtime. The optimizing JIT compiler is even more assembly among other things[^2]. -->
<!--### Deegen: High Performance VMs at Low Engineering Cost-->
<p><em>Deegen</em> is my research meta-compiler to make high-performance VMs easier to write. Deegen takes in a semantic description of the VM bytecodes in C++, and use it as the single source of truth to <em>automatically generate</em> a high-performance VM at build time, as illustrated below.</p>
<p><img src="https://sillycross.github.io/images/2023-05-12/deegen-arch.png" alt="Deegen: automatically generating a high-performance VM!"></p>
<p>In <a href="https://sillycross.github.io/2022/11/22/2022-11-22/">a prior post</a>, we used Deegen to automatically generate the fastest Lua 5.1 interpreter to date, outperforming LuaJIT’s interpreter by an average of 34% across a variety of Lua benchmarks. The VM was named <a href="https://github.com/luajit-remake/luajit-remake" target="_blank" rel="noopener"><em>LuaJIT Remake</em></a>, even though it had no JIT tiers at that time.</p>
<p>Today, after months of additional work, <a href="https://github.com/luajit-remake/luajit-remake" target="_blank" rel="noopener"><em>LuaJIT Remake</em></a> is finally a JIT-capable VM. It is now equipped with a state-of-the-art baseline JIT compiler, also automatically generated by Deegen. The baseline JIT features:</p>
<ul>
<li>Extremely fast compilation speed.</li>
<li>High-quality machine code (under the design constraints of a baseline JIT).</li>
<li>Automatic call inline caching (IC) with two modes (direct/closure call).</li>
<li>Automatic generic inline caching (IC) <a href="https://sillycross.github.io/2022/11/22/2022-11-22/#deegen_generic_inline_caching_api">driven by Deegen API</a>.</li>
<li>Self-modifying-code-based IC implementation for best performance.</li>
<li>Hot-cold-splitted JIT code for less branches and better code locality.</li>
</ul>
<p>It is important to note that the baseline JIT is generated from the <em>same</em> bytecode semantic description that Deegen uses to generate the interpreter. Therefore, for a language implementer, the baseline JIT comes <em>for free</em>:</p>
<ul>
<li>No need to have any assembly knowledge.</li>
<li>No need to manually engineer the JIT.</li>
<li>No need to manually keep the JIT updated with new language features.</li>
</ul>
<p>Because Deegen does all the work automatically!</p>
<p>Of course, this is no easy feat. In order to generate the baseline JIT automatically, a sophiscated build-time pipeline is employed, as illustrated below.</p>
<p><img src="https://sillycross.github.io/images/2023-05-12/deegen-jit-pipeline.png" alt="The pipeline that automatically generates the baseline JIT from bytecode semantics"></p>
<p>As a side note, LLVM is only used at build time to generate the JIT. The generated baseline JIT is self-contained, and does not use LLVM at runtime.</p>
<p>At runtime, the generated baseline JIT generates machine code using <em>Copy-and-Patch</em> (we’ll cover it in a minute). Except that, it heavily follows the design of the <a href="https://webkit.org/blog/10308/speculation-in-javascriptcore/" target="_blank" rel="noopener">baseline JIT in JavaScriptCore</a>, and has employed most of their optimizations. As such, we claim that our baseline JIT qualifies as a state-of-the-art.</p>
<p>In the rest of the post, we will explore the internals of how Deegen generates the baseline JIT in more detail. It is organized as follows:</p>
<ul>
<li>A gentle introduction of the relavent backgrounds (VM, JIT, IC, etc.).</li>
<li>An overview of the Copy-and-Patch technique, the core tool employed by the generated JIT to generate machine code at runtime.</li>
<li>How Deegen further extends Copy-and-Patch to fully automate the process and fit it for the domain-specific use cases of dynamic languages.</li>
<li>An end-to-end example of the machine code generated by the baseline JIT.</li>
<li>Performance evaluation and conclusion thoughts.</li>
</ul>
<h3 id="Background-a-name-deegen-baseline-jit-background-section-a">Background<a name="deegen_baseline_jit_background_section"></a></h3>
<div id="folded_modern_dynamic_language_vm_background_section">
<p>Not everyone is familiar with topics like modern dynamic language VM, multi-tier JIT compilers, baseline JIT, speculative compilation, inline caching, OSR exit…  Therefore, I prepared a background section to gently introduce the background contexts for this post.</p>
<p>Due to its length, I folded this section by default. <a onclick="document.getElementById('modern_dynamic_language_vm_background_section').style.display='inline'; document.getElementById('folded_modern_dynamic_language_vm_background_section').style.display = 'none';">To unfold, click here</a>.</p>
</div> <!-- background section -->
<h3 id="How-to-Generate-Machine-Code-a-name-after-background-section-a">How to Generate Machine Code?<a name="after_background_section"></a></h3>
<p>For every JIT, this is an unavoidable problem: how do you generate machine code?</p>
<p>A typical solution used by many (<a href="https://github.com/WebKit/WebKit/tree/main/Source/JavaScriptCore" target="_blank" rel="noopener">JSC</a>, <a href="https://v8.dev/" target="_blank" rel="noopener">V8</a>, <a href="https://luajit.org/" target="_blank" rel="noopener">LuaJIT</a>, etc) is a <a href="https://sillycross.github.io/r/WebKit/Source/JavaScriptCore/assembler/AbstractMacroAssembler.h.html">hand-coded assembler</a>. The assembler provides APIs (e.g., <code>EmitMovRegReg64</code>) to the JIT, which the JIT uses to emit assembly instructions as machine code one by one.</p>
<p>However, such an approach is clearly infeasible for a meta-compiler like Deegen, as our input is expressed as C++ bytecode semantics.</p>
<p>So can we use LLVM directly at runtime to generate code? Unfortunately this is also impractical, as LLVM’s compilation speed is <a href="https://webkit.org/blog/5852/introducing-the-b3-jit-compiler/" target="_blank" rel="noopener">too slow even for a heavyweight optimizing JIT</a>, not to mention a baseline JIT where fast compilation is a top concern.</p>
<h3 id="Copy-and-Patch-the-Art-of-Repurposing-Existing-Tools">Copy-and-Patch: the Art of Repurposing Existing Tools</h3>
<p>The solution is a paper I wrote years ago: <a href="https://sillycross.github.io/assets/copy-and-patch.pdf">Copy-and-Patch Compilation</a>.</p>
<p>In one sentence, Copy-and-Patch is a trick that allows one to generate code without knowing anything about how to generate code.</p>
<p>How is that even possible? While the paper is long, the trick is actually extremely simple, which I will explain here.</p>
<p>Consider the following C++ function:</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br></pre></td><td><pre><span><span><span>int</span> <span>evaluate_lhs</span><span>()</span></span>;</span><br><span><span><span>int</span> <span>evaluate_rhs</span><span>()</span></span>;</span><br><span><span><span>int</span> <span>evaluate_add</span><span>()</span> </span>{</span><br><span>  <span>return</span> evaluate_lhs() + evaluate_rhs();</span><br><span>}</span><br></pre></td></tr></tbody></table></figure>
<p>When the C++ compiler compiles the above code, it knows nothing about the definition of <code>evaluate_lhs</code> and <code>evaluate_rhs</code>. But it can somehow produce an object file, and the linker can link the object file to <em>any</em> definition of <code>evaluate_lhs</code> and <code>evaluate_rhs</code>, and the final executable would just work.</p>
<h4 id="Relocation-Code-Generation">Relocation = Code Generation</h4>
<p>What does it mean? The object file must contain structured information on how to link <code>evaluate_add</code> against <em>any</em> definition of <code>evaluate_lhs</code> and <code>evaluate_rhs</code>. So if we parse the object file to get that info, at runtime, we can act as the linker, and “link” <code>evaluate_add</code> against any runtime-known <code>evaluate_lhs</code> and <code>evaluate_rhs</code> of our choice to perform an <code>add</code>. This is effectively a JIT<sup><a href="#fn8" id="fnref8">[8]</a></sup>!</p>
<p>Of course, the “structured information” has its formal name: <em>linker relocation records</em>. But the name is not important. The important thing is as long as we parsed out those information, we can use them at runtime to emit executable code. And this process is extremely cheap: all it takes is a <code>memcpy</code> followed by a few scalar additions, thus the name “Copy-and-Patch”.</p>
<p>For example, the <code>evaluate_add</code> we just saw will produce an object file with the following contents:</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br></pre></td><td><pre><span>evaluate_add:</span><br><span>  53 e8 00 00 00 00 89 c3 e8 00 00 00 00 01 d8 5b c3 </span><br></pre></td></tr></tbody></table></figure>
<p>with the following linker relocation record:</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br></pre></td><td><pre><span>offset = 2, type = R_X86_64_PLT32, sym = evaluate_lhs, addend = -4</span><br><span>offset = 9, type = R_X86_64_PLT32, sym = evaluate_rhs, addend = -4</span><br></pre></td></tr></tbody></table></figure>
<p>Then, the following copy-and-patch logic would allow one to JIT this function at any address with any desired <code>evaluate_lhs</code> and <code>evaluate_rhs</code> targets:</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br></pre></td><td><pre><span><span><span>void</span> <span>codegen</span><span>(<span>uint8_t</span>* dst, <span>uint8_t</span>* lhsFn, <span>uint8_t</span>* rhsFn)</span> </span>{</span><br><span>  </span><br><span>  <span>constexpr</span> <span>uint8_t</span> code[] = { </span><br><span>    <span>0x53</span>, <span>0xe8</span>, <span>0x00</span>, <span>0x00</span>, <span>0x00</span>, <span>0x00</span>, <span>0x89</span>, <span>0xc3</span>, </span><br><span>    <span>0xe8</span>, <span>0x00</span>, <span>0x00</span>, <span>0x00</span>, <span>0x00</span>, <span>0x01</span>, <span>0xd8</span>, <span>0x5b</span>, <span>0xc3</span> };</span><br><span>  </span><br><span>  <span>memcpy</span>(dst, code, <span>sizeof</span>(code));</span><br><span>  </span><br><span>  *(<span>uint32_t</span>*)(dst + <span>2</span>) = (<span>uint32_t</span>)(lhsFn - (dst + <span>2</span>) - <span>4</span>);</span><br><span>  *(<span>uint32_t</span>*)(dst + <span>9</span>) = (<span>uint32_t</span>)(rhsFn - (dst + <span>9</span>) - <span>4</span>);</span><br><span>}</span><br></pre></td></tr></tbody></table></figure>
<p>Yes, that’s all of the core trick of Copy-and-Patch: at build time, compile the logic pieces we want to JIT into object file, and parse the object file to obtain the unrelocated code and relocation records (a <em>stencil</em> in the paper’s terminology). At runtime, code generation is simply wiring up the stencils and materializing them into executable code by a <em>copy</em> (<code>memcpy</code>) and a few <em>patches</em> (scalar additions).</p>
<h4 id="Continuation-Passing-Style-Branch">Continuation-Passing Style = Branch</h4>
<p>The generated code above works, but the code quality is miserable. Everything is executed by a <code>call</code> to another function, which is a lot of overhead.</p>
<p>However, what if one rewrites the code to <a href="https://dl.acm.org/doi/10.1145/800179.810196" target="_blank" rel="noopener">continuation-passing style</a>?</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br></pre></td><td><pre><span><span><span>void</span> <span>continuation</span><span>(<span>int</span> result)</span></span>;</span><br><span><span><span>void</span> <span>evaluate_add</span><span>(<span>int</span> lhs, <span>int</span> rhs)</span> </span>{</span><br><span>  <span>int</span> result = lhs + rhs;</span><br><span>  [[clang::musttail]] <span>return</span> continuation(result);</span><br><span>}</span><br></pre></td></tr></tbody></table></figure>
<p>Now, the calls are gone. Furthermore, the function will end with a <code>jmp</code> instruction to function <code>continuation</code> (since the call is a <a href="https://en.wikipedia.org/wiki/Tail_call" target="_blank" rel="noopener">tail call</a>). Since we have control over where to put each function at, if we put <code>continuation</code> right after <code>evaluate_add</code>, then we can even eliminate the <code>jmp</code> to a fallthrough altogether<sup><a href="#fn9" id="fnref9">[9]</a></sup>.</p>
<p>After employing this trick, it’s fairly easy to prove that the generated code will not contain unnecessary <code>jmp</code> instructions: all the branches must correspond to actual control flow edges in the generated logic.</p>
<p>One of the main reasons that interpreters are slow is the unpredictable indirect dispatch. At this stage, our generated code has no indirect dispatch, in fact, no unnecessary branches at all. This is already a big speedup over an interpreter.</p>
<h4 id="Address-of-External-Symbol-Runtime-Constant">Address of External Symbol = Runtime Constant</h4>
<p>Another important reason that JITs are faster than interpreters is the ability of JIT to burn runtime constants (bytecode operands, etc) into the instruction stream. Can we support it as well?</p>
<p>Of course! The trick is simple:</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br></pre></td><td><pre><span><span>extern</span> <span>char</span> x;  </span><br><span><span><span>void</span> <span>continuation</span><span>(<span>uint64_t</span> value)</span></span></span><br><span><span><span>void</span> <span>pointer_dereference_at_fixed_offset</span><span>(<span>void</span>* ptr)</span> </span>{</span><br><span>  </span><br><span>  <span>uint64_t</span> result = *(<span>uint64_t</span>*)((<span>uint64_t</span>)ptr + (<span>uint64_t</span>)&amp;x);</span><br><span>  [[clang::musttail]] <span>return</span> continuation(result);</span><br><span>}</span><br></pre></td></tr></tbody></table></figure>
<p>All it takes is to define an external symbol, and use its <em>address</em> as the runtime constant we want to use. Since by definition an external symbol is external, the compiler cannot assume anything about where it resides at<sup><a href="#fn10" id="fnref10">[10]</a></sup>. This gives us a way to represent an opaque constant value.</p>
<p>Of course, the linker knows how to patch the code to make the external symbol point to the right location. Thus, we can patch it at runtime to make it represent any runtime constant as well :)</p>
<h4 id="Function-Prototype-Register-Allocation-Combinatorial-Explosion-Instruction-Selection">Function Prototype = Register Allocation / Combinatorial Explosion = Instruction Selection</h4>
<p>Finally, there are <a href="https://developers.redhat.com/blog/2020/01/20/mir-a-lightweight-jit-compiler-project#lightweight_jit_compiler_project_goals" target="_blank" rel="noopener">two most important codegen-level optimizations</a>: register allocation and instruction selection<sup><a href="#fn11" id="fnref11">[11]</a></sup>. Can we support them as well? The answer is yes. However, these optimizations are primarily only useful for the static language use cases where each bytecode only implements very simple logic<sup><a href="#fn12" id="fnref12">[12]</a></sup>. So to keep this post focused, I will not go into details.</p>
<h4 id="Copy-and-Patch-Wrapping-up">Copy-and-Patch: Wrapping up</h4>
<p>I wouldn’t mind at all if you view Copy-and-Patch as a big hack: because it is! But it works! And it works nicely!</p>
<p>As shown <a href="https://sillycross.github.io/assets/copy-and-patch.pdf">in the paper</a>, one can use Copy-and-Patch to construct extremely fast baseline JIT that <em>significantly</em> outperforms the existing state-of-the-arts:</p>
<ul>
<li>For WebAssembly, we <a href="https://github.com/sillycross/WasmNow" target="_blank" rel="noopener">implemented a baseline JIT</a> that compiles 4.9x-6.5x faster than Google Chrome’s <a href="https://v8.dev/blog/liftoff" target="_blank" rel="noopener">Liftoff baseline compiler</a>, while also generating 39%-63% faster code.</li>
<li>For SQL database, we implemented a prototype <a href="https://github.com/sillycross/PochiVM" target="_blank" rel="noopener">SQL query baseline JIT</a> that on TPC-H queries, compiles &gt;1000x faster than LLVM -O3, while only generating 24% slower code.</li>
</ul>
<p>Furthermore, Copy-and-Patch perfectly suits Deegen’s needs for a JIT:</p>
<ol>
<li>It does not know or care about what is being JIT’ed. The logic we want to JIT is directly compiled by a C++ compiler into an object file at build time. C&amp;P merely parses the object file to produce the <em>stencils</em>, which can then be used to JIT code at runtime.</li>
<li>The code generation at runtime is extremely fast, which perfectly matches the requirement of a baseline JIT. Note that we are doing a lot of expensive preprocessing work, but all of them happen at build time.</li>
</ol>
<h3 id="Deegen-the-Art-of-Repurposing-Existing-Tools-Continued">Deegen: the Art of Repurposing Existing Tools, Continued</h3>
<p>While Copy-and-Patch is a nice technique, its vanilla form as described above is still not enough to fulfill Deegen’s use case. Specifically, the vanilla Copy-and-Patch still requires quite a bit of manual work to implement the stencils and the runtime logic, whereas in Deegen, all must be fully automatic.</p>
<p>As it turns out, fully automating Copy-and-Patch requires significant design-level improvements to the original technique, which we will cover in this section.</p>
<!-- The vanilla Copy-and-Patch does not support the important domain-specific optimizations required to make dynamic languages fast, e.g., inline caching and hot-cold code splitting. -->
<p>To make things easier to understand, we will use the following hypothetical <code>Add</code> bytecode as example (see <a href="https://sillycross.github.io/2022/11/22/2022-11-22/">prior post</a> for a detailed explanation of the code):</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br></pre></td><td><pre><span><span><span>void</span> <span>Add</span><span>(TValue lhs, TValue rhs)</span> </span>{</span><br><span>  <span>if</span> (!lhs.Is&lt;tDouble&gt;() || !rhs.Is&lt;tDouble&gt;()) {</span><br><span>    ThrowError(<span>"Can't add!"</span>);</span><br><span>  } <span>else</span> {</span><br><span>    <span>double</span> res = lhs.As&lt;tDouble&gt;() + rhs.As&lt;tDouble&gt;();</span><br><span>    Return(TValue::Create&lt;tDouble&gt;(res));</span><br><span>  }</span><br><span>}</span><br></pre></td></tr></tbody></table></figure>
<h4 id="Identifying-the-Runtime-Constants">Identifying the Runtime Constants</h4>
<p>To get good performance, it is almost mandetory for a JIT to be able to burn runtime constants (bytecode operands, etc.) into the instruction flow. In vanilla Copy-and-Patch, the programmer is required to declare the runtime constants by special macros. So our first improvement is to make this step automatic.</p>
<p>Fortunately this is fairly easy. In our case, the runtime constants are the bytecode operands, and for the IC, everything in the IC state. Since Deegen is already responsible for generating the bytecode decoding logic and the encoding / decoding of the IC state, all we need to do is to not emit the decoding logic, but a magic function call, so that the later processing stages knows that the value is a runtime constant.</p>
<p>For example, for <code>Add</code>, we know that the bytecode slot ordinal of <code>lhs</code>, <code>rhs</code> and the output slot are runtime constants. So the bytecode semantic function will be lowered to LLVM IR that conceptually resembles the following C logic:</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br></pre></td><td><pre><span><span>size_t</span> lhs_slot = __runtime_constant_lhs_slot();</span><br><span><span>size_t</span> rhs_slot = __runtime_constant_rhs_slot();</span><br><span>TValue lhs = <span>stack</span>[lhs_slot];</span><br><span>TValue rhs = <span>stack</span>[rhs_slot];</span><br><span><span>if</span> (!lhs.Is&lt;tDouble&gt;() || !rhs.Is&lt;tDouble&gt;()) {</span><br><span>  ThrowError(<span>"Can't add!"</span>);</span><br><span>} <span>else</span> {</span><br><span>  <span>double</span> res = lhs.As&lt;tDouble&gt;() + rhs.As&lt;tDouble&gt;();</span><br><span>  </span><br><span>  <span>size_t</span> output_slot = __runtime_constant_output_slot();</span><br><span>  <span>stack</span>[output_slot] = TValue::Create&lt;tDouble&gt;(res);</span><br><span>  __dispatch_to_next_bytecode();</span><br><span>}</span><br></pre></td></tr></tbody></table></figure>
<p>Correspondingly, at runtime, in order for the generated JIT to generate code, it needs to decode the bytecode struct to retrieve all the operand values and use these values to materialize the copy-and-patch stencils: we will showcase the concrete generated implementation of the <code>__codegen_Add</code> function (which emits machine code for <code>Add</code> at runtime) later in the post.</p>
<h4 id="Propagating-the-Runtime-Constants">Propagating the Runtime Constants</h4>
<p>Acute readers may have noticed that the C logic above cannot result in optimal code. Consider line 3: <code>TValue lhs = stack[lhs_slot]</code>. What actually happens in this line is that we are decoding address <code>(uint64_t)stack + lhs_slot * 8</code> (since each <code>TValue</code> is 8 bytes). If we only make <code>lhs_slot</code> a runtime constant (as we are doing right now), there is no way for LLVM to fold <code>lhs_slot * 8</code> into a constant (recall that at LLVM level, a runtime constant is really the address of an external symbol). As a result, it will generate less-optimal code like <code>mov $XXXX, %rax; shl 3, %rax</code>.</p>
<p>Therefore, we need a customized LLVM constant propagation pass to identify all the constant expressions derived from the “root” runtime constants. Then, we should make each constant expression a runtime constant. Of course, this also means that at runtime, in order to populate these derived runtime constants with concrete values, the codegen function needs to replay the computation of the expression using the concrete values of the root runtime constants.</p>
<p>After this transform, the LLVM IR of our <code>Add</code> example would resemble the following C logic:</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br></pre></td><td><pre><span><span>size_t</span> tmp1 = __derived_runtime_constant_1();</span><br><span><span>size_t</span> tmp2 = __derived_runtime_constant_2();</span><br><span>TValue lhs = *(TValue*)((<span>uint64_t</span>)<span>stack</span> + tmp1);</span><br><span>TValue rhs = *(TValue*)((<span>uint64_t</span>)<span>stack</span> + tmp2);</span><br><span><span>if</span> (!lhs.Is&lt;tDouble&gt;() || !rhs.Is&lt;tDouble&gt;()) {</span><br><span>  ThrowError(<span>"Can't add!"</span>);</span><br><span>} <span>else</span> {</span><br><span>  <span>double</span> res = lhs.As&lt;tDouble&gt;() + rhs.As&lt;tDouble&gt;();</span><br><span>  </span><br><span>  <span>size_t</span> tmp3 = __derived_runtime_constant_3();</span><br><span>  *(TValue*)((<span>uint64_t</span>)<span>stack</span>+tmp3) = TValue::Create&lt;tDouble&gt;(res);</span><br><span>  __dispatch_to_next_bytecode();</span><br><span>}</span><br></pre></td></tr></tbody></table></figure>
<p>where the derived runtime constants <code>__derived_runtime_constant_1/2/3</code> are defined as follow:</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br></pre></td><td><pre><span>__derived_runtime_constant_1 := lhs_slot * 8</span><br><span>__derived_runtime_constant_2 := rhs_slot * 8</span><br><span>__derived_runtime_constant_3 := output_slot * 8</span><br></pre></td></tr></tbody></table></figure>
<h4 id="Fixing-the-Symbol-Range-Assumption">Fixing the Symbol Range Assumption</h4>
<p>As we already explained, in Copy-and-Patch, a runtime constant is expressed by the address of an external symbol.</p>
<p>While it is a neat trick that is crucial for high-quality code, it could break down and cause miscompilation in edge cases. For example, consider the code below:</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br></pre></td><td><pre><span><span>extern</span> <span>char</span> x;</span><br><span><span><span>void</span> <span>f</span><span>()</span> </span>{</span><br><span>  <span>uint64_t</span> val = (<span>uint64_t</span>)&amp;x;</span><br><span>  <span>if</span> (val == <span>0</span>) {</span><br><span>    </span><br><span>  }</span><br><span>}</span><br></pre></td></tr></tbody></table></figure>
<p>LLVM would deduce that the <code>val == 0</code> check is trivially false, and “optimize away” the whole if-clause. Why? Because <code>val</code> is the address of variable <code>x</code>, and of course the address of a variable is never <code>0</code>, good game.</p>
<p>In vanilla Copy-and-Patch, the programmer is responsible for avoiding such corner cases. But in Deegen, where stencils are automatically generated, we must find a systematic and provably-correct solution.</p>
<p>So what’s the issue? You might think the issue is “symbol must not be null”. That’s what I initially believed as well<sup><a href="#fn13" id="fnref13">[13]</a></sup>, but I later realized it is only the symptom of a much larger issue.</p>
<p>As it turns out, according to <a href="https://refspecs.linuxbase.org/elf/x86_64-abi-0.99.pdf" target="_blank" rel="noopener">x86-64 ABI</a>, every symbol will reside in address range <code>[1, 2^31 - 2^24)</code><sup><a href="#fn14" id="fnref14">[14]</a></sup>. This is also exactly the assumption held by LLVM, and used by LLVM to do optimizations (e.g., in the example above, it deduces that the address of a symbol must not equal <code>0</code>). So the “<code>val == 0</code> check” example is not the only buggy case. LLVM can, for example, do a zero extension instead of a sign extension, as it believes that the address of the symbol must have bit <code>31</code> being <code>0</code> thus a <code>ZExt</code> is equivalent to a <code>SExt</code>, causing buggy code if the runtime constant were to represent a negative value.</p>
<p>One might think the <code>[1, 2^31 - 2^24)</code> range assumption is artificial, but it isn’t. This range assumption is actually important to generate correct code. For a simple example, the code <code>movq sym+100(%rax), %rax</code> would not work correctly due to an <code>int32_t</code> overflow in the imm32 addressing mode field of the instruction, if <code>sym</code> were to have value <code>2^31 - 50</code>.</p>
<p>Therefore, for a provably correct solution, we must make sure that whenever we use an external symbol to represent a runtime constant, the runtime constant we want to express must fit in <code>[1, 2^31 - 2^24)</code>.</p>
<p>In Deegen, this is accomplished by a customized Constant Range Analysis pass to track the range of every constant expression based on the runtime constants. Of course, we also need to know the possible range for the “root” runtime constants – the bytecode operands, and the values captured by the IC state. Fortunately, for most of them, the range is implicit (for example, a bytecode slot is known to be a small non-negative integer, and an operand with type <code>uint16_t</code> obviously fits in <code>[0, 65535]</code>) and requires no user intervention. For the rest, a new Deegen API is added so the user can tell us the range assumption of the value.</p>
<p>Once we figured out the proven range of each runtime constant expression, we can retrofit it into our target range <code>[1, 2^31 - 2^24)</code> by simple transformation. To explain how it works, let’s revisit our <code>Add</code> example:</p>
<ul>
<li><code>lhs_slot</code> is a root runtime constant. Since it represents a bytecode slot ordinal, it is known to be a small non-negative integer, say <code>[0, 10000]</code>.</li>
<li>And we have a derived runtime constant <code>lhs_slot * 8</code>, which is known to fit in <code>[0, 80000]</code> by range analysis.</li>
<li>The range <code>[0, 80000]</code> does not fit in <code>[1, 2^31 - 2^24)</code>.</li>
<li>However, if we define a new expression <code>new_expr := lhs_slot * 8 + 1</code>, the new expression would have range <code>[1, 80001]</code> and fit the assumption.</li>
<li>Therefore, we use an external symbol <code>sym</code> to represent <code>lhs_slot * 8 + 1</code>, and rewrite the LLVM IR to substitute <code>lhs * 8</code> with <code>sym - 1</code>.</li>
</ul>
<p>Now, we are guaranteed correct code as the symbol range assumption is met.</p>
<p>Lastly, if the range of an expression is too large to fit in <code>[1, 2^31 - 2^24)</code>, we simply give up. This means the expression will be evaluated at runtime, but this is rare, and is only a minor performance issue, not a correctness issue.</p>
<p>After this transformation, the conceptual logic of the <code>Add</code> example would look like something below:</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br></pre></td><td><pre><span><span>size_t</span> tmp1 = __derived_runtime_constant_1() - <span>1</span>;</span><br><span><span>size_t</span> tmp2 = __derived_runtime_constant_2() - <span>1</span>;</span><br><span>TValue lhs = *(TValue*)((<span>uint64_t</span>)<span>stack</span> + tmp1);</span><br><span>TValue rhs = *(TValue*)((<span>uint64_t</span>)<span>stack</span> + tmp2);</span><br><span><span>if</span> (!lhs.Is&lt;tDouble&gt;() || !rhs.Is&lt;tDouble&gt;()) {</span><br><span>  ThrowError(<span>"Can't add!"</span>);</span><br><span>} <span>else</span> {</span><br><span>  <span>double</span> res = lhs.As&lt;tDouble&gt;() + rhs.As&lt;tDouble&gt;();</span><br><span>  </span><br><span>  <span>size_t</span> tmp3 = __derived_runtime_constant_3() - <span>1</span>;</span><br><span>  *(TValue*)((<span>uint64_t</span>)<span>stack</span>+tmp3) = TValue::Create&lt;tDouble&gt;(res);</span><br><span>  __dispatch_to_next_bytecode();</span><br><span>}</span><br></pre></td></tr></tbody></table></figure>
<p>where the derived runtime constants <code>__derived_runtime_constant_1/2/3</code> are defined as follow:</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br></pre></td><td><pre><span>__derived_runtime_constant_1 := lhs_slot * 8 + 1</span><br><span>__derived_runtime_constant_2 := rhs_slot * 8 + 1</span><br><span>__derived_runtime_constant_3 := output_slot * 8 + 1</span><br></pre></td></tr></tbody></table></figure>
<p>Note that in normal cases, those <code>+1 / -1</code> adjustments will not end up as machine instructions in the resulting JIT code, as normally all of those computation ends up being an imm32 field of an instruction, as we’ll see in the example below.</p>
<h4 id="Example-Generated-Code-for-the-AddVV-Bytecode">Example: Generated Code for the <code>AddVV</code> Bytecode</h4>
<p>For a concrete example, the figure below demonstrates the disassembly of the actual JIT code generated for the Lua <code>AddVV</code> bytecode, which performs a Lua <code>add</code> on the given two bytecode values. The C++ bytecode semantic that Deegen takes as input is <a href="https://github.com/luajit-remake/luajit-remake/blob/f8fb972ec91c28b849bd263f164832f0ff434d1f/annotated/bytecodes/arithmetic_bytecodes.cpp" target="_blank" rel="noopener">here</a>.</p>
<p><img src="https://sillycross.github.io/images/2023-05-12/add-generated-code.png" alt="Disassembly of the JIT'ed machine code for the Lua AddVV bytecode"></p>
<p>The blue boxes indicates the runtime constants that gets burnt into the instruction stream, with their value definitions shown on the right.</p>
<p>Note that the code contains two separated parts: <code>fast_path</code> and <code>slow_path</code>. We will explain this in detail in the next section: for now focus on <code>fast_path</code> only.</p>
<p>As one can see, the code quality has no problem rivalling a hand-written baseline JIT. It loads the two operands from the stack frame, and checks if any of them is <code>NaN</code>, which means either double <code>NaN</code> or a non-double value (which will exhibit as <code>NaN</code> in our NaN-boxing scheme). If so, it branches to <code>slow_path</code>. Otherwise, it performs a <code>double</code> addition and stores the result back to the <code>output_slot</code> in the stack frame. Finally, the control implicitly fallthroughs to the next bytecode.</p>
<p>The implementation of the JIT compiler logic that generates the above code at runtime will be showcased in the next section.</p>
<h3 id="Design-of-the-Baseline-JIT">Design of the Baseline JIT</h3>
<p>Having covered the core of the JIT code generation system, we are finally ready to explore the design of the JIT itself and the supporting components.</p>
<p>For a quick overview, the following figure illustrates the high-level architecture of the baseline JIT (except inline caching, which is complex enough that deserves its own section):</p>
<p><img src="https://sillycross.github.io/images/2023-05-12/baseline-jit-arch.png" alt="A summary of the high-level architecture of Deegen's baseline JIT (except IC)"></p>
<h4 id="The-AOT-Slow-Path">The AOT Slow Path</h4>
<p>A distinctive “feature” of dynamic languages is the pervasive existence of slow paths. For example, if you call a boolean value like <code>true</code> (why would anyone do that?), it could trigger some complicated metamethod lookup in Lua, ending up with a function to call or an error. In Deegen, a slow path can be created by both automatic type-based quickening and explicit user annotation (the <code>EnterSlowPath</code> API). But for the JIT, there are some extra complexity in implementing them.</p>
<p>Obviously, the slow path logic should be AOT-compiled, not JIT’ed. However, this introduces two problems:</p>
<ol>
<li>How the JIT’ed code could transfer control to the AOT slow path.</li>
<li>How the AOT slow path could transfer control back to the JIT’ed code.</li>
</ol>
<p>Let’s look at the second problem first. The bytecode stream does not contain any information about the JIT’ed code. Also, the slow path could make branches to other bytecodes and calls to other functions, so it’s not as easy as letting the JIT’ed code pass the JIT address of the next bytecode to the slow path.</p>
<p>Deegen’s solution is a dedicated <code>SlowPathData</code> stream. The <code>SlowPathData</code> stream is similar to the bytecode stream, except that it is intended to be used by the AOT slow path of the JIT tier, instead of the interpreter. A <code>SlowPathData</code> contains all the information needed by the slow path, such as bytecode operands, JIT address for this bytecode, JIT address of the conditional branch target of this bytecode, etc. When the JIT’ed code wants to transfer control to the slow path, it would pass the <code>SlowPathData</code> pointer corresponding to the current bytecode to the AOT slow path. The AOT slow path can then have access to all the data it needs to complete the execution and transfer control back to JIT’ed code.</p>
<p>Of course, the <code>SlowPathData</code> stream has to be generated. Fortunately, since Deegen understands the bytecode stream, it is not hard to generate logic that transcribes the bytecode stream to the <code>SlowPathData</code> stream. Specifically, the generated JIT compiler will generate the <code>SlowPathData</code> stream alongside the executable code.</p>
<p>Now let’s look at the first problem. Transferring control from JIT’ed code to the AOT slow path requires some set up logic, for example, to correctly set up the <code>SlowPathData</code> pointer. However, these logic are rarely executed, as slow paths are, of course, rarely used. If no special handling is taken, the resulted code would have cold logic and hot logic mixed together, resulting in unnecessary additional branches and worse code locality. Of course, this is not a correctness problem, but ideally we want to handle it without sacrificing compilation time.</p>
<p>Deegen employs the solution used in JavaScriptCore: <em>hot-cold code splitting</em>, except that Deegen must accomplish it automatically. Specifically, every stencil will be split into a hot part and a cold part. The JIT will generate two streams of executable code, one holding all the hot path logic, and one holding all the slow path logic. The hot-cold splitting is accomplished by an ASM transformation pass, which we will elaborate in the next section.</p>
<h4 id="The-Baseline-JIT-Algorithm">The Baseline JIT Algorithm</h4>
<p>We now have all the pretexts to understand how the baseline JIT itself works.</p>
<p>In addition to the logic that actually generates machine code, Deegen also generates a <em>bytecode trait table</em> that contains various info about the generated code for each bytecode, e.g., the length of the JIT’ed code’s hot part and cold part, the length and alignment of the data section accompanying the JIT’ed code, the length of the <code>SlowPathData</code> for this bytecode, etc. This allows the baseline JIT to precompute all the buffer sizes in advance.</p>
<p>The baseline JIT compiler works in two passes.</p>
<p>In the first pass, we iterates through the bytecode stream, and use the bytecode trait table to compute various buffer sizes of the generated code and data. All the buffers are then allocated in advance, knowing that a buffer overrun will never happen when we actually fill contents (code, data, etc.) into the buffers. This pass is very cheap because no indirect dispatch is needed.</p>
<p>In the second pass, we iterates through the bytecode stream again, and generate everything (executable code, the accompanying data, the <code>SlowPathData</code>, etc.) for each bytecode by populating the pre-allocated buffers. This step conceptually works similar to an interpreter. We have a pre-built dispatch table storing the codegen functions for each bytecode kind. Control is first transferred to the codegen function for the first bytecode. The function would generate everything needed for the first bytecode, advance buffer pointers accordingly, and then transfer control to the codegen function for the next bytecode. This process repeats until the end of the bytecode stream is reached.</p>
<p>Thanks to Copy-and-Patch, each codegen function is completely branchless, except the tail dispatch that transfers control to the next codegen function, as we shall see in the <code>Add</code> example below. This allows a modern CPU to utilize its Instruction-Level Paralleism (ILP) capabilities to the utmost, yielding an extremely fast compilation process.</p>
<p>Finally, due to the nature of one-pass code generation, bytecodes that can branch to other bytecodes would not know their branch destination address at the time their own code is being generated. To solve this issue, those bytecodes would push information about how the branch destination address shall be fixed up into a late-patch buffer. After all code generation is done, we need to iterate through the late-patch buffer and fix up all the branch targets.</p>
<h4 id="Example-Code-Generation-Function-for-the-AddVV-Bytecode">Example: Code Generation Function for the <code>AddVV</code> Bytecode</h4>
<p>Below is the actual code-generation logic generated by Deegen that generates code for the Lua <code>AddVV</code> bytecode. The machine code generated by the logic is demonstrated in the right half of the figure for cross-reference.</p>
<p><img src="https://sillycross.github.io/images/2023-05-12/add-code-gen.png" alt="Generated JIT logic that generates code for AddVV (left) and the generated code (right)"></p>
<p>As one can see, the code-generation logic is just what we have explained in the previous subsection. It first decodes the bytecode, then performs a copy-and-patch to generate the JIT fast path and the JIT slow path logic. The expression that defines each runtime constant is replayed to compute the patch value in the instruction stream. Besides the machine code, it also generates the <code>SlowPathData</code> stream and other minor support data. Finally, it advances pointers and dispatch to the next codegen function to codegen the next bytecode. The whole process is completely branchless (except the tail dispatch) by design.</p>
<h3 id="Supporting-Inline-Caching-the-Art-of-Repurposing-Existing-Tools-Evermore">Supporting Inline Caching: the Art of Repurposing Existing Tools, Evermore</h3>
<p>Due to inherent design constraints of a baseline JIT (e.g., compilation must be fast, no OSR-exit is allowed), inline caching (IC) is the only high-level optimization tool available to the baseline JIT.</p>
<p>And inline caching is powerful: on benchmarks that extensively work with Lua tables, a baseline JIT with IC can often be more than 100% faster than the same baseline JIT without IC<sup><a href="#fn15" id="fnref15">[15]</a></sup>.</p>
<p>In this section, we will elaborate how Deegen supports inline caching.</p>
<h4 id="How-IC-works-in-Deegen-a-Step-by-Step-Example-of-Call-IC">How IC works in Deegen: a Step-by-Step Example of Call IC</h4>
<p>For a beginner’s introduction to what IC is, please read the <a href="#deegen_baseline_jit_background_section">background section</a>. However, to understand how IC actually works in Deegen’s baseline JIT, the easiest way is to walk through an assembly example. Here, we will use a simplified <code>Call</code> bytecode, which performs a call with no arguments and discards all return values, to demonstrate how <em>call IC</em> works.</p>
<p>The C++ bytecode semantic description is very simple:</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br></pre></td><td><pre><span><span><span>void</span> <span>ReturnContinuation</span><span>(TValue* )</span> </span>{ Return(); }</span><br><span><span><span>void</span> <span>Call</span><span>(TValue* base)</span> </span>{</span><br><span>  TValue func = *base;</span><br><span>  <span>if</span> (likely(func.Is&lt;tFunction&gt;())) {</span><br><span>    TValue* newStackFrame = base + x_numSlotsForStackFrameHeader;</span><br><span>    MakeInPlaceCall(func.As&lt;tFunction&gt;(), </span><br><span>                    newStackFrame, </span><br><span>                    <span>0</span> , </span><br><span>                    ReturnContinuation);</span><br><span>  } <span>else</span> {</span><br><span>    EnterSlowPath&lt;CheckMetatableSlowPath&gt;();</span><br><span>  }</span><br><span>}</span><br></pre></td></tr></tbody></table></figure>
<p>It checks if the callee is a function object. If so, it uses Deegen’s <code>MakeInPlaceCall</code> API to make a call, and the return continuation simply discards all return values and transfer control to the next bytecode. Otherwise, it enters the outlined slow path function (omitted) that checks for a metatable call.</p>
<p>Deegen would generate the following JIT code for this bytecode:</p>
<p><img src="https://sillycross.github.io/images/2023-05-12/call-bytecode-main-logic.png" alt="JIT code generated for the example Call bytecode"></p>
<p>Note that runtime constants are marked in purple in the form of <code>${X}</code>.</p>
<p>Let’s pretend for now that the <code>codegen_call_ic</code> thing doesn’t exist, and look at the naive implementation. If you stare at the assembly code a little bit, you will notice that the logic involves:</p>
<ol>
<li>Two branches to check that <code>func</code> is a function object.</li>
<li>Two dependent memory loads: one loads the function prototype <code>proto</code> from <code>func</code>, and one loads the actual entry point address from <code>proto</code>.</li>
<li>One indirect branch to branch to the entry point.</li>
</ol>
<p>Unfortunately, dependent memory loads and unpredictable indirect branchs are <em>exactly</em> the two things modern CPUs hate the most. Even predictable branches can be slow, if there are too many of them so that the BTB is overwhelmed.</p>
<p>So how does IC speeds up this code?</p>
<p>As one might have expected, <code>codegen_call_ic</code> will be called on the first time this code is executed. What <code>codegen_call_ic</code> does is that it will emit a piece of IC code snippet, and chain it to the main logic by repatching the self-modifying-code (SMC) region, as shown below:</p>
<p><img src="https://sillycross.github.io/images/2023-05-12/call-bytecode-direct-call-ic-1.png" alt="The JIT code after one IC entry is created"></p>
<p>As one can see, the next time the same function is called, thanks to the SMC region, the IC will hit, and the optimized logic will be executed. The optimized logic does not check that <code>func</code> is a function object (because we already checked it last time), has no memory loads, and the branch is direct.</p>
<p>This process can be repeated to chain any number of IC entries into a chain:</p>
<ul>
<li>SMC region branches to IC <code>#N</code></li>
<li>IC <code>#N</code> branches to IC <code>#(N-1)</code> if the cached value does not hit</li>
<li>… etc …</li>
<li>IC <code>#1</code> branches to the IC miss slow path, which will create a new IC snippet <code>#(N+1)</code> and chain it at the head of the chain.</li>
</ul>
<p>Of course, at a certain point the overhead from the check chain would cancel out the benefit of the optimized code, and we will stop chaining more cases.</p>
<h4 id="Call-IC’s-Direct-Call-Mode-vs-Closure-Call-Mode">Call IC’s Direct Call Mode vs Closure Call Mode</h4>
<p>While the above approach works well if a Lua function is used like a C function (monomorphism) or a C++ virtual method (class-like polymorphism), it would work very poorly for the <em>function factory</em> design pattern. For example, consider the following Lua snippet:</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br></pre></td><td><pre><span>createCounter = <span><span>function</span><span>()</span></span> </span><br><span>  <span>local</span> value = <span>0</span></span><br><span>  <span>return</span> <span><span>function</span><span>()</span></span> </span><br><span>    value = value + <span>1</span></span><br><span>    <span>return</span> value</span><br><span>  <span>end</span></span><br><span><span>end</span></span><br><span>incrementCounter = <span><span>function</span><span>(counter)</span></span></span><br><span>  <span>return</span> counter()</span><br><span><span>end</span></span><br></pre></td></tr></tbody></table></figure>
<p>In this example, the call in line <code>9</code> is likely to see a lot of different function objects, even though all of them share the same prototype (the counter lambda in line <code>3</code>). Since our current call IC strategy caches on the function object, not the function prototype, it is completely ineffective for this use pattern.</p>
<p>Again, we employ the solution used in JavaScriptCore. Our call IC supports two modes: <em>direct call</em> mode and <em>closure call</em> mode. A call IC site always starts in direct call mode, in which it caches on function objects, as we have shown above.</p>
<p>But when a call IC site first sees a IC miss that has the same function prototype as one of the already-cached function objects, it will transition the IC to closure-call mode. To do this, it rewrites the self-modifying code region and invalidates all existing ICs at this site, and from now on, this IC site will instead cache on the function prototypes. This is demonstrated by the figure below:</p>
<p><img src="https://sillycross.github.io/images/2023-05-12/call-bytecode-closure-call-ic-1.png" alt="The JIT code after the Call IC transitions to Closure Call Mode"></p>
<p>As one can see, the SMC region is repatched to completely different logic: it checks if <code>func</code> is a heap object (which is required for us to load its hidden class), then load the hidden class of the heap object and branch to the first IC case.</p>
<p>Each closure call IC case caches on a function prototype. So it compares if the hidden class matches the cached prototype. If yes, it knows that <code>func</code> must be a function object with the cached prototype, so it can perform an optimized call similar to before.</p>
<p>Of course, one can also chain up as many IC cases in closure call as desired, until the chained check overhead overwhelms the perf gain from the optimized code.</p>
<p>As one can see, closure call mode is less efficient than direct call mode as it performs one extra check and one extra memory load, but it works effectively for the function factory design pattern. This is why a call IC site always starts in direct call mode, and only transitions to closure call mode when it actually observes a closure call pattern.</p>
<h4 id="So-How-to-Automatically-Generate-All-of-These">So, How to Automatically Generate All of These?</h4>
<p>Having understood how IC works in Deegen (we only demonstrated Call IC, but the case for Deegen’s Generic IC API is similar), the next question is: how could Deegen generate all of these automatically?</p>
<p>However, as you can already see, what we want to do is something totally outside the operating envelope of LLVM. LLVM is simply not designed to generate a function that can dynamically patch itself at runtime to append a dynamic chain of parametrizable code snippets.</p>
<p>As before, our solution is to repurpose existing tools to trick LLVM into helping us without its knowledge. And as it turns out, the core of the trick is to repurpose a completely-irrelevant little-known GCC feature in the dark corner.</p>
<h4 id="Thank-you-GCC-ASM-goto">Thank you, GCC ASM-goto!</h4>
<p>GCC supports a little-known extension feature called <a href="https://gcc.gnu.org/onlinedocs/gcc/Extended-Asm.html#:~:text=6.47.2.7%20Goto%20Labels" target="_blank" rel="noopener">ASM-goto</a>, which basically allows one to write inline assembly that branches from assembler code to C labels. And LLVM, aiming for compatibility with GCC, <a href="https://lists.llvm.org/pipermail/llvm-dev/2018-October/127239.html" target="_blank" rel="noopener">has also supported this feature a few years ago</a> by a special <code>CallBr</code> IR instruction.</p>
<p>I just want to say a big thank you to the GCC developers who designed this feature and the LLVM developers who added support for it! Without this feature, it’s very likely Deegen couldn’t support inline caching at all.</p>
<p>So how does ASM-goto have anything to do with inline caching?</p>
<p>As you might have seen from the assembly example above, the hard part of IC is that each IC case is a piece of machine code that directly “clings” to the main logic. It cannot be implemented by a separate function due to the call overhead and the requirements of Lua’s stackful coroutine. It must work directly using the context (e.g., which register holds which value) of the main logic, and could transfer control to different destinations in the main logic.</p>
<p>ASM-goto (and its underlying <code>CallBr</code> LLVM IR) provided <em>exactly</em> the semantics we want. Since it is an <code>InlineAsm</code>, LLVM is <em>required</em> to treat its contents as opaque. All LLVM knows is that after executing the <code>InlineAsm</code>, control will be transferred to one of the destinations specified in the <code>CallBr</code>.</p>
<p>In other words, we repurpose <code>CallBr</code> as a way to model “a control flow transfer in an unspecified manner”. At runtime, we are building up a dynamic chain of IC cases; but if one views the chain-check logic as a black box, then it can be characterized as: after the black box is executed, control is transferred to either an IC hit case specialized to the cached values, or the IC miss slowpath. This is exactly the semantics <code>CallBr</code> provided, so we can safely model it using <code>CallBr</code>.</p>
<p>But this is still far from enough. Now we have a way to model the control flow of the dynamic IC chain in LLVM IR, but it’s still unclear how we can extract the IC logic from the main function, implement the IC check chain, and do all the self-modifying code stuff.</p>
<p>This is where the last piece of the puzzle comes in: ASM transformation.</p>
<h4 id="ASM-Transformation-the-Last-Piece-of-the-Puzzle">ASM Transformation: the Last Piece of the Puzzle</h4>
<p>I know this might scare off people, as directly messing with assembly sounds like an extremely fragile approach.</p>
<p>But it really isn’t. Deegen treats most of the assembly instructions as opaque and will not modify any of them. The ASM transformation is limited to reordering and extracting ASM blocks.</p>
<p>As a result, Deegen’s assembly knowledge is extremely limited. All it knows is that:</p>
<ul>
<li>A <code>jmp</code> does a direct jump.</li>
<li>A <code>jmpq</code> does an indirect jump.</li>
<li>Any other instruction starting with <code>j</code> does a conditional jump.</li>
</ul>
<p>However, as it turns out, with some clever tricks and cooperation from LLVM IR, doing only ASM block rearrangements is already sufficient to achieve a lot: we can support all the inline caching stuffs, among other things.</p>
<p>The full trick is the following. Recall that we are only using <code>CallBr</code> as a device to express an opaque control flow, and the <code>InlineAsm</code> inside <code>CallBr</code> does not matter. So we will use this <code>InlineAsm</code> to carry down information to the textual assembly level, as shown below.</p>
<p><img src="https://sillycross.github.io/images/2023-05-12/callbr-example.png" alt="The CallBr trick at LLVM IR level and the resulting assembly"></p>
<p>As one can see, the previledged instruction <code>hlt</code> is used as a magic to allows us to identify the <code>CallBr</code> in the textual assembly. Then, the fake branches following the <code>hlt</code> allows us to know the assembly labels that implements each logic case.</p>
<p>Having parsed these information, we no longer need the <code>CallBr</code> payload, so we remove it from assembly, and make it branch to the slow path directly.</p>
<p>Next, we perform a CFG analysis of the assembly. The only hard part about the CFG analysis is to know the possible destinations of the indirect branches. This ideally should be implemented as a LLVM backend pass, but I haven’t figured out how to do it due to limited documentation about LLVM backend. So currently, the indirect branch target analysis is done via some hacks that map the indirect branch back to LLVM IR by debug info.</p>
<p>Now we have the CFG of the assembly, we can then figure out the ASM blocks only reachable from the function entry, and only reachable from each IC logic kind, as shown below.</p>
<p><img src="https://sillycross.github.io/images/2023-05-12/ic-extraction.png" alt="ASM CFG Analysis and IC Extraction"></p>
<p>Note that the logic entry of each IC kind must not be reachable from the function entry, because they are only reachable by the <code>CallBr</code>, but we have removed those control flow edges as the <code>CallBr</code> has been removed by us.</p>
<p>Finally, we can separate out the IC logic from the main function logic. For the main function, we only retain ASM blocks reachable from the function entry. And for each IC kind, we only retain ASM blocks reachable from its logic entry but not the main function entry. Each piece of extracted assembly is then compiled to object file and extracted to a Copy-and-Patch stencil, so we can JIT it at runtime.</p>
<p>There are still some minor issues that we haven’t covered, such as how we build up the dynamic IC check chain, and how exactly the self-modifying code region is constructed. But the idea is similar to how we supported inline caching: most of the heavy-lifting of actually building up the logic is done at LLVM, and <code>InlineAsm</code> is repurposed as a tool to pass down information to assembly. Then at assembly level, Deegen can piece everything together by very simple transformations that requires little to no assembly knowledge.</p>
<h4 id="The-Inline-Slab-Optimization">The Inline Slab Optimization</h4>
<p>Deegen employed one additional optimization for IC: the <em>Inline Slab</em> optimization (again, the terminology is a JavaScriptCore jargon).</p>
<p>Conceptually, the idea is very simple: currently, each IC case is an outlined piece of JIT code. As a result, control has to branch from main logic to the IC case, and then from the IC case back to the main logic in the end. So why not use the SMC region to hold one IC case? Now, the “blessed” IC case sitting directly inside the SMC region can be executed directly, saving one or two jumps<sup><a href="#fn16" id="fnref16">[16]</a></sup>.</p>
<p>Of course, it is harder to do than said. One has to decide a good size for the inline slab (i.e., the SMC region), as only IC whose size is less than the inline slab size can sit in the inline slab. And updating the patchable jump in the SMC region is more complicated, as the location of the jump is different depending on whether the inline slab exists. Finally, the inline slab version of the IC has slightly different logic from the normal version: the tail branch could potentially be eliminated, and one must pad NOPs to exactly the size of the inline slab.</p>
<p>As a result, even in JavaScriptCore, the inline slab optimization requires quite some engineering efforts, e.g., more hand-rolled assembly, <a href="https://sillycross.github.io/r/WebKit/Source/JavaScriptCore/bytecode/InlineAccess.h.html">manual book-keeping of the inline slab sizes</a> that has to be updated whenever the generated assembly changes, etc.</p>
<p>Fortunately, in Deegen, the inline slab optimization is employed fully automatically. So for a language implementer, the additional ~4% performance on average (and occasionally 10+% on IC intensive workloads) from inline slab comes for free!</p>
<h4 id="Runtime-Support-and-IC-Design-Summary">Runtime Support, and IC Design Summary</h4>
<p>Finally, the VM runtime needs to manage the IC. For example, it needs to reclaim the JIT code memory when the IC is invalidated, and upon tiering-up, it needs to update all the call IC cases to point to the new entry point.</p>
<p>Therefore, in additional to the actual JIT’ed code, we also need to allocate a piece of metadata to manage each IC. The metadata are chained into a linked list at the use site of the IC (the <code>ICSite</code>), which resides in the <code>SlowPathData</code> stream.</p>
<p>Putting everything about Deegen’s IC design together into one figure:</p>
<p><img src="https://sillycross.github.io/images/2023-05-12/deegen-ic-design-detail.png" alt="An overview of Deegen's design of Inline Caching in more detail"></p>
<h4 id="Full-Example-for-Deegen’s-Generic-IC-TableGetById-Bytecode">Full Example for Deegen’s Generic IC: <code>TableGetById</code> Bytecode</h4>
<p>To conclude our discussions on inline caching, we will present a full example for the <code>TableGetById</code> (aka., <code>TGETS</code> in LuaJIT) bytecode.</p>
<p>The bytecode takes two operands: <code>base</code> and <code>index</code>, where <code>index</code> is a constant string, and returns <code>base[index]</code>. Any Lua string property access, for example, <code>employee.name</code> or <code>animal.weight</code>, would generate this bytecode.</p>
<p>In <em>LuaJIT Remake</em>, a Lua table is not implemented by a plain hash table with an array part, but employs <a href="https://github.com/luajit-remake/luajit-remake/blob/b9b5274d15373a24f9297cf551621506f87b375f/runtime/structure.h" target="_blank" rel="noopener">hidden class</a> for better performance, using a design <a href="https://webkit.org/blog/10308/speculation-in-javascriptcore/" target="_blank" rel="noopener">mostly mirroring JavaScriptCore’s</a>. Deegen supports <a href="https://sillycross.github.io/2022/11/22/2022-11-22/#deegen_generic_inline_caching_api">generic IC API</a> to allow easy deployment of IC via clever use of C++ lambdas. The actual implementation of the C++ bytecode semantic for <code>TableGetById</code> can be <a href="https://github.com/luajit-remake/luajit-remake/blob/b9b5274d15373a24f9297cf551621506f87b375f/annotated/bytecodes/table_get_by_id.cpp#L91" target="_blank" rel="noopener">found here</a>.</p>
<p>The figure below is the disassembly of the actual machine code generated by the baseline JIT, alongside the JIT’ed code for all 6 kinds of IC stubs, as well as their inline-slab versions. As before, the runtime constants burnt into the instruction stream are shown in purple text.</p>
<p><img src="https://sillycross.github.io/images/2023-05-12/getbyid_ic.png" alt="Disassembly of the main logic and all IC logic generated for the TableGetById bytecode"></p>
<p>As you can see above, in the good case of an inline-slab IC hit for a table without metatable (which is very common), a <code>TableGetById</code> can be accomplished with no taken branches and in less than 10 instructions. This is why IC could drastically speed up table operations.</p>
<p>On the other hand, as you can also see above, implementing IC by hand requires a deep understanding of assembly and a significant amount of engineering. This is exactly where Deegen comes in. With Deegen’s generic IC API that makes all of these happen automatically, a language implementer can enjoy the benefits of IC without the high engineering cost.</p>
<h4 id="The-Hot-Cold-Splitting-Pass-and-Jump-to-Fallthrough-Pass">The Hot-Cold Splitting Pass and Jump-to-Fallthrough Pass</h4>
<p>Finally, since we already have an ASM transformation infrastructure, why not use it for more good?</p>
<p>The Hot-Cold Splitting Pass works by reordering ASM blocks and move cold blocks to a separated text section, which reduces some unnecessary branches and improves code locality. Of course, the stencil extraction logic that generates the copy-and-patch stencil from the object file needs to be made aware of this and extract both sections, but this is not hard to do. To figure out which blocks are cold, ideally, one should write a LLVM backend pass. However, as explained before, I still haven’t figured out how to write a LLVM backend pass, so currently this is accomplished by injecting debug info to map assembly blocks back to LLVM IR blocks, and use LLVM IR’s block frequency infrastructure to determine the cold blocks.</p>
<p>The Jump-to-Fallthrough transformation pass attempts to move the dispatch to the next bytecode to the last instruction, so that the jump could be eliminated to a fallthrough, reducing an unnecessary branch. This is needed because at LLVM IR level, a dispatch is a tail call, and LLVM is not aware of the fact that a dispatch to the next bytecode could potentially<sup><a href="#fn17" id="fnref17">[17]</a></sup> be implemented by a fallthrough if it were the last instruction. Deegen implemented a simple pass to address this issue, which attempts to make the fallthrough possible by reordering ASM blocks and doing very limited rewrites like flipping branch conditions.</p>
<h3 id="An-End-to-End-Example">An End-to-End Example</h3>
<p>To demonstrate how the actual end-to-end JIT’ed code generated by the baseline JIT looks like, we will use the following Lua example that computes a factorial:</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br></pre></td><td><pre><span></span><br><span></span><br><span></span><br><span>fact = <span><span>function</span><span>(n)</span></span></span><br><span>  <span>if</span> (n &lt; <span>1</span>) <span>then</span></span><br><span>    <span>return</span> <span>1</span></span><br><span>  <span>end</span> </span><br><span>  <span>return</span> fact(n<span>-1</span>) * n</span><br><span><span>end</span></span><br></pre></td></tr></tbody></table></figure>
<p>While it is a simple example, it demonstrates almost all the important things in a baseline JIT: basic operations such as arithmetic and comparison, control flow, function calls, call inline caching (automatically provided as part of Deegen) and table inline caching (implement using Deegen’s generic IC API).</p>
<p>The above Lua function results in 8 bytecodes:</p>
<ul>
<li>(Bytecode #0) BranchIfNotLessThan {<code>Slot(0)</code>, <code>Double(1)</code>} → #3</li>
<li>(Bytecode #1) ConstInt16  {<code>1</code>} → <code>Slot(1)</code></li>
<li>(Bytecode #2) Return {<code>SlotRange [1, 2)</code>}</li>
<li>(Bytecode #3) GlobalGet {<code>String("fact")</code>} → <code>Slot(1)</code></li>
<li>(Bytecode #4) ArithmeticSub {<code>Slot(0)</code>, <code>Double(1)</code>} → <code>Slot(5)</code></li>
<li>(Bytecode #5) Call { Frame: <code>Slot(1)</code>, #arg: <code>1</code>, #ret: <code>1</code> }</li>
<li>(Bytecode #6) ArithmeticMul {<code>Slot(1)</code>,<code>Slot(0)</code>} → <code>Slot(1)</code></li>
<li>(Bytecode #7) Return {<code>SlotRange [1, 2)</code>}</li>
</ul>
<p>For clarity, we demonstrate the code <em>after</em> the function has been executed, so all the self-modifying code regions (including inline slabs) have been repatched, and outlined IC stubs have been created.</p>
<p>I manually grabbed the JIT’ed code using GDB and hand-remapped all the labels, so please pardon me if I made an mistake. The disassembly is as follows:</p>
<p><img src="https://sillycross.github.io/images/2023-05-12/factorial-jit-code.png" alt="Disassembly of the JIT'ed code for the factorial function"></p>
<p>Note that under normal circumstances (i.e., a number is passed in as parameter to <code>fact</code>), the <code>GlobalGet</code> and <code>Call</code> slow path will be executed once to create the IC. After that, none of the slow path logic will ever be executed, and none of the self-modifying code region in the fast path will get repatched further.</p>
<h3 id="Tiering-up-Logic">Tiering-up Logic</h3>
<p>The last bit of complexity is the tiering-up logic. In a multi-tier VM, the user program starts execution in the interpreter tier, and hot functions are eventually tiered up to the baseline JIT tier (and potentially further to the optimizing JIT tier, but that’s future work).</p>
<p>To support tiering-up, we have two problems to solve. First, how hot functions could be identified. Second, how future calls to the tiered-up function could get redirected to the JIT’ed version.</p>
<p>Let’s look at the second problem first. While seemingly trivial (just change the entry point stored in the function prototype), it is actually not that trivial due to the existence of call IC. When a function gets tiered-up, every call IC that caches on the function must be updated and redirected to the new entry point. To achieve this, all the call IC are chained into a circular doubly-linked list on the function prototype that it caches on. In addition, Deegen generates information on how one can update the JIT’ed code of a call IC to change the function entry point it branches to. Then, whenever a function is tiered up, one can iterate through all the call ICs caching on the function using the doubly-linked list, and update each of them to point to the new entry.</p>
<p>For the first problem, the idea is to increment a per-function counter whenever the interpreter reaches a loop bytecode or a function return bytecode. When the counter reaches a threshold, we trigger JIT compilation and redirect control to the JIT’ed code. Unfortunately, the engineering of this part has not been finished. I have to publish this post now, because this post is used as the reading material for a talk a couple of days later :(</p>
<p>This also means that currently we cannot tier-up from interpreter to baseline JIT, so for the benchmarks, everything is directly compiled by the baseline JIT and executed in baseline JIT tier.</p>
<h3 id="Performance-Evaluation">Performance Evaluation</h3>
<p>In this section, we will analyze the performance of <a href="https://github.com/luajit-remake/luajit-remake" target="_blank" rel="noopener">LuaJIT Remake</a> (LJR)'s baseline JIT on 44 synthetic benchmarks from <a href="https://github.com/smarr/are-we-fast-yet" target="_blank" rel="noopener">Are-we-fast-yet</a>, <a href="https://benchmarksgame-team.pages.debian.net/benchmarksgame/" target="_blank" rel="noopener">CLBG</a>, <a href="https://github.com/LuaJIT/LuaJIT-test-cleanup/tree/master/bench" target="_blank" rel="noopener">LuaJIT Benchmarks</a> and <a href="https://github.com/gligneul/Lua-Benchmarks" target="_blank" rel="noopener">Lua Benchmarks</a>.</p>
<p>Disclaimer: synthetic benchmarks are well-known to be misleading and unable to reflect real workloads (see [<a href="https://blog.mozilla.org/nnethercote/2014/06/16/a-browser-benchmarking-manifesto/" target="_blank" rel="noopener">1</a>,<a href="https://v8.dev/blog/retiring-octane" target="_blank" rel="noopener">2</a>,<a href="https://www.microsoft.com/en-us/research/publication/jsmeter-characterizing-real-world-behavior-of-javascript-programs/" target="_blank" rel="noopener">3</a>,<a href="https://benchmarksgame-team.pages.debian.net/benchmarksgame/why-measure-toy-benchmark-programs.html" target="_blank" rel="noopener">4</a>]). The sole purpose of this section is to put our results within the context of the existing works, to give a <em>rough sense</em> on the performance of our baseline JIT.</p>
<h4 id="Compilation-Throughput">Compilation Throughput</h4>
<p>The top priority of a baseline JIT is to generate machine code as fast as possible. Therefore, our first evaluation is the compilation throughput.</p>
<p>We measured the compilation throughput of our baseline JIT by timing the main compilation function, which performs the end-to-end work of compiling a input Lua bytecode stream to machine code. We also recorded the total number of Lua bytecodes processed by the baseline JIT, and the total size of the generated machine code.</p>
<p>The average result over all 44 benchmarks is as follows:</p>
<ul>
<li>In terms of Lua bytecodes processed per second, LJR’s baseline JIT can compile 19.1 million Lua bytecodes per second.</li>
<li>In terms of machine code generated per second, LJR’s baseline JIT can generate 1.62GB of machine code per second.</li>
</ul>
<p>To demonstrate what 19.1 million Lua bytecodes means, the 44 Lua benchmark programs (254KB total) contains 17197 Lua bytecodes in total. So our baseline JIT generated machine code for all 44 benchmarks in less than one millisecond total.</p>
<p>As such, we claim that the compilation throughput of our baseline JIT is extremely fast, to the point that the start delay can be considered negligible<sup><a href="#fn18" id="fnref18">[18]</a></sup>.</p>
<h4 id="Generated-Code-Performance">Generated Code Performance</h4>
<p>While the baseline JIT is designed to generate code fast, generating fast code is still a second priority.</p>
<p>In this section, we will evaluate the execution performance of the machine code generated by LJR’s baseline JIT by comparing with LuaJIT and PUC Lua.</p>
<p>LJR and LuaJIT have drastically different high-level architecture, mid-level design and low-level implementation choices. For the most obvious part, a baseline JIT performs few optimizations <em>by design</em>, while the tracing JIT in LuaJIT does a lot of optimizations. Therefore, the sole purpose of the benchmark is to put the end performance of LJR’s baseline JIT within the context of the existing works, as shown in the figure below:</p>
<p><img src="https://sillycross.github.io/images/2023-05-12/perf-numbers.png" alt="Performance comparison of LJR's baseline JIT, LuaJIT and PUC Lua, higher is better"></p>
<p>As one can see, LuaJIT’s optimizing tracing JIT generally works better than our baseline JIT, which is no surprise.</p>
<p>However, it’s worth noting that with IC as the only high-level optimization, we are already outperforming LuaJIT on 13 of the 44 benchmarks. On geometric average, we are about 34% slower than LuaJIT, and 4.6x faster than PUC Lua.</p>
<p>In my opinion, it is fair to say that Deegen is now on a very stable ground. With its excellent interpreter and baseline JIT that can already achieve pretty good execution performance at a negligble startup delay, the optimizing JIT (to come in the future) would have much less pressure in doing expensive optimizations.</p>
<h3 id="Conclusion-Thoughts-and-Future-Works">Conclusion Thoughts and Future Works</h3>
<p>This post demonstrated the second phase of the Deegen project – to build a high-performance baseline JIT compiler automatically from the bytecode semantic.</p>
<p>Of course, this is far from the end. Our next step is to automatically generate an optimizing compiler, which will likely follow the design of JavaScriptCore’s DFG lightweight JIT compiler. If you have any comments, suggestions or thoughts, please do not hesitate to <a href="https://sillycross.github.io/about/">shoot me an email</a> :)</p>
<h4 id="Acknowledgements">Acknowledgements</h4>
<p>I thank <a href="https://fredrikbk.com/" target="_blank" rel="noopener">Fredrik Kjolstad</a> and <a href="https://saambarati.org/" target="_blank" rel="noopener">Saam Barati</a> for their comments and discussions on the draft version of this post. Fredrik also did the image editing work for the <code>Add</code> bytecode figure.</p>
<hr>
<h4 id="Footnotes">Footnotes</h4>
<hr>
<section>
<ol>
<li id="fn1"><p>Since a JIT compiler works at runtime, the overall latency experienced by the user is the sum of the compilation time to generate the code (startup delay) and the execution time of the generated code. So for maximum throughput, one wants a multi-tier architecture. The <em>baseline JIT</em> is a JIT compiler that specializes at fast compilation, and is used to compile not-so-hot code. For a small set of hot functions identified by the baseline JIT, the <em>optimizing JIT</em> kicks in to generate better code at a much higher compilation cost. <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>And even worse, due to the nature of assembly, there is little code sharing across the VM tiers or across hardware architectures, despite that all the VM tiers must stay in sync and exhibit identical behavior, or you get a VM bug. <!--This is another reason one might want to use `Deegen`: by automatically generating all the VM tiers from a single source of truth, `Deegen` removed all the code duplications, and all the generated VM tiers are automatically in sync.--> <a href="#fnref2">↩︎</a></p>
</li>
<li id="fn3"><p>There are other reasons from the usability and engineering side as well. For example, by removing the “compile” step in the edit-compile-run cycle, dynamic languages have faster iterative development cycle. And libraries written in dynamic languages can be distributed as source code. <a href="#fnref3">↩︎</a></p>
</li>
<li id="fn4"><p>Directly translating a dynamic language program to native code will result in a huge amount of native code. This is mainly due to the dynamic typed nature: every innocent operation in dynamic language is actually a huge switch depending on the input types, and can have drastically complex slow paths. The statically-generated logic must deal with all such cases for correctness, even though most of the slow paths are never hit at runtime. <a href="#fnref4">↩︎</a></p>
</li>
<li id="fn5"><p>Recall that in most dynamic languages, functions are first-class value. So the function held by <code>f</code> can always be changed (and in fact, <code>f</code> is not even necessarily a function object), even though in the majority of use cases, the function is used like a C function so <code>f</code> always just hold the same value. <a href="#fnref5">↩︎</a></p>
</li>
<li id="fn6"><p>This is the simplest strategy, but one can clearly do some fancier stuffs here. For example, JavaScriptCore will generate a binary search tree to reduce the number of jump instructions executed, so that they can support a higher number of IC entries. However, based on words from JSC developer, this optimization has very limited effect in practice. <a href="#fnref6">↩︎</a></p>
</li>
<li id="fn7"><p>OSR stands for On-Stack Replacement. OSR-Exit is also more widely known as <a href="https://dl.acm.org/doi/10.1145/143103.143114" target="_blank" rel="noopener"><em>deoptimization</em></a>, a technique originally used to enhance the debuggability of optimized code. However, we will stick to the term “OSR-Exit” as it better reflects the nature of this technique in the VM use case: exiting to a lower tier using an exotic method (on-stack replacement). <a href="#fnref7">↩︎</a></p>
</li>
<li id="fn8"><p>Being an acute reader like you, I’m sure you can already imagine a primitive JIT that works by wiring up different functions that implement basic functionalities like constant, addition, pointer dereference, etc. <a href="#fnref8">↩︎</a></p>
</li>
<li id="fn9"><p>While seemingly scary to mess up with assembly code directly, one can easily prove the correctness from the semantics of <code>jmp</code>. <a href="#fnref9">↩︎</a></p>
</li>
<li id="fn10"><p>Acute readers might have noticed that this statement is not 100% true. We will revisit it later. <a href="#fnref10">↩︎</a></p>
</li>
<li id="fn11"><p>Note that here, by saying register allocation and instruction selection, we meant those that work <em>across stencils</em>. Inside a stencil, since the code is compiled by LLVM, we already have good RA and ISel. <a href="#fnref11">↩︎</a></p>
</li>
<li id="fn12"><p>The decision to give up non-local RA/ISel is justified by designs of existing state-of-the-art VMs. For example, in JavaScriptCore, only the fourth-tier heavyweight optimizing JIT (FTL) employs codegen-level optimizations (RA, ISel, etc.) that works across bytecode or DFG IR nodes. <a href="#fnref12">↩︎</a></p>
</li>
<li id="fn13"><p>Unfortunately, I never realized the larger picture until after the Copy-and-Patch paper was already published, so this wrong belief is also published with the paper… <a href="#fnref13">↩︎</a></p>
</li>
<li id="fn14"><p>Under small code model, and not considering complications like position-independent code (PIC) and position-independent executable (PIE). <a href="#fnref14">↩︎</a></p>
</li>
<li id="fn15"><p>This is in fact an exaggregated statement. Hidden class <em>without</em> inline caching is much slower than a naive implementation (i.e., no hidden class at all). Therefore, if one were to actually seriously implement a baseline JIT without inline caching, he/she wouldn’t employ hidden class either. So IC is only responsible for a portion of the “&gt;100% speedup on IC-intensive benchmarks”: the rest are the slowdown from the overhead of hidden class that could be avoided. <a href="#fnref15">↩︎</a></p>
</li>
<li id="fn16"><p>One jump is clearly saved by not branching to the outlined JIT stub. If the IC happens to fallthrough to the logic directly after the SMC region (which is quite common), an additional jump can be saved. <a href="#fnref16">↩︎</a></p>
</li>
<li id="fn17"><p>Note that the JIT code for a bytecode may consist of multiple stencils, for example, a <code>Call</code> needs both the main logic and the return continuation. Clearly, the jump-to-fallthrough transform is only valid for the last instruction of the last stencil for the bytecode. <a href="#fnref17">↩︎</a></p>
</li>
<li id="fn18"><p>However, note that the memory overhead is not negligble at all: each Lua bytecode (which is usually a few bytes) expands to an average of 91 bytes of machine code in baseline JIT. This is why even with a baseline JIT, we still want programs to start executing in the interpreter. <a href="#fnref18">↩︎</a></p>
</li>
</ol>
</section>

      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HTML Spirograph (2021) (101 pts)]]></title>
            <link>http://htmlspirograph.com/</link>
            <guid>38948642</guid>
            <pubDate>Thu, 11 Jan 2024 06:43:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://htmlspirograph.com/">http://htmlspirograph.com/</a>, See on <a href="https://news.ycombinator.com/item?id=38948642">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
				<label for="rrpm">Arm 1 speed (RPM):</label>
				 <br>
				<label for="rarm1">Arm 1 length:</label>
				 <br>
	
				<label for="rarm2">Arm 2 length:</label>
				 <br>
				<label for="rarmext">Arm 2 extension:</label>
				 <br>
				<span>ALT + Click/Enter force Redraw immediatly<br>SHIFT + Click for fine tuning</span>
			</p><p>
				<label>Spirograph image:</label> <br>
				<label>Submit your drawing:</label> <br>
				<span>Only clean drawings can be uploaded: Redraw it first.<br>If needed use 500x acceleration.</span>
				<!--<textarea id="out"></textarea>-->
			</p><div id="about">
				<br>
				<h2>About</h2>
				<p>First i wanted to make a Lego Spirograph, but i couldn't do it with my childs around me, so i made it in HTML with JS using Canvas element to draw on it. Just like with the <a href="http://geargenerator.com/" target="_blank"><strong>Gear Generator</strong></a>, i wasn't able to stop working on it, and finally here is a not so user friendly, but a quiet good little tool, which generates beautiful drawings.</p>
				<h2>What is what?</h2>
				<p><strong>Start/Stop</strong>: start/continue drawing without ereasing the canvas, and stop it (pause). <strong>Redraw</strong>: Draw on a new canvas. <strong>Slowdown/Speedup</strong>: recalculates arm rotation speeds to change the drawing speed, it usually results in brighter lines. <strong>Zoom In/Out</strong>: recalculates arm size, and position parameters to change the drawing size. <strong>Randomizer</strong>: generates random parameters and start drawing on a new canvas. A is the simple one, B is the most complex. Imagine what C could be... <strong>Pen style</strong>: there are a few different pen to draw with: C as Colored line, R as Rainbow, A as Analogue, Z as Zebra (sorry!) and Y as yellow. <strong>Brightness</strong>: you can add extra brightness. <strong>Dim</strong>: makes the canvas 50% transparent. <strong>Clear</strong>: ereases the canvas. <strong>Live drawing</strong>: Slow (or realtime) motion of the drawing arms, as it draws on the rotating canvas. You can change parameters and see its results live. <strong>Acceleration</strong>: specifies how many segment are calculated per frame, it doesn't change the resulting image, just make it fast. Different acceleration value is used for the live and non live drawing, so if you switch off live drawing, it will be accelerated 50x by default, which is cool seeing how the drawing evolves. It can be set up to 500x.<br>In live drawing mode you can easy figure out which value is handling which part of the mechanism...</p>
				<h2>How to use it?</h2>
				<p>You can start with the Randomizer, and set the Live drawing off, so you can quickly see the whole drawing. If the drawing is too big or small use the zoom. If the lines are too thins or dark, or too shiny, use speedup/slowdown. Test the drawing with different pen styles, and Redraw it. The drawing stops automatically if the starting point is reached. You can then press Start again to continue if you will. But it is better to change some parameters, and Start then, so the new drawing will be made on top of the previous one. You can Dim it before starting over. This way you can combine different shapes with different pen styles on top of each other. Isn't it cool?</p>
				<h2>Saving, sharing</h2>
				<p>The current set of parameters, and settings are reflected in the URL hash in the addressbar. If you created a great drawing, just bookmark or save the URL from the address bar. Copy-Pasting the URL is a great choice for sharing. Using this URL will bring the visitor to your drawing's beginning, and will start drawing the way you did (live/non-live, acceleration, pen style, brightness) so the resulting image will be the same.</p>
				<p>
					<strong>Notes about browser compatibility</strong>: nothing to say yet... just: Safari wins Canvas render quality, Chrome is a bit dirty. One more thing: yes, it is Retina compatible on retina displays, which looks awesome:)
				</p>
				<p>
					And finally a big thanks to <a href="http://pageflip-books.com/" target="_blank"><strong>Pageflip-books.com</strong></a>, without it, i could never done this tool... <a href="mailto:abel@iparigrafika.hu">Feedbacks are welcome!</a>
				</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Cuts Jobs in Engineering and Other Divisions (587 pts)]]></title>
            <link>https://www.nytimes.com/2024/01/11/technology/google-layoffs.html</link>
            <guid>38948444</guid>
            <pubDate>Thu, 11 Jan 2024 06:14:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/01/11/technology/google-layoffs.html">https://www.nytimes.com/2024/01/11/technology/google-layoffs.html</a>, See on <a href="https://news.ycombinator.com/item?id=38948444">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/01/11/technology/google-layoffs.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[epoll: The API that powers the modern internet (2022) (177 pts)]]></title>
            <link>https://darkcoding.net/software/epoll-the-api-that-powers-the-modern-internet/</link>
            <guid>38948091</guid>
            <pubDate>Thu, 11 Jan 2024 05:25:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://darkcoding.net/software/epoll-the-api-that-powers-the-modern-internet/">https://darkcoding.net/software/epoll-the-api-that-powers-the-modern-internet/</a>, See on <a href="https://news.ycombinator.com/item?id=38948091">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
      <p>You used <a href="https://man7.org/linux/man-pages/man7/epoll.7.html">epoll</a> to fetch this blog post. For almost anything you do on the Internet the server will be running Linux and it will use <code>epoll</code> to receive and answer your request in a timely and affordable manner.</p>
<ul>
<li>epoll is what makes Go such a great language for writing server software. Here is epoll in <a href="https://github.com/golang/go/blob/f229e7031a6efb2f23241b5da000c3b3203081d6/src/runtime/netpoll_epoll.go#L101-L126">Go’s netpoll</a>.</li>
<li>epoll is what makes nginx the <a href="https://news.netcraft.com/archives/2021/12/22/december-2021-web-server-survey.html">most popular web server</a> in the world (this blog runs nginx). Here is <a href="https://github.com/nginx/nginx/blob/a64190933e06758d50eea926e6a55974645096fd/src/event/modules/ngx_epoll_module.c#L784-L800">nginx’s use of epoll</a>.</li>
<li>and it is often what we mean when we say ‘async’ in most programming languages. For example, of Rust’s two main async frameworks, async-std uses <a href="https://github.com/smol-rs/polling/blob/master/src/epoll.rs#L156-L157">polling</a> and tokio uses <a href="https://github.com/tokio-rs/mio/blob/dca2134ef355b3c0d00e8e338e44e7d9ed63edac/src/sys/unix/selector/epoll.rs#L68">mio</a>.</li>
</ul>
<p>Aside: All of the above work on many operating systems and support API’s other than <code>epoll</code>, which is Linux specific. The Internet is <a href="https://en.wikipedia.org/wiki/Usage_share_of_operating_systems#Public_servers_on_the_Internet">mostly made of</a> <a href="https://w3techs.com/technologies/details/os-unix">Linux</a>, so epoll is the API that matters.</p>
<h4 id="the-problem">The problem </h4>
<p>The core problem of running a network service, the problem epoll fixes, is that your network is very fast and your clients network is very slow. A server handling a request typically looks like this:</p>
<pre tabindex="0"><code>read the user's request (e.g. a browser HTTP GET)
do what they asked (e.g. load some information from the database)
write a response (e.g. HTML that the browser will display)
</code></pre><p>During the “read” and “write” parts above the server is idle, waiting for data or acknowledgments of that data to move across the network.</p>
<h4 id="before-epoll">Before epoll </h4>
<p>Before epoll the standard way to overcome this was to run a pool of processes each handling a different user request, typically with Apache <a href="https://httpd.apache.org/docs/2.4/mod/prefork.html">mod_prefork</a>. While one process waits on the user to acknowledge a packet of data, a different process can use the CPU. An emerging alternative was to use a thread pool which is lighter than a process pool and could handle low-hundreds of concurrent users. Multi-threading was risky as many libraries were not thread safe. Steven’s 2004 reference <a href="https://amzn.to/3FUlV4n">UNIX Network Programming</a> has a chapter discussing preforked vs prethreaded designs, because those were your options back then.</p>
<p>Then along came everybody, and even hundreds of concurrent users turned out not to be enough. An influential article, <a href="http://www.kegel.com/c10k.html">The C10K problem</a> started this discussion in 1999. It was not uncommon for web requests to timeout. People would <a href="https://en.wikipedia.org/wiki/Mirror_site">mirror</a> popular sites in an effort to spread the traffic.</p>
<h4 id="the-solution">The solution </h4>
<p>In 2000 Jonathan Lemon solved this problem for FreeBSD 4.3 by designing and building <a href="https://people.freebsd.org/~jlemon/papers/kqueue.pdf">kqueue/kevent</a>, making BSD the early choice for high performance networking.</p>
<p>Independently, in July 2001 Davide Libenzi solved the problem for Linux, with the <a href="http://www.xmailserver.org/linux-patches/nio-improve.html">first draft of epoll</a>, which <a href="https://lwn.net/Articles/16026/">evolved</a>, was <a href="https://lwn.net/Articles/13481/">merged</a> into Linux kernel 2.5.44 (a development release) in October 2002 and became widely available in December 2003 with the release of stable kernel 2.6.</p>
<p>Jim Blandy has a fantastic <a href="https://github.com/jimblandy/context-switch">comparison of threads vs epoll-based async here</a>.</p>
<h4 id="how-it-works">How it works </h4>
<p>epoll allows a single thread or process to register interest in a long list of network sockets (it supports things other than network sockets such as pipes and terminals, but you rarely have thousands of those). An <code>epoll_wait</code> call will then block until one of those is ready for reading or writing. A single thread using epoll can handle tens of thousands of concurrent (and mostly idle) requests.</p>
<p>The downside of epoll is that it changes the architecture of your application. Instead of a handling each connection with a straightforward <code>{read request, handle, write response}</code>, you now have a main loop more akin to a game engine. The code becomes:</p>
<pre tabindex="0"><code>loop
 epoll_wait on all the connections
 for each of the ready connections:
   continue from where you left off
</code></pre><p>You might be part way through reading a request on one of the ready sockets, and part way through writing a response on another socket. You have to remember your state, do only as much I/O as the socket can take without blocking, and then epoll_wait again. A large part of the popularity of Go, and of the ‘async/await’ model in languages like C#, Javascript and Rust, is that they hide that event loop, allowing you to write straight-line code as if you were still doing thread-per-connection.</p>
<h4 id="conclusion">Conclusion </h4>
<p>Without epoll either the economics of today’s Internet would look quite different (fewer requests per machine, so more machines, costing more money), or we’d be running our servers on a BSD. And without BSD’s kqueue (which preceded epoll by two years), we’d really be in trouble because the only alternatives were proprietary (/dev/poll in Solaris 8 and I/O Completion Ports in Windows NT 3.5).</p>
<p>epoll has been improved since it’s initial release, particularly with EPOLLONESHOT and EPOLLEXCLUSIVE flags, but the core API has stayed the same. epoll solved the C10K problem on Linux, which powers the Internet, allowing us to build fast and cheap Internet services.</p>
<p>Thanks Davide!</p>
<hr>
<h3 id="addendum-predecessors">Addendum: predecessors </h3>
<ul>
<li>
<p>Linux had <code>poll</code> and <code>select</code> before <code>epoll</code>. They were designed to handle a handful of file descriptors and they scale O(n) on that count. epoll scales O(1). <a href="https://amzn.to/3r3IVIf">Kerrisk</a> has performance numbers showing poll and select becoming unusable beyond the hundreds of file descriptors, while epoll remains fast into the tens of thousands.</p>
</li>
<li>
<p>Linux also had signal-driven I/O before <code>epoll</code>. To quote <a href="https://amzn.to/3FUlV4n">UNIX Network Programming</a>:</p>
</li>
</ul>
<blockquote>
<p>Unfortunately, signal-driven I/O is next to useless with a TCP socket</p>
</blockquote>
<p>and</p>
<blockquote>
<p>The only real-world use of signal-driven I/O with sockets that the authors were able to find is the NTP server, which uses UDP.</p>
</blockquote>
<p><small>Davide Libenzi kindly read this post before publication</small></p>

    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google lays off hundreds in hardware, voice assistant teams (179 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-01-11/google-lays-off-hundreds-in-hardware-voice-assistant-teams</link>
            <guid>38947654</guid>
            <pubDate>Thu, 11 Jan 2024 04:36:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-01-11/google-lays-off-hundreds-in-hardware-voice-assistant-teams">https://www.bloomberg.com/news/articles/2024-01-11/google-lays-off-hundreds-in-hardware-voice-assistant-teams</a>, See on <a href="https://news.ycombinator.com/item?id=38947654">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Tell HN: Google-Wide Layoffs (235 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=38947580</link>
            <guid>38947580</guid>
            <pubDate>Thu, 11 Jan 2024 04:25:59 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=38947580">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Leads across Google are sending coordinated emails within their orgs announcing layoffs. They've all gone out in the last 2 hours or so.</p><p>They've fragmented the layoffs on purpose to make them seem more localized, but it's affecting the whole company.</p><p>Orgs that I've heard/I'm part of that have been impacted so far include Ads, Search, Assistant, Maps Android &amp; Core.</p><p>I'm sure there are more, but I can't confirm.</p><p>I can't find WARN notices yet, so I assume they haven't been filed yet in any state.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google lays off hundreds working on its voice-activated assistant – Semafor (228 pts)]]></title>
            <link>https://www.semafor.com/article/01/10/2024/google-lays-off-hundreds-working-on-its-voice-activated-assistant</link>
            <guid>38947224</guid>
            <pubDate>Thu, 11 Jan 2024 03:37:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.semafor.com/article/01/10/2024/google-lays-off-hundreds-working-on-its-voice-activated-assistant">https://www.semafor.com/article/01/10/2024/google-lays-off-hundreds-working-on-its-voice-activated-assistant</a>, See on <a href="https://news.ycombinator.com/item?id=38947224">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Google is laying off hundreds of people working on its voice-activated Google Assistant software and eliminating a similar number of roles on its  knowledge and information product teams, a Google spokesperson confirmed to Semafor on Wednesday.</p><p>Google told Semafor the restructuring would help improve Google Assistant as it explores integrating newer artificial intelligence technology into its products. The company announced in October that it was using its generative AI chatbot Bard to <a href="https://blog.google/products/assistant/google-assistant-bard-generative-ai/" rel="no-referrer">build a new version</a> of Google Assistant that “extends beyond voice, understands and adapts to you and handles personal tasks in new ways.”</p><p>Google has been making periodic job cuts over the past year, including in its <a href="https://www.semafor.com/article/09/13/2023/google-lays-off-hundreds-on-recruiting-team" rel="no-referrer">recruiting</a> and <a href="https://www.cnbc.com/2023/10/18/google-cuts-dozens-of-jobs-in-news-division-.html" rel="no-referrer">news divisions</a>, but it has not conducted company-wide layoffs since last January, when it eliminated approximately 12,000 roles. Its parent Alphabet reported having <a href="https://abc.xyz/assets/c2/3e/0d6d568e4f56a1d14ca6b70c3443/goog-10-q-q3-2023.pdf" rel="no-referrer">over 180,000 employees</a> as of September 2023.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to get coworkers to stop giving me ChatGPT-generated suggestions? (130 pts)]]></title>
            <link>https://workplace.stackexchange.com/questions/194969/how-to-get-coworkers-to-stop-giving-me-chatgpt-generated-suggestions</link>
            <guid>38946188</guid>
            <pubDate>Thu, 11 Jan 2024 01:26:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://workplace.stackexchange.com/questions/194969/how-to-get-coworkers-to-stop-giving-me-chatgpt-generated-suggestions">https://workplace.stackexchange.com/questions/194969/how-to-get-coworkers-to-stop-giving-me-chatgpt-generated-suggestions</a>, See on <a href="https://news.ycombinator.com/item?id=38946188">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mainbar" role="main" aria-label="question and answers">
                
<div data-questionid="194969" data-position-on-page="0" data-score="61" id="question">
        

        

<div>
    
    <div itemprop="text">
                
<p>I work in computer graphics for a small company on our own game engine. We also have an in house team of artists creating content for this engine.</p>
<p>I am often tasked with taking suggestions from the artists and implementing them in the engine. The artists have no technical expertise, so I meet with them to understand their ideas and needs, and do so again to explain the functionality, limitations, and so on of my solution.</p>
<p>Recently it happened quite a few times that after such a meeting (where I explained that their requirements aren't 100% achievable and provided an alternative working solution) I get a message from the artists, along with a screenshot of an ostensible but frankly ludicrous solution proposed by ChatGPT. They then ask why I could not do what ChatGPT suggests.</p>
<p>I then have to take the time to explain why ChatGPT's proposed solution wouldn't work, which  is tedious and difficult when the other persons do not understand many of the basic ideas involved. They also seem skeptical, and I get the idea they feel I'm incompetent because as I understand it ChatGPT is very useful in their setting, and they have come to believe it to be the ultimate source of knowledge.</p>
<p>How can I, without being condescending to either my coworkers or their use of ChatGPT, ask them to not make suggestions to me that they personally don't understand and are based solely on ChatGPT?</p>
    </div>

        

    <div>
                <div>
    <p>
        asked <span title="2024-01-09 12:27:07Z">yesterday</span>
    </p>
    <div>
        <a href="https://workplace.stackexchange.com/users/143201/user140244"><p><img src="https://www.gravatar.com/avatar/30b6a9dcba19b607640e838d5813ceff?s=64&amp;d=identicon&amp;r=PG&amp;f=y&amp;so-version=2" alt="user140244's user avatar" width="32" height="32"></p></a>
    </div>
    
</div>

    <div>
        <p><span> New contributor</span>
        </p>
        <div>
            
            <p><a href="https://workplace.stackexchange.com/users/143201/user140244">user140244</a> is a new contributor to this site. Take care in asking for clarification, commenting, and answering.
Check out our <a href="https://workplace.stackexchange.com/conduct">Code of Conduct</a>.        </p></div>
    </div>

            </div>
    
</div>




            <p><span itemprop="commentCount">5</span></p>
    </div>



                
                
                <div id="answers">
                    


                                    
<div id="answer-194972" data-answerid="194972" data-parentid="194969" data-score="43" data-position-on-page="1" data-highest-scored="1" data-question-has-accepted-highest-score="1" itemprop="acceptedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>Thank for your clarifying in the comments, and to bring them up here:</p>
<blockquote>
<p>I take the time to understand and consider each suggestion, not rejecting anything out of hand, and share them with my team members (of which there are 17). Their advice has been, thus far, to "just ignore". However, I would not like to treat my co-workers that way.</p>
</blockquote>
<p>Sounds like you need to break that pattern, but also you don't necessarily want to be the hammer that drops in order to break it - as that rarely gets received in a friendly fashion, especially when they are used to just getting you to answer and consider the ideas.</p>
<p>The solution in that case is quite simple - reach out to your boss/manager, explain that while you appreciate the suggestions, they are not helpful (have example or two ready here), they drain a lot of your time and focus (example or two) but also that you do not wish to be rude to your well-meaning co-workers (keep your face straight saying that) and ask them to step in and be the hammer to curtail the flow of unhelpful and uneducated advice. Focus on the points you can easily prove - that the advice is wrong and it's a time drain - don't go into motives or anything else.</p>
<p>Ultimately that's the best way, as you are not in the place to tell them what to do, just as they cannot really tell you how to resolve an issue, and trying to pull some imaginary rank would likely alienate relationships further, and speaking in friendly way for them to see error of their way was dismissed. So time to call the actual powers to be, and let them manage as they are hired to do.</p>
<p>If the drain then continues, raise it with your boss/manager again.</p>
    </div>
    <div>
            
            <div>
        <a href="https://workplace.stackexchange.com/users/69035/alex-robinson"><p><img src="https://i.stack.imgur.com/PZa2B.jpg?s=64&amp;g=1" alt="Alex Robinson's user avatar" width="32" height="32"></p></a>
    </div>


            <div>
    <p>
        answered <span title="2024-01-09 14:08:55Z">yesterday</span>
    </p>
    <div>
        <a href="https://workplace.stackexchange.com/users/66740/aida-paul"><p><img src="https://i.stack.imgur.com/D35Mx.jpg?s=64&amp;g=1" alt="Aida Paul's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://workplace.stackexchange.com/users/66740/aida-paul">Aida Paul</a><span itemprop="name">Aida Paul</span></p><p><span title="reputation score 34,031" dir="ltr">34k</span><span>15 gold badges</span><span>90 silver badges</span><span>124 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">5</span></p>
    </div>


                                    
<div id="answer-194970" data-answerid="194970" data-parentid="194969" data-score="20" data-position-on-page="2" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p><strong>TL;DR -</strong> If you need this to stop, you need to have proof that the suggestions given to you are of low quality or outright wrong / infeasible. You do not need to differentiate yourself between a suggestion generated by a human or a machine. When someone presents a solution, and you have genuine reasons to turn them down, feel free.</p>
<p>Treat this as, they are not learning from their past interactions with you, and keep providing solutions which are not feasible / plausible in your setting / context. Whether the ideas or suggestions are generated by themselves or via prompt engineering, is not really relevant in this case. As needed, document these interactions, and present to your supervisor / manager, as how you are presented with below-par suggestions and you are investing time to prove the suggestions wrong, time and again.</p>
<p>Eventually when they see a "copy-pasted" content is not going to cut it, they'll put real efforts to make the content worthy.</p>
<hr>
<p>Edit after the comments:</p>
<p>Yes, at the beginning, it is an uphill battle, as the suggestions can be generated using ChatGPT, (or a simple web search, without context and having proper research and adaptation done) in relatively very less time, but it takes much longer to disprove them. However, this is still necessary for below reasons:</p>
<ul>
<li>It creates a document trail of your efforts. This does not only show that you are putting effort to filter out the suggestions, it also shows that you are providing explanations and feedback on why or how the suggestions are unfit for the purpose, and you're trying to help your co-workers to improve on themselves.</li>
<li>It shows that you are actually evaluating the solutions, not their source. People cannot cry foul that their suggestions are being ignored because they are "smarter" and generated suggestions by "clever" or "efficient" ways (whatever that means).</li>
</ul>
<hr>
<p><em>On a lighter note: Use ChatGPT to come up with the reasoning why the ChatGPT provided answers cannot be used.</em></p>
    </div>
    <div>
    <p>
        answered <span title="2024-01-09 12:41:22Z">yesterday</span>
    </p>
    <div>
        <a href="https://workplace.stackexchange.com/users/61983/sourav-ghosh"><p><img src="https://i.stack.imgur.com/4dwoM.jpg?s=64&amp;g=1" alt="Sourav Ghosh's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://workplace.stackexchange.com/users/61983/sourav-ghosh">Sourav Ghosh</a><span itemprop="name">Sourav Ghosh</span></p><p><span title="reputation score 70,419" dir="ltr">70.4k</span><span>45 gold badges</span><span>238 silver badges</span><span>296 bronze badges</span>
        </p>
    </div>
</div>
    
</div>




            <p><span itemprop="commentCount">13</span></p>
    </div>

                                    
<div id="answer-194978" data-answerid="194978" data-parentid="194969" data-score="11" data-position-on-page="3" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p><strong>Demonstrate it in their own domain</strong></p>
<p>If all else fails, how hard would it be to take one of the artists’ tools that is scriptable and generate scripts for it with ChatGPT? E.g. Blender is scriptable if they use Blender. My guess is many of their tools will be scriptable. Then you could as a teaching moment (maybe as part of a seminar), generate some scripts to create art assets or do something else with the art tool and demonstrate how while the AI gets close and can be helpful for simple tasks, for complex tasks the amount of human intervention is more trouble than it’s worth. This might take a good bit of work on your part finding out which tools are suitably scriptable and coming up with use cases that are of comparable complexity to the code the artists are giving you, and verifying that the AI generated scripts are wrong in a way that the artists will understand, but it could make the case pretty strongly. Obviously don’t do this on the fly—do your homework, verify that the AI really does a consistently and convincingly bad job before hand and run canned examples in the actual seminar; don’t come up with ones on the fly.</p>
<p>The artists may already be using AI but I’m guessing they’re not using it to generate scripts to fully-automate their tools since that’s effectively code and ChatGPT still has a long long way to go to be a competent coder. So if you can show them it’s bad at generating code in their domain, it may click.</p>
    </div>
    <div>
    <p>
        answered <span title="2024-01-10 02:59:19Z">yesterday</span>
    </p>
    <div>
        <a href="https://workplace.stackexchange.com/users/43182/bob"><p><img src="https://www.gravatar.com/avatar/5aa5263454ebce9dc0b2b885a6928fe8?s=64&amp;d=identicon&amp;r=PG&amp;f=y&amp;so-version=2" alt="bob's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://workplace.stackexchange.com/users/43182/bob">bob</a><span itemprop="name">bob</span></p><p><span title="reputation score " dir="ltr">6,973</span><span>1 gold badge</span><span>17 silver badges</span><span>43 bronze badges</span>
        </p>
    </div>
</div>
    
</div>




            <p><span itemprop="commentCount">8</span></p>
    </div>


                                    
<div id="answer-194979" data-answerid="194979" data-parentid="194969" data-score="11" data-position-on-page="4" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>As a former game engineer, I note the OP's comment to the question:</p>
<blockquote>
<p>I take the time to understand and consider each suggestion, not
rejecting anything out of hand, and share them with my team members
(of which there are 17). Their advice has been, thus far, to "just
ignore". However, I would not like to treat my co-workers that way.</p>
</blockquote>
<p>If you have a team of 17 all of whom are telling you to "just ignore" it, then I would recommend that you follow the team's consensus. If you must make it a bit more diplomatic, then you could briefly reply, "Our team policy is to not take outside technical suggestions", and/or "If you think this needs escalating, you can address it with our manager".</p>
<p>Edit the exact phrasing to your situation or taste, of course, but I would keep it curt to avoid inviting further interaction on the issue. That is, the further you stray from "just ignore", the worse you're making it for your team. In particular, addressing the ChatGPT-ness invites debate/subterfuge about whether it was really ChatGPT or something else.</p>
    </div>
    <div>
    <p>
        answered <span title="2024-01-10 05:16:12Z">yesterday</span>
    </p>
    <div>
        <a href="https://workplace.stackexchange.com/users/102797/daniel-r-collins"><p><img src="https://i.stack.imgur.com/BnWVU.jpg?s=64&amp;g=1" alt="Daniel R. Collins's user avatar" width="32" height="32"></p></a>
    </div>
    
</div>
    
</div>




            <p><span itemprop="commentCount">5</span></p>
    </div>

                                    
<div id="answer-194971" data-answerid="194971" data-parentid="194969" data-score="10" data-position-on-page="5" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>“Sorry, but ChatGPT does not have a deeper understanding of what it is talking about, which is very important in computer programs.  It is at best a rough suggestion created by stitching together various snippets and require an experienced developer to debug and fix.  This will most likely take longer and be of inferior quality to what we have already. Thanks anyway for your efforts “</p>
<p>I had ChatGPT tell me how to build OS/2 programs with GitHub actions.  Everything looked fine except GitHub didn’t support it. Hopefully this will also improve - it is still impressive what we have now.</p>
    </div>
    <div>
    <p>
        answered <span title="2024-01-09 13:10:42Z">yesterday</span>
    </p>
    <div>
        <a href="https://workplace.stackexchange.com/users/873/thorbj%c3%b8rn-ravn-andersen"><p><img src="https://i.stack.imgur.com/enNdg.jpg?s=64&amp;g=1" alt="Thorbjørn Ravn Andersen's user avatar" width="32" height="32"></p></a>
    </div>
    
</div>
    
</div>




            <p><span itemprop="commentCount">3</span></p>
    </div>

                                    
<div id="answer-194973" data-answerid="194973" data-parentid="194969" data-score="10" data-position-on-page="6" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<blockquote>
<p>How can I, without being condescending to either my coworkers or their use of ChatGPT, ask them to not make suggestions to me that they personally don't understand and are based solely on ChatGPT?</p>
</blockquote>
<p>Don't ask your coworkers.</p>
<p>There is a direct conflict in what you want from your coworkers (to not make suggestions that they don't fully understand) and what you have been tasked to do (to take suggestions from artists without any technical expertise).  You need to speak to your boss and clearly explain this conflict and follow their guidance on how to proceed.</p>
    </div>
    <div>
            
            <div>
        <a href="https://workplace.stackexchange.com/users/478/peter-mortensen"><p><img src="https://www.gravatar.com/avatar/78a0a4bb106d07b6c6f33a51988155e3?s=64&amp;d=identicon&amp;r=PG" alt="Peter Mortensen's user avatar" width="32" height="32"></p></a>
    </div>


            <div>
    <p>
        answered <span title="2024-01-09 16:29:38Z">yesterday</span>
    </p>
    <div>
        <a href="https://workplace.stackexchange.com/users/93810/sf02"><p><img src="https://www.gravatar.com/avatar/ac277f6f12e623b0a942cf11d31235ff?s=64&amp;d=identicon&amp;r=PG&amp;f=y&amp;so-version=2" alt="sf02's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://workplace.stackexchange.com/users/93810/sf02">sf02</a><span itemprop="name">sf02</span></p><p><span title="reputation score 78,612" dir="ltr">78.6k</span><span>39 gold badges</span><span>179 silver badges</span><span>251 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">2</span></p>
    </div>

                                    
<div id="answer-194990" data-answerid="194990" data-parentid="194969" data-score="0" data-position-on-page="7" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>You should talk to your boss about the ChatGPT fans.</p>
<p>There is a high chance your amazed-by-ChatGPT colleagues will use it to do part of their work. This implies possible business-critical data leaks, as well as potential copyright infringement.</p>
    </div>
    <div>
            
            <div>
    
    <div>
        <a href="https://workplace.stackexchange.com/users/12989/keshlam"><p><img src="https://i.stack.imgur.com/f6pBp.gif?s=64&amp;g=1" alt="keshlam's user avatar" width="32" height="32"></p></a>
    </div>
    <div>
        <p><a href="https://workplace.stackexchange.com/users/12989/keshlam">keshlam</a></p><p><span title="reputation score 61,091" dir="ltr">61.1k</span><span>13 gold badges</span><span>114 silver badges</span><span>209 bronze badges</span>
        </p>
    </div>
</div>


            <div>
                <div>
    <p>
        answered <span title="2024-01-10 20:14:28Z">11 hours ago</span>
    </p>
    <div>
        <a href="https://workplace.stackexchange.com/users/143228/user882813"><p><img src="https://www.gravatar.com/avatar/e49c05b5c0bf93ef572175bcae5c84e8?s=64&amp;d=identicon&amp;r=PG" alt="user882813's user avatar" width="32" height="32"></p></a>
    </div>
    
</div>

    <div>
        <p><span> New contributor</span>
        </p>
        <div>
            
            <p><a href="https://workplace.stackexchange.com/users/143228/user882813">user882813</a> is a new contributor to this site. Take care in asking for clarification, commenting, and answering.
Check out our <a href="https://workplace.stackexchange.com/conduct">Code of Conduct</a>.        </p></div>
    </div>

            </div>
        </div>
    
</div>




            <p><span itemprop="commentCount">1</span></p>
    </div>

                                    
<div id="answer-195001" data-answerid="195001" data-parentid="194969" data-score="0" data-position-on-page="8" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
    
    <div itemprop="text">
<p>There is an answer (with quite a few up votes) that says</p>
<blockquote>
<p>If you need this to stop, you need to have proof that the suggestions given to you are of low quality or outright wrong / infeasible.</p>
</blockquote>
<p>This is rather wrong. Your boss expects you to use your judgement about which technologies to use, which to investigate, and how much time to spend investigating different technologies. Go to your boss, present your view on the likely effectiveness of ChatGPT suggestions, and ask your boss how much time they want you to spend understanding them and explaining their problems to other employees. If your boss wants you to waste your time on this, tell them you will, and tell them you expect nothing useful to come from it.</p>
    </div>
    <div>
    <p>
        answered <span title="2024-01-11 05:47:54Z">2 hours ago</span>
    </p>
    <div>
        <a href="https://workplace.stackexchange.com/users/136217/jonathanz"><p><img src="https://i.stack.imgur.com/NJaj1.png?s=64&amp;g=1" alt="JonathanZ's user avatar" width="32" height="32"></p></a>
    </div>
    
</div>
    
</div>

                                    
<div id="answer-194984" data-answerid="194984" data-parentid="194969" data-score="-2" data-position-on-page="9" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>Get a ChatGPT subscription and, with the permission of the company, feed the codebase documentation into it.  Create a company-specific version of ChatGPT that might generate better answers.</p>
<p>Get the artists to use that version of ChatGPT to form their suggestions.</p>
    </div>
    <div>
                <div>
    <p>
        answered <span title="2024-01-10 16:04:02Z">15 hours ago</span>
    </p>
    <div>
        <a href="https://workplace.stackexchange.com/users/2382/peter-k"><p><img src="https://i.stack.imgur.com/jzpJM.png?s=64&amp;g=1" alt="Peter K.'s user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://workplace.stackexchange.com/users/2382/peter-k">Peter K.</a><span itemprop="name">Peter K.</span></p><p><span title="reputation score " dir="ltr">113</span><span>2 silver badges</span><span>7 bronze badges</span>
        </p>
    </div>
</div>

    <div>
        <p><span> New contributor</span>
        </p>
        <div>
            
            <p><a href="https://workplace.stackexchange.com/users/2382/peter-k">Peter K.</a> is a new contributor to this site. Take care in asking for clarification, commenting, and answering.
Check out our <a href="https://workplace.stackexchange.com/conduct">Code of Conduct</a>.        </p></div>
    </div>

            </div>
    
</div>




            <p><span itemprop="commentCount">1</span></p>
    </div>

                                <h2>
                                    You must <a href="https://workplace.stackexchange.com/users/login?ssrc=question_page&amp;returnurl=https%3a%2f%2fworkplace.stackexchange.com%2fquestions%2f194969">log in</a> to answer this question.
                                </h2>

                                    



                            <h2 data-loc="1">
                                <div><p>
Not the answer you're looking for? Browse other questions tagged </p><p>.                                </p></div>
                            </h2>
                </div>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Super Mario 64 on the Web (232 pts)]]></title>
            <link>https://probablykam.github.io/Mario64webgl/</link>
            <guid>38945687</guid>
            <pubDate>Thu, 11 Jan 2024 00:30:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://probablykam.github.io/Mario64webgl/">https://probablykam.github.io/Mario64webgl/</a>, See on <a href="https://news.ycombinator.com/item?id=38945687">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Software Engineers Say the Job Market Is Getting Much Worse (140 pts)]]></title>
            <link>https://www.vice.com/en/article/g5y37j/thousands-of-software-engineers-say-the-job-market-is-getting-much-worse</link>
            <guid>38945437</guid>
            <pubDate>Thu, 11 Jan 2024 00:02:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vice.com/en/article/g5y37j/thousands-of-software-engineers-say-the-job-market-is-getting-much-worse">https://www.vice.com/en/article/g5y37j/thousands-of-software-engineers-say-the-job-market-is-getting-much-worse</a>, See on <a href="https://news.ycombinator.com/item?id=38945437">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="BodyComponentRenderer"><p><span data-component="TextBlock"><p>For much of the 21st century, software engineering has been seen as one of the safest havens in the tenuous and ever-changing American job market.</p></span><span data-component="TextBlock"><p>But there are a growing number of signs that the field is starting to become a little less secure and comfortable, due to an industry-wide downturn and the <a href="https://www.vice.com/en/article/n7zxn7/learn-to-write-ai-advancements-mean-code-will-be-written-in-natural-english">looming threat of artificial intelligence</a> that is spurring growing competition for software jobs.&nbsp;</p></span></p><p><span data-component="TextBlock"><p>“The amount of competition is insane,” said Joe Forzano, an unemployed software engineer who has worked at the mental health startup Alma and private equity giant Blackstone.&nbsp;</p></span></p><blockquote data-component="QuoteBlock"><p><em><strong>Has AI affected your job? We want to hear from you. From a non-work device, contact our reporter at maxwell.strachan@vice.com or via Signal at 310-614-3752 for extra security.</strong></em></p></blockquote><p><span data-component="TextBlock"><p>Since he lost his job in March, Forzano has applied to over 250 jobs. In six cases, he went through the “full interview gauntlet,” which included between six and eight interviews each, before learning he had been passed over. “It has been very, very rough,” he told Motherboard.&nbsp;</p></span><span data-component="TextBlock"><p>Forzano is not alone in his pessimism, according to a December survey of 9,338 software engineers performed on behalf of Motherboard by Blind, an online anonymous platform for verified employees. In the poll, nearly nine in 10 surveyed software engineers said it is more difficult to get a job now than it was before the pandemic, with 66 percent saying it was “much harder.”&nbsp;</p></span><span data-component="TextBlock"><p>Nearly 80 percent of respondents said the job market has even become more competitive over the last year. Only 6 percent of the software engineers were “extremely confident” they could find another job with the same total compensation if they lost their job today while 32 percent said they were “not at all confident.”&nbsp;</p></span><span data-component="TextBlock"><p>Over 2022 and 2023, the tech sector incurred <a href="https://layoffs.fyi/" target="_blank">more than 400,000 layoffs</a>, according to the tracking site Layoffs.fyi. But up until recently, it seemed software engineers were more often spared compared to their co-workers in <a href="https://www.vice.com/en/article/3akddn/i-got-to-break-free-laid-off-tech-workers-search-for-new-purpose-outside-silicon-valley">non-technical fields</a>. One <a href="https://www.wsj.com/articles/silicon-valleys-talent-spotters-are-now-looking-for-jobs-11669725577" target="_blank">analysis found tech companies cut their recruiting teams</a> by 50 percent, compared to only 10 percent of their engineering departments. At Salesforce, engineers were four times less likely to lose their jobs than those in marketing and sales, which <a href="https://www.wsj.com/articles/these-tech-workers-say-they-were-hired-to-do-nothing-762ff158" target="_blank">Bloomberg has said</a> is a trend replicated at other tech companies such as Dell and Zoom.&nbsp;&nbsp;</p></span><span></span><span data-component="TextBlock"><p>But signs of dread among software engineers have started to become more common online. In December, one Amazon employee wrote a long post on the anonymous employee platform Blind saying that the “<a href="https://www.teamblind.com/post/Job-Market-is-Terrible-7anaSof8" target="_blank">job market is terrible</a>” and that he was struggling to get interviews of any sort.&nbsp;</p></span><span data-component="TextBlock"><p>The situation is a stark shift from much of the past two decades, when <a href="https://www.princetonreview.com/college-advice/top-ten-college-majors" target="_blank">computer science degrees and coding bootcamps exploded in popularity</a> due to the financial security they both promised. <a href="https://archive.is/jjBty" target="_blank">Entry-level Google software engineers reportedly</a> earned almost $200,000 a year and lived a life full of splashy perks, and engineers always seemed in high demand, meaning the next job was never hard to find.&nbsp;</p></span><span data-component="TextBlock"><p>As an undergraduate at the University of Pennsylvania in the early 2010s, Forzano had decided to major in computer science. The degree had put him in $180,000 of debt, but he saw it as a calculated bet on a sturdy field of work. “The whole concept was [that] it was a good investment to have that ‘Ivy League degree’ in an engineering field,” he said. He thought he’d be set for life.&nbsp;</p></span><span data-component="TextBlock"><p>Early in his career, that seemed to be true. Recruiters spammed him with opportunities, and he was easily able to jump from job to job and became a manager. The field felt so secure that the phrase “learn to code” became a mocking rejoinder whenever people in other fields expressed concern about their own job prospects online.&nbsp;&nbsp;</p></span><span></span><span data-component="TextBlock"><p>But the messages from recruiters have largely dried up since the pandemic, and getting the sort of jobs software engineers took for granted has become much harder. “There's just so much fucking competition,” he said. “It's a completely different landscape.” Thinking back to his decision to major in computer science as an undergraduate, he said he now feels “very naive.”</p></span><span data-component="TextBlock"><p>With the entrance of artificial intelligence into the conversation recently, there have been signs of a sea change in the coding world. AI programs that allow users to write code using natural language or auto-complete code were among the first wave of AI tools to take off. <a href="https://www.vice.com/en/article/n7zxn7/learn-to-write-ai-advancements-mean-code-will-be-written-in-natural-english">Google CEO Sundar Pichai said last year</a> that AI-powered coding tools had reduced the time it takes workers to complete code by 6 percent.&nbsp;</p></span><span data-component="TextBlock"><p>“In the age of AI, computer science is no longer the safe major,” <a href="https://www.theatlantic.com/technology/archive/2023/09/computer-science-degree-value-generative-ai-age/675452/" target="_blank">Kelli María Korducki wrote in </a><em><a href="https://www.theatlantic.com/technology/archive/2023/09/computer-science-degree-value-generative-ai-age/675452/" target="_blank">The Atlantic</a></em> in September. Matt Welsh, an entrepreneur who used to serve as a computer science professor at Harvard, told the magazine that the ability of AI to perform software engineering functions could lead to less job security and lower compensation for all but the very best in the software trade.</p></span><span data-component="TextBlock"><p>As of December, software engineers were not expressing much concern about AI making their jobs redundant. Only 28 percent saying they were “very” or “slightly” concerned in the Blind poll, and 72 percent saying they were “not really” or “not at all” concerned.&nbsp;</p></span><span data-component="TextBlock"><p>But when not considering their own situation, the software engineering world’s views on AI became markedly less optimistic. More than 60 percent of those surveyed said they believed their company would hire fewer people because of AI moving forward. &nbsp;</p></span><span data-component="TextBlock"><p>Forzano has not been shy about his trouble, sharing his pursuit for a new job on social media. The decision has led him to feel less alone, he said, as other tech workers expressed similar frustration about not being able to get interviews for jobs they felt overqualified for.&nbsp;</p></span><span data-component="TextBlock"><p>“We're all kind of like, ‘What the fuck is happening?’” he said.</p></span></p></div><div><p><h3>ORIGINAL REPORTING ON EVERYTHING THAT MATTERS IN YOUR INBOX.</h3></p><p>By signing up, you agree to the<!-- --> <a href="https://vice-web-statics-cdn.vice.com/privacy-policy/en_us/page/terms-of-use.html">Terms of Use</a> <!-- -->and<!-- --> <a href="https://vice-web-statics-cdn.vice.com/privacy-policy/en_us/page/privacy-policy.html">Privacy Policy</a> <!-- -->&amp; to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Drawing.garden (452 pts)]]></title>
            <link>https://drawing.garden/</link>
            <guid>38945125</guid>
            <pubDate>Wed, 10 Jan 2024 23:32:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://drawing.garden/">https://drawing.garden/</a>, See on <a href="https://news.ycombinator.com/item?id=38945125">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[FreeTaxUSA (201 pts)]]></title>
            <link>https://www.freetaxusa.com/</link>
            <guid>38944173</guid>
            <pubDate>Wed, 10 Jan 2024 22:17:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.freetaxusa.com/">https://www.freetaxusa.com/</a>, See on <a href="https://news.ycombinator.com/item?id=38944173">Hacker News</a></p>
Couldn't get https://www.freetaxusa.com/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[The memory remains: Permanent memory with systemd and a Rust allocator (123 pts)]]></title>
            <link>https://darkcoding.net/software/rust-systemd-memory-remains/</link>
            <guid>38944147</guid>
            <pubDate>Wed, 10 Jan 2024 22:16:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://darkcoding.net/software/rust-systemd-memory-remains/">https://darkcoding.net/software/rust-systemd-memory-remains/</a>, See on <a href="https://news.ycombinator.com/item?id=38944147">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
      <p>We are going to <strong>stitch three things</strong> together to make Rust objects that <strong>survive program restart</strong>. The memory backing the object will continue existing while the program restarts.</p>
<ol>
<li>
<p>Systemd’s <a href="https://systemd.io/FILE_DESCRIPTOR_STORE/">File Descriptor Store</a> (systemd 254+) allows us to give any file descriptor to systemd, and later ask for it back. The file stays open when we <code>systemctl restart &lt;prog&gt;</code>. Yes this is similar to <a href="https://darkcoding.net/software/systemd-socket-activation-in-go/">socket activation</a>.</p>
</li>
<li>
<p>The <a href="https://www.man7.org/linux/man-pages/man2/memfd_create.2.html">memfd_create</a> syscall (Linux 3.17+) creates an anonymous file that only exists in memory, and only as long as a reference to it exists. It effectively gives us a file descriptor pointing to a chunk of allocated memory. It behaves like a regular file so we can map it into regular memory with <a href="https://www.man7.org/linux/man-pages/man2/mmap.2.html">mmap</a>.</p>
</li>
<li>
<p>Rust’s <a href="https://doc.rust-lang.org/std/alloc/trait.Allocator.html">Allocator trait</a> which allows us to control the memory an object lives in (Rust nightly since at least 2 years ago). We will select our allocator with <a href="https://doc.rust-lang.org/std/boxed/struct.Box.html#method.new_in">Box::new_in</a>.</p>
</li>
</ol>
<p>We will make a Rust allocator which creates a memfd, shares it with systemd, mmap’s it, and uses that memory as the allocation our allocator returns. When the program restarts we ask systemd for our memfds, mmap it, and convert that into our object. Finally we wrap the allocator and the ‘restorer’ in a function which figures out whether we are allocating a new object or recovering an existing one.</p>
<p>Here’s an arbitrary object we will use throught the post:</p>
<pre tabindex="0"><code>// An arbitrary object we'll be using throughout this post
#[allow(dead_code)]
#[derive(Debug, Default)]
struct Person {
    name: [u8; 8],
    credits: u32,
    age: u8,
    is_admin: bool,
    count: u32,
}
</code></pre><p>Note the rather awkward <code>name</code> array. We don’t want any pointers in our object, so no <code>String</code> or <code>Vec</code> because those would also need to go in our permanent memory store.</p>
<h2 id="a-rust-allocator">A Rust Allocator </h2>
<p>If we allocate an object on the heap with <code>Box</code>, by default it will use the sytem allocator, which on Linux uses malloc. <a href="https://darkcoding.net/software/underrust-rust-assembly-output/">Looking at the assembly</a> we see it call <code>__rust_alloc</code> (via <code>GLOBAL_OFFSET_TABLE</code>) which calls <code>__rdl_alloc</code> which calls glibc’s <code>malloc</code>.</p>
<p><code>malloc</code> will get memory from the operating system (<a href="https://darkcoding.net/software/how-memory-is-allocated/">details</a>) by either moving the program break (<code>sbrk</code> syscall) or by memory mapping an anonymous region (via <code>mmap</code> syscall).</p>
<p>Rust (nightly) allows us to implement our own allocator. Here is an allocator that delegates to the system allocator - the allocator you’re using if you don’t know which one you’re using.</p>
<pre tabindex="0"><code>#![feature(allocator_api)]
use std::{
    alloc::{AllocError, Allocator, GlobalAlloc, Layout, System},
    ptr::NonNull,
};

struct DelegateAlloc {}
unsafe impl Allocator for DelegateAlloc {
    fn allocate(&amp;self, layout: Layout) -&gt; Result&lt;NonNull&lt;[u8]&gt;, AllocError&gt; {
        unsafe {
            let ptr = System.alloc(layout);
            let slice = std::slice::from_raw_parts_mut(ptr, layout.size());
            Ok(NonNull::new_unchecked(slice))
        }
    }

    unsafe fn deallocate(&amp;self, ptr: NonNull&lt;u8&gt;, layout: Layout) {
        System.dealloc(ptr.as_ptr(), layout)
    }
}
</code></pre><p>We use it like this:</p>
<pre tabindex="0"><code>    let d = DelegateAlloc {};
    let p = Box::new_in(Person::default(), d);
</code></pre><h2 id="an-allocator-backed-by-persistent-memory">An allocator backed by persistent memory </h2>
<p>Now that we know how to write an allocator, let’s make the memory it uses be in a file systemd owns. We need to call <code>memfd_create</code>, then <code>mmap</code> that file descriptor, and finally give it to systemd.</p>

<p>There are two cases:</p>
<ul>
<li>
<p>Create: When we are creating a new allocation we need to call <code>memfd_create</code>, and then give that file descriptor to systemd by calling <a href="https://www.freedesktop.org/software/systemd/man/latest/sd_pid_notify_with_fds.html">sd_pid_notify_with_fds</a>.</p>
</li>
<li>
<p>Restore: When we are starting up we ask systemd for the fd by calling <a href="https://www.freedesktop.org/software/systemd/man/latest/sd_listen_fds_with_names.html">sd_listen_fds_with_names</a>. Then we only need to <code>mmap</code>.</p>
</li>
</ul>
<p>In either case after <code>mmap</code> we end up with a bag of bytes, which we need to convince Rust is a <code>Person</code>. That’s what <code>buf_addr as *mut u8 as *mut T</code> is doing (here <code>T</code> is <code>Person</code>).</p>
<p>We use the allocator like this:</p>
<pre tabindex="0"><code>fn main() -&gt; anyhow::Result&lt;()&gt; {
    let salloc = SystemdAlloc::new("person1".to_string())?;
    let mut p1: Box&lt;Person, SystemdAlloc&gt; = salloc.into_box(Person::default())?;
    p1.count += 1;
    println!("{}", p1.count);
    std:🧵:sleep(std::time::Duration::from_secs(3600));
}
</code></pre><p><code>Person</code> is an arbitrary object, there’s one at the start of this post.</p>
<p>Each object must have a name which we associate with the FD in systemd. On restore that allows us to map the correct FD back to the object.</p>
<p>The memory only exists while the program is active (meaning running or restarting). If the program is stopped systemd will evict it’s file descriptor from the store. Hence the long sleep at the end of main.</p>
<h2 id="systemd-file-descriptor-store">systemd file descriptor store </h2>
<p>Typically now we’d create a systemd <code>.service</code> file making sure to enable the File Descriptor store - it doesn’t default. Here we’ll use <code>systemd-run</code>:</p>
<p><code>systemd-run --user --service-type=exec --unit=remains -p FileDescriptorStoreMax=16 ./target/debug/remains</code></p>
<p>Note that here ‘remains’ is the unit name (–unit=) not the binary. Then we treat it like any other systemd service except it’s in the user scope.</p>
<p>It’s output is in <code>journalctl --user -u remains</code>:</p>
<pre tabindex="0"><code>gktest[277943]: Received 0 fds from systemd
gktest[277943]: Creating new store
gktest[277943]: 1
</code></pre><p>Restart it:</p>
<pre tabindex="0"><code>systemctl --user restart remains
</code></pre><p>Now <code>p1.count</code> increases, it survives restart:</p>
<pre tabindex="0"><code>systemd[4357]: Stopping remains.service
systemd[4357]: Stopped remains.service
systemd[4357]: Starting remains.service
systemd[4357]: Started remains.service
gktest[278030]: Received 1 fds from systemd
gktest[278030]: systemd sent us: person1
gktest[278030]: Restoring
gktest[278030]: 2
</code></pre><p>Stop it like this: <code>systemctl --user stop remains</code>.</p>
<p>After that <code>stop</code> the fd is no longer stored. If we <code>start</code> now the count will reset.</p>
<p>systemd 254+ (Fedora 39+) allows us to set <code>FileDescriptorStorePreserve=yes</code> to keep the store alive even when our program is stopped.</p>
<h2 id="improvements">Improvements </h2>
<p>When I have done <a href="https://darkcoding.net/software/online-upgrades-in-go/">online upgrades in the past</a> the amount of state that I couldn’t recreate was quite small. I either stored that state in Redis or wrote it to a temporary file, as a single serialized object. Hence the systemd allocator above can only allocate once, hopefully that’s all we’d need.</p>
<p>To make it work with multiple objects ideally we could estimate up-front how much space we might need, and size the file and mmap space on first call. This is helpful if all we want is a couple of <code>String</code> or a <code>Vec</code> in we persistent object. The objects could go sequentially in the file, we’d maintain an index of their positions. An mmap can grow (<code>mremap</code> syscall) but there’s no guarantee that there is space to grow into.</p>
<p>If estimating size up-front is not possible we’d need to add a new file whenever the current one can no longer grow. systemd’s <code>FileDescriptorStoreMax</code> will need to be set large enough, and each file will need a unique string passed to <code>sd_pid_notify_with_fds</code>’s <code>FDNAME</code>. A global index points to the file, and each file has an index of offsets to the ojects. And now we’re writing a real memory allocator.</p>
<p>Instead of packing the objects we could make an allocator factory (the design pattern) and have each allocator deal with a single object in a single file. Simpler, but only suitable if we have a limited number of objects.</p>
<p>A fun exercise is to change the allocator to be backed by a disk file. The bytes on disk will be exactly how the object is laid out in memory.</p>
<h2 id="clone-and-pin-notes">Clone and Pin notes </h2>
<p>Let’s go back to this allocation:</p>
<pre tabindex="0"><code>    let mut p1: Box&lt;Person, SystemdAlloc&gt; = salloc.into_box(Person::default())?;
</code></pre><p>If we call this <code>let p2 = p1.clone()</code> (after adding <code>#[derive(Clone)]</code> to <code>Person</code>) the Box isn’t cloned but dereferenced. <code>clone</code> is called directly on <code>Person</code>. We end up with a new Person object on the stack. No problems there.</p>
<p>If instead we do <code>let p2 = Box::clone(p1)</code> the box itself is cloned. A <code>Box</code> internally is a tuple of a pointer and an allocator. We would need to <code>#[derive(Clone)]</code> on <code>SystemdAlloc</code> for it to compile, but that’s just where our problems will start. Rust will clone the allocator and call allocate on it. Our allocator is only designed for single use. See the previous section for notes on that.</p>
<p><code>Pin</code> and async contexts in general are not something an allocator needs to worry about. The object in the allocation might change (e.g. with <code>std::mem::swap</code>) but we are only concerned with the allocation itself, not with what’s inside it. As cliched programmers, we care about the <code>Box</code>, not the <code>Person</code> :-)</p>
<p>Happy allocating!</p>
<p><small>The title is indeed a reference to the Metallica / Marianne Faithfull song. This is not an endorsement of Metallica after “… And Justice for All”.</small></p>

    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Spot Bitcoin ETF receives official approval from the SEC (451 pts)]]></title>
            <link>https://cointelegraph.com/news/sec-spot-bitcoin-etf-approvals</link>
            <guid>38943291</guid>
            <pubDate>Wed, 10 Jan 2024 21:16:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cointelegraph.com/news/sec-spot-bitcoin-etf-approvals">https://cointelegraph.com/news/sec-spot-bitcoin-etf-approvals</a>, See on <a href="https://news.ycombinator.com/item?id=38943291">Hacker News</a></p>
Couldn't get https://cointelegraph.com/news/sec-spot-bitcoin-etf-approvals: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Signs that it's time to leave a company (509 pts)]]></title>
            <link>https://adrianco.medium.com/signs-that-its-time-to-leave-a-company-5f8759ad018e</link>
            <guid>38943040</guid>
            <pubDate>Wed, 10 Jan 2024 20:59:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://adrianco.medium.com/signs-that-its-time-to-leave-a-company-5f8759ad018e">https://adrianco.medium.com/signs-that-its-time-to-leave-a-company-5f8759ad018e</a>, See on <a href="https://news.ycombinator.com/item?id=38943040">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://adrianco.medium.com/?source=post_page-----5f8759ad018e--------------------------------"><div aria-hidden="false"><p><img alt="adrian cockcroft" src="https://miro.medium.com/v2/resize:fill:88:88/0*KKUkpPpAqpTcWefw.png" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><figure><figcaption>Seagull taking off — Picture by Adrian</figcaption></figure><p id="522d">When the company stops growing, when the founder moves on, when HR is calling the shots, when the executives care more about real estate than products…</p><p id="01dd">It’s time to leave, and find a new company to work for.</p><p id="e073">I joined AWS about seven years ago, when it was growing fast, there was headcount and flexibility in hiring, and Amazon was famously cheap about the office space it occupied. I retired a year and a half ago, and many of the people I worked with have also left. Justin Garrison hasn’t even left yet, and he just wrote a critical blog post “<a href="https://justingarrison.com/blog/2023-12-30-amazons-silent-sacking/" rel="noopener ugc nofollow" target="_blank">Amazon’s Silent Sacking</a>” about the way they are pushing people out. What changed?</p><p id="d3de">Growth slowed. When companies transition from high growth to slow growth or begin shrinking it isn’t just a small change, it needs a fundamentally different approach to management and the culture of the company. I saw this play out while I was at Sun Microsystems as the dot com bubble burst in 2001–2004. Adding headcount and budget every year covers up all kind of problems, it brings a flexibility to operations that becomes part of the culture. When growth is flat to shrinking companies freeze like deer in the headlights, and to make it worse managers start to hoard headcount and play politics to preserve their products. Innovation goes away, micromanagement appears and everyone gets less useful work done. I haven’t seen a good way to gradually manage this situation. The most successful approaches I’ve seen (which Sun didn’t do) are to cut deeply and early by shutting down entire product lines and removing layers of management, so that the company can grow back by opening headcount again. It’s like pruning a sickly tree. If you cut off the dead leaves one by one, you end up with too many branches and shriveled fruit. If you cut back hard, and remove whole branches, the roots provide enough energy for the tree to grow back. This does mean that you have to shut down or sell off entire product lines, and shrink the business. It’s very hard to contemplate and navigate this, which is why most companies don’t do it agressively enough. As an employee, it’s usually best to leave in the first wave of cuts.</p><p id="a158">In the unlikely event that a deep pruning and grow back strategy is happening at your company, and you are still there, the product teams that are left after pruning should try to keep the most experienced employees, lay off the junior ones, and return managers to individual contributor positions where possible. Experienced employees have been through this before, make better judgement calls under stress, and communicate better. There is a tendency to micromanage and add process overhead that needs to be resisted. Simplify processes to <a href="https://www.amazon.com/Delicate-Art-Bureaucracy-Transformation-Wrestler/dp/1950508153" rel="noopener ugc nofollow" target="_blank">reduce bureaucracy</a> and management overhead, speed up time-to-value, and take advantage of the gaps in the market that appear as competitors fail.</p><p id="c50c">Founder led companies often have problems maintaining their innovation culture when the founder moves on. I think this is part of the problem at Amazon, and I was happy to be leaving as Andy Jassy took over from Jeff Bezos and Adam Selipsky took over AWS. Jeff Bezos was always focused on keeping the “Day 1” culture at Amazon, and everyone I talk to there is clear that it’s now “Day 2”. Politics and micromanagement have taken over, and HR processes take up far too much of everyone’s time.</p><p id="977f">There’s another red flag for me when large real estate construction projects take up too much management attention. The plans for Amazon HQ2, and the building of ever larger and fancier office tower blocks in Seattle by Amazon collided with Covid, lockdown and an accelerated work from home movement. The right thing to do coming out of lockdown would have been to write down the real estate investment in one go, re-negotiate the tax incentives that cities provide, embrace remote working, and continue the policy Amazon had at the time, where each director level leader could decide what was best for their teams. Instead, we now have the situation that Amazon management care more about real estate than product. Where is the customer obsession in that? Customers don’t care what the companies buildings are like. As Justin says, they are using Return To Office (RTO) to drive people out of the company without needing to fire them. By analogy, pruning by cutting off the best remaining leaves and fruit buds one at a time, leaving the dead wood behind.</p><p id="674e">[Update] Since posting this I’ve had private messages from people at Amazon thanking me for saying what they aren’t allowed to say internally. “Disagree and commit” has been weaponized to force the RTO policy on everyone. In a group making a decision, it’s usually a majority that agree with the decision, and the people that don’t agree see that they are outnumbered and commit to it. However with RTO I think it is a minority imposing an unpopular policy on everyone else, without supporting data or discussion. Disagreeing is career-limiting, so what we are seeing is “Disagree and quit”. People are interviewing while waiting for their next RSU grant. Someone also said that it’s not “Day 2” it’s “Day 3”, and did I know anywhere that is hiring?.</p><p id="76f2">In summary, I don’t think the situation for Amazon is as bad as it was for Sun in 2002, and in the short term they are going to continue to grow the business slowly. However I do think there’s lessons to be learned, and that the delusion that they can roll back work from home and enforce RTO without killing off innovation is a big problem that will increasingly hurt them over time. I personally hired a bunch of people into AWS, in my own team and by encouraging people to join elsewhere. Nowadays I’d say a hard no to anyone thinking of working there, they have shown that you can’t trust whatever they say during the interview process. Local management gets overruled to relocate people hired to be remote, and to move to locations that they can change arbitrarily. Try and get a job at somewhere growing rapidly with a sensible work location policy like NVIDIA instead.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Conditional Git Configuration (266 pts)]]></title>
            <link>https://blog.scottlowe.org/2023/12/15/conditional-git-configuration/</link>
            <guid>38942892</guid>
            <pubDate>Wed, 10 Jan 2024 20:49:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.scottlowe.org/2023/12/15/conditional-git-configuration/">https://blog.scottlowe.org/2023/12/15/conditional-git-configuration/</a>, See on <a href="https://news.ycombinator.com/item?id=38942892">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
  <p><span>&nbsp;Published on 15 Dec 2023 ·
    &nbsp;Filed in <a href="https://blog.scottlowe.org/categories/explanation">Explanation</a> ·
    &nbsp;304 words (estimated 2 minutes to read)</span></p><p>Building on the earlier article on <a href="https://blog.scottlowe.org/2023/12/11/automatically-transforming-git-urls/">automatically transforming Git URLs</a>, I’m back with another article on a (potentially powerful) feature of <a href="https://www.git-scm.com/">Git</a>—the ability to conditionally include Git configuration files. This means you can configure Git to be configured (and behave) differently based on certain conditions, simply by including or not including Git configuration files. Let’s look at a pretty straightforward example taken from my own workflow.</p>
<p>Here’s a configuration stanza from my own system-wide Git configuration:</p>
<div><pre tabindex="0"><code data-lang="toml"><span><span>[<span>includeIf</span> <span>"gitdir:~/Work/Code/Repos/"</span>]
</span></span><span><span>    <span>path</span> = <span>~/</span><span>Work</span><span>/</span><span>Code</span><span>/</span><span>Repos</span><span>/</span>.<span>gitconfig</span>
</span></span></code></pre></div><p>The key here is the <code>includeIf</code> keyword. In this case, Git will include the referenced configuration file specified by <code>path</code>, <em>if</em> the location of the Git repository matches the path specification after <code>gitdir</code>. Basically, what this means is that <em>all</em> repositories under <code>~/Work/Code/Repos</code> will trigger the inclusion of the additional configuration file.</p>
<p>Here’s the additional configuration file:</p>
<div><pre tabindex="0"><code data-lang="toml"><span><span>[<span>user</span>]
</span></span><span><span>    <span>email</span> = <span>name</span><span>@</span><span>work-domain</span>.<span>com</span>
</span></span><span><span>    <span>name</span> = <span>Scott</span> <span>Lowe</span>
</span></span><span><span>[<span>commit</span>]
</span></span><span><span>    <span>gpgsign</span> = <span>false</span>
</span></span></code></pre></div><p>As long as I group all work-relatd repositories in the specified directory path, these values override the system-wide values. This means I can specify my work e-mail address as the e-mail address to be associated with commits to work-related repositories while all others use a different e-mail address. This configuration also allows me to disable GPG signing of commits for work-related repositories (i.e., repositories in the specified path), since I don’t have a GPG key associated with my work e-mail address.</p>
<p>Could you do this with per-repository configuration settings? <em>Absolutely.</em> This configuration mechanism allows you to apply configuration settings to groups of repositories based on their filesystem location, instead of having to do the same thing on a per-repository basis.</p>
<p>I hope you find this information useful. Do feel free to hit me up—<a href="https://fosstodon.org/@scottslowe">on the Fediverse</a>, <a href="https://twitter.com/scott_lowe">on Twitter</a>, or in any of a variety of Slack communities—if you have any questions or any feedback!</p>

  <h3>Metadata and Navigation</h3>
  <p><span>
  &nbsp;<a href="https://blog.scottlowe.org/tags/git">Git</a> &nbsp;<a href="https://blog.scottlowe.org/tags/cli">CLI</a> 

  <br>
   Previous Post: <a href="https://blog.scottlowe.org/2023/12/11/automatically-transforming-git-urls/">Automatically Transforming Git URLs</a>
  
  <br>
    Next Post: <a href="https://blog.scottlowe.org/2023/12/18/dynamically-enabling-azure-cli-with-direnv/">Dynamically Enabling the Azure CLI with Direnv</a>
  </span>

  <span>
  <p>Be social and share this post!<br>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fblog.scottlowe.org%2f2023%2f12%2f15%2fconditional-git-configuration%2f" title="Share on Facebook"><i></i></a>
  <a href="https://twitter.com/intent/tweet?url=https%3a%2f%2fblog.scottlowe.org%2f2023%2f12%2f15%2fconditional-git-configuration%2f&amp;text=Conditional%20Git%20Configuration" title="Share on Twitter"><i></i></a>
  <a href="https://plus.google.com/share?url=https%3a%2f%2fblog.scottlowe.org%2f2023%2f12%2f15%2fconditional-git-configuration%2f" title="Share on Google Plus"><i></i></a></p>
  </span>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Netflix never used its $1M algorithm (2012) (283 pts)]]></title>
            <link>https://thenextweb.com/news/remember-netflixs-1m-algorithm-contest-well-heres-why-it-didnt-use-the-winning-entry</link>
            <guid>38942867</guid>
            <pubDate>Wed, 10 Jan 2024 20:48:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thenextweb.com/news/remember-netflixs-1m-algorithm-contest-well-heres-why-it-didnt-use-the-winning-entry">https://thenextweb.com/news/remember-netflixs-1m-algorithm-contest-well-heres-why-it-didnt-use-the-winning-entry</a>, See on <a href="https://news.ycombinator.com/item?id=38942867">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-main-content">
                            <p><a href="http://netflix.com/" target="_blank" rel="nofollow noopener">Netflix</a> was founded in the US in 1997, starting initially as a DVD-rental delivery service before finally morphing into more of a VoD streaming service. Today, it serves the US and Canada, Latin America, the Caribbean, and, as of January this year, the <a href="https://thenextweb.com/news/netflix-arrives-in-the-uk-at-5-99-per-month-launches-with-one-month-free-trial" target="_blank" rel="noopener">UK and Ireland</a>, where it finally arrived to exchange blows with LoveFilm.</p>
<p>Cast your mind back to when the company launched the <a href="http://www.netflixprize.com/" target="_blank" rel="nofollow noopener">Netflix Prize</a>. Announced in 2006, the prize sought to reward clever people for developing a recommendation algorithm. The company offered $1 million to whoever could improve the accuracy of its existing system, Cinematch, by 10%.</p>
<p>On September 21, 2009 <a href="http://www.netflixprize.com/community/viewtopic.php?id=1537" target="_blank" rel="nofollow noopener">the mammoth prize was given</a> to team <em>BellKor’s Pragmatic Chaos</em>, who produced an algorithm which apparently <a href="http://www.netflixprize.com//leaderboard" target="_blank" rel="nofollow noopener">improved the search by 10.06%</a>.</p>
<figure><img loading="lazy" title="" src="https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2012/04/NFlix-520x285.png" alt="" width="520" height="285" srcset="https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2012/04/NFlix-520x285.png 520w, https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2012/04/NFlix-220x120.png 220w, https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2012/04/NFlix-82x45.png 82w, https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2012/04/NFlix.png 773w" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2012/04/NFlix-520x285.png" data-srcset="https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2012/04/NFlix-520x285.png 520w, https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2012/04/NFlix-220x120.png 220w, https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2012/04/NFlix-82x45.png 82w, https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2012/04/NFlix.png 773w"></figure>
<h3>Whatever happened to that $1m algorithm?</h3>
<div id="hs-embed-tnw"><p><img src="https://s3.amazonaws.com/events.tnw/hardfork-2018/uploads/visuals/tnw-newsletter.png"></p><div><p>The &lt;3 of EU tech</p><p>The latest rumblings from the EU tech scene, a story from our wise ol' founder Boris, and some questionable AI art. It's free, every week, in your inbox. Sign up now!</p></div></div><p>As <a href="https://www.techdirt.com/blog/innovation/articles/20120409/03412518422/why-netflix-never-implemented-algorithm-that-won-netflix-1-million-challenge.shtml" target="_blank" rel="nofollow noopener">pointed out</a> by TechDirt’s Mike Masnick, last week Netflix launched a <a href="http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html" target="_blank" rel="nofollow noopener">two-part blog post</a> on its recommendations system, one of which was interesting if only for what it revealed about the outcome of this supposed winning formula which it coughed up big bucks for. Indeed, by the time the algorithm was good to go, Netflix as a business had moved on.</p>
<p>Whilst they acknowledge that the work that went into the final product was immense, Xavier Amatriain and Justin Basilico, Personalization Science and Engineering at Netflix, say that the engineering effort required to achieve the accuracy gains they measured, wasn’t entirely justified.</p>
<p>“This is a truly impressive compilation and culmination of years of work, blending hundreds of predictive models to finally cross the finish line,” they say. “We evaluated some of the new methods offline but the additional accuracy gains that we measured did not seem to justify the engineering effort needed to bring them into a production environment. Also, our focus on improving Netflix personalization had shifted to the next level by then.”</p>
<p>Basically, streaming changed the way its members interacted with Netflix, as well as the type of data available in its algorithms. “For DVDs our goal is to help people fill their queue with titles to receive in the mail over the coming days and weeks; selection is distant in time from viewing, people select carefully because exchanging a DVD for another takes more than a day, and we get no feedback during viewing,” they say. “For streaming members who are looking for something great to watch right now; they can sample a few videos before settling on one, they can consume several in one session, and we can observe viewing statistics such as whether a video was watched fully or only partially.”</p>
<p>But this is an interesting point, in that it highlights the differences between ‘watch now’ and ‘watch later’ behaviours. As TechDirt’s Masnick <a href="https://www.techdirt.com/blog/innovation/articles/20120409/03412518422/why-netflix-never-implemented-algorithm-that-won-netflix-1-million-challenge.shtml" target="_blank" rel="nofollow noopener">says</a>:</p>
<blockquote><p>“I also find it interesting that there’s a clear distinction in the kinds of recommendations people that work if people are going to ‘watch now’ vs. ‘watch in the future’.</p>
<p>I think this is an issue that Netflix probably has faced on the DVD side for years: when people rent a movie that won’t arrive for a few days, they’re making a bet on what they want at some future point. And, people tend to have a more… optimistic viewpoint of their future selves. That is, they may be willing to rent, say, an “artsy” movie that won’t show up for a few days, feeling that they’ll be in the mood to watch it a few days (weeks?) in the future, knowing they’re not in the mood immediately.</p>
<p>But when the choice is immediate, they deal with their present selves, and that choice can be quite different. It would be great if Netflix revealed a bit more about those differences, but it is already interesting to see that the shift from delayed gratification to instant gratification clearly makes a difference in the kinds of recommendations that work for people.”</p></blockquote>
<p>Netflix also says that another big change to come about was the move from a single platform onto hundreds of devices, for example when it integrated with Roku and Xbox in 2008, which was already a couple of years into the competition. Then Netflix streaming made it to the iPhone, before hitting Android and range of other connected devices. In short, a one-algorithm approach clearly won’t work across such a wide range of platforms.</p>
<p>It’s also worth noting the global availability of Netflix. It’s now extended beyond the US and in to Canada, 43 Latin-American countries plus the UK and Ireland. It has 23 million subscribers in 47 countries, who streamed 2 billion hours from multiple devices in Q4 2011 alone. Every day 2 million movies and TV shows are queued, and 4 million ratings are generated. So how does personalization cope with such a massive multifaceted jump in usage?</p>
<p>“We have adapted our personalization algorithms to this new scenario in such a way that now 75% of what people watch is from some sort of recommendation,” says Netflix. “We reached this point by continuously optimizing the member experience and have measured significant gains in member satisfaction whenever we improved the personalization for our members.”</p>
<p>Well, $1m may sound like a lot of money for Netflix to pay for something that wasn’t really used, but we’re sure Bob Bell, Martin Chabbert, Michael Jahrer, Yehuda Koren, Martin Piotte, Andreas Töscher and Chris Volinsky from <em>BellKor’s Pragmatic Chaos</em> don’t mind too much.</p>
<figure><img loading="lazy" title="" src="https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2012/04/NFlixwinners-520x341.jpg" alt="" width="520" height="341" srcset="https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2012/04/NFlixwinners-520x341.jpg 520w, https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2012/04/NFlixwinners-220x144.jpg 220w, https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2012/04/NFlixwinners.jpg 594w" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2012/04/NFlixwinners-520x341.jpg" data-srcset="https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2012/04/NFlixwinners-520x341.jpg 520w, https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2012/04/NFlixwinners-220x144.jpg 220w, https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2012/04/NFlixwinners.jpg 594w"></figure>

                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Shoelace: A library of web components (262 pts)]]></title>
            <link>https://shoelace.style/</link>
            <guid>38942847</guid>
            <pubDate>Wed, 10 Jan 2024 20:46:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shoelace.style/">https://shoelace.style/</a>, See on <a href="https://news.ycombinator.com/item?id=38942847">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="content">
        <div>
          <div>
            <div>
              <p><img src="https://shoelace.style/assets/images/wordmark.svg" alt="Shoelace"></p>
              <ul>
                <li>Works with all frameworks 🧩</li>
                <li>Works with CDNs 🚛</li>
                <li>Fully customizable with CSS 🎨</li>
                <li>Includes a dark theme 🌛</li>
                <li>Built with accessibility in mind ♿️</li>
                <li>First-class <a href="https://shoelace.style/frameworks/react">React support</a> ⚛️</li>
                <li>Built-in localization 💬</li>
                <li>Open source 😸</li>
                <li>
                  <a href="https://blog.fontawesome.com/shoelace-joins-font-awesome/" rel="noopener noreferrer" target="_blank">More awesome than ever</a>
                  <img src="https://shoelace.style/assets/images/awesome.svg" alt="Awesome emoji">
                </li>
              </ul>
            </div>
            <p><img src="https://shoelace.style/assets/images/undraw-content-team.svg" alt="Cartoon of people assembling components while standing on a giant laptop.">
            </p>
          </div>
          
          <h2 id="quick-start">
            Quick Start<a href="#quick-start" aria-label="Direct link to &quot;Quick Start&quot;"></a>
          </h2>
          <p>Add the following code to your page.</p>
          <!-- prettier-ignore -->
          <pre><code id="code-block-1"><span><span><span>&lt;</span>link</span> <span>rel</span><span><span>=</span><span>"</span>stylesheet<span>"</span></span> <span>href</span><span><span>=</span><span>"</span>https://cdn.jsdelivr.net/npm/@shoelace-style/shoelace@2.12.0/cdn/themes/light.css<span>"</span></span> <span>/&gt;</span></span>
<span><span><span>&lt;</span>script</span> <span>type</span><span><span>=</span><span>"</span>module<span>"</span></span> <span>src</span><span><span>=</span><span>"</span>https://cdn.jsdelivr.net/npm/@shoelace-style/shoelace@2.12.0/cdn/shoelace-autoloader.js<span>"</span></span><span>&gt;</span></span><span></span><span><span><span>&lt;/</span>script</span><span>&gt;</span></span>
</code><sl-copy-button from="code-block-1"></sl-copy-button></pre>
          <p>Now you have access to all of Shoelace’s components! Try adding a button:</p>

          <div id="code-preview-source-group-1">
                <pre><code id="code-block-2"><span><span><span>&lt;</span>sl-button</span><span>&gt;</span></span>Click me<span><span><span>&lt;/</span>sl-button</span><span>&gt;</span></span>
</code><sl-copy-button from="code-block-2"></sl-copy-button></pre>
              </div>

          
          <h2 id="new-to-web-components">
            New to Web Components?<a href="#new-to-web-components" aria-label="Direct link to &quot;New to Web Components?&quot;"></a>
          </h2>
          <p>
            <strong>TL;DR</strong> –&nbsp;we finally have a way to create
            <a href="https://html.spec.whatwg.org/multipage/custom-elements.html" rel="noopener noreferrer" target="_blank">our own HTML elements</a>
            and use them in any framework we want!
          </p>
          <p>
            Thanks to the popularity of frameworks such as Angular, Vue, and React, component-driven development has
            become a part of our every day lives. Components help us encapsulate styles and behaviors into reusable
            building blocks. They make a lot of sense in terms of design, development, and testing.
          </p>
          <p>Unfortunately, <em>framework-specific</em> components fail us in a number of ways:</p>
          <ul>
            <li>You can only use them in the framework they’re designed for 🔒</li>
            <li>Their lifespan is limited to that of the framework’s ⏳</li>
            <li>
              New frameworks/versions can lead to breaking changes, requiring substantial effort to update components 😭
            </li>
          </ul>
          <p>
            Web components solve these problems. They’re
            <a href="https://caniuse.com/#feat=custom-elementsv1" rel="noopener noreferrer" target="_blank">supported by all modern browsers</a>, they’re framework-agnostic, and they’re
            <a href="https://developer.mozilla.org/en-US/docs/Web/Web_Components" rel="noopener noreferrer" target="_blank">part of the standard</a>, so we know they’ll be supported for many years to come.
          </p>
          <p>This is the technology that Shoelace is built on.</p>
          <h2 id="what-problem-does-this-solve">
            What Problem Does This Solve?<a href="#what-problem-does-this-solve" aria-label="Direct link to &quot;What Problem Does This Solve?&quot;"></a>
          </h2>
          <p>
            Shoelace provides a collection of professionally designed, highly customizable UI components built on a
            framework agnostic technology. Why spend hundreds of hours (or more) building a design system from scratch?
            Why make a component library that only works with one framework?
          </p>
          <p>With Shoelace, you can:</p>
          <ul>
            <li>Start building things faster (no need to roll your own buttons)</li>
            <li>Build multiple apps with different frameworks that all share the same UI components</li>
            <li>Fully customize components to match your existing designs</li>
            <li>Incrementally adopt components as needed (no need to ditch your framework)</li>
            <li>Upgrade or switch frameworks without rebuilding foundational components</li>
          </ul>
          <p>
            If your organization is looking to build a design system,
            <a href="https://medium.com/eightshapes-llc/and-you-thought-buttons-were-easy-26eb5b5c1871" rel="noopener noreferrer" target="_blank">Shoelace will save you thousands of dollars</a>. All the foundational components you need are right here, ready to be customized for your brand. And since
            it’s built on web standards, browsers will continue to support it for many years to come.
          </p>
          <p>
            Whether you use Shoelace as a starting point for your organization’s design system or for a fun personal
            project, there’s no limit to what you can do with it.
          </p>
          <h2 id="browser-support">
            Browser Support<a href="#browser-support" aria-label="Direct link to &quot;Browser Support&quot;"></a>
          </h2>
          <p>Shoelace is tested in the latest two versions of the following browsers.</p>
          <p><img src="https://shoelace.style/assets/images/chrome.png" alt="Chrome" width="64" height="64">
          <img src="https://shoelace.style/assets/images/edge.png" alt="Edge" width="64" height="64">
          <img src="https://shoelace.style/assets/images/firefox.png" alt="Firefox" width="64" height="64">
          <img src="https://shoelace.style/assets/images/opera.png" alt="Opera" width="64" height="64">
          <img src="https://shoelace.style/assets/images/safari.png" alt="Safari" width="64" height="64"></p><p>Critical bug fixes in earlier versions will be addressed based on their severity and impact.</p>
          <p>
            If you need to support IE11 or pre-Chromium Edge, this library isn’t for you. Although web components can
            (to some degree) be polyfilled for legacy browsers, supporting them is outside the scope of this project. If
            you’re using Shoelace in such a browser, you’re gonna have a bad time. ⛷
          </p>
          <h2 id="license">
            License<a href="#license" aria-label="Direct link to &quot;License&quot;"></a>
          </h2>
          <p>
            Shoelace was created in New Hampshire by
            <a href="https://twitter.com/claviska" rel="noopener noreferrer" target="_blank">Cory LaViska</a>. It’s available under the terms of the
            <a href="https://github.com/shoelace-style/shoelace/blob/next/LICENSE.md" rel="noopener noreferrer" target="_blank">MIT license</a>.
          </p>
          <h2 id="attribution">
            Attribution<a href="#attribution" aria-label="Direct link to &quot;Attribution&quot;"></a>
          </h2>
          <p>Special thanks to the following projects and individuals that help make Shoelace possible.</p>
          <ul>
            <li>
              Components are built with
              <a href="https://lit.dev/" rel="noopener noreferrer" target="_blank">Lit</a>
            </li>
            <li>
              Component metadata is generated by the
              <a href="https://github.com/open-wc/custom-elements-manifest" rel="noopener noreferrer" target="_blank">Custom Elements Manifest Analyzer</a>
            </li>
            <li>
              Documentation is powered by
              <a href="https://www.11ty.dev/" rel="noopener noreferrer" target="_blank">11ty</a>
            </li>
            <li>
              CDN services are provided by
              <a href="https://www.jsdelivr.com/" rel="noopener noreferrer" target="_blank">jsDelivr</a>
            </li>
            <li>
              Color primitives are inspired by
              <a href="https://tailwindcss.com/" rel="noopener noreferrer" target="_blank">Tailwind</a>
            </li>
            <li>
              Icons are courtesy of
              <a href="https://icons.getbootstrap.com/" rel="noopener noreferrer" target="_blank">Bootstrap Icons</a>
            </li>
            <li>
              The homepage illustration is courtesy of
              <a href="https://undraw.co/" rel="noopener noreferrer" target="_blank">unDraw</a>
            </li>
            <li>
              Positioning of dropdowns, tooltips, et al is handled by
              <a href="https://floating-ui.com/" rel="noopener noreferrer" target="_blank">Floating UI</a>
            </li>
            <li>
              Animations are courtesy of
              <a href="https://animate.style/" rel="noopener noreferrer" target="_blank">animate.css</a>
            </li>
            <li>
              Search is powered by
              <a href="https://lunrjs.com/" rel="noopener noreferrer" target="_blank">Lunr</a>
            </li>
            <li>
              The Shoelace logo was designed with a single shoelace by
              <a href="https://twitter.com/adamkolson" rel="noopener noreferrer" target="_blank">Adam K Olson</a>
            </li>
          </ul>
        </div>
      </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tidy First? (129 pts)]]></title>
            <link>https://henrikwarne.com/2024/01/10/tidy-first/</link>
            <guid>38942400</guid>
            <pubDate>Wed, 10 Jan 2024 19:59:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://henrikwarne.com/2024/01/10/tidy-first/">https://henrikwarne.com/2024/01/10/tidy-first/</a>, See on <a href="https://news.ycombinator.com/item?id=38942400">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
<p><em>“Software design is preparation for change; change of behavior”</em></p>



<p><a href="https://www.goodreads.com/book/show/171691901-tidy-first">Tidy First?</a> is a new book by Kent Beck. It is a short little book, only about 100 pages (and lots of white space on them), but it contains some deep insights about software development. The book has three parts, going from concrete to abstract. First there is a list of 15 <em>tidyings</em>, which are small refactorings. The next part, <em>Managing</em>, discusses how and when to perform the tidyings. The final part, <em>Theory</em>, presents a great framework for how to think about software development, using the concepts of <em>time value of money</em> and <em>optionality</em>.</p>



<figure><a href="https://henrikwarne1.files.wordpress.com/2024/01/tidyfirst.jpg"><img data-attachment-id="2355" data-permalink="https://henrikwarne.com/2024/01/10/tidy-first/tidyfirst/" data-orig-file="https://henrikwarne1.files.wordpress.com/2024/01/tidyfirst.jpg" data-orig-size="4624,2080" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.79&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;EB2103&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1704798804&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4.73&quot;,&quot;iso&quot;:&quot;160&quot;,&quot;shutter_speed&quot;:&quot;0.0083333333333333&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;,&quot;latitude&quot;:&quot;0&quot;,&quot;longitude&quot;:&quot;0&quot;}" data-image-title="tidyfirst" data-image-description="" data-image-caption="" data-medium-file="https://henrikwarne1.files.wordpress.com/2024/01/tidyfirst.jpg?w=300" data-large-file="https://henrikwarne1.files.wordpress.com/2024/01/tidyfirst.jpg?w=500" width="1024" height="460" src="https://henrikwarne1.files.wordpress.com/2024/01/tidyfirst.jpg?w=1024" alt="" srcset="https://henrikwarne1.files.wordpress.com/2024/01/tidyfirst.jpg?w=1024 1024w, https://henrikwarne1.files.wordpress.com/2024/01/tidyfirst.jpg?w=2045 2045w, https://henrikwarne1.files.wordpress.com/2024/01/tidyfirst.jpg?w=150 150w, https://henrikwarne1.files.wordpress.com/2024/01/tidyfirst.jpg?w=300 300w, https://henrikwarne1.files.wordpress.com/2024/01/tidyfirst.jpg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>The author, Kent Beck, is of course the creator of extreme programming (XP). As <a href="https://henrikwarne.com/2020/04/27/20-5-years-of-xp-and-agile/">I have written before</a>, his article in 1999, presenting XP, gave me the biggest productivity boost of my entire career as a software developer. Even though this is a very short book, it contains a lot of wisdom. It is worth reading slowly, to really digest the content.</p>



<p>A key idea in the book is that before you implement a behavior change (B) in the code, it may be beneficial to first perform one or more structural changes (S). These changes do not alter the program behavior, and are almost trivially simple. These changes are called tidyings. The idea is that by doing these tidyings, the behavior change will be easier to implement.</p>



<h3>Tidyings</h3>



<p>There are 15 tidyings, and they are presented in very short, almost tweet-like, chapters.</p>



<h4>Tidyings I Like the Most:</h4>



<p><strong>Guard clause</strong> – exit a function early if certain conditions are not met. This makes the rest of the function easier to write (no nested if-statements). <strong>Normalize symmetries</strong> – the same logic should be expressed in the same way everywhere it appears, since it makes reading the code easier.</p>



<p>I have noticed that many developers are reluctant to introduce <strong>explaining variables/constants</strong>. The idea here is to extract a subexpression into a variable named after the intention of the expression – typically done after reading the code and realizing what some part of it means. In the author’s words: <em>“In this tidying, you are taking your hard-won understanding and putting it back into the code”</em>.</p>



<p><strong>New interface, old implementation</strong>. If the design was like this, it would be easier to make the change. So create that new interface, and in it delegate to the old interface (for now). I really like this way of thinking, and I am using it often: first I assume I have a function that does XXX, and using that makes the solution easier. Then I create the function that does XXX. In a way it is working backwards – first assuming you have something available, and later implementing it. </p>



<p>The simplest tidying of them all is <strong>chunk statements</strong>, which just means using blank lines to indicate which parts of code are closely related, and which parts a separate. An underestimated practice, even though sometimes it can be hard to know how to chunk things. Too many blank lines can also mean you fit less code on your screen, so it needs to be balanced. <strong>Extract helper</strong> is another underused technique. Like explaining variable, it lets you name a part of the logic. <em>“Interfaces become tools for thinking about problems”</em>. </p>



<p>One tidying that I don’t think I have used before is <strong>one pile</strong>. Normally, tidyings will divide the code up in parts, where each part can be understood in isolation. However, sometimes the way the code is divided can hinder understanding. In this case, bringing it all together in one place can be a way to understand it better. Then it can be subdivided (in a new, easier to understand, way).</p>



<p><strong>Explaining comments/delete redundant comments</strong>. When needed, <a href="https://henrikwarne.com/2021/06/15/on-comments-in-code/">I am all for adding a comment</a> with extra information that is not obvious from the code. Also, if the comment says exactly what the code does, delete the comment (there is a good example in the book how this can happen when tidying).</p>



<h4>Other Tidyings:</h4>



<p>Delete <strong>dead code</strong> – this should be easy, but I often see either dead code, or commented out code, still left in code bases. <strong>Reading order</strong> – put the code in the file in the order that makes the most sense when reading it. This advice is less important in the age of IDEs, where navigating in and out of functions is very easy. Still, keeping functions in a good order doesn’t hurt. It is also similar to <strong>cohesion order</strong> – keeping elements that change together close to each other. There is a similar theme for <strong>move declaration and initialization together</strong> – keep related things together.</p>



<p>If the arguments to a function are in the form of a map/dict, then use <strong>explicit parameters</strong> to make clear what the inputs are.</p>



<p>For many of these, the best way may be to try them and see if the resulting code is better than before. If not, undo the change. Many times I have been too reluctant to make a change to see how it looks (somehow it feels like wasted effort). But I have come to realize that actually seeing the changed code (not just contemplating it) is the best way of evaluating the change.</p>



<h3>Managing</h3>



<p>Each individual tidying is very simple. They only change the structure of the code, never the behavior. Even chaining several tidyings together will result in a change that is easy to understand, and easy to undo if necessary. Sometimes the behavior change will be easier if we tidy first, then implement the change. In other cases, it is better to make the structural changes later, or not at all. This is the reason there is a question mark in the book title. Regardless, structural and behavioral changes should be kept in separate PRs (or at least in separate commits).</p>



<p>In many work places, there are high fixed costs (in time and effort) associated with PR reviews. The ideal solution for this, according to the author, is to not require PR reviews for only tidyings. If this is not feasible, then at least keep the changes in separate commits. </p>



<p>A problem I often encounter is that once you start making behavior changes, you see structural changes that should be done. This results in a mix of B and S changes. Separating them out can be hard. There is a good discussion on how to handle this in the chapter <em>Getting Untangled</em>. Either you ship it as it is (tangled), or you untangle the different changes (I have been doing this using git’s interactive rebase), or you discard all the changes and re-implement the changes. The last option sounds a bit crazy, but the author thinks that this may lead to even better code in the end.</p>



<h3>Theory</h3>



<h4>Beneficially Relating Elements</h4>



<p>Software design is <em>beneficially relating elements</em>. On one extreme there is a single gigantic soup of tiny subelements, for example assembly code with a global namespace. Even though such a program can work and produce the correct output, it would be virtually impossible to modify. The key then is to structure the program to make it understandable and changeable. This is done by creating and deleting elements, and creating and deleting relationships between the elements in a way that aids the overall understandability (this is the <em>beneficially</em> part).</p>



<h4>Time Value of Money, Optionality</h4>



<p>How do we balance keeping the program well-structured with the need to add behavior? Now we get to perhaps my favorite part of the book – relating software development to the concepts of <em>time value of money</em> and <em>optionality</em>. These are in tension with each other, and explain the question mark in the title.</p>



<p>The time value of money simply means that a dollar today is worth more than a dollar tomorrow. Therefore, getting features out quickly, so you can start earning money earlier, is imperative. So don’t tidy first.</p>



<p>However, software creates value in two ways: what it does today, but also in what it <em>could </em>do tomorrow. As noted in the book: <em>“The mere presence of a system behaving a certain way changes the desire for how the system should behave”</em>. This explains why software is never done – using a system makes you continually see new usages for it. Just like options in finance have value even before they are exercised, so do options in software. The options in this case is the structuring of the code that enables quick changes. Tidyings improve the structure, thus creating more, and more valuable, options. Therefore you should tidy first.</p>



<p>Because of this tension between the cases, you have to find a balance for when, and how much, to tidy.</p>



<h4>Coupling</h4>



<p>The key reason a program is expensive to change is that changing one element requires changing other elements (because the elements are coupled with respect to that change). Changing the other elements can in turn necessitate more changes, i.e. cascading changes. Therefore, reducing coupling will reduce the cost of change.</p>



<p><strong>Constantine’s Equivalence</strong> states that the cost of software is roughly equal to the cost of changing it. This cost of change is dominated by the cost of the big, cascading changes. Therefore, the cost of software is approximately equal to the coupling.</p>



<h3>To Keep in Mind</h3>



<p>Here are the key lessons from the book that I want to keep in mind when developing software:</p>



<ul>
<li>What structural change(s) (S) will make the next behavioral change (B) easier to implement?</li>



<li>Keep S and B in separate commits (or even separate PRs).</li>



<li>Create future behavior options by keeping a structure that supports change.</li>



<li>Constantine’s Equivalence: cost(software) ~= coupling</li>
</ul>



<h3>Conclusion</h3>



<p>There is a lot to like about this book. It has many concrete code tidyings you can put to use right away. It also has interesting discussions on how and when to perform them, as well as models to help you think about the tradeoffs present. Throughout the text there are numerous indications that the author has long practical experience, and has thought long and hard about software development.</p>



<p>This book is focused on the individual developer, and is the first in a series of three books. The next book will be about teams of software developers, and the third book will be about the cooperation between developers and non-developers. I really enjoyed Tidy First?, and I am looking forward to reading the next books in the series.</p>
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Free MIT Course: Performance Engineering of Software Systems (213 pts)]]></title>
            <link>https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/download/</link>
            <guid>38942206</guid>
            <pubDate>Wed, 10 Jan 2024 19:39:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/download/">https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/download/</a>, See on <a href="https://news.ycombinator.com/item?id=38942206">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <h2>Download</h2>
  <div>
      <div>
        <p><a href="https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/6.172-fall-2018.zip">
           Download course
        </a>
      </p></div>
      <p>
        This package contains the same content as the online version of the course, <em>except for the audio/video materials</em>. These can be downloaded below.
        For help downloading and using course materials, read our <a target="_blank" href="https://mitocw.zendesk.com/hc/en-us/articles/4414681093659-I-have-downloaded-an-MIT-OpenCourseWare-course-but-I-can-t-access-the-materials-How-do-I-get-started-">FAQs</a>.
      </p>
    </div>
  <p><strong>Note: </strong>
    The downloaded course may not work on mobile devices.
    We recommend using a computer with the downloaded course package.
  </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ChatGPT for Teams (265 pts)]]></title>
            <link>https://openai.com/chatgpt/team</link>
            <guid>38941942</guid>
            <pubDate>Wed, 10 Jan 2024 19:08:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/chatgpt/team">https://openai.com/chatgpt/team</a>, See on <a href="https://news.ycombinator.com/item?id=38941942">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><!--[--><!--[--><!--[--><header></header><header></header><!--]--><div><div><p><img src="https://images.openai.com/blob/c9878554-6f89-4dc8-a9b5-c00456e4e828/chatgpt-for-teams-hero.jpg?trim=362,0,333,0&amp;width=10&amp;height=10&amp;quality=50" width="2731" height="3401" alt="A team of people working on their laptops in a plant-filled room" loading="eager" data-nuxt-img="" sizes="(max-width: 744px) 100vw, (max-width: 1280px) 100vw, (max-width: 1440px) 100vw, 100vw" srcset="https://images.openai.com/blob/c9878554-6f89-4dc8-a9b5-c00456e4e828/chatgpt-for-teams-hero.jpg?trim=362,0,333,0&amp;width=400 400w, https://images.openai.com/blob/c9878554-6f89-4dc8-a9b5-c00456e4e828/chatgpt-for-teams-hero.jpg?trim=362,0,333,0&amp;width=800 800w, https://images.openai.com/blob/c9878554-6f89-4dc8-a9b5-c00456e4e828/chatgpt-for-teams-hero.jpg?trim=362,0,333,0&amp;width=1000 1000w, https://images.openai.com/blob/c9878554-6f89-4dc8-a9b5-c00456e4e828/chatgpt-for-teams-hero.jpg?trim=362,0,333,0&amp;width=1400 1400w, https://images.openai.com/blob/c9878554-6f89-4dc8-a9b5-c00456e4e828/chatgpt-for-teams-hero.jpg?trim=362,0,333,0&amp;width=2000 2000w, https://images.openai.com/blob/c9878554-6f89-4dc8-a9b5-c00456e4e828/chatgpt-for-teams-hero.jpg?trim=362,0,333,0&amp;width=2600 2600w, https://images.openai.com/blob/c9878554-6f89-4dc8-a9b5-c00456e4e828/chatgpt-for-teams-hero.jpg?trim=362,0,333,0&amp;width=3200 3200w" aria-hidden="false"></p><!----></div><div><div><h2>ChatGPT<br>for teams</h2><div><p>A customized, always-improving superassistant for every member of your&nbsp;team.<br></p></div><!--[--><h2>Quick links</h2><ul><!--[--><li><a href="https://chat.openai.com/#pricing" rel="noopener" target="_blank" aria-label="Start now"><span><!--[--><!----><span>Start now</span><!--]--></span></a></li><!--]--></ul><!--]--></div><div><p><img src="https://cdn.openai.com/chatgpt-team/r4/team-hero.svg"></p></div></div></div><div id="content"><!--[--><!--[--><div><p>Powering the most innovative teams at companies of all sizes<br></p></div><!--]--><!--[--><div><ul><!--[--><li><div><h3 id="post130title">Advanced models &amp; analysis</h3><p><span>Generate better code, craft emails, analyze data, and anything else your team needs with our most powerful models.</span></p><!----><!----><!----><!----><!----></div><!----></li><li><div><h3 id="post131title">Customized for your team</h3><p><span>Collaborate by creating and sharing GPTs—custom versions of ChatGPT for specific use cases, departments, or proprietary datasets.</span></p><!----><!----><!----><!----><!----></div><!----></li><li><div><h3 id="post132title">Secured for your workplace</h3><p><span>Get a dedicated workspace for your team with admin controls, team management, and stringent security. We never train on your data or conversations.</span></p><!----><!----><!----><!----><!----></div><!----></li><!--]--></ul></div><!--]--><!--[--><div id="supercharge-your-team-s-work"><p><h2>Supercharge your team’s work</h2></p><!----></div><!--]--><!--[--><div id="tailor-chatgpt-for-any-type-of-work"><!----><p><h2>Tailor ChatGPT for any type of work</h2><!----></p><div><p>Build custom versions of ChatGPT for almost any area of expertise—with specific instructions, knowledge and capabilities—and publish for others to use.<br></p><!----><h2 id="block-undefined-links-heading" level="2">Tailor ChatGPT for any type of work links</h2><!----></div></div><!--]--><!--[--><!--]--><!--[--><div id="use-cases-for-every-team"><p><h2>Use cases for every team</h2></p><!----></div><!--]--><!--[--><div id="teams-do-more-faster-with-openai"><div><p><h2>Teams do more, faster with OpenAI</h2></p></div><div><!--[--><div><p><span>12.2%</span></p><p>more tasks completed</p></div><div><p><span>25.1%</span></p><p>faster task completion</p></div><!--]--></div><div><p>Source: <a href="https://www.hbs.edu/faculty/Pages/item.aspx?num=64700" rel="noopener noreferrer" target="_blank">Harvard Business School</a>. Employees at Boston Consulting Group given access to GPT-4 compared to peers without access.<br></p></div></div><!--]--><!--[--><div><!--[--><div><div><p><img src="https://images.openai.com/blob/3969e4ef-0a9b-48cd-a5cf-31c399598b66/testimonial-sourcegraph.svg?width=10&amp;height=10&amp;quality=50" width="36" height="36" alt="Sourcegraph logo" loading="lazy" data-nuxt-img="" sizes="(max-width: 744px) 100vw, (max-width: 1280px) 100vw, (max-width: 1440px) 100vw, 100vw" aria-hidden="false"></p><!----></div><p>“Sourcegraph uses ChatGPT in almost every part of our business from financial modeling for pricing and packaging to internal and external communications to board prep to recruiting and note taking, it’s accelerated everything we do allowing us to execute at a high level.”</p><p><span>Connor O’Brien</span><span>VP of GTM Strategy &amp; Operations, Sourcegraph</span></p></div><div><div><p><img src="https://images.openai.com/blob/86cceb2f-d913-410f-879b-8b1e196575e5/testimonial-vouch.svg?width=10&amp;height=10&amp;quality=50" width="36" height="36" alt="Vouch" loading="lazy" data-nuxt-img="" sizes="(max-width: 744px) 100vw, (max-width: 1280px) 100vw, (max-width: 1440px) 100vw, 100vw" aria-hidden="false"></p><!----></div><p>“As a company at the forefront of innovation in insurance, Vouch’s teams require agility and precision, and ChatGPT Team has become an indispensable tool for many on our team. With ChatGPT, we're making decisions faster, and with a level of detail that was previously impossible.”</p><p><span>Sam Hodges</span><span>CEO and co-founder, Vouch</span></p></div><!--]--></div><!--]--><!--[--><div><!--[--><div><h2>Advanced models &amp; tools</h2><ul><!--[--><li><svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" style="width:1em;height:1em;" data-new="" data-v-e1bdab2c=""><polygon fill="currentColor" points="6 12.98 1.51 8.49 2.49 7.51 6 11.02 13.51 3.51 14.49 4.49 6 12.98" data-v-e1bdab2c=""></polygon></svg><p>GPT-4, with 32k context<br></p></li><li><svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" style="width:1em;height:1em;" data-new="" data-v-e1bdab2c=""><polygon fill="currentColor" points="6 12.98 1.51 8.49 2.49 7.51 6 11.02 13.51 3.51 14.49 4.49 6 12.98" data-v-e1bdab2c=""></polygon></svg><p>Latest DALL·E 3 model for image generation<br></p></li><li><svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" style="width:1em;height:1em;" data-new="" data-v-e1bdab2c=""><polygon fill="currentColor" points="6 12.98 1.51 8.49 2.49 7.51 6 11.02 13.51 3.51 14.49 4.49 6 12.98" data-v-e1bdab2c=""></polygon></svg><p>Advanced Data Analysis<br></p></li><li><svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" style="width:1em;height:1em;" data-new="" data-v-e1bdab2c=""><polygon fill="currentColor" points="6 12.98 1.51 8.49 2.49 7.51 6 11.02 13.51 3.51 14.49 4.49 6 12.98" data-v-e1bdab2c=""></polygon></svg><p>Browsing, for up-to-date information from the web<br></p></li><li><svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" style="width:1em;height:1em;" data-new="" data-v-e1bdab2c=""><polygon fill="currentColor" points="6 12.98 1.51 8.49 2.49 7.51 6 11.02 13.51 3.51 14.49 4.49 6 12.98" data-v-e1bdab2c=""></polygon></svg><p>Image and voice input &amp; output<br></p></li><li><svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" style="width:1em;height:1em;" data-new="" data-v-e1bdab2c=""><polygon fill="currentColor" points="6 12.98 1.51 8.49 2.49 7.51 6 11.02 13.51 3.51 14.49 4.49 6 12.98" data-v-e1bdab2c=""></polygon></svg><p>Create customized GPTs<br></p></li><li><svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" style="width:1em;height:1em;" data-new="" data-v-e1bdab2c=""><polygon fill="currentColor" points="6 12.98 1.51 8.49 2.49 7.51 6 11.02 13.51 3.51 14.49 4.49 6 12.98" data-v-e1bdab2c=""></polygon></svg><p>Share GPTs and chats with your workspace<br></p></li><!--]--></ul></div><div><h2>Security &amp; privacy</h2><ul><!--[--><li><svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" style="width:1em;height:1em;" data-new="" data-v-e1bdab2c=""><polygon fill="currentColor" points="6 12.98 1.51 8.49 2.49 7.51 6 11.02 13.51 3.51 14.49 4.49 6 12.98" data-v-e1bdab2c=""></polygon></svg><p>Admin console<br></p></li><li><svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" style="width:1em;height:1em;" data-new="" data-v-e1bdab2c=""><polygon fill="currentColor" points="6 12.98 1.51 8.49 2.49 7.51 6 11.02 13.51 3.51 14.49 4.49 6 12.98" data-v-e1bdab2c=""></polygon></svg><p>Dedicated workspace<br></p></li><li><svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" style="width:1em;height:1em;" data-new="" data-v-e1bdab2c=""><polygon fill="currentColor" points="6 12.98 1.51 8.49 2.49 7.51 6 11.02 13.51 3.51 14.49 4.49 6 12.98" data-v-e1bdab2c=""></polygon></svg><p>Bulk member management<br></p></li><li><svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" style="width:1em;height:1em;" data-new="" data-v-e1bdab2c=""><polygon fill="currentColor" points="6 12.98 1.51 8.49 2.49 7.51 6 11.02 13.51 3.51 14.49 4.49 6 12.98" data-v-e1bdab2c=""></polygon></svg><p>Admin roles<br></p></li><li><svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" style="width:1em;height:1em;" data-new="" data-v-e1bdab2c=""><polygon fill="currentColor" points="6 12.98 1.51 8.49 2.49 7.51 6 11.02 13.51 3.51 14.49 4.49 6 12.98" data-v-e1bdab2c=""></polygon></svg><p>We never train on your business data or conversations<br></p></li><!--]--></ul></div><!--]--></div><!--]--><!--[--><div layout="full-bleed" id="LandingChatGPTTeamGetStarted-9"><h2>Join thousands of teams evolving how they work with ChatGPT.</h2><p><a href="https://chat.openai.com/#pricing" rel="noopener" target="_blank"><span><!--[--><!----><span>Start now</span><!--]--></span></a></p></div><!--]--><!--[--><div><div><p>80% of Fortune 500 companies have employees already using ChatGPT for work.*<br></p></div><div><p>*Percentage of Fortune 500 companies with registered ChatGPT consumer accounts, as determined by analyzing accounts associated with corporate email domains.<br></p></div></div><!--]--><!--]--></div><!--]--><!--]--><!----></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The biggest losers: Metabolic damage or constrained energy? (130 pts)]]></title>
            <link>https://physiqonomics.com/biggest-losers/</link>
            <guid>38941847</guid>
            <pubDate>Wed, 10 Jan 2024 18:58:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://physiqonomics.com/biggest-losers/">https://physiqonomics.com/biggest-losers/</a>, See on <a href="https://news.ycombinator.com/item?id=38941847">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
		
<p>As you lose weight, several adaptations take place in the body to reduce your daily energy expenditure.&nbsp;</p>



<p>For example, a smaller body burns fewer calories than a larger body. Thus, it takes less energy to maintain your weight at the end of a diet than when you began the diet. A smaller body also means you burn fewer calories when exercising and moving around. If you’re in a caloric deficit, you’re eating fewer calories by default, which means less energy is used to digest and store the foods you eat.</p>



<p>So you might start the diet expending 2500 kcals/day, but by the time you end the diet, due to all the mechanisms mentioned above, your daily expenditure might have dropped to 2100 kcals/day.</p>



<figure><picture><source srcset="https://physiqonomics.com/wp-content/uploads/2023/08/before-after-weight-loss-metabolic-adaptation-1024x437.webp 1024w,https://physiqonomics.com/wp-content/uploads/2023/08/before-after-weight-loss-metabolic-adaptation-300x128.webp 300w,https://physiqonomics.com/wp-content/uploads/2023/08/before-after-weight-loss-metabolic-adaptation-768x327.webp 768w,https://physiqonomics.com/wp-content/uploads/2023/08/before-after-weight-loss-metabolic-adaptation-1536x655.webp 1536w,https://physiqonomics.com/wp-content/uploads/2023/08/before-after-weight-loss-metabolic-adaptation-2048x873.webp 2048w" sizes="(max-width: 1024px) 100vw, 1024px" type="image/webp"><img src="https://physiqonomics.com/wp-content/uploads/2023/08/before-after-weight-loss-metabolic-adaptation-1024x437.png" height="437" width="1024" srcset="https://physiqonomics.com/wp-content/uploads/2023/08/before-after-weight-loss-metabolic-adaptation-1024x437.png 1024w, https://physiqonomics.com/wp-content/uploads/2023/08/before-after-weight-loss-metabolic-adaptation-300x128.png 300w, https://physiqonomics.com/wp-content/uploads/2023/08/before-after-weight-loss-metabolic-adaptation-768x327.png 768w, https://physiqonomics.com/wp-content/uploads/2023/08/before-after-weight-loss-metabolic-adaptation-1536x655.png 1536w, https://physiqonomics.com/wp-content/uploads/2023/08/before-after-weight-loss-metabolic-adaptation-2048x873.png 2048w" sizes="(max-width: 1024px) 100vw, 1024px" alt="Bar graphs showing the reduction in different components of the metabolism" fetchpriority="high" decoding="async"> </picture></figure>



<p>These adaptations are collectively called ‘metabolic adaptation’, and it’s totally normal and to be expected. After all, the body is just doing its job by trying to save you from what it thinks is impending starvation.</p>



<p>But once you end the diet and transition from a caloric deficit to caloric maintenance, metabolic adaptation is largely reversed. <span title=" Nunes CL et al. 2022 ">1</span></p>



<p>Except in one study.&nbsp;</p>



<p>Not only did the participants experience a staggering amount of metabolic adaptation, but it persisted several years after they’d finished the diet. Since then, this study has become the poster child for metabolic damage––the idea that dieting causes irreversible damage to the metabolism, which makes weight maintenance impossible and sets people up to regain weight.</p>



<p>Recently, Kevin Hall––the lead researcher on the original study––went back and reanalysed the paper and offered a tentative alternative as to why these individuals were experiencing such a massive reduction in their metabolism. Spoiler alert: It likely wasn’t ‘metabolic damage’.</p>



<p>Let’s talk about that.</p>



<h2 id="h-setting-the-scene">Setting the scene</h2>



<p>In 2012, a study investigated the effects of metabolic adaptation on contestants of “The Biggest Loser”––an absolute dumpster fire of reality TV where contestants with obesity engage in several hours of intense exercise and severe caloric restriction to lose as much weight as possible in the hopes of winning a cash prize.<span title=" Johannsen DL et al. 2012 ">2</span></p>



<p>Exercise–especially <a href="https://physiqonomics.com/cardio-or-weights/" target="_blank" rel="noreferrer noopener">resistance training</a>–can help preserve muscle mass during weight loss, which can mitigate reductions in metabolism and help prevent weight regain. The researchers wanted to determine whether or not this would hold true in the Biggest Loser contestants.</p>



<p>The results showed the contestants did indeed largely <a href="https://physiqonomics.com/preventing-muscle-loss/" target="_blank" rel="noreferrer noopener">preserve their muscle mass</a>, with most of the weight loss coming from body fat.</p>


<div>
<figure><picture><source srcset="https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-metabolic-adaptation-composition-of-weight-loss-1024x859.webp 1024w,https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-metabolic-adaptation-composition-of-weight-loss-300x252.webp 300w,https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-metabolic-adaptation-composition-of-weight-loss-768x644.webp 768w,https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-metabolic-adaptation-composition-of-weight-loss-1536x1289.webp 1536w,https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-metabolic-adaptation-composition-of-weight-loss-2048x1718.webp 2048w" sizes="(max-width: 1024px) 100vw, 1024px" type="image/webp"><img src="https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-metabolic-adaptation-composition-of-weight-loss-1024x859.png" height="859" width="1024" srcset="https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-metabolic-adaptation-composition-of-weight-loss-1024x859.png 1024w, https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-metabolic-adaptation-composition-of-weight-loss-300x252.png 300w, https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-metabolic-adaptation-composition-of-weight-loss-768x644.png 768w, https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-metabolic-adaptation-composition-of-weight-loss-1536x1289.png 1536w, https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-metabolic-adaptation-composition-of-weight-loss-2048x1718.png 2048w" sizes="(max-width: 1024px) 100vw, 1024px" alt="Graph showing the composition of weight loss in the biggest losers. Red bars denote fat mass; blue bars denote muscle mass." decoding="async"> </picture><figcaption>Adapted from Johannsen DL et al.</figcaption></figure></div>


<p>Yet, despite the contestants managing to hold onto most of their muscles, their <a href="https://physiqonomics.com/slow-metabolism/" target="_blank" rel="noreferrer noopener">metabolism</a> took a massive hit. By the end of the 30-week competition, their metabolism had dropped by a whopping 789 (± 483) kcal/d. An amount that was ~500 kcal/d greater than it should have been based on the change in body composition.&nbsp;</p>


<div>
<figure><picture><source srcset="https://physiqonomics.com/wp-content/uploads/2023/08/metabolic-adaptation-measured-vs-predicted-1024x859.webp 1024w,https://physiqonomics.com/wp-content/uploads/2023/08/metabolic-adaptation-measured-vs-predicted-300x252.webp 300w,https://physiqonomics.com/wp-content/uploads/2023/08/metabolic-adaptation-measured-vs-predicted-768x644.webp 768w,https://physiqonomics.com/wp-content/uploads/2023/08/metabolic-adaptation-measured-vs-predicted-1536x1289.webp 1536w,https://physiqonomics.com/wp-content/uploads/2023/08/metabolic-adaptation-measured-vs-predicted-2048x1718.webp 2048w" sizes="(max-width: 1024px) 100vw, 1024px" type="image/webp"><img src="https://physiqonomics.com/wp-content/uploads/2023/08/metabolic-adaptation-measured-vs-predicted-1024x859.png" height="859" width="1024" srcset="https://physiqonomics.com/wp-content/uploads/2023/08/metabolic-adaptation-measured-vs-predicted-1024x859.png 1024w, https://physiqonomics.com/wp-content/uploads/2023/08/metabolic-adaptation-measured-vs-predicted-300x252.png 300w, https://physiqonomics.com/wp-content/uploads/2023/08/metabolic-adaptation-measured-vs-predicted-768x644.png 768w, https://physiqonomics.com/wp-content/uploads/2023/08/metabolic-adaptation-measured-vs-predicted-1536x1289.png 1536w, https://physiqonomics.com/wp-content/uploads/2023/08/metabolic-adaptation-measured-vs-predicted-2048x1718.png 2048w" sizes="(max-width: 1024px) 100vw, 1024px" alt="Bar graphs showing the difference in predicted and measured energy expenditure. Yellow box illustrating the metabolic adaptation that occurred" decoding="async"> </picture><figcaption>Metabolic adaptation is calculated as the difference between the predicted energy expenditure and the measured energy expenditure. In the case of the Biggest Losers, their predicted energy expenditure was 2393 kcals/day but their measured energy expenditure was 1890 kcals/day; thus, metabolic adaptation amounted to 504 kcals/day.</figcaption></figure></div>


<p>Just to contextualise this: Up until this study was published, the consensus was metabolic adaptation did occur but was fairly small (3-5%). The Biggest Loser contestants’ metabolism had adapted closer to 20%. <span title=" Astrup A et al. 1999 ">3</span></p>



<p>While this finding was surprising, it wasn’t <em>that</em> shocking; metabolic rate tends to decrease with weight loss, and the greater-than-expected drop in metabolism experienced by the Biggest Losers was likely just a byproduct of the magnitude of weight lost and the extreme training and nutrition protocol they were following. </p>



<p>Just to illustrate how extreme their weight loss protocol was, Kevin Hall, the senior researcher on the Biggest Loser study, did some mathematical modelling and calculated that, while the contestants were on the show, they averaged a 65% caloric deficit while doing more than three hours of intense exercise every day. <span title=" Hall KD. 2013 ">4</span></p>



<h2>The part where it gets weird</h2>



<p>Fast forward to 2016, and 14 of the original 16 Biggest Loser contestants were brought back into the spotlight as researchers wanted to investigate whether the participants were still experiencing metabolic adaptation. <span title=" Fothergill E et al. 2016 ">5</span></p>



<p>To reiterate the point I made at the start: While some metabolic adaptation does occur during weight loss, it’s largely reversed once you return to maintenance.&nbsp;</p>



<p>But this wasn’t the case for the Biggest Losers, who were still experiencing metabolic adaptation years after the show had ended to the tune of ~500 kcals/d. That is, their metabolism was ~500 kcals lower than it should have been based on the weight that was regained.</p>


<div>
<figure><picture><source srcset="https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-before-after-RMR-change-923x1024.webp 923w,https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-before-after-RMR-change-270x300.webp 270w,https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-before-after-RMR-change-768x852.webp 768w,https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-before-after-RMR-change-1384x1536.webp 1384w,https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-before-after-RMR-change-1846x2048.webp 1846w" sizes="(max-width: 662px) 100vw, 662px" type="image/webp"><img src="https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-before-after-RMR-change-923x1024.png" height="734" width="662" srcset="https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-before-after-RMR-change-923x1024.png 923w, https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-before-after-RMR-change-270x300.png 270w, https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-before-after-RMR-change-768x852.png 768w, https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-before-after-RMR-change-1384x1536.png 1384w, https://physiqonomics.com/wp-content/uploads/2023/08/biggest-losers-before-after-RMR-change-1846x2048.png 1846w" sizes="(max-width: 662px) 100vw, 662px" alt="Graph showing the difference in RMR at 30 weeks and 6 years of the biggest loser competition. Black rectangles represent the mean change in RMR." loading="lazy" decoding="async"> </picture><figcaption>Individual (●) and mean (black rectangles) changes in resting metabolic rate between week 30 of the competition and 6 years after the competition</figcaption></figure></div>


<p>This finding led to the notion the Biggest Loser participants had ‘damaged’ their metabolism, and all diets were doomed to fail (despite the fact the contestants maintained, on average, a clinically significant ~12% weight loss).</p>



<h2>Time for a plot twist: Metabolic adaptation <em>wasn’t</em> linked with weight regain</h2>



<p>That wasn’t a typo.&nbsp;</p>



<p>Contrary to the researchers’ hypotheses that participants with the highest degree of metabolic adaptation would be the ones who would regain the most weight, the participants with the highest degree of metabolic adaptation were also the ones who more successfully maintained their weight loss.&nbsp;</p>



<h3>What the hell’s going on here?</h3>



<p>A 2017 study looked at how changes in energy intake and physical activity were related to weight regain in The Biggest Loser competition. <span title=" Kerns JC et al. 2017 ">6</span> It found changes in energy intake were <em>not</em> significantly correlated with the weight regained in the six years after the Biggest Loser competition (Image A below). However, physical activity was “significantly inversely related to weight regained” (Image B below).</p>


<div>
<figure><picture><source srcset="https://physiqonomics.com/wp-content/uploads/2023/08/energy-intake-vs-physical-activity-weight-regain-637x1024.webp 637w,https://physiqonomics.com/wp-content/uploads/2023/08/energy-intake-vs-physical-activity-weight-regain-187x300.webp 187w,https://physiqonomics.com/wp-content/uploads/2023/08/energy-intake-vs-physical-activity-weight-regain-768x1234.webp 768w,https://physiqonomics.com/wp-content/uploads/2023/08/energy-intake-vs-physical-activity-weight-regain-956x1536.webp 956w,https://physiqonomics.com/wp-content/uploads/2023/08/energy-intake-vs-physical-activity-weight-regain-1274x2048.webp 1274w" sizes="(max-width: 612px) 100vw, 612px" type="image/webp"><img src="https://physiqonomics.com/wp-content/uploads/2023/08/energy-intake-vs-physical-activity-weight-regain-637x1024.png" height="984" width="612" srcset="https://physiqonomics.com/wp-content/uploads/2023/08/energy-intake-vs-physical-activity-weight-regain-637x1024.png 637w, https://physiqonomics.com/wp-content/uploads/2023/08/energy-intake-vs-physical-activity-weight-regain-187x300.png 187w, https://physiqonomics.com/wp-content/uploads/2023/08/energy-intake-vs-physical-activity-weight-regain-768x1234.png 768w, https://physiqonomics.com/wp-content/uploads/2023/08/energy-intake-vs-physical-activity-weight-regain-956x1536.png 956w, https://physiqonomics.com/wp-content/uploads/2023/08/energy-intake-vs-physical-activity-weight-regain-1274x2048.png 1274w" sizes="(max-width: 612px) 100vw, 612px" alt="" loading="lazy" decoding="async"> </picture><figcaption>(A) Energy intake changes were not significantly correlated with weight regained in the six years after the Biggest Loser competition. (B) Physical activity changes were significantly inversely related to weight regained. Adapted from Kerns et al.<br></figcaption></figure></div>


<p>The participants who maintained greater weight loss (~25%) increased their physical activity by 160% compared to an increase of only 34% in weight regainers.</p>


<div>
<figure><picture><source srcset="https://physiqonomics.com/wp-content/uploads/2023/08/13weightloss-1024x710.webp 1024w,https://physiqonomics.com/wp-content/uploads/2023/08/13weightloss-300x208.webp 300w,https://physiqonomics.com/wp-content/uploads/2023/08/13weightloss-768x532.webp 768w,https://physiqonomics.com/wp-content/uploads/2023/08/13weightloss-1536x1065.webp 1536w,https://physiqonomics.com/wp-content/uploads/2023/08/13weightloss-2048x1420.webp 2048w" sizes="(max-width: 1024px) 100vw, 1024px" type="image/webp"><img src="https://physiqonomics.com/wp-content/uploads/2023/08/13weightloss-1024x710.png" height="710" width="1024" srcset="https://physiqonomics.com/wp-content/uploads/2023/08/13weightloss-1024x710.png 1024w, https://physiqonomics.com/wp-content/uploads/2023/08/13weightloss-300x208.png 300w, https://physiqonomics.com/wp-content/uploads/2023/08/13weightloss-768x532.png 768w, https://physiqonomics.com/wp-content/uploads/2023/08/13weightloss-1536x1065.png 1536w, https://physiqonomics.com/wp-content/uploads/2023/08/13weightloss-2048x1420.png 2048w" sizes="(max-width: 1024px) 100vw, 1024px" alt="" loading="lazy" decoding="async"> </picture><figcaption>Adapted from Kerns et al.</figcaption></figure></div>


<p>If you’re wondering where I’m going with all of this, hang tight; we’re almost at the finish line.</p>



<h2>Metabolic damage or constrained energy?</h2>



<p>Around the same time the initial Biggest Loser study was published, a group of researchers led by anthropologist and evolutionary biologist Herman Pontzer published a study on a topic that, on the surface, didn’t seem related at all.&nbsp;</p>



<p>Pontzer had been studying the Hadza––a modern hunter-gatherer population in Tanzania––when he found something bizarre. Despite the Hadza’s high levels of physical activity, once adjusted for fat-free mass, their daily energy expenditure wasn’t too dissimilar from their more sedentary Western counterparts in the US and Europe. <span title=" Pontzer H et al. 2012 ">7</span></p>



<p>In 2016, Pontzer and colleagues published a study putting forward <a href="https://physiqonomics.com/constrained-energy-model/" target="_blank" rel="noreferrer noopener">the constrained energy model</a>: Energy expenditure does increase with more activity, but only to a point. Once physical activity gets really high, the body will adjust other components of the metabolism to keep your daily energy expenditure within a narrow range. <span title=" Pontzer H et al. 2016 ">8</span></p>


<div>
<figure><picture><source srcset="https://physiqonomics.com/wp-content/uploads/2023/08/additive-constrained-1024x443.webp 1024w,https://physiqonomics.com/wp-content/uploads/2023/08/additive-constrained-300x130.webp 300w,https://physiqonomics.com/wp-content/uploads/2023/08/additive-constrained-768x332.webp 768w,https://physiqonomics.com/wp-content/uploads/2023/08/additive-constrained-1536x665.webp 1536w,https://physiqonomics.com/wp-content/uploads/2023/08/additive-constrained-2048x886.webp 2048w" sizes="(max-width: 1024px) 100vw, 1024px" type="image/webp"><img src="https://physiqonomics.com/wp-content/uploads/2023/08/additive-constrained-1024x443.png" height="443" width="1024" srcset="https://physiqonomics.com/wp-content/uploads/2023/08/additive-constrained-1024x443.png 1024w, https://physiqonomics.com/wp-content/uploads/2023/08/additive-constrained-300x130.png 300w, https://physiqonomics.com/wp-content/uploads/2023/08/additive-constrained-768x332.png 768w, https://physiqonomics.com/wp-content/uploads/2023/08/additive-constrained-1536x665.png 1536w, https://physiqonomics.com/wp-content/uploads/2023/08/additive-constrained-2048x886.png 2048w" sizes="(max-width: 1024px) 100vw, 1024px" alt="" loading="lazy" decoding="async"> </picture><figcaption><span>Additive model (left): The more activity you do, the more calories you burn. The constrained model (right): As levels of physical activity increase, the body adjusts other aspects of energy expenditure to maintain TDEE within a certain range</span></figcaption></figure></div>


<p><strong>So, could a better explanation for the significant reduction in the Biggest Loser contestants’ metabolic rate be a result of the constrained energy model?&nbsp;</strong></p>



<p>Recently, Kevin Hall went back and reinterpreted the Biggest Loser study through the lens of the constrained energy model. He noted the contestants’ physical activity levels remained high six years later, with a median increase of ~80% compared to before the competition. Moreover, those participants who sustained the highest levels of physical activity were also the participants who exhibited the greatest degree of metabolic adaptation.&nbsp;<span title=" Hall KD. 2022 ">9</span></p>


<div>
<figure><picture><source srcset="https://physiqonomics.com/wp-content/uploads/2023/08/changeinenergyexpenditure-hall-kd-1024x714.webp 1024w,https://physiqonomics.com/wp-content/uploads/2023/08/changeinenergyexpenditure-hall-kd-300x209.webp 300w,https://physiqonomics.com/wp-content/uploads/2023/08/changeinenergyexpenditure-hall-kd-768x535.webp 768w,https://physiqonomics.com/wp-content/uploads/2023/08/changeinenergyexpenditure-hall-kd-1536x1070.webp 1536w,https://physiqonomics.com/wp-content/uploads/2023/08/changeinenergyexpenditure-hall-kd-2048x1427.webp 2048w" sizes="(max-width: 1024px) 100vw, 1024px" type="image/webp"><img src="https://physiqonomics.com/wp-content/uploads/2023/08/changeinenergyexpenditure-hall-kd-1024x714.png" height="714" width="1024" srcset="https://physiqonomics.com/wp-content/uploads/2023/08/changeinenergyexpenditure-hall-kd-1024x714.png 1024w, https://physiqonomics.com/wp-content/uploads/2023/08/changeinenergyexpenditure-hall-kd-300x209.png 300w, https://physiqonomics.com/wp-content/uploads/2023/08/changeinenergyexpenditure-hall-kd-768x535.png 768w, https://physiqonomics.com/wp-content/uploads/2023/08/changeinenergyexpenditure-hall-kd-1536x1070.png 1536w, https://physiqonomics.com/wp-content/uploads/2023/08/changeinenergyexpenditure-hall-kd-2048x1427.png 2048w" sizes="(max-width: 1024px) 100vw, 1024px" alt="" loading="lazy" decoding="async"> </picture><figcaption>Contestants who increased their physical activity the most experienced the greatest metabolic adaptation.</figcaption></figure></div>


<p>In other words, because these contestants maintained high levels of physical activity, their bodies had decreased other components of their metabolism to keep their daily energy expenditure ‘constrained’ within a narrow range per the constrained energy model.</p>



<p>Now, it’s important to mention Kevin Hall’s reinterpretation wasn’t a randomized controlled trial but speculation based on newer research published on the constrained energy model. That said, it does make more sense than the whole, ‘the participants damaged their metabolism’. Especially when considering the Biggest Loser study is the only study to have reported such an extreme level of metabolic adaptation (and one that’s not been replicated since). The rest of the research––where the weight loss protocols haven’t been this insane––suggests some metabolic adaptation does occur during weight loss, but this largely dissipates once you return to maintenance. <span title=" Nunes CL et al. 2022 ">1</span></p>



<h2>What to make of all of this?</h2>



<p>In this reinterpretation of The Biggest Loser study, it seems the greater-than-expected reduction in the contestants’ resting metabolic rate was likely a result of their bodies constraining energy to account for their high levels of physical activity and not because their metabolism was ‘damaged’.</p>



<p>More importantly, metabolic adaptation doesn’t make weight loss impossible, nor does it mean you’re doomed to regain weight. The Biggest Losers saw their metabolic rate tank by over 700 kcals, yet they all lost a serious amount of weight. Additionally, the follow-up study found those who experienced the most metabolic adaptation were also the ones who maintained the most weight loss.&nbsp;</p>



<h3>But we need to keep this study in context</h3>



<p>This was one study done in a very specific population that was engaging in methods of weight loss that, let’s be honest, aren’t practical for any normal person to adhere to. Do you think you can stick to a 65% calorie deficit and engage in several hours of exercise every day for over half a year?</p>



<p>Yeah, didn’t think so.</p>



<p>For everyone else, metabolic adaptation is par for the course when dieting and will largely dissipate as you return to maintenance. And even if some metabolic adaptation does hang around, the real question is: Does it occur to a degree that would meaningfully impact your progress? The research so far would lean towards no.&nbsp;</p>



<p>Metabolic adaptation might result in less weight loss than anticipated <span title=" Martins C et al. 2021 ">10</span> or slightly stretch the timeline to your goal. <span title=" Martins C et al. 2022 ">11</span> But neither of these things will prevent you from losing weight.&nbsp;</p>



<p>This reality underscores two important truths. First, you’re not going to lose weight linearly, and sooner or later, you’ll need to make adjustments to your diet to overcome the metabolism adapting to weight loss. Second, accepting the fact you might well need to eat fewer calories than you <em>think</em> you should be in comparison to someone of the same height, weight, and sex. I’m not going to sit here and act like this doesn’t suck or that it won’t make weight loss a bit more challenging for some, but unfortunately, it is what it is.</p>



<p>Taking all of this together, it’s no surprise then that studies have consistently demonstrated metabolic adaptation isn’t a major roadblock to maintaining weight loss, nor is it the weight gain boogeyman it’s made out to be. <span title=" Martins C et al. 2020 ">12</span></p>



<h2>An important, yet often missed lesson from the Biggest Loser study</h2>



<p>With all of the buzz that ‘metabolic damage’ generates, the more pertinent lesson from the Biggest Loser study tends to get glossed over (aside from Hey, maybe don’t do dumb shit to lose weight): The importance of physical activity for long-term weight maintenance. And it can’t be understated. To quote from a recent systematic review: “Increased physical activity was the most consistent positive correlate of weight loss maintenance.” <span title=" Paixão C et al. 2020 ">13</span></p>



<p>There are several reasons why exercise can help with weight maintenance, but an idea gaining traction over the past few years is ‘energy flux’.&nbsp;</p>



<p>At the end of a diet, you can approach weight maintenance in two ways:</p>



<ol>
<li>You can eat less (compared to what you were eating before the weight loss) paired with low levels of physical activity, known as a ‘low-flux state.’</li>



<li>You can eat slightly more paired with higher levels of physical activity, i.e. a ‘high-flux state.’</li>
</ol>



<p>While both states lead to energy balance, a high-flux state can help mitigate weight regain due to better regulation of appetite and closing the ‘energy gap’––the difference in energy requirements before and after weight loss. <span title=" Melby CL et al. 2019 ">14</span></p>



<figure><picture><source srcset="https://physiqonomics.com/wp-content/uploads/2023/08/energy-flux-1-1024x831.webp 1024w,https://physiqonomics.com/wp-content/uploads/2023/08/energy-flux-1-300x244.webp 300w,https://physiqonomics.com/wp-content/uploads/2023/08/energy-flux-1-768x624.webp 768w,https://physiqonomics.com/wp-content/uploads/2023/08/energy-flux-1-1536x1247.webp 1536w" sizes="(max-width: 1024px) 100vw, 1024px" type="image/webp"><img src="https://physiqonomics.com/wp-content/uploads/2023/08/energy-flux-1-1024x831.png" height="831" width="1024" srcset="https://physiqonomics.com/wp-content/uploads/2023/08/energy-flux-1-1024x831.png 1024w, https://physiqonomics.com/wp-content/uploads/2023/08/energy-flux-1-300x244.png 300w, https://physiqonomics.com/wp-content/uploads/2023/08/energy-flux-1-768x624.png 768w, https://physiqonomics.com/wp-content/uploads/2023/08/energy-flux-1-1536x1247.png 1536w" sizes="(max-width: 1024px) 100vw, 1024px" alt="" loading="lazy" decoding="async"> </picture></figure>



<p>For example, a 2018 review noted the relationship between physical activity and energy intake being J-shaped: Appetite is dysregulated at low levels of physical activity, creating a mismatch between the energy expended and the amount of food consumed, leading to weight gain. On the other hand, as physical activity increases, so too does satiety signalling, which reduces the amount of food consumed.&nbsp;<span title=" Beaulieu K et al. 2018 ">15</span></p>


<div>
<figure><picture><source srcset="https://physiqonomics.com/wp-content/uploads/2023/08/j-shaped-1024x831.webp 1024w,https://physiqonomics.com/wp-content/uploads/2023/08/j-shaped-300x243.webp 300w,https://physiqonomics.com/wp-content/uploads/2023/08/j-shaped-768x623.webp 768w,https://physiqonomics.com/wp-content/uploads/2023/08/j-shaped-1536x1247.webp 1536w,https://physiqonomics.com/wp-content/uploads/2023/08/j-shaped-2048x1662.webp 2048w" sizes="(max-width: 1024px) 100vw, 1024px" type="image/webp"><img src="https://physiqonomics.com/wp-content/uploads/2023/08/j-shaped-1024x831.png" height="831" width="1024" srcset="https://physiqonomics.com/wp-content/uploads/2023/08/j-shaped-1024x831.png 1024w, https://physiqonomics.com/wp-content/uploads/2023/08/j-shaped-300x243.png 300w, https://physiqonomics.com/wp-content/uploads/2023/08/j-shaped-768x623.png 768w, https://physiqonomics.com/wp-content/uploads/2023/08/j-shaped-1536x1247.png 1536w, https://physiqonomics.com/wp-content/uploads/2023/08/j-shaped-2048x1662.png 2048w" sizes="(max-width: 1024px) 100vw, 1024px" alt="" loading="lazy" decoding="async"> </picture><figcaption>Adapted from Beaulieu et al.</figcaption></figure></div>


<p>This is all fine and dandy. But there’s something that’s probably been gnawing at you as you’ve been reading the last few paragraphs: Won’t the constrained energy model render any increases in physical activity moot?</p>



<p>No, because that isn’t what the constrained energy model suggests. As I mentioned above, the constrained energy model states that physical activity <em>does</em> increase energy expenditure, but after a point, the increase isn’t linear.</p>



<p>For example, Careau and colleagues found energy compensation averaged 28%, with a lot of variance around the mean; leaner individuals experienced less compensation (29.7%), while individuals with more body fat experienced more compensation (45.7%). <span title=" Careau V et al. 2021 ">16</span> So, while some people will experience more compensation than others, any increase in expenditure isn’t wholly negated.</p>







<p>A few years back, I was having a conversation with a friend about metabolic adaptation, and he quipped, “People worry so much about metabolic adaptation, but nobody talks about behavioural adaptation.”</p>



<p>He’s right.</p>



<p>The behaviours and habits you adopt are going to have a far greater impact on your ability to both make and maintain your progress than some transient adaptations that occur during weight loss.</p>



<p>Here’s an example of what I mean. In a recent study, metabolic adaptation was present after participants had lost 5% of their baseline weight (-85 kcals/d), and it persisted after a subsequent 8-month weight maintenance period (-72 kcals/d). <span title=" Nunes CL et al. 2022 ">17</span></p>



<p>But here’s the interesting bit: The researchers found participants slightly decreased their sedentary time and increased the amount of physical activity during the 4-month weight-loss period. But by the end of the study (12-month mark), the participants increased their sedentary time, and their physical activity levels dropped back to baseline. As the researchers write:</p>



<blockquote>
<p>The lack of a successful WL [weight loss] and its maintenance may be mostly due to behavioral issues, such as increasing food intake and/or decreasing physical activity.</p>
</blockquote>



<p>And isn’t this what we see in the real world?</p>



<p>Someone starts a fad diet and does <em>all the things</em> for a few weeks or months, and as soon as the diet ends, they go right back to their old behaviours and gain back the weight.</p>



<p>Here’s the thing: People wrongly assume maintenance is this thing that happens after the diet ends. But maintenance begins on day <em>one</em> of your diet. The same habits and behaviours that will help you lose weight are the same habits and behaviours that will help you maintain your weight. Notably:</p>



<ul>
<li>Eat a majority whole food nutrient-rich diet with adequate protein.</li>
</ul>



<ul>
<li>Don’t unnecessarily restrict foods you enjoy, but learn how to make these foods a part of your eating plan.</li>
</ul>



<ul>
<li>Try to get in movement throughout the day and be as physically active as is feasible for you.</li>
</ul>



<ul>
<li>Focus on strength training to build and <a href="https://physiqonomics.com/preventing-muscle-loss/" target="_blank" rel="noreferrer noopener">maintain muscle</a>​.</li>
</ul>



<ul>
<li>Monitor your progress in some capacity.</li>
</ul>



<p>To slightly amend a popular quote: Genetics load the gun, but your habits and behaviours pull the trigger.</p>



<p>Instead of worrying about things largely out of your control (like metabolic adaptation), be more concerned about the things you can control (see above) because these are the things that will help you make and later maintain progress.</p>







<h2 id="h-thanks-for-reading-if-you-enjoyed-this-you-d-love-the-vitamin"><strong>Thanks for reading. If you enjoyed this, you’d love the Vitamin</strong></h2>


<div>
<figure><picture><source srcset="https://physiqonomics.com/wp-content/uploads/2022/06/the-vitamin-phone-template-872x1024.webp 872w,https://physiqonomics.com/wp-content/uploads/2022/06/the-vitamin-phone-template-256x300.webp 256w,https://physiqonomics.com/wp-content/uploads/2022/06/the-vitamin-phone-template-768x902.webp 768w,https://physiqonomics.com/wp-content/uploads/2022/06/the-vitamin-phone-template.webp 920w" sizes="(max-width: 334px) 100vw, 334px" type="image/webp"><img src="https://physiqonomics.com/wp-content/uploads/2022/06/the-vitamin-phone-template-872x1024.jpg" height="392" width="334" srcset="https://physiqonomics.com/wp-content/uploads/2022/06/the-vitamin-phone-template-872x1024.jpg 872w, https://physiqonomics.com/wp-content/uploads/2022/06/the-vitamin-phone-template-256x300.jpg 256w, https://physiqonomics.com/wp-content/uploads/2022/06/the-vitamin-phone-template-768x902.jpg 768w, https://physiqonomics.com/wp-content/uploads/2022/06/the-vitamin-phone-template.jpg 920w" sizes="(max-width: 334px) 100vw, 334px" alt="" loading="lazy" decoding="async"> </picture></figure></div>


<p>95% of my new content is only being sent to my email list. One email every Thursday, filled with actionable, evidence-based fitness advice to help you with your goals. If you enjoyed this, you’d love my emails. You can learn more and subscribe for free&nbsp;<a href="https://physiqonomics.com/weekly-emails/" target="_blank" rel="noreferrer noopener">here</a></p>




	</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A battery has replaced Hawaii's last coal plant (520 pts)]]></title>
            <link>https://www.canarymedia.com/articles/energy-storage/a-huge-battery-has-replaced-hawaiis-last-coal-plant</link>
            <guid>38941747</guid>
            <pubDate>Wed, 10 Jan 2024 18:46:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.canarymedia.com/articles/energy-storage/a-huge-battery-has-replaced-hawaiis-last-coal-plant">https://www.canarymedia.com/articles/energy-storage/a-huge-battery-has-replaced-hawaiis-last-coal-plant</a>, See on <a href="https://news.ycombinator.com/item?id=38941747">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p dir="ltr">Hawaiian Electric’s modeling suggests it can reduce curtailment of renewables by an estimated <span>69</span>% for the first five years thanks to Kapolei Energy Storage, allowing surplus clean electricity that would otherwise go to waste to get onto the&nbsp;grid.</p>
<p dir="ltr">The utility also requested <span></span>​<span>“</span>black-start capability.” If a&nbsp;disaster, like a&nbsp;cyclone or earthquake, knocks out the grid completely, Hawaiian Electric needs a&nbsp;power source to restart it. The Kapolei batteries are programmed to hold some energy in reserve for that purpose. Plus Power located the project near a&nbsp;substation connected to three other power plants so the battery <span></span>​<span>“</span>can be <span>AAA</span> to jump-start those other plants,” Keefe&nbsp;said.</p>
<p dir="ltr">The combination of all these abilities in one site — capacity, grid services, black start — leads Keefe to call Kapolei <span></span>​<span>“</span>the most advanced battery energy storage facility on the planet.”</p>
<h2><strong>Model for a&nbsp;reliable clean-energy grid</strong></h2>
<p dir="ltr">The new battery is just the latest dispatch from Hawaii’s long-held spot at the vanguard of the energy transition. This is the state that hit mass rooftop solar adoption first and crafted the <a href="https://www.canarymedia.com/articles/clean-energy/kauai-is-a-clean-energy-leader-its-secret-a-publicly-owned-grid">first utility-scale solar-battery plant</a> in Kauai (not coincidentally, Plus Power <span>CCO</span> Bob Rudd had a&nbsp;hand in that project during his tenure at&nbsp;Tesla).</p>
<p dir="ltr">But when renewables growth and fossil-plant retirements pass a&nbsp;certain threshold, as they have in Hawaii, simply adding more wind, solar or batteries isn’t sufficient. The clean technologies, which run on digitally controlled inverters, have to start maintaining the grid, not just feeding it.</p>
<p dir="ltr">Plenty of other batteries provide frequency services to other grids, and a&nbsp;few of them are larger than Kapolei. But this is the only large-scale battery that we’ve seen capable of combining the basic peak capacity, frequency response, synthetic inertia and grid-rebooting tasks. That’s because Kapolei plays a&nbsp;more central role in its grid than battery plants do elsewhere.</p>
<p dir="ltr">After years of construction, California’s grid battery fleet <a href="https://www.canarymedia.com/articles/batteries/chart-the-remarkable-rise-of-californias-grid-battery-capacity#">surpassed <span>5</span>,<span>000</span>&nbsp;megawatts installed</a> last year, but that only equates to <span>7</span>.<span>6</span>% of the mammoth nameplate capacity of the state’s grid. Kapolei alone constitutes about <span>17</span>% of Oahu’s peak capacity. Hawaiian Electric needed it to take on more responsibility than batteries elsewhere have ever had&nbsp;to.</p>
<p dir="ltr">Take <a href="https://www.nrel.gov/news/program/2020/inertia-and-the-power-grid-a-guide-without-the-spin.html">inertia</a>, which stabilizes grid frequency, as one example. Old plants provide this passively, through the spinning mass of their turbines; inertia didn’t need to be defined and compensated for separately in bygone decades because it was part of the package of running a&nbsp;power plant.</p>
<p dir="ltr">Now, across the country, the grid is moving to a&nbsp;model of maximizing cheap renewables when they are available and burning fuel when renewables aren’t. But the thermal plants need to be spinning to provide inertia — sometimes, on the mainland, renewables get curtailed to keep old coal plants running so they can deliver these grid services, Keefe said. This can be a&nbsp;bad deal for electricity customers, not to mention the climate.</p>
<p dir="ltr">Advanced batteries provide a&nbsp;synthetic version of this inertia through savvy programming of their inverters. This offers a&nbsp;more economic alternative while avoiding unnecessary carbon emissions. They also are faster and more precise — Keefe likened the Kapolei battery to a&nbsp;zippy electric sports car compared to the lumbering diesel bus of old thermal plants. That makes batteries a&nbsp;good technical fit for grids that are becoming increasingly volatile due to the fluctuations of renewable production.</p>
<p dir="ltr">Longer-term, U.S. climate goals require a&nbsp;phaseout of fossil fuels from the electric grid. Hydropower and nuclear plants help deliver valuable grid inertia without carbon emissions, but they aren’t on track to&nbsp;grow.</p>
<p dir="ltr">That’s why this project matters to the clean energy shift everywhere: It’s one of the first real-life examples of how to shift critical grid functions from fossil-fueled plants to clean energy plants. And eventually, the kind of grid services Kapolei has pioneered will have to scale nationwide.<br></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Israeli group claims it’s using back channels to censor “inflammatory” content (247 pts)]]></title>
            <link>https://theintercept.com/2024/01/10/israel-disinformation-social-media-iron-truth/</link>
            <guid>38941719</guid>
            <pubDate>Wed, 10 Jan 2024 18:43:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theintercept.com/2024/01/10/israel-disinformation-social-media-iron-truth/">https://theintercept.com/2024/01/10/israel-disinformation-social-media-iron-truth/</a>, See on <a href="https://news.ycombinator.com/item?id=38941719">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    
<p><span>A small group</span> of volunteers from Israel’s tech sector is working tirelessly to remove content it says doesn’t belong on platforms like Facebook and TikTok, tapping personal connections at those and other Big Tech companies to have posts deleted outside official channels, the project’s founder told The Intercept.</p>



<p>The project’s moniker, “Iron Truth,” echoes the Israeli military’s vaunted Iron Dome rocket interception system. The brainchild of Dani Kaganovitch, a Tel Aviv-based software engineer at Google, Iron Truth claims its tech industry back channels have led to the removal of roughly 1,000 posts tagged by its members as false, antisemitic, or “pro-terrorist” across platforms such as X, YouTube, and TikTok.</p>



<p>In an interview, Kaganovitch said he launched the project after the October 7 Hamas attack, when he saw a Facebook video that cast doubt on alleged Hamas atrocities. “It had some elements of disinformation,” he told The Intercept. “The person who made the video said there were no beheaded babies, no women were raped, 200 bodies is a fake. As I saw this video, I was very pissed off. I copied the URL of the video and sent it to a team in [Facebook parent company] Meta, some Israelis that work for Meta, and I told them that this video needs to be removed and actually they removed it after a few days.”</p>



<p>Billed as both a fight against falsehood and a “fight for public opinion,” according to a post announcing the project on Kaganovitch’s <a href="https://web.archive.org/web/20231209002510/https://www.linkedin.com/posts/dannykg_israel-hamas-hamasisis-activity-7119952867225985024-nzuV/">LinkedIn profile</a>, Iron Truth vividly illustrates the perils and pitfalls of terms like “misinformation” and “disinformation” in wartime, as well as the mission creep they enable. The project’s public face is <a href="https://t.me/IronTruthBot">a Telegram bot</a> that crowdsources reports of “inflammatory” posts, which Iron Truth’s organizers then forward to sympathetic insiders. “We have direct channels with Israelis who work in the big companies,” Kaganovitch said in an October 13 message to the Iron Truth Telegram group. “There are compassionate ones who take care of a quick removal.” The Intercept used Telegram’s built-in translation feature to review the Hebrew-language chat transcripts.</p>



<!-- BLOCK(pullquote)[0](%7B%22componentName%22%3A%22PULLQUOTE%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%22pull%22%3A%22center%22%7D) --><blockquote data-shortcode-type="pullquote" data-pull="center"><!-- CONTENT(pullquote)[0] -->Iron Truth vividly illustrates the perils and pitfalls of terms like “misinformation” and “disinformation” in wartime.<!-- END-CONTENT(pullquote)[0] --></blockquote><!-- END-BLOCK(pullquote)[0] -->



<p>So far, nearly 2,000 participants have flagged a wide variety of posts for removal, from content that’s clearly racist or false to posts that are merely critical of Israel or sympathetic to Palestinians, according to chat logs reviewed by The Intercept. “In the U.S. there is free speech,” Kaganovitch explained. “Anyone can say anything with disinformation. This is very dangerous, we can see now.”</p>



<p>“The interests of a fact checking or counter-disinformation group working in the context of a war belongs to one belligerent or another. Their job is to look out for the interests of their side,” explained Emerson Brooking, a fellow with the Atlantic Council’s Digital Forensic Research Lab. “They’re not trying to ensure an open, secure, accessible online space for all, free from disinformation. They’re trying to target and remove information and disinformation that they see as harmful or dangerous to Israelis.”</p>



<p>While Iron Truth appears to have frequently conflated criticism or even mere discussion of Israeli state violence with misinformation or antisemitism, Kaganovitch says his views on this are evolving. “In the beginning of the war, it was anger, most of the reporting was anger,” he told The Intercept. “Anti-Israel, anti-Zionist, anything related to this was received as fake, even if it was not.”</p>



<p>The Intercept was unable to independently confirm that sympathetic workers at Big Tech firms are responding to the group’s complaints or verify that the group was behind the removal of the content it has taken credit for having deleted. Iron Truth’s founder declined to share the names of its “insiders,” stating that they did not want to discuss their respective back channels with the press. In general, “they are not from the policy team but they have connections to the policy team,” Kaganovitch told The Intercept, referring to the personnel at social media firms who set rules for permissible speech. “Most of them are product managers, software developers. … They work with the policy teams with an internal set of tools to forward links and explanations about why they need to be removed.” While companies like Meta routinely engage with various civil society groups and NGOs to discuss and remove content, these discussions are typically run through their official content policy teams, not rank-and-file employees.</p>



<p>The Iron Truth Telegram account regularly credits these supposed insiders. “Thanks to the TikTok Israel team who fight for us and for the truth,” read an October 28 post on the group’s Telegram channel. “We work closely with Facebook, today we spoke with more senior managers,” according to another post on October 17. Soon after a Telegram chat member complained that something they’d posted to LinkedIn had attracted “inflammatory commenters,” the Iron Truth account replied, “Kudos to the social network LinkedIn who recruited a special team and have so far removed 60% of the content we reported on.”</p>


<!-- BLOCK(newsletter)[1](%7B%22componentName%22%3A%22NEWSLETTER%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%7D) -->

<!-- END-BLOCK(newsletter)[1] -->



<p>Kaganovitch said the project has allies outside Israel’s Silicon Valley annexes as well. Iron Truth’s organizers met with the director of a controversial Israeli government cyber unit, he said, and its core team of more than 50 volunteers and 10 programmers includes a former member of the Israeli Parliament.</p>



<p>“Eventually our main goal is to get the tech companies to differentiate between freedom of speech and posts that their only goal is to harm Israel and to interfere with the relationship between Israel and Palestine to make the war even worse,” Inbar Bezek, the former Knesset member working with Iron Truth, told The Intercept in a WhatsApp message.</p>



<p>“Across our products, we have policies in place to mitigate abuse, prevent harmful content and help keep users safe. We enforce them consistently and without bias,” Google spokesperson Christa Muldoon told The Intercept. “If a user or employee believes they’ve found content that violates these policies, we encourage them to report it through the dedicated online channels.” Muldoon added that Google “encourages employees to use their time and skills to volunteer for causes they care about.” In interviews with The Intercept, Kaganovitch emphasized that he works on Iron Truth only in his free time, and said the project is entirely distinct from his day job at Google.</p>



<p>Meta spokesperson Ryan Daniels pushed back on the notion that Iron Truth was able to get content taken down outside the platform’s official processes, but declined to comment on Iron Truth’s underlying claim of a back channel to company employees. “Multiple pieces of content this group claims to have gotten removed from Facebook and Instagram are still live and visible today because they don’t violate our policies,” Daniels told The Intercept in an emailed statement. “The idea that we remove content based on someone’s personal beliefs, religion, or ethnicity is simply inaccurate.” Daniels added, “We receive feedback about potentially violating content from a variety of people, including employees, and we encourage anyone who sees this type of content to report it so we can investigate and take action according to our policies,” noting that Meta employees have access to internal content reporting tools, but that this system can only be used to remove posts that violate the company’s public Community Standards.</p>



<p>Neither TikTok nor LinkedIn responded to questions about Iron Truth. X could not be reached for comment.</p>


<!-- BLOCK(photo)[2](%7B%22componentName%22%3A%22PHOTO%22%2C%22entityType%22%3A%22RESOURCE%22%7D)(%7B%22scroll%22%3Afalse%2C%22align%22%3A%22bleed%22%2C%22bleed%22%3A%22xtra-large%22%2C%22width%22%3A%22auto%22%7D) --><figure><!-- CONTENT(photo)[2] --> <img fetchpriority="high" decoding="async" width="2835" height="1890" src="https://theintercept.com/wp-content/uploads/2024/01/GettyImages-1730738212.jpg" alt="GAZA CITY, GAZA - OCTOBER 18: A Palestinian woman around the belongings of Palestinians cries at the garden of Al-Ahli Arabi Baptist Hospital after it was hit in Gaza City, Gaza on October 18, 2023. Over 500 people were killed on Al-Ahli Arabi Baptist Hospital in Gaza on Tuesday, Health Ministry spokesman Ashraf al-Qudra told. According to the Palestinian authorities, Israeli army is responsible for the deadly bombing. (Photo by Mustafa Hassona/Anadolu via Getty Images)" srcset="https://theintercept.com/wp-content/uploads/2024/01/GettyImages-1730738212.jpg?w=2835 2835w, https://theintercept.com/wp-content/uploads/2024/01/GettyImages-1730738212.jpg?w=300 300w, https://theintercept.com/wp-content/uploads/2024/01/GettyImages-1730738212.jpg?w=768 768w, https://theintercept.com/wp-content/uploads/2024/01/GettyImages-1730738212.jpg?w=1024 1024w, https://theintercept.com/wp-content/uploads/2024/01/GettyImages-1730738212.jpg?w=1536 1536w, https://theintercept.com/wp-content/uploads/2024/01/GettyImages-1730738212.jpg?w=2048 2048w, https://theintercept.com/wp-content/uploads/2024/01/GettyImages-1730738212.jpg?w=540 540w, https://theintercept.com/wp-content/uploads/2024/01/GettyImages-1730738212.jpg?w=1000 1000w, https://theintercept.com/wp-content/uploads/2024/01/GettyImages-1730738212.jpg?w=2400 2400w" sizes="(max-width: 1200px) 100vw, 1200px">
<p>A Palestinian woman cries in the garden of Al-Ahli Arab Hospital after it was hit in Gaza City, Gaza, on Oct. 18, 2023.</p>
<figcaption>Photo by Mustafa Hassona/Anadolu via Getty Images</figcaption><!-- END-CONTENT(photo)[2] --></figure><!-- END-BLOCK(photo)[2] -->


<h2 id="h-keep-bombing">“Keep Bombing!”</h2>



<p>Though confusion and recrimination are natural byproducts of any armed conflict, Iron Truth has routinely used the fog of war as evidence of anti-Israeli disinformation.</p>



<p>At the start of the project in the week after Hamas’s attack, for example, Iron Truth volunteers were encouraged to find and report posts expressing skepticism about claims of the mass decapitation of babies in an Israeli kibbutz. They quickly surfaced posts casting doubt on reports of “40 beheaded babies” during the Hamas attack, tagging them “fake news” and “disinformation” and sending them to platforms for removal. Among a list of LinkedIn content that Iron Truth told its Telegram followers it had passed along to the company was a post demanding evidence for the beheaded baby claim, categorized by the project as “Terror/Fake.”</p>



<p>But the skepticism they were attacking proved warranted. While many of Hamas’s atrocities against Israelis on October 7 are indisputable, the Israeli government itself ultimately said it <a href="https://theintercept.com/2023/10/11/israel-hamas-disinformation/">couldn’t verify</a> the horrific claim about<a href="https://theintercept.com/2023/12/14/israel-biden-beheaded-babies-false/"> beheaded babies</a>. Similarly, Iron Truth’s early efforts to take down “disinformation” about Israel bombing hospitals now contrast with weeks of well-documented airstrikes against multiple hospitals and the deaths of <a href="https://www.npr.org/2023/11/16/1213307710/gaza-doctors-al-shifa-hospital">hundreds of doctors</a> from Israeli bombs.</p>



<p>On October 16, Iron Truth shared a list of Facebook and Instagram posts it claimed responsibility for removing, writing on Telegram, “Significant things reported today and deleted. Good job! Keep bombing! 💪”</p>



<p>While most of the links no longer work, several are still active. One is a video of grievously wounded Palestinians in a hospital, including young children, with a caption accusing Israel of crimes against humanity. Another is a video from Mohamed El-Attar, a Canadian social media personality who posts under the name “That Muslim Guy.” In the post, shared the day after the Hamas attack, El-Attar argued the October 7 assault was not an act of terror, but of armed resistance to Israeli occupation. While this statement is no doubt inflammatory to many, particularly in Israel, Meta is supposed to allow for this sort of discussion, according to internal policy guidance previously reported by The Intercept. The <a href="https://theintercept.com/document/facebook-praise-support-and-representation-moderation-guidelines-reproduced-snapshot/">internal language</a>, which detailed the company’s Dangerous Individuals and Organizations policy, lists this sentence among examples of permitted speech: “The IRA were pushed towards violence by the brutal practices of the British government in Ireland.”</p>



<!-- BLOCK(promote-post)[3](%7B%22componentName%22%3A%22PROMOTE_POST%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%22slug%22%3A%22israel-palestine%22%2C%22crop%22%3A%22promo%22%7D) -->  
<!-- END-BLOCK(promote-post)[3] -->



<p>While it’s possible for Meta posts to be deleted by moderators and later reinstated, Daniels, the spokesperson, disputed Iron Truth’s claim, saying links from the list that remain active had never been taken down in the first place. Daniels added that other links on the list had indeed been removed because they violated Meta policy but declined to comment on specific posts.</p>



<p>Under their own rules, the major social platforms aren’t supposed to remove content simply because it is controversial. While content moderation trigger-happiness around mere mentions of designated terror organizations has led to <a href="https://theintercept.com/2022/09/21/facebook-censorship-palestine-israel-algorithm/">undue censorship </a>of Palestinian and other Middle Eastern users, Big Tech policies on misinformation are, on paper, much more conservative. Facebook, Instagram, TikTok, and YouTube, for example, only prohibit misinformation when it might cause physical harm, like snake oil cures for Covid-19, or posts meant to interfere with civic functions such as elections. None of the platforms targeted by Iron Truth prohibit merely “inflammatory” speech; indeed, such a policy would likely be the end of social media as we know it.</p>



<p>Still, content moderation rules are known to be vaguely conceived and erratically enforced. Meta for instance, says it categorically prohibits violent incitement, and touts various machine learning-based technologies to detect and remove such speech. Last month, however, The Intercept reported that the company had approved <a href="https://theintercept.com/2023/11/21/facebook-ad-israel-palestine-violence/">Facebook ads calling for the assassination</a> of a prominent Palestinian rights advocate, along with explicit calls for the murder of civilians in Gaza. On Instagram, users leaving comments with Palestinian flag emojis have seen these responses <a href="https://theintercept.com/2023/10/28/instagram-palestinian-flag-emoji/">inexplicably vanished</a>. 7amleh, a Palestinian digital rights organization that formally partners with Meta on speech issues, has documented over 800 reports of undue social media censorship since the war’s start, <a href="https://7or.7amleh.org/charts?start_date=10%2F07%2F2023&amp;end_date=12%2F12%2F2023&amp;type=14&amp;platform=&amp;platformIntegrate=&amp;gender=&amp;person_type=&amp;suspended_account=&amp;suspended_content=&amp;affiliation=&amp;affiliation=">according to its public database</a>.</p>



<h2 id="h-disinformation-in-the-eye-of-the-beholder">Disinformation in the Eye of the Beholder</h2>



<p>“It’s really hard to identify disinformation,” Kaganovitch acknowledged in an interview, conceding that what’s considered a conspiracy today might be corroborated tomorrow, and pointing to a recent Haaretz report that an <a href="https://www.haaretz.co.il/news/politics/2023-11-18/ty-article/0000018b-e1a5-d168-a3ef-f5ff4d070000">Israel Defense Forces helicopter may have inadvertently killed Israelis</a> on October 7 in the course of firing at Hamas.</p>



<p>Throughout October, Iron Truth provided a list of suggested keywords for volunteers in the project’s Telegram group to use when searching for content to report to the bot. Some of these terms, like “Kill Jewish” and “Kill Israelis,” pertained to content flagrantly against the rules of major social media platforms, which uniformly ban explicit violent incitement. Others reflected stances that might understandably offend Israeli social media users still reeling from the Hamas attack, like “Nazi flag israel.”</p>



<p>But many other suggestions included terms commonly found in news coverage or general discussion of the war, particularly in reference to Israel’s brutal bombardment of Gaza. Some of those phrases — including “Israel bomb hospital”; “Israel bomb churches”; “Israel bomb humanitarian”; and “Israel committing genocide” — were suggested as disinformation keywords as the Israeli military was being credibly accused of doing those very things. While some allegations against both Hamas and the IDF were and continue to be bitterly disputed — notably who bombed the Al-Ahli Arab Hospital on October 17 — Iron Truth routinely treated contested claims as “fake news,” siding against the sort of <a href="https://theintercept.com/2023/12/20/gaza-israel-biden-netanyahu-war-strategy/">analysis</a> or<a href="https://theintercept.com/2023/11/21/al-shifa-hospital-hamas-israel/"> discussion </a>often necessary to reach the truth.</p>



<!-- BLOCK(pullquote)[4](%7B%22componentName%22%3A%22PULLQUOTE%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%22pull%22%3A%22right%22%7D) --><blockquote data-shortcode-type="pullquote" data-pull="right"><!-- CONTENT(pullquote)[4] -->“This post must be taken down, he is a really annoying liar and the amount of exposure he has is crazy.”<!-- END-CONTENT(pullquote)[4] --></blockquote><!-- END-BLOCK(pullquote)[4] -->



<p>Even the words “Israel lied” were suggested to Iron Truth volunteers on the grounds that they could be used in “false posts.” On October 16, two days after an Israeli airstrike killed 70 Palestinians <a href="https://www.nbcnews.com/news/world/live-blog/israel-hamas-war-live-updates-rcna120252">evacuating from northern Gaza</a>, one Telegram group member shared a TikTok containing imagery of one of the bombed convoys. “This post must be taken down, he is a really annoying liar and the amount of exposure he has is crazy,” the member added. A minute later, the Iron Truth administrator account encouraged this member to report the post to the Iron Truth bot.</p>



<p>Although The Intercept is unable to see which links have been submitted to the bot, Telegram transcripts show the group’s administrator frequently encouraged users to flag posts accusing Israel of genocide or other war crimes. When a chat member shared a link to an Instagram post arguing “It has BEEN a genocide since the Nakba in 1948 when Palestinians were forcibly removed from their land by Israel with Britain’s support and it has continued for the past 75 years with US tax payer dollars,” the group administrator encouraged them to report the post to the bot three minutes later. Links to similar allegations of Israeli war crimes from figures such as popular Twitch streamer Hasan Piker; Colombian President Gustavo Petro; psychologist Gabor Maté; and a variety of obscure, ordinary social media users have received the same treatment.</p>



<p>Iron Truth has acknowledged its alleged back channel has limits: “It’s not immediate unfortunately, things go through a chain of people on the way,” Kaganovitch explained to one Telegram group member who complained a post they’d reported was still online. “There are companies that implement faster and there are companies that work more slowly. There is internal pressure from the Israelis in the big companies to speed up the reports and removal of the content. We are in constant contact with them 24/7.”</p>



<!-- BLOCK(cta)[5](%7B%22componentName%22%3A%22CTA%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%7D) -->

<!-- END-BLOCK(cta)[5] -->



<p>Since the war began, social media users in Gaza and beyond have complained that content has been censored without any clear violation of a given company’s policies, a well-documented phenomenon long before the current conflict. But Brooking, of the Atlantic Council, cautioned that it can be difficult to determine the process that led to the removal of&nbsp;a given social media post. “There are almost certainly people from tech companies who are receptive to and will work with a civil society organization like this,” he said. “But there’s a considerable gulf between claiming those tech company contacts and having a major influence on tech company decision making.”</p>



<p>Iron Truth has found targets outside social media too. On November 27, one volunteer shared a link to NoThanks, an Android app that helps users boycott companies related to Israel. The Iron Truth administrator account quickly noted that the complaint had been forwarded to Google. Days later, Google<a href="https://www.middleeastmonitor.com/20231205-google-removes-israel-boycott-app/"> pulled NoThanks from its app store</a>, though it was later reinstated. </p>



<p>The group has also gone after efforts to fundraise for Gaza. “These cuties are raising money,” said one volunteer, sharing a link to the Instagram account of Medical Aid for Palestinians. Again, the Iron Truth admin quickly followed up, saying the post had been “transferred” accordingly.</p>



<p>But Kaganovitch says his thinking around the topic of Israeli genocide has shifted. “I changed my thoughts a bit during the war,” he explained. Though he doesn’t agree that Israel is committing a genocide in Gaza, where the death toll has exceeded 20,000, according to the Gaza Health Ministry, he understands how others might. “The genocide, I stopped reporting it in about the third week [of the war].”</p>



<p>Several weeks after its launch, Iron Truth shared an infographic in its Telegram channel asking its followers not to pass along posts that were simply anti-Zionist. But OCT7, an Israeli group that “monitors the social web in real-time … and guides digital warriors,” lists Iron Truth as one of its partner organizations, alongside the Israeli Ministry for Diaspora Affairs, and cites “anti-Zionist bias” as part of the “challenge” it’s “battling against.”</p>



<p>Despite Iron Truth’s occasional attempts to rein in its volunteers and focus them on finding posts that might actually violate platform rules, getting everyone on board has proven difficult. Chat transcripts show many Iron Truth volunteers conflating Palestinian advocacy with material support for Hamas or characterizing news coverage as “misinformation” or “disinformation,” perennially vague terms whose meaning is further diluted in times of war and crisis.</p>



<p>“By the way, it would not be bad to go through the profiles of [United Nations] employees, the majority are local there and they are all supporters of terrorists,” recommended one follower in October. “Friends, report a profile of someone who is raising funds for Gaza!” said another Telegram group member, linking to the Instagram account of a New York-based beauty influencer. “Report this profile, it’s someone I met on a trip and it turns out she’s completely pro-Palestinian!” the same user added later that day. Social media accounts of Palestinian journalist Yara Eid; Palestinian photojournalist Motaz Azaiza; and many others involved in Palestinian human rights advocacy were similarly flagged by Iron Truth volunteers for allegedly spreading “false information.”</p>



<p>Iron Truth has at times struggled with its own followers. When one proposed reporting a link about <a href="https://www.aljazeera.com/news/2023/10/16/why-egypt-gaza-rafah-crossing-vital-for-supplying-aid-amid-israeli-strikes">Israeli airstrikes at the Rafah border crossing between Gaza and Egypt</a>, the administrator account pointed out that the IDF had indeed conducted the attacks, urging the group: “Let’s focus on disinformation, we are not fighting media organizations.” On another occasion, the administrator discouraged a user from reporting a page belonging to a news organization: “What’s the problem with that?” the administrator asked, noting that the outlet was “not pro-Israel, but is there fake news?”</p>



<p>But Iron Truth’s standards often seem muddled or contradictory. When one volunteer suggested going after B’Tselem, an Israeli human rights organization that advocates <a href="https://theintercept.com/2023/10/13/israel-settlers-gaza-palestinians-west-bank/">against the country’s military occupation</a> and <a href="https://theintercept.com/2023/11/29/intercepted-israel-palestine-prisoner-hostage/">broader repression of Palestinians</a>, the administrator account replied: “With all due respect, B’Tselem does publish pro-Palestinian content and this was also reported to us and passed on to the appropriate person. But B’Tselem is not Hamas bots or terrorist supporters, we have tens of thousands of posts to deal with.”</p>


<!-- BLOCK(photo)[6](%7B%22componentName%22%3A%22PHOTO%22%2C%22entityType%22%3A%22RESOURCE%22%7D)(%7B%22scroll%22%3Afalse%2C%22align%22%3A%22bleed%22%2C%22bleed%22%3A%22xtra-large%22%2C%22width%22%3A%22auto%22%7D) --><figure><!-- CONTENT(photo)[6] --> <img loading="lazy" decoding="async" width="2500" height="1663" src="https://theintercept.com/wp-content/uploads/2024/01/AP22254300455661.jpg" alt="11 September 2022, Israel, Jerusalem: Israeli flags fly in front of the Knesset, the unicameral parliament of the State of Israel. Photo by: Christophe Gateau/picture-alliance/dpa/AP Images" srcset="https://theintercept.com/wp-content/uploads/2024/01/AP22254300455661.jpg?w=2500 2500w, https://theintercept.com/wp-content/uploads/2024/01/AP22254300455661.jpg?w=300 300w, https://theintercept.com/wp-content/uploads/2024/01/AP22254300455661.jpg?w=768 768w, https://theintercept.com/wp-content/uploads/2024/01/AP22254300455661.jpg?w=1024 1024w, https://theintercept.com/wp-content/uploads/2024/01/AP22254300455661.jpg?w=1536 1536w, https://theintercept.com/wp-content/uploads/2024/01/AP22254300455661.jpg?w=2048 2048w, https://theintercept.com/wp-content/uploads/2024/01/AP22254300455661.jpg?w=540 540w, https://theintercept.com/wp-content/uploads/2024/01/AP22254300455661.jpg?w=1000 1000w, https://theintercept.com/wp-content/uploads/2024/01/AP22254300455661.jpg?w=2400 2400w" sizes="(max-width: 1200px) 100vw, 1200px">
<p>Israeli flags fly in front of the Knesset, the unicameral parliament of the state of Israel, on Sept. 11, 2022, in Jerusalem.</p>
<figcaption> Photo: Christophe Gateau/AP</figcaption><!-- END-CONTENT(photo)[6] --></figure><!-- END-BLOCK(photo)[6] -->


<h2 id="h-friends-in-high-places"><strong>Friends in High Places</strong></h2>



<p>Though Iron Truth is largely a byproduct of Israel’s thriving tech economy — the country is home to many regional offices of American tech giants — it also claims support from the Israeli government.</p>



<p>The group’s founder says that Iron Truth leadership have met with Haim Wismonsky, director of the controversial Cyber Unit of the Israeli State Attorney’s Office. While the Cyber Unit purports to combat terrorism and miscellaneous cybercrime, critics say it’s used to censor unwanted criticism and Palestinian perspectives, relaying thousands upon thousands of content takedown demands. American Big Tech has proven largely willing to play ball with these demands: A 2018 report from the Israeli Ministry of Justice claimed a <a href="https://www.hrw.org/news/2021/10/08/israel/palestine-facebook-censors-discussion-rights-issues">90 percent compliance rate across social media platforms</a>.</p>



<p>Following an in-person presentation to the Cyber Unit, Iron Truth’s organizers have remained in contact, and sometimes forward the office links they need help removing, Kaganovitch said. “We showed them the presentation, they asked us also to monitor Reddit and Discord, but Reddit is not really popular here in Israel, so we focus on the big platforms right now.”</p>



<p>Wismonsky did not respond to a request for comment.</p>



<p>Kaganovitch noted that Bezek, the former Knesset member, “helps us with diplomatic and government&nbsp;relationships.” In an interview, Bezek confirmed her role and corroborated the group’s claims, saying that while Iron Truth had contacts with “many other employees” at social media firms, she is not involved in that aspect of the group’s work, adding, “I took on myself to be more like the legislation and legal connection.”</p>



<p>“What we’re doing on a daily basis is that we have a few groups of people who have social media profiles in different medias — LinkedIn, X, Meta, etc. — and if one of us is finding content that is antisemitic or content that is hate claims against Israel or against Jews, we are informing the other people in the group, and few people at the same time are reporting to the tech companies,” Bezek explained.</p>



<p>Bezek’s governmental outreach has so far included organizing meetings with Israel’s Ministry of Foreign Affairs and “European ambassadors in Israel.” Bezek declined to name the Israeli politicians or European diplomatic personnel involved because their communications are ongoing. These meetings have included allegations of foreign, state-sponsored “antisemitic campaigns and anti-Israeli campaigns,” which Bezek says Iron Truth is collecting evidence about in the hope of pressuring the United Nations to act.</p>



<p>Iron Truth has also collaborated with Digital Dome, a similar volunteer effort spearheaded by the Israeli anti-disinformation organization FakeReporter, which helps coordinate the mass reporting of unwanted social media content. Israeli American investment fund J-Ventures, which has reportedly worked directly with the IDF to advance Israeli military interests, has <a href="https://jackpoulson.substack.com/p/inside-the-pro-israel-information">promoted both Iron Truth and Digital Dome</a>.</p>



<p>FakeReporter did not respond to a request for comment.</p>



<p>While most counter-misinformation efforts betray some geopolitical loyalty, Iron Truth is openly nationalistic. An <a href="https://www.ynetnews.com/magazine/article/sy0ojiig6">October 28 write-up</a> in the popular Israeli news website Ynet — “Want to Help With Public Diplomacy? This is How You Start”— cited the Telegram bot as an example of how ordinary Israelis could help their country, noting: “In the absence of a functioning Information Ministry, Israeli men and women hope to be able to influence even a little bit the sounding board on the net.” A <a href="https://www.bizportal.co.il/globalmarkets/news/article/820311">mention</a> in the Israeli financial news website BizPortal described Iron Truth as fighting “false and inciting content against Israel.”</p>



<p>Iron Truth is “a powerful reminder that it’s still people who run these companies at the end of the day,” said Brooking. “I think it’s natural to try to create these coordinated reporting groups when you feel that your country is at war in or in danger, and it’s natural to use every tool at your disposal, including the language of disinformation or fact checking, to try to remove as much content as possible if you think it’s harmful to you or people you love.”</p>



<p>The real risk, Brooking said, lies not in the back channel, but in the extent to which companies that control the speech of billions around the world are receptive to insiders arbitrarily policing expression. “If it’s elevating content for review that gets around trust and safety teams, standing policy, policy [into] which these companies put a lot of work,” he said, “then that’s a problem.”</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The GPT Store (144 pts)]]></title>
            <link>https://openai.com/blog/introducing-the-gpt-store</link>
            <guid>38940911</guid>
            <pubDate>Wed, 10 Jan 2024 17:27:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/blog/introducing-the-gpt-store">https://openai.com/blog/introducing-the-gpt-store</a>, See on <a href="https://news.ycombinator.com/item?id=38940911">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><!--[--><!--[--><div><p>It’s been two months since we&nbsp;<a href="https://openai.com/blog/introducing-gpts" rel="noopener noreferrer" target="_blank">announced&nbsp;</a>GPTs, and users have already created over 3 million custom versions of ChatGPT. Many builders have shared their GPTs for others to use. Today, we're starting to roll out the GPT Store to ChatGPT Plus, Team and Enterprise users so you can find useful and popular GPTs. Visit chat.openai.com/gpts to explore.<br></p></div><!--]--><!--[--><div id="discover-what-s-trending-in-the-store" data-heading=""><p><h2>Discover what’s trending in the store</h2></p></div><!--]--><!--[--><div><p>The store features a diverse range of GPTs developed by our partners and the community. Browse popular and trending GPTs on the community leaderboard, with categories like DALL·E, writing, research, programming, education, and lifestyle. <br></p></div><!--]--><!--[--><div id="new-featured-gpts-every-week" data-heading=""><p><h2>New featured GPTs every week</h2></p></div><!--]--><!--[--><div><p>We will also highlight useful and impactful GPTs. Some of our first featured GPTs include:</p><ul><li>Personalized trail recommendations from <a href="https://chat.openai.com/g/g-KpF6lTka3-alltrails" rel="noopener noreferrer" target="_blank">AllTrails</a></li><li>Search and synthesize results from 200M academic papers with <a href="https://chat.openai.com/g/g-bo0FiWLY7-researchgpt" rel="noopener noreferrer" target="_blank">Consensus</a></li><li>Expand your coding skills with Khan Academy’s <a href="https://chat.openai.com/g/g-HxPrv1p8v-code-tutor-khanmigo-lite" rel="noopener noreferrer" target="_blank">Code Tutor</a></li><li>Design presentations or social posts with <a href="https://chat.openai.com/g/g-alKfVrz9K-canva" rel="noopener noreferrer" target="_blank">Canva</a></li><li>Find your next read with <a href="https://chat.openai.com/g/g-z77yDe7Vu" rel="noopener noreferrer" target="_blank">Books</a></li><li>Learn math and science anytime, anywhere with the <a href="https://chat.openai.com/g/g-cEEXd8Dpb-ck-12-flexi" rel="noopener noreferrer" target="_blank">CK-12 Flexi</a> AI tutor<br></li></ul></div><!--]--><!--[--><div id="include-your-gpt-in-the-store" data-heading=""><p><h2>Include your GPT in the store</h2></p></div><!--]--><!--[--><div><p>Building your own GPT is simple and doesn't require any coding skills.</p><p>If you’d like to share a GPT in the store, you’ll need to:</p><ol><li>Save your GPT for <strong>Everyone </strong>(<strong>Anyone with a link</strong> will not be shown in the store).</li><li>Verify your Builder Profile (<strong>Settings</strong> → <strong>Builder profile</strong> → <strong>Enable your name or a verified website</strong>).</li></ol><p>Please review our latest <a href="https://openai.com/policies/usage-policies" rel="noopener noreferrer" target="_blank">usage policies</a> and <a href="https://openai.com/brand#gpts-in-chatgpt" rel="noopener noreferrer" target="_blank">GPT brand guidelines</a> to ensure your GPT is compliant. To help ensure GPTs adhere to our policies, we've established a new review system in addition to the existing safety measures we've built into our products. The review process includes both human and automated review. Users are also<a href="https://help.openai.com/en/articles/8554982-how-can-i-report-an-inappropriate-gpt" rel="noopener noreferrer" target="_blank"> able to report</a> GPTs.<br></p></div><!--]--><!--[--><div id="builders-can-earn-based-on-gpt-usage" data-heading=""><p><h2>Builders can earn based on GPT usage</h2></p></div><!--]--><!--[--><div><p>In Q1 we will launch a GPT builder revenue program. As a first step, US builders will be paid based on user engagement with their GPTs. We'll provide details on the criteria for payments as we get closer.<br></p></div><!--]--><!--[--><div id="team-and-enterprise-customers-can-manage-gpts" data-heading=""><p><h2>Team and Enterprise customers can manage GPTs</h2></p></div><!--]--><!--[--><div><p>Today, we announced our new <a href="http://openai.com/chatgpt/team" rel="noopener noreferrer" target="_blank">ChatGPT Team</a> plan for teams of all sizes. Team customers have access to a private section of the GPT Store which includes GPTs securely published to your workspace. The GPT Store will be available soon for <a href="https://openai.com/enterprise" rel="noopener noreferrer" target="_blank">ChatGPT Enterprise</a> customers and will include enhanced admin controls like choosing how internal-only GPTs are shared and which external GPTs may be used inside your business. Like all usage on ChatGPT Team and Enterprise, we do not use your conversations with GPTs to improve our models.</p><p>Explore GPTs at <a href="https://chat.openai.com/gpts" rel="noopener noreferrer" target="_blank">chat.openai.com/gpts</a>.<br></p></div><!--]--><!--[--><!--]--><!--]--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Invisible Ink: At the CIA's Creative Writing Group (116 pts)]]></title>
            <link>https://www.theparisreview.org/blog/2024/01/09/invisible-ink-at-the-cias-creative-writing-group/</link>
            <guid>38940731</guid>
            <pubDate>Wed, 10 Jan 2024 17:08:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theparisreview.org/blog/2024/01/09/invisible-ink-at-the-cias-creative-writing-group/">https://www.theparisreview.org/blog/2024/01/09/invisible-ink-at-the-cias-creative-writing-group/</a>, See on <a href="https://news.ycombinator.com/item?id=38940731">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<div id="attachment_166477"><p><img fetchpriority="high" decoding="async" aria-describedby="caption-attachment-166477" src="https://www.theparisreview.org/blog/wp-content/uploads/2024/01/cia-langley-1024x455.jpeg" alt="" width="1024" height="455" srcset="https://www.theparisreview.org/blog/wp-content/uploads/2024/01/cia-langley-1024x455.jpeg 1024w, https://www.theparisreview.org/blog/wp-content/uploads/2024/01/cia-langley-300x133.jpeg 300w, https://www.theparisreview.org/blog/wp-content/uploads/2024/01/cia-langley-768x341.jpeg 768w, https://www.theparisreview.org/blog/wp-content/uploads/2024/01/cia-langley-1536x683.jpeg 1536w, https://www.theparisreview.org/blog/wp-content/uploads/2024/01/cia-langley-2048x910.jpeg 2048w" sizes="(min-width: 62.5em) 67vw, 100vw"></p><p id="caption-attachment-166477">Aerial view of the Central Intelligence Agency headquarters some time between 1990 and 2006. Photographs in the Carol M. Highsmith Archive, Library of Congress, Prints and Photographs Division. Courtesy of <a href="https://commons.wikimedia.org/wiki/File:Aerial_view_of_the_Central_Intelligence_Agency_headquarters,_Langley,_Virginia_-_Corrected_and_Cropped.jpg">Wikimedia Commons.</a></p></div>
<p>Last spring, a friend of a friend visited my office and invited me to Langley to speak to Invisible Ink, the CIA’s creative writing group.</p>
<p>I asked Vivian (not her real name) what she wanted me to talk about.</p>
<p>She said that the topic of the talk was entirely up to me.</p>
<p>I asked what level the writers in the group were.</p>
<p>She said the group had writers of all levels.</p>
<p>I asked what the speaking fee was.</p>
<p>She said that as far as she knew, there was no speaking fee.</p>
<p>I dwelled a little on this point.</p>
<p>She confirmed that there was no speaking fee.</p>
<p>When an organization has, say, financed the overthrow of the government of Guatemala, you would think there might be a speaking fee. But I was told that, in lieu of payment, the writing group would take me out to lunch in the executive dining room afterward. I would also have my picture taken in front of the CIA seal, and I could post that picture anywhere I wanted.</p>
<p>“So my visit wouldn’t be classified?”</p>
<p>Vivian confirmed that I could tell anyone I wanted. “Just don’t tell them my name—or I’ll have to kill you. Just kidding!”<span id="more-166475"></span></p>
<p>As I considered the invitation, I kept wondering why I’d been invited. I don’t write about CIA-adjacent topics, nor am I successful enough a novelist that people outside a small circle—one that I doubt includes U.S. intelligence agencies—know my name. So the invite was a bit of a mystery. This was the second-most common question that came up when I told writer friends about it, topped only by: “No speaking fee?” At first, I wondered whether the gig was part of a recruitment strategy. But it doesn’t take a vast intelligence apparatus to know that I am not intelligence material, not least because I am a professional writer.</p>
<p>Next I wondered if my visit could be used as soft-diplomacy propaganda. Look how harmless we are! We let writers come to our headquarters and pose for pictures. The CIA had veered into this type of literary boosterism before—supporting, for example, the founding of the very magazine for which I am writing this piece. So it wasn’t out of the question. In 2021, I had turned down an invitation from the government of Saudi Arabia for an all-expenses-paid trip to a writers’ retreat at al-‘Ulā, as I didn’t want to be a part of their arts and culture whitewashing. But in the end, I couldn’t think of a way that I’d be a useful propaganda tool for the CIA—unless they anticipated me writing this essay (in which case, kudos CIA)—and so I said yes.</p>
<p>***</p>
<p>On the agreed-upon morning a few weeks later, I left my apartment in D.C. and drove into the haze of Canadian wildfire smoke that was floating over the city. By the time I turned off the George Washington Parkway at the George Bush Center for Intelligence exit, and on to a restricted usage road, I was already nervous. I’m the kind of person who weighs and measures my suitcases before flying, lest I be scolded at the airport, and I do not like driving down roads with signs like <small>EMPLOYEES ONLY</small> and <small>WILL BE ARRESTED.</small></p>
<p>At the gate intercom, I gave my name and social security number—Vivian had gathered this information and more ahead of time, over a series of phone calls, each from a different phone number—and a police officer gave me a visitor’s badge that was to be <em>displayed on my person at all times</em>. He warned me that I was to be<em> escorted at all times</em>.</p>
<p>I met Vivian in a lot between the first gate and the second gate, where her car was the only one parked. She gave me another badge that appeared identical to the first. I left my phone in my car as instructed, and we got into Vivian’s car and drove to the second gate. That was when things started not going as planned.</p>
<p>Four agitated police officers blocked our way.</p>
<p>“He can’t leave his car here!” they yelled when Vivian rolled down her window.</p>
<p>“But I cleared this ahead of time,” Vivian said.</p>
<p>“He can’t leave his car here. It’s a security risk.”</p>
<p>“But how am I supposed to escort him if we can’t drive together?”</p>
<p>“Ma’am,” one of them said, “I just do parking.”</p>
<p>It turned out that, like in many bureaucracies, the individual parts that made up the CIA were siloed, and there was no point in arguing about logical contradictions.</p>
<p>Vivian gave up and drove me back to my car, clearly stressed. I told her it wasn’t a big deal—I would just follow her.</p>
<p>The problem, she said, was that we wouldn’t be able to park in the same lot. And I had to be escorted at all times. And employee parking at the CIA was a mess. “It’ll take me forever just to walk to you.”</p>
<p>She resolved that she would simply park in VIP visitor parking with me, and if she got a ticket, she got a ticket. “Just follow me.”</p>
<p>I got in my car and followed her to the gate. I watched from behind the wheel as she drove up to the gate, talked to one of the police officers, and drove off past the gate at a good clip, very much not being followed by me.</p>
<p>I pulled up to the gate, and an aggressive police officer questioned me about why I had two badges.</p>
<p>“Didn’t it seem strange to you to get a second badge when you’d just got your first one?”</p>
<p>“I’ve never been here before,” I said. “Everything seems strange to me.”</p>
<p>A different cop told him to give it a rest, handed me a third badge, and asked if I needed directions to VIP parking. I have a terrible sense of direction—I once got lost at Costco for so long that they had to call my mom over the PA; I was fifteen—and Google Maps isn’t much use at Langley.</p>
<p>The nice cop said that I needed to turn right and follow the road until the sixth left. There I would see a line of squad cars and a gate, where my badge would swipe me in.</p>
<p>“If you see a helicopter, you’ve gone too far,” he said. “Just loop back around. Don’t make a U-turn.”</p>
<p>When I later told Vivian about the mean cop and the nice one, she said, “They’re always doing that good cop–bad cop thing.”</p>
<p>“For parking?”</p>
<p>“For everything!”</p>
<p>I found the VIP parking on my first try. I held my badge out to the scanner. The gate rose! I drove in. And drove. And drove. And drove. In circles, because all the spaces in the small VIP lot were taken. I couldn’t leave the parking lot—I wasn’t supposed to be unescorted anywhere on campus, but at least in visitor parking my presence was somewhat explainable—so I kept circling the lot, accumulating sweat. Finally, someone left. I parked, got out, took a breath of ashy air, and wondered what to do next. I was relieved to see Vivian’s car stuck at the VIP gate, negotiating with the voice on the intercom.</p>
<p>“They won’t let me into VIP parking,” she explained as I got into her car. “They said it’s a security risk.”</p>
<p>We turned back onto the main road and drove for a bit. And then, after a bend, there appeared an abundance of parked cars. Cars upon cars upon cars. I’d never seen a parking lot this big, outside of professional sporting events. The quadrants were labeled by color, the rows by letter; we weaved through row after row of Virginia plates, from Blue D all the way up to Purple V without finding a spot.</p>
<p>I asked Vivian how many people worked at the CIA.</p>
<p>“Maybe two million?” She smiled and confessed that she had no idea, even though I was made to understand that she had been at the CIA, and in the writing group, for a number of years.</p>
<p>As we snaked through line after line of cars, Vivian told me that if you worked here and wanted to avoid a twenty-minute walk from your car, you had to be at the office by 7 <small>A.M.</small> I wondered if this was intentional—a way to encourage long hours, like the tech companies that offer employees free dinners in the cafeterias that don’t open until 6:30 <small>P.M.</small> Or if it was the result of expansion necessitated by the post-9/11 surveillance state and the popularity of phones that record our every movement. As Kerry Howley notes in <em>Bottoms Up and the Devil Laughs: A Journey Through the Deep State</em>, we have created and stored more data in the twenty-first century than in the rest of human history combined. If the government wants to find coherent stories in all that data, I thought as I looked at the vastness of the lot, someone has to comb through it.</p>
<p>At first, we couldn’t find the conference room. Like me, Vivian wasn’t allowed to bring her phone into the main building, but even if she had, I don’t know who she would’ve called for directions. CIA officers generally don’t know their coworkers’ last names. (The Starbucks at Langley is the only Starbucks where baristas aren’t allowed to ask for your name.) So I am without photos or notes, but walking through the main building at Langley, is, in my memory, like walking through an airport terminal in a major metropolis, crossed with a hospital, crossed with an American mall, crossed with an Eastern European university. It’s big and gleaming and cold and brutal, all at once. There was a hall of presidential portraits with notes from commanders in chief to the Secret Service, all of them written in elegant fountain pen, except for Donald Trump’s, which was written in Sharpie and said “I’M SO PROUD OF YOU!”</p>
<p>We finally found the conference room, through a side door in the CIA Museum. It was unclear who this museum was for, but it was not a bad museum, full of objects of interest: pieces of the Berlin Wall, tie-clip cameras, Soviet bugging devices, et cetera, displayed in glass cases. Six people were seated at the conference table inside the conference room, which was windowless and had a big CIA seal on the wall.</p>
<p>“Sorry we’re late!” Vivian announced.</p>
<p>“Strip search?” one of the men joked.</p>
<p>“Parking,” I said.</p>
<p>A collective groan. The goddamned parking.</p>
<p>I began by asking what people were writing. Surprisingly, none of the CIA writers were writing spy novels. They were working on short stories. Self-published dystopian sci-fi. A presidential biography. Upmarket fiction. A personal blog, which I was told to check out if I ever wanted a really good muffin recipe. The writing group was organized around what sounded like a listserv announcing periodic meetings to whatever members were available that day. Only about half the people in the room seemed to know one another.</p>
<p>I talked a little bit about writing beginnings and working through false starts. I read the first page of my latest novel, explained why I’d set the first scene in the U.S. when the rest of the novel takes place in Ukraine, and went through all the false starts I’d taken to get where I was going. One officer raised their hand and asked about establishing voice in first versus third person. Another asked about revision techniques. Another about the shift from writing alone to working with an editor. It was the least remarkable Q&amp;A I’ve ever been a part of.</p>
<p>I had a little time to kill before our lunch reservation—seating time in the executive dining room was not flexible—so Vivian took me to the gift shop.</p>
<p>Given that almost no one’s allowed inside Langley and the people who work for the CIA aren’t supposed to advertise it, it was, like with the museum, a bit of a mystery who the gift shop was for. The shelves were stocked with T-shirts (Central Intelligence Agency), mugs (Central Intelligence Agency), and novelty barbecue sauce (Top Secret Recipe!). There was also a Pride Month display (Central Intelligence Agency in rainbow). I bought a Pride Month pen for four dollars.</p>
<p>***</p>
<p>The dining room was long and mostly empty—apparently a security thing—with white tablecloths and a long wall of windows looking out at the swampy greenery of northern Virginia. Or I was told that it normally looked out at greenery. Today it looked out at wildfire smoke. The menu was essentially cafeteria food—normal American fare. I ordered a burger with sweet potato fries and a Coke from a businesslike waitress in a white dress shirt.</p>
<p>The CIA officer seated next to me asked if I thought it was worth getting a literary agent. I said yes, and she seemed skeptical.</p>
<p>“In my other work,” she explained, “I can get movie people attached.”</p>
<p>I still have no idea what she meant.</p>
<p>While we waited for our food, the writer of dystopian sci-fi confirmed that if you work for the CIA, lawyers have to vet anything you publish. But they were more lenient than I would’ve guessed. She said that one of her novels had helped change how the agency viewed fiction versus nonfiction. While reading her novel, the lawyers decided that just because a character in a novel says something doesn’t mean that the author necessarily agrees, so there should be more leeway for CIA fiction writers. (Which suggests CIA lawyers are more nuanced literary critics than half of Goodreads.)</p>
<p>Obviously you can’t share classified information, I was told. You can’t violate the Hatch Act, showing your political affiliation, and you’re also not supposed to violate the <em>Washington Post</em> rule, which was: Would the CIA be embarrassed if this were in tomorrow’s <em>Washington</em> <em>Post</em>? (This seemed trickiest to determine.)</p>
<p>Another officer mentioned that, since the CIA has people doing things abroad that could be considered dubious, you had to be sensitive about that. I asked what they meant when they said dubious, which resulted in a change of topic. I asked if they knew of any issues with someone trying to publish something that they couldn’t get approved. One of the older writers said that she had heard of an officer who had tried to publish a memoir that discussed his experience of racism in the CIA and was told he couldn’t until he retired.</p>
<p>After lunch—everyone paid at the register, in cash, and Vivian paid for me—Vivian walked me out to my car.</p>
<p>“It was interesting to learn what you all can and can’t write about,” I said to Vivian. “I didn’t realize you had so much freedom to write about your jobs.”</p>
<p>We passed through the security turnstile and walked over a giant CIA seal, which I recognized from several movies, painted on the marble floor.</p>
<p>“The last thing in the world I’d want to write about is this place,” Vivian said at the door. “I can’t imagine anything more boring.”</p>

<p><em>Johannes Lichtman’s debut novel,</em> Such Good Work, <em>was a National Book Foundation 5 Under 35 honoree. His second novel,</em> Calling Ukraine,<em> is available in hardcover and will be published in paperback in April.</em></p>
	</div></div>]]></description>
        </item>
    </channel>
</rss>