<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 20 Apr 2025 14:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[100 Years to Solve an Integral (2020) (158 pts)]]></title>
            <link>https://liorsinai.github.io/mathematics/2020/08/27/secant-mercator.html</link>
            <guid>43741273</guid>
            <pubDate>Sun, 20 Apr 2025 03:16:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://liorsinai.github.io/mathematics/2020/08/27/secant-mercator.html">https://liorsinai.github.io/mathematics/2020/08/27/secant-mercator.html</a>, See on <a href="https://news.ycombinator.com/item?id=43741273">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content">
        <p><em>The integral of sec(x) is well known to any beginners calculus student. Yet this integral was once a major outstanding maths problem. It was first introduced by Geradus Mercator who needed it to make his famous map in 1569. He couldn’t find it and used an approximation instead. The exact solution was found accidentally 86 years later without calculus in 1645. It then took another two decades until a formal proof was given in 1668, 99 years after Mercator first proposed the problem.</em></p>

<p><em>Update 13 March 2021: added a note on how Napier calculated logarithm trigonometry tables. This was prompted by a correction raised in a discussion of this post on <a href="https://news.ycombinator.com/item?id=24304311">HackerNews</a>.</em></p>

<p><em>Update 10 October 2021: the great circle and rhumb line images are now made with a script that uses <a href="https://scitools.org.uk/cartopy/docs/latest/">Cartopy</a>. Previously they were made with a Matlab application. You can see the new script at my <a href="https://github.com/LiorSinai/Navigation">Github repository</a> and enter your co-ordinates to generate your own lines.</em></p>

<p>As this <a href="https://www.smbc-comics.com/comic/how-math-works">comic</a> by SMBC rightly teases, the history of mathematics is often not so straightforward. 
Theorems, formulas and notation that are routinely discussed in class, were once insights or accidents themselves.
This is the story of one such formula, the integral of the secant. 
I first read about it almost a decade ago when I got interested in cartography: the science and art of map making.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup> 
This integral is of vital importance to the Mercator map and therefore many online maps that use it like <a href="https://www.apple.com/ios/maps/">Apple Maps</a> and <a href="https://www.google.com/maps/">Google Maps</a>.</p>

<p>This story has been told several times before: see <a href="https://www.jstor.org/stable/3603395">1</a>, <a href="https://doi.org/10.1080/0025570X.1980.11976846">2</a> or <a href="https://scholarworks.umt.edu/tme/vol7/iss2/12/">3</a>.
But these are all journal articles, consigned mostly to academics.
I want to present it here in a less formal and more colourful setting to make it more accessible.</p>

<p>This is an article about mathematics so familiarity with the following is helpful: algebra, trigonometry, radians and basic calculus. 
These are usually covered in advanced high school maths classes or first year maths courses.</p>

<h2 id="first-year-maths">First year maths</h2>

<p>In first year maths at university after a month of differentiation we were starting the inverse problem: integration. 
Differentiation  is the mathematics of finding gradient functions for curves.
Integration is the mathematics of inverting this - given a gradient function, what is the curve? 
My lecturer was introducing the integration of trigonometric functions.
He started off with:</p><p>

\[\int \sin(x) dx = -\cos(x) + c \:\text{  and } \int \cos(x) dx = \sin(x) + c\]

</p><p>This relationship made sense because sine and cosine derivatives were opposites. Just had to be careful of minus signs. Next he derived the integral for the tangent:</p><p>

\[\int \tan(x) dx = \int \frac{\sin(x)}{\cos(x)}dx = -\ln|\cos(x)| + c\]

</p><p>Ok, that was tricky. It was not immediately obvious that the inverse of the chain rule could be used here, because the function $\cos(x)$ was present with its derivative $\sin(x)$.
But given enough thought it made sense. Then he said, this is the integral of the secant and learn it off by heart:</p><p>

\[\int \sec(x) dx = \ln|\sec(x) + \tan(x)| + c\]

</p><p>OK, where did that come from? My lecturer offered no explanation. It was easy to verify that it worked by finding the derivative.<sup id="fnref:derivative" role="doc-noteref"><a href="#fn:derivative" rel="footnote">2</a></sup>
(Paper <a href="https://doi.org/10.1080/0025570X.1980.11976846">2</a> has a more complex proof using only integration.)
But how had he come up with that?</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/integral_secant_graph.png" alt="Secant integral graph">
	<figcaption>The curve $\ln|\sec(x) + \tan(x)|$ has tangents with a gradient of $\sec(x)$. Discovering this fact took 100 years.</figcaption>
</figure>

<p>I think at this point most first year calculus students like me have the following fleeting thoughts:</p>
<ol>
  <li>Integration is much harder than differentiation.</li>
  <li>Some mathematician must have stumbled on this through differentiation first.</li>
  <li>It doesn’t matter anyway because where will I ever use this?</li>
</ol>

<p>Actually number 1 is true as many a student can testify after writing a test. 
Number 2 is false - in fact it was found by a teacher while looking at raw numbers. 
Such a method for finding an integral is so unusual that one might conjecture it is the <em>only</em> integral that has been found like this.
Surely in calculus class where raw numbers are so rare you would be laughed at if you attempted to solve an integral like that.
Lastly, number 3 remains true for me. But this doesn’t mean this integral isn’t useful - it is used to construct the Mercator map. 
That is why a teacher was crunching numbers when he serendipitously realised what the formula was.</p>

<h2 id="quick-revision-trigonometry">Quick revision: trigonometry</h2>

<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/secant_def.png" alt="Definition of the secant">
</figure>

<p>The secant is a standard trigonometric function. It is defined as the ratio of the hypotenuse $c$ to the adjacent side $a$ for an angle $\varphi$ in a right angled triangle.
In mathematical notation the definition is:</p><p>

\[\sec(\varphi) = \frac{c}{a}\]

</p><p>It is the reciprocal of the more widely used cosine function:</p><p>

\[\sec(\varphi) = \frac{1}{\cos(\varphi)}\]

</p><p>Here are the graphs of the secant and cosine for the angles from $-2\pi$ (-360°) to $2\pi$ (360°):</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/secant_graph.png" alt="Secant graph">
</figure>

<p>The integral of the secant can be interpreted as the area under the graph.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">3</a></sup> This is illustrated by the shaded region</p>

<h2 id="an-introduction-to-cartography">An introduction to cartography</h2>

<p>The earth cannot be projected onto a flat map without distortion. 
Over the years cartographers have devised many different map projections which try to balance minimising distortion with other properties. 
They come in all shapes and sizes.
Lists of these projections can be found <a href="https://map-projections.net/singleview.php">here</a> or <a href="https://en.wikipedia.org/wiki/List_of_map_projections">here</a>.
I will explain two of the simplest here, which will help with understanding the Mercator map in the next section.</p>

<p>All map projections can be represented as equations that transform spherical co-ordinates to flat map co-ordinates.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">4</a></sup>
The co-ordinates on the sphere are the angles $\varphi$ and $\lambda$. These correspond to lines of latitude (parallels) and longitude (meridans) respectively. 
The co-ordinates on the flat map are $x$ and $y$. A map projection is therefore a transformation from $\varphi$ and $\lambda$ to $x$ and $y$.</p>

<p>One of the simplest and oldest known projections is the equirectangular projection:</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/equirectangular.png" alt="Equirectangular map">
	<figcaption>Equirectangular map. From <a href="https://map-projections.net/single-view/rectang-0">map-projections.net/singleviewp/rectang-0</a>  </figcaption>
</figure>

<p>It is made by mapping meridians and parallels to vertical and horizontal straight lines of constant spacing.
This has the affect of stretching out objects along the parallels.
The equations for this projection are:</p><p>

\[\begin{align} y &amp;= R\varphi\\ x &amp;= R\lambda \end{align}\]

</p><p>While the equations are simple the construction process can be hard to visualise. Here is my attempt:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/equirectangular_construction.png" alt="Equirectangular map construction">
	<figcaption>A segment of the sphere is peeled off and flattened.
The right most image shows a top view of the flattening. The arcs are pulled flat in the longitudinal direction so that they become straight lines.
They do not change length in this process but that requires stretching out the sides along the $x$-axis. 
Thus a single point where all the arcs meet is stretched into a line.   </figcaption>
</figure>

<p>The equirectangular map has a total area of $(2\pi R)(\pi R)=2\pi^2 R^2$ while the surface area of the sphere is $4\pi R^2$. 
Therefore this projection distorts the area by a factor of $\frac{\pi}{2}\approx1.57$.</p>

<p>A different kind of projection can be obtained by projecting lines from the sphere onto the mapping plane. 
An example is the Lambert cylindrical projection. 
It is made by wrapping a cylinder around the sphere and projecting points onto it via lines parallel to the $x$-axis. 
Here is a visualisation of this construction:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/Cilinderprojectie-constructie.jpg" alt="Lambert cylindrical map construction">
	<figcaption>From <a href="https://en.wikipedia.org/wiki/File:Cilinderprojectie-constructie.jpg">Wikipedia</a>  </figcaption>
</figure>

<p>And here is a cross section of the sphere along side the final map:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/Lambert_construction.png" alt="Lambert cylindrical  map">
	<figcaption>Lambert cylindrical  map. Edited from <a href="https://map-projections.net/single-view/lambert">map-projections.net/singleviewp/lambert</a>  </figcaption>
</figure>

<p>The equations are:</p><p>

\[\begin{align} y &amp;= R\sin(\varphi)\\ x &amp;= R\lambda \end{align}\]

</p><p>For objects near the equator this results in very little distortion such as for Africa. But objects near the poles are compressed because of the sphere’s curvature. 
This can be seen clearly with Greenland.</p>

<p>This map has the useful property that its area is equal to the surface area of the sphere. 
The area of the flat map is $(2\pi R)(2R) = 4\pi R^2$ which is the same as that of the sphere.
Thus, while objects are distorted, their areas are still correct.
The relative scale of Greenland to Africa is therefore accurately represented in this map.</p>

<h2 id="the-mercator-map">The Mercator map</h2>

<p>In 1569mGerardus Mercator wanted to make a global world map that would be useful for navigation. 
He lived in a time when sailing across vast ocean distances was the norm. (In 1492 Christopher Columbus had discovered America by sailing all the way from Spain.)
The maps shown above are fine for artistic impressions and applications but not for navigation.
The distortions prevent doing any accurate distance and bearing measurements on the map. 
At a local level lines (eg. roads) which in reality intersect perpendicularly to each other would be appear to be slanted with respect to each other.</p>

<p>In particular Mercator wanted to make a map where rhumb lines would be straight. Rhumb lines are curves of constant bearing relative to meridians. 
To follow a rhumb line a navigator only needs to maintain the same bearing on their compass for the whole journey.
For example, here is the rhumb line through modern day New York and Cape Town:</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/NY_to_CPT.png" alt="Arcs from NY to CPT">
	<figcaption>Orthographic map with great circle and rhumb line. Source code: <a href="https://github.com/LiorSinai/Navigation">link</a></figcaption>
</figure>

<p>At each point along the rhumb line the angle $\theta$ with the meridian is 48.56° which corresponds to a bearing of 311°26’18’’ from Cape Town to New York. 
I’ve also shown the great circle which is a circle whose centre lies on the centre of the sphere. 
Traveling along the great circle is always shorter. In this case the distance is 12550 km instead of 12600 km along the rhumb line.
With modern technology it is easy for ships and aeroplanes to stick to the great circles. But back in Mercator’s day this was rather difficult. 
So sailors preferred to stick to rhumb lines. They would rather get to their destination by traveling a little longer then get lost and travel a lot more.</p>

<p>Mercator’s idea was to stretch out a cylindrical projection map in the North-South direction to preserve shapes and angles.
Looking at the Lambert projection,<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">5</a></sup> it can be seen that a different stretch factor is required for each latitude. At the equator no stretch is required.
At the 45° parallel only a small amount of upward stretching is required. The objects close to the poles have to be stretched a lot to uncompress them.</p>

<p>This stretch factor can be calculated as follows:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/Mercator_construction.png" alt="Mercator construction">
	<figcaption>Construction process for the Mercator map. Edited from <a href="https://en.wikipedia.org/wiki/Mercator_projection"> Wikipedia</a> images </figcaption>
</figure>

<p>First Mercator divided the globe into graticules of equal spacing $\delta\varphi$ and $\delta\lambda$. 
Along the meridians the arc length of each graticule is $R\delta\varphi$.
Along the parallels the radius of the circle is $R\cos(\varphi)$ so that the arc length is $(R\cos(\varphi))\delta\lambda$.
The tangent can be then approximated as:</p><p>

\[\tan(\alpha) \approx \frac{R\cos(\varphi)\delta\lambda}{R\delta\varphi}\]

</p><p>This graticule is then flattened into the rectangle with the following two requirements:</p>

<ol>
  <li>The angles are kept constant by setting $\alpha =\beta$.</li>
  <li>The parallels are projected on to the $x$-axis like in the Lambert projection. This means $\delta x = R\delta \lambda$.</li>
</ol>

<p>Therefore the transformation is:</p><p>

\[\begin{align} \tan(\alpha) &amp;= \tan(\beta) \\ 
\frac{R\cos(\varphi)\delta\lambda}{R\delta\varphi} &amp;= \frac{\delta x}{\delta y} \\
\delta y &amp;=  \frac{\delta x}{\delta \lambda} \frac{1}{\cos(\varphi)} \delta \varphi = R \sec(\varphi) \delta \varphi
 \end{align}\]

</p><p>From here it is a small step to turn this into an integral. However, calculus was only properly invented a century later after Mercator published his map. 
Instead what Mercator did was realise that he could add up the stretch factors at each point. 
The stretch at graticule <em>n</em> is approximately the stretch of the graticule below it plus $R \sec(\varphi) \delta \varphi$. This can then be turned into a sum:</p><p>

\[\begin{align} y_n &amp;\approx R \sec(n \cdot \delta \varphi) \delta \varphi + y_{n-1} \\
                     &amp;=\sum^{n}_{k=0} R \sec(k \cdot \delta \varphi) \delta \varphi \end{align}\]

</p><p>Using a constant value for $\delta \varphi $ Mercator was able to calculate the spacings for his map. Then he drew a world map over it.
This is a modern rendering of the final result:</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/mercator_NY_to_CPT.png" alt="Mercator map">
	<figcaption>Mercator map with great circle and rhumb line. Source code: <a href="https://github.com/LiorSinai/Navigation">link</a></figcaption>
</figure>

<p>This map is very heavily distorted. Greenland now looks larger than Africa. The great circles lie along strange curves. But the rhumb lines are straight!
Calculating the bearing can be done simply with a ruler and a projector. 
No thinking or maths required! (And no fancy instruments on globes either.) In modern day terms we would say, it was a big hit with sailors.</p>

<p>For online maps the local projection is very important. If you are looking for directions in a city what matters most to you is that the roads look correct.
This is why the Mercator map is used - to preserve angles between grids in roads. Other map projections don’t meet this simple requirement.
A minor problem is that the scale changes at each latitude but online maps can easily calculate this at each point. 
Try this with <a href="https://www.google.com/maps/">Google Maps</a>. For the same zoom factor, the scale bar is not the same length at each latitude. 
Also, along the equator, the scale bar has a minimum of 5m. Up near the poles, which is way more stretched out, the scale bar goes down to 1m.
But you don’t notice this when zooming in on a specific point in the map.</p>

<p>For the record, if you are going to look at long distances on Google Maps it’s best to turn the “Globe view” option on.</p>

<h2 id="tables-for-trig-mercator-and-logs">Tables for trig, Mercator and logs</h2>

<p>This is where things take an unexpected turn. 
But in order to understand how, I want to explain another part of history: mathematical  tables. 
These days pocket calculators are so common that we have forgotten that they were once ubiquitous with maths.
This was true even into the ’90s.</p>

<p>In the past if you wanted to calculate sec(36°) you could draw a big triangle and physically measure the angle and distances with the ruler. 
Then you could write out the long division calculation.
More likely, however, you would read up the value in a trigonometry table. 
The numbers in these tables were painstakingly calculated using approximation formulas and trigonometric identities. 
But for the user they were very simple. You just had to look up the numbers and occasionally interpolate between numbers if you wanted higher accuracy. 
These tables had the added benefit of making inversion easy. 
For example if you looked for the number 1.23606 in the secant table, you would see it was next to 36°.</p>

<p>In 1599 Edward Wright published tables for the equator Mercator map equation. He used $\delta \varphi = 1’ = \frac{1}{60} 1^{\circ}$. 
He also gave the first mathematical description of the Mercator map which Mercator himself did not explain fully. 
This made it easier for others to make their own Mercator maps.</p>

<p>In 1614 John Napier introduced logarithms. This is the inverse of the exponential operation. In modern terms, the logarithm $y$ of $x$ to base $b$ is written as:</p><p>

\[y = \log_b x  \quad ; \quad b^y = x\]

</p><p>Napier’s main motivation was to find an easier way to do multiplication and division. 
For example from the laws of exponents:</p><p>

\[2^a \div 2^b = 2^{a-b}\]

</p><p>Therefore a division can be done as follows:</p><p>

\[3764 \div 873 = 2^{11.878} \div 2^{9.770} = 2^{2.108} = 4.311\]

</p><p>Where $\log_{2}(3764) = 11.878 $ and $\log_{2}(873) = 9.770 $</p>

<p>The logarithms again had to be painstakingly calculated through approximation calculations. 
Napier did this using a kinetic framework. While this idea may be unusual today, it has to do with how Napier originally visualised logarithms.<sup id="fnref:Napier" role="doc-noteref"><a href="#fn:Napier" rel="footnote">6</a></sup>
His final table related numbers to logarithms and their sines.<sup id="fnref:correction" role="doc-noteref"><a href="#fn:correction" rel="footnote">7</a></sup> Here is an example:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/Napiertable.png" alt="Napier trigonometric table">
	<figcaption>Napier's original trigonometric table. From <a href="https://jscholarship.library.jhu.edu/bitstream/handle/1774.2/34187/31151005337641.pdf">John Napier and the Invention of Logarithms</a></figcaption>
</figure>

<p>A  user could use such a table to look up a logarithm with the added benefit that inversion was easy.
These proved to be very popular - people clearly did not like doing multiplication and division in the past.
Using addition and subtraction in their place also made calculations less error prone, especially with successive calculations.</p>

<p>Next, mathematicians extended these tables to other trigonometric function:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/APN2002-table3-30deg.png" alt="Logarithmic trigonometric table">
	<figcaption>A page from the 2002 American Practical Navigator. Values are calculated as $log_{10}(f(n^\circ + \frac{k}{60})) + 10$. From <a href="https://en.wikipedia.org/wiki/File:APN2002-table3-30deg.tiff"> Wikipedia</a></figcaption>
</figure>

<p>In 1645, according to legend, a teacher named Henry Bond noticed something strange. 
The numbers in Wright’s Mercator table were similar to the numbers in a $\log_e(\tan(\varphi))$ table.
They just were offset by a factor of 2 and 45° in the tables. So he essentially conjectured that:</p><p>

\[\int_0^{\varphi_1} \sec(\varphi) d\varphi = \ln \left| \tan \left( \frac{\varphi_1}{2} + 45^\circ \right) \right |\]

</p><p>Mathematicians caught on to the claim but could not prove it. Calculus was still in its infancy. 
In 1668, 99 years after Mercator first made his map and 23 years after Bond gave the solution, it was finally proven by James Gregory.
This proof however was considered long-winded and “wearisome”.
In 1670 Isaac Barrow offered a more succinct proof through integration with partial fractions which can be found in <a href="https://doi.org/10.1080/0025570X.1980.11976846">2</a>.</p>

<p>Lastly, through trigonometric identities, it can be proven that the following three formulas are all equivalent:</p><p>

\[\int \sec(\varphi) d\varphi = 
\begin{cases} 
\ln |\sec(\varphi) + \tan(\varphi) | + c \\
\ln \left| \tan \left( \frac{\varphi}{2} + 45^\circ \right) \right | + c\\ 
\frac{1}{2}\ln \left| \frac{1+\sin(\varphi)}{1-\sin(\varphi)}  \right| + c
\end{cases}\]

</p><h2 id="conclusion">Conclusion</h2>

<p>This has been a long post. I hope you found this history as fascinating as I did.
It truly is remarkable to me that this little formula on my first year exam had such a colourful and varied history.
I really think it should be taught more in class. This was already tried and tested by <a href="https://scholarworks.umt.edu/tme/vol7/iss2/12/">3</a>. 
At a small scale they found it worked.</p>

<p>If you are more interested in how Google makes its map I highly suggest reading this blog post by a Google engineer:
<a href="https://medium.com/google-design/google-maps-cb0326d165f5">medium.com/google-design/google-maps-cb0326d165f5</a>.
Can you spot the integral of the secant in the Google code?</p>

<p>There is one last comment I would like to add. 
There is a lot of controversy surrounding the Mercator map.
It is an extremely common projection. When I was younger I had a map of the world on my wall in the Mercator projection.
However I hope you now fully appreciate its main purpose is navigation.
Outside of that, it unnecessarily distorts shapes and in particular makes the Americas and Europe look much larger than they actually are.
This has been linked, not without rational, to colonialism and racism. 
For decades cartographers have bemoaned its use in applications where it really has no right to be.<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" rel="footnote">8</a></sup>
Here is even an amusing clip from a 90’s TV show: <a href="https://www.youtube.com/watch?v=vVX-PrBRtTY">www.youtube.com/watch?v=vVX-PrBRtTY</a>.</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/Robinson&amp;Winkel.png" alt="Robinson and Winkel Triple projections">
	<figcaption>Robinson projection (left) and Winkel Triple projection (right). Don't they look so much more natural? Source: <a href="https://en.wikipedia.org/wiki/List_of_map_projections">Wikipedia</a></figcaption>
</figure>

<p>There are many different projections out there, all with their own purpose. 
My personal favourite is the Winkel Triple. 
It is the official map of the National Geographic Society.
It is an elegant compromise between form and scale, in both the final representation and in the mathematics. 
A more general favourite is the Robinson Projection.
It was designed with an “artistic approach”. Unlike the other projections, instead of using equations, Arthur H. Robinson manually fixed the scale factors at 5° intervals.</p>

<hr>



        <hr>
        
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pretty State Machine Patterns in Rust (2016) (102 pts)]]></title>
            <link>https://hoverbear.org/blog/rust-state-machine-pattern/</link>
            <guid>43741051</guid>
            <pubDate>Sun, 20 Apr 2025 02:14:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hoverbear.org/blog/rust-state-machine-pattern/">https://hoverbear.org/blog/rust-state-machine-pattern/</a>, See on <a href="https://news.ycombinator.com/item?id=43741051">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <p>Lately I've been thinking a lot about the <em>patterns</em> and <em>structures</em> which we program with. It's really wonderful to start exploring a project and see familiar patterns and styles which you've already used before. It makes it easier to understand the project, and empowers you to start working on the project faster.</p>
<p>Sometimes you're working on a new project and realize that you need to do something in the same way as you did in another project. This <em>thing</em> might not be a functionality or a library, it might not be something which you can encode into some clever macro or small crate. Instead, it may be simply a pattern, or a structural concept which addresses a problem nicely.</p>
<p>One interesting pattern that is commonly applied to problems is that of the 'State Machine'. Let's take some time to consider what exactly we mean when we say that, and why they're interesting.</p>
<span id="continue-reading"></span>
<blockquote>
<p>Throughout this post you can run all examples in <a rel="noopener" target="_blank" href="https://play.rust-lang.org/">the playground</a>, I typically use 'Nightly' out of habit.</p>
</blockquote>
<h2 id="founding-our-concepts"><a href="#founding-our-concepts" aria-label="Anchor link for: founding-our-concepts">Founding Our Concepts</a></h2>
<p>There are a <strong>lot</strong> of resources and topical articles about state machines out there on the internet. Even more so, there are a lot of <strong>implementations</strong> of state machines.</p>
<p>Just to get to this web page you used one. You can model TCP as a state machine. You can model HTTP requests with one too. You can model any <em>regular</em> language, such as a regex, as a state machine. They're everywhere, hiding inside things we use every day.</p>
<p>So, a State Machine is any <strong>'machine'</strong> which has a set of <strong>'states'</strong> and <strong>'transitions'</strong> defined between them.</p>
<p>When we talk about a machine we're referring to the abstract concept of something which <em>does something</em>. For example, your 'Hello World!' function is a machine. It is started and eventually outputs what we expect it to. Some model which you use to interact with your database is just the same. We'll regard our most basic machine simply as a <code>struct</code> that can be created and destroyed.</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>struct</span> </span><span><span>Machine</span></span><span>;</span>
</span><span>
</span><span><span><span><span>fn</span> </span><span>main</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>  <span>let</span> my_machine <span>=</span> Machine<span>;</span> <span> Create.
</span></span></span></span><span><span><span>  <span> `my_machine` is destroyed when it falls out of scope below.
</span></span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>States are a way to reason about <em>where</em> a machine is in its process. For example, we can think about a bottle filling machine as an example. The machine is in a 'waiting' state when it is waiting for a new bottle. Once it detects a bottle it moves to the 'filling' state. Upon detecting the bottle is filled it enters the 'done' state. After the bottle is left the machine we return to the 'waiting' state.</p>
<p>A key takeaway here is that none of the states have any information relevant for the other states. The 'filling' state doesn't care how long the 'waiting' state waited. The 'done' state doesn't care about what rate the bottle was filled at. Each state has <em>discrete responsibilities and concerns</em>. The natural way to consider these <em>variants</em> is as an <code>enum</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>enum</span> <span>BottleFillerState</span> <span><span>{</span>
</span></span></span><span><span><span>  Waiting <span><span>{</span> waiting_time<span>:</span> <span>std<span>::</span></span><span>time<span>::</span></span>Duration </span><span><span>}</span></span><span>,</span>
</span></span></span><span><span><span>  Filling <span><span>{</span> rate<span>:</span> <span>usize</span> </span><span><span>}</span></span><span>,</span>
</span></span></span><span><span><span>  Done<span>,</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>struct</span> </span><span><span>BottleFiller</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>  <span>state</span><span>:</span> BottleFillerState,
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>Using an <code>enum</code> in this way means all the states are mutually exclusive, you can only be in one at a time. Rust's 'fat enums' allow us to have each of these states to carry data with them as well. As far as our current definition is concerned, everything is totally okay.</p>
<p>But there is a bit of a problem here. When we described our bottle filling machine above we described three transitions: <code>Waiting -&gt; Filling</code>, <code>Filling -&gt; Done</code>, and <code>Done -&gt; Waiting</code>. We never described <code>Waiting -&gt; Done</code> or <code>Done -&gt; Filling</code>, those don't make sense!</p>
<p>This brings us to the idea of transitions. One of the nicest things about a true state machine is we never have to worry about our bottle machine going from <code>Done -&gt; Filling</code>, for example. The state machine pattern should <strong>enforce</strong> that this can never happen. Ideally this would be done before we even start running our machine, at compile time.</p>
<p>Let's look again at the transitions we described for our bottle filler in a diagram:</p>
<pre><code><span>  +++++++++++   +++++++++++   ++++++++
</span><span>  |         |   |         |   |      |
</span><span>  | Waiting +--&gt;+ Filling +--&gt;+ Done |
</span><span>  |         |   |         |   |      |
</span><span>  ++++-++++-+   +++++++++++   +--+++++
</span><span>       ^                         |
</span><span>       +++++++++++++++++++++++++-+
</span></code></pre>
<p>As we can see here there are a finite number of states, and a finite number of transitions between these states. Now, it is possible to have a valid transition between each state and every other state, but in most cases this is not true.</p>
<p>This means moving between a state such as 'Waiting' to a state such as 'Filling' should have defined semantics. In our example this can be defined as "There is a bottle in place." In the case of a TCP stream it might be "We have received a FIN packet" which means we need to finish closing out the stream.</p>
<h2 id="determining-what-we-want"><a href="#determining-what-we-want" aria-label="Anchor link for: determining-what-we-want">Determining What We Want</a></h2>
<p>Now that we know what a state machine is, how do we represent them in Rust? First, let's think about what we <strong>want</strong> from some pattern.</p>
<p>Ideally, we'd like to see the following characteristics:</p>
<ul>
<li>Can only be in one state at a time.</li>
<li>Each state should have its own associated values if required.</li>
<li>Transitioning between states should have well defined semantics.</li>
<li>It should be possible to have some level of shared state.</li>
<li>Only explicitly defined transitions should be permitted.</li>
<li>Changing from one state to another should <strong>consume</strong> the state so it can no longer be used.</li>
<li>We shouldn't need to allocate memory for <strong>all</strong> states. No more than largest sized state certainly</li>
<li>Any error messages should be easy to understand.</li>
<li>We shouldn't need to resort to heap allocations to do this. Everything should be possible on the stack.</li>
<li>The type system should be harnessed to our greatest ability.</li>
<li>As many errors as possible should be at <strong>compile-time</strong>.</li>
</ul>
<p>So if we could have a design pattern which allowed for all these things it'd be truly fantastic. Having a pattern which allowed for most would be pretty good too.</p>
<h2 id="exploring-possible-implementation-options"><a href="#exploring-possible-implementation-options" aria-label="Anchor link for: exploring-possible-implementation-options">Exploring Possible Implementation Options</a></h2>
<p>With a type system as powerful and flexible as Rusts we should be able to represent this. The truth is: there are a number of ways to try, each has valuable characteristics, and each teaches us lessons.</p>
<h3 id="a-second-shot-with-enums"><a href="#a-second-shot-with-enums" aria-label="Anchor link for: a-second-shot-with-enums">A Second Shot with Enums</a></h3>
<p>As we saw above the most natural way to attempt this is an <code>enum</code>, but we noted already that you can't control which transitions are actually permitted in this case. So can we just wrap it? We sure can! Let's take a look:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>enum</span> <span>State</span> <span><span>{</span>
</span></span></span><span><span><span>    Waiting <span><span>{</span> waiting_time<span>:</span> <span>std<span>::</span></span><span>time<span>::</span></span>Duration </span><span><span>}</span></span><span>,</span>
</span></span></span><span><span><span>    Filling <span><span>{</span> rate<span>:</span> <span>usize</span> </span><span><span>}</span></span><span>,</span>
</span></span></span><span><span><span>    Done
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>struct</span> </span><span><span>StateMachine</span> </span><span><span><span>{</span> <span>state</span><span>:</span> State </span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>impl</span> </span><span><span>StateMachine</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>new</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Self</span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        StateMachine <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> <span>State<span>::</span></span>Waiting <span><span>{</span> waiting_time<span>:</span> <span>std<span>::</span></span><span>time<span>::</span></span><span>Duration<span>::</span></span>new<span><span>(</span><span>0</span><span>,</span> <span>0</span></span><span><span>)</span></span> </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>to_filling</span></span><span><span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        <span>self</span><span>.</span>state <span>=</span> <span>match</span> <span>self</span><span>.</span>state <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            </span></span></span></span></span></span><span><span><span><span><span><span>            <span>State<span>::</span></span>Waiting <span><span>{</span> <span>..</span> </span><span><span>}</span></span> <span>=&gt;</span> <span>State<span>::</span></span>Filling <span><span>{</span> rate<span>:</span> <span>1</span> </span><span><span>}</span></span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            </span></span></span></span></span></span><span><span><span><span><span><span>            <span>_</span> <span>=&gt;</span> <span>panic!</span><span><span>(</span><span><span>"</span>Invalid state transition!<span>"</span></span></span><span><span>)</span></span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span>    <span> ...
</span></span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span><span>fn</span> </span><span>main</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>let</span> <span>mut</span> state_machine <span>=</span> <span>StateMachine<span>::</span></span>new<span><span>(</span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>    state_machine<span>.</span><span>to_filling</span><span><span>(</span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>At first glance it seems okay. But notice some problems?</p>
<ul>
<li>Invalid transition errors happen at runtime, which is awful!</li>
<li>This only prevents invalid transitions <em>outside</em> of the module, since the private fields can be manipulated freely inside the module. For example, <code>state_machine.state = State::Done</code> is perfectly valid inside the module.</li>
<li>Every function we implement that works with the state has to include a match statement!</li>
</ul>
<p>However this does have some good characteristics:</p>
<ul>
<li>The memory required to represent the state machine is only the size of the largest state. This is because a fat enum is only as big as its biggest variant.</li>
<li>Everything happens on the stack.</li>
<li>Transitioning between states has well defined semantics... It either works or it crashes!</li>
</ul>
<p>Now you might be thinking "Hoverbear you could totally wrap the <code>to_filling()</code> output with a <code>Result&lt;T,E&gt;</code> or have an <code>InvalidState</code> variant!" But let's face it: That doesn't make things that much better, if at all. Even if we get rid of the runtime failures we still have to deal with a lot of clumsiness with the match statements and our errors would still only be found at runtime! Ugh! We can do better, I promise.</p>
<p>So let's keep looking!</p>
<h3 id="structures-with-transitions"><a href="#structures-with-transitions" aria-label="Anchor link for: structures-with-transitions">Structures With Transitions</a></h3>
<p>So what if we just used a set of structs? We could have them all implement traits which all states should share. We could use special functions that transitioned the type into the new type! How would it look?</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>//</span> This is some functionality shared by all of the states.
</span></span><span><span><span>trait</span> <span>SharedFunctionality</span> <span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>get_shared_value</span></span><span><span><span>(</span><span>&amp;</span><span>self</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>usize</span></span></span><span>;</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>struct</span> </span><span><span>Waiting</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>waiting_time</span><span>:</span> <span>std<span>::</span></span><span>time<span>::</span></span>Duration,
</span></span></span><span><span><span>    <span> Value shared by all states.
</span></span></span></span><span><span><span>    <span>shared_value</span><span>:</span> <span>usize</span>,
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span><span><span>impl</span> </span><span><span>Waiting</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>new</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Self</span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        Waiting <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            waiting_time<span>:</span> <span>std<span>::</span></span><span>time<span>::</span></span><span>Duration<span>::</span></span>new<span><span>(</span><span>0</span><span>,</span><span>0</span></span><span><span>)</span></span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            shared_value<span>:</span> <span>0</span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span>    <span> Consumes the value!
</span></span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>to_filling</span></span><span><span><span>(</span><span>self</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> Filling</span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        Filling <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            rate<span>:</span> <span>1</span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            shared_value<span>:</span> <span>0</span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span><span><span>impl</span> </span><span>SharedFunctionality <span>for</span></span><span> <span>Waiting</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>get_shared_value</span></span><span><span><span>(</span><span>&amp;</span><span>self</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>usize</span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        <span>self</span><span>.</span>shared_value
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>struct</span> </span><span><span>Filling</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>rate</span><span>:</span> <span>usize</span>,
</span></span></span><span><span><span>    <span> Value shared by all states.
</span></span></span></span><span><span><span>    <span>shared_value</span><span>:</span> <span>usize</span>,
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span><span><span>impl</span> </span><span>SharedFunctionality <span>for</span></span><span> <span>Filling</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>get_shared_value</span></span><span><span><span>(</span><span>&amp;</span><span>self</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>usize</span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        <span>self</span><span>.</span>shared_value
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> ...
</span></span><span>
</span><span><span><span><span>fn</span> </span><span>main</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>let</span> in_waiting_state <span>=</span> <span>Waiting<span>::</span></span>new<span><span>(</span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>    <span>let</span> in_filling_state <span>=</span> in_waiting_state<span>.</span><span>to_filling</span><span><span>(</span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>Gosh that's a buncha code! So the idea here was that all states have some common shared values along with their own specialized values. As you can see from the <code>to_filling()</code> function we can consume a given 'Waiting' state and transition it into a 'Filling' state. Let's do a little rundown:</p>
<ul>
<li>Transition errors are caught at compile time! For example you can't even create a <code>Filling</code> state accidentally without first starting with a <code>Waiting</code> state. (You could on purpose, but this is beside the matter.)</li>
<li>Transition enforcement happens everywhere.</li>
<li>When a transition between states is made the old value is <strong>consumed</strong> instead of just modified. We could have done this with the enum example above as well though.</li>
<li>We don't have to <code>match</code> all the time.</li>
<li>Memory consumption is still lean, at any given time the size is that of the state.</li>
</ul>
<p>There are some downsides though:</p>
<ul>
<li>There is a bunch of code repetition. You have to implement the same functions and traits for multiple structures.</li>
<li>It's not always clear what values are shared between all states and just one. Updating code later could be a pain due to this.</li>
<li>Since the size of the state is variable we end up needing to wrap this in an <code>enum</code> as above for it to be usable where the state machine is simply one component of a more complex system. Here's what this could look like:</li>
</ul>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>enum</span> <span>State</span> <span><span>{</span>
</span></span></span><span><span><span>    Waiting<span><span>(</span>Waiting</span><span><span>)</span></span><span>,</span>
</span></span></span><span><span><span>    Filling<span><span>(</span>Filling</span><span><span>)</span></span><span>,</span>
</span></span></span><span><span><span>    Done<span><span>(</span>Done</span><span><span>)</span></span><span>,</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span><span>fn</span> </span><span>main</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>let</span> in_waiting_state <span>=</span> <span>State<span>::</span></span>Waiting<span><span>(</span><span>Waiting<span>::</span></span>new<span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>    <span> This doesn't work since the `Waiting` struct is wrapped! We need to `match` to get it out.
</span></span></span></span><span><span><span>    <span>let</span> in_filling_state <span>=</span> <span>State<span>::</span></span>Filling<span><span>(</span>in_waiting_state<span>.</span><span>to_filling</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>As you can see, this isn't very ergonomic. We're getting closer to what we want though. The idea of moving between distinct types seems to be a good way forward! Before we go try something entirely different though, let's talk about a simple way to change our example that could enlighten further thinking.</p>
<p>The Rust standard library defines two highly related traits: <a rel="noopener" target="_blank" href="https://doc.rust-lang.org/std/convert/trait.From.html"><code>From</code></a> and <a rel="noopener" target="_blank" href="https://doc.rust-lang.org/std/convert/trait.Into.html"><code>Into</code></a> that are extremely useful and worth checking out. An important thing to note is that implementing one of these automatically implements the other. In general implementing <code>From</code> is preferable as it's a bit more flexible. We can implement them very easily for our above example like so:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>//</span> ...
</span></span><span><span><span>impl</span> </span><span><span><span>From</span><span>&lt;</span>Waiting<span>&gt;</span></span> <span>for</span></span><span> <span>Filling</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>from</span></span><span><span><span>(</span><span>val</span><span>:</span> Waiting</span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> Filling</span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        Filling <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            rate<span>:</span> <span>1</span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            shared_value<span>:</span> val<span>.</span>shared_value<span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span><span><span>//</span> ...
</span></span></code></pre>
<p>Not only does this give us a common function for transitioning, but it also is nice to read about in the source code! This reduces mental burden on us and makes it easier for readers to comprehend. <em>Instead of implementing custom functions we're just using a pattern already existing.</em> Building our pattern on top of already existing patterns is a great way forward.</p>
<p>So this is cool, but how do we deal with all this nasty code repetition and the repeating <code>shared_value</code> stuff? Let's explore a bit more!</p>
<h3 id="generically-sophistication"><a href="#generically-sophistication" aria-label="Anchor link for: generically-sophistication">Generically Sophistication</a></h3>
<p>In this adventure we'll combine lessons and ideas from the first two, along with a few new ideas, to get something more satisfying. The core of this is to harness the power of generics. Let's take a look at a fairly bare structure representing this:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>struct</span> </span><span><span><span>BottleFillingMachine</span><span><span>&lt;</span>S<span>&gt;</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>shared_value</span><span>:</span> <span>usize</span>,
</span></span></span><span><span><span>    <span>state</span><span>:</span> S
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> The following states can be the 'S' in StateMachine&lt;S&gt;
</span></span><span>
</span><span><span><span>struct</span> </span><span><span>Waiting</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>waiting_time</span><span>:</span> <span>std<span>::</span></span><span>time<span>::</span></span>Duration,
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>struct</span> </span><span><span>Filling</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>rate</span><span>:</span> <span>usize</span>,
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>struct</span> </span><span><span>Done</span></span><span>;</span>
</span></code></pre>
<p>So here we're actually building the state into the type signature of the <code>BottleFillingMachine</code> itself. A state machine in the 'Filling' state is <code>BottleFillingMachine&lt;Filling&gt;</code> which is just <strong>awesome</strong> since it means when we see it as part of an error message or something we know immediately what state the machine is in.</p>
<p>From there we can go ahead and implement <code>From&lt;T&gt;</code> for some of these specific generic variants like so:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>impl</span> </span><span><span><span>From</span><span>&lt;</span><span>BottleFillingMachine<span>&lt;</span>Waiting<span>&gt;</span></span><span>&gt;</span></span> <span>for</span></span><span> <span>BottleFillingMachine</span><span><span>&lt;</span>Filling<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>from</span></span><span><span><span>(</span><span>val</span><span>:</span> <span>BottleFillingMachine<span>&lt;</span>Waiting<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>BottleFillingMachine<span>&lt;</span>Filling<span>&gt;</span></span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        BottleFillingMachine <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            shared_value<span>:</span> val<span>.</span>shared_value<span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> Filling <span><span>{</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>                rate<span>:</span> <span>1</span><span>,</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>            </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>impl</span> </span><span><span><span>From</span><span>&lt;</span><span>BottleFillingMachine<span>&lt;</span>Filling<span>&gt;</span></span><span>&gt;</span></span> <span>for</span></span><span> <span>BottleFillingMachine</span><span><span>&lt;</span>Done<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>from</span></span><span><span><span>(</span><span>val</span><span>:</span> <span>BottleFillingMachine<span>&lt;</span>Filling<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>BottleFillingMachine<span>&lt;</span>Done<span>&gt;</span></span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        BottleFillingMachine <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            shared_value<span>:</span> val<span>.</span>shared_value<span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> Done<span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>Defining a starting state for the machine looks like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>impl</span> </span><span><span>BottleFillingMachine</span><span><span>&lt;</span>Waiting<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>new</span></span><span><span><span>(</span><span>shared_value</span><span>:</span> <span>usize</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Self</span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        BottleFillingMachine <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            shared_value<span>:</span> shared_value<span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> Waiting <span><span>{</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>                waiting_time<span>:</span> <span>std<span>::</span></span><span>time<span>::</span></span><span>Duration<span>::</span></span>new<span><span>(</span><span>0</span><span>,</span> <span>0</span></span><span><span>)</span></span><span>,</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>            </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>So how does it look to change between two states? Like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span><span>fn</span> </span><span>main</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>let</span> in_waiting <span>=</span> <span>BottleFillingMachine<span>::</span></span><span><span>&lt;</span>Waiting<span>&gt;</span></span><span><span>::</span></span>new<span><span>(</span><span>0</span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>    <span>let</span> in_filling <span>=</span> <span>BottleFillingMachine<span>::</span></span><span><span>&lt;</span>Filling<span>&gt;</span></span><span><span>::</span></span>from<span><span>(</span>in_waiting</span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>Alternatively if you're doing this inside of a function whose type signature restricts the possible outputs it might look like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span><span>fn</span> </span><span>transition_the_states</span></span><span><span><span>(</span><span>val</span><span>:</span> <span>BottleFillingMachine<span>&lt;</span>Waiting<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>BottleFillingMachine<span>&lt;</span>Filling<span>&gt;</span></span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    val<span>.</span><span>into</span><span><span>(</span></span><span><span>)</span></span> <span> Nice right?
</span></span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>What do the <strong>compile time</strong> error messages look like?</p>
<pre><code><span>error[E0277]: the trait bound `BottleFillingMachine&lt;Done&gt;: std::convert::From&lt;BottleFillingMachine&lt;Waiting&gt;&gt;` is not satisfied
</span><span>  --&gt; &lt;anon&gt;:50:22
</span><span>   |
</span><span>50 |     let in_filling = BottleFillingMachine::&lt;Done&gt;::from(in_waiting);
</span><span>   |                      ^^^^^^^^^^^^^^^^^^^^^^^^^^
</span><span>   |
</span><span>   = help: the following implementations were found:
</span><span>   = help:   &lt;BottleFillingMachine&lt;Filling&gt; as std::convert::From&lt;BottleFillingMachine&lt;Waiting&gt;&gt;&gt;
</span><span>   = help:   &lt;BottleFillingMachine&lt;Done&gt; as std::convert::From&lt;BottleFillingMachine&lt;Filling&gt;&gt;&gt;
</span><span>   = note: required by `std::convert::From::from`
</span></code></pre>
<p>It's pretty clear what's wrong from that. The error message even hints to us some valid transitions!</p>
<p>So what does this scheme give us?</p>
<ul>
<li>Transitions are ensured to be valid at compile time.</li>
<li>The error messages about invalid transitions are very understandable and even list valid options.</li>
<li>We have a 'parent' structure which can have traits and values associated with it that aren't repeated.</li>
<li>Once a transition is made the old state no longer exists, it is consumed. Indeed, the entire structure is consumed so if there are side effects of the transition on the parent (for example altering the average waiting time) we can't access stale values.</li>
<li>Memory consumption is lean and everything is on the stack.</li>
</ul>
<p>There are some downsides still:</p>
<ul>
<li>Our <code>From&lt;T&gt;</code> implementations suffer from a fair bit of "type noise". This is a highly minor concern though.</li>
<li>Each <code>BottleFillingMachine&lt;S&gt;</code> has a different size, with our previous example, so we'll need to use an enum. Because of our structure though we can do this in a way that doesn't completely suck.</li>
</ul>
<blockquote>
<p>You can play with this example <a rel="noopener" target="_blank" href="https://is.gd/CyuJlH"><strong>here</strong></a></p>
</blockquote>
<h3 id="getting-messy-with-the-parents"><a href="#getting-messy-with-the-parents" aria-label="Anchor link for: getting-messy-with-the-parents">Getting Messy With the Parents</a></h3>
<p>So how can we have some parent structure hold our state machine without it being a gigantic pain to interact with? Well, this circles us back around to the <code>enum</code> idea we had at first.</p>
<p>If you recall the primary problem with the <code>enum</code> example above was that we had to deal with no ability to enforce transitions, and the only errors we got were at runtime when we did try.</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>enum</span> <span>BottleFillingMachineWrapper</span> <span><span>{</span>
</span></span></span><span><span><span>    Waiting<span><span>(</span><span>BottleFillingMachine<span>&lt;</span>Waiting<span>&gt;</span></span></span><span><span>)</span></span><span>,</span>
</span></span></span><span><span><span>    Filling<span><span>(</span><span>BottleFillingMachine<span>&lt;</span>Filling<span>&gt;</span></span></span><span><span>)</span></span><span>,</span>
</span></span></span><span><span><span>    Done<span><span>(</span><span>BottleFillingMachine<span>&lt;</span>Done<span>&gt;</span></span></span><span><span>)</span></span><span>,</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span><span><span>struct</span> </span><span><span>Factory</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>bottle_filling_machine</span><span>:</span> BottleFillingMachineWrapper,
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span><span><span>impl</span> </span><span><span>Factory</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>new</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Self</span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        Factory <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            bottle_filling_machine<span>:</span> <span>BottleFillingMachineWrapper<span>::</span></span>Waiting<span><span>(</span><span>BottleFillingMachine<span>::</span></span>new<span><span>(</span><span>0</span></span><span><span>)</span></span></span><span><span>)</span></span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>At this point your first reaction is likely "Gosh, Hoverbear, look at that awful and long type signature!" You're quite right! Frankly it's rather long, but I picked long, explanatory type names! You'll be able to use all your favorite arcane abbreviations and type aliases in your own code. Have at!</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>impl</span> </span><span><span>BottleFillingMachineWrapper</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>step</span></span><span><span><span>(</span><span>mut</span> <span>self</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Self</span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        <span>match</span> <span>self</span> <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            <span>BottleFillingMachineWrapper<span>::</span></span>Waiting<span><span>(</span>val</span><span><span>)</span></span> <span>=&gt;</span> <span>BottleFillingMachineWrapper<span>::</span></span>Filling<span><span>(</span>val<span>.</span><span>into</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            <span>BottleFillingMachineWrapper<span>::</span></span>Filling<span><span>(</span>val</span><span><span>)</span></span> <span>=&gt;</span> <span>BottleFillingMachineWrapper<span>::</span></span>Done<span><span>(</span>val<span>.</span><span>into</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            <span>BottleFillingMachineWrapper<span>::</span></span>Done<span><span>(</span>val</span><span><span>)</span></span> <span>=&gt;</span> <span>BottleFillingMachineWrapper<span>::</span></span>Waiting<span><span>(</span>val<span>.</span><span>into</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span><span>fn</span> </span><span>main</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>let</span> <span>mut</span> the_factory <span>=</span> <span>Factory<span>::</span></span>new<span><span>(</span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>    the_factory<span>.</span>bottle_filling_machine <span>=</span> the_factory<span>.</span>bottle_filling_machine<span>.</span><span>step</span><span><span>(</span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>Again you may notice that this works by <strong>consumption</strong> not mutation. Using <code>match</code> the way we are above <em>moves</em> <code>val</code> so that it can be used with <code>.into()</code> which we've already determined should consume the state. If you'd really like to use mutation you can consider having your states <code>#[derive(Clone)]</code> or even <code>Copy</code>, but that's your call.</p>
<p>Despite this being a bit less ergonomic and pleasant to work with than we might want we still get strongly enforced state transitions and all the guarantees that come with them.</p>
<p>One thing you will notice is this scheme <strong>does</strong> force you to handle all potential states when manipulating the machine, and that makes sense. You are reaching into a structure with a state machine and manipulating it, you need to have defined actions for each state that it is in.</p>
<p>Or you can just <code>panic!()</code> if that's what you really want. But if you just wanted to <code>panic!()</code> then why didn't you just use the first attempt?</p>
<blockquote>
<p>You can see a fully worked example of this Factory example <a rel="noopener" target="_blank" href="https://is.gd/s03IaQ"><strong>here</strong></a></p>
</blockquote>
<h2 id="worked-examples"><a href="#worked-examples" aria-label="Anchor link for: worked-examples">Worked Examples</a></h2>
<p>This is the kind of thing it's always nice to have some examples for. So below I've put together a couple worked examples with comments for you to explore.</p>
<h3 id="three-state-two-transitions"><a href="#three-state-two-transitions" aria-label="Anchor link for: three-state-two-transitions">Three State, Two Transitions</a></h3>
<p>This example is very similar to the Bottle Filling Machine above, but instead it <strong>actually</strong> does work, albeit trivial work. It takes a string and returns the number of words in it.</p>
<blockquote>
<p><a rel="noopener" target="_blank" href="https://is.gd/4ITDyV">Playground link</a></p>
</blockquote>
<pre data-lang="rust"><code data-lang="rust"><span><span><span><span>fn</span> </span><span>main</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span> The `&lt;StateA&gt;` is implied here. We don't need to add type annotations!
</span></span></span></span><span><span><span>    <span>let</span> in_state_a <span>=</span> <span>StateMachine<span>::</span></span>new<span><span>(</span><span><span>"</span>Blah blah blah<span>"</span></span><span>.</span><span>into</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span> This is okay here. But later once we've changed state it won't work anymore.
</span></span></span></span><span><span><span>    in_state_a<span>.</span>some_unrelated_value<span>;</span>
</span></span></span><span><span><span>    <span>println!</span><span><span>(</span></span><span><span><span>"</span>Starting Value: <span>{}</span><span>"</span></span></span><span><span>,</span> in_state_a<span>.</span>state<span>.</span>start_value<span>)</span></span><span>;</span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span> Transition to the new state. This consumes the old state.
</span></span></span></span><span><span><span>    <span> Here we need type annotations (since not all StateMachines are linear in their state).
</span></span></span></span><span><span><span>    <span>let</span> in_state_b <span>=</span> <span>StateMachine<span>::</span></span><span><span>&lt;</span>StateB<span>&gt;</span></span><span><span>::</span></span>from<span><span>(</span>in_state_a</span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span> This doesn't work! The value is moved when we transition!
</span></span></span></span><span><span><span>    <span> in_state_a.some_unrelated_value;
</span></span></span></span><span><span><span>    <span> Instead, we can use the existing value.
</span></span></span></span><span><span><span>    in_state_b<span>.</span>some_unrelated_value<span>;</span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span>println!</span><span><span>(</span></span><span><span><span>"</span>Interm Value: <span>{:?}</span><span>"</span></span></span><span><span>,</span> in_state_b<span>.</span>state<span>.</span>interm_value<span>)</span></span><span>;</span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span> And our final state.
</span></span></span></span><span><span><span>    <span>let</span> in_state_c <span>=</span> <span>StateMachine<span>::</span></span><span><span>&lt;</span>StateC<span>&gt;</span></span><span><span>::</span></span>from<span><span>(</span>in_state_b</span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span> This doesn't work either! The state doesn't even contain this value.
</span></span></span></span><span><span><span>    <span> in_state_c.state.start_value;
</span></span></span></span><span><span><span>
</span></span></span><span><span><span>    <span>println!</span><span><span>(</span></span><span><span><span>"</span>Final state: <span>{}</span><span>"</span></span></span><span><span>,</span> in_state_c<span>.</span>state<span>.</span>final_value<span>)</span></span><span>;</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> Here is our pretty state machine.
</span></span><span><span><span>struct</span> </span><span><span><span>StateMachine</span><span><span>&lt;</span>S<span>&gt;</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>some_unrelated_value</span><span>:</span> <span>usize</span>,
</span></span></span><span><span><span>    <span>state</span><span>:</span> S,
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> It starts, predictably, in `StateA`
</span></span><span><span><span>impl</span> </span><span><span>StateMachine</span><span><span>&lt;</span>StateA<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>new</span></span><span><span><span>(</span><span>val</span><span>:</span> String</span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Self</span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        StateMachine <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            some_unrelated_value<span>:</span> <span>0</span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> <span>StateA<span>::</span></span>new<span><span>(</span>val</span><span><span>)</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> State A starts the machine with a string.
</span></span><span><span><span>struct</span> </span><span><span>StateA</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>start_value</span><span>:</span> String,
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span><span><span>impl</span> </span><span><span>StateA</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>new</span></span><span><span><span>(</span><span>start_value</span><span>:</span> String</span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Self</span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        StateA <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            start_value<span>:</span> start_value<span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> State B goes and breaks up that String into words.
</span></span><span><span><span>struct</span> </span><span><span>StateB</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>interm_value</span><span>:</span> <span><span>Vec</span><span>&lt;</span><span>String</span><span>&gt;</span></span>,
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span><span><span>impl</span> </span><span><span><span>From</span><span>&lt;</span><span>StateMachine<span>&lt;</span>StateA<span>&gt;</span></span><span>&gt;</span></span> <span>for</span></span><span> <span>StateMachine</span><span><span>&lt;</span>StateB<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>from</span></span><span><span><span>(</span><span>val</span><span>:</span> <span>StateMachine<span>&lt;</span>StateA<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>StateMachine<span>&lt;</span>StateB<span>&gt;</span></span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        StateMachine <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            some_unrelated_value<span>:</span> val<span>.</span>some_unrelated_value<span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> StateB <span><span>{</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>                interm_value<span>:</span> val<span>.</span>state<span>.</span>start_value<span>.</span><span>split</span><span><span>(</span><span><span>"</span> <span>"</span></span></span><span><span>)</span></span><span>.</span><span>map</span><span><span>(</span><span><span><span>|</span></span></span><span><span><span>x</span><span>|</span></span> </span><span>x<span>.</span><span>into</span><span><span>(</span></span><span><span>)</span></span></span></span><span><span>)</span></span><span>.</span><span>collect</span><span><span>(</span></span><span><span>)</span></span><span>,</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>            </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> Finally, StateC gives us the length of the vector, or the word count.
</span></span><span><span><span>struct</span> </span><span><span>StateC</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>final_value</span><span>:</span> <span>usize</span>,
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span><span><span>impl</span> </span><span><span><span>From</span><span>&lt;</span><span>StateMachine<span>&lt;</span>StateB<span>&gt;</span></span><span>&gt;</span></span> <span>for</span></span><span> <span>StateMachine</span><span><span>&lt;</span>StateC<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>from</span></span><span><span><span>(</span><span>val</span><span>:</span> <span>StateMachine<span>&lt;</span>StateB<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>StateMachine<span>&lt;</span>StateC<span>&gt;</span></span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        StateMachine <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            some_unrelated_value<span>:</span> val<span>.</span>some_unrelated_value<span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> StateC <span><span>{</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>                final_value<span>:</span> val<span>.</span>state<span>.</span>interm_value<span>.</span><span>len</span><span><span>(</span></span><span><span>)</span></span><span>,</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>            </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<h3 id="a-raft-example"><a href="#a-raft-example" aria-label="Anchor link for: a-raft-example">A Raft Example</a></h3>
<p>If you've followed my posts for awhile you may know I rather enjoy thinking about Raft. Raft, and a discussion with <a rel="noopener" target="_blank" href="https://twitter.com/Argorak"><strong>@argorak</strong></a> were the primary motivators behind all of this research.</p>
<p>Raft is a bit more complex than the above examples as it does not just have linear states where <code>A-&gt;B-&gt;C</code>. Here is the transition diagram:</p>
<pre><code><span>++++++++++-+    ++++++++++--+    +++++++--+
</span><span>|          ++++-&gt;           |    |        |
</span><span>| Follower |    | Candidate ++++-&gt; Leader |
</span><span>|          &lt;+++-+           |    |        |
</span><span>+++++++--^-+    ++++++++++--+    +-++++++++
</span><span>         |                         |
</span><span>         +++++++++++++++++++++++++-+
</span></code></pre>
<blockquote>
<p><a rel="noopener" target="_blank" href="https://is.gd/HDZeGR">Playground link</a></p>
</blockquote>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>//</span> You can play around in this function.
</span></span><span><span><span><span>fn</span> </span><span>main</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>let</span> is_follower <span>=</span> <span>Raft<span>::</span></span>new<span><span>(</span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>    <span> Raft typically comes in groups of 3, 5, or 7. Just 1 for us. :)
</span></span></span></span><span><span><span>
</span></span></span><span><span><span>    <span> Simulate this node timing out first.
</span></span></span></span><span><span><span>    <span>let</span> is_candidate <span>=</span> <span>Raft<span>::</span></span><span><span>&lt;</span>Candidate<span>&gt;</span></span><span><span>::</span></span>from<span><span>(</span>is_follower</span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span> It wins! How unexpected.
</span></span></span></span><span><span><span>    <span>let</span> is_leader <span>=</span> <span>Raft<span>::</span></span><span><span>&lt;</span>Leader<span>&gt;</span></span><span><span>::</span></span>from<span><span>(</span>is_candidate</span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span> Then it fails and rejoins later, becoming a Follower again.
</span></span></span></span><span><span><span>    <span>let</span> is_follower_again <span>=</span> <span>Raft<span>::</span></span><span><span>&lt;</span>Follower<span>&gt;</span></span><span><span>::</span></span>from<span><span>(</span>is_leader</span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span> And goes up for election...
</span></span></span></span><span><span><span>    <span>let</span> is_candidate_again <span>=</span> <span>Raft<span>::</span></span><span><span>&lt;</span>Candidate<span>&gt;</span></span><span><span>::</span></span>from<span><span>(</span>is_follower_again</span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span> But this time it fails!
</span></span></span></span><span><span><span>    <span>let</span> is_follower_another_time <span>=</span> <span>Raft<span>::</span></span><span><span>&lt;</span>Follower<span>&gt;</span></span><span><span>::</span></span>from<span><span>(</span>is_candidate_again</span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span>
</span><span><span><span>//</span> This is our state machine.
</span></span><span><span><span>struct</span> </span><span><span><span>Raft</span><span><span>&lt;</span>S<span>&gt;</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span> ... Shared Values
</span></span></span></span><span><span><span>    <span>state</span><span>:</span> S
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> The three cluster states a Raft node can be in
</span></span><span>
</span><span><span><span>//</span> If the node is the Leader of the cluster services requests and replicates its state.
</span></span><span><span><span>struct</span> </span><span><span>Leader</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span> ... Specific State Values
</span></span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> If it is a Candidate it is attempting to become a leader due to timeout or initialization.
</span></span><span><span><span>struct</span> </span><span><span>Candidate</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span> ... Specific State Values
</span></span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> Otherwise the node is a follower and is replicating state it receives.
</span></span><span><span><span>struct</span> </span><span><span>Follower</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span> ... Specific State Values
</span></span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> Raft starts in the Follower state
</span></span><span><span><span>impl</span> </span><span><span>Raft</span><span><span>&lt;</span>Follower<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>new</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Self</span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        </span></span></span></span></span><span><span><span><span><span>        Raft <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            </span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> Follower <span><span>{</span>  </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> The following are the defined transitions between states.
</span></span><span>
</span><span><span><span>//</span> When a follower timeout triggers it begins to campaign
</span></span><span><span><span>impl</span> </span><span><span><span>From</span><span>&lt;</span><span>Raft<span>&lt;</span>Follower<span>&gt;</span></span><span>&gt;</span></span> <span>for</span></span><span> <span>Raft</span><span><span>&lt;</span>Candidate<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>from</span></span><span><span><span>(</span><span>val</span><span>:</span> <span>Raft<span>&lt;</span>Follower<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Raft<span>&lt;</span>Candidate<span>&gt;</span></span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        </span></span></span></span></span><span><span><span><span><span>        Raft <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            </span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> Candidate <span><span>{</span>  </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> If it doesn't receive a majority of votes it loses and becomes a follower again.
</span></span><span><span><span>impl</span> </span><span><span><span>From</span><span>&lt;</span><span>Raft<span>&lt;</span>Candidate<span>&gt;</span></span><span>&gt;</span></span> <span>for</span></span><span> <span>Raft</span><span><span>&lt;</span>Follower<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>from</span></span><span><span><span>(</span><span>val</span><span>:</span> <span>Raft<span>&lt;</span>Candidate<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Raft<span>&lt;</span>Follower<span>&gt;</span></span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        </span></span></span></span></span><span><span><span><span><span>        Raft <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            </span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> Follower <span><span>{</span>  </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> If it wins it becomes the leader.
</span></span><span><span><span>impl</span> </span><span><span><span>From</span><span>&lt;</span><span>Raft<span>&lt;</span>Candidate<span>&gt;</span></span><span>&gt;</span></span> <span>for</span></span><span> <span>Raft</span><span><span>&lt;</span>Leader<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>from</span></span><span><span><span>(</span><span>val</span><span>:</span> <span>Raft<span>&lt;</span>Candidate<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Raft<span>&lt;</span>Leader<span>&gt;</span></span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        </span></span></span></span></span><span><span><span><span><span>        Raft <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            </span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> Leader <span><span>{</span>  </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> If the leader becomes disconnected it may rejoin to discover it is no longer leader
</span></span><span><span><span>impl</span> </span><span><span><span>From</span><span>&lt;</span><span>Raft<span>&lt;</span>Leader<span>&gt;</span></span><span>&gt;</span></span> <span>for</span></span><span> <span>Raft</span><span><span>&lt;</span>Follower<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>from</span></span><span><span><span>(</span><span>val</span><span>:</span> <span>Raft<span>&lt;</span>Leader<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Raft<span>&lt;</span>Follower<span>&gt;</span></span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        </span></span></span></span></span><span><span><span><span><span>        Raft <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            </span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> Follower <span><span>{</span>  </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<h2 id="alternatives-from-feedback"><a href="#alternatives-from-feedback" aria-label="Anchor link for: alternatives-from-feedback">Alternatives From Feedback</a></h2>
<p>I saw an interesting comment by <a rel="noopener" target="_blank" href="https://www.reddit.com/r/rust/comments/57ccds/pretty_state_machine_patterns_in_rust/d8rhwq4">I-impv on Reddit</a> showing off <a rel="noopener" target="_blank" href="https://play.rust-lang.org/?gist=ee3e4df093c136ced7b394dc7ffb78e1&amp;version=stable&amp;backtrace=0">this approach based on our examples above</a>. Here's what they had to say about it:</p>
<blockquote>
<p>I like the way you did it. I am working on a fairly complex FSM myself currently and did it slightly different.</p>
<p>Some things I did different:</p>
<ul>
<li>I also modeled the input for the state machine. That way you can model your transitions as a match over (State, Event) every invalid combination is handled by the 'default' pattern</li>
<li>Instead of using panic for invalid transitions I used a Failure state, So every invalid combination transitions to that Failure state</li>
</ul>
</blockquote>
<p>I really like the idea of modeling the input in the transitions!</p>
<h2 id="closing-thoughts"><a href="#closing-thoughts" aria-label="Anchor link for: closing-thoughts">Closing Thoughts</a></h2>
<p>Rust lets us represent State Machines in a fairly good way. In an ideal situation we'd be able to make <code>enum</code>s with restricted transitions between variants, but that's not the case. Instead, we can harness the power of generics and the ownership system to create something expressive, safe, and understandable.</p>
<p>If you have any feedback or suggestions on this article I'd suggest checking out the footer of this page for contact details. I also hang out on Mozilla's IRC as Hoverbear.</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Novel color via stimulation of individual photoreceptors at population scale (134 pts)]]></title>
            <link>https://www.science.org/doi/10.1126/sciadv.adu1052</link>
            <guid>43741013</guid>
            <pubDate>Sun, 20 Apr 2025 02:03:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/doi/10.1126/sciadv.adu1052">https://www.science.org/doi/10.1126/sciadv.adu1052</a>, See on <a href="https://news.ycombinator.com/item?id=43741013">Hacker News</a></p>
Couldn't get https://www.science.org/doi/10.1126/sciadv.adu1052: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[A unique sound alleviates motion sickness (163 pts)]]></title>
            <link>https://www.nagoya-u.ac.jp/researchinfo/result-en/2025/04/20250408-01.html</link>
            <guid>43740021</guid>
            <pubDate>Sat, 19 Apr 2025 22:35:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nagoya-u.ac.jp/researchinfo/result-en/2025/04/20250408-01.html">https://www.nagoya-u.ac.jp/researchinfo/result-en/2025/04/20250408-01.html</a>, See on <a href="https://news.ycombinator.com/item?id=43740021">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
           <p>A research group led by Takumi Kagawa and Masashi Kato at Nagoya University Graduate School of Medicine has discovered that using “a unique sound stimulation technology”—a device that stimulates the inner ear with a specific wavelength of sound—reduces motion sickness. Even a single minute of stimulation reduced the staggering and discomfort felt by people that read in a moving vehicle. The results, published in <em>Environmental Health and Preventive Medicine</em>, suggest a simple and effective way to treat this common disorder.</p>

<p>“Our study demonstrated that short-term stimulation using a unique sound called 'sound spice®' alleviates symptoms of motion sickness, such as nausea and dizziness,” Kagawa said. “The effective sound level falls within the range of everyday environmental noise exposure, suggesting that the sound technology is both effective and safe.”</p>

<p>The discovery is an important expansion of recent findings about sound and its effect on the inner ear. Increasing evidence has suggested that stimulating the part of the inner ear associated with balance using a unique sound can potentially improve balance. Using a mouse model and humans, the researchers identified a unique sound at 100 Hz as being the optimal frequency.</p>

<p>“Vibrations at the unique sound stimulate the otolithic organs in the inner ear, which detect linear acceleration and gravity,” Kato explained. “This suggests that a unique sound stimulation can broadly activate the vestibular system, which is responsible for maintaining balance and spatial orientation.”</p>

<p>To test the effectiveness of the devices, they recruited voluntary participants who were exposed to the unique sound. Following the stimulation, motion sickness was induced by a swing, a driving simulator, or riding in a car. The researchers used postural control, ECG readings, and Motion Sickness Assessment Questionnaire results to assess the effectiveness of the stimulation.</p>

<p>Exposure to the unique sound before being exposed to the driving simulator enhanced sympathetic nerve activation. The researchers found symptoms such as “lightheadedness” and “nausea,” which are often seen with motion sickness, were alleviated.</p>

<p>“These results suggest that activation of sympathetic nerves, which are often dysregulated in motion sickness, was objectively improved by the unique sound exposure,” Kato said.</p>

<p>“The health risk of short-term exposure to our unique sound is minimal,” Kagawa said. “Given that the stimulus level is well below workplace noise safety standards, this stimulation is expected to be safe when used properly.”</p>

<p>Their results suggest a safe and effective way to improve motion sickness, potentially offering help to millions of sufferers. The researchers plan to further develop the technology with the aim of practical application for a variety of travel situations including air and sea travel.</p>

<p>The study, “Just 1-min exposure to a pure tone at 100 Hz with daily exposable sound pressure levels may improve motion sickness,” was published in <em>Environmental Health and Preventive Medicine</em> on March 25, 2025, at DOI: <a href="https://www.jstage.jst.go.jp/article/ehpm/30/0/30_24-00247/_article" target="_blank" rel="noopener noreferrer">10.1265/ehpm.24-00247</a>.</p>

<p><strong>Authors:</strong><br>Yishuo Gu, Nobutaka Ohgami, Tingchao He, Takumi Kagawa, Fitri Kurniasari, Keming Tong, Xiang Li, Akira Tazaki, Kodai Takeda, Masahiro Mouri, Masashi Kato</p>

<p><strong>Media Contact:</strong><br>Matthew Coslett<br>International Communications Office, Nagoya University<br>Email: icomm_research@t.mail.nagoya-u.ac.jp</p>

<p>Top image: Induction of motion sickness using a driving simulator (credit: Masashi Kato)</p>
          
          

		  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ultrathink is a Claude Code a magic word (104 pts)]]></title>
            <link>https://simonwillison.net/2025/Apr/19/claude-code-best-practices/</link>
            <guid>43739997</guid>
            <pubDate>Sat, 19 Apr 2025 22:31:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2025/Apr/19/claude-code-best-practices/">https://simonwillison.net/2025/Apr/19/claude-code-best-practices/</a>, See on <a href="https://news.ycombinator.com/item?id=43739997">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



<p><strong><a href="https://www.anthropic.com/engineering/claude-code-best-practices">Claude Code: Best practices for agentic coding</a></strong> (<a href="https://twitter.com/HamelHusain/status/1913702157108592719" title="@HamelHusain">via</a>) Extensive new documentation from Anthropic on how to get the best results out of their <a href="https://github.com/anthropics/claude-code">Claude Code</a> CLI coding agent tool, which includes this fascinating tip:</p>
<blockquote>
<p>We recommend using the word "think" to trigger extended thinking mode, which gives Claude additional computation time to evaluate alternatives more thoroughly. These specific phrases are mapped directly to increasing levels of thinking budget in the system: "think" &lt; "think hard" &lt; "think harder" &lt; "ultrathink." Each level allocates progressively more thinking budget for Claude to use.</p>
</blockquote>
<p>Apparently <strong>ultrathink</strong> is a magic word!</p>
<p>I was curious if this was a feature of the Claude model itself or Claude Code in particular. Claude Code isn't open source but you can view the obfuscated JavaScript for it, and make it a tiny bit less obfuscated by running it through <a href="https://prettier.io/">Prettier</a>. With <a href="https://claude.ai/share/77c398ec-6a8b-4390-91d3-6e9f0403916e">Claude's help</a> I used this recipe:</p>
<pre><code>mkdir -p /tmp/claude-code-examine
cd /tmp/claude-code-examine
npm init -y
npm install @anthropic-ai/claude-code
cd node_modules/@anthropic-ai/claude-code
npx prettier --write cli.js
</code></pre>
<p>Then used <a href="https://github.com/BurntSushi/ripgrep">ripgrep</a> to search for "ultrathink":</p>
<pre><code>rg ultrathink -C 30
</code></pre>
<p>And found this chunk of code:</p>
<pre><span>let</span> <span>B</span> <span>=</span> <span>W</span><span>.</span><span>message</span><span>.</span><span>content</span><span>.</span><span>toLowerCase</span><span>(</span><span>)</span><span>;</span>
<span>if</span> <span>(</span>
  <span>B</span><span>.</span><span>includes</span><span>(</span><span>"think harder"</span><span>)</span> <span>||</span>
  <span>B</span><span>.</span><span>includes</span><span>(</span><span>"think intensely"</span><span>)</span> <span>||</span>
  <span>B</span><span>.</span><span>includes</span><span>(</span><span>"think longer"</span><span>)</span> <span>||</span>
  <span>B</span><span>.</span><span>includes</span><span>(</span><span>"think really hard"</span><span>)</span> <span>||</span>
  <span>B</span><span>.</span><span>includes</span><span>(</span><span>"think super hard"</span><span>)</span> <span>||</span>
  <span>B</span><span>.</span><span>includes</span><span>(</span><span>"think very hard"</span><span>)</span> <span>||</span>
  <span>B</span><span>.</span><span>includes</span><span>(</span><span>"ultrathink"</span><span>)</span>
<span>)</span>
  <span>return</span> <span>(</span>
    <span>l1</span><span>(</span><span>"tengu_thinking"</span><span>,</span> <span>{</span> <span>tokenCount</span>: <span>31999</span><span>,</span> <span>messageId</span>: <span>Z</span><span>,</span> <span>provider</span>: <span>G</span> <span>}</span><span>)</span><span>,</span>
    <span>31999</span>
  <span>)</span><span>;</span>
<span>if</span> <span>(</span>
  <span>B</span><span>.</span><span>includes</span><span>(</span><span>"think about it"</span><span>)</span> <span>||</span>
  <span>B</span><span>.</span><span>includes</span><span>(</span><span>"think a lot"</span><span>)</span> <span>||</span>
  <span>B</span><span>.</span><span>includes</span><span>(</span><span>"think deeply"</span><span>)</span> <span>||</span>
  <span>B</span><span>.</span><span>includes</span><span>(</span><span>"think hard"</span><span>)</span> <span>||</span>
  <span>B</span><span>.</span><span>includes</span><span>(</span><span>"think more"</span><span>)</span> <span>||</span>
  <span>B</span><span>.</span><span>includes</span><span>(</span><span>"megathink"</span><span>)</span>
<span>)</span>
  <span>return</span> <span>(</span>
    <span>l1</span><span>(</span><span>"tengu_thinking"</span><span>,</span> <span>{</span> <span>tokenCount</span>: <span>1e4</span><span>,</span> <span>messageId</span>: <span>Z</span><span>,</span> <span>provider</span>: <span>G</span> <span>}</span><span>)</span><span>,</span> <span>1e4</span>
  <span>)</span><span>;</span>
<span>if</span> <span>(</span><span>B</span><span>.</span><span>includes</span><span>(</span><span>"think"</span><span>)</span><span>)</span>
  <span>return</span> <span>(</span>
    <span>l1</span><span>(</span><span>"tengu_thinking"</span><span>,</span> <span>{</span> <span>tokenCount</span>: <span>4000</span><span>,</span> <span>messageId</span>: <span>Z</span><span>,</span> <span>provider</span>: <span>G</span> <span>}</span><span>)</span><span>,</span>
    <span>4000</span>
  <span>)</span><span>;</span></pre>

<p>So yeah, it looks like "ultrathink" is a Claude Code feature - presumably that 31999 is a number that affects the token <a href="https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#implementing-extended-thinking">thinking budget</a>, especially since "megathink" maps to 1e4 tokens (10,000) and just plain "think" maps to 4,000.</p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Electromagnetism as a Purely Geometric Theory (120 pts)]]></title>
            <link>https://iopscience.iop.org/article/10.1088/1742-6596/2987/1/012001</link>
            <guid>43739529</guid>
            <pubDate>Sat, 19 Apr 2025 21:14:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://iopscience.iop.org/article/10.1088/1742-6596/2987/1/012001">https://iopscience.iop.org/article/10.1088/1742-6596/2987/1/012001</a>, See on <a href="https://news.ycombinator.com/item?id=43739529">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <h2>We apologize for the inconvenience...</h2>
      <p>To ensure we keep this website safe, please can you confirm you are a human by ticking the box below. </p>
      <p>If you are unable to complete the above request please contact us using the below link, providing a screenshot of your experience.</p>
      <p>
        <a href="https://ioppublishing.org/contacts/">https://ioppublishing.org/contacts/</a>
      </p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I built an AI that turns GitHub codebases into easy tutorials (434 pts)]]></title>
            <link>https://github.com/The-Pocket/Tutorial-Codebase-Knowledge</link>
            <guid>43739456</guid>
            <pubDate>Sat, 19 Apr 2025 21:04:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">https://github.com/The-Pocket/Tutorial-Codebase-Knowledge</a>, See on <a href="https://news.ycombinator.com/item?id=43739456">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Turns Codebase into Easy Tutorial with AI</h2><a id="user-content-turns-codebase-into-easy-tutorial-with-ai" aria-label="Permalink: Turns Codebase into Easy Tutorial with AI" href="#turns-codebase-into-easy-tutorial-with-ai"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6cd0120cc4c5ac11d28b2c60f76033b52db98dac641de3b2644bb054b449d60c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667"><img src="https://camo.githubusercontent.com/6cd0120cc4c5ac11d28b2c60f76033b52db98dac641de3b2644bb054b449d60c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"></a></p>
<blockquote>
<p dir="auto"><em>Ever stared at a new codebase written by others feeling completely lost? This tutorial shows you how to build an AI agent that analyzes GitHub repositories and creates beginner-friendly tutorials explaining exactly how the code works.</em></p>
</blockquote>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/assets/banner.png"><img src="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/raw/main/assets/banner.png" width="800"></a>
</p>
<p dir="auto">This is a tutorial project of <a href="https://github.com/The-Pocket/PocketFlow">Pocket Flow</a>, a 100-line LLM framework. It crawls GitHub repositories and build a knowledge base from the code. It analyzes entire codebases to identify core abstractions and how they interact, and transforms complex code into beginner-friendly tutorials with clear visualizations.</p>
<ul dir="auto">
<li>
<p dir="auto">Check out the <a href="https://youtu.be/AFY67zOpbSo" rel="nofollow">YouTube Development Tutorial</a> for more!</p>
</li>
<li>
<p dir="auto">Check out the <a href="https://zacharyhuang.substack.com/p/ai-codebase-knowledge-builder-full" rel="nofollow">Substack Post Tutorial</a> for more!</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">⭐ Example Results for Popular GitHub Repositories!</h2><a id="user-content--example-results-for-popular-github-repositories" aria-label="Permalink: ⭐ Example Results for Popular GitHub Repositories!" href="#-example-results-for-popular-github-repositories"></a></p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/assets/example.png"><img src="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/raw/main/assets/example.png" width="600"></a>
</p>
<p dir="auto">🤯 All these tutorials are generated <strong>entirely by AI</strong> by crawling the GitHub repo!</p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://the-pocket.github.io/Tutorial-Codebase-Knowledge/AutoGen%20Core" rel="nofollow">AutoGen Core</a> - Build AI teams that talk, think, and solve problems together like coworkers!</p>
</li>
<li>
<p dir="auto"><a href="https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Browser%20Use" rel="nofollow">Browser Use</a> - Let AI surf the web for you, clicking buttons and filling forms like a digital assistant!</p>
</li>
<li>
<p dir="auto"><a href="https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Celery" rel="nofollow">Celery</a> - Supercharge your app with background tasks that run while you sleep!</p>
</li>
<li>
<p dir="auto"><a href="https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Click" rel="nofollow">Click</a> - Turn Python functions into slick command-line tools with just a decorator!</p>
</li>
<li>
<p dir="auto"><a href="https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Codex" rel="nofollow">Codex</a> - Turn plain English into working code with this AI terminal wizard!</p>
</li>
<li>
<p dir="auto"><a href="https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Crawl4AI" rel="nofollow">Crawl4AI</a> - Train your AI to extract exactly what matters from any website!</p>
</li>
<li>
<p dir="auto"><a href="https://the-pocket.github.io/Tutorial-Codebase-Knowledge/CrewAI" rel="nofollow">CrewAI</a> - Assemble a dream team of AI specialists to tackle impossible problems!</p>
</li>
<li>
<p dir="auto"><a href="https://the-pocket.github.io/Tutorial-Codebase-Knowledge/DSPy" rel="nofollow">DSPy</a> - Build LLM apps like Lego blocks that optimize themselves!</p>
</li>
<li>
<p dir="auto"><a href="https://the-pocket.github.io/Tutorial-Codebase-Knowledge/FastAPI" rel="nofollow">FastAPI</a> - Create APIs at lightning speed with automatic docs that clients will love!</p>
</li>
<li>
<p dir="auto"><a href="https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Flask" rel="nofollow">Flask</a> - Craft web apps with minimal code that scales from prototype to production!</p>
</li>
<li>
<p dir="auto"><a href="https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Google%20A2A" rel="nofollow">Google A2A</a> - The universal language that lets AI agents collaborate across borders!</p>
</li>
<li>
<p dir="auto"><a href="https://the-pocket.github.io/Tutorial-Codebase-Knowledge/LangGraph" rel="nofollow">LangGraph</a> - Design AI agents as flowcharts where each step remembers what happened before!</p>
</li>
<li>
<p dir="auto"><a href="https://the-pocket.github.io/Tutorial-Codebase-Knowledge/LevelDB" rel="nofollow">LevelDB</a> - Store data at warp speed with Google's engine that powers blockchains!</p>
</li>
<li>
<p dir="auto"><a href="https://the-pocket.github.io/Tutorial-Codebase-Knowledge/MCP%20Python%20SDK" rel="nofollow">MCP Python SDK</a> - Build powerful apps that communicate through an elegant protocol without sweating the details!</p>
</li>
<li>
<p dir="auto"><a href="https://the-pocket.github.io/Tutorial-Codebase-Knowledge/NumPy%20Core" rel="nofollow">NumPy Core</a> - Master the engine behind data science that makes Python as fast as C!</p>
</li>
<li>
<p dir="auto"><a href="https://the-pocket.github.io/Tutorial-Codebase-Knowledge/OpenManus" rel="nofollow">OpenManus</a> - Build AI agents with digital brains that think, learn, and use tools just like humans do!</p>
</li>
<li>
<p dir="auto"><a href="https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Pydantic%20Core" rel="nofollow">Pydantic Core</a> - Validate data at rocket speed with just Python type hints!</p>
</li>
<li>
<p dir="auto"><a href="https://the-pocket.github.io/Tutorial-Codebase-Knowledge/Requests" rel="nofollow">Requests</a> - Talk to the internet in Python with code so simple it feels like cheating!</p>
</li>
<li>
<p dir="auto"><a href="https://the-pocket.github.io/Tutorial-Codebase-Knowledge/SmolaAgents" rel="nofollow">SmolaAgents</a> - Build tiny AI agents that punch way above their weight class!</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Getting Started</h2><a id="user-content--getting-started" aria-label="Permalink: 🚀 Getting Started" href="#-getting-started"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Clone this repository</p>
</li>
<li>
<p dir="auto">Install dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
</li>
<li>
<p dir="auto">Set up LLM in <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/utils/call_llm.py"><code>utils/call_llm.py</code></a> by providing credentials. By default, you can use the AI Studio key with this client for Gemini Pro 2.5:</p>
<div dir="auto" data-snippet-clipboard-copy-content="client = genai.Client(
  api_key=os.getenv(&quot;GEMINI_API_KEY&quot;, &quot;your-api_key&quot;),
)"><pre><span>client</span> <span>=</span> <span>genai</span>.<span>Client</span>(
  <span>api_key</span><span>=</span><span>os</span>.<span>getenv</span>(<span>"GEMINI_API_KEY"</span>, <span>"your-api_key"</span>),
)</pre></div>
<p dir="auto">You can use your own models. We highly recommend the latest models with thinking capabilities (Claude 3.7 with thinking, O1). You can verify that it is correctly set up by running:</p>

</li>
<li>
<p dir="auto">Generate a complete codebase tutorial by running the main script:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Analyze a GitHub repository
python main.py --repo https://github.com/username/repo --include &quot;*.py&quot; &quot;*.js&quot; --exclude &quot;tests/*&quot; --max-size 50000

# Or, analyze a local directory
python main.py --dir /path/to/your/codebase --include &quot;*.py&quot; --exclude &quot;*test*&quot;

# Or, generate a tutorial in Chinese
python main.py --repo https://github.com/username/repo --language &quot;Chinese&quot;"><pre><span><span>#</span> Analyze a GitHub repository</span>
python main.py --repo https://github.com/username/repo --include <span><span>"</span>*.py<span>"</span></span> <span><span>"</span>*.js<span>"</span></span> --exclude <span><span>"</span>tests/*<span>"</span></span> --max-size 50000

<span><span>#</span> Or, analyze a local directory</span>
python main.py --dir /path/to/your/codebase --include <span><span>"</span>*.py<span>"</span></span> --exclude <span><span>"</span>*test*<span>"</span></span>

<span><span>#</span> Or, generate a tutorial in Chinese</span>
python main.py --repo https://github.com/username/repo --language <span><span>"</span>Chinese<span>"</span></span></pre></div>
<ul dir="auto">
<li><code>--repo</code> or <code>--dir</code> - Specify either a GitHub repo URL or a local directory path (required, mutually exclusive)</li>
<li><code>-n, --name</code> - Project name (optional, derived from URL/directory if omitted)</li>
<li><code>-t, --token</code> - GitHub token (or set GITHUB_TOKEN environment variable)</li>
<li><code>-o, --output</code> - Output directory (default: ./output)</li>
<li><code>-i, --include</code> - Files to include (e.g., "<em>.py" "</em>.js")</li>
<li><code>-e, --exclude</code> - Files to exclude (e.g., "tests/<em>" "docs/</em>")</li>
<li><code>-s, --max-size</code> - Maximum file size in bytes (default: 100KB)</li>
<li><code>--language</code> - Language for the generated tutorial (default: "english")</li>
</ul>
</li>
</ol>
<p dir="auto">The application will crawl the repository, analyze the codebase structure, generate tutorial content in the specified language, and save the output in the specified directory (default: ./output).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">💡 Development Tutorial</h2><a id="user-content--development-tutorial" aria-label="Permalink: 💡 Development Tutorial" href="#-development-tutorial"></a></p>
<ul dir="auto">
<li>
<p dir="auto">I built using <a href="https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to" rel="nofollow"><strong>Agentic Coding</strong></a>, the fastest development paradigm, where humans simply <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/docs/design.md">design</a> and agents <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/flow.py">code</a>.</p>
</li>
<li>
<p dir="auto">The secret weapon is <a href="https://github.com/The-Pocket/PocketFlow">Pocket Flow</a>, a 100-line LLM framework that lets Agents (e.g., Cursor AI) build for you</p>
</li>
<li>
<p dir="auto">Check out the Step-by-step YouTube development tutorial:</p>
</li>
</ul>

<p><a href="https://youtu.be/AFY67zOpbSo" rel="nofollow">
    <img src="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/raw/main/assets/youtube_thumbnail.png" width="500" alt="IMAGE ALT TEXT">
  </a>
</p>
<br>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Art of Assembly Language (2010) (114 pts)]]></title>
            <link>https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AoATOC.html</link>
            <guid>43739285</guid>
            <pubDate>Sat, 19 Apr 2025 20:38:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AoATOC.html">https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AoATOC.html</a>, See on <a href="https://news.ycombinator.com/item?id=43739285">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<hr>
<h2>
  <a name="5032"> </a></h2>
<h2>
  <a name="5033"> </a></h2>
<dl>
  <dl>
    <dt> <a name="277484"> </a>"The Art of Assembly Language Programming" is now <span>hard</span>. This text is now available in published form from "No Starch Press" (http://www.nostarch.com). Please check out their website for more details.    </dt><dt> <a name="277485"> </a>
  </dt></dl>
</dl>
<h2>
  <a name="277482"> </a></h2>
<h2>
  <a name="280150"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Foreword.html#63">1.1 Foreword to the HLA Version of "The Art of Assembly..."</a></h2>
<h2>
  <a name="280152"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Foreword.html#522">1.2 Intended Audience</a></h2>
<h2>
  <a name="280154"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Foreword.html#635">1.3 Teaching From This Text</a></h2>
<h2>
  <a name="280156"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Foreword.html#623">1.4 Copyright Notice</a></h2>
<h2>
  <a name="280158"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Foreword.html#687">1.5 How to Get a Hard Copy of This Text</a></h2>
<h2>
  <a name="280160"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Foreword.html#695">1.6 Obtaining Program Source Listings and Other Materials in This Text</a></h2>
<h2>
  <a name="280162"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Foreword.html#704">1.7 Where to Get Help</a></h2>
<h2>
  <a name="280164"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Foreword.html#2307">1.8 Other Materials You Will Need (Windows Version)</a></h2>
<h2>
  <a name="280166"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Foreword.html#2314">1.9 Other Materials You Will Need (Linux Version)</a></h2>
<h2>
  <a name="280168"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorld.html#1034401">2.1 Chapter Overview</a></h2>
<h2>
  <a name="280170"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorld.html#1011931">2.2 Installing the HLA Distribution Package</a></h2>
<dl>
  <dd> <a name="280172"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorld.html#1011943">2.2.1 Installation Under Windows</a>
  </dd><dd> <a name="280174"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorld.html#1012028">2.2.2 Installation Under Linux</a>
  </dd><dd> <a name="280176"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorld.html#1012114">2.2.3 Installing "Art of Assembly" Related Files</a>
</dd></dl>
<h2>
  <a name="280178"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda2.html#1023185">2.3 The Anatomy of an HLA Program</a></h2>
<h2>
  <a name="280180"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda2.html#998296">2.4 Some Basic HLA Data Declarations</a></h2>
<h2>
  <a name="280182"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda2.html#998344">2.5 Boolean Values</a></h2>
<h2>
  <a name="280184"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda2.html#998359">2.6 Character Values</a></h2>
<h2>
  <a name="280186"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda3.html#998367">2.7 An Introduction to the Intel 80x86 CPU Family</a></h2>
<h2>
  <a name="280188"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda3.html#998414">2.8 Some Basic Machine Instructions</a></h2>
<h2>
  <a name="280190"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda3.html#998591">2.9 Some Basic HLA Control Structures</a></h2>
<dl>
  <dd> <a name="280192"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda3.html#998595">2.9.1 Boolean Expressions in HLA Statements</a>
  </dd><dd> <a name="280194"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda3.html#998715">2.9.2 The HLA IF..THEN..ELSEIF..ELSE..ENDIF Statement</a>
  </dd><dd> <a name="280196"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda3.html#998766">2.9.3 The WHILE..ENDWHILE Statement</a>
  </dd><dd> <a name="280198"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda3.html#998784">2.9.4 The FOR..ENDFOR Statement</a>
  </dd><dd> <a name="280200"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda3.html#998820">2.9.5 The REPEAT..UNTIL Statement</a>
  </dd><dd> <a name="280202"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda3.html#998841">2.9.6 The BREAK and BREAKIF Statements</a>
  </dd><dd> <a name="280204"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda3.html#998851">2.9.7 The FOREVER..ENDFOR Statement</a>
  </dd><dd> <a name="280206"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda3.html#998870">2.9.8 The TRY..EXCEPTION..ENDTRY Statement</a>
</dd></dl>
<h2>
  <a name="280208"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda4.html#1023187">2.10 Introduction to the HLA Standard Library</a></h2>
<dl>
  <dd> <a name="280210"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda4.html#999008">2.10.1 Predefined Constants in the STDIO Module</a>
  </dd><dd> <a name="280212"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda4.html#999039">2.10.2 Standard In and Standard Out</a>
  </dd><dd> <a name="280214"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda4.html#999051">2.10.3 The stdout.newln Routine</a>
  </dd><dd> <a name="280216"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda4.html#999054">2.10.4 The stdout.putiX Routines</a>
  </dd><dd> <a name="280218"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda4.html#999060">2.10.5 The stdout.putiXSize Routines</a>
  </dd><dd> <a name="280220"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda4.html#999109">2.10.6 The stdout.put Routine</a>
  </dd><dd> <a name="280222"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda4.html#999157">2.10.7 The stdin.getc Routine.</a>
  </dd><dd> <a name="280224"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda4.html#999206">2.10.8 The stdin.getiX Routines</a>
  </dd><dd> <a name="280226"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda4.html#999257">2.10.9 The stdin.readLn and stdin.flushInput Routines</a>
  </dd><dd> <a name="280228"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda4.html#999280">2.10.10 The stdin.get Macro</a>
</dd></dl>
<h2>
  <a name="280230"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda4.html#999298">2.11 Putting It All Together</a></h2>
<h2>
  <a name="280232"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda5.html#999302">2.12 Sample Programs</a></h2>
<dl>
  <dd> <a name="280234"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda5.html#999304">2.12.1 Powers of Two Table Generation</a>
  </dd><dd> <a name="280236"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda5.html#999356">2.12.2 Checkerboard Program</a>
  </dd><dd> <a name="280238"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HelloWorlda5.html#999428">2.12.3 Fibonacci Number Generation</a>
</dd></dl>
<h2>
  <a name="280240"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation.html#998841">3.1 Chapter Overview</a></h2>
<h2>
  <a name="280242"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation.html#998843">3.2 Numbering Systems</a></h2>
<dl>
  <dd> <a name="280244"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation.html#998846">3.2.1 A Review of the Decimal System</a>
  </dd><dd> <a name="280246"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation.html#998859">3.2.2 The Binary Numbering System</a>
  </dd><dd> <a name="280248"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation.html#998884">3.2.3 Binary Formats</a>
</dd></dl>
<h2>
  <a name="280250"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation2.html#998906">3.3 Data Organization</a></h2>
<dl>
  <dd> <a name="280252"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation2.html#998917">3.3.1 Bits</a>
  </dd><dd> <a name="280254"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation2.html#998925">3.3.2 Nibbles</a>
  </dd><dd> <a name="280256"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation2.html#998957">3.3.3 Bytes</a>
  </dd><dd> <a name="280258"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation2.html#999005">3.3.4 Words</a>
  </dd><dd> <a name="280260"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation2.html#999053">3.3.5 Double Words</a>
</dd></dl>
<h2>
  <a name="280262"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation3.html#1012324">3.4 The Hexadecimal Numbering System</a></h2>
<h2>
  <a name="280264"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation3.html#999193">3.5 Arithmetic Operations on Binary and Hexadecimal Numbers</a></h2>
<h2>
  <a name="280266"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation3.html#999216">3.6 A Note About Numbers vs. Representation</a></h2>
<h2>
  <a name="280268"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation4.html#1016732">3.7 Logical Operations on Bits</a></h2>
<h2>
  <a name="280270"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation4.html#999443">3.8 Logical Operations on Binary Numbers and Bit Strings</a></h2>
<h2>
  <a name="280272"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation5.html#999504">3.9 Signed and Unsigned Numbers</a></h2>
<h2>
  <a name="280274"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation5.html#999653">3.10 Sign Extension, Zero Extension, Contraction, and Saturation</a></h2>
<h2>
  <a name="280276"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation6.html#1016733">3.11 Shifts and Rotates</a></h2>
<h2>
  <a name="280278"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation7.html#999881">3.12 Bit Fields and Packed Data</a></h2>
<h2>
  <a name="280280"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DataRepresentation7.html#1001839">3.13 Putting It All Together</a></h2>
<h2>
  <a name="280282"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MoreDataRepresentation.html#998834">4.1 Chapter Overview</a></h2>
<h2>
  <a name="280284"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MoreDataRepresentation.html#1000036">4.2 An Introduction to Floating Point Arithmetic</a></h2>
<dl>
  <dd> <a name="280286"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MoreDataRepresentation.html">4.2.1 IEEE Floating Point Formats</a>
  </dd><dd> <a name="280288"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MoreDataRepresentation.html#1000155">4.2.2 HLA Support for Floating Point Values</a>
</dd></dl>
<h2>
  <a name="280290"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MoreDataRepresentation.html">4.3 Binary Coded Decimal (BCD) Representation</a></h2>
<h2>
  <a name="280292"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MoreDataRepresentationa2.html#1010580">4.4 Characters</a></h2>
<dl>
  <dd> <a name="280294"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MoreDataRepresentationa2.html#1000232">4.4.1 The ASCII Character Encoding</a>
  </dd><dd> <a name="280296"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MoreDataRepresentationa2.html#1000372">4.4.2 HLA Support for ASCII Characters</a>
  </dd><dd> <a name="280298"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MoreDataRepresentationa2.html#1000510">4.4.3 The ASCII Character Set</a>
</dd></dl>
<h2>
  <a name="280300"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MoreDataRepresentationa2.html#1001554">4.5 The UNICODE Character Set</a></h2>
<h2>
  <a name="280302"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MoreDataRepresentationa3.html#1010581">4.6 Other Data Representations</a></h2>
<dl>
  <dd> <a name="280304"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MoreDataRepresentationa3.html#1002170">4.6.1 Representing Colors on a Video Display</a>
  </dd><dd> <a name="280306"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MoreDataRepresentationa3.html#1002175">4.6.2 Representing Audio Information</a>
  </dd><dd> <a name="280308"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MoreDataRepresentationa3.html#1002177">4.6.3 Representing Musical Information</a>
  </dd><dd> <a name="280310"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MoreDataRepresentationa3.html#1002373">4.6.4 Representing Video Information</a>
  </dd><dd> <a name="280312"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MoreDataRepresentationa3.html#1002185">4.6.5 Where to Get More Information About Data Types</a>
</dd></dl>
<h2>
  <a name="280314"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MoreDataRepresentationa3.html#1001555">4.7 Putting It All Together</a></h2>
<h2>
  <a name="280316"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/SystemOrganization.html#998834">1.1 Chapter Overview</a></h2>
<h2>
  <a name="280318"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/SystemOrganization.html#998843">1.2 The Basic System Components</a></h2>
<dl>
  <dd> <a name="280320"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/SystemOrganization.html#998862">1.2.1 The System Bus</a>
  <dl>
    <dd> <a name="280322"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/SystemOrganization.html#998874">1.2.1.1 The Data Bus</a>
    </dd><dd> <a name="280324"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/SystemOrganization.html#998935">1.2.1.2 The Address Bus</a>
    </dd><dd> <a name="280326"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/SystemOrganization.html#999059">1.2.1.3 The Control Bus</a>
  </dd></dl>
  </dd><dd> <a name="280328"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/SystemOrganizationa2.html#1013690">1.2.2 The Memory Subsystem</a>
  </dd><dd> <a name="280330"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/SystemOrganizationa2.html#999196">1.2.3 The I/O Subsystem</a>
</dd></dl>
<h2>
  <a name="280332"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/SystemOrganizationa3.html#1013694">1.3 HLA Support for Data Alignment</a></h2>
<h2>
  <a name="280334"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/SystemOrganizationa4.html#1013695">1.4 System Timing</a></h2>
<dl>
  <dd> <a name="280336"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/SystemOrganization.html">1.4.1 The System Clock</a>
  </dd><dd> <a name="280338"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/SystemOrganizationa4.html#999338">1.4.2 Memory Access and the System Clock</a>
  </dd><dd> <a name="280340"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/SystemOrganizationa4.html#999367">1.4.3 Wait States</a>
  </dd><dd> <a name="280342"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/SystemOrganizationa4.html#999395">1.4.4 Cache Memory</a>
</dd></dl>
<h2>
  <a name="280344"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/SystemOrganizationa4.html#998197">1.5 Putting It All Together</a></h2>
<h2>
  <a name="280346"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#998834">2.1 Chapter Overview</a></h2>
<h2>
  <a name="280348"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#999494">2.2 The 80x86 Addressing Modes</a></h2>
<dl>
  <dd> <a name="280350"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#999500">2.2.1 80x86 Register Addressing Modes</a>
  </dd><dd> <a name="280352"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#999515">2.2.2 80x86 32-bit Memory Addressing Modes</a>
  <dl>
    <dd> <a name="280354"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#999520">2.2.2.1 The Displacement Only Addressing Mode</a>
    </dd><dd> <a name="280356"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#999545">2.2.2.2 The Register Indirect Addressing Modes</a>
    </dd><dd> <a name="280358"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#999581">2.2.2.3 Indexed Addressing Modes</a>
    </dd><dd> <a name="280360"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#999608">2.2.2.4 Variations on the Indexed Addressing Mode</a>
    </dd><dd> <a name="280362"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#999656">2.2.2.5 Scaled Indexed Addressing Modes</a>
    </dd><dd> <a name="280364"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#1002325">2.2.2.6 Addressing Mode Wrap-up</a>
  </dd></dl>
</dd></dl>
<h2>
  <a name="280366"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#999687">2.3 Run-Time Memory Organization</a></h2>
<dl>
  <dd> <a name="280368"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#999697">2.3.1 The Code Section</a>
  </dd><dd> <a name="280370"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#1012128">2.3.2 The Static Sections</a>
  </dd><dd> <a name="280372"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#999744">2.3.3 The Read-Only Data Section</a>
  </dd><dd> <a name="280374"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#999765">2.3.4 The Storage Section</a>
  </dd><dd> <a name="280376"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#1008215">2.3.5 The @NOSTORAGE Attribute</a>
  </dd><dd> <a name="280378"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#999848">2.3.6 The Var Section</a>
  </dd><dd> <a name="280380"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#999864">2.3.7 Organization of Declaration Sections Within Your Programs</a>
</dd></dl>
<h2>
  <a name="280382"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#999911">2.4 Address Expressions</a></h2>
<h2>
  <a name="280384"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#999974">2.5 Type Coercion</a></h2>
<h2>
  <a name="280386"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#1000029">2.6 Register Type Coercion</a></h2>
<h2>
  <a name="280388"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#1000052">2.7 The Stack Segment and the Push and Pop Instructions</a></h2>
<dl>
  <dd> <a name="280390"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#1000069">2.7.1 The Basic PUSH Instruction</a>
  </dd><dd> <a name="280392"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#1000098">2.7.2 The Basic POP Instruction</a>
  </dd><dd> <a name="280394"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#1000132">2.7.3 Preserving Registers With the PUSH and POP Instructions</a>
  </dd><dd> <a name="280396"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#1000154">2.7.4 The Stack is a LIFO Data Structure</a>
  </dd><dd> <a name="280398"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#1000220">2.7.5 Other PUSH and POP Instructions</a>
  </dd><dd> <a name="280400"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#1000261">2.7.6 Removing Data From the Stack Without Popping It</a>
  </dd><dd> <a name="280402"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#1000319">2.7.7 Accessing Data You've Pushed on the Stack Without Popping It</a>
</dd></dl>
<h2>
  <a name="280404"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#1000360">2.8 Dynamic Memory Allocation and the Heap Segment</a></h2>
<h2>
  <a name="280406"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#1000436">2.9 The INC and DEC Instructions</a></h2>
<h2>
  <a name="280408"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#1000458">2.10 Obtaining the Address of a Memory Object</a></h2>
<h2>
  <a name="280410"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryAccessandOrg.html#1002585">2.11 Putting It All Together</a></h2>
<h2>
  <a name="280412"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DigitalDesign.html#362">3.1 Boolean Algebra</a></h2>
<h2>
  <a name="280414"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DigitalDesign.html#3996">3.2 Boolean Functions and Truth Tables</a></h2>
<h2>
  <a name="280416"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DigitalDesign2.html#20769">3.3 Algebraic Manipulation of Boolean Expressions</a></h2>
<h2>
  <a name="280418"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DigitalDesign2.html#4874">3.4 Canonical Forms</a></h2>
<h2>
  <a name="280420"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DigitalDesign3.html#20770">3.5 Simplification of Boolean Functions</a></h2>
<h2>
  <a name="280422"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DigitalDesign4.html#20771">3.6 What Does This Have To Do With Computers, Anyway?</a></h2>
<dl>
  <dd> <a name="280424"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DigitalDesign4.html#6176">3.6.1 Correspondence Between Electronic Circuits and Boolean Functions</a>
  </dd><dd> <a name="280426"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DigitalDesign4.html#6234">3.6.2 Combinatorial Circuits</a>
  </dd><dd> <a name="280428"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DigitalDesign4.html#6293">3.6.3 Sequential and Clocked Logic</a>
</dd></dl>
<h2>
  <a name="280430"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DigitalDesign4.html#6140">3.7 Okay, What Does It Have To Do With Programming, Then?</a></h2>
<h2>
  <a name="280432"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DigitalDesign4.html#11030">3.8 Putting It All Together</a></h2>
<h2>
  <a name="280434"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecture.html#998197">4.1 Chapter Overview</a></h2>
<h2>
  <a name="280436"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecture.html#998840">4.2 The History of the 80x86 CPU Family</a></h2>
<h2>
  <a name="280438"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecturea2.html#1013324">4.3 A History of Software Development for the x86</a></h2>
<h2>
  <a name="280440"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecturea3.html#1029343">4.4 Basic CPU Design</a></h2>
<h2>
  <a name="280442"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecturea3.html#1013328">4.5 Decoding and Executing Instructions: Random Logic Versus Microcode</a></h2>
<h2>
  <a name="280444"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecturea3.html#1014719">4.6 RISC vs. CISC vs. VLIW</a></h2>
<h2>
  <a name="280446"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecturea3.html#1014746">4.7 Instruction Execution, Step-By-Step</a></h2>
<h2>
  <a name="280448"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecturea3.html#1014785">4.8 Parallelism - the Key to Faster Processors</a></h2>
<dl>
  <dd> <a name="280450"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecturea3.html#1013332">4.8.1 The Prefetch Queue - Using Unused Bus Cycles</a>
  </dd><dd> <a name="280452"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecturea3.html#1013340">4.8.2 Pipelining - Overlapping the Execution of Multiple Instructions</a>
  <dl>
    <dd> <a name="280454"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecturea3.html#1015976">4.8.2.1 A Typical Pipeline</a>
    </dd><dd> <a name="280456"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecturea3.html#1016115">4.8.2.2 Stalls in a Pipeline</a>
  </dd></dl>
  </dd><dd> <a name="280458"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecturea3.html#1016111">4.8.3 Instruction Caches - Providing Multiple Paths to Memory</a>
  </dd><dd> <a name="280460"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecturea3.html#1013342">4.8.4 Hazards</a>
  </dd><dd> <a name="280462"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecturea3.html#1016344">4.8.5 Superscalar Operation- Executing Instructions in Parallel</a>
  </dd><dd> <a name="280464"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecturea3.html#1013344">4.8.6 Out of Order Execution</a>
  </dd><dd> <a name="280466"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecturea3.html#1016558">4.8.7 Register Renaming</a>
  </dd><dd> <a name="280468"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecturea3.html#1016686">4.8.8 Very Long Instruction Word Architecture (VLIW)</a>
  </dd><dd> <a name="280470"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecturea3.html#1016572">4.8.9 Parallel Processing</a>
  </dd><dd> <a name="280472"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecturea3.html#1016641">4.8.10 Multiprocessing</a>
</dd></dl>
<h2>
  <a name="280474"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CPUArchitecturea3.html#1016617">4.9 Putting It All Together</a></h2>
<h2>
  <a name="280476"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ISA.html">5.1 Chapter Overview</a></h2>
<h2>
  <a name="280478"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ISA.html#1013376">5.2 The Importance of the Design of the Instruction Set</a></h2>
<h2>
  <a name="280480"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ISA.html#1013428">5.3 Basic Instruction Design Goals</a></h2>
<dl>
  <dd> <a name="280482"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ISA.html#1013211">5.3.1 Addressing Modes on the Y86</a>
  </dd><dd> <a name="280484"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ISA.html#1013253">5.3.2 Encoding Y86 Instructions</a>
  </dd><dd> <a name="280486"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ISA.html#1013175">5.3.3 Hand Encoding Instructions</a>
  </dd><dd> <a name="280488"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ISA.html#1014039">5.3.4 Using an Assembler to Encode Instructions</a>
  </dd><dd> <a name="280490"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ISA.html#1014056">5.3.5 Extending the Y86 Instruction Set</a>
</dd></dl>
<h2>
  <a name="280492"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ISA.html#1014076">5.4 Encoding 80x86 Instructions</a></h2>
<dl>
  <dd> <a name="280494"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ISA.html#1014111">5.4.1 Encoding Instruction Operands</a>
  </dd><dd> <a name="280496"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ISA.html#1015674">5.4.2 Encoding the ADD Instruction: Some Examples</a>
  </dd><dd> <a name="280498"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ISA.html">5.4.3 Encoding Immediate Operands</a>
  </dd><dd> <a name="280500"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ISA.html#1015672">5.4.4 Encoding Eight, Sixteen, and Thirty-Two Bit Operands</a>
  </dd><dd> <a name="280502"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ISA.html#1015679">5.4.5 Alternate Encodings for Instructions</a>
</dd></dl>
<h2>
  <a name="280504"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ISA.html#1015686">5.5 Putting It All Together</a></h2>
<h2>
  <a name="280506"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryArchitecture.html">6.1 Chapter Overview</a></h2>
<h2>
  <a name="280508"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryArchitecture.html#1013180">6.2 The Memory Hierarchy</a></h2>
<h2>
  <a name="280510"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryArchitecture.html#1013244">6.3 How the Memory Hierarchy Operates</a></h2>
<h2>
  <a name="280512"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryArchitecture.html#1013266">6.4 Relative Performance of Memory Subsystems</a></h2>
<h2>
  <a name="280514"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryArchitecturea2.html#1023796">6.5 Cache Architecture</a></h2>
<h2>
  <a name="280516"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryArchitecturea3.html#1023797">6.6 Virtual Memory, Protection, and Paging</a></h2>
<h2>
  <a name="280518"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryArchitecturea3.html#1013375">6.7 Thrashing</a></h2>
<h2>
  <a name="280520"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryArchitecturea4.html#1023802">6.8 NUMA and Peripheral Devices</a></h2>
<h2>
  <a name="280522"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryArchitecturea4.html#1013371">6.9 Segmentation</a></h2>
<h2>
  <a name="280524"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MemoryArchitecture.html">6.10 Putting it All Together</a></h2>
<h2>
  <a name="280526"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IO.html">7.1 Chapter Overview</a></h2>
<h2>
  <a name="280528"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IO.html#1013159">7.2 Connecting a CPU to the Outside World</a></h2>
<h2>
  <a name="280530"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IO.html#1013600">7.3 Read-Only, Write-Only, Read/Write, and Dual I/O Ports</a></h2>
<h2>
  <a name="280532"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IO.html#1013414">7.4 I/O (Input/Output) Mechanisms</a></h2>
<dl>
  <dd> <a name="280534"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IO.html#1013591">7.4.1 Memory Mapped Input/Output</a>
  </dd><dd> <a name="280536"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IO.html#1013592">7.4.2 I/O Mapped Input/Output</a>
  </dd><dd> <a name="280538"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IO.html#1013228">7.4.3 Direct Memory Access</a>
</dd></dl>
<h2>
  <a name="280540"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IO2.html#1027142">7.5 I/O Speed Hierarchy</a></h2>
<h2>
  <a name="280542"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IO2.html#1013772">7.6 System Busses and Data Transfer Rates</a></h2>
<h2>
  <a name="280544"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IO2.html#1013752">7.7 The AGP Bus</a></h2>
<h2>
  <a name="280546"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IO3.html#1027143">7.8 Handshaking</a></h2>
<h2>
  <a name="280548"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IO3.html#1013258">7.9 Time-outs on an I/O Port</a></h2>
<h2>
  <a name="280550"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IO4.html#1027144">7.10 Interrupts and Polled I/O</a></h2>
<h2>
  <a name="280552"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IO4.html#1014216">7.11 Using a Circular Queue to Buffer Input Data from an ISR</a></h2>
<h2>
  <a name="280554"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IO4.html#1014217">7.12 Using a Circular Queue to Buffer Output Data for an ISR</a></h2>
<h2>
  <a name="280556"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IO5.html#1027145">7.13 I/O and the Cache</a></h2>
<h2>
  <a name="280558"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IO5.html#1014737">7.14 Protected Mode Operation</a></h2>
<h2>
  <a name="280560"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IO5.html#998840">7.15 Device Drivers</a></h2>
<h2>
  <a name="280562"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IO5.html#1014462">7.16 Putting It All Together</a></h2>
<h2>
  <a name="280564"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ConstsVarsAndDataTypes.html">1.1 Chapter Overview</a></h2>
<h2>
  <a name="280566"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ConstsVarsAndDataTypes.html#998838">1.2 Some Additional Instructions: INTMUL, BOUND, INTO</a></h2>
<h2>
  <a name="280568"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ConstsVarsAndDataTypes.html#999026">1.3 The QWORD and TBYTE Data Types</a></h2>
<h2>
  <a name="280570"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ConstsVarsAndDataTypes.html#999031">1.4 HLA Constant and Value Declarations</a></h2>
<dl>
  <dd> <a name="280572"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ConstsVarsAndDataTypes.html#999172">1.4.1 Constant Types</a>
  </dd><dd> <a name="280574"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ConstsVarsAndDataTypes.html#999199">1.4.2 String and Character Literal Constants</a>
  </dd><dd> <a name="280576"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ConstsVarsAndDataTypes.html#999255">1.4.3 String and Text Constants in the CONST Section</a>
  </dd><dd> <a name="280578"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ConstsVarsAndDataTypes.html#999287">1.4.4 Constant Expressions</a>
  </dd><dd> <a name="280580"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ConstsVarsAndDataTypes.html#999367">1.4.5 Multiple CONST Sections and Their Order in an HLA Program</a>
  </dd><dd> <a name="280582"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ConstsVarsAndDataTypes.html#999378">1.4.6 The HLA VAL Section</a>
  </dd><dd> <a name="280584"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ConstsVarsAndDataTypes.html#999397">1.4.7 Modifying VAL Objects at Arbitrary Points in Your Programs</a>
</dd></dl>
<h2>
  <a name="280586"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ConstsVarsAndDataTypes2.html#1015617">1.5 The HLA TYPE Section</a></h2>
<h2>
  <a name="280588"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ConstsVarsAndDataTypes2.html#999450">1.6 ENUM and HLA Enumerated Data Types</a></h2>
<h2>
  <a name="280590"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ConstsVarsAndDataTypes2.html#999488">1.7 Pointer Data Types</a></h2>
<dl>
  <dd> <a name="280592"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ConstsVarsAndDataTypes2.html#999516">1.7.1 Using Pointers in Assembly Language</a>
  </dd><dd> <a name="280594"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ConstsVarsAndDataTypes2.html#999543">1.7.2 Declaring Pointers in HLA</a>
  </dd><dd> <a name="280596"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ConstsVarsAndDataTypes2.html#999560">1.7.3 Pointer Constants and Pointer Constant Expressions</a>
  </dd><dd> <a name="280598"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ConstsVarsAndDataTypes2.html#999598">1.7.4 Pointer Variables and Dynamic Memory Allocation</a>
  </dd><dd> <a name="280600"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ConstsVarsAndDataTypes2.html#999626">1.7.5 Common Pointer Problems</a>
</dd></dl>
<h2>
  <a name="280602"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ConstsVarsAndDataTypes2.html#999757">1.8 Putting It All Together</a></h2>
<h2>
  <a name="280604"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharacterStrings.html">2.1 Chapter Overview</a></h2>
<h2>
  <a name="280606"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharacterStrings.html#999760">2.2 Composite Data Types</a></h2>
<h2>
  <a name="280608"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharacterStrings.html#999763">2.3 Character Strings</a></h2>
<h2>
  <a name="280610"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharacterStrings.html#999809">2.4 HLA Strings</a></h2>
<h2>
  <a name="280612"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharacterStrings.html#999979">2.5 Accessing the Characters Within a String</a></h2>
<h2>
  <a name="280614"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharacterStringsa2.html#1000046">2.6 The HLA String Module and Other String-Related Routines</a></h2>
<h2>
  <a name="280616"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharacterStringsa2.html#1004476">2.7 In-Memory Conversions</a></h2>
<h2>
  <a name="280618"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharacterStringsa2.html#1004058">2.8 Putting It All Together</a></h2>
<h2>
  <a name="280620"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharactersAndCharSets.html">3.1 Chapter Overview</a></h2>
<h2>
  <a name="280622"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharactersAndCharSets.html#1000417">3.2 The HLA Standard Library CHARS.HHF Module</a></h2>
<h2>
  <a name="280624"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharactersAndCharSets.html#1004846">3.3 Character Sets</a></h2>
<h2>
  <a name="280626"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharactersAndCharSets.html#1000420">3.4 Character Set Implementation in HLA</a></h2>
<h2>
  <a name="280628"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharactersAndCharSets.html#1000479">3.5 HLA Character Set Constants and Character Set Expressions</a></h2>
<h2>
  <a name="280630"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharactersAndCharSets.html#1000520">3.6 The IN Operator in HLA HLL Boolean Expressions</a></h2>
<h2>
  <a name="280632"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharactersAndCharSets.html#1000547">3.7 Character Set Support in the HLA Standard Library</a></h2>
<h2>
  <a name="280634"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharactersAndCharSets.html#1000617">3.8 Using Character Sets in Your HLA Programs</a></h2>
<h2>
  <a name="280636"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharactersAndCharSets.html#998197">3.9 Low-level Implementation of Set Operations</a></h2>
<dl>
  <dd> <a name="280638"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharactersAndCharSets.html#1003924">3.9.1 Character Set Functions That Build Sets</a>
  </dd><dd> <a name="280640"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharactersAndCharSets.html#1004344">3.9.2 Traditional Set Operations</a>
  </dd><dd> <a name="280642"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharactersAndCharSets.html#1003925">3.9.3 Testing Character Sets</a>
</dd></dl>
<h2>
  <a name="280644"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/CharactersAndCharSets.html#1003920">3.10 Putting It All Together</a></h2>
<h2>
  <a name="280646"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Arrays.html">4.1 Chapter Overview</a></h2>
<h2>
  <a name="280648"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Arrays.html#1000674">4.2 Arrays</a></h2>
<h2>
  <a name="280650"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Arrays.html#1000702">4.3 Declaring Arrays in Your HLA Programs</a></h2>
<h2>
  <a name="280652"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Arrays.html#1000728">4.4 HLA Array Constants</a></h2>
<h2>
  <a name="280654"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Arrays.html#1000750">4.5 Accessing Elements of a Single Dimension Array</a></h2>
<dl>
  <dd> <a name="280656"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Arrays.html#1000794">4.5.1 Sorting an Array of Values</a>
</dd></dl>
<h2>
  <a name="280658"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Arraysa2.html#1010609">4.6 Multidimensional Arrays</a></h2>
<dl>
  <dd> <a name="280660"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Arraysa2.html#1000862">4.6.1 Row Major Ordering</a>
  </dd><dd> <a name="280662"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Arraysa2.html#1000988">4.6.2 Column Major Ordering</a>
</dd></dl>
<h2>
  <a name="280664"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Arraysa2.html#1001023">4.7 Allocating Storage for Multidimensional Arrays</a></h2>
<h2>
  <a name="280666"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Arraysa2.html#1001071">4.8 Accessing Multidimensional Array Elements in Assembly Language</a></h2>
<h2>
  <a name="280668"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Arraysa3.html#1010610">4.9 Large Arrays and MASM</a></h2>
<h2>
  <a name="280670"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Arraysa3.html#1009426">4.10 Dynamic Arrays in Assembly Language</a></h2>
<h2>
  <a name="280672"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Arraysa3.html#1001202">4.11 HLA Standard Library Array Support</a></h2>
<h2>
  <a name="280674"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Arraysa3.html#998197">4.12 Putting It All Together</a></h2>
<h2>
  <a name="280676"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RecordsUnionsNamespaces.html">5.1 Chapter Overview</a></h2>
<h2>
  <a name="280678"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RecordsUnionsNamespaces.html#1001281">5.2 Records</a></h2>
<h2>
  <a name="280680"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RecordsUnionsNamespaces.html#1001353">5.3 Record Constants</a></h2>
<h2>
  <a name="280682"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RecordsUnionsNamespaces.html#1004084">5.4 Arrays of Records</a></h2>
<h2>
  <a name="280684"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RecordsUnionsNamespaces.html#1004035">5.5 Arrays/Records as Record Fields</a></h2>
<h2>
  <a name="280686"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RecordsUnionsNamespaces.html#1003944">5.6 Controlling Field Offsets Within a Record</a></h2>
<h2>
  <a name="280688"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RecordsUnionsNamespaces.html#1003984">5.7 Aligning Fields Within a Record</a></h2>
<h2>
  <a name="280690"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RecordsUnionsNamespaces.html#1001449">5.8 Pointers to Records</a></h2>
<h2>
  <a name="280692"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RecordsUnionsNamespacesa2.html#1004223">5.9 Unions</a></h2>
<h2>
  <a name="280694"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RecordsUnionsNamespacesa2.html#1004361">5.10 Anonymous Unions</a></h2>
<h2>
  <a name="280696"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RecordsUnionsNamespacesa2.html#1004283">5.11 Variant Types</a></h2>
<h2>
  <a name="280698"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RecordsUnionsNamespaces3.html#1012321">5.12 Namespaces</a></h2>
<h2>
  <a name="280700"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RecordsUnionsNamespaces3.html#1004026">5.13 Putting It All Together</a></h2>
<h2>
  <a name="280702"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DatesAndTimes.html">6.1 Chapter Overview</a></h2>
<h2>
  <a name="280704"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DatesAndTimes.html#1001465">6.2 Dates</a></h2>
<h2>
  <a name="280706"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DatesAndTimes.html#1003951">6.3 A Brief History of the Calendar</a></h2>
<h2>
  <a name="280708"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DatesAndTimes2.html#1013953">6.4 HLA Date Functions</a></h2>
<dl>
  <dd> <a name="280710"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DatesAndTimes2.html#1001486">6.4.1 date.IsValid and date.validate</a>
  </dd><dd> <a name="280712"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DatesAndTimes2.html#1001568">6.4.2 Checking for Leap Years</a>
  </dd><dd> <a name="280714"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DatesAndTimes2.html#1001675">6.4.3 Obtaining the System Date</a>
  </dd><dd> <a name="280716"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DatesAndTimes2.html#1001715">6.4.4 Date to String Conversions and Date Output</a>
  </dd><dd> <a name="280718"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DatesAndTimes2.html#998197">6.4.5 date.unpack and data.pack</a>
  </dd><dd> <a name="280720"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DatesAndTimes2.html#1004157">6.4.6 date.Julian, date.fromJulian</a>
  </dd><dd> <a name="280722"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DatesAndTimes2.html#1004034">6.4.7 date.datePlusDays, date.datePlusMonths, and date.daysBetween</a>
  </dd><dd> <a name="280724"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DatesAndTimes2.html#1004072">6.4.8 date.dayNumber, date.daysLeft, and date.dayOfWeek</a>
</dd></dl>
<h2>
  <a name="280726"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DatesAndTimes3.html#1004142">6.5 Times</a></h2>
<dl>
  <dd> <a name="280728"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DatesAndTimes3.html#1004208">6.5.1 time.curTime</a>
  </dd><dd> <a name="280730"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DatesAndTimes3.html#1004214">6.5.2 time.hmsToSecs and time.secstoHMS</a>
  </dd><dd> <a name="280732"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DatesAndTimes3.html#1004232">6.5.3 Time Input/Output</a>
</dd></dl>
<h2>
  <a name="280734"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DatesAndTimes3.html#1004357">6.6 Putting It All Together</a></h2>
<h2>
  <a name="280736"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Files.html">7.1 Chapter Overview</a></h2>
<h2>
  <a name="280738"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Files.html#1004585">7.2 File Organization</a></h2>
<dl>
  <dd> <a name="280740"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Files.html#1004624">7.2.1 Files as Lists of Records</a>
  </dd><dd> <a name="280742"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Files.html#1004641">7.2.2 Binary vs. Text Files</a>
</dd></dl>
<h2>
  <a name="280744"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Filesa2.html#1017815">7.3 Sequential Files</a></h2>
<h2>
  <a name="280746"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Files3.html#1017816">7.4 Random Access Files</a></h2>
<h2>
  <a name="280748"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Files3.html#998197">7.5 ISAM (Indexed Sequential Access Method) Files</a></h2>
<h2>
  <a name="280750"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Files4.html#1017817">7.6 Truncating a File</a></h2>
<h2>
  <a name="280752"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Files4.html#1005206">7.7 File Utility Routines</a></h2>
<dl>
  <dd> <a name="280754"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Files4.html#1005713">7.7.1 Computing the File Size</a>
  </dd><dd> <a name="280756"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Files4.html#1005719">7.7.2 Deleting Files</a>
</dd></dl>
<h2>
  <a name="280758"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Files4.html#1005204">7.8 Directory Operations</a></h2>
<h2>
  <a name="280760"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Files4.html#1005182">7.9 Putting It All Together</a></h2>
<h2>
  <a name="280762"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntroductionToProcedures.html">8.1 Chapter Overview</a></h2>
<h2>
  <a name="280764"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntroductionToProcedures.html#998840">8.2 Procedures</a></h2>
<h2>
  <a name="280766"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntroductionToProcedures.html#998926">8.3 Saving the State of the Machine</a></h2>
<h2>
  <a name="280768"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntroductionToProcedures.html#999081">8.4 Prematurely Returning from a Procedure</a></h2>
<h2>
  <a name="280770"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntroductionToProcedures2.html#1010444">8.5 Local Variables</a></h2>
<h2>
  <a name="280772"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntroductionToProcedures2.html#999264">8.6 Other Local and Global Symbol Types</a></h2>
<h2>
  <a name="280774"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntroductionToProcedures3.html#1010445">8.7 Parameters</a></h2>
<dl>
  <dd> <a name="280776"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntroductionToProcedures3.html#999276">8.7.1 Pass by Value</a>
  </dd><dd> <a name="280778"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntroductionToProcedures3.html#999382">8.7.2 Pass by Reference</a>
</dd></dl>
<h2>
  <a name="280780"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntroductionToProcedures3.html#999477">8.8 Functions and Function Results</a></h2>
<dl>
  <dd> <a name="280782"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntroductionToProcedures3.html#999484">8.8.1 Returning Function Results</a>
  </dd><dd> <a name="280784"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntroductionToProcedures3.html#999493">8.8.2 Instruction Composition in HLA</a>
  </dd><dd> <a name="280786"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntroductionToProcedures3.html#999539">8.8.3 The HLA RETURNS Option in Procedures</a>
</dd></dl>
<h2>
  <a name="280788"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntroductionToProcedures4.html#999601">8.9 Side Effects</a></h2>
<h2>
  <a name="280790"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntroductionToProcedures4.html#999651">8.10 Recursion</a></h2>
<h2>
  <a name="280792"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntroductionToProcedures4.html#999835">8.11 Forward Procedures</a></h2>
<h2>
  <a name="280794"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntroductionToProcedures4.html#1001269">8.12 Putting It All Together</a></h2>
<h2>
  <a name="280796"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ManagingLargePrograms.html">9.1 Chapter Overview</a></h2>
<h2>
  <a name="280798"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ManagingLargePrograms.html#999854">9.2 Managing Large Programs</a></h2>
<h2>
  <a name="280800"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ManagingLargePrograms.html#999866">9.3 The #INCLUDE Directive</a></h2>
<h2>
  <a name="280802"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ManagingLargePrograms.html#1001437">9.4 Ignoring Duplicate Include Operations</a></h2>
<h2>
  <a name="280804"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ManagingLargePrograms.html#999913">9.5 UNITs and the EXTERNAL Directive</a></h2>
<dl>
  <dd> <a name="280806"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ManagingLargePrograms.html#1000044">9.5.1 Behavior of the EXTERNAL Directive</a>
  </dd><dd> <a name="280808"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ManagingLargePrograms.html#1000078">9.5.2 Header Files in HLA</a>
</dd></dl>
<h2>
  <a name="280810"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ManagingLargePrograms2.html#1010906">9.6 Make Files</a></h2>
<h2>
  <a name="280812"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ManagingLargePrograms2.html#998197">9.7 Code Reuse</a></h2>
<h2>
  <a name="280814"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ManagingLargePrograms2.html#1001309">9.8 Creating and Managing Libraries</a></h2>
<h2>
  <a name="280816"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ManagingLargePrograms2.html#1001302">9.9 Name Space Pollution</a></h2>
<h2>
  <a name="280818"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ManagingLargePrograms2.html#1001411">9.10 Putting It All Together</a></h2>
<h2>
  <a name="280820"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntegerArithmetic.html">10.1 Chapter Overview</a></h2>
<h2>
  <a name="280822"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntegerArithmetic.html#998837">10.2 80x86 Integer Arithmetic Instructions</a></h2>
<dl>
  <dd> <a name="280824"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntegerArithmetic.html#998840">10.2.1 The MUL and IMUL Instructions</a>
  </dd><dd> <a name="280826"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntegerArithmetic.html#998921">10.2.2 The DIV and IDIV Instructions</a>
  </dd><dd> <a name="280828"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntegerArithmetic.html#999016">10.2.3 The CMP Instruction</a>
  </dd><dd> <a name="280830"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntegerArithmetic.html#999085">10.2.4 The SETcc Instructions</a>
  </dd><dd> <a name="280832"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntegerArithmetic.html#999449">10.2.5 The TEST Instruction</a>
</dd></dl>
<h2>
  <a name="280834"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntegerArithmetica2.html#999459">10.3 Arithmetic Expressions</a></h2>
<dl>
  <dd> <a name="280836"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntegerArithmetica2.html#999473">10.3.1 Simple Assignments</a>
  </dd><dd> <a name="280838"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntegerArithmetica2.html#999504">10.3.2 Simple Expressions</a>
  </dd><dd> <a name="280840"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntegerArithmetica2.html#999582">10.3.3 Complex Expressions</a>
  </dd><dd> <a name="280842"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntegerArithmetica2.html#999744">10.3.4 Commutative Operators</a>
</dd></dl>
<h2>
  <a name="280844"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntegerArithmetica2.html#999863">10.4 Logical (Boolean) Expressions</a></h2>
<h2>
  <a name="280846"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntegerArithmetic3.html#999930">10.5 Machine and Arithmetic Idioms</a></h2>
<dl>
  <dd> <a name="280848"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntegerArithmetic3.html#999935">10.5.1 Multiplying without MUL, IMUL, or INTMUL</a>
  </dd><dd> <a name="280850"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntegerArithmetic3.html#999966">10.5.2 Division Without DIV or IDIV</a>
  </dd><dd> <a name="280852"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntegerArithmetic3.html#999977">10.5.3 Implementing Modulo-N Counters with AND</a>
  </dd><dd> <a name="280854"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntegerArithmetic3.html#998197">10.5.4 Careless Use of Machine Idioms</a>
</dd></dl>
<h2>
  <a name="280856"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntegerArithmetic3.html#1003091">10.6 The HLA (Pseudo) Random Number Unit</a></h2>
<h2>
  <a name="280858"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntegerArithmetic3.html#1003066">10.7 Putting It All Together</a></h2>
<h2>
  <a name="280860"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetic.html">11.1 Chapter Overview</a></h2>
<h2>
  <a name="280862"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetic.html#999987">11.2 Floating Point Arithmetic</a></h2>
<dl>
  <dd> <a name="280864"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetic.html#999994">11.2.1 FPU Registers</a>
  <dl>
    <dd> <a name="280866"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetic.html#999996">11.2.1.1 FPU Data Registers</a>
    </dd><dd> <a name="280868"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetic.html#1000010">11.2.1.2 The FPU Control Register</a>
    </dd><dd> <a name="280870"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetic.html#1000117">11.2.1.3 The FPU Status Register</a>
  </dd></dl>
  </dd><dd> <a name="280872"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetic.html#1000504">11.2.2 FPU Data Types</a>
  </dd><dd> <a name="280874"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1013284">11.2.3 The FPU Instruction Set</a>
  </dd><dd> <a name="280876"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000540">11.2.4 FPU Data Movement Instructions</a>
  <dl>
    <dd> <a name="280878"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000543">11.2.4.1 The FLD Instruction</a>
    </dd><dd> <a name="280880"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000560">11.2.4.2 The FST and FSTP Instructions</a>
    </dd><dd> <a name="280882"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000578">11.2.4.3 The FXCH Instruction</a>
  </dd></dl>
  </dd><dd> <a name="280884"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000587">11.2.5 Conversions</a>
  <dl>
    <dd> <a name="280886"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000590">11.2.5.1 The FILD Instruction</a>
    </dd><dd> <a name="280888"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000597">11.2.5.2 The FIST and FISTP Instructions</a>
    </dd><dd> <a name="280890"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000626">11.2.5.3 The FBLD and FBSTP Instructions</a>
  </dd></dl>
  </dd><dd> <a name="280892"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000642">11.2.6 Arithmetic Instructions</a>
  <dl>
    <dd> <a name="280894"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000646">11.2.6.1 The FADD and FADDP Instructions</a>
    </dd><dd> <a name="280896"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000664">11.2.6.2 The FSUB, FSUBP, FSUBR, and FSUBRP Instructions</a>
    </dd><dd> <a name="280898"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000691">11.2.6.3 The FMUL and FMULP Instructions</a>
    </dd><dd> <a name="280900"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000710">11.2.6.4 The FDIV, FDIVP, FDIVR, and FDIVRP Instructions</a>
    </dd><dd> <a name="280902"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000745">11.2.6.5 The FSQRT Instruction</a>
    </dd><dd> <a name="280904"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000766">11.2.6.6 The FPREM and FPREM1 Instructions</a>
    </dd><dd> <a name="280906"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000788">11.2.6.7 The FRNDINT Instruction</a>
    </dd><dd> <a name="280908"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000794">11.2.6.8 The FABS Instruction</a>
    </dd><dd> <a name="280910"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000808">11.2.6.9 The FCHS Instruction</a>
  </dd></dl>
  </dd><dd> <a name="280912"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000819">11.2.7 Comparison Instructions</a>
  <dl>
    <dd> <a name="280914"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000829">11.2.7.1 The FCOM, FCOMP, and FCOMPP Instructions</a>
    </dd><dd> <a name="280916"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000862">11.2.7.2 The FTST Instruction</a>
  </dd></dl>
  </dd><dd> <a name="280918"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000876">11.2.8 Constant Instructions</a>
  </dd><dd> <a name="280920"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000886">11.2.9 Transcendental Instructions</a>
  <dl>
    <dd> <a name="280922"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000891">11.2.9.1 The F2XM1 Instruction</a>
    </dd><dd> <a name="280924"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000909">11.2.9.2 The FSIN, FCOS, and FSINCOS Instructions</a>
    </dd><dd> <a name="280926"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000916">11.2.9.3 The FPTAN Instruction</a>
    </dd><dd> <a name="280928"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000922">11.2.9.4 The FPATAN Instruction</a>
    </dd><dd> <a name="280930"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000927">11.2.9.5 The FYL2X Instruction</a>
    </dd><dd> <a name="280932"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000934">11.2.9.6 The FYL2XP1 Instruction</a>
  </dd></dl>
  </dd><dd> <a name="280934"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000942">11.2.10 Miscellaneous instructions</a>
  <dl>
    <dd> <a name="280936"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000946">11.2.10.1 The FINIT and FNINIT Instructions</a>
    </dd><dd> <a name="280938"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000962">11.2.10.2 The FLDCW and FSTCW Instructions</a>
    </dd><dd> <a name="280940"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000976">11.2.10.3 The FCLEX and FNCLEX Instructions</a>
    </dd><dd> <a name="280942"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1000988">11.2.10.4 The FSTSW and FNSTSW Instructions</a>
  </dd></dl>
  </dd><dd> <a name="280944"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica2.html#1001007">11.2.11 Integer Operations</a>
</dd></dl>
<h2>
  <a name="280946"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica3.html#1013285">11.3 Converting Floating Point Expressions to Assembly Language</a></h2>
<dl>
  <dd> <a name="280948"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica3.html#1001073">11.3.1 Converting Arithmetic Expressions to Postfix Notation</a>
  </dd><dd> <a name="280950"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica3.html#1001126">11.3.2 Converting Postfix Notation to Assembly Language</a>
  </dd><dd> <a name="280952"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica3.html#1001174">11.3.3 Mixed Integer and Floating Point Arithmetic</a>
</dd></dl>
<h2>
  <a name="280954"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica4.html#1001196">11.4 HLA Standard Library Support for Floating Point Arithmetic</a></h2>
<dl>
  <dd> <a name="280956"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica4.html#1001204">11.4.1 The stdin.getf and fileio.getf Functions</a>
  </dd><dd> <a name="280958"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica4.html#1001209">11.4.2 Trigonometric Functions in the HLA Math Library</a>
  </dd><dd> <a name="280960"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica4.html#1001236">11.4.3 Exponential and Logarithmic Functions in the HLA Math Library</a>
</dd></dl>
<h2>
  <a name="280962"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica5.html#998197">11.5 Sample Program</a></h2>
<h2>
  <a name="280964"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/RealArithmetica5.html#1003085">11.6 Putting It All Together</a></h2>
<h2>
  <a name="280966"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TableLookups.html">12.1 Chapter Overview</a></h2>
<h2>
  <a name="280968"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TableLookups.html#1001270">12.2 Tables</a></h2>
<dl>
  <dd> <a name="280970"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TableLookups.html#1001275">12.2.1 Function Computation via Table Look-up</a>
  </dd><dd> <a name="280972"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TableLookups.html#1001377">12.2.2 Domain Conditioning</a>
  </dd><dd> <a name="280974"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TableLookups.html#1001410">12.2.3 Generating Tables</a>
</dd></dl>
<h2>
  <a name="280976"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TableLookups.html#998197">12.3 High Performance Implementation of cs.rangeChar</a></h2>
<h2>
  <a name="280978"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedCtrlStructures.html">1.1 Chapter Overview</a></h2>
<h2>
  <a name="280980"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedCtrlStructures.html#998267">1.2 Conjunction, Disjunction, and Negation in Boolean Expressions</a></h2>
<h2>
  <a name="280982"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedCtrlStructures.html#1002647">1.3 TRY..ENDTRY</a></h2>
<dl>
  <dd> <a name="280984"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedCtrlStructures.html#998303">1.3.1 Nesting TRY..ENDTRY Statements</a>
  </dd><dd> <a name="280986"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedCtrlStructures.html#998406">1.3.2 The UNPROTECTED Clause in a TRY..ENDTRY Statement</a>
  </dd><dd> <a name="280988"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedCtrlStructures.html#998521">1.3.3 The ANYEXCEPTION Clause in a TRY..ENDTRY Statement</a>
  </dd><dd> <a name="280990"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedCtrlStructures.html#998526">1.3.4 Raising User-Defined Exceptions</a>
  </dd><dd> <a name="280992"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedCtrlStructures.html#998588">1.3.5 Reraising Exceptions in a TRY..ENDTRY Statement</a>
  </dd><dd> <a name="280994"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedCtrlStructures.html#998592">1.3.6 A List of the Predefined HLA Exceptions</a>
  </dd><dd> <a name="280996"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedCtrlStructures.html#998596">1.3.7 How to Handle Exceptions in Your Programs</a>
  </dd><dd> <a name="280998"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedCtrlStructures.html#998660">1.3.8 Registers and the TRY..ENDTRY Statement</a>
</dd></dl>
<h2>
  <a name="281000"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedCtrlStructuresa2.html#1006201">1.4 BEGIN..EXIT..EXITIF..END</a></h2>
<h2>
  <a name="281002"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedCtrlStructuresa2.html#998882">1.5 CONTINUE..CONTINUEIF</a></h2>
<h2>
  <a name="281004"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedCtrlStructuresa3.html#998908">1.6 SWITCH..CASE..DEFAULT..ENDSWITCH</a></h2>
<h2>
  <a name="281006"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedCtrlStructuresa3.html#998197">1.7 Putting It All Together</a></h2>
<h2>
  <a name="281008"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructs.html">2.1 Chapter Overview</a></h2>
<h2>
  <a name="281010"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructs.html#998931">2.2 Low Level Control Structures</a></h2>
<h2>
  <a name="281012"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructs.html#998935">2.3 Statement Labels</a></h2>
<h2>
  <a name="281014"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructs.html#998996">2.4 Unconditional Transfer of Control (JMP)</a></h2>
<h2>
  <a name="281016"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructs.html#999101">2.5 The Conditional Jump Instructions</a></h2>
<h2>
  <a name="281018"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructs.html#999571">2.6 "Medium-Level" Control Structures: JT and JF</a></h2>
<h2>
  <a name="281020"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa2.html#1004579">2.7 Implementing Common Control Structures in Assembly Language</a></h2>
<h2>
  <a name="281022"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa2.html#999582">2.8 Introduction to Decisions</a></h2>
<dl>
  <dd> <a name="281024"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa2.html#999642">2.8.1 IF..THEN..ELSE Sequences</a>
  </dd><dd> <a name="281026"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa2.html#999781">2.8.2 Translating HLA IF Statements into Pure Assembly Language</a>
  </dd><dd> <a name="281028"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa2.html#1000014">2.8.3 Implementing Complex IF Statements Using Complete Boolean Evaluation</a>
  </dd><dd> <a name="281030"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa2.html#1000046">2.8.4 Short Circuit Boolean Evaluation</a>
  </dd><dd> <a name="281032"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa2.html#1000096">2.8.5 Short Circuit vs. Complete Boolean Evaluation</a>
  </dd><dd> <a name="281034"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa2.html#1000168">2.8.6 Efficient Implementation of IF Statements in Assembly Language</a>
  </dd><dd> <a name="281036"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa2.html#1000309">2.8.7 SWITCH/CASE Statements</a>
</dd></dl>
<h2>
  <a name="281038"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa3.html#1004580">2.9 State Machines and Indirect Jumps</a></h2>
<h2>
  <a name="281040"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa3.html#1000799">2.10 Spaghetti Code</a></h2>
<h2>
  <a name="281042"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa4.html#1004581">2.11 Loops</a></h2>
<dl>
  <dd> <a name="281044"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa4.html#1000819">2.11.1 While Loops</a>
  </dd><dd> <a name="281046"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa4.html#1000869">2.11.2 Repeat..Until Loops</a>
  </dd><dd> <a name="281048"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa4.html#1000915">2.11.3 FOREVER..ENDFOR Loops</a>
  </dd><dd> <a name="281050"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa4.html#1000945">2.11.4 FOR Loops</a>
  </dd><dd> <a name="281052"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa4.html#1000997">2.11.5 The BREAK and CONTINUE Statements</a>
  </dd><dd> <a name="281054"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa4.html#1001145">2.11.6 Register Usage and Loops</a>
</dd></dl>
<h2>
  <a name="281056"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa5.html#1004582">2.12 Performance Improvements</a></h2>
<dl>
  <dd> <a name="281058"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa5.html#1001192">2.12.1 Moving the Termination Condition to the End of a Loop</a>
  </dd><dd> <a name="281060"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa5.html#1001281">2.12.2 Executing the Loop Backwards</a>
  </dd><dd> <a name="281062"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa5.html#1001317">2.12.3 Loop Invariant Computations</a>
  </dd><dd> <a name="281064"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa5.html#1001363">2.12.4 Unraveling Loops</a>
  </dd><dd> <a name="281066"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa5.html#1001387">2.12.5 Induction Variables</a>
</dd></dl>
<h2>
  <a name="281068"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa6.html#1004583">2.13 Hybrid Control Structures in HLA</a></h2>
<h2>
  <a name="281070"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/LowLevelControlStructsa6.html#998197">2.14 Putting It All Together</a></h2>
<h2>
  <a name="281072"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProcedures.html">3.1 Chapter Overview</a></h2>
<h2>
  <a name="281074"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProcedures.html#998262">3.2 Procedures and the CALL Instruction</a></h2>
<h2>
  <a name="281076"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProcedures.html#998341">3.3 Procedures and the Stack</a></h2>
<h2>
  <a name="281078"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa2.html#1004777">3.4 Activation Records</a></h2>
<h2>
  <a name="281080"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa2.html#998487">3.5 The Standard Entry Sequence</a></h2>
<h2>
  <a name="281082"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa2.html#998519">3.6 The Standard Exit Sequence</a></h2>
<h2>
  <a name="281084"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa2.html#998543">3.7 HLA Local Variables</a></h2>
<h2>
  <a name="281086"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa3.html#998595">3.8 Parameters</a></h2>
<dl>
  <dd> <a name="281088"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa3.html#998603">3.8.1 Pass by Value</a>
  </dd><dd> <a name="281090"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa3.html#998610">3.8.2 Pass by Reference</a>
  </dd><dd> <a name="281092"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa3.html#998645">3.8.3 Passing Parameters in Registers</a>
  </dd><dd> <a name="281094"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa3.html#998689">3.8.4 Passing Parameters in the Code Stream</a>
  </dd><dd> <a name="281096"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa3.html#998787">3.8.5 Passing Parameters on the Stack</a>
  <dl>
    <dd> <a name="281098"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa3.html#998876">3.8.5.1 Accessing Value Parameters on the Stack</a>
    </dd><dd> <a name="281100"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa3.html#998906">3.8.5.2 Passing Value Parameters on the Stack</a>
    </dd><dd> <a name="281102"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa3.html#999053">3.8.5.3 Accessing Reference Parameters on the Stack</a>
    </dd><dd> <a name="281104"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa3.html#999181">3.8.5.4 Passing Reference Parameters on the Stack</a>
    </dd><dd> <a name="281106"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa3.html#999278">3.8.5.5 Passing Formal Parameters as Actual Parameters</a>
    </dd><dd> <a name="281108"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa3.html#999335">3.8.5.6 HLA Hybrid Parameter Passing Facilities</a>
    </dd><dd> <a name="281110"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa3.html#999388">3.8.5.7 Mixing Register and Stack Based Parameters</a>
  </dd></dl>
</dd></dl>
<h2>
  <a name="281112"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa4.html#1004782">3.9 Procedure Pointers</a></h2>
<h2>
  <a name="281114"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa4.html#999481">3.10 Procedural Parameters</a></h2>
<h2>
  <a name="281116"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa4.html#999498">3.11 Untyped Reference Parameters</a></h2>
<h2>
  <a name="281118"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa5.html#1004783">3.12 Iterators and the FOREACH Loop</a></h2>
<h2>
  <a name="281120"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa6.html#1004784">3.13 Sample Programs</a></h2>
<dl>
  <dd> <a name="281122"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa6.html#1001074">3.13.1 Generating the Fibonacci Sequence Using an Iterator</a>
  </dd><dd> <a name="281124"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa6.html#1001163">3.13.2 Outer Product Computation with Procedural Parameters</a>
</dd></dl>
<h2>
  <a name="281126"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/IntermediateProceduresa6.html#1001415">3.14 Putting It All Together</a></h2>
<h2>
  <a name="281128"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetic.html">4.1 Chapter Overview</a></h2>
<h2>
  <a name="281130"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetic.html#998265">4.2 Multiprecision Operations</a></h2>
<dl>
  <dd> <a name="281132"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetic.html#998271">4.2.1 Multiprecision Addition Operations</a>
  </dd><dd> <a name="281134"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetic.html#998360">4.2.2 Multiprecision Subtraction Operations</a>
  </dd><dd> <a name="281136"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetic.html#998408">4.2.3 Extended Precision Comparisons</a>
  </dd><dd> <a name="281138"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica2.html#1007619">4.2.4 Extended Precision Multiplication</a>
  </dd><dd> <a name="281140"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica2.html#998729">4.2.5 Extended Precision Division</a>
  </dd><dd> <a name="281142"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica3.html#1007620">4.2.6 Extended Precision NEG Operations</a>
  </dd><dd> <a name="281144"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica3.html#999140">4.2.7 Extended Precision AND Operations</a>
  </dd><dd> <a name="281146"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica3.html#999152">4.2.8 Extended Precision OR Operations</a>
  </dd><dd> <a name="281148"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica3.html#999166">4.2.9 Extended Precision XOR Operations</a>
  </dd><dd> <a name="281150"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica3.html#999177">4.2.10 Extended Precision NOT Operations</a>
  </dd><dd> <a name="281152"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica3.html#999184">4.2.11 Extended Precision Shift Operations</a>
  </dd><dd> <a name="281154"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica3.html#999280">4.2.12 Extended Precision Rotate Operations</a>
  </dd><dd> <a name="281156"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica4.html#1007621">4.2.13 Extended Precision I/O</a>
  <dl>
    <dd> <a name="281158"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica4.html#999309">4.2.13.1 Extended Precision Hexadecimal Output</a>
    </dd><dd> <a name="281160"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica4.html#999332">4.2.13.2 Extended Precision Unsigned Decimal Output</a>
    </dd><dd> <a name="281162"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica4.html#999497">4.2.13.3 Extended Precision Signed Decimal Output</a>
    </dd><dd> <a name="281164"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica4.html#999535">4.2.13.4 Extended Precision Formatted I/O</a>
    </dd><dd> <a name="281166"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica4.html#999539">4.2.13.5 Extended Precision Input Routines</a>
    </dd><dd> <a name="281168"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica4.html#999641">4.2.13.6 Extended Precision Hexadecimal Input</a>
    </dd><dd> <a name="281170"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica4.html#999853">4.2.13.7 Extended Precision Unsigned Decimal Input</a>
    </dd><dd> <a name="281172"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica4.html#1000078">4.2.13.8 Extended Precision Signed Decimal Input</a>
  </dd></dl>
</dd></dl>
<h2>
  <a name="281174"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica5.html#1007612">4.3 Operating on Different Sized Operands</a></h2>
<h2>
  <a name="281176"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica6.html">4.4 Decimal Arithmetic</a></h2>
<dl>
  <dd> <a name="281178"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica6.html#1000228">4.4.1 Literal BCD Constants</a>
  </dd><dd> <a name="281180"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica6.html#1000233">4.4.2 The 80x86 DAA and DAS Instructions</a>
  </dd><dd> <a name="281182"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica6.html#1000281">4.4.3 The 80x86 AAA, AAS, AAM, and AAD Instructions</a>
  </dd><dd> <a name="281184"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica6.html#1000296">4.4.4 Packed Decimal Arithmetic Using the FPU</a>
</dd></dl>
<h2>
  <a name="281186"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica7.html#1007618">4.5 Sample Program</a></h2>
<h2>
  <a name="281188"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/AdvancedArithmetica7.html#998197">4.6 Putting It All Together</a></h2>
<h2>
  <a name="281190"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/BitManipulation.html">5.1 Chapter Overview</a></h2>
<h2>
  <a name="281192"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/BitManipulation.html#1002653">5.2 What is Bit Data, Anyway?</a></h2>
<h2>
  <a name="281194"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/BitManipulation.html#1002699">5.3 Instructions That Manipulate Bits</a></h2>
<h2>
  <a name="281196"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/BitManipulation.html#1003096">5.4 The Carry Flag as a Bit Accumulator</a></h2>
<h2>
  <a name="281198"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/BitManipulationa2.html#1005792">5.5 Packing and Unpacking Bit Strings</a></h2>
<h2>
  <a name="281200"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/BitManipulationa2.html#1002708">5.6 Coalescing Bit Sets and Distributing Bit Strings</a></h2>
<h2>
  <a name="281202"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/BitManipulationa2.html#1003252">5.7 Packed Arrays of Bit Strings</a></h2>
<h2>
  <a name="281204"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/BitManipulationa2.html#1002659">5.8 Searching for a Bit</a></h2>
<h2>
  <a name="281206"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/BitManipulationa2.html#1002667">5.9 Counting Bits</a></h2>
<h2>
  <a name="281208"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/BitManipulationa2.html#1002673">5.10 Reversing a Bit String</a></h2>
<h2>
  <a name="281210"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/BitManipulationa2.html#1004431">5.11 Merging Bit Strings</a></h2>
<h2>
  <a name="281212"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/BitManipulationa2.html#1004453">5.12 Extracting Bit Strings</a></h2>
<h2>
  <a name="281214"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/BitManipulationa2.html#1002682">5.13 Searching for a Bit Pattern</a></h2>
<h2>
  <a name="281216"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/BitManipulationa3.html#1005793">5.14 The HLA Standard Library Bits Module</a></h2>
<h2>
  <a name="281218"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/BitManipulationa3.html#1002679">5.15 Putting It All Together</a></h2>
<h2>
  <a name="281220"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/StringInstructions.html">6.1 Chapter Overview</a></h2>
<h2>
  <a name="281222"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/StringInstructions.html#998860">6.2 The 80x86 String Instructions</a></h2>
<dl>
  <dd> <a name="281224"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/StringInstructions.html#998872">6.2.1 How the String Instructions Operate</a>
  </dd><dd> <a name="281226"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/StringInstructions.html#998888">6.2.2 The REP/REPE/REPZ and REPNZ/REPNE Prefixes</a>
  </dd><dd> <a name="281228"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/StringInstructions.html#998925">6.2.3 The Direction Flag</a>
  </dd><dd> <a name="281230"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/StringInstructions.html#998982">6.2.4 The MOVS Instruction</a>
  </dd><dd> <a name="281232"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/StringInstructions.html#999086">6.2.5 The CMPS Instruction</a>
  </dd><dd> <a name="281234"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/StringInstructions.html#999151">6.2.6 The SCAS Instruction</a>
  </dd><dd> <a name="281236"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/StringInstructions.html#999169">6.2.7 The STOS Instruction</a>
  </dd><dd> <a name="281238"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/StringInstructions.html#999192">6.2.8 The LODS Instruction</a>
  </dd><dd> <a name="281240"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/StringInstructions.html#999209">6.2.9 Building Complex String Functions from LODS and STOS</a>
</dd></dl>
<h2>
  <a name="281242"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/StringInstructions.html#998840">6.3 Putting It All Together</a></h2>
<h2>
  <a name="281244"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HLACompileTimeLanguage.html">7.1 Chapter Overview</a></h2>
<h2>
  <a name="281246"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HLACompileTimeLanguage.html#998261">7.2 Introduction to the Compile-Time Language (CTL)</a></h2>
<h2>
  <a name="281248"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HLACompileTimeLanguage.html#998289">7.3 The #PRINT and #ERROR Statements</a></h2>
<h2>
  <a name="281250"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HLACompileTimeLanguage.html#998334">7.4 Compile-Time Constants and Variables</a></h2>
<h2>
  <a name="281252"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HLACompileTimeLanguage.html#998355">7.5 Compile-Time Expressions and Operators</a></h2>
<h2>
  <a name="281254"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HLACompileTimeLanguagea2.html#1006163">7.6 Compile-Time Functions</a></h2>
<dl>
  <dd> <a name="281256"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HLACompileTimeLanguagea2.html#998787">7.6.1 Type Conversion Compile-time Functions</a>
  </dd><dd> <a name="281258"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HLACompileTimeLanguagea2.html#998805">7.6.2 Numeric Compile-Time Functions</a>
  </dd><dd> <a name="281260"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HLACompileTimeLanguagea2.html#998816">7.6.3 Character Classification Compile-Time Functions</a>
  </dd><dd> <a name="281262"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HLACompileTimeLanguagea2.html#998824">7.6.4 Compile-Time String Functions</a>
  </dd><dd> <a name="281264"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HLACompileTimeLanguagea2.html#998839">7.6.5 Compile-Time Pattern Matching Functions</a>
  </dd><dd> <a name="281266"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HLACompileTimeLanguagea2.html#998866">7.6.6 Compile-Time Symbol Information</a>
  </dd><dd> <a name="281268"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HLACompileTimeLanguagea2.html#998889">7.6.7 Compile-Time Expression Classification Functions</a>
  </dd><dd> <a name="281270"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HLACompileTimeLanguagea2.html#998898">7.6.8 Miscellaneous Compile-Time Functions</a>
  </dd><dd> <a name="281272"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HLACompileTimeLanguagea2.html#998912">7.6.9 Predefined Compile-Time Variables</a>
  </dd><dd> <a name="281274"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HLACompileTimeLanguagea2.html#998927">7.6.10 Compile-Time Type Conversions of TEXT Objects</a>
</dd></dl>
<h2>
  <a name="281276"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HLACompileTimeLanguagea3.html#998964">7.7 Conditional Compilation (Compile-Time Decisions)</a></h2>
<h2>
  <a name="281278"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HLACompileTimeLanguagea3.html#999072">7.8 Repetitive Compilation (Compile-Time Loops)</a></h2>
<h2>
  <a name="281280"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/HLACompileTimeLanguagea3.html#998197">7.9 Putting It All Together</a></h2>
<h2>
  <a name="281282"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Macros.html">8.1 Chapter Overview</a></h2>
<h2>
  <a name="281284"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Macros.html#999147">8.2 Macros (Compile-Time Procedures)</a></h2>
<dl>
  <dd> <a name="281286"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Macros.html#999150">8.2.1 Standard Macros</a>
  </dd><dd> <a name="281288"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Macros.html#999206">8.2.2 Macro Parameters</a>
  <dl>
    <dd> <a name="281290"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Macros.html#999222">8.2.2.1 Standard Macro Parameter Expansion</a>
    </dd><dd> <a name="281292"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Macros.html#999333">8.2.2.2 Macros with a Variable Number of Parameters</a>
    </dd><dd> <a name="281294"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Macros.html#999411">8.2.2.3 Required Versus Optional Macro Parameters</a>
    </dd><dd> <a name="281296"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Macros.html#999432">8.2.2.4 The "#(" and ")#" Macro Parameter Brackets</a>
    </dd><dd> <a name="281298"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Macros.html#999461">8.2.2.5 Eager vs. Deferred Macro Parameter Evaluation</a>
  </dd></dl>
  </dd><dd> <a name="281300"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Macros.html#999614">8.2.3 Local Symbols in a Macro</a>
  </dd><dd> <a name="281302"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Macros.html#999773">8.2.4 Macros as Compile-Time Procedures</a>
  </dd><dd> <a name="281304"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Macros.html#999785">8.2.5 Multi-part (Context-Free) Macros</a>
  </dd><dd> <a name="281306"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Macros.html#1000000">8.2.6 Simulating Function Overloading with Macros</a>
</dd></dl>
<h2>
  <a name="281308"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Macros2.html#1009074">8.3 Writing Compile-Time "Programs"</a></h2>
<dl>
  <dd> <a name="281310"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Macros2.html#1000254">8.3.1 Constructing Data Tables at Compile Time</a>
  </dd><dd> <a name="281312"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Macros2.html#1000458">8.3.2 Unrolling Loops</a>
</dd></dl>
<h2>
  <a name="281314"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Macros2.html#1004715">8.4 Using Macros in Different Source Files</a></h2>
<h2>
  <a name="281316"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/Macros2.html#998197">8.5 Putting It All Together</a></h2>
<h2>
  <a name="281318"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DSLs.html">9.1 Chapter Overview</a></h2>
<h2>
  <a name="281320"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DSLs.html#1000514">9.2 Introduction to DSELs in HLA</a></h2>
<dl>
  <dd> <a name="281322"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DSLs.html#1000519">9.2.1 Implementing the Standard HLA Control Structures</a>
  <dl>
    <dd> <a name="281324"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DSLs.html#1000523">9.2.1.1 The FOREVER Loop</a>
    </dd><dd> <a name="281326"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DSLs.html#1000654">9.2.1.2 The WHILE Loop</a>
    </dd><dd> <a name="281328"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DSLs.html#1000784">9.2.1.3 The IF Statement</a>
  </dd></dl>
  </dd><dd> <a name="281330"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DSLs2.html#1016090">9.2.2 The HLA SWITCH/CASE Statement</a>
  </dd><dd> <a name="281332"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DSLs3.html#1016095">9.2.3 A Modified WHILE Loop</a>
  </dd><dd> <a name="281334"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DSLs3.html#1001893">9.2.4 A Modified IF..ELSE..ENDIF Statement</a>
</dd></dl>
<h2>
  <a name="281336"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DSLs4.html#1016089">9.3 Sample Program: A Simple Expression Compiler</a></h2>
<h2>
  <a name="281338"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/DSLs4.html#998197">9.4 Putting It All Together</a></h2>
<h2>
  <a name="281340"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjects.html">10.1 Chapter Overview</a></h2>
<h2>
  <a name="281342"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjects.html#998263">10.2 General Principles</a></h2>
<h2>
  <a name="281344"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjects.html#998285">10.3 Classes in HLA</a></h2>
<h2>
  <a name="281346"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjects.html#998346">10.4 Objects</a></h2>
<h2>
  <a name="281348"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjects.html#998375">10.5 Inheritance</a></h2>
<h2>
  <a name="281350"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjects.html#998403">10.6 Overriding</a></h2>
<h2>
  <a name="281352"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjects.html#998446">10.7 Virtual Methods vs. Static Procedures</a></h2>
<h2>
  <a name="281354"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjectsa2.html#1010624">10.8 Writing Class Methods, Iterators, and Procedures</a></h2>
<h2>
  <a name="281356"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjectsa2.html#998490">10.9 Object Implementation</a></h2>
<dl>
  <dd> <a name="281358"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjectsa2.html#998492">10.9.1 Virtual Method Tables</a>
  </dd><dd> <a name="281360"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjectsa2.html#998494">10.9.2 Object Representation with Inheritance</a>
</dd></dl>
<h2>
  <a name="281362"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjectsa3.html#1010625">10.10 Constructors and Object Initialization</a></h2>
<dl>
  <dd> <a name="281364"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjectsa3.html#999424">10.10.1 Dynamic Object Allocation Within the Constructor</a>
  </dd><dd> <a name="281366"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjectsa3.html#999413">10.10.2 Constructors and Inheritance</a>
  </dd><dd> <a name="281368"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjectsa3.html#999435">10.10.3 Constructor Parameters and Procedure Overloading</a>
</dd></dl>
<h2>
  <a name="281370"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjectsa4.html#1010626">10.11 Destructors</a></h2>
<h2>
  <a name="281372"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjectsa4.html#998498">10.12 HLA's "_initialize_" and "_finalize_" Strings</a></h2>
<h2>
  <a name="281374"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjectsa4.html#999764">10.13 Abstract Methods</a></h2>
<h2>
  <a name="281376"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjectsa5.html#1010627">10.14 Run-time Type Information (RTTI)</a></h2>
<h2>
  <a name="281378"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjectsa5.html#1000428">10.15 Calling Base Class Methods</a></h2>
<h2>
  <a name="281380"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/ClassesAndObjectsa5.html#1000463">10.16 Putting It All Together</a></h2>
<h2>
  <a name="281382"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TheMMXInstructionSet.html">11.1 Chapter Overview</a></h2>
<h2>
  <a name="281384"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TheMMXInstructionSet.html#1006134">11.2 Determining if a CPU Supports the MMX Instruction Set</a></h2>
<h2>
  <a name="281386"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TheMMXInstructionSet.html#1006233">11.3 The MMX Programming Environment</a></h2>
<dl>
  <dd> <a name="281388"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TheMMXInstructionSet.html#1005042">11.3.1 The MMX Registers</a>
  </dd><dd> <a name="281390"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TheMMXInstructionSet.html#1005053">11.3.2 The MMX Data Types</a>
</dd></dl>
<h2>
  <a name="281392"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TheMMXInstructionSet.html#1005111">11.4 The Purpose of the MMX Instruction Set</a></h2>
<h2>
  <a name="281394"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TheMMXInstructionSet.html#1005169">11.5 Saturation Arithmetic and Wraparound Mode</a></h2>
<h2>
  <a name="281396"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TheMMXInstructionSet.html#1005195">11.6 MMX Instruction Operands</a></h2>
<h2>
  <a name="281398"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TheMMXInstructionSeta2.html#1011009">11.7 MMX Technology Instructions</a></h2>
<dl>
  <dd> <a name="281400"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TheMMXInstructionSeta2.html#1004344">11.7.1 MMX Data Transfer Instructions</a>
  </dd><dd> <a name="281402"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TheMMXInstructionSeta2.html#1004347">11.7.2 MMX Conversion Instructions</a>
  </dd><dd> <a name="281404"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TheMMXInstructionSeta2.html#1004358">11.7.3 MMX Packed Arithmetic Instructions</a>
  </dd><dd> <a name="281406"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TheMMXInstructionSeta2.html#1006533">11.7.4 MMX Logic Instructions</a>
  </dd><dd> <a name="281408"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TheMMXInstructionSeta2.html#1004376">11.7.5 MMX Comparison Instructions</a>
  </dd><dd> <a name="281410"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TheMMXInstructionSeta2.html#1004388">11.7.6 MMX Shift Instructions</a>
</dd></dl>
<h2>
  <a name="281412"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TheMMXInstructionSeta2.html#1004800">11.8 The EMMS Instruction</a></h2>
<h2>
  <a name="281414"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TheMMXInstructionSeta3.html#1011010">11.9 The MMX Programming Paradigm</a></h2>
<h2>
  <a name="281416"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/TheMMXInstructionSeta3.html#1007023">11.10 Putting It All Together</a></h2>
<h2>
  <a name="281418"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming.html">12.1 Chapter Overview</a></h2>
<h2>
  <a name="281420"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming.html#1007902">12.2 Mixing HLA and MASM/Gas Code in the Same Program</a></h2>
<dl>
  <dd> <a name="281422"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming.html#1008009">12.2.1 In-Line (MASM/Gas) Assembly Code in Your HLA Programs</a>
  </dd><dd> <a name="281424"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming.html#1008008">12.2.2 Linking MASM/Gas-Assembled Modules with HLA Modules</a>
</dd></dl>
<h2>
  <a name="281426"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming2.html#1038288">12.3 Programming in Delphi/Kylix and HLA</a></h2>
<dl>
  <dd> <a name="281428"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming2.html#1008248">12.3.1 Linking HLA Modules With Delphi Programs</a>
  </dd><dd> <a name="281430"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming2.html#1008250">12.3.2 Register Preservation</a>
  </dd><dd> <a name="281432"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming2.html#1008423">12.3.3 Function Results</a>
  </dd><dd> <a name="281434"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming2.html#1008420">12.3.4 Calling Conventions</a>
  </dd><dd> <a name="281436"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming2.html#1009175">12.3.5 Pass by Value, Reference, CONST, and OUT in Delphi</a>
  </dd><dd> <a name="281438"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming2.html#1009207">12.3.6 Scalar Data Type Correspondence Between Delphi and HLA</a>
  </dd><dd> <a name="281440"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming2.html#1009787">12.3.7 Passing String Data Between Delphi and HLA Code</a>
  </dd><dd> <a name="281442"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming2.html#1009854">12.3.8 Passing Record Data Between HLA and Delphi</a>
  </dd><dd> <a name="281444"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming2.html#1010136">12.3.9 Passing Set Data Between Delphi and HLA</a>
  </dd><dd> <a name="281446"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming2.html#1010158">12.3.10 Passing Array Data Between HLA and Delphi</a>
  </dd><dd> <a name="281448"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming2.html#1010914">12.3.11 Referencing Delphi Objects from HLA Code</a>
</dd></dl>
<h2>
  <a name="281450"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming3.html#1030674">12.4 Programming in C/C++ and HLA</a></h2>
<dl>
  <dd> <a name="281452"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming3.html#1014554">12.4.1 Linking HLA Modules With C/C++ Programs</a>
  </dd><dd> <a name="281454"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming3.html#1011249">12.4.2 Register Preservation</a>
  </dd><dd> <a name="281456"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming3.html#1011255">12.4.3 Function Results</a>
  </dd><dd> <a name="281458"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming3.html#1011281">12.4.4 Calling Conventions</a>
  </dd><dd> <a name="281460"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming3.html#1011285">12.4.5 Pass by Value and Reference in C/C++</a>
  </dd><dd> <a name="281462"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming3.html#1011297">12.4.6 Scalar Data Type Correspondence Between C/C++ and HLA</a>
  </dd><dd> <a name="281464"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming3.html#1011756">12.4.7 Passing String Data Between C/C++ and HLA Code</a>
  </dd><dd> <a name="281466"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming3.html#1011783">12.4.8 Passing Record/Structure Data Between HLA and C/C++</a>
  </dd><dd> <a name="281468"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming3.html#1011972">12.4.9 Passing Array Data Between HLA and C/C++</a>
</dd></dl>
<h2>
  <a name="281470"> </a><a href="https://www.plantation-productions.com/Webster/www.artofasm.com/Linux/HTML/MixedLanguageProgramming3.html#1007023">12.5 Putting It All Together</a></h2>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vibe Coding is not an excuse for low-quality work (224 pts)]]></title>
            <link>https://addyo.substack.com/p/vibe-coding-is-not-an-excuse-for</link>
            <guid>43739037</guid>
            <pubDate>Sat, 19 Apr 2025 20:00:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://addyo.substack.com/p/vibe-coding-is-not-an-excuse-for">https://addyo.substack.com/p/vibe-coding-is-not-an-excuse-for</a>, See on <a href="https://news.ycombinator.com/item?id=43739037">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><strong>“Move faster and break even more things.”</strong><span> </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29688e26-bc5a-4f72-9fea-f423288af3d0_2160x1022.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29688e26-bc5a-4f72-9fea-f423288af3d0_2160x1022.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29688e26-bc5a-4f72-9fea-f423288af3d0_2160x1022.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29688e26-bc5a-4f72-9fea-f423288af3d0_2160x1022.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29688e26-bc5a-4f72-9fea-f423288af3d0_2160x1022.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29688e26-bc5a-4f72-9fea-f423288af3d0_2160x1022.png" width="2160" height="1022" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/29688e26-bc5a-4f72-9fea-f423288af3d0_2160x1022.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1022,&quot;width&quot;:2160,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:215932,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://addyo.substack.com/i/161584260?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1e97c40-53bb-47f7-b305-dea867d9a293_2160x2160.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29688e26-bc5a-4f72-9fea-f423288af3d0_2160x1022.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29688e26-bc5a-4f72-9fea-f423288af3d0_2160x1022.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29688e26-bc5a-4f72-9fea-f423288af3d0_2160x1022.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29688e26-bc5a-4f72-9fea-f423288af3d0_2160x1022.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>That twist on Silicon Valley’s old mantra echoes through recent engineering circles as “vibe coding” enters the chat. </span><strong>Yes, AI-assisted development is transforming how we build software, but it’s not a free pass to abandon rigor, review, or craftsmanship.</strong><span> "Vibe coding" is not an excuse for low-quality work.</span></p><p><span>Let’s acknowledge the good: AI-assisted coding </span><em>can</em><span> be a game-changer. It </span><strong>lowers barriers</strong><span> for new programmers and non-programmers, allowing them to produce working software by simply describing what they need. This unblocks creativity – more people can solve their own problems with custom software, part of a trend some call the </span><em><a href="https://addyo.substack.com/p/personal-software-the-unbundling" rel="">unbundling of personal software</a></em><span> (using small AI-built tools instead of one-size-fits-all apps). Even experienced engineers stand to benefit. </span></p><p><span>However, as any seasoned engineer will tell you, </span><strong>speed means nothing if the wheels fall off down the road</strong><span>. And this is where the cracks begin to show – in the gap between the </span><em>vibe</em><span> and the </span><em>reality</em><span> of building maintainable, robust software.</span></p><p><span>For all the hype, vibe coding has earned plenty of skepticism from veteran developers. The core critique: </span><em>just because an AI can spit out code quickly doesn’t mean that code is any good</em><span>. In fact, it can be downright dangerous to accept AI-generated output at face value. The joking complaint that </span><em>“two engineers can now create the tech debt of fifty”</em><span> contains a grain of truth. </span><strong>Unchecked AI-generated code can massively amplify technical debt</strong><span>, the hidden problems that make software brittle and costly to maintain.</span></p><p><span>Many early vibe-coded projects look good on the surface (“it works, ship it!”) but hide a minefield of issues: no error handling, poor performance, questionable security practices, and logically brittle code. One might say such projects are built on </span><strong>sand</strong><span>. I’ve used the term </span><strong><span>“</span><a href="https://addyo.substack.com/p/the-70-problem-hard-truths-about" rel="">house of cards code</a><span>”</span></strong><span> – code that “looks complete but collapses under real-world pressure”. If you’ve ever seen a junior developer’s first big feature that </span><em>almost</em><span> works but crumbles with one unexpected input, you get the idea. AI can churn out a lot of code quickly, but volume ≠ quality.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35589043-ada7-46f5-8243-b9b4430aa2f2_2050x2050.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35589043-ada7-46f5-8243-b9b4430aa2f2_2050x2050.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35589043-ada7-46f5-8243-b9b4430aa2f2_2050x2050.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35589043-ada7-46f5-8243-b9b4430aa2f2_2050x2050.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35589043-ada7-46f5-8243-b9b4430aa2f2_2050x2050.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35589043-ada7-46f5-8243-b9b4430aa2f2_2050x2050.jpeg" width="1456" height="1456" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/35589043-ada7-46f5-8243-b9b4430aa2f2_2050x2050.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:343905,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://addyo.substack.com/i/161584260?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35589043-ada7-46f5-8243-b9b4430aa2f2_2050x2050.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35589043-ada7-46f5-8243-b9b4430aa2f2_2050x2050.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35589043-ada7-46f5-8243-b9b4430aa2f2_2050x2050.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35589043-ada7-46f5-8243-b9b4430aa2f2_2050x2050.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35589043-ada7-46f5-8243-b9b4430aa2f2_2050x2050.jpeg 1456w" sizes="100vw"></picture></div></a></figure></div><p><em><span>"AI is like having a very eager junior developer on your team" an idea well illustrated in this illustration by </span></em></p><p><em><span>.</span></em><span> </span></p><p><span>The dangers aren’t purely hypothetical. Consider maintainability: Who will maintain an AI-written module if it’s obscure or overly complex? If even the original developer doesn’t fully understand the AI’s solution, future modifications become nightmares. Security is another huge concern – AI might generate code that </span><em>appears</em><span> to work but has SQL injection flaws or unsafe error handling. Without rigorous review, these can slip into production. There’s also the risk of </span><strong>overfitting to the prompt</strong><span>: an AI will do exactly what you ask, which might not be exactly what you truly need. Human coders often adjust a design as they implement, discovering misassumptions along the way. AI won’t catch those misassumptions unless the human in the loop notices and corrects it.</span></p><p><span>None of this is to say AI can’t write </span><strong>good</strong><span> code – it sometimes does – but rather that </span><strong>context, scrutiny, and expertise are required to discern good from bad</strong><span>. In 2025, we are essentially using a very eager but inexperienced assistant. You wouldn’t let a first-year junior dev architect your entire system unsupervised; similarly you shouldn’t blindly trust an AI’s code without oversight. The hype of “AI magic” needs to meet the reality of software engineering principles.</span></p><p><span>So, how do we strike the balance? The key is </span><strong>not</strong><span> to throw vibe coding out entirely – it </span><em>can</em><span> be incredibly useful – but to integrate it in a disciplined way. Engineers must approach AI assistance as a tool </span><em>with known limitations</em><span>, not a mystical code genie. In practice, that means keeping the human in the loop and maintaining our standards of quality. Let’s explore what that looks like.</span></p><p><span>To use vibe coding effectively, change your mindset: </span><strong>treat the AI like a super-speedy but junior developer on your team</strong><span>. In other words, you – the senior engineer or team lead – are still the one responsible for the outcome. The AI might crank out the first draft of code, but you must review it with a critical eye, refine it, and verify it meets your quality bar.</span></p><p>Experienced developers who successfully incorporate AI follow this pattern intuitively. When an AI assistant suggests code, they don’t just hit “accept” and move on. Instead, they:</p><ul><li><p><strong>Read and understand</strong><span> what the AI wrote, as if a junior dev on their team wrote it.</span></p></li><li><p><strong>Refactor</strong><span> the code into clean, modular parts if the AI’s output is monolithic or messy (which it often is). Senior engineers will break the AI’s blob into “smaller, focused modules” for clarity.</span></p></li><li><p><strong>Add missing edge-case handling</strong><span>. AI often misses corner cases or error conditions, so the human needs to insert those (null checks, input validation, etc.)</span></p></li><li><p><strong>Strengthen types and interfaces</strong><span>. If the AI used loose types or a leaky abstraction, a human can firm that up, turning implicit assumptions into explicit contracts </span></p></li><li><p><strong>Question the architecture</strong><span>. Did the AI choose an inefficient approach? Maybe it brute-forced something that should use a more optimal algorithm, or perhaps it introduced global state where a pure function would suffice. A human should examine these decisions critically.</span></p></li><li><p><strong>Write tests</strong><span> (or at least, manually test the code’s behavior). Treat AI code like any PR from a coworker: it doesn’t go in until it’s tested. If the AI wrote unit tests (some tools do), double-check those tests aren’t superficial.</span></p></li></ul><p><span>By doing this, you inject </span><em>engineering wisdom</em><span> into the AI-generated code. The combination can be powerful – the AI gets you a lot of code quickly, and your oversight ensures it’s solid. In fact, studies and anecdotes suggest senior devs get </span><strong>more value</strong><span> from AI coding tools than juniors. The reason is clear: seniors have the knowledge to steer the AI properly and fix its mistakes. Juniors may be tempted to treat the AI as an infallible authority, which it isn’t. </span></p><p><span>So, a critical rule emerges: </span><strong>Never accept AI-written code into your codebase unreviewed.</strong><span> Treat it like code from a new hire: inspect every line, ensure you </span><em>get it</em><span>. If something doesn’t make sense to you, don’t assume the AI knows better – often it doesn’t. Either refine the prompt to have the AI clarify, or rewrite that part yourself. Consider AI output as a draft that </span><strong>must</strong><span> go through code review (even if that review is just you). On a team, this means if a developer used AI to generate a chunk of code, they should be prepared to explain and defend it in the code review with peers. “It works, trust me” won’t fly – the team needs confidence that the code is understandable and maintainable by humans.</span></p><p><span>Another best practice: </span><strong>keep humans in the driver’s seat of design</strong><span>. Use the AI to implement, not to decide fundamental architectures. For example, you might use vibe coding to quickly create a CRUD REST API based on an existing schema – that’s well-defined work. But you shouldn’t ask the AI to “design a scalable microservice architecture for our product” and then blindly follow it. High-level design and critical decisions should remain human-led, with AI as a helper for the tedious parts. In essence, let the AI handle the </span><em>grunt work</em><span>, not the </span><em>brain work</em><span>. </span></p><p><strong>Communication and documentation</strong><span> also become crucial. If you prompt an AI to generate a non-trivial algorithm or use an unfamiliar library, take the time to document </span><em>why</em><span> that solution was chosen (just as you would if you wrote it yourself after research). Future maintainers – or your future self – shouldn’t be left guessing about the intent behind AI-crafted code. Some teams even log the prompts used to generate important code, effectively documenting the “conversation” that led to the code. This can help when debugging later: you can see the assumptions that were given to the AI.</span></p><p><span>In summary, </span><strong>human oversight isn’t a “nice-to-have” – it’s mandatory</strong><span>. The moment you remove the human from the loop, you’re just rolling dice on your software quality. Until AI can truly replace a senior engineer’s holistic understanding (we’re not there yet), vibe coding must be a partnership: AI accelerates, human validates.</span></p><p><span>Let’s crystallize the discussion into some actionable rules and best practices for teams adopting AI-assisted development. Think of these as the new </span><em>“move fast, but don’t break everything”</em><span> handbook – a set of guardrails to keep </span><strong>quality</strong><span> high when you’re vibing with the code.</span></p><p><strong>Rule 1: Always Review AI-Generated Code</strong><span> – No exceptions. Every block of code that AI produces should be treated as if a junior engineer wrote it. Do a code review either individually or with a peer. This includes code from Copilot, ChatGPT, Cursor, or any AI agent. If you don’t have time to review it, you don’t have time to use it. Blindly merging AI code is asking for trouble.</span></p><p><strong>Rule 2: Establish Coding Standards and Follow Them</strong><span> – AI tools will mimic whatever code they were trained on, which is a mixed bag. Define your team’s style guides, architecture patterns, and best practices, and ensure that any AI-generated code is refactored to comply. For instance, if your rule is “all functions need JSDoc comments and unit tests,” then AI output must get those comments and tests before it’s done. If your project uses a specific architecture (say, layered architecture with service/repository classes), don’t let the AI shove some ad-hoc database calls in UI code – fix it to fit your layers. Consider creating </span><strong>linting or static analysis checks</strong><span> specifically for common AI mistakes (e.g. flagging use of deprecated APIs or overly complex functions). This automates quality control on AI contributions.</span></p><p><strong>Rule 3: Use AI for Acceleration, Not Autopilot</strong><span> – In practice, this means use vibe coding to speed up well-understood tasks, not to do thinking for you. Great uses: generate boilerplate, scaffold a component, translate one language to another, draft a simple algorithm from pseudocode. Risky uses: have the AI design your module from scratch with minimal guidance, or generate code in a domain you don’t understand at all (you won’t know if it’s wrong). If you intend to keep the code, don’t stay in vibe mode – switch into engineering mode and tighten it up.</span></p><p><strong>Rule 4: Test, Test, Test</strong><span> – AI doesn’t magically guarantee correctness. Write tests for all critical paths of AI-written code. If the AI wrote the code, it may even help you write some tests – but don’t rely solely on AI-generated tests, as they might miss edge cases (or could be falsely passing due to the same flawed logic). Do manual testing too, especially for user-facing features: click through the UI, try odd inputs, see how it behaves. Many vibe-coded applications work fine for the “happy path” but fall apart with unexpected input – you want to catch that before your users do. </span></p><p><strong>Rule 5: Iterate and Refine</strong><span> – Don’t accept the first thing the AI gives you if it’s not satisfactory. Vibe coding is an iterative dialogue. If the initial output is clunky or confusing, you can prompt the AI to improve it (“simplify this code,” “split this into smaller functions,” etc.). Or you can take the draft and refactor it yourself. Often, a good approach is using the AI in </span><strong>cycles</strong><span>: prompt for an implementation, identify weaknesses, then either prompt fixes or manually adjust, and repeat. </span></p><p><strong>Rule 6: Know When to Say No</strong><span> – Sometimes, vibe coding just isn’t the right tool. Part of using it responsibly is recognizing scenarios where manual coding or deeper design work is needed. For example, if you’re dealing with a critical security module, you probably want to design it carefully and maybe only use AI to assist with small pieces (if at all). Or if the AI keeps producing a convoluted solution to a simple problem, stop and write it yourself – you might save time in the end. It’s important not to become overly reliant on the AI to solve every problem. </span><strong>Don’t let “AI did it” become an excuse for not understanding your own code.</strong><span> If after a few attempts the AI isn’t producing what you need, take back control and code it the old-fashioned way; you’ll at least have full understanding then.</span></p><p><strong>Rule 7: Document and Share Knowledge</strong><span> – Ensure that any code coming from AI is documented just as thoroughly as hand-written code (if not more). If there were non-obvious decisions or if you suspect others might be confused by what the AI produced, add comments. In team discussions, be open about what was AI-generated and what wasn’t. This helps reviewers pay extra attention to those sections. </span></p><p><span>Following these rules, teams can enjoy the productivity perks of vibe coding while mitigating the worst risks. Think of it as </span><strong>augmenting</strong><span> human developers, not replacing them. The goal is to </span><strong>co-create</strong><span> with AI: let it handle the repetitive drudge work at high speed, while humans handle the creative and critical thinking parts.</span></p><p><span>It’s also important to recognize </span><strong>where vibe coding shines and where it doesn’t</strong><span>. Not every project or task is equally suited to an AI-driven workflow. Here’s a breakdown drawn from industry discussions and early experiences:</span></p><p><strong>👍 Great Use Cases:</strong><span> </span></p><ul><li><p><em>Rapid prototyping</em><span> is perhaps the sweet spot of vibe coding. If you have an idea for a small app or feature, using an AI assistant to throw together a quick prototype or proof-of-concept can be incredibly effective. In such cases, you don’t mind if the code is a bit hacky; you just want to validate the idea. Many engineers have found success in weekend projects using only AI to code – a fun way to test a concept fast. Another good use case is </span><strong>one-off scripts or internal tools</strong><span>: e.g., a script to parse a log file, a small tool to automate a personal task, or an internal dashboard for your team. These are typically low-stakes; if the script breaks, it’s not the end of the world. Here, vibe coding can save time because you don’t need production-grade perfection, just something that works for now.</span></p></li><li><p><span>Vibe coding also works well for </span><strong>learning and exploration</strong><span>. If you’re working in a new language or API, asking an AI to generate examples can accelerate your learning. (Of course, double-check the AI’s output against official docs!) . In exploratory mode, even if the AI’s code isn’t perfect, it gives you material to tinker with and learn from. It’s like having a teaching assistant who can show you attempts, which you then refine.</span></p></li><li><p><span>Additionally, AI code generation can excel at </span><strong>structured, boilerplate-heavy tasks</strong><span>. Need to create 10 similar data classes? Or implement a rote CRUD layer? AI is great at that kind of mechanical repetition, freeing you from the tedium. As long as the pattern is clear, the AI will follow it and save you keystrokes (just verify it followed the pattern correctly).</span></p></li></ul><p><strong>👎 Not-So-Great Use Cases:</strong><span> </span></p><ul><li><p><span>On the flip side, </span><em>enterprise-grade software and complex systems</em><span> are where vibe coding often falls flat. Anything that requires a deep understanding of business logic, heavy concurrency, rigorous security, or compliance is not something to fully trust to AI generation. The AI doesn’t know your business constraints or performance requirements unless you explicitly spell them out (and even then, it may not get it right). For example, a fintech application handling payments or an aerospace control system must meet standards that current AI simply isn’t equipped to guarantee. In these domains, AI can assist in parts, but human expertise and careful QA are paramount for the final product.</span></p></li><li><p><span>Vibe coding also struggles with </span><strong>long-term maintainability</strong><span>. If you’re building a codebase that will live for years and be worked on by many developers, starting it off with a hodge-podge of AI-generated code can be a poor foundation. Without strong guidance, the architecture might be inconsistent. It’s often better to spend extra time up front building a clean framework (with or without AI help) than to patchwork a whole product via successive AI prompts. Many early adopters have observed that the initial time saved by vibe coding can be lost later during code cleanup and refactoring when the project needs to scale or adapt.</span></p></li><li><p><span>Another scenario to be wary of is </span><strong>critical algorithms or optimizations</strong><span>. AI can produce a sorting algorithm or a database query, sure – but if you need it to be highly optimized (say, a custom memory management routine, or an algorithm that must run in sub-linear time), you’re in territory where human ingenuity and deep understanding are still superior. AI might give you something that works on small data, but falls over at scale, and it won’t necessarily warn you about that. A human performance engineer would design and test with those considerations in mind from the start.</span></p></li><li><p><span>Finally, any situation where </span><strong>explainability and clarity</strong><span> are top priorities might not be ideal for vibe coding. Sometimes you need code that other people (or auditors) can easily read and reason about. If the AI comes up with a convoluted approach, it could hinder clarity. Until AI can reliably produce simple and clearly structured code (which it’s not always incentivized to do – sometimes it’s overly verbose or oddly abstract), a human touch is needed to keep things straightforward.</span></p></li></ul><p><span>In summary, </span><em>vibe coding is a powerful accelerator, but not a silver bullet.</em><span> </span></p><p><span>Use it where speed matters more than polish, and where you have the leeway to iterate and fix things. </span><strong>Avoid using it as a one-shot solution for mission-critical software</strong><span> – that’s like hiring a race car driver to drive a school bus; wrong tool for the job. Maybe one day AI will be so advanced that vibe coding truly can be the default for all development – but </span><em>today is not that day</em><span>. Today, it works best as a helper for the right problems and with the right oversight.</span></p><p><span>Vibe coding, and AI-assisted software development in general, represents a thrilling leap forward in our tools. It’s </span><strong>here to stay</strong><span>, and it will only get more sophisticated from here. Forward-looking engineering teams shouldn’t ignore it – those who harness AI effectively will likely outpace those who don’t, just as teams that embraced earlier waves of automation and better frameworks outpaced those writing everything from scratch. The message of this article is </span><em>not</em><span> to reject vibe coding, but to </span><strong>approach it with eyes open and with engineering discipline intact</strong><span>.</span></p><p><span>The big takeaway is that </span><strong>speed means nothing without quality</strong><span>. Shipping buggy, unmaintainable code faster is a false victory – you’re just speeding towards a cliff. The best engineers will balance the two: using AI to move faster </span><em>without</em><span> breaking things (at least not breaking things any more than we already do!). It’s about finding that sweet spot where AI does the heavy lifting and humans ensure everything stands up properly.</span></p><p><span>For tech leads and engineering managers, the call to action is clear: set the tone that AI is a tool to be used </span><em>responsibly</em><span>. Encourage experimentation with vibe coding, but also establish the expectations (perhaps via some of the rules we outlined) that safeguard your codebase. Make code reviews mandatory for AI-generated contributions, create an environment where asking “hey, does this make sense?” is welcome, and invest in upskilling your team to work </span><em>with</em><span> AI effectively. This might even mean training developers on how to write good prompts or how to evaluate AI suggestions critically. It’s a new skill set, akin to the shift to high-level languages or to distributed version control in the past – those who adapt sooner will reap benefits.</span></p><p>We should also keep in perspective what truly matters in software engineering: solving user problems, creating reliable systems, and continuously learning. Vibe coding is a means to an end, not an end itself. If it helps us serve users better and faster, fantastic. But if it tempts us to skip the due diligence that users ultimately rely on (like quality and security), then we must rein it in. The fundamentals – clear thinking, understanding requirements, designing for change, testing thoroughly – remain as important as ever, if not more so.</p><p><span>In the end, perhaps the ethos should be: </span><strong>“Move fast, but don’t break things – or if you do, make sure you know how to fix them.”</strong><span> Leverage the vibes to code at light speed, but back it up with the solid bedrock of engineering excellence. AI can coexist with craftsmanship; in fact, in the hands of a craftsman, it can be a powerful chisel. But the craftsman’s hand is still needed to guide that chisel to create something truly enduring and well-made.</span></p><p><span>So, vibe on, developers – just do it with care. Embrace the future, but don’t abandon the principles that got us here. </span><strong>Vibe coding is not an excuse for low-quality work</strong><span>; rather, it’s an opportunity to elevate what we can achieve </span><em>when we pair human judgment with machine generative power</em><span>. The teams that internalize this will not only move fast – they’ll build things worth keeping. </span></p><p><span>Happy coding, and keep the vibes high </span><em>and</em><span> the quality higher.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd180e6c3-45f2-45f2-8b12-af5bccbf8aa1_1890x1890.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd180e6c3-45f2-45f2-8b12-af5bccbf8aa1_1890x1890.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd180e6c3-45f2-45f2-8b12-af5bccbf8aa1_1890x1890.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd180e6c3-45f2-45f2-8b12-af5bccbf8aa1_1890x1890.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd180e6c3-45f2-45f2-8b12-af5bccbf8aa1_1890x1890.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd180e6c3-45f2-45f2-8b12-af5bccbf8aa1_1890x1890.png" width="1456" height="1456" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d180e6c3-45f2-45f2-8b12-af5bccbf8aa1_1890x1890.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:159543,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://addyo.substack.com/i/161584260?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd180e6c3-45f2-45f2-8b12-af5bccbf8aa1_1890x1890.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd180e6c3-45f2-45f2-8b12-af5bccbf8aa1_1890x1890.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd180e6c3-45f2-45f2-8b12-af5bccbf8aa1_1890x1890.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd180e6c3-45f2-45f2-8b12-af5bccbf8aa1_1890x1890.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd180e6c3-45f2-45f2-8b12-af5bccbf8aa1_1890x1890.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unpowered SSD endurance investigation finds data loss and performance issues (109 pts)]]></title>
            <link>https://www.tomshardware.com/pc-components/storage/unpowered-ssd-endurance-investigation-finds-severe-data-loss-and-performance-issues-reminds-us-of-the-importance-of-refreshing-backups</link>
            <guid>43739028</guid>
            <pubDate>Sat, 19 Apr 2025 19:59:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/pc-components/storage/unpowered-ssd-endurance-investigation-finds-severe-data-loss-and-performance-issues-reminds-us-of-the-importance-of-refreshing-backups">https://www.tomshardware.com/pc-components/storage/unpowered-ssd-endurance-investigation-finds-severe-data-loss-and-performance-issues-reminds-us-of-the-importance-of-refreshing-backups</a>, See on <a href="https://news.ycombinator.com/item?id=43739028">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/W7vXqDuaaPJa4CV6hZQyGV-994-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/W7vXqDuaaPJa4CV6hZQyGV-994-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/W7vXqDuaaPJa4CV6hZQyGV-994-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/W7vXqDuaaPJa4CV6hZQyGV-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/W7vXqDuaaPJa4CV6hZQyGV-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/W7vXqDuaaPJa4CV6hZQyGV-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/W7vXqDuaaPJa4CV6hZQyGV-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/W7vXqDuaaPJa4CV6hZQyGV.jpg" alt="Long-term SSD durability tests" srcset="https://cdn.mos.cms.futurecdn.net/W7vXqDuaaPJa4CV6hZQyGV-994-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/W7vXqDuaaPJa4CV6hZQyGV-994-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/W7vXqDuaaPJa4CV6hZQyGV-994-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/W7vXqDuaaPJa4CV6hZQyGV-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/W7vXqDuaaPJa4CV6hZQyGV-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/W7vXqDuaaPJa4CV6hZQyGV-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/W7vXqDuaaPJa4CV6hZQyGV-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/W7vXqDuaaPJa4CV6hZQyGV.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/W7vXqDuaaPJa4CV6hZQyGV.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>(Image credit: HTWingNut)</span>
</figcaption>
</div>

<div id="article-body">
<p>You may not know it, but SSDs will lose data after a period of time if they are simply left unplugged, which can be a serious threat to your data if you store backups or precious files on unplugged SSDs. A year-two update on the <a data-analytics-id="inline-link" href="https://www.youtube.com/watch?v=rx3Y5x6uzKQ" data-url="https://www.youtube.com/watch?v=rx3Y5x6uzKQ" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">how long can SSDs store data unpowered</a> video series is another reminder about the importance of regularly refreshing your backups with a bit of juice. The tests consist of storing data on an SSD and then leaving it unplugged for years to see the impact on the stored data.</p><p>As a reminder, an SSD's endurance rating is calculated based on how long it can store data if left unplugged after a certain amount of data has been written, hence the importance of this testing.</p><p>TechTuber HTWingNut is back with a report on his modest experiment involving a quartet of SATA SSDs. The key finding was that the two-year-old, well-worn drive exhibited noticeable performance degradation and was affected by a handful of corrupt files. These are signs that this particular SSD was on its way to silicon heaven. HTWingNut's video is an update on an episode from a year earlier, and further updates are promised.</p><div data-nosnippet="">
<div>
<p><span>How Long Can SSD Store Data Unpowered? Year 2 Update - YouTube</span>
<img src="https://img.youtube.com/vi/rx3Y5x6uzKQ/maxresdefault.jpg" alt="How Long Can SSD Store Data Unpowered? Year 2 Update - YouTube" data-aspect-ratio="16/9" loading="lazy">
</p>
</div>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 234.67 165.33"><path fill="red" d="M229.763 25.817c-2.699-10.162-10.65-18.165-20.748-20.881C190.716 0 117.333 0 117.333 0S43.951 0 25.651 4.936C15.553 7.652 7.6 15.655 4.903 25.817 0 44.236 0 82.667 0 82.667s0 38.429 4.903 56.85C7.6 149.68 15.553 157.681 25.65 160.4c18.3 4.934 91.682 4.934 91.682 4.934s73.383 0 91.682-4.934c10.098-2.718 18.049-10.72 20.748-20.882 4.904-18.421 4.904-56.85 4.904-56.85s0-38.431-4.904-56.85"></path><path fill="#fff" d="m93.333 117.559 61.333-34.89-61.333-34.894z"></path></svg>
<a href="https://youtu.be/rx3Y5x6uzKQ" target="_blank" data-url="https://youtu.be/rx3Y5x6uzKQ" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">Watch On <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 507.9 113.39"><g fill="#fff"><path d="M64.792 80.99V32.396l42.082 24.297zm93.803-63.285a20.285 20.285 0 0 0-14.32-14.32C131.642 0 80.99 0 80.99 0S30.337 0 17.705 3.385a20.286 20.286 0 0 0-14.32 14.32C0 30.338 0 56.693 0 56.693S0 83.049 3.385 95.68A20.285 20.285 0 0 0 17.705 110c12.632 3.386 63.285 3.386 63.285 3.386s50.652 0 63.285-3.386a20.284 20.284 0 0 0 14.32-14.32c3.385-12.632 3.385-38.988 3.385-38.988s0-26.355-3.385-38.988m94.473 74.326c.887-2.314 1.332-6.098 1.332-11.35V58.556c0-5.097-.445-8.822-1.332-11.178-.888-2.355-2.452-3.533-4.69-3.533-2.163 0-3.69 1.178-4.577 3.533-.888 2.356-1.332 6.081-1.332 11.178V80.68c0 5.25.424 9.035 1.275 11.35.848 2.318 2.392 3.475 4.633 3.475 2.239 0 3.803-1.157 4.691-3.475zm-17.953 11.122c-3.207-2.16-5.486-5.52-6.835-10.079-1.352-4.554-2.027-10.617-2.027-18.185v-10.31c0-7.644.771-13.784 2.316-18.417 1.544-4.633 3.956-8.011 7.24-10.135 3.282-2.123 7.587-3.186 12.916-3.186 5.251 0 9.459 1.082 12.626 3.243 3.165 2.162 5.482 5.542 6.95 10.136 1.466 4.595 2.2 10.715 2.2 18.36v10.31c0 7.567-.714 13.65-2.142 18.243-1.43 4.595-3.747 7.955-6.951 10.077-3.205 2.124-7.548 3.186-13.03 3.186-5.64 0-10.06-1.082-13.263-3.243m248.053-57.981c-.81 1.005-1.352 2.646-1.621 4.923-.272 2.278-.404 5.734-.404 10.367v5.097h11.697V60.46c0-4.555-.155-8.011-.463-10.367-.309-2.355-.868-4.014-1.678-4.98-.812-.966-2.067-1.449-3.766-1.449-1.7 0-2.954.503-3.765 1.506zm-2.025 29.886v3.591c0 4.557.132 7.974.404 10.251.269 2.279.828 3.94 1.68 4.982.849 1.041 2.16 1.564 3.938 1.564 2.392 0 4.035-.927 4.923-2.781.887-1.853 1.37-4.942 1.447-9.268l13.785.812c.077.62.116 1.469.116 2.548 0 6.565-1.795 11.47-5.387 14.712-3.589 3.242-8.669 4.865-15.232 4.865-7.876 0-13.398-2.47-16.564-7.414-3.168-4.94-4.75-12.586-4.75-22.935V63.589c0-10.657 1.641-18.436 4.924-23.342 3.281-4.903 8.9-7.355 16.854-7.355 5.482 0 9.691 1.004 12.626 3.012 2.933 2.01 5 5.137 6.197 9.383 1.197 4.247 1.796 10.117 1.796 17.607v12.163h-26.757m-284.953-1.33-18.187-65.68h15.869l6.37 29.77c1.623 7.339 2.82 13.594 3.591 18.766h.464c.54-3.706 1.738-9.922 3.591-18.65l6.603-29.886h15.869l-18.417 65.68v31.51h-15.754v-31.51M322.115 34.23v71.007h-12.511l-1.39-8.688h-.347c-3.399 6.564-8.496 9.845-15.291 9.845-4.71 0-8.185-1.543-10.425-4.633-2.24-3.087-3.359-7.915-3.359-14.48V34.23h15.985v52.126c0 3.168.348 5.426 1.043 6.776.695 1.353 1.853 2.027 3.475 2.027 1.39 0 2.722-.423 3.996-1.275 1.274-.849 2.22-1.928 2.838-3.241V34.229h15.986m81.995.001v71.007h-12.511l-1.391-8.688h-.345c-3.402 6.564-8.498 9.845-15.292 9.845-4.711 0-8.186-1.543-10.426-4.633-2.24-3.087-3.358-7.915-3.358-14.48V34.23h15.985v52.126c0 3.168.347 5.426 1.041 6.776.696 1.353 1.855 2.027 3.476 2.027 1.391 0 2.723-.423 3.996-1.275 1.275-.849 2.22-1.928 2.839-3.241V34.229h15.985"></path><path d="M365.552 20.908h-15.87v84.329h-15.637v-84.33h-15.869V8.05h47.376v12.858m76.811 53.636c0 5.174-.215 9.229-.639 12.162-.424 2.937-1.139 5.021-2.143 6.255-1.004 1.236-2.357 1.854-4.053 1.854a7.404 7.404 0 0 1-3.65-.927c-1.12-.618-2.026-1.544-2.722-2.78V50.796c.54-1.93 1.467-3.513 2.78-4.749 1.313-1.234 2.74-1.853 4.285-1.853 1.623 0 2.876.637 3.766 1.91.886 1.275 1.505 3.418 1.853 6.43.348 3.011.523 7.297.523 12.857zm14.652-28.964c-.967-4.478-2.531-7.721-4.692-9.73-2.163-2.007-5.136-3.011-8.919-3.011-2.935 0-5.676.83-8.224 2.49a16.926 16.926 0 0 0-5.908 6.545h-.117l.001-37.416h-15.405v100.777h13.204l1.622-6.717h.347c1.235 2.393 3.088 4.285 5.56 5.675 2.47 1.39 5.213 2.085 8.225 2.085 5.404 0 9.382-2.491 11.931-7.471 2.548-4.982 3.823-12.76 3.823-23.341V64.23c0-7.953-.484-14.17-1.448-18.65"></path></g></svg></a>
</div><p>The four tested 'Leven JS-600' branded SSDs are basically bog-standard no-name units. HTWingNut says they are all TLC SSDs of 128GB capacity and rated to withstand 60 TB of written data. Every drive has 100GB of files containing random data, with hash values for all the content provided for later verification.</p><p>The two 'Fresh' samples have barely been used; perhaps only the 100GB data set was put on there and verified, and that's it. Meanwhile, the 'Worn' drives have been subjected to 280 terabytes of written data churn, much more than their rated 60 Terabytes Written (TBW) endurance rating..</p><p>If you watch the previous <a data-analytics-id="inline-link" href="https://www.youtube.com/watch?v=igJK5YDb73w" data-url="https://www.youtube.com/watch?v=igJK5YDb73w" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">year-one video</a>, you will have seen there were no issues with either 'Worn' or 'Fresh' drives. However, time has now taken its toll. Let's take a look at the year-two samples in turn.</p><h2 id="fresh-ssd-tests-3">'Fresh' SSD tests</h2><p>The data on this SSD, which hadn't been used or powered up for two years, was 100% good on initial inspection. All the data hashes verified, but it was noted that the verification time took a smidgen longer than two years previously. HD Sentinel tests also showed good, consistent performance for a SATA SSD.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-7yDTgvWNkW6o6Rmwfza8wm"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div><p>Digging deeper, all isn't well, though. Firing up Crystal Disk Info, HTWingNut noted that this SSD had a Hardware ECC Recovered value of over 400. In other words, the disk's error correction had to step in to fix hundreds of data-based parity bits.</p><p>Seeing these errors means "the SSD is on its way out," according to HTWingNut. Indeed, if there is anything iffy about your data storage integrity, it is at least a warning. However, the errors could also have something to do with the drive being left unpowered for two years.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/ymKMWvrmR7iadoC8R5stGV-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/ymKMWvrmR7iadoC8R5stGV-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/ymKMWvrmR7iadoC8R5stGV-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/ymKMWvrmR7iadoC8R5stGV-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/ymKMWvrmR7iadoC8R5stGV-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/ymKMWvrmR7iadoC8R5stGV-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/ymKMWvrmR7iadoC8R5stGV.jpg" alt="Long-term SSD durability tests" srcset="https://cdn.mos.cms.futurecdn.net/ymKMWvrmR7iadoC8R5stGV-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/ymKMWvrmR7iadoC8R5stGV-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/ymKMWvrmR7iadoC8R5stGV-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/ymKMWvrmR7iadoC8R5stGV-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/ymKMWvrmR7iadoC8R5stGV-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/ymKMWvrmR7iadoC8R5stGV-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/ymKMWvrmR7iadoC8R5stGV.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/ymKMWvrmR7iadoC8R5stGV.jpg"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: HTWingNut)</span></figcaption></figure><h2 id="worn-ssd-tests-3">'Worn' SSD tests</h2><p>As the worn SSD's data was being verified, there were already signs of performance degradation. The hashing audit eventually revealed that four files were corrupt (hash not matching). Looking at the elapsed time, it was observed that this operation astonishingly took over 4x longer, up from 10 minutes and 3 seconds to 42 minutes and 43 seconds.</p><p>Further investigations in HD Sentinel showed that three out of 10,000 sectors were bad and performance was 'spiky.' Returning to Crystal Disk Info, things look even worse. HTWingNut notes that the uncorrectable sectors count went from 0 to 12 on this drive, and the hardware ECC recovered value went from 11,745 before to 201,273 after tests on the day.</p><h2 id="some-takeaways-3">Some takeaways</h2><p>In summary, the year-one fresh and well-worn drives had no issues. However, the year-two heavily worn SSD had file corruption and performance was poor. The so-called fresh drive was still good, but ECC figures still raised concern. Come back in late 2025 for the next update from HTWingNut.</p><p>We also want to say this is a very small test sample, highlighted out of our interest in the topic rather than for its hard empirical data. I have also experienced SSD data loss after leaving a Mini PC unpowered for just six months or so at my pied-à-terre in Taiwan. On return, Windows refused to boot or be repaired, but a reformat and reinstall seemed to return everything to normal.</p><p><em>Follow&nbsp;</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank" data-url="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u><em>Tom's Hardware on Google News</em></u></a><em>&nbsp;to get our up-to-date news, analysis and reviews in your <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tag/google" data-auto-tag-linker="true" data-before-rewrite-localise="https://www.tomshardware.com/tag/google">Google</a> feeds. Make sure to click the Follow button.</em></p>
</div>



<!-- Drop in a standard article here maybe? -->


<div id="slice-container-authorBio-7yDTgvWNkW6o6Rmwfza8wm"><p>Mark Tyson is a news editor at Tom's Hardware. He enjoys covering the full breadth of PC tech; from business and semiconductor design to products approaching the edge of reason.</p></div>
</section>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Icelandic Voting System (2024) (118 pts)]]></title>
            <link>https://smarimccarthy.is/posts/2024-11-25-voting-system/</link>
            <guid>43738675</guid>
            <pubDate>Sat, 19 Apr 2025 19:10:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://smarimccarthy.is/posts/2024-11-25-voting-system/">https://smarimccarthy.is/posts/2024-11-25-voting-system/</a>, See on <a href="https://news.ycombinator.com/item?id=43738675">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><p>It’s election season here in Iceland! The election is on Saturday, 30th of November, so next Saturday from when this is written.</p>
<p>Every time elections are upcoming, somebody inevitably asks me how the voting system here works, probably expecting a simple answer. So, here’s a stab at it.</p>
<p>Iceland uses a biproportional apportionment system, as do Norway, some cantons of Switzerland, some German regions, and a few other places. Such systems have a few general features:</p>
<ul>
<li>There is some number of constituencies, each of which has:
<ul>
<li>Some number of constituency seats (CS)</li>
<li>Some number of adjustment seats (AS)</li>
</ul>
</li>
</ul>
<p>Technically, you could have 0 constituency seats, but most places have some. In Iceland every constituency must have either 1 or 2 adjustment seats. So like, we have Southwest with 14 seats total, of which 2 are AS, while Northwest has 7 total, 6 CS and 1 AS. These are decided after each election (for the next election) by the national electoral commission, and take into account population in each constituency, so as to try to minimize the disparity of voting power.</p>
<h2 id="divisor-rules">Divisor rules</h2>
<p>For this kind of system, you have a divisor rule, such as d’Hondt or Sainte-Lague. We use d’Hondt in Iceland. Scandinavia uses a modified Sainte-Lague. The rule is basically a quotient between votes and seats. d’Hondt is</p>
<p>$q = V/(s+1)$</p>
<p>Where q is the quotient, V the number of votes a party gets, and s the number of seats they’ve already been allocated.</p>
<p>By comparison, the Sainte-Lague method is:</p>
<p>$q = V / (2s+1)$</p>
<p>Norway and Sweden use a modified Sainte-Lague, which starts at 1.5 instead of 1, so you get a sequence of 1.5, 3, 5, …. There are a number of <a href="https://github.com/smari/voting/blob/master/backend/division_rules.py" target="_blank" rel="noopener noreffer">other divisor rules</a>, each optimized for a slightly different use case.</p>
<p>The key difference here is that by scaling up the divisor, you’re valuing each seat much higher, resulting in it becoming proportionally easier to get the first seat, etc.</p>
<p>It should be said that in some ways d’Hondt is the least fair of them, clearly favoring big parties, but it’s also the only such rule that actually adds up in such a way that there’s no strategic benefit to breaking a large party up into many smaller parties. You win some, you lose some.</p>
<h2 id="allocating-the-constituency-seats">Allocating the constituency seats</h2>
<p>Now that we have our divisor rule, we should start allocating, by calculating each constituency’s seats.</p>
<p>For each constituency, each party running in that constituency starts with 0 seats, so every party’s divisor is 1 (in d’Hondt). You then look at which party has the highest quotient ($q$), and you give them a seat.</p>
<p>Now recalculate the quotients (really only the one you just allocated to, the others remain unchanged). Oh, look, now the party that got a seat has a lower $q$, because their divisor is bigger. So somebody else (probably) has the highest $q$. Give them a seat. Continue until all constituency seats are allocated.</p>
<h2 id="allocating-the-adjustment-seats">Allocating the adjustment seats</h2>
<p>This is where the fun begins – the real biproportional part. Once all the constituency seats have been allocated, the next step is to calculate for the entire country. So we add up each party’s votes over the entire country and each party’s seats over the entire country.</p>
<p>If there’s a cutoff (such as the 5% rule we have in Iceland), you eliminate any parties that fall under the 5% threshold from the following process (but they keep any CS they’ve already been allocated).</p>
<p>Now, we’ve got some total number of AS, which are to be allocated, but we are not just allocating them to the parties, we’re allocating them to parties <em>in constituencies</em>. So the task is to figure out which party deserves the next seat <em>and</em> where it is most logical for them to get a seat, given the votes they got in each constituency.</p>
<p>What this effectively comes down to is, once all the CS have been allocated, you figure out which parties are most deserving of seats nationally, and give them extras, ideally in the constituencies where they got the most votes. But the devil is in the details here.</p>
<p>It has been proven that there is one and only one mathematically sound way to calculate the results, as shown in <a href="https://hal.science/hal-00686748/document" target="_blank" rel="noopener noreffer">this paper by Balinski &amp; DeMange</a>, which describes a bilinear optimization exercise aiming at minimizing entropy. Weirdly, as far as I know, only the canton of Geneva uses this correct method.</p>
<p>By “correct” here, we mean that the result is Exact, Relevant, Uniform, Monotonic and Homogenous, as per these axioms:</p>
<p><img src="https://smarimccarthy.is/img/axioms.png" data-src="/img/axioms.png" data-srcset="/img/axioms.png, /img/axioms.png 1.5x, /img/axioms.png 2x" data-sizes="auto" alt="/img/axioms.png" title="Axioms" srcset="https://smarimccarthy.is/img/axioms.png, https://smarimccarthy.is/img/axioms.png 1.5x, https://smarimccarthy.is/img/axioms.png 2x"></p>
<p>Each of these comes with a lengthy description – I’ll just refer to the paper, but basically, you don’t really want to fail any of these tests.</p>
<p>So it’s a bit weird that everywhere other than Geneva that uses this kind of method, including Iceland, uses an approximation method that is provably incorrect, but in different ways. <a href="https://github.com/smari/voting/blob/master/backend/methods/icelandic_law.py" target="_blank" rel="noopener noreffer">I wrote some code for the Icelandic law method</a>, but more about that below.</p>
<p>Specifically, the Icelandic system fails the monotonicity criteria, frequently resulting in one party losing a seat because it got more votes.</p>
<p>In 2013, Framsóknarflokkurinn got one extra seat that should have gone to Vinstri-Græn; in 2016 the Sjálfstæðisflokkur got an extra seat that should have gone to Vinstri-Græn; in 2017 Framsóknarflokkurinn got one that should have gone to Samfylkingin, and in 2021 Framsóknarflokkurinn got one that should have gone to Sjálfstæðisflokkurinn.</p>
<p>(The fact that Framsóknarflokkurinn benefits from this error more often than other parties is kind of random, or it’s a quirk of where their votes come from, but either way it’s a fun irony that the decision to use this approximation method was decided on around the kitchen table of the chairman of Framsóknarflokkurinn back in the day, during a meeting of the then coalition leaders.)</p>
<p>The main reason it’s being calculated incorrectly in Iceland is that lawyers and politicians don’t generally understand (bi)linear optimization. So rather than explain a fairly simple math operation in law, they explain this really convoluted drawn out, inaccurate method instead, because it’s less mathy. Oh well.</p>
<h2 id="some-thoughts">Some thoughts</h2>
<p>There are a lot of ways to skin a cat. This is not really a bad system; one might even argue that it’s quite good. The biggest drawback is that it might seem unfair to require the average voter to understand entropy minimization in order to have a clear sense of how elections work. It’s really not <em>super</em> complex math, but it does take a moment to wrap one’s head around it, and the tradition of rather terse notation doesn’t help at all.</p>
<p>Within this system, the biggest improvement I could suggest is increasing the number of AS quite significantly in the Icelandic system. Making it be something closer to 50% of the total seats would go a long way towards reducing vote inequality (votes in NW are worth almost 2x what they’re worth in SW) and reducing “provincialism” in politics. I would say move it to 100% AS, which has almost the same effect as just having one constituency (except you’re still having party lists in each constituency, which causes some geographic distribution to exist), but there is value in <em>some</em> provincialism and recognition of different needs and issues in different geographies. It’s just a bit too much at the moment.</p>
<p>If one were to change the system, I might suggest that the same Michel Balinski wrote a fantastic book along with Rida Laraki some years ago, <a href="https://mitpress.mit.edu/9780262545716/majority-judgment/" target="_blank" rel="noopener noreffer">Majority Judgement</a>, which outlines and proves a set of methods that are simultaneously really easy to understand, very easy to implement, and most notably they avoid the classic <a href="https://en.wikipedia.org/wiki/Arrow%27s_impossibility_theorem" target="_blank" rel="noopener noreffer">Arrow Impossibility Theorem</a> by virtue of not being prioritization systems, but rather scoring systems – where each voter essentially gives each party a grade, or “judgement” in the parlance of the book.</p>
<h2 id="some-code">Some code</h2>
<p>Many years ago, <a href="https://thorkellhelgason.is/" target="_blank" rel="noopener noreffer">Þorkell Helgason</a> contacted me about helping him build a simulator for this category of voting systems, in order to better be able to compare different systems and see how they behave under combinations of settings and weird vote outcomes. After a few remote sessions, Þorkell came and visited me in Sarajevo, where I lived at the time, and over the course of a weekend we banged out the first version, supporting the Icelandic system and the optimal entropy method described in the Balinski paper.</p>
<p>Since then we and various others have added a whole host of features and methods to the software. I haven’t much touched it myself for quite a while, but it’s interesting to take a fresh look in light of the upcoming elections.</p>
<p><a href="https://github.com/smari/voting" target="_blank" rel="noopener noreffer">Github link to the voting simulator</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Web Is Broken – Botnet Part 2 (369 pts)]]></title>
            <link>https://jan.wildeboer.net/2025/04/Web-is-Broken-Botnet-Part-2/</link>
            <guid>43738603</guid>
            <pubDate>Sat, 19 Apr 2025 18:59:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jan.wildeboer.net/2025/04/Web-is-Broken-Botnet-Part-2/">https://jan.wildeboer.net/2025/04/Web-is-Broken-Botnet-Part-2/</a>, See on <a href="https://news.ycombinator.com/item?id=43738603">Hacker News</a></p>
<div id="readability-page-1" class="page"><section itemprop="text">
        <p><strong>I guess you have all heard about the growing problem of AI companies trying to aggressively collect whatever data they can get their hands on to train their models. This has caused an explosive surge in web crawlers relentlessly hitting servers big and small. But who runs these crawlers? Turns out — it could be you!</strong></p>

<table>
  <tbody>
    <tr>
      <td>1.</td>
      <td><a href="https://jan.wildeboer.net/2025/02/Blocking-Stealthy-Botnets/">Those stealthy botnets</a> - How I found out about a not so new class of botnets</td>
    </tr>
    <tr>
      <td>2.</td>
      <td><a href="https://jan.wildeboer.net/2025/04/Web-is-Broken-Botnet-Part-2/">The Web is Broken</a> - <strong>Certain companies recruit app developers to create botnets by injecting “network sharing” SDKs into their apps. These botnets then use the network bandwidth of unsuspecting users of said apps to crawl the web, brute-force mail servers and other nasty things.</strong></td>
    </tr>
  </tbody>
</table>

<p>So there is a (IMHO) shady market out there that gives app developers on iOS, Android, MacOS and Windows money for including a library into their apps that sells users network bandwidth. Infatica<sup id="fnref:infaticasdk"><a href="#fn:infaticasdk" rel="footnote" role="doc-noteref">1</a></sup> is just one example, there are many more.</p>

<p>I am 99% sure that these companies cause what effectively are DDoS attacks by aggressive AI crawlers that many webmasters have to deal with since months. This business model should simply not exist. Apple, Microsoft and Google should act.</p>

<p><img src="https://jan.wildeboer.net/images/2025/04/botnet01.png" alt="Infatica explains their SDK" title="How does the monetization work?  We connect your users’ IP addresses to the Infatica peer-to-business network, which allows companies to access web data to build price aggregation platforms, perform search engine optimization, create brand protection and marketing strategies, conduct academic research, produce uptime and performance services, ensure corporate data protection, and more.">
<em>From the <a href="https://infatica.io/sdk-monetization/">Infatica SDK page</a>, explaining how app developers can make money by including the Infatica SDK</em></p>

<p>What these companies then sell to <em>their</em> customers is network access through the devices/PCs that have an app with this SDK installed<sup id="fnref:infaticasell"><a href="#fn:infaticasell" rel="footnote" role="doc-noteref">2</a></sup>. They are proud to tell you how you can funnel your (AI) web scraping etc through millions of rotating, residential and mobile IP addresses. Exactly the pattern we see hitting our servers.</p>

<p><img src="https://jan.wildeboer.net/images/2025/04/botnet02.png" alt="The offer to customers: residential IPs, Static IPs, mobile IPs etc." title="We’re offering a set of pricing plans with varying parameters including available traffic, IP address count, and other features – or you can use our flexible pricing option to fine-tune the parameters yourself.">
<em>Infatica claiming they have millions and millions of IP addresses to hand to you</em></p>

<p><img src="https://jan.wildeboer.net/images/2025/04/botnet03.png" alt="Infatica promising millions of IP addresses " title="Global Portfolio of Residential IPs Residential IP addresses make web scraping and similar activities much easier: buy proxy IPs from residential zones, your connection is safer and more anonymous.   United States 226090 IPs Russia 792251 IPs Ukraine 367600 IPs Germany 116173 IPs India 274277 IPs Poland 305109 IPs China 670301 IPs Turkey 374577 IPs Brazil 1123823 IPs Indonesia 367978 IPs Vietnam 579580 IPs Saudi Arabia 64697 IPs">
<em>What I would call “infected users” are called “residential IPs” in this specific market</em></p>

<h2 id="there-are-many">There are many</h2>

<p>Now, again, this company is just one of many selling similar services. And they all promise that they carefully check what commands their customers send to the (IMHO) infected apps on your phone and PC. Yeah, I am sure they “do no evil”. And when they do, they can claim it’s not their problem because they are merely the proxy. Again, IMHO, a shady business model.</p>

<p>Trend Micro did some research on these companies back in 2023 and it confirms my suspicions. And I guess with AI scraping this kind of business is booming.</p>

<blockquote>
  <p>„There are malicious actors who repacked freeware and shareware written by other people to conduct drive-by downloads of the Infatica peer-to-business (P2B) service“ <sup id="fnref:trendmicro"><a href="#fn:trendmicro" rel="footnote" role="doc-noteref">3</a></sup></p>
</blockquote>

<p><img src="https://jan.wildeboer.net/images/2025/04/botnet04.png" alt="Trend Micro's finding on the real use of these offerings" title="During our one month of observation, we have seen the following suspicious or malicious behaviors that Infatica proxy customers are doing via the service:  Bruteforcing of Simperium, a cross-site data synchronization service Bruteforcing of Bitwarden Scraping of house prices Scraping of Lazada and Walmart prices Creating accounts on Live.com, Instagram, and Mail.RU"></p>

<p>But IMHO (In My Humble Opinion) this also explains the explosion of bot traffic that really cripples a lot of smaller services (like my forgejo instance, that I had to make non-public).</p>

<p>So if you as an app developer include such a 3rd party SDK in your app to make some money — you are part of the problem and I think you should be held responsible for delivering malware to your users, making them botnet members.</p>

<p>Unfortunately it is next to impossible for normal users to detect the inclusion of such shady SDKs and the network traffic they cause. Not even mentioning how hard this is for admins of (small) web servers.</p>

<p>I already blogged about this <a href="https://jan.wildeboer.net/2025/02/Blocking-Stealthy-Botnets/">back in February 2025</a> but I think it is better to put what I have learned since then in this new post. I guess it won’t be my last on this topic.</p>

<h2 id="see-for-yourself">See for yourself!</h2>

<p>If you want to feel really dirty, go to <a href="https://proxyway.com/reviews?e-filter-da2a7bc-reviews_categories=proxy-providers">https://proxyway.com/reviews?e-filter-da2a7bc-reviews_categories=proxy-providers</a> for a collection of reviews on these services. It’s a huge market and I am 100% convinced that “AI” web scraping is currently the biggest “growth” driver for these companies.</p>

<p>And when I see that quite some of them rely on injecting SDKs into 3rd party apps to “extend” their “Reach” and fill their pools of “residential proxies”, I would call out these companies for distributing malware and creating botnets. But that’s just my personal opinion. I am sure they are all legit.</p>

<p><img src="https://jan.wildeboer.net/images/2025/04/botnet05.png" alt="Reviews of proxy providers" title="Page 1 of 3 of review of &quot;residential proxy providers&quot;, listing smart proxy, Oxylabs, brightdata, netnut, soax, webshare, Nimble, Infatica, Evomi, Massive, Proxyseller, Ayobyte">
<em>Page 1 of 3 with reviews of “residential proxy” providers</em></p>

<h2 id="my-conclusion">My conclusion</h2>

<p>I am now of the opinion that <em>every</em> form of web-scraping should be considered abusive behaviour and web servers should block all of them. If you think your web-scraping is acceptable behaviour, you can thank these shady companies and the “AI” hype for moving you to the bad corner.</p>

<p>Thank you for your time and interest! I hope it helps you understand why web crawlers have become a real problem and how this is more and more an attack on the foundation of the Web as it was intended to be. This “residential proxy” business is just one part of this. And we webmasters/admins can only try to block. It is getting more and more difficult to keep up with these waves. Thanks “AI”, I guess?</p>



</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Raspberry Pi Lidar Scanner (531 pts)]]></title>
            <link>https://github.com/PiLiDAR/PiLiDAR</link>
            <guid>43738561</guid>
            <pubDate>Sat, 19 Apr 2025 18:53:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/PiLiDAR/PiLiDAR">https://github.com/PiLiDAR/PiLiDAR</a>, See on <a href="https://news.ycombinator.com/item?id=43738561">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">PiLiDAR - DIY 360° 3D Panorama Scanner</h2><a id="user-content-pilidar---diy-360-3d-panorama-scanner" aria-label="Permalink: PiLiDAR - DIY 360° 3D Panorama Scanner" href="#pilidar---diy-360-3d-panorama-scanner"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><em>WORK IN PROGRESS</em></h2><a id="user-content-work-in-progress" aria-label="Permalink: WORK IN PROGRESS" href="#work-in-progress"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Core Features:</h2><a id="user-content-core-features" aria-label="Permalink: Core Features:" href="#core-features"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>LiDAR</strong>: custom serial driver for LDRobot <strong>LD06</strong>, <strong>LD19</strong> or <strong>STL27L</strong></p>
<ul dir="auto">
<li>CRC package integrity check</li>
<li><a href="https://github.com/Pioreactor/rpi_hardware_pwm">Hardware PWM</a> calibrated using curve fitting</li>
<li>2D live visualization and export (numpy or CSV)</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Panorama</strong>: 6K 360° spherical map</p>
<ul dir="auto">
<li>stitched from fisheye photos using <a href="https://hugin.sourceforge.io/" rel="nofollow"><strong>Hugin</strong> Panorama photo stitcher</a></li>
<li>constant camera exposure by reading EXIF data of automatic</li>
<li>constant white balance by iterative optimization of color gains</li>
</ul>
</li>
<li>
<p dir="auto"><strong>3D Scene</strong>: assembly of 3D scenes from 2D planes based on angle and offsets</p>
<ul dir="auto">
<li>sampling <strong>vertex colors from panorama</strong></li>
<li>Open3D visualization and export (PCD, PLY or <a href="https://github.com/davidcaron/pye57">e57</a>)</li>
<li>aligning multiple scenes using <strong>global registration</strong> and <strong>ICP fine-tuning</strong></li>
<li><strong>Poisson Surface Meshing</strong> (very slow on Pi4, recommended to run on PC)</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">preliminary results</h2><a id="user-content-preliminary-results" aria-label="Permalink: preliminary results" href="#preliminary-results"></a></p>
<p dir="auto">single scans, no registration, no post processing.<br>
klick the images to open the pointclouds in Sketchfab.</p>
<p dir="auto"><a href="https://sketchfab.com/models/7997b63a3cb747f99b8f161862318bec/embed?autostart=1&amp;ui_animations=0&amp;ui_stop=0&amp;ui_inspector=0&amp;ui_watermark_link=0&amp;ui_watermark=0&amp;ui_ar=0&amp;ui_help=0&amp;ui_settings=0&amp;ui_vr=0&amp;ui_fullscreen=0&amp;ui_annotations=0" rel="nofollow"><img src="https://github.com/PiLiDAR/PiLiDAR/raw/main/images/exterior.jpeg" alt="Exterior"></a></p>
<p dir="auto"><em>Exterior Scan (colormapped Intensity)</em></p>
<p dir="auto"><a href="https://sketchfab.com/models/0311c098c57b458abe3a5d3dda9fe92b/embed?autospin=0&amp;autostart=1&amp;ui_animations=0&amp;ui_inspector=0&amp;ui_watermark_link=0&amp;ui_watermark=0&amp;ui_ar=0&amp;ui_help=0&amp;ui_settings=0&amp;ui_vr=0&amp;ui_fullscreen=0&amp;ui_annotations=0" rel="nofollow"><img src="https://github.com/PiLiDAR/PiLiDAR/raw/main/images/interior.jpeg" alt="Interior"></a></p>
<p dir="auto"><em>Interior Scan (Vertex Colors)</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hardware Specs:</h2><a id="user-content-hardware-specs" aria-label="Permalink: Hardware Specs:" href="#hardware-specs"></a></p>
<ul dir="auto">
<li>
<p dir="auto">LDRobot LD06, LD19 or STL27L LiDAR</p>
</li>
<li>
<p dir="auto">Raspberry Pi HQ Camera with ArduCam M12 Lens <a href="https://www.arducam.com/doc/Arducam_M12_Lens_Kit_for_Pi_HQ_Camera.pdf" rel="nofollow">(M25156H18, p.7)</a></p>
</li>
<li>
<p dir="auto">Raspberry Pi 4</p>
</li>
<li>
<p dir="auto">NEMA17 42-23 stepper with A4988 driver</p>
</li>
<li>
<p dir="auto">Power Supply:</p>
<ul dir="auto">
<li>v1: 2x <em>18650</em> Batteries (7.2V) with step-down converter</li>
<li>v2: 10.000 mAh USB Powerbank with step-up converter</li>
</ul>
</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/PiLiDAR/PiLiDAR/blob/main/images/pilidar_covershot.jpg"><img src="https://github.com/PiLiDAR/PiLiDAR/raw/main/images/pilidar_covershot.jpg" alt="PiLiDAR v1"></a>
<em>Rev. 1 using 2x 18650 Batteries and Buck Converter</em></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/PiLiDAR/PiLiDAR/blob/main/images/pilidar_covershot_v2.jpg"><img src="https://github.com/PiLiDAR/PiLiDAR/raw/main/images/pilidar_covershot_v2.jpg" alt="PiLiDAR v2"></a>
<em>Rev. 2 using 10.000 mAh Powerbank and Boost Converter</em></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">stepper driver, motor and gearbox</h3><a id="user-content-stepper-driver-motor-and-gearbox" aria-label="Permalink: stepper driver, motor and gearbox" href="#stepper-driver-motor-and-gearbox"></a></p>
<ul dir="auto">
<li>A4988 bipolar stepper driver (<a href="https://www.youtube.com/watch?v=PMS5jY7RTjo" rel="nofollow">tutorial</a>)</li>
<li>NEMA17 42x42x23 bipolar stepper (<a href="https://www.omc-stepperonline.com/e-series-nema-17-bipolar-1-8deg-17ncm-24-07oz-in-1a-42x42x23mm-4-wires-17he08-1004s" rel="nofollow">17HE08-1004S</a>, 17 Ncm torque)</li>
<li>3D-printed planetary reduction gearbox (see <a href="#fdm--3d-printing">FDM / 3D printing</a>)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">LDRobot LiDAR Specs</h3><a id="user-content-ldrobot-lidar-specs" aria-label="Permalink: LDRobot LiDAR Specs" href="#ldrobot-lidar-specs"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/PiLiDAR/PiLiDAR/blob/main/images/lidar_comparison.jpg"><img src="https://github.com/PiLiDAR/PiLiDAR/raw/main/images/lidar_comparison.jpg" alt="LD06 vs. STL27L"></a>
<em>angular resolution of LD06 (left) vs. STL27L (right)</em></p>
<p dir="auto">LD06:</p>
<ul dir="auto">
<li>sampling frequency: 4500 Hz</li>
<li>baudrate 230400</li>
<li><a href="https://www.inno-maker.com/product/lidar-ld06/" rel="nofollow">Sales page</a></li>
<li><a href="https://www.inno-maker.com/wp-content/uploads/2020/11/LDROBOT_LD06_Datasheet.pdf" rel="nofollow">mechanical Datasheet</a></li>
<li><a href="https://storage.googleapis.com/mauser-public-images/prod_description_document/2021/315/8fcea7f5d479f4f4b71316d80b77ff45_096-6212_a.pdf" rel="nofollow">Protocol Description</a></li>
</ul>
<p dir="auto">STL27L:</p>
<ul dir="auto">
<li>sampling frequency: 21600 Hz</li>
<li>baudrate 921600</li>
<li><a href="https://github.com/May-DFRobot/DFRobot/blob/master/SEN0589_Datasheet.pdf">datasheet</a></li>
<li><a href="https://www.waveshare.com/wiki/DTOF_LIDAR_STL27L" rel="nofollow">wiki</a></li>
<li>ROS2 driver <a href="https://github.com/ldrobotSensorTeam/ldlidar_stl_ros2?tab=readme-ov-file#Instructions">git</a></li>
</ul>
<p dir="auto">Scan duration:
12s initialisation
17s shooting 4x photos
1:24m scanning 0.167° x 0.18°
37s stitching, cleanup</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">wiring</h2><a id="user-content-wiring" aria-label="Permalink: wiring" href="#wiring"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/PiLiDAR/PiLiDAR/blob/main/images/pilidar_breadboard.jpg"><img src="https://github.com/PiLiDAR/PiLiDAR/raw/main/images/pilidar_breadboard.jpg" alt="breadboard version 2"></a>
<em>Breadboard Rev. 2</em></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">LD06 / STL27L:</h3><a id="user-content-ld06--stl27l" aria-label="Permalink: LD06 / STL27L:" href="#ld06--stl27l"></a></p>
<ul dir="auto">
<li>UART Tx (yellow)</li>
<li>PWM (white)</li>
<li>GND (black)</li>
<li>VCC 5V (red)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Raspberry Pi:</h3><a id="user-content-raspberry-pi" aria-label="Permalink: Raspberry Pi:" href="#raspberry-pi"></a></p>
<ul dir="auto">
<li>LD06 UART0 Rx: GP15</li>
<li>LD06 PWM0: GP18</li>
<li>Power Button: GP03</li>
<li>Scan Button: GP17</li>
<li>A4988 direction: GP26, step: GP19</li>
<li>A4988 microstepping mode: GP5, GP6, GP13</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Power Button (Wakeup &amp; Shutdown)</h3><a id="user-content-power-button-wakeup--shutdown" aria-label="Permalink: Power Button (Wakeup &amp; Shutdown)" href="#power-button-wakeup--shutdown"></a></p>
<ul dir="auto">
<li>
<p dir="auto">Wakeup is hardwired to Pin 3</p>
</li>
<li>
<p dir="auto">enable gpio-shutdown</p>
<div data-snippet-clipboard-copy-content="  echo &quot;dtoverlay=gpio-shutdown&quot; >> /boot/firmware/config.txt "><pre><code>  echo "dtoverlay=gpio-shutdown" &gt;&gt; /boot/firmware/config.txt 
</code></pre></div>
</li>
<li>
<p dir="auto">if necesessary:</p>
<div data-snippet-clipboard-copy-content="  sudo nano /etc/systemd/logind.conf
  HandlePowerKey=poweroff"><pre><code>  sudo nano /etc/systemd/logind.conf
  HandlePowerKey=poweroff
</code></pre></div>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">enable i2c-GPIO for GY-521 Accelerometer</h3><a id="user-content-enable-i2c-gpio-for-gy-521-accelerometer" aria-label="Permalink: enable i2c-GPIO for GY-521 Accelerometer" href="#enable-i2c-gpio-for-gy-521-accelerometer"></a></p>
<p dir="auto">GY-521 (MPU 6060): Accelerometer, Gyroscope and thermometer<br>
i2c adress: 0x68<br>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/cf7b26631a1deb5b33cb19cdac3382494d9599a97105f380cc05b0846986a4c2/68747470733a2f2f7777772e6d616b657273686f702e64652f646f776e6c6f61642f4d5055363035302d50696e6f75742e706e67"><img src="https://camo.githubusercontent.com/cf7b26631a1deb5b33cb19cdac3382494d9599a97105f380cc05b0846986a4c2/68747470733a2f2f7777772e6d616b657273686f702e64652f646f776e6c6f61642f4d5055363035302d50696e6f75742e706e67" alt="GY-521" data-canonical-src="https://www.makershop.de/download/MPU6050-Pinout.png"></a></p>
<p dir="auto">Since GPIO3 is hardwired to the Power Button, we need to use i2c-GPIO to map custom i2c pins (<a href="https://www.instructables.com/Raspberry-PI-Multiple-I2c-Devices/" rel="nofollow">tutorial</a>). Unlike serial is not getting crossed, so we connect SDA-SDA and SCL-SCL.<br>
SDA: GPIO22<br>
SCL: GPIO27</p>
<p dir="auto">disable ic2_arm and enable i2c-gpio in /boot/firmware/config.txt</p>
<div data-snippet-clipboard-copy-content="dtparam=i2c_arm=off
dtoverlay=i2c-gpio,bus=3,i2c_gpio_delay_us=1,i2c_gpio_sda=22,i2c_gpio_scl=27"><pre><code>dtparam=i2c_arm=off
dtoverlay=i2c-gpio,bus=3,i2c_gpio_delay_us=1,i2c_gpio_sda=22,i2c_gpio_scl=27
</code></pre></div>
<p dir="auto">search for devices on i2c bus 3:</p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Power LED and CPU fan</h3><a id="user-content-power-led-and-cpu-fan" aria-label="Permalink: Power LED and CPU fan" href="#power-led-and-cpu-fan"></a></p>
<div data-snippet-clipboard-copy-content="# CPU fan at lower temp
echo &quot;dtoverlay=gpio-fan,gpiopin=4,temp=45000&quot; >> /boot/firmware/config.txt


# Power LED Heartbeat:
echo &quot;dtparam=pwr_led_trigger=timer&quot; >> /boot/firmware/config.txt"><pre><code># CPU fan at lower temp
echo "dtoverlay=gpio-fan,gpiopin=4,temp=45000" &gt;&gt; /boot/firmware/config.txt


# Power LED Heartbeat:
echo "dtparam=pwr_led_trigger=timer" &gt;&gt; /boot/firmware/config.txt
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Scan Button: register GPIO interrupt</h3><a id="user-content-scan-button-register-gpio-interrupt" aria-label="Permalink: Scan Button: register GPIO interrupt" href="#scan-button-register-gpio-interrupt"></a></p>
<p dir="auto">make script executable:</p>
<div data-snippet-clipboard-copy-content="chmod +x gpio_interrupt.py"><pre><code>chmod +x gpio_interrupt.py
</code></pre></div>
<p dir="auto">create new service for autostart</p>
<div data-snippet-clipboard-copy-content="sudo nano /etc/systemd/system/pilidar.service"><pre><code>sudo nano /etc/systemd/system/pilidar.service
</code></pre></div>
<p dir="auto">content:</p>
<div data-snippet-clipboard-copy-content="[Unit]
Description=PiLiDAR-Button
After=network.target

[Service]
Type=simple
User=pi
Environment=LG_WD=/tmp
ExecStart=/usr/bin/python3 /home/pi/PiLiDAR/gpio_interrupt.py
Restart=no

[Install]
WantedBy=multi-user.target"><pre><code>[Unit]
Description=PiLiDAR-Button
After=network.target

[Service]
Type=simple
User=pi
Environment=LG_WD=/tmp
ExecStart=/usr/bin/python3 /home/pi/PiLiDAR/gpio_interrupt.py
Restart=no

[Install]
WantedBy=multi-user.target
</code></pre></div>
<p dir="auto">reload daemon, enable and start service:</p>
<div data-snippet-clipboard-copy-content="sudo systemctl daemon-reload
sudo systemctl enable pilidar.service
sudo systemctl start pilidar.service"><pre><code>sudo systemctl daemon-reload
sudo systemctl enable pilidar.service
sudo systemctl start pilidar.service
</code></pre></div>
<p dir="auto">check service if necessary:</p>
<div data-snippet-clipboard-copy-content="sudo systemctl status pilidar.service"><pre><code>sudo systemctl status pilidar.service
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">set Permission for UART on Raspberry Pi</h3><a id="user-content-set-permission-for-uart-on-raspberry-pi" aria-label="Permalink: set Permission for UART on Raspberry Pi" href="#set-permission-for-uart-on-raspberry-pi"></a></p>
<p dir="auto">temporary solution:</p>
<div data-snippet-clipboard-copy-content="sudo chmod a+rw /dev/ttyS0"><pre><code>sudo chmod a+rw /dev/ttyS0
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">old solution: make it permanent by disabling password for chmod:</h4><a id="user-content-old-solution-make-it-permanent-by-disabling-password-for-chmod" aria-label="Permalink: old solution: make it permanent by disabling password for chmod:" href="#old-solution-make-it-permanent-by-disabling-password-for-chmod"></a></p>
<div data-snippet-clipboard-copy-content="sudo visudo
pi ALL=(ALL:ALL) NOPASSWD: /usr/bin/chmod a+rw /dev/ttyS0"><pre><code>sudo visudo
pi ALL=(ALL:ALL) NOPASSWD: /usr/bin/chmod a+rw /dev/ttyS0
</code></pre></div>
<p dir="auto">then execute the <em>temporary</em> solution from python:</p>
<div data-snippet-clipboard-copy-content="import subprocess
command = &quot;sudo chmod a+rw /dev/ttyS0&quot;
process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)
output, error = process.communicate()"><pre><code>import subprocess
command = "sudo chmod a+rw /dev/ttyS0"
process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)
output, error = process.communicate()
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">new solution: grant permissions to the serial port using udev rules</h4><a id="user-content-new-solution-grant-permissions-to-the-serial-port-using-udev-rules" aria-label="Permalink: new solution: grant permissions to the serial port using udev rules" href="#new-solution-grant-permissions-to-the-serial-port-using-udev-rules"></a></p>
<p dir="auto">(TODO: check and remove old!)</p>
<ul dir="auto">
<li>forget about <code>visudo</code> and the subprocess call above.</li>
<li>Open a terminal and run the following command: <code>sudo nano /etc/udev/rules.d/50-ttyS0.rules</code></li>
<li>Write the following line in the file and save it: <code>KERNEL=="ttyS0",GROUP="dialout",MODE="0660"</code></li>
<li>Run the following command to check if your user is a member of the dialout group: <code>groups</code></li>
<li>If you see <code>dialout</code> in the output, you are already a member of the group. If not, run the following command to add your user to the group: <code>sudo usermod -a -G dialout pi</code></li>
<li>Run the following command to reload the udev rules: <code>sudo udevadm control --reload-rules</code></li>
<li>Unplug and replug the serial device, or reboot the system, to apply the changes.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hardware PWM on Raspberry Pi</h3><a id="user-content-hardware-pwm-on-raspberry-pi" aria-label="Permalink: Hardware PWM on Raspberry Pi" href="#hardware-pwm-on-raspberry-pi"></a></p>
<p dir="auto">enable GPIO_18 (PWM0) and GPIO_19 (PWM1)</p>
<div data-snippet-clipboard-copy-content="echo &quot;dtoverlay=pwm-2chan&quot; >> /boot/firmware/config.txt "><pre><code>echo "dtoverlay=pwm-2chan" &gt;&gt; /boot/firmware/config.txt 
</code></pre></div>
<p dir="auto">check if "pwm_bcm2835" now exists:</p>

<p dir="auto">Install <a href="https://github.com/Pioreactor/rpi_hardware_pwm">RPi Hardware PWM library</a>:</p>
<div data-snippet-clipboard-copy-content="pip install rpi-hardware-pwm"><pre><code>pip install rpi-hardware-pwm
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Panorama Stitching</h3><a id="user-content-panorama-stitching" aria-label="Permalink: Panorama Stitching" href="#panorama-stitching"></a></p>
<p dir="auto">install Hugin with enblend plugin</p>
<div data-snippet-clipboard-copy-content="sudo apt-get install hugin-tools enblend"><pre><code>sudo apt-get install hugin-tools enblend
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">power switching the USB port</h3><a id="user-content-power-switching-the-usb-port" aria-label="Permalink: power switching the USB port" href="#power-switching-the-usb-port"></a></p>
<p dir="auto">using <a href="https://www.baeldung.com/linux/control-usb-power-supply" rel="nofollow">uhubctl</a> cli tool. install:</p>
<div data-snippet-clipboard-copy-content="sudo apt-get install uhubctl"><pre><code>sudo apt-get install uhubctl
</code></pre></div>
<p dir="auto">list all available hubs and devices</p>

<p dir="auto">powering Raspberry Pi's USB-3-Ports (Hub 2) off / on</p>
<div data-snippet-clipboard-copy-content="sudo uhubctl -l 2 -a off
sudo uhubctl -l 2 -a on"><pre><code>sudo uhubctl -l 2 -a off
sudo uhubctl -l 2 -a on
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">jupyter over remote-ssh</h3><a id="user-content-jupyter-over-remote-ssh" aria-label="Permalink: jupyter over remote-ssh" href="#jupyter-over-remote-ssh"></a></p>
<p dir="auto">start jupyter for network access:</p>
<div data-snippet-clipboard-copy-content="jupyter notebook --ip 192.168.1.16 --no-browser PiLiDAR.ipynb"><pre><code>jupyter notebook --ip 192.168.1.16 --no-browser PiLiDAR.ipynb
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">FDM / 3D printing</h2><a id="user-content-fdm--3d-printing" aria-label="Permalink: FDM / 3D printing" href="#fdm--3d-printing"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">3D model files:</h3><a id="user-content-3d-model-files" aria-label="Permalink: 3D model files:" href="#3d-model-files"></a></p>
<ul dir="auto">
<li>
<p dir="auto">Housing and additional parts (obj and 3mf)  in <a href="https://github.com/PiLiDAR/PiLiDAR/blob/main/mechanical_design/v2/export">mechanical_design/v2/export</a>.</p>
</li>
<li>
<p dir="auto">M12 to C-Mount lens adapter (<a href="https://www.thingiverse.com/thing:4444398" rel="nofollow">thingiverse.com</a>)</p>
</li>
<li>
<p dir="auto">NEMA17 planetary reduction gearbox (<a href="https://www.printables.com/de/model/782336-nema17-planetary-gearbox-fixed" rel="nofollow">printables.com</a>)</p>
</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/PiLiDAR/PiLiDAR/blob/main/images/CAD_v2.jpg"><img src="https://github.com/PiLiDAR/PiLiDAR/raw/main/images/CAD_v2.jpg" alt="CAD model"></a>
<em>Housing CAD model Rev. 2</em></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/PiLiDAR/PiLiDAR/blob/main/images/FDM.jpg"><img src="https://github.com/PiLiDAR/PiLiDAR/raw/main/images/FDM.jpg" alt="3D printing"></a>
<em>FDM printing the old front panel (Rev. 1) in PETG</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Serial Protocol</h2><a id="user-content-serial-protocol" aria-label="Permalink: Serial Protocol" href="#serial-protocol"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">LD06</h3><a id="user-content-ld06" aria-label="Permalink: LD06" href="#ld06"></a></p>
<p dir="auto">baudrate 230400, data bits 8, no parity, 1 stopbit<br>
sampling frequency 4500 Hz, scan frequency 5-13 Hz, distance 2cm - 12 meter, ambient light 30 kLux</p>
<p dir="auto">total package size: 48 Byte, big endian.</p>
<ul dir="auto">
<li>starting character：Length 1 Byte, fixed value 0x54, means the beginning of data packet;</li>
<li>Data Length: Length 1 Byte, the first three digits reserved, the last five digits represent the number of measured points in a packet, currently fixed value 12;</li>
<li>speed：Length 2 Byte, in degrees per second;</li>
<li>Start angle: Length: 2 Byte; unit: 0.01 degree;</li>
<li>Data: Length 36 Byte; containing 12 data points with 3 Byte each: 2 Byte distance (unit: 1 mm), 1 Byte luminance. For white objects within 6m, the typical luminance is around 200.</li>
<li>End Angle: Length: 2 Byte; unit: 0.01 degree；</li>
<li>Timestamp: Length 2 Bytes in ms, recount if reaching to MAX 30000；</li>
<li>CRC check: Length 1 Byte</li>
</ul>
<p dir="auto">The Angle value of each data point is obtained by linear interpolation of the starting angle and the ending angle.<br>
The calculation method of the angle is as following:</p>
<div data-snippet-clipboard-copy-content="step = (end_angle – start_angle)/(len – 1)  
angle = start_angle + step*i  "><pre><code>step = (end_angle – start_angle)/(len – 1)  
angle = start_angle + step*i  
</code></pre></div>
<p dir="auto">len is the length of the packet, and the i value range is [0, len].</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">remote Open3D Visualization</h2><a id="user-content-remote-open3d-visualization" aria-label="Permalink: remote Open3D Visualization" href="#remote-open3d-visualization"></a></p>
<p dir="auto">using <del><a href="https://www.open3d.org/docs/release/tutorial/visualization/web_visualizer.html" rel="nofollow"><em>Web Visualizer</em></a></del> <a href="https://plotly.com/python/" rel="nofollow">Plotly</a> to display 3D pointclouds works great in Jupyter.</p>
<p dir="auto">Plotly seems to render client-sided, unlike Open3D Web Visualizer which renders host-sided and streams jpg sequences, which strains the Pi's both CPU and WIFI.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Dumping Scans to USB Storage</h2><a id="user-content-dumping-scans-to-usb-storage" aria-label="Permalink: Dumping Scans to USB Storage" href="#dumping-scans-to-usb-storage"></a></p>
<ol dir="auto">
<li>Clone the Repo and run the installer:
<div data-snippet-clipboard-copy-content="cd /home/pi/PiLiDAR
git clone https://github.com/LaserBorg/usb_dump --depth 1
cd usb_dump &amp;&amp; chmod +x install.sh &amp;&amp; ./install.sh &quot;$(pwd)&quot;"><pre><code>cd /home/pi/PiLiDAR
git clone https://github.com/LaserBorg/usb_dump --depth 1
cd usb_dump &amp;&amp; chmod +x install.sh &amp;&amp; ./install.sh "$(pwd)"
</code></pre></div>
</li>
<li>Create the config file:
<div data-snippet-clipboard-copy-content="echo '{&quot;source_directories&quot;: [&quot;/home/pi/PiLiDAR/scans&quot;], &quot;target_root_directory&quot;: null}' > usbdump.json"><pre><code>echo '{"source_directories": ["/home/pi/PiLiDAR/scans"], "target_root_directory": null}' &gt; usbdump.json
</code></pre></div>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Troubleshoot USB_dump:</h3><a id="user-content-troubleshoot-usb_dump" aria-label="Permalink: Troubleshoot USB_dump:" href="#troubleshoot-usb_dump"></a></p>
<ul dir="auto">
<li>
<p dir="auto">Check the log file:</p>

</li>
<li>
<p dir="auto">to uninstall the service, run</p>
<div data-snippet-clipboard-copy-content="chmod +x uninstall.sh &amp;&amp; ./uninstall.sh"><pre><code>chmod +x uninstall.sh &amp;&amp; ./uninstall.sh
</code></pre></div>
</li>
<li>
<p dir="auto">if the mount point is still persistend after being removed, just delete them.</p>
<div data-snippet-clipboard-copy-content="sudo rm -rf /media/pi/<your device name>"><pre><code>sudo rm -rf /media/pi/&lt;your device name&gt;
</code></pre></div>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Windows Serial Driver</h3><a id="user-content-windows-serial-driver" aria-label="Permalink: Windows Serial Driver" href="#windows-serial-driver"></a></p>
<p dir="auto">get <a href="https://files.waveshare.com/upload/6/63/CP210x_Universal_Windows_Driver.zip" rel="nofollow">CP210x_Universal_Windows_Driver.zip</a> here:<br>
<a href="https://www.waveshare.com/wiki/DTOF_LIDAR_STL27L#Software_Download" rel="nofollow">https://www.waveshare.com/wiki/DTOF_LIDAR_STL27L#Software_Download</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">RPi.GPIO RuntimeError: Failed to add edge detection</h3><a id="user-content-rpigpio-runtimeerror-failed-to-add-edge-detection" aria-label="Permalink: RPi.GPIO RuntimeError: Failed to add edge detection" href="#rpigpio-runtimeerror-failed-to-add-edge-detection"></a></p>
<p dir="auto">current bookworm version has deprecated sysfs GPIO interface removed.<br>
use <a href="https://pypi.org/project/rpi-lgpio/" rel="nofollow">LGPIO</a> as described <a href="https://raspberrypi.stackexchange.com/questions/147332/rpi-gpio-runtimeerror-failed-to-add-edge-detection" rel="nofollow">here</a>:</p>
<div data-snippet-clipboard-copy-content="sudo apt remove python3-rpi.gpio
sudo apt update

sudo apt install python3-rpi-lgpio

# or in an env without system packages:
pip3 install rpi-lgpio"><pre><code>sudo apt remove python3-rpi.gpio
sudo apt update

sudo apt install python3-rpi-lgpio

# or in an env without system packages:
pip3 install rpi-lgpio
</code></pre></div>
<p dir="auto">LGPIO creates temp-files (<a href="https://github.com/joan2937/lg/issues/12" data-hovercard-type="issue" data-hovercard-url="/joan2937/lg/issues/12/hovercard">issue</a>) like ".lgd-nfy0". gpio-interrupt.py executes 'export LG_WD=/tmp' to set it's CWD.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">poor performance of VS Code on Raspberry Pi</h3><a id="user-content-poor-performance-of-vs-code-on-raspberry-pi" aria-label="Permalink: poor performance of VS Code on Raspberry Pi" href="#poor-performance-of-vs-code-on-raspberry-pi"></a></p>
<p dir="auto">disable hardware acceleration for VS Code (<a href="https://code.visualstudio.com/docs/setup/raspberry-pi" rel="nofollow">source</a>)</p>
<div data-snippet-clipboard-copy-content="Preferences: Configure Runtime Arguments  
Set &quot;disable-hardware-acceleration&quot;: true"><pre><code>Preferences: Configure Runtime Arguments  
Set "disable-hardware-acceleration": true
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">pye57 on Raspberry Pi</h3><a id="user-content-pye57-on-raspberry-pi" aria-label="Permalink: pye57 on Raspberry Pi" href="#pye57-on-raspberry-pi"></a></p>
<p dir="auto">there is no wheel for arm64. build requires libxerces:</p>
<div data-snippet-clipboard-copy-content="sudo apt install libxerces-c-dev
pip install pye57"><pre><code>sudo apt install libxerces-c-dev
pip install pye57
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">add WIFI via SSH</h3><a id="user-content-add-wifi-via-ssh" aria-label="Permalink: add WIFI via SSH" href="#add-wifi-via-ssh"></a></p>
<p dir="auto"><a href="https://u-labs.de/portal/raspberry-pi-wlan-verbindung-nachtraeglich-einrichten-oder-aendern-so-geht-es-grafisch-konsole/" rel="nofollow">tutorial</a>:</p>
<div data-snippet-clipboard-copy-content="sudo nano /etc/wpa_supplicant/wpa_supplicant.conf

# make sure country code is set:
country=DE"><pre><code>sudo nano /etc/wpa_supplicant/wpa_supplicant.conf

# make sure country code is set:
country=DE
</code></pre></div>
<p dir="auto">add entry to wpa_supplicant.conf</p>
<div data-snippet-clipboard-copy-content="sudo wpa_passphrase &quot;YOUR_SSID&quot; &quot;YOUR_PASSWORD&quot; | sudo tee -a /etc/wpa_supplicant/wpa_supplicant.conf"><pre><code>sudo wpa_passphrase "YOUR_SSID" "YOUR_PASSWORD" | sudo tee -a /etc/wpa_supplicant/wpa_supplicant.conf
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">references:</h2><a id="user-content-references" aria-label="Permalink: references:" href="#references"></a></p>
<p dir="auto">inspirations</p>
<ul dir="auto">
<li><a href="https://github.com/henjin0/LIDAR_LD06_python_loder">LIDAR_LD06_python_loder</a> and <a href="https://github.com/henjin0/Lidar_LD06_for_Arduino">Lidar_LD06_for_Arduino</a> by Inoue Minoru ("<a href="https://github.com/henjin0">henjin0</a>")</li>
<li><a href="https://github.com/ShaunPrice/360-camera">ShaunPrice's</a> StereoPi-supporting fork of <a href="https://github.com/BrianBock/360-camera">BrianBock's</a> 360-camera script (Article on <a href="https://medium.com/stereopi/stitching-360-panorama-with-raspberry-pi-cm3-stereopi-and-two-fisheye-cameras-step-by-step-guide-aeca3ff35871" rel="nofollow">Medium</a>)</li>
</ul>
<p dir="auto">another Lidar implementation in Python</p>
<ul dir="auto">
<li><a href="https://github.com/Paradoxdruid/pyLIDAR">pyLIDAR</a></li>
</ul>
<p dir="auto">hardware PWM using <a href="https://gpiozero.readthedocs.io/en/stable/migrating_from_rpigpio.html#pwm-pulse-width-modulation" rel="nofollow">GPIOZero</a></p>
<p dir="auto">ICP implementations:</p>
<ul dir="auto">
<li>Aeva <a href="https://github.com/aevainc/Doppler-ICP/blob/main/README.md">Doppler-ICP</a></li>
<li>Photogrammetry &amp; Robotics Bonn <a href="https://github.com/PRBonn/kiss-icp">KISS-ICP</a> and <a href="https://github.com/PRBonn/lidar-visualizer">Lidar-Visualizer</a></li>
</ul>
<p dir="auto">3D Demo Data for global registration, ICP, meshing etc.:</p>
<ul dir="auto">
<li><a href="https://github.com/isl-org/open3d_downloads/releases/download/20220201-data/BunnyMesh.ply">BunnyMesh.ply</a> from <a href="https://github.com/isl-org/open3d_downloads/releases/tag/20220201-data">20220201-data</a></li>
<li><a href="https://github.com/isl-org/open3d_downloads/releases/download/20220301-data/DemoICPPointClouds.zip">DemoICPPointClouds.zip</a> from <a href="https://github.com/isl-org/open3d_downloads/releases/tag/20220301-data">20220301-data</a></li>
</ul>
<p dir="auto">Using a MOSFET for switching: <a href="https://elinux.org/RPi_GPIO_Interface_Circuits#Using_a_FET" rel="nofollow">tutorial</a></p>
<p dir="auto">A4988 Enable, Sleep and Reset <a href="https://www.youtube.com/watch?v=PMS5jY7RTjo" rel="nofollow">tutorial</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ssl.com: DCV bypass and issue fake certificates for any MX hostname (196 pts)]]></title>
            <link>https://bugzilla.mozilla.org/show_bug.cgi?id=1961406</link>
            <guid>43738485</guid>
            <pubDate>Sat, 19 Apr 2025 18:44:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1961406">https://bugzilla.mozilla.org/show_bug.cgi?id=1961406</a>, See on <a href="https://news.ycombinator.com/item?id=43738485">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrapper">

 


<main id="bugzilla-body" tabindex="-1">



<div id="main-inner">













<div id="module-categories">
        <p><span id="field-value-component">
      <div>
        <p><span id="component-name" tabindex="0" role="button" aria-haspopup="menu" aria-controls="component-info">CA Certificate Compliance
          
        </span></p>
      </div>
        </span>
    </p></div>






































<meta name="firefox-versions" content="{&quot;FIREFOX_AURORA&quot;:&quot;&quot;,&quot;FIREFOX_DEVEDITION&quot;:&quot;138.0b9&quot;,&quot;FIREFOX_ESR&quot;:&quot;128.9.0esr&quot;,&quot;FIREFOX_ESR115&quot;:&quot;115.22.0esr&quot;,&quot;FIREFOX_ESR_NEXT&quot;:&quot;&quot;,&quot;FIREFOX_NIGHTLY&quot;:&quot;139.0a1&quot;,&quot;LAST_MERGE_DATE&quot;:&quot;2025-03-31&quot;,&quot;LAST_RELEASE_DATE&quot;:&quot;2025-04-01&quot;,&quot;LAST_SOFTFREEZE_DATE&quot;:&quot;2025-03-27&quot;,&quot;LAST_STRINGFREEZE_DATE&quot;:&quot;2025-03-28&quot;,&quot;LATEST_FIREFOX_DEVEL_VERSION&quot;:&quot;138.0b9&quot;,&quot;LATEST_FIREFOX_OLDER_VERSION&quot;:&quot;3.6.28&quot;,&quot;LATEST_FIREFOX_RELEASED_DEVEL_VERSION&quot;:&quot;138.0b9&quot;,&quot;LATEST_FIREFOX_VERSION&quot;:&quot;137.0.2&quot;,&quot;NEXT_MERGE_DATE&quot;:&quot;2025-04-28&quot;,&quot;NEXT_RELEASE_DATE&quot;:&quot;2025-04-29&quot;,&quot;NEXT_SOFTFREEZE_DATE&quot;:&quot;2025-04-24&quot;,&quot;NEXT_STRINGFREEZE_DATE&quot;:&quot;2025-04-25&quot;}">



<div id="c0" data-comment-id="17449976"><p>User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36 Edg/135.0.0.0</p>
<p>Steps to reproduce:</p>
<p>SSL.com failed to conduct accurate domain validation control when utilizing the BR 3.2.2.4.14 DCV method (Email to DNS TXT Contact). <strong>It incorrectly marks the hostname of the approver's email address as a verified domain</strong>, which is completely erroneous.</p>
<h2>Steps to reproduce:</h2>
<ul>
<li>Navigate to <a href="https://dcv-inspector.com/" rel="nofollow">https://dcv-inspector.com</a> and click "Start Test". You will be redirected to a URL such as <a href="https://dcv-inspector.com/test/d2b4eee07de5efcb8598f0586cbf2690" rel="nofollow">https://dcv-inspector.com/test/d2b4eee07de5efcb8598f0586cbf2690</a>.</li>
<li>Create a TXT record for the domain <code>_validation-contactemail.d2b4eee07de5efcb8598f0586cbf2690.test.dcv-inspector.com</code> with the value <code>myusername@aliyun.com</code>. Here, aliyun.com is both a cloud provider and an email provider, similar to @Yahoo.com, @Gmail.com, or @iCloud.com.</li>
<li>Visit SSL.com and request a certificate for the domain <code>d2b4eee07de5efcb8598f0586cbf2690.test.dcv-inspector.com</code>. Then, select <code>myusername@aliyun.com</code> from the email approvers list.</li>
<li>Log in to <code>myusername@aliyun.com</code>, retrieve the email that contains the DCV random value, and finalize the DCV validation process.</li>
<li>SSL.com will add the domain name of the email address (the part after the <code>@</code>. in this case, aliyun.com) to your list of verified domains.</li>
<li>To obtain certificates for aliyun.com and <a href="http://www.aliyun.com/" rel="nofollow">www.aliyun.com</a>, initiate the certificate request. SSL.com will then issue certificates for both aliyun.com and <a href="http://www.aliyun.com/" rel="nofollow">www.aliyun.com</a>.</li>
</ul>
<h2>Affected Certificates</h2>
<ul>
<li><a href="https://crt.sh/?id=17926238129" rel="nofollow">https://crt.sh/?id=17926238129</a></li>
</ul>
<p>Actual results:</p>
<p>SSL.com verified and issued aliyun.com.<br>
I'm not administrator、admin、hostmaster、postmaster、or webmaster of aliyun.com. and also, <code>_validation-contactemail</code> with the value of my email is never configured for <code>aliyun.com</code>.<br>
So,  this is wrong.</p>
<p>Expected results:</p>
<p>Don't list the email domain into verified domains.</p>
</div><div id="a1145_771641"><p>Summary: SSL.com: DCV bypass and issue certificates for any MX hostname → SSL.com: DCV bypass and issue fake certificates for any MX hostname</p></div><div id="c1"><p>SSL.com&nbsp;acknowledges this bug report and we are investigating further.</p></div><div id="c2"><p>Out of an abundance of caution, we have disabled domain validation method 3.2.2.4.14 that was used in the bug report for all SSL/TLS certificates while we investigate. We will provide a preliminary report on or before 2025-04-21.</p>
</div>



</div> 
</main> 
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Don't force your kids to do math (188 pts)]]></title>
            <link>https://blog.avocados.ovh/posts/how-to-force-your-kids-to-do-math/</link>
            <guid>43738195</guid>
            <pubDate>Sat, 19 Apr 2025 18:08:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.avocados.ovh/posts/how-to-force-your-kids-to-do-math/">https://blog.avocados.ovh/posts/how-to-force-your-kids-to-do-math/</a>, See on <a href="https://news.ycombinator.com/item?id=43738195">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><p>Well… you probably shouldn’t.</p><p>This is my one rule: if my son ever says he doesn’t want to do math, we simply stop. No arguing, no bribing, no pushing. We do something else instead.</p><p>Why? Because math is not a chore—it’s a way of experiencing the world. Just like tasting new food, enjoying music, or feeling amazed by nature, math should always feel like play, never like work.</p><p><strong>Kids are born explorers</strong>. They naturally want to discover new things, including math. My main goal is simply to keep that natural curiosity alive and growing.</p><p>Before my son could even talk—as every parenting book suggests—I talked to him constantly. Counting stairs, naming colors, explaining everything around us. I emphasized numbers because I genuinely enjoy them. And that’s perhaps the most important lesson I’ve learned: <strong>children sense your true passions and naturally want to join in</strong>.</p><p>Just play. <a href="https://amzn.to/3RqZNWY">A simple wooden game</a> with numbers and colored bars was our playground. At first, it was sorting by colors or matching bars to numbers. Attention spans started short, a few moments here and there. But gradually, these moments grew into twenty or even thirty delightful minutes.</p><p>Watching him connect five bars to the numeral ‘5’ was magical—it was the spark of mathematical abstraction. Soon, we created addition games and countless imaginative scenarios. I think I enjoyed inventing these simple, playful activities just as much as he did.</p><p>To keep math exciting, we built it into stories. For a while, I drew a burning house next to a 3x3 grid with missing numbers, sums waiting to be solved. Each correct answer earned him a blue pencil to draw water, putting out fires—he loved pretending to be a fireman. Without realizing it, he was doing algebra. Train rides became great opportunities for these little games, free from distractions. Math is everywhere if you look for it—calculating how much he could buy with 20 euros at the toy store, counting steps to reach a certain location, comparing which fruit is heavier at the market, or even timing how fast he could run across the park. You just need to open your child’s eyes to see math in daily life. Repetition is key!</p><p>The games evolved as he grew older. Card games like <a href="https://amzn.to/42CcKSV">Rat-a-Tat Cat</a> also became math games—adding card values became natural. Soon, calculations like 14 + 11 happened effortlessly in his head.</p><p><strong>Repetition is key!</strong> Time flying by is probably the sneakiest thing with kids. With our busy jobs, household chores, and daily demands, it’s easy to forget to spend meaningful time with your children. I have an internal KPI: if in the last three days I haven’t spent at least 30 minutes playing with my kid, there’s something seriously wrong.</p><p>Yet, <strong>the hardest part remains balancing passion and pressure</strong>. I deeply love math, coding, music, and sports, and naturally, I want to share these joys. But there’s a thin line between sharing and imposing. Children don’t always express their discomfort openly—family dynamics can be subtle and easily overlooked. Actually, this is one of the hardest things as a parent. It should be fine, but sometimes we don’t realize that sharing our passions might actually be about our own ego. <strong>Understanding this is challenging, and we all fail at some point</strong>. Still, it’s important to keep asking ourselves the question.</p><p>In fact, we stopped regularly engaging with structured math games before he started school at six, as his interests naturally shifted toward other exciting activities like building paper airplanes, painting, and drawing—things that didn’t interest him a year earlier. He’s probably above average at math, but that isn’t the point. What I genuinely cherish now is watching his curiosity spark questions about infinity. He wonders about adding infinities together and eagerly discusses different sizes of infinity with me, imagining them as creatures that can even ’eat’ each other.</p><p>Because, in the end, <strong>the real goal isn’t math itself</strong>—it’s nurturing a child’s natural eagerness to learn, explore, and wonder about the world.</p><p>Don’t force math.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An image of the Australian desert illuminates satellite pollution (158 pts)]]></title>
            <link>https://www.thisiscolossal.com/2025/04/a-stunning-image-of-the-australian-desert-illuminates-the-growing-problem-of-satellite-pollution/</link>
            <guid>43737469</guid>
            <pubDate>Sat, 19 Apr 2025 16:32:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thisiscolossal.com/2025/04/a-stunning-image-of-the-australian-desert-illuminates-the-growing-problem-of-satellite-pollution/">https://www.thisiscolossal.com/2025/04/a-stunning-image-of-the-australian-desert-illuminates-the-growing-problem-of-satellite-pollution/</a>, See on <a href="https://news.ycombinator.com/item?id=43737469">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="content">
		<main id="main">

			
<article id="post-453876">

	<div>
		




<p>In January 2021, <a href="https://www.instagram.com/joshua_rozells/?hl=en" target="_blank" rel="noreferrer noopener">Joshua Rozells</a> ventured out into the Pinnacles Desert in Western Australia, intending to photograph a star trail. But after shooting for more than three hours and reviewing his images, he realized that the light patterns he captured weren’t what he had hoped for. </p>



<p>“There were satellite trails visible in almost every single photo,” he wrote on <a href="https://www.instagram.com/joshua_rozells/?hl=en" target="_blank" rel="noreferrer noopener">Instagram</a>. “Instead of trying to get rid of them for a star trail, I decided to put the satellite trails together into a single image to show how polluted the night sky is becoming.”</p>



<p>Stitching together 343 distinct photos, Rozells illuminates a growing problem. When Elon Musk’s SpaceX launched Starlink in 2019, 60 satellites filled the skies, with a race from other companies to follow. That number <a href="https://www.salon.com/2025/04/04/erasing-the-stars-satellite-megaconstellations-are-a-mega-problem-for-earth-and-sky/" target="_blank" rel="noreferrer noopener">has now topped 10,000</a>, with tens of thousands more in the works. SpaceX alone plans to launch 40,000 more.</p>



<p>Rozells’ composite visually echoes pleas from astronomers, who warn that although satellites collect essential data, the staggering amount filling our skies will only worsen light pollution and our ability to study what lies beyond. Because this industry has little regulation, the problem could go unchecked. </p>



<p>“Thankfully, astronomers across the globe have taken notice of this growing issue and are starting to speak up,” Rozells adds. “Organisations such as the International Astronomical Union’s Center for the Protection of the Dark and Quiet Sky are advocating for the regulation and protection of the night sky.” (via <a href="https://kottke.org/25/04/swamped-skies" target="_blank" rel="noreferrer noopener">Kottke</a>)</p>

					
		
		
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

	
			
	</article><!-- #post-${ID} -->

		</main><!-- #main -->
	</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Librarians Are Dangerous (574 pts)]]></title>
            <link>https://bradmontague.substack.com/p/librarians-are-dangerous</link>
            <guid>43736791</guid>
            <pubDate>Sat, 19 Apr 2025 14:49:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bradmontague.substack.com/p/librarians-are-dangerous">https://bradmontague.substack.com/p/librarians-are-dangerous</a>, See on <a href="https://news.ycombinator.com/item?id=43736791">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>Dear Enthusiasts, </p><p><span>I write to you today as a concerned citizen.</span><br><span>Many aren’t brave enough to say it, but the time has come. So, I will say it:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff585147d-6a94-4cdf-a583-98db4ce88992_3840x2160.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff585147d-6a94-4cdf-a583-98db4ce88992_3840x2160.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff585147d-6a94-4cdf-a583-98db4ce88992_3840x2160.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff585147d-6a94-4cdf-a583-98db4ce88992_3840x2160.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff585147d-6a94-4cdf-a583-98db4ce88992_3840x2160.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff585147d-6a94-4cdf-a583-98db4ce88992_3840x2160.png" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f585147d-6a94-4cdf-a583-98db4ce88992_3840x2160.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:9474047,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://bradmontague.substack.com/i/160909743?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff585147d-6a94-4cdf-a583-98db4ce88992_3840x2160.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff585147d-6a94-4cdf-a583-98db4ce88992_3840x2160.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff585147d-6a94-4cdf-a583-98db4ce88992_3840x2160.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff585147d-6a94-4cdf-a583-98db4ce88992_3840x2160.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff585147d-6a94-4cdf-a583-98db4ce88992_3840x2160.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><strong>Librarians are dangerous.</strong></p><p><em>(Dramatic music should be playing in your head right now.)</em></p><p><span>You thought they were just keepers of the quiet. Guardians of the “shhh.”</span><br><span>The ones who handed you a dusty book and pointed to a beanbag chair.</span></p><p><strong>Wrong.</strong></p><p><span>Let’s look at the facts.</span><br><span>Sure, they sit there … calmly. Quietly. With their little computers and Dewey Decimal Systems. But make no mistake …. these are </span><em>not</em><span> peaceful people.</span></p><p>Some think they’re just shelving books. No! </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F650123a6-ab75-414e-b935-2ee8be90e5bc_3600x3600.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F650123a6-ab75-414e-b935-2ee8be90e5bc_3600x3600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F650123a6-ab75-414e-b935-2ee8be90e5bc_3600x3600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F650123a6-ab75-414e-b935-2ee8be90e5bc_3600x3600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F650123a6-ab75-414e-b935-2ee8be90e5bc_3600x3600.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F650123a6-ab75-414e-b935-2ee8be90e5bc_3600x3600.png" width="1456" height="1456" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/650123a6-ab75-414e-b935-2ee8be90e5bc_3600x3600.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:16191144,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://bradmontague.substack.com/i/160909743?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F650123a6-ab75-414e-b935-2ee8be90e5bc_3600x3600.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F650123a6-ab75-414e-b935-2ee8be90e5bc_3600x3600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F650123a6-ab75-414e-b935-2ee8be90e5bc_3600x3600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F650123a6-ab75-414e-b935-2ee8be90e5bc_3600x3600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F650123a6-ab75-414e-b935-2ee8be90e5bc_3600x3600.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>They’re </span><em>plotting … </em></p><p><em><strong>Plotting the overthrow of ignorance.</strong></em></p><p><span>As a kid, my idea of danger was eating Pop Rocks and Coke at the same time. At no point did I look at the librarians in my life and think, “Now </span><em>that’s</em><span> someone who could dismantle society using nothing but a hardcover and a knowing glance.”</span></p><p>But I should have.</p><p><span>Because </span><strong>librarians are dangerous</strong><span>.</span></p><p>Not in a “leap-out-of-the-shadows” kind of way. More in a “mercenaries of media literacy-knowers of where things are - masters of organized rebellion” way.</p><p><span>They can look at you and hand you a book that will COMPLETELY DESTROY YOUR WORLDVIEW… but… like… in a gentle, respectful, and </span><em>possibly laminated</em><span> way.</span></p><p><span>Let me be clear: these modern librarians are not the nostalgic memory you may have of a woman with a bun and a stamp. Today’s dangerous librarians are much more. They are </span><strong>part educator</strong><span>, </span><strong>part tech wizard, part data analyst, </strong><em><strong>and</strong></em><strong> part myth-slayer</strong><span>.</span></p><p>They can code. They can curate. They can find a book you only remember as, “blue, with a sad fox, maybe?”</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bdc7e12-86fa-4c04-b83c-75de5f51530d_3600x3600.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bdc7e12-86fa-4c04-b83c-75de5f51530d_3600x3600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bdc7e12-86fa-4c04-b83c-75de5f51530d_3600x3600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bdc7e12-86fa-4c04-b83c-75de5f51530d_3600x3600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bdc7e12-86fa-4c04-b83c-75de5f51530d_3600x3600.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bdc7e12-86fa-4c04-b83c-75de5f51530d_3600x3600.png" width="1456" height="1456" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7bdc7e12-86fa-4c04-b83c-75de5f51530d_3600x3600.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:23061561,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://bradmontague.substack.com/i/160909743?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bdc7e12-86fa-4c04-b83c-75de5f51530d_3600x3600.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bdc7e12-86fa-4c04-b83c-75de5f51530d_3600x3600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bdc7e12-86fa-4c04-b83c-75de5f51530d_3600x3600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bdc7e12-86fa-4c04-b83c-75de5f51530d_3600x3600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bdc7e12-86fa-4c04-b83c-75de5f51530d_3600x3600.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><br><span>They host storytimes, teach kids about misinformation, explain how to 3D print a prosthetic hand, and calmly help a grown man named Todd recover his Gmail password for the seventh time. All before lunch.</span></p><p><strong>WHO DOES THAT? </strong><span>Librarians do that. </span></p><p><span>These dangerous folks believe in wild things like access. They believe in stories. They believe in </span><em>you. </em><span>They even believe in</span><em> me. </em></p><p><strong>My books wouldn’t have found nearly as many people without them.</strong><span> </span><em>WHY ARE THEY DOING THIS? What is in it for them?!?!</em></p><p><span>They believe that every student…. no matter who they are or where they’re from… deserves to find a book that says, </span><em>“You belong here.” </em><span>They give access to stories that whisper, </span><em>“You matter. You’re not alone.” </em><span>And … I mean, COME ON. This is all obviously emotional destabilization through narrative arc.</span></p><p><span>Librarians are </span><em>dangerous</em><span> and </span><em>fearless</em><span>. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4c7ac60-9495-43d5-8347-088a10e401b3_3600x3600.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4c7ac60-9495-43d5-8347-088a10e401b3_3600x3600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4c7ac60-9495-43d5-8347-088a10e401b3_3600x3600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4c7ac60-9495-43d5-8347-088a10e401b3_3600x3600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4c7ac60-9495-43d5-8347-088a10e401b3_3600x3600.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4c7ac60-9495-43d5-8347-088a10e401b3_3600x3600.png" width="1456" height="1456" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e4c7ac60-9495-43d5-8347-088a10e401b3_3600x3600.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:22997621,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://bradmontague.substack.com/i/160909743?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4c7ac60-9495-43d5-8347-088a10e401b3_3600x3600.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4c7ac60-9495-43d5-8347-088a10e401b3_3600x3600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4c7ac60-9495-43d5-8347-088a10e401b3_3600x3600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4c7ac60-9495-43d5-8347-088a10e401b3_3600x3600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4c7ac60-9495-43d5-8347-088a10e401b3_3600x3600.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Many have tried to stop them. Yet, they stare down budget cuts, criticism, rowdy teenagers, and that one weird smell in the YA section without flinching. They do their librarian things with such a gracious sort of grit that it’s terrifying.</p><p><span>And let me tell you something:</span><br><span>Do not try to debate a librarian. </span></p><p><span>You will not win.</span><br><span>You’ll walk in confident. Sure.</span><br><span>You’ll leave, though, with a tote bag and a </span><em>brand-new worldview.</em></p><p><strong>They don’t just help students find books. They help them find </strong><em><strong>themselves.</strong></em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0379646f-5767-46ed-89a0-2c6766ad0a40_3600x3600.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0379646f-5767-46ed-89a0-2c6766ad0a40_3600x3600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0379646f-5767-46ed-89a0-2c6766ad0a40_3600x3600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0379646f-5767-46ed-89a0-2c6766ad0a40_3600x3600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0379646f-5767-46ed-89a0-2c6766ad0a40_3600x3600.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0379646f-5767-46ed-89a0-2c6766ad0a40_3600x3600.png" width="383" height="383" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0379646f-5767-46ed-89a0-2c6766ad0a40_3600x3600.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:383,&quot;bytes&quot;:22410237,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://bradmontague.substack.com/i/160909743?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0379646f-5767-46ed-89a0-2c6766ad0a40_3600x3600.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0379646f-5767-46ed-89a0-2c6766ad0a40_3600x3600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0379646f-5767-46ed-89a0-2c6766ad0a40_3600x3600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0379646f-5767-46ed-89a0-2c6766ad0a40_3600x3600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0379646f-5767-46ed-89a0-2c6766ad0a40_3600x3600.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>So take note. This is your warning:</p><p><strong>Librarians are dangerous.</strong></p><p>They are dangerous to:</p><ul><li><p>Misinformation</p></li><li><p>Censorship</p></li><li><p>Outdated printer settings</p></li><li><p>Small thinking</p></li><li><p>apathy</p></li><li><p>loneliness</p></li><li><p>Silence where there should be a story</p></li><li><p>Anyone who underestimates a kid with a library card</p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76b33d-7dd1-4c73-94d9-a7423b58bbfb_3600x3600.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76b33d-7dd1-4c73-94d9-a7423b58bbfb_3600x3600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76b33d-7dd1-4c73-94d9-a7423b58bbfb_3600x3600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76b33d-7dd1-4c73-94d9-a7423b58bbfb_3600x3600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76b33d-7dd1-4c73-94d9-a7423b58bbfb_3600x3600.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76b33d-7dd1-4c73-94d9-a7423b58bbfb_3600x3600.png" width="1456" height="1456" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0d76b33d-7dd1-4c73-94d9-a7423b58bbfb_3600x3600.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:22042984,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://bradmontague.substack.com/i/160909743?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76b33d-7dd1-4c73-94d9-a7423b58bbfb_3600x3600.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76b33d-7dd1-4c73-94d9-a7423b58bbfb_3600x3600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76b33d-7dd1-4c73-94d9-a7423b58bbfb_3600x3600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76b33d-7dd1-4c73-94d9-a7423b58bbfb_3600x3600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76b33d-7dd1-4c73-94d9-a7423b58bbfb_3600x3600.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>They do not just guard books. They unleash them. And when a kid finds the </span><em>right</em><span> book…  the one that makes them feel seen, understood, </span><em><strong>alive</strong></em><span>... dangerous librarians know what it can do.</span></p><p><span>They are not just keepers of knowledge.</span><strong> </strong><span>They’re igniters of minds. Builders of empathy. Activists with a barcode scanner. Architects of a freer, wiser, kinder world. </span></p><p>They are the reason so many kids will grow up and realize the world is bigger, messier, and more beautiful than their textbooks ever admitted.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb907edd3-df8a-4b89-97f6-c2487017822e_3840x2160.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb907edd3-df8a-4b89-97f6-c2487017822e_3840x2160.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb907edd3-df8a-4b89-97f6-c2487017822e_3840x2160.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb907edd3-df8a-4b89-97f6-c2487017822e_3840x2160.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb907edd3-df8a-4b89-97f6-c2487017822e_3840x2160.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb907edd3-df8a-4b89-97f6-c2487017822e_3840x2160.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b907edd3-df8a-4b89-97f6-c2487017822e_3840x2160.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:888500,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://bradmontague.substack.com/i/160909743?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb907edd3-df8a-4b89-97f6-c2487017822e_3840x2160.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb907edd3-df8a-4b89-97f6-c2487017822e_3840x2160.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb907edd3-df8a-4b89-97f6-c2487017822e_3840x2160.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb907edd3-df8a-4b89-97f6-c2487017822e_3840x2160.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb907edd3-df8a-4b89-97f6-c2487017822e_3840x2160.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>So next time you walk past a library and think,</span><br><em>“Oh look, a quiet place for quiet people doing quiet things...”</em></p><p>Think again. I want you to remember that librarians aren’t just standing behind a desk. They stand dangerously at the frontlines of curiosity, creativity, compassion, and the fight for a better tomorrow through what we imagine today. </p><p>So go ahead. Underestimate them, but do so at your own peril.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da3752-a2d8-4f45-89f3-d03f08c2dad2_3600x3600.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da3752-a2d8-4f45-89f3-d03f08c2dad2_3600x3600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da3752-a2d8-4f45-89f3-d03f08c2dad2_3600x3600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da3752-a2d8-4f45-89f3-d03f08c2dad2_3600x3600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da3752-a2d8-4f45-89f3-d03f08c2dad2_3600x3600.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da3752-a2d8-4f45-89f3-d03f08c2dad2_3600x3600.png" width="1456" height="1456" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/90da3752-a2d8-4f45-89f3-d03f08c2dad2_3600x3600.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:23025469,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://bradmontague.substack.com/i/160909743?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da3752-a2d8-4f45-89f3-d03f08c2dad2_3600x3600.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da3752-a2d8-4f45-89f3-d03f08c2dad2_3600x3600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da3752-a2d8-4f45-89f3-d03f08c2dad2_3600x3600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da3752-a2d8-4f45-89f3-d03f08c2dad2_3600x3600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90da3752-a2d8-4f45-89f3-d03f08c2dad2_3600x3600.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><br><span>Because librarians know: The right book, in the right hands, at the right moment,</span><br><span>can change </span><strong>everything.</strong></p><p>And thank goodness for that.</p><p><span>To every librarian reading this:</span><br><strong>Stay dangerous.</strong></p><p>(Also… sorry about all the late books.)</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b64fe8c-5fc2-4587-b039-68f666dbf861_464x74.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b64fe8c-5fc2-4587-b039-68f666dbf861_464x74.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b64fe8c-5fc2-4587-b039-68f666dbf861_464x74.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b64fe8c-5fc2-4587-b039-68f666dbf861_464x74.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b64fe8c-5fc2-4587-b039-68f666dbf861_464x74.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b64fe8c-5fc2-4587-b039-68f666dbf861_464x74.png" width="232" height="37" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2b64fe8c-5fc2-4587-b039-68f666dbf861_464x74.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:74,&quot;width&quot;:464,&quot;resizeWidth&quot;:232,&quot;bytes&quot;:20403,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://bradmontague.substack.com/i/160909743?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b64fe8c-5fc2-4587-b039-68f666dbf861_464x74.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b64fe8c-5fc2-4587-b039-68f666dbf861_464x74.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b64fe8c-5fc2-4587-b039-68f666dbf861_464x74.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b64fe8c-5fc2-4587-b039-68f666dbf861_464x74.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b64fe8c-5fc2-4587-b039-68f666dbf861_464x74.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Last week was </span><strong>National Library Week</strong><span>! I had the privilege of speaking at the Garland County Library in Hot Springs, Arkansas, during their </span><strong>Children’s Librarian Workshop</strong><span>. Librarians and library staff from across the state gathered to share ideas and inspiration for serving young readers. Here’s a picture of me and some of their team: </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4289d6bd-e91a-48af-acae-fdfe2690d02a_1085x815.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4289d6bd-e91a-48af-acae-fdfe2690d02a_1085x815.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4289d6bd-e91a-48af-acae-fdfe2690d02a_1085x815.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4289d6bd-e91a-48af-acae-fdfe2690d02a_1085x815.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4289d6bd-e91a-48af-acae-fdfe2690d02a_1085x815.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4289d6bd-e91a-48af-acae-fdfe2690d02a_1085x815.jpeg" width="455" height="341.7741935483871" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4289d6bd-e91a-48af-acae-fdfe2690d02a_1085x815.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:815,&quot;width&quot;:1085,&quot;resizeWidth&quot;:455,&quot;bytes&quot;:397866,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://bradmontague.substack.com/i/160909743?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4289d6bd-e91a-48af-acae-fdfe2690d02a_1085x815.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4289d6bd-e91a-48af-acae-fdfe2690d02a_1085x815.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4289d6bd-e91a-48af-acae-fdfe2690d02a_1085x815.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4289d6bd-e91a-48af-acae-fdfe2690d02a_1085x815.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4289d6bd-e91a-48af-acae-fdfe2690d02a_1085x815.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>After several weeks of travel, this was my final stop before heading home and it was the </span><em>perfect</em><span> place to conclude refreshed and encouraged. Nothing beats being in a room full of great humans like these. So grateful for these amazing librarians who very much embody the danger I wrote about above.</span></p><p><span>ALSO! </span><em><strong>VERY</strong></em><strong> IMPORTANT </strong><em><strong>ENTHUSIAST ALERT</strong></em><strong>!</strong><span> I want to spotlight </span><strong>Beth Quarles</strong><span>, a third-grade teacher and the owner of the incredible indie bookstore </span><em><strong><a href="https://www.paperheartsbooks.com/" rel="">Paper Hearts Bookstore</a><span> </span></strong></em><span>in Little Rock, Arkansas. I signed copies of </span><em>Fail-a-Bration!, The Fantastic Bureau of Imagination, The Circles All Around Us,</em><span> and </span><em>Becoming Better Grownups</em><span> for her store. </span><strong>She still has several in stock!</strong><span> </span><strong>Go buy them out!</strong><span> </span></p><p><span>Consider supporting Beth, her team, and this wonderful shop. </span><em><strong>You can purchase signed copies from her here:</strong></em><span> </span><strong><a href="https://bookshop.org/beta-search?keywords=brad+montague" rel="">Paper Hearts Bookstore</a></strong></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6cb3f25-0edc-41a4-9e1b-c24426583af4_2316x2232.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6cb3f25-0edc-41a4-9e1b-c24426583af4_2316x2232.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6cb3f25-0edc-41a4-9e1b-c24426583af4_2316x2232.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6cb3f25-0edc-41a4-9e1b-c24426583af4_2316x2232.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6cb3f25-0edc-41a4-9e1b-c24426583af4_2316x2232.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6cb3f25-0edc-41a4-9e1b-c24426583af4_2316x2232.jpeg" width="360" height="346.8956043956044" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c6cb3f25-0edc-41a4-9e1b-c24426583af4_2316x2232.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1403,&quot;width&quot;:1456,&quot;resizeWidth&quot;:360,&quot;bytes&quot;:1850108,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://bradmontague.substack.com/i/160909743?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6cb3f25-0edc-41a4-9e1b-c24426583af4_2316x2232.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6cb3f25-0edc-41a4-9e1b-c24426583af4_2316x2232.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6cb3f25-0edc-41a4-9e1b-c24426583af4_2316x2232.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6cb3f25-0edc-41a4-9e1b-c24426583af4_2316x2232.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6cb3f25-0edc-41a4-9e1b-c24426583af4_2316x2232.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>teacher. bookseller. menace.</figcaption></figure></div><p><span>Next week, I’m back in Michigan for the </span><strong><a href="https://sc4a.org/event/curiosity-in-action/" rel="">Curiosity in Action Conference</a></strong><span>. It’s happening April 26. </span><a href="https://sc4a.org/event/curiosity-in-action/" rel=""> Some tickets still available! I’d love to see you there!</a></p><div><figure><a target="_blank" href="https://sc4a.org/event/curiosity-in-action/" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eb5aee0-c172-4b1d-9ca8-73682bdf854e_2500x834.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eb5aee0-c172-4b1d-9ca8-73682bdf854e_2500x834.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eb5aee0-c172-4b1d-9ca8-73682bdf854e_2500x834.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eb5aee0-c172-4b1d-9ca8-73682bdf854e_2500x834.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eb5aee0-c172-4b1d-9ca8-73682bdf854e_2500x834.jpeg" width="503" height="167.89697802197801" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9eb5aee0-c172-4b1d-9ca8-73682bdf854e_2500x834.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:486,&quot;width&quot;:1456,&quot;resizeWidth&quot;:503,&quot;bytes&quot;:351426,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:&quot;https://sc4a.org/event/curiosity-in-action/&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://bradmontague.substack.com/i/160909743?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eb5aee0-c172-4b1d-9ca8-73682bdf854e_2500x834.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eb5aee0-c172-4b1d-9ca8-73682bdf854e_2500x834.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eb5aee0-c172-4b1d-9ca8-73682bdf854e_2500x834.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eb5aee0-c172-4b1d-9ca8-73682bdf854e_2500x834.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9eb5aee0-c172-4b1d-9ca8-73682bdf854e_2500x834.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>As always, </span><strong>thank you for reading! Thanks for sharing! Thanks for supporting!</strong><span> </span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to Write a Fast Matrix Multiplication from Scratch with Tensor Cores (2024) (129 pts)]]></title>
            <link>https://alexarmbr.github.io/2024/08/10/How-To-Write-A-Fast-Matrix-Multiplication-From-Scratch-With-Tensor-Cores.html</link>
            <guid>43736739</guid>
            <pubDate>Sat, 19 Apr 2025 14:42:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alexarmbr.github.io/2024/08/10/How-To-Write-A-Fast-Matrix-Multiplication-From-Scratch-With-Tensor-Cores.html">https://alexarmbr.github.io/2024/08/10/How-To-Write-A-Fast-Matrix-Multiplication-From-Scratch-With-Tensor-Cores.html</a>, See on <a href="https://news.ycombinator.com/item?id=43736739">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    
<!-- ---
layout: post
title:  "How To Write A Fast Matrix Multiplication From Scratch With NVIDIA Tensor Cores"
date:   2024-08-10 08:52:08 -0600
categories: jekyll update
--- -->




<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li>
<a href="#background" id="markdown-toc-background">Background</a>    <ul>
      <li><a href="#the-memory-wall" id="markdown-toc-the-memory-wall">The memory wall</a></li>
      <li><a href="#roofline-charts" id="markdown-toc-roofline-charts">Roofline charts</a></li>
      <li>
<a href="#rooflines-for-the-nvidia-tesla-t4" id="markdown-toc-rooflines-for-the-nvidia-tesla-t4">Rooflines for the NVIDIA Tesla T4</a>        <ul>
          <li><a href="#tensor-core-vs-ffma" id="markdown-toc-tensor-core-vs-ffma">Tensor Core vs. FFMA</a></li>
          <li><a href="#shared-memory-vs-l2-cache-vs-global-memory" id="markdown-toc-shared-memory-vs-l2-cache-vs-global-memory">Shared memory vs. L2 cache vs. global memory</a></li>
        </ul>
      </li>
      <li>
<a href="#theoretical-arithmetic-intensity" id="markdown-toc-theoretical-arithmetic-intensity">Theoretical arithmetic intensity</a>        <ul>
          <li><a href="#matrix-multiplication-vs-matrix-addition" id="markdown-toc-matrix-multiplication-vs-matrix-addition">Matrix Multiplication vs Matrix Addition</a></li>
        </ul>
      </li>
      <li>
<a href="#achievable-arithmetic-intensity-on-a-simple-computer" id="markdown-toc-achievable-arithmetic-intensity-on-a-simple-computer">Achievable arithmetic intensity on a simple computer</a>        <ul>
          <li><a href="#worst-case" id="markdown-toc-worst-case">worst case</a></li>
          <li><a href="#best-case" id="markdown-toc-best-case">best case</a></li>
          <li><a href="#realistic-case" id="markdown-toc-realistic-case">realistic case</a></li>
          <li><a href="#in-summary" id="markdown-toc-in-summary">In Summary</a></li>
        </ul>
      </li>
      <li>
<a href="#parallelized-matrix-multiplication-on-a-gpu" id="markdown-toc-parallelized-matrix-multiplication-on-a-gpu">Parallelized matrix multiplication on a GPU</a>        <ul>
          <li><a href="#hierarchical-tiling-simple-gpu" id="markdown-toc-hierarchical-tiling-simple-gpu">Hierarchical Tiling (simple gpu)</a></li>
          <li><a href="#hierarchical-tiling-real-gpu" id="markdown-toc-hierarchical-tiling-real-gpu">Hierarchical Tiling (real gpu)</a></li>
          <li>
<a href="#performance-considerations-on-a-real-gpu" id="markdown-toc-performance-considerations-on-a-real-gpu">Performance considerations on a real GPU</a>            <ul>
              <li><a href="#arithmetic-intensity-as-a-function-of-tile-dimensions" id="markdown-toc-arithmetic-intensity-as-a-function-of-tile-dimensions">Arithmetic intensity as a function of tile dimensions</a></li>
              <li><a href="#overlap-between-compute-and-data-movement" id="markdown-toc-overlap-between-compute-and-data-movement">Overlap between compute and data movement</a></li>
              <li><a href="#maximizing-memory-bandwidth" id="markdown-toc-maximizing-memory-bandwidth">Maximizing memory bandwidth</a></li>
            </ul>
          </li>
          <li><a href="#how-to-use-tensor-cores" id="markdown-toc-how-to-use-tensor-cores">How to use Tensor Cores</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<a href="#kernels" id="markdown-toc-kernels">Kernels</a>    <ul>
      <li><a href="#kernel-1---hierarchical-tiling" id="markdown-toc-kernel-1---hierarchical-tiling">Kernel 1 - Hierarchical Tiling</a></li>
      <li><a href="#kernel-2---vectorized-memory-copy-and-loop-unrolling" id="markdown-toc-kernel-2---vectorized-memory-copy-and-loop-unrolling">Kernel 2 - Vectorized memory copy and loop unrolling</a></li>
      <li>
<a href="#kernel-3---shared-memory-swizzling" id="markdown-toc-kernel-3---shared-memory-swizzling">Kernel 3 - Shared Memory Swizzling</a>        <ul>
          <li><a href="#background-bank-conflicts-and-wavefronts" id="markdown-toc-background-bank-conflicts-and-wavefronts">Background: Bank Conflicts and Wavefronts</a></li>
          <li><a href="#ldmatrix-bank-conflicts" id="markdown-toc-ldmatrix-bank-conflicts">ldmatrix bank conflicts</a></li>
          <li><a href="#padding" id="markdown-toc-padding">Padding</a></li>
          <li><a href="#swizzling-toy-example" id="markdown-toc-swizzling-toy-example">Swizzling (toy example)</a></li>
          <li><a href="#swizzling-real-world" id="markdown-toc-swizzling-real-world">Swizzling (real world)</a></li>
        </ul>
      </li>
      <li>
<a href="#kernel-4---makeshift-async-copy" id="markdown-toc-kernel-4---makeshift-async-copy">Kernel 4 - Makeshift Async Copy</a>        <ul>
          <li><a href="#gpu-occupancy-digression" id="markdown-toc-gpu-occupancy-digression">GPU occupancy (digression)</a></li>
        </ul>
      </li>
      <li>
<a href="#kernel-5---tune-tile-dimensions" id="markdown-toc-kernel-5---tune-tile-dimensions">Kernel 5 - Tune Tile Dimensions</a>        <ul>
          <li>
<a href="#tune-tile-dimensions" id="markdown-toc-tune-tile-dimensions">tune tile dimensions</a>            <ul>
              <li><a href="#m-and-n-dimensions--l2-cache-locality" id="markdown-toc-m-and-n-dimensions--l2-cache-locality">M and N Dimensions / L2 cache locality</a></li>
              <li><a href="#k-dimension" id="markdown-toc-k-dimension">K Dimension</a></li>
            </ul>
          </li>
          <li><a href="#tile-dimensions---longer-and-thinner" id="markdown-toc-tile-dimensions---longer-and-thinner">tile dimensions - longer and thinner</a></li>
        </ul>
      </li>
      <li><a href="#kernel-5---optimize-index-calculation" id="markdown-toc-kernel-5---optimize-index-calculation">Kernel 5 - Optimize Index Calculation</a></li>
      <li><a href="#kernel-6---double-buffering" id="markdown-toc-kernel-6---double-buffering">Kernel 6 - Double Buffering</a></li>
    </ul>
  </li>
  <li>
<a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a>    <ul>
      <li><a href="#things-i-didnt-do" id="markdown-toc-things-i-didnt-do">things I didn’t do</a></li>
      <li><a href="#performance-on-different-matrix-sizes" id="markdown-toc-performance-on-different-matrix-sizes">performance on different matrix sizes</a></li>
      <li><a href="#lessons-learned-newer-gpus-are-better" id="markdown-toc-lessons-learned-newer-gpus-are-better">lessons learned, newer GPUs are better</a></li>
    </ul>
  </li>
  <li><a href="#resources--acknowledgements" id="markdown-toc-resources--acknowledgements">Resources / Acknowledgements</a></li>
  <li><a href="#are-you-hiring-gpu-nerds" id="markdown-toc-are-you-hiring-gpu-nerds">Are you hiring GPU nerds?</a></li>
</ul>

<h2 id="introduction">Introduction</h2>
<p>This post details my recent efforts to write an optimized matrix multiplication kernel in CUDA using tensor cores on a NVIDIA Tesla T4 GPU. The goal is to compute $D = \alpha * A * B + \beta * C$, as fast as possible. In this equation $D,A,B$ and $C$ are large matrices full of half precision floating point numbers, and $\alpha$, $\beta$ are constants. This problem is usually referred to as a <strong>H</strong>alf-precision <strong>Ge</strong>neralized <strong>M</strong>atrix <strong>M</strong>ultiply, or <strong>HGEMM</strong> for short.</p>

<p>Tensor Cores are specialized hardware units on NVIDIA chips that implement a small matrix multiplication in hardware. I recently became interested in tensor cores for two reasons. First, it seems like <a href="https://www.semianalysis.com/i/136469751/the-gpu-rich">most</a> generative AI training and inference these days happens on A100s and H100s. Second, all of this training and inference is almost certainly running on the tensor cores of these devices, because they offer a massive throughput increase for matrix math as compared to what you get if you dont use them. From <a href="https://hazyresearch.stanford.edu/blog/2024-05-12-tk">here</a></p>
<blockquote>
  <p>An H100 GPU has 989 TFLOPs of half-precision matrix multiply compute, and ~60 TFLOPs of “everything else”. So, every cycle the tensor core is in use, you’re getting at least 94% utilization of the hardware. And every cycle the tensor core is not in use, you’re getting no more than 6% utilization of the hardware.</p>
</blockquote>

<p>Given their huge importance in the world today, when I started this project it felt to me like there is disproportionately little info and dialogue on the internet about how to use them directly. I quickly learned this lack of dialogue on the internet is probably because writing algorithms that use them is a bit of a niche interest. The basic mechanics of how to call them are not hard, however writing a kernel that can use them at anywhere close their full potential <em>is</em> hard. Their huge throughput means that in order to use them at anywhere close to their full potential, you need to move bytes though the memory hierarchy of the GPU in a maximally efficient way, and overlap the computing with this data movement. There are certain algorithmic techniques that you need to use if you want get your moneys worth from your tensor cores, this article is an exploration of these techniques.</p>

<p>I figured out the implementation details mostly by digging around the NVIDIA <a href="https://github.com/NVIDIA/cutlass/tree/main">CUTLASS</a> forums and source, and I wrote this article in order to make sure I actually understand what I am doing, and also in the hope that some fellow GPU nerds trying to work with tensor cores might find it helpful. It should be noted that this whole project was done on a Turing architecture GPU, which was state of the art in 2018, and some details of some of the optimizations discussed in this article are somewhat specific to the Turing architecture. I noticed while working on this is that the more modern Hopper architecture has dedicated hardware support that directly addresses some of the performance issues and bottlenecks that I ran into along the way when working on optimizations that target an older GPU. More modern GPUs justify their increased price tag not only with increased floating point throughput, but also with features that ease the cognitive burden on programmers who are trying to optimize kernels for them.</p>

<p>When I started my goal was to write a kernel with comparable performance to the cuBLAS <a href="https://docs.nvidia.com/cuda/cublas/#cublas-level-3-function-reference">hgemm</a> implementation, which is the closed-source, gold standard implementation released by NVIDIA. I iteratively optimized a series of 6 kernels, with the <a href="https://github.com/alexarmbr/matmul-playground/blob/main/src/kernel1.cu">first</a> achieving a measly 8% of the cuBLAS throughput, and the <a href="https://github.com/alexarmbr/matmul-playground/blob/main/src/kernel6.cu">last</a> achieving a decent 96% of the cuBLAS throughput for 8192x8192 matrices.</p>

<p>This article contains a background section that explains some theory that is helpful to have in your head when thinking about how to optimize kernels that operate on matrices. The rest of the article explains six algorithmic techniques that I used to make my kernel run as fast as possible. Code can be found <a href="https://github.com/alexarmbr/matmul-playground">here</a> on github.</p>

<p>Here is a table with the performance comparison of all of the kernels:
<img src="https://alexarmbr.github.io/images/table6.png" alt="table6"></p>

<h2 id="background">Background</h2>
<h2 id="the-memory-wall">The memory wall</h2>

<p>In the 70 or so years it has been since humanity started building transistor based computers, the capacity for performing arithmetic has been growing along the moores law exponential, while the capacity for moving data from where it is stored to where it is computed upon has not been growing exponentially. This problem is called the <a href="https://en.wikipedia.org/wiki/Random-access_memory#Memory_wall">memory wall</a> and it is one of the central problems in computer architecture today, <a href="https://horace.io/brrr_intro.html">especially</a> when it comes to deep learning workloads, and especially especially when it comes to tensor core algorithms. What this means for us is that if we want to be able to use the ~65 trillion FLOPs per second that our tensor cores are capable of, moving the corresponding number of bytes per second from DRAM may be a challenge.</p>

<h2 id="roofline-charts">Roofline charts</h2>
<p>The <a href="https://en.wikipedia.org/wiki/Roofline_model">roofline</a> model allows us to think about this conundrum a bit more precisely. The basic idea is that we imagine a simplified computer with a two level memory hierarchy, fast memory and slow memory. We can only perform computation on data that is resident in fast memory, at a peak rate of $\tau$ FLOP/sec. The slow memory has unlimited size, and it can move $\beta$ bytes/sec of data into the fast memory. Because of the memory wall, $\tau$ is way larger than $\beta$.</p>

<p><img src="https://alexarmbr.github.io/images/simple_computer.png" alt="simple_computer"></p>

<p>Any given computation has a certain number of FLOPs that need to be performed, for example to multiply a $M$ by $K$ matrix with a $K$ by $N$ matrix we need to perform $2 * M * N * K$ FLOPs. The more FLOP/sec our algorithm can achieve, the faster we can get the matrix multiplication done. The roofline model gives us an upper bound on the FLOP/sec we can achieve, subject to $\tau$ and $\beta$ which are fixed properties of our hardware. We will refer to achieved FLOP/sec as $T$ for throughput, and the upper bound on T as $T_{max}$.</p>

<p>The maximum FLOP/sec we can achieve ($T_{max}$) is modeled as a function of a variable called <em>computational intensity</em> or $I$ for short, this is a property of the algorithm we write. This metric measures the “data reuse” of our algorithm in units of FLOP/byte: for each byte moved from slow memory to fast memory, how many FLOPs do we perform on it. According to the roofline model, if you are an algorithm designer, your primary concern is to write an algorithm with high computational intensity, or in other words to maximize $I$. In practice, this means moving a chunk of data from slow memory to fast memory, and then performing as many useful operations on it as allowed by whatever algorithm you are writing. Reusing data in fast memory is important for performance, because our memory bandwidth $\beta$ limited; it is a small number compared to $\tau$, which means the transfer of this chunk of data from slow to fast memory is costly. We make the most of it by performing as many useful FLOPs as possible on it.</p>

<p>The roofline model says the upper bound on FLOP/sec ($T_{max}$) we can achieve is the minimum of our computational intensity times memory bandwidth, and the peak floating point throughput of our hardware.</p><p>

\[T_{max}=min(\beta * I, \tau)\]

</p><p>This model is saying there are two ways $T_{max}$ can be limited:</p>
<ul>
  <li>$T_{max}$ can never exceed $\tau$. Even if we perform infinity operations on each byte we move into fast memory, we are still limited by the peak floating point throughput of the hardware. $\tau$ is typically a very big number, for example for the T4 GPU, $\tau$ equals 65,000,000,000,000 FLOP/second. If $\tau$ is our limiting factor, we are in good shape, this scenario is referred to as being <em>compute bound</em>.</li>
  <li>However, $T_{max}$ may also be limited by the memory bandwidth of the device, times the computational intensity of the algorithm. If $\tau$ were infinite, the achieved floating point throughput would simply be the number of bytes/sec being moved into fast memory, times the number of FLOPs performed per byte moved, this is $\beta * I$ (notice how when you multiply $\beta * I$, the units cancel out to give FLOP/sec). If $\beta * I$ is less than $\tau$, this term becomes the limiting factor on $T_{max}$, this scenario is referred to as being <em>memory bound</em>. The thing to do in this situation is to rewrite your algorithm to increase $I$ in the hopes of your algorithm becoming compute bound.</li>
</ul>

<p>Here is the whole thing in a picture, notice how we can go from being memory bound to being compute bound by varying $I$:
<img src="https://alexarmbr.github.io/images/roofline.png" alt="roofline"></p>

<p>The red dotted line in this picture is referred to as the “balance point” of the hardware, it is the level of arithmetic intensity in units of (FLOP/byte) that we need to surpass in order to go from being memory bound to being compute bound. If we call this value $I^* $, then $I^* * \beta=\tau$ or equivalently $I^*=\frac{\tau}{\beta}$. It is a property of a particular computer, the peak floating point throughput, divided by the memory bandwidth. Because of Moore’s law, arithmetic throughput has been improving much faster than memory bandwidth, the consequence of this is that generally speaking, the newer the computer, the higher the balance point.</p>

<h2 id="rooflines-for-the-nvidia-tesla-t4">Rooflines for the NVIDIA Tesla T4</h2>
<p>Plugging in some numbers specific to the GPU we are using, and looking at the resulting roofline can inform our algorithm design, and give us some perspective on what we are in for. On a real computer, there isn’t just a single $\tau$ and $\beta$, there are multiple hardware instructions, each with a different peak throughput $\tau$, and different types of memory, each with a different bandwidth $\beta$.</p>

<h3 id="tensor-core-vs-ffma">Tensor Core vs. FFMA</h3>
<p>I found it helpful first to compare the balance point of the tensor cores with the balance point for the regular single precision math units, both with respect to global memory. This rooflines provides some intuition about why writing an efficient kernel is more challenging if you are using tensor core instructions, as opposed to the more standard, less specialized math instructions.</p>

<p>First, we need to know the global memory bandwidth $\beta_{gmem}$ of our device. NVIDIA spec sheets report <em>theoretical</em> memory bandwidth, which is <a href="https://forums.developer.nvidia.com/t/theoretical-bandwidth-vs-effective-bandwidth/48005/3?u=a14armbr">never</a> achievable in practice. The real number can be found with a benchmark, according to <a href="https://arxiv.org/pdf/1903.07486">this</a> whitepaper the achievable memory bandwidth of the T4 is 220 GB/sec (this is 68% of the 320 GB/sec theoretical memory bandwidth).</p>

<p>Next, we need to know the peak floating point throughput with the tensor core, and the peak floating point throughput without it. Similarly to memory, the theoretical numbers are <a href="https://www.thonking.ai/p/strangely-matrix-multiplications">not actually achievable</a> without the GPU catching fire or melting. I find it reasonable to use the measured throughput of the cuBLAS half precision (uses tensor cores) and single precision (doesn’t use tensor cores) GEMM kernels as the achievable floating point throughput numbers. Looking at the assembly of the cuBLAS half precision kernel we can see that the grunt work is done by <code>HMMA.1688</code>, this instruction performs a single small hardware accelerated matmul (more on this later). For the single precision GEMM kernel, the instruction that does the work is called <code>FFMA</code>, this is a scalar multiply/accumulate operation, $d=a*b+c$. According to my benchmarks, the tensor core HMMA.1688 throughput is 49439 GFLOP/sec, which we will call $\tau_{HMMA}$. The non-tensor core FFMA throughput is 7455 GFLOP/sec, which we will call $\tau_{FFMA}$. These are respectively 76% and 92% of the theoretical peak throughputs, which seems reasonable enough. The resulting rooflines look like this (these plots are typically shown on a log/log scale, this one is not):</p>

<p><img src="https://alexarmbr.github.io/images/t4_roofline.png" alt="t4_roofline"></p>

<p>This plot should give us some intuition about the comparative hardness of writing a kernel that achieves peak FLOP/sec with tensor core instructions vs. writing a kernel that achieves peak FLOP/sec with fused multiply add instructions. The hardness comes from the fact that if we want to reach a throughput of $\tau_{HMMA}$, we need ~6.6x more arithmetic intensity than we need if our goal is to reach $\tau_{FFMA}$. The two balance points in this plot tell us that with FFMA instructions we can perform ~33 FLOPs in the time it takes a byte to travel from global memory, whereas with tensor cores we can perform 224 FLOPs in this same amount of time. This means that if we took a kernel that reached peak flops achievable with FFMA instructions, simply replacing the fused multiply adds in the inner loop with tensor core instructions would <em>not</em> be sufficient to get high tensor core utilization. We would additionally need to improve the code that moves data around to increase the computational intensity by a factor of six. This is one of the things that makes writing a tensor core GEMM interesting!</p>

<h3 id="shared-memory-vs-l2-cache-vs-global-memory">Shared memory vs. L2 cache vs. global memory</h3>
<p>If we want to write a kernel that can make good use of the tensor cores, we need to be conscious of our computers memory hierarchy. The roofline model simplifies the memory hierarchy down to two storage types, one large and slow, and the other fast and instantaneous. In reality, there are more than two levels, each level has different a bandwidth and capacity, and also different considerations that must be considered in order to facilitate efficient access.</p>

<p><img src="https://alexarmbr.github.io/images/t4_memory_hierarchy.png" alt="t4_memory_hierarchy"></p>

<p>In these days of the memory wall, using the faster and smaller levels of the memory hierarchy effectively is critical. This requires some ingenuity because of the smallness: for example on the T4 the on chip shared memory has 16.6x the bandwidth of global memory, but on a given streaming multiprocessor (SM for short) it only fits 64 KiB. If we are multiplying large matrices, this is only enough space to fit a tiny portion of the problem.</p>

<p><img src="https://alexarmbr.github.io/images/t4_memory_roofline.png" alt="t4_memory_roofline"></p>

<p>The plot compares the balance point of tensor cores with respect to:</p>
<ul>
  <li>global memory or DRAM, the largest and slowest level of the memory hierarchy</li>
  <li>the L2 cache which stores recently accessed data from DRAM, and is shared between the 16 SMs on the T4</li>
  <li>shared memory, per SM fast memory that is explicitly managed.</li>
</ul>

<p>Global memory has a balance point of 224, this means that if all of our memory accesses go to DRAM, we will need to perform 224 FLOPs for each byte read from DRAM in order to keep our tensor cores busy. This turns out to be a very tall order, as we will see later when we work out how parameters in our algorithm affect the balance point (the sneak preview is that given the amount of fast memory on the T4 and some other performance considerations, achieving this balance point would be counterproductive). However, the L2 cache comes to the rescue, its balance point with respect to tensor cores is 38, which is a much more manageable number. If a good number of our memory accesses can hit the L2 cache rather than going all the way to global memory, we will have a good shot at being compute bound rather than memory bound. The moral of this story is that we need the L2 cache.</p>

<p>Shared memory is used as an explicitly managed cache that will hold small portions of the input matrices local to a particular SM (a SM is kind of analogous to a single CPU core). Within the SM, threads will load their own local portion of the problem from shared memory into register memory, which is where data must reside in order for it to be computed upon. When shared memory is operating at full bandwidth, its balance point with respect to the tensor core is 13, which means we need to cache enough data in registers to perform 13 FLOPs for each byte read from shared memory. It turns out that each SM has enough register memory to make this easily achievable. When we are optimizing this part of the algorithm, the challenge will be to enable shared memory to operate at full bandwidth, which in practice means organizing the data layout in such a way that we can read it and write it without bank conflicts. Once shared memory is at full bandwidth, sufficient arithmetic intensity will be easy to achieve. I think the shared memory balance point of 13 is worth noting though, because it tells us that shared memory alone is not fast enough to achieve peak tensor core throughput. The moral of this story is that we need registers.</p>

<h2 id="theoretical-arithmetic-intensity">Theoretical arithmetic intensity</h2>
<p>So modern computers generally have an imbalance between their arithmetic throughput and their memory bandwidth, consequently kernels that perform lots of arithmetic relative to data movement make better use of the hardware. At this point we need to think about the algorithm we are running, and forget about hardware for a moment.</p>

<h3 id="matrix-multiplication-vs-matrix-addition">Matrix Multiplication vs Matrix Addition</h3>

<p>Any given algorithm has a maximum amount of arithmetic intensity that is possible, and our goal as an algorithm designer is to write a kernel that achieves an arithmetic intensity as close to this upper bound as we can manage. Comparing the maximum arithmetic intensity that is achievable when adding two $N$ by $N$ matrices, vs. multiplying them, illustrates how different algorithms have different upper bounds in this regard.</p>

<p><img src="https://alexarmbr.github.io/images/multiplication_vs_addition.png" alt="multiplication_vs_addition"></p>

<p>In the case of matrix addition, computing a single output element requires a single arithmetic operation, which means that when we run this algorithm the amount of data movement and compute will always be directly proportional. If we are adding two $N$x$N$ matrices, the amount of data involved is $O(N^2)$, and the amount of compute required is also $O(N^2)$. So the ratio of compute to data is $\frac{O(N^2)}{O(N^2)}=O(1)$, which means matrix addition will probably be memory bound on any modern device, regardless of how clever an algorithm we write. Relative to the amount of data movement, there just isn’t that much math required, so the upper bound on achievable arithmetic intensity is low. Lots of operations in deep learning fall into this low arithmetic intensity category, a technique called kernel fusion can be helpful here.</p>

<p>Matrix multiplication however is not doomed to be memory bound, because there is more arithmetic required relative to the problem size. When multiplying two $N$ by $N$ matrices, the amount of data involved is also $O(N^2)$, but the amount of compute required is $O(N^3)$ ($O(N)$ operations per output element, times $O(N^2)$ output elements). So the ratio of compute to data is $\frac{O(N^3)}{O(N^2)}=O(N)$. There is a factor of $N$ more compute required than data movement. The upper bound on the arithmetic intensity we can achieve grows with the matrix dimension $N$. If we are multiplying sufficiently large matrices, we should be able to write an algorithm that has sufficient arithmetic intensity to be compute bound rather than memory bound.</p>

<p>So in summary the arithmetic intensity we achieve depends on the kernel we write, and it must be less than or equal to an upper bound imposed by the algorithm our kernel is implementing. Achieved arithmetic intensity, given our machine parameters $\tau$ and $\beta$ determines whether we are memory bound or compute bound. If our algorithms upper bound on arithmetic intensity allows it, we want to optimize our kernel until it is compute bound rather than memory bound.</p>

<h2 id="achievable-arithmetic-intensity-on-a-simple-computer">Achievable arithmetic intensity on a simple computer</h2>
<p>For multiplying two $N$ by $N$ matrices, the best possible arithmetic intensity we can achieve is $O(N)$. Now the question is, how do we think about all of this when it comes time to actually write a kernel? To get at this question we need a model of the computer we are running on, to start out we will use the simple computer with fast and slow memory.</p>

<h3 id="worst-case">worst case</h3>
<p>The first implementation of multiplication between two N x N matrices ($C=A*B$) on the simple computer looks like this. We load each value as soon as we need it, and store each output as soon as we are done with it. What is the ratio of compute to data movement? Is it close to the ideal of $O(N)$?</p>
<div><pre><code>allocate registers a,b,c in fast memory
for i=1...N:
    for j=1...N:
        c = 0
        for k=1...N:
            load A(i,k) into a
            load B(k,j) into b
            c += a * b
        store c into C(i,j) 

</code></pre></div>
<p>My mental model of this implementation looks something like this
<img src="https://alexarmbr.github.io/images/simple_computer_matmul_naive.png" alt="simple_computer_matmul_naive"></p>

<p>This arithmetic intensity of this implementation on the simple computer is $O(1)$, because on each iteration of the inner loop a single multiply/accumulate is performed, and only the data operated on during that iteration is loaded. There is $O(N^3)$ data movement, and $O(N^3)$ compute, which means $\frac{O(N^3)}{O(N^3)}=O(1)$ intensity, which is worse than the ideal by a factor of $O(N)$. This turns out to be the worst case.</p>

<h3 id="best-case">best case</h3>
<p>The poor intensity of the above implementation is the result of the fact that we load single elements from fast memory one at a time, only when they are needed. Only three matrix elements at a time are stored in fast memory. We can improve intensity by making better use of fast memory. To illustrate the best case scenario, imagine that fast memory was large enough to fit $A,B$ and $C$ in their entirety. If this were the case we could allocate space in fast memory for $C$, transfer the entire $A$ and $B$ upfront, perform the three nested loops with all the data already present in fast memory, and then once we are done store the entire $C$ matrix all at once back to slow memory.
<img src="https://alexarmbr.github.io/images/simple_computer_matmul_best_case.png" alt="simple_computer_matmul_best_case">
In this case, because we move each matrix only once, data movement is $O(N^2)$. Compute is the same as above, $O(N^3)$. Looking at the ratio of the two, we achieve the best case intensity, $\frac{O(N^3)}{O(N^2)}=O(N)$. However, this is unrealistic, because the entire problem will generally not fit in fast memory.</p>

<h3 id="realistic-case">realistic case</h3>
<p>We want to move more than three elements at a time between slow memory and fast memory. But we can’t move the full matrices all at once. We can compromise by moving subtiles of $A$ and $B$ from slow memory to fast memory (as large as we can fit). Each pair of input tiles we move to fast memory corresponds to a tile of the output which can be computed with a mini matrix multiplication between the input tiles we have resident in fast memory. We then move the next pair of input tiles to fast memory and then compute again.</p>

<p><img src="https://alexarmbr.github.io/images/simple_computer_matmul_realistic_case.png" alt="simple_computer_matmul_realistic_case"></p>

<p>Here is some pseudocode corresponding to the above diagram:</p>
<div><pre><code>Allocate A_tile[BN, BN], B_tile[BN,BN], C_tile[BN,BN] in fast memory

# outer loop over tiles of A and B
for i=1...N in steps of size BN:
    for j=1...N in steps of size BN:
        C_tile[: , :] = 0
        for k=1...N in steps of size BN:
            Load A[i : i+BN, k : k+BN] into A_tile
            Load B[k : k+BN, j : j+BN] into B_tile
            
            # inner loop, do a mini matmul between tiles of A and B
            # store the result in C_tile
            for tile_i=1...BN:
                for tile_j=1...BN:
                    for tile_k=1...BN:
                        C_tile[tile_i, tile_j] +=
                            A_tile[tile_i, tile_k] * B_tile[tile_k, tile_j]
            
        # once we have looped over all the tiles along the K dimension of A,B
        # store C_tile back to its place in slow memory
        Store C_tile into C[i : i + BN, j : j+BN]

</code></pre></div>
<p>What is the ratio of compute to data movement? How does it compare to the worst cast and the best case? We can answer these questions by looking at the loop structure.</p>

<p>Lets think about data movement first. There are three nested loops on the outside, each of which go from $1$ to $N$ in $BN$ sized steps. Each loop iterates $\frac{N}{BN}$ times, and since we have three levels of nesting, whatever is inside the nested loop body will happen $(\frac{N}{BN})^3$ times. Inside the loop nest, we load two tiles of size $BN^2$, one corresponding to each of the input matrices. Asymptotically this works out to $O((\frac{N}{BN})^3 * BN^2)$ data movement (we can ignore the storing of the <code>C_tile</code>, since this is only inside two of the loop nests, it only happens $\frac{N}{BN}^2$ times). Cancelling things out gives us $O(\frac{N^3}{BN})$ data movement. Notice that this a factor of $BN$ less data movement than the naive case.</p>

<p>Now compute. Same as above, we have three nested loops, the inside of this loop body will execute $(\frac{N}{BN})^3$ times. Inside the loop nests, the compute consists of the mini matmul between two $BN$ by $BN$ tiles, the three nested loops have a total of $O(BN^3)$ steps what is what we expect for multiplying together two $BN$ by $BN$ matrices. So the total amount of compute is $O((\frac{N}{BN})^3 * BN^3)$ which simplifies to just $O(N^3)$. This is the number of steps we expect for multiplying two $N$ by $N$ matrices, and it is the same as the naive case.</p>

<p>So this tiled approach has the same number of compute steps as the naive implementation, but a factor of $O(BN)$ less data movement. The arithmetic intensity works out to $O(\frac{N^3}{\frac{N^3}{BN}})=O(BN)$. In english, this is telling us that our achieved arithmetic intensity will scale linearly with the dimension of the tiles that we are fitting in fast memory.</p>

<h3 id="in-summary">In Summary</h3>

<p>The final takeaway is fairly intuitive. The best possible intensity we can achieve when multiplying two $N$ by $N$ matrices scales with the matrix dimension $N$. However, achieving this upper bound would require fitting the entire $O(N^2)$ sized problem in fast memory, which wont be possible. So we compromise by breaking down the $O(N^2)$ sized problem into lots of smaller $O(BN^2)$ sized problems, and we choose $BN$ such that all of our fast memory is filled up. The intensity we can then achieve scales with $BN$. So in practice, the intensity we can achieve is limited by the size of fast memory on our device.</p>

<h2 id="parallelized-matrix-multiplication-on-a-gpu">Parallelized matrix multiplication on a GPU</h2>
<p>Thinking about matrix multiplication on the simple computer helps build intuition about how using the memory hierarchy to our advantage can result in higher arithmetic intensity, which will help for maximizing the performance of our kernel. However the simple computer model is a bit too simple, it consists of a two level memory hierarchy and some compute that can operate at a rate of $\tau$ on the data in fast memory. Our goal is to write a fast matrix multiplication kernel that will run on a GPU, which raises the question of how a GPU is different from the simple computer.</p>

<p>On the most fundamental level that answer is that GPUs, like the simple computer, have a memory hierarchy. But on a GPU the memory hierarchy fits within a hierarchy of concurrent compute units. Here is a diagram of a simple GPU that illustrates this.</p>

<p><img src="https://alexarmbr.github.io/images/simple_gpu.png" alt="simple_gpu"></p>

<p>On the simple GPU there are three levels to the combined compute/memory hierarchy.</p>
<ul>
  <li>At the highest level is the whole GPU, which owns a big piece of DRAM (global memory). The GPU is composed of four multiprocessors, each of which are independent units of a compute, run concurrently with respect to each other and can all read/write to the same DRAM.</li>
  <li>At the middle level there is a multiprocessor which owns a piece of SRAM (shared memory), and is composed of four cores which are independent units of compute that run concurrently and can all read and write the same shared memory that is local to the multiprocessor.</li>
  <li>At the lowest level is a single compute core which owns some private register memory, and can execute a single thread and perform arithmetic independently of the rest of the computer.</li>
</ul>

<h3 id="hierarchical-tiling-simple-gpu">Hierarchical Tiling (simple gpu)</h3>
<p>So how do we use this type of computer to perform a matrix multiplication? The first useful observation is that the matrix multiplication problem can be broken down hierarchically into nested tiles. This is good news, because a hierarchical algorithm is a good fit for a hierarchical computer.</p>

<p><img src="https://alexarmbr.github.io/images/matmul_hierarchies.png" alt="matmul_hierarchies"></p>

<p>If we are computing a matrix multiplication $C=A*B$, we can divide the output matrix $C$ into non-overlapping tiles, and assign each tile to a compute unit. Each of these output tiles can then be computed with a matrix multiplication between corresponding tiles of the input, independently of the other tiles. Since our machine is hierarchical, there are compute units within compute units, and correspondingly there are matrix multiplications within matrix multiplications. We recursively break down the problem into nested tiles, until we end up at an atomic element of compute which physically is usually a single core of some sort, and logically is a single thread of execution. At this level the single thread computes a small matrix multiplication between its tiles of the input.
<img src="https://alexarmbr.github.io/images/hierarchy_combined.png" alt="hierarchy_combined"></p>

<h3 id="hierarchical-tiling-real-gpu">Hierarchical Tiling (real gpu)</h3>
<p>The above diagram shows a coarse, high level view of what a GPU implementation of hierarchical tiling looks like. When implementing this in CUDA for an NVIDIA GPU, there are some finer details we need to fill in. This tiling structure is created by:</p>
<ul>
  <li>a series of global, shared, and register memory allocations of fixed dimension</li>
  <li>nested loops which control the positions of the tiles</li>
  <li>synchronization points between threads running within a multiprocessor</li>
  <li>compute at the lowest level, which in this case is a small matrix multiplication that runs on the tensor core</li>
</ul>

<p>This kernel was my starting point, but if you are interested in reading about a series of 10 kernels which build up to one like this, I recommend reading <a href="https://siboehm.com/articles/22/CUDA-MMM">this</a>.</p>

<p><img src="https://alexarmbr.github.io/images/my_tiles_2.png" alt="tiling"></p>

<p>With this diagram my attempt is to show the correspondence between loop nests and the tiling structure. There are four levels, each level corresponds to a level of the compute hierarchy, memory hierarchy, and tile shape.</p>

<p>Here is a quick description of each level from the perspective of the compute unit relevant for that level:</p>

<ul>
  <li>
    <p><strong>CUDA Kernel / GPU level</strong>: The GPU is reading the three input matrices, $A$, $B$, and $C$ from <strong>global memory</strong>, and writing the output matrix $D$ to global memory. Each thread block is looping over the <code>K</code> dimension (aka the ‘inner’ dimension) of $A$ and $B$. This loop is incrementing <code>block_k</code> in steps of size <code>BK</code>. At each iteration we are copying the blue blocktiles from global memory to shared memory.</p>
  </li>
  <li>
    <p><strong>Thread Block / SM level</strong>: At this point the blue subtiles of $A$ and $B$ that a particular thread block needs to compute a <code>BM,BN</code> tile of the output have been copied into <strong>shared memory</strong>. This thread block is running on one of the 16 SMs on the GPU, and the shared memory is local to that SM and fast to access. Within the thread block there are 256 threads, which is 8 warps containing 32 threads each. Within the thread block, the <code>BM,BN</code> tile of the output is partitioned 8 ways, so that each of the 8 warps can work concurrently on the compute. Each of the warps is looping over the inner dimension within the block tile, this loop is incrementing <code>warp_k</code> in steps of size <code>WK</code>. At each iteration we are copying the green warp tiles from shared memory to register memory.</p>
  </li>
  <li>
    <p><strong>Warp / SM Partition</strong>: At this point the green warp tiles within the blue block tiles have been copied into <strong>register memory</strong>, and it is the responsibility of a particular warp, running on one of the 4 partitions on the <a href="https://images.app.goo.gl/Z2VVQQgXWTMddBraA">Turing SM</a> to compute the <code>WM</code> by <code>WN</code> tile of the output. Each warp computes its tile of the output by taking an outer product between the <code>WM,WK</code> tile of A and the <code>WK,WN</code> tile of B. Inside the three nested loops that compute the outer product, the we an MMA sync operation.</p>
  </li>
  <li>
    <p><strong>Tensor Core Op</strong>: Finally we get down to the last level of the hierarchy, which is a single tensor core op, this is a single hardware accelerated (16,8) x (8,8) = (16,8) matrix multiply that takes place in and out of <strong>register memory</strong>.</p>
  </li>
</ul>

<h3 id="performance-considerations-on-a-real-gpu">Performance considerations on a real GPU</h3>
<p>When implementing this structure in a CUDA kernel that targets a particular GPU architecture, there are a number of things that must be considered given that we are trying to squeeze every last drop of performance out of the hardware. I divide the performance considerations into three buckets, each optimization discussed in the rest of this article falls into one or two of these buckets.</p>

<h4 id="arithmetic-intensity-as-a-function-of-tile-dimensions">Arithmetic intensity as a function of tile dimensions</h4>
<p>The necessity of achieving high arithmetic intensity is why we have this structure of tiles within tiles, and the tile dimension is the primary knob we can turn that determines the arithmetic intensity of our kernel. In our kernel we are first loading data from global memory to shared memory, and then shared memory into registers. In both cases we are loading two rectangular tiles corresponding to the input data from slower memory to faster memory, and then eventually computing a matrix multiplication between these two inputs at the lowest level of the hierarchy. The arithmetic intensity we should achieve is a function of the tile dimensions we choose (larger is better), this is worked out below.</p>

<p><img src="https://alexarmbr.github.io/images/intensity_tile_dims.png" alt="intensity_tile_dims"></p>

<ul>
  <li>
<strong>FLOPs</strong>: At each iteration of the inner loop, each thread block multiplies a $(BM,BK)$ shaped matrix with a $(BK,BN)$, to produce a $(BM,BN)$ tile of the output. This matrix product consists of $2 * BM * BK * BN $ FLOPs (three nested loops over the dimensions, with a multiply and accumulate operation in the inner loop)</li>
  <li>
<strong>memory</strong>: The $(BM,BK)$ and $(BK,BN)$ shaped matrices are read from global memory each iteration, since each element is two bytes this comes out to a total of $2(BM * BK + BK * BN) = 2BK(BM + BN)$ bytes read, and we don’t perform any writes in the inner loop, all writes happen in the kernel epilogue.</li>
</ul>

<p>Taking the ratio of these two, the arithmetic intensity we should achieve for a given block tile size works out nicely to $\frac{BM*BN}{BM+BN} \frac{FLOP}{byte}$. For the thread block level tiles at the second level of the hierarchy, we will want to choose our tile dimensions such that this ratio is larger than the balance point of the tensor cores with respect to global memory, but we will be limited by the size of shared memory. Likewise for the warp tiles at the next level down in the hierarchy, we will want to choose the tile dimensions such that this ratio is larger than the balance point of the tensor cores with respect to shared memory, but we will be limited by the size of register memory. The former turns out to be a bit more challenging than the later.</p>

<h4 id="overlap-between-compute-and-data-movement">Overlap between compute and data movement</h4>
<p>The roofline model gives us an upper bound on arithmetic throughput $T_{max}=min(\beta * I, \tau)$. In order to achieve this upper bound, we need perfect overlap between compute and data movement. In order to see why this is, imagine we achieve an arithmetic intensity sufficient to put us in the compute bound regime of the roofline model. At this point in order for our achieved throughput to actually equal the upper bound $T_{max}=\tau$, we need to be continuously computing, any time that our compute spends idle will mean that our achieved throughput is less than the machine peak $\tau$. There are a number of reasons why our compute will spend periods of time idle, such as memory latency, data dependencies, and synchronization points.
<img src="https://alexarmbr.github.io/images/compute_data_movement_overlap.png" alt="compute_data_movement_overlap">
As illustrated above our initial loop structure has some inefficiencies in this regard.</p>

<h4 id="maximizing-memory-bandwidth">Maximizing memory bandwidth</h4>
<p>According to <a href="https://arxiv.org/pdf/1903.07486">unofficial benchmarks</a> the best achievable global memory bandwidth on the T4 is ~220 GB/sec, and the best achievable shared memory bandwidth is ~3662 GB/sec. However, an unoptimized kernel will only achieve a fraction of these numbers. The first consideration is access pattern; when groups of adjacent threads are requesting memory, some mappings of threads to data in memory are more efficient than others. The hardware that implements global memory vs. shared memory functions differently, consequently an access pattern that is optimal for reading shared memory may not be optimal for reading global memory.</p>

<p>The main consideration for global memory access is called coalescing, the one sentence summary is that maximum global memory bandwidth is achieved when adjacent threads access adjacent data in global memory (explained <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses">here</a>). Shared memory is dove into in a <a href="#background-bank-conflicts-and-wavefronts">later</a> chapter.</p>

<h3 id="how-to-use-tensor-cores">How to use Tensor Cores</h3>
<p>This section is a brief overview of the mechanics of using tensor cores.</p>

<p>All tensor core operations are performed at the warp level in the compute hierarchy; 32 threads collaboratively load data into their registers and then synchronously execute a small hardware accelerated matrix multiply. When thinking about tensor core algorithms, we should think of the warp as an atomic element of compute, even though in reality a warp contains 32 threads capable of doing their own thing. By comparison if we were writing GEMM kernel without tensor cores, individual threads performing scalar multiply accumulate operations would be our atomic element of compute.</p>

<p>Tensor cores are accessible via two different methods. The first is via the <code>wmma</code> <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#wmma-description">api</a> which is part of the CUDA toolkit. <code>wmma</code> seems to be regarded as the more portable and less performant way to program tensor cores. I gave up on it pretty quickly, as it abstracts away the loading of input data from shared memory into register memory, and it turns out there are some details here which are critical for performance.</p>

<p>The other route is to use the <code>mma</code> family of instructions which are part of PTX, this option is more flexible and performant than the <code>wmma</code> route. PTX is an intermediate representation for NVIDIA GPUs that is lower level than CUDA, but higher level than SASS (this is the assembly language that NVIDIA GPUs run). PTX can be inlined in a kernel in order to call tensor cores.</p>

<p>The PTX instruction I used is <code>mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16</code> (documentation <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#matrix-fragments-for-mma-m16n8k8">here</a>), each part of this instruction means something:</p>
<ul>
  <li>
<code>mma</code>: we are performing a matrix multiply accumulate operation</li>
  <li>
<code>sync</code>: this instruction is synchronous, all 32 threads will wait until all 32 threads are done before resuming execution</li>
  <li>
<code>aligned</code>: all 32 threads in a warp must execute this instruction, if less than 32 threads in a warp were to execute this instruction, behavior is undefined</li>
  <li>
<code>m16n8k8</code>: this is the identifier for the matrix fragment shape. This means the fragment of matrix 
$A$ has shape (16,8), the fragment of $B$ has shape (8,8), the fragments of $D$ and $C$ have shape (8,8). (Remember, the formula for a GEMM is $D = \alpha * A * B + \beta * C$). If you look at the PTX documentation linked above, there are lots of different shapes to choose from, however the Turing/Volta architectures only support a limited number. Ampere supports more, and Hopper supports even more.</li>
  <li>
<code>row</code>: the $A$ fragment should be stored in registers in a row-major layout</li>
  <li>
<code>col</code>: the $B$ fragment should be stored in register in a column-major layout</li>
  <li>
<code>f16</code>: $D$ is an fp16 matrix</li>
  <li>
<code>f16</code>: $A$ is an fp16 matrix</li>
  <li>
<code>f16</code>:&nbsp;$B$ is an fp16 matrix</li>
  <li>
<code>f16</code>: $C$ is an fp16 matrix</li>
</ul>

<p>Each <code>mma.sync</code> instruction expects a specific layout of fragment elements across the registers of the 32 threads in a warp, these layouts can be found in the PTX docs. Here is the <code>m16n8k8</code> layout:
<img src="https://alexarmbr.github.io/images/mma_fragments.png" alt="matrix_fragments"></p>

<p>These diagrams are describing a mapping between threads, registers, and matrix elements:</p>
<ul>
  <li>
<code>T0, T1, T2 ...</code> refers to the index of the thread. Thread indices in these diagrams range from 0-31 since there are 32 threads in a warp.</li>
  <li>
<code>a0, a1, a2, ... b0, b1, b2, ... c0, c1, c2</code> refer to registers that hold matrix elements.</li>
  <li>The position of each thread/register pair tells us which matrix elements go in which registers of which thread. For example, <code>T0: {a0,a1}</code> is at the top left corner of matrix fragment A, this means elements <code>(0,0)</code> and <code>(0,1)</code> in this fragment are placed in registers <code>a0</code> and <code>a1</code> of thread 0.</li>
</ul>

<p>Luckily there is another PTX instruction called <code>ldmatrix</code> (docs <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-load-instruction-ldmatrix">here</a>) which loads a rectangular tile of data from shared memory, and shuffle matrix elements within a warp in order to create this layout for us. It can optionally transpose matrix elements as it moves them from shared memory to register, which is convenient for matrix B above, which is in a column major, or “transposed” layout.</p>

<p>The inner loop our our kernels will consist of repeatedly calling <code>ldmatrix</code> to move data from shared memory into register memory, and then repeatedly calling the <code>m16n8k8</code> variation of <code>mma.sync</code> in order to multiply tiles together with the tensor core. For this project I used a Turing architecture GPU, on Ampere the tensor core API is very similar, but with more matrix shapes supported. On Hopper, the API is expanded substantially, PTX instructions are introduced that allow a group of 128 threads to asynchronously execute a much larger matrix multiplication than <code>m16n8k8</code>.</p>

<h2 id="kernels">Kernels</h2>

<p>For the rest of this article I will discuss a series of kernels that got me to ~96% of cuBLAS level performance on a tensor core GEMM, for 8192x8192 matrices. Each kernel builds on the previous one, and the themes of each are:</p>
<ol>
  <li><a href="#kernel-1---hierarchical-tiling">hierarchical tiling</a></li>
  <li><a href="#kernel-2---vectorized-memory-copy-and-loop-unrolling">vectorized/unrolled gmem-&gt;smem transfer</a></li>
  <li><a href="#swizzling">shared memory swizzling</a></li>
  <li><a href="#kernel-4---makeshift-async-copy">makeshift async copy</a></li>
  <li><a href="#tune-tile-dimensions">tune tile dimensions</a></li>
  <li><a href="#kernel-5---optimize-index-calculation">optimized index calculation</a></li>
  <li><a href="#kernel-6---double-buffering">double buffering</a></li>
</ol>

<h2 id="kernel-1---hierarchical-tiling">Kernel 1 - Hierarchical Tiling</h2>
<p>The first kernel I wrote is an implementation of the hierarchical tiling structure shown <a href="#hierarchical-tiling-real-gpu">above</a>. Here is pseudocode for the loop structure that performs the matrix multiplication.</p>

<div><pre><code><span>// outer loop over block tiles</span>
<span>for</span> <span>(</span><span>block_k</span> <span>=</span> <span>0</span><span>;</span> <span>block_k</span> <span>&lt;</span> <span>K</span><span>;</span> <span>block_k</span> <span>+=</span> <span>BK</span><span>)</span>
<span>{</span>
    <span>// global memory to shared memory transfer</span>
    <span>A_smem</span><span>[</span><span>:</span><span>,</span><span>:</span><span>]</span> <span>=</span> <span>A_gmem</span><span>[</span><span>block_m</span><span>:</span><span>block_m</span><span>+</span><span>BM</span><span>,</span> <span>block_k</span><span>:</span><span>block_k</span><span>+</span><span>BK</span><span>]</span>
    <span>B_smem</span><span>[</span><span>:</span><span>,</span><span>:</span><span>]</span> <span>=</span> <span>B_gmem</span><span>[</span><span>block_k</span><span>:</span><span>block_k</span><span>+</span><span>BK</span><span>,</span> <span>block_n</span><span>:</span><span>block_n</span><span>+</span><span>BN</span><span>]</span>
    
    <span>// synchronize across the thread block in between</span>
    <span>// writing shared memory and reading shared memory</span>
    <span>__syncthreads</span><span>();</span>

    <span>for</span> <span>(</span><span>warp_k</span> <span>=</span> <span>0</span><span>;</span> <span>warp_k</span> <span>&lt;</span> <span>BK</span><span>;</span> <span>warp_k</span> <span>+=</span> <span>WK</span><span>)</span>
    <span>{</span>
        <span>// load from shared memory into register memory in preparation for compute phase</span>
        <span>A_reg</span><span>[</span><span>:</span> <span>,</span><span>:</span><span>]</span> <span>=</span> <span>A_smem</span><span>[</span><span>warp_m</span><span>:</span><span>warp_m</span><span>+</span><span>WM</span><span>,</span> <span>warp_k</span><span>:</span><span>warp_k</span><span>+</span><span>WK</span><span>]</span>
        <span>B_reg</span><span>[</span><span>:</span><span>,</span> <span>:</span><span>]</span> <span>=</span> <span>B_smem</span><span>[</span><span>warp_k</span><span>:</span><span>warp_k</span><span>+</span><span>WK</span><span>,</span> <span>warp_n</span><span>:</span><span>warp_n</span><span>+</span><span>WN</span><span>]</span>

        <span>// outer product over mma tiles</span>
        <span>for</span> <span>(</span><span>mma_k</span> <span>=</span> <span>0</span><span>;</span> <span>mma_k</span> <span>&lt;</span> <span>WK</span><span>;</span> <span>mma_k</span> <span>+=</span> <span>MMA_K</span><span>)</span>
        <span>{</span>
            <span>for</span> <span>(</span><span>mma_m</span> <span>=</span> <span>0</span><span>;</span> <span>mma_m</span> <span>&lt;</span> <span>WM</span><span>;</span> <span>mma_m</span> <span>+=</span> <span>MMA_M</span><span>)</span>
            <span>{</span>
                <span>for</span> <span>(</span><span>mma_n</span> <span>=</span> <span>0</span><span>;</span> <span>mma_n</span> <span>&lt;</span> <span>WN</span><span>;</span> <span>mma_n</span> <span>+=</span> <span>MMA_N</span><span>)</span>
                <span>{</span>
                    <span>mma_sync_m16n8k8</span><span>(</span>
                        <span>acc_reg</span><span>[</span><span>mma_m</span><span>:</span><span>mma_m</span><span>+</span><span>MMA_M</span><span>,</span> <span>mma_n</span><span>:</span><span>mma_n</span><span>+</span><span>MMA_N</span><span>],</span>
                        <span>A_reg</span><span>[</span><span>mma_m</span><span>:</span><span>mma_m</span><span>+</span><span>MMA_M</span><span>,</span> <span>mma_k</span><span>:</span><span>mma_k</span><span>+</span><span>MMA_K</span><span>],</span>
                        <span>B_reg</span><span>[</span><span>mma_k</span><span>:</span><span>mma_k</span><span>+</span><span>MMA_K</span><span>,</span> <span>mma_n</span><span>:</span><span>mma_n</span><span>+</span><span>MMA_N</span><span>],</span>
                        <span>acc_reg</span><span>[</span><span>mma_m</span><span>:</span><span>mma_m</span><span>+</span><span>MMA_M</span><span>,</span> <span>mma_n</span><span>:</span><span>mma_n</span><span>+</span><span>MMA_N</span><span>]</span>
                    <span>)</span>

                <span>}</span>
            <span>}</span>
        <span>}</span>
    <span>}</span>
    <span>__syncthreads</span><span>();</span>

<span>}</span>
</code></pre></div>
<p>The 8% of cublas throughput it achieves is the starting point. The rest of this article delves into some techniques I used to make it faster.</p>

<p><img src="https://alexarmbr.github.io/images/table1.png" alt="table1"></p>

<h2 id="kernel-2---vectorized-memory-copy-and-loop-unrolling">Kernel 2 - Vectorized memory copy and loop unrolling</h2>
<p>In order to improve the performance of our code, we need to know why it is slow. When writing CUDA kernels, the best tool to use for this is called NSight Compute, a profiler developed by NVIDIA that gives lots of detailed metrics about what is happening in hardware while a kernel executes. The first place I typically look is the section called “Warp State Statistics”. As a kernel is executing, each warp is being issued instructions by a scheduler. In an ideal world, the scheduler would be able to issue a new instruction each clock cycle. In the real world, it is very hard to write a kernel that can issue a new instruction every cycle, there are all sorts of reasons why on a given cycle, a warp may not be capable of executing its next instruction and will instead “stall” i.e. do nothing. The reasons for stalling can be due to capacity limits of various hardware pipelines, memory latency, or synchronization points in our kernel which require all the threads running on an SM to wait for all the other threads to catch up. The Warp State Statistics section tells us how many clock cycles the average warp spends stalled, per average instruction issued, broken down across a bunch of different categories. This gives us the information we need to target our optimizations to the least performant parts of our kernel. Here is a screenshot of what the Warp State section for Kernel 1.
<img src="https://alexarmbr.github.io/images/warp_state_kernel1.png" alt="warp_state_kernel1">
The “Warp Cycles Per Issued Instruction” field tells us that on average for each instruction issued, warps spend about ~30 cycles idle, and the table below tells us that 16 of these 30 cycles are due to the “Long Scoreboard” stall category.</p>

<p><a href="https://en.wikipedia.org/wiki/Scoreboarding">Scoreboarding</a> is an algorithm implemented in the hardware of most processors for tracking when the data dependencies for the next instruction have arrived in the registers they need to be in for the instruction to execute. Most modern CPUs are able to reorder instructions on the fly such that instructions whose operands are ready can execute ahead of instructions whose operands have yet to arrive in registers. The reordering is done in hardware, subject to constraints imposed by the data dependencies between subsequent instructions. This is called <a href="https://en.wikipedia.org/wiki/Out-of-order_execution">out of order execution</a> and it is a rather fancy technique for hiding latency. GPUs do not reorder instructions as they are executing, I would imagine because the logic required consumes a fair amount of precious transistors on the chip, and since GPUs are designed for <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#the-benefits-of-using-gpus">throughput</a> these transistors are better spent on things like tensor cores.</p>

<p>GPUs do however track data dependencies, but with a lot more help from the compiler as compared to CPUs. When the data required to execute the next instruction has not arrived in register memory, the warp that is executing just waits for its data to arrive. The “Long Scoreboard Stall” approximates the average number of cycles that warps spend stalled waiting for data dependencies. The fact that this stall reason accounts for ~50% of all the cycles that warps spend idle tells us that the performance of Kernel 1 is primarily limited by memory latency. This tells us we should focus on the code that is moving data from global memory onto the chip, and figure out how to minimize the latency per byte moved.</p>

<p>Reading a rectangular tile of data from global memory, and writing it to shared memory is the first thing that occurs on each iteration of the outer loop of the kernel. The easiest way to do this is for adjacent threads to access adjacent values in global memory, and write data to shared memory in the same layout that it came from in global memory. This access pattern is optimal both for reading global memory, and writing shared memory. Here is the first data transfer that I wrote:</p>

<div><pre><code><span>__device__</span> <span>void</span> <span>tileMemcpy</span><span>(</span>
    <span>half</span><span>*</span> <span>src</span><span>,</span>
    <span>half</span><span>*</span> <span>dst</span><span>,</span>
    <span>const</span> <span>unsigned</span> <span>int</span> <span>src_stride</span><span>,</span>
    <span>const</span> <span>unsigned</span> <span>int</span> <span>tile_rows</span><span>,</span>
    <span>const</span> <span>unsigned</span> <span>int</span> <span>tile_cols</span>
<span>)</span>
<span>{</span>
    <span>// flatten out 2d grid of threads into in order of increasing threadIdx.x</span>
    <span>const</span> <span>unsigned</span> <span>int</span> <span>thread_idx</span> <span>=</span> <span>threadIdx</span><span>.</span><span>y</span> <span>*</span> <span>blockDim</span><span>.</span><span>x</span> <span>+</span> <span>threadIdx</span><span>.</span><span>x</span><span>;</span>
    <span>const</span> <span>unsigned</span> <span>int</span> <span>num_threads</span> <span>=</span> <span>blockDim</span><span>.</span><span>x</span> <span>*</span> <span>blockDim</span><span>.</span><span>y</span><span>;</span>
    
    <span>// # of threads is multiple of # of columns in the tile</span>
    <span>assert</span><span>(</span><span>num_threads</span> <span>%</span> <span>tile_cols</span> <span>==</span> <span>0</span><span>);</span>
    
    <span>// assign each thread a row/column in the tile, calculate the row step</span>
    <span>const</span> <span>unsigned</span> <span>int</span> <span>row_step</span> <span>=</span> <span>num_threads</span> <span>/</span> <span>tile_cols</span><span>;</span>
    <span>const</span> <span>unsigned</span> <span>int</span> <span>thread_row</span> <span>=</span> <span>thread_idx</span> <span>/</span> <span>tile_cols</span><span>;</span>
    <span>const</span> <span>unsigned</span> <span>int</span> <span>thread_col</span> <span>=</span> <span>thread_idx</span> <span>%</span> <span>tile_cols</span><span>;</span>
    
    <span>for</span> <span>(</span><span>unsigned</span> <span>int</span> <span>r</span> <span>=</span> <span>thread_row</span><span>;</span> <span>r</span> <span>&lt;</span> <span>tile_rows</span><span>;</span> <span>r</span><span>+=</span><span>row_step</span><span>)</span>
    <span>{</span>
        <span>dst</span><span>[</span><span>r</span> <span>*</span> <span>tile_cols</span> <span>+</span> <span>thread_col</span><span>]</span> <span>=</span>  <span>src</span><span>[</span><span>r</span> <span>*</span> <span>src_stride</span> <span>+</span> <span>thread_col</span><span>];</span>
    <span>}</span>
<span>}</span>
</code></pre></div>
<p>Looking at the SASS corresponding to this <code>tileMemcpy</code> function in <a href="https://godbolt.org/z/1MeavE3GG">godbolt</a>, we can see that the copy operation inside the loop <code>dst[...] = src[...]</code> compiles to two operations from the lower level perspective of SASS, a two byte load from global memory (<code>LDG.U16</code> in SASS), followed by a two byte store (<code>STS.U16</code>), along with a bunch of index calculations and loop overhead. The long scoreboard stall prevents the store from taking place until the value we are loading has arrived in the register.</p>

<p>Here is a visualization of how this loop is executing, for a single thread:
<img src="https://alexarmbr.github.io/images/memory_latency.png" alt="memory_latency">
Latency in between the load and the store is inevitable: a request is sent to a DRAM controller, data is fetched from DRAM and then transmitted over bus. Unless we hack the laws of physics or invent a time machine we can’t get rid of the latency. But what we can do is hide it.</p>

<p>Latency hiding is a central concept in computing, and at its core is very simple. It just means that if we are performing an operation $X$ that has some latency, we want to be doing other useful work while $X$ is happening, rather than wait and do nothing. For example, if I wake up and decide I want an omlette, I would first turn on the burner and let the pan warm up, and while that is happening I would crack the eggs and grate cheese. This order of operations hides the latency of warming up the pan with the cracking of eggs and grating of cheese. If I am hungry and eager to eat the finished omlette as soon as possible, it would be silly to idly stand there and watch as the pan warms up.</p>

<p>The same principle applies to hiding the latency of the global memory loads in <code>tileMemcpy</code>. Since the copy operation is happening inside a loop, each thread is performing multiple loads and multiple stores, in an order like <code>load (stall) store, load (stall) store, ...</code>. What if we were able to rearrange these so that the order is <code>load load load (stall) store, store, store</code>. In this later ordering the data requested by the three loads will be in flight at the same time, and we can say that the latency of each load is being hidden by the other loads. The easiest way to accomplish the later ordering is by unrolling the loop in <code>tileMemcpy</code>. If we can unroll the loop, <code>nvcc</code> should be smart enough to reorder the instructions so that the global memory loads are hiding each others latency. In this case the compiler is doing for us what a CPU would do in hardware on the fly.</p>

<p>If we want to unroll the loop, the number of loop iterations must be known at compile time. The number of loop iterations is a function of the number of threads per block, and the block tile dimensions. Both of these are fixed at compile time, so passing them as template parameters into <code>tileMemcpy</code> and calculating the number of iterations as a function of these, and adding a <code>#pragma unroll</code> does the trick.</p>

<div><pre><code><span>template</span><span>&lt;</span><span>unsigned</span> <span>int</span> <span>TILE_ROWS</span><span>,</span>
<span>unsigned</span> <span>int</span> <span>TILE_COLS</span><span>,</span>
<span>unsigned</span> <span>int</span> <span>NUM_THREADS</span><span>&gt;</span>
<span>__device__</span> <span>__forceinline__</span> <span>void</span> <span>tileMemcpyUnrolled</span><span>(</span>
    <span>half</span><span>*</span> <span>src</span><span>,</span>
    <span>half</span><span>*</span> <span>dst</span><span>,</span>
    <span>const</span> <span>unsigned</span> <span>int</span> <span>src_stride</span>
<span>)</span>
<span>{</span>
    <span>// # of threads is multiple of # of columns in the tile</span>
    <span>static_assert</span><span>(</span><span>NUM_THREADS</span> <span>%</span> <span>TILE_COLS</span> <span>==</span> <span>0</span><span>);</span>
    
    <span>// flatten out 2d grid of threads into in order of increasing threadIdx.x</span>
    <span>const</span> <span>unsigned</span> <span>int</span> <span>thread_idx</span> <span>=</span> <span>threadIdx</span><span>.</span><span>y</span> <span>*</span> <span>blockDim</span><span>.</span><span>x</span> <span>+</span> <span>threadIdx</span><span>.</span><span>x</span><span>;</span>

    <span>// assign each thread a row/column in the tile, calculate how many iterations we need</span>
    <span>// to cover the whole tile</span>
    <span>constexpr</span> <span>unsigned</span> <span>int</span> <span>ROW_STEP</span> <span>=</span> <span>NUM_THREADS</span> <span>/</span> <span>TILE_COLS</span><span>;</span>
    <span>constexpr</span> <span>unsigned</span> <span>int</span> <span>NUM_ITERS</span> <span>=</span> <span>TILE_ROWS</span> <span>/</span> <span>ROW_STEP</span><span>;</span>
    <span>unsigned</span> <span>int</span> <span>thread_row</span> <span>=</span> <span>thread_idx</span> <span>/</span> <span>TILE_COLS</span><span>;</span>
    <span>const</span> <span>unsigned</span> <span>int</span> <span>thread_col</span> <span>=</span> <span>thread_idx</span> <span>%</span> <span>TILE_COLS</span><span>;</span>
    
    <span>#pragma unroll
</span>    <span>for</span> <span>(</span><span>unsigned</span> <span>int</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>NUM_ITERS</span><span>;</span> <span>i</span><span>++</span><span>)</span>
    <span>{</span>
        <span>dst</span><span>[</span><span>thread_row</span> <span>*</span> <span>TILE_COLS</span> <span>+</span> <span>thread_col</span><span>]</span> <span>=</span>  <span>src</span><span>[</span><span>thread_row</span> <span>*</span> <span>src_stride</span> <span>+</span> <span>thread_col</span><span>];</span>
        <span>thread_row</span> <span>+=</span> <span>ROW_STEP</span><span>;</span>
    <span>}</span>
    
<span>}</span>
</code></pre></div>
<p>This gives us something more along the lines of:
<img src="https://alexarmbr.github.io/images/memory_latency_unrolled.png" alt="memory_latency_unrolled">
In the initial version, the total latency of the copy operation is roughly proportional to the memory latency of the device, times the number of loop iterations. After unrolling the loop, the total latency compared to the first version should be reduced by a factor of the number of loads the compiler decides to overlap with eachother (ish).</p>

<p>The other fairly easy optimization we can make here is to increase the number of bytes being loaded per instruction. Our load operation is currently compiling to <code>LDG.U16</code>, each of these instructions loads 16 bits/2 bytes from DRAM. The widest load instruction in SASS is <code>LDG.128</code>, which loads 128 bits/16 bytes. Since our kernel is bound by memory latency and not memory bandwidth, if we use a wider load instruction will experience the same latency per memory request, but move more bytes per request. We are amortizing the latency over more bytes moved, which is a win for efficiency.</p>

<p><img src="https://alexarmbr.github.io/images/memory_latency_vectorized.png" alt="memory_latency_vectorized"></p>

<p>A quick and hacky way to accomplish this is by <code>reinterpret_cast</code>ing the <code>src</code> and <code>dst</code> pointers from <code>half</code> to <code>float4</code>, and updating the index and loop calculations accordingly. Here is a <a href="https://godbolt.org/z/v3T3x14ns">godbolt link</a> to a kernel with the vectorized and unrolled memory copy, and <a href="https://github.com/alexarmbr/matmul-playground/blob/main/src/device_utils.cuh#L73">here</a> is the code.</p>

<p>These optimizations to the memcpy increase the throughput over the first kernel by about 3x. But there is still a long way to go before we approach cuBLAS level performance
<img src="https://alexarmbr.github.io/images/table2.png" alt="table2"></p>


<p>Back to the warp state section of NSight Compute
<img src="https://alexarmbr.github.io/images/kernel2_nsight_compute.png" alt="kernel2_nsight_compute">
The long scoreboard stall is no longer the leading offender in terms of warp stalls, and our kernel got about 3x more performant after applying the optimizations described in the last section. Warps are now spending an average of ~19 cycles stalled per issued instruction due to something called “MIO Throttling.” What is MIO Throttling, and how do we address it? According to nsight compute <a href="https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html">docs</a> this means:</p>
<blockquote>
  <p>Warp was stalled waiting for the MIO (memory input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory instructions.</p>
</blockquote>

<p>In our case, this stalling is almost certainly due to shared memory instructions, since our kernel has very few dynamic branches, and no trigonometry or any other <a href="https://developer.nvidia.com/cuda-math-library">special math</a> instructions. Specifically, it is due to shared memory bank conflicts. According to <a href="https://forums.developer.nvidia.com/t/shared-memory-bank-conflicts-and-nsight-metric/115731/2?u=a14armbr">here</a> two symptoms of shared memory bank conflicts are very high L1/TEX thoughput number (currently at 97% of peak) and MIO Throttle stalls, these are both second order effects of shared memory bank conflicts. I learned at this point that if you have a kernel whose performance is being killed due to shared memory bank conflicts, this is not blatantly obvious when you look at NSight Compute, however the information is definetly there. I found that in order to see where shared memory bank conflicts were occuring, and understand their severity, I had to learn the terminology of a “wavefront”. In order to understand this term, a bit of background on shared memory is required.</p>

<h3 id="background-bank-conflicts-and-wavefronts">Background: Bank Conflicts and Wavefronts</h3>
<p>From the perspective of a CUDA program, shared memory works as follows (<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory-5-x">here</a> is the official guide). If you declare a <code>__shared__</code> array in your kernel, it corresponds to physical memory that is located on a specific streaming multiprocessor. Consequently this array is fast to access, but only accessible by threads on the SM, which in the language of CUDA means that shared memory arrays are local to a particular thread block. Physically the memory is spread between 32 “banks” with each bank storing an adjacent 4 bytes, like so:
<img src="https://alexarmbr.github.io/images/shmem_1.png" alt="shmem_1">
Each bank can produce a single 4 byte value per clock cycle. If our goal is to maximize our reading and writing bandwidth from shared memory, we need to keep this in mind when deciding on an access pattern. Full bandwidth is achieved when the 32 threads in a warp spread their access uniformly across the 32 banks. Bank “conflicts” occur when a single bank must produce data for more than one thread for a given request. In order to show how the ideas of bank conflicts and wavefronts tie together, here are 3 scenarios, all in a simplified world where we have 4 threads and 4 memory banks
<img src="https://alexarmbr.github.io/images/bank_conflicts_wavefronts.png" alt="bank_conflicts">
When loading or storing from shared memory, each thread requests a particular memory address that in our simplified world falls into one of the four memory banks. In scenario one, each thread is accessing data in a different bank, and the hardware calculates that these four accesses can be combined into a single transaction for the hardware to process, the word for this transaction is a <a href="https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#id26">wavefront</a>. In scenario two, the four threads access addresses that fall into two of the four banks. Since each bank is only capable of sending one word at a time, the hardware groups these four requests into two wavefronts, and the memory hardware processes the two wavefronts one after the other. Scenario three is the worst case scenario, the four threads access addresses that all fall to the 0th memory bank, and in this case the four seperate wavefronts are required to service the transactions from the four threads.</p>

<p>For four threads accessing four bytes, the “ideal” number of wavefronts is one, because (ideally) regardless of which threads are accessing which bytes, we should be able to arange our data such that all of our accesses are spread nicely accross the banks. For example scenario three as shown is less than ideal, but we could make it ideal by transposing the bytes in shared memory, this would result in the four accesses falling evenly accross the four banks. But for the layout as shown, the actual number of wavefronts is four.</p>

<p>NSight Compute will tell us per memory access:</p>
<ol>
  <li>the ideal number of wavefronts</li>
  <li>the actual number of wavefronts</li>
  <li>the number of wavefronts that are excessive, which is just 2 - 1</li>
</ol>

<p>According to the analysis above, if our code has an $n$ way bank conflict, $n$ should be equal to $\frac{actual\ wavefronts}{ideal\ wavefronts}$. We want the actual to equal the ideal, this often requires some careful thinking about how data is being laid out and how threads are accessing it.</p>

<h3 id="ldmatrix-bank-conflicts">ldmatrix bank conflicts</h3>
<p>Here is a screenshot of the per instruction actual/ideal wavefronts in NSight Compute:
<img src="https://alexarmbr.github.io/images/l1_wavefronts_source_view.png" alt="l1_wavefronts_source_view">
These <code>ldmatrix</code> commands are loading data from shared memory into thread local register memory in preparation for the MMA operations. NSight Compute tells us the ratio of actual to ideal is ~8ish, which suggests this memory access results in an 8-way bank conflict. In order to form a strategy for fixing this performance killer, we need to understand why it is happening.</p>

<p>In the tiling structure shown for Kernel 1, in each iteration of the warp loop (the green one), a single warp is responsible for reading a 64x64 tile of data from shared memory, and writing it to registers. The shared memory reads are where the bank conflicts occur. In the visualization below, on the top is a very zoomed out version of one of these 64x64 tiles, the layout across memory banks is visualized by the color of the columns. We can see that a row of 64 elements, which are 2 bytes each, nicely spans the 32 memory banks.
On the bottom is a zoomed in version of a single 8x8 tile that is brought from shared memory into registers by <code>ldmatrix</code>. Each warp is iterating over its own local 64x64 tile in 8x8 increments, calling <code>ldmatrix</code> on each little tile, this PTX instruction loads values from shared memory, and shuffles the loaded data among the registers in a warp to match the register layout that the tensor core instruction expects.
<img src="https://alexarmbr.github.io/images/mma_tile_zoom_in.png" alt="mma_tile_zoom_in">
The inner workings of <code>ldmatrix</code> are a bit opaque, it compiles to a single SASS instruction <code>LDSM...</code>, rather than multiple explicit shared memory loads and register shuffles, as one might expect. However, we dont need an understanding of <code>ldmatrix</code>s inner workings to see why the 8 way bank conflict is occuring each time we call it. Rather the 8-way bank conflict is an inevitable result of the fact that each row in a given tile is spread across the same four memory banks. One wavefront is required to read each row, and there are eight rows, which means eight wavefronts. Ideally, if the eight rows in each tile were spread evenly across the thirty two memory banks, the entire tile could be read with a single wavefront. Reading these tiles is in the inner loop of the kernel, for $8192$x$8192$ operands we read a total of $ (8192/8)^3=1,073,741,824$ of these tiles which works out to a ~shitload~ of bank conflicts. So if we care about performance, it is worth the time to fix it.</p>

<h3 id="padding">Padding</h3>
<p>In order to have a bank conflict free kernel, we need to rearrange the layout of data in shared memory such that we can read and write to shared memory without any excessive wavefronts. The challenge comes from the fact that the thread to data mapping for shared memory reads is different from that of shared memory writes. When writing, adjacent threads write adjacent values in a row, whereas when reading adjacent threads read adjacent values down a column.</p>

<p><img src="https://alexarmbr.github.io/images/row_vs_column_shmem_access.png" alt="row_vs_column_shmem_access"></p>

<p>This is a common situation in kernels that use 2d shared memory tiles, and the standard fix is to add a bit of padding (i.e. empty space) at the end of each row in the shared memory array. If we add this padding in such a way that a single row of our array no longer fits perfectly into the 32 memory banks, adjacent values in a column no longer fall into the same bank, which means we can read columns with no excessive wavefronts. This makes more sense in a picture than in words, here again is a simplified case of a mini-array (4 columns and 4 rows) stored on a mini-gpu with only 4 memory banks:
<img src="https://alexarmbr.github.io/images/simple_smem_padding.png" alt="simple_smem_padding">
Array elements are color coded by column. Notice that in the no padding case, all the array elements in a given column fall into the same memory bank. After adding the column of padding, the array elements in a given column are spread across all 4 memory banks. The padding technique could be used here to fully eliminate bank conflicts. Since we are using <a href="#kernel-2---vectorized-memory-copy-and-loop-unrolling">vectorized</a> writes to shared memory, we are writing to shared memory in 16 byte chunks at a time, and each chunk must be <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#device-memory-accesses">aligned</a>. Adding 16 bytes of padding to each row of shared memory would result in each 8x8 mma tile being spread across all 32 memory banks (exercise of convincing yourself of this left to reader).</p>

<p>The drawback of using the padding technique is that it requires us to allocate extra, unused space in shared memory. In Kernel 2, the shared memory tile for $A$ is 256x64, and the shared memory tile for $B$ is 128x64. If we add an extra 16 byte, or 8 element column to both of these, that will increase the amount of shared memory we allocate by 25%, for a total of increase of 6144 bytes. This wasted space turns out to be a significant drawback, when writing a high performance kernel shared memory is very precious stuff - this becomes especially apparent later down in the road when using a technique called double buffering, each threadblock in future kernels will end up using 100% of the 65536 bytes of shared memory on each SM. So, we should wonder whether there is a way to eliminate bank conflicts without wasting any shared memory space. It turns out this is very possible!</p>

<h3 id="swizzling-toy-example">Swizzling (toy example)</h3>
<p>Swizzling is probably my favorite technique that I learned in the process of working on this. The word “swizzle” has several different uses, when used in the context of cocktails it means to <a href="https://en.wikipedia.org/wiki/Swizzle_stick">stir</a> and when used in the context of GPUs it means to <a href="https://en.wikipedia.org/wiki/Swizzling_(computer_graphics)">rearrange</a>. In our context of eliminating shared memory bank conflicts in 2D tiles of data, swizzling means permuting the elements within a tile of shared memory such that we can access the data without any bank conflicts. This is one of those techniques that seemed like black magic to me until I took the time to understand it, and now I appreciate its cleverness and elegance.</p>

<p>In our 4x4 tile, we add the padding because it shifts the alignment between data and memory banks in a desirable way. Swizzling is based on the observation that we don’t need the extra padding bytes to spread column elements evenly over memory banks. Instead we can just figure out a permutation of matrix elements that spreads around the columns in the right way, and apply this permutation when we write to shared memory. Here is an illustration of a “swizzle” i.e. a permutation of elements that can eliminate bank conflicts.
<img src="https://alexarmbr.github.io/images/simple_smem_swizzled.png" alt="simple_smem_swizzled">
It is worth remembering at this point that our shared memory layout must satisfy two requirements, bank conflict free row access for writing, and bank conflict free column access for reading.</p>

<p>In all three cases, each row is consecutive in memory and spread across all four memory banks, which means each row can be written without any bank conflicts. The observation here is that when we apply our permutation or “swizzle”, we don’t want to permute elements across rows, only within rows; otherwise we might lose this property of bank conflict free writes.</p>

<p>The problem that motivated us to think about shared memory layouts was the bank conflicts that occur when we read columns. Adding the padding fixes the bank conflicts here, but at the expense of wasted shared memory. Swizzling gives us the best of both worlds; we can read columns with no bank conflicts, and no shared memory is wasted. So how do we think about applying this permutation?</p>

<p>The swizzle shown above can be implemented as a function <code>f</code> that maps indices to new indices. If <code>A</code> is the original array, <code>A_s</code> is the swizzled array, and <code>i</code> is the index of an element, then <code>A_s[f(i)] = A[i]</code>. So what is <code>f</code> here?</p>

<p>Since <code>f</code> is operating on array indices, we should think about the different ways these indices can be represented and viewed:
<img src="https://alexarmbr.github.io/images/simple_smem_indices.png" alt="simple_smem_indices">
On the far left are the 2D row and column indices. Moving to the middle, these indices can be linearized into a sequential (and in this case row major) ordering of the 16 elements in the array. Moving to the right, when we look at the sequential indices in binary we can see that the 2d structure is present in the index bits. The two least significant bits in the index encode the column and the two other bits encode the row. As a spoiler alert, <code>f</code> is going to operate from the perspective of the view on the right, the binary representation of the flat array index. Here are two observations about what <code>f</code> needs to do:</p>
<ul>
  <li>In order to avoid bank conflicts on write, we want to permute elements within a row, or in other words no elements should switch row. This means that <code>f</code> should modify the bits which encode the column, and leave alone the bits that encode the row.</li>
  <li>We want to apply a different permutation to each row, and for any given column, we want the elements in that column to be spread across all four columns in the swizzled array.</li>
</ul>

<p>We can accomplish both of these aims using the XOR function, specifically by XORing the row bits of each element with its column bits, and using the result as the new row bits. Here is a row by row break down that shows how XORing the column bits with the row bits moves around values within a row:
<img src="https://alexarmbr.github.io/images/swizzled_rows.png" alt="swizzled_rows">
The <code>f</code> that does this for us is <code>f(i) = i ^ ((i &amp; 0b1100) &gt;&gt; 2)</code>. The mask is selecting the two column bits from <code>i</code>, these two bits are then shifted right two places so that they line up with the two row bits for <code>i</code>, and then we XOR. <code>i</code>’s column bits remain unchanged.</p>

<p>Here is a visualization of the result of applying this function for all rows together:
<img src="https://alexarmbr.github.io/images/2d-swizzle.png" alt="2d-swizzle"></p>

<h3 id="swizzling-real-world">Swizzling (real world)</h3>
<p>Now we need to figure out how to use this technique to permute our shared memory layout in such a way that we can read a single 8x8 mma tile with 0 excessive wavefronts. As a reminder, here is a view of our shared memory layout, with a single tile of interest highlighted.
<img src="https://alexarmbr.github.io/images/mma_tile_zoom_in_blank.png" alt="mma_tile_zoom_in_blank"></p>

<p>Our goal is to figure out a swizzle function that spreads the 8 rows in this tile across all 32 memory banks, rather than having all 8 rows stuffed into 4 memory banks which is the case above. From the view of the full tile, the rows of the tile above would be spread like this.</p>

<p><img src="https://alexarmbr.github.io/images/mma_tile_zoom_in_swizzle.png" alt="mma_tile_zoom_in_swizzle"></p>

<p>In order to figure out what swizzle function we should use, lets look at the binary representation of an index into this tile, and assign it some structure that corresponds to our tiling scheme.</p>

<p><img src="https://alexarmbr.github.io/images/swizzle_index_groups.png" alt="swizzle_index_groups"></p>

<p>Some notes about what our swizzling function should do and not do:</p>
<ul>
  <li>We want to keep the eight elements in each MMA tile row together. In other words, eight adjacent elements in a single row of an 8x8 MMA tile are going to stay together when we apply the swizzle. This means our swizzle function is not going to touch the orange bits.</li>
  <li>Bank conflicts occur because the 8 rows within an MMA tile are all perfectly stacked on top of each other. Within an MMA tile, we want to spread these 8 rows horizontally across the entire warp tile. The blue bits encode where in the 64 element wide warp tile each MMA tile falls, so these blue bits are the ones we want our swizzle function to modify.</li>
  <li>We dont want to move elements between rows, so our swizzle function is not going to modify the green row bits. However, these green row bits provide a nice alternating pattern that we can XOR with the blue bits to mix around the MMA tiles within their row.</li>
  <li>Again, we dont want to be moving elements between rows, and the black bits (the most significant ones shown in this diagram) encode the starting row of each MMA tile. Our swizzle function is going to ignore them.</li>
</ul>

<p>So what this all means is that for each index, we want to take the blue bits, XOR them with the green bits, and replace the original blue bits with the result of this XOR. If <code>i</code> is the index we want to swizzle, this works out to:
<img src="https://alexarmbr.github.io/images/swizzled_vs_unswizzled.png" alt="swizzled_vs_unswizzled">
And just like that, we have no bank conflicts. Swizzling takes a bit more figuring out than the padding technique, the choice of swizzle function depends on the shared memory array dimensions, and the vector width we are using for reads/writes (i.e. <code>float4</code>, <code>float2</code>, <code>int</code>, e.t.c.). As a result, if we use swizzling it adds an extra consideration each time we consider changing either of these. But if you want to eliminate bank conflicts and dont want to increase your shared memory footprint in the process, swizzling becomes necessary. I think it is very elegant and clever, if you compare kernel 2 with kernel 3, there is a total of ~4 lines of code that change, these four lines are the addition of the swizzle into the shared memory index calculation.</p>

<p>I figured all this out by looking at the <code>Swizzle</code> class implemented <a href="https://github.com/NVIDIA/cutlass/blob/main/python/pycute/swizzle.py">here</a> in the CUTLASS repository. Via its three parameters, <code>bits</code>, <code>base</code>, and <code>shift</code>, this class represents a family of swizzle functions that shift and XOR bits of array indices. I have also seen examples of more exotic swizzle functions (see slide 27 <a href="https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9593-cutensor-high-performance-tensor-operations-in-cuda-v2.pdf">here</a>) that go beyond what is representable by the implementation in CUTLASS. I found it helpful to visualize the permutations applied by different swizzle functions, to help with this I wrote a bit of python <a href="https://github.com/alexarmbr/matmul-playground/blob/main/scripts/shmem_layout_viz.py">code</a> that pretty-prints arrays, applies swizzle functions, and counts bank conflicts.</p>

<p>Eliminating bank conflicts results in a ~2x speedup, and gets us to about 50% of the cuBLAS level thoughput.
<img src="https://alexarmbr.github.io/images/table3.png" alt="table3"></p>

<h2 id="kernel-4---makeshift-async-copy">Kernel 4 - Makeshift Async Copy</h2>
<p>Each optimization addresses the least performant part of the previoius kernel. After applying each optimization, if it worked, the least performant part of the kernel should change. Before fixing the shared memory bank conflicts, the shared memory operations inside the inner loop were the bottleneck. After eliminating bank conflicts, the inner loop becomes much more efficient, and the bottleneck is once again the latency of the global memory to shared memory transfer. This was addressed with vectorizing and loop unrolling in <a href="#kernel-2---vectorized-memory-copy-and-loop-unrolling">Kernel 2</a>, but after fixing bank conflicts, NSight Compute is telling us that there is more latency here to hide. Here is pseudocode of the current loop nests, with a zoomed in view of the code that needs to be improved:
<img src="https://alexarmbr.github.io/images/long_scoreboard_stall_kernel3.png" alt="long_scoreboard_stall_kernel3">
Once again the issue is that the line which performs the global memory to shared memory copy:</p>

<div><pre><code><span>dst_float4</span><span>[</span><span>dst_index</span><span>]</span> <span>=</span> <span>src_float4</span><span>[</span><span>src_index</span><span>];</span>
<span>// shared memory        // global memory</span>
</code></pre></div>

<p>This^ is a blocking operation from the perspective of hardware, in the sense that when a given thread executes the resulting assembly the thread will be stalled for the full duration of to data arriving from global memory. The above line is equivalent to this:</p>

<div><pre><code><span>float4</span> <span>tmp</span> <span>=</span> <span>src_float4</span><span>[</span><span>src_index</span><span>];</span> <span>// global memory to register</span>
<span>dst_float4</span><span>[</span><span>dst_index</span><span>]</span> <span>=</span>  <span>tmp</span><span>;</span> <span>// register to shared memory</span>
</code></pre></div>
<p>The global memory to register transfer, which is the first line, incurs latency because data is coming from off chip. When it comes time to store from register to shared memory (second line) the hardware detects that the data needed from global memory has not yet arrived in <code>tmp</code>, and execution stalls until it arrives. In <a href="#kernel-2---vectorized-memory-copy-and-loop-unrolling">Kernel 2</a> we addressed this performance issue by amortizing the latency over more data moved per transaction (vectorizing) and helping the compiler to interleave multiple loads/stores, which hides latency (loop unrolling). But NSight Compute tells us that even after these optimizations, this sort of stall, on this line specifically, accounts for about ~20% of the total clock cycles that the kernel spends stalled.</p>

<p>The key observation here is that if we break down the <code>dst[...] = src[...]</code> line into its two constituent parts, we can break them apart so that other useful work is being done while the data is in flight from global memory.
The general idea is that we can prefetch data from global memory into register storage, one <code>block_k</code> ahead of the <code>block_k</code> we are currently computing on. At a very high level we want to go from this:</p>
<div><pre><code><span>float4</span> <span>tmp</span> <span>=</span> <span>src_float4</span><span>[</span><span>src_index</span><span>];</span> <span>// global memory to register</span>
<span>// (stall while we wait for data to arrive from memory)</span>
<span>dst_float4</span><span>[</span><span>dst_index</span><span>]</span> <span>=</span>  <span>tmp</span><span>;</span> <span>// register to shared memory</span>
<span>{</span>
    <span>// compute inner loop for current block tile</span>
<span>}</span>
</code></pre></div>

<p>to this:</p>
<div><pre><code><span>float4</span> <span>tmp</span> <span>=</span> <span>src_float4</span><span>[</span><span>src_index</span><span>];</span> <span>// global memory to register</span>
<span>{</span>
    <span>// compute inner loop for previous block tile</span>
<span>}</span>
<span>dst_float4</span><span>[</span><span>dst_index</span><span>]</span> <span>=</span>  <span>tmp</span><span>;</span> <span>// register to shared memory</span>
</code></pre></div>

<p>The key improvement being made here is that we are initiating the load of data from global memory corresponding to <code>block_k</code>, and performing the compute corresponding to <code>block_k</code>-1 concurrently. In doing so we are hiding the latency of loading the <code>block_k</code> tiles of $A$ and $B$ with the computation corresponding to the <code>block_k</code>-1 tiles.</p>

<p><img src="https://alexarmbr.github.io/images/concurrent_fetch_compute.png" alt="concurrent_fetch_compute"></p>

<p>This improved overlapping of data movement and compute is accomplished by</p>
<ul>
  <li>adding new register storage to hold the data that is prefetched from global memory</li>
  <li>breaking up the global to shared memory transfer into its two components, putting these two components on opposite sides of the inner loop (over warp tiles and mma tiles)</li>
  <li>and tweaking the position of the two <code>__syncthreads()</code> in the outer loop to allow for the concurrency we want, while still preventing race conditions.</li>
</ul>

<p>Here is before/after pseudocode which shows how the data movement changes.
<img src="https://alexarmbr.github.io/images/prefetch.png" alt="prefetch"></p>

<p>This produces a nice speedup over the previous kernel, and gets us to ~70% of the HGEMM kernel.</p>

<p><img src="https://alexarmbr.github.io/images/table4.png" alt="table4"></p>

<h3 id="gpu-occupancy-digression">GPU occupancy (digression)</h3>
<p>The potential cost of this optimization is that it requires additional register storage, each thread block stores two additional block tiles worth of data in register memory. According to the Launch Statistics section in NSight Compute, we go from using 104 registers per thread in Kernel 3, to 166 registers per thread in Kernel 4. This increase resource usage per thread has the <em>potential</em> to hurt kernel performance because it can impact how many threads the hardware is capable of executing concurrently. This is a quick digression on why increasing register use per thread has the potential to hurt performance, but why in this case, it doesn’t.</p>

<p>This gets to a topic called occupancy which is central to the CUDA hardware and software implementation. Each streaming multiprocessor (SM) will maintain block, warp, and thread execution state (shared memory, registers, program counter) on chip, for as many thread blocks as can be fit. The amount of thread blocks that can be fit on an SM depends on:</p>
<ol>
  <li>how much shared memory, registers per thread, and number of threads each thread block needs to execute (this a property of a given kernel, and the launch configuration of that kernel)</li>
  <li>how much shared memory, registers per thread, and number of threads the SM can handle at once (this is a property the device, and improves from generation to genereation)</li>
</ol>

<p>If a given kernel implementation and launch configuration require only a small number of registers, a few threads, and a small amount of shared memory per block, an SM can execute lots of thread blocks concurrently. When multiple thread blocks are executing concurrently on an SM, context switching between them is free. This allows the hardware to hide stalls and latency simply by tracking which threads are capable of executing their next instruction and which are not, and issuing instructions for whichever threads are ready. The more threads the SM has to choose from, the better this works. This is called <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-multithreading">hardware multithreading</a>, and lots of older resources on CUDA performance talk about it as the primary guiding principle for writing fast kernels.</p>

<p>At this point, the limiting factor on the number of thread blocks that can be resident on an SM is shared memory. Each thread block is allocating a $(256,64)$ tile of shared memory for for the $A$ matrix, and a $(64,128)$ tile of shared memory for the $B$ matrix. This works out to 49KB of the total of 62KB of shared memory per SM, which limits the number of thread blocks that can be resident on an SM to one at a time. So in this instance, since shared memory is the limiting factor, using more registers per thread doesn’t matter.</p>

<p>The high performance GEMM kernels typically have lower occupancy, meaning they use more shared memory and register memory per thread, and have less threads resident on an SM at once. This is primarily because of the need for high arithmetic intensity; in order to keep compute units busy with limited memory bandwidth, the more compute per thread at low levels of the memory hierarchy, the better. But the drawback of low occupancy is that the GPU will be less effective at automatic latency hiding via context switching. We can deal with this tradeoff by structuring our kernel to allow overlap between compute and data movement, this chapter is an example of this.</p>

<p>The two newest NVIDIA architectures, Ampere and especially Hopper, introduce dedicated hardware suppport that allows us to perform several components of GEMM kernels asychronously (more on this in <a href="#lessons-learned-newer-gpus-are-better">conclusion</a>). This hardware support makes writing efficient, low occupancy kernels such as these ones much easier.</p>

<h2 id="kernel-5---tune-tile-dimensions">Kernel 5 - Tune Tile Dimensions</h2>
<p>Up until this point, I found that after 10 minutes or so of looking at profiling results in NSight Compute, I would know exactly where the bottleneck in the kernel was, and what was causing it. After Kernel 4, which achieves about 70% of the cuBLAS thoughput, the profiler generally would not point to a single performance issue. In hindsight this was because the remaining 30% between kernel 4 and cuBLAS is the product of many smaller inneficiences, as opposed to one single one, and performance optimization began to have more of a trial and error flavor based on hunches, some of which turned out to be wrong. This chapter contains a description of two optimizations, that when implemented together produced a decent speedup.</p>

<h3 id="tune-tile-dimensions">tune tile dimensions</h3>

<p>At this point I began to wonder, if my kernel were still memory bound, how would I know? If you are using single precision FFMA instructions the “Speed of Light” section in NSight Compute will show you a roofline chart, but not if you are using tensor cores. I was inspired by <a href="https://www.cse.ust.hk/~weiwa/papers/yan-ipdps20.pdf">this</a> paper to try to figure it out myself in a back of the envelope sort of way.</p>

<p>A more actionable way to formulate the “am I memory bound?” question is “does the arithmetic intensity of my kernel exceed the machine’s balance point?”</p><p>

\[\frac{FLOPs\ performed}{bytes\ moved} \stackrel{?}{&gt;} \frac{\tau}{\beta}\]

</p><p>So for the left hand side, we need to substitute in numbers that are specific to our kernel, and for the right hand side we need to subsitute numbers specific to our hardware. <a href="#arithmetic-intensity-as-a-function-of-tile-dimensions">This</a> section went over how arithmetic intensity is a function of tile sizes. Namely for tile dimensions $BM,BN$ and $BK$, the arithmetic intensity we should expect is $\frac{BM * BN}{BM+BN}$. Here is a refresher of this, specific to the block tile level
<img src="https://alexarmbr.github.io/images/intensity_block_tile_dims.png" alt="intensity_block_tile_dims"> 
Notice how $BK$ cancels out in this calculation. This means when thinking about arithmetic intensity, the size of our tiles along the $K$ dimension are irrelavent. However, when thinking about other aspects of performance it is not irrelavent (more on this later).</p>

<h4 id="m-and-n-dimensions--l2-cache-locality">M and N Dimensions / L2 cache locality</h4>
<p>We now need to subsitute in numbers for our machine balance. Earlier in the roofline charts we set $\tau_{HMMA}$ to the throughput of the cuBLAS hgemm kernel, which likely errs on the side of being an underestimate. In this case, the goal is to choose tile dimensions that are large enough to put us comfortably in the compute bound regime of the roofline chart, so I would like to instead err on the side of overestimating the arithmetic throughput in the numerator of the machine balance, and err on the side of underestimating the memory bandwidth in the denominator.</p>

<p>A reasonable overestimate of $\tau_{HMMA}$ is 65,000 GFLOP/sec, which is the theoretical peak found on the T4 data sheet.</p>

<p>When it comes to the memory bandwidth in the denominator, we want to conservatively estimate our achieved memory bandwidth. In order to do this, we need to consider the effect of the L2 cache. The L2 cache is shared between the 40 streaming multiprocessors on the T4. In practice this means that when one thread block accesses data from DRAM, it is moved into the L2 cache, and accesses to the same data originating from other thread blocks will hit the L2 cache, until this piece of data is evicted.</p>

<p>According to <a href="https://stackoverflow.com/questions/46660053/is-blockidx-correlated-to-the-order-of-block-execution">people on the internet</a>, thread blocks execute in increasing order of their flattened block index. The official CUDA programming guide says that different thread blocks execute independently, and the programmer should not assume any relationship between different thread blocks. So relying on this assumption for correctness would probably be unwise, but for a quick and approximate calculation on L2 cache locality it is helpful.
<img src="https://alexarmbr.github.io/images/l2_cache_locality.png" alt="l2_cache_locality">
The basic idea here is that he accesses to the $A$ matrix from thread blocks executing at the same time have much better locality than accesses to the $B$ matrix. Most of the access to $A$ should hit the L2 cache, whereas most of the accesses to $B$ should miss, which means we should achieve roughly a 50% hit rate for global memory accesses. This means our <em>achieved</em> memory bandwidth is a 50/50 weighted sum between the DRAM bandwidth and our L2 cache bandwidth. Substituting this weighted sum in the denominator of the expression for machine balance finally gives us:</p><p>

\[\frac{BM * BN}{BM+BN} \stackrel{?}{&gt;} \frac{\tau_{HMMA}}{0.5 * \beta_{DRAM} + 0.5 * \beta_{L2}}\]

</p><p>Plugging the current block tile dimensions ($BM=256$ and $BN=128$), memory bandwidths, and theoretical arithmetic throughputs gives us</p><p>

\[\frac{256 * 128\ FLOPs}{256 + 128\ bytes} \stackrel{?}{&gt;} \frac{65,000 * 10^9\ FLOPs/sec}{0.5 * 220 * 10^9 + 0.5 * 1280 * 10^9\ bytes/sec}\]

</p><p>which works out to an arithmetic intensity of $85.3 \frac{FLOPs}{byte}$ and a machine balance of $87.24 \frac{FLOPs}{byte}$. The fact that these two numbers are very close suggests that global memory access may still be dominating our overall runtime. If we can spare the space in shared memory, increasing our $BN$ dimension from 128 to 256 might be worthwhile. If $BM$ and $BN$ are both 256, our estimated arithmetic intensity becomes $128.0 \frac{FLOPs}{byte}$, which should hopefully put us comfortablly in the compute bound regime.</p>

<p>When considering the next level down in the hierarchy, the high shared memory bandwidth gives us a bit more wiggle room. Our swizzled shared memory layouts should result in bank conflict free access, giving us the full bandwidth of 3662 GB/sec. The $WM$ and $WN$ dimensions of the warp tiles are both 64. Plugging numbers into here:</p><p>

\[\frac{WM * WN}{WM+WN} \stackrel{?}{&gt;} \frac{\tau_{HMMA}}{\beta_{shmem}}\]

</p><p>gives an arithmetic intensity of $32 \frac{FLOP}{byte}$ and a balance point of $17.7 \frac{FLOP}{byte}$. Thus it is safe to assume that shared memory loads are not the dominant factor in our kernel’s runtime. However, in order to err on the size of more arithmetic intensity I also ended up increasing $WM$ and $WN$ while making $WK$ smaller.</p>

<h4 id="k-dimension">K Dimension</h4>
<p>Different considerations come into play when considering our tile sizes along the K dimension. In our pencil and paper analysis, the tile size along the K dimension cancels out of the expression for arithmetic intensity. When thinking about tile lengths along this dimension, different considerations come into play. First, we can use it to adjust the total size of our tiles without affecting arithmetic intensity. In the case of block tiles, the total amount of bytes of shared memory they consume is $BK* (BM+ BN)* sizeof(half)$, thus increasing $BK$ by a unit increases the total size of the block tiles by $(BM+ BN)* sizeof(half)$. In deciding the length of the block tiles along the K dimension, this becomes the primary consideration. With $BN=256,BM=256$ we choose $BK=32$, with these dimensions the total amount of shared memory used by tiles of $A$ and $B$ works out to 32KiB, which is exactly half of the shared memory per streaming multiprocessor. This decision makes sense in the next section, which discusses a technique called shared memory double buffering. This optimization involves allocating two buffers in shared memory corresponding to each input matrix, so one can be written to while the other is being read. When double buffering is implemented, with these tile dimensions we will be using every available byte of shared memory on the device.</p>

<h3 id="tile-dimensions---longer-and-thinner">tile dimensions - longer and thinner</h3>
<p>Here is a visualization of the before/after:
<img src="https://alexarmbr.github.io/images/tile_dims_adjustment.png" alt="tile_dims_adjustment.png">
Both block tiles and warp tiles are made longer, and narrower along the K dimension, in order to increase arithmetic intensity. For the sake of time I combined this optimizations with the optimizations discussed below, so I did not measure the performance improvmement of this in isolation.</p>

<h2 id="kernel-5---optimize-index-calculation">Kernel 5 - Optimize Index Calculation</h2>
<p>At this point I was at about 70% of cuBLAS performance, my main strategy for using NSight Compute was to compare kernel metrics between my kernels and the cuBLAS HGEMM kernel. While the source code of the cuBLAS HGEMM implementation is not released by NVIDIA, looking at its metrics collected by NSight Compute can give us some insights into the sorts of optimization techniques that the clever folks at NVIDIA might have used when writing it.</p>

<p>The one thing that jumped out at me was that the total number of executed instructions of cuBLAS HGEMM was $94,175,232$, whereas Kernel 4 was executing $216,227,840$, over twice as many instructions as compared to Kernel 4. While Kernel 4 partly compensates for this by having a lower cycles per instruction ratio (8ish, vs 12ish for cuBLAS), this is certainly worth looking into.</p>

<p>So I wondered, why is my kernel executing twice as many instructions? Expanding the instruction mix section in NSight Compute gives us more information.
<img src="https://alexarmbr.github.io/images/instruction_mix_comparison.png" alt="instruction_mix_comparison">
The answer is that Kernel 4 is performing way more index calcuation related instructions than the cuBLAS kernel. The <code>LOP</code>, <code>IADD3</code>, and <code>SHF</code> instructions are integer and logical instructions, these are different pipelines from the tensor core and can execute concurrently with floating point math happening elsewhere on the chip. However, each warp scheduler on a streaming multiprocessor can only issue a single instruction per cycle, and so the large number of index calculation instructions is likely crowding out the issuing of the <code>HMMA</code> instructions, these are the tensor core instructions doing the heavy lifting. So what are these integer and logical instructions doing, and why are there so many of them?</p>

<p>According to NSight Compute, 92% of the total instructions executed by Kernel 4 are in the loop nest where each warp loads its region of data from shared memory into register memory, and then performs an outer product over local matrices stored in register memory with a series of <code>HMMA</code> instructions. The three nested loops that map the <code>HMMA</code> instructions to their position are all fully unrolled, so there isn’t any runtime index calculation required there.</p>

<p>However, the <code>HMMA</code> instructions operate on $8$ by $8$ tiles stored in registers, and before the compute phase the threads in each warp work collaboratively to load all of these tiles from swizzled shared memory into register memory using the <code>ldmatrix</code> PTX instruction (see <a href="#how-to-use-tensor-cores">here</a>) for an explanation of <code>ldmatrix</code>. Since at this point we are all the down at the bottom level of the tile hierarchy, the tiles are very small, and consequently we are doing this index calculation <em>lots</em> of times ($O(\frac{N^3}{8})$), and it involves multiplying by a bunch of strides, computing a modulo WRT the thread index, and several logical operations to apply the swizzling function, all of which happens at runtime.</p>

<p><img src="https://alexarmbr.github.io/images/index_calculation_inneficient.png" alt="index_calculation_inneficient"></p>

<p>In order to make this more performant, we should move as much of this calculation as possible to happen at compile time, and whatever needs to happen at runtime should be as streamlined as possible. In the index calculation code shown above, fundamentally there are three distinct and dependent steps</p>
<ol>
  <li>First each warp computes the memory address of the top left corner of the mma tile</li>
  <li>Each thread calculates the memory address of the element it will load, relative to (1)</li>
  <li>Because our shared memory layout is swizzled, each thread applies the swizzle function to the address computued in (2)
in order to get the correct memory address in the swizzled layout.</li>
</ol>

<p>All three steps are done for each of the 8x8 MMA tiles. Below is a visualization of this, the diagram below is a mini example where each MMA tile is four rows and one column, and each warp tile has 2x8 MMA tiles (using simpler examples like this allows us to make all the details as explicit as possible, and the <img title=":smiling_imp:" alt=":smiling_imp:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f608.png" height="20" width="20"> is in the details).</p>

<p><img src="https://alexarmbr.github.io/images/swizzled_index_calculation_inneficient.png" alt="swizzled_index_calculation_inneficient"></p>

<p>In the middle column, each thread has calculated the address of the value it is going to load, in the unswizzled layout. Each iteration, these pointers are advanced to the right by one column, until we get to the end of the warp tile at which point we go down to the next set of rows. If it weren’t for the swizzled layout, we could just advance the pointers by one each iteration, i.e. <code>thread_row+=1</code>. However, because the data is stored in a swizzled layout, advancing the pointers over to the next group of MMA tiles is not simply a matter of incrementing by one.</p>

<p>While incrementing by one will not work for iterating over a swizzled layout, we can achieve the equivalent effect by XORing each threads pointer with a constant.
<img src="https://alexarmbr.github.io/images/swizzled_index_calculation_efficient.png" alt="swizzled_index_calculation_efficient">
This reduces the amount of index calculation from ~13 operations in between each <code>ldmatrix</code>, down to a single XOR. After applying this optimization, the total number of instructions executed goes down to ~90M, which is slightly less than cuBLAS.</p>

<p>This illustrates the basic principle of efficiently iterating through a swizzled data layout.  In the <a href="https://github.com/alexarmbr/matmul-playground/blob/main/src/kernel5.cu#L10">actual code</a>, it is a bit more complicated because the swizzle function is more complicated, and we need to iterate through the tiles of A and B which have different dimensions from each other. Also the loops containing the <code>ldmatrix</code> instructions are manually unrolled, this makes the XORing easier, and also might allow the compiler to do a better job of interleaving the <code>ldmatrix</code> and <code>mma.sync</code> instructions to balance load between the two different pipelines.</p>

<p>The optimized index calcuation, loop unrolling, and adjusted tile dimensions are all implemented as part of the same kernel, that achieves a hard fought 1.2x speedup over the last one, and gets us to 86.7% of cuBLAS throughput.
<img src="https://alexarmbr.github.io/images/table5.png" alt="table5"></p>

<h2 id="kernel-6---double-buffering">Kernel 6 - Double Buffering</h2>
<p>Back to the profiler (for the last time). At this point many of the metrics between my kernel and cuBLAS were starting to look somewhat similiar. One thing that jumped out at me was that the threads in my kernel spend more time stalled on <code>__syncthreads()</code> than the cuBLAS kernel. At this point my kernel had a CPI (cycles per instruction) of 14, and about 2.6 of these cycles come from sychronization stalling. So this was not an egregious performance issue, but noticeable. A technique called double buffering enables you to remove one of the two <code>__syncthreads()</code> in the inner loop. After a bit of pondering I realized that this provides no guaruntee of a proportional decrease in cycles stalled on <code>__syncthreads()</code> (if you remove one <code>__syncthreads()</code>, threads might spend twice as much time stalled on the other). However, double buffering should also allow for a bit more instruction level parallelism inside the main loop, and it is implemented in CUTLASS kernels, and I had the shared memory to spare, so why not.</p>

<p>The data dependencies inside the main loop of our current GEMM kernel necessitate having two <code>__syncthreads()</code> in order to prevent race conditions in shared memory
<img src="https://alexarmbr.github.io/images/two_syncthreads.png" alt="two_syncthreads">
If we removed either, race conditions would occur because the thread to data mapping is different when writing to shared memory vs. reading it. This is because any given thread is computing on different values than the ones that it fetched from global memory and wrote to shared memory. This means that sychronization points are required to prevent race conditions, because the whole thread block must wait until all threads are done writing to shared memory before any thread starts reading from shared memory.</p>

<p>The cost of these sychronization points is less parallelism and potentially less hardware utilization. As the diagram above shows, there are four main components to the main loop.</p>
<ol>
  <li>prefetching the next block tile into registers</li>
  <li>shared memory to register transfer in preparation for compute</li>
  <li>compute</li>
  <li>writing the prefetched data back from registers to shared memory</li>
</ol>

<p>As the diagram above illustrates #4 is kept seperate from the other three because it involves writing to the data that is being read in #2, i.e. all 256 threads in a block must complete #2 before any start #4. This seperation is bad for performance, because it limits the compilers ability to interleave instructions of different types to balance load across different hardware pipelines.</p>

<p>The idea behind double buffering is that if we allocate an extra pair of shared memory buffers for the block tiles of $A$ and $B$, we can write to one pair of buffers concurrently with the other being read. This allows us to remove the second <code>__syncthreads()</code> from the mainloop. This should make things a bit faster.</p>

<p><img src="https://alexarmbr.github.io/images/one_syncthreads.png" alt="one_syncthreads"></p>

<p>The two things that changed here are the removal of one of the <code>__syncthreads()</code>, and the addition of an index that we always use  (<code>%2</code>) to track which of the two buffers is being read, and which is being written on any given iteration. The buffer that is being read and the buffer that is being written switches each iteration.</p>

<p><img src="https://alexarmbr.github.io/images/double_buffering.png" alt="double_buffering"></p>

<p>This results in a small speedup over the previous kernel. But at this stage of trying to optimize an already highly optimized kernel, I’ll take what I can get.</p>

<p><img src="https://alexarmbr.github.io/images/table6.png" alt="table_6"></p>

<h2 id="conclusion">Conclusion</h2>
<h2 id="things-i-didnt-do">things I didn’t do</h2>
<p>And this is where I called it a day! There are two avenues for further performance improvement, but the time I alloted to work on this ran out. The former is a lot easier than the latter.</p>
<ul>
  <li>
<strong>optimized epiloge</strong>- As a reminder, the GEMM problem is $D=\alpha * A * B + \beta * C$. This is two computations stuffed into one kernel. The bulk of the compute is in the matrix multiplication $C^*=A * B$. Once we multiply the two matrices, then we do $D = \alpha * C^*  + \beta * C$, this is generally referred to as the kernel epilogue. The former is an $O(N^3)$ problem, the later is $O(N^2)$. When N is large, the matrix multiplication dominates the runtime of the combined algorithm, when N is smaller the epilogue is more significant. The article focused entirely on the matrix multiplication, as this is the most interesting and important component of the GEMM problem. The kernel epilogue I used in all six kernels is innefficient - once the matrix multiplication is complete, the result is scattered across thread registers according to the <code>m16n8k8</code> MMA layout (see <a href="#appendix">below</a>), and they are written directly back to memory. This write is uncoalesced and consequently achieves less than ideal bandwidth and latency. Improving this would likely narrow the gap between Kernel 6 and cuBLAS for smaller matrix sizes.</li>
  <li>
<strong>manual instruction mix tuning for inner loop</strong>- Projects like <a href="https://github.com/NervanaSystems/maxas/wiki/SGEMM">this</a> and <a href="https://github.com/daadaada/turingas">this</a> match/exceed the performance of cuBLAS using custom built assemblers that allow them to write the kernel entirely in SASS. The inner loop of a GEMM kernel consists of shared memory loads and math instructions. If too many instructions of one type are grouped together, hardware pipelines will get overloaded and stall cycles will be incurred. If you want to write your kernels entirely in CUDA and PTX as I did, then instruction scheduling is the job of the compiler, the fact that I was able to get &gt;90% of cuBLAS performance without any inlined assembly means that nvcc probably does a pretty good job at it. However, if one were really determined to write a kernel that is as fast or faster than cuBLAS for a range of matrix sizes, this would avenue would likely be necesssary.</li>
</ul>

<h2 id="performance-on-different-matrix-sizes">performance on different matrix sizes</h2>
<p>Here is a plot that shows the performance of the kernels I wrote, compared to cuBLAS, for a bunch of different matrix dimensions.
<img src="https://alexarmbr.github.io/images/hgemm_performance.png" alt="hgemm_performance"></p>

<p>Note that the gap between the fastest kernel I wrote, and cuBLAS HGEMM is slightly larger for smaller matrices, possibly due to my unoptimized epilogue. Also possibly due to the fact that cuBLAS is selecting kernels that have been specifically tuned for those matrix dimensions.</p>

<h2 id="lessons-learned-newer-gpus-are-better">lessons learned, newer GPUs are better</h2>
<p>Given how many people and companies these days are buying NVIDIA GPUs almost exclusively for the purpose of running matrix multiplications, it seems like lots of work goes into improving the tensor cores in terms of programmability and performance between successive architectures. The tensor core throughput goes up by an order of magnitude with each new SM architecture, and the memory bandwidth also increases, but not proportionally.</p>

<p>In order to make the task of programming these powerful but imbalanced machines more manageable, the more recent Ampere and Hopper architectures introduced hardware support that enable several important parts of a GEMM kernel to run asychronously with respect to the rest of the SM. Ampere introduced hardware support for <a href="https://docs.nvidia.com/cuda/ampere-tuning-guide/index.html#asynchronous-data-copy-from-global-memory-to-shared-memory">asychronous data copying</a> from global memory to shared memory, I implemented a sort of hacked version of this using extra registers in <a href="#kernel-4---makeshift-async-copy">Kernel 4</a>. The Hopper architecture introduced something even fancier called the <a href="https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html#tensor-memory-accelerator">Tensor Memory Accelerator</a>, which is essentially a copy engine that can perform index calculation, and initiate global memory transfers asychronously with respect to the rest of the SM. Thus developers writing kernels for Hopper probably dont have to worry about the efficiency of index calculation (like I did <a href="#kernel-5---optimize-index-calculation">here</a>), because this is offloaded to dedicated hardware in the TMA. Hopper also has asychronous tensor core instructions, that can read/write from/to shared memory, rather than registers (see <a href="https://research.colfax-intl.com/cutlass-tutorial-wgmma-hopper/">here</a>).</p>

<p>All of this asychronicity is a great thing for low occupancy, register hungry GEMM kernels. As discussed <a href="#gpu-occupancy-digression">here</a>, large arithmetic throughput means we need lots of fast memory to cache data, which means we cant run that many threads per SM, which means the GPU wont automatically hide our latency by context switching, which means we the programmer need to think more about how our latency is being hidden. This is where asychronicity is helpful.</p>

<p>All of this means that Hopper is kind of a new and different beast, if you look at GEMM kernels in CUTLASS that target Hopper the code has a different stucture than all of the other pre <code>sm_90</code> kernels. Hopper kernels use a producer/consumer pattern, where a relatively small number of producer threads are initiating asynchronous data copies with the TMA, and then consumer threads are managing the tensor cores. I have never worked on kernels targetting Hopper so I dont know much about this at the moment, <a href="https://hazyresearch.stanford.edu/blog/2024-05-12-tk">this</a> article provides an interesting overview of the user experience of writing kernels for Hopper.</p>

<p>This is all to say that the kernels discussed here target the Turing architecture, which was SOTA in 2018, and if you are writing kernels targeting Ampere or Hopper, the techniques you employ for latency hiding will be different and easier. I used the Tesla T4 GPU because you can rent them on AWS for ~50 cents/hour, which is about as much money as I want to spend on EC2 instances. Using an older GPU was a blessing and a curse for this project, the curse was that no special hardware support was available for hiding memory latency on calculating indices, the blessing was that I had to do all this myself which was an educational experience!</p>

<h2 id="resources--acknowledgements">Resources / Acknowledgements</h2>
<p>Most of these resources have already been linked to in various places throughout this article, but I wanted to put them all in one place. These are some resources that have educated and inspired me, in no particular order</p>

<ul>
  <li>I learned about the roofline model from Prof. Vuduc’s Intro to High Performance Computing class at Georgia Tech, all of the course videos are available <a href="https://edstem.org/us/join/GT3Qcc">here</a>, these videos are an amazing free resource if you have the time and inclination to watch them. The part of this article about rooflines and computational intensity is similiar to what is presented in the “Basic Model of Locality” section.</li>
  <li>
<a href="https://siboehm.com/articles/22/CUDA-MMM">This</a> article was a major source of inspiration for this project. Simon’s other articles are excellent as well, this is probably one of my favorite blogs on the internet at the moment.</li>
  <li>
<a href="https://horace.io/brrr_intro.html">Another</a> excellent blog about a systems view of ML. This article in particular is a very readable explanation of why things like memory bandwidth and arithmetic intensity matter when training neural nets on GPUs.</li>
  <li>Great <a href="https://hazyresearch.stanford.edu/blog/2024-05-12-tk">article</a> from a systems ML lab at stanford about the kernel engineering for the Hopper architecture.</li>
  <li>
<a href="https://github.com/NVIDIA/cutlass">This</a> is NVIDIA’s CUTLASS project, provides a bunch of abstractions that make it easier to write fast kernels.</li>
</ul>

<h2 id="are-you-hiring-gpu-nerds">Are you hiring GPU nerds?</h2>
<p>I am usually not one for self promotion, but I recently took a bit of a break from work, and now am back on the job market. If you are a hiring manager who is looking for someone to fiddle around with kernels, profilers, and/or compilers please email me!</p>


  </div>

</article>

      </div></div>]]></description>
        </item>
    </channel>
</rss>