<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 26 Sep 2025 17:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Fast UDP I/O for Firefox in Rust (161 pts)]]></title>
            <link>https://max-inden.de/post/fast-udp-io-in-firefox/</link>
            <guid>45387462</guid>
            <pubDate>Fri, 26 Sep 2025 15:14:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://max-inden.de/post/fast-udp-io-in-firefox/">https://max-inden.de/post/fast-udp-io-in-firefox/</a>, See on <a href="https://news.ycombinator.com/item?id=45387462">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2 id="motivation">Motivation</h2>
<p>Around 20% of Firefox’s HTTP traffic today uses HTTP/3, which runs over QUIC, which in turn runs over UDP.
This translates to substantial UDP I/O activity.</p>
<p>Firefox uses <a href="https://www-archive.mozilla.org/projects/nspr/">NSPR</a> for most of its network I/O.
When it comes to UDP I/O, NSPR only offers a limited set of dated APIs, most relevant here <a href="https://firefox-source-docs.mozilla.org/nspr/reference/pr_sendto.html"><code>PR_SendTo</code></a> and <a href="https://firefox-source-docs.mozilla.org/nspr/reference/pr_recvfrom.html"><code>PR_RecvFrom</code></a>, wrappers around POSIX’s <code>sendto</code> and <code>recvfrom</code>.
The N in NSPR stands for Netscape, giving you a hint of its age.</p>
<p>Operating systems have evolved since.
Many offer multi-message APIs like <code>sendmmsg</code> and <code>recvmmsg</code>.
Some offer segmentation offloading like GSO (Generic Segmentation Offload) and GRO (Generic Receive Offload).
Each of these promise significant performance improvements for UDP I/O.</p>
<p>Can Firefox benefit from replacing its aging UDP I/O stack with modern system calls?</p>
<h2 id="overview">Overview</h2>
<p>This project began in mid-2024 with the goal of rewriting Firefox’s QUIC UDP I/O stack using modern system calls across all supported operating systems.
Beyond performance improvements, we wanted to increase security by using a memory-safe language to do UDP I/O.
Firefox’s QUIC state machine itself is implemented in Rust already.
We thereby chose Rust for this project as well, giving us both increased security and easy integration with the existing QUIC codebase.</p>
<p>Instead of starting from scratch, we built on top of <a href="https://github.com/quinn-rs/quinn/tree/main/quinn-udp"><code>quinn-udp</code></a>, the UDP I/O library of the Quinn project, a QUIC implementation in Rust.
This sped up our development efforts significantly.
Big thank you to the Quinn project.
Operating system calls are complex, with a myriad of idiosyncrasies, especially across versions.
Firefox is multi-platform, focusing on Windows, Android, MacOS and Linux as <a href="https://firefox-source-docs.mozilla.org/build/buildsystem/supported-configurations.html#supported-build-targets">tier 1</a>.
The main complexity though stems from Firefox supporting ancient versions of each of them, e.g.
Android 5.</p>
<p>One year later, i.e., mid 2025, this project is now rolling out to the majority of Firefox users.
Performance benchmark results are promising.
In extreme cases, on purely CPU bound benchmarks, we’re seeing a jump from &lt; 1Gbit/s <a href="https://github.com/mozilla/neqo/actions/workflows/bench.yml">to 4 Gbit/s</a>.
Looking at CPU flamegraphs, the majority of CPU time is now spent in I/O system calls and cryptography code.</p>
<p>Below are the many improvements we were able to land, plus the ones we weren’t.
I hope other projects in need of fast UDP I/O can benefit from our work.
To make their lifes easier, below I am documenting the many learnings we made.</p>
<h2 id="the-basics">The basics</h2>
<p>To understand the improvements, it’s helpful to first examine how UDP I/O traditionally works and how modern optimizations change this picture.</p>
<h3 id="single-datagram">Single datagram</h3>
<p>Previously Firefox would send (and receive) single UDP datagrams to (and from) the OS via <code>sendto</code> (and <code>recvfrom</code>) system call family.
The OS would send (and receive) that UDP datagram to (and from) the network interface card (NIC).
The NIC would send (and receive) it to (and from) <em>the Internet</em>.</p>
<p>Thus each datagram would require leaving user space which is cheap for one UDP datagram, but <a href="https://max-inden.de/post/2020-06-19-latencies/">expensive when sending at say a 500 Mbit/s rate</a>.
In addition all user space and kernel space overhead independent of the number of bytes sent and received, is paid per datagram, i.e. per &lt; 1500 bytes.</p>
<pre tabindex="0"><code>    +----------------------+
    |       Firefox        |
    |    +-----------+     |
    |    |   QUIC    |     |
    |    +-----------+     |
    +----------------------+
              |
         [ datagram ]
              |
    === User / Kernel ===
              |
         [ datagram ]
              |
    +----------------------+
    |         OS           |
    +----------------------+
              |
         [ datagram ]
              |
    +----------------------+
    |         NIC          |
    +----------------------+
              |
         [ datagram ]
              |
    +----------------------+
    |      Internet        |
    +----------------------+
</code></pre><h3 id="batch-of-datagrams">Batch of datagrams</h3>
<p>Instead of sending a single datagram at a time, some operating systems nowadays offer multi-message system call families, e.g. on Linux <code>sendmmsg</code> and <code>recvmmsg</code>.
The idea is simple.
Send and receive multiple UDP datagrams at once, save on the costs that are independent of the number of bytes sent and received.</p>
<pre tabindex="0"><code>    +--------------------------+
    |         Firefox          |
    |      +-----------+       |
    |      |   QUIC    |       |
    |      +-----------+       |
    +--------------------------+
                  |
  [ datagram, datagram, datagram ]
                  |
    ===== User / Kernel =====
                  |
  [ datagram, datagram, datagram ]
                  |
    +--------------------------+
    |           OS             |
    +--------------------------+
                  |
  [ datagram, datagram, datagram ]
                  |
    +--------------------------+
    |           NIC            |
    +--------------------------+
                  |
  [ datagram, datagram, datagram ]
                  |
    +--------------------------+
    |        Internet          |
    +--------------------------+
</code></pre><h3 id="single-large-segmented-datagram">Single large segmented datagram</h3>
<p>Some modern operating systems and network interface cards also support system call families with UDP segmentation offloading, e.g. <code>GSO</code> and <code>GRO</code> on Linux.
Instead of sending multiple UDP datagrams in a batch, it enables the application to send a single large UDP datagram, i.e. larger than the Maximum Transmission Unit, to the kernel.
Next, either the kernel, but really ideally the network interface card, will segment it into multiple smaller packets, add a header to each and calculates the UDP checksum.
The reverse happens on the receive path, where multiple incoming packets can be coalesced into a single large UDP datagram delivered to the application all at once.</p>
<pre tabindex="0"><code>    +------------------------------+
    |           Firefox            |
    |        +-----------+         |
    |        |   QUIC    |         |
    |        +-----------+         |
    +------------------------------+
                    |
      [ large segmented datagram ]
                    |
      ====== User / Kernel ======
                    |
      [ large segmented datagram ]
                    |
    +------------------------------+
    |             OS               |
    +------------------------------+
                    |
      [ large segmented datagram ]
                    |
    +------------------------------+
    |             NIC              |
    +------------------------------+
                    |
    [ datagram, datagram, datagram ]
                    |
    +------------------------------+
    |          Internet            |
    +------------------------------+
</code></pre><p><em>Note: Unfortunately, <a href="https://gitlab.com/wireshark/wireshark/-/issues/19109">Wireshark does not yet support GSO</a>, making network-level debugging more challenging when these optimizations are active.</em></p>
<p>For performance analysis of these different approaches, <a href="https://blog.cloudflare.com/accelerating-udp-packet-transmission-for-quic/">Cloudflare’s comprehensive study</a> provides excellent benchmarks and detailed explanations.</p>
<h2 id="replacing-nspr-in-firefox">Replacing NSPR in Firefox</h2>
<p>Batching and segmentation offloading aside for now, first step in the project was to replace usage of NSPR with quinn-udp, still sending and receiving one UDP datagram at a time.
We updated the <a href="https://github.com/mozilla/neqo/pull/1604">Mozilla QUIC client and server test implementation</a>, then <a href="https://phabricator.services.mozilla.com/D216308">integrated quinn-udp into Firefox itself</a>.</p>
<p>Next we rewrote the UDP datagram processing pipeline in the Mozilla QUIC implementation to <a href="https://github.com/mozilla/neqo/pull/2184">send</a> and <a href="https://github.com/mozilla/neqo/pull/2184">receive</a> batches of datagrams.
This is done in a way, such that we can leverage both the multi-message style system calls, as well as the segmentation offloading style, if available.
We added this along with various other I/O improvements, e.g. Lars added <a href="https://github.com/mozilla/neqo/pull/2385">in-place en-/decryption</a>.
Going into detail here is better done in a separate blog post.
Let’s focus on UDP I/O here.</p>
<p>So far so good.
This was the easy part.
Up next, the edge cases by platform.</p>
<h2 id="platform-details">Platform details</h2>
<h3 id="windows">Windows</h3>
<p>Windows offers <a href="https://learn.microsoft.com/en-us/windows/win32/api/winsock2/nf-winsock2-wsasendmsg?redirectedfrom=MSDN"><code>WSASendMsg</code></a> and <a href="https://learn.microsoft.com/en-us/previous-versions/windows/desktop/legacy/ms741687(v=vs.85)"><code>WSARecvMsg</code></a> to send and receive a single UDP datagram.
That UDP datagram can either be a classic MTU size datagram, or a large segmented datagram.
For the latter, what Linux calls <code>GSO</code> and <code>GRO</code>, Windows call <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/network/udp-segmentation-offload-uso-"><code>USO</code></a> and <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/network/udp-rsc-offload"><code>URO</code></a>.
As described above, we started off rolling out quinn-udp using single-datagram system calls only.
This went without issues on Windows.</p>
<p>Next we tested <code>WSARecvMsg</code> with <code>URO</code>, i.e. receiving a batch of inbound datagrams as a single large segmented datagram, but got <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1916558">the following bug report</a>:</p>
<blockquote>
<p>fosstodon.org doesn’t load with network.http.http3.use_nspr_for_io=false on ARM64 Windows</p></blockquote>
<p>fosstodon is a Mastodon server.
It is hosted behind the CDN provider Fastly.
Fastly is a heavy user of Linux’s GSO, i.e. sends larger UDP datagram trains, perfect to be coalesced into a single large segmented UDP datagram when Firefox receives it.
Why would Window’s <code>URO</code> prevent Firefox from loading the site?</p>
<p>After many hours of back and forth with the reporter, luckily a Mozilla employee as well, I ended up buying the exact same laptop, <strong>same color</strong>, in a desperate attempt to reproduce the issue.
Without much luck at first, I eventually needed a Linux command line tool, thus installed WSL, and to my surprise, that triggered the bug (<a href="https://github.com/quinn-rs/quinn/issues/2041#issuecomment-2495419003">reproducer</a>).
Turns out, on Windows on ARM, with WSL enabled, a <code>WSARecvMsg</code> call with <code>URO</code> would not return a segment size, thus Firefox was unable to differentiate a single datagram, from a single segmented datagram.
QUIC short header packets don’t carry a length, thus there is no way to tell where one QUIC packet ends and another starts, leading to the above page load failures.</p>
<p>We have been in touch with Microsoft since.
No progress thus far.
Thereby we are keeping <a href="https://github.com/quinn-rs/quinn/pull/2092"><code>URO</code> on Windows disabled</a> in Firefox for now.</p>
<p>After <code>URO</code> we started using <code>WSASendMsg</code> <code>USO</code>, i.e. sending a single large segmented datagram per system call.
But this too we rolled back quickly, seeing <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1979279">increased packet loss on Firefox Windows installations</a>.
In addition, we have at least one report of a user, seeing their network driver crash <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1978821">due to Firefox’s usage of <code>USO</code></a>.
More debugging needed.</p>
<h3 id="macos">MacOS</h3>
<p>The transition on MacOS from NSPR to quinn-udp for HTTP/3 QUIC UDP I/O involved switching from the system calls <code>sendto</code> and <code>recvfrom</code> to the system calls <code>sendmsg</code> and <code>recvmsg</code>.
As with Windows, no issues on this first step, ignoring one <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1987606">report</a> where MacOS 10.15 might be seeing <a href="https://github.com/quinn-rs/quinn/pull/2387">IP packets other than v4 and v6</a> (fixed since).</p>
<p>Unfortunately MacOS does not offer UDP segmentation offloading, neither on the send, nor on the receive side.
What it does offer though are two undocumented system calls, namely <a href="https://github.com/apple-oss-distributions/xnu/blob/94d3b452840153a99b38a3a9659680b2a006908e/bsd/sys/socket.h#L1457-L1487"><code>sendmsg_x</code></a> and <a href="https://github.com/apple-oss-distributions/xnu/blob/94d3b452840153a99b38a3a9659680b2a006908e/bsd/sys/socket.h#L1425-L1455"><code>recvmsg_x</code></a>, allowing a user to send and receive batches of UDP datagrams at once.
Lars from Mozilla added it to quinn-udp, exposed behind the <code>fast-apple-datapath</code> feature flag, off by default.
After multiple iterations with smaller bugfixes (<a href="https://github.com/quinn-rs/quinn/pull/2154">#2154</a>, <a href="https://github.com/quinn-rs/quinn/issues/2214">#2214</a>, <a href="https://github.com/quinn-rs/quinn/pull/2216">#2216</a> …) we decided to <a href="https://github.com/mozilla/neqo/pull/2638">not ship it to users</a>, not knowing how MacOS would behave, in case Apple ever decides to remove it, but with Firefox still calling it.</p>
<h3 id="linux">Linux</h3>
<p>Linux provides the most comprehensive and mature UDP optimization support, offering both multi-message APIs (<code>sendmmsg</code>/<code>recvmmsg</code>) and segmentation offloading (GSO/GRO).
The quinn-udp library makes a deliberate choice to <a href="https://github.com/quinn-rs/quinn/pull/1729#issuecomment-1866939467">prioritize GSO over <code>sendmmsg</code></a> for transmission, as GSO typically provides superior performance with diminishing returns when both techniques are combined.
Thus far, this has proven the right choice for Firefox as well.</p>
<p>In addition to segmentation offloading being superior in the first place, Firefox uses one UDP socket per connection in order to improve privacy.
As each socket gets its own source port it is harder to correlate connections.
Why is this relevant here?
<code>GSO</code> (and <code>GRO</code>) can only segment (and coalesce) datagrams from the same 4-tuple (src IP, src port, dst IP, dst port), <code>sendmmsg</code> and <code>recvmmsg</code> on the other hand can send and receive across 4-tuples.
Given that Firefox uses one socket per connection, it cannot make use of that distinct benefit of <code>sendmmsg</code> (and <code>recvmmsg</code>), making segmentation offloading yet again the obvious choice for Firefox.</p>
<p>Ignoring minor changes required to <a href="https://hg-edge.mozilla.org/integration/autoland/rev/5f3a2655d2f4">Firefox’s optional network sandboxing</a>, and an additional at runtime <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1989895">GSO support check</a>, replacing Firefox’s QUIC UDP I/O stack on Linux has been without issues, now enjoying all the benefits of segmentation offloading.</p>
<h3 id="android">Android</h3>
<p>During the time of this project I learned quickly that (a) Android is not Linux and (b) that <a href="https://support.mozilla.org/en-US/kb/will-firefox-work-my-mobile-device">Firefox still supports Android 5</a>, …, on x86 (32 bit).</p>
<p>On x86, <a href="https://github.com/quinn-rs/quinn/pull/1964">Android dispatches advanced socket calls through <code>socketcall</code> system call</a> instead of calling e.g. <code>sendmsg</code> directly.
In addition Android has various default seccomp filters, crashing an app when e.g. not going through the required <code>socketcall</code> system call.
<a href="https://github.com/quinn-rs/quinn/pull/1966">The combination of the two</a> did cost me a couple of days, resulting in <a href="https://github.com/quinn-rs/quinn/pull/1966">this (basically single line) change in quinn-udp</a>.</p>
<p>On Android API level 25 and below, calling <code>sendmsg</code> with an ECN bit set <a href="https://github.com/quinn-rs/quinn/pull/1975">results in an error <code>EINVAL</code></a>.
<a href="https://github.com/quinn-rs/quinn/pull/2079">quinn-udp will now simply retry on <code>EINVAL</code></a> disabling various optional settings (e.g. ECN) on the second attempt.</p>
<p>Great benefit of the Quinn community is that Firefox will benefit from any improvements made to quinn-udp.
For example <a href="https://github.com/quinn-rs/quinn/pull/2050">this excellent find by Thomas</a> where Android in some cases would complain if we did a <code>GSO</code> with a single segment only.</p>
<h2 id="explicit-congestion-notifications-ecn">Explicit congestion notifications (ECN)</h2>
<p>With Firefox using modern system calls across all major operating systems, a nice additional benefit is the ability to send and receive ancillary data like <a href="https://en.wikipedia.org/wiki/Explicit_Congestion_Notification">IP ECN</a>.
This too <a href="https://github.com/quinn-rs/quinn/pull/1765">came with some minor surprises</a>, but QUIC ECN in Firefox is well on its way now.
Firefox Nightly telemetry shows around <a href="https://glam.telemetry.mozilla.org/fog/probe/networking_http_3_ecn_path_capability/explore?">50% of all QUIC connections running on ECN outbound capable paths</a>.
With <a href="https://datatracker.ietf.org/doc/rfc9330/">L4S</a> and thus ECN becoming more and more relevant in today’s Internet, this is a great step forward.</p>
<h2 id="summary">Summary</h2>
<p>We successfully replaced Firefox’s QUIC UDP I/O stack with a modern memory-safe implementation using quinn-udp.
Instead of limited and dated system calls like <code>sendto</code> and <code>recvfrom</code>, Firefox now uses modern OS specific system calls across all major platforms, resulting in HTTP/3 QUIC throughput improvements when CPU bound, and enabling QUIC ECN support across all major platforms.
Some OS specific optimizations still need more work, e.g. <code>USO</code> and <code>URO</code> on Windows.
That said, especially given QUIC’s growing adoption and thus increased UDP usage, I am optimistic that OS and driver support will continue to improve.</p>


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Context is the bottleneck for coding agents now (113 pts)]]></title>
            <link>https://runnercode.com/blog/context-is-the-bottleneck-for-coding-agents-now</link>
            <guid>45387374</guid>
            <pubDate>Fri, 26 Sep 2025 15:06:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://runnercode.com/blog/context-is-the-bottleneck-for-coding-agents-now">https://runnercode.com/blog/context-is-the-bottleneck-for-coding-agents-now</a>, See on <a href="https://news.ycombinator.com/item?id=45387374">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Intelligence is rapidly improving with each model release. Just last week it was announced that OpenAI got a perfect score on the 2025 ICPC programming contest, beating every single human contestant. They achieved this using a version (presumably a very high compute version, but still) of their publicly available GPT-5 model.</p><p>And yet, coding agents are nowhere near capable of replacing software developers. Why is that?</p><p>I’m going to argue that the limiting factor is no longer raw intelligence, but rather context. Existing coding agents simply do not have enough context about the problems they’re being asked to solve. This severely limits how long they can work effectively without human guidance.</p><h2>Intelligence and context</h2><p>How autonomous are existing coding agents? Let’s think about autonomy as a spectrum and see how far along that spectrum we are.</p><div><p><strong>Level 1 - A few lines of code</strong><br>This is what autocomplete does, and it works very well.</p><p><strong>Level 2 - One commit</strong><br>Cursor and Claude Code work well for tasks in this size range.</p><p><strong>Level 3 - One PR</strong><br>Devin and other async agents are built to tackle tasks of this size. But do they work reliably? Only on relatively simple tasks.</p><p><strong>Level 4 - Major feature or refactor</strong><br>Doing this autonomously on an existing codebase is beyond the reach of current agents.</p><p><strong>Level 5 - Entire codebase</strong><br>This is what the vibe coding products like Lovable and Replit do now, but it only works because they can start from scratch. The problem is that they usually hit a wall well before they get to a production-ready application.</p></div><p>I’d say Level 2 is all we can reliably do on production codebases right now. And even that requires substantial human guidance and review. What will it take to move further along the autonomy spectrum, without sacrificing quality?</p><p>When an agent fails at a task, the cause is usually one of two things. It’s either an intelligence failure, or it’s a context failure. Either the model didn’t have the information it needed, or it didn’t have the mental horsepower to process that information properly. There are other aspects that can affect performance, such as taste, but if we’re just talking about whether the agent succeeds or fails at a task, it’s sufficient to just consider intelligence and context. Also note that I’m including general world knowledge as part of intelligence, both for simplicity and because I think it’s hard to fully separate those two out.</p><p>Programming competitions are competitions of intelligence. The entire context needed to solve a problem is provided in the problem statement itself. There’s no existing codebase to understand, no business requirements to consider, and no unwritten development processes you need to follow.</p><p>The superhuman ICPC performance we saw this week, as well as the IOI gold medal-level performances from last month, strongly suggest that the raw intelligence and general programming knowledge of frontier models is sufficient to automate most software engineering work.</p><p>Now these performances were achieved using models that are quite a bit stronger than the models used on a daily basis by developers, like Claude 4 and GPT-5. So we can’t quite say that lack of intelligence is never a cause of failure in current coding agents. They still do some pretty dumb stuff sometimes. But as models improve, more and more of the failures in agentic coding are failures of context, not failures of intelligence.</p><h2>What context does a coding agent need?</h2><p>Context isn’t just code. It’s also specs, dev practices, conversations, etc. When human developers write code, they’re drawing from a reservoir of implicit knowledge that goes far beyond what’s visible in the codebase itself. Current coding agents are operating with maybe 20% of this context, at best.</p><p>What context does an agent need to reliably operate autonomously and ship code that’s as good or better than human developers? It’s the same things a human developer needs.</p><p>There are the basics:</p><div><p><strong>It needs to be able to access all code files</strong><br>Most coding agents can already do this.</p><p><strong>It needs to be able to access documentation</strong><br>Most coding agents can do this if set up properly.</p><p><strong>It needs to be able to run code and see the output</strong><br>Most coding agents can already do this pretty well.</p></div><p>And then there are the more subtle forms of context:</p><div><p><strong>It needs to have a high-level understanding of how the codebase is organized and where different code lives</strong><br>This is important for efficient execution and also for making sure you don’t miss things. Most tool-based agents, like Cursor and Claude Code, do not have this. Some agents are provided with something along these lines.</p><p><strong>It needs to understand all of the existing architectural patterns and conventions in the codebase</strong><br>Every codebase has its own dialect. Maybe you always use dependency injection in a specific way. Perhaps there’s an unwritten rule about where business logic lives versus presentation logic. Maybe you have a specific pattern for handling async operations that evolved organically over three years.</p><p>Current agents struggle here because many of these patterns are emergent properties of the codebase that aren’t documented in any single place. They’re distributed across thousands of commits, pull requests, and code reviews.</p><p><strong>It needs to understand why things were done the way they were</strong><br>Why does the authentication system work the way it does? Because two years ago, there was a security incident that led to a complete redesign. Why don’t we use library X even though it seems perfect? Because it caused production issues in 2022.</p><p>This tribal knowledge lives in Slack threads, meeting notes, incident post-mortems, and developers’ heads.</p><p><strong>It needs to understand development and deployment practices</strong><br>Testing expectations, style and comment guidelines, etc.</p><p>Every team has unwritten rules about how code ships. Maybe you deploy to staging-east first because of a subtle dependency. Perhaps certain tests look weird because they’re working around a known race condition. The CI/CD pipeline has manual approval steps that seem redundant but prevent real disasters that happened in the past.</p><p>Current agents can read your test configs and deployment scripts, but they don’t understand the “why” behind them. They might remove a “redundant” check that’s actually preventing a production issue, or follow official docs that everyone knows are outdated.</p><p><strong>It needs to understand product and business requirements</strong><br>Code doesn’t exist in a vacuum. That seemingly arbitrary validation rule? It’s there because of a regulatory requirement in the EU market. That weird data transformation? It’s handling an edge case for your biggest enterprise customer.</p><p>I don’t know of any coding agents that are plugged into this kind of data right now.</p></div><p>Notice how all the basic forms of context use the word “access” while the more subtle forms of context use the word “understand.” This is important. Most of this context is not written down in a single document that the agent can just read. To the extent that it’s written down at all, it’s often scattered across many different files and apps. Some of that information will be conflicting and out of date. Giving the agent this context is not as simple as just giving it an MCP connector to your Google Drive and Linear accounts. The information needs to be processed and synthesized by the agent.</p><p>What does this mean for coding agents?</p><p>First, we need to give them access to way more context. Much of this new context will require sophisticated preprocessing to make it usable, so this is not an easy problem.</p><p>Second, not everything is written down. That means experienced human developers will still need to fill in the gaps for a very long time to come.</p><p>Third, agents need to learn to identify when they’re missing context so they can ask for human guidance. Right now they seem to be trained to just plow forward with what they have.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A little notebook for learning linear algebra with Python (160 pts)]]></title>
            <link>https://little-book-of.github.io/linear-algebra/books/en-US/lab.html</link>
            <guid>45384617</guid>
            <pubDate>Fri, 26 Sep 2025 09:46:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://little-book-of.github.io/linear-algebra/books/en-US/lab.html">https://little-book-of.github.io/linear-algebra/books/en-US/lab.html</a>, See on <a href="https://news.ycombinator.com/item?id=45384617">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="quarto-document-content">




<section id="chapter-1.-vectors-scalars-and-geometry">
<h2 data-anchor-id="chapter-1.-vectors-scalars-and-geometry">Chapter 1. Vectors, scalars, and geometry</h2>
<section id="scalars-vectors-and-coordinate-systems">
<h3 data-anchor-id="scalars-vectors-and-coordinate-systems">1. Scalars, Vectors, and Coordinate Systems</h3>
<p>Let’s get our hands dirty! This lab is about playing with the <em>building blocks</em> of linear algebra: scalars and vectors. Think of a scalar as just a plain number, like <code>3</code> or <code>-1.5</code>. A vector is a small list of numbers, which you can picture as an arrow in space.</p>
<p>We’ll use Python (with NumPy) to explore them. Don’t worry if this is your first time with NumPy - we’ll go slowly.</p>
<section id="set-up-your-lab">
<h4 data-anchor-id="set-up-your-lab">Set Up Your Lab</h4>

<p>That’s it - we’re ready! NumPy is the main tool we’ll use for linear algebra.</p>
</section>
<section id="step-by-step-code-walkthrough">
<h4 data-anchor-id="step-by-step-code-walkthrough">Step-by-Step Code Walkthrough</h4>
<p>Scalars are just numbers.</p>
<div id="8def0671" data-execution_count="2"><pre><code><span id="cb2-1">a <span>=</span> <span>5</span>       <span># a scalar</span></span>
<span id="cb2-2">b <span>=</span> <span>-</span><span>2.5</span>    <span># another scalar</span></span>
<span id="cb2-3"></span>
<span id="cb2-4"><span>print</span>(a <span>+</span> b)   <span># add them</span></span>
<span id="cb2-5"><span>print</span>(a <span>*</span> b)   <span># multiply them</span></span></code></pre></div>
<p>Vectors are lists of numbers.</p>
<div id="846e01a0" data-execution_count="3"><pre><code><span id="cb4-1">v <span>=</span> np.array([<span>2</span>, <span>3</span>])      <span># a vector in 2D</span></span>
<span id="cb4-2">w <span>=</span> np.array([<span>1</span>, <span>-</span><span>1</span>, <span>4</span>])  <span># a vector in 3D</span></span>
<span id="cb4-3"></span>
<span id="cb4-4"><span>print</span>(v)</span>
<span id="cb4-5"><span>print</span>(w)</span></code></pre></div>
<p>Coordinates tell us where we are. Think of <code>[2, 3]</code> as “go 2 steps in the x-direction, 3 steps in the y-direction.”</p>
<p>We can even <em>draw</em> it:</p>
<div id="87e544fb" data-execution_count="4">
<div id="cb6"><pre><code><span id="cb6-1"><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb6-2"></span>
<span id="cb6-3"><span># plot vector v</span></span>
<span id="cb6-4">plt.quiver(<span>0</span>, <span>0</span>, v[<span>0</span>], v[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'r'</span>)</span>
<span id="cb6-5">plt.xlim(<span>0</span>, <span>4</span>)</span>
<span id="cb6-6">plt.ylim(<span>0</span>, <span>4</span>)</span>
<span id="cb6-7">plt.grid()</span>
<span id="cb6-8">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-5-output-1.png" width="581" height="416"></p>
</figure>
</div>
</div>
<p>This makes a little arrow from the origin <code>(0,0)</code> to <code>(2,3)</code>.</p>
</section>
<section id="try-it-yourself">
<h4 data-anchor-id="try-it-yourself">Try It Yourself</h4>
<ol type="1">
<li>Change the vector <code>v</code> to <code>[4, 1]</code>. Where does the arrow point now?</li>
<li>Try making a 3D vector with 4 numbers, like <code>[1, 2, 3, 4]</code>. What happens?</li>
<li>Replace <code>np.array([2,3])</code> with <code>np.array([0,0])</code>. What does the arrow look like?</li>
</ol>
</section>
</section>
<section id="vector-notation-components-and-arrows">
<h3 data-anchor-id="vector-notation-components-and-arrows">2. Vector Notation, Components, and Arrows</h3>
<p>In this lab, we’ll practice reading, writing, and visualizing vectors in different ways. A vector can look simple at first - just a list of numbers - but how we <em>write</em> it and how we <em>interpret</em> it really matters. This is where notation and components come into play.</p>
<p>A vector has:</p>
<ul>
<li>A symbol (we might call it <code>v</code>, <code>w</code>, or even <code>→AB</code> in geometry).</li>
<li>Components (the individual numbers, like <code>2</code> and <code>3</code> in <code>[2, 3]</code>).</li>
<li>An arrow picture (a geometric way to see the vector as a directed line segment).</li>
</ul>
<p>Let’s see all three in action with Python.</p>
<section id="set-up-your-lab-1">
<h4 data-anchor-id="set-up-your-lab-1">Set Up Your Lab</h4>
<div id="ed61dc56" data-execution_count="5"><pre><code><span id="cb7-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb7-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-1">
<h4 data-anchor-id="step-by-step-code-walkthrough-1">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Writing vectors in Python</li>
</ol>
<div id="ac675ca4" data-execution_count="6"><pre><code><span id="cb8-1"><span># Two-dimensional vector</span></span>
<span id="cb8-2">v <span>=</span> np.array([<span>2</span>, <span>3</span>])</span>
<span id="cb8-3"></span>
<span id="cb8-4"><span># Three-dimensional vector</span></span>
<span id="cb8-5">w <span>=</span> np.array([<span>1</span>, <span>-</span><span>1</span>, <span>4</span>])</span>
<span id="cb8-6"></span>
<span id="cb8-7"><span>print</span>(<span>"v ="</span>, v)</span>
<span id="cb8-8"><span>print</span>(<span>"w ="</span>, w)</span></code></pre></div>
<p>Here <code>v</code> has components <code>(2, 3)</code> and <code>w</code> has components <code>(1, -1, 4)</code>.</p>
<ol start="2" type="1">
<li>Accessing components Each number in the vector is a <em>component</em>. We can pick them out using indexing.</li>
</ol>
<div id="4dfb373e" data-execution_count="7">
<div id="cb10"><pre><code><span id="cb10-1"><span>print</span>(<span>"First component of v:"</span>, v[<span>0</span>])</span>
<span id="cb10-2"><span>print</span>(<span>"Second component of v:"</span>, v[<span>1</span>])</span></code></pre></div>
<div>
<pre><code>First component of v: 2
Second component of v: 3</code></pre>
</div>
</div>
<p>Notice: in Python, indices start at <code>0</code>, so <code>v[0]</code> is the <em>first</em> component.</p>
<ol start="3" type="1">
<li>Visualizing vectors as arrows In 2D, it’s easy to draw a vector from the origin <code>(0,0)</code> to its endpoint <code>(x,y)</code>.</li>
</ol>
<div id="925b4653" data-execution_count="8">
<div id="cb12"><pre><code><span id="cb12-1">plt.quiver(<span>0</span>, <span>0</span>, v[<span>0</span>], v[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'r'</span>)</span>
<span id="cb12-2">plt.xlim(<span>-</span><span>1</span>, <span>4</span>)</span>
<span id="cb12-3">plt.ylim(<span>-</span><span>2</span>, <span>4</span>)</span>
<span id="cb12-4">plt.axhline(<span>0</span>, color<span>=</span><span>'black'</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb12-5">plt.axvline(<span>0</span>, color<span>=</span><span>'black'</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb12-6">plt.grid()</span>
<span id="cb12-7">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-9-output-1.png" width="573" height="416"></p>
</figure>
</div>
</div>
<p>This shows vector v as a red arrow from <code>(0,0)</code> to <code>(2,3)</code>.</p>
<ol start="4" type="1">
<li>Drawing multiple vectors We can plot several arrows at once to compare them.</li>
</ol>
<div id="7ce280f5" data-execution_count="9">
<div id="cb13"><pre><code><span id="cb13-1">u <span>=</span> np.array([<span>3</span>, <span>1</span>])</span>
<span id="cb13-2">z <span>=</span> np.array([<span>-</span><span>1</span>, <span>2</span>])</span>
<span id="cb13-3"></span>
<span id="cb13-4"><span># Draw v, u, z in different colors</span></span>
<span id="cb13-5">plt.quiver(<span>0</span>, <span>0</span>, v[<span>0</span>], v[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'r'</span>, label<span>=</span><span>'v'</span>)</span>
<span id="cb13-6">plt.quiver(<span>0</span>, <span>0</span>, u[<span>0</span>], u[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'b'</span>, label<span>=</span><span>'u'</span>)</span>
<span id="cb13-7">plt.quiver(<span>0</span>, <span>0</span>, z[<span>0</span>], z[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'g'</span>, label<span>=</span><span>'z'</span>)</span>
<span id="cb13-8"></span>
<span id="cb13-9">plt.xlim(<span>-</span><span>2</span>, <span>4</span>)</span>
<span id="cb13-10">plt.ylim(<span>-</span><span>2</span>, <span>4</span>)</span>
<span id="cb13-11">plt.axhline(<span>0</span>, color<span>=</span><span>'black'</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb13-12">plt.axvline(<span>0</span>, color<span>=</span><span>'black'</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb13-13">plt.grid()</span>
<span id="cb13-14">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-10-output-1.png" width="573" height="416"></p>
</figure>
</div>
</div>
<p>Now you’ll see three arrows starting at the same point, each pointing in a different direction.</p>
</section>
<section id="try-it-yourself-1">
<h4 data-anchor-id="try-it-yourself-1">Try It Yourself</h4>
<ol type="1">
<li>Change <code>v</code> to <code>[5, 0]</code>. What does the arrow look like now?</li>
<li>Try a vector like <code>[0, -3]</code>. Which axis does it line up with?</li>
<li>Make a new vector <code>q = np.array([2, 0, 0])</code>. What happens if you try to plot it with <code>plt.quiver</code> in 2D?</li>
</ol>
</section>
</section>
<section id="vector-addition-and-scalar-multiplication">
<h3 data-anchor-id="vector-addition-and-scalar-multiplication">3. Vector Addition and Scalar Multiplication</h3>
<p>In this lab, we’ll explore the two most fundamental operations you can perform with vectors: adding them together and scaling them by a number (a scalar). These operations form the basis of everything else in linear algebra, from geometry to machine learning. Understanding how they work, both in code and visually, is key to building intuition.</p>
<section id="set-up-your-lab-2">
<h4 data-anchor-id="set-up-your-lab-2">Set Up Your Lab</h4>
<div id="1d4a49d6" data-execution_count="10"><pre><code><span id="cb14-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb14-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-2">
<h4 data-anchor-id="step-by-step-code-walkthrough-2">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Vector addition When you add two vectors, you simply add their components one by one.</li>
</ol>
<div id="dc42f49c" data-execution_count="11"><pre><code><span id="cb15-1">v <span>=</span> np.array([<span>2</span>, <span>3</span>])</span>
<span id="cb15-2">u <span>=</span> np.array([<span>1</span>, <span>-</span><span>1</span>])</span>
<span id="cb15-3"></span>
<span id="cb15-4">sum_vector <span>=</span> v <span>+</span> u</span>
<span id="cb15-5"><span>print</span>(<span>"v + u ="</span>, sum_vector)</span></code></pre></div>
<p>Here, <code>(2,3) + (1,-1) = (3,2)</code>.</p>
<ol start="2" type="1">
<li>Visualizing vector addition (tip-to-tail method) Graphically, vector addition means placing the tail of one vector at the head of the other. The resulting vector goes from the start of the first to the end of the second.</li>
</ol>
<div id="bad2574c" data-execution_count="12">
<div id="cb17"><pre><code><span id="cb17-1">plt.quiver(<span>0</span>, <span>0</span>, v[<span>0</span>], v[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'r'</span>, label<span>=</span><span>'v'</span>)</span>
<span id="cb17-2">plt.quiver(v[<span>0</span>], v[<span>1</span>], u[<span>0</span>], u[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'b'</span>, label<span>=</span><span>'u placed at end of v'</span>)</span>
<span id="cb17-3">plt.quiver(<span>0</span>, <span>0</span>, sum_vector[<span>0</span>], sum_vector[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'g'</span>, label<span>=</span><span>'v + u'</span>)</span>
<span id="cb17-4"></span>
<span id="cb17-5">plt.xlim(<span>-</span><span>1</span>, <span>5</span>)</span>
<span id="cb17-6">plt.ylim(<span>-</span><span>2</span>, <span>5</span>)</span>
<span id="cb17-7">plt.axhline(<span>0</span>, color<span>=</span><span>'black'</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb17-8">plt.axvline(<span>0</span>, color<span>=</span><span>'black'</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb17-9">plt.grid()</span>
<span id="cb17-10">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-13-output-1.png" width="573" height="416"></p>
</figure>
</div>
</div>
<p>The green arrow is the result of adding <code>v</code> and <code>u</code>.</p>
<ol start="3" type="1">
<li>Scalar multiplication Multiplying a vector by a scalar stretches or shrinks it. If the scalar is negative, the vector flips direction.</li>
</ol>
<div id="785c4156" data-execution_count="13">
<div id="cb18"><pre><code><span id="cb18-1">c <span>=</span> <span>2</span></span>
<span id="cb18-2">scaled_v <span>=</span> c <span>*</span> v</span>
<span id="cb18-3"><span>print</span>(<span>"2 * v ="</span>, scaled_v)</span>
<span id="cb18-4"></span>
<span id="cb18-5">d <span>=</span> <span>-</span><span>1</span></span>
<span id="cb18-6">scaled_v_neg <span>=</span> d <span>*</span> v</span>
<span id="cb18-7"><span>print</span>(<span>"-1 * v ="</span>, scaled_v_neg)</span></code></pre></div>
<div>
<pre><code>2 * v = [4 6]
-1 * v = [-2 -3]</code></pre>
</div>
</div>
<p>So <code>2 * (2,3) = (4,6)</code> and <code>-1 * (2,3) = (-2,-3)</code>.</p>
<ol start="4" type="1">
<li>Visualizing scalar multiplication</li>
</ol>
<div id="3a2ffac7" data-execution_count="14">
<div id="cb20"><pre><code><span id="cb20-1">plt.quiver(<span>0</span>, <span>0</span>, v[<span>0</span>], v[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'r'</span>, label<span>=</span><span>'v'</span>)</span>
<span id="cb20-2">plt.quiver(<span>0</span>, <span>0</span>, scaled_v[<span>0</span>], scaled_v[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'b'</span>, label<span>=</span><span>'2 * v'</span>)</span>
<span id="cb20-3">plt.quiver(<span>0</span>, <span>0</span>, scaled_v_neg[<span>0</span>], scaled_v_neg[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'g'</span>, label<span>=</span><span>'-1 * v'</span>)</span>
<span id="cb20-4"></span>
<span id="cb20-5">plt.xlim(<span>-</span><span>5</span>, <span>5</span>)</span>
<span id="cb20-6">plt.ylim(<span>-</span><span>5</span>, <span>7</span>)</span>
<span id="cb20-7">plt.axhline(<span>0</span>, color<span>=</span><span>'black'</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb20-8">plt.axvline(<span>0</span>, color<span>=</span><span>'black'</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb20-9">plt.grid()</span>
<span id="cb20-10">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-15-output-1.png" width="569" height="411"></p>
</figure>
</div>
</div>
<p>Here, the blue arrow is twice as long as the red arrow, while the green arrow points in the opposite direction.</p>
<ol start="5" type="1">
<li>Combining both operations We can scale vectors and then add them. This is called a linear combination (and it’s the foundation for the next section).</li>
</ol>
<div id="dbfa1cd3" data-execution_count="15"><pre><code><span id="cb21-1">combo <span>=</span> <span>3</span><span>*</span>v <span>+</span> (<span>-</span><span>2</span>)<span>*</span>u</span>
<span id="cb21-2"><span>print</span>(<span>"3*v - 2*u ="</span>, combo)</span></code></pre></div>
</section>
<section id="try-it-yourself-2">
<h4 data-anchor-id="try-it-yourself-2">Try It Yourself</h4>
<ol type="1">
<li>Replace <code>c = 2</code> with <code>c = 0.5</code>. What happens to the vector?</li>
<li>Try adding three vectors: <code>v + u + np.array([-1,2])</code>. Can you predict the result before printing?</li>
<li>Visualize <code>3*v + 2*u</code> using arrows. How does it compare to just <code>v + u</code>?</li>
</ol>
</section>
</section>
<section id="linear-combinations-and-span">
<h3 data-anchor-id="linear-combinations-and-span">4. Linear Combinations and Span</h3>
<p>Now that we know how to add vectors and scale them, we can combine these two moves to create linear combinations. A linear combination is just a recipe: multiply vectors by scalars, then add them together. The set of all possible results you can get from such recipes is called the span.</p>
<p>This idea is powerful because span tells us what directions and regions of space we can reach using given vectors.</p>
<section id="set-up-your-lab-3">
<h4 data-anchor-id="set-up-your-lab-3">Set Up Your Lab</h4>
<div id="42faf404" data-execution_count="16"><pre><code><span id="cb23-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb23-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-3">
<h4 data-anchor-id="step-by-step-code-walkthrough-3">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Linear combinations in Python</li>
</ol>
<div id="3cbd9e33" data-execution_count="17">
<div id="cb24"><pre><code><span id="cb24-1">v <span>=</span> np.array([<span>2</span>, <span>1</span>])</span>
<span id="cb24-2">u <span>=</span> np.array([<span>1</span>, <span>3</span>])</span>
<span id="cb24-3"></span>
<span id="cb24-4">combo1 <span>=</span> <span>2</span><span>*</span>v <span>+</span> <span>3</span><span>*</span>u</span>
<span id="cb24-5">combo2 <span>=</span> <span>-</span><span>1</span><span>*</span>v <span>+</span> <span>4</span><span>*</span>u</span>
<span id="cb24-6"></span>
<span id="cb24-7"><span>print</span>(<span>"2*v + 3*u ="</span>, combo1)</span>
<span id="cb24-8"><span>print</span>(<span>"-v + 4*u ="</span>, combo2)</span></code></pre></div>
<div>
<pre><code>2*v + 3*u = [ 7 11]
-v + 4*u = [ 2 11]</code></pre>
</div>
</div>
<p>Here, we multiplied and added vectors using scalars. Each result is a new vector.</p>
<ol start="2" type="1">
<li>Visualizing linear combinations Let’s plot <code>v</code>, <code>u</code>, and their combinations.</li>
</ol>
<div id="1f4b0ecb" data-execution_count="18">
<div id="cb26"><pre><code><span id="cb26-1">plt.quiver(<span>0</span>, <span>0</span>, v[<span>0</span>], v[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'r'</span>, label<span>=</span><span>'v'</span>)</span>
<span id="cb26-2">plt.quiver(<span>0</span>, <span>0</span>, u[<span>0</span>], u[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'b'</span>, label<span>=</span><span>'u'</span>)</span>
<span id="cb26-3">plt.quiver(<span>0</span>, <span>0</span>, combo1[<span>0</span>], combo1[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'g'</span>, label<span>=</span><span>'2v + 3u'</span>)</span>
<span id="cb26-4">plt.quiver(<span>0</span>, <span>0</span>, combo2[<span>0</span>], combo2[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'m'</span>, label<span>=</span><span>'-v + 4u'</span>)</span>
<span id="cb26-5"></span>
<span id="cb26-6">plt.xlim(<span>-</span><span>5</span>, <span>10</span>)</span>
<span id="cb26-7">plt.ylim(<span>-</span><span>5</span>, <span>10</span>)</span>
<span id="cb26-8">plt.axhline(<span>0</span>, color<span>=</span><span>'black'</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb26-9">plt.axvline(<span>0</span>, color<span>=</span><span>'black'</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb26-10">plt.grid()</span>
<span id="cb26-11">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-19-output-1.png" width="577" height="416"></p>
</figure>
</div>
</div>
<p>This shows how new arrows can be generated from scaling and adding the original ones.</p>
<ol start="3" type="1">
<li>Exploring the span The span of two 2D vectors is either:</li>
</ol>
<ul>
<li>A line (if one is a multiple of the other).</li>
<li>The whole 2D plane (if they are independent).</li>
</ul>
<div id="63bfdd3f" data-execution_count="19">
<div id="cb27"><pre><code><span id="cb27-1"><span># Generate many combinations</span></span>
<span id="cb27-2">coeffs <span>=</span> <span>range</span>(<span>-</span><span>5</span>, <span>6</span>)</span>
<span id="cb27-3">points <span>=</span> []</span>
<span id="cb27-4"><span>for</span> a <span>in</span> coeffs:</span>
<span id="cb27-5">    <span>for</span> b <span>in</span> coeffs:</span>
<span id="cb27-6">        point <span>=</span> a<span>*</span>v <span>+</span> b<span>*</span>u</span>
<span id="cb27-7">        points.append(point)</span>
<span id="cb27-8"></span>
<span id="cb27-9">points <span>=</span> np.array(points)</span>
<span id="cb27-10"></span>
<span id="cb27-11">plt.scatter(points[:,<span>0</span>], points[:,<span>1</span>], s<span>=</span><span>10</span>, color<span>=</span><span>'gray'</span>)</span>
<span id="cb27-12">plt.quiver(<span>0</span>, <span>0</span>, v[<span>0</span>], v[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'r'</span>)</span>
<span id="cb27-13">plt.quiver(<span>0</span>, <span>0</span>, u[<span>0</span>], u[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'b'</span>)</span>
<span id="cb27-14"></span>
<span id="cb27-15">plt.xlim(<span>-</span><span>10</span>, <span>10</span>)</span>
<span id="cb27-16">plt.ylim(<span>-</span><span>10</span>, <span>10</span>)</span>
<span id="cb27-17">plt.axhline(<span>0</span>, color<span>=</span><span>'black'</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb27-18">plt.axvline(<span>0</span>, color<span>=</span><span>'black'</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb27-19">plt.grid()</span>
<span id="cb27-20">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-20-output-1.png" width="605" height="416"></p>
</figure>
</div>
</div>
<p>The gray dots show all reachable points with combinations of <code>v</code> and <code>u</code>.</p>
<ol start="4" type="1">
<li>Special case: dependent vectors</li>
</ol>
<div id="ecbce6ba" data-execution_count="20">
<div id="cb28"><pre><code><span id="cb28-1">w <span>=</span> np.array([<span>4</span>, <span>2</span>])  <span># notice w = 2*v</span></span>
<span id="cb28-2">coeffs <span>=</span> <span>range</span>(<span>-</span><span>5</span>, <span>6</span>)</span>
<span id="cb28-3">points <span>=</span> []</span>
<span id="cb28-4"><span>for</span> a <span>in</span> coeffs:</span>
<span id="cb28-5">    <span>for</span> b <span>in</span> coeffs:</span>
<span id="cb28-6">        points.append(a<span>*</span>v <span>+</span> b<span>*</span>w)</span>
<span id="cb28-7"></span>
<span id="cb28-8">points <span>=</span> np.array(points)</span>
<span id="cb28-9"></span>
<span id="cb28-10">plt.scatter(points[:,<span>0</span>], points[:,<span>1</span>], s<span>=</span><span>10</span>, color<span>=</span><span>'gray'</span>)</span>
<span id="cb28-11">plt.quiver(<span>0</span>, <span>0</span>, v[<span>0</span>], v[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'r'</span>)</span>
<span id="cb28-12">plt.quiver(<span>0</span>, <span>0</span>, w[<span>0</span>], w[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'b'</span>)</span>
<span id="cb28-13"></span>
<span id="cb28-14">plt.xlim(<span>-</span><span>10</span>, <span>10</span>)</span>
<span id="cb28-15">plt.ylim(<span>-</span><span>10</span>, <span>10</span>)</span>
<span id="cb28-16">plt.axhline(<span>0</span>, color<span>=</span><span>'black'</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb28-17">plt.axvline(<span>0</span>, color<span>=</span><span>'black'</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb28-18">plt.grid()</span>
<span id="cb28-19">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-21-output-1.png" width="605" height="416"></p>
</figure>
</div>
</div>
<p>Here, the span collapses to a line because <code>w</code> is just a scaled copy of <code>v</code>.</p>
</section>
<section id="try-it-yourself-3">
<h4 data-anchor-id="try-it-yourself-3">Try It Yourself</h4>
<ol type="1">
<li>Replace <code>u = [1,3]</code> with <code>u = [-1,2]</code>. What does the span look like?</li>
<li>Try three vectors in 2D (e.g., <code>v, u, w</code>). Do you get more than the whole plane?</li>
<li>Experiment with 3D vectors. Use <code>np.array([x,y,z])</code> and check whether different vectors span a plane or all of space.</li>
</ol>
</section>
</section>
<section id="length-norm-and-distance">
<h3 data-anchor-id="length-norm-and-distance">5. Length (Norm) and Distance</h3>
<p>In this lab, we’ll measure how big a vector is (its length, also called its norm) and how far apart two vectors are (their distance). These ideas connect algebra to geometry: when we compute a norm, we’re measuring the size of an arrow; when we compute a distance, we’re measuring the gap between two points in space.</p>
<section id="set-up-your-lab-4">
<h4 data-anchor-id="set-up-your-lab-4">Set Up Your Lab</h4>
<div id="3b0770ab" data-execution_count="21"><pre><code><span id="cb29-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb29-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-4">
<h4 data-anchor-id="step-by-step-code-walkthrough-4">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Vector length (norm) in 2D The length of a vector is computed using the Pythagorean theorem. For a vector <code>(x, y)</code>, the length is <code>sqrt(x² + y²)</code>.</li>
</ol>
<div id="5fcb476e" data-execution_count="22"><pre><code><span id="cb30-1">v <span>=</span> np.array([<span>3</span>, <span>4</span>])</span>
<span id="cb30-2">length <span>=</span> np.linalg.norm(v)</span>
<span id="cb30-3"><span>print</span>(<span>"Length of v ="</span>, length)</span></code></pre></div>
<p>This prints <code>5.0</code>, because <code>(3,4)</code> forms a right triangle with sides 3 and 4, and <code>sqrt(3²+4²)=5</code>.</p>
<ol start="2" type="1">
<li>Manual calculation vs NumPy</li>
</ol>
<div id="be0dc49c" data-execution_count="23">
<div id="cb32"><pre><code><span id="cb32-1">manual_length <span>=</span> (v[<span>0</span>]<span>**</span><span>2</span> <span>+</span> v[<span>1</span>]<span>**</span><span>2</span>)<span>**</span><span>0.5</span></span>
<span id="cb32-2"><span>print</span>(<span>"Manual length ="</span>, manual_length)</span>
<span id="cb32-3"><span>print</span>(<span>"NumPy length ="</span>, np.linalg.norm(v))</span></code></pre></div>
<div>
<pre><code>Manual length = 5.0
NumPy length = 5.0</code></pre>
</div>
</div>
<p>Both give the same result.</p>
<ol start="3" type="1">
<li>Visualizing vector length</li>
</ol>
<div id="62751078" data-execution_count="24">
<div id="cb34"><pre><code><span id="cb34-1">plt.quiver(<span>0</span>, <span>0</span>, v[<span>0</span>], v[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'r'</span>)</span>
<span id="cb34-2">plt.xlim(<span>0</span>, <span>5</span>)</span>
<span id="cb34-3">plt.ylim(<span>0</span>, <span>5</span>)</span>
<span id="cb34-4">plt.axhline(<span>0</span>, color<span>=</span><span>'black'</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb34-5">plt.axvline(<span>0</span>, color<span>=</span><span>'black'</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb34-6">plt.text(v[<span>0</span>]<span>/</span><span>2</span>, v[<span>1</span>]<span>/</span><span>2</span>, <span>f"Length=</span><span>{</span>length<span>}</span><span>"</span>, fontsize<span>=</span><span>10</span>, color<span>=</span><span>'blue'</span>)</span>
<span id="cb34-7">plt.grid()</span>
<span id="cb34-8">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-25-output-1.png" width="562" height="416"></p>
</figure>
</div>
</div>
<p>You’ll see the arrow <code>(3,4)</code> with its length labeled.</p>
<ol start="4" type="1">
<li>Distance between two vectors The distance between <code>v</code> and another vector <code>u</code> is the length of their difference: <code>‖v - u‖</code>.</li>
</ol>
<div id="2a74f2c7" data-execution_count="25">
<div id="cb35"><pre><code><span id="cb35-1">u <span>=</span> np.array([<span>0</span>, <span>0</span>])   <span># the origin</span></span>
<span id="cb35-2">dist <span>=</span> np.linalg.norm(v <span>-</span> u)</span>
<span id="cb35-3"><span>print</span>(<span>"Distance between v and u ="</span>, dist)</span></code></pre></div>
<div>
<pre><code>Distance between v and u = 5.0</code></pre>
</div>
</div>
<p>Since <code>u</code> is the origin, this is just the length of <code>v</code>.</p>
<ol start="5" type="1">
<li>A more interesting distance</li>
</ol>
<div id="4d218420" data-execution_count="26">
<div id="cb37"><pre><code><span id="cb37-1">u <span>=</span> np.array([<span>1</span>, <span>1</span>])</span>
<span id="cb37-2">dist <span>=</span> np.linalg.norm(v <span>-</span> u)</span>
<span id="cb37-3"><span>print</span>(<span>"Distance between v and u ="</span>, dist)</span></code></pre></div>
<div>
<pre><code>Distance between v and u = 3.605551275463989</code></pre>
</div>
</div>
<p>This measures how far <code>(3,4)</code> is from <code>(1,1)</code>.</p>
<ol start="6" type="1">
<li>Visualizing distance between points</li>
</ol>
<div id="d3c95c45" data-execution_count="27">
<div id="cb39"><pre><code><span id="cb39-1">plt.scatter([v[<span>0</span>], u[<span>0</span>]], [v[<span>1</span>], u[<span>1</span>]], color<span>=</span>[<span>'red'</span>,<span>'blue'</span>])</span>
<span id="cb39-2">plt.plot([v[<span>0</span>], u[<span>0</span>]], [v[<span>1</span>], u[<span>1</span>]], <span>'k--'</span>)</span>
<span id="cb39-3">plt.text(v[<span>0</span>], v[<span>1</span>], <span>'v'</span>, fontsize<span>=</span><span>12</span>, color<span>=</span><span>'red'</span>)</span>
<span id="cb39-4">plt.text(u[<span>0</span>], u[<span>1</span>], <span>'u'</span>, fontsize<span>=</span><span>12</span>, color<span>=</span><span>'blue'</span>)</span>
<span id="cb39-5">plt.grid()</span>
<span id="cb39-6">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-28-output-1.png" width="571" height="411"></p>
</figure>
</div>
</div>
<p>The dashed line shows the distance between the two points.</p>
<ol start="7" type="1">
<li>Higher-dimensional vectors Norms and distances work the same way in any dimension:</li>
</ol>
<div id="0a3ac301" data-execution_count="28">
<div id="cb40"><pre><code><span id="cb40-1">a <span>=</span> np.array([<span>1</span>,<span>2</span>,<span>3</span>])</span>
<span id="cb40-2">b <span>=</span> np.array([<span>4</span>,<span>0</span>,<span>8</span>])</span>
<span id="cb40-3"><span>print</span>(<span>"‖a‖ ="</span>, np.linalg.norm(a))</span>
<span id="cb40-4"><span>print</span>(<span>"‖b‖ ="</span>, np.linalg.norm(b))</span>
<span id="cb40-5"><span>print</span>(<span>"Distance between a and b ="</span>, np.linalg.norm(a<span>-</span>b))</span></code></pre></div>
<div>
<pre><code>‖a‖ = 3.7416573867739413
‖b‖ = 8.94427190999916
Distance between a and b = 6.164414002968976</code></pre>
</div>
</div>
<p>Even though we can’t draw 3D easily on paper, the formulas still apply.</p>
</section>
<section id="try-it-yourself-4">
<h4 data-anchor-id="try-it-yourself-4">Try It Yourself</h4>
<ol type="1">
<li>Compute the length of <code>np.array([5,12])</code>. What do you expect?</li>
<li>Find the distance between <code>(2,3)</code> and <code>(7,7)</code>. Can you sketch it by hand and check?</li>
<li>In 3D, try vectors <code>(1,1,1)</code> and <code>(2,2,2)</code>. Why is the distance exactly <code>sqrt(3)</code>?</li>
</ol>
</section>
</section>
<section id="dot-product">
<h3 data-anchor-id="dot-product">6. Dot Product</h3>
<p>The dot product is one of the most important operations in linear algebra. It takes two vectors and gives you a single number. That number combines both the lengths of the vectors and how much they point in the same direction. In this lab, we’ll calculate dot products in several ways, see how they relate to geometry, and visualize their meaning.</p>
<section id="set-up-your-lab-5">
<h4 data-anchor-id="set-up-your-lab-5">Set Up Your Lab</h4>
<div id="4a818a9a" data-execution_count="29"><pre><code><span id="cb42-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb42-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-5">
<h4 data-anchor-id="step-by-step-code-walkthrough-5">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Algebraic definition The dot product of two vectors is the sum of the products of their components:</li>
</ol>
<div id="e5c2b122" data-execution_count="30">
<div id="cb43"><pre><code><span id="cb43-1">v <span>=</span> np.array([<span>2</span>, <span>3</span>])</span>
<span id="cb43-2">u <span>=</span> np.array([<span>4</span>, <span>-</span><span>1</span>])</span>
<span id="cb43-3"></span>
<span id="cb43-4">dot_manual <span>=</span> v[<span>0</span>]<span>*</span>u[<span>0</span>] <span>+</span> v[<span>1</span>]<span>*</span>u[<span>1</span>]</span>
<span id="cb43-5">dot_numpy <span>=</span> np.dot(v, u)</span>
<span id="cb43-6"></span>
<span id="cb43-7"><span>print</span>(<span>"Manual dot product:"</span>, dot_manual)</span>
<span id="cb43-8"><span>print</span>(<span>"NumPy dot product:"</span>, dot_numpy)</span></code></pre></div>
<div>
<pre><code>Manual dot product: 5
NumPy dot product: 5</code></pre>
</div>
</div>
<p>Here, <code>(2*4) + (3*-1) = 8 - 3 = 5</code>.</p>
<ol start="2" type="1">
<li>Geometric definition The dot product also equals the product of the lengths of the vectors times the cosine of the angle between them:</li>
</ol>
<p><span>\[
v \cdot u = \|v\| \|u\| \cos \theta
\]</span></p>
<p>We can compute the angle:</p>
<div id="0df8c69b" data-execution_count="31">
<div id="cb45"><pre><code><span id="cb45-1">norm_v <span>=</span> np.linalg.norm(v)</span>
<span id="cb45-2">norm_u <span>=</span> np.linalg.norm(u)</span>
<span id="cb45-3"></span>
<span id="cb45-4">cos_theta <span>=</span> np.dot(v, u) <span>/</span> (norm_v <span>*</span> norm_u)</span>
<span id="cb45-5">theta <span>=</span> np.arccos(cos_theta)</span>
<span id="cb45-6"></span>
<span id="cb45-7"><span>print</span>(<span>"cos(theta) ="</span>, cos_theta)</span>
<span id="cb45-8"><span>print</span>(<span>"theta (in radians) ="</span>, theta)</span>
<span id="cb45-9"><span>print</span>(<span>"theta (in degrees) ="</span>, np.degrees(theta))</span></code></pre></div>
<div>
<pre><code>cos(theta) = 0.33633639699815626
theta (in radians) = 1.2277723863741932
theta (in degrees) = 70.3461759419467</code></pre>
</div>
</div>
<p>This gives the angle between <code>v</code> and <code>u</code>.</p>
<ol start="3" type="1">
<li>Visualizing the dot product Let’s draw the two vectors:</li>
</ol>
<div id="034e1e64" data-execution_count="32">
<div id="cb47"><pre><code><span id="cb47-1">plt.quiver(<span>0</span>,<span>0</span>,v[<span>0</span>],v[<span>1</span>],angles<span>=</span><span>'xy'</span>,scale_units<span>=</span><span>'xy'</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>'r'</span>,label<span>=</span><span>'v'</span>)</span>
<span id="cb47-2">plt.quiver(<span>0</span>,<span>0</span>,u[<span>0</span>],u[<span>1</span>],angles<span>=</span><span>'xy'</span>,scale_units<span>=</span><span>'xy'</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>'b'</span>,label<span>=</span><span>'u'</span>)</span>
<span id="cb47-3">plt.xlim(<span>-</span><span>1</span>,<span>5</span>)</span>
<span id="cb47-4">plt.ylim(<span>-</span><span>2</span>,<span>4</span>)</span>
<span id="cb47-5">plt.axhline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb47-6">plt.axvline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb47-7">plt.grid()</span>
<span id="cb47-8">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-33-output-1.png" width="573" height="416"></p>
</figure>
</div>
</div>
<p>The dot product is positive if the angle is less than 90°, negative if greater than 90°, and zero if the vectors are perpendicular.</p>
<ol start="4" type="1">
<li>Projections and dot product The dot product lets us compute how much of one vector lies in the direction of another.</li>
</ol>
<div id="d7b5d4c7" data-execution_count="33">
<div id="cb48"><pre><code><span id="cb48-1">proj_length <span>=</span> np.dot(v, u) <span>/</span> np.linalg.norm(u)</span>
<span id="cb48-2"><span>print</span>(<span>"Projection length of v onto u:"</span>, proj_length)</span></code></pre></div>
<div>
<pre><code>Projection length of v onto u: 1.212678125181665</code></pre>
</div>
</div>
<p>This is the length of the shadow of <code>v</code> onto <code>u</code>.</p>
<ol start="5" type="1">
<li>Special cases</li>
</ol>
<ul>
<li>If vectors point in the same direction, the dot product is large and positive.</li>
<li>If vectors are perpendicular, the dot product is zero.</li>
<li>If vectors point in opposite directions, the dot product is negative.</li>
</ul>
<div id="f8ad307c" data-execution_count="34">
<div id="cb50"><pre><code><span id="cb50-1">a <span>=</span> np.array([<span>1</span>,<span>0</span>])</span>
<span id="cb50-2">b <span>=</span> np.array([<span>0</span>,<span>1</span>])</span>
<span id="cb50-3">c <span>=</span> np.array([<span>-</span><span>1</span>,<span>0</span>])</span>
<span id="cb50-4"></span>
<span id="cb50-5"><span>print</span>(<span>"a · b ="</span>, np.dot(a,b))   <span># perpendicular</span></span>
<span id="cb50-6"><span>print</span>(<span>"a · a ="</span>, np.dot(a,a))   <span># length squared</span></span>
<span id="cb50-7"><span>print</span>(<span>"a · c ="</span>, np.dot(a,c))   <span># opposite</span></span></code></pre></div>
<div>
<pre><code>a · b = 0
a · a = 1
a · c = -1</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-5">
<h4 data-anchor-id="try-it-yourself-5">Try It Yourself</h4>
<ol type="1">
<li>Compute the dot product of <code>(3,4)</code> with <code>(4,3)</code>. Is the result larger or smaller than the product of their lengths?</li>
<li>Try <code>(1,2,3) · (4,5,6)</code>. Does the geometric meaning still work in 3D?</li>
<li>Create two perpendicular vectors (e.g.&nbsp;<code>(2,0)</code> and <code>(0,5)</code>). Verify the dot product is zero.</li>
</ol>
</section>
</section>
<section id="angles-between-vectors-and-cosine">
<h3 data-anchor-id="angles-between-vectors-and-cosine">7. Angles Between Vectors and Cosine</h3>
<p>In this lab, we’ll go deeper into the connection between vectors and geometry by calculating angles. Angles tell us how much two vectors “point in the same direction.” The bridge between algebra and geometry here is the cosine formula, which comes directly from the dot product.</p>
<section id="set-up-your-lab-6">
<h4 data-anchor-id="set-up-your-lab-6">Set Up Your Lab</h4>
<div id="f229ec15" data-execution_count="35"><pre><code><span id="cb52-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb52-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-6">
<h4 data-anchor-id="step-by-step-code-walkthrough-6">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Formula for the angle The angle <span>\(\theta\)</span> between two vectors <span>\(v\)</span> and <span>\(u\)</span> is given by:</li>
</ol>
<p><span>\[
\cos \theta = \frac{v \cdot u}{\|v\| \, \|u\|}
\]</span></p>
<p>This means:</p>
<ul>
<li>If <span>\(\cos \theta = 1\)</span>, the vectors point in exactly the same direction.</li>
<li>If <span>\(\cos \theta = 0\)</span>, they are perpendicular.</li>
<li>If <span>\(\cos \theta = -1\)</span>, they point in opposite directions.</li>
</ul>
<ol start="2" type="1">
<li>Computing the angle in Python</li>
</ol>
<div id="c36587e8" data-execution_count="36">
<div id="cb53"><pre><code><span id="cb53-1">v <span>=</span> np.array([<span>2</span>, <span>3</span>])</span>
<span id="cb53-2">u <span>=</span> np.array([<span>3</span>, <span>-</span><span>1</span>])</span>
<span id="cb53-3"></span>
<span id="cb53-4">dot <span>=</span> np.dot(v, u)</span>
<span id="cb53-5">norm_v <span>=</span> np.linalg.norm(v)</span>
<span id="cb53-6">norm_u <span>=</span> np.linalg.norm(u)</span>
<span id="cb53-7"></span>
<span id="cb53-8">cos_theta <span>=</span> dot <span>/</span> (norm_v <span>*</span> norm_u)</span>
<span id="cb53-9">theta <span>=</span> np.arccos(cos_theta)</span>
<span id="cb53-10"></span>
<span id="cb53-11"><span>print</span>(<span>"cos(theta) ="</span>, cos_theta)</span>
<span id="cb53-12"><span>print</span>(<span>"theta in radians ="</span>, theta)</span>
<span id="cb53-13"><span>print</span>(<span>"theta in degrees ="</span>, np.degrees(theta))</span></code></pre></div>
<div>
<pre><code>cos(theta) = 0.2631174057921088
theta in radians = 1.3045442776439713
theta in degrees = 74.74488129694222</code></pre>
</div>
</div>
<p>This gives both the cosine value and the actual angle.</p>
<ol start="3" type="1">
<li>Visualizing the vectors</li>
</ol>
<div id="fa3d4270" data-execution_count="37">
<div id="cb55"><pre><code><span id="cb55-1">plt.quiver(<span>0</span>,<span>0</span>,v[<span>0</span>],v[<span>1</span>],angles<span>=</span><span>'xy'</span>,scale_units<span>=</span><span>'xy'</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>'r'</span>,label<span>=</span><span>'v'</span>)</span>
<span id="cb55-2">plt.quiver(<span>0</span>,<span>0</span>,u[<span>0</span>],u[<span>1</span>],angles<span>=</span><span>'xy'</span>,scale_units<span>=</span><span>'xy'</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>'b'</span>,label<span>=</span><span>'u'</span>)</span>
<span id="cb55-3"></span>
<span id="cb55-4">plt.xlim(<span>-</span><span>1</span>,<span>4</span>)</span>
<span id="cb55-5">plt.ylim(<span>-</span><span>2</span>,<span>4</span>)</span>
<span id="cb55-6">plt.axhline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb55-7">plt.axvline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb55-8">plt.grid()</span>
<span id="cb55-9">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-38-output-1.png" width="573" height="416"></p>
</figure>
</div>
</div>
<p>You can see the angle between <code>v</code> and <code>u</code> as the gap between the red and blue arrows.</p>
<ol start="4" type="1">
<li>Checking special cases</li>
</ol>
<div id="96246b33" data-execution_count="38">
<div id="cb56"><pre><code><span id="cb56-1">a <span>=</span> np.array([<span>1</span>,<span>0</span>])</span>
<span id="cb56-2">b <span>=</span> np.array([<span>0</span>,<span>1</span>])</span>
<span id="cb56-3">c <span>=</span> np.array([<span>-</span><span>1</span>,<span>0</span>])</span>
<span id="cb56-4"></span>
<span id="cb56-5"><span>print</span>(<span>"Angle between a and b ="</span>, np.degrees(np.arccos(np.dot(a,b)<span>/</span>(np.linalg.norm(a)<span>*</span>np.linalg.norm(b)))))</span>
<span id="cb56-6"><span>print</span>(<span>"Angle between a and c ="</span>, np.degrees(np.arccos(np.dot(a,c)<span>/</span>(np.linalg.norm(a)<span>*</span>np.linalg.norm(c)))))</span></code></pre></div>
<div>
<pre><code>Angle between a and b = 90.0
Angle between a and c = 180.0</code></pre>
</div>
</div>
<ul>
<li>Angle between <code>(1,0)</code> and <code>(0,1)</code> is 90°.</li>
<li>Angle between <code>(1,0)</code> and <code>(-1,0)</code> is 180°.</li>
</ul>
<ol start="5" type="1">
<li>Using cosine as a similarity measure In data science and machine learning, people often use cosine similarity instead of raw angles. It’s just the cosine value itself:</li>
</ol>
<div id="16faf51b" data-execution_count="39">
<div id="cb58"><pre><code><span id="cb58-1">cosine_similarity <span>=</span> np.dot(v,u)<span>/</span>(np.linalg.norm(v)<span>*</span>np.linalg.norm(u))</span>
<span id="cb58-2"><span>print</span>(<span>"Cosine similarity ="</span>, cosine_similarity)</span></code></pre></div>
<div>
<pre><code>Cosine similarity = 0.2631174057921088</code></pre>
</div>
</div>
<p>Values close to <code>1</code> mean vectors are aligned, values near <code>0</code> mean unrelated, and values near <code>-1</code> mean opposite.</p>
</section>
<section id="try-it-yourself-6">
<h4 data-anchor-id="try-it-yourself-6">Try It Yourself</h4>
<ol type="1">
<li>Create two random vectors with <code>np.random.randn(3)</code> and compute the angle between them.</li>
<li>Verify that swapping the vectors gives the same angle (symmetry).</li>
<li>Find two vectors where cosine similarity is exactly <code>0</code>. Can you come up with an example in 2D?</li>
</ol>
</section>
</section>
<section id="projections-and-decompositions">
<h3 data-anchor-id="projections-and-decompositions">8. Projections and Decompositions</h3>
<p>In this lab, we’ll learn how to split one vector into parts: one part that lies <em>along</em> another vector, and one part that is <em>perpendicular</em>. This process is called projection and decomposition. Projections let us measure “how much of a vector points in a given direction,” and decompositions give us a way to break vectors into useful components.</p>
<section id="set-up-your-lab-7">
<h4 data-anchor-id="set-up-your-lab-7">Set Up Your Lab</h4>
<div id="9bce7458" data-execution_count="40"><pre><code><span id="cb60-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb60-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-7">
<h4 data-anchor-id="step-by-step-code-walkthrough-7">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Projection formula The projection of vector <span>\(v\)</span> onto vector <span>\(u\)</span> is:</li>
</ol>
<p><span>\[
\text{proj}_u(v) = \frac{v \cdot u}{u \cdot u} \, u
\]</span></p>
<p>This gives the component of <span>\(v\)</span> that points in the direction of <span>\(u\)</span>.</p>
<ol start="2" type="1">
<li>Computing projection in Python</li>
</ol>
<div id="7852d8c3" data-execution_count="41">
<div id="cb61"><pre><code><span id="cb61-1">v <span>=</span> np.array([<span>3</span>, <span>2</span>])</span>
<span id="cb61-2">u <span>=</span> np.array([<span>2</span>, <span>0</span>])</span>
<span id="cb61-3"></span>
<span id="cb61-4">proj_u_v <span>=</span> (np.dot(v, u) <span>/</span> np.dot(u, u)) <span>*</span> u</span>
<span id="cb61-5"><span>print</span>(<span>"Projection of v onto u:"</span>, proj_u_v)</span></code></pre></div>
<div>
<pre><code>Projection of v onto u: [3. 0.]</code></pre>
</div>
</div>
<p>Here, <span>\(v = (3,2)\)</span> and <span>\(u = (2,0)\)</span>. The projection of <code>v</code> onto <code>u</code> is a vector pointing along the x-axis.</p>
<ol start="3" type="1">
<li>Decomposing into parallel and perpendicular parts</li>
</ol>
<p>We can write:</p>
<p><span>\[
v = \text{proj}_u(v) + (v - \text{proj}_u(v))
\]</span></p>
<p>The first part is parallel to <code>u</code>, the second part is perpendicular.</p>
<div id="6f00c0ff" data-execution_count="42">
<div id="cb63"><pre><code><span id="cb63-1">perp <span>=</span> v <span>-</span> proj_u_v</span>
<span id="cb63-2"><span>print</span>(<span>"Parallel part:"</span>, proj_u_v)</span>
<span id="cb63-3"><span>print</span>(<span>"Perpendicular part:"</span>, perp)</span></code></pre></div>
<div>
<pre><code>Parallel part: [3. 0.]
Perpendicular part: [0. 2.]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Visualizing projection and decomposition</li>
</ol>
<div id="ca9d206f" data-execution_count="43">
<div id="cb65"><pre><code><span id="cb65-1">plt.quiver(<span>0</span>, <span>0</span>, v[<span>0</span>], v[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'r'</span>, label<span>=</span><span>'v'</span>)</span>
<span id="cb65-2">plt.quiver(<span>0</span>, <span>0</span>, u[<span>0</span>], u[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'b'</span>, label<span>=</span><span>'u'</span>)</span>
<span id="cb65-3">plt.quiver(<span>0</span>, <span>0</span>, proj_u_v[<span>0</span>], proj_u_v[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'g'</span>, label<span>=</span><span>'proj_u(v)'</span>)</span>
<span id="cb65-4">plt.quiver(proj_u_v[<span>0</span>], proj_u_v[<span>1</span>], perp[<span>0</span>], perp[<span>1</span>], angles<span>=</span><span>'xy'</span>, scale_units<span>=</span><span>'xy'</span>, scale<span>=</span><span>1</span>, color<span>=</span><span>'m'</span>, label<span>=</span><span>'perpendicular'</span>)</span>
<span id="cb65-5"></span>
<span id="cb65-6">plt.xlim(<span>-</span><span>1</span>, <span>5</span>)</span>
<span id="cb65-7">plt.ylim(<span>-</span><span>1</span>, <span>4</span>)</span>
<span id="cb65-8">plt.axhline(<span>0</span>, color<span>=</span><span>'black'</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb65-9">plt.axvline(<span>0</span>, color<span>=</span><span>'black'</span>, linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb65-10">plt.grid()</span>
<span id="cb65-11">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-44-output-1.png" width="573" height="416"></p>
</figure>
</div>
</div>
<p>You’ll see <code>v</code> (red), <code>u</code> (blue), the projection (green), and the perpendicular remainder (magenta).</p>
<ol start="5" type="1">
<li>Projection in higher dimensions</li>
</ol>
<p>This formula works in any dimension:</p>
<div id="033ca9c4" data-execution_count="44">
<div id="cb66"><pre><code><span id="cb66-1">a <span>=</span> np.array([<span>1</span>,<span>2</span>,<span>3</span>])</span>
<span id="cb66-2">b <span>=</span> np.array([<span>0</span>,<span>1</span>,<span>0</span>])</span>
<span id="cb66-3"></span>
<span id="cb66-4">proj <span>=</span> (np.dot(a,b)<span>/</span>np.dot(b,b)) <span>*</span> b</span>
<span id="cb66-5">perp <span>=</span> a <span>-</span> proj</span>
<span id="cb66-6"></span>
<span id="cb66-7"><span>print</span>(<span>"Projection of a onto b:"</span>, proj)</span>
<span id="cb66-8"><span>print</span>(<span>"Perpendicular component:"</span>, perp)</span></code></pre></div>
<div>
<pre><code>Projection of a onto b: [0. 2. 0.]
Perpendicular component: [1. 0. 3.]</code></pre>
</div>
</div>
<p>Even in 3D or higher, projections are about splitting into “along” and “across.”</p>
</section>
<section id="try-it-yourself-7">
<h4 data-anchor-id="try-it-yourself-7">Try It Yourself</h4>
<ol type="1">
<li>Try projecting <code>(2,3)</code> onto <code>(0,5)</code>. Where does it land?</li>
<li>Take a 3D vector like <code>(4,2,6)</code> and project it onto <code>(1,0,0)</code>. What does this give you?</li>
<li>Change the base vector <code>u</code> to something not aligned with the axes, like <code>(1,1)</code>. Does the projection still work?</li>
</ol>
</section>
</section>
<section id="cauchyschwarz-and-triangle-inequalities">
<h3 data-anchor-id="cauchyschwarz-and-triangle-inequalities">9. Cauchy–Schwarz and Triangle Inequalities</h3>
<p>This lab introduces two fundamental inequalities in linear algebra. They may look abstract at first, but they provide guarantees that always hold true for vectors. We’ll explore them with small examples in Python to see why they matter.</p>
<section id="set-up-your-lab-8">
<h4 data-anchor-id="set-up-your-lab-8">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-8">
<h4 data-anchor-id="step-by-step-code-walkthrough-8">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Cauchy–Schwarz inequality</li>
</ol>
<p>The inequality states:</p>
<p><span>\[
|v \cdot u| \leq \|v\| \, \|u\|
\]</span></p>
<p>It means the dot product is never “bigger” than the product of the vector lengths. Equality happens only if the two vectors are pointing in exactly the same (or opposite) direction.</p>
<div id="1431feab" data-execution_count="46">
<div id="cb69"><pre><code><span id="cb69-1">v <span>=</span> np.array([<span>3</span>, <span>4</span>])</span>
<span id="cb69-2">u <span>=</span> np.array([<span>1</span>, <span>2</span>])</span>
<span id="cb69-3"></span>
<span id="cb69-4">lhs <span>=</span> <span>abs</span>(np.dot(v, u))</span>
<span id="cb69-5">rhs <span>=</span> np.linalg.norm(v) <span>*</span> np.linalg.norm(u)</span>
<span id="cb69-6"></span>
<span id="cb69-7"><span>print</span>(<span>"Left-hand side (|v·u|):"</span>, lhs)</span>
<span id="cb69-8"><span>print</span>(<span>"Right-hand side (‖v‖‖u‖):"</span>, rhs)</span>
<span id="cb69-9"><span>print</span>(<span>"Inequality holds?"</span>, lhs <span>&lt;=</span> rhs)</span></code></pre></div>
<div>
<pre><code>Left-hand side (|v·u|): 11
Right-hand side (‖v‖‖u‖): 11.180339887498949
Inequality holds? True</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Testing Cauchy–Schwarz with different vectors</li>
</ol>
<div id="2c7a1089" data-execution_count="47">
<div id="cb71"><pre><code><span id="cb71-1">pairs <span>=</span> [</span>
<span id="cb71-2">    (np.array([<span>1</span>,<span>0</span>]), np.array([<span>0</span>,<span>1</span>])),  <span># perpendicular</span></span>
<span id="cb71-3">    (np.array([<span>2</span>,<span>3</span>]), np.array([<span>4</span>,<span>6</span>])),  <span># multiples</span></span>
<span id="cb71-4">    (np.array([<span>-</span><span>1</span>,<span>2</span>]), np.array([<span>3</span>,<span>-</span><span>6</span>])) <span># opposite multiples</span></span>
<span id="cb71-5">]</span>
<span id="cb71-6"></span>
<span id="cb71-7"><span>for</span> v,u <span>in</span> pairs:</span>
<span id="cb71-8">    lhs <span>=</span> <span>abs</span>(np.dot(v, u))</span>
<span id="cb71-9">    rhs <span>=</span> np.linalg.norm(v) <span>*</span> np.linalg.norm(u)</span>
<span id="cb71-10">    <span>print</span>(<span>f"v=</span><span>{</span>v<span>}</span><span>, u=</span><span>{</span>u<span>}</span><span> -&gt; |v·u|=</span><span>{</span>lhs<span>}</span><span>, ‖v‖‖u‖=</span><span>{</span>rhs<span>}</span><span>, holds=</span><span>{</span>lhs<span>&lt;=</span>rhs<span>}</span><span>"</span>)</span></code></pre></div>
<div>
<pre><code>v=[1 0], u=[0 1] -&gt; |v·u|=0, ‖v‖‖u‖=1.0, holds=True
v=[2 3], u=[4 6] -&gt; |v·u|=26, ‖v‖‖u‖=25.999999999999996, holds=False
v=[-1  2], u=[ 3 -6] -&gt; |v·u|=15, ‖v‖‖u‖=15.000000000000002, holds=True</code></pre>
</div>
</div>
<ul>
<li>Perpendicular vectors give <code>|v·u| = 0</code>, far less than the product of norms.</li>
<li>Multiples give equality (<code>lhs = rhs</code>).</li>
</ul>
<ol start="3" type="1">
<li>Triangle inequality</li>
</ol>
<p>The triangle inequality states:</p>
<p><span>\[
\|v + u\| \leq \|v\| + \|u\|
\]</span></p>
<p>Geometrically, the length of one side of a triangle can never be longer than the sum of the other two sides.</p>
<div id="9a8dfb9d" data-execution_count="48">
<div id="cb73"><pre><code><span id="cb73-1">v <span>=</span> np.array([<span>3</span>, <span>4</span>])</span>
<span id="cb73-2">u <span>=</span> np.array([<span>1</span>, <span>2</span>])</span>
<span id="cb73-3"></span>
<span id="cb73-4">lhs <span>=</span> np.linalg.norm(v <span>+</span> u)</span>
<span id="cb73-5">rhs <span>=</span> np.linalg.norm(v) <span>+</span> np.linalg.norm(u)</span>
<span id="cb73-6"></span>
<span id="cb73-7"><span>print</span>(<span>"‖v+u‖ ="</span>, lhs)</span>
<span id="cb73-8"><span>print</span>(<span>"‖v‖ + ‖u‖ ="</span>, rhs)</span>
<span id="cb73-9"><span>print</span>(<span>"Inequality holds?"</span>, lhs <span>&lt;=</span> rhs)</span></code></pre></div>
<div>
<pre><code>‖v+u‖ = 7.211102550927978
‖v‖ + ‖u‖ = 7.23606797749979
Inequality holds? True</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Visual demonstration with a triangle</li>
</ol>
<div id="68dd049c" data-execution_count="49">
<div id="cb75"><pre><code><span id="cb75-1"><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb75-2"></span>
<span id="cb75-3">origin <span>=</span> np.array([<span>0</span>,<span>0</span>])</span>
<span id="cb75-4">points <span>=</span> np.array([origin, v, v<span>+</span>u, origin])</span>
<span id="cb75-5"></span>
<span id="cb75-6">plt.plot(points[:,<span>0</span>], points[:,<span>1</span>], <span>'ro-'</span>)  <span># triangle outline</span></span>
<span id="cb75-7">plt.text(v[<span>0</span>], v[<span>1</span>], <span>'v'</span>)</span>
<span id="cb75-8">plt.text(v[<span>0</span>]<span>+</span>u[<span>0</span>], v[<span>1</span>]<span>+</span>u[<span>1</span>], <span>'v+u'</span>)</span>
<span id="cb75-9">plt.text(u[<span>0</span>], u[<span>1</span>], <span>'u'</span>)</span>
<span id="cb75-10"></span>
<span id="cb75-11">plt.grid()</span>
<span id="cb75-12">plt.axhline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb75-13">plt.axvline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb75-14">plt.axis(<span>'equal'</span>)</span>
<span id="cb75-15">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-50-output-1.png" width="558" height="411"></p>
</figure>
</div>
</div>
<p>This triangle shows why the inequality is called the “triangle” inequality.</p>
<ol start="5" type="1">
<li>Testing triangle inequality with random vectors</li>
</ol>
<div id="0cfd0ecb" data-execution_count="50">
<div id="cb76"><pre><code><span id="cb76-1"><span>for</span> _ <span>in</span> <span>range</span>(<span>5</span>):</span>
<span id="cb76-2">    v <span>=</span> np.random.randn(<span>2</span>)</span>
<span id="cb76-3">    u <span>=</span> np.random.randn(<span>2</span>)</span>
<span id="cb76-4">    lhs <span>=</span> np.linalg.norm(v<span>+</span>u)</span>
<span id="cb76-5">    rhs <span>=</span> np.linalg.norm(v) <span>+</span> np.linalg.norm(u)</span>
<span id="cb76-6">    <span>print</span>(<span>f"‖v+u‖=</span><span>{</span>lhs<span>:.3f}</span><span>, ‖v‖+‖u‖=</span><span>{</span>rhs<span>:.3f}</span><span>, holds=</span><span>{</span>lhs <span>&lt;=</span> rhs<span>}</span><span>"</span>)</span></code></pre></div>
<div>
<pre><code>‖v+u‖=0.778, ‖v‖+‖u‖=2.112, holds=True
‖v+u‖=1.040, ‖v‖+‖u‖=2.621, holds=True
‖v+u‖=1.632, ‖v‖+‖u‖=2.482, holds=True
‖v+u‖=1.493, ‖v‖+‖u‖=2.250, holds=True
‖v+u‖=2.653, ‖v‖+‖u‖=2.692, holds=True</code></pre>
</div>
</div>
<p>No matter what vectors you try, the inequality always holds.</p>
</section>
<section id="the-takeaway">
<h4 data-anchor-id="the-takeaway">The Takeaway</h4>
<ul>
<li>Cauchy–Schwarz: The dot product is always bounded by the product of vector lengths.</li>
<li>Triangle inequality: The length of one side of a triangle can’t exceed the sum of the other two.</li>
<li>These inequalities form the backbone of geometry, analysis, and many proofs in linear algebra.</li>
</ul>
</section>
</section>
<section id="orthonormal-sets-in-ℝ²ℝ³">
<h3 data-anchor-id="orthonormal-sets-in-ℝ²ℝ³">10. Orthonormal Sets in ℝ²/ℝ³</h3>
<p>In this lab, we’ll explore orthonormal sets - collections of vectors that are both orthogonal (perpendicular) and normalized (length = 1). These sets are the “nicest” possible bases for vector spaces. In 2D and 3D, they correspond to the coordinate axes we already know, but we can also construct and test new ones.</p>
<section id="set-up-your-lab-9">
<h4 data-anchor-id="set-up-your-lab-9">Set Up Your Lab</h4>
<div id="b47d5e69" data-execution_count="51"><pre><code><span id="cb78-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb78-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-9">
<h4 data-anchor-id="step-by-step-code-walkthrough-9">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Orthogonal vectors Two vectors are orthogonal if their dot product is zero.</li>
</ol>
<div id="a6fb3cd0" data-execution_count="52"><pre><code><span id="cb79-1">x_axis <span>=</span> np.array([<span>1</span>, <span>0</span>])</span>
<span id="cb79-2">y_axis <span>=</span> np.array([<span>0</span>, <span>1</span>])</span>
<span id="cb79-3"></span>
<span id="cb79-4"><span>print</span>(<span>"x_axis · y_axis ="</span>, np.dot(x_axis, y_axis))  <span># should be 0</span></span></code></pre></div>
<p>So the standard axes are orthogonal.</p>
<ol start="2" type="1">
<li>Normalizing vectors Normalization means dividing a vector by its length to make its norm equal to 1.</li>
</ol>
<div id="09bf0d78" data-execution_count="53">
<div id="cb81"><pre><code><span id="cb81-1">v <span>=</span> np.array([<span>3</span>, <span>4</span>])</span>
<span id="cb81-2">v_normalized <span>=</span> v <span>/</span> np.linalg.norm(v)</span>
<span id="cb81-3"></span>
<span id="cb81-4"><span>print</span>(<span>"Original v:"</span>, v)</span>
<span id="cb81-5"><span>print</span>(<span>"Normalized v:"</span>, v_normalized)</span>
<span id="cb81-6"><span>print</span>(<span>"Length of normalized v:"</span>, np.linalg.norm(v_normalized))</span></code></pre></div>
<div>
<pre><code>Original v: [3 4]
Normalized v: [0.6 0.8]
Length of normalized v: 1.0</code></pre>
</div>
</div>
<p>Now <code>v_normalized</code> points in the same direction as <code>v</code> but has unit length.</p>
<ol start="3" type="1">
<li>Building an orthonormal set in 2D</li>
</ol>
<div id="741fb64a" data-execution_count="54">
<div id="cb83"><pre><code><span id="cb83-1">u1 <span>=</span> np.array([<span>1</span>, <span>0</span>])</span>
<span id="cb83-2">u2 <span>=</span> np.array([<span>0</span>, <span>1</span>])</span>
<span id="cb83-3"></span>
<span id="cb83-4"><span>print</span>(<span>"u1 length:"</span>, np.linalg.norm(u1))</span>
<span id="cb83-5"><span>print</span>(<span>"u2 length:"</span>, np.linalg.norm(u2))</span>
<span id="cb83-6"><span>print</span>(<span>"u1 · u2 ="</span>, np.dot(u1,u2))</span></code></pre></div>
<div>
<pre><code>u1 length: 1.0
u2 length: 1.0
u1 · u2 = 0</code></pre>
</div>
</div>
<p>Both have length 1, and their dot product is 0. That makes <code>{u1, u2}</code> an orthonormal set in 2D.</p>
<ol start="4" type="1">
<li>Visualizing 2D orthonormal vectors</li>
</ol>
<div id="1866c318" data-execution_count="55">
<div id="cb85"><pre><code><span id="cb85-1">plt.quiver(<span>0</span>,<span>0</span>,u1[<span>0</span>],u1[<span>1</span>],angles<span>=</span><span>'xy'</span>,scale_units<span>=</span><span>'xy'</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>'r'</span>)</span>
<span id="cb85-2">plt.quiver(<span>0</span>,<span>0</span>,u2[<span>0</span>],u2[<span>1</span>],angles<span>=</span><span>'xy'</span>,scale_units<span>=</span><span>'xy'</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>'b'</span>)</span>
<span id="cb85-3"></span>
<span id="cb85-4">plt.xlim(<span>-</span><span>1.5</span>,<span>1.5</span>)</span>
<span id="cb85-5">plt.ylim(<span>-</span><span>1.5</span>,<span>1.5</span>)</span>
<span id="cb85-6">plt.axhline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb85-7">plt.axvline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb85-8">plt.grid()</span>
<span id="cb85-9">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-56-output-1.png" width="592" height="416"></p>
</figure>
</div>
</div>
<p>You’ll see the red and blue arrows at right angles, each of length 1.</p>
<ol start="5" type="1">
<li>Orthonormal set in 3D In 3D, the standard basis vectors are:</li>
</ol>
<div id="e1af4bea" data-execution_count="56">
<div id="cb86"><pre><code><span id="cb86-1">i <span>=</span> np.array([<span>1</span>,<span>0</span>,<span>0</span>])</span>
<span id="cb86-2">j <span>=</span> np.array([<span>0</span>,<span>1</span>,<span>0</span>])</span>
<span id="cb86-3">k <span>=</span> np.array([<span>0</span>,<span>0</span>,<span>1</span>])</span>
<span id="cb86-4"></span>
<span id="cb86-5"><span>print</span>(<span>"‖i‖ ="</span>, np.linalg.norm(i))</span>
<span id="cb86-6"><span>print</span>(<span>"‖j‖ ="</span>, np.linalg.norm(j))</span>
<span id="cb86-7"><span>print</span>(<span>"‖k‖ ="</span>, np.linalg.norm(k))</span>
<span id="cb86-8"><span>print</span>(<span>"i·j ="</span>, np.dot(i,j))</span>
<span id="cb86-9"><span>print</span>(<span>"j·k ="</span>, np.dot(j,k))</span>
<span id="cb86-10"><span>print</span>(<span>"i·k ="</span>, np.dot(i,k))</span></code></pre></div>
<div>
<pre><code>‖i‖ = 1.0
‖j‖ = 1.0
‖k‖ = 1.0
i·j = 0
j·k = 0
i·k = 0</code></pre>
</div>
</div>
<p>Lengths are all 1, and dot products are 0. So <code>{i, j, k}</code> is an orthonormal set in ℝ³.</p>
<ol start="6" type="1">
<li>Testing if a set is orthonormal We can write a helper function:</li>
</ol>
<div id="0ff7a610" data-execution_count="57"><pre><code><span id="cb88-1"><span>def</span> is_orthonormal(vectors):</span>
<span id="cb88-2">    <span>for</span> i <span>in</span> <span>range</span>(<span>len</span>(vectors)):</span>
<span id="cb88-3">        <span>for</span> j <span>in</span> <span>range</span>(<span>len</span>(vectors)):</span>
<span id="cb88-4">            dot <span>=</span> np.dot(vectors[i], vectors[j])</span>
<span id="cb88-5">            <span>if</span> i <span>==</span> j:</span>
<span id="cb88-6">                <span>if</span> <span>not</span> np.isclose(dot, <span>1</span>): <span>return</span> <span>False</span></span>
<span id="cb88-7">            <span>else</span>:</span>
<span id="cb88-8">                <span>if</span> <span>not</span> np.isclose(dot, <span>0</span>): <span>return</span> <span>False</span></span>
<span id="cb88-9">    <span>return</span> <span>True</span></span>
<span id="cb88-10"></span>
<span id="cb88-11"><span>print</span>(is_orthonormal([i, j, k]))  <span># True</span></span></code></pre></div>
<ol start="7" type="1">
<li>Constructing a new orthonormal pair Not all orthonormal sets look like the axes.</li>
</ol>
<div id="f321baec" data-execution_count="58">
<div id="cb90"><pre><code><span id="cb90-1">u1 <span>=</span> np.array([<span>1</span>,<span>1</span>]) <span>/</span> np.sqrt(<span>2</span>)</span>
<span id="cb90-2">u2 <span>=</span> np.array([<span>-</span><span>1</span>,<span>1</span>]) <span>/</span> np.sqrt(<span>2</span>)</span>
<span id="cb90-3"></span>
<span id="cb90-4"><span>print</span>(<span>"u1·u2 ="</span>, np.dot(u1,u2))</span>
<span id="cb90-5"><span>print</span>(<span>"‖u1‖ ="</span>, np.linalg.norm(u1))</span>
<span id="cb90-6"><span>print</span>(<span>"‖u2‖ ="</span>, np.linalg.norm(u2))</span></code></pre></div>
<div>
<pre><code>u1·u2 = 0.0
‖u1‖ = 0.9999999999999999
‖u2‖ = 0.9999999999999999</code></pre>
</div>
</div>
<p>This gives a rotated orthonormal basis in 2D.</p>
</section>
<section id="try-it-yourself-8">
<h4 data-anchor-id="try-it-yourself-8">Try It Yourself</h4>
<ol type="1">
<li>Normalize <code>(2,2,1)</code> to make it a unit vector.</li>
<li>Test whether the set <code>{[1,0,0], [0,2,0], [0,0,3]}</code> is orthonormal.</li>
<li>Construct two vectors in 2D that are not perpendicular. Normalize them and check if the dot product is still zero.</li>
</ol>
</section>
</section>
</section>
<section id="chapter-2.-matrices-and-basic-operations">
<h2 data-anchor-id="chapter-2.-matrices-and-basic-operations">Chapter 2. Matrices and basic operations</h2>
<section id="matrices-as-tables-and-as-machines">
<h3 data-anchor-id="matrices-as-tables-and-as-machines">11. Matrices as Tables and as Machines</h3>
<p>Matrices can feel mysterious at first, but there are two simple ways to think about them:</p>
<ol type="1">
<li>As tables of numbers - just a grid you can store and manipulate.</li>
<li>As machines - something that takes a vector in and spits a new vector out.</li>
</ol>
<p>In this lab, we’ll explore both views and see how they connect.</p>
<section id="set-up-your-lab-10">
<h4 data-anchor-id="set-up-your-lab-10">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-10">
<h4 data-anchor-id="step-by-step-code-walkthrough-10">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>A matrix as a table of numbers</li>
</ol>
<div id="a758b9ca" data-execution_count="60">
<div id="cb93"><pre><code><span id="cb93-1">A <span>=</span> np.array([</span>
<span id="cb93-2">    [<span>1</span>, <span>2</span>, <span>3</span>],</span>
<span id="cb93-3">    [<span>4</span>, <span>5</span>, <span>6</span>]</span>
<span id="cb93-4">])</span>
<span id="cb93-5"></span>
<span id="cb93-6"><span>print</span>(<span>"Matrix A:</span><span>\n</span><span>"</span>, A)</span>
<span id="cb93-7"><span>print</span>(<span>"Shape of A:"</span>, A.shape)</span></code></pre></div>
<div>
<pre><code>Matrix A:
 [[1 2 3]
 [4 5 6]]
Shape of A: (2, 3)</code></pre>
</div>
</div>
<p>Here, <code>A</code> is a 2×3 matrix (2 rows, 3 columns).</p>
<ul>
<li>Rows = horizontal slices → <code>[1,2,3]</code> and <code>[4,5,6]</code></li>
<li>Columns = vertical slices → <code>[1,4]</code>, <code>[2,5]</code>, <code>[3,6]</code></li>
</ul>
<ol start="2" type="1">
<li>Accessing rows and columns</li>
</ol>
<div id="390a96fe" data-execution_count="61">
<div id="cb95"><pre><code><span id="cb95-1">first_row <span>=</span> A[<span>0</span>]        <span># row 0</span></span>
<span id="cb95-2">second_column <span>=</span> A[:,<span>1</span>]  <span># column 1</span></span>
<span id="cb95-3"></span>
<span id="cb95-4"><span>print</span>(<span>"First row:"</span>, first_row)</span>
<span id="cb95-5"><span>print</span>(<span>"Second column:"</span>, second_column)</span></code></pre></div>
<div>
<pre><code>First row: [1 2 3]
Second column: [2 5]</code></pre>
</div>
</div>
<p>Rows are whole vectors too, and so are columns.</p>
<ol start="3" type="1">
<li>A matrix as a machine</li>
</ol>
<p>A matrix can “act” on a vector. If <code>x = [x1, x2, x3]</code>, then <code>A·x</code> is computed by taking linear combinations of the columns of <code>A</code>.</p>
<div id="59cfb580" data-execution_count="62"><pre><code><span id="cb97-1">x <span>=</span> np.array([<span>1</span>, <span>0</span>, <span>-</span><span>1</span>])  <span># a 3D vector</span></span>
<span id="cb97-2">result <span>=</span> A.dot(x)</span>
<span id="cb97-3"></span>
<span id="cb97-4"><span>print</span>(<span>"A·x ="</span>, result)</span></code></pre></div>
<p>Interpretation: multiply <code>A</code> by <code>x</code> = combine columns of <code>A</code> with weights from <code>x</code>.</p>
<p><span>\[
A \cdot x = 1 \cdot \text{(col 1)} + 0 \cdot \text{(col 2)} + (-1) \cdot \text{(col 3)}
\]</span></p>
<ol start="4" type="1">
<li>Verifying column combination view</li>
</ol>
<div id="a26fdaa9" data-execution_count="63">
<div id="cb99"><pre><code><span id="cb99-1">col1 <span>=</span> A[:,<span>0</span>]</span>
<span id="cb99-2">col2 <span>=</span> A[:,<span>1</span>]</span>
<span id="cb99-3">col3 <span>=</span> A[:,<span>2</span>]</span>
<span id="cb99-4"></span>
<span id="cb99-5">manual <span>=</span> <span>1</span><span>*</span>col1 <span>+</span> <span>0</span><span>*</span>col2 <span>+</span> (<span>-</span><span>1</span>)<span>*</span>col3</span>
<span id="cb99-6"><span>print</span>(<span>"Manual combination:"</span>, manual)</span>
<span id="cb99-7"><span>print</span>(<span>"A·x result:"</span>, result)</span></code></pre></div>
<div>
<pre><code>Manual combination: [-2 -2]
A·x result: [-2 -2]</code></pre>
</div>
</div>
<p>They match exactly. This shows the “machine” interpretation is just a shortcut for column combinations.</p>
<ol start="5" type="1">
<li>Geometric intuition (2D example)</li>
</ol>
<div id="c06ed7b1" data-execution_count="64"><pre><code><span id="cb101-1">B <span>=</span> np.array([</span>
<span id="cb101-2">    [<span>2</span>, <span>0</span>],</span>
<span id="cb101-3">    [<span>0</span>, <span>1</span>]</span>
<span id="cb101-4">])</span>
<span id="cb101-5"></span>
<span id="cb101-6">v <span>=</span> np.array([<span>1</span>,<span>2</span>])</span>
<span id="cb101-7"><span>print</span>(<span>"B·v ="</span>, B.dot(v))</span></code></pre></div>
<p>Here, <code>B</code> scales the x-direction by 2 while leaving the y-direction alone. So <code>(1,2)</code> becomes <code>(2,2)</code>.</p>
</section>
<section id="try-it-yourself-9">
<h4 data-anchor-id="try-it-yourself-9">Try It Yourself</h4>
<ol type="1">
<li>Create a 3×3 identity matrix with <code>np.eye(3)</code> and multiply it by different vectors. What happens?</li>
<li>Build a matrix <code>[[0,-1],[1,0]]</code>. Try multiplying it by <code>(1,0)</code> and <code>(0,1)</code>. What transformation is this?</li>
<li>Create your own 2×2 matrix that flips vectors across the x-axis. Test it on <code>(1,2)</code> and <code>(−3,4)</code>.</li>
</ol>
</section>
<section id="the-takeaway-1">
<h4 data-anchor-id="the-takeaway-1">The Takeaway</h4>
<ul>
<li>A matrix is both a grid of numbers and a machine that transforms vectors.</li>
<li>Matrix–vector multiplication is the same as combining columns with given weights.</li>
<li>Thinking of matrices as machines helps build intuition for rotations, scalings, and other transformations later.</li>
</ul>
</section>
</section>
<section id="matrix-shapes-indexing-and-block-views">
<h3 data-anchor-id="matrix-shapes-indexing-and-block-views">12. Matrix Shapes, Indexing, and Block Views</h3>
<p>Matrices come in many shapes, and learning to read their structure is essential. Shape tells us how many rows and columns a matrix has. Indexing lets us grab specific entries, rows, or columns. Block views let us zoom in on submatrices, which is extremely useful for both theory and computation.</p>
<section id="set-up-your-lab-11">
<h4 data-anchor-id="set-up-your-lab-11">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-11">
<h4 data-anchor-id="step-by-step-code-walkthrough-11">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Matrix shapes</li>
</ol>
<p>The shape of a matrix is <code>(rows, columns)</code>.</p>
<div id="40e360b9" data-execution_count="66">
<div id="cb104"><pre><code><span id="cb104-1">A <span>=</span> np.array([</span>
<span id="cb104-2">    [<span>1</span>, <span>2</span>, <span>3</span>],</span>
<span id="cb104-3">    [<span>4</span>, <span>5</span>, <span>6</span>],</span>
<span id="cb104-4">    [<span>7</span>, <span>8</span>, <span>9</span>]</span>
<span id="cb104-5">])</span>
<span id="cb104-6"></span>
<span id="cb104-7"><span>print</span>(<span>"Matrix A:</span><span>\n</span><span>"</span>, A)</span>
<span id="cb104-8"><span>print</span>(<span>"Shape of A:"</span>, A.shape)</span></code></pre></div>
<div>
<pre><code>Matrix A:
 [[1 2 3]
 [4 5 6]
 [7 8 9]]
Shape of A: (3, 3)</code></pre>
</div>
</div>
<p>Here, <code>A</code> is a 3×3 matrix.</p>
<ol start="2" type="1">
<li>Indexing elements</li>
</ol>
<p>In NumPy, rows and columns are 0-based. The first entry is <code>A[0,0]</code>.</p>
<div id="b0e93cd3" data-execution_count="67"><pre><code><span id="cb106-1"><span>print</span>(<span>"A[0,0] ="</span>, A[<span>0</span>,<span>0</span>])  <span># top-left element</span></span>
<span id="cb106-2"><span>print</span>(<span>"A[1,2] ="</span>, A[<span>1</span>,<span>2</span>])  <span># second row, third column</span></span></code></pre></div>
<ol start="3" type="1">
<li>Extracting rows and columns</li>
</ol>
<div id="ceed2300" data-execution_count="68">
<div id="cb108"><pre><code><span id="cb108-1">row1 <span>=</span> A[<span>0</span>]       <span># first row</span></span>
<span id="cb108-2">col2 <span>=</span> A[:,<span>1</span>]     <span># second column</span></span>
<span id="cb108-3"></span>
<span id="cb108-4"><span>print</span>(<span>"First row:"</span>, row1)</span>
<span id="cb108-5"><span>print</span>(<span>"Second column:"</span>, col2)</span></code></pre></div>
<div>
<pre><code>First row: [1 2 3]
Second column: [2 5 8]</code></pre>
</div>
</div>
<p>Notice: <code>A[i]</code> gives a row, <code>A[:,j]</code> gives a column.</p>
<ol start="4" type="1">
<li>Slicing submatrices (block view)</li>
</ol>
<p>You can slice multiple rows and columns to form a smaller matrix.</p>
<div id="ae55a2f7" data-execution_count="69">
<div id="cb110"><pre><code><span id="cb110-1">block <span>=</span> A[<span>0</span>:<span>2</span>, <span>1</span>:<span>3</span>]  <span># rows 0–1, columns 1–2</span></span>
<span id="cb110-2"><span>print</span>(<span>"Block submatrix:</span><span>\n</span><span>"</span>, block)</span></code></pre></div>
<div>
<pre><code>Block submatrix:
 [[2 3]
 [5 6]]</code></pre>
</div>
</div>
<p>This block is:</p>
<p><span>\[
\begin{bmatrix}
2 &amp; 3 \\
5 &amp; 6
\end{bmatrix}
\]</span></p>
<ol start="5" type="1">
<li>Modifying parts of a matrix</li>
</ol>
<div id="f7ec2ac0" data-execution_count="70">
<div id="cb112"><pre><code><span id="cb112-1">A[<span>0</span>,<span>0</span>] <span>=</span> <span>99</span></span>
<span id="cb112-2"><span>print</span>(<span>"Modified A:</span><span>\n</span><span>"</span>, A)</span>
<span id="cb112-3"></span>
<span id="cb112-4">A[<span>1</span>,:] <span>=</span> [<span>10</span>, <span>11</span>, <span>12</span>]   <span># replace row 1</span></span>
<span id="cb112-5"><span>print</span>(<span>"After replacing row 1:</span><span>\n</span><span>"</span>, A)</span></code></pre></div>
<div>
<pre><code>Modified A:
 [[99  2  3]
 [ 4  5  6]
 [ 7  8  9]]
After replacing row 1:
 [[99  2  3]
 [10 11 12]
 [ 7  8  9]]</code></pre>
</div>
</div>
<ol start="6" type="1">
<li>Non-square matrices</li>
</ol>
<p>Not all matrices are square. Shapes can be rectangular, too.</p>
<div id="57287bcd" data-execution_count="71">
<div id="cb114"><pre><code><span id="cb114-1">B <span>=</span> np.array([</span>
<span id="cb114-2">    [<span>1</span>, <span>2</span>],</span>
<span id="cb114-3">    [<span>3</span>, <span>4</span>],</span>
<span id="cb114-4">    [<span>5</span>, <span>6</span>]</span>
<span id="cb114-5">])</span>
<span id="cb114-6"></span>
<span id="cb114-7"><span>print</span>(<span>"Matrix B:</span><span>\n</span><span>"</span>, B)</span>
<span id="cb114-8"><span>print</span>(<span>"Shape of B:"</span>, B.shape)</span></code></pre></div>
<div>
<pre><code>Matrix B:
 [[1 2]
 [3 4]
 [5 6]]
Shape of B: (3, 2)</code></pre>
</div>
</div>
<p>Here, <code>B</code> is 3×2 (3 rows, 2 columns).</p>
<ol start="7" type="1">
<li>Block decomposition idea</li>
</ol>
<p>We can think of large matrices as made of smaller blocks. This is common in linear algebra proofs and algorithms.</p>
<div id="c4971b80" data-execution_count="72">
<div id="cb116"><pre><code><span id="cb116-1">C <span>=</span> np.array([</span>
<span id="cb116-2">    [<span>1</span>,<span>2</span>,<span>3</span>,<span>4</span>],</span>
<span id="cb116-3">    [<span>5</span>,<span>6</span>,<span>7</span>,<span>8</span>],</span>
<span id="cb116-4">    [<span>9</span>,<span>10</span>,<span>11</span>,<span>12</span>],</span>
<span id="cb116-5">    [<span>13</span>,<span>14</span>,<span>15</span>,<span>16</span>]</span>
<span id="cb116-6">])</span>
<span id="cb116-7"></span>
<span id="cb116-8">top_left <span>=</span> C[<span>0</span>:<span>2</span>, <span>0</span>:<span>2</span>]</span>
<span id="cb116-9">bottom_right <span>=</span> C[<span>2</span>:<span>4</span>, <span>2</span>:<span>4</span>]</span>
<span id="cb116-10"></span>
<span id="cb116-11"><span>print</span>(<span>"Top-left block:</span><span>\n</span><span>"</span>, top_left)</span>
<span id="cb116-12"><span>print</span>(<span>"Bottom-right block:</span><span>\n</span><span>"</span>, bottom_right)</span></code></pre></div>
<div>
<pre><code>Top-left block:
 [[1 2]
 [5 6]]
Bottom-right block:
 [[11 12]
 [15 16]]</code></pre>
</div>
</div>
<p>This is the start of block matrix notation.</p>
</section>
<section id="try-it-yourself-10">
<h4 data-anchor-id="try-it-yourself-10">Try It Yourself</h4>
<ol type="1">
<li>Create a 4×5 matrix with values 1–20 using <code>np.arange(1,21).reshape(4,5)</code>. Find its shape.</li>
<li>Extract the middle row and last column.</li>
<li>Cut it into four 2×2 blocks. Can you reassemble them in a different order?</li>
</ol>
</section>
</section>
<section id="matrix-addition-and-scalar-multiplication">
<h3 data-anchor-id="matrix-addition-and-scalar-multiplication">13. Matrix Addition and Scalar Multiplication</h3>
<p>Now that we understand matrix shapes and indexing, let’s practice two of the simplest but most important operations: adding matrices and scaling them with numbers (scalars). These operations extend the rules we already know for vectors.</p>
<section id="set-up-your-lab-12">
<h4 data-anchor-id="set-up-your-lab-12">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-12">
<h4 data-anchor-id="step-by-step-code-walkthrough-12">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Adding two matrices You can add two matrices if (and only if) they have the same shape. Addition happens entry by entry.</li>
</ol>
<div id="7444c906" data-execution_count="74"><pre><code><span id="cb119-1">A <span>=</span> np.array([</span>
<span id="cb119-2">    [<span>1</span>, <span>2</span>],</span>
<span id="cb119-3">    [<span>3</span>, <span>4</span>]</span>
<span id="cb119-4">])</span>
<span id="cb119-5"></span>
<span id="cb119-6">B <span>=</span> np.array([</span>
<span id="cb119-7">    [<span>5</span>, <span>6</span>],</span>
<span id="cb119-8">    [<span>7</span>, <span>8</span>]</span>
<span id="cb119-9">])</span>
<span id="cb119-10"></span>
<span id="cb119-11">C <span>=</span> A <span>+</span> B</span>
<span id="cb119-12"><span>print</span>(<span>"A + B =</span><span>\n</span><span>"</span>, C)</span></code></pre></div>
<p>Each element in <code>C</code> is the sum of corresponding elements in <code>A</code> and <code>B</code>.</p>
<ol start="2" type="1">
<li>Scalar multiplication Multiplying a matrix by a scalar multiplies every entry by that number.</li>
</ol>
<div id="babe7c4a" data-execution_count="75"><pre><code><span id="cb121-1">k <span>=</span> <span>3</span></span>
<span id="cb121-2">D <span>=</span> k <span>*</span> A</span>
<span id="cb121-3"><span>print</span>(<span>"3 * A =</span><span>\n</span><span>"</span>, D)</span></code></pre></div>
<p>Here, each element of <code>A</code> is tripled.</p>
<ol start="3" type="1">
<li>Combining both operations We can mix addition and scaling, just like with vectors.</li>
</ol>
<div id="6a9c40dc" data-execution_count="76">
<div id="cb123"><pre><code><span id="cb123-1">combo <span>=</span> <span>2</span><span>*</span>A <span>-</span> B</span>
<span id="cb123-2"><span>print</span>(<span>"2A - B =</span><span>\n</span><span>"</span>, combo)</span></code></pre></div>
<div>
<pre><code>2A - B =
 [[-3 -2]
 [-1  0]]</code></pre>
</div>
</div>
<p>This creates new matrices as linear combinations of others.</p>
<ol start="4" type="1">
<li>Zero matrix A matrix of all zeros acts like “nothing happens” for addition.</li>
</ol>
<div id="f79f2289" data-execution_count="77">
<div id="cb125"><pre><code><span id="cb125-1">zero <span>=</span> np.zeros((<span>2</span>,<span>2</span>))</span>
<span id="cb125-2"><span>print</span>(<span>"Zero matrix:</span><span>\n</span><span>"</span>, zero)</span>
<span id="cb125-3"><span>print</span>(<span>"A + Zero =</span><span>\n</span><span>"</span>, A <span>+</span> zero)</span></code></pre></div>
<div>
<pre><code>Zero matrix:
 [[0. 0.]
 [0. 0.]]
A + Zero =
 [[1. 2.]
 [3. 4.]]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Shape mismatch (what fails) If shapes don’t match, NumPy throws an error.</li>
</ol>
<div id="2ea1b396" data-execution_count="78">
<div id="cb127"><pre><code><span id="cb127-1">X <span>=</span> np.array([</span>
<span id="cb127-2">    [<span>1</span>,<span>2</span>,<span>3</span>],</span>
<span id="cb127-3">    [<span>4</span>,<span>5</span>,<span>6</span>]</span>
<span id="cb127-4">])</span>
<span id="cb127-5"></span>
<span id="cb127-6"><span>try</span>:</span>
<span id="cb127-7">    <span>print</span>(A <span>+</span> X)</span>
<span id="cb127-8"><span>except</span> <span>ValueError</span> <span>as</span> e:</span>
<span id="cb127-9">    <span>print</span>(<span>"Error:"</span>, e)</span></code></pre></div>
<div>
<pre><code>Error: operands could not be broadcast together with shapes (2,2) (2,3) </code></pre>
</div>
</div>
<p>This shows why shape consistency matters.</p>
</section>
<section id="try-it-yourself-11">
<h4 data-anchor-id="try-it-yourself-11">Try It Yourself</h4>
<ol type="1">
<li>Create two random 3×3 matrices with <code>np.random.randint(0,10,(3,3))</code> and add them.</li>
<li>Multiply a 4×4 matrix by <code>-1</code>. What happens to its entries?</li>
<li>Compute <code>3A + 2B</code> with the matrices from above. Compare with doing each step manually.</li>
</ol>
</section>
</section>
<section id="matrixvector-product-linear-combinations-of-columns">
<h3 data-anchor-id="matrixvector-product-linear-combinations-of-columns">14. Matrix–Vector Product (Linear Combinations of Columns)</h3>
<p>This lab introduces the matrix–vector product, one of the most important operations in linear algebra. Multiplying a matrix by a vector doesn’t just crunch numbers - it produces a new vector by combining the matrix’s columns in a weighted way.</p>
<section id="set-up-your-lab-13">
<h4 data-anchor-id="set-up-your-lab-13">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-13">
<h4 data-anchor-id="step-by-step-code-walkthrough-13">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>A simple matrix and vector</li>
</ol>
<div id="818eaa3c" data-execution_count="80"><pre><code><span id="cb130-1">A <span>=</span> np.array([</span>
<span id="cb130-2">    [<span>1</span>, <span>2</span>],</span>
<span id="cb130-3">    [<span>3</span>, <span>4</span>],</span>
<span id="cb130-4">    [<span>5</span>, <span>6</span>]</span>
<span id="cb130-5">])  <span># 3×2 matrix</span></span>
<span id="cb130-6"></span>
<span id="cb130-7">x <span>=</span> np.array([<span>2</span>, <span>-</span><span>1</span>])  <span># 2D vector</span></span></code></pre></div>
<p>Here, <code>A</code> has 2 columns, so we can multiply it by a 2D vector <code>x</code>.</p>
<ol start="2" type="1">
<li>Matrix–vector multiplication in NumPy</li>
</ol>
<div id="1b59b9d1" data-execution_count="81"><pre><code><span id="cb131-1">y <span>=</span> A.dot(x)</span>
<span id="cb131-2"><span>print</span>(<span>"A·x ="</span>, y)</span></code></pre></div>
<p>Result: a 3D vector.</p>
<ol start="3" type="1">
<li>Interpreting the result as linear combinations</li>
</ol>
<p>Matrix <code>A</code> has two columns:</p>
<div id="393d622e" data-execution_count="82">
<div id="cb133"><pre><code><span id="cb133-1">col1 <span>=</span> A[:,<span>0</span>]   <span># first column</span></span>
<span id="cb133-2">col2 <span>=</span> A[:,<span>1</span>]   <span># second column</span></span>
<span id="cb133-3"></span>
<span id="cb133-4">manual <span>=</span> <span>2</span><span>*</span>col1 <span>+</span> (<span>-</span><span>1</span>)<span>*</span>col2</span>
<span id="cb133-5"><span>print</span>(<span>"Manual linear combination:"</span>, manual)</span></code></pre></div>
<div>
<pre><code>Manual linear combination: [0 2 4]</code></pre>
</div>
</div>
<p>This matches <code>A·x</code>. In words: <em>multiply each column by the corresponding entry of <code>x</code> and then add them up</em>.</p>
<ol start="4" type="1">
<li>Another example (geometry)</li>
</ol>
<div id="c6149e36" data-execution_count="83"><pre><code><span id="cb135-1">B <span>=</span> np.array([</span>
<span id="cb135-2">    [<span>2</span>, <span>0</span>],</span>
<span id="cb135-3">    [<span>0</span>, <span>1</span>]</span>
<span id="cb135-4">])  <span># stretches x-axis by 2</span></span>
<span id="cb135-5"></span>
<span id="cb135-6">v <span>=</span> np.array([<span>1</span>, <span>3</span>])</span>
<span id="cb135-7"><span>print</span>(<span>"B·v ="</span>, B.dot(v))</span></code></pre></div>
<p>Here, <code>(1,3)</code> becomes <code>(2,3)</code>. The x-component was doubled, while y stayed the same.</p>
<ol start="5" type="1">
<li>Visualization of matrix action</li>
</ol>
<div id="099f57e9" data-execution_count="84">
<div id="cb137"><pre><code><span id="cb137-1"><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb137-2"></span>
<span id="cb137-3"><span># draw original vector</span></span>
<span id="cb137-4">plt.quiver(<span>0</span>,<span>0</span>,v[<span>0</span>],v[<span>1</span>],angles<span>=</span><span>'xy'</span>,scale_units<span>=</span><span>'xy'</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>'r'</span>,label<span>=</span><span>'v'</span>)</span>
<span id="cb137-5"></span>
<span id="cb137-6"><span># draw transformed vector</span></span>
<span id="cb137-7">v_transformed <span>=</span> B.dot(v)</span>
<span id="cb137-8">plt.quiver(<span>0</span>,<span>0</span>,v_transformed[<span>0</span>],v_transformed[<span>1</span>],angles<span>=</span><span>'xy'</span>,scale_units<span>=</span><span>'xy'</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>'b'</span>,label<span>=</span><span>'B·v'</span>)</span>
<span id="cb137-9"></span>
<span id="cb137-10">plt.xlim(<span>-</span><span>1</span>,<span>4</span>)</span>
<span id="cb137-11">plt.ylim(<span>-</span><span>1</span>,<span>4</span>)</span>
<span id="cb137-12">plt.axhline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb137-13">plt.axvline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb137-14">plt.grid()</span>
<span id="cb137-15">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-85-output-1.png" width="573" height="416"></p>
</figure>
</div>
</div>
<p>Red arrow = original vector, blue arrow = transformed vector.</p>
</section>
<section id="try-it-yourself-12">
<h4 data-anchor-id="try-it-yourself-12">Try It Yourself</h4>
<ol type="1">
<li><p>Multiply</p>
<p><span>\[
A = \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 1 \\ -1 &amp; 2\end{bmatrix},\; x = [3,1]
\]</span></p>
<p>What’s the result?</p></li>
<li><p>Replace <code>B</code> with <code>[[0,-1],[1,0]]</code>. Multiply it by <code>(1,0)</code> and <code>(0,1)</code>. What geometric transformation does this represent?</p></li>
<li><p>For a 4×4 identity matrix (<code>np.eye(4)</code>), try multiplying by any 4D vector. What do you observe?</p></li>
</ol>
</section>
</section>
<section id="matrixmatrix-product-composition-of-linear-steps">
<h3 data-anchor-id="matrixmatrix-product-composition-of-linear-steps">15. Matrix–Matrix Product (Composition of Linear Steps)</h3>
<p>Matrix–matrix multiplication is how we combine two linear transformations into one. Instead of applying one transformation, then another, we can multiply their matrices and get a single matrix that does both at once.</p>
<section id="set-up-your-lab-14">
<h4 data-anchor-id="set-up-your-lab-14">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-14">
<h4 data-anchor-id="step-by-step-code-walkthrough-14">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Matrix–matrix multiplication in NumPy</li>
</ol>
<div id="920114d3" data-execution_count="86"><pre><code><span id="cb139-1">A <span>=</span> np.array([</span>
<span id="cb139-2">    [<span>1</span>, <span>2</span>],</span>
<span id="cb139-3">    [<span>3</span>, <span>4</span>]</span>
<span id="cb139-4">])  <span># 2×2</span></span>
<span id="cb139-5"></span>
<span id="cb139-6">B <span>=</span> np.array([</span>
<span id="cb139-7">    [<span>2</span>, <span>0</span>],</span>
<span id="cb139-8">    [<span>1</span>, <span>2</span>]</span>
<span id="cb139-9">])  <span># 2×2</span></span>
<span id="cb139-10"></span>
<span id="cb139-11">C <span>=</span> A.dot(B)   <span># or A @ B</span></span>
<span id="cb139-12"><span>print</span>(<span>"A·B =</span><span>\n</span><span>"</span>, C)</span></code></pre></div>
<p>The result <code>C</code> is another 2×2 matrix.</p>
<ol start="2" type="1">
<li>Manual computation</li>
</ol>
<p>Each entry of <code>C</code> is computed as a row of A dotted with a column of B:</p>
<div id="91f8a6a2" data-execution_count="87">
<div id="cb141"><pre><code><span id="cb141-1">c11 <span>=</span> A[<span>0</span>,:].dot(B[:,<span>0</span>])</span>
<span id="cb141-2">c12 <span>=</span> A[<span>0</span>,:].dot(B[:,<span>1</span>])</span>
<span id="cb141-3">c21 <span>=</span> A[<span>1</span>,:].dot(B[:,<span>0</span>])</span>
<span id="cb141-4">c22 <span>=</span> A[<span>1</span>,:].dot(B[:,<span>1</span>])</span>
<span id="cb141-5"></span>
<span id="cb141-6"><span>print</span>(<span>"Manual C =</span><span>\n</span><span>"</span>, np.array([[c11,c12],[c21,c22]]))</span></code></pre></div>
<div>
<pre><code>Manual C =
 [[ 4  4]
 [10  8]]</code></pre>
</div>
</div>
<p>This should match <code>A·B</code>.</p>
<ol start="3" type="1">
<li>Geometric interpretation</li>
</ol>
<p>Let’s see how two transformations combine.</p>
<ul>
<li>Matrix <code>B</code> scales x by 2 and stretches y by 2.</li>
<li>Matrix <code>A</code> applies another linear transformation.</li>
</ul>
<p>Together, <code>C = A·B</code> does both in one step.</p>
<div id="0049a10b" data-execution_count="88">
<div id="cb143"><pre><code><span id="cb143-1">v <span>=</span> np.array([<span>1</span>,<span>1</span>])</span>
<span id="cb143-2"></span>
<span id="cb143-3"><span>print</span>(<span>"First apply B:"</span>, B.dot(v))</span>
<span id="cb143-4"><span>print</span>(<span>"Then apply A:"</span>, A.dot(B.dot(v)))</span>
<span id="cb143-5"><span>print</span>(<span>"Directly with C:"</span>, C.dot(v))</span></code></pre></div>
<div>
<pre><code>First apply B: [2 3]
Then apply A: [ 8 18]
Directly with C: [ 8 18]</code></pre>
</div>
</div>
<p>The result is the same: applying <code>B</code> then <code>A</code> is equivalent to applying <code>C</code>.</p>
<ol start="4" type="1">
<li>Non-square matrices</li>
</ol>
<p>Matrix multiplication also works for rectangular matrices, as long as the inner dimensions match.</p>
<div id="acba7cd2" data-execution_count="89"><pre><code><span id="cb145-1">M <span>=</span> np.array([</span>
<span id="cb145-2">    [<span>1</span>, <span>0</span>, <span>2</span>],</span>
<span id="cb145-3">    [<span>0</span>, <span>1</span>, <span>3</span>]</span>
<span id="cb145-4">])  <span># 2×3</span></span>
<span id="cb145-5"></span>
<span id="cb145-6">N <span>=</span> np.array([</span>
<span id="cb145-7">    [<span>1</span>, <span>2</span>],</span>
<span id="cb145-8">    [<span>0</span>, <span>1</span>],</span>
<span id="cb145-9">    [<span>4</span>, <span>0</span>]</span>
<span id="cb145-10">])  <span># 3×2</span></span>
<span id="cb145-11"></span>
<span id="cb145-12">P <span>=</span> M.dot(N)  <span># result is 2×2</span></span>
<span id="cb145-13"><span>print</span>(<span>"M·N =</span><span>\n</span><span>"</span>, P)</span></code></pre></div>
<p>Shape rule: <code>(2×3)·(3×2) = (2×2)</code>.</p>
<ol start="5" type="1">
<li>Associativity (but not commutativity)</li>
</ol>
<p>Matrix multiplication is associative: <code>(A·B)·C = A·(B·C)</code>. But it’s not commutative: in general, <code>A·B ≠ B·A</code>.</p>
<div id="025d7f5b" data-execution_count="90">
<div id="cb147"><pre><code><span id="cb147-1">A <span>=</span> np.array([[<span>1</span>,<span>2</span>],[<span>3</span>,<span>4</span>]])</span>
<span id="cb147-2">B <span>=</span> np.array([[<span>0</span>,<span>1</span>],[<span>1</span>,<span>0</span>]])</span>
<span id="cb147-3"></span>
<span id="cb147-4"><span>print</span>(<span>"A·B =</span><span>\n</span><span>"</span>, A.dot(B))</span>
<span id="cb147-5"><span>print</span>(<span>"B·A =</span><span>\n</span><span>"</span>, B.dot(A))</span></code></pre></div>
<div>
<pre><code>A·B =
 [[2 1]
 [4 3]]
B·A =
 [[3 4]
 [1 2]]</code></pre>
</div>
</div>
<p>The two results are different.</p>
</section>
<section id="try-it-yourself-13">
<h4 data-anchor-id="try-it-yourself-13">Try It Yourself</h4>
<ol type="1">
<li><p>Multiply</p>
<p><span>\[
A = \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 1\end{bmatrix},\;
B = \begin{bmatrix}0 &amp; -1 \\ 1 &amp; 0\end{bmatrix}
\]</span></p>
<p>What transformation does <code>A·B</code> represent?</p></li>
<li><p>Create a random 3×2 matrix and a 2×4 matrix. Multiply them. What shape is the result?</p></li>
<li><p>Verify with Python that <code>(A·B)·C = A·(B·C)</code> for some 3×3 random matrices.</p></li>
</ol>
</section>
</section>
<section id="identity-inverse-and-transpose">
<h3 data-anchor-id="identity-inverse-and-transpose">16. Identity, Inverse, and Transpose</h3>
<p>In this lab, we’ll meet three special matrix operations and objects: the identity matrix, the inverse, and the transpose. These are the building blocks of matrix algebra, each with a simple meaning but deep importance.</p>
<section id="set-up-your-lab-15">
<h4 data-anchor-id="set-up-your-lab-15">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-15">
<h4 data-anchor-id="step-by-step-code-walkthrough-15">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Identity matrix The identity matrix is like the number <code>1</code> for matrices: multiplying by it changes nothing.</li>
</ol>
<div id="93cf229b" data-execution_count="92">
<div id="cb150"><pre><code><span id="cb150-1">I <span>=</span> np.eye(<span>3</span>)  <span># 3×3 identity matrix</span></span>
<span id="cb150-2"><span>print</span>(<span>"Identity matrix:</span><span>\n</span><span>"</span>, I)</span>
<span id="cb150-3"></span>
<span id="cb150-4">A <span>=</span> np.array([</span>
<span id="cb150-5">    [<span>2</span>, <span>1</span>, <span>0</span>],</span>
<span id="cb150-6">    [<span>0</span>, <span>1</span>, <span>3</span>],</span>
<span id="cb150-7">    [<span>4</span>, <span>0</span>, <span>1</span>]</span>
<span id="cb150-8">])</span>
<span id="cb150-9"></span>
<span id="cb150-10"><span>print</span>(<span>"A·I =</span><span>\n</span><span>"</span>, A.dot(I))</span>
<span id="cb150-11"><span>print</span>(<span>"I·A =</span><span>\n</span><span>"</span>, I.dot(A))</span></code></pre></div>
<div>
<pre><code>Identity matrix:
 [[1. 0. 0.]
 [0. 1. 0.]
 [0. 0. 1.]]
A·I =
 [[2. 1. 0.]
 [0. 1. 3.]
 [4. 0. 1.]]
I·A =
 [[2. 1. 0.]
 [0. 1. 3.]
 [4. 0. 1.]]</code></pre>
</div>
</div>
<p>Both equal <code>A</code>.</p>
<ol start="2" type="1">
<li>Transpose The transpose flips rows and columns.</li>
</ol>
<div id="63cbc75e" data-execution_count="93">
<div id="cb152"><pre><code><span id="cb152-1">B <span>=</span> np.array([</span>
<span id="cb152-2">    [<span>1</span>, <span>2</span>, <span>3</span>],</span>
<span id="cb152-3">    [<span>4</span>, <span>5</span>, <span>6</span>]</span>
<span id="cb152-4">])</span>
<span id="cb152-5"></span>
<span id="cb152-6"><span>print</span>(<span>"B:</span><span>\n</span><span>"</span>, B)</span>
<span id="cb152-7"><span>print</span>(<span>"B.T:</span><span>\n</span><span>"</span>, B.T)</span></code></pre></div>
<div>
<pre><code>B:
 [[1 2 3]
 [4 5 6]]
B.T:
 [[1 4]
 [2 5]
 [3 6]]</code></pre>
</div>
</div>
<ul>
<li>Original: 2×3</li>
<li>Transpose: 3×2</li>
</ul>
<p>Geometrically, transpose swaps the axes when vectors are viewed in row/column form.</p>
<ol start="3" type="1">
<li>Inverse The inverse matrix is like dividing by a number: multiplying a matrix by its inverse gives the identity.</li>
</ol>
<div id="7d0b964e" data-execution_count="94">
<div id="cb154"><pre><code><span id="cb154-1">C <span>=</span> np.array([</span>
<span id="cb154-2">    [<span>2</span>, <span>1</span>],</span>
<span id="cb154-3">    [<span>5</span>, <span>3</span>]</span>
<span id="cb154-4">])</span>
<span id="cb154-5"></span>
<span id="cb154-6">C_inv <span>=</span> np.linalg.inv(C)</span>
<span id="cb154-7"><span>print</span>(<span>"Inverse of C:</span><span>\n</span><span>"</span>, C_inv)</span>
<span id="cb154-8"></span>
<span id="cb154-9"><span>print</span>(<span>"C·C_inv =</span><span>\n</span><span>"</span>, C.dot(C_inv))</span>
<span id="cb154-10"><span>print</span>(<span>"C_inv·C =</span><span>\n</span><span>"</span>, C_inv.dot(C))</span></code></pre></div>
<div>
<pre><code>Inverse of C:
 [[ 3. -1.]
 [-5.  2.]]
C·C_inv =
 [[ 1.00000000e+00  2.22044605e-16]
 [-8.88178420e-16  1.00000000e+00]]
C_inv·C =
 [[1.00000000e+00 3.33066907e-16]
 [0.00000000e+00 1.00000000e+00]]</code></pre>
</div>
</div>
<p>Both products are (approximately) the identity.</p>
<ol start="4" type="1">
<li>Matrices that don’t have inverses Not every matrix is invertible. If a matrix is singular (determinant = 0), it has no inverse.</li>
</ol>
<div id="a9b5364c" data-execution_count="95"><pre><code><span id="cb156-1">D <span>=</span> np.array([</span>
<span id="cb156-2">    [<span>1</span>, <span>2</span>],</span>
<span id="cb156-3">    [<span>2</span>, <span>4</span>]</span>
<span id="cb156-4">])</span>
<span id="cb156-5"></span>
<span id="cb156-6"><span>try</span>:</span>
<span id="cb156-7">    np.linalg.inv(D)</span>
<span id="cb156-8"><span>except</span> np.linalg.LinAlgError <span>as</span> e:</span>
<span id="cb156-9">    <span>print</span>(<span>"Error:"</span>, e)</span></code></pre></div>
<p>Here, the second row is a multiple of the first, so <code>D</code> can’t be inverted.</p>
<ol start="5" type="1">
<li>Transpose and inverse together For invertible matrices,</li>
</ol>
<p><span>\[
(A^T)^{-1} = (A^{-1})^T
\]</span></p>
<p>We can check this numerically:</p>
<div id="c1a422ee" data-execution_count="96"><pre><code><span id="cb158-1">A <span>=</span> np.array([</span>
<span id="cb158-2">    [<span>1</span>, <span>2</span>],</span>
<span id="cb158-3">    [<span>3</span>, <span>5</span>]</span>
<span id="cb158-4">])</span>
<span id="cb158-5"></span>
<span id="cb158-6">lhs <span>=</span> np.linalg.inv(A.T)</span>
<span id="cb158-7">rhs <span>=</span> np.linalg.inv(A).T</span>
<span id="cb158-8"></span>
<span id="cb158-9"><span>print</span>(<span>"Do they match?"</span>, np.allclose(lhs, rhs))</span></code></pre></div>
</section>
<section id="try-it-yourself-14">
<h4 data-anchor-id="try-it-yourself-14">Try It Yourself</h4>
<ol type="1">
<li>Create a 4×4 identity matrix. Multiply it by any 4×1 vector. Does it change?</li>
<li>Take a random 2×2 matrix with <code>np.random.randint</code>. Compute its inverse and check if multiplying gives identity.</li>
<li>Pick a rectangular 3×2 matrix. What happens when you try <code>np.linalg.inv</code>? Why?</li>
<li>Compute <code>(A.T).T</code> for some matrix <code>A</code>. What do you notice?</li>
</ol>
</section>
</section>
<section id="symmetric-diagonal-triangular-and-permutation-matrices">
<h3 data-anchor-id="symmetric-diagonal-triangular-and-permutation-matrices">17. Symmetric, Diagonal, Triangular, and Permutation Matrices</h3>
<p>In this lab, we’ll meet four important families of special matrices. They have patterns that make them easier to understand, compute with, and use in algorithms.</p>
<section id="set-up-your-lab-16">
<h4 data-anchor-id="set-up-your-lab-16">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-16">
<h4 data-anchor-id="step-by-step-code-walkthrough-16">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Symmetric matrices A matrix is symmetric if it equals its transpose: <span>\(A = A^T\)</span>.</li>
</ol>
<div id="9742147a" data-execution_count="98">
<div id="cb161"><pre><code><span id="cb161-1">A <span>=</span> np.array([</span>
<span id="cb161-2">    [<span>2</span>, <span>3</span>, <span>4</span>],</span>
<span id="cb161-3">    [<span>3</span>, <span>5</span>, <span>6</span>],</span>
<span id="cb161-4">    [<span>4</span>, <span>6</span>, <span>8</span>]</span>
<span id="cb161-5">])</span>
<span id="cb161-6"></span>
<span id="cb161-7"><span>print</span>(<span>"A:</span><span>\n</span><span>"</span>, A)</span>
<span id="cb161-8"><span>print</span>(<span>"A.T:</span><span>\n</span><span>"</span>, A.T)</span>
<span id="cb161-9"><span>print</span>(<span>"Is symmetric?"</span>, np.allclose(A, A.T))</span></code></pre></div>
<div>
<pre><code>A:
 [[2 3 4]
 [3 5 6]
 [4 6 8]]
A.T:
 [[2 3 4]
 [3 5 6]
 [4 6 8]]
Is symmetric? True</code></pre>
</div>
</div>
<p>Symmetric matrices appear in physics, optimization, and statistics (e.g., covariance matrices).</p>
<ol start="2" type="1">
<li>Diagonal matrices A diagonal matrix has nonzero entries only on the main diagonal.</li>
</ol>
<div id="518501f7" data-execution_count="99">
<div id="cb163"><pre><code><span id="cb163-1">D <span>=</span> np.diag([<span>1</span>, <span>5</span>, <span>9</span>])</span>
<span id="cb163-2"><span>print</span>(<span>"Diagonal matrix:</span><span>\n</span><span>"</span>, D)</span>
<span id="cb163-3"></span>
<span id="cb163-4">x <span>=</span> np.array([<span>2</span>, <span>3</span>, <span>4</span>])</span>
<span id="cb163-5"><span>print</span>(<span>"D·x ="</span>, D.dot(x))  <span># scales each component</span></span></code></pre></div>
<div>
<pre><code>Diagonal matrix:
 [[1 0 0]
 [0 5 0]
 [0 0 9]]
D·x = [ 2 15 36]</code></pre>
</div>
</div>
<p>Diagonal multiplication simply scales each coordinate separately.</p>
<ol start="3" type="1">
<li>Triangular matrices Upper triangular: all entries below the diagonal are zero. Lower triangular: all entries above the diagonal are zero.</li>
</ol>
<div id="d5505799" data-execution_count="100">
<div id="cb165"><pre><code><span id="cb165-1">U <span>=</span> np.array([</span>
<span id="cb165-2">    [<span>1</span>, <span>2</span>, <span>3</span>],</span>
<span id="cb165-3">    [<span>0</span>, <span>4</span>, <span>5</span>],</span>
<span id="cb165-4">    [<span>0</span>, <span>0</span>, <span>6</span>]</span>
<span id="cb165-5">])</span>
<span id="cb165-6"></span>
<span id="cb165-7">L <span>=</span> np.array([</span>
<span id="cb165-8">    [<span>7</span>, <span>0</span>, <span>0</span>],</span>
<span id="cb165-9">    [<span>8</span>, <span>9</span>, <span>0</span>],</span>
<span id="cb165-10">    [<span>1</span>, <span>2</span>, <span>3</span>]</span>
<span id="cb165-11">])</span>
<span id="cb165-12"></span>
<span id="cb165-13"><span>print</span>(<span>"Upper triangular U:</span><span>\n</span><span>"</span>, U)</span>
<span id="cb165-14"><span>print</span>(<span>"Lower triangular L:</span><span>\n</span><span>"</span>, L)</span></code></pre></div>
<div>
<pre><code>Upper triangular U:
 [[1 2 3]
 [0 4 5]
 [0 0 6]]
Lower triangular L:
 [[7 0 0]
 [8 9 0]
 [1 2 3]]</code></pre>
</div>
</div>
<p>These are important in solving linear systems (e.g., Gaussian elimination).</p>
<ol start="4" type="1">
<li>Permutation matrices A permutation matrix rearranges the order of coordinates. Each row and each column has exactly one <code>1</code>, everything else is <code>0</code>.</li>
</ol>
<div id="be2a43c4" data-execution_count="101">
<div id="cb167"><pre><code><span id="cb167-1">P <span>=</span> np.array([</span>
<span id="cb167-2">    [<span>0</span>, <span>1</span>, <span>0</span>],</span>
<span id="cb167-3">    [<span>0</span>, <span>0</span>, <span>1</span>],</span>
<span id="cb167-4">    [<span>1</span>, <span>0</span>, <span>0</span>]</span>
<span id="cb167-5">])</span>
<span id="cb167-6"></span>
<span id="cb167-7"><span>print</span>(<span>"Permutation matrix P:</span><span>\n</span><span>"</span>, P)</span>
<span id="cb167-8"></span>
<span id="cb167-9">v <span>=</span> np.array([<span>10</span>, <span>20</span>, <span>30</span>])</span>
<span id="cb167-10"><span>print</span>(<span>"P·v ="</span>, P.dot(v))</span></code></pre></div>
<div>
<pre><code>Permutation matrix P:
 [[0 1 0]
 [0 0 1]
 [1 0 0]]
P·v = [20 30 10]</code></pre>
</div>
</div>
<p>Here, <code>P</code> cycles <code>(10,20,30)</code> into <code>(20,30,10)</code>.</p>
<ol start="5" type="1">
<li>Checking properties</li>
</ol>
<div id="f335c6be" data-execution_count="102">
<div id="cb169"><pre><code><span id="cb169-1"><span>def</span> is_symmetric(M): <span>return</span> np.allclose(M, M.T)</span>
<span id="cb169-2"><span>def</span> is_diagonal(M): <span>return</span> np.count_nonzero(M <span>-</span> np.diag(np.diag(M))) <span>==</span> <span>0</span></span>
<span id="cb169-3"><span>def</span> is_upper_triangular(M): <span>return</span> np.allclose(M, np.triu(M))</span>
<span id="cb169-4"><span>def</span> is_lower_triangular(M): <span>return</span> np.allclose(M, np.tril(M))</span>
<span id="cb169-5"></span>
<span id="cb169-6"><span>print</span>(<span>"A symmetric?"</span>, is_symmetric(A))</span>
<span id="cb169-7"><span>print</span>(<span>"D diagonal?"</span>, is_diagonal(D))</span>
<span id="cb169-8"><span>print</span>(<span>"U upper triangular?"</span>, is_upper_triangular(U))</span>
<span id="cb169-9"><span>print</span>(<span>"L lower triangular?"</span>, is_lower_triangular(L))</span></code></pre></div>
<div>
<pre><code>A symmetric? True
D diagonal? True
U upper triangular? True
L lower triangular? True</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-15">
<h4 data-anchor-id="try-it-yourself-15">Try It Yourself</h4>
<ol type="1">
<li>Create a random symmetric matrix by generating any matrix <code>M</code> and computing <code>(M + M.T)/2</code>.</li>
<li>Build a 4×4 diagonal matrix with diagonal entries <code>[2,4,6,8]</code> and multiply it by <code>[1,1,1,1]</code>.</li>
<li>Make a permutation matrix that swaps the first and last components of a 3D vector.</li>
<li>Check whether the identity matrix is diagonal, symmetric, upper triangular, and lower triangular all at once.</li>
</ol>
</section>
</section>
<section id="trace-and-basic-matrix-properties">
<h3 data-anchor-id="trace-and-basic-matrix-properties">18. Trace and Basic Matrix Properties</h3>
<p>In this lab, we’ll introduce the trace of a matrix and a few quick properties that often appear in proofs, algorithms, and applications. The trace is simple to compute but surprisingly powerful.</p>
<section id="set-up-your-lab-17">
<h4 data-anchor-id="set-up-your-lab-17">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-17">
<h4 data-anchor-id="step-by-step-code-walkthrough-17">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>What is the trace? The trace of a square matrix is the sum of its diagonal entries:</li>
</ol>
<p><span>\[
\text{tr}(A) = \sum_i A_{ii}
\]</span></p>
<div id="0727a617" data-execution_count="104">
<div id="cb172"><pre><code><span id="cb172-1">A <span>=</span> np.array([</span>
<span id="cb172-2">    [<span>2</span>, <span>1</span>, <span>3</span>],</span>
<span id="cb172-3">    [<span>0</span>, <span>4</span>, <span>5</span>],</span>
<span id="cb172-4">    [<span>7</span>, <span>8</span>, <span>6</span>]</span>
<span id="cb172-5">])</span>
<span id="cb172-6"></span>
<span id="cb172-7">trace_A <span>=</span> np.trace(A)</span>
<span id="cb172-8"><span>print</span>(<span>"Matrix A:</span><span>\n</span><span>"</span>, A)</span>
<span id="cb172-9"><span>print</span>(<span>"Trace of A ="</span>, trace_A)</span></code></pre></div>
<div>
<pre><code>Matrix A:
 [[2 1 3]
 [0 4 5]
 [7 8 6]]
Trace of A = 12</code></pre>
</div>
</div>
<p>Here, trace = <span>\(2 + 4 + 6 = 12\)</span>.</p>
<ol start="2" type="1">
<li>Trace is linear For matrices <code>A</code> and <code>B</code>:</li>
</ol>
<p><span>\[
\text{tr}(A+B) = \text{tr}(A) + \text{tr}(B)
\]</span></p>
<p><span>\[
\text{tr}(cA) = c \cdot \text{tr}(A)
\]</span></p>
<div id="a6ff0d5a" data-execution_count="105">
<div id="cb174"><pre><code><span id="cb174-1">B <span>=</span> np.array([</span>
<span id="cb174-2">    [<span>1</span>, <span>0</span>, <span>0</span>],</span>
<span id="cb174-3">    [<span>0</span>, <span>2</span>, <span>0</span>],</span>
<span id="cb174-4">    [<span>0</span>, <span>0</span>, <span>3</span>]</span>
<span id="cb174-5">])</span>
<span id="cb174-6"></span>
<span id="cb174-7"><span>print</span>(<span>"tr(A+B) ="</span>, np.trace(A<span>+</span>B))</span>
<span id="cb174-8"><span>print</span>(<span>"tr(A) + tr(B) ="</span>, np.trace(A) <span>+</span> np.trace(B))</span>
<span id="cb174-9"></span>
<span id="cb174-10"><span>print</span>(<span>"tr(3A) ="</span>, np.trace(<span>3</span><span>*</span>A))</span>
<span id="cb174-11"><span>print</span>(<span>"3 * tr(A) ="</span>, <span>3</span><span>*</span>np.trace(A))</span></code></pre></div>
<div>
<pre><code>tr(A+B) = 18
tr(A) + tr(B) = 18
tr(3A) = 36
3 * tr(A) = 36</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Trace of a product One important property is:</li>
</ol>
<p><span>\[
\text{tr}(AB) = \text{tr}(BA)
\]</span></p>
<div id="4958c8fd" data-execution_count="106"><pre><code><span id="cb176-1">C <span>=</span> np.array([</span>
<span id="cb176-2">    [<span>0</span>,<span>1</span>],</span>
<span id="cb176-3">    [<span>2</span>,<span>3</span>]</span>
<span id="cb176-4">])</span>
<span id="cb176-5"></span>
<span id="cb176-6">D <span>=</span> np.array([</span>
<span id="cb176-7">    [<span>4</span>,<span>5</span>],</span>
<span id="cb176-8">    [<span>6</span>,<span>7</span>]</span>
<span id="cb176-9">])</span>
<span id="cb176-10"></span>
<span id="cb176-11"><span>print</span>(<span>"tr(CD) ="</span>, np.trace(C.dot(D)))</span>
<span id="cb176-12"><span>print</span>(<span>"tr(DC) ="</span>, np.trace(D.dot(C)))</span></code></pre></div>
<p>Both are equal, even though <code>CD</code> and <code>DC</code> are different matrices.</p>
<ol start="4" type="1">
<li>Trace and eigenvalues The trace equals the sum of eigenvalues of a matrix (counting multiplicities).</li>
</ol>
<div id="50502a18" data-execution_count="107">
<div id="cb178"><pre><code><span id="cb178-1">vals, vecs <span>=</span> np.linalg.eig(A)</span>
<span id="cb178-2"><span>print</span>(<span>"Eigenvalues:"</span>, vals)</span>
<span id="cb178-3"><span>print</span>(<span>"Sum of eigenvalues ="</span>, np.<span>sum</span>(vals))</span>
<span id="cb178-4"><span>print</span>(<span>"Trace ="</span>, np.trace(A))</span></code></pre></div>
<div>
<pre><code>Eigenvalues: [12.83286783  2.13019807 -2.9630659 ]
Sum of eigenvalues = 12.000000000000007
Trace = 12</code></pre>
</div>
</div>
<p>The results should match (within rounding error).</p>
<ol start="5" type="1">
<li>Quick invariants</li>
</ol>
<ul>
<li>Trace doesn’t change under transpose: <code>tr(A) = tr(A.T)</code></li>
<li>Trace doesn’t change under similarity transforms: <code>tr(P^-1 A P) = tr(A)</code></li>
</ul>
<div id="3eec7482" data-execution_count="108"><pre><code><span id="cb180-1"><span>print</span>(<span>"tr(A) ="</span>, np.trace(A))</span>
<span id="cb180-2"><span>print</span>(<span>"tr(A.T) ="</span>, np.trace(A.T))</span></code></pre></div>
</section>
<section id="try-it-yourself-16">
<h4 data-anchor-id="try-it-yourself-16">Try It Yourself</h4>
<ol type="1">
<li><p>Create a 2×2 rotation matrix for 90°:</p>
<p><span>\[
R = \begin{bmatrix}0 &amp; -1 \\ 1 &amp; 0\end{bmatrix}
\]</span></p>
<p>What is its trace? What does that tell you about its eigenvalues?</p></li>
<li><p>Make a random 3×3 matrix and compare <code>tr(A)</code> with the sum of eigenvalues.</p></li>
<li><p>Test <code>tr(AB)</code> and <code>tr(BA)</code> with a rectangular matrix <code>A</code> (e.g.&nbsp;2×3) and <code>B</code> (3×2). Do they still match?</p></li>
</ol>
</section>
</section>
<section id="affine-transforms-and-homogeneous-coordinates">
<h3 data-anchor-id="affine-transforms-and-homogeneous-coordinates">19. Affine Transforms and Homogeneous Coordinates</h3>
<p>Affine transformations let us do more than just linear operations - they include translations (shifting points), which ordinary matrices can’t handle alone. To unify rotations, scalings, reflections, and translations, we use homogeneous coordinates.</p>
<section id="set-up-your-lab-18">
<h4 data-anchor-id="set-up-your-lab-18">Set Up Your Lab</h4>
<div id="4421cd9a" data-execution_count="109"><pre><code><span id="cb182-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb182-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-18">
<h4 data-anchor-id="step-by-step-code-walkthrough-18">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Linear transformations vs affine transformations</li>
</ol>
<ul>
<li>A linear transformation can rotate, scale, or shear, but always keeps the origin fixed.</li>
<li>An affine transformation allows translation as well.</li>
</ul>
<p>For example, shifting every point by <code>(2,3)</code> is affine but not linear.</p>
<ol start="2" type="1">
<li>Homogeneous coordinates idea We add an extra coordinate (usually <code>1</code>) to vectors.</li>
</ol>
<ul>
<li>A 2D point <code>(x,y)</code> becomes <code>(x,y,1)</code>.</li>
<li>A 3D point <code>(x,y,z)</code> becomes <code>(x,y,z,1)</code>.</li>
</ul>
<p>This trick lets us represent translations using matrix multiplication.</p>
<ol start="3" type="1">
<li>2D translation matrix</li>
</ol>
<p><span>\[
T = \begin{bmatrix}
1 &amp; 0 &amp; t_x \\
0 &amp; 1 &amp; t_y \\
0 &amp; 0 &amp; 1
\end{bmatrix}
\]</span></p>
<div id="e420eb7a" data-execution_count="110">
<div id="cb183"><pre><code><span id="cb183-1">T <span>=</span> np.array([</span>
<span id="cb183-2">    [<span>1</span>, <span>0</span>, <span>2</span>],</span>
<span id="cb183-3">    [<span>0</span>, <span>1</span>, <span>3</span>],</span>
<span id="cb183-4">    [<span>0</span>, <span>0</span>, <span>1</span>]</span>
<span id="cb183-5">])</span>
<span id="cb183-6"></span>
<span id="cb183-7">p <span>=</span> np.array([<span>1</span>, <span>1</span>, <span>1</span>])  <span># point at (1,1)</span></span>
<span id="cb183-8">p_translated <span>=</span> T.dot(p)</span>
<span id="cb183-9"></span>
<span id="cb183-10"><span>print</span>(<span>"Original point:"</span>, p)</span>
<span id="cb183-11"><span>print</span>(<span>"Translated point:"</span>, p_translated)</span></code></pre></div>
<div>
<pre><code>Original point: [1 1 1]
Translated point: [3 4 1]</code></pre>
</div>
</div>
<p>This shifts <code>(1,1)</code> to <code>(3,4)</code>.</p>
<ol start="4" type="1">
<li>Combining rotation and translation</li>
</ol>
<p>A 90° rotation around the origin in 2D:</p>
<div id="0700f9de" data-execution_count="111">
<div id="cb185"><pre><code><span id="cb185-1">R <span>=</span> np.array([</span>
<span id="cb185-2">    [<span>0</span>, <span>-</span><span>1</span>, <span>0</span>],</span>
<span id="cb185-3">    [<span>1</span>,  <span>0</span>, <span>0</span>],</span>
<span id="cb185-4">    [<span>0</span>,  <span>0</span>, <span>1</span>]</span>
<span id="cb185-5">])</span>
<span id="cb185-6"></span>
<span id="cb185-7">M <span>=</span> T.dot(R)  <span># rotate then translate</span></span>
<span id="cb185-8"><span>print</span>(<span>"Combined transform:</span><span>\n</span><span>"</span>, M)</span>
<span id="cb185-9"></span>
<span id="cb185-10">p <span>=</span> np.array([<span>1</span>, <span>0</span>, <span>1</span>])</span>
<span id="cb185-11"><span>print</span>(<span>"Rotated + translated point:"</span>, M.dot(p))</span></code></pre></div>
<div>
<pre><code>Combined transform:
 [[ 0 -1  2]
 [ 1  0  3]
 [ 0  0  1]]
Rotated + translated point: [2 4 1]</code></pre>
</div>
</div>
<p>Now we can apply rotation and translation in one step.</p>
<ol start="5" type="1">
<li>Visualization of translation</li>
</ol>
<div id="e2fd8359" data-execution_count="112">
<div id="cb187"><pre><code><span id="cb187-1">points <span>=</span> np.array([</span>
<span id="cb187-2">    [<span>0</span>,<span>0</span>,<span>1</span>],</span>
<span id="cb187-3">    [<span>1</span>,<span>0</span>,<span>1</span>],</span>
<span id="cb187-4">    [<span>1</span>,<span>1</span>,<span>1</span>],</span>
<span id="cb187-5">    [<span>0</span>,<span>1</span>,<span>1</span>]</span>
<span id="cb187-6">])  <span># a unit square</span></span>
<span id="cb187-7"></span>
<span id="cb187-8">transformed <span>=</span> points.dot(T.T)</span>
<span id="cb187-9"></span>
<span id="cb187-10">plt.scatter(points[:,<span>0</span>], points[:,<span>1</span>], color<span>=</span><span>'r'</span>, label<span>=</span><span>'original'</span>)</span>
<span id="cb187-11">plt.scatter(transformed[:,<span>0</span>], transformed[:,<span>1</span>], color<span>=</span><span>'b'</span>, label<span>=</span><span>'translated'</span>)</span>
<span id="cb187-12"></span>
<span id="cb187-13"><span>for</span> i <span>in</span> <span>range</span>(<span>len</span>(points)):</span>
<span id="cb187-14">    plt.arrow(points[i,<span>0</span>], points[i,<span>1</span>],</span>
<span id="cb187-15">              transformed[i,<span>0</span>]<span>-</span>points[i,<span>0</span>],</span>
<span id="cb187-16">              transformed[i,<span>1</span>]<span>-</span>points[i,<span>1</span>],</span>
<span id="cb187-17">              head_width<span>=</span><span>0.05</span>, color<span>=</span><span>'gray'</span>)</span>
<span id="cb187-18"></span>
<span id="cb187-19">plt.legend()</span>
<span id="cb187-20">plt.axis(<span>'equal'</span>)</span>
<span id="cb187-21">plt.grid()</span>
<span id="cb187-22">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-113-output-1.png" width="571" height="411"></p>
</figure>
</div>
</div>
<p>You’ll see the red unit square moved to a blue unit square shifted by <code>(2,3)</code>.</p>
<ol start="6" type="1">
<li>Extending to 3D In 3D, homogeneous coordinates use 4×4 matrices. Translations, rotations, and scalings all fit the same framework.</li>
</ol>
<div id="5677d97b" data-execution_count="113">
<div id="cb188"><pre><code><span id="cb188-1">T3 <span>=</span> np.array([</span>
<span id="cb188-2">    [<span>1</span>,<span>0</span>,<span>0</span>,<span>5</span>],</span>
<span id="cb188-3">    [<span>0</span>,<span>1</span>,<span>0</span>,<span>-</span><span>2</span>],</span>
<span id="cb188-4">    [<span>0</span>,<span>0</span>,<span>1</span>,<span>3</span>],</span>
<span id="cb188-5">    [<span>0</span>,<span>0</span>,<span>0</span>,<span>1</span>]</span>
<span id="cb188-6">])</span>
<span id="cb188-7"></span>
<span id="cb188-8">p3 <span>=</span> np.array([<span>1</span>,<span>2</span>,<span>3</span>,<span>1</span>])</span>
<span id="cb188-9"><span>print</span>(<span>"Translated 3D point:"</span>, T3.dot(p3))</span></code></pre></div>
<div>
<pre><code>Translated 3D point: [6 0 6 1]</code></pre>
</div>
</div>
<p>This shifts <code>(1,2,3)</code> to <code>(6,0,6)</code>.</p>
</section>
<section id="try-it-yourself-17">
<h4 data-anchor-id="try-it-yourself-17">Try It Yourself</h4>
<ol type="1">
<li>Build a scaling matrix in homogeneous coordinates that doubles both x and y, and apply it to <code>(1,1)</code>.</li>
<li>Create a 2D transform that rotates by 90° and then shifts by <code>(−2,1)</code>. Apply it to <code>(0,2)</code>.</li>
<li>In 3D, translate <code>(0,0,0)</code> by <code>(10,10,10)</code>. What homogeneous matrix did you use?</li>
</ol>
</section>
</section>
<section id="computing-with-matrices-cost-counts-and-simple-speedups">
<h3 data-anchor-id="computing-with-matrices-cost-counts-and-simple-speedups">20. Computing with Matrices (Cost Counts and Simple Speedups)</h3>
<p>Working with matrices is not just about theory - in practice, we care about how much computation it takes to perform operations, and how we can make them faster. This lab introduces basic cost analysis (counting operations) and demonstrates simple NumPy optimizations.</p>
<section id="set-up-your-lab-19">
<h4 data-anchor-id="set-up-your-lab-19">Set Up Your Lab</h4>
<div id="103969ab" data-execution_count="114"><pre><code><span id="cb190-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb190-2"><span>import</span> time</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-19">
<h4 data-anchor-id="step-by-step-code-walkthrough-19">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Counting operations (matrix–vector multiply)</li>
</ol>
<p>If <code>A</code> is an <span>\(m \times n\)</span> matrix and <code>x</code> is an <span>\(n\)</span>-dimensional vector, computing <code>A·x</code> takes about <span>\(m \times n\)</span> multiplications and the same number of additions.</p>
<div id="e1461123" data-execution_count="115">
<div id="cb191"><pre><code><span id="cb191-1">m, n <span>=</span> <span>3</span>, <span>4</span></span>
<span id="cb191-2">A <span>=</span> np.random.randint(<span>1</span>,<span>10</span>,(m,n))</span>
<span id="cb191-3">x <span>=</span> np.random.randint(<span>1</span>,<span>10</span>,n)</span>
<span id="cb191-4"></span>
<span id="cb191-5"><span>print</span>(<span>"Matrix A:</span><span>\n</span><span>"</span>, A)</span>
<span id="cb191-6"><span>print</span>(<span>"Vector x:"</span>, x)</span>
<span id="cb191-7"><span>print</span>(<span>"A·x ="</span>, A.dot(x))</span></code></pre></div>
<div>
<pre><code>Matrix A:
 [[6 6 6 2]
 [1 1 1 1]
 [1 8 7 4]]
Vector x: [6 5 4 5]
A·x = [100  20  94]</code></pre>
</div>
</div>
<p>Here the cost is <span>\(3 \times 4 = 12\)</span> multiplications + 12 additions.</p>
<ol start="2" type="1">
<li>Counting operations (matrix–matrix multiply)</li>
</ol>
<p>For an <span>\(m \times n\)</span> times <span>\(n \times p\)</span> multiplication, the cost is about <span>\(m \times n \times p\)</span>.</p>
<div id="bf7e874f" data-execution_count="116">
<div id="cb193"><pre><code><span id="cb193-1">m, n, p <span>=</span> <span>3</span>, <span>4</span>, <span>2</span></span>
<span id="cb193-2">A <span>=</span> np.random.randint(<span>1</span>,<span>10</span>,(m,n))</span>
<span id="cb193-3">B <span>=</span> np.random.randint(<span>1</span>,<span>10</span>,(n,p))</span>
<span id="cb193-4"></span>
<span id="cb193-5">C <span>=</span> A.dot(B)</span>
<span id="cb193-6"><span>print</span>(<span>"A·B =</span><span>\n</span><span>"</span>, C)</span></code></pre></div>
<div>
<pre><code>A·B =
 [[ 59  92]
 [ 43  81]
 [ 65 102]]</code></pre>
</div>
</div>
<p>Here the cost is <span>\(3 \times 4 \times 2 = 24\)</span> multiplications + 24 additions.</p>
<ol start="3" type="1">
<li>Timing with NumPy (vectorized vs loop)</li>
</ol>
<p>NumPy is optimized in C and Fortran under the hood. Let’s compare matrix multiplication with and without vectorization.</p>
<div id="f9bebf65" data-execution_count="117">
<div id="cb195"><pre><code><span id="cb195-1">n <span>=</span> <span>50</span></span>
<span id="cb195-2">A <span>=</span> np.random.randn(n,n)</span>
<span id="cb195-3">B <span>=</span> np.random.randn(n,n)</span>
<span id="cb195-4"></span>
<span id="cb195-5"><span># Vectorized</span></span>
<span id="cb195-6">start <span>=</span> time.time()</span>
<span id="cb195-7">C1 <span>=</span> A.dot(B)</span>
<span id="cb195-8">end <span>=</span> time.time()</span>
<span id="cb195-9"><span>print</span>(<span>"Vectorized dot:"</span>, <span>round</span>(end<span>-</span>start,<span>3</span>), <span>"seconds"</span>)</span>
<span id="cb195-10"></span>
<span id="cb195-11"><span># Manual loops</span></span>
<span id="cb195-12">C2 <span>=</span> np.zeros((n,n))</span>
<span id="cb195-13">start <span>=</span> time.time()</span>
<span id="cb195-14"><span>for</span> i <span>in</span> <span>range</span>(n):</span>
<span id="cb195-15">    <span>for</span> j <span>in</span> <span>range</span>(n):</span>
<span id="cb195-16">        <span>for</span> k <span>in</span> <span>range</span>(n):</span>
<span id="cb195-17">            C2[i,j] <span>+=</span> A[i,k]<span>*</span>B[k,j]</span>
<span id="cb195-18">end <span>=</span> time.time()</span>
<span id="cb195-19"><span>print</span>(<span>"Triple loop:"</span>, <span>round</span>(end<span>-</span>start,<span>3</span>), <span>"seconds"</span>)</span></code></pre></div>
<div>
<pre><code>Vectorized dot: 0.0 seconds
Triple loop: 0.026 seconds</code></pre>
</div>
</div>
<p>The vectorized version should be thousands of times faster.</p>
<ol start="4" type="1">
<li>Broadcasting tricks</li>
</ol>
<p>NumPy lets us avoid loops by broadcasting operations across entire rows or columns.</p>
<div id="0c94fbbf" data-execution_count="118">
<div id="cb197"><pre><code><span id="cb197-1">A <span>=</span> np.array([</span>
<span id="cb197-2">    [<span>1</span>,<span>2</span>,<span>3</span>],</span>
<span id="cb197-3">    [<span>4</span>,<span>5</span>,<span>6</span>]</span>
<span id="cb197-4">])</span>
<span id="cb197-5"></span>
<span id="cb197-6"><span># Add 10 to every entry</span></span>
<span id="cb197-7"><span>print</span>(<span>"A+10 =</span><span>\n</span><span>"</span>, A<span>+</span><span>10</span>)</span>
<span id="cb197-8"></span>
<span id="cb197-9"><span># Multiply each row by a different scalar</span></span>
<span id="cb197-10">scales <span>=</span> np.array([<span>1</span>,<span>10</span>])[:,<span>None</span>]</span>
<span id="cb197-11"><span>print</span>(<span>"Row-scaled A =</span><span>\n</span><span>"</span>, A<span>*</span>scales)</span></code></pre></div>
<div>
<pre><code>A+10 =
 [[11 12 13]
 [14 15 16]]
Row-scaled A =
 [[ 1  2  3]
 [40 50 60]]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Memory and data types</li>
</ol>
<p>For large computations, data type matters.</p>
<div id="c61e5792" data-execution_count="119">
<div id="cb199"><pre><code><span id="cb199-1">A <span>=</span> np.random.randn(<span>1000</span>,<span>1000</span>).astype(np.float32)  <span># 32-bit floats</span></span>
<span id="cb199-2">B <span>=</span> np.random.randn(<span>1000</span>,<span>1000</span>).astype(np.float32)</span>
<span id="cb199-3"></span>
<span id="cb199-4">start <span>=</span> time.time()</span>
<span id="cb199-5">C <span>=</span> A.dot(B)</span>
<span id="cb199-6"><span>print</span>(<span>"Result shape:"</span>, C.shape, <span>"dtype:"</span>, C.dtype)</span>
<span id="cb199-7"><span>print</span>(<span>"Time:"</span>, <span>round</span>(time.time()<span>-</span>start,<span>3</span>), <span>"seconds"</span>)</span></code></pre></div>
<div>
<pre><code>Result shape: (1000, 1000) dtype: float32
Time: 0.002 seconds</code></pre>
</div>
</div>
<p>Using <code>float32</code> instead of <code>float64</code> halves memory use and can speed up computation (at the cost of some precision).</p>
</section>
<section id="try-it-yourself-18">
<h4 data-anchor-id="try-it-yourself-18">Try It Yourself</h4>
<ol type="1">
<li>Compute the cost of multiplying a 200×500 matrix with a 500×1000 matrix. How many multiplications are needed?</li>
<li>Time matrix multiplication for sizes 100, 500, 1000 in NumPy. How does the time scale?</li>
<li>Experiment with <code>float32</code> vs <code>float64</code> in NumPy. How do speed and memory change?</li>
<li>Try broadcasting: multiply each column of a matrix by <code>[1,2,3,...]</code>.</li>
</ol>
</section>
<section id="the-takeaway-2">
<h4 data-anchor-id="the-takeaway-2">The Takeaway</h4>
<ul>
<li>Matrix operations have predictable computational costs: <code>A·x</code> ~ <span>\(m \times n\)</span>, <code>A·B</code> ~ <span>\(m \times n \times p\)</span>.</li>
<li>Vectorized NumPy operations are vastly faster than Python loops.</li>
<li>Broadcasting and choosing the right data type are simple speedups every beginner should learn.</li>
</ul>
</section>
</section>
</section>
<section id="chapter-3.-linear-systems-and-elimination">
<h2 data-anchor-id="chapter-3.-linear-systems-and-elimination">Chapter 3. Linear Systems and Elimination</h2>
<section id="from-equations-to-matrices-augmenting-and-encoding">
<h3 data-anchor-id="from-equations-to-matrices-augmenting-and-encoding">21. From Equations to Matrices (Augmenting and Encoding)</h3>
<p>Linear algebra often begins with solving systems of linear equations. For example:</p>
<p><span>\[
\begin{cases}  
x + 2y = 5 \\  
3x - y = 4  
\end{cases}
\]</span></p>
<p>Instead of juggling symbols, we can encode the entire system into a matrix. This is the key idea that lets computers handle thousands or millions of equations efficiently.</p>
<section id="set-up-your-lab-20">
<h4 data-anchor-id="set-up-your-lab-20">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-20">
<h4 data-anchor-id="step-by-step-code-walkthrough-20">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Write a system of equations</li>
</ol>
<p>We’ll use this small example:</p>
<p><span>\[
\begin{cases}  
2x + y = 8 \\  
-3x + 4y = -11  
\end{cases}
\]</span></p>
<ol start="2" type="1">
<li>Encode coefficients and constants</li>
</ol>
<ul>
<li>Coefficient matrix <span>\(A\)</span>: numbers multiplying variables.</li>
<li>Variable vector <span>\(x\)</span>: unknowns <code>[x, y]</code>.</li>
<li>Constant vector <span>\(b\)</span>: right-hand side.</li>
</ul>
<div id="6a6169f5" data-execution_count="121">
<div id="cb202"><pre><code><span id="cb202-1">A <span>=</span> np.array([</span>
<span id="cb202-2">    [<span>2</span>, <span>1</span>],</span>
<span id="cb202-3">    [<span>-</span><span>3</span>, <span>4</span>]</span>
<span id="cb202-4">])</span>
<span id="cb202-5"></span>
<span id="cb202-6">b <span>=</span> np.array([<span>8</span>, <span>-</span><span>11</span>])</span>
<span id="cb202-7"></span>
<span id="cb202-8"><span>print</span>(<span>"Coefficient matrix A:</span><span>\n</span><span>"</span>, A)</span>
<span id="cb202-9"><span>print</span>(<span>"Constants vector b:"</span>, b)</span></code></pre></div>
<div>
<pre><code>Coefficient matrix A:
 [[ 2  1]
 [-3  4]]
Constants vector b: [  8 -11]</code></pre>
</div>
</div>
<p>So the system is <span>\(A·x = b\)</span>.</p>
<ol start="3" type="1">
<li>Augmented matrix</li>
</ol>
<p>We can bundle the system into one compact matrix:</p>
<p><span>\[
[A|b] = \begin{bmatrix}2 &amp; 1 &amp; | &amp; 8 \\ -3 &amp; 4 &amp; | &amp; -11 \end{bmatrix}
\]</span></p>
<div id="a55e8e75" data-execution_count="122">
<div id="cb204"><pre><code><span id="cb204-1">augmented <span>=</span> np.column_stack((A, b))</span>
<span id="cb204-2"><span>print</span>(<span>"Augmented matrix:</span><span>\n</span><span>"</span>, augmented)</span></code></pre></div>
<div>
<pre><code>Augmented matrix:
 [[  2   1   8]
 [ -3   4 -11]]</code></pre>
</div>
</div>
<p>This format is useful for elimination algorithms.</p>
<ol start="4" type="1">
<li>Solving directly with NumPy</li>
</ol>
<div id="84bd6cc5" data-execution_count="123">
<div id="cb206"><pre><code><span id="cb206-1">solution <span>=</span> np.linalg.solve(A, b)</span>
<span id="cb206-2"><span>print</span>(<span>"Solution (x,y):"</span>, solution)</span></code></pre></div>
<div>
<pre><code>Solution (x,y): [3.90909091 0.18181818]</code></pre>
</div>
</div>
<p>Here NumPy solves the system using efficient algorithms.</p>
<ol start="5" type="1">
<li>Checking the solution</li>
</ol>
<p>Always verify:</p>
<div id="8039438f" data-execution_count="124">
<div id="cb208"><pre><code><span id="cb208-1">check <span>=</span> A.dot(solution)</span>
<span id="cb208-2"><span>print</span>(<span>"A·x ="</span>, check, <span>"should equal b ="</span>, b)</span></code></pre></div>
<div>
<pre><code>A·x = [  8. -11.] should equal b = [  8 -11]</code></pre>
</div>
</div>
<ol start="6" type="1">
<li>Another example (3 variables)</li>
</ol>
<p><span>\[
\begin{cases}  
x + y + z = 6 \\  
2x - y + z = 3 \\  
- x + 2y - z = 2  
\end{cases}
\]</span></p>
<div id="57d31f2e" data-execution_count="125">
<div id="cb210"><pre><code><span id="cb210-1">A <span>=</span> np.array([</span>
<span id="cb210-2">    [<span>1</span>, <span>1</span>, <span>1</span>],</span>
<span id="cb210-3">    [<span>2</span>, <span>-</span><span>1</span>, <span>1</span>],</span>
<span id="cb210-4">    [<span>-</span><span>1</span>, <span>2</span>, <span>-</span><span>1</span>]</span>
<span id="cb210-5">])</span>
<span id="cb210-6"></span>
<span id="cb210-7">b <span>=</span> np.array([<span>6</span>, <span>3</span>, <span>2</span>])</span>
<span id="cb210-8"></span>
<span id="cb210-9"><span>print</span>(<span>"Augmented matrix:</span><span>\n</span><span>"</span>, np.column_stack((A, b)))</span>
<span id="cb210-10"><span>print</span>(<span>"Solution:"</span>, np.linalg.solve(A, b))</span></code></pre></div>
<div>
<pre><code>Augmented matrix:
 [[ 1  1  1  6]
 [ 2 -1  1  3]
 [-1  2 -1  2]]
Solution: [2.33333333 2.66666667 1.        ]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-19">
<h4 data-anchor-id="try-it-yourself-19">Try It Yourself</h4>
<ol type="1">
<li><p>Encode the system:</p>
<p><span>\[
\begin{cases}  
2x - y = 1 \\  
x + 3y = 7  
\end{cases}
\]</span></p>
<p>Write <code>A</code> and <code>b</code>, then solve.</p></li>
<li><p>For a 3×3 system, try creating a random coefficient matrix with <code>np.random.randint(-5,5,(3,3))</code> and a random <code>b</code>. Use <code>np.linalg.solve</code>.</p></li>
<li><p>Modify the constants <code>b</code> slightly and see how the solution changes. This introduces the idea of sensitivity.</p></li>
</ol>
</section>
<section id="the-takeaway-3">
<h4 data-anchor-id="the-takeaway-3">The Takeaway</h4>
<ul>
<li>Systems of linear equations can be neatly written as <span>\(A·x = b\)</span>.</li>
<li>The augmented matrix <span>\([A|b]\)</span> is a compact way to set up elimination.</li>
<li>This matrix encoding transforms algebra problems into matrix problems - the gateway to all of linear algebra.</li>
</ul>
</section>
</section>
<section id="row-operations-legal-moves-that-keep-solutions">
<h3 data-anchor-id="row-operations-legal-moves-that-keep-solutions">22. Row Operations (Legal Moves That Keep Solutions)</h3>
<p>When solving linear systems, we don’t want to change the solutions - just simplify the system into an easier form. This is where row operations come in. They are the “legal moves” we can do on an augmented matrix <span>\([A|b]\)</span> without changing the solution set.</p>
<section id="set-up-your-lab-21">
<h4 data-anchor-id="set-up-your-lab-21">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-21">
<h4 data-anchor-id="step-by-step-code-walkthrough-21">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li><p>Three legal row operations</p></li>
<li><p>Swap two rows <span>\((R_i \leftrightarrow R_j)\)</span></p></li>
<li><p>Multiply a row by a nonzero scalar <span>\((R_i \to c·R_i)\)</span></p></li>
<li><p>Replace a row with itself plus a multiple of another row <span>\((R_i \to R_i + c·R_j)\)</span></p></li>
</ol>
<p>These preserve the solution set.</p>
<ol start="2" type="1">
<li>Start with an augmented matrix</li>
</ol>
<p>System:</p>
<p><span>\[
\begin{cases}  
x + 2y = 5 \\  
3x + 4y = 6  
\end{cases}
\]</span></p>
<div id="8c223c3c" data-execution_count="127">
<div id="cb213"><pre><code><span id="cb213-1">A <span>=</span> np.array([</span>
<span id="cb213-2">    [<span>1</span>, <span>2</span>, <span>5</span>],</span>
<span id="cb213-3">    [<span>3</span>, <span>4</span>, <span>6</span>]</span>
<span id="cb213-4">], dtype<span>=</span><span>float</span>)</span>
<span id="cb213-5"></span>
<span id="cb213-6"><span>print</span>(<span>"Initial augmented matrix:</span><span>\n</span><span>"</span>, A)</span></code></pre></div>
<div>
<pre><code>Initial augmented matrix:
 [[1. 2. 5.]
 [3. 4. 6.]]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Row swap</li>
</ol>
<p>Swap row 0 and row 1.</p>
<div id="86fba1f6" data-execution_count="128">
<div id="cb215"><pre><code><span id="cb215-1">A[[<span>0</span>,<span>1</span>]] <span>=</span> A[[<span>1</span>,<span>0</span>]]</span>
<span id="cb215-2"><span>print</span>(<span>"After swapping rows:</span><span>\n</span><span>"</span>, A)</span></code></pre></div>
<div>
<pre><code>After swapping rows:
 [[3. 4. 6.]
 [1. 2. 5.]]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Multiply a row by a scalar</li>
</ol>
<p>Make the pivot in row 0 equal to 1.</p>
<div id="df5c841e" data-execution_count="129">
<div id="cb217"><pre><code><span id="cb217-1">A[<span>0</span>] <span>=</span> A[<span>0</span>] <span>/</span> A[<span>0</span>,<span>0</span>]</span>
<span id="cb217-2"><span>print</span>(<span>"After scaling first row:</span><span>\n</span><span>"</span>, A)</span></code></pre></div>
<div>
<pre><code>After scaling first row:
 [[1.         1.33333333 2.        ]
 [1.         2.         5.        ]]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Add a multiple of another row</li>
</ol>
<p>Eliminate the first column of row 1.</p>
<div id="d24792e2" data-execution_count="130">
<div id="cb219"><pre><code><span id="cb219-1">A[<span>1</span>] <span>=</span> A[<span>1</span>] <span>-</span> <span>3</span><span>*</span>A[<span>0</span>]</span>
<span id="cb219-2"><span>print</span>(<span>"After eliminating x from second row:</span><span>\n</span><span>"</span>, A)</span></code></pre></div>
<div>
<pre><code>After eliminating x from second row:
 [[ 1.          1.33333333  2.        ]
 [-2.         -2.         -1.        ]]</code></pre>
</div>
</div>
<p>Now the system is simpler: second row has only <code>y</code>.</p>
<ol start="6" type="1">
<li>Solving from the new system</li>
</ol>
<div id="db898428" data-execution_count="131">
<div id="cb221"><pre><code><span id="cb221-1">y <span>=</span> A[<span>1</span>,<span>2</span>] <span>/</span> A[<span>1</span>,<span>1</span>]</span>
<span id="cb221-2">x <span>=</span> (A[<span>0</span>,<span>2</span>] <span>-</span> A[<span>0</span>,<span>1</span>]<span>*</span>y) <span>/</span> A[<span>0</span>,<span>0</span>]</span>
<span id="cb221-3"><span>print</span>(<span>"Solution: x ="</span>, x, <span>", y ="</span>, y)</span></code></pre></div>
<div>
<pre><code>Solution: x = 1.3333333333333335 , y = 0.5</code></pre>
</div>
</div>
<ol start="7" type="1">
<li>Using NumPy step-by-step vs solver</li>
</ol>
<div id="c69385f3" data-execution_count="132">
<div id="cb223"><pre><code><span id="cb223-1">coeff <span>=</span> np.array([[<span>1</span>,<span>2</span>],[<span>3</span>,<span>4</span>]])</span>
<span id="cb223-2">const <span>=</span> np.array([<span>5</span>,<span>6</span>])</span>
<span id="cb223-3"><span>print</span>(<span>"np.linalg.solve result:"</span>, np.linalg.solve(coeff,const))</span></code></pre></div>
<div>
<pre><code>np.linalg.solve result: [-4.   4.5]</code></pre>
</div>
</div>
<p>Both methods give the same solution.</p>
</section>
<section id="try-it-yourself-20">
<h4 data-anchor-id="try-it-yourself-20">Try It Yourself</h4>
<ol type="1">
<li><p>Take the system:</p>
<p><span>\[
\begin{cases}  
2x + y = 7 \\  
x - y = 1  
\end{cases}
\]</span></p>
<p>Write its augmented matrix, then:</p>
<ul>
<li>Swap rows.</li>
<li>Scale the first row.</li>
<li>Eliminate one variable.</li>
</ul></li>
<li><p>Create a random 3×3 system with integers between -5 and 5. Perform at least one of each row operation manually in code.</p></li>
<li><p>Experiment with multiplying a row by <code>0</code>. What happens, and why is this not allowed as a legal operation?</p></li>
</ol>
</section>
<section id="the-takeaway-4">
<h4 data-anchor-id="the-takeaway-4">The Takeaway</h4>
<ul>
<li>The three legal row operations are row swap, row scaling, and row replacement.</li>
<li>These steps preserve the solution set while moving toward a simpler form.</li>
<li>They are the foundation of Gaussian elimination, the standard algorithm for solving linear systems.</li>
</ul>
</section>
</section>
<section id="row-echelon-and-reduced-row-echelon-forms-target-shapes">
<h3 data-anchor-id="row-echelon-and-reduced-row-echelon-forms-target-shapes">23. Row-Echelon and Reduced Row-Echelon Forms (Target Shapes)</h3>
<p>When solving systems, our goal is to simplify the augmented matrix into a standard shape where the solutions are easy to read. These shapes are called row-echelon form (REF) and reduced row-echelon form (RREF).</p>
<section id="set-up-your-lab-22">
<h4 data-anchor-id="set-up-your-lab-22">Set Up Your Lab</h4>
<div id="24aacc77" data-execution_count="133"><pre><code><span id="cb225-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb225-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
<p>We’ll use NumPy for basic work and SymPy for exact RREF (since NumPy doesn’t have it built-in).</p>
</section>
<section id="step-by-step-code-walkthrough-22">
<h4 data-anchor-id="step-by-step-code-walkthrough-22">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Row-Echelon Form (REF)</li>
</ol>
<ul>
<li>All nonzero rows are above any zero rows.</li>
<li>Each leading entry (pivot) is to the right of the pivot in the row above.</li>
<li>Pivots are usually scaled to 1, but not strictly required.</li>
</ul>
<p>Example system:</p>
<p><span>\[
\begin{cases}  
x + 2y + z = 7 \\  
2x + 4y + z = 12 \\  
3x + 6y + 2z = 17  
\end{cases}
\]</span></p>
<div id="c783cd66" data-execution_count="134">
<div id="cb226"><pre><code><span id="cb226-1">A <span>=</span> np.array([</span>
<span id="cb226-2">    [<span>1</span>, <span>2</span>, <span>1</span>, <span>7</span>],</span>
<span id="cb226-3">    [<span>2</span>, <span>4</span>, <span>1</span>, <span>12</span>],</span>
<span id="cb226-4">    [<span>3</span>, <span>6</span>, <span>2</span>, <span>17</span>]</span>
<span id="cb226-5">], dtype<span>=</span><span>float</span>)</span>
<span id="cb226-6"></span>
<span id="cb226-7"><span>print</span>(<span>"Augmented matrix:</span><span>\n</span><span>"</span>, A)</span></code></pre></div>
<div>
<pre><code>Augmented matrix:
 [[ 1.  2.  1.  7.]
 [ 2.  4.  1. 12.]
 [ 3.  6.  2. 17.]]</code></pre>
</div>
</div>
<p>Perform elimination manually:</p>
<div id="554a715c" data-execution_count="135">
<div id="cb228"><pre><code><span id="cb228-1"><span># eliminate first column entries below pivot</span></span>
<span id="cb228-2">A[<span>1</span>] <span>=</span> A[<span>1</span>] <span>-</span> <span>2</span><span>*</span>A[<span>0</span>]</span>
<span id="cb228-3">A[<span>2</span>] <span>=</span> A[<span>2</span>] <span>-</span> <span>3</span><span>*</span>A[<span>0</span>]</span>
<span id="cb228-4"><span>print</span>(<span>"After eliminating first column:</span><span>\n</span><span>"</span>, A)</span></code></pre></div>
<div>
<pre><code>After eliminating first column:
 [[ 1.  2.  1.  7.]
 [ 0.  0. -1. -2.]
 [ 0.  0. -1. -4.]]</code></pre>
</div>
</div>
<p>Now the pivots move diagonally across the matrix - this is row-echelon form.</p>
<ol start="2" type="1">
<li>Reduced Row-Echelon Form (RREF) In RREF, we go further:</li>
</ol>
<ul>
<li>Every pivot = 1.</li>
<li>Every pivot is the only nonzero in its column.</li>
</ul>
<p>Instead of coding manually, we’ll let SymPy handle it:</p>
<div id="0fe6c71b" data-execution_count="136">
<div id="cb230"><pre><code><span id="cb230-1">M <span>=</span> Matrix([</span>
<span id="cb230-2">    [<span>1</span>, <span>2</span>, <span>1</span>, <span>7</span>],</span>
<span id="cb230-3">    [<span>2</span>, <span>4</span>, <span>1</span>, <span>12</span>],</span>
<span id="cb230-4">    [<span>3</span>, <span>6</span>, <span>2</span>, <span>17</span>]</span>
<span id="cb230-5">])</span>
<span id="cb230-6"></span>
<span id="cb230-7">M_rref <span>=</span> M.rref()</span>
<span id="cb230-8"><span>print</span>(<span>"RREF form:</span><span>\n</span><span>"</span>, M_rref[<span>0</span>])</span></code></pre></div>
<div>
<pre><code>RREF form:
 Matrix([[1, 2, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])</code></pre>
</div>
</div>
<p>SymPy shows the final canonical form.</p>
<ol start="3" type="1">
<li>Reading solutions from RREF</li>
</ol>
<p>If the RREF looks like:</p>
<p><span>\[
\begin{bmatrix}  
1 &amp; 0 &amp; a &amp; b \\  
0 &amp; 1 &amp; c &amp; d \\  
0 &amp; 0 &amp; 0 &amp; 0  
\end{bmatrix}
\]</span></p>
<p>It means:</p>
<ul>
<li>The first two variables are leading (pivots).</li>
<li>The third variable is free.</li>
<li>Solutions can be written in terms of the free variable.</li>
</ul>
<ol start="4" type="1">
<li>A quick example with free variables</li>
</ol>
<p>System:</p>
<p><span>\[
x + y + z = 3 \\  
2x + y - z = 0  
\]</span></p>
<div id="3a701896" data-execution_count="137">
<div id="cb232"><pre><code><span id="cb232-1">M2 <span>=</span> Matrix([</span>
<span id="cb232-2">    [<span>1</span>,<span>1</span>,<span>1</span>,<span>3</span>],</span>
<span id="cb232-3">    [<span>2</span>,<span>1</span>,<span>-</span><span>1</span>,<span>0</span>]</span>
<span id="cb232-4">])</span>
<span id="cb232-5"></span>
<span id="cb232-6">M2_rref <span>=</span> M2.rref()</span>
<span id="cb232-7"><span>print</span>(<span>"RREF form:</span><span>\n</span><span>"</span>, M2_rref[<span>0</span>])</span></code></pre></div>
<div>
<pre><code>RREF form:
 Matrix([[1, 0, -2, -3], [0, 1, 3, 6]])</code></pre>
</div>
</div>
<p>Here, one column will not have a pivot → that variable is free.</p>
</section>
<section id="try-it-yourself-21">
<h4 data-anchor-id="try-it-yourself-21">Try It Yourself</h4>
<ol type="1">
<li><p>Take the system:</p>
<p><span>\[
2x + 3y = 6, \quad 4x + 6y = 12
\]</span></p>
<p>Write the augmented matrix and compute its RREF. What does it tell you about solutions?</p></li>
<li><p>Create a random 3×4 matrix in NumPy. Use SymPy’s <code>Matrix.rref()</code> to compute its reduced form. Identify the pivot columns.</p></li>
<li><p>For the system:</p>
<p><span>\[
x + 2y + 3z = 4, \quad 2x + 4y + 6z = 8
\]</span></p>
<p>Check if the equations are independent or multiples of each other by looking at the RREF.</p></li>
</ol>
</section>
<section id="the-takeaway-5">
<h4 data-anchor-id="the-takeaway-5">The Takeaway</h4>
<ul>
<li>REF organizes equations into a staircase shape.</li>
<li>RREF goes further, making each pivot the only nonzero in its column.</li>
<li>These canonical forms make it easy to identify pivot variables, free variables, and the solution set structure.</li>
</ul>
</section>
</section>
<section id="pivots-free-variables-and-leading-ones-reading-solutions">
<h3 data-anchor-id="pivots-free-variables-and-leading-ones-reading-solutions">24. Pivots, Free Variables, and Leading Ones (Reading Solutions)</h3>
<p>Once a matrix is in row-echelon or reduced row-echelon form, the solutions to the system become visible. The key is identifying pivots, leading ones, and free variables.</p>
<section id="set-up-your-lab-23">
<h4 data-anchor-id="set-up-your-lab-23">Set Up Your Lab</h4>
<div id="891e0aed" data-execution_count="138"><pre><code><span id="cb234-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb234-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-23">
<h4 data-anchor-id="step-by-step-code-walkthrough-23">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>What are pivots?</li>
</ol>
<ul>
<li>A pivot is the first nonzero entry in a row (after elimination).</li>
<li>In RREF, pivots are scaled to <code>1</code> and are called leading ones.</li>
<li>Pivot columns correspond to basic variables.</li>
</ul>
<ol start="2" type="1">
<li>Example system</li>
</ol>
<p><span>\[
\begin{cases}  
x + y + z = 6 \\  
2x + 3y + z = 10  
\end{cases}
\]</span></p>
<div id="59c90403" data-execution_count="139">
<div id="cb235"><pre><code><span id="cb235-1">M <span>=</span> Matrix([</span>
<span id="cb235-2">    [<span>1</span>,<span>1</span>,<span>1</span>,<span>6</span>],</span>
<span id="cb235-3">    [<span>2</span>,<span>3</span>,<span>1</span>,<span>10</span>]</span>
<span id="cb235-4">])</span>
<span id="cb235-5"></span>
<span id="cb235-6">M_rref <span>=</span> M.rref()</span>
<span id="cb235-7"><span>print</span>(<span>"RREF form:</span><span>\n</span><span>"</span>, M_rref[<span>0</span>])</span></code></pre></div>
<div>
<pre><code>RREF form:
 Matrix([[1, 0, 2, 8], [0, 1, -1, -2]])</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Interpreting the RREF</li>
</ol>
<p>Suppose the RREF comes out as:</p>
<p><span>\[
\begin{bmatrix}  
1 &amp; 0 &amp; -2 &amp; 4 \\  
0 &amp; 1 &amp; 1 &amp; 2  
\end{bmatrix}
\]</span></p>
<p>This means:</p>
<ul>
<li><p>Pivot columns: 1 and 2 → variables <span>\(x\)</span> and <span>\(y\)</span> are basic.</p></li>
<li><p>Free variable: <span>\(z\)</span>.</p></li>
<li><p>Equations:</p>
<p><span>\[
x - 2z = 4, \quad y + z = 2
\]</span></p></li>
<li><p>Solution in terms of <span>\(z\)</span>:</p>
<p><span>\[
x = 4 + 2z, \quad y = 2 - z, \quad z = z
\]</span></p></li>
</ul>
<ol start="4" type="1">
<li>Coding the solution extraction</li>
</ol>
<div id="f357876e" data-execution_count="140">
<div id="cb237"><pre><code><span id="cb237-1">rref_matrix, pivots <span>=</span> M_rref</span>
<span id="cb237-2"><span>print</span>(<span>"Pivot columns:"</span>, pivots)</span>
<span id="cb237-3"></span>
<span id="cb237-4"><span># free variables are the columns not in pivots</span></span>
<span id="cb237-5">all_vars <span>=</span> <span>set</span>(<span>range</span>(rref_matrix.shape[<span>1</span>]<span>-</span><span>1</span>))  <span># exclude last column (constants)</span></span>
<span id="cb237-6">free_vars <span>=</span> all_vars <span>-</span> <span>set</span>(pivots)</span>
<span id="cb237-7"><span>print</span>(<span>"Free variable indices:"</span>, free_vars)</span></code></pre></div>
<div>
<pre><code>Pivot columns: (0, 1)
Free variable indices: {2}</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Another example with infinitely many solutions</li>
</ol>
<p><span>\[
x + 2y + 3z = 4, \quad 2x + 4y + 6z = 8
\]</span></p>
<div id="06e82a94" data-execution_count="141">
<div id="cb239"><pre><code><span id="cb239-1">M2 <span>=</span> Matrix([</span>
<span id="cb239-2">    [<span>1</span>,<span>2</span>,<span>3</span>,<span>4</span>],</span>
<span id="cb239-3">    [<span>2</span>,<span>4</span>,<span>6</span>,<span>8</span>]</span>
<span id="cb239-4">])</span>
<span id="cb239-5"></span>
<span id="cb239-6">M2_rref <span>=</span> M2.rref()</span>
<span id="cb239-7"><span>print</span>(<span>"RREF form:</span><span>\n</span><span>"</span>, M2_rref[<span>0</span>])</span></code></pre></div>
<div>
<pre><code>RREF form:
 Matrix([[1, 2, 3, 4], [0, 0, 0, 0]])</code></pre>
</div>
</div>
<p>The second row becomes all zeros, showing redundancy. Pivot in column 1, free variables in columns 2 and 3.</p>
<ol start="6" type="1">
<li>Solving underdetermined systems</li>
</ol>
<p>If you have more variables than equations, expect free variables. Example:</p>
<p><span>\[
x + y = 3
\]</span></p>
<div id="57b89a4a" data-execution_count="142">
<div id="cb241"><pre><code><span id="cb241-1">M3 <span>=</span> Matrix([[<span>1</span>,<span>1</span>,<span>3</span>]])</span>
<span id="cb241-2"><span>print</span>(<span>"RREF form:</span><span>\n</span><span>"</span>, M3.rref()[<span>0</span>])</span></code></pre></div>
<div>
<pre><code>RREF form:
 Matrix([[1, 1, 3]])</code></pre>
</div>
</div>
<p>Here, <span>\(x = 3 - y\)</span>. Variable <span>\(y\)</span> is free.</p>
</section>
<section id="try-it-yourself-22">
<h4 data-anchor-id="try-it-yourself-22">Try It Yourself</h4>
<ol type="1">
<li><p>Take the system:</p>
<p><span>\[
x + y + z = 2, \quad y + z = 1
\]</span></p>
<p>Compute its RREF and identify pivot and free variables.</p></li>
<li><p>Create a random 3×4 system and compute its pivots. How many free variables do you get?</p></li>
<li><p>For the system:</p>
<p><span>\[
x - y = 0, \quad 2x - 2y = 0
\]</span></p>
<p>Verify that the system has infinitely many solutions and describe them in terms of a free variable.</p></li>
</ol>
</section>
<section id="the-takeaway-6">
<h4 data-anchor-id="the-takeaway-6">The Takeaway</h4>
<ul>
<li>Pivots / leading ones mark the basic variables.</li>
<li>Free variables correspond to non-pivot columns.</li>
<li>Solutions are written in terms of free variables, showing whether the system has a unique, infinite, or no solution.</li>
</ul>
</section>
</section>
<section id="solving-consistent-systems-unique-vs.-infinite-solutions">
<h3 data-anchor-id="solving-consistent-systems-unique-vs.-infinite-solutions">25. Solving Consistent Systems (Unique vs.&nbsp;Infinite Solutions)</h3>
<p>Now that we can spot pivots and free variables, we can classify systems of equations as having a unique solution or infinitely many solutions (assuming they’re consistent). In this lab, we’ll practice solving both types.</p>
<section id="set-up-your-lab-24">
<h4 data-anchor-id="set-up-your-lab-24">Set Up Your Lab</h4>
<div id="5ba48c72" data-execution_count="143"><pre><code><span id="cb243-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb243-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-24">
<h4 data-anchor-id="step-by-step-code-walkthrough-24">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Unique solution example</li>
</ol>
<p>System:</p>
<p><span>\[
x + y = 3, \quad 2x - y = 0
\]</span></p>
<div id="7f049f3f" data-execution_count="144">
<div id="cb244"><pre><code><span id="cb244-1"><span>from</span> sympy <span>import</span> Matrix</span>
<span id="cb244-2"></span>
<span id="cb244-3">M <span>=</span> Matrix([</span>
<span id="cb244-4">    [<span>1</span>, <span>1</span>, <span>3</span>],</span>
<span id="cb244-5">    [<span>2</span>, <span>-</span><span>1</span>, <span>0</span>]</span>
<span id="cb244-6">])</span>
<span id="cb244-7"></span>
<span id="cb244-8">M_rref <span>=</span> M.rref()</span>
<span id="cb244-9"><span>print</span>(<span>"RREF form:</span><span>\n</span><span>"</span>, M_rref[<span>0</span>])</span>
<span id="cb244-10"></span>
<span id="cb244-11"><span># Split into coefficient matrix A and right-hand side b</span></span>
<span id="cb244-12">A <span>=</span> M[:, :<span>2</span>]</span>
<span id="cb244-13">b <span>=</span> M[:, <span>2</span>]</span>
<span id="cb244-14"></span>
<span id="cb244-15">solution <span>=</span> A.solve_least_squares(b)</span>
<span id="cb244-16"><span>print</span>(<span>"Solution:"</span>, solution)</span></code></pre></div>
<div>
<pre><code>RREF form:
 Matrix([[1, 0, 1], [0, 1, 2]])
Solution: Matrix([[1], [2]])</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Infinite solution example</li>
</ol>
<p>System:</p>
<p><span>\[
x + y + z = 2, \quad 2x + 2y + 2z = 4
\]</span></p>
<div id="5eeb9587" data-execution_count="145">
<div id="cb246"><pre><code><span id="cb246-1">M2 <span>=</span> Matrix([</span>
<span id="cb246-2">    [<span>1</span>, <span>1</span>, <span>1</span>, <span>2</span>],</span>
<span id="cb246-3">    [<span>2</span>, <span>2</span>, <span>2</span>, <span>4</span>]</span>
<span id="cb246-4">])</span>
<span id="cb246-5"></span>
<span id="cb246-6">M2_rref <span>=</span> M2.rref()</span>
<span id="cb246-7"><span>print</span>(<span>"RREF form:</span><span>\n</span><span>"</span>, M2_rref[<span>0</span>])</span></code></pre></div>
<div>
<pre><code>RREF form:
 Matrix([[1, 1, 1, 2], [0, 0, 0, 0]])</code></pre>
</div>
</div>
<p>Only one pivot → two free variables.</p>
<p>Interpretation:</p>
<ul>
<li><span>\(x = 2 - y - z\)</span></li>
<li><span>\(y, z\)</span> are free</li>
<li>Infinite solutions described by parameters.</li>
</ul>
<ol start="3" type="1">
<li>Classifying consistency</li>
</ol>
<p>A system is consistent if the RREF does <em>not</em> have a row like:</p>
<p><span>\[
[0, 0, 0, c] \quad (c \neq 0)
\]</span></p>
<p>Example consistent system:</p>
<div id="955af846" data-execution_count="146">
<div id="cb248"><pre><code><span id="cb248-1">M3 <span>=</span> Matrix([</span>
<span id="cb248-2">    [<span>1</span>, <span>2</span>, <span>3</span>],</span>
<span id="cb248-3">    [<span>0</span>, <span>1</span>, <span>4</span>]</span>
<span id="cb248-4">])</span>
<span id="cb248-5"><span>print</span>(<span>"RREF:</span><span>\n</span><span>"</span>, M3.rref()[<span>0</span>])</span></code></pre></div>
<div>
<pre><code>RREF:
 Matrix([[1, 0, -5], [0, 1, 4]])</code></pre>
</div>
</div>
<p>Example inconsistent system (no solution):</p>
<div id="7bbeaf13" data-execution_count="147">
<div id="cb250"><pre><code><span id="cb250-1">M4 <span>=</span> Matrix([</span>
<span id="cb250-2">    [<span>1</span>, <span>1</span>, <span>2</span>],</span>
<span id="cb250-3">    [<span>2</span>, <span>2</span>, <span>5</span>]</span>
<span id="cb250-4">])</span>
<span id="cb250-5"><span>print</span>(<span>"RREF:</span><span>\n</span><span>"</span>, M4.rref()[<span>0</span>])</span></code></pre></div>
<div>
<pre><code>RREF:
 Matrix([[1, 1, 0], [0, 0, 1]])</code></pre>
</div>
</div>
<p>The second one ends with <code>[0,0,1]</code>, meaning contradiction (0 = 1).</p>
<ol start="4" type="1">
<li>Quick NumPy comparison</li>
</ol>
<p>For systems with unique solutions:</p>
<div id="81f5beeb" data-execution_count="148">
<div id="cb252"><pre><code><span id="cb252-1">A <span>=</span> np.array([[<span>1</span>,<span>1</span>],[<span>2</span>,<span>-</span><span>1</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb252-2">b <span>=</span> np.array([<span>3</span>,<span>0</span>], dtype<span>=</span><span>float</span>)</span>
<span id="cb252-3"><span>print</span>(<span>"Unique solution with np.linalg.solve:"</span>, np.linalg.solve(A,b))</span></code></pre></div>
<div>
<pre><code>Unique solution with np.linalg.solve: [1. 2.]</code></pre>
</div>
</div>
<p>For systems with infinite solutions, <code>np.linalg.solve</code> will fail, but SymPy handles parametric solutions.</p>
</section>
<section id="try-it-yourself-23">
<h4 data-anchor-id="try-it-yourself-23">Try It Yourself</h4>
<ol type="1">
<li><p>Solve:</p>
<p><span>\[
x + y + z = 1, \quad 2x + 3y + z = 2
\]</span></p>
<p>Is the solution unique or infinite?</p></li>
<li><p>Check consistency of:</p>
<p><span>\[
x + 2y = 3, \quad 2x + 4y = 8
\]</span></p></li>
<li><p>Build a random 3×4 augmented matrix and compute its RREF. Identify:</p>
<ul>
<li>Does it have a unique solution, infinitely many, or none?</li>
</ul></li>
</ol>
</section>
<section id="the-takeaway-7">
<h4 data-anchor-id="the-takeaway-7">The Takeaway</h4>
<ul>
<li>Unique solution: pivot in every variable column.</li>
<li>Infinite solutions: free variables remain, system is still consistent.</li>
<li>No solution: an inconsistent row appears.</li>
</ul>
<p>Understanding pivots and free variables gives a complete picture of the solution set.</p>
</section>
</section>
<section id="detecting-inconsistency-when-no-solution-exists">
<h3 data-anchor-id="detecting-inconsistency-when-no-solution-exists">26. Detecting Inconsistency (When No Solution Exists)</h3>
<p>Not all systems of linear equations can be solved. Some are inconsistent, meaning the equations contradict each other. In this lab, we’ll learn how to recognize inconsistency using augmented matrices and RREF.</p>
<section id="set-up-your-lab-25">
<h4 data-anchor-id="set-up-your-lab-25">Set Up Your Lab</h4>
<div id="e0f4b5ae" data-execution_count="149"><pre><code><span id="cb254-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb254-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-25">
<h4 data-anchor-id="step-by-step-code-walkthrough-25">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>An inconsistent system</li>
</ol>
<p><span>\[
x + y = 2, \quad 2x + 2y = 5
\]</span></p>
<p>Notice the second equation looks like a multiple of the first, but the constant doesn’t match - contradiction.</p>
<div id="d872ab30" data-execution_count="150">
<div id="cb255"><pre><code><span id="cb255-1">M <span>=</span> Matrix([</span>
<span id="cb255-2">    [<span>1</span>, <span>1</span>, <span>2</span>],</span>
<span id="cb255-3">    [<span>2</span>, <span>2</span>, <span>5</span>]</span>
<span id="cb255-4">])</span>
<span id="cb255-5"></span>
<span id="cb255-6">M_rref <span>=</span> M.rref()</span>
<span id="cb255-7"><span>print</span>(<span>"RREF:</span><span>\n</span><span>"</span>, M_rref[<span>0</span>])</span></code></pre></div>
<div>
<pre><code>RREF:
 Matrix([[1, 1, 0], [0, 0, 1]])</code></pre>
</div>
</div>
<p>RREF gives:</p>
<p><span>\[
\begin{bmatrix}  
1 &amp; 1 &amp; 2 \\  
0 &amp; 0 &amp; 1  
\end{bmatrix}
\]</span></p>
<p>The last row means <span>\(0 = 1\)</span>, so no solution exists.</p>
<ol start="2" type="1">
<li>A consistent system (for contrast)</li>
</ol>
<p><span>\[
x + y = 2, \quad 2x + 2y = 4
\]</span></p>
<div id="83b2f8c5" data-execution_count="151">
<div id="cb257"><pre><code><span id="cb257-1">M2 <span>=</span> Matrix([</span>
<span id="cb257-2">    [<span>1</span>, <span>1</span>, <span>2</span>],</span>
<span id="cb257-3">    [<span>2</span>, <span>2</span>, <span>4</span>]</span>
<span id="cb257-4">])</span>
<span id="cb257-5"></span>
<span id="cb257-6"><span>print</span>(<span>"RREF:</span><span>\n</span><span>"</span>, M2.rref()[<span>0</span>])</span></code></pre></div>
<div>
<pre><code>RREF:
 Matrix([[1, 1, 2], [0, 0, 0]])</code></pre>
</div>
</div>
<p>This reduces to one equation and a redundant row of zeros → infinitely many solutions.</p>
<ol start="3" type="1">
<li>Visualizing inconsistency (2D case)</li>
</ol>
<p>System:</p>
<p><span>\[
x + y = 2 \quad \text{and} \quad x + y = 3
\]</span></p>
<p>These are parallel lines that never meet.</p>
<div id="1596c237" data-execution_count="152">
<div id="cb259"><pre><code><span id="cb259-1"><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb259-2"></span>
<span id="cb259-3">x_vals <span>=</span> np.linspace(<span>-</span><span>1</span>, <span>3</span>, <span>100</span>)</span>
<span id="cb259-4">y1 <span>=</span> <span>2</span> <span>-</span> x_vals</span>
<span id="cb259-5">y2 <span>=</span> <span>3</span> <span>-</span> x_vals</span>
<span id="cb259-6"></span>
<span id="cb259-7">plt.plot(x_vals, y1, label<span>=</span><span>"x+y=2"</span>)</span>
<span id="cb259-8">plt.plot(x_vals, y2, label<span>=</span><span>"x+y=3"</span>)</span>
<span id="cb259-9"></span>
<span id="cb259-10">plt.legend()</span>
<span id="cb259-11">plt.axhline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb259-12">plt.axvline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb259-13">plt.grid()</span>
<span id="cb259-14">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-153-output-1.png" width="569" height="411"></p>
</figure>
</div>
</div>
<p>The two lines are parallel → no solution.</p>
<ol start="4" type="1">
<li>Detecting inconsistency automatically</li>
</ol>
<p>We can scan the RREF for a row of the form <span>\([0, 0, …, c]\)</span> with <span>\(c \neq 0\)</span>.</p>
<div id="55c10e5c" data-execution_count="153">
<div id="cb260"><pre><code><span id="cb260-1"><span>def</span> is_inconsistent(M):</span>
<span id="cb260-2">    rref_matrix, _ <span>=</span> M.rref()</span>
<span id="cb260-3">    <span>for</span> row <span>in</span> rref_matrix.tolist():</span>
<span id="cb260-4">        <span>if</span> <span>all</span>(v <span>==</span> <span>0</span> <span>for</span> v <span>in</span> row[:<span>-</span><span>1</span>]) <span>and</span> row[<span>-</span><span>1</span>] <span>!=</span> <span>0</span>:</span>
<span id="cb260-5">            <span>return</span> <span>True</span></span>
<span id="cb260-6">    <span>return</span> <span>False</span></span>
<span id="cb260-7"></span>
<span id="cb260-8"><span>print</span>(<span>"System 1 inconsistent?"</span>, is_inconsistent(M))</span>
<span id="cb260-9"><span>print</span>(<span>"System 2 inconsistent?"</span>, is_inconsistent(M2))</span></code></pre></div>
<div>
<pre><code>System 1 inconsistent? True
System 2 inconsistent? False</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-24">
<h4 data-anchor-id="try-it-yourself-24">Try It Yourself</h4>
<ol type="1">
<li><p>Test the system:</p>
<p><span>\[
x + 2y = 4, \quad 2x + 4y = 10
\]</span></p>
<p>Write the augmented matrix and check if it’s inconsistent.</p></li>
<li><p>Build a random 2×3 augmented matrix with integer entries. Use <code>is_inconsistent</code> to check.</p></li>
<li><p>Plot two linear equations in 2D. Adjust constants to see when they intersect (consistent) vs when they are parallel (inconsistent).</p></li>
</ol>
</section>
<section id="the-takeaway-8">
<h4 data-anchor-id="the-takeaway-8">The Takeaway</h4>
<ul>
<li><p>A system is inconsistent if RREF contains a row like <span>\([0,0,…,c]\)</span> with <span>\(c \neq 0\)</span>.</p></li>
<li><p>Geometrically, this means the equations describe parallel lines (2D), parallel planes (3D), or higher-dimensional contradictions.</p></li>
<li><p>Recognizing inconsistency quickly saves time and avoids chasing impossible solutions.</p></li>
</ul>
</section>
</section>
<section id="gaussian-elimination-by-hand-a-disciplined-procedure">
<h3 data-anchor-id="gaussian-elimination-by-hand-a-disciplined-procedure">27. Gaussian Elimination by Hand (A Disciplined Procedure)</h3>
<p>Gaussian elimination is the systematic way to solve linear systems using row operations. It transforms the augmented matrix into row-echelon form (REF) and then uses back substitution to find solutions. In this lab, we’ll walk step by step through the process.</p>
<section id="set-up-your-lab-26">
<h4 data-anchor-id="set-up-your-lab-26">Set Up Your Lab</h4>
<div id="2a6906e3" data-execution_count="154"><pre><code><span id="cb262-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb262-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-26">
<h4 data-anchor-id="step-by-step-code-walkthrough-26">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Example system</li>
</ol>
<p><span>\[
\begin{cases}  
x + y + z = 6 \\  
2x + 3y + z = 14 \\  
x + 2y + 3z = 14  
\end{cases}
\]</span></p>
<div id="6ec8fdf5" data-execution_count="155">
<div id="cb263"><pre><code><span id="cb263-1">A <span>=</span> np.array([</span>
<span id="cb263-2">    [<span>1</span>, <span>1</span>, <span>1</span>, <span>6</span>],</span>
<span id="cb263-3">    [<span>2</span>, <span>3</span>, <span>1</span>, <span>14</span>],</span>
<span id="cb263-4">    [<span>1</span>, <span>2</span>, <span>3</span>, <span>14</span>]</span>
<span id="cb263-5">], dtype<span>=</span><span>float</span>)</span>
<span id="cb263-6"></span>
<span id="cb263-7"><span>print</span>(<span>"Initial augmented matrix:</span><span>\n</span><span>"</span>, A)</span></code></pre></div>
<div>
<pre><code>Initial augmented matrix:
 [[ 1.  1.  1.  6.]
 [ 2.  3.  1. 14.]
 [ 1.  2.  3. 14.]]</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Step 1: Get a pivot in the first column</li>
</ol>
<p>Make the pivot at (0,0) into 1 (it already is). Now eliminate below it.</p>
<div id="8fb97f24" data-execution_count="156">
<div id="cb265"><pre><code><span id="cb265-1">A[<span>1</span>] <span>=</span> A[<span>1</span>] <span>-</span> <span>2</span><span>*</span>A[<span>0</span>]   <span># Row2 → Row2 - 2*Row1</span></span>
<span id="cb265-2">A[<span>2</span>] <span>=</span> A[<span>2</span>] <span>-</span> A[<span>0</span>]     <span># Row3 → Row3 - Row1</span></span>
<span id="cb265-3"><span>print</span>(<span>"After eliminating first column:</span><span>\n</span><span>"</span>, A)</span></code></pre></div>
<div>
<pre><code>After eliminating first column:
 [[ 1.  1.  1.  6.]
 [ 0.  1. -1.  2.]
 [ 0.  1.  2.  8.]]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Step 2: Pivot in the second column</li>
</ol>
<p>Make the pivot in row 1, col 1 equal to 1.</p>
<div id="099a1149" data-execution_count="157">
<div id="cb267"><pre><code><span id="cb267-1">A[<span>1</span>] <span>=</span> A[<span>1</span>] <span>/</span> A[<span>1</span>,<span>1</span>]</span>
<span id="cb267-2"><span>print</span>(<span>"After scaling second row:</span><span>\n</span><span>"</span>, A)</span></code></pre></div>
<div>
<pre><code>After scaling second row:
 [[ 1.  1.  1.  6.]
 [ 0.  1. -1.  2.]
 [ 0.  1.  2.  8.]]</code></pre>
</div>
</div>
<p>Now eliminate below:</p>
<div id="c069ba01" data-execution_count="158">
<div id="cb269"><pre><code><span id="cb269-1">A[<span>2</span>] <span>=</span> A[<span>2</span>] <span>-</span> A[<span>2</span>,<span>1</span>]<span>*</span>A[<span>1</span>]</span>
<span id="cb269-2"><span>print</span>(<span>"After eliminating second column:</span><span>\n</span><span>"</span>, A)</span></code></pre></div>
<div>
<pre><code>After eliminating second column:
 [[ 1.  1.  1.  6.]
 [ 0.  1. -1.  2.]
 [ 0.  0.  3.  6.]]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Step 3: Pivot in the third column</li>
</ol>
<p>Make the bottom-right entry into 1.</p>
<div id="9a63fca4" data-execution_count="159">
<div id="cb271"><pre><code><span id="cb271-1">A[<span>2</span>] <span>=</span> A[<span>2</span>] <span>/</span> A[<span>2</span>,<span>2</span>]</span>
<span id="cb271-2"><span>print</span>(<span>"After scaling third row:</span><span>\n</span><span>"</span>, A)</span></code></pre></div>
<div>
<pre><code>After scaling third row:
 [[ 1.  1.  1.  6.]
 [ 0.  1. -1.  2.]
 [ 0.  0.  1.  2.]]</code></pre>
</div>
</div>
<p>At this point, the matrix is in row-echelon form (REF).</p>
<ol start="5" type="1">
<li>Back substitution</li>
</ol>
<p>Now solve from the bottom up:</p>
<div id="f6622884" data-execution_count="160">
<div id="cb273"><pre><code><span id="cb273-1">z <span>=</span> A[<span>2</span>,<span>3</span>]</span>
<span id="cb273-2">y <span>=</span> A[<span>1</span>,<span>3</span>] <span>-</span> A[<span>1</span>,<span>2</span>]<span>*</span>z</span>
<span id="cb273-3">x <span>=</span> A[<span>0</span>,<span>3</span>] <span>-</span> A[<span>0</span>,<span>1</span>]<span>*</span>y <span>-</span> A[<span>0</span>,<span>2</span>]<span>*</span>z</span>
<span id="cb273-4"></span>
<span id="cb273-5"><span>print</span>(<span>f"Solution: x=</span><span>{</span>x<span>}</span><span>, y=</span><span>{</span>y<span>}</span><span>, z=</span><span>{</span>z<span>}</span><span>"</span>)</span></code></pre></div>
<div>
<pre><code>Solution: x=0.0, y=4.0, z=2.0</code></pre>
</div>
</div>
<ol start="6" type="1">
<li>Verification</li>
</ol>
<div id="9d0f23f5" data-execution_count="161">
<div id="cb275"><pre><code><span id="cb275-1">coeff <span>=</span> np.array([</span>
<span id="cb275-2">    [<span>1</span>,<span>1</span>,<span>1</span>],</span>
<span id="cb275-3">    [<span>2</span>,<span>3</span>,<span>1</span>],</span>
<span id="cb275-4">    [<span>1</span>,<span>2</span>,<span>3</span>]</span>
<span id="cb275-5">], dtype<span>=</span><span>float</span>)</span>
<span id="cb275-6">const <span>=</span> np.array([<span>6</span>,<span>14</span>,<span>14</span>], dtype<span>=</span><span>float</span>)</span>
<span id="cb275-7"></span>
<span id="cb275-8"><span>print</span>(<span>"Check with np.linalg.solve:"</span>, np.linalg.solve(coeff,const))</span></code></pre></div>
<div>
<pre><code>Check with np.linalg.solve: [0. 4. 2.]</code></pre>
</div>
</div>
<p>The results match.</p>
</section>
<section id="try-it-yourself-25">
<h4 data-anchor-id="try-it-yourself-25">Try It Yourself</h4>
<ol type="1">
<li><p>Solve:</p>
<p><span>\[
2x + y = 5, \quad 4x - 6y = -2
\]</span></p>
<p>using Gaussian elimination manually in code.</p></li>
<li><p>Create a random 3×4 augmented matrix and reduce it step by step, printing after each row operation.</p></li>
<li><p>Compare your manual elimination to SymPy’s RREF with <code>Matrix.rref()</code>.</p></li>
</ol>
</section>
<section id="the-takeaway-9">
<h4 data-anchor-id="the-takeaway-9">The Takeaway</h4>
<ul>
<li>Gaussian elimination is a disciplined sequence of row operations.</li>
<li>It reduces the matrix to row-echelon form, from which back substitution is straightforward.</li>
<li>This method is the backbone of solving systems by hand and underlies many numerical algorithms.</li>
</ul>
</section>
</section>
<section id="back-substitution-and-solution-sets-finishing-cleanly">
<h3 data-anchor-id="back-substitution-and-solution-sets-finishing-cleanly">28. Back Substitution and Solution Sets (Finishing Cleanly)</h3>
<p>Once Gaussian elimination reduces a system to row-echelon form (REF), the final step is back substitution. This means solving variables starting from the last equation and working upward. In this lab, we’ll practice both unique and infinite solution cases.</p>
<section id="set-up-your-lab-27">
<h4 data-anchor-id="set-up-your-lab-27">Set Up Your Lab</h4>
<div id="392ff2cb" data-execution_count="162"><pre><code><span id="cb277-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb277-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-27">
<h4 data-anchor-id="step-by-step-code-walkthrough-27">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Unique solution example</li>
</ol>
<p>System:</p>
<p><span>\[
x + y + z = 6, \quad 2y + 5z = -4, \quad z = 3
\]</span></p>
<p>Row-echelon form looks like:</p>
<p><span>\[
\begin{bmatrix}  
1 &amp; 1 &amp; 1 &amp; 6 \\  
0 &amp; 2 &amp; 5 &amp; -4 \\  
0 &amp; 0 &amp; 1 &amp; 3  
\end{bmatrix}
\]</span></p>
<p>Solve bottom-up:</p>
<div id="07d3e20c" data-execution_count="163">
<div id="cb278"><pre><code><span id="cb278-1">z <span>=</span> <span>3</span></span>
<span id="cb278-2">y <span>=</span> (<span>-</span><span>4</span> <span>-</span> <span>5</span><span>*</span>z)<span>/</span><span>2</span></span>
<span id="cb278-3">x <span>=</span> <span>6</span> <span>-</span> y <span>-</span> z</span>
<span id="cb278-4"><span>print</span>(<span>f"Solution: x=</span><span>{</span>x<span>}</span><span>, y=</span><span>{</span>y<span>}</span><span>, z=</span><span>{</span>z<span>}</span><span>"</span>)</span></code></pre></div>
<div>
<pre><code>Solution: x=12.5, y=-9.5, z=3</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Infinite solution example</li>
</ol>
<p>System:</p>
<p><span>\[
x + y + z = 2, \quad 2x + 2y + 2z = 4
\]</span></p>
<p>After elimination:</p>
<p><span>\[
\begin{bmatrix}  
1 &amp; 1 &amp; 1 &amp; 2 \\  
0 &amp; 0 &amp; 0 &amp; 0  
\end{bmatrix}
\]</span></p>
<p>This means:</p>
<ul>
<li>Equation: <span>\(x + y + z = 2\)</span>.</li>
<li>Free variables: choose <span>\(y\)</span> and <span>\(z\)</span>.</li>
</ul>
<p>Let <span>\(y = s, z = t\)</span>. Then:</p>
<p><span>\[
x = 2 - s - t
\]</span></p>
<p>So the solution set is:</p>
<div id="f9d18c24" data-execution_count="164">
<div id="cb280"><pre><code><span id="cb280-1"><span>from</span> sympy <span>import</span> symbols</span>
<span id="cb280-2">s, t <span>=</span> symbols(<span>'s t'</span>)</span>
<span id="cb280-3">x <span>=</span> <span>2</span> <span>-</span> s <span>-</span> t</span>
<span id="cb280-4">y <span>=</span> s</span>
<span id="cb280-5">z <span>=</span> t</span>
<span id="cb280-6"><span>print</span>(<span>"General solution:"</span>)</span>
<span id="cb280-7"><span>print</span>(<span>"x ="</span>, x, <span>", y ="</span>, y, <span>", z ="</span>, z)</span></code></pre></div>
<div>
<pre><code>General solution:
x = -s - t + 2 , y = s , z = t</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Consistency check with RREF</li>
</ol>
<p>We can use SymPy to confirm solution sets:</p>
<div id="88ac642d" data-execution_count="165">
<div id="cb282"><pre><code><span id="cb282-1">M <span>=</span> Matrix([</span>
<span id="cb282-2">    [<span>1</span>,<span>1</span>,<span>1</span>,<span>2</span>],</span>
<span id="cb282-3">    [<span>2</span>,<span>2</span>,<span>2</span>,<span>4</span>]</span>
<span id="cb282-4">])</span>
<span id="cb282-5"></span>
<span id="cb282-6"><span>print</span>(<span>"RREF form:</span><span>\n</span><span>"</span>, M.rref()[<span>0</span>])</span></code></pre></div>
<div>
<pre><code>RREF form:
 Matrix([[1, 1, 1, 2], [0, 0, 0, 0]])</code></pre>
</div>
</div>
<p>The second row disappears, showing infinite solutions.</p>
<ol start="4" type="1">
<li>Encoding solution sets</li>
</ol>
<p>General solutions are often written in parametric vector form.</p>
<p>For the infinite solution above:</p>
<p><span>\[
(x,y,z) = (2,0,0) + s(-1,1,0) + t(-1,0,1)
\]</span></p>
<p>This shows the solution space is a plane in <span>\(\mathbb{R}^3\)</span>.</p>
</section>
<section id="try-it-yourself-26">
<h4 data-anchor-id="try-it-yourself-26">Try It Yourself</h4>
<ol type="1">
<li><p>Solve:</p>
<p><span>\[
x + 2y = 5, \quad y = 1
\]</span></p>
<p>Do back substitution by hand and check with NumPy.</p></li>
<li><p>Take the system:</p>
<p><span>\[
x + y + z = 1, \quad 2x + 2y + 2z = 2
\]</span></p>
<p>Write its solution set in parametric form.</p></li>
<li><p>Use <code>Matrix.rref()</code> on a 3×4 random augmented matrix. Identify pivot and free variables, then describe the solution set.</p></li>
</ol>
</section>
<section id="the-takeaway-10">
<h4 data-anchor-id="the-takeaway-10">The Takeaway</h4>
<ul>
<li>Back substitution is the cleanup step after Gaussian elimination.</li>
<li>It reveals whether the system has a unique solution or infinitely many.</li>
<li>Solutions can be expressed explicitly (unique case) or parametrically (infinite case).</li>
</ul>
</section>
</section>
<section id="rank-and-its-first-meaning-pivots-as-information">
<h3 data-anchor-id="rank-and-its-first-meaning-pivots-as-information">29. Rank and Its First Meaning (Pivots as Information)</h3>
<p>The rank of a matrix tells us how much independent information it contains. Rank is one of the most important concepts in linear algebra because it connects to pivots, independence, dimension, and the number of solutions to a system.</p>
<section id="set-up-your-lab-28">
<h4 data-anchor-id="set-up-your-lab-28">Set Up Your Lab</h4>
<div id="0b673d87" data-execution_count="166"><pre><code><span id="cb284-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb284-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-28">
<h4 data-anchor-id="step-by-step-code-walkthrough-28">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Rank definition The rank is the number of pivots (leading ones) in the row-echelon form of a matrix.</li>
</ol>
<p>Example:</p>
<div id="f4a67a22" data-execution_count="167">
<div id="cb285"><pre><code><span id="cb285-1">A <span>=</span> Matrix([</span>
<span id="cb285-2">    [<span>1</span>, <span>2</span>, <span>3</span>],</span>
<span id="cb285-3">    [<span>2</span>, <span>4</span>, <span>6</span>],</span>
<span id="cb285-4">    [<span>1</span>, <span>1</span>, <span>1</span>]</span>
<span id="cb285-5">])</span>
<span id="cb285-6"></span>
<span id="cb285-7"><span>print</span>(<span>"RREF:</span><span>\n</span><span>"</span>, A.rref()[<span>0</span>])</span>
<span id="cb285-8"><span>print</span>(<span>"Rank of A:"</span>, A.rank())</span></code></pre></div>
<div>
<pre><code>RREF:
 Matrix([[1, 0, -1], [0, 1, 2], [0, 0, 0]])
Rank of A: 2</code></pre>
</div>
</div>
<ul>
<li>The second row is a multiple of the first, so the rank is less than 3.</li>
<li>Only two independent rows → rank = 2.</li>
</ul>
<ol start="2" type="1">
<li>Rank and solutions to <span>\(A·x = b\)</span></li>
</ol>
<p>Consider:</p>
<p><span>\[
\begin{cases}  
x + y + z = 3 \\  
2x + 2y + 2z = 6 \\  
x - y = 0  
\end{cases}
\]</span></p>
<div id="35026cd1" data-execution_count="168">
<div id="cb287"><pre><code><span id="cb287-1">M <span>=</span> Matrix([</span>
<span id="cb287-2">    [<span>1</span>, <span>1</span>, <span>1</span>, <span>3</span>],</span>
<span id="cb287-3">    [<span>2</span>, <span>2</span>, <span>2</span>, <span>6</span>],</span>
<span id="cb287-4">    [<span>1</span>, <span>-</span><span>1</span>, <span>0</span>, <span>0</span>]</span>
<span id="cb287-5">])</span>
<span id="cb287-6"></span>
<span id="cb287-7"><span>print</span>(<span>"RREF:</span><span>\n</span><span>"</span>, M.rref()[<span>0</span>])</span>
<span id="cb287-8"><span>print</span>(<span>"Rank of coefficient matrix:"</span>, M[:, :<span>-</span><span>1</span>].rank())</span>
<span id="cb287-9"><span>print</span>(<span>"Rank of augmented matrix:"</span>, M.rank())</span></code></pre></div>
<div>
<pre><code>RREF:
 Matrix([[1, 0, 1/2, 3/2], [0, 1, 1/2, 3/2], [0, 0, 0, 0]])
Rank of coefficient matrix: 2
Rank of augmented matrix: 2</code></pre>
</div>
</div>
<ul>
<li>If rank(A) = rank([A|b]) = number of variables → unique solution.</li>
<li>If rank(A) = rank([A|b]) &lt; number of variables → infinite solutions.</li>
<li>If rank(A) &lt; rank([A|b]) → no solution.</li>
</ul>
<ol start="3" type="1">
<li>NumPy comparison</li>
</ol>
<div id="ec8b453e" data-execution_count="169"><pre><code><span id="cb289-1">A <span>=</span> np.array([</span>
<span id="cb289-2">    [<span>1</span>, <span>2</span>, <span>3</span>],</span>
<span id="cb289-3">    [<span>2</span>, <span>4</span>, <span>6</span>],</span>
<span id="cb289-4">    [<span>1</span>, <span>1</span>, <span>1</span>]</span>
<span id="cb289-5">], dtype<span>=</span><span>float</span>)</span>
<span id="cb289-6"></span>
<span id="cb289-7"><span>print</span>(<span>"Rank with NumPy:"</span>, np.linalg.matrix_rank(A))</span></code></pre></div>
<ol start="4" type="1">
<li>Rank as “dimension of information”</li>
</ol>
<p>The rank equals:</p>
<ul>
<li>The number of independent rows.</li>
<li>The number of independent columns.</li>
<li>The dimension of the column space.</li>
</ul>
<div id="9eb404bd" data-execution_count="170"><pre><code><span id="cb291-1">B <span>=</span> Matrix([</span>
<span id="cb291-2">    [<span>1</span>,<span>2</span>],</span>
<span id="cb291-3">    [<span>2</span>,<span>4</span>],</span>
<span id="cb291-4">    [<span>3</span>,<span>6</span>]</span>
<span id="cb291-5">])</span>
<span id="cb291-6"></span>
<span id="cb291-7"><span>print</span>(<span>"Rank of B:"</span>, B.rank())</span></code></pre></div>
<p>All columns are multiples → only one independent direction → rank = 1.</p>
</section>
<section id="try-it-yourself-27">
<h4 data-anchor-id="try-it-yourself-27">Try It Yourself</h4>
<ol type="1">
<li><p>Compute the rank of:</p>
<p><span>\[
\begin{bmatrix}  
1 &amp; 2 &amp; 3 \\  
2 &amp; 4 &amp; 6 \\  
3 &amp; 6 &amp; 9  
\end{bmatrix}
\]</span></p>
<p>What do you expect?</p></li>
<li><p>Create a random 4×4 matrix with <code>np.random.randint</code>. Compute its rank with both SymPy and NumPy.</p></li>
<li><p>Test solution consistency using rank: build a system where rank(A) ≠ rank([A|b]) and show it has no solution.</p></li>
</ol>
</section>
<section id="the-takeaway-11">
<h4 data-anchor-id="the-takeaway-11">The Takeaway</h4>
<ul>
<li>Rank = number of pivots = dimension of independent information.</li>
<li>Rank reveals whether a system has no solution, one solution, or infinitely many.</li>
<li>Rank connects algebra (pivots) with geometry (dimension of subspaces).</li>
</ul>
</section>
</section>
<section id="lu-factorization-elimination-captured-as-l-and-u">
<h3 data-anchor-id="lu-factorization-elimination-captured-as-l-and-u">30. LU Factorization (Elimination Captured as L and U)</h3>
<p>Gaussian elimination can be recorded in a neat factorization:</p>
<p><span>\[
A = LU
\]</span></p>
<p>where <span>\(L\)</span> is a lower triangular matrix (recording the multipliers we used) and <span>\(U\)</span> is an upper triangular matrix (the result of elimination). This is called LU factorization. It’s a powerful tool for solving systems efficiently.</p>
<section id="set-up-your-lab-29">
<h4 data-anchor-id="set-up-your-lab-29">Set Up Your Lab</h4>
<div id="9b763c62" data-execution_count="171"><pre><code><span id="cb293-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb293-2"><span>from</span> scipy.linalg <span>import</span> lu</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-29">
<h4 data-anchor-id="step-by-step-code-walkthrough-29">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Example matrix</li>
</ol>
<div id="eef236fb" data-execution_count="172">
<div id="cb294"><pre><code><span id="cb294-1">A <span>=</span> np.array([</span>
<span id="cb294-2">    [<span>2</span>, <span>3</span>, <span>1</span>],</span>
<span id="cb294-3">    [<span>4</span>, <span>7</span>, <span>7</span>],</span>
<span id="cb294-4">    [<span>6</span>, <span>18</span>, <span>22</span>]</span>
<span id="cb294-5">], dtype<span>=</span><span>float</span>)</span>
<span id="cb294-6"></span>
<span id="cb294-7"><span>print</span>(<span>"Matrix A:</span><span>\n</span><span>"</span>, A)</span></code></pre></div>
<div>
<pre><code>Matrix A:
 [[ 2.  3.  1.]
 [ 4.  7.  7.]
 [ 6. 18. 22.]]</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>LU decomposition with SciPy</li>
</ol>
<div id="6cbe45f3" data-execution_count="173">
<div id="cb296"><pre><code><span id="cb296-1">P, L, U <span>=</span> lu(A)</span>
<span id="cb296-2"></span>
<span id="cb296-3"><span>print</span>(<span>"Permutation matrix P:</span><span>\n</span><span>"</span>, P)</span>
<span id="cb296-4"><span>print</span>(<span>"Lower triangular L:</span><span>\n</span><span>"</span>, L)</span>
<span id="cb296-5"><span>print</span>(<span>"Upper triangular U:</span><span>\n</span><span>"</span>, U)</span></code></pre></div>
<div>
<pre><code>Permutation matrix P:
 [[0. 0. 1.]
 [0. 1. 0.]
 [1. 0. 0.]]
Lower triangular L:
 [[1.         0.         0.        ]
 [0.66666667 1.         0.        ]
 [0.33333333 0.6        1.        ]]
Upper triangular U:
 [[ 6.         18.         22.        ]
 [ 0.         -5.         -7.66666667]
 [ 0.          0.         -1.73333333]]</code></pre>
</div>
</div>
<p>Here, <span>\(P\)</span> handles row swaps (partial pivoting), <span>\(L\)</span> is lower triangular, and <span>\(U\)</span> is upper triangular.</p>
<ol start="3" type="1">
<li>Verifying the factorization</li>
</ol>
<div id="f8dbffe3" data-execution_count="174"><pre><code><span id="cb298-1">reconstructed <span>=</span> P <span>@</span> L <span>@</span> U</span>
<span id="cb298-2"><span>print</span>(<span>"Does P·L·U equal A?</span><span>\n</span><span>"</span>, np.allclose(reconstructed, A))</span></code></pre></div>
<ol start="4" type="1">
<li>Solving a system with LU</li>
</ol>
<p>Suppose we want to solve <span>\(Ax = b\)</span>. Instead of working directly with <span>\(A\)</span>, we solve in two steps:</p>
<ol type="1">
<li>Solve <span>\(Ly = Pb\)</span> (forward substitution).</li>
<li>Solve <span>\(Ux = y\)</span> (back substitution).</li>
</ol>
<div id="cc490e4e" data-execution_count="175">
<div id="cb300"><pre><code><span id="cb300-1">b <span>=</span> np.array([<span>1</span>, <span>2</span>, <span>3</span>], dtype<span>=</span><span>float</span>)</span>
<span id="cb300-2"></span>
<span id="cb300-3"><span># Step 1: Pb</span></span>
<span id="cb300-4">Pb <span>=</span> P <span>@</span> b</span>
<span id="cb300-5"></span>
<span id="cb300-6"><span># Step 2: forward substitution Ly = Pb</span></span>
<span id="cb300-7">y <span>=</span> np.linalg.solve(L, Pb)</span>
<span id="cb300-8"></span>
<span id="cb300-9"><span># Step 3: back substitution Ux = y</span></span>
<span id="cb300-10">x <span>=</span> np.linalg.solve(U, y)</span>
<span id="cb300-11"></span>
<span id="cb300-12"><span>print</span>(<span>"Solution x:"</span>, x)</span></code></pre></div>
<div>
<pre><code>Solution x: [ 0.5 -0.  -0. ]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Efficiency advantage</li>
</ol>
<p>If we have to solve many systems with the same <span>\(A\)</span> but different <span>\(b\)</span>, we only compute <span>\(LU\)</span> once, then reuse it. This saves a lot of computation.</p>
<ol start="6" type="1">
<li>NumPy’s built-in rank-revealing factorization</li>
</ol>
<p>While NumPy doesn’t have <code>lu</code> directly, it works seamlessly with SciPy. For large matrices, LU decomposition is the backbone of solvers like <code>np.linalg.solve</code>.</p>
</section>
<section id="try-it-yourself-28">
<h4 data-anchor-id="try-it-yourself-28">Try It Yourself</h4>
<ol type="1">
<li><p>Compute LU decomposition for</p>
<p><span>\[
A = \begin{bmatrix} 1 &amp; 2 &amp; 0 \\ 3 &amp; 4 &amp; 4 \\ 5 &amp; 6 &amp; 3 \end{bmatrix}
\]</span></p>
<p>Verify <span>\(P·L·U = A\)</span>.</p></li>
<li><p>Solve <span>\(Ax = b\)</span> with</p>
<p><span>\[
b = [3,7,8]
\]</span></p>
<p>using LU factorization.</p></li>
<li><p>Compare solving with LU factorization vs directly using <code>np.linalg.solve(A,b)</code>. Are the answers the same?</p></li>
</ol>
</section>
<section id="the-takeaway-12">
<h4 data-anchor-id="the-takeaway-12">The Takeaway</h4>
<ul>
<li>LU factorization captures Gaussian elimination in matrix form: <span>\(A = P·L·U\)</span>.</li>
<li>It allows fast repeated solving of systems with different right-hand sides.</li>
<li>LU decomposition is a core technique in numerical linear algebra and the basis of many solvers.</li>
</ul>
</section>
</section>
</section>
<section id="chapter-4.-vector-spaces-and-subspaces">
<h2 data-anchor-id="chapter-4.-vector-spaces-and-subspaces">Chapter 4. Vector Spaces and Subspaces</h2>
<section id="axioms-of-vector-spaces-what-space-really-means">
<h3 data-anchor-id="axioms-of-vector-spaces-what-space-really-means">31. Axioms of Vector Spaces (What “Space” Really Means)</h3>
<p>Vector spaces generalize what we’ve been doing with vectors and matrices. Instead of just <span>\(\mathbb{R}^n\)</span>, a vector space is any collection of objects (vectors) where addition and scalar multiplication follow specific axioms (rules). In this lab, we’ll explore these axioms concretely with Python.</p>
<section id="set-up-your-lab-30">
<h4 data-anchor-id="set-up-your-lab-30">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-30">
<h4 data-anchor-id="step-by-step-code-walkthrough-30">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Vector space example: <span>\(\mathbb{R}^2\)</span></li>
</ol>
<p>Let’s check two rules (axioms): closure under addition and scalar multiplication.</p>
<div id="640f924c" data-execution_count="177">
<div id="cb303"><pre><code><span id="cb303-1">u <span>=</span> np.array([<span>1</span>, <span>2</span>])</span>
<span id="cb303-2">v <span>=</span> np.array([<span>3</span>, <span>-</span><span>1</span>])</span>
<span id="cb303-3"></span>
<span id="cb303-4"><span># Closure under addition</span></span>
<span id="cb303-5"><span>print</span>(<span>"u + v ="</span>, u <span>+</span> v)</span>
<span id="cb303-6"></span>
<span id="cb303-7"><span># Closure under scalar multiplication</span></span>
<span id="cb303-8">k <span>=</span> <span>5</span></span>
<span id="cb303-9"><span>print</span>(<span>"k * u ="</span>, k <span>*</span> u)</span></code></pre></div>
<div>
<pre><code>u + v = [4 1]
k * u = [ 5 10]</code></pre>
</div>
</div>
<p>Both results are still in <span>\(\mathbb{R}^2\)</span>.</p>
<ol start="2" type="1">
<li>Zero vector and additive inverses</li>
</ol>
<p>Every vector space must contain a zero vector, and every vector must have an additive inverse.</p>
<div id="e8f19c9e" data-execution_count="178">
<div id="cb305"><pre><code><span id="cb305-1">zero <span>=</span> np.array([<span>0</span>, <span>0</span>])</span>
<span id="cb305-2">inverse_u <span>=</span> <span>-</span>u</span>
<span id="cb305-3"><span>print</span>(<span>"Zero vector:"</span>, zero)</span>
<span id="cb305-4"><span>print</span>(<span>"u + (-u) ="</span>, u <span>+</span> inverse_u)</span></code></pre></div>
<div>
<pre><code>Zero vector: [0 0]
u + (-u) = [0 0]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Distributive and associative properties</li>
</ol>
<p>Check:</p>
<ul>
<li><span>\(a(u+v) = au + av\)</span></li>
<li><span>\((a+b)u = au + bu\)</span></li>
</ul>
<div id="4e3797ce" data-execution_count="179">
<div id="cb307"><pre><code><span id="cb307-1">a, b <span>=</span> <span>2</span>, <span>3</span></span>
<span id="cb307-2"></span>
<span id="cb307-3">lhs1 <span>=</span> a <span>*</span> (u <span>+</span> v)</span>
<span id="cb307-4">rhs1 <span>=</span> a<span>*</span>u <span>+</span> a<span>*</span>v</span>
<span id="cb307-5"><span>print</span>(<span>"a(u+v) ="</span>, lhs1, <span>", au+av ="</span>, rhs1)</span>
<span id="cb307-6"></span>
<span id="cb307-7">lhs2 <span>=</span> (a<span>+</span>b) <span>*</span> u</span>
<span id="cb307-8">rhs2 <span>=</span> a<span>*</span>u <span>+</span> b<span>*</span>u</span>
<span id="cb307-9"><span>print</span>(<span>"(a+b)u ="</span>, lhs2, <span>", au+bu ="</span>, rhs2)</span></code></pre></div>
<div>
<pre><code>a(u+v) = [8 2] , au+av = [8 2]
(a+b)u = [ 5 10] , au+bu = [ 5 10]</code></pre>
</div>
</div>
<p>Both equalities hold → distributive laws confirmed.</p>
<ol start="4" type="1">
<li>A set that fails to be a vector space</li>
</ol>
<p>Consider only positive numbers with normal addition and scalar multiplication.</p>
<div id="3b2fd0b5" data-execution_count="180">
<div id="cb309"><pre><code><span id="cb309-1">positive_numbers <span>=</span> [<span>1</span>, <span>2</span>, <span>3</span>]</span>
<span id="cb309-2"><span>try</span>:</span>
<span id="cb309-3">    <span>print</span>(<span>"Closure under negatives?"</span>, <span>-</span><span>1</span> <span>*</span> np.array(positive_numbers))</span>
<span id="cb309-4"><span>except</span> <span>Exception</span> <span>as</span> e:</span>
<span id="cb309-5">    <span>print</span>(<span>"Error:"</span>, e)</span></code></pre></div>
<div>
<pre><code>Closure under negatives? [-1 -2 -3]</code></pre>
</div>
</div>
<p>Negative results leave the set → not a vector space.</p>
<ol start="5" type="1">
<li>Python helper to check axioms</li>
</ol>
<p>We can quickly check if a set of vectors is closed under addition and scalar multiplication.</p>
<div id="68817a60" data-execution_count="181">
<div id="cb311"><pre><code><span id="cb311-1"><span>def</span> check_closure(vectors, scalars):</span>
<span id="cb311-2">    <span>for</span> v <span>in</span> vectors:</span>
<span id="cb311-3">        <span>for</span> u <span>in</span> vectors:</span>
<span id="cb311-4">            <span>if</span> <span>not</span> <span>any</span>(np.array_equal(v<span>+</span>u, w) <span>for</span> w <span>in</span> vectors):</span>
<span id="cb311-5">                <span>return</span> <span>False</span></span>
<span id="cb311-6">        <span>for</span> k <span>in</span> scalars:</span>
<span id="cb311-7">            <span>if</span> <span>not</span> <span>any</span>(np.array_equal(k<span>*</span>v, w) <span>for</span> w <span>in</span> vectors):</span>
<span id="cb311-8">                <span>return</span> <span>False</span></span>
<span id="cb311-9">    <span>return</span> <span>True</span></span>
<span id="cb311-10"></span>
<span id="cb311-11">vectors <span>=</span> [np.array([<span>0</span>,<span>0</span>]), np.array([<span>1</span>,<span>0</span>]), np.array([<span>0</span>,<span>1</span>]), np.array([<span>1</span>,<span>1</span>])]</span>
<span id="cb311-12">scalars <span>=</span> [<span>0</span>,<span>1</span>,<span>-</span><span>1</span>]</span>
<span id="cb311-13"><span>print</span>(<span>"Closed under addition and scalar multiplication?"</span>, check_closure(vectors, scalars))</span></code></pre></div>
<div>
<pre><code>Closed under addition and scalar multiplication? False</code></pre>
</div>
</div>
<p>This small set is closed → it forms a vector space (a subspace of <span>\(\mathbb{R}^2\)</span>).</p>
</section>
<section id="try-it-yourself-29">
<h4 data-anchor-id="try-it-yourself-29">Try It Yourself</h4>
<ol type="1">
<li>Verify that <span>\(\mathbb{R}^3\)</span> satisfies the vector space axioms using random vectors.</li>
<li>Test whether the set of all 2×2 matrices forms a vector space under normal addition and scalar multiplication.</li>
<li>Find an example of a set that fails closure (e.g., integers under division).</li>
</ol>
</section>
<section id="the-takeaway-13">
<h4 data-anchor-id="the-takeaway-13">The Takeaway</h4>
<ul>
<li>A vector space is any set where addition and scalar multiplication satisfy 10 standard axioms.</li>
<li>These rules ensure consistent algebraic behavior.</li>
<li>Many objects beyond arrows in <span>\(\mathbb{R}^n\)</span> (like polynomials or matrices) are vector spaces too.</li>
</ul>
</section>
</section>
<section id="subspaces-column-space-and-null-space-where-solutions-live">
<h3 data-anchor-id="subspaces-column-space-and-null-space-where-solutions-live">32. Subspaces, Column Space, and Null Space (Where Solutions Live)</h3>
<p>A subspace is a smaller vector space sitting inside a bigger one. For matrices, two subspaces show up all the time:</p>
<ul>
<li>Column space: all combinations of the matrix’s columns (possible outputs of <span>\(Ax\)</span>).</li>
<li>Null space: all vectors <span>\(x\)</span> such that <span>\(Ax = 0\)</span> (inputs that vanish).</li>
</ul>
<p>This lab explores both in Python.</p>
<section id="set-up-your-lab-31">
<h4 data-anchor-id="set-up-your-lab-31">Set Up Your Lab</h4>
<div id="23a11874" data-execution_count="182"><pre><code><span id="cb313-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb313-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-31">
<h4 data-anchor-id="step-by-step-code-walkthrough-31">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Column space basics</li>
</ol>
<p>Take:</p>
<p><span>\[
A = \begin{bmatrix} 1 &amp; 2 \\ 2 &amp; 4 \\ 3 &amp; 6 \end{bmatrix}
\]</span></p>
<div id="abb321dc" data-execution_count="183">
<div id="cb314"><pre><code><span id="cb314-1">A <span>=</span> Matrix([</span>
<span id="cb314-2">    [<span>1</span>,<span>2</span>],</span>
<span id="cb314-3">    [<span>2</span>,<span>4</span>],</span>
<span id="cb314-4">    [<span>3</span>,<span>6</span>]</span>
<span id="cb314-5">])</span>
<span id="cb314-6"></span>
<span id="cb314-7"><span>print</span>(<span>"Matrix A:</span><span>\n</span><span>"</span>, A)</span>
<span id="cb314-8"><span>print</span>(<span>"Column space basis:</span><span>\n</span><span>"</span>, A.columnspace())</span>
<span id="cb314-9"><span>print</span>(<span>"Rank (dimension of column space):"</span>, A.rank())</span></code></pre></div>
<div>
<pre><code>Matrix A:
 Matrix([[1, 2], [2, 4], [3, 6]])
Column space basis:
 [Matrix([
[1],
[2],
[3]])]
Rank (dimension of column space): 1</code></pre>
</div>
</div>
<ul>
<li>The second column is a multiple of the first → column space has dimension 1.</li>
<li>All outputs of <span>\(Ax\)</span> lie on a line in <span>\(\mathbb{R}^3\)</span>.</li>
</ul>
<ol start="2" type="1">
<li>Null space basics</li>
</ol>
<div id="72fb7eb0" data-execution_count="184">
<div id="cb316"><pre><code><span id="cb316-1"><span>print</span>(<span>"Null space basis:</span><span>\n</span><span>"</span>, A.nullspace())</span></code></pre></div>
<div>
<pre><code>Null space basis:
 [Matrix([
[-2],
[ 1]])]</code></pre>
</div>
</div>
<p>The null space contains all <span>\(x\)</span> where <span>\(Ax=0\)</span>. Here, the null space is 1-dimensional (vectors like <span>\([-2,1]\)</span>).</p>
<ol start="3" type="1">
<li>A full-rank example</li>
</ol>
<div id="e75a5e8b" data-execution_count="185">
<div id="cb318"><pre><code><span id="cb318-1">B <span>=</span> Matrix([</span>
<span id="cb318-2">    [<span>1</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb318-3">    [<span>0</span>,<span>1</span>,<span>0</span>],</span>
<span id="cb318-4">    [<span>0</span>,<span>0</span>,<span>1</span>]</span>
<span id="cb318-5">])</span>
<span id="cb318-6"></span>
<span id="cb318-7"><span>print</span>(<span>"Column space basis:</span><span>\n</span><span>"</span>, B.columnspace())</span>
<span id="cb318-8"><span>print</span>(<span>"Null space basis:</span><span>\n</span><span>"</span>, B.nullspace())</span></code></pre></div>
<div>
<pre><code>Column space basis:
 [Matrix([
[1],
[0],
[0]]), Matrix([
[0],
[1],
[0]]), Matrix([
[0],
[0],
[1]])]
Null space basis:
 []</code></pre>
</div>
</div>
<ul>
<li>Column space = all of <span>\(\mathbb{R}^3\)</span>.</li>
<li>Null space = only the zero vector.</li>
</ul>
<ol start="4" type="1">
<li>Geometry link</li>
</ol>
<p>For <span>\(A\)</span> (rank 1, 2 columns):</p>
<ul>
<li>Column space: line in <span>\(\mathbb{R}^3\)</span>.</li>
<li>Null space: line in <span>\(\mathbb{R}^2\)</span>.</li>
</ul>
<p>Together they explain the system <span>\(Ax = b\)</span>:</p>
<ul>
<li>If <span>\(b\)</span> is outside the column space, no solution exists.</li>
<li>If <span>\(b\)</span> is inside, solutions differ by a vector in the null space.</li>
</ul>
<ol start="5" type="1">
<li>Quick NumPy version</li>
</ol>
<p>NumPy doesn’t directly give null space, but we can compute it with SVD.</p>
<div id="7384887f" data-execution_count="186">
<div id="cb320"><pre><code><span id="cb320-1"><span>from</span> numpy.linalg <span>import</span> svd</span>
<span id="cb320-2"></span>
<span id="cb320-3">A <span>=</span> np.array([[<span>1</span>,<span>2</span>],[<span>2</span>,<span>4</span>],[<span>3</span>,<span>6</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb320-4">U, S, Vt <span>=</span> svd(A)</span>
<span id="cb320-5"></span>
<span id="cb320-6">tol <span>=</span> <span>1e-10</span></span>
<span id="cb320-7">null_mask <span>=</span> (S <span>&lt;=</span> tol)</span>
<span id="cb320-8">null_space <span>=</span> Vt.T[:, null_mask]</span>
<span id="cb320-9"><span>print</span>(<span>"Null space (via SVD):</span><span>\n</span><span>"</span>, null_space)</span></code></pre></div>
<div>
<pre><code>Null space (via SVD):
 [[-0.89442719]
 [ 0.4472136 ]]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-30">
<h4 data-anchor-id="try-it-yourself-30">Try It Yourself</h4>
<ol type="1">
<li><p>Find the column space and null space of</p>
<p><span>\[
\begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \\ 0 &amp; 0 \end{bmatrix}
\]</span></p>
<p>How many dimensions does each have?</p></li>
<li><p>Generate a random 3×3 matrix. Compute its rank, column space, and null space.</p></li>
<li><p>Solve <span>\(Ax = b\)</span> with</p>
<p><span>\[
A = \begin{bmatrix} 1 &amp; 2 \\ 2 &amp; 4 \\ 3 &amp; 6 \end{bmatrix}, \quad b = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}
\]</span></p>
<p>and describe why it has infinitely many solutions.</p></li>
</ol>
</section>
<section id="the-takeaway-14">
<h4 data-anchor-id="the-takeaway-14">The Takeaway</h4>
<ul>
<li>The column space = all possible outputs of a matrix.</li>
<li>The null space = all inputs that map to zero.</li>
<li>These subspaces give the complete picture of what a matrix does.</li>
</ul>
</section>
</section>
<section id="span-and-generating-sets-coverage-of-a-space">
<h3 data-anchor-id="span-and-generating-sets-coverage-of-a-space">33. Span and Generating Sets (Coverage of a Space)</h3>
<p>The span of a set of vectors is all the linear combinations you can make from them. If a set of vectors can “cover” a whole space, we call it a generating set. This lab shows how to compute and visualize spans.</p>
<section id="set-up-your-lab-32">
<h4 data-anchor-id="set-up-your-lab-32">Set Up Your Lab</h4>
<div id="248ce6d5" data-execution_count="187"><pre><code><span id="cb322-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb322-2"><span>from</span> sympy <span>import</span> Matrix</span>
<span id="cb322-3"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-32">
<h4 data-anchor-id="step-by-step-code-walkthrough-32">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Span in <span>\(\mathbb{R}^2\)</span></li>
</ol>
<p>Two vectors that aren’t multiples span the whole plane.</p>
<div id="c4e2e6e7" data-execution_count="188"><pre><code><span id="cb323-1">u <span>=</span> np.array([<span>1</span>, <span>0</span>])</span>
<span id="cb323-2">v <span>=</span> np.array([<span>0</span>, <span>1</span>])</span>
<span id="cb323-3"></span>
<span id="cb323-4">M <span>=</span> Matrix.hstack(Matrix(u), Matrix(v))</span>
<span id="cb323-5"><span>print</span>(<span>"Rank:"</span>, M.rank())</span></code></pre></div>
<p>Rank = 2 → the span of <span>\(\{u,v\}\)</span> is all of <span>\(\mathbb{R}^2\)</span>.</p>
<ol start="2" type="1">
<li>Dependent vectors (smaller span)</li>
</ol>
<div id="fc19c39c" data-execution_count="189"><pre><code><span id="cb325-1">u <span>=</span> np.array([<span>1</span>, <span>2</span>])</span>
<span id="cb325-2">v <span>=</span> np.array([<span>2</span>, <span>4</span>])</span>
<span id="cb325-3"></span>
<span id="cb325-4">M <span>=</span> Matrix.hstack(Matrix(u), Matrix(v))</span>
<span id="cb325-5"><span>print</span>(<span>"Rank:"</span>, M.rank())</span></code></pre></div>
<p>Rank = 1 → these vectors only span a line.</p>
<ol start="3" type="1">
<li>Visualizing a span</li>
</ol>
<p>Let’s see what the span of two vectors looks like.</p>
<div id="1e1f9ebb" data-execution_count="190">
<div id="cb327"><pre><code><span id="cb327-1">u <span>=</span> np.array([<span>1</span>, <span>2</span>])</span>
<span id="cb327-2">v <span>=</span> np.array([<span>2</span>, <span>1</span>])</span>
<span id="cb327-3"></span>
<span id="cb327-4">coeffs <span>=</span> np.linspace(<span>-</span><span>2</span>, <span>2</span>, <span>11</span>)</span>
<span id="cb327-5">points <span>=</span> []</span>
<span id="cb327-6"><span>for</span> a <span>in</span> coeffs:</span>
<span id="cb327-7">    <span>for</span> b <span>in</span> coeffs:</span>
<span id="cb327-8">        points.append(a<span>*</span>u <span>+</span> b<span>*</span>v)</span>
<span id="cb327-9">points <span>=</span> np.array(points)</span>
<span id="cb327-10"></span>
<span id="cb327-11">plt.scatter(points[:,<span>0</span>], points[:,<span>1</span>], s<span>=</span><span>10</span>)</span>
<span id="cb327-12">plt.axhline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb327-13">plt.axvline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb327-14">plt.title(<span>"Span of {u,v}"</span>)</span>
<span id="cb327-15">plt.grid()</span>
<span id="cb327-16">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-191-output-1.png" width="569" height="431"></p>
</figure>
</div>
</div>
<p>You’ll see a filled grid - the entire plane, because the two vectors are independent.</p>
<ol start="4" type="1">
<li>Generating set of a space</li>
</ol>
<p>For <span>\(\mathbb{R}^3\)</span>:</p>
<div id="01c900f8" data-execution_count="191"><pre><code><span id="cb328-1">basis <span>=</span> [Matrix([<span>1</span>,<span>0</span>,<span>0</span>]), Matrix([<span>0</span>,<span>1</span>,<span>0</span>]), Matrix([<span>0</span>,<span>0</span>,<span>1</span>])]</span>
<span id="cb328-2">M <span>=</span> Matrix.hstack(<span>*</span>basis)</span>
<span id="cb328-3"><span>print</span>(<span>"Rank:"</span>, M.rank())</span></code></pre></div>
<p>Rank = 3 → this set spans the whole space.</p>
<ol start="5" type="1">
<li>Testing if a vector is in the span</li>
</ol>
<p>Example: Is <span>\([3,5]\)</span> in the span of <span>\([1,2]\)</span> and <span>\([2,1]\)</span>?</p>
<div id="d5069889" data-execution_count="192">
<div id="cb330"><pre><code><span id="cb330-1">u <span>=</span> Matrix([<span>1</span>,<span>2</span>])</span>
<span id="cb330-2">v <span>=</span> Matrix([<span>2</span>,<span>1</span>])</span>
<span id="cb330-3">target <span>=</span> Matrix([<span>3</span>,<span>5</span>])</span>
<span id="cb330-4"></span>
<span id="cb330-5">M <span>=</span> Matrix.hstack(u,v)</span>
<span id="cb330-6">solution <span>=</span> M.gauss_jordan_solve(target)</span>
<span id="cb330-7"><span>print</span>(<span>"Coefficients (a,b):"</span>, solution)</span></code></pre></div>
<div>
<pre><code>Coefficients (a,b): (Matrix([
[7/3],
[1/3]]), Matrix(0, 1, []))</code></pre>
</div>
</div>
<p>If a solution exists, the target is in the span.</p>
</section>
<section id="try-it-yourself-31">
<h4 data-anchor-id="try-it-yourself-31">Try It Yourself</h4>
<ol type="1">
<li>Test if <span>\([4,6]\)</span> is in the span of <span>\([1,2]\)</span>.</li>
<li>Visualize the span of <span>\([1,0,0]\)</span> and <span>\([0,1,0]\)</span> in <span>\(\mathbb{R}^3\)</span>. What does it look like?</li>
<li>Create a random 3×3 matrix. Use <code>rank()</code> to check if its columns span <span>\(\mathbb{R}^3\)</span>.</li>
</ol>
</section>
<section id="the-takeaway-15">
<h4 data-anchor-id="the-takeaway-15">The Takeaway</h4>
<ul>
<li>Span = all linear combinations of a set of vectors.</li>
<li>Independent vectors span bigger spaces; dependent ones collapse to smaller spaces.</li>
<li>Generating sets are the foundation of bases and coordinate systems.</li>
</ul>
</section>
</section>
<section id="linear-independence-and-dependence-no-redundancy-vs.-redundancy">
<h3 data-anchor-id="linear-independence-and-dependence-no-redundancy-vs.-redundancy">34. Linear Independence and Dependence (No Redundancy vs.&nbsp;Redundancy)</h3>
<p>A set of vectors is linearly independent if none of them can be written as a combination of the others. If at least one can, the set is dependent. This distinction tells us whether a set of vectors has redundancy.</p>
<section id="set-up-your-lab-33">
<h4 data-anchor-id="set-up-your-lab-33">Set Up Your Lab</h4>
<div id="9fb82c4e" data-execution_count="193"><pre><code><span id="cb332-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb332-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-33">
<h4 data-anchor-id="step-by-step-code-walkthrough-33">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Independent vectors example</li>
</ol>
<div id="c53a8ae7" data-execution_count="194">
<div id="cb333"><pre><code><span id="cb333-1">v1 <span>=</span> Matrix([<span>1</span>, <span>0</span>, <span>0</span>])</span>
<span id="cb333-2">v2 <span>=</span> Matrix([<span>0</span>, <span>1</span>, <span>0</span>])</span>
<span id="cb333-3">v3 <span>=</span> Matrix([<span>0</span>, <span>0</span>, <span>1</span>])</span>
<span id="cb333-4"></span>
<span id="cb333-5">M <span>=</span> Matrix.hstack(v1, v2, v3)</span>
<span id="cb333-6"><span>print</span>(<span>"Rank:"</span>, M.rank(), <span>" Number of vectors:"</span>, M.shape[<span>1</span>])</span></code></pre></div>
<div>
<pre><code>Rank: 3  Number of vectors: 3</code></pre>
</div>
</div>
<p>Rank = 3, number of vectors = 3 → all independent.</p>
<ol start="2" type="1">
<li>Dependent vectors example</li>
</ol>
<div id="9968ffae" data-execution_count="195">
<div id="cb335"><pre><code><span id="cb335-1">v1 <span>=</span> Matrix([<span>1</span>, <span>2</span>, <span>3</span>])</span>
<span id="cb335-2">v2 <span>=</span> Matrix([<span>2</span>, <span>4</span>, <span>6</span>])</span>
<span id="cb335-3">v3 <span>=</span> Matrix([<span>3</span>, <span>6</span>, <span>9</span>])</span>
<span id="cb335-4"></span>
<span id="cb335-5">M <span>=</span> Matrix.hstack(v1, v2, v3)</span>
<span id="cb335-6"><span>print</span>(<span>"Rank:"</span>, M.rank(), <span>" Number of vectors:"</span>, M.shape[<span>1</span>])</span></code></pre></div>
<div>
<pre><code>Rank: 1  Number of vectors: 3</code></pre>
</div>
</div>
<p>Rank = 1, number of vectors = 3 → they’re dependent (multiples of each other).</p>
<ol start="3" type="1">
<li>Checking dependence automatically</li>
</ol>
<p>A quick test: if rank &lt; number of vectors → dependent.</p>
<div id="3a3fd570" data-execution_count="196">
<div id="cb337"><pre><code><span id="cb337-1"><span>def</span> check_independence(vectors):</span>
<span id="cb337-2">    M <span>=</span> Matrix.hstack(<span>*</span>vectors)</span>
<span id="cb337-3">    <span>return</span> M.rank() <span>==</span> M.shape[<span>1</span>]</span>
<span id="cb337-4"></span>
<span id="cb337-5"><span>print</span>(<span>"Independent?"</span>, check_independence([Matrix([<span>1</span>,<span>0</span>]), Matrix([<span>0</span>,<span>1</span>])]))</span>
<span id="cb337-6"><span>print</span>(<span>"Independent?"</span>, check_independence([Matrix([<span>1</span>,<span>2</span>]), Matrix([<span>2</span>,<span>4</span>])]))</span></code></pre></div>
<div>
<pre><code>Independent? True
Independent? False</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Solving for dependence relation</li>
</ol>
<p>If vectors are dependent, we can find coefficients <span>\(c_1, c_2, …\)</span> such that</p>
<p><span>\[
c_1 v_1 + c_2 v_2 + … + c_k v_k = 0
\]</span></p>
<p>with some <span>\(c_i \neq 0\)</span>.</p>
<div id="079e501c" data-execution_count="197">
<div id="cb339"><pre><code><span id="cb339-1">M <span>=</span> Matrix.hstack(Matrix([<span>1</span>,<span>2</span>]), Matrix([<span>2</span>,<span>4</span>]))</span>
<span id="cb339-2">null_space <span>=</span> M.nullspace()</span>
<span id="cb339-3"><span>print</span>(<span>"Dependence relation (coefficients):"</span>, null_space)</span></code></pre></div>
<div>
<pre><code>Dependence relation (coefficients): [Matrix([
[-2],
[ 1]])]</code></pre>
</div>
</div>
<p>This shows the exact linear relation.</p>
<ol start="5" type="1">
<li>Random example</li>
</ol>
<div id="e1843605" data-execution_count="198">
<div id="cb341"><pre><code><span id="cb341-1">np.random.seed(<span>0</span>)</span>
<span id="cb341-2">R <span>=</span> Matrix(np.random.randint(<span>-</span><span>3</span>, <span>4</span>, (<span>3</span>,<span>3</span>)))</span>
<span id="cb341-3"><span>print</span>(<span>"Random matrix:</span><span>\n</span><span>"</span>, R)</span>
<span id="cb341-4"><span>print</span>(<span>"Rank:"</span>, R.rank())</span></code></pre></div>
<div>
<pre><code>Random matrix:
 Matrix([[1, 2, -3], [0, 0, 0], [-2, 0, 2]])
Rank: 2</code></pre>
</div>
</div>
<p>Depending on the rank, the columns may be independent (rank = 3) or dependent (rank &lt; 3).</p>
</section>
<section id="try-it-yourself-32">
<h4 data-anchor-id="try-it-yourself-32">Try It Yourself</h4>
<ol type="1">
<li>Test if <span>\([1,1,0], [0,1,1], [1,2,1]\)</span> are independent.</li>
<li>Generate 4 random vectors in <span>\(\mathbb{R}^3\)</span>. Can they ever be independent? Why or why not?</li>
<li>Find the dependence relation for <span>\([2,4], [3,6]\)</span>.</li>
</ol>
</section>
<section id="the-takeaway-16">
<h4 data-anchor-id="the-takeaway-16">The Takeaway</h4>
<ul>
<li>Independent set: no redundancy, each vector adds a new direction.</li>
<li>Dependent set: at least one vector is unnecessary (it lies in the span of others).</li>
<li>Independence is the key to defining basis and dimension.</li>
</ul>
</section>
</section>
<section id="basis-and-coordinates-naming-every-vector-uniquely">
<h3 data-anchor-id="basis-and-coordinates-naming-every-vector-uniquely">35. Basis and Coordinates (Naming Every Vector Uniquely)</h3>
<p>A basis is a set of independent vectors that span a space. It’s like choosing a coordinate system: every vector in the space can be expressed uniquely as a combination of basis vectors. In this lab, we’ll see how to find bases and compute coordinates relative to them.</p>
<section id="set-up-your-lab-34">
<h4 data-anchor-id="set-up-your-lab-34">Set Up Your Lab</h4>
<div id="c56e8fc3" data-execution_count="199"><pre><code><span id="cb343-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb343-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-34">
<h4 data-anchor-id="step-by-step-code-walkthrough-34">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Standard basis in <span>\(\mathbb{R}^3\)</span></li>
</ol>
<div id="46c37a8d" data-execution_count="200"><pre><code><span id="cb344-1">e1 <span>=</span> Matrix([<span>1</span>,<span>0</span>,<span>0</span>])</span>
<span id="cb344-2">e2 <span>=</span> Matrix([<span>0</span>,<span>1</span>,<span>0</span>])</span>
<span id="cb344-3">e3 <span>=</span> Matrix([<span>0</span>,<span>0</span>,<span>1</span>])</span>
<span id="cb344-4"></span>
<span id="cb344-5">M <span>=</span> Matrix.hstack(e1, e2, e3)</span>
<span id="cb344-6"><span>print</span>(<span>"Rank:"</span>, M.rank())</span></code></pre></div>
<p>These three independent vectors form the standard basis of <span>\(\mathbb{R}^3\)</span>. Any vector like <span>\([2,5,-1]\)</span> can be expressed as</p>
<p><span>\[
2e_1 + 5e_2 - 1e_3
\]</span></p>
<ol start="2" type="1">
<li>Finding a basis from dependent vectors</li>
</ol>
<div id="0482e8f6" data-execution_count="201">
<div id="cb346"><pre><code><span id="cb346-1">v1 <span>=</span> Matrix([<span>1</span>,<span>2</span>,<span>3</span>])</span>
<span id="cb346-2">v2 <span>=</span> Matrix([<span>2</span>,<span>4</span>,<span>6</span>])</span>
<span id="cb346-3">v3 <span>=</span> Matrix([<span>1</span>,<span>0</span>,<span>1</span>])</span>
<span id="cb346-4"></span>
<span id="cb346-5">M <span>=</span> Matrix.hstack(v1,v2,v3)</span>
<span id="cb346-6"><span>print</span>(<span>"Column space basis:"</span>, M.columnspace())</span></code></pre></div>
<div>
<pre><code>Column space basis: [Matrix([
[1],
[2],
[3]]), Matrix([
[1],
[0],
[1]])]</code></pre>
</div>
</div>
<p>SymPy extracts independent columns automatically. This gives a basis for the column space.</p>
<ol start="3" type="1">
<li>Coordinates relative to a basis</li>
</ol>
<p>Suppose basis = <span>\(\{ [1,0], [1,1] \}\)</span>. Express vector <span>\([3,5]\)</span> in this basis.</p>
<div id="e49caec7" data-execution_count="202">
<div id="cb348"><pre><code><span id="cb348-1">B <span>=</span> Matrix.hstack(Matrix([<span>1</span>,<span>0</span>]), Matrix([<span>1</span>,<span>1</span>]))</span>
<span id="cb348-2">target <span>=</span> Matrix([<span>3</span>,<span>5</span>])</span>
<span id="cb348-3"></span>
<span id="cb348-4">coords <span>=</span> B.solve_least_squares(target)</span>
<span id="cb348-5"><span>print</span>(<span>"Coordinates in basis B:"</span>, coords)</span></code></pre></div>
<div>
<pre><code>Coordinates in basis B: Matrix([[-2], [5]])</code></pre>
</div>
</div>
<p>So <span>\([3,5] = 3·[1,0] + 2·[1,1]\)</span>.</p>
<ol start="4" type="1">
<li>Basis change</li>
</ol>
<p>If we switch to a different basis, coordinates change but the vector stays the same.</p>
<div id="64af4393" data-execution_count="203">
<div id="cb350"><pre><code><span id="cb350-1">new_basis <span>=</span> Matrix.hstack(Matrix([<span>2</span>,<span>1</span>]), Matrix([<span>1</span>,<span>2</span>]))</span>
<span id="cb350-2">coords_new <span>=</span> new_basis.solve_least_squares(target)</span>
<span id="cb350-3"><span>print</span>(<span>"Coordinates in new basis:"</span>, coords_new)</span></code></pre></div>
<div>
<pre><code>Coordinates in new basis: Matrix([[1/3], [7/3]])</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Random example</li>
</ol>
<p>Generate 3 random vectors in <span>\(\mathbb{R}^3\)</span>. Check if they form a basis.</p>
<div id="bf61379e" data-execution_count="204">
<div id="cb352"><pre><code><span id="cb352-1">np.random.seed(<span>1</span>)</span>
<span id="cb352-2">R <span>=</span> Matrix(np.random.randint(<span>-</span><span>3</span>,<span>4</span>,(<span>3</span>,<span>3</span>)))</span>
<span id="cb352-3"><span>print</span>(<span>"Random matrix:</span><span>\n</span><span>"</span>, R)</span>
<span id="cb352-4"><span>print</span>(<span>"Rank:"</span>, R.rank())</span></code></pre></div>
<div>
<pre><code>Random matrix:
 Matrix([[2, 0, 1], [-3, -2, 0], [2, -3, -3]])
Rank: 3</code></pre>
</div>
</div>
<p>If rank = 3 → basis for <span>\(\mathbb{R}^3\)</span>. Otherwise, only span a subspace.</p>
</section>
<section id="try-it-yourself-33">
<h4 data-anchor-id="try-it-yourself-33">Try It Yourself</h4>
<ol type="1">
<li>Check if <span>\([1,2], [3,4]\)</span> form a basis of <span>\(\mathbb{R}^2\)</span>.</li>
<li>Express vector <span>\([7,5]\)</span> in that basis.</li>
<li>Create 4 random vectors in <span>\(\mathbb{R}^3\)</span>. Find a basis for their span.</li>
</ol>
</section>
<section id="the-takeaway-17">
<h4 data-anchor-id="the-takeaway-17">The Takeaway</h4>
<ul>
<li>A basis = minimal set of vectors that span a space.</li>
<li>Every vector has a unique coordinate representation in a given basis.</li>
<li>Changing bases changes the coordinates, not the vector itself.</li>
</ul>
</section>
</section>
<section id="dimension-how-many-directions">
<h3 data-anchor-id="dimension-how-many-directions">36. Dimension (How Many Directions)</h3>
<p>The dimension of a vector space is the number of independent directions it has. Formally, it’s the number of vectors in any basis of the space. Dimension tells us the “size” of a space in terms of degrees of freedom.</p>
<section id="set-up-your-lab-35">
<h4 data-anchor-id="set-up-your-lab-35">Set Up Your Lab</h4>
<div id="5386066d" data-execution_count="205"><pre><code><span id="cb354-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb354-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-35">
<h4 data-anchor-id="step-by-step-code-walkthrough-35">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Dimension of <span>\(\mathbb{R}^n\)</span></li>
</ol>
<p>The dimension of <span>\(\mathbb{R}^n\)</span> is <span>\(n\)</span>.</p>
<div id="9f615d93" data-execution_count="206">
<div id="cb355"><pre><code><span id="cb355-1">n <span>=</span> <span>4</span></span>
<span id="cb355-2">basis <span>=</span> [Matrix.eye(n)[:,i] <span>for</span> i <span>in</span> <span>range</span>(n)]</span>
<span id="cb355-3"><span>print</span>(<span>"Basis for R^4:"</span>, basis)</span>
<span id="cb355-4"><span>print</span>(<span>"Dimension of R^4:"</span>, <span>len</span>(basis))</span></code></pre></div>
<div>
<pre><code>Basis for R^4: [Matrix([
[1],
[0],
[0],
[0]]), Matrix([
[0],
[1],
[0],
[0]]), Matrix([
[0],
[0],
[1],
[0]]), Matrix([
[0],
[0],
[0],
[1]])]
Dimension of R^4: 4</code></pre>
</div>
</div>
<p>Each standard unit vector adds one independent direction → dimension = 4.</p>
<ol start="2" type="1">
<li>Dimension via rank</li>
</ol>
<p>The rank of a matrix equals the dimension of its column space.</p>
<div id="672075bc" data-execution_count="207">
<div id="cb357"><pre><code><span id="cb357-1">A <span>=</span> Matrix([</span>
<span id="cb357-2">    [<span>1</span>,<span>2</span>,<span>3</span>],</span>
<span id="cb357-3">    [<span>2</span>,<span>4</span>,<span>6</span>],</span>
<span id="cb357-4">    [<span>1</span>,<span>0</span>,<span>1</span>]</span>
<span id="cb357-5">])</span>
<span id="cb357-6"></span>
<span id="cb357-7"><span>print</span>(<span>"Rank (dimension of column space):"</span>, A.rank())</span></code></pre></div>
<div>
<pre><code>Rank (dimension of column space): 2</code></pre>
</div>
</div>
<p>Here, rank = 2 → the column space is a 2D plane inside <span>\(\mathbb{R}^3\)</span>.</p>
<ol start="3" type="1">
<li>Null space dimension</li>
</ol>
<p>The null space dimension is given by:</p>
<p><span>\[
\text{dim(Null(A))} = \#\text{variables} - \text{rank(A)}
\]</span></p>
<div id="1a5c37b6" data-execution_count="208">
<div id="cb359"><pre><code><span id="cb359-1"><span>print</span>(<span>"Null space basis:"</span>, A.nullspace())</span>
<span id="cb359-2"><span>print</span>(<span>"Dimension of null space:"</span>, <span>len</span>(A.nullspace()))</span></code></pre></div>
<div>
<pre><code>Null space basis: [Matrix([
[-1],
[-1],
[ 1]])]
Dimension of null space: 1</code></pre>
</div>
</div>
<p>This is the number of free variables in a solution.</p>
<ol start="4" type="1">
<li>Dimension in practice</li>
</ol>
<ul>
<li>A line through the origin in <span>\(\mathbb{R}^3\)</span> has dimension 1.</li>
<li>A plane through the origin has dimension 2.</li>
<li>The whole <span>\(\mathbb{R}^3\)</span> has dimension 3.</li>
</ul>
<p>Example:</p>
<div id="440ec6ac" data-execution_count="209"><pre><code><span id="cb361-1">v1 <span>=</span> Matrix([<span>1</span>,<span>2</span>,<span>3</span>])</span>
<span id="cb361-2">v2 <span>=</span> Matrix([<span>2</span>,<span>4</span>,<span>6</span>])</span>
<span id="cb361-3">span <span>=</span> Matrix.hstack(v1,v2)</span>
<span id="cb361-4"><span>print</span>(<span>"Dimension of span:"</span>, span.rank())</span></code></pre></div>
<p>Result = 1 → they only generate a line.</p>
<ol start="5" type="1">
<li>Random example</li>
</ol>
<div id="51471bae" data-execution_count="210">
<div id="cb363"><pre><code><span id="cb363-1">np.random.seed(<span>2</span>)</span>
<span id="cb363-2">R <span>=</span> Matrix(np.random.randint(<span>-</span><span>3</span>,<span>4</span>,(<span>4</span>,<span>4</span>)))</span>
<span id="cb363-3"><span>print</span>(<span>"Random 4x4 matrix:</span><span>\n</span><span>"</span>, R)</span>
<span id="cb363-4"><span>print</span>(<span>"Column space dimension:"</span>, R.rank())</span></code></pre></div>
<div>
<pre><code>Random 4x4 matrix:
 Matrix([[-3, 2, -3, 3], [0, -1, 0, -3], [-1, -2, 0, 2], [-1, 1, 1, 1]])
Column space dimension: 4</code></pre>
</div>
</div>
<p>Rank may be 4 (full space) or smaller (collapsed).</p>
</section>
<section id="try-it-yourself-34">
<h4 data-anchor-id="try-it-yourself-34">Try It Yourself</h4>
<ol type="1">
<li><p>Find the dimension of the column space of</p>
<p><span>\[
\begin{bmatrix}  
1 &amp; 1 &amp; 1 \\  
0 &amp; 1 &amp; 1 \\  
0 &amp; 0 &amp; 0  
\end{bmatrix}
\]</span></p></li>
<li><p>Compute the dimension of the null space of a 3×3 singular matrix.</p></li>
<li><p>Generate a 5×3 random matrix and compute its column space dimension.</p></li>
</ol>
</section>
<section id="the-takeaway-18">
<h4 data-anchor-id="the-takeaway-18">The Takeaway</h4>
<ul>
<li>Dimension = number of independent directions.</li>
<li>Found by counting basis vectors (or rank).</li>
<li>Dimensions describe lines (1D), planes (2D), and higher subspaces inside larger spaces.</li>
</ul>
</section>
</section>
<section id="ranknullity-theorem-dimensions-that-add-up">
<h3 data-anchor-id="ranknullity-theorem-dimensions-that-add-up">37. Rank–Nullity Theorem (Dimensions That Add Up)</h3>
<p>The rank–nullity theorem ties together the dimension of the column space and the null space of a matrix. It says:</p>
<p><span>\[
\text{rank}(A) + \text{nullity}(A) = \text{number of columns of } A
\]</span></p>
<p>This is a powerful consistency check in linear algebra.</p>
<section id="set-up-your-lab-36">
<h4 data-anchor-id="set-up-your-lab-36">Set Up Your Lab</h4>
<div id="70c2dd79" data-execution_count="211"><pre><code><span id="cb365-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb365-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-36">
<h4 data-anchor-id="step-by-step-code-walkthrough-36">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Simple 3×3 example</li>
</ol>
<div id="1d4ef096" data-execution_count="212">
<div id="cb366"><pre><code><span id="cb366-1">A <span>=</span> Matrix([</span>
<span id="cb366-2">    [<span>1</span>, <span>2</span>, <span>3</span>],</span>
<span id="cb366-3">    [<span>2</span>, <span>4</span>, <span>6</span>],</span>
<span id="cb366-4">    [<span>1</span>, <span>0</span>, <span>1</span>]</span>
<span id="cb366-5">])</span>
<span id="cb366-6"></span>
<span id="cb366-7">rank <span>=</span> A.rank()</span>
<span id="cb366-8">nullity <span>=</span> <span>len</span>(A.nullspace())</span>
<span id="cb366-9"><span>print</span>(<span>"Rank:"</span>, rank)</span>
<span id="cb366-10"><span>print</span>(<span>"Nullity:"</span>, nullity)</span>
<span id="cb366-11"><span>print</span>(<span>"Rank + Nullity ="</span>, rank <span>+</span> nullity)</span>
<span id="cb366-12"><span>print</span>(<span>"Number of columns ="</span>, A.shape[<span>1</span>])</span></code></pre></div>
<div>
<pre><code>Rank: 2
Nullity: 1
Rank + Nullity = 3
Number of columns = 3</code></pre>
</div>
</div>
<p>You should see that rank + nullity = 3, the number of columns.</p>
<ol start="2" type="1">
<li>Full-rank case</li>
</ol>
<div id="54491b51" data-execution_count="213"><pre><code><span id="cb368-1">B <span>=</span> Matrix([</span>
<span id="cb368-2">    [<span>1</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb368-3">    [<span>0</span>,<span>1</span>,<span>0</span>],</span>
<span id="cb368-4">    [<span>0</span>,<span>0</span>,<span>1</span>]</span>
<span id="cb368-5">])</span>
<span id="cb368-6"></span>
<span id="cb368-7"><span>print</span>(<span>"Rank:"</span>, B.rank())</span>
<span id="cb368-8"><span>print</span>(<span>"Nullity:"</span>, <span>len</span>(B.nullspace()))</span></code></pre></div>
<ul>
<li>Rank = 3 (all independent).</li>
<li>Nullity = 0 (only zero solution to <span>\(Bx=0\)</span>).</li>
<li>Rank + Nullity = 3 columns.</li>
</ul>
<ol start="3" type="1">
<li>Wide matrix (more columns than rows)</li>
</ol>
<div id="04f71e46" data-execution_count="214">
<div id="cb370"><pre><code><span id="cb370-1">C <span>=</span> Matrix([</span>
<span id="cb370-2">    [<span>1</span>,<span>2</span>,<span>3</span>,<span>4</span>],</span>
<span id="cb370-3">    [<span>0</span>,<span>1</span>,<span>1</span>,<span>2</span>],</span>
<span id="cb370-4">    [<span>0</span>,<span>0</span>,<span>0</span>,<span>0</span>]</span>
<span id="cb370-5">])</span>
<span id="cb370-6"></span>
<span id="cb370-7">rank <span>=</span> C.rank()</span>
<span id="cb370-8">nullity <span>=</span> <span>len</span>(C.nullspace())</span>
<span id="cb370-9"><span>print</span>(<span>"Rank:"</span>, rank, <span>" Nullity:"</span>, nullity, <span>" Columns:"</span>, C.shape[<span>1</span>])</span></code></pre></div>
<div>
<pre><code>Rank: 2  Nullity: 2  Columns: 4</code></pre>
</div>
</div>
<p>Here, nullity &gt; 0 because there are more variables than independent equations.</p>
<ol start="4" type="1">
<li>Verifying with random matrices</li>
</ol>
<div id="72e5f1e3" data-execution_count="215">
<div id="cb372"><pre><code><span id="cb372-1">np.random.seed(<span>3</span>)</span>
<span id="cb372-2">R <span>=</span> Matrix(np.random.randint(<span>-</span><span>3</span>,<span>4</span>,(<span>4</span>,<span>5</span>)))</span>
<span id="cb372-3"><span>print</span>(<span>"Random 4x5 matrix:</span><span>\n</span><span>"</span>, R)</span>
<span id="cb372-4"><span>print</span>(<span>"Rank + Nullity ="</span>, R.rank() <span>+</span> <span>len</span>(R.nullspace()))</span>
<span id="cb372-5"><span>print</span>(<span>"Number of columns ="</span>, R.shape[<span>1</span>])</span></code></pre></div>
<div>
<pre><code>Random 4x5 matrix:
 Matrix([[-1, -3, -2, 0, -3], [-3, -3, 2, 2, 0], [-1, 0, -2, -2, -1], [2, 3, -3, 1, 1]])
Rank + Nullity = 5
Number of columns = 5</code></pre>
</div>
</div>
<p>Always consistent: rank + nullity = number of columns.</p>
<ol start="5" type="1">
<li>Geometric interpretation</li>
</ol>
<p>For an <span>\(m \times n\)</span> matrix:</p>
<ul>
<li>Rank(A) = dimension of outputs (column space).</li>
<li>Nullity(A) = dimension of hidden directions that collapse to 0.</li>
<li>Together, they use up all the “input dimensions” (n).</li>
</ul>
</section>
<section id="try-it-yourself-35">
<h4 data-anchor-id="try-it-yourself-35">Try It Yourself</h4>
<ol type="1">
<li><p>Compute rank and nullity of</p>
<p><span>\[
\begin{bmatrix}  
1 &amp; 1 &amp; 1 \\  
0 &amp; 1 &amp; 1  
\end{bmatrix}
\]</span></p>
<p>Check the theorem.</p></li>
<li><p>Create a 2×4 random integer matrix. Confirm that rank + nullity = 4.</p></li>
<li><p>Explain why a tall full-rank <span>\(5 \times 3\)</span> matrix must have nullity = 0.</p></li>
</ol>
</section>
<section id="the-takeaway-19">
<h4 data-anchor-id="the-takeaway-19">The Takeaway</h4>
<ul>
<li>Rank + Nullity = number of columns (always true).</li>
<li>Rank measures independent outputs; nullity measures hidden freedom.</li>
<li>This theorem connects solutions of <span>\(Ax=0\)</span> with the structure of <span>\(A\)</span>.</li>
</ul>
</section>
</section>
<section id="coordinates-relative-to-a-basis-changing-the-ruler">
<h3 data-anchor-id="coordinates-relative-to-a-basis-changing-the-ruler">38. Coordinates Relative to a Basis (Changing the “Ruler”)</h3>
<p>Once we choose a basis, every vector can be described with coordinates relative to that basis. This is like changing the “ruler” we use to measure vectors. In this lab, we’ll practice computing coordinates in different bases.</p>
<section id="set-up-your-lab-37">
<h4 data-anchor-id="set-up-your-lab-37">Set Up Your Lab</h4>
<div id="6e51ab7b" data-execution_count="216"><pre><code><span id="cb374-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb374-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-37">
<h4 data-anchor-id="step-by-step-code-walkthrough-37">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Standard basis coordinates</li>
</ol>
<p>Vector <span>\(v = [4,5]\)</span> in <span>\(\mathbb{R}^2\)</span>:</p>
<div id="1e01d5bd" data-execution_count="217">
<div id="cb375"><pre><code><span id="cb375-1">v <span>=</span> Matrix([<span>4</span>,<span>5</span>])</span>
<span id="cb375-2">e1 <span>=</span> Matrix([<span>1</span>,<span>0</span>])</span>
<span id="cb375-3">e2 <span>=</span> Matrix([<span>0</span>,<span>1</span>])</span>
<span id="cb375-4"></span>
<span id="cb375-5">B <span>=</span> Matrix.hstack(e1,e2)</span>
<span id="cb375-6">coords <span>=</span> B.solve_least_squares(v)</span>
<span id="cb375-7"><span>print</span>(<span>"Coordinates in standard basis:"</span>, coords)</span></code></pre></div>
<div>
<pre><code>Coordinates in standard basis: Matrix([[4], [5]])</code></pre>
</div>
</div>
<p>Result is just <span>\([4,5]\)</span>. Easy - the standard basis matches the components directly.</p>
<ol start="2" type="1">
<li>Non-standard basis</li>
</ol>
<p>Suppose basis = <span>\(\{ [1,1], [1,-1] \}\)</span>. Express <span>\(v = [4,5]\)</span> in this basis.</p>
<div id="81a03896" data-execution_count="218">
<div id="cb377"><pre><code><span id="cb377-1">B2 <span>=</span> Matrix.hstack(Matrix([<span>1</span>,<span>1</span>]), Matrix([<span>1</span>,<span>-</span><span>1</span>]))</span>
<span id="cb377-2">coords2 <span>=</span> B2.solve_least_squares(v)</span>
<span id="cb377-3"><span>print</span>(<span>"Coordinates in new basis:"</span>, coords2)</span></code></pre></div>
<div>
<pre><code>Coordinates in new basis: Matrix([[9/2], [-1/2]])</code></pre>
</div>
</div>
<p>Now <span>\(v\)</span> has different coordinates.</p>
<ol start="3" type="1">
<li>Changing coordinates back</li>
</ol>
<p>To reconstruct the vector from coordinates:</p>
<div id="e7125077" data-execution_count="219">
<div id="cb379"><pre><code><span id="cb379-1">reconstructed <span>=</span> B2 <span>*</span> coords2</span>
<span id="cb379-2"><span>print</span>(<span>"Reconstructed vector:"</span>, reconstructed)</span></code></pre></div>
<div>
<pre><code>Reconstructed vector: Matrix([[4], [5]])</code></pre>
</div>
</div>
<p>It matches the original <span>\([4,5]\)</span>.</p>
<ol start="4" type="1">
<li>Random basis in <span>\(\mathbb{R}^3\)</span></li>
</ol>
<div id="700e082d" data-execution_count="220">
<div id="cb381"><pre><code><span id="cb381-1">basis <span>=</span> Matrix.hstack(</span>
<span id="cb381-2">    Matrix([<span>1</span>,<span>0</span>,<span>1</span>]),</span>
<span id="cb381-3">    Matrix([<span>0</span>,<span>1</span>,<span>1</span>]),</span>
<span id="cb381-4">    Matrix([<span>1</span>,<span>1</span>,<span>0</span>])</span>
<span id="cb381-5">)</span>
<span id="cb381-6">v <span>=</span> Matrix([<span>2</span>,<span>3</span>,<span>4</span>])</span>
<span id="cb381-7"></span>
<span id="cb381-8">coords <span>=</span> basis.solve_least_squares(v)</span>
<span id="cb381-9"><span>print</span>(<span>"Coordinates of v in random basis:"</span>, coords)</span></code></pre></div>
<div>
<pre><code>Coordinates of v in random basis: Matrix([[3/2], [5/2], [1/2]])</code></pre>
</div>
</div>
<p>Any independent set of 3 vectors in <span>\(\mathbb{R}^3\)</span> works as a basis.</p>
<ol start="5" type="1">
<li>Visualization in 2D</li>
</ol>
<p>Let’s compare coordinates in two bases.</p>
<div id="a322ae91" data-execution_count="221">
<div id="cb383"><pre><code><span id="cb383-1"><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb383-2"></span>
<span id="cb383-3">v <span>=</span> np.array([<span>4</span>,<span>5</span>])</span>
<span id="cb383-4">b1 <span>=</span> np.array([<span>1</span>,<span>1</span>])</span>
<span id="cb383-5">b2 <span>=</span> np.array([<span>1</span>,<span>-</span><span>1</span>])</span>
<span id="cb383-6"></span>
<span id="cb383-7">plt.quiver(<span>0</span>,<span>0</span>,v[<span>0</span>],v[<span>1</span>],angles<span>=</span><span>'xy'</span>,scale_units<span>=</span><span>'xy'</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>'blue'</span>,label<span>=</span><span>'v'</span>)</span>
<span id="cb383-8">plt.quiver(<span>0</span>,<span>0</span>,b1[<span>0</span>],b1[<span>1</span>],angles<span>=</span><span>'xy'</span>,scale_units<span>=</span><span>'xy'</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>'red'</span>,label<span>=</span><span>'basis1'</span>)</span>
<span id="cb383-9">plt.quiver(<span>0</span>,<span>0</span>,b2[<span>0</span>],b2[<span>1</span>],angles<span>=</span><span>'xy'</span>,scale_units<span>=</span><span>'xy'</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>'green'</span>,label<span>=</span><span>'basis2'</span>)</span>
<span id="cb383-10"></span>
<span id="cb383-11">plt.xlim(<span>-</span><span>1</span>,<span>6</span>)</span>
<span id="cb383-12">plt.ylim(<span>-</span><span>6</span>,<span>6</span>)</span>
<span id="cb383-13">plt.legend()</span>
<span id="cb383-14">plt.grid()</span>
<span id="cb383-15">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-222-output-1.png" width="573" height="416"></p>
</figure>
</div>
</div>
<p>Even though the basis vectors look different, they span the same space, and <span>\(v\)</span> can be expressed in terms of them.</p>
</section>
<section id="try-it-yourself-36">
<h4 data-anchor-id="try-it-yourself-36">Try It Yourself</h4>
<ol type="1">
<li>Express <span>\([7,3]\)</span> in the basis <span>\(\{[2,0], [0,3]\}\)</span>.</li>
<li>Pick three independent random vectors in <span>\(\mathbb{R}^3\)</span>. Write down the coordinates of <span>\([1,2,3]\)</span> in that basis.</li>
<li>Verify that reconstructing always gives the original vector.</li>
</ol>
</section>
<section id="the-takeaway-20">
<h4 data-anchor-id="the-takeaway-20">The Takeaway</h4>
<ul>
<li>A basis provides a coordinate system for vectors.</li>
<li>Coordinates depend on the basis, but the underlying vector doesn’t change.</li>
<li>Changing the basis is like changing the “ruler” you measure vectors with.</li>
</ul>
</section>
</section>
<section id="change-of-basis-matrices-moving-between-coordinate-systems">
<h3 data-anchor-id="change-of-basis-matrices-moving-between-coordinate-systems">39. Change-of-Basis Matrices (Moving Between Coordinate Systems)</h3>
<p>When we switch from one basis to another, we need a change-of-basis matrix. This matrix acts like a translator: it converts coordinates in one system to coordinates in another.</p>
<section id="set-up-your-lab-38">
<h4 data-anchor-id="set-up-your-lab-38">Set Up Your Lab</h4>
<div id="05b0cd38" data-execution_count="222"><pre><code><span id="cb384-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb384-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-38">
<h4 data-anchor-id="step-by-step-code-walkthrough-38">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Two bases in <span>\(\mathbb{R}^2\)</span></li>
</ol>
<p>Let’s define:</p>
<ul>
<li>Basis <span>\(B = \{ [1,0], [0,1] \}\)</span> (standard basis).</li>
<li>Basis <span>\(C = \{ [1,1], [1,-1] \}\)</span>.</li>
</ul>
<div id="16054eed" data-execution_count="223"><pre><code><span id="cb385-1">B <span>=</span> Matrix.hstack(Matrix([<span>1</span>,<span>0</span>]), Matrix([<span>0</span>,<span>1</span>]))</span>
<span id="cb385-2">C <span>=</span> Matrix.hstack(Matrix([<span>1</span>,<span>1</span>]), Matrix([<span>1</span>,<span>-</span><span>1</span>]))</span></code></pre></div>
<ol start="2" type="1">
<li>Change-of-basis matrix</li>
</ol>
<p>The matrix that converts C-coordinates → standard coordinates is just <span>\(C\)</span>.</p>
<div id="eff0e703" data-execution_count="224">
<div id="cb386"><pre><code><span id="cb386-1"><span>print</span>(<span>"C (basis matrix):</span><span>\n</span><span>"</span>, C)</span></code></pre></div>
<div>
<pre><code>C (basis matrix):
 Matrix([[1, 1], [1, -1]])</code></pre>
</div>
</div>
<p>To go the other way (standard → C), we compute the inverse of <span>\(C\)</span>.</p>
<div id="dc2a2a7b" data-execution_count="225">
<div id="cb388"><pre><code><span id="cb388-1">C_inv <span>=</span> C.inv()</span>
<span id="cb388-2"><span>print</span>(<span>"C inverse:</span><span>\n</span><span>"</span>, C_inv)</span></code></pre></div>
<div>
<pre><code>C inverse:
 Matrix([[1/2, 1/2], [1/2, -1/2]])</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Converting coordinates</li>
</ol>
<p>Vector <span>\(v = [4,5]\)</span>.</p>
<ul>
<li>In standard basis:</li>
</ul>
<div id="6816b3d1" data-execution_count="226">
<div id="cb390"><pre><code><span id="cb390-1">v <span>=</span> Matrix([<span>4</span>,<span>5</span>])</span>
<span id="cb390-2">coords_in_standard <span>=</span> v</span>
<span id="cb390-3"><span>print</span>(<span>"Coordinates in standard basis:"</span>, coords_in_standard)</span></code></pre></div>
<div>
<pre><code>Coordinates in standard basis: Matrix([[4], [5]])</code></pre>
</div>
</div>
<ul>
<li>In basis <span>\(C\)</span>:</li>
</ul>
<div id="1af938ce" data-execution_count="227">
<div id="cb392"><pre><code><span id="cb392-1">coords_in_C <span>=</span> C_inv <span>*</span> v</span>
<span id="cb392-2"><span>print</span>(<span>"Coordinates in C basis:"</span>, coords_in_C)</span></code></pre></div>
<div>
<pre><code>Coordinates in C basis: Matrix([[9/2], [-1/2]])</code></pre>
</div>
</div>
<ul>
<li>Convert back:</li>
</ul>
<div id="d65bdc32" data-execution_count="228">
<div id="cb394"><pre><code><span id="cb394-1">reconstructed <span>=</span> C <span>*</span> coords_in_C</span>
<span id="cb394-2"><span>print</span>(<span>"Reconstructed vector:"</span>, reconstructed)</span></code></pre></div>
<div>
<pre><code>Reconstructed vector: Matrix([[4], [5]])</code></pre>
</div>
</div>
<p>The reconstruction matches the original vector.</p>
<ol start="4" type="1">
<li>General formula</li>
</ol>
<p>If <span>\(P\)</span> is the change-of-basis matrix from basis <span>\(B\)</span> to basis <span>\(C\)</span>:</p>
<p><span>\[
[v]_C = P^{-1}[v]_B
\]</span></p>
<p><span>\[
[v]_B = P[v]_C
\]</span></p>
<p>Here, <span>\(P\)</span> is the matrix of new basis vectors written in terms of the old basis.</p>
<ol start="5" type="1">
<li>Random 3D example</li>
</ol>
<div id="176726ff" data-execution_count="229">
<div id="cb396"><pre><code><span id="cb396-1">B <span>=</span> Matrix.eye(<span>3</span>)  <span># standard basis</span></span>
<span id="cb396-2">C <span>=</span> Matrix.hstack(</span>
<span id="cb396-3">    Matrix([<span>1</span>,<span>0</span>,<span>1</span>]),</span>
<span id="cb396-4">    Matrix([<span>0</span>,<span>1</span>,<span>1</span>]),</span>
<span id="cb396-5">    Matrix([<span>1</span>,<span>1</span>,<span>0</span>])</span>
<span id="cb396-6">)</span>
<span id="cb396-7"></span>
<span id="cb396-8">v <span>=</span> Matrix([<span>2</span>,<span>3</span>,<span>4</span>])</span>
<span id="cb396-9"></span>
<span id="cb396-10">C_inv <span>=</span> C.inv()</span>
<span id="cb396-11">coords_in_C <span>=</span> C_inv <span>*</span> v</span>
<span id="cb396-12"><span>print</span>(<span>"Coordinates in new basis C:"</span>, coords_in_C)</span>
<span id="cb396-13"></span>
<span id="cb396-14"><span>print</span>(<span>"Back to standard:"</span>, C <span>*</span> coords_in_C)</span></code></pre></div>
<div>
<pre><code>Coordinates in new basis C: Matrix([[3/2], [5/2], [1/2]])
Back to standard: Matrix([[2], [3], [4]])</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-37">
<h4 data-anchor-id="try-it-yourself-37">Try It Yourself</h4>
<ol type="1">
<li>Convert <span>\([7,3]\)</span> from the standard basis to the basis <span>\(\{[2,0], [0,3]\}\)</span>.</li>
<li>Pick a random invertible 3×3 matrix as a basis. Write a vector in that basis, then convert it back to the standard basis.</li>
<li>Prove that converting back and forth always returns the same vector.</li>
</ol>
</section>
<section id="the-takeaway-21">
<h4 data-anchor-id="the-takeaway-21">The Takeaway</h4>
<ul>
<li>A change-of-basis matrix converts coordinates between bases.</li>
<li>Going from new basis → old basis uses the basis matrix.</li>
<li>Going from old basis → new basis requires its inverse.</li>
<li>The vector itself never changes - only the description of it does.</li>
</ul>
</section>
</section>
<section id="affine-subspaces-lines-and-planes-not-through-the-origin">
<h3 data-anchor-id="affine-subspaces-lines-and-planes-not-through-the-origin">40. Affine Subspaces (Lines and Planes Not Through the Origin)</h3>
<p>So far, subspaces always passed through the origin. But many familiar objects - like lines offset from the origin or planes floating in space - are affine subspaces. They look like subspaces, just shifted away from zero.</p>
<section id="set-up-your-lab-39">
<h4 data-anchor-id="set-up-your-lab-39">Set Up Your Lab</h4>
<div id="3eb7f41c" data-execution_count="230"><pre><code><span id="cb398-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb398-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb398-3"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-39">
<h4 data-anchor-id="step-by-step-code-walkthrough-39">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Line through the origin (a subspace)</li>
</ol>
<p><span>\[
L = \{ t \cdot [1,2] : t \in \mathbb{R} \}
\]</span></p>
<div id="636712e9" data-execution_count="231">
<div id="cb399"><pre><code><span id="cb399-1">t <span>=</span> np.linspace(<span>-</span><span>3</span>,<span>3</span>,<span>20</span>)</span>
<span id="cb399-2">line_origin <span>=</span> np.array([t, <span>2</span><span>*</span>t]).T</span>
<span id="cb399-3">plt.plot(line_origin[:,<span>0</span>], line_origin[:,<span>1</span>], label<span>=</span><span>"Through origin"</span>)</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-232-output-1.png" width="569" height="411"></p>
</figure>
</div>
</div>
<ol start="2" type="1">
<li>Line not through the origin (affine subspace)</li>
</ol>
<p><span>\[
L' = \{ [3,1] + t \cdot [1,2] : t \in \mathbb{R} \}
\]</span></p>
<div id="2b78c455" data-execution_count="232">
<div id="cb400"><pre><code><span id="cb400-1">point <span>=</span> np.array([<span>3</span>,<span>1</span>])</span>
<span id="cb400-2">direction <span>=</span> np.array([<span>1</span>,<span>2</span>])</span>
<span id="cb400-3">line_shifted <span>=</span> np.array([point <span>+</span> k<span>*</span>direction <span>for</span> k <span>in</span> t])</span>
<span id="cb400-4">plt.plot(line_shifted[:,<span>0</span>], line_shifted[:,<span>1</span>], label<span>=</span><span>"Shifted line"</span>)</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-233-output-1.png" width="569" height="411"></p>
</figure>
</div>
</div>
<ol start="3" type="1">
<li>Visualizing together</li>
</ol>
<div id="8c58022f" data-execution_count="233">
<div id="cb401"><pre><code><span id="cb401-1">plt.scatter(<span>*</span>point, color<span>=</span><span>"red"</span>, label<span>=</span><span>"Shift point"</span>)</span>
<span id="cb401-2">plt.axhline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb401-3">plt.axvline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb401-4">plt.legend()</span>
<span id="cb401-5">plt.grid()</span>
<span id="cb401-6">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-234-output-1.png" width="571" height="411"></p>
</figure>
</div>
</div>
<p>One line passes through the origin, the other is parallel but shifted.</p>
<ol start="4" type="1">
<li>Plane example</li>
</ol>
<p>A plane in <span>\(\mathbb{R}^3\)</span>:</p>
<p><span>\[
P = \{ [1,2,3] + s[1,0,0] + t[0,1,0] : s,t \in \mathbb{R} \}
\]</span></p>
<p>This is an affine plane parallel to the <span>\(xy\)</span>-plane, but shifted.</p>
<div id="9f66eb93" data-execution_count="234">
<div id="cb402"><pre><code><span id="cb402-1">s_vals <span>=</span> np.linspace(<span>-</span><span>2</span>,<span>2</span>,<span>10</span>)</span>
<span id="cb402-2">t_vals <span>=</span> np.linspace(<span>-</span><span>2</span>,<span>2</span>,<span>10</span>)</span>
<span id="cb402-3"></span>
<span id="cb402-4">points <span>=</span> []</span>
<span id="cb402-5"><span>for</span> s <span>in</span> s_vals:</span>
<span id="cb402-6">    <span>for</span> t <span>in</span> t_vals:</span>
<span id="cb402-7">        points.append([<span>1</span>,<span>2</span>,<span>3</span>] <span>+</span> s<span>*</span>np.array([<span>1</span>,<span>0</span>,<span>0</span>]) <span>+</span> t<span>*</span>np.array([<span>0</span>,<span>1</span>,<span>0</span>]))</span>
<span id="cb402-8"></span>
<span id="cb402-9">points <span>=</span> np.array(points)</span>
<span id="cb402-10"></span>
<span id="cb402-11"><span>from</span> mpl_toolkits.mplot3d <span>import</span> Axes3D</span>
<span id="cb402-12">fig <span>=</span> plt.figure()</span>
<span id="cb402-13">ax <span>=</span> fig.add_subplot(<span>111</span>, projection<span>=</span><span>'3d'</span>)</span>
<span id="cb402-14">ax.scatter(points[:,<span>0</span>], points[:,<span>1</span>], points[:,<span>2</span>])</span>
<span id="cb402-15">ax.set_title(<span>"Affine plane in R^3"</span>)</span>
<span id="cb402-16">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-235-output-1.png" width="412" height="416"></p>
</figure>
</div>
</div>
<ol start="5" type="1">
<li>Algebraic difference</li>
</ol>
<ul>
<li>A subspace must satisfy closure under addition and scalar multiplication, and must include 0.</li>
<li>An affine subspace is just a subspace plus a fixed shift vector.</li>
</ul>
</section>
<section id="try-it-yourself-38">
<h4 data-anchor-id="try-it-yourself-38">Try It Yourself</h4>
<ol type="1">
<li><p>Define a line in <span>\(\mathbb{R}^2\)</span>:</p>
<p><span>\[
(x,y) = (2,3) + t(1,-1)
\]</span></p>
<p>Plot it and compare with the subspace spanned by <span>\((1,-1)\)</span>.</p></li>
<li><p>Construct an affine plane in <span>\(\mathbb{R}^3\)</span> shifted by vector <span>\((5,5,5)\)</span>.</p></li>
<li><p>Show algebraically that subtracting the shift point turns an affine subspace back into a regular subspace.</p></li>
</ol>
</section>
<section id="the-takeaway-22">
<h4 data-anchor-id="the-takeaway-22">The Takeaway</h4>
<ul>
<li>Subspaces go through the origin.</li>
<li>Affine subspaces are shifted copies of subspaces.</li>
<li>They’re essential in geometry, computer graphics, and optimization (e.g., feasible regions in linear programming).</li>
</ul>
</section>
</section>
</section>
<section id="chapter-5.-linear-transformation-and-structure">
<h2 data-anchor-id="chapter-5.-linear-transformation-and-structure">Chapter 5. Linear Transformation and Structure</h2>
<section id="linear-transformations-preserving-lines-and-sums">
<h3 data-anchor-id="linear-transformations-preserving-lines-and-sums">41. Linear Transformations (Preserving Lines and Sums)</h3>
<p>A linear transformation is a function between vector spaces that preserves two key properties:</p>
<ol type="1">
<li>Additivity: <span>\(T(u+v) = T(u) + T(v)\)</span></li>
<li>Homogeneity: <span>\(T(cu) = cT(u)\)</span></li>
</ol>
<p>In practice, every linear transformation can be represented by a matrix. This lab will help you understand and experiment with linear transformations in Python.</p>
<section id="set-up-your-lab-40">
<h4 data-anchor-id="set-up-your-lab-40">Set Up Your Lab</h4>
<div id="aba519e8" data-execution_count="235"><pre><code><span id="cb403-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb403-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-40">
<h4 data-anchor-id="step-by-step-code-walkthrough-40">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Simple linear transformation (scaling)</li>
</ol>
<p>Let’s scale vectors by 2 in the x-direction and by 0.5 in the y-direction.</p>
<div id="09875e6b" data-execution_count="236">
<div id="cb404"><pre><code><span id="cb404-1">A <span>=</span> np.array([</span>
<span id="cb404-2">    [<span>2</span>, <span>0</span>],</span>
<span id="cb404-3">    [<span>0</span>, <span>0.5</span>]</span>
<span id="cb404-4">])</span>
<span id="cb404-5"></span>
<span id="cb404-6">v <span>=</span> np.array([<span>1</span>, <span>2</span>])</span>
<span id="cb404-7">Tv <span>=</span> A <span>@</span> v</span>
<span id="cb404-8"><span>print</span>(<span>"Original v:"</span>, v)</span>
<span id="cb404-9"><span>print</span>(<span>"Transformed Tv:"</span>, Tv)</span></code></pre></div>
<div>
<pre><code>Original v: [1 2]
Transformed Tv: [2. 1.]</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Visualizing multiple vectors</li>
</ol>
<div id="4debd414" data-execution_count="237">
<div id="cb406"><pre><code><span id="cb406-1">vectors <span>=</span> [np.array([<span>1</span>,<span>1</span>]), np.array([<span>2</span>,<span>0</span>]), np.array([<span>-</span><span>1</span>,<span>2</span>])]</span>
<span id="cb406-2"></span>
<span id="cb406-3"><span>for</span> v <span>in</span> vectors:</span>
<span id="cb406-4">    Tv <span>=</span> A <span>@</span> v</span>
<span id="cb406-5">    plt.arrow(<span>0</span>,<span>0</span>,v[<span>0</span>],v[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>'blue'</span>,length_includes_head<span>=</span><span>True</span>)</span>
<span id="cb406-6">    plt.arrow(<span>0</span>,<span>0</span>,Tv[<span>0</span>],Tv[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>'red'</span>,length_includes_head<span>=</span><span>True</span>)</span>
<span id="cb406-7"></span>
<span id="cb406-8">plt.axhline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb406-9">plt.axvline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb406-10">plt.xlim(<span>-</span><span>3</span>,<span>5</span>)</span>
<span id="cb406-11">plt.ylim(<span>-</span><span>1</span>,<span>5</span>)</span>
<span id="cb406-12">plt.grid()</span>
<span id="cb406-13">plt.title(<span>"Blue = original, Red = transformed"</span>)</span>
<span id="cb406-14">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-238-output-1.png" width="573" height="431"></p>
</figure>
</div>
</div>
<p>Blue arrows are the original vectors; red arrows are the transformed ones. Notice how the transformation stretches and compresses consistently.</p>
<ol start="3" type="1">
<li>Rotation as a linear transformation</li>
</ol>
<p>Rotating vectors by <span>\(\theta = 90^\circ\)</span>:</p>
<div id="36cd5c80" data-execution_count="238">
<div id="cb407"><pre><code><span id="cb407-1">theta <span>=</span> np.pi<span>/</span><span>2</span></span>
<span id="cb407-2">R <span>=</span> np.array([</span>
<span id="cb407-3">    [np.cos(theta), <span>-</span>np.sin(theta)],</span>
<span id="cb407-4">    [np.sin(theta),  np.cos(theta)]</span>
<span id="cb407-5">])</span>
<span id="cb407-6"></span>
<span id="cb407-7">v <span>=</span> np.array([<span>1</span>,<span>0</span>])</span>
<span id="cb407-8"><span>print</span>(<span>"Rotate [1,0] by 90°:"</span>, R <span>@</span> v)</span></code></pre></div>
<div>
<pre><code>Rotate [1,0] by 90°: [6.123234e-17 1.000000e+00]</code></pre>
</div>
</div>
<p>The result is <span>\([0,1]\)</span>, a perfect rotation.</p>
<ol start="4" type="1">
<li>Checking linearity</li>
</ol>
<div id="dd7e8f74" data-execution_count="239">
<div id="cb409"><pre><code><span id="cb409-1">u <span>=</span> np.array([<span>1</span>,<span>2</span>])</span>
<span id="cb409-2">v <span>=</span> np.array([<span>3</span>,<span>4</span>])</span>
<span id="cb409-3">c <span>=</span> <span>5</span></span>
<span id="cb409-4"></span>
<span id="cb409-5">lhs <span>=</span> A <span>@</span> (u<span>+</span>v)</span>
<span id="cb409-6">rhs <span>=</span> A<span>@</span>u <span>+</span> A<span>@</span>v</span>
<span id="cb409-7"><span>print</span>(<span>"Additivity holds?"</span>, np.allclose(lhs,rhs))</span>
<span id="cb409-8"></span>
<span id="cb409-9">lhs <span>=</span> A <span>@</span> (c<span>*</span>u)</span>
<span id="cb409-10">rhs <span>=</span> c<span>*</span>(A<span>@</span>u)</span>
<span id="cb409-11"><span>print</span>(<span>"Homogeneity holds?"</span>, np.allclose(lhs,rhs))</span></code></pre></div>
<div>
<pre><code>Additivity holds? True
Homogeneity holds? True</code></pre>
</div>
</div>
<p>Both checks return <code>True</code>, proving <span>\(T\)</span> is linear.</p>
<ol start="5" type="1">
<li>Non-linear example (for contrast)</li>
</ol>
<p>A transformation like <span>\(T(x,y) = (x^2, y)\)</span> is not linear.</p>
<div id="544a0ed6" data-execution_count="240">
<div id="cb411"><pre><code><span id="cb411-1"><span>def</span> nonlinear(v):</span>
<span id="cb411-2">    <span>return</span> np.array([v[<span>0</span>]<span>**</span><span>2</span>, v[<span>1</span>]])</span>
<span id="cb411-3"></span>
<span id="cb411-4"><span>print</span>(<span>"T([2,3]) ="</span>, nonlinear(np.array([<span>2</span>,<span>3</span>])))</span>
<span id="cb411-5"><span>print</span>(<span>"Check additivity:"</span>, nonlinear(np.array([<span>1</span>,<span>2</span>])<span>+</span>np.array([<span>3</span>,<span>4</span>])) <span>==</span> (nonlinear([<span>1</span>,<span>2</span>])<span>+</span>nonlinear([<span>3</span>,<span>4</span>])))</span></code></pre></div>
<div>
<pre><code>T([2,3]) = [4 3]
Check additivity: [False  True]</code></pre>
</div>
</div>
<p>This fails the additivity test, so it’s not linear.</p>
</section>
<section id="try-it-yourself-39">
<h4 data-anchor-id="try-it-yourself-39">Try It Yourself</h4>
<ol type="1">
<li><p>Define a shear matrix</p>
<p><span>\[
S = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix}
\]</span></p>
<p>Apply it to vectors and plot before/after.</p></li>
<li><p>Verify linearity for rotation by 45°.</p></li>
<li><p>Test whether <span>\(T(x,y) = (x+y, y)\)</span> is linear.</p></li>
</ol>
</section>
<section id="the-takeaway-23">
<h4 data-anchor-id="the-takeaway-23">The Takeaway</h4>
<ul>
<li>A linear transformation preserves vector addition and scalar multiplication.</li>
<li>Every linear transformation can be represented by a matrix.</li>
<li>Visualizing with arrows helps build geometric intuition: stretching, rotating, and shearing are all linear.</li>
</ul>
</section>
</section>
<section id="matrix-representation-of-a-linear-map-choosing-a-basis">
<h3 data-anchor-id="matrix-representation-of-a-linear-map-choosing-a-basis">42. Matrix Representation of a Linear Map (Choosing a Basis)</h3>
<p>Every linear transformation can be written as a matrix, but the exact matrix depends on the basis you choose. This lab shows how to build and interpret matrix representations.</p>
<section id="set-up-your-lab-41">
<h4 data-anchor-id="set-up-your-lab-41">Set Up Your Lab</h4>
<div id="e8929758" data-execution_count="241"><pre><code><span id="cb413-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb413-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-41">
<h4 data-anchor-id="step-by-step-code-walkthrough-41">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>From transformation to matrix</li>
</ol>
<p>Suppose <span>\(T: \mathbb{R}^2 \to \mathbb{R}^2\)</span> is defined by:</p>
<p><span>\[
T(x,y) = (2x + y, \; x - y)
\]</span></p>
<p>To find its matrix in the standard basis, apply <span>\(T\)</span> to each basis vector:</p>
<div id="eefd2425" data-execution_count="242">
<div id="cb414"><pre><code><span id="cb414-1">e1 <span>=</span> Matrix([<span>1</span>,<span>0</span>])</span>
<span id="cb414-2">e2 <span>=</span> Matrix([<span>0</span>,<span>1</span>])</span>
<span id="cb414-3"></span>
<span id="cb414-4"><span>def</span> T(v):</span>
<span id="cb414-5">    x, y <span>=</span> v</span>
<span id="cb414-6">    <span>return</span> Matrix([<span>2</span><span>*</span>x <span>+</span> y, x <span>-</span> y])</span>
<span id="cb414-7"></span>
<span id="cb414-8"><span>print</span>(<span>"T(e1):"</span>, T(e1))</span>
<span id="cb414-9"><span>print</span>(<span>"T(e2):"</span>, T(e2))</span></code></pre></div>
<div>
<pre><code>T(e1): Matrix([[2], [1]])
T(e2): Matrix([[1], [-1]])</code></pre>
</div>
</div>
<p>Stacking results as columns gives the matrix:</p>
<div id="703e13a4" data-execution_count="243">
<div id="cb416"><pre><code><span id="cb416-1">A <span>=</span> Matrix.hstack(T(e1), T(e2))</span>
<span id="cb416-2"><span>print</span>(<span>"Matrix representation in standard basis:</span><span>\n</span><span>"</span>, A)</span></code></pre></div>
<div>
<pre><code>Matrix representation in standard basis:
 Matrix([[2, 1], [1, -1]])</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Using the matrix for computations</li>
</ol>
<div id="3a731b42" data-execution_count="244">
<div id="cb418"><pre><code><span id="cb418-1">v <span>=</span> Matrix([<span>3</span>,<span>4</span>])</span>
<span id="cb418-2"><span>print</span>(<span>"T(v) via definition:"</span>, T(v))</span>
<span id="cb418-3"><span>print</span>(<span>"T(v) via matrix:"</span>, A<span>*</span>v)</span></code></pre></div>
<div>
<pre><code>T(v) via definition: Matrix([[10], [-1]])
T(v) via matrix: Matrix([[10], [-1]])</code></pre>
</div>
</div>
<p>Both methods match.</p>
<ol start="3" type="1">
<li>Matrix in a different basis</li>
</ol>
<p>Now suppose we use basis</p>
<p><span>\[
B = \{ [1,1], [1,-1] \}
\]</span></p>
<p>To represent <span>\(T\)</span> in this basis:</p>
<ol type="1">
<li>Build the change-of-basis matrix <span>\(P\)</span>.</li>
<li>Compute <span>\(A_B = P^{-1}AP\)</span>.</li>
</ol>
<div id="13047e66" data-execution_count="245">
<div id="cb420"><pre><code><span id="cb420-1">B <span>=</span> Matrix.hstack(Matrix([<span>1</span>,<span>1</span>]), Matrix([<span>1</span>,<span>-</span><span>1</span>]))</span>
<span id="cb420-2">P <span>=</span> B</span>
<span id="cb420-3">A_B <span>=</span> P.inv() <span>*</span> A <span>*</span> P</span>
<span id="cb420-4"><span>print</span>(<span>"Matrix representation in new basis:</span><span>\n</span><span>"</span>, A_B)</span></code></pre></div>
<div>
<pre><code>Matrix representation in new basis:
 Matrix([[3/2, 3/2], [3/2, -1/2]])</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Interpretation</li>
</ol>
<ul>
<li>In standard basis, <span>\(A\)</span> tells us how <span>\(T\)</span> acts on unit vectors.</li>
<li>In basis <span>\(B\)</span>, <span>\(A_B\)</span> shows how <span>\(T\)</span> looks when described using different coordinates.</li>
</ul>
<ol start="5" type="1">
<li>Random linear map in <span>\(\mathbb{R}^3\)</span></li>
</ol>
<div id="13e37e9e" data-execution_count="246">
<div id="cb422"><pre><code><span id="cb422-1">np.random.seed(<span>1</span>)</span>
<span id="cb422-2">A3 <span>=</span> Matrix(np.random.randint(<span>-</span><span>3</span>,<span>4</span>,(<span>3</span>,<span>3</span>)))</span>
<span id="cb422-3"><span>print</span>(<span>"Random transformation matrix:</span><span>\n</span><span>"</span>, A3)</span>
<span id="cb422-4"></span>
<span id="cb422-5">B3 <span>=</span> Matrix.hstack(Matrix([<span>1</span>,<span>0</span>,<span>1</span>]), Matrix([<span>0</span>,<span>1</span>,<span>1</span>]), Matrix([<span>1</span>,<span>1</span>,<span>0</span>]))</span>
<span id="cb422-6">A3_B <span>=</span> B3.inv() <span>*</span> A3 <span>*</span> B3</span>
<span id="cb422-7"><span>print</span>(<span>"Representation in new basis:</span><span>\n</span><span>"</span>, A3_B)</span></code></pre></div>
<div>
<pre><code>Random transformation matrix:
 Matrix([[2, 0, 1], [-3, -2, 0], [2, -3, -3]])
Representation in new basis:
 Matrix([[5/2, -3/2, 3], [-7/2, -9/2, -4], [1/2, 5/2, -1]])</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-40">
<h4 data-anchor-id="try-it-yourself-40">Try It Yourself</h4>
<ol type="1">
<li>Define <span>\(T(x,y) = (x+2y, 3x+y)\)</span>. Find its matrix in the standard basis.</li>
<li>Use a new basis <span>\(\{[2,0],[0,3]\}\)</span>. Compute the representation <span>\(A_B\)</span>.</li>
<li>Verify that applying <span>\(T\)</span> directly to a vector matches computing via <span>\(A_B\)</span> and change-of-basis.</li>
</ol>
</section>
<section id="the-takeaway-24">
<h4 data-anchor-id="the-takeaway-24">The Takeaway</h4>
<ul>
<li>A linear transformation becomes a matrix representation once a basis is chosen.</li>
<li>Columns of the matrix = images of basis vectors.</li>
<li>Changing the basis changes the matrix, but the transformation itself stays the same.</li>
</ul>
</section>
</section>
<section id="kernel-and-image-inputs-that-vanish-outputs-we-can-reach">
<h3 data-anchor-id="kernel-and-image-inputs-that-vanish-outputs-we-can-reach">43. Kernel and Image (Inputs That Vanish; Outputs We Can Reach)</h3>
<p>Two fundamental subspaces describe any linear transformation <span>\(T(x) = Ax\)</span>:</p>
<ul>
<li>Kernel (null space): all vectors <span>\(x\)</span> such that <span>\(Ax = 0\)</span>.</li>
<li>Image (column space): all possible outputs <span>\(Ax\)</span>.</li>
</ul>
<p>The kernel tells us what inputs collapse to zero, while the image tells us what outputs are achievable.</p>
<section id="set-up-your-lab-42">
<h4 data-anchor-id="set-up-your-lab-42">Set Up Your Lab</h4>
<div id="c36dc299" data-execution_count="247"><pre><code><span id="cb424-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb424-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-42">
<h4 data-anchor-id="step-by-step-code-walkthrough-42">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Kernel of a matrix</li>
</ol>
<p>Consider</p>
<p><span>\[
A = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 2 &amp; 4 &amp; 6 \end{bmatrix}
\]</span></p>
<div id="aca4b8fd" data-execution_count="248">
<div id="cb425"><pre><code><span id="cb425-1">A <span>=</span> Matrix([</span>
<span id="cb425-2">    [<span>1</span>,<span>2</span>,<span>3</span>],</span>
<span id="cb425-3">    [<span>2</span>,<span>4</span>,<span>6</span>]</span>
<span id="cb425-4">])</span>
<span id="cb425-5"></span>
<span id="cb425-6"><span>print</span>(<span>"Null space (kernel):"</span>, A.nullspace())</span></code></pre></div>
<div>
<pre><code>Null space (kernel): [Matrix([
[-2],
[ 1],
[ 0]]), Matrix([
[-3],
[ 0],
[ 1]])]</code></pre>
</div>
</div>
<p>The null space basis shows dependencies among columns. Here, the kernel is 2-dimensional because columns are dependent.</p>
<ol start="2" type="1">
<li>Image (column space)</li>
</ol>
<div id="022cd2d3" data-execution_count="249">
<div id="cb427"><pre><code><span id="cb427-1"><span>print</span>(<span>"Column space (image):"</span>, A.columnspace())</span>
<span id="cb427-2"><span>print</span>(<span>"Rank (dimension of image):"</span>, A.rank())</span></code></pre></div>
<div>
<pre><code>Column space (image): [Matrix([
[1],
[2]])]
Rank (dimension of image): 1</code></pre>
</div>
</div>
<p>The image is spanned by <span>\([1,2]^T\)</span>. So all outputs of <span>\(A\)</span> are multiples of this vector.</p>
<ol start="3" type="1">
<li>Interpretation</li>
</ol>
<ul>
<li>Kernel vectors → directions that map to zero.</li>
<li>Image vectors → directions we can actually reach in the output space.</li>
</ul>
<p>If <span>\(x \in \ker(A)\)</span>, then <span>\(Ax = 0\)</span>. If <span>\(b\)</span> is not in the image, the system <span>\(Ax = b\)</span> has no solution.</p>
<ol start="4" type="1">
<li>Example with full rank</li>
</ol>
<div id="f04f6b4b" data-execution_count="250">
<div id="cb429"><pre><code><span id="cb429-1">B <span>=</span> Matrix([</span>
<span id="cb429-2">    [<span>1</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb429-3">    [<span>0</span>,<span>1</span>,<span>0</span>],</span>
<span id="cb429-4">    [<span>0</span>,<span>0</span>,<span>1</span>]</span>
<span id="cb429-5">])</span>
<span id="cb429-6"></span>
<span id="cb429-7"><span>print</span>(<span>"Kernel of B:"</span>, B.nullspace())</span>
<span id="cb429-8"><span>print</span>(<span>"Image of B:"</span>, B.columnspace())</span></code></pre></div>
<div>
<pre><code>Kernel of B: []
Image of B: [Matrix([
[1],
[0],
[0]]), Matrix([
[0],
[1],
[0]]), Matrix([
[0],
[0],
[1]])]</code></pre>
</div>
</div>
<ul>
<li>Kernel = only zero vector.</li>
<li>Image = all of <span>\(\mathbb{R}^3\)</span>.</li>
</ul>
<ol start="5" type="1">
<li>NumPy version (image via column space)</li>
</ol>
<div id="646fed4b" data-execution_count="251"><pre><code><span id="cb431-1">A <span>=</span> np.array([[<span>1</span>,<span>2</span>,<span>3</span>],[<span>2</span>,<span>4</span>,<span>6</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb431-2">rank <span>=</span> np.linalg.matrix_rank(A)</span>
<span id="cb431-3"><span>print</span>(<span>"Rank with NumPy:"</span>, rank)</span></code></pre></div>
<p>NumPy doesn’t compute null spaces directly, but we can use SVD for that if needed.</p>
</section>
<section id="try-it-yourself-41">
<h4 data-anchor-id="try-it-yourself-41">Try It Yourself</h4>
<ol type="1">
<li><p>Compute kernel and image for</p>
<p><span>\[
\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 1 \end{bmatrix}
\]</span></p>
<p>What do they look like?</p></li>
<li><p>Take a random 3×4 matrix and find its kernel and image dimensions.</p></li>
<li><p>Solve <span>\(Ax = b\)</span> for a matrix <span>\(A\)</span>. Try two different <span>\(b\)</span>: one inside the image, one outside. Observe the difference.</p></li>
</ol>
</section>
<section id="the-takeaway-25">
<h4 data-anchor-id="the-takeaway-25">The Takeaway</h4>
<ul>
<li>Kernel = inputs that vanish under <span>\(A\)</span>.</li>
<li>Image = outputs that can be reached by <span>\(A\)</span>.</li>
<li>Together, they fully describe what a linear map does: what it “kills” and what it “produces.”</li>
</ul>
</section>
</section>
<section id="invertibility-and-isomorphisms-perfectly-reversible-maps">
<h3 data-anchor-id="invertibility-and-isomorphisms-perfectly-reversible-maps">44. Invertibility and Isomorphisms (Perfectly Reversible Maps)</h3>
<p>A matrix (or linear map) is invertible if it has an inverse <span>\(A^{-1}\)</span> such that</p>
<p><span>\[
A^{-1}A = I \quad \text{and} \quad AA^{-1} = I
\]</span></p>
<p>An invertible map is also called an isomorphism, because it preserves all information - every input has exactly one output, and every output comes from exactly one input.</p>
<section id="set-up-your-lab-43">
<h4 data-anchor-id="set-up-your-lab-43">Set Up Your Lab</h4>
<div id="aa3bee28" data-execution_count="252"><pre><code><span id="cb433-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb433-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-43">
<h4 data-anchor-id="step-by-step-code-walkthrough-43">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Checking invertibility</li>
</ol>
<div id="02b909d0" data-execution_count="253">
<div id="cb434"><pre><code><span id="cb434-1">A <span>=</span> Matrix([</span>
<span id="cb434-2">    [<span>2</span>,<span>1</span>],</span>
<span id="cb434-3">    [<span>5</span>,<span>3</span>]</span>
<span id="cb434-4">])</span>
<span id="cb434-5"></span>
<span id="cb434-6"><span>print</span>(<span>"Determinant:"</span>, A.det())</span>
<span id="cb434-7"><span>print</span>(<span>"Is invertible?"</span>, A.det() <span>!=</span> <span>0</span>)</span></code></pre></div>
<div>
<pre><code>Determinant: 1
Is invertible? True</code></pre>
</div>
</div>
<p>If determinant ≠ 0 → invertible.</p>
<ol start="2" type="1">
<li>Computing the inverse</li>
</ol>
<div id="7fe6f3b9" data-execution_count="254">
<div id="cb436"><pre><code><span id="cb436-1">A_inv <span>=</span> A.inv()</span>
<span id="cb436-2"><span>print</span>(<span>"Inverse matrix:</span><span>\n</span><span>"</span>, A_inv)</span>
<span id="cb436-3"></span>
<span id="cb436-4"><span>print</span>(<span>"Check A*A_inv = I:</span><span>\n</span><span>"</span>, A <span>*</span> A_inv)</span></code></pre></div>
<div>
<pre><code>Inverse matrix:
 Matrix([[3, -1], [-5, 2]])
Check A*A_inv = I:
 Matrix([[1, 0], [0, 1]])</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Solving systems with inverses</li>
</ol>
<p>For <span>\(Ax = b\)</span>, if <span>\(A\)</span> is invertible:</p>
<div id="027788cf" data-execution_count="255">
<div id="cb438"><pre><code><span id="cb438-1">b <span>=</span> Matrix([<span>1</span>,<span>2</span>])</span>
<span id="cb438-2">x <span>=</span> A_inv <span>*</span> b</span>
<span id="cb438-3"><span>print</span>(<span>"Solution x:"</span>, x)</span></code></pre></div>
<div>
<pre><code>Solution x: Matrix([[1], [-1]])</code></pre>
</div>
</div>
<p>This is equivalent to <code>A.solve(b)</code> in SymPy or <code>np.linalg.solve</code> in NumPy.</p>
<ol start="4" type="1">
<li>Non-invertible (singular) example</li>
</ol>
<div id="995292d9" data-execution_count="256">
<div id="cb440"><pre><code><span id="cb440-1">B <span>=</span> Matrix([</span>
<span id="cb440-2">    [<span>1</span>,<span>2</span>],</span>
<span id="cb440-3">    [<span>2</span>,<span>4</span>]</span>
<span id="cb440-4">])</span>
<span id="cb440-5"></span>
<span id="cb440-6"><span>print</span>(<span>"Determinant:"</span>, B.det())</span>
<span id="cb440-7"><span>print</span>(<span>"Is invertible?"</span>, B.det() <span>!=</span> <span>0</span>)</span></code></pre></div>
<div>
<pre><code>Determinant: 0
Is invertible? False</code></pre>
</div>
</div>
<p>Determinant = 0 → no inverse. The matrix collapses space onto a line, losing information.</p>
<ol start="5" type="1">
<li>NumPy version</li>
</ol>
<div id="b59354a1" data-execution_count="257">
<div id="cb442"><pre><code><span id="cb442-1">A <span>=</span> np.array([[<span>2</span>,<span>1</span>],[<span>5</span>,<span>3</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb442-2"><span>print</span>(<span>"Determinant:"</span>, np.linalg.det(A))</span>
<span id="cb442-3"><span>print</span>(<span>"Inverse:</span><span>\n</span><span>"</span>, np.linalg.inv(A))</span></code></pre></div>
<div>
<pre><code>Determinant: 1.0000000000000002
Inverse:
 [[ 3. -1.]
 [-5.  2.]]</code></pre>
</div>
</div>
<ol start="6" type="1">
<li>Geometric intuition</li>
</ol>
<ul>
<li>Invertible transformation = reversible (like rotating, scaling by nonzero).</li>
<li>Non-invertible transformation = squashing space into a lower dimension (like flattening a plane onto a line).</li>
</ul>
</section>
<section id="try-it-yourself-42">
<h4 data-anchor-id="try-it-yourself-42">Try It Yourself</h4>
<ol type="1">
<li><p>Test whether</p>
<p><span>\[
\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}
\]</span></p>
<p>is invertible and find its inverse.</p></li>
<li><p>Compute the determinant of a 3×3 random integer matrix. If it’s nonzero, find its inverse.</p></li>
<li><p>Create a singular 3×3 matrix (make one row a multiple of another). Confirm it has no inverse.</p></li>
</ol>
</section>
<section id="the-takeaway-26">
<h4 data-anchor-id="the-takeaway-26">The Takeaway</h4>
<ul>
<li>Invertible matrix ↔︎ isomorphism: perfectly reversible, no information lost.</li>
<li>Determinant ≠ 0 → invertible; determinant = 0 → singular.</li>
<li>Inverses are useful conceptually, but in computation we usually solve systems directly instead of calculating <span>\(A^{-1}\)</span>.</li>
</ul>
</section>
</section>
<section id="composition-powers-and-iteration-doing-it-again-and-again">
<h3 data-anchor-id="composition-powers-and-iteration-doing-it-again-and-again">45. Composition, Powers, and Iteration (Doing It Again and Again)</h3>
<p>Linear transformations can be chained together. Applying one after another is called composition, and in matrix form this becomes multiplication. Repeated application of the same transformation leads to powers of a matrix.</p>
<section id="set-up-your-lab-44">
<h4 data-anchor-id="set-up-your-lab-44">Set Up Your Lab</h4>
<div id="699c0c7e" data-execution_count="258"><pre><code><span id="cb444-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb444-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-44">
<h4 data-anchor-id="step-by-step-code-walkthrough-44">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Composition of transformations</li>
</ol>
<p>Suppose we have two linear maps:</p>
<ul>
<li><span>\(T_1\)</span>: rotate by 90°</li>
<li><span>\(T_2\)</span>: scale x by 2</li>
</ul>
<div id="0b4c98d5" data-execution_count="259">
<div id="cb445"><pre><code><span id="cb445-1">theta <span>=</span> np.pi<span>/</span><span>2</span></span>
<span id="cb445-2">R <span>=</span> np.array([</span>
<span id="cb445-3">    [np.cos(theta), <span>-</span>np.sin(theta)],</span>
<span id="cb445-4">    [np.sin(theta),  np.cos(theta)]</span>
<span id="cb445-5">])</span>
<span id="cb445-6">S <span>=</span> np.array([</span>
<span id="cb445-7">    [<span>2</span>,<span>0</span>],</span>
<span id="cb445-8">    [<span>0</span>,<span>1</span>]</span>
<span id="cb445-9">])</span>
<span id="cb445-10"></span>
<span id="cb445-11"><span># Compose: apply R then S</span></span>
<span id="cb445-12">C <span>=</span> S <span>@</span> R</span>
<span id="cb445-13"><span>print</span>(<span>"Composite matrix:</span><span>\n</span><span>"</span>, C)</span></code></pre></div>
<div>
<pre><code>Composite matrix:
 [[ 1.2246468e-16 -2.0000000e+00]
 [ 1.0000000e+00  6.1232340e-17]]</code></pre>
</div>
</div>
<p>Applying the composite matrix is equivalent to applying both maps in sequence.</p>
<ol start="2" type="1">
<li>Verifying with a vector</li>
</ol>
<div id="5f5226f6" data-execution_count="260">
<div id="cb447"><pre><code><span id="cb447-1">v <span>=</span> np.array([<span>1</span>,<span>1</span>])</span>
<span id="cb447-2">step1 <span>=</span> R <span>@</span> v</span>
<span id="cb447-3">step2 <span>=</span> S <span>@</span> step1</span>
<span id="cb447-4">composite <span>=</span> C <span>@</span> v</span>
<span id="cb447-5"></span>
<span id="cb447-6"><span>print</span>(<span>"Step-by-step:"</span>, step2)</span>
<span id="cb447-7"><span>print</span>(<span>"Composite:"</span>, composite)</span></code></pre></div>
<div>
<pre><code>Step-by-step: [-2.  1.]
Composite: [-2.  1.]</code></pre>
</div>
</div>
<p>Both results are the same → composition = matrix multiplication.</p>
<ol start="3" type="1">
<li>Powers of a matrix</li>
</ol>
<p>Repeatedly applying a transformation corresponds to matrix powers.</p>
<p>Example: scaling by 2.</p>
<div id="97823ab5" data-execution_count="261">
<div id="cb449"><pre><code><span id="cb449-1">A <span>=</span> np.array([[<span>2</span>,<span>0</span>],[<span>0</span>,<span>2</span>]])</span>
<span id="cb449-2">v <span>=</span> np.array([<span>1</span>,<span>1</span>])</span>
<span id="cb449-3"></span>
<span id="cb449-4"><span>print</span>(<span>"A @ v ="</span>, A <span>@</span> v)</span>
<span id="cb449-5"><span>print</span>(<span>"A^2 @ v ="</span>, np.linalg.matrix_power(A,<span>2</span>) <span>@</span> v)</span>
<span id="cb449-6"><span>print</span>(<span>"A^5 @ v ="</span>, np.linalg.matrix_power(A,<span>5</span>) <span>@</span> v)</span></code></pre></div>
<div>
<pre><code>A @ v = [2 2]
A^2 @ v = [4 4]
A^5 @ v = [32 32]</code></pre>
</div>
</div>
<p>Each step doubles the scaling effect.</p>
<ol start="4" type="1">
<li>Iteration dynamics</li>
</ol>
<p>Let’s iterate a transformation many times and see what happens.</p>
<p>Example:</p>
<p><span>\[
A = \begin{bmatrix} 0.5 &amp; 0 \\ 0 &amp; 0.5 \end{bmatrix}
\]</span></p>
<div id="b37d0270" data-execution_count="262">
<div id="cb451"><pre><code><span id="cb451-1">A <span>=</span> np.array([[<span>0.5</span>,<span>0</span>],[<span>0</span>,<span>0.5</span>]])</span>
<span id="cb451-2">v <span>=</span> np.array([<span>4</span>,<span>4</span>])</span>
<span id="cb451-3"></span>
<span id="cb451-4"><span>for</span> i <span>in</span> <span>range</span>(<span>5</span>):</span>
<span id="cb451-5">    v <span>=</span> A <span>@</span> v</span>
<span id="cb451-6">    <span>print</span>(<span>f"Step </span><span>{</span>i<span>+</span><span>1</span><span>}</span><span>:"</span>, v)</span></code></pre></div>
<div>
<pre><code>Step 1: [2. 2.]
Step 2: [1. 1.]
Step 3: [0.5 0.5]
Step 4: [0.25 0.25]
Step 5: [0.125 0.125]</code></pre>
</div>
</div>
<p>Each step shrinks the vector → iteration can reveal stability.</p>
<ol start="5" type="1">
<li>Random example</li>
</ol>
<div id="187b110b" data-execution_count="263">
<div id="cb453"><pre><code><span id="cb453-1">np.random.seed(<span>0</span>)</span>
<span id="cb453-2">M <span>=</span> np.random.randint(<span>-</span><span>2</span>,<span>3</span>,(<span>2</span>,<span>2</span>))</span>
<span id="cb453-3"><span>print</span>(<span>"Random matrix:</span><span>\n</span><span>"</span>, M)</span>
<span id="cb453-4"></span>
<span id="cb453-5"><span>print</span>(<span>"M^2:</span><span>\n</span><span>"</span>, np.linalg.matrix_power(M,<span>2</span>))</span>
<span id="cb453-6"><span>print</span>(<span>"M^3:</span><span>\n</span><span>"</span>, np.linalg.matrix_power(M,<span>3</span>))</span></code></pre></div>
<div>
<pre><code>Random matrix:
 [[ 2 -2]
 [ 1  1]]
M^2:
 [[ 2 -6]
 [ 3 -1]]
M^3:
 [[ -2 -10]
 [  5  -7]]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-43">
<h4 data-anchor-id="try-it-yourself-43">Try It Yourself</h4>
<ol type="1">
<li>Create two transformations: reflection across x-axis and scaling by 3. Compose them.</li>
<li>Take a shear matrix and compute <span>\(A^5\)</span>. What happens to a vector after repeated application?</li>
<li>Experiment with a rotation matrix raised to higher powers. What cycle do you see?</li>
</ol>
</section>
<section id="the-takeaway-27">
<h4 data-anchor-id="the-takeaway-27">The Takeaway</h4>
<ul>
<li>Composition of linear maps = matrix multiplication.</li>
<li>Powers of a matrix represent repeated application.</li>
<li>Iteration reveals long-term dynamics: shrinking, growing, or oscillating behavior.</li>
</ul>
</section>
</section>
<section id="similarity-and-conjugation-same-action-different-basis">
<h3 data-anchor-id="similarity-and-conjugation-same-action-different-basis">46. Similarity and Conjugation (Same Action, Different Basis)</h3>
<p>Two matrices <span>\(A\)</span> and <span>\(B\)</span> are called similar if there exists an invertible matrix <span>\(P\)</span> such that</p>
<p><span>\[
B = P^{-1} A P
\]</span></p>
<p>This means <span>\(A\)</span> and <span>\(B\)</span> represent the same linear transformation, but in different bases. This lab explores similarity and why it matters.</p>
<section id="set-up-your-lab-45">
<h4 data-anchor-id="set-up-your-lab-45">Set Up Your Lab</h4>
<div id="6dd9fbe4" data-execution_count="264"><pre><code><span id="cb455-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb455-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-45">
<h4 data-anchor-id="step-by-step-code-walkthrough-45">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Example with a change of basis</li>
</ol>
<div id="de12b0eb" data-execution_count="265">
<div id="cb456"><pre><code><span id="cb456-1">A <span>=</span> Matrix([</span>
<span id="cb456-2">    [<span>2</span>,<span>1</span>],</span>
<span id="cb456-3">    [<span>0</span>,<span>2</span>]</span>
<span id="cb456-4">])</span>
<span id="cb456-5"></span>
<span id="cb456-6">P <span>=</span> Matrix([</span>
<span id="cb456-7">    [<span>1</span>,<span>1</span>],</span>
<span id="cb456-8">    [<span>0</span>,<span>1</span>]</span>
<span id="cb456-9">])</span>
<span id="cb456-10"></span>
<span id="cb456-11">B <span>=</span> P.inv() <span>*</span> A <span>*</span> P</span>
<span id="cb456-12"><span>print</span>(<span>"Original A:</span><span>\n</span><span>"</span>, A)</span>
<span id="cb456-13"><span>print</span>(<span>"Similar matrix B:</span><span>\n</span><span>"</span>, B)</span></code></pre></div>
<div>
<pre><code>Original A:
 Matrix([[2, 1], [0, 2]])
Similar matrix B:
 Matrix([[2, 1], [0, 2]])</code></pre>
</div>
</div>
<p>Here, <span>\(A\)</span> and <span>\(B\)</span> are similar: they describe the same transformation in different coordinates.</p>
<ol start="2" type="1">
<li>Eigenvalues stay the same</li>
</ol>
<p>Similarity preserves eigenvalues.</p>
<div id="95d9b21d" data-execution_count="266">
<div id="cb458"><pre><code><span id="cb458-1"><span>print</span>(<span>"Eigenvalues of A:"</span>, A.eigenvals())</span>
<span id="cb458-2"><span>print</span>(<span>"Eigenvalues of B:"</span>, B.eigenvals())</span></code></pre></div>
<div>
<pre><code>Eigenvalues of A: {2: 2}
Eigenvalues of B: {2: 2}</code></pre>
</div>
</div>
<p>Both matrices have the same eigenvalues, even though their entries differ.</p>
<ol start="3" type="1">
<li>Similarity and diagonalization</li>
</ol>
<p>If a matrix is diagonalizable, there exists <span>\(P\)</span> such that</p>
<p><span>\[
D = P^{-1} A P
\]</span></p>
<p>where <span>\(D\)</span> is diagonal.</p>
<div id="d304255e" data-execution_count="267">
<div id="cb460"><pre><code><span id="cb460-1">C <span>=</span> Matrix([</span>
<span id="cb460-2">    [<span>4</span>,<span>1</span>],</span>
<span id="cb460-3">    [<span>0</span>,<span>2</span>]</span>
<span id="cb460-4">])</span>
<span id="cb460-5"></span>
<span id="cb460-6">P, D <span>=</span> C.diagonalize()</span>
<span id="cb460-7"><span>print</span>(<span>"Diagonal form D:</span><span>\n</span><span>"</span>, D)</span>
<span id="cb460-8"><span>print</span>(<span>"Check similarity (P^-1 C P = D):</span><span>\n</span><span>"</span>, P.inv()<span>*</span>C<span>*</span>P)</span></code></pre></div>
<div>
<pre><code>Diagonal form D:
 Matrix([[2, 0], [0, 4]])
Check similarity (P^-1 C P = D):
 Matrix([[2, 0], [0, 4]])</code></pre>
</div>
</div>
<p>Diagonalization is a special case of similarity, where the new matrix is as simple as possible.</p>
<ol start="4" type="1">
<li>NumPy version</li>
</ol>
<div id="01596e17" data-execution_count="268">
<div id="cb462"><pre><code><span id="cb462-1">A <span>=</span> np.array([[<span>2</span>,<span>1</span>],[<span>0</span>,<span>2</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb462-2">eigvals, eigvecs <span>=</span> np.linalg.eig(A)</span>
<span id="cb462-3"><span>print</span>(<span>"Eigenvalues:"</span>, eigvals)</span>
<span id="cb462-4"><span>print</span>(<span>"Eigenvectors (basis P):</span><span>\n</span><span>"</span>, eigvecs)</span></code></pre></div>
<div>
<pre><code>Eigenvalues: [2. 2.]
Eigenvectors (basis P):
 [[ 1.0000000e+00 -1.0000000e+00]
 [ 0.0000000e+00  4.4408921e-16]]</code></pre>
</div>
</div>
<p>Here, eigenvectors form the change-of-basis matrix <span>\(P\)</span>.</p>
<ol start="5" type="1">
<li>Geometric interpretation</li>
</ol>
<ul>
<li>Similar matrices = same transformation, different “ruler” (basis).</li>
<li>Diagonalization = finding a ruler that makes the transformation look like pure stretching along axes.</li>
</ul>
</section>
<section id="try-it-yourself-44">
<h4 data-anchor-id="try-it-yourself-44">Try It Yourself</h4>
<ol type="1">
<li><p>Take</p>
<p><span>\[
A = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix}
\]</span></p>
<p>and find a matrix <span>\(P\)</span> that gives a similar <span>\(B\)</span>.</p></li>
<li><p>Show that two similar matrices have the same determinant and trace.</p></li>
<li><p>For a random 3×3 matrix, check if it is diagonalizable using SymPy’s <code>.diagonalize()</code> method.</p></li>
</ol>
</section>
<section id="the-takeaway-28">
<h4 data-anchor-id="the-takeaway-28">The Takeaway</h4>
<ul>
<li>Similarity = same linear map, different basis.</li>
<li>Similar matrices share eigenvalues, determinant, and trace.</li>
<li>Diagonalization is the simplest similarity form, making repeated computations (like powers) much easier.</li>
</ul>
</section>
</section>
<section id="projections-and-reflections-idempotent-and-involutive-maps">
<h3 data-anchor-id="projections-and-reflections-idempotent-and-involutive-maps">47. Projections and Reflections (Idempotent and Involutive Maps)</h3>
<p>Two very common geometric linear maps are projections and reflections. They show up in graphics, physics, and optimization.</p>
<ul>
<li>A projection squashes vectors onto a subspace (like dropping a shadow).</li>
<li>A reflection flips vectors across a line or plane (like a mirror).</li>
</ul>
<section id="set-up-your-lab-46">
<h4 data-anchor-id="set-up-your-lab-46">Set Up Your Lab</h4>
<div id="de26b86e" data-execution_count="269"><pre><code><span id="cb464-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb464-2"><span>from</span> sympy <span>import</span> Matrix</span>
<span id="cb464-3"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-46">
<h4 data-anchor-id="step-by-step-code-walkthrough-46">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Projection onto a line</li>
</ol>
<p>If we want to project onto the line spanned by <span>\(u\)</span>, the projection matrix is:</p>
<p><span>\[
P = \frac{uu^T}{u^T u}
\]</span></p>
<div id="69dd8354" data-execution_count="270">
<div id="cb465"><pre><code><span id="cb465-1">u <span>=</span> np.array([<span>2</span>,<span>1</span>], dtype<span>=</span><span>float</span>)</span>
<span id="cb465-2">u <span>=</span> u <span>/</span> np.linalg.norm(u)   <span># normalize</span></span>
<span id="cb465-3">P <span>=</span> np.outer(u,u)</span>
<span id="cb465-4"></span>
<span id="cb465-5"><span>print</span>(<span>"Projection matrix:</span><span>\n</span><span>"</span>, P)</span></code></pre></div>
<div>
<pre><code>Projection matrix:
 [[0.8 0.4]
 [0.4 0.2]]</code></pre>
</div>
</div>
<p>Apply projection:</p>
<div id="f46e93c1" data-execution_count="271">
<div id="cb467"><pre><code><span id="cb467-1">v <span>=</span> np.array([<span>3</span>,<span>4</span>], dtype<span>=</span><span>float</span>)</span>
<span id="cb467-2">proj_v <span>=</span> P <span>@</span> v</span>
<span id="cb467-3"><span>print</span>(<span>"Original v:"</span>, v)</span>
<span id="cb467-4"><span>print</span>(<span>"Projection of v onto u:"</span>, proj_v)</span></code></pre></div>
<div>
<pre><code>Original v: [3. 4.]
Projection of v onto u: [4. 2.]</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Visualization of projection</li>
</ol>
<div id="08fb08be" data-execution_count="272">
<div id="cb469"><pre><code><span id="cb469-1">plt.arrow(<span>0</span>,<span>0</span>,v[<span>0</span>],v[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>"blue"</span>,length_includes_head<span>=</span><span>True</span>)</span>
<span id="cb469-2">plt.arrow(<span>0</span>,<span>0</span>,proj_v[<span>0</span>],proj_v[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>"red"</span>,length_includes_head<span>=</span><span>True</span>)</span>
<span id="cb469-3">plt.arrow(proj_v[<span>0</span>],proj_v[<span>1</span>],v[<span>0</span>]<span>-</span>proj_v[<span>0</span>],v[<span>1</span>]<span>-</span>proj_v[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>"gray"</span>,linestyle<span>=</span><span>"dashed"</span>)</span>
<span id="cb469-4"></span>
<span id="cb469-5">plt.axhline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb469-6">plt.axvline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb469-7">plt.grid()</span>
<span id="cb469-8">plt.title(<span>"Blue = original, Red = projection, Gray = error vector"</span>)</span>
<span id="cb469-9">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-273-output-1.png" width="558" height="431"></p>
</figure>
</div>
</div>
<p>The projection is the closest point on the line to the original vector.</p>
<ol start="3" type="1">
<li>Reflection across a line</li>
</ol>
<p>The reflection matrix across the line spanned by <span>\(u\)</span> is:</p>
<p><span>\[
R = 2P - I
\]</span></p>
<div id="14ec34e6" data-execution_count="273">
<div id="cb470"><pre><code><span id="cb470-1">I <span>=</span> np.eye(<span>2</span>)</span>
<span id="cb470-2">R <span>=</span> <span>2</span><span>*</span>P <span>-</span> I</span>
<span id="cb470-3"></span>
<span id="cb470-4">reflect_v <span>=</span> R <span>@</span> v</span>
<span id="cb470-5"><span>print</span>(<span>"Reflection of v across line u:"</span>, reflect_v)</span></code></pre></div>
<div>
<pre><code>Reflection of v across line u: [ 5.0000000e+00 -4.4408921e-16]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Checking algebraic properties</li>
</ol>
<ul>
<li>Projection: <span>\(P^2 = P\)</span> (idempotent).</li>
<li>Reflection: <span>\(R^2 = I\)</span> (involutive).</li>
</ul>
<div id="a2d8560d" data-execution_count="274">
<div id="cb472"><pre><code><span id="cb472-1"><span>print</span>(<span>"P^2 =</span><span>\n</span><span>"</span>, P <span>@</span> P)</span>
<span id="cb472-2"><span>print</span>(<span>"R^2 =</span><span>\n</span><span>"</span>, R <span>@</span> R)</span></code></pre></div>
<div>
<pre><code>P^2 =
 [[0.8 0.4]
 [0.4 0.2]]
R^2 =
 [[ 1.00000000e+00 -1.59872116e-16]
 [-1.59872116e-16  1.00000000e+00]]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Projection in higher dimensions</li>
</ol>
<p>Project onto the plane spanned by two vectors in <span>\(\mathbb{R}^3\)</span>.</p>
<div id="f8219988" data-execution_count="275">
<div id="cb474"><pre><code><span id="cb474-1">u1 <span>=</span> np.array([<span>1</span>,<span>0</span>,<span>0</span>], dtype<span>=</span><span>float</span>)</span>
<span id="cb474-2">u2 <span>=</span> np.array([<span>0</span>,<span>1</span>,<span>0</span>], dtype<span>=</span><span>float</span>)</span>
<span id="cb474-3"></span>
<span id="cb474-4">U <span>=</span> np.column_stack((u1,u2))   <span># basis for plane</span></span>
<span id="cb474-5">P_plane <span>=</span> U <span>@</span> np.linalg.inv(U.T <span>@</span> U) <span>@</span> U.T</span>
<span id="cb474-6"></span>
<span id="cb474-7">v <span>=</span> np.array([<span>1</span>,<span>2</span>,<span>3</span>], dtype<span>=</span><span>float</span>)</span>
<span id="cb474-8">proj_plane <span>=</span> P_plane <span>@</span> v</span>
<span id="cb474-9"><span>print</span>(<span>"Projection onto xy-plane:"</span>, proj_plane)</span></code></pre></div>
<div>
<pre><code>Projection onto xy-plane: [1. 2. 0.]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-45">
<h4 data-anchor-id="try-it-yourself-45">Try It Yourself</h4>
<ol type="1">
<li>Project <span>\([4,5]\)</span> onto the x-axis and verify the result.</li>
<li>Reflect <span>\([1,2]\)</span> across the line <span>\(y=x\)</span>.</li>
<li>Create a random 3D vector and project it onto the plane spanned by <span>\([1,1,0]\)</span> and <span>\([0,1,1]\)</span>.</li>
</ol>
</section>
<section id="the-takeaway-29">
<h4 data-anchor-id="the-takeaway-29">The Takeaway</h4>
<ul>
<li>Projection: idempotent (<span>\(P^2 = P\)</span>), finds the closest vector in a subspace.</li>
<li>Reflection: involutive (<span>\(R^2 = I\)</span>), flips across a line/plane but preserves lengths.</li>
<li>Both are simple but powerful examples of linear transformations with clear geometry.</li>
</ul>
</section>
</section>
<section id="rotations-and-shear-geometric-intuition">
<h3 data-anchor-id="rotations-and-shear-geometric-intuition">48. Rotations and Shear (Geometric Intuition)</h3>
<p>Two transformations often used in geometry, graphics, and physics are rotations and shears. Both are linear maps, but they behave differently:</p>
<ul>
<li>Rotation preserves lengths and angles.</li>
<li>Shear preserves area (in 2D) but distorts shapes, turning squares into parallelograms.</li>
</ul>
<section id="set-up-your-lab-47">
<h4 data-anchor-id="set-up-your-lab-47">Set Up Your Lab</h4>
<div id="8138157f" data-execution_count="276"><pre><code><span id="cb476-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb476-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-47">
<h4 data-anchor-id="step-by-step-code-walkthrough-47">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Rotation in 2D</li>
</ol>
<p>The rotation matrix by angle <span>\(\theta\)</span> is:</p>
<p><span>\[
R(\theta) = \begin{bmatrix}
\cos\theta &amp; -\sin\theta \\
\sin\theta &amp; \cos\theta
\end{bmatrix}
\]</span></p>
<div id="ee1be257" data-execution_count="277">
<div id="cb477"><pre><code><span id="cb477-1"><span>def</span> rotation_matrix(theta):</span>
<span id="cb477-2">    <span>return</span> np.array([</span>
<span id="cb477-3">        [np.cos(theta), <span>-</span>np.sin(theta)],</span>
<span id="cb477-4">        [np.sin(theta),  np.cos(theta)]</span>
<span id="cb477-5">    ])</span>
<span id="cb477-6"></span>
<span id="cb477-7">theta <span>=</span> np.pi<span>/</span><span>4</span>   <span># 45 degrees</span></span>
<span id="cb477-8">R <span>=</span> rotation_matrix(theta)</span>
<span id="cb477-9"></span>
<span id="cb477-10">v <span>=</span> np.array([<span>2</span>,<span>1</span>])</span>
<span id="cb477-11">rotated_v <span>=</span> R <span>@</span> v</span>
<span id="cb477-12"><span>print</span>(<span>"Original v:"</span>, v)</span>
<span id="cb477-13"><span>print</span>(<span>"Rotated v (45°):"</span>, rotated_v)</span></code></pre></div>
<div>
<pre><code>Original v: [2 1]
Rotated v (45°): [0.70710678 2.12132034]</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Visualizing rotation</li>
</ol>
<div id="d95c464c" data-execution_count="278">
<div id="cb479"><pre><code><span id="cb479-1">plt.arrow(<span>0</span>,<span>0</span>,v[<span>0</span>],v[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>"blue"</span>,length_includes_head<span>=</span><span>True</span>)</span>
<span id="cb479-2">plt.arrow(<span>0</span>,<span>0</span>,rotated_v[<span>0</span>],rotated_v[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>"red"</span>,length_includes_head<span>=</span><span>True</span>)</span>
<span id="cb479-3"></span>
<span id="cb479-4">plt.axhline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb479-5">plt.axvline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb479-6">plt.grid()</span>
<span id="cb479-7">plt.title(<span>"Blue = original, Red = rotated (45°)"</span>)</span>
<span id="cb479-8">plt.axis(<span>"equal"</span>)</span>
<span id="cb479-9">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-279-output-1.png" width="571" height="431"></p>
</figure>
</div>
</div>
<p>The vector rotates counterclockwise by 45°.</p>
<ol start="3" type="1">
<li>Shear in 2D</li>
</ol>
<p>A shear along the x-axis by factor <span>\(k\)</span>:</p>
<p><span>\[
S = \begin{bmatrix}
1 &amp; k \\
0 &amp; 1
\end{bmatrix}
\]</span></p>
<div id="6634c5cb" data-execution_count="279"><pre><code><span id="cb480-1">k <span>=</span> <span>1.0</span></span>
<span id="cb480-2">S <span>=</span> np.array([</span>
<span id="cb480-3">    [<span>1</span>,k],</span>
<span id="cb480-4">    [<span>0</span>,<span>1</span>]</span>
<span id="cb480-5">])</span>
<span id="cb480-6"></span>
<span id="cb480-7">sheared_v <span>=</span> S <span>@</span> v</span>
<span id="cb480-8"><span>print</span>(<span>"Sheared v:"</span>, sheared_v)</span></code></pre></div>
<ol start="4" type="1">
<li>Visualizing shear</li>
</ol>
<div id="4f31d543" data-execution_count="280">
<div id="cb482"><pre><code><span id="cb482-1">plt.arrow(<span>0</span>,<span>0</span>,v[<span>0</span>],v[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>"blue"</span>,length_includes_head<span>=</span><span>True</span>)</span>
<span id="cb482-2">plt.arrow(<span>0</span>,<span>0</span>,sheared_v[<span>0</span>],sheared_v[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>"green"</span>,length_includes_head<span>=</span><span>True</span>)</span>
<span id="cb482-3"></span>
<span id="cb482-4">plt.axhline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb482-5">plt.axvline(<span>0</span>,color<span>=</span><span>'black'</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb482-6">plt.grid()</span>
<span id="cb482-7">plt.title(<span>"Blue = original, Green = sheared"</span>)</span>
<span id="cb482-8">plt.axis(<span>"equal"</span>)</span>
<span id="cb482-9">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-281-output-1.png" width="582" height="431"></p>
</figure>
</div>
</div>
<p>The shear moves the vector sideways, distorting its angle.</p>
<ol start="5" type="1">
<li>Properties check</li>
</ol>
<ul>
<li>Rotation preserves length:</li>
</ul>
<div id="60690774" data-execution_count="281">
<div id="cb483"><pre><code><span id="cb483-1"><span>print</span>(<span>"||v|| ="</span>, np.linalg.norm(v))</span>
<span id="cb483-2"><span>print</span>(<span>"||R v|| ="</span>, np.linalg.norm(rotated_v))</span></code></pre></div>
<div>
<pre><code>||v|| = 2.23606797749979
||R v|| = 2.2360679774997894</code></pre>
</div>
</div>
<ul>
<li>Shear preserves area (determinant = 1):</li>
</ul>
<div id="5e097c88" data-execution_count="282"><pre><code><span id="cb485-1"><span>print</span>(<span>"det(S) ="</span>, np.linalg.det(S))</span></code></pre></div>
</section>
<section id="try-it-yourself-46">
<h4 data-anchor-id="try-it-yourself-46">Try It Yourself</h4>
<ol type="1">
<li>Rotate <span>\([1,0]\)</span> by 90° and check it becomes <span>\([0,1]\)</span>.</li>
<li>Apply shear with <span>\(k=2\)</span> to a square (points <span>\((0,0),(1,0),(1,1),(0,1)\)</span>) and plot before/after.</li>
<li>Combine rotation and shear: apply shear first, then rotation. What happens?</li>
</ol>
</section>
<section id="the-takeaway-30">
<h4 data-anchor-id="the-takeaway-30">The Takeaway</h4>
<ul>
<li>Rotation: length- and angle-preserving, determinant = 1.</li>
<li>Shear: shape-distorting but area-preserving, determinant = 1.</li>
<li>Both are linear maps that provide geometric intuition and real-world modeling tools.</li>
</ul>
</section>
</section>
<section id="rank-and-operator-viewpoint-rank-beyond-elimination">
<h3 data-anchor-id="rank-and-operator-viewpoint-rank-beyond-elimination">49. Rank and Operator Viewpoint (Rank Beyond Elimination)</h3>
<p>The rank of a matrix tells us how much “information” a linear map carries. Algebraically, it is the dimension of the image (column space). Geometrically, it measures how many independent directions survive the transformation.</p>
<p>From the operator viewpoint:</p>
<ul>
<li>A matrix <span>\(A\)</span> is not just a table of numbers - it is a linear operator that maps vectors to other vectors.</li>
<li>The rank is the dimension of the output space that <span>\(A\)</span> actually reaches.</li>
</ul>
<section id="set-up-your-lab-48">
<h4 data-anchor-id="set-up-your-lab-48">Set Up Your Lab</h4>
<div id="c5e36221" data-execution_count="283"><pre><code><span id="cb487-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb487-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-48">
<h4 data-anchor-id="step-by-step-code-walkthrough-48">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Rank via elimination (SymPy)</li>
</ol>
<div id="61affbdc" data-execution_count="284">
<div id="cb488"><pre><code><span id="cb488-1">A <span>=</span> Matrix([</span>
<span id="cb488-2">    [<span>1</span>,<span>2</span>,<span>3</span>],</span>
<span id="cb488-3">    [<span>2</span>,<span>4</span>,<span>6</span>],</span>
<span id="cb488-4">    [<span>1</span>,<span>1</span>,<span>1</span>]</span>
<span id="cb488-5">])</span>
<span id="cb488-6"></span>
<span id="cb488-7"><span>print</span>(<span>"Matrix A:</span><span>\n</span><span>"</span>, A)</span>
<span id="cb488-8"><span>print</span>(<span>"Rank of A:"</span>, A.rank())</span></code></pre></div>
<div>
<pre><code>Matrix A:
 Matrix([[1, 2, 3], [2, 4, 6], [1, 1, 1]])
Rank of A: 2</code></pre>
</div>
</div>
<p>Here, the second row is a multiple of the first → less independence → rank &lt; 3.</p>
<ol start="2" type="1">
<li>Rank via NumPy</li>
</ol>
<div id="7e538165" data-execution_count="285"><pre><code><span id="cb490-1">A_np <span>=</span> np.array([[<span>1</span>,<span>2</span>,<span>3</span>],[<span>2</span>,<span>4</span>,<span>6</span>],[<span>1</span>,<span>1</span>,<span>1</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb490-2"><span>print</span>(<span>"Rank (NumPy):"</span>, np.linalg.matrix_rank(A_np))</span></code></pre></div>
<ol start="3" type="1">
<li>Operator viewpoint</li>
</ol>
<p>Let’s apply <span>\(A\)</span> to random vectors:</p>
<div id="8e7fa128" data-execution_count="286">
<div id="cb492"><pre><code><span id="cb492-1"><span>for</span> v <span>in</span> [np.array([<span>1</span>,<span>0</span>,<span>0</span>]), np.array([<span>0</span>,<span>1</span>,<span>0</span>]), np.array([<span>0</span>,<span>0</span>,<span>1</span>])]:</span>
<span id="cb492-2">    <span>print</span>(<span>"A @"</span>, v, <span>"="</span>, A_np <span>@</span> v)</span></code></pre></div>
<div>
<pre><code>A @ [1 0 0] = [1. 2. 1.]
A @ [0 1 0] = [2. 4. 1.]
A @ [0 0 1] = [3. 6. 1.]</code></pre>
</div>
</div>
<p>Even though we started in 3D, all outputs lie in a plane in <span>\(\mathbb{R}^3\)</span>. That’s why rank = 2.</p>
<ol start="4" type="1">
<li>Full rank vs reduced rank</li>
</ol>
<ul>
<li>Full rank: the transformation preserves dimension (no collapse).</li>
<li>Reduced rank: the transformation collapses onto a lower-dimensional subspace.</li>
</ul>
<p>Example full-rank:</p>
<div id="fbf0fc2f" data-execution_count="287"><pre><code><span id="cb494-1">B <span>=</span> Matrix([</span>
<span id="cb494-2">    [<span>1</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb494-3">    [<span>0</span>,<span>1</span>,<span>0</span>],</span>
<span id="cb494-4">    [<span>0</span>,<span>0</span>,<span>1</span>]</span>
<span id="cb494-5">])</span>
<span id="cb494-6"></span>
<span id="cb494-7"><span>print</span>(<span>"Rank of B:"</span>, B.rank())</span></code></pre></div>
<ol start="5" type="1">
<li>Connection to nullity</li>
</ol>
<p>The rank-nullity theorem:</p>
<p><span>\[
\text{rank}(A) + \text{nullity}(A) = \text{number of columns of } A
\]</span></p>
<p>Check with SymPy:</p>
<div id="9f45440e" data-execution_count="288">
<div id="cb496"><pre><code><span id="cb496-1"><span>print</span>(<span>"Null space (basis):"</span>, A.nullspace())</span>
<span id="cb496-2"><span>print</span>(<span>"Nullity:"</span>, <span>len</span>(A.nullspace()))</span>
<span id="cb496-3"><span>print</span>(<span>"Rank + Nullity ="</span>, A.rank() <span>+</span> <span>len</span>(A.nullspace()))</span></code></pre></div>
<div>
<pre><code>Null space (basis): [Matrix([
[ 1],
[-2],
[ 1]])]
Nullity: 1
Rank + Nullity = 3</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-47">
<h4 data-anchor-id="try-it-yourself-47">Try It Yourself</h4>
<ol type="1">
<li><p>Take</p>
<p><span>\[
\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 1 \end{bmatrix}
\]</span></p>
<p>and compute its rank. Why is it 1?</p></li>
<li><p>For a random 4×4 matrix, use <code>np.linalg.matrix_rank</code> to check if it’s invertible.</p></li>
<li><p>Verify rank-nullity theorem for a 3×5 random integer matrix.</p></li>
</ol>
</section>
<section id="the-takeaway-31">
<h4 data-anchor-id="the-takeaway-31">The Takeaway</h4>
<ul>
<li>Rank = dimension of the image (how many independent outputs a transformation has).</li>
<li>Operator viewpoint: rank shows how much of the input space survives after transformation.</li>
<li>Rank-nullity links the image and kernel - together they fully describe a linear operator.</li>
</ul>
</section>
</section>
<section id="block-matrices-and-block-maps-divide-and-conquer-structure">
<h3 data-anchor-id="block-matrices-and-block-maps-divide-and-conquer-structure">50. Block Matrices and Block Maps (Divide and Conquer Structure)</h3>
<p>Sometimes matrices can be arranged in blocks (submatrices). Treating a big matrix as smaller pieces helps simplify calculations, especially in systems with structure (networks, coupled equations, or partitioned variables).</p>
<section id="set-up-your-lab-49">
<h4 data-anchor-id="set-up-your-lab-49">Set Up Your Lab</h4>
<div id="b62e27a6" data-execution_count="289"><pre><code><span id="cb498-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb498-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-49">
<h4 data-anchor-id="step-by-step-code-walkthrough-49">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Constructing block matrices</li>
</ol>
<p>We can build a block matrix from smaller pieces:</p>
<div id="4044e4c4" data-execution_count="290">
<div id="cb499"><pre><code><span id="cb499-1">A11 <span>=</span> Matrix([[<span>1</span>,<span>2</span>],[<span>3</span>,<span>4</span>]])</span>
<span id="cb499-2">A12 <span>=</span> Matrix([[<span>5</span>,<span>6</span>],[<span>7</span>,<span>8</span>]])</span>
<span id="cb499-3">A21 <span>=</span> Matrix([[<span>9</span>,<span>10</span>]])</span>
<span id="cb499-4">A22 <span>=</span> Matrix([[<span>11</span>,<span>12</span>]])</span>
<span id="cb499-5"></span>
<span id="cb499-6"><span># Combine into a block matrix</span></span>
<span id="cb499-7">A <span>=</span> Matrix.vstack(</span>
<span id="cb499-8">    Matrix.hstack(A11, A12),</span>
<span id="cb499-9">    Matrix.hstack(A21, A22)</span>
<span id="cb499-10">)</span>
<span id="cb499-11"><span>print</span>(<span>"Block matrix A:</span><span>\n</span><span>"</span>, A)</span></code></pre></div>
<div>
<pre><code>Block matrix A:
 Matrix([[1, 2, 5, 6], [3, 4, 7, 8], [9, 10, 11, 12]])</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Block multiplication</li>
</ol>
<p>If a matrix is partitioned into blocks, multiplication follows block rules:</p>
<p><span>\[
\begin{bmatrix} A &amp; B \\ C &amp; D \end{bmatrix}
\begin{bmatrix} x \\ y \end{bmatrix}
= \begin{bmatrix} Ax + By \\ Cx + Dy \end{bmatrix}
\]</span></p>
<p>Example:</p>
<div id="42fccb67" data-execution_count="291">
<div id="cb501"><pre><code><span id="cb501-1">A <span>=</span> Matrix([</span>
<span id="cb501-2">    [<span>1</span>,<span>2</span>,<span>5</span>,<span>6</span>],</span>
<span id="cb501-3">    [<span>3</span>,<span>4</span>,<span>7</span>,<span>8</span>],</span>
<span id="cb501-4">    [<span>9</span>,<span>10</span>,<span>11</span>,<span>12</span>]</span>
<span id="cb501-5">])</span>
<span id="cb501-6"></span>
<span id="cb501-7">x <span>=</span> Matrix([<span>1</span>,<span>1</span>,<span>2</span>,<span>2</span>])</span>
<span id="cb501-8"><span>print</span>(<span>"A * x ="</span>, A<span>*</span>x)</span></code></pre></div>
<div>
<pre><code>A * x = Matrix([[25], [37], [65]])</code></pre>
</div>
</div>
<p>Here the vector is split into blocks <span>\([x,y]\)</span>.</p>
<ol start="3" type="1">
<li>Block diagonal matrices</li>
</ol>
<p>Block diagonal = independent subproblems:</p>
<div id="f8cd3d71" data-execution_count="292">
<div id="cb503"><pre><code><span id="cb503-1">B1 <span>=</span> Matrix([[<span>2</span>,<span>0</span>],[<span>0</span>,<span>2</span>]])</span>
<span id="cb503-2">B2 <span>=</span> Matrix([[<span>3</span>,<span>1</span>],[<span>0</span>,<span>3</span>]])</span>
<span id="cb503-3"></span>
<span id="cb503-4">BlockDiag <span>=</span> Matrix([</span>
<span id="cb503-5">    [<span>2</span>,<span>0</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb503-6">    [<span>0</span>,<span>2</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb503-7">    [<span>0</span>,<span>0</span>,<span>3</span>,<span>1</span>],</span>
<span id="cb503-8">    [<span>0</span>,<span>0</span>,<span>0</span>,<span>3</span>]</span>
<span id="cb503-9">])</span>
<span id="cb503-10"></span>
<span id="cb503-11"><span>print</span>(<span>"Block diagonal matrix:</span><span>\n</span><span>"</span>, BlockDiag)</span></code></pre></div>
<div>
<pre><code>Block diagonal matrix:
 Matrix([[2, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 1], [0, 0, 0, 3]])</code></pre>
</div>
</div>
<p>Applying this matrix acts separately on each block - like running two smaller transformations in parallel.</p>
<ol start="4" type="1">
<li>Inverse of block diagonal</li>
</ol>
<p>The inverse of a block diagonal is just the block diagonal of inverses:</p>
<div id="237dc21d" data-execution_count="293">
<div id="cb505"><pre><code><span id="cb505-1">B1_inv <span>=</span> B1.inv()</span>
<span id="cb505-2">B2_inv <span>=</span> B2.inv()</span>
<span id="cb505-3">BlockDiagInv <span>=</span> Matrix([</span>
<span id="cb505-4">    [B1_inv[<span>0</span>,<span>0</span>],<span>0</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb505-5">    [<span>0</span>,B1_inv[<span>1</span>,<span>1</span>],<span>0</span>,<span>0</span>],</span>
<span id="cb505-6">    [<span>0</span>,<span>0</span>,B2_inv[<span>0</span>,<span>0</span>],B2_inv[<span>0</span>,<span>1</span>]],</span>
<span id="cb505-7">    [<span>0</span>,<span>0</span>,B2_inv[<span>1</span>,<span>0</span>],B2_inv[<span>1</span>,<span>1</span>]]</span>
<span id="cb505-8">])</span>
<span id="cb505-9"><span>print</span>(<span>"Inverse block diag:</span><span>\n</span><span>"</span>, BlockDiagInv)</span></code></pre></div>
<div>
<pre><code>Inverse block diag:
 Matrix([[1/2, 0, 0, 0], [0, 1/2, 0, 0], [0, 0, 1/3, -1/9], [0, 0, 0, 1/3]])</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Practical example - coupled equations</li>
</ol>
<p>Suppose we have two independent systems:</p>
<ul>
<li>System 1: <span>\(Ax = b\)</span></li>
<li>System 2: <span>\(Cy = d\)</span></li>
</ul>
<p>We can represent both together:</p>
<p><span>\[
\begin{bmatrix} A &amp; 0 \\ 0 &amp; C \end{bmatrix}
\begin{bmatrix} x \\ y \end{bmatrix}
= \begin{bmatrix} b \\ d \end{bmatrix}
\]</span></p>
<p>This shows how block matrices organize multiple systems in one big equation.</p>
</section>
<section id="try-it-yourself-48">
<h4 data-anchor-id="try-it-yourself-48">Try It Yourself</h4>
<ol type="1">
<li>Build a block diagonal matrix with three 2×2 blocks. Apply it to a vector.</li>
<li>Verify block multiplication rule by manually computing <span>\(Ax + By\)</span> and <span>\(Cx + Dy\)</span>.</li>
<li>Write two small systems of equations and combine them into one block system.</li>
</ol>
</section>
<section id="the-takeaway-32">
<h4 data-anchor-id="the-takeaway-32">The Takeaway</h4>
<ul>
<li>Block matrices let us break down big systems into smaller parts.</li>
<li>Block diagonal matrices = independent subsystems.</li>
<li>Thinking in blocks simplifies algebra, programming, and numerical computation.</li>
</ul>
</section>
</section>
</section>
<section id="chapter-6.-determinants-and-volume">
<h2 data-anchor-id="chapter-6.-determinants-and-volume">Chapter 6. Determinants and volume</h2>
<section id="areas-volumes-and-signed-scale-factors-geometric-entry-point">
<h3 data-anchor-id="areas-volumes-and-signed-scale-factors-geometric-entry-point">51. Areas, Volumes, and Signed Scale Factors (Geometric Entry Point)</h3>
<p>The determinant of a matrix has a deep geometric meaning: it tells us how a linear transformation scales area (in 2D), volume (in 3D), or higher-dimensional content. It can also flip orientation (sign).</p>
<section id="set-up-your-lab-50">
<h4 data-anchor-id="set-up-your-lab-50">Set Up Your Lab</h4>
<div id="71c5e507" data-execution_count="294"><pre><code><span id="cb507-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb507-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-50">
<h4 data-anchor-id="step-by-step-code-walkthrough-50">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Determinant in 2D (area scaling)</li>
</ol>
<p>Let’s take a matrix that stretches and shears:</p>
<div id="2cea41a3" data-execution_count="295"><pre><code><span id="cb508-1">A <span>=</span> Matrix([</span>
<span id="cb508-2">    [<span>2</span>,<span>1</span>],</span>
<span id="cb508-3">    [<span>1</span>,<span>1</span>]</span>
<span id="cb508-4">])</span>
<span id="cb508-5"></span>
<span id="cb508-6"><span>print</span>(<span>"Determinant:"</span>, A.det())</span></code></pre></div>
<p>The determinant = 1 → areas are preserved, even though the shape is distorted.</p>
<ol start="2" type="1">
<li>Unit square under transformation</li>
</ol>
<p>Transform the square with corners <span>\((0,0),(1,0),(1,1),(0,1)\)</span>:</p>
<div id="066c6553" data-execution_count="296">
<div id="cb510"><pre><code><span id="cb510-1">square <span>=</span> Matrix([</span>
<span id="cb510-2">    [<span>0</span>,<span>0</span>],</span>
<span id="cb510-3">    [<span>1</span>,<span>0</span>],</span>
<span id="cb510-4">    [<span>1</span>,<span>1</span>],</span>
<span id="cb510-5">    [<span>0</span>,<span>1</span>]</span>
<span id="cb510-6">])</span>
<span id="cb510-7"></span>
<span id="cb510-8">transformed <span>=</span> (A <span>*</span> square.T).T</span>
<span id="cb510-9"><span>print</span>(<span>"Original square:</span><span>\n</span><span>"</span>, square)</span>
<span id="cb510-10"><span>print</span>(<span>"Transformed square:</span><span>\n</span><span>"</span>, transformed)</span></code></pre></div>
<div>
<pre><code>Original square:
 Matrix([[0, 0], [1, 0], [1, 1], [0, 1]])
Transformed square:
 Matrix([[0, 0], [2, 1], [3, 2], [1, 1]])</code></pre>
</div>
</div>
<p>The area of the transformed shape equals <span>\(|\det(A)|\)</span>.</p>
<ol start="3" type="1">
<li>Determinant in 3D (volume scaling)</li>
</ol>
<div id="fb061fe8" data-execution_count="297"><pre><code><span id="cb512-1">B <span>=</span> Matrix([</span>
<span id="cb512-2">    [<span>1</span>,<span>2</span>,<span>0</span>],</span>
<span id="cb512-3">    [<span>0</span>,<span>1</span>,<span>0</span>],</span>
<span id="cb512-4">    [<span>0</span>,<span>0</span>,<span>3</span>]</span>
<span id="cb512-5">])</span>
<span id="cb512-6"></span>
<span id="cb512-7"><span>print</span>(<span>"Determinant:"</span>, B.det())</span></code></pre></div>
<p><span>\(\det(B)=3\)</span> means that volumes are scaled by 3.</p>
<ol start="4" type="1">
<li>Negative determinant = orientation flip</li>
</ol>
<div id="353c8f1d" data-execution_count="298"><pre><code><span id="cb514-1">C <span>=</span> Matrix([</span>
<span id="cb514-2">    [<span>0</span>,<span>1</span>],</span>
<span id="cb514-3">    [<span>1</span>,<span>0</span>]</span>
<span id="cb514-4">])</span>
<span id="cb514-5"></span>
<span id="cb514-6"><span>print</span>(<span>"Determinant:"</span>, C.det())</span></code></pre></div>
<p>The determinant = -1 → area preserved but orientation flipped (like a mirror reflection).</p>
<ol start="5" type="1">
<li>NumPy version</li>
</ol>
<div id="bacd8dee" data-execution_count="299"><pre><code><span id="cb516-1">A <span>=</span> np.array([[<span>2</span>,<span>1</span>],[<span>1</span>,<span>1</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb516-2"><span>print</span>(<span>"Det (NumPy):"</span>, np.linalg.det(A))</span></code></pre></div>
</section>
<section id="try-it-yourself-49">
<h4 data-anchor-id="try-it-yourself-49">Try It Yourself</h4>
<ol type="1">
<li><p>Take</p>
<p><span>\[
\begin{bmatrix} 3 &amp; 0 \\ 0 &amp; 2 \end{bmatrix}
\]</span></p>
<p>and compute the determinant. Verify it scales areas by 6.</p></li>
<li><p>Build a 3×3 shear matrix and check how it affects volume.</p></li>
<li><p>Test a reflection matrix and confirm that the determinant is negative.</p></li>
</ol>
</section>
<section id="the-takeaway-33">
<h4 data-anchor-id="the-takeaway-33">The Takeaway</h4>
<ul>
<li>Determinant measures how a linear map scales area, volume, or hypervolume.</li>
<li>Positive determinant = preserves orientation; negative = flips it.</li>
<li>Magnitude of determinant = scaling factor of geometric content.</li>
</ul>
</section>
</section>
<section id="determinant-via-linear-rules-multilinearity-sign-normalization">
<h3 data-anchor-id="determinant-via-linear-rules-multilinearity-sign-normalization">52. Determinant via Linear Rules (Multilinearity, Sign, Normalization)</h3>
<p>The determinant isn’t just a formula; it’s defined by three elegant rules that make it unique. These rules capture its geometric meaning as a volume-scaling factor.</p>
<ol type="1">
<li>Multilinearity: Linear in each row (or column).</li>
<li>Sign Change: Swapping two rows flips the sign.</li>
<li>Normalization: The determinant of the identity matrix is 1.</li>
</ol>
<section id="set-up-your-lab-51">
<h4 data-anchor-id="set-up-your-lab-51">Set Up Your Lab</h4>
<div id="1ff33309" data-execution_count="300"><pre><code><span id="cb518-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb518-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-51">
<h4 data-anchor-id="step-by-step-code-walkthrough-51">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Multilinearity</li>
</ol>
<p>If one row is scaled, the determinant scales the same way.</p>
<div id="cdb0ef6d" data-execution_count="301"><pre><code><span id="cb519-1">A <span>=</span> Matrix([[<span>1</span>,<span>2</span>],[<span>3</span>,<span>4</span>]])</span>
<span id="cb519-2"><span>print</span>(<span>"det(A):"</span>, A.det())</span>
<span id="cb519-3"></span>
<span id="cb519-4">B <span>=</span> Matrix([[<span>2</span>,<span>4</span>],[<span>3</span>,<span>4</span>]])  <span># first row doubled</span></span>
<span id="cb519-5"><span>print</span>(<span>"det(B):"</span>, B.det())</span></code></pre></div>
<p>You’ll see <code>det(B) = 2 * det(A)</code>.</p>
<ol start="2" type="1">
<li>Sign change by row swap</li>
</ol>
<div id="7cec5d4c" data-execution_count="302">
<div id="cb521"><pre><code><span id="cb521-1">C <span>=</span> Matrix([[<span>1</span>,<span>2</span>],[<span>3</span>,<span>4</span>]])</span>
<span id="cb521-2">C_swapped <span>=</span> Matrix([[<span>3</span>,<span>4</span>],[<span>1</span>,<span>2</span>]])</span>
<span id="cb521-3"></span>
<span id="cb521-4"><span>print</span>(<span>"det(C):"</span>, C.det())</span>
<span id="cb521-5"><span>print</span>(<span>"det(C_swapped):"</span>, C_swapped.det())</span></code></pre></div>
<div>
<pre><code>det(C): -2
det(C_swapped): 2</code></pre>
</div>
</div>
<p>Swapping rows flips the sign of the determinant.</p>
<ol start="3" type="1">
<li>Normalization rule</li>
</ol>
<div id="b17d1fc0" data-execution_count="303"><pre><code><span id="cb523-1">I <span>=</span> Matrix.eye(<span>3</span>)</span>
<span id="cb523-2"><span>print</span>(<span>"det(I):"</span>, I.det())</span></code></pre></div>
<p>The determinant of the identity is always 1 - this fixes the scaling baseline.</p>
<ol start="4" type="1">
<li>Combining rules (example in 3×3)</li>
</ol>
<div id="91d1d320" data-execution_count="304"><pre><code><span id="cb525-1">M <span>=</span> Matrix([[<span>1</span>,<span>2</span>,<span>3</span>],[<span>4</span>,<span>5</span>,<span>6</span>],[<span>7</span>,<span>8</span>,<span>9</span>]])</span>
<span id="cb525-2"><span>print</span>(<span>"det(M):"</span>, M.det())</span></code></pre></div>
<p>Here, rows are linearly dependent, so the determinant is 0 - consistent with multilinearity (since one row can be written as a combo of others).</p>
<ol start="5" type="1">
<li>NumPy check</li>
</ol>
<div id="8d75fc67" data-execution_count="305">
<div id="cb527"><pre><code><span id="cb527-1">A <span>=</span> np.array([[<span>1</span>,<span>2</span>],[<span>3</span>,<span>4</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb527-2"><span>print</span>(<span>"det(A) NumPy:"</span>, np.linalg.det(A))</span></code></pre></div>
<div>
<pre><code>det(A) NumPy: -2.0000000000000004</code></pre>
</div>
</div>
<p>Both SymPy and NumPy confirm the same result.</p>
</section>
<section id="try-it-yourself-50">
<h4 data-anchor-id="try-it-yourself-50">Try It Yourself</h4>
<ol type="1">
<li>Scale a row of a 3×3 matrix by 3. Confirm the determinant scales by 3.</li>
<li>Swap two rows twice in a row - does the determinant return to its original value?</li>
<li>Compute determinant of a triangular matrix. What pattern do you see?</li>
</ol>
</section>
<section id="the-takeaway-34">
<h4 data-anchor-id="the-takeaway-34">The Takeaway</h4>
<ul>
<li>Determinant is defined by multilinearity, sign change, and normalization.</li>
<li>These rules uniquely pin down the determinant’s behavior.</li>
<li>Every formula (cofactor expansion, row-reduction method, etc.) comes from these core principles.</li>
</ul>
</section>
</section>
<section id="determinant-and-row-operations-how-each-move-changes-det">
<h3 data-anchor-id="determinant-and-row-operations-how-each-move-changes-det">53. Determinant and Row Operations (How Each Move Changes det)</h3>
<p>Row operations are at the heart of Gaussian elimination, and the determinant has simple, predictable reactions to them. Understanding these reactions gives both computational shortcuts and geometric intuition.</p>
<section id="the-three-key-rules">
<h4 data-anchor-id="the-three-key-rules">The Three Key Rules</h4>
<ol type="1">
<li>Row swap: Swapping two rows flips the sign of the determinant.</li>
<li>Row scaling: Multiplying a row by a scalar <span>\(c\)</span> multiplies the determinant by <span>\(c\)</span>.</li>
<li>Row replacement: Adding a multiple of one row to another leaves the determinant unchanged.</li>
</ol>
</section>
<section id="set-up-your-lab-52">
<h4 data-anchor-id="set-up-your-lab-52">Set Up Your Lab</h4>
<div id="5f8cb320" data-execution_count="306"><pre><code><span id="cb529-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb529-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-52">
<h4 data-anchor-id="step-by-step-code-walkthrough-52">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Row swap</li>
</ol>
<div id="15012150" data-execution_count="307">
<div id="cb530"><pre><code><span id="cb530-1">A <span>=</span> Matrix([[<span>1</span>,<span>2</span>],[<span>3</span>,<span>4</span>]])</span>
<span id="cb530-2"><span>print</span>(<span>"det(A):"</span>, A.det())</span>
<span id="cb530-3"></span>
<span id="cb530-4">A_swapped <span>=</span> Matrix([[<span>3</span>,<span>4</span>],[<span>1</span>,<span>2</span>]])</span>
<span id="cb530-5"><span>print</span>(<span>"det(after swap):"</span>, A_swapped.det())</span></code></pre></div>
<div>
<pre><code>det(A): -2
det(after swap): 2</code></pre>
</div>
</div>
<p>The result flips sign.</p>
<ol start="2" type="1">
<li>Row scaling</li>
</ol>
<div id="0a416b46" data-execution_count="308">
<div id="cb532"><pre><code><span id="cb532-1">B <span>=</span> Matrix([[<span>1</span>,<span>2</span>],[<span>3</span>,<span>4</span>]])</span>
<span id="cb532-2">B_scaled <span>=</span> Matrix([[<span>2</span>,<span>4</span>],[<span>3</span>,<span>4</span>]])  <span># first row × 2</span></span>
<span id="cb532-3"></span>
<span id="cb532-4"><span>print</span>(<span>"det(B):"</span>, B.det())</span>
<span id="cb532-5"><span>print</span>(<span>"det(after scaling row 1 by 2):"</span>, B_scaled.det())</span></code></pre></div>
<div>
<pre><code>det(B): -2
det(after scaling row 1 by 2): -4</code></pre>
</div>
</div>
<p>Determinant is multiplied by 2.</p>
<ol start="3" type="1">
<li>Row replacement (no change)</li>
</ol>
<div id="1bc15aa2" data-execution_count="309">
<div id="cb534"><pre><code><span id="cb534-1">C <span>=</span> Matrix([[<span>1</span>,<span>2</span>],[<span>3</span>,<span>4</span>]])</span>
<span id="cb534-2">C_replaced <span>=</span> Matrix([[<span>1</span>,<span>2</span>],[<span>3</span><span>-</span><span>2</span><span>*</span><span>1</span>, <span>4</span><span>-</span><span>2</span><span>*</span><span>2</span>]])  <span># row2 → row2 - 2*row1</span></span>
<span id="cb534-3"></span>
<span id="cb534-4"><span>print</span>(<span>"det(C):"</span>, C.det())</span>
<span id="cb534-5"><span>print</span>(<span>"det(after row replacement):"</span>, C_replaced.det())</span></code></pre></div>
<div>
<pre><code>det(C): -2
det(after row replacement): -2</code></pre>
</div>
</div>
<p>Determinant stays the same.</p>
<ol start="4" type="1">
<li>Triangular form shortcut</li>
</ol>
<p>Since elimination only uses row replacement (which doesn’t change the determinant) and row swaps/scales (which we can track), the determinant of a triangular matrix is just the product of its diagonal entries.</p>
<div id="8f802720" data-execution_count="310">
<div id="cb536"><pre><code><span id="cb536-1">D <span>=</span> Matrix([[<span>2</span>,<span>1</span>,<span>3</span>],[<span>0</span>,<span>4</span>,<span>5</span>],[<span>0</span>,<span>0</span>,<span>6</span>]])</span>
<span id="cb536-2"><span>print</span>(<span>"det(D):"</span>, D.det())</span>
<span id="cb536-3"><span>print</span>(<span>"Product of diagonals:"</span>, <span>2</span><span>*</span><span>4</span><span>*</span><span>6</span>)</span></code></pre></div>
<div>
<pre><code>det(D): 48
Product of diagonals: 48</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>NumPy confirmation</li>
</ol>
<div id="aacca2ba" data-execution_count="311">
<div id="cb538"><pre><code><span id="cb538-1">A <span>=</span> np.array([[<span>1</span>,<span>2</span>,<span>3</span>],[<span>0</span>,<span>4</span>,<span>5</span>],[<span>1</span>,<span>0</span>,<span>6</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb538-2"><span>print</span>(<span>"det(A):"</span>, np.linalg.det(A))</span></code></pre></div>
<div>
<pre><code>det(A): 22.000000000000004</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-51">
<h4 data-anchor-id="try-it-yourself-51">Try It Yourself</h4>
<ol type="1">
<li><p>Take</p>
<p><span>\[
\begin{bmatrix} 2 &amp; 3 \\ 4 &amp; 6 \end{bmatrix}
\]</span></p>
<p>and scale the second row by <span>\(\tfrac{1}{2}\)</span>. Compare determinants before and after.</p></li>
<li><p>Do Gaussian elimination on a 3×3 matrix, and track how each row operation changes the determinant.</p></li>
<li><p>Compute determinant by reducing to triangular form and compare with SymPy’s <code>.det()</code>.</p></li>
</ol>
</section>
<section id="the-takeaway-35">
<h4 data-anchor-id="the-takeaway-35">The Takeaway</h4>
<ul>
<li>Determinant reacts predictably to row operations.</li>
<li>Row replacement is “safe” (no change), scaling multiplies by the factor, and swapping flips the sign.</li>
<li>This makes elimination not just a solving tool, but also a method to compute determinants efficiently.</li>
</ul>
</section>
</section>
<section id="triangular-matrices-and-product-of-diagonals-fast-wins">
<h3 data-anchor-id="triangular-matrices-and-product-of-diagonals-fast-wins">54. Triangular Matrices and Product of Diagonals (Fast Wins)</h3>
<p>For triangular matrices (upper or lower), the determinant is simply the product of the diagonal entries. This rule is one of the biggest shortcuts in linear algebra - no expansion or elimination needed.</p>
<section id="why-it-works">
<h4 data-anchor-id="why-it-works">Why It Works</h4>
<ul>
<li>Triangular matrices already look like the end result of Gaussian elimination.</li>
<li>Since row replacement operations don’t change the determinant, what’s left is just the product of the diagonal.</li>
</ul>
</section>
<section id="set-up-your-lab-53">
<h4 data-anchor-id="set-up-your-lab-53">Set Up Your Lab</h4>
<div id="71f0e038" data-execution_count="312"><pre><code><span id="cb540-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb540-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-53">
<h4 data-anchor-id="step-by-step-code-walkthrough-53">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Upper triangular example</li>
</ol>
<div id="8776e95a" data-execution_count="313">
<div id="cb541"><pre><code><span id="cb541-1">A <span>=</span> Matrix([</span>
<span id="cb541-2">    [<span>2</span>,<span>1</span>,<span>3</span>],</span>
<span id="cb541-3">    [<span>0</span>,<span>4</span>,<span>5</span>],</span>
<span id="cb541-4">    [<span>0</span>,<span>0</span>,<span>6</span>]</span>
<span id="cb541-5">])</span>
<span id="cb541-6"></span>
<span id="cb541-7"><span>print</span>(<span>"det(A):"</span>, A.det())</span>
<span id="cb541-8"><span>print</span>(<span>"Product of diagonals:"</span>, <span>2</span><span>*</span><span>4</span><span>*</span><span>6</span>)</span></code></pre></div>
<div>
<pre><code>det(A): 48
Product of diagonals: 48</code></pre>
</div>
</div>
<p>Both values match exactly.</p>
<ol start="2" type="1">
<li>Lower triangular example</li>
</ol>
<div id="287924be" data-execution_count="314">
<div id="cb543"><pre><code><span id="cb543-1">B <span>=</span> Matrix([</span>
<span id="cb543-2">    [<span>7</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb543-3">    [<span>2</span>,<span>5</span>,<span>0</span>],</span>
<span id="cb543-4">    [<span>3</span>,<span>4</span>,<span>9</span>]</span>
<span id="cb543-5">])</span>
<span id="cb543-6"></span>
<span id="cb543-7"><span>print</span>(<span>"det(B):"</span>, B.det())</span>
<span id="cb543-8"><span>print</span>(<span>"Product of diagonals:"</span>, <span>7</span><span>*</span><span>5</span><span>*</span><span>9</span>)</span></code></pre></div>
<div>
<pre><code>det(B): 315
Product of diagonals: 315</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Diagonal matrix (special case)</li>
</ol>
<p>For diagonal matrices, determinant = product of diagonal entries directly.</p>
<div id="3b1859a8" data-execution_count="315">
<div id="cb545"><pre><code><span id="cb545-1">C <span>=</span> Matrix.diag(<span>3</span>,<span>5</span>,<span>7</span>)</span>
<span id="cb545-2"><span>print</span>(<span>"det(C):"</span>, C.det())</span>
<span id="cb545-3"><span>print</span>(<span>"Product of diagonals:"</span>, <span>3</span><span>*</span><span>5</span><span>*</span><span>7</span>)</span></code></pre></div>
<div>
<pre><code>det(C): 105
Product of diagonals: 105</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>NumPy version</li>
</ol>
<div id="5f7c0777" data-execution_count="316">
<div id="cb547"><pre><code><span id="cb547-1">A <span>=</span> np.array([[<span>2</span>,<span>1</span>,<span>3</span>],[<span>0</span>,<span>4</span>,<span>5</span>],[<span>0</span>,<span>0</span>,<span>6</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb547-2"><span>print</span>(<span>"det(A):"</span>, np.linalg.det(A))</span>
<span id="cb547-3"><span>print</span>(<span>"Product of diagonals:"</span>, np.prod(np.diag(A)))</span></code></pre></div>
<div>
<pre><code>det(A): 47.999999999999986
Product of diagonals: 48.0</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Quick elimination to triangular form</li>
</ol>
<p>Even for non-triangular matrices, elimination reduces them to triangular form, where this rule applies.</p>
<div id="158a699d" data-execution_count="317">
<div id="cb549"><pre><code><span id="cb549-1">D <span>=</span> Matrix([[<span>1</span>,<span>2</span>,<span>3</span>],[<span>4</span>,<span>5</span>,<span>6</span>],[<span>7</span>,<span>8</span>,<span>10</span>]])</span>
<span id="cb549-2"><span>print</span>(<span>"det(D) via SymPy:"</span>, D.det())</span>
<span id="cb549-3"><span>print</span>(<span>"det(D) via LU decomposition:"</span>, D.LUdecomposition()[<span>0</span>].det() <span>*</span> D.LUdecomposition()[<span>1</span>].det())</span></code></pre></div>
<div>
<pre><code>det(D) via SymPy: -3
det(D) via LU decomposition: -3</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-52">
<h4 data-anchor-id="try-it-yourself-52">Try It Yourself</h4>
<ol type="1">
<li>Compute the determinant of a 4×4 diagonal matrix quickly.</li>
<li>Verify that triangular matrices with a zero on the diagonal always have determinant 0.</li>
<li>Use SymPy to check that elimination to triangular form preserves determinant (except for swaps/scales).</li>
</ol>
</section>
<section id="the-takeaway-36">
<h4 data-anchor-id="the-takeaway-36">The Takeaway</h4>
<ul>
<li><p>For triangular (and diagonal) matrices:</p>
<p><span>\[
\det(A) = \prod_{i} a_{ii}
\]</span></p></li>
<li><p>This shortcut makes determinant computation trivial.</p></li>
<li><p>Gaussian elimination leverages this fact: once reduced to triangular form, the determinant is just the product of pivots (with sign adjustments for swaps).</p></li>
</ul>
</section>
</section>
<section id="detab-detadetb-multiplicative-magic">
<h3 data-anchor-id="detab-detadetb-multiplicative-magic">55. det(AB) = det(A)det(B) (Multiplicative Magic)</h3>
<p>One of the most elegant properties of determinants is multiplicativity:</p>
<p><span>\[
\det(AB) = \det(A)\,\det(B)
\]</span></p>
<p>This rule is powerful because it connects algebra (matrix multiplication) with geometry (volume scaling).</p>
<section id="geometric-intuition">
<h4 data-anchor-id="geometric-intuition">Geometric Intuition</h4>
<ul>
<li>If <span>\(A\)</span> scales volumes by factor <span>\(\det(A)\)</span>, and <span>\(B\)</span> scales them by <span>\(\det(B)\)</span>, then applying <span>\(B\)</span> followed by <span>\(A\)</span> scales volumes by <span>\(\det(A)\det(B)\)</span>.</li>
<li>This property works in all dimensions.</li>
</ul>
</section>
<section id="set-up-your-lab-54">
<h4 data-anchor-id="set-up-your-lab-54">Set Up Your Lab</h4>
<div id="73e32013" data-execution_count="318"><pre><code><span id="cb551-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb551-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-54">
<h4 data-anchor-id="step-by-step-code-walkthrough-54">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>2×2 example</li>
</ol>
<div id="5ae84434" data-execution_count="319">
<div id="cb552"><pre><code><span id="cb552-1">A <span>=</span> Matrix([[<span>2</span>,<span>1</span>],[<span>0</span>,<span>3</span>]])</span>
<span id="cb552-2">B <span>=</span> Matrix([[<span>1</span>,<span>4</span>],[<span>2</span>,<span>5</span>]])</span>
<span id="cb552-3"></span>
<span id="cb552-4">detA <span>=</span> A.det()</span>
<span id="cb552-5">detB <span>=</span> B.det()</span>
<span id="cb552-6">detAB <span>=</span> (A<span>*</span>B).det()</span>
<span id="cb552-7"></span>
<span id="cb552-8"><span>print</span>(<span>"det(A):"</span>, detA)</span>
<span id="cb552-9"><span>print</span>(<span>"det(B):"</span>, detB)</span>
<span id="cb552-10"><span>print</span>(<span>"det(AB):"</span>, detAB)</span>
<span id="cb552-11"><span>print</span>(<span>"det(A)*det(B):"</span>, detA<span>*</span>detB)</span></code></pre></div>
<div>
<pre><code>det(A): 6
det(B): -3
det(AB): -18
det(A)*det(B): -18</code></pre>
</div>
</div>
<p>The two results match.</p>
<ol start="2" type="1">
<li>3×3 random matrix check</li>
</ol>
<div id="2508f6be" data-execution_count="320">
<div id="cb554"><pre><code><span id="cb554-1">np.random.seed(<span>1</span>)</span>
<span id="cb554-2">A <span>=</span> Matrix(np.random.randint(<span>-</span><span>3</span>,<span>4</span>,(<span>3</span>,<span>3</span>)))</span>
<span id="cb554-3">B <span>=</span> Matrix(np.random.randint(<span>-</span><span>3</span>,<span>4</span>,(<span>3</span>,<span>3</span>)))</span>
<span id="cb554-4"></span>
<span id="cb554-5"><span>print</span>(<span>"det(A):"</span>, A.det())</span>
<span id="cb554-6"><span>print</span>(<span>"det(B):"</span>, B.det())</span>
<span id="cb554-7"><span>print</span>(<span>"det(AB):"</span>, (A<span>*</span>B).det())</span>
<span id="cb554-8"><span>print</span>(<span>"det(A)*det(B):"</span>, A.det()<span>*</span>B.det())</span></code></pre></div>
<div>
<pre><code>det(A): 25
det(B): -15
det(AB): -375
det(A)*det(B): -375</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Special cases</li>
</ol>
<ul>
<li>If <span>\(\det(A)=0\)</span>, then <span>\(\det(AB)=0\)</span>.</li>
<li>If <span>\(\det(A)=\pm1\)</span>, it acts like a “volume-preserving” transformation (rotation/reflection).</li>
</ul>
<div id="c0f48a84" data-execution_count="321"><pre><code><span id="cb556-1">A <span>=</span> Matrix([[<span>1</span>,<span>0</span>],[<span>0</span>,<span>0</span>]])  <span># singular</span></span>
<span id="cb556-2">B <span>=</span> Matrix([[<span>2</span>,<span>3</span>],[<span>4</span>,<span>5</span>]])</span>
<span id="cb556-3"></span>
<span id="cb556-4"><span>print</span>(<span>"det(A):"</span>, A.det())</span>
<span id="cb556-5"><span>print</span>(<span>"det(AB):"</span>, (A<span>*</span>B).det())</span></code></pre></div>
<p>Both are 0.</p>
<ol start="4" type="1">
<li>NumPy version</li>
</ol>
<div id="0ef4be24" data-execution_count="322">
<div id="cb558"><pre><code><span id="cb558-1">A <span>=</span> np.array([[<span>2</span>,<span>1</span>],[<span>0</span>,<span>3</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb558-2">B <span>=</span> np.array([[<span>1</span>,<span>4</span>],[<span>2</span>,<span>5</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb558-3"></span>
<span id="cb558-4">lhs <span>=</span> np.linalg.det(A <span>@</span> B)</span>
<span id="cb558-5">rhs <span>=</span> np.linalg.det(A) <span>*</span> np.linalg.det(B)</span>
<span id="cb558-6"></span>
<span id="cb558-7"><span>print</span>(<span>"det(AB) ="</span>, lhs)</span>
<span id="cb558-8"><span>print</span>(<span>"det(A)*det(B) ="</span>, rhs)</span></code></pre></div>
<div>
<pre><code>det(AB) = -17.999999999999996
det(A)*det(B) = -17.999999999999996</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-53">
<h4 data-anchor-id="try-it-yourself-53">Try It Yourself</h4>
<ol type="1">
<li>Construct two triangular matrices and verify multiplicativity (diagonal products multiply too).</li>
<li>Test the property with an orthogonal matrix <span>\(Q\)</span> (<span>\(\det(Q)=\pm 1\)</span>). What happens?</li>
<li>Try with one matrix singular - confirm the product is always singular.</li>
</ol>
</section>
<section id="the-takeaway-37">
<h4 data-anchor-id="the-takeaway-37">The Takeaway</h4>
<ul>
<li>Determinant is multiplicative, not additive.</li>
<li><span>\(\det(AB) = \det(A)\det(B)\)</span> is a cornerstone identity in linear algebra.</li>
<li>This property connects geometry (volume scaling) with algebra (matrix multiplication).</li>
</ul>
</section>
</section>
<section id="invertibility-and-zero-determinant-flat-vs.-full-volume">
<h3 data-anchor-id="invertibility-and-zero-determinant-flat-vs.-full-volume">56. Invertibility and Zero Determinant (Flat vs.&nbsp;Full Volume)</h3>
<p>The determinant gives a quick test for invertibility:</p>
<ul>
<li>If <span>\(\det(A) \neq 0\)</span>, the matrix is invertible.</li>
<li>If <span>\(\det(A) = 0\)</span>, the matrix is singular (non-invertible).</li>
</ul>
<p>Geometrically:</p>
<ul>
<li>Nonzero determinant → transformation keeps full dimension (no collapse).</li>
<li>Zero determinant → transformation flattens space into a lower dimension (volume = 0).</li>
</ul>
<section id="set-up-your-lab-55">
<h4 data-anchor-id="set-up-your-lab-55">Set Up Your Lab</h4>
<div id="5003767c" data-execution_count="323"><pre><code><span id="cb560-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb560-2"><span>from</span> sympy <span>import</span> Matrix</span>
<span id="cb560-3"><span>from</span> sympy.matrices.common <span>import</span> NonInvertibleMatrixError</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-55">
<h4 data-anchor-id="step-by-step-code-walkthrough-55">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Invertible example</li>
</ol>
<div id="495c8043" data-execution_count="324">
<div id="cb561"><pre><code><span id="cb561-1">A <span>=</span> Matrix([[<span>2</span>,<span>1</span>],[<span>5</span>,<span>3</span>]])</span>
<span id="cb561-2"><span>print</span>(<span>"det(A):"</span>, A.det())</span>
<span id="cb561-3"><span>print</span>(<span>"Inverse exists?"</span>, A.det() <span>!=</span> <span>0</span>)</span>
<span id="cb561-4"><span>print</span>(<span>"A inverse:</span><span>\n</span><span>"</span>, A.inv())</span></code></pre></div>
<div>
<pre><code>det(A): 1
Inverse exists? True
A inverse:
 Matrix([[3, -1], [-5, 2]])</code></pre>
</div>
</div>
<p>The determinant is nonzero → invertible.</p>
<ol start="2" type="1">
<li>Singular example (zero determinant)</li>
</ol>
<div id="99cad634" data-execution_count="325">
<div id="cb563"><pre><code><span id="cb563-1">B <span>=</span> Matrix([[<span>1</span>,<span>2</span>],[<span>2</span>,<span>4</span>]])</span>
<span id="cb563-2"><span>print</span>(<span>"det(B):"</span>, B.det())</span>
<span id="cb563-3"><span>print</span>(<span>"Inverse exists?"</span>, B.det() <span>!=</span> <span>0</span>)</span></code></pre></div>
<div>
<pre><code>det(B): 0
Inverse exists? False</code></pre>
</div>
</div>
<p>Since the second row is a multiple of the first, determinant = 0 → no inverse.</p>
<ol start="3" type="1">
<li>Solving systems with determinant check</li>
</ol>
<p>If <span>\(\det(A)=0\)</span>, the system <span>\(Ax=b\)</span> may have no solutions or infinitely many.</p>
<div id="155ea5d1" data-execution_count="326">
<div id="cb565"><pre><code><span id="cb565-1"><span># 3. Solving systems with determinant check</span></span>
<span id="cb565-2">b <span>=</span> Matrix([<span>1</span>,<span>2</span>])</span>
<span id="cb565-3"><span>try</span>:</span>
<span id="cb565-4">    <span>print</span>(<span>"Solve Ax=b with singular B:"</span>, B.solve(b))</span>
<span id="cb565-5"><span>except</span> NonInvertibleMatrixError <span>as</span> e:</span>
<span id="cb565-6">    <span>print</span>(<span>"Error when solving Ax=b:"</span>, e)</span></code></pre></div>
<div>
<pre><code>Error when solving Ax=b: Matrix det == 0; not invertible.</code></pre>
</div>
</div>
<p>SymPy indicates inconsistency or multiple solutions.</p>
<ol start="4" type="1">
<li>Higher-dimensional example</li>
</ol>
<div id="26c78cdc" data-execution_count="327">
<div id="cb567"><pre><code><span id="cb567-1">C <span>=</span> Matrix([</span>
<span id="cb567-2">    [<span>1</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb567-3">    [<span>0</span>,<span>2</span>,<span>0</span>],</span>
<span id="cb567-4">    [<span>0</span>,<span>0</span>,<span>3</span>]</span>
<span id="cb567-5">])</span>
<span id="cb567-6"><span>print</span>(<span>"det(C):"</span>, C.det())</span>
<span id="cb567-7"><span>print</span>(<span>"Invertible?"</span>, C.det() <span>!=</span> <span>0</span>)</span></code></pre></div>
<div>
<pre><code>det(C): 6
Invertible? True</code></pre>
</div>
</div>
<p>Diagonal entries all nonzero → invertible.</p>
<ol start="5" type="1">
<li>NumPy version</li>
</ol>
<div id="287a6afc" data-execution_count="328">
<div id="cb569"><pre><code><span id="cb569-1">A <span>=</span> np.array([[<span>2</span>,<span>1</span>],[<span>5</span>,<span>3</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb569-2"><span>print</span>(<span>"det(A):"</span>, np.linalg.det(A))</span>
<span id="cb569-3"><span>print</span>(<span>"Inverse:</span><span>\n</span><span>"</span>, np.linalg.inv(A))</span>
<span id="cb569-4"></span>
<span id="cb569-5">B <span>=</span> np.array([[<span>1</span>,<span>2</span>],[<span>2</span>,<span>4</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb569-6"><span>print</span>(<span>"det(B):"</span>, np.linalg.det(B))</span>
<span id="cb569-7"><span># np.linalg.inv(B) would fail because det=0</span></span></code></pre></div>
<div>
<pre><code>det(A): 1.0000000000000002
Inverse:
 [[ 3. -1.]
 [-5.  2.]]
det(B): 0.0</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-54">
<h4 data-anchor-id="try-it-yourself-54">Try It Yourself</h4>
<ol type="1">
<li>Build a 3×3 matrix with determinant 0 by making one row a multiple of another. Confirm singularity.</li>
<li>Generate a random 4×4 matrix and check whether it’s invertible using <code>.det()</code>.</li>
<li>Test if two different 2×2 matrices are invertible, then multiply them together - is the product invertible too?</li>
</ol>
</section>
<section id="the-takeaway-38">
<h4 data-anchor-id="the-takeaway-38">The Takeaway</h4>
<ul>
<li><span>\(\det(A) \neq 0 \implies\)</span> invertible (full volume).</li>
<li><span>\(\det(A) = 0 \implies\)</span> singular (space collapsed).</li>
<li>Determinant gives both algebraic and geometric insight into when a matrix is reversible.</li>
</ul>
</section>
</section>
<section id="cofactor-expansion-laplaces-method">
<h3 data-anchor-id="cofactor-expansion-laplaces-method">57. Cofactor Expansion (Laplace’s Method)</h3>
<p>The cofactor expansion is a systematic way to compute determinants using minors. It’s not efficient for large matrices, but it reveals the recursive structure of determinants.</p>
<section id="definition">
<h4 data-anchor-id="definition">Definition</h4>
<p>For an <span>\(n \times n\)</span> matrix <span>\(A\)</span>,</p>
<p><span>\[
\det(A) = \sum_{j=1}^{n} (-1)^{i+j} a_{ij} \det(M_{ij})
\]</span></p>
<p>where:</p>
<ul>
<li><span>\(i\)</span> = chosen row (or column),</li>
<li><span>\(M_{ij}\)</span> = minor matrix after removing row <span>\(i\)</span>, column <span>\(j\)</span>.</li>
</ul>
</section>
<section id="set-up-your-lab-56">
<h4 data-anchor-id="set-up-your-lab-56">Set Up Your Lab</h4>
<div id="454a38ee" data-execution_count="329"><pre><code><span id="cb571-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb571-2"><span>from</span> sympy <span>import</span> Matrix, symbols</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-56">
<h4 data-anchor-id="step-by-step-code-walkthrough-56">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>2×2 case (base rule)</li>
</ol>
<div id="08e9ecac" data-execution_count="330">
<div id="cb572"><pre><code><span id="cb572-1"><span># declare symbols</span></span>
<span id="cb572-2">a, b, c, d <span>=</span> symbols(<span>'a b c d'</span>)</span>
<span id="cb572-3"></span>
<span id="cb572-4"><span># build the matrix</span></span>
<span id="cb572-5">A <span>=</span> Matrix([[a, b],[c, d]])</span>
<span id="cb572-6"></span>
<span id="cb572-7"><span># compute determinant</span></span>
<span id="cb572-8">detA <span>=</span> A.det()</span>
<span id="cb572-9"><span>print</span>(<span>"Determinant 2x2:"</span>, detA)</span></code></pre></div>
<div>
<pre><code>Determinant 2x2: a*d - b*c</code></pre>
</div>
</div>
<p>Formula: <span>\(\det(A) = ad - bc\)</span>.</p>
<ol start="2" type="1">
<li>3×3 example using cofactor expansion</li>
</ol>
<div id="02cf46c6" data-execution_count="331"><pre><code><span id="cb574-1">A <span>=</span> Matrix([</span>
<span id="cb574-2">    [<span>1</span>,<span>2</span>,<span>3</span>],</span>
<span id="cb574-3">    [<span>4</span>,<span>5</span>,<span>6</span>],</span>
<span id="cb574-4">    [<span>7</span>,<span>8</span>,<span>9</span>]</span>
<span id="cb574-5">])</span>
<span id="cb574-6"></span>
<span id="cb574-7">detA <span>=</span> A.det()</span>
<span id="cb574-8"><span>print</span>(<span>"Determinant via SymPy:"</span>, detA)</span></code></pre></div>
<p>Let’s compute manually along the first row:</p>
<div id="64497be9" data-execution_count="332">
<div id="cb576"><pre><code><span id="cb576-1">cofactor_expansion <span>=</span> (</span>
<span id="cb576-2">    <span>1</span> <span>*</span> Matrix([[<span>5</span>,<span>6</span>],[<span>8</span>,<span>9</span>]]).det()</span>
<span id="cb576-3">    <span>-</span> <span>2</span> <span>*</span> Matrix([[<span>4</span>,<span>6</span>],[<span>7</span>,<span>9</span>]]).det()</span>
<span id="cb576-4">    <span>+</span> <span>3</span> <span>*</span> Matrix([[<span>4</span>,<span>5</span>],[<span>7</span>,<span>8</span>]]).det()</span>
<span id="cb576-5">)</span>
<span id="cb576-6"><span>print</span>(<span>"Cofactor expansion result:"</span>, cofactor_expansion)</span></code></pre></div>
<div>
<pre><code>Cofactor expansion result: 0</code></pre>
</div>
</div>
<p>Both match (here = 0 because rows are dependent).</p>
<ol start="3" type="1">
<li>Expansion along different rows/columns</li>
</ol>
<p>The result is the same no matter which row/column you expand along.</p>
<div id="118c6339" data-execution_count="333">
<div id="cb578"><pre><code><span id="cb578-1">cofactor_col1 <span>=</span> (</span>
<span id="cb578-2">    <span>1</span> <span>*</span> Matrix([[<span>2</span>,<span>3</span>],[<span>8</span>,<span>9</span>]]).det()</span>
<span id="cb578-3">    <span>-</span> <span>4</span> <span>*</span> Matrix([[<span>2</span>,<span>3</span>],[<span>5</span>,<span>6</span>]]).det()</span>
<span id="cb578-4">    <span>+</span> <span>7</span> <span>*</span> Matrix([[<span>2</span>,<span>3</span>],[<span>5</span>,<span>6</span>]]).det()</span>
<span id="cb578-5">)</span>
<span id="cb578-6"><span>print</span>(<span>"Expansion along col1:"</span>, cofactor_col1)</span></code></pre></div>
<div>
<pre><code>Expansion along col1: -15</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Larger example (4×4)</li>
</ol>
<div id="27a15c1b" data-execution_count="334"><pre><code><span id="cb580-1">B <span>=</span> Matrix([</span>
<span id="cb580-2">    [<span>2</span>,<span>0</span>,<span>1</span>,<span>3</span>],</span>
<span id="cb580-3">    [<span>1</span>,<span>2</span>,<span>0</span>,<span>4</span>],</span>
<span id="cb580-4">    [<span>0</span>,<span>1</span>,<span>1</span>,<span>0</span>],</span>
<span id="cb580-5">    [<span>3</span>,<span>0</span>,<span>2</span>,<span>1</span>]</span>
<span id="cb580-6">])</span>
<span id="cb580-7"></span>
<span id="cb580-8"><span>print</span>(<span>"Determinant 4x4:"</span>, B.det())</span></code></pre></div>
<p>SymPy handles it directly, but conceptually it’s still the same recursive expansion.</p>
<ol start="5" type="1">
<li>NumPy vs SymPy</li>
</ol>
<div id="31b9b26e" data-execution_count="335"><pre><code><span id="cb582-1">B_np <span>=</span> np.array([[<span>2</span>,<span>0</span>,<span>1</span>,<span>3</span>],[<span>1</span>,<span>2</span>,<span>0</span>,<span>4</span>],[<span>0</span>,<span>1</span>,<span>1</span>,<span>0</span>],[<span>3</span>,<span>0</span>,<span>2</span>,<span>1</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb582-2"><span>print</span>(<span>"NumPy determinant:"</span>, np.linalg.det(B_np))</span></code></pre></div>
</section>
<section id="try-it-yourself-55">
<h4 data-anchor-id="try-it-yourself-55">Try It Yourself</h4>
<ol type="1">
<li>Compute a 3×3 determinant manually using cofactor expansion and confirm with <code>.det()</code>.</li>
<li>Expand along a different row and check that the result is unchanged.</li>
<li>Build a 4×4 diagonal matrix and expand it - what simplification do you notice?</li>
</ol>
</section>
<section id="the-takeaway-39">
<h4 data-anchor-id="the-takeaway-39">The Takeaway</h4>
<ul>
<li>Cofactor expansion defines determinant recursively.</li>
<li>Works on any row or column, with consistent results.</li>
<li>Important for proofs and theory, though not practical for computation on large matrices.</li>
</ul>
</section>
</section>
<section id="permutations-and-sign-the-combinatorial-core">
<h3 data-anchor-id="permutations-and-sign-the-combinatorial-core">58. Permutations and Sign (The Combinatorial Core)</h3>
<p>The determinant can also be defined using permutations of indices. This looks abstract, but it’s the most fundamental definition:</p>
<p><span>\[
\det(A) = \sum_{\sigma \in S_n} \text{sgn}(\sigma) \prod_{i=1}^n a_{i,\sigma(i)}
\]</span></p>
<ul>
<li><span>\(S_n\)</span> = set of all permutations of <span>\(\{1,\dots,n\}\)</span></li>
<li><span>\(\text{sgn}(\sigma)\)</span> = +1 if the permutation is even, -1 if odd</li>
<li>Each term = one product of entries, one from each row and column</li>
</ul>
<p>This formula explains why determinants mix signs, why row swaps flip the determinant, and why dependence kills it.</p>
<section id="set-up-your-lab-57">
<h4 data-anchor-id="set-up-your-lab-57">Set Up Your Lab</h4>
<div id="c8b61d11" data-execution_count="336"><pre><code><span id="cb584-1"><span>import</span> itertools</span>
<span id="cb584-2"><span>import</span> numpy <span>as</span> np</span>
<span id="cb584-3"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-57">
<h4 data-anchor-id="step-by-step-code-walkthrough-57">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Determinant by permutation expansion (3×3)</li>
</ol>
<div id="ee9092fb" data-execution_count="337">
<div id="cb585"><pre><code><span id="cb585-1"><span>def</span> determinant_permutation(A):</span>
<span id="cb585-2">    n <span>=</span> A.shape[<span>0</span>]</span>
<span id="cb585-3">    total <span>=</span> <span>0</span></span>
<span id="cb585-4">    <span>for</span> perm <span>in</span> itertools.permutations(<span>range</span>(n)):</span>
<span id="cb585-5">        sign <span>=</span> (<span>-</span><span>1</span>)<span>**</span>(<span>sum</span>(<span>1</span> <span>for</span> i <span>in</span> <span>range</span>(n) <span>for</span> j <span>in</span> <span>range</span>(i) <span>if</span> perm[j] <span>&gt;</span> perm[i]))</span>
<span id="cb585-6">        product <span>=</span> <span>1</span></span>
<span id="cb585-7">        <span>for</span> i <span>in</span> <span>range</span>(n):</span>
<span id="cb585-8">            product <span>*=</span> A[i, perm[i]]</span>
<span id="cb585-9">        total <span>+=</span> sign <span>*</span> product</span>
<span id="cb585-10">    <span>return</span> total</span>
<span id="cb585-11"></span>
<span id="cb585-12">A <span>=</span> np.array([[<span>1</span>,<span>2</span>,<span>3</span>],</span>
<span id="cb585-13">              [<span>4</span>,<span>5</span>,<span>6</span>],</span>
<span id="cb585-14">              [<span>7</span>,<span>8</span>,<span>9</span>]])</span>
<span id="cb585-15"></span>
<span id="cb585-16"><span>print</span>(<span>"Permutation formula det:"</span>, determinant_permutation(A))</span>
<span id="cb585-17"><span>print</span>(<span>"NumPy det:"</span>, np.linalg.det(A))</span></code></pre></div>
<div>
<pre><code>Permutation formula det: 0
NumPy det: -9.51619735392994e-16</code></pre>
</div>
</div>
<p>Both results ≈ 0, since rows are dependent.</p>
<ol start="2" type="1">
<li>Count permutations</li>
</ol>
<p>For <span>\(n=3\)</span>, there are <span>\(3! = 6\)</span> terms:</p>
<p><span>\[
\det(A) = a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32}  
- a_{13}a_{22}a_{31} - a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33}
\]</span></p>
<p>You can see the alternating signs explicitly.</p>
<ol start="3" type="1">
<li>Verification with SymPy</li>
</ol>
<div id="eeb431c5" data-execution_count="338"><pre><code><span id="cb587-1">M <span>=</span> Matrix([[<span>2</span>,<span>1</span>,<span>0</span>],</span>
<span id="cb587-2">            [<span>1</span>,<span>3</span>,<span>4</span>],</span>
<span id="cb587-3">            [<span>0</span>,<span>2</span>,<span>5</span>]])</span>
<span id="cb587-4"><span>print</span>(<span>"SymPy det:"</span>, M.det())</span></code></pre></div>
<p>Matches the permutation expansion.</p>
<ol start="4" type="1">
<li>Growth of terms</li>
</ol>
<ul>
<li>2×2 → 2 terms</li>
<li>3×3 → 6 terms</li>
<li>4×4 → 24 terms</li>
<li><span>\(n\)</span> → <span>\(n!\)</span> terms (factorial growth!)</li>
</ul>
<p>This is why cofactor or LU is preferred computationally.</p>
</section>
<section id="try-it-yourself-56">
<h4 data-anchor-id="try-it-yourself-56">Try It Yourself</h4>
<ol type="1">
<li>Write out the 2×2 permutation formula explicitly and check it equals <span>\(ad - bc\)</span>.</li>
<li>Expand a 3×3 determinant by hand using the six terms.</li>
<li>Modify the code to count how many multiplications are required for a 5×5 matrix using the permutation definition.</li>
</ol>
</section>
<section id="the-takeaway-40">
<h4 data-anchor-id="the-takeaway-40">The Takeaway</h4>
<ul>
<li>Determinant = signed sum over all permutations.</li>
<li>Signs come from permutation parity (even/odd swaps).</li>
<li>This definition is the combinatorial foundation that unifies all determinant properties.</li>
</ul>
</section>
</section>
<section id="cramers-rule-solving-with-determinants-and-when-not-to-use-it">
<h3 data-anchor-id="cramers-rule-solving-with-determinants-and-when-not-to-use-it">59. Cramer’s Rule (Solving with Determinants, and When Not to Use It)</h3>
<p>Cramer’s Rule gives an explicit formula for solving a system of linear equations <span>\(Ax = b\)</span> using determinants. It is elegant but inefficient for large systems.</p>
<p>For <span>\(A \in \mathbb{R}^{n \times n}\)</span> with <span>\(\det(A) \neq 0\)</span>:</p>
<p><span>\[
x_i = \frac{\det(A_i)}{\det(A)}
\]</span></p>
<p>where <span>\(A_i\)</span> is <span>\(A\)</span> with its <span>\(i\)</span>-th column replaced by <span>\(b\)</span>.</p>
<section id="set-up-your-lab-58">
<h4 data-anchor-id="set-up-your-lab-58">Set Up Your Lab</h4>
<div id="f4aec65f" data-execution_count="339"><pre><code><span id="cb589-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb589-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-58">
<h4 data-anchor-id="step-by-step-code-walkthrough-58">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Simple 2×2 example</li>
</ol>
<p>Solve:</p>
<p><span>\[
\begin{cases}
2x + y = 5 \\
x - y = 1
\end{cases}
\]</span></p>
<div id="9d98db65" data-execution_count="340">
<div id="cb590"><pre><code><span id="cb590-1">A <span>=</span> Matrix([[<span>2</span>,<span>1</span>],[<span>1</span>,<span>-</span><span>1</span>]])</span>
<span id="cb590-2">b <span>=</span> Matrix([<span>5</span>,<span>1</span>])</span>
<span id="cb590-3"></span>
<span id="cb590-4">detA <span>=</span> A.det()</span>
<span id="cb590-5"><span>print</span>(<span>"det(A):"</span>, detA)</span>
<span id="cb590-6"></span>
<span id="cb590-7"><span># Replace columns</span></span>
<span id="cb590-8">A1 <span>=</span> A.copy()</span>
<span id="cb590-9">A1[:,<span>0</span>] <span>=</span> b</span>
<span id="cb590-10">A2 <span>=</span> A.copy()</span>
<span id="cb590-11">A2[:,<span>1</span>] <span>=</span> b</span>
<span id="cb590-12"></span>
<span id="cb590-13">x1 <span>=</span> A1.det() <span>/</span> detA</span>
<span id="cb590-14">x2 <span>=</span> A2.det() <span>/</span> detA</span>
<span id="cb590-15"><span>print</span>(<span>"Solution via Cramer's Rule:"</span>, [x1, x2])</span>
<span id="cb590-16"></span>
<span id="cb590-17"><span># Check with built-in solver</span></span>
<span id="cb590-18"><span>print</span>(<span>"SymPy solve:"</span>, A.LUsolve(b))</span></code></pre></div>
<div>
<pre><code>det(A): -3
Solution via Cramer's Rule: [2, 1]
SymPy solve: Matrix([[2], [1]])</code></pre>
</div>
</div>
<p>Both give the same solution.</p>
<ol start="2" type="1">
<li>3×3 example</li>
</ol>
<div id="5b0995a9" data-execution_count="341">
<div id="cb592"><pre><code><span id="cb592-1">A <span>=</span> Matrix([</span>
<span id="cb592-2">    [<span>1</span>,<span>2</span>,<span>3</span>],</span>
<span id="cb592-3">    [<span>0</span>,<span>1</span>,<span>4</span>],</span>
<span id="cb592-4">    [<span>5</span>,<span>6</span>,<span>0</span>]</span>
<span id="cb592-5">])</span>
<span id="cb592-6">b <span>=</span> Matrix([<span>7</span>,<span>8</span>,<span>9</span>])</span>
<span id="cb592-7"></span>
<span id="cb592-8">detA <span>=</span> A.det()</span>
<span id="cb592-9"><span>print</span>(<span>"det(A):"</span>, detA)</span>
<span id="cb592-10"></span>
<span id="cb592-11">solutions <span>=</span> []</span>
<span id="cb592-12"><span>for</span> i <span>in</span> <span>range</span>(A.shape[<span>1</span>]):</span>
<span id="cb592-13">    Ai <span>=</span> A.copy()</span>
<span id="cb592-14">    Ai[:,i] <span>=</span> b</span>
<span id="cb592-15">    solutions.append(Ai.det()<span>/</span>detA)</span>
<span id="cb592-16"></span>
<span id="cb592-17"><span>print</span>(<span>"Solution via Cramer's Rule:"</span>, solutions)</span>
<span id="cb592-18"><span>print</span>(<span>"SymPy solve:"</span>, A.LUsolve(b))</span></code></pre></div>
<div>
<pre><code>det(A): 1
Solution via Cramer's Rule: [21, -16, 6]
SymPy solve: Matrix([[21], [-16], [6]])</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>NumPy version (inefficient but illustrative)</li>
</ol>
<div id="7a0855f9" data-execution_count="342">
<div id="cb594"><pre><code><span id="cb594-1">A <span>=</span> np.array([[<span>2</span>,<span>1</span>],[<span>1</span>,<span>-</span><span>1</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb594-2">b <span>=</span> np.array([<span>5</span>,<span>1</span>], dtype<span>=</span><span>float</span>)</span>
<span id="cb594-3"></span>
<span id="cb594-4">detA <span>=</span> np.linalg.det(A)</span>
<span id="cb594-5"></span>
<span id="cb594-6">solutions <span>=</span> []</span>
<span id="cb594-7"><span>for</span> i <span>in</span> <span>range</span>(A.shape[<span>1</span>]):</span>
<span id="cb594-8">    Ai <span>=</span> A.copy()</span>
<span id="cb594-9">    Ai[:,i] <span>=</span> b</span>
<span id="cb594-10">    solutions.append(np.linalg.det(Ai)<span>/</span>detA)</span>
<span id="cb594-11"></span>
<span id="cb594-12"><span>print</span>(<span>"Solution:"</span>, solutions)</span></code></pre></div>
<div>
<pre><code>Solution: [np.float64(2.0000000000000004), np.float64(1.0)]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Why not use it in practice?</li>
</ol>
<ul>
<li>Requires computing <span>\(n+1\)</span> determinants.</li>
<li>Determinant computation via cofactor expansion is factorial-time.</li>
<li>Gaussian elimination or LU is far more efficient.</li>
</ul>
</section>
<section id="try-it-yourself-57">
<h4 data-anchor-id="try-it-yourself-57">Try It Yourself</h4>
<ol type="1">
<li>Solve a 3×3 system using Cramer’s Rule and confirm with <code>A.solve(b)</code>.</li>
<li>Try Cramer’s Rule when <span>\(\det(A)=0\)</span>. What happens?</li>
<li>Compare runtime of Cramer’s Rule vs LU for a random 5×5 matrix.</li>
</ol>
</section>
<section id="the-takeaway-41">
<h4 data-anchor-id="the-takeaway-41">The Takeaway</h4>
<ul>
<li>Cramer’s Rule gives explicit formulas for solutions using determinants.</li>
<li>Beautiful for theory, useful for small cases, but not computationally practical.</li>
<li>It highlights the deep connection between determinants and solving linear systems.</li>
</ul>
</section>
</section>
<section id="computing-determinants-in-practice-use-lu-mind-stability">
<h3 data-anchor-id="computing-determinants-in-practice-use-lu-mind-stability">60. Computing Determinants in Practice (Use LU, Mind Stability)</h3>
<p>While definitions like cofactor expansion and permutations are beautiful, they are too slow for large matrices. In practice, determinants are computed using row reduction or LU decomposition, with careful attention to numerical stability.</p>
<section id="set-up-your-lab-59">
<h4 data-anchor-id="set-up-your-lab-59">Set Up Your Lab</h4>
<div id="d68bd293" data-execution_count="343"><pre><code><span id="cb596-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb596-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-59">
<h4 data-anchor-id="step-by-step-code-walkthrough-59">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Cofactor expansion is too slow</li>
</ol>
<div id="9ab3ce3b" data-execution_count="344">
<div id="cb597"><pre><code><span id="cb597-1">A <span>=</span> Matrix([</span>
<span id="cb597-2">    [<span>1</span>,<span>2</span>,<span>3</span>],</span>
<span id="cb597-3">    [<span>4</span>,<span>5</span>,<span>6</span>],</span>
<span id="cb597-4">    [<span>7</span>,<span>8</span>,<span>10</span>]</span>
<span id="cb597-5">])</span>
<span id="cb597-6"><span>print</span>(<span>"det via cofactor expansion:"</span>, A.det())</span></code></pre></div>
<div>
<pre><code>det via cofactor expansion: -3</code></pre>
</div>
</div>
<p>This works for 3×3, but complexity grows factorially.</p>
<ol start="2" type="1">
<li>Determinant via triangular form (LU decomposition)</li>
</ol>
<p>LU decomposition factorizes <span>\(A = LU\)</span>, where <span>\(L\)</span> is lower triangular and <span>\(U\)</span> is upper triangular. Determinant = product of diagonals of <span>\(U\)</span>, up to sign corrections for row swaps.</p>
<div id="a2d32cbb" data-execution_count="345">
<div id="cb599"><pre><code><span id="cb599-1">L, U, perm <span>=</span> A.LUdecomposition()</span>
<span id="cb599-2">detA <span>=</span> A.det()</span>
<span id="cb599-3"><span>print</span>(<span>"L:</span><span>\n</span><span>"</span>, L)</span>
<span id="cb599-4"><span>print</span>(<span>"U:</span><span>\n</span><span>"</span>, U)</span>
<span id="cb599-5"><span>print</span>(<span>"Permutation matrix:</span><span>\n</span><span>"</span>, perm)</span>
<span id="cb599-6"><span>print</span>(<span>"det via LU product:"</span>, detA)</span></code></pre></div>
<div>
<pre><code>L:
 Matrix([[1, 0, 0], [4, 1, 0], [7, 2, 1]])
U:
 Matrix([[1, 2, 3], [0, -3, -6], [0, 0, 1]])
Permutation matrix:
 []
det via LU product: -3</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>NumPy efficient method</li>
</ol>
<div id="c933a5fa" data-execution_count="346">
<div id="cb601"><pre><code><span id="cb601-1">A_np <span>=</span> np.array([[<span>1</span>,<span>2</span>,<span>3</span>],[<span>4</span>,<span>5</span>,<span>6</span>],[<span>7</span>,<span>8</span>,<span>10</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb601-2"><span>print</span>(<span>"NumPy det:"</span>, np.linalg.det(A_np))</span></code></pre></div>
<div>
<pre><code>NumPy det: -3.000000000000001</code></pre>
</div>
</div>
<p>NumPy uses optimized routines (LAPACK under the hood).</p>
<ol start="4" type="1">
<li>Large random matrix</li>
</ol>
<div id="64550d72" data-execution_count="347">
<div id="cb603"><pre><code><span id="cb603-1">np.random.seed(<span>0</span>)</span>
<span id="cb603-2">B <span>=</span> np.random.rand(<span>5</span>,<span>5</span>)</span>
<span id="cb603-3"><span>print</span>(<span>"NumPy det (5x5):"</span>, np.linalg.det(B))</span></code></pre></div>
<div>
<pre><code>NumPy det (5x5): 0.009658225505885114</code></pre>
</div>
</div>
<p>Computes quickly even for larger matrices.</p>
<ol start="5" type="1">
<li>Stability issues</li>
</ol>
<p>Determinants of large or ill-conditioned matrices can suffer from floating-point errors. For example, if rows are nearly dependent:</p>
<div id="61363ba8" data-execution_count="348">
<div id="cb605"><pre><code><span id="cb605-1">C <span>=</span> np.array([[<span>1</span>,<span>2</span>,<span>3</span>],[<span>2</span>,<span>4.0000001</span>,<span>6</span>],[<span>3</span>,<span>6</span>,<span>9</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb605-2"><span>print</span>(<span>"det(C):"</span>, np.linalg.det(C))</span></code></pre></div>
<div>
<pre><code>det(C): -4.996003624823549e-23</code></pre>
</div>
</div>
<p>The result may not be exactly 0 due to floating-point approximations.</p>
</section>
<section id="try-it-yourself-58">
<h4 data-anchor-id="try-it-yourself-58">Try It Yourself</h4>
<ol type="1">
<li>Compute the determinant of a random 10×10 matrix with <code>np.linalg.det</code>.</li>
<li>Compare results between SymPy (exact rational arithmetic) and NumPy (floating-point).</li>
<li>Test determinant of a nearly singular matrix - notice numerical instability.</li>
</ol>
</section>
<section id="the-takeaway-42">
<h4 data-anchor-id="the-takeaway-42">The Takeaway</h4>
<ul>
<li>Determinants in practice are computed with LU decomposition or equivalent.</li>
<li>Always be mindful of numerical stability - small errors matter when determinant ≈ 0.</li>
<li>For exact answers (small cases), use symbolic tools like SymPy; for speed, use NumPy.</li>
</ul>
</section>
</section>
</section>
<section id="chapter-7.-eigenvalues-eigenvectors-and-dynamics">
<h2 data-anchor-id="chapter-7.-eigenvalues-eigenvectors-and-dynamics">Chapter 7. Eigenvalues, Eigenvectors, and Dynamics</h2>
<section id="eigenvalues-and-eigenvectors-directions-that-stay-put">
<h3 data-anchor-id="eigenvalues-and-eigenvectors-directions-that-stay-put">61. Eigenvalues and Eigenvectors (Directions That Stay Put)</h3>
<p>An eigenvector of a matrix <span>\(A\)</span> is a special vector that doesn’t change direction when multiplied by <span>\(A\)</span>. Instead, it only gets stretched or shrunk by a scalar called the eigenvalue.</p>
<p>Formally:</p>
<p><span>\[
A v = \lambda v
\]</span></p>
<p>where <span>\(v\)</span> is an eigenvector and <span>\(\lambda\)</span> is the eigenvalue.</p>
<p>Geometrically: eigenvectors are “preferred directions” of a linear transformation.</p>
<section id="set-up-your-lab-60">
<h4 data-anchor-id="set-up-your-lab-60">Set Up Your Lab</h4>
<div id="e70236e9" data-execution_count="349"><pre><code><span id="cb607-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb607-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-60">
<h4 data-anchor-id="step-by-step-code-walkthrough-60">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>A simple 2×2 example</li>
</ol>
<div id="604db1bd" data-execution_count="350">
<div id="cb608"><pre><code><span id="cb608-1">A <span>=</span> Matrix([</span>
<span id="cb608-2">    [<span>2</span>,<span>1</span>],</span>
<span id="cb608-3">    [<span>1</span>,<span>2</span>]</span>
<span id="cb608-4">])</span>
<span id="cb608-5"></span>
<span id="cb608-6">eigs <span>=</span> A.eigenvects()</span>
<span id="cb608-7"><span>print</span>(<span>"Eigenvalues and eigenvectors:"</span>, eigs)</span></code></pre></div>
<div>
<pre><code>Eigenvalues and eigenvectors: [(1, 1, [Matrix([
[-1],
[ 1]])]), (3, 1, [Matrix([
[1],
[1]])])]</code></pre>
</div>
</div>
<p>This outputs eigenvalues and their associated eigenvectors.</p>
<ol start="2" type="1">
<li>Verify the eigen equation</li>
</ol>
<p>Pick one eigenpair <span>\((\lambda, v)\)</span>:</p>
<div id="a0a85681" data-execution_count="351">
<div id="cb610"><pre><code><span id="cb610-1">lam <span>=</span> eigs[<span>0</span>][<span>0</span>]</span>
<span id="cb610-2">v <span>=</span> eigs[<span>0</span>][<span>2</span>][<span>0</span>]</span>
<span id="cb610-3"><span>print</span>(<span>"Check Av = λv:"</span>, A<span>*</span>v, lam<span>*</span>v)</span></code></pre></div>
<div>
<pre><code>Check Av = λv: Matrix([[-1], [1]]) Matrix([[-1], [1]])</code></pre>
</div>
</div>
<p>Both sides match → confirming the eigenpair.</p>
<ol start="3" type="1">
<li>NumPy version</li>
</ol>
<div id="eee97db5" data-execution_count="352">
<div id="cb612"><pre><code><span id="cb612-1">A_np <span>=</span> np.array([[<span>2</span>,<span>1</span>],[<span>1</span>,<span>2</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb612-2">eigvals, eigvecs <span>=</span> np.linalg.eig(A_np)</span>
<span id="cb612-3"></span>
<span id="cb612-4"><span>print</span>(<span>"Eigenvalues:"</span>, eigvals)</span>
<span id="cb612-5"><span>print</span>(<span>"Eigenvectors:</span><span>\n</span><span>"</span>, eigvecs)</span></code></pre></div>
<div>
<pre><code>Eigenvalues: [3. 1.]
Eigenvectors:
 [[ 0.70710678 -0.70710678]
 [ 0.70710678  0.70710678]]</code></pre>
</div>
</div>
<p>Columns of <code>eigvecs</code> are eigenvectors.</p>
<ol start="4" type="1">
<li>Geometric interpretation (plot)</li>
</ol>
<div id="5efaee68" data-execution_count="353">
<div id="cb614"><pre><code><span id="cb614-1"><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb614-2"></span>
<span id="cb614-3">v1 <span>=</span> np.array(eigvecs[:,<span>0</span>])</span>
<span id="cb614-4">v2 <span>=</span> np.array(eigvecs[:,<span>1</span>])</span>
<span id="cb614-5"></span>
<span id="cb614-6">plt.arrow(<span>0</span>,<span>0</span>,v1[<span>0</span>],v1[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>"blue"</span>,length_includes_head<span>=</span><span>True</span>)</span>
<span id="cb614-7">plt.arrow(<span>0</span>,<span>0</span>,v2[<span>0</span>],v2[<span>1</span>],head_width<span>=</span><span>0.1</span>,color<span>=</span><span>"red"</span>,length_includes_head<span>=</span><span>True</span>)</span>
<span id="cb614-8"></span>
<span id="cb614-9">plt.axhline(<span>0</span>,color<span>=</span><span>"black"</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb614-10">plt.axvline(<span>0</span>,color<span>=</span><span>"black"</span>,linewidth<span>=</span><span>0.5</span>)</span>
<span id="cb614-11">plt.axis(<span>"equal"</span>)</span>
<span id="cb614-12">plt.grid()</span>
<span id="cb614-13">plt.title(<span>"Eigenvectors: directions that stay put"</span>)</span>
<span id="cb614-14">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-354-output-1.png" width="571" height="431"></p>
</figure>
</div>
</div>
<p>Both eigenvectors define directions where the transformation acts by scaling only.</p>
<ol start="5" type="1">
<li>Random 3×3 matrix example</li>
</ol>
<div id="742dc66d" data-execution_count="354">
<div id="cb615"><pre><code><span id="cb615-1">np.random.seed(<span>1</span>)</span>
<span id="cb615-2">B <span>=</span> Matrix(np.random.randint(<span>-</span><span>2</span>,<span>3</span>,(<span>3</span>,<span>3</span>)))</span>
<span id="cb615-3"><span>print</span>(<span>"Matrix B:</span><span>\n</span><span>"</span>, B)</span>
<span id="cb615-4"><span>print</span>(<span>"Eigenvalues/vectors:"</span>, B.eigenvects())</span></code></pre></div>
<div>
<pre><code>Matrix B:
 Matrix([[1, 2, -2], [-1, 1, -2], [-2, -1, 2]])
Eigenvalues/vectors: [(4/3 + (-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3) + 13/(9*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)), 1, [Matrix([
[ -16/27 - 91/(81*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)) + (4/3 + (-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3) + 13/(9*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)))**2/9 - 7*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)/9],
[50/27 + 5*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)/9 - 2*(4/3 + (-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3) + 13/(9*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)))**2/9 + 65/(81*(-1/2 - sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3))],
[                                                                                                                                                                                                                                                              1]])]), (4/3 + 13/(9*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)) + (-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3), 1, [Matrix([
[ -16/27 - 7*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)/9 + (4/3 + 13/(9*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)) + (-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3))**2/9 - 91/(81*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3))],
[50/27 + 65/(81*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)) - 2*(4/3 + 13/(9*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)) + (-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3))**2/9 + 5*(-1/2 + sqrt(3)*I/2)*(2*sqrt(43)/3 + 127/27)**(1/3)/9],
[                                                                                                                                                                                                                                                              1]])]), (13/(9*(2*sqrt(43)/3 + 127/27)**(1/3)) + 4/3 + (2*sqrt(43)/3 + 127/27)**(1/3), 1, [Matrix([
[  -7*(2*sqrt(43)/3 + 127/27)**(1/3)/9 - 16/27 - 91/(81*(2*sqrt(43)/3 + 127/27)**(1/3)) + (13/(9*(2*sqrt(43)/3 + 127/27)**(1/3)) + 4/3 + (2*sqrt(43)/3 + 127/27)**(1/3))**2/9],
[-2*(13/(9*(2*sqrt(43)/3 + 127/27)**(1/3)) + 4/3 + (2*sqrt(43)/3 + 127/27)**(1/3))**2/9 + 65/(81*(2*sqrt(43)/3 + 127/27)**(1/3)) + 5*(2*sqrt(43)/3 + 127/27)**(1/3)/9 + 50/27],
[                                                                                                                                                                           1]])])]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-59">
<h4 data-anchor-id="try-it-yourself-59">Try It Yourself</h4>
<ol type="1">
<li><p>Compute eigenvalues and eigenvectors of</p>
<p><span>\[
\begin{bmatrix} 3 &amp; 0 \\ 0 &amp; 2 \end{bmatrix}
\]</span></p>
<p>and verify that they match the diagonal entries.</p></li>
<li><p>Use NumPy to find eigenvectors of a rotation matrix by 90°. What do you notice?</p></li>
<li><p>For a singular matrix, check if 0 is an eigenvalue.</p></li>
</ol>
</section>
<section id="the-takeaway-43">
<h4 data-anchor-id="the-takeaway-43">The Takeaway</h4>
<ul>
<li>Eigenvalues = scale factors; eigenvectors = directions that stay put.</li>
<li>The eigen equation <span>\(Av=\lambda v\)</span> captures the essence of a matrix’s action.</li>
<li>They form the foundation for deeper topics like diagonalization, stability, and dynamics.</li>
</ul>
</section>
</section>
<section id="characteristic-polynomial-where-eigenvalues-come-from">
<h3 data-anchor-id="characteristic-polynomial-where-eigenvalues-come-from">62. Characteristic Polynomial (Where Eigenvalues Come From)</h3>
<p>Eigenvalues don’t appear out of thin air - they come from the characteristic polynomial of a matrix. For a square matrix <span>\(A\)</span>,</p>
<p><span>\[
p(\lambda) = \det(A - \lambda I)
\]</span></p>
<p>The roots of this polynomial are the eigenvalues of <span>\(A\)</span>.</p>
<section id="set-up-your-lab-61">
<h4 data-anchor-id="set-up-your-lab-61">Set Up Your Lab</h4>
<div id="8137db83" data-execution_count="355"><pre><code><span id="cb617-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb617-2"><span>from</span> sympy <span>import</span> Matrix, symbols</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-61">
<h4 data-anchor-id="step-by-step-code-walkthrough-61">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>2×2 example</li>
</ol>
<p><span>\[
A = \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{bmatrix}
\]</span></p>
<div id="a33dbb77" data-execution_count="356">
<div id="cb618"><pre><code><span id="cb618-1">λ <span>=</span> symbols(<span>'λ'</span>)</span>
<span id="cb618-2">A <span>=</span> Matrix([[<span>2</span>,<span>1</span>],[<span>1</span>,<span>2</span>]])</span>
<span id="cb618-3">char_poly <span>=</span> A.charpoly(λ)</span>
<span id="cb618-4"><span>print</span>(<span>"Characteristic polynomial:"</span>, char_poly.as_expr())</span>
<span id="cb618-5"><span>print</span>(<span>"Eigenvalues (roots):"</span>, char_poly.all_roots())</span></code></pre></div>
<div>
<pre><code>Characteristic polynomial: λ**2 - 4*λ + 3
Eigenvalues (roots): [1, 3]</code></pre>
</div>
</div>
<p>Polynomial: <span>\(\lambda^2 - 4\lambda + 3\)</span>. Roots: <span>\(\lambda = 3, 1\)</span>.</p>
<ol start="2" type="1">
<li>Verify with eigen computation</li>
</ol>
<div id="a617732a" data-execution_count="357">
<div id="cb620"><pre><code><span id="cb620-1"><span>print</span>(<span>"Eigenvalues directly:"</span>, A.eigenvals())</span></code></pre></div>
<div>
<pre><code>Eigenvalues directly: {3: 1, 1: 1}</code></pre>
</div>
</div>
<p>Matches the roots of the polynomial.</p>
<ol start="3" type="1">
<li>3×3 example</li>
</ol>
<div id="f52be399" data-execution_count="358">
<div id="cb622"><pre><code><span id="cb622-1">B <span>=</span> Matrix([</span>
<span id="cb622-2">    [<span>1</span>,<span>2</span>,<span>3</span>],</span>
<span id="cb622-3">    [<span>0</span>,<span>1</span>,<span>4</span>],</span>
<span id="cb622-4">    [<span>5</span>,<span>6</span>,<span>0</span>]</span>
<span id="cb622-5">])</span>
<span id="cb622-6"></span>
<span id="cb622-7">char_poly_B <span>=</span> B.charpoly(λ)</span>
<span id="cb622-8"><span>print</span>(<span>"Characteristic polynomial of B:"</span>, char_poly_B.as_expr())</span>
<span id="cb622-9"><span>print</span>(<span>"Eigenvalues of B:"</span>, char_poly_B.all_roots())</span></code></pre></div>
<div>
<pre><code>Characteristic polynomial of B: λ**3 - 2*λ**2 - 38*λ - 1
Eigenvalues of B: [CRootOf(x**3 - 2*x**2 - 38*x - 1, 0), CRootOf(x**3 - 2*x**2 - 38*x - 1, 1), CRootOf(x**3 - 2*x**2 - 38*x - 1, 2)]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>NumPy version</li>
</ol>
<p>NumPy doesn’t give the polynomial directly, but eigenvalues can be checked:</p>
<div id="8508629a" data-execution_count="359">
<div id="cb624"><pre><code><span id="cb624-1">B_np <span>=</span> np.array([[<span>1</span>,<span>2</span>,<span>3</span>],[<span>0</span>,<span>1</span>,<span>4</span>],[<span>5</span>,<span>6</span>,<span>0</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb624-2">eigvals <span>=</span> np.linalg.eigvals(B_np)</span>
<span id="cb624-3"><span>print</span>(<span>"NumPy eigenvalues:"</span>, eigvals)</span></code></pre></div>
<div>
<pre><code>NumPy eigenvalues: [-5.2296696  -0.02635282  7.25602242]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Relation to trace and determinant</li>
</ol>
<p>For a 2×2 matrix</p>
<p><span>\[
A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix},
\]</span></p>
<p>the characteristic polynomial is</p>
<p><span>\[
\lambda^2 - (a+d)\lambda + (ad - bc).
\]</span></p>
<ul>
<li>Coefficient of <span>\(\lambda\)</span>: <span>\(-\text{trace}(A)\)</span>.</li>
<li>Constant term: <span>\(\det(A)\)</span>.</li>
</ul>
<div id="9d84d870" data-execution_count="360"><pre><code><span id="cb626-1"><span>print</span>(<span>"Trace:"</span>, A.trace())</span>
<span id="cb626-2"><span>print</span>(<span>"Determinant:"</span>, A.det())</span></code></pre></div>
</section>
<section id="try-it-yourself-60">
<h4 data-anchor-id="try-it-yourself-60">Try It Yourself</h4>
<ol type="1">
<li><p>Compute the characteristic polynomial of</p>
<p><span>\[
\begin{bmatrix} 4 &amp; 0 \\ 0 &amp; 5 \end{bmatrix}
\]</span></p>
<p>and confirm eigenvalues are 4 and 5.</p></li>
<li><p>Check the relationship between polynomial coefficients, trace, and determinant for a 3×3 case.</p></li>
<li><p>Verify with NumPy that the roots of the polynomial equal the eigenvalues.</p></li>
</ol>
</section>
<section id="the-takeaway-44">
<h4 data-anchor-id="the-takeaway-44">The Takeaway</h4>
<ul>
<li>The characteristic polynomial encodes eigenvalues as its roots.</li>
<li>Coefficients are tied to invariants: trace and determinant.</li>
<li>This polynomial viewpoint is the bridge from algebraic formulas to geometric eigen-behavior.</li>
</ul>
</section>
</section>
<section id="algebraic-vs.-geometric-multiplicity-how-many-and-how-independent">
<h3 data-anchor-id="algebraic-vs.-geometric-multiplicity-how-many-and-how-independent">63. Algebraic vs.&nbsp;Geometric Multiplicity (How Many and How Independent)</h3>
<p>Eigenvalues can repeat, and when they do, two notions of multiplicity arise:</p>
<ul>
<li>Algebraic multiplicity: how many times the eigenvalue appears as a root of the characteristic polynomial.</li>
<li>Geometric multiplicity: the dimension of the eigenspace (number of independent eigenvectors).</li>
</ul>
<p>Always:</p>
<p><span>\[
1 \leq \text{geometric multiplicity} \leq \text{algebraic multiplicity}
\]</span></p>
<section id="set-up-your-lab-62">
<h4 data-anchor-id="set-up-your-lab-62">Set Up Your Lab</h4>
<div id="afb7b717" data-execution_count="361"><pre><code><span id="cb628-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb628-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-62">
<h4 data-anchor-id="step-by-step-code-walkthrough-62">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Matrix with repeated eigenvalue</li>
</ol>
<div id="e37ca5a6" data-execution_count="362">
<div id="cb629"><pre><code><span id="cb629-1">A <span>=</span> Matrix([</span>
<span id="cb629-2">    [<span>2</span>,<span>1</span>],</span>
<span id="cb629-3">    [<span>0</span>,<span>2</span>]</span>
<span id="cb629-4">])</span>
<span id="cb629-5"></span>
<span id="cb629-6"><span>print</span>(<span>"Eigenvalues and algebraic multiplicity:"</span>, A.eigenvals())</span>
<span id="cb629-7"><span>print</span>(<span>"Eigenvectors:"</span>, A.eigenvects())</span></code></pre></div>
<div>
<pre><code>Eigenvalues and algebraic multiplicity: {2: 2}
Eigenvectors: [(2, 2, [Matrix([
[1],
[0]])])]</code></pre>
</div>
</div>
<ul>
<li>Eigenvalue 2 has algebraic multiplicity = 2.</li>
<li>But only 1 independent eigenvector → geometric multiplicity = 1.</li>
</ul>
<ol start="2" type="1">
<li>Diagonal matrix with repetition</li>
</ol>
<div id="73b04e56" data-execution_count="363">
<div id="cb631"><pre><code><span id="cb631-1">B <span>=</span> Matrix([</span>
<span id="cb631-2">    [<span>3</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb631-3">    [<span>0</span>,<span>3</span>,<span>0</span>],</span>
<span id="cb631-4">    [<span>0</span>,<span>0</span>,<span>3</span>]</span>
<span id="cb631-5">])</span>
<span id="cb631-6"></span>
<span id="cb631-7"><span>print</span>(<span>"Eigenvalues:"</span>, B.eigenvals())</span>
<span id="cb631-8"><span>print</span>(<span>"Eigenvectors:"</span>, B.eigenvects())</span></code></pre></div>
<div>
<pre><code>Eigenvalues: {3: 3}
Eigenvectors: [(3, 3, [Matrix([
[1],
[0],
[0]]), Matrix([
[0],
[1],
[0]]), Matrix([
[0],
[0],
[1]])])]</code></pre>
</div>
</div>
<p>Here, eigenvalue 3 has algebraic multiplicity = 3, and geometric multiplicity = 3.</p>
<ol start="3" type="1">
<li>NumPy check</li>
</ol>
<div id="dcc7594b" data-execution_count="364">
<div id="cb633"><pre><code><span id="cb633-1">A_np <span>=</span> np.array([[<span>2</span>,<span>1</span>],[<span>0</span>,<span>2</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb633-2">eigvals, eigvecs <span>=</span> np.linalg.eig(A_np)</span>
<span id="cb633-3"><span>print</span>(<span>"Eigenvalues:"</span>, eigvals)</span>
<span id="cb633-4"><span>print</span>(<span>"Eigenvectors:</span><span>\n</span><span>"</span>, eigvecs)</span></code></pre></div>
<div>
<pre><code>Eigenvalues: [2. 2.]
Eigenvectors:
 [[ 1.0000000e+00 -1.0000000e+00]
 [ 0.0000000e+00  4.4408921e-16]]</code></pre>
</div>
</div>
<p>NumPy won’t show multiplicities directly, but you can see repeated eigenvalues.</p>
<ol start="4" type="1">
<li>Comparing two cases</li>
</ol>
<ul>
<li>Defective matrix: Algebraic &gt; geometric (like the upper triangular <span>\(A\)</span>).</li>
<li>Diagonalizable matrix: Algebraic = geometric (like <span>\(B\)</span>).</li>
</ul>
<p>This distinction determines whether a matrix can be diagonalized.</p>
</section>
<section id="try-it-yourself-61">
<h4 data-anchor-id="try-it-yourself-61">Try It Yourself</h4>
<ol type="1">
<li><p>Compute algebraic and geometric multiplicities of</p>
<p><span>\[
\begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix}
\]</span></p>
<p>(hint: only one eigenvector).</p></li>
<li><p>Take a diagonal matrix with repeated entries - what happens to multiplicities?</p></li>
<li><p>Test a random 3×3 singular matrix. Does 0 have algebraic multiplicity &gt; 1?</p></li>
</ol>
</section>
<section id="the-takeaway-45">
<h4 data-anchor-id="the-takeaway-45">The Takeaway</h4>
<ul>
<li>Algebraic multiplicity = count of root in characteristic polynomial.</li>
<li>Geometric multiplicity = dimension of eigenspace.</li>
<li>If they match for all eigenvalues → matrix is diagonalizable.</li>
</ul>
</section>
</section>
<section id="diagonalization-when-a-matrix-becomes-simple">
<h3 data-anchor-id="diagonalization-when-a-matrix-becomes-simple">64. Diagonalization (When a Matrix Becomes Simple)</h3>
<p>A matrix <span>\(A\)</span> is diagonalizable if it can be written as</p>
<p><span>\[
A = P D P^{-1}
\]</span></p>
<ul>
<li><span>\(D\)</span> is diagonal (containing eigenvalues).</li>
<li>Columns of <span>\(P\)</span> are the eigenvectors.</li>
</ul>
<p>This means <span>\(A\)</span> acts like simple scaling in a “better” coordinate system.</p>
<section id="set-up-your-lab-63">
<h4 data-anchor-id="set-up-your-lab-63">Set Up Your Lab</h4>
<div id="cdee09d8" data-execution_count="365"><pre><code><span id="cb635-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb635-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-63">
<h4 data-anchor-id="step-by-step-code-walkthrough-63">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>A diagonalizable 2×2 matrix</li>
</ol>
<div id="b6dea603" data-execution_count="366">
<div id="cb636"><pre><code><span id="cb636-1">A <span>=</span> Matrix([</span>
<span id="cb636-2">    [<span>4</span>,<span>1</span>],</span>
<span id="cb636-3">    [<span>2</span>,<span>3</span>]</span>
<span id="cb636-4">])</span>
<span id="cb636-5"></span>
<span id="cb636-6">P, D <span>=</span> A.diagonalize()</span>
<span id="cb636-7"><span>print</span>(<span>"P (eigenvectors):"</span>)</span>
<span id="cb636-8"><span>print</span>(P)</span>
<span id="cb636-9"><span>print</span>(<span>"D (eigenvalues on diagonal):"</span>)</span>
<span id="cb636-10"><span>print</span>(D)</span>
<span id="cb636-11"></span>
<span id="cb636-12"><span># Verify A = P D P^-1</span></span>
<span id="cb636-13"><span>print</span>(<span>"Check:"</span>, P<span>*</span>D<span>*</span>P.inv())</span></code></pre></div>
<div>
<pre><code>P (eigenvectors):
Matrix([[-1, 1], [2, 1]])
D (eigenvalues on diagonal):
Matrix([[2, 0], [0, 5]])
Check: Matrix([[4, 1], [2, 3]])</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>A non-diagonalizable matrix</li>
</ol>
<div id="2d3d7ba0" data-execution_count="367">
<div id="cb638"><pre><code><span id="cb638-1">B <span>=</span> Matrix([</span>
<span id="cb638-2">    [<span>2</span>,<span>1</span>],</span>
<span id="cb638-3">    [<span>0</span>,<span>2</span>]</span>
<span id="cb638-4">])</span>
<span id="cb638-5"></span>
<span id="cb638-6"><span>try</span>:</span>
<span id="cb638-7">    P, D <span>=</span> B.diagonalize()</span>
<span id="cb638-8">    <span>print</span>(<span>"Diagonalization successful"</span>)</span>
<span id="cb638-9"><span>except</span> <span>Exception</span> <span>as</span> e:</span>
<span id="cb638-10">    <span>print</span>(<span>"Not diagonalizable:"</span>, e)</span></code></pre></div>
<div>
<pre><code>Not diagonalizable: Matrix is not diagonalizable</code></pre>
</div>
</div>
<p>This fails because eigenvalue 2 has algebraic multiplicity 2 but geometric multiplicity 1.</p>
<ol start="3" type="1">
<li>Diagonalization with NumPy</li>
</ol>
<p>NumPy doesn’t diagonalize explicitly, but we can build <span>\(P\)</span> and <span>\(D\)</span> ourselves:</p>
<div id="74fcdc53" data-execution_count="368">
<div id="cb640"><pre><code><span id="cb640-1">A_np <span>=</span> np.array([[<span>4</span>,<span>1</span>],[<span>2</span>,<span>3</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb640-2">eigvals, eigvecs <span>=</span> np.linalg.eig(A_np)</span>
<span id="cb640-3"></span>
<span id="cb640-4">P <span>=</span> eigvecs</span>
<span id="cb640-5">D <span>=</span> np.diag(eigvals)</span>
<span id="cb640-6">Pinv <span>=</span> np.linalg.inv(P)</span>
<span id="cb640-7"></span>
<span id="cb640-8"><span>print</span>(<span>"Check A = PDP^-1:</span><span>\n</span><span>"</span>, P <span>@</span> D <span>@</span> Pinv)</span></code></pre></div>
<div>
<pre><code>Check A = PDP^-1:
 [[4. 1.]
 [2. 3.]]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Powers of a diagonalizable matrix</li>
</ol>
<p>One reason diagonalization is powerful:</p>
<p><span>\[
A^k = P D^k P^{-1}
\]</span></p>
<p>Since <span>\(D^k\)</span> is trivial (just raise each diagonal entry to power <span>\(k\)</span>).</p>
<div id="02191ad1" data-execution_count="369">
<div id="cb642"><pre><code><span id="cb642-1">k <span>=</span> <span>5</span></span>
<span id="cb642-2"></span>
<span id="cb642-3">A_power <span>=</span> np.linalg.matrix_power(A, k)</span>
<span id="cb642-4">D_power <span>=</span> np.linalg.matrix_power(D, k)</span>
<span id="cb642-5">A_via_diag <span>=</span> P <span>@</span> D_power <span>@</span> np.linalg.inv(P)</span>
<span id="cb642-6"></span>
<span id="cb642-7"><span>print</span>(<span>"A^5 via diagonalization:</span><span>\n</span><span>"</span>, A_via_diag)</span>
<span id="cb642-8"><span>print</span>(<span>"Direct A^5:</span><span>\n</span><span>"</span>, A_power)</span></code></pre></div>
<div>
<pre><code>A^5 via diagonalization:
 [[2094. 1031.]
 [2062. 1063.]]
Direct A^5:
 [[2094 1031]
 [2062 1063]]</code></pre>
</div>
</div>
<p>Both match.</p>
</section>
<section id="try-it-yourself-62">
<h4 data-anchor-id="try-it-yourself-62">Try It Yourself</h4>
<ol type="1">
<li><p>Check whether</p>
<p><span>\[
\begin{bmatrix} 5 &amp; 0 \\ 0 &amp; 5 \end{bmatrix}
\]</span></p>
<p>is diagonalizable.</p></li>
<li><p>Try diagonalizing a rotation matrix by 90°. Do you get complex eigenvalues?</p></li>
<li><p>Verify the formula <span>\(A^k = P D^k P^{-1}\)</span> for a 3×3 diagonalizable matrix.</p></li>
</ol>
</section>
<section id="the-takeaway-46">
<h4 data-anchor-id="the-takeaway-46">The Takeaway</h4>
<ul>
<li>Diagonalization rewrites a matrix in its simplest form.</li>
<li>Works if there are enough independent eigenvectors.</li>
<li>It makes powers of <span>\(A\)</span> easy, and is the gateway to analyzing dynamics.</li>
</ul>
</section>
</section>
<section id="powers-of-a-matrix-long-term-behavior-via-eigenvalues">
<h3 data-anchor-id="powers-of-a-matrix-long-term-behavior-via-eigenvalues">65. Powers of a Matrix (Long-Term Behavior via Eigenvalues)</h3>
<p>One of the most useful applications of eigenvalues and diagonalization is computing powers of a matrix:</p>
<p><span>\[
A^k = P D^k P^{-1}
\]</span></p>
<p>where <span>\(D\)</span> is diagonal with eigenvalues of <span>\(A\)</span>. Each eigenvalue <span>\(\lambda\)</span> raised to <span>\(k\)</span> dictates how its eigenvector direction grows, decays, or oscillates over time.</p>
<section id="set-up-your-lab-64">
<h4 data-anchor-id="set-up-your-lab-64">Set Up Your Lab</h4>
<div id="2941621f" data-execution_count="370"><pre><code><span id="cb644-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb644-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-64">
<h4 data-anchor-id="step-by-step-code-walkthrough-64">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Simple diagonal matrix</li>
</ol>
<p>If <span>\(D = \text{diag}(2,3)\)</span>:</p>
<div id="a0e1cb10" data-execution_count="371">
<div id="cb645"><pre><code><span id="cb645-1">D <span>=</span> Matrix([[<span>2</span>,<span>0</span>],[<span>0</span>,<span>3</span>]])</span>
<span id="cb645-2"><span>print</span>(<span>"D^5 ="</span>)</span>
<span id="cb645-3"><span>print</span>(D<span>**</span><span>5</span>)</span></code></pre></div>
<div>
<pre><code>D^5 =
Matrix([[32, 0], [0, 243]])</code></pre>
</div>
</div>
<p>Eigenvalues are 2 and 3. Raising to the 5th power just raises each eigenvalue to the 5th: <span>\(2^5, 3^5\)</span>.</p>
<ol start="2" type="1">
<li>Non-diagonal matrix</li>
</ol>
<div id="8e6caa7c" data-execution_count="372">
<div id="cb647"><pre><code><span id="cb647-1">A <span>=</span> Matrix([</span>
<span id="cb647-2">    [<span>4</span>,<span>1</span>],</span>
<span id="cb647-3">    [<span>2</span>,<span>3</span>]</span>
<span id="cb647-4">])</span>
<span id="cb647-5"></span>
<span id="cb647-6">P, D <span>=</span> A.diagonalize()</span>
<span id="cb647-7"><span>print</span>(<span>"D (eigenvalues):"</span>)</span>
<span id="cb647-8"><span>print</span>(D)</span>
<span id="cb647-9"></span>
<span id="cb647-10"><span># Compute A^10 via diagonalization</span></span>
<span id="cb647-11">A10 <span>=</span> P <span>*</span> (D<span>**</span><span>10</span>) <span>*</span> P.inv()</span>
<span id="cb647-12"><span>print</span>(<span>"A^10 ="</span>)</span>
<span id="cb647-13"><span>print</span>(A10)</span></code></pre></div>
<div>
<pre><code>D (eigenvalues):
Matrix([[2, 0], [0, 5]])
A^10 =
Matrix([[6510758, 3254867], [6509734, 3255891]])</code></pre>
</div>
</div>
<p>Much easier than multiplying <span>\(A\)</span> ten times!</p>
<ol start="3" type="1">
<li>NumPy version</li>
</ol>
<div id="0e95ef08" data-execution_count="373">
<div id="cb649"><pre><code><span id="cb649-1">A_np <span>=</span> np.array([[<span>4</span>,<span>1</span>],[<span>2</span>,<span>3</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb649-2">eigvals, eigvecs <span>=</span> np.linalg.eig(A_np)</span>
<span id="cb649-3"></span>
<span id="cb649-4">k <span>=</span> <span>10</span></span>
<span id="cb649-5">D_power <span>=</span> np.diag(eigvals<span>**</span>k)</span>
<span id="cb649-6">A10_np <span>=</span> eigvecs <span>@</span> D_power <span>@</span> np.linalg.inv(eigvecs)</span>
<span id="cb649-7"></span>
<span id="cb649-8"><span>print</span>(<span>"A^10 via eigen-decomposition:</span><span>\n</span><span>"</span>, A10_np)</span></code></pre></div>
<div>
<pre><code>A^10 via eigen-decomposition:
 [[6510758. 3254867.]
 [6509734. 3255891.]]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Long-term behavior</li>
</ol>
<p>Eigenvalues tell us what happens as <span>\(k \to \infty\)</span>:</p>
<ul>
<li>If <span>\(|\lambda| &lt; 1\)</span> → decay to 0.</li>
<li>If <span>\(|\lambda| &gt; 1\)</span> → grows unbounded.</li>
<li>If <span>\(|\lambda| = 1\)</span> → oscillates or stabilizes.</li>
</ul>
<div id="f0707dee" data-execution_count="374">
<div id="cb651"><pre><code><span id="cb651-1">B <span>=</span> Matrix([</span>
<span id="cb651-2">    [<span>0.5</span>,<span>0</span>],</span>
<span id="cb651-3">    [<span>0</span>,<span>1.2</span>]</span>
<span id="cb651-4">])</span>
<span id="cb651-5"></span>
<span id="cb651-6">P, D <span>=</span> B.diagonalize()</span>
<span id="cb651-7"><span>print</span>(<span>"Eigenvalues:"</span>, D)</span>
<span id="cb651-8"><span>print</span>(<span>"B^20:"</span>, P<span>*</span>(D<span>**</span><span>20</span>)<span>*</span>P.inv())</span></code></pre></div>
<div>
<pre><code>Eigenvalues: Matrix([[0.500000000000000, 0], [0, 1.20000000000000]])
B^20: Matrix([[9.53674316406250e-7, 0], [0, 38.3375999244747]])</code></pre>
</div>
</div>
<p>Here, the component along eigenvalue 0.5 decays, while eigenvalue 1.2 grows.</p>
</section>
<section id="try-it-yourself-63">
<h4 data-anchor-id="try-it-yourself-63">Try It Yourself</h4>
<ol type="1">
<li>Compute <span>\(A^{50}\)</span> for a diagonal matrix with eigenvalues 0.9 and 1.1. Which component dominates?</li>
<li>Take a stochastic (Markov) matrix and compute powers. Do the rows stabilize?</li>
<li>Experiment with complex eigenvalues (like a rotation) and check if the powers oscillate.</li>
</ol>
</section>
<section id="the-takeaway-47">
<h4 data-anchor-id="the-takeaway-47">The Takeaway</h4>
<ul>
<li>Matrix powers are simple when using eigenvalues.</li>
<li>Long-term dynamics are controlled by eigenvalue magnitudes.</li>
<li>This insight is critical in Markov chains, stability analysis, and dynamical systems.</li>
</ul>
</section>
</section>
<section id="real-vs.-complex-spectra-rotations-and-oscillations">
<h3 data-anchor-id="real-vs.-complex-spectra-rotations-and-oscillations">66. Real vs.&nbsp;Complex Spectra (Rotations and Oscillations)</h3>
<p>Not all eigenvalues are real. Some matrices, especially those involving rotations, have complex eigenvalues. Complex eigenvalues often describe oscillations or rotations in systems.</p>
<section id="set-up-your-lab-65">
<h4 data-anchor-id="set-up-your-lab-65">Set Up Your Lab</h4>
<div id="1e419a10" data-execution_count="375"><pre><code><span id="cb653-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb653-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-65">
<h4 data-anchor-id="step-by-step-code-walkthrough-65">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Rotation matrix in 2D</li>
</ol>
<p>A 90° rotation matrix:</p>
<p><span>\[
R = \begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{bmatrix}
\]</span></p>
<div id="31a2ef4a" data-execution_count="376">
<div id="cb654"><pre><code><span id="cb654-1">R <span>=</span> Matrix([[<span>0</span>, <span>-</span><span>1</span>],</span>
<span id="cb654-2">            [<span>1</span>,  <span>0</span>]])</span>
<span id="cb654-3"></span>
<span id="cb654-4"><span>print</span>(<span>"Characteristic polynomial:"</span>, R.charpoly())</span>
<span id="cb654-5"><span>print</span>(<span>"Eigenvalues:"</span>, R.eigenvals())</span></code></pre></div>
<div>
<pre><code>Characteristic polynomial: PurePoly(lambda**2 + 1, lambda, domain='ZZ')
Eigenvalues: {-I: 1, I: 1}</code></pre>
</div>
</div>
<p>Result: eigenvalues are <span>\(i\)</span> and <span>\(-i\)</span> (purely imaginary).</p>
<ol start="2" type="1">
<li>Verify eigen-equation with complex numbers</li>
</ol>
<div id="ba7b49d9" data-execution_count="377">
<div id="cb656"><pre><code><span id="cb656-1">eigs <span>=</span> R.eigenvects()</span>
<span id="cb656-2"><span>for</span> eig <span>in</span> eigs:</span>
<span id="cb656-3">    lam <span>=</span> eig[<span>0</span>]</span>
<span id="cb656-4">    v <span>=</span> eig[<span>2</span>][<span>0</span>]</span>
<span id="cb656-5">    <span>print</span>(<span>f"λ = </span><span>{</span>lam<span>}</span><span>, Av = </span><span>{</span>R<span>*</span>v<span>}</span><span>, λv = </span><span>{</span>lam<span>*</span>v<span>}</span><span>"</span>)</span></code></pre></div>
<div>
<pre><code>λ = -I, Av = Matrix([[-1], [-I]]), λv = Matrix([[-1], [-I]])
λ = I, Av = Matrix([[-1], [I]]), λv = Matrix([[-1], [I]])</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>NumPy version</li>
</ol>
<div id="959f2be4" data-execution_count="378">
<div id="cb658"><pre><code><span id="cb658-1">R_np <span>=</span> np.array([[<span>0</span>,<span>-</span><span>1</span>],[<span>1</span>,<span>0</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb658-2">eigvals, eigvecs <span>=</span> np.linalg.eig(R_np)</span>
<span id="cb658-3"><span>print</span>(<span>"Eigenvalues:"</span>, eigvals)</span>
<span id="cb658-4"><span>print</span>(<span>"Eigenvectors:</span><span>\n</span><span>"</span>, eigvecs)</span></code></pre></div>
<div>
<pre><code>Eigenvalues: [0.+1.j 0.-1.j]
Eigenvectors:
 [[0.70710678+0.j         0.70710678-0.j        ]
 [0.        -0.70710678j 0.        +0.70710678j]]</code></pre>
</div>
</div>
<p>NumPy shows complex eigenvalues with <code>j</code> (Python’s imaginary unit).</p>
<ol start="4" type="1">
<li>Rotation by arbitrary angle</li>
</ol>
<p>General 2D rotation:</p>
<p><span>\[
R(\theta) = \begin{bmatrix} \cos\theta &amp; -\sin\theta \\ \sin\theta &amp; \cos\theta \end{bmatrix}
\]</span></p>
<p>Eigenvalues:</p>
<p><span>\[
\lambda = e^{\pm i\theta} = \cos\theta \pm i\sin\theta
\]</span></p>
<div id="772ead5f" data-execution_count="379">
<div id="cb660"><pre><code><span id="cb660-1">theta <span>=</span> np.pi<span>/</span><span>4</span>  <span># 45 degrees</span></span>
<span id="cb660-2">R_theta <span>=</span> np.array([[np.cos(theta), <span>-</span>np.sin(theta)],</span>
<span id="cb660-3">                    [np.sin(theta),  np.cos(theta)]])</span>
<span id="cb660-4"></span>
<span id="cb660-5">eigvals, eigvecs <span>=</span> np.linalg.eig(R_theta)</span>
<span id="cb660-6"><span>print</span>(<span>"Eigenvalues (rotation 45°):"</span>, eigvals)</span></code></pre></div>
<div>
<pre><code>Eigenvalues (rotation 45°): [0.70710678+0.70710678j 0.70710678-0.70710678j]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Oscillation insight</li>
</ol>
<ul>
<li>Complex eigenvalues with <span>\(|\lambda|=1\)</span> → pure oscillation (no growth).</li>
<li>If <span>\(|\lambda|&lt;1\)</span> → decaying spiral.</li>
<li>If <span>\(|\lambda|&gt;1\)</span> → growing spiral.</li>
</ul>
<p>Example:</p>
<div id="07cbe87b" data-execution_count="380">
<div id="cb662"><pre><code><span id="cb662-1">A <span>=</span> np.array([[<span>0.8</span>, <span>-</span><span>0.6</span>],</span>
<span id="cb662-2">              [<span>0.6</span>,  <span>0.8</span>]])</span>
<span id="cb662-3"></span>
<span id="cb662-4">eigvals, _ <span>=</span> np.linalg.eig(A)</span>
<span id="cb662-5"><span>print</span>(<span>"Eigenvalues:"</span>, eigvals)</span></code></pre></div>
<div>
<pre><code>Eigenvalues: [0.8+0.6j 0.8-0.6j]</code></pre>
</div>
</div>
<p>These eigenvalues lie inside the unit circle → spiral decay.</p>
</section>
<section id="try-it-yourself-64">
<h4 data-anchor-id="try-it-yourself-64">Try It Yourself</h4>
<ol type="1">
<li>Compute eigenvalues of a 180° rotation. What happens?</li>
<li>Modify the rotation matrix to include scaling (e.g., multiply by 1.1). Do the eigenvalues lie outside the unit circle?</li>
<li>Plot the trajectory of repeatedly applying a rotation matrix to a vector.</li>
</ol>
</section>
<section id="the-takeaway-48">
<h4 data-anchor-id="the-takeaway-48">The Takeaway</h4>
<ul>
<li>Complex eigenvalues naturally appear in rotations and oscillatory systems.</li>
<li>Their magnitude controls growth or decay; their angle controls oscillation.</li>
<li>This is a key link between linear algebra and dynamics in physics and engineering.</li>
</ul>
</section>
</section>
<section id="defective-matrices-and-a-peek-at-jordan-form-when-diagonalization-fails">
<h3 data-anchor-id="defective-matrices-and-a-peek-at-jordan-form-when-diagonalization-fails">67. Defective Matrices and a Peek at Jordan Form (When Diagonalization Fails)</h3>
<p>Not every matrix has enough independent eigenvectors to be diagonalized. Such matrices are called defective. To handle them, mathematicians use the Jordan normal form, which extends diagonalization with extra structure.</p>
<section id="set-up-your-lab-66">
<h4 data-anchor-id="set-up-your-lab-66">Set Up Your Lab</h4>
<div id="240fb6c2" data-execution_count="381"><pre><code><span id="cb664-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb664-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-66">
<h4 data-anchor-id="step-by-step-code-walkthrough-66">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>A defective example</li>
</ol>
<p><span>\[
A = \begin{bmatrix} 2 &amp; 1 \\ 0 &amp; 2 \end{bmatrix}
\]</span></p>
<div id="af88f139" data-execution_count="382">
<div id="cb665"><pre><code><span id="cb665-1">A <span>=</span> Matrix([[<span>2</span>,<span>1</span>],</span>
<span id="cb665-2">            [<span>0</span>,<span>2</span>]])</span>
<span id="cb665-3"></span>
<span id="cb665-4"><span>print</span>(<span>"Eigenvalues:"</span>, A.eigenvals())</span>
<span id="cb665-5"><span>print</span>(<span>"Eigenvectors:"</span>, A.eigenvects())</span></code></pre></div>
<div>
<pre><code>Eigenvalues: {2: 2}
Eigenvectors: [(2, 2, [Matrix([
[1],
[0]])])]</code></pre>
</div>
</div>
<ul>
<li>Eigenvalue 2 has algebraic multiplicity = 2.</li>
<li>Only 1 eigenvector exists → geometric multiplicity = 1.</li>
</ul>
<p>Thus <span>\(A\)</span> is defective, not diagonalizable.</p>
<ol start="2" type="1">
<li>Attempt diagonalization</li>
</ol>
<div id="7c003c11" data-execution_count="383">
<div id="cb667"><pre><code><span id="cb667-1"><span>try</span>:</span>
<span id="cb667-2">    P, D <span>=</span> A.diagonalize()</span>
<span id="cb667-3">    <span>print</span>(<span>"Diagonal form:"</span>, D)</span>
<span id="cb667-4"><span>except</span> <span>Exception</span> <span>as</span> e:</span>
<span id="cb667-5">    <span>print</span>(<span>"Diagonalization failed:"</span>, e)</span></code></pre></div>
<div>
<pre><code>Diagonalization failed: Matrix is not diagonalizable</code></pre>
</div>
</div>
<p>You’ll see an error - confirming <span>\(A\)</span> is not diagonalizable.</p>
<ol start="3" type="1">
<li>Jordan form in SymPy</li>
</ol>
<div id="5ab5033a" data-execution_count="384">
<div id="cb669"><pre><code><span id="cb669-1">J, P <span>=</span> A.jordan_form()</span>
<span id="cb669-2"><span>print</span>(<span>"Jordan form J:"</span>)</span>
<span id="cb669-3"><span>print</span>(J)</span>
<span id="cb669-4"><span>print</span>(<span>"P (generalized eigenvectors):"</span>)</span>
<span id="cb669-5"><span>print</span>(P)</span></code></pre></div>
<div>
<pre><code>Jordan form J:
Matrix([[1, 0], [0, 1]])
P (generalized eigenvectors):
Matrix([[2, 1], [0, 2]])</code></pre>
</div>
</div>
<p>The Jordan form shows a Jordan block:</p>
<p><span>\[
J = \begin{bmatrix} 2 &amp; 1 \\ 0 &amp; 2 \end{bmatrix}
\]</span></p>
<p>This block structure represents the failure of diagonalization.</p>
<ol start="4" type="1">
<li>NumPy perspective</li>
</ol>
<p>NumPy doesn’t compute Jordan form, but you can see repeated eigenvalues and lack of eigenvectors:</p>
<div id="ff458933" data-execution_count="385">
<div id="cb671"><pre><code><span id="cb671-1">A_np <span>=</span> np.array([[<span>2</span>,<span>1</span>],[<span>0</span>,<span>2</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb671-2">eigvals, eigvecs <span>=</span> np.linalg.eig(A_np)</span>
<span id="cb671-3"><span>print</span>(<span>"Eigenvalues:"</span>, eigvals)</span>
<span id="cb671-4"><span>print</span>(<span>"Eigenvectors:</span><span>\n</span><span>"</span>, eigvecs)</span></code></pre></div>
<div>
<pre><code>Eigenvalues: [2. 2.]
Eigenvectors:
 [[ 1.0000000e+00 -1.0000000e+00]
 [ 0.0000000e+00  4.4408921e-16]]</code></pre>
</div>
</div>
<p>The eigenvectors matrix has fewer independent columns than expected.</p>
<ol start="5" type="1">
<li>Generalized eigenvectors</li>
</ol>
<p>Jordan form introduces generalized eigenvectors, which satisfy:</p>
<p><span>\[
(A - \lambda I)^k v = 0 \quad \text{for some } k&gt;1
\]</span></p>
<p>They “fill the gap” when ordinary eigenvectors are insufficient.</p>
</section>
<section id="try-it-yourself-65">
<h4 data-anchor-id="try-it-yourself-65">Try It Yourself</h4>
<ol type="1">
<li><p>Test diagonalizability of</p>
<p><span>\[
\begin{bmatrix} 3 &amp; 1 \\ 0 &amp; 3 \end{bmatrix}
\]</span></p>
<p>and compare with its Jordan form.</p></li>
<li><p>Try a 3×3 defective matrix with one Jordan block of size 3.</p></li>
<li><p>Verify that Jordan blocks still capture the correct eigenvalues.</p></li>
</ol>
</section>
<section id="the-takeaway-49">
<h4 data-anchor-id="the-takeaway-49">The Takeaway</h4>
<ul>
<li>Defective matrices lack enough eigenvectors for diagonalization.</li>
<li>Jordan form replaces diagonalization with blocks, keeping eigenvalues on the diagonal.</li>
<li>Understanding Jordan blocks is essential for advanced linear algebra and differential equations.</li>
</ul>
</section>
</section>
<section id="stability-and-spectral-radius-grow-decay-or-oscillate">
<h3 data-anchor-id="stability-and-spectral-radius-grow-decay-or-oscillate">68. Stability and Spectral Radius (Grow, Decay, or Oscillate)</h3>
<p>The spectral radius of a matrix <span>\(A\)</span> is defined as</p>
<p><span>\[
\rho(A) = \max_i |\lambda_i|
\]</span></p>
<p>where <span>\(\lambda_i\)</span> are the eigenvalues. It tells us the long-term behavior of repeated applications of <span>\(A\)</span>:</p>
<ul>
<li>If <span>\(\rho(A) &lt; 1\)</span> → powers of <span>\(A\)</span> tend to 0 (stable/decay).</li>
<li>If <span>\(\rho(A) = 1\)</span> → powers neither blow up nor vanish (neutral, may oscillate).</li>
<li>If <span>\(\rho(A) &gt; 1\)</span> → powers diverge (unstable/growth).</li>
</ul>
<section id="set-up-your-lab-67">
<h4 data-anchor-id="set-up-your-lab-67">Set Up Your Lab</h4>
<div id="bc2e59a4" data-execution_count="386"><pre><code><span id="cb673-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb673-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-67">
<h4 data-anchor-id="step-by-step-code-walkthrough-67">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Stable matrix (<span>\(\rho &lt; 1\)</span>)</li>
</ol>
<div id="5432057f" data-execution_count="387">
<div id="cb674"><pre><code><span id="cb674-1">A <span>=</span> np.array([[<span>0.5</span>, <span>0</span>],</span>
<span id="cb674-2">              [<span>0</span>, <span>0.3</span>]])</span>
<span id="cb674-3"></span>
<span id="cb674-4">eigvals <span>=</span> np.linalg.eigvals(A)</span>
<span id="cb674-5">spectral_radius <span>=</span> <span>max</span>(<span>abs</span>(eigvals))</span>
<span id="cb674-6"></span>
<span id="cb674-7"><span>print</span>(<span>"Eigenvalues:"</span>, eigvals)</span>
<span id="cb674-8"><span>print</span>(<span>"Spectral radius:"</span>, spectral_radius)</span>
<span id="cb674-9"></span>
<span id="cb674-10"><span>print</span>(<span>"A^10:</span><span>\n</span><span>"</span>, np.linalg.matrix_power(A, <span>10</span>))</span></code></pre></div>
<div>
<pre><code>Eigenvalues: [0.5 0.3]
Spectral radius: 0.5
A^10:
 [[9.765625e-04 0.000000e+00]
 [0.000000e+00 5.904900e-06]]</code></pre>
</div>
</div>
<p>All entries shrink toward zero.</p>
<ol start="2" type="1">
<li>Unstable matrix (<span>\(\rho &gt; 1\)</span>)</li>
</ol>
<div id="acd293dd" data-execution_count="388">
<div id="cb676"><pre><code><span id="cb676-1">B <span>=</span> np.array([[<span>1.2</span>, <span>0</span>],</span>
<span id="cb676-2">              [<span>0</span>, <span>0.9</span>]])</span>
<span id="cb676-3"></span>
<span id="cb676-4">eigvals <span>=</span> np.linalg.eigvals(B)</span>
<span id="cb676-5"><span>print</span>(<span>"Eigenvalues:"</span>, eigvals, <span>"Spectral radius:"</span>, <span>max</span>(<span>abs</span>(eigvals)))</span>
<span id="cb676-6"><span>print</span>(<span>"B^10:</span><span>\n</span><span>"</span>, np.linalg.matrix_power(B, <span>10</span>))</span></code></pre></div>
<div>
<pre><code>Eigenvalues: [1.2 0.9] Spectral radius: 1.2
B^10:
 [[6.19173642 0.        ]
 [0.         0.34867844]]</code></pre>
</div>
</div>
<p>The component along eigenvalue 1.2 grows quickly.</p>
<ol start="3" type="1">
<li>Neutral/oscillatory case (<span>\(\rho = 1\)</span>)</li>
</ol>
<p>90° rotation matrix:</p>
<div id="40e391b1" data-execution_count="389">
<div id="cb678"><pre><code><span id="cb678-1">R <span>=</span> np.array([[<span>0</span>, <span>-</span><span>1</span>],</span>
<span id="cb678-2">              [<span>1</span>,  <span>0</span>]])</span>
<span id="cb678-3"></span>
<span id="cb678-4">eigvals <span>=</span> np.linalg.eigvals(R)</span>
<span id="cb678-5"><span>print</span>(<span>"Eigenvalues:"</span>, eigvals)</span>
<span id="cb678-6"><span>print</span>(<span>"Spectral radius:"</span>, <span>max</span>(<span>abs</span>(eigvals)))</span>
<span id="cb678-7"><span>print</span>(<span>"R^4:</span><span>\n</span><span>"</span>, np.linalg.matrix_power(R, <span>4</span>))</span></code></pre></div>
<div>
<pre><code>Eigenvalues: [0.+1.j 0.-1.j]
Spectral radius: 1.0
R^4:
 [[1 0]
 [0 1]]</code></pre>
</div>
</div>
<p>Eigenvalues are ±i, with modulus 1 → pure oscillation.</p>
<ol start="4" type="1">
<li>Spectral radius with SymPy</li>
</ol>
<div id="6145d4c9" data-execution_count="390">
<div id="cb680"><pre><code><span id="cb680-1">M <span>=</span> Matrix([[<span>2</span>,<span>1</span>],[<span>1</span>,<span>2</span>]])</span>
<span id="cb680-2">eigs <span>=</span> M.eigenvals()</span>
<span id="cb680-3"><span>print</span>(<span>"Eigenvalues:"</span>, eigs)</span>
<span id="cb680-4"><span>print</span>(<span>"Spectral radius:"</span>, <span>max</span>(<span>abs</span>(ev) <span>for</span> ev <span>in</span> eigs))</span></code></pre></div>
<div>
<pre><code>Eigenvalues: {3: 1, 1: 1}
Spectral radius: 3</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-66">
<h4 data-anchor-id="try-it-yourself-66">Try It Yourself</h4>
<ol type="1">
<li>Build a diagonal matrix with entries 0.8, 1.0, and 1.1. Predict which direction dominates as powers grow.</li>
<li>Apply a random matrix repeatedly to a vector. Does it shrink, grow, or oscillate?</li>
<li>Check if a Markov chain transition matrix always has spectral radius 1.</li>
</ol>
</section>
<section id="the-takeaway-50">
<h4 data-anchor-id="the-takeaway-50">The Takeaway</h4>
<ul>
<li>The spectral radius is the key number that predicts growth, decay, or oscillation.</li>
<li>Long-term stability in dynamical systems is governed entirely by eigenvalue magnitudes.</li>
<li>This connects linear algebra directly to control theory, Markov chains, and differential equations.</li>
</ul>
</section>
</section>
<section id="markov-chains-and-steady-states-probabilities-as-linear-algebra">
<h3 data-anchor-id="markov-chains-and-steady-states-probabilities-as-linear-algebra">69. Markov Chains and Steady States (Probabilities as Linear Algebra)</h3>
<p>A Markov chain is a process that moves between states according to probabilities. The transitions are encoded in a stochastic matrix <span>\(P\)</span>:</p>
<ul>
<li>Each entry <span>\(p_{ij} \geq 0\)</span></li>
<li>Each row sums to 1</li>
</ul>
<p>If we start with a probability vector <span>\(v_0\)</span>, then after <span>\(k\)</span> steps:</p>
<p><span>\[
v_k = v_0 P^k
\]</span></p>
<p>A steady state is a probability vector <span>\(v\)</span> such that <span>\(vP = v\)</span>. It corresponds to eigenvalue <span>\(\lambda = 1\)</span>.</p>
<section id="set-up-your-lab-68">
<h4 data-anchor-id="set-up-your-lab-68">Set Up Your Lab</h4>
<div id="7cde0441" data-execution_count="391"><pre><code><span id="cb682-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb682-2"><span>from</span> sympy <span>import</span> Matrix</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-68">
<h4 data-anchor-id="step-by-step-code-walkthrough-68">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Simple two-state chain</li>
</ol>
<div id="455a26a2" data-execution_count="392">
<div id="cb683"><pre><code><span id="cb683-1">P <span>=</span> np.array([</span>
<span id="cb683-2">    [<span>0.9</span>, <span>0.1</span>],</span>
<span id="cb683-3">    [<span>0.5</span>, <span>0.5</span>]</span>
<span id="cb683-4">])</span>
<span id="cb683-5"></span>
<span id="cb683-6">v0 <span>=</span> np.array([<span>1.0</span>, <span>0.0</span>])  <span># start in state 1</span></span>
<span id="cb683-7"><span>for</span> k <span>in</span> [<span>1</span>, <span>2</span>, <span>5</span>, <span>10</span>, <span>50</span>]:</span>
<span id="cb683-8">    vk <span>=</span> v0 <span>@</span> np.linalg.matrix_power(P, k)</span>
<span id="cb683-9">    <span>print</span>(<span>f"Step </span><span>{</span>k<span>}</span><span>: </span><span>{</span>vk<span>}</span><span>"</span>)</span></code></pre></div>
<div>
<pre><code>Step 1: [0.9 0.1]
Step 2: [0.86 0.14]
Step 5: [0.83504 0.16496]
Step 10: [0.83335081 0.16664919]
Step 50: [0.83333333 0.16666667]</code></pre>
</div>
</div>
<p>The distribution stabilizes as <span>\(k\)</span> increases.</p>
<ol start="2" type="1">
<li>Steady state via eigenvector</li>
</ol>
<p>Find eigenvector for eigenvalue 1:</p>
<div id="cb0a204a" data-execution_count="393">
<div id="cb685"><pre><code><span id="cb685-1">eigvals, eigvecs <span>=</span> np.linalg.eig(P.T)</span>
<span id="cb685-2">steady_state <span>=</span> eigvecs[:, np.isclose(eigvals, <span>1</span>)]</span>
<span id="cb685-3">steady_state <span>=</span> steady_state <span>/</span> steady_state.<span>sum</span>()</span>
<span id="cb685-4"><span>print</span>(<span>"Steady state:"</span>, steady_state.real.flatten())</span></code></pre></div>
<div>
<pre><code>Steady state: [0.83333333 0.16666667]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>SymPy exact check</li>
</ol>
<div id="80b906eb" data-execution_count="394">
<div id="cb687"><pre><code><span id="cb687-1">P_sym <span>=</span> Matrix([[<span>0.9</span>,<span>0.1</span>],[<span>0.5</span>,<span>0.5</span>]])</span>
<span id="cb687-2">steady <span>=</span> P_sym.eigenvects()</span>
<span id="cb687-3"><span>print</span>(<span>"Eigen info:"</span>, steady)</span></code></pre></div>
<div>
<pre><code>Eigen info: [(1.00000000000000, 1, [Matrix([
[0.707106781186548],
[0.707106781186547]])]), (0.400000000000000, 1, [Matrix([
[-0.235702260395516],
[  1.17851130197758]])])]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>A 3-state example</li>
</ol>
<div id="b7878813" data-execution_count="395">
<div id="cb689"><pre><code><span id="cb689-1">Q <span>=</span> np.array([</span>
<span id="cb689-2">    [<span>0.3</span>, <span>0.7</span>, <span>0.0</span>],</span>
<span id="cb689-3">    [<span>0.2</span>, <span>0.5</span>, <span>0.3</span>],</span>
<span id="cb689-4">    [<span>0.1</span>, <span>0.2</span>, <span>0.7</span>]</span>
<span id="cb689-5">])</span>
<span id="cb689-6"></span>
<span id="cb689-7">eigvals, eigvecs <span>=</span> np.linalg.eig(Q.T)</span>
<span id="cb689-8">steady <span>=</span> eigvecs[:, np.isclose(eigvals, <span>1</span>)]</span>
<span id="cb689-9">steady <span>=</span> steady <span>/</span> steady.<span>sum</span>()</span>
<span id="cb689-10"><span>print</span>(<span>"Steady state for Q:"</span>, steady.real.flatten())</span></code></pre></div>
<div>
<pre><code>Steady state for Q: [0.17647059 0.41176471 0.41176471]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-67">
<h4 data-anchor-id="try-it-yourself-67">Try It Yourself</h4>
<ol type="1">
<li>Create a transition matrix where one state is absorbing (e.g., row = [0,0,1]). What happens to the steady state?</li>
<li>Simulate a random walk on 3 states. Does the steady state distribute evenly?</li>
<li>Compare long-run simulation with eigenvector computation.</li>
</ol>
</section>
<section id="the-takeaway-51">
<h4 data-anchor-id="the-takeaway-51">The Takeaway</h4>
<ul>
<li>Markov chains evolve by repeated multiplication with a stochastic matrix.</li>
<li>Steady states are eigenvectors with eigenvalue 1.</li>
<li>This framework powers real applications like PageRank, weather models, and queuing systems.</li>
</ul>
</section>
</section>
<section id="linear-differential-systems-solutions-via-eigen-decomposition">
<h3 data-anchor-id="linear-differential-systems-solutions-via-eigen-decomposition">70. Linear Differential Systems (Solutions via Eigen-Decomposition)</h3>
<p>Linear differential equations often reduce to systems of the form:</p>
<p><span>\[
\frac{d}{dt}x(t) = A x(t)
\]</span></p>
<p>where <span>\(A\)</span> is a matrix and <span>\(x(t)\)</span> is a vector of functions. The solution is given by the matrix exponential:</p>
<p><span>\[
x(t) = e^{At} x(0)
\]</span></p>
<p>If <span>\(A\)</span> is diagonalizable, this becomes simple using eigenvalues and eigenvectors.</p>
<section id="set-up-your-lab-69">
<h4 data-anchor-id="set-up-your-lab-69">Set Up Your Lab</h4>
<div id="025b57c6" data-execution_count="396"><pre><code><span id="cb691-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb691-2"><span>from</span> sympy <span>import</span> Matrix, exp, symbols</span>
<span id="cb691-3"><span>from</span> scipy.linalg <span>import</span> expm</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-69">
<h4 data-anchor-id="step-by-step-code-walkthrough-69">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Simple system with diagonal matrix</li>
</ol>
<p><span>\[
A = \begin{bmatrix} -1 &amp; 0 \\ 0 &amp; 2 \end{bmatrix}
\]</span></p>
<div id="84603a9b" data-execution_count="397">
<div id="cb692"><pre><code><span id="cb692-1">A <span>=</span> Matrix([[<span>-</span><span>1</span>,<span>0</span>],</span>
<span id="cb692-2">            [<span>0</span>, <span>2</span>]])</span>
<span id="cb692-3">t <span>=</span> symbols(<span>'t'</span>)</span>
<span id="cb692-4">expAt <span>=</span> (A<span>*</span>t).exp()</span>
<span id="cb692-5"><span>print</span>(<span>"e^</span><span>{At}</span><span> ="</span>)</span>
<span id="cb692-6"><span>print</span>(expAt)</span></code></pre></div>
<div>
<pre><code>e^{At} =
Matrix([[exp(-t), 0], [0, exp(2*t)]])</code></pre>
</div>
</div>
<p>Solution:</p>
<p><span>\[
x(t) = \begin{bmatrix} e^{-t} &amp; 0 \\ 0 &amp; e^{2t} \end{bmatrix} x(0)
\]</span></p>
<p>One component decays, the other grows.</p>
<ol start="2" type="1">
<li>Non-diagonal example</li>
</ol>
<div id="65e26cf2" data-execution_count="398">
<div id="cb694"><pre><code><span id="cb694-1">B <span>=</span> Matrix([[<span>0</span>,<span>1</span>],</span>
<span id="cb694-2">            [<span>-</span><span>2</span>,<span>-</span><span>3</span>]])</span>
<span id="cb694-3">expBt <span>=</span> (B<span>*</span>t).exp()</span>
<span id="cb694-4"><span>print</span>(<span>"e^</span><span>{Bt}</span><span> ="</span>)</span>
<span id="cb694-5"><span>print</span>(expBt)</span></code></pre></div>
<div>
<pre><code>e^{Bt} =
Matrix([[2*exp(-t) - exp(-2*t), exp(-t) - exp(-2*t)], [-2*exp(-t) + 2*exp(-2*t), -exp(-t) + 2*exp(-2*t)]])</code></pre>
</div>
</div>
<p>Here the solution involves exponentials and possibly sines/cosines (oscillatory behavior).</p>
<ol start="3" type="1">
<li>Numeric computation with SciPy</li>
</ol>
<div id="fe38411c" data-execution_count="399">
<div id="cb696"><pre><code><span id="cb696-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb696-2"><span>from</span> scipy.linalg <span>import</span> expm</span>
<span id="cb696-3"></span>
<span id="cb696-4">A <span>=</span> np.array([[<span>-</span><span>1</span>,<span>0</span>],[<span>0</span>,<span>2</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb696-5">t <span>=</span> <span>1.0</span></span>
<span id="cb696-6"><span>print</span>(<span>"Matrix exponential e^</span><span>{At}</span><span> at t=1:</span><span>\n</span><span>"</span>, expm(A<span>*</span>t))</span></code></pre></div>
<div>
<pre><code>Matrix exponential e^{At} at t=1:
 [[0.36787944 0.        ]
 [0.         7.3890561 ]]</code></pre>
</div>
</div>
<p>This computes <span>\(e^{At}\)</span> numerically.</p>
<ol start="4" type="1">
<li>Simulation of a trajectory</li>
</ol>
<div id="7e2a7903" data-execution_count="400">
<div id="cb698"><pre><code><span id="cb698-1">x0 <span>=</span> np.array([<span>1.0</span>, <span>1.0</span>])</span>
<span id="cb698-2"><span>for</span> t <span>in</span> [<span>0</span>, <span>0.5</span>, <span>1</span>, <span>2</span>]:</span>
<span id="cb698-3">    xt <span>=</span> expm(A<span>*</span>t) <span>@</span> x0</span>
<span id="cb698-4">    <span>print</span>(<span>f"x(</span><span>{</span>t<span>}</span><span>) = </span><span>{</span>xt<span>}</span><span>"</span>)</span></code></pre></div>
<div>
<pre><code>x(0) = [1. 1.]
x(0.5) = [0.60653066 2.71828183]
x(1) = [0.36787944 7.3890561 ]
x(2) = [ 0.13533528 54.59815003]</code></pre>
</div>
</div>
<p>One coordinate decays, the other explodes with time.</p>
</section>
<section id="try-it-yourself-68">
<h4 data-anchor-id="try-it-yourself-68">Try It Yourself</h4>
<ol type="1">
<li>Solve the system <span>\(\dot{x} = \begin{bmatrix} 0 &amp; 1 \\ -1 &amp; 0 \end{bmatrix}x\)</span>. What kind of motion do you see?</li>
<li>Use SciPy to simulate a system with eigenvalues less than 0. Does it always decay?</li>
<li>Try an unstable system with eigenvalues &gt; 0 and watch how trajectories diverge.</li>
</ol>
</section>
<section id="the-takeaway-52">
<h4 data-anchor-id="the-takeaway-52">The Takeaway</h4>
<ul>
<li>Linear systems <span>\(\dot{x} = Ax\)</span> are solved via the matrix exponential.</li>
<li>Eigenvalues determine stability: negative real parts = stable, positive = unstable, imaginary = oscillations.</li>
<li>This ties linear algebra directly to differential equations and dynamical systems.</li>
</ul>
</section>
</section>
</section>
<section id="chapter-8.-orthogonality-least-squars-and-qr">
<h2 data-anchor-id="chapter-8.-orthogonality-least-squars-and-qr">Chapter 8. Orthogonality, least squars, and QR</h2>
<section id="inner-products-beyond-dot-product-custom-notions-of-angle">
<h3 data-anchor-id="inner-products-beyond-dot-product-custom-notions-of-angle">71. Inner Products Beyond Dot Product (Custom Notions of Angle)</h3>
<p>The dot product is the standard inner product in <span>\(\mathbb{R}^n\)</span>, but linear algebra allows us to define more general inner products that measure length and angle in different ways.</p>
<p>An inner product on a vector space is a function <span>\(\langle u, v \rangle\)</span> that satisfies:</p>
<ol type="1">
<li>Linearity in the first argument.</li>
<li>Symmetry: <span>\(\langle u, v \rangle = \langle v, u \rangle\)</span>.</li>
<li>Positive definiteness: <span>\(\langle v, v \rangle \geq 0\)</span> and equals 0 only if <span>\(v=0\)</span>.</li>
</ol>
<section id="set-up-your-lab-70">
<h4 data-anchor-id="set-up-your-lab-70">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-70">
<h4 data-anchor-id="step-by-step-code-walkthrough-70">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Standard dot product</li>
</ol>
<div id="6ebd434c" data-execution_count="402"><pre><code><span id="cb701-1">u <span>=</span> np.array([<span>1</span>,<span>2</span>,<span>3</span>])</span>
<span id="cb701-2">v <span>=</span> np.array([<span>4</span>,<span>5</span>,<span>6</span>])</span>
<span id="cb701-3"></span>
<span id="cb701-4"><span>print</span>(<span>"Dot product:"</span>, np.dot(u,v))</span></code></pre></div>
<p>This is the familiar formula: <span>\(1·4 + 2·5 + 3·6 = 32\)</span>.</p>
<ol start="2" type="1">
<li>Weighted inner product</li>
</ol>
<p>We can define:</p>
<p><span>\[
\langle u, v \rangle_W = u^T W v
\]</span></p>
<p>where <span>\(W\)</span> is a positive definite matrix.</p>
<div id="2295fdeb" data-execution_count="403">
<div id="cb703"><pre><code><span id="cb703-1">W <span>=</span> np.array([[<span>2</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb703-2">              [<span>0</span>,<span>1</span>,<span>0</span>],</span>
<span id="cb703-3">              [<span>0</span>,<span>0</span>,<span>3</span>]])</span>
<span id="cb703-4"></span>
<span id="cb703-5"><span>def</span> weighted_inner(u,v,W):</span>
<span id="cb703-6">    <span>return</span> u.T <span>@</span> W <span>@</span> v</span>
<span id="cb703-7"></span>
<span id="cb703-8"><span>print</span>(<span>"Weighted inner product:"</span>, weighted_inner(u,v,W))</span></code></pre></div>
<div>
<pre><code>Weighted inner product: 72</code></pre>
</div>
</div>
<p>Here, some coordinates “count more” than others.</p>
<ol start="3" type="1">
<li>Check symmetry and positivity</li>
</ol>
<div id="a9f8b616" data-execution_count="404">
<div id="cb705"><pre><code><span id="cb705-1"><span>print</span>(<span>"⟨u,v⟩ == ⟨v,u⟩ ?"</span>, weighted_inner(u,v,W) <span>==</span> weighted_inner(v,u,W))</span>
<span id="cb705-2"><span>print</span>(<span>"⟨u,u⟩ (should be &gt;0):"</span>, weighted_inner(u,u,W))</span></code></pre></div>
<div>
<pre><code>⟨u,v⟩ == ⟨v,u⟩ ? True
⟨u,u⟩ (should be &gt;0): 33</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Angle with weighted inner product</li>
</ol>
<p><span>\[
\cos\theta = \frac{\langle u,v \rangle_W}{\|u\|_W \, \|v\|_W}
\]</span></p>
<div id="ddb96cd3" data-execution_count="405">
<div id="cb707"><pre><code><span id="cb707-1"><span>def</span> weighted_norm(u,W):</span>
<span id="cb707-2">    <span>return</span> np.sqrt(weighted_inner(u,u,W))</span>
<span id="cb707-3"></span>
<span id="cb707-4">cos_theta <span>=</span> weighted_inner(u,v,W) <span>/</span> (weighted_norm(u,W) <span>*</span> weighted_norm(v,W))</span>
<span id="cb707-5"><span>print</span>(<span>"Cosine of angle (weighted):"</span>, cos_theta)</span></code></pre></div>
<div>
<pre><code>Cosine of angle (weighted): 0.97573875381809</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Custom example: correlation inner product</li>
</ol>
<p>For statistics, an inner product can be defined as covariance or correlation. Example with mean-centered vectors:</p>
<div id="306d54b4" data-execution_count="406">
<div id="cb709"><pre><code><span id="cb709-1">x <span>=</span> np.array([<span>2</span>,<span>4</span>,<span>6</span>])</span>
<span id="cb709-2">y <span>=</span> np.array([<span>1</span>,<span>3</span>,<span>5</span>])</span>
<span id="cb709-3"></span>
<span id="cb709-4">x_centered <span>=</span> x <span>-</span> x.mean()</span>
<span id="cb709-5">y_centered <span>=</span> y <span>-</span> y.mean()</span>
<span id="cb709-6"></span>
<span id="cb709-7">corr_inner <span>=</span> np.dot(x_centered,y_centered)</span>
<span id="cb709-8"><span>print</span>(<span>"Correlation-style inner product:"</span>, corr_inner)</span></code></pre></div>
<div>
<pre><code>Correlation-style inner product: 8.0</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-69">
<h4 data-anchor-id="try-it-yourself-69">Try It Yourself</h4>
<ol type="1">
<li>Define a custom inner product with <span>\(W = \text{diag}(1,10,100)\)</span>. How does it change angles between vectors?</li>
<li>Verify positivity: compute <span>\(\langle v, v \rangle_W\)</span> for a random vector <span>\(v\)</span>.</li>
<li>Compare dot product vs weighted inner product on the same pair of vectors.</li>
</ol>
</section>
<section id="the-takeaway-53">
<h4 data-anchor-id="the-takeaway-53">The Takeaway</h4>
<ul>
<li>Inner products generalize the dot product to new “geometries.”</li>
<li>By changing the weight matrix <span>\(W\)</span>, you change how lengths and angles are measured.</li>
<li>This flexibility is essential in statistics, optimization, and machine learning.</li>
</ul>
</section>
</section>
<section id="orthogonality-and-orthonormal-bases-perpendicular-power">
<h3 data-anchor-id="orthogonality-and-orthonormal-bases-perpendicular-power">72. Orthogonality and Orthonormal Bases (Perpendicular Power)</h3>
<p>Two vectors are orthogonal if their inner product is zero:</p>
<p><span>\[
\langle u, v \rangle = 0
\]</span></p>
<p>If, in addition, each vector has length 1, the set is orthonormal. Orthonormal bases are extremely useful because they simplify computations: projections, decompositions, and coordinate changes all become clean.</p>
<section id="set-up-your-lab-71">
<h4 data-anchor-id="set-up-your-lab-71">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-71">
<h4 data-anchor-id="step-by-step-code-walkthrough-71">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Check orthogonality</li>
</ol>
<div id="a037b3e3" data-execution_count="408"><pre><code><span id="cb712-1">u <span>=</span> np.array([<span>1</span>, <span>-</span><span>1</span>])</span>
<span id="cb712-2">v <span>=</span> np.array([<span>1</span>, <span>1</span>])</span>
<span id="cb712-3"></span>
<span id="cb712-4"><span>print</span>(<span>"Dot product:"</span>, np.dot(u,v))</span></code></pre></div>
<p>Since the dot product is 0, they’re orthogonal.</p>
<ol start="2" type="1">
<li>Normalizing vectors</li>
</ol>
<p><span>\[
\hat{u} = \frac{u}{\|u\|}
\]</span></p>
<div id="036d482b" data-execution_count="409">
<div id="cb714"><pre><code><span id="cb714-1"><span>def</span> normalize(vec):</span>
<span id="cb714-2">    <span>return</span> vec <span>/</span> np.linalg.norm(vec)</span>
<span id="cb714-3"></span>
<span id="cb714-4">u_norm <span>=</span> normalize(u)</span>
<span id="cb714-5">v_norm <span>=</span> normalize(v)</span>
<span id="cb714-6"></span>
<span id="cb714-7"><span>print</span>(<span>"Normalized u:"</span>, u_norm)</span>
<span id="cb714-8"><span>print</span>(<span>"Normalized v:"</span>, v_norm)</span></code></pre></div>
<div>
<pre><code>Normalized u: [ 0.70710678 -0.70710678]
Normalized v: [0.70710678 0.70710678]</code></pre>
</div>
</div>
<p>Now both have length 1.</p>
<ol start="3" type="1">
<li>Form an orthonormal basis</li>
</ol>
<div id="95a9b9aa" data-execution_count="410">
<div id="cb716"><pre><code><span id="cb716-1">basis <span>=</span> np.column_stack((u_norm, v_norm))</span>
<span id="cb716-2"><span>print</span>(<span>"Orthonormal basis:</span><span>\n</span><span>"</span>, basis)</span>
<span id="cb716-3"></span>
<span id="cb716-4"><span>print</span>(<span>"Check inner products:</span><span>\n</span><span>"</span>, basis.T <span>@</span> basis)</span></code></pre></div>
<div>
<pre><code>Orthonormal basis:
 [[ 0.70710678  0.70710678]
 [-0.70710678  0.70710678]]
Check inner products:
 [[ 1.00000000e+00 -2.23711432e-17]
 [-2.23711432e-17  1.00000000e+00]]</code></pre>
</div>
</div>
<p>The result is the identity matrix → perfectly orthonormal.</p>
<ol start="4" type="1">
<li>Apply to coordinates</li>
</ol>
<p>If <span>\(x = [2,3]\)</span>, coordinates in the orthonormal basis are:</p>
<div id="0452785a" data-execution_count="411">
<div id="cb718"><pre><code><span id="cb718-1">x <span>=</span> np.array([<span>2</span>,<span>3</span>])</span>
<span id="cb718-2">coords <span>=</span> basis.T <span>@</span> x</span>
<span id="cb718-3"><span>print</span>(<span>"Coordinates in new basis:"</span>, coords)</span>
<span id="cb718-4"><span>print</span>(<span>"Reconstruction:"</span>, basis <span>@</span> coords)</span></code></pre></div>
<div>
<pre><code>Coordinates in new basis: [-0.70710678  3.53553391]
Reconstruction: [2. 3.]</code></pre>
</div>
</div>
<p>It reconstructs exactly.</p>
<ol start="5" type="1">
<li>Random example with QR</li>
</ol>
<p>Any set of linearly independent vectors can be orthonormalized (Gram–Schmidt, or QR decomposition):</p>
<div id="bd09a889" data-execution_count="412">
<div id="cb720"><pre><code><span id="cb720-1">M <span>=</span> np.random.rand(<span>3</span>,<span>3</span>)</span>
<span id="cb720-2">Q, R <span>=</span> np.linalg.qr(M)</span>
<span id="cb720-3"><span>print</span>(<span>"Q (orthonormal basis):</span><span>\n</span><span>"</span>, Q)</span>
<span id="cb720-4"><span>print</span>(<span>"Check Q^T Q = I:</span><span>\n</span><span>"</span>, Q.T <span>@</span> Q)</span></code></pre></div>
<div>
<pre><code>Q (orthonormal basis):
 [[-0.37617518  0.91975919 -0.111961  ]
 [-0.82070726 -0.38684608 -0.42046368]
 [-0.430037   -0.06628079  0.90037494]]
Check Q^T Q = I:
 [[1.00000000e+00 5.55111512e-17 5.55111512e-17]
 [5.55111512e-17 1.00000000e+00 3.47849792e-17]
 [5.55111512e-17 3.47849792e-17 1.00000000e+00]]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-70">
<h4 data-anchor-id="try-it-yourself-70">Try It Yourself</h4>
<ol type="1">
<li>Create two 3D vectors and check if they’re orthogonal.</li>
<li>Normalize them to form an orthonormal set.</li>
<li>Use <code>np.linalg.qr</code> on a 4×3 random matrix and verify that the columns of <span>\(Q\)</span> are orthonormal.</li>
</ol>
</section>
<section id="the-takeaway-54">
<h4 data-anchor-id="the-takeaway-54">The Takeaway</h4>
<ul>
<li>Orthogonality means perpendicularity; orthonormality adds unit length.</li>
<li>Orthonormal bases simplify coordinate systems, making inner products and projections easy.</li>
<li>QR decomposition is the practical tool to generate orthonormal bases in higher dimensions.</li>
</ul>
</section>
</section>
<section id="gramschmidt-process-constructing-orthonormal-bases">
<h3 data-anchor-id="gramschmidt-process-constructing-orthonormal-bases">73. Gram–Schmidt Process (Constructing Orthonormal Bases)</h3>
<p>The Gram–Schmidt process takes a set of linearly independent vectors and turns them into an orthonormal basis. This is crucial for working with subspaces, projections, and numerical stability.</p>
<p>Given vectors <span>\(v_1, v_2, \dots, v_n\)</span>:</p>
<ol type="1">
<li><p>Set <span>\(u_1 = v_1\)</span>.</p></li>
<li><p>Subtract projections to make each new vector orthogonal to the earlier ones:</p>
<p><span>\[
u_k = v_k - \sum_{j=1}^{k-1} \frac{\langle v_k, u_j \rangle}{\langle u_j, u_j \rangle} u_j
\]</span></p></li>
<li><p>Normalize:</p>
<p><span>\[
e_k = \frac{u_k}{\|u_k\|}
\]</span></p></li>
</ol>
<p>The set <span>\(\{e_1, e_2, \dots, e_n\}\)</span> is orthonormal.</p>
<section id="set-up-your-lab-72">
<h4 data-anchor-id="set-up-your-lab-72">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-72">
<h4 data-anchor-id="step-by-step-code-walkthrough-72">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Define vectors</li>
</ol>
<div id="8a5f3246" data-execution_count="414"><pre><code><span id="cb723-1">v1 <span>=</span> np.array([<span>1.0</span>, <span>1.0</span>, <span>0.0</span>])</span>
<span id="cb723-2">v2 <span>=</span> np.array([<span>1.0</span>, <span>0.0</span>, <span>1.0</span>])</span>
<span id="cb723-3">v3 <span>=</span> np.array([<span>0.0</span>, <span>1.0</span>, <span>1.0</span>])</span>
<span id="cb723-4">V <span>=</span> [v1, v2, v3]</span></code></pre></div>
<ol start="2" type="1">
<li>Implement Gram–Schmidt</li>
</ol>
<div id="5034c58e" data-execution_count="415">
<div id="cb724"><pre><code><span id="cb724-1"><span>def</span> gram_schmidt(V):</span>
<span id="cb724-2">    U <span>=</span> []</span>
<span id="cb724-3">    <span>for</span> v <span>in</span> V:</span>
<span id="cb724-4">        u <span>=</span> v.copy()</span>
<span id="cb724-5">        <span>for</span> uj <span>in</span> U:</span>
<span id="cb724-6">            u <span>-=</span> np.dot(v, uj) <span>/</span> np.dot(uj, uj) <span>*</span> uj</span>
<span id="cb724-7">        U.append(u)</span>
<span id="cb724-8">    <span># Normalize</span></span>
<span id="cb724-9">    E <span>=</span> [u<span>/</span>np.linalg.norm(u) <span>for</span> u <span>in</span> U]</span>
<span id="cb724-10">    <span>return</span> np.array(E)</span>
<span id="cb724-11"></span>
<span id="cb724-12">E <span>=</span> gram_schmidt(V)</span>
<span id="cb724-13"><span>print</span>(<span>"Orthonormal basis:</span><span>\n</span><span>"</span>, E)</span>
<span id="cb724-14"><span>print</span>(<span>"Check orthonormality:</span><span>\n</span><span>"</span>, np.<span>round</span>(E <span>@</span> E.T, <span>6</span>))</span></code></pre></div>
<div>
<pre><code>Orthonormal basis:
 [[ 0.70710678  0.70710678  0.        ]
 [ 0.40824829 -0.40824829  0.81649658]
 [-0.57735027  0.57735027  0.57735027]]
Check orthonormality:
 [[1. 0. 0.]
 [0. 1. 0.]
 [0. 0. 1.]]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Compare with NumPy QR</li>
</ol>
<div id="adac0897" data-execution_count="416">
<div id="cb726"><pre><code><span id="cb726-1">Q, R <span>=</span> np.linalg.qr(np.column_stack(V))</span>
<span id="cb726-2"><span>print</span>(<span>"QR-based orthonormal basis:</span><span>\n</span><span>"</span>, Q)</span>
<span id="cb726-3"><span>print</span>(<span>"Check Q^T Q = I:</span><span>\n</span><span>"</span>, np.<span>round</span>(Q.T <span>@</span> Q, <span>6</span>))</span></code></pre></div>
<div>
<pre><code>QR-based orthonormal basis:
 [[-0.70710678  0.40824829 -0.57735027]
 [-0.70710678 -0.40824829  0.57735027]
 [-0.          0.81649658  0.57735027]]
Check Q^T Q = I:
 [[ 1.  0. -0.]
 [ 0.  1. -0.]
 [-0. -0.  1.]]</code></pre>
</div>
</div>
<p>Both methods give orthonormal bases.</p>
<ol start="4" type="1">
<li>Application: projection</li>
</ol>
<p>To project a vector <span>\(x\)</span> onto the span of <span>\(V\)</span>:</p>
<div id="31c8f020" data-execution_count="417">
<div id="cb728"><pre><code><span id="cb728-1">x <span>=</span> np.array([<span>2.0</span>, <span>2.0</span>, <span>2.0</span>])</span>
<span id="cb728-2">proj <span>=</span> <span>sum</span>((x <span>@</span> e) <span>*</span> e <span>for</span> e <span>in</span> E)</span>
<span id="cb728-3"><span>print</span>(<span>"Projection of x onto span(V):"</span>, proj)</span></code></pre></div>
<div>
<pre><code>Projection of x onto span(V): [2. 2. 2.]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-71">
<h4 data-anchor-id="try-it-yourself-71">Try It Yourself</h4>
<ol type="1">
<li>Run Gram–Schmidt on two vectors in 2D. Compare with just normalizing and checking orthogonality.</li>
<li>Replace one vector with a linear combination of others. What happens?</li>
<li>Use QR decomposition on a 4×3 random matrix and compare with Gram–Schmidt.</li>
</ol>
</section>
<section id="the-takeaway-55">
<h4 data-anchor-id="the-takeaway-55">The Takeaway</h4>
<ul>
<li>Gram–Schmidt converts arbitrary independent vectors into an orthonormal basis.</li>
<li>Orthonormal bases simplify projections, decompositions, and computations.</li>
<li>In practice, QR decomposition is often used as a numerically stable implementation.</li>
</ul>
</section>
</section>
<section id="orthogonal-projections-onto-subspaces-closest-point-principle">
<h3 data-anchor-id="orthogonal-projections-onto-subspaces-closest-point-principle">74. Orthogonal Projections onto Subspaces (Closest Point Principle)</h3>
<p>Given a subspace spanned by vectors, the orthogonal projection of a vector <span>\(x\)</span> onto the subspace is the point in the subspace that is closest to <span>\(x\)</span>. This is a cornerstone idea in least squares, data fitting, and signal processing.</p>
<section id="formula-recap">
<h4 data-anchor-id="formula-recap">Formula Recap</h4>
<p>If <span>\(Q\)</span> is a matrix with orthonormal columns spanning the subspace, the projection of <span>\(x\)</span> is:</p>
<p><span>\[
\text{proj}(x) = Q Q^T x
\]</span></p>
</section>
<section id="set-up-your-lab-73">
<h4 data-anchor-id="set-up-your-lab-73">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-73">
<h4 data-anchor-id="step-by-step-code-walkthrough-73">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Projection onto a line (1D subspace)</li>
</ol>
<p>Suppose the subspace is spanned by <span>\(u = [1,2]\)</span>.</p>
<div id="086fb728" data-execution_count="419">
<div id="cb731"><pre><code><span id="cb731-1">u <span>=</span> np.array([<span>1.0</span>,<span>2.0</span>])</span>
<span id="cb731-2">x <span>=</span> np.array([<span>3.0</span>,<span>1.0</span>])</span>
<span id="cb731-3"></span>
<span id="cb731-4">u_norm <span>=</span> u <span>/</span> np.linalg.norm(u)</span>
<span id="cb731-5">proj <span>=</span> np.dot(x, u_norm) <span>*</span> u_norm</span>
<span id="cb731-6"><span>print</span>(<span>"Projection of x onto span(u):"</span>, proj)</span></code></pre></div>
<div>
<pre><code>Projection of x onto span(u): [1. 2.]</code></pre>
</div>
</div>
<p>This gives the closest point to <span>\(x\)</span> along the line spanned by <span>\(u\)</span>.</p>
<ol start="2" type="1">
<li>Projection onto a plane (2D subspace in 3D)</li>
</ol>
<div id="c7c42046" data-execution_count="420">
<div id="cb733"><pre><code><span id="cb733-1">u1 <span>=</span> np.array([<span>1.0</span>,<span>0.0</span>,<span>0.0</span>])</span>
<span id="cb733-2">u2 <span>=</span> np.array([<span>0.0</span>,<span>1.0</span>,<span>0.0</span>])</span>
<span id="cb733-3">Q <span>=</span> np.column_stack([u1,u2])   <span># Orthonormal basis for xy-plane</span></span>
<span id="cb733-4"></span>
<span id="cb733-5">x <span>=</span> np.array([<span>2.0</span>,<span>3.0</span>,<span>5.0</span>])</span>
<span id="cb733-6">proj <span>=</span> Q <span>@</span> Q.T <span>@</span> x</span>
<span id="cb733-7"><span>print</span>(<span>"Projection of x onto xy-plane:"</span>, proj)</span></code></pre></div>
<div>
<pre><code>Projection of x onto xy-plane: [2. 3. 0.]</code></pre>
</div>
</div>
<p>Result drops the z-component → projection onto the plane.</p>
<ol start="3" type="1">
<li>General projection using QR</li>
</ol>
<div id="3cdebdd9" data-execution_count="421">
<div id="cb735"><pre><code><span id="cb735-1">A <span>=</span> np.array([[<span>1</span>,<span>1</span>,<span>0</span>],</span>
<span id="cb735-2">              [<span>0</span>,<span>1</span>,<span>1</span>],</span>
<span id="cb735-3">              [<span>1</span>,<span>0</span>,<span>1</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb735-4"></span>
<span id="cb735-5">Q, R <span>=</span> np.linalg.qr(A)</span>
<span id="cb735-6">Q <span>=</span> Q[:, :<span>2</span>]   <span># take first 2 independent columns</span></span>
<span id="cb735-7">x <span>=</span> np.array([<span>2</span>,<span>2</span>,<span>2</span>], dtype<span>=</span><span>float</span>)</span>
<span id="cb735-8"></span>
<span id="cb735-9">proj <span>=</span> Q <span>@</span> Q.T <span>@</span> x</span>
<span id="cb735-10"><span>print</span>(<span>"Projection of x onto span(A):"</span>, proj)</span></code></pre></div>
<div>
<pre><code>Projection of x onto span(A): [2.66666667 1.33333333 1.33333333]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Visualization (2D case)</li>
</ol>
<div id="3356fbdc" data-execution_count="422">
<div id="cb737"><pre><code><span id="cb737-1"><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb737-2"></span>
<span id="cb737-3">plt.quiver(<span>0</span>,<span>0</span>,x[<span>0</span>],x[<span>1</span>],angles<span>=</span><span>'xy'</span>,scale_units<span>=</span><span>'xy'</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>'red'</span>,label<span>=</span><span>"x"</span>)</span>
<span id="cb737-4">plt.quiver(<span>0</span>,<span>0</span>,proj[<span>0</span>],proj[<span>1</span>],angles<span>=</span><span>'xy'</span>,scale_units<span>=</span><span>'xy'</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>'blue'</span>,label<span>=</span><span>"Projection"</span>)</span>
<span id="cb737-5">plt.quiver(<span>0</span>,<span>0</span>,u[<span>0</span>],u[<span>1</span>],angles<span>=</span><span>'xy'</span>,scale_units<span>=</span><span>'xy'</span>,scale<span>=</span><span>1</span>,color<span>=</span><span>'green'</span>,label<span>=</span><span>"Subspace"</span>)</span>
<span id="cb737-6">plt.axis(<span>'equal'</span>)<span>;</span> plt.grid()<span>;</span> plt.legend()<span>;</span> plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-423-output-1.png" width="590" height="411"></p>
</figure>
</div>
</div>
</section>
<section id="try-it-yourself-72">
<h4 data-anchor-id="try-it-yourself-72">Try It Yourself</h4>
<ol type="1">
<li>Project a vector onto the line spanned by <span>\([2,1]\)</span>.</li>
<li>Project <span>\([1,2,3]\)</span> onto the plane spanned by <span>\([1,0,1]\)</span> and <span>\([0,1,1]\)</span>.</li>
<li>Compare projection via formula <span>\(Q Q^T x\)</span> with manually solving least squares.</li>
</ol>
</section>
<section id="the-takeaway-56">
<h4 data-anchor-id="the-takeaway-56">The Takeaway</h4>
<ul>
<li>Orthogonal projection finds the closest point in a subspace.</li>
<li>Formula <span>\(Q Q^T x\)</span> works perfectly when <span>\(Q\)</span> has orthonormal columns.</li>
<li>Projections are the foundation of least squares, PCA, and many geometric algorithms.</li>
</ul>
</section>
</section>
<section id="least-squares-problems-fit-when-exact-solve-is-impossible">
<h3 data-anchor-id="least-squares-problems-fit-when-exact-solve-is-impossible">75. Least-Squares Problems (Fit When Exact Solve Is Impossible)</h3>
<p>Sometimes a system of equations <span>\(Ax = b\)</span> has no exact solution - usually because it’s overdetermined (more equations than unknowns). In this case, we look for an approximate solution <span>\(x^*\)</span> that minimizes the error:</p>
<p><span>\[
x^* = \arg\min_x \|Ax - b\|^2
\]</span></p>
<p>This is the least-squares solution, which geometrically is the projection of <span>\(b\)</span> onto the column space of <span>\(A\)</span>.</p>
<section id="set-up-your-lab-74">
<h4 data-anchor-id="set-up-your-lab-74">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-74">
<h4 data-anchor-id="step-by-step-code-walkthrough-74">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Overdetermined system</li>
</ol>
<p>3 equations, 2 unknowns:</p>
<div id="8f69aaee" data-execution_count="424"><pre><code><span id="cb739-1">A <span>=</span> np.array([[<span>1</span>,<span>1</span>],</span>
<span id="cb739-2">              [<span>1</span>,<span>2</span>],</span>
<span id="cb739-3">              [<span>1</span>,<span>3</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb739-4">b <span>=</span> np.array([<span>6</span>, <span>0</span>, <span>0</span>], dtype<span>=</span><span>float</span>)</span></code></pre></div>
<ol start="2" type="1">
<li>Solve least squares with NumPy</li>
</ol>
<div id="edfb6240" data-execution_count="425">
<div id="cb740"><pre><code><span id="cb740-1">x_star, residuals, rank, s <span>=</span> np.linalg.lstsq(A, b, rcond<span>=</span><span>None</span>)</span>
<span id="cb740-2"><span>print</span>(<span>"Least squares solution:"</span>, x_star)</span>
<span id="cb740-3"><span>print</span>(<span>"Residual norm squared:"</span>, residuals)</span></code></pre></div>
<div>
<pre><code>Least squares solution: [ 8. -3.]
Residual norm squared: [6.]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Compare with normal equations</li>
</ol>
<p><span>\[
A^T A x = A^T b
\]</span></p>
<div id="12303360" data-execution_count="426">
<div id="cb742"><pre><code><span id="cb742-1">x_normal <span>=</span> np.linalg.inv(A.T <span>@</span> A) <span>@</span> (A.T <span>@</span> b)</span>
<span id="cb742-2"><span>print</span>(<span>"Solution via normal equations:"</span>, x_normal)</span></code></pre></div>
<div>
<pre><code>Solution via normal equations: [ 8. -3.]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Geometric picture</li>
</ol>
<p>The least-squares solution projects <span>\(b\)</span> onto the column space of <span>\(A\)</span>:</p>
<div id="56f4051a" data-execution_count="427">
<div id="cb744"><pre><code><span id="cb744-1">proj <span>=</span> A <span>@</span> x_star</span>
<span id="cb744-2"><span>print</span>(<span>"Projection of b onto Col(A):"</span>, proj)</span>
<span id="cb744-3"><span>print</span>(<span>"Original b:"</span>, b)</span>
<span id="cb744-4"><span>print</span>(<span>"Error vector (b - proj):"</span>, b <span>-</span> proj)</span></code></pre></div>
<div>
<pre><code>Projection of b onto Col(A): [ 5.  2. -1.]
Original b: [6. 0. 0.]
Error vector (b - proj): [ 1. -2.  1.]</code></pre>
</div>
</div>
<p>The error vector is orthogonal to the column space.</p>
<ol start="5" type="1">
<li>Verify orthogonality condition</li>
</ol>
<p><span>\[
A^T (b - Ax^*) = 0
\]</span></p>
<div id="10b4733e" data-execution_count="428">
<div id="cb746"><pre><code><span id="cb746-1"><span>print</span>(<span>"Check orthogonality:"</span>, A.T <span>@</span> (b <span>-</span> A <span>@</span> x_star))</span></code></pre></div>
<div>
<pre><code>Check orthogonality: [0. 0.]</code></pre>
</div>
</div>
<p>The result should be (close to) zero.</p>
</section>
<section id="try-it-yourself-73">
<h4 data-anchor-id="try-it-yourself-73">Try It Yourself</h4>
<ol type="1">
<li>Create a taller <span>\(A\)</span> (say 5×2) with random numbers and solve least squares for a random <span>\(b\)</span>.</li>
<li>Compare the residual from <code>np.linalg.lstsq</code> with geometric intuition (projection).</li>
<li>Modify <span>\(b\)</span> so that the system has an exact solution. Check if least squares gives it exactly.</li>
</ol>
</section>
<section id="the-takeaway-57">
<h4 data-anchor-id="the-takeaway-57">The Takeaway</h4>
<ul>
<li>Least-squares finds the best-fit solution when no exact solution exists.</li>
<li>It works by projecting <span>\(b\)</span> onto the column space of <span>\(A\)</span>.</li>
<li>This principle underlies regression, curve fitting, and countless applications in data science.</li>
</ul>
</section>
</section>
<section id="normal-equations-and-geometry-of-residuals-why-it-works">
<h3 data-anchor-id="normal-equations-and-geometry-of-residuals-why-it-works">76. Normal Equations and Geometry of Residuals (Why It Works)</h3>
<p>The least-squares solution can be found by solving the normal equations:</p>
<p><span>\[
A^T A x = A^T b
\]</span></p>
<p>This comes from the condition that the residual vector</p>
<p><span>\[
r = b - Ax
\]</span></p>
<p>is orthogonal to the column space of <span>\(A\)</span>.</p>
<section id="set-up-your-lab-75">
<h4 data-anchor-id="set-up-your-lab-75">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-75">
<h4 data-anchor-id="step-by-step-code-walkthrough-75">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Build an overdetermined system</li>
</ol>
<div id="20d8f734" data-execution_count="430"><pre><code><span id="cb749-1">A <span>=</span> np.array([[<span>1</span>,<span>1</span>],</span>
<span id="cb749-2">              [<span>1</span>,<span>2</span>],</span>
<span id="cb749-3">              [<span>1</span>,<span>3</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb749-4">b <span>=</span> np.array([<span>6</span>, <span>0</span>, <span>0</span>], dtype<span>=</span><span>float</span>)</span></code></pre></div>
<ol start="2" type="1">
<li>Solve least squares via normal equations</li>
</ol>
<div id="f2d9f0e2" data-execution_count="431">
<div id="cb750"><pre><code><span id="cb750-1">ATA <span>=</span> A.T <span>@</span> A</span>
<span id="cb750-2">ATb <span>=</span> A.T <span>@</span> b</span>
<span id="cb750-3">x_star <span>=</span> np.linalg.solve(ATA, ATb)</span>
<span id="cb750-4"></span>
<span id="cb750-5"><span>print</span>(<span>"Least-squares solution x*:"</span>, x_star)</span></code></pre></div>
<div>
<pre><code>Least-squares solution x*: [ 8. -3.]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Compute residual and check orthogonality</li>
</ol>
<div id="5729da1a" data-execution_count="432">
<div id="cb752"><pre><code><span id="cb752-1">residual <span>=</span> b <span>-</span> A <span>@</span> x_star</span>
<span id="cb752-2"><span>print</span>(<span>"Residual vector:"</span>, residual)</span>
<span id="cb752-3"><span>print</span>(<span>"Check A^T r ≈ 0:"</span>, A.T <span>@</span> residual)</span></code></pre></div>
<div>
<pre><code>Residual vector: [ 1. -2.  1.]
Check A^T r ≈ 0: [0. 0.]</code></pre>
</div>
</div>
<p>This verifies the residual is perpendicular to the column space of <span>\(A\)</span>.</p>
<ol start="4" type="1">
<li>Compare with NumPy’s least squares solver</li>
</ol>
<div id="bec5c567" data-execution_count="433">
<div id="cb754"><pre><code><span id="cb754-1">x_lstsq, <span>*</span>_ <span>=</span> np.linalg.lstsq(A, b, rcond<span>=</span><span>None</span>)</span>
<span id="cb754-2"><span>print</span>(<span>"NumPy lstsq solution:"</span>, x_lstsq)</span></code></pre></div>
<div>
<pre><code>NumPy lstsq solution: [ 8. -3.]</code></pre>
</div>
</div>
<p>The solutions should match (within numerical precision).</p>
<ol start="5" type="1">
<li>Geometric picture</li>
</ol>
<ul>
<li><span>\(b\)</span> is a point in <span>\(\mathbb{R}^3\)</span>.</li>
<li><span>\(Ax\)</span> is restricted to lie in the 2D column space of <span>\(A\)</span>.</li>
<li>The least-squares solution picks the <span>\(Ax\)</span> closest to <span>\(b\)</span>.</li>
<li>The error vector <span>\(r = b - Ax^*\)</span> is orthogonal to the subspace.</li>
</ul>
<div id="66ac1fcd" data-execution_count="434">
<div id="cb756"><pre><code><span id="cb756-1">proj <span>=</span> A <span>@</span> x_star</span>
<span id="cb756-2"><span>print</span>(<span>"Projection of b onto Col(A):"</span>, proj)</span></code></pre></div>
<div>
<pre><code>Projection of b onto Col(A): [ 5.  2. -1.]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-74">
<h4 data-anchor-id="try-it-yourself-74">Try It Yourself</h4>
<ol type="1">
<li>Change <span>\(b\)</span> to <span>\([1,1,1]\)</span>. Solve again and check the residual.</li>
<li>Use a random tall <span>\(A\)</span> (say 6×2) and verify that the residual is always orthogonal.</li>
<li>Compute <span>\(\|r\|\)</span> and see how it changes when you change <span>\(b\)</span>.</li>
</ol>
</section>
<section id="the-takeaway-58">
<h4 data-anchor-id="the-takeaway-58">The Takeaway</h4>
<ul>
<li>Least squares works by making the residual orthogonal to the column space.</li>
<li>Normal equations are the algebraic way to encode this condition.</li>
<li>This orthogonality principle is the geometric heart of least-squares fitting.</li>
</ul>
</section>
</section>
<section id="qr-factorization-stable-least-squares-via-orthogonality">
<h3 data-anchor-id="qr-factorization-stable-least-squares-via-orthogonality">77. QR Factorization (Stable Least Squares via Orthogonality)</h3>
<p>While normal equations solve least squares, they can be numerically unstable if <span>\(A^T A\)</span> is ill-conditioned. A more stable method uses QR factorization:</p>
<p><span>\[
A = Q R
\]</span></p>
<ul>
<li><span>\(Q\)</span>: matrix with orthonormal columns</li>
<li><span>\(R\)</span>: upper triangular matrix</li>
</ul>
<p>Then the least-squares problem reduces to solving:</p>
<p><span>\[
Rx = Q^T b
\]</span></p>
<section id="set-up-your-lab-76">
<h4 data-anchor-id="set-up-your-lab-76">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-76">
<h4 data-anchor-id="step-by-step-code-walkthrough-76">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Overdetermined system</li>
</ol>
<div id="7c13ca99" data-execution_count="436"><pre><code><span id="cb759-1">A <span>=</span> np.array([[<span>1</span>,<span>1</span>],</span>
<span id="cb759-2">              [<span>1</span>,<span>2</span>],</span>
<span id="cb759-3">              [<span>1</span>,<span>3</span>]], dtype<span>=</span><span>float</span>)</span>
<span id="cb759-4">b <span>=</span> np.array([<span>6</span>, <span>0</span>, <span>0</span>], dtype<span>=</span><span>float</span>)</span></code></pre></div>
<ol start="2" type="1">
<li>QR factorization</li>
</ol>
<div id="a6360d97" data-execution_count="437">
<div id="cb760"><pre><code><span id="cb760-1">Q, R <span>=</span> np.linalg.qr(A)</span>
<span id="cb760-2"><span>print</span>(<span>"Q (orthonormal basis):</span><span>\n</span><span>"</span>, Q)</span>
<span id="cb760-3"><span>print</span>(<span>"R (upper triangular):</span><span>\n</span><span>"</span>, R)</span></code></pre></div>
<div>
<pre><code>Q (orthonormal basis):
 [[-5.77350269e-01  7.07106781e-01]
 [-5.77350269e-01 -1.73054947e-16]
 [-5.77350269e-01 -7.07106781e-01]]
R (upper triangular):
 [[-1.73205081 -3.46410162]
 [ 0.         -1.41421356]]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Solve least squares using QR</li>
</ol>
<div id="d5f20abe" data-execution_count="438">
<div id="cb762"><pre><code><span id="cb762-1">y <span>=</span> Q.T <span>@</span> b</span>
<span id="cb762-2">x_star <span>=</span> np.linalg.solve(R[:<span>2</span>,:], y[:<span>2</span>])  <span># only top rows matter</span></span>
<span id="cb762-3"><span>print</span>(<span>"Least squares solution via QR:"</span>, x_star)</span></code></pre></div>
<div>
<pre><code>Least squares solution via QR: [ 8. -3.]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Compare with NumPy’s <code>lstsq</code></li>
</ol>
<div id="2d1664dd" data-execution_count="439"><pre><code><span id="cb764-1">x_lstsq, <span>*</span>_ <span>=</span> np.linalg.lstsq(A, b, rcond<span>=</span><span>None</span>)</span>
<span id="cb764-2"><span>print</span>(<span>"NumPy lstsq:"</span>, x_lstsq)</span></code></pre></div>
<p>The answers should match closely.</p>
<ol start="5" type="1">
<li>Residual check</li>
</ol>
<div id="a3f1538a" data-execution_count="440">
<div id="cb766"><pre><code><span id="cb766-1">residual <span>=</span> b <span>-</span> A <span>@</span> x_star</span>
<span id="cb766-2"><span>print</span>(<span>"Residual vector:"</span>, residual)</span>
<span id="cb766-3"><span>print</span>(<span>"Check orthogonality (Q^T r):"</span>, Q.T <span>@</span> residual)</span></code></pre></div>
<div>
<pre><code>Residual vector: [ 1. -2.  1.]
Check orthogonality (Q^T r): [0.00000000e+00 3.46109895e-16]</code></pre>
</div>
</div>
<p>Residual is orthogonal to the column space, confirming correctness.</p>
</section>
<section id="try-it-yourself-75">
<h4 data-anchor-id="try-it-yourself-75">Try It Yourself</h4>
<ol type="1">
<li>Solve least squares for a 5×2 random matrix using both normal equations and QR. Compare results.</li>
<li>Check stability by making columns of <span>\(A\)</span> nearly dependent - see if QR behaves better than normal equations.</li>
<li>Compute projection of <span>\(b\)</span> using <span>\(Q Q^T b\)</span> and confirm it equals <span>\(A x^*\)</span>.</li>
</ol>
</section>
<section id="the-takeaway-59">
<h4 data-anchor-id="the-takeaway-59">The Takeaway</h4>
<ul>
<li>QR factorization provides a numerically stable way to solve least squares.</li>
<li>It avoids the instability of normal equations.</li>
<li>In practice, modern solvers (like NumPy’s <code>lstsq</code>) rely on QR or SVD under the hood.</li>
</ul>
</section>
</section>
<section id="orthogonal-matrices-length-preserving-transforms">
<h3 data-anchor-id="orthogonal-matrices-length-preserving-transforms">78. Orthogonal Matrices (Length-Preserving Transforms)</h3>
<p>An orthogonal matrix <span>\(Q\)</span> is a square matrix whose columns (and rows) are orthonormal vectors. Formally:</p>
<p><span>\[
Q^T Q = Q Q^T = I
\]</span></p>
<p>Key properties:</p>
<ul>
<li>Preserves lengths: <span>\(\|Qx\| = \|x\|\)</span></li>
<li>Preserves dot products: <span>\(\langle Qx, Qy \rangle = \langle x, y \rangle\)</span></li>
<li>Determinant is either <span>\(+1\)</span> (rotation) or <span>\(-1\)</span> (reflection)</li>
</ul>
<section id="set-up-your-lab-77">
<h4 data-anchor-id="set-up-your-lab-77">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-77">
<h4 data-anchor-id="step-by-step-code-walkthrough-77">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Construct a simple orthogonal matrix</li>
</ol>
<p>90° rotation in 2D:</p>
<div id="9253571b" data-execution_count="442"><pre><code><span id="cb769-1">Q <span>=</span> np.array([[<span>0</span>, <span>-</span><span>1</span>],</span>
<span id="cb769-2">              [<span>1</span>,  <span>0</span>]])</span>
<span id="cb769-3"></span>
<span id="cb769-4"><span>print</span>(<span>"Q^T Q =</span><span>\n</span><span>"</span>, Q.T <span>@</span> Q)</span></code></pre></div>
<p>Result = identity → confirms orthogonality.</p>
<ol start="2" type="1">
<li>Check length preservation</li>
</ol>
<div id="85611c49" data-execution_count="443">
<div id="cb771"><pre><code><span id="cb771-1">x <span>=</span> np.array([<span>3</span>,<span>4</span>])</span>
<span id="cb771-2"><span>print</span>(<span>"Original length:"</span>, np.linalg.norm(x))</span>
<span id="cb771-3"><span>print</span>(<span>"Transformed length:"</span>, np.linalg.norm(Q <span>@</span> x))</span></code></pre></div>
<div>
<pre><code>Original length: 5.0
Transformed length: 5.0</code></pre>
</div>
</div>
<p>Both lengths match.</p>
<ol start="3" type="1">
<li>Check dot product preservation</li>
</ol>
<div id="89b4257e" data-execution_count="444">
<div id="cb773"><pre><code><span id="cb773-1">u <span>=</span> np.array([<span>1</span>,<span>0</span>])</span>
<span id="cb773-2">v <span>=</span> np.array([<span>0</span>,<span>1</span>])</span>
<span id="cb773-3"></span>
<span id="cb773-4"><span>print</span>(<span>"Dot(u,v):"</span>, np.dot(u,v))</span>
<span id="cb773-5"><span>print</span>(<span>"Dot(Q u, Q v):"</span>, np.dot(Q <span>@</span> u, Q <span>@</span> v))</span></code></pre></div>
<div>
<pre><code>Dot(u,v): 0
Dot(Q u, Q v): 0</code></pre>
</div>
</div>
<p>Dot product is preserved.</p>
<ol start="4" type="1">
<li>Reflection matrix</li>
</ol>
<p>Reflection about the x-axis:</p>
<div id="c9fd18c3" data-execution_count="445">
<div id="cb775"><pre><code><span id="cb775-1">R <span>=</span> np.array([[<span>1</span>,<span>0</span>],</span>
<span id="cb775-2">              [<span>0</span>,<span>-</span><span>1</span>]])</span>
<span id="cb775-3"></span>
<span id="cb775-4"><span>print</span>(<span>"R^T R =</span><span>\n</span><span>"</span>, R.T <span>@</span> R)</span>
<span id="cb775-5"><span>print</span>(<span>"Determinant of R:"</span>, np.linalg.det(R))</span></code></pre></div>
<div>
<pre><code>R^T R =
 [[1 0]
 [0 1]]
Determinant of R: -1.0</code></pre>
</div>
</div>
<p>Determinant = -1 → reflection.</p>
<ol start="5" type="1">
<li>Random orthogonal matrix via QR</li>
</ol>
<div id="6438b79c" data-execution_count="446">
<div id="cb777"><pre><code><span id="cb777-1">M <span>=</span> np.random.rand(<span>3</span>,<span>3</span>)</span>
<span id="cb777-2">Q, _ <span>=</span> np.linalg.qr(M)</span>
<span id="cb777-3"><span>print</span>(<span>"Q (random orthogonal):</span><span>\n</span><span>"</span>, Q)</span>
<span id="cb777-4"><span>print</span>(<span>"Check Q^T Q ≈ I:</span><span>\n</span><span>"</span>, np.<span>round</span>(Q.T <span>@</span> Q, <span>6</span>))</span></code></pre></div>
<div>
<pre><code>Q (random orthogonal):
 [[-0.59472353  0.03725157 -0.80306677]
 [-0.61109913 -0.67000966  0.42147943]
 [-0.52236172  0.74141714  0.42123492]]
Check Q^T Q ≈ I:
 [[ 1.  0. -0.]
 [ 0.  1. -0.]
 [-0. -0.  1.]]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-76">
<h4 data-anchor-id="try-it-yourself-76">Try It Yourself</h4>
<ol type="1">
<li>Build a 2D rotation matrix for 45°. Verify it’s orthogonal.</li>
<li>Check whether scaling matrices (e.g., <span>\(\text{diag}(2,1)\)</span>) are orthogonal. Why or why not?</li>
<li>Generate a random orthogonal matrix with <code>np.linalg.qr</code> and test its determinant.</li>
</ol>
</section>
<section id="the-takeaway-60">
<h4 data-anchor-id="the-takeaway-60">The Takeaway</h4>
<ul>
<li>Orthogonal matrices are rigid motions: they rotate or reflect without distorting lengths or angles.</li>
<li>They play a key role in numerical stability, geometry, and physics.</li>
<li>Every orthonormal basis corresponds to an orthogonal matrix.</li>
</ul>
</section>
</section>
<section id="fourier-viewpoint-expanding-in-orthogonal-waves">
<h3 data-anchor-id="fourier-viewpoint-expanding-in-orthogonal-waves">79. Fourier Viewpoint (Expanding in Orthogonal Waves)</h3>
<p>The Fourier viewpoint treats functions or signals as combinations of orthogonal waves (sines and cosines). This is just linear algebra: sine and cosine functions form an orthogonal basis, and any signal can be expressed as a linear combination of them.</p>
<section id="formula-recap-1">
<h4 data-anchor-id="formula-recap-1">Formula Recap</h4>
<p>For a discrete signal <span>\(x\)</span>, the Discrete Fourier Transform (DFT) is:</p>
<p><span>\[
X_k = \sum_{n=0}^{N-1} x_n e^{-2\pi i kn / N}, \quad k=0,\dots,N-1
\]</span></p>
<p>The inverse DFT reconstructs the signal. Orthogonality of complex exponentials makes this work.</p>
</section>
<section id="set-up-your-lab-78">
<h4 data-anchor-id="set-up-your-lab-78">Set Up Your Lab</h4>
<div id="e77c2a30" data-execution_count="447"><pre><code><span id="cb779-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb779-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-78">
<h4 data-anchor-id="step-by-step-code-walkthrough-78">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Build a simple signal</li>
</ol>
<div id="51fb4f0e" data-execution_count="448">
<div id="cb780"><pre><code><span id="cb780-1">t <span>=</span> np.linspace(<span>0</span>, <span>1</span>, <span>100</span>, endpoint<span>=</span><span>False</span>)</span>
<span id="cb780-2">signal <span>=</span> np.sin(<span>2</span><span>*</span>np.pi<span>*</span><span>3</span><span>*</span>t) <span>+</span> <span>0.5</span><span>*</span>np.sin(<span>2</span><span>*</span>np.pi<span>*</span><span>5</span><span>*</span>t)</span>
<span id="cb780-3">plt.plot(t, signal)</span>
<span id="cb780-4">plt.title(<span>"Signal = sin(3Hz) + 0.5 sin(5Hz)"</span>)</span>
<span id="cb780-5">plt.xlabel(<span>"Time"</span>)</span>
<span id="cb780-6">plt.ylabel(<span>"Amplitude"</span>)</span>
<span id="cb780-7">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-449-output-1.png" width="600" height="449"></p>
</figure>
</div>
</div>
<ol start="2" type="1">
<li>Compute Fourier transform (DFT)</li>
</ol>
<div id="07c06869" data-execution_count="449">
<div id="cb781"><pre><code><span id="cb781-1">X <span>=</span> np.fft.fft(signal)</span>
<span id="cb781-2">freqs <span>=</span> np.fft.fftfreq(<span>len</span>(t), d<span>=</span><span>1</span><span>/</span><span>100</span>)  <span># sampling rate = 100Hz</span></span>
<span id="cb781-3"></span>
<span id="cb781-4">plt.stem(freqs[:<span>50</span>], np.<span>abs</span>(X[:<span>50</span>]), basefmt<span>=</span><span>" "</span>)</span>
<span id="cb781-5">plt.title(<span>"Fourier spectrum"</span>)</span>
<span id="cb781-6">plt.xlabel(<span>"Frequency (Hz)"</span>)</span>
<span id="cb781-7">plt.ylabel(<span>"Magnitude"</span>)</span>
<span id="cb781-8">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-450-output-1.png" width="585" height="449"></p>
</figure>
</div>
</div>
<p>Peaks appear at 3Hz and 5Hz → the frequencies of the original signal.</p>
<ol start="3" type="1">
<li>Reconstruct signal using inverse FFT</li>
</ol>
<div id="e4642538" data-execution_count="450">
<div id="cb782"><pre><code><span id="cb782-1">signal_reconstructed <span>=</span> np.fft.ifft(X).real</span>
<span id="cb782-2"><span>print</span>(<span>"Reconstruction error:"</span>, np.linalg.norm(signal <span>-</span> signal_reconstructed))</span></code></pre></div>
<div>
<pre><code>Reconstruction error: 1.4664679821708477e-15</code></pre>
</div>
</div>
<p>Error is near zero → perfect reconstruction.</p>
<ol start="4" type="1">
<li>Orthogonality check of sinusoids</li>
</ol>
<div id="285e0b74" data-execution_count="451">
<div id="cb784"><pre><code><span id="cb784-1">u <span>=</span> np.sin(<span>2</span><span>*</span>np.pi<span>*</span><span>3</span><span>*</span>t)</span>
<span id="cb784-2">v <span>=</span> np.sin(<span>2</span><span>*</span>np.pi<span>*</span><span>5</span><span>*</span>t)</span>
<span id="cb784-3"></span>
<span id="cb784-4">inner <span>=</span> np.dot(u, v)</span>
<span id="cb784-5"><span>print</span>(<span>"Inner product of 3Hz and 5Hz sinusoids:"</span>, inner)</span></code></pre></div>
<div>
<pre><code>Inner product of 3Hz and 5Hz sinusoids: 1.2961853812498703e-14</code></pre>
</div>
</div>
<p>The result is ≈ 0 → confirms orthogonality.</p>
</section>
<section id="try-it-yourself-77">
<h4 data-anchor-id="try-it-yourself-77">Try It Yourself</h4>
<ol type="1">
<li>Change the frequencies to 7Hz and 9Hz. Do the Fourier peaks move accordingly?</li>
<li>Mix in some noise and check how the spectrum looks.</li>
<li>Try cosine signals instead of sine. Do you still see orthogonality?</li>
</ol>
</section>
<section id="the-takeaway-61">
<h4 data-anchor-id="the-takeaway-61">The Takeaway</h4>
<ul>
<li>Fourier analysis = linear algebra with orthogonal sinusoidal basis functions.</li>
<li>Any signal can be decomposed into orthogonal waves.</li>
<li>This orthogonal viewpoint powers audio, image compression, and signal processing.</li>
</ul>
</section>
</section>
<section id="polynomial-and-multifeature-least-squares-fitting-more-flexibly">
<h3 data-anchor-id="polynomial-and-multifeature-least-squares-fitting-more-flexibly">80. Polynomial and Multifeature Least Squares (Fitting More Flexibly)</h3>
<p>Least squares isn’t limited to straight lines. By adding polynomial or multiple features, we can fit curves and capture more complex relationships. This is the foundation of regression models in data science.</p>
<section id="formula-recap-2">
<h4 data-anchor-id="formula-recap-2">Formula Recap</h4>
<p>Given data <span>\((x_i, y_i)\)</span>, we build a design matrix <span>\(A\)</span>:</p>
<ul>
<li>For polynomial fit of degree <span>\(d\)</span>:</li>
</ul>
<p><span>\[
A = \begin{bmatrix}
1 &amp; x_1 &amp; x_1^2 &amp; \dots &amp; x_1^d \\
1 &amp; x_2 &amp; x_2^2 &amp; \dots &amp; x_2^d \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_n &amp; x_n^2 &amp; \dots &amp; x_n^d
\end{bmatrix}
\]</span></p>
<p>Then solve least squares:</p>
<p><span>\[
\hat{c} = \arg\min_c \|Ac - y\|^2
\]</span></p>
</section>
<section id="set-up-your-lab-79">
<h4 data-anchor-id="set-up-your-lab-79">Set Up Your Lab</h4>
<div id="36db2604" data-execution_count="452"><pre><code><span id="cb786-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb786-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-79">
<h4 data-anchor-id="step-by-step-code-walkthrough-79">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Generate noisy quadratic data</li>
</ol>
<div id="195fc36b" data-execution_count="453">
<div id="cb787"><pre><code><span id="cb787-1">np.random.seed(<span>0</span>)</span>
<span id="cb787-2">x <span>=</span> np.linspace(<span>-</span><span>3</span>, <span>3</span>, <span>30</span>)</span>
<span id="cb787-3">y_true <span>=</span> <span>1</span> <span>-</span> <span>2</span><span>*</span>x <span>+</span> <span>0.5</span><span>*</span>x<span>**</span><span>2</span></span>
<span id="cb787-4">y_noisy <span>=</span> y_true <span>+</span> np.random.normal(scale<span>=</span><span>2.0</span>, size<span>=</span>x.shape)</span>
<span id="cb787-5"></span>
<span id="cb787-6">plt.scatter(x, y_noisy, label<span>=</span><span>"Noisy data"</span>)</span>
<span id="cb787-7">plt.plot(x, y_true, <span>"g--"</span>, label<span>=</span><span>"True curve"</span>)</span>
<span id="cb787-8">plt.legend()</span>
<span id="cb787-9">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-454-output-1.png" width="569" height="411"></p>
</figure>
</div>
</div>
<ol start="2" type="1">
<li>Build polynomial design matrix (degree 2)</li>
</ol>
<div id="13c91ce0" data-execution_count="454">
<div id="cb788"><pre><code><span id="cb788-1">A <span>=</span> np.column_stack([np.ones_like(x), x, x<span>**</span><span>2</span>])</span>
<span id="cb788-2">coeffs, <span>*</span>_ <span>=</span> np.linalg.lstsq(A, y_noisy, rcond<span>=</span><span>None</span>)</span>
<span id="cb788-3"><span>print</span>(<span>"Fitted coefficients:"</span>, coeffs)</span></code></pre></div>
<div>
<pre><code>Fitted coefficients: [ 1.15666306 -2.25753954  0.72733812]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Plot fitted polynomial</li>
</ol>
<div id="44256511" data-execution_count="455">
<div id="cb790"><pre><code><span id="cb790-1">y_fit <span>=</span> A <span>@</span> coeffs</span>
<span id="cb790-2">plt.scatter(x, y_noisy, label<span>=</span><span>"Noisy data"</span>)</span>
<span id="cb790-3">plt.plot(x, y_fit, <span>"r-"</span>, label<span>=</span><span>"Fitted quadratic"</span>)</span>
<span id="cb790-4">plt.legend()</span>
<span id="cb790-5">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-456-output-1.png" width="569" height="411"></p>
</figure>
</div>
</div>
<ol start="4" type="1">
<li>Higher-degree fit (overfitting demonstration)</li>
</ol>
<div id="bdc0c108" data-execution_count="456">
<div id="cb791"><pre><code><span id="cb791-1">A_high <span>=</span> np.column_stack([x<span>**</span>i <span>for</span> i <span>in</span> <span>range</span>(<span>6</span>)])  <span># degree 5</span></span>
<span id="cb791-2">coeffs_high, <span>*</span>_ <span>=</span> np.linalg.lstsq(A_high, y_noisy, rcond<span>=</span><span>None</span>)</span>
<span id="cb791-3"></span>
<span id="cb791-4">y_fit_high <span>=</span> A_high <span>@</span> coeffs_high</span>
<span id="cb791-5">plt.scatter(x, y_noisy, label<span>=</span><span>"Noisy data"</span>)</span>
<span id="cb791-6">plt.plot(x, y_fit_high, <span>"r-"</span>, label<span>=</span><span>"Degree 5 polynomial"</span>)</span>
<span id="cb791-7">plt.plot(x, y_true, <span>"g--"</span>, label<span>=</span><span>"True curve"</span>)</span>
<span id="cb791-8">plt.legend()</span>
<span id="cb791-9">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-457-output-1.png" width="569" height="411"></p>
</figure>
</div>
</div>
<ol start="5" type="1">
<li>Multifeature regression example</li>
</ol>
<p>Suppose we predict <span>\(y\)</span> from features <span>\([x, x^2, \sin(x)]\)</span>:</p>
<div id="3a6a7b6a" data-execution_count="457">
<div id="cb792"><pre><code><span id="cb792-1">A_multi <span>=</span> np.column_stack([np.ones_like(x), x, x<span>**</span><span>2</span>, np.sin(x)])</span>
<span id="cb792-2">coeffs_multi, <span>*</span>_ <span>=</span> np.linalg.lstsq(A_multi, y_noisy, rcond<span>=</span><span>None</span>)</span>
<span id="cb792-3"><span>print</span>(<span>"Multi-feature coefficients:"</span>, coeffs_multi)</span></code></pre></div>
<div>
<pre><code>Multi-feature coefficients: [ 1.15666306 -2.0492999   0.72733812 -0.65902274]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-78">
<h4 data-anchor-id="try-it-yourself-78">Try It Yourself</h4>
<ol type="1">
<li>Fit degree 3, 4, 5 polynomials to the same data. Watch how the curve changes.</li>
<li>Add features like <span>\(\cos(x)\)</span> or <span>\(\exp(x)\)</span> - does the fit improve?</li>
<li>Compare training error (fit to noisy data) vs error on new test points.</li>
</ol>
</section>
<section id="the-takeaway-62">
<h4 data-anchor-id="the-takeaway-62">The Takeaway</h4>
<ul>
<li>Least squares can fit polynomials and arbitrary feature combinations.</li>
<li>The design matrix encodes how input variables transform into features.</li>
<li>This is the basis of regression, curve fitting, and many machine learning models.</li>
</ul>
</section>
</section>
</section>
<section id="chapter-9.-svd-pca-and-conditioning">
<h2 data-anchor-id="chapter-9.-svd-pca-and-conditioning">Chapter 9. SVD, PCA, and Conditioning</h2>
<section id="singular-values-and-svd-universal-factorization">
<h3 data-anchor-id="singular-values-and-svd-universal-factorization">81. Singular Values and SVD (Universal Factorization)</h3>
<p>The Singular Value Decomposition (SVD) is one of the most powerful results in linear algebra. It says any <span>\(m \times n\)</span> matrix <span>\(A\)</span> can be factored as:</p>
<p><span>\[
A = U \Sigma V^T
\]</span></p>
<ul>
<li><span>\(U\)</span>: orthogonal <span>\(m \times m\)</span> matrix (left singular vectors)</li>
<li><span>\(\Sigma\)</span>: diagonal <span>\(m \times n\)</span> matrix with nonnegative numbers (singular values)</li>
<li><span>\(V\)</span>: orthogonal <span>\(n \times n\)</span> matrix (right singular vectors)</li>
</ul>
<p>Singular values are always nonnegative and sorted <span>\(\sigma_1 \geq \sigma_2 \geq \dots\)</span>.</p>
<section id="set-up-your-lab-80">
<h4 data-anchor-id="set-up-your-lab-80">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-80">
<h4 data-anchor-id="step-by-step-code-walkthrough-80">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Compute SVD of a matrix</li>
</ol>
<div id="58314483" data-execution_count="459">
<div id="cb795"><pre><code><span id="cb795-1">A <span>=</span> np.array([[<span>3</span>,<span>1</span>,<span>1</span>],</span>
<span id="cb795-2">              [<span>-</span><span>1</span>,<span>3</span>,<span>1</span>]])</span>
<span id="cb795-3"></span>
<span id="cb795-4">U, S, Vt <span>=</span> np.linalg.svd(A, full_matrices<span>=</span><span>True</span>)</span>
<span id="cb795-5"></span>
<span id="cb795-6"><span>print</span>(<span>"U:</span><span>\n</span><span>"</span>, U)</span>
<span id="cb795-7"><span>print</span>(<span>"Singular values:"</span>, S)</span>
<span id="cb795-8"><span>print</span>(<span>"V^T:</span><span>\n</span><span>"</span>, Vt)</span></code></pre></div>
<div>
<pre><code>U:
 [[-0.70710678 -0.70710678]
 [-0.70710678  0.70710678]]
Singular values: [3.46410162 3.16227766]
V^T:
 [[-4.08248290e-01 -8.16496581e-01 -4.08248290e-01]
 [-8.94427191e-01  4.47213595e-01  5.27355937e-16]
 [-1.82574186e-01 -3.65148372e-01  9.12870929e-01]]</code></pre>
</div>
</div>
<ul>
<li><code>U</code>: orthogonal basis in input space.</li>
<li><code>S</code>: singular values (as a 1D array).</li>
<li><code>V^T</code>: orthogonal basis in output space.</li>
</ul>
<ol start="2" type="1">
<li>Reconstruct <span>\(A\)</span> from decomposition</li>
</ol>
<div id="d62b5722" data-execution_count="460">
<div id="cb797"><pre><code><span id="cb797-1">Sigma <span>=</span> np.zeros((U.shape[<span>1</span>], Vt.shape[<span>0</span>]))</span>
<span id="cb797-2">Sigma[:<span>len</span>(S), :<span>len</span>(S)] <span>=</span> np.diag(S)</span>
<span id="cb797-3"></span>
<span id="cb797-4">A_reconstructed <span>=</span> U <span>@</span> Sigma <span>@</span> Vt</span>
<span id="cb797-5"><span>print</span>(<span>"Reconstruction error:"</span>, np.linalg.norm(A <span>-</span> A_reconstructed))</span></code></pre></div>
<div>
<pre><code>Reconstruction error: 1.5895974606912448e-15</code></pre>
</div>
</div>
<p>The error should be near zero.</p>
<ol start="3" type="1">
<li>Rank from SVD</li>
</ol>
<p>Number of nonzero singular values = rank of <span>\(A\)</span>.</p>
<div id="8bf70821" data-execution_count="461"><pre><code><span id="cb799-1">rank <span>=</span> np.<span>sum</span>(S <span>&gt;</span> <span>1e-10</span>)</span>
<span id="cb799-2"><span>print</span>(<span>"Rank of A:"</span>, rank)</span></code></pre></div>
<ol start="4" type="1">
<li>Geometry: effect of <span>\(A\)</span></li>
</ol>
<p>SVD says:</p>
<ol type="1">
<li><span>\(V\)</span> rotates input space.</li>
<li><span>\(\Sigma\)</span> scales along orthogonal directions (by singular values).</li>
<li><span>\(U\)</span> rotates to output space.</li>
</ol>
<p>This explains why SVD works for any matrix (not just square ones).</p>
<ol start="5" type="1">
<li>Low-rank approximation preview</li>
</ol>
<p>Keep only the top singular value(s) → best approximation of <span>\(A\)</span>.</p>
<div id="b1c4c20b" data-execution_count="462">
<div id="cb801"><pre><code><span id="cb801-1">k <span>=</span> <span>1</span></span>
<span id="cb801-2">A_approx <span>=</span> np.outer(U[:,<span>0</span>], Vt[<span>0</span>]) <span>*</span> S[<span>0</span>]</span>
<span id="cb801-3"><span>print</span>(<span>"Rank-1 approximation:</span><span>\n</span><span>"</span>, A_approx)</span></code></pre></div>
<div>
<pre><code>Rank-1 approximation:
 [[1. 2. 1.]
 [1. 2. 1.]]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-79">
<h4 data-anchor-id="try-it-yourself-79">Try It Yourself</h4>
<ol type="1">
<li>Compute SVD for a random 5×3 matrix. Check if <span>\(U\)</span> and <span>\(V\)</span> are orthogonal.</li>
<li>Compare singular values of a diagonal matrix vs a rotation matrix.</li>
<li>Zero out small singular values and see how much of <span>\(A\)</span> is preserved.</li>
</ol>
</section>
<section id="the-takeaway-63">
<h4 data-anchor-id="the-takeaway-63">The Takeaway</h4>
<ul>
<li>SVD factorizes any matrix into rotations and scalings.</li>
<li>Singular values reveal rank and strength of directions.</li>
<li>It’s the universal tool of numerical linear algebra: the backbone of PCA, compression, and stability analysis.</li>
</ul>
</section>
</section>
<section id="geometry-of-svd-rotations-stretching">
<h3 data-anchor-id="geometry-of-svd-rotations-stretching">82. Geometry of SVD (Rotations + Stretching)</h3>
<p>The Singular Value Decomposition (SVD) has a beautiful geometric interpretation: every matrix is just a combination of two rotations (or reflections) and a stretching.</p>
<p>For <span>\(A = U \Sigma V^T\)</span>:</p>
<ol type="1">
<li><span>\(V^T\)</span>: rotates (or reflects) the input space.</li>
<li><span>\(\Sigma\)</span>: stretches space along orthogonal axes by singular values <span>\(\sigma_i\)</span>.</li>
<li><span>\(U\)</span>: rotates (or reflects) the result into the output space.</li>
</ol>
<p>This turns any linear transformation into a rotation → stretching → rotation pipeline.</p>
<section id="set-up-your-lab-81">
<h4 data-anchor-id="set-up-your-lab-81">Set Up Your Lab</h4>
<div id="25cbcce5" data-execution_count="463"><pre><code><span id="cb803-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb803-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-81">
<h4 data-anchor-id="step-by-step-code-walkthrough-81">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Make a 2D matrix</li>
</ol>
<div id="6fca29ba" data-execution_count="464"><pre><code><span id="cb804-1">A <span>=</span> np.array([[<span>2</span>, <span>1</span>],</span>
<span id="cb804-2">              [<span>1</span>, <span>3</span>]])</span></code></pre></div>
<ol start="2" type="1">
<li>Apply SVD</li>
</ol>
<div id="56205e0f" data-execution_count="465">
<div id="cb805"><pre><code><span id="cb805-1">U, S, Vt <span>=</span> np.linalg.svd(A)</span>
<span id="cb805-2"></span>
<span id="cb805-3"><span>print</span>(<span>"U:</span><span>\n</span><span>"</span>, U)</span>
<span id="cb805-4"><span>print</span>(<span>"Singular values:"</span>, S)</span>
<span id="cb805-5"><span>print</span>(<span>"V^T:</span><span>\n</span><span>"</span>, Vt)</span></code></pre></div>
<div>
<pre><code>U:
 [[-0.52573111 -0.85065081]
 [-0.85065081  0.52573111]]
Singular values: [3.61803399 1.38196601]
V^T:
 [[-0.52573111 -0.85065081]
 [-0.85065081  0.52573111]]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Visualize effect on the unit circle</li>
</ol>
<p>The unit circle is often used to visualize linear transformations.</p>
<div id="66031540" data-execution_count="466">
<div id="cb807"><pre><code><span id="cb807-1">theta <span>=</span> np.linspace(<span>0</span>, <span>2</span><span>*</span>np.pi, <span>200</span>)</span>
<span id="cb807-2">circle <span>=</span> np.vstack((np.cos(theta), np.sin(theta)))</span>
<span id="cb807-3"></span>
<span id="cb807-4">transformed <span>=</span> A <span>@</span> circle</span>
<span id="cb807-5"></span>
<span id="cb807-6">plt.plot(circle[<span>0</span>], circle[<span>1</span>], <span>'b--'</span>, label<span>=</span><span>"Unit circle"</span>)</span>
<span id="cb807-7">plt.plot(transformed[<span>0</span>], transformed[<span>1</span>], <span>'r-'</span>, label<span>=</span><span>"Transformed"</span>)</span>
<span id="cb807-8">plt.axis(<span>"equal"</span>)</span>
<span id="cb807-9">plt.legend()</span>
<span id="cb807-10">plt.title(<span>"Action of A on the unit circle"</span>)</span>
<span id="cb807-11">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-467-output-1.png" width="569" height="431"></p>
</figure>
</div>
</div>
<p>The circle becomes an ellipse. Its axes align with the singular vectors, and its radii are the singular values.</p>
<ol start="4" type="1">
<li>Compare with decomposition steps</li>
</ol>
<div id="6a855ce5" data-execution_count="467">
<div id="cb808"><pre><code><span id="cb808-1"><span># Apply V^T</span></span>
<span id="cb808-2">step1 <span>=</span> Vt <span>@</span> circle</span>
<span id="cb808-3"><span># Apply Σ</span></span>
<span id="cb808-4">Sigma <span>=</span> np.diag(S)</span>
<span id="cb808-5">step2 <span>=</span> Sigma <span>@</span> step1</span>
<span id="cb808-6"><span># Apply U</span></span>
<span id="cb808-7">step3 <span>=</span> U <span>@</span> step2</span>
<span id="cb808-8"></span>
<span id="cb808-9">plt.plot(circle[<span>0</span>], circle[<span>1</span>], <span>'b--'</span>, label<span>=</span><span>"Unit circle"</span>)</span>
<span id="cb808-10">plt.plot(step3[<span>0</span>], step3[<span>1</span>], <span>'g-'</span>, label<span>=</span><span>"U Σ V^T circle"</span>)</span>
<span id="cb808-11">plt.axis(<span>"equal"</span>)</span>
<span id="cb808-12">plt.legend()</span>
<span id="cb808-13">plt.title(<span>"SVD decomposition of transformation"</span>)</span>
<span id="cb808-14">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-468-output-1.png" width="569" height="431"></p>
</figure>
</div>
</div>
<p>Both transformed shapes match → confirms SVD’s geometric picture.</p>
</section>
<section id="try-it-yourself-80">
<h4 data-anchor-id="try-it-yourself-80">Try It Yourself</h4>
<ol type="1">
<li>Change <span>\(A\)</span> to a pure shear, like <code>[[1,2],[0,1]]</code>. How does the ellipse look?</li>
<li>Try a diagonal matrix, like <code>[[3,0],[0,1]]</code>. Do the singular vectors match the coordinate axes?</li>
<li>Scale the input circle to a square and see if geometry still works.</li>
</ol>
</section>
<section id="the-takeaway-64">
<h4 data-anchor-id="the-takeaway-64">The Takeaway</h4>
<ul>
<li>SVD = rotate → stretch → rotate.</li>
<li>The unit circle becomes an ellipse: axes = singular vectors, radii = singular values.</li>
<li>This geometric lens makes SVD intuitive and explains why it’s so widely used in data, graphics, and signal processing.</li>
</ul>
</section>
</section>
<section id="relation-to-eigen-decompositions-ata-and-aat">
<h3 data-anchor-id="relation-to-eigen-decompositions-ata-and-aat">83. Relation to Eigen-Decompositions (ATA and AAT)</h3>
<p>Singular values and eigenvalues are closely connected. While eigen-decomposition applies only to square matrices, SVD works for any rectangular matrix. The bridge between them is:</p>
<p><span>\[
A^T A v = \sigma^2 v \quad \text{and} \quad A A^T u = \sigma^2 u
\]</span></p>
<ul>
<li><span>\(v\)</span>: right singular vector (from eigenvectors of <span>\(A^T A\)</span>)</li>
<li><span>\(u\)</span>: left singular vector (from eigenvectors of <span>\(A A^T\)</span>)</li>
<li><span>\(\sigma\)</span>: singular values (square roots of eigenvalues of <span>\(A^T A\)</span> or <span>\(A A^T\)</span>)</li>
</ul>
<section id="set-up-your-lab-82">
<h4 data-anchor-id="set-up-your-lab-82">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-82">
<h4 data-anchor-id="step-by-step-code-walkthrough-82">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Define a rectangular matrix</li>
</ol>
<div id="0da0b156" data-execution_count="469"><pre><code><span id="cb810-1">A <span>=</span> np.array([[<span>2</span>, <span>0</span>],</span>
<span id="cb810-2">              [<span>1</span>, <span>1</span>],</span>
<span id="cb810-3">              [<span>0</span>, <span>1</span>]])  <span># shape 3x2</span></span></code></pre></div>
<ol start="2" type="1">
<li>Compute SVD directly</li>
</ol>
<div id="48165fb1" data-execution_count="470">
<div id="cb811"><pre><code><span id="cb811-1">U, S, Vt <span>=</span> np.linalg.svd(A)</span>
<span id="cb811-2"><span>print</span>(<span>"Singular values:"</span>, S)</span></code></pre></div>
<div>
<pre><code>Singular values: [2.30277564 1.30277564]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Compare with eigenvalues of <span>\(A^T A\)</span></li>
</ol>
<div id="d1a65d7c" data-execution_count="471">
<div id="cb813"><pre><code><span id="cb813-1">ATA <span>=</span> A.T <span>@</span> A</span>
<span id="cb813-2">eigvals, eigvecs <span>=</span> np.linalg.eig(ATA)</span>
<span id="cb813-3"></span>
<span id="cb813-4"><span>print</span>(<span>"Eigenvalues of A^T A:"</span>, eigvals)</span>
<span id="cb813-5"><span>print</span>(<span>"Square roots (sorted):"</span>, np.sqrt(np.sort(eigvals)[::<span>-</span><span>1</span>]))</span></code></pre></div>
<div>
<pre><code>Eigenvalues of A^T A: [5.30277564 1.69722436]
Square roots (sorted): [2.30277564 1.30277564]</code></pre>
</div>
</div>
<p>Notice: singular values from SVD = square roots of eigenvalues of <span>\(A^T A\)</span>.</p>
<ol start="4" type="1">
<li>Compare with eigenvalues of <span>\(A A^T\)</span></li>
</ol>
<div id="1e2e32d6" data-execution_count="472">
<div id="cb815"><pre><code><span id="cb815-1">AAT <span>=</span> A <span>@</span> A.T</span>
<span id="cb815-2">eigvals2, eigvecs2 <span>=</span> np.linalg.eig(AAT)</span>
<span id="cb815-3"></span>
<span id="cb815-4"><span>print</span>(<span>"Eigenvalues of A A^T:"</span>, eigvals2)</span>
<span id="cb815-5"><span>print</span>(<span>"Square roots:"</span>, np.sqrt(np.sort(eigvals2)[::<span>-</span><span>1</span>]))</span></code></pre></div>
<div>
<pre><code>Eigenvalues of A A^T: [ 5.30277564e+00  1.69722436e+00 -2.01266546e-17]
Square roots: [2.30277564 1.30277564        nan]</code></pre>
</div>
<div>
<pre><code>/var/folders/_g/lq_pglm508df70x751kkxrl80000gp/T/ipykernel_31637/436251338.py:5: RuntimeWarning: invalid value encountered in sqrt
  print("Square roots:", np.sqrt(np.sort(eigvals2)[::-1]))</code></pre>
</div>
</div>
<p>They match too → confirming the relationship.</p>
<ol start="5" type="1">
<li>Verify singular vectors</li>
</ol>
<ul>
<li>Right singular vectors (<span>\(V\)</span>) = eigenvectors of <span>\(A^T A\)</span>.</li>
<li>Left singular vectors (<span>\(U\)</span>) = eigenvectors of <span>\(A A^T\)</span>.</li>
</ul>
<div id="6dcf66b9" data-execution_count="473">
<div id="cb818"><pre><code><span id="cb818-1"><span>print</span>(<span>"Right singular vectors (V):</span><span>\n</span><span>"</span>, Vt.T)</span>
<span id="cb818-2"><span>print</span>(<span>"Eigenvectors of A^T A:</span><span>\n</span><span>"</span>, eigvecs)</span>
<span id="cb818-3"></span>
<span id="cb818-4"><span>print</span>(<span>"Left singular vectors (U):</span><span>\n</span><span>"</span>, U)</span>
<span id="cb818-5"><span>print</span>(<span>"Eigenvectors of A A^T:</span><span>\n</span><span>"</span>, eigvecs2)</span></code></pre></div>
<div>
<pre><code>Right singular vectors (V):
 [[-0.95709203  0.28978415]
 [-0.28978415 -0.95709203]]
Eigenvectors of A^T A:
 [[ 0.95709203 -0.28978415]
 [ 0.28978415  0.95709203]]
Left singular vectors (U):
 [[-0.83125078  0.44487192  0.33333333]
 [-0.54146663 -0.51222011 -0.66666667]
 [-0.12584124 -0.73465607  0.66666667]]
Eigenvectors of A A^T:
 [[-0.83125078  0.44487192  0.33333333]
 [-0.54146663 -0.51222011 -0.66666667]
 [-0.12584124 -0.73465607  0.66666667]]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-81">
<h4 data-anchor-id="try-it-yourself-81">Try It Yourself</h4>
<ol type="1">
<li>Try a square symmetric matrix and compare SVD with eigen-decomposition. Do they match?</li>
<li>For a tall vs wide rectangular matrix, check whether <span>\(U\)</span> and <span>\(V\)</span> differ.</li>
<li>Compute eigenvalues manually with <code>np.linalg.eig</code> for a random <span>\(A\)</span> and confirm singular values.</li>
</ol>
</section>
<section id="the-takeaway-65">
<h4 data-anchor-id="the-takeaway-65">The Takeaway</h4>
<ul>
<li>Singular values = square roots of eigenvalues of <span>\(A^T A\)</span> (or <span>\(A A^T\)</span>).</li>
<li>Right singular vectors = eigenvectors of <span>\(A^T A\)</span>.</li>
<li>Left singular vectors = eigenvectors of <span>\(A A^T\)</span>.</li>
<li>SVD generalizes eigen-decomposition to all matrices, rectangular or square.</li>
</ul>
</section>
</section>
<section id="low-rank-approximation-best-small-models">
<h3 data-anchor-id="low-rank-approximation-best-small-models">84. Low-Rank Approximation (Best Small Models)</h3>
<p>One of the most useful applications of SVD is low-rank approximation: compressing a large matrix into a smaller one while keeping most of the important information.</p>
<p>The Eckart–Young theorem says: If <span>\(A = U \Sigma V^T\)</span>, then the best rank-<span>\(k\)</span> approximation (in least-squares sense) is:</p>
<p><span>\[
A_k = U_k \Sigma_k V_k^T
\]</span></p>
<p>where we keep only the top <span>\(k\)</span> singular values (and corresponding vectors).</p>
<section id="set-up-your-lab-83">
<h4 data-anchor-id="set-up-your-lab-83">Set Up Your Lab</h4>
<div id="1a39b776" data-execution_count="474"><pre><code><span id="cb820-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb820-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-83">
<h4 data-anchor-id="step-by-step-code-walkthrough-83">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Create a matrix with hidden low-rank structure</li>
</ol>
<div id="724a09b4" data-execution_count="475"><pre><code><span id="cb821-1">np.random.seed(<span>0</span>)</span>
<span id="cb821-2">U <span>=</span> np.random.randn(<span>50</span>, <span>5</span>)   <span># 50 x 5</span></span>
<span id="cb821-3">V <span>=</span> np.random.randn(<span>5</span>, <span>30</span>)   <span># 5 x 30</span></span>
<span id="cb821-4">A <span>=</span> U <span>@</span> V  <span># true rank ≤ 5</span></span></code></pre></div>
<ol start="2" type="1">
<li>Full SVD</li>
</ol>
<div id="f5740317" data-execution_count="476">
<div id="cb822"><pre><code><span id="cb822-1">U, S, Vt <span>=</span> np.linalg.svd(A, full_matrices<span>=</span><span>False</span>)</span>
<span id="cb822-2"><span>print</span>(<span>"Singular values:"</span>, S[:<span>10</span>])</span></code></pre></div>
<div>
<pre><code>Singular values: [4.90672194e+01 4.05935057e+01 3.39228766e+01 3.07883338e+01
 2.29261740e+01 3.97150036e-15 3.97150036e-15 3.97150036e-15
 3.97150036e-15 3.97150036e-15]</code></pre>
</div>
</div>
<p>Only the first ~5 should be large; the rest close to zero.</p>
<ol start="3" type="1">
<li>Build rank-1 approximation</li>
</ol>
<div id="5176ac50" data-execution_count="477">
<div id="cb824"><pre><code><span id="cb824-1">k <span>=</span> <span>1</span></span>
<span id="cb824-2">A1 <span>=</span> U[:, :k] <span>@</span> np.diag(S[:k]) <span>@</span> Vt[:k, :]</span>
<span id="cb824-3">error1 <span>=</span> np.linalg.norm(A <span>-</span> A1)</span>
<span id="cb824-4"><span>print</span>(<span>"Rank-1 approximation error:"</span>, error1)</span></code></pre></div>
<div>
<pre><code>Rank-1 approximation error: 65.36149641872869</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Rank-5 approximation (should be almost exact)</li>
</ol>
<div id="2853198b" data-execution_count="478">
<div id="cb826"><pre><code><span id="cb826-1">k <span>=</span> <span>5</span></span>
<span id="cb826-2">A5 <span>=</span> U[:, :k] <span>@</span> np.diag(S[:k]) <span>@</span> Vt[:k, :]</span>
<span id="cb826-3">error5 <span>=</span> np.linalg.norm(A <span>-</span> A5)</span>
<span id="cb826-4"><span>print</span>(<span>"Rank-5 approximation error:"</span>, error5)</span></code></pre></div>
<div>
<pre><code>Rank-5 approximation error: 5.756573247253659e-14</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Visual comparison (image compression demo)</li>
</ol>
<p>Let’s see it on an image.</p>
<div id="a6058968" data-execution_count="479">
<div id="cb828"><pre><code><span id="cb828-1"><span>from</span> sklearn.datasets <span>import</span> load_digits</span>
<span id="cb828-2">digits <span>=</span> load_digits()</span>
<span id="cb828-3">img <span>=</span> digits.images[<span>0</span>]  <span># 8x8 grayscale digit</span></span>
<span id="cb828-4"></span>
<span id="cb828-5">U, S, Vt <span>=</span> np.linalg.svd(img, full_matrices<span>=</span><span>False</span>)</span>
<span id="cb828-6"></span>
<span id="cb828-7"><span># Keep only top 2 singular values</span></span>
<span id="cb828-8">k <span>=</span> <span>2</span></span>
<span id="cb828-9">img2 <span>=</span> U[:, :k] <span>@</span> np.diag(S[:k]) <span>@</span> Vt[:k, :]</span>
<span id="cb828-10"></span>
<span id="cb828-11">plt.subplot(<span>1</span>,<span>2</span>,<span>1</span>)</span>
<span id="cb828-12">plt.imshow(img, cmap<span>=</span><span>"gray"</span>)</span>
<span id="cb828-13">plt.title(<span>"Original"</span>)</span>
<span id="cb828-14"></span>
<span id="cb828-15">plt.subplot(<span>1</span>,<span>2</span>,<span>2</span>)</span>
<span id="cb828-16">plt.imshow(img2, cmap<span>=</span><span>"gray"</span>)</span>
<span id="cb828-17">plt.title(<span>"Rank-2 Approximation"</span>)</span>
<span id="cb828-18">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-480-output-1.png" width="558" height="298"></p>
</figure>
</div>
</div>
<p>Even with just 2 singular values, the digit shape is recognizable.</p>
</section>
<section id="try-it-yourself-82">
<h4 data-anchor-id="try-it-yourself-82">Try It Yourself</h4>
<ol type="1">
<li>Vary <span>\(k\)</span> in the image example (1, 2, 5, 10). How much detail do you keep?</li>
<li>Compare the approximation error <span>\(\|A - A_k\|\)</span> as <span>\(k\)</span> increases.</li>
<li>Apply low-rank approximation to random noisy data. Does it denoise?</li>
</ol>
</section>
<section id="the-takeaway-66">
<h4 data-anchor-id="the-takeaway-66">The Takeaway</h4>
<ul>
<li>SVD gives the best possible low-rank approximation in terms of error.</li>
<li>By truncating singular values, you compress data while keeping its essential structure.</li>
<li>This is the backbone of image compression, recommender systems, and dimensionality reduction.</li>
</ul>
</section>
</section>
<section id="principal-component-analysis-variance-and-directions">
<h3 data-anchor-id="principal-component-analysis-variance-and-directions">85. Principal Component Analysis (Variance and Directions)</h3>
<p>Principal Component Analysis (PCA) is one of the most important applications of SVD. It finds the directions (principal components) where data varies the most, and projects the data onto them to reduce dimensionality while preserving as much information as possible.</p>
<p>Mathematically:</p>
<ol type="1">
<li>Center the data (subtract the mean).</li>
<li>Compute covariance matrix <span>\(C = \frac{1}{n} X^T X\)</span>.</li>
<li>Eigenvectors of <span>\(C\)</span> = principal directions.</li>
<li>Eigenvalues = variance explained.</li>
<li>Equivalently: PCA = SVD of centered data matrix.</li>
</ol>
<section id="set-up-your-lab-84">
<h4 data-anchor-id="set-up-your-lab-84">Set Up Your Lab</h4>
<div id="a86d5597" data-execution_count="480"><pre><code><span id="cb829-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb829-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb829-3"><span>from</span> sklearn.datasets <span>import</span> load_digits</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-84">
<h4 data-anchor-id="step-by-step-code-walkthrough-84">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Generate synthetic 2D data</li>
</ol>
<div id="17aaa593" data-execution_count="481">
<div id="cb830"><pre><code><span id="cb830-1">np.random.seed(<span>0</span>)</span>
<span id="cb830-2">X <span>=</span> np.random.randn(<span>200</span>, <span>2</span>) <span>@</span> np.array([[<span>3</span>,<span>1</span>],[<span>1</span>,<span>0.5</span>]])  <span># stretched cloud</span></span>
<span id="cb830-3"></span>
<span id="cb830-4">plt.scatter(X[:,<span>0</span>], X[:,<span>1</span>], alpha<span>=</span><span>0.3</span>)</span>
<span id="cb830-5">plt.title(<span>"Original data"</span>)</span>
<span id="cb830-6">plt.axis(<span>"equal"</span>)</span>
<span id="cb830-7">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-482-output-1.png" width="569" height="431"></p>
</figure>
</div>
</div>
<ol start="2" type="1">
<li>Center the data</li>
</ol>
<div id="f3675632" data-execution_count="482"><pre><code><span id="cb831-1">X_centered <span>=</span> X <span>-</span> X.mean(axis<span>=</span><span>0</span>)</span></code></pre></div>
<ol start="3" type="1">
<li>Compute SVD</li>
</ol>
<div id="831e9685" data-execution_count="483">
<div id="cb832"><pre><code><span id="cb832-1">U, S, Vt <span>=</span> np.linalg.svd(X_centered, full_matrices<span>=</span><span>False</span>)</span>
<span id="cb832-2"><span>print</span>(<span>"Principal directions (V):</span><span>\n</span><span>"</span>, Vt)</span></code></pre></div>
<div>
<pre><code>Principal directions (V):
 [[-0.94430098 -0.32908307]
 [ 0.32908307 -0.94430098]]</code></pre>
</div>
</div>
<p>Rows of <code>Vt</code> are the principal components.</p>
<ol start="4" type="1">
<li>Project data onto first component</li>
</ol>
<div id="c305b208" data-execution_count="484">
<div id="cb834"><pre><code><span id="cb834-1">X_pca1 <span>=</span> X_centered <span>@</span> Vt.T[:,<span>0</span>]</span>
<span id="cb834-2"></span>
<span id="cb834-3">plt.scatter(X_pca1, np.zeros_like(X_pca1), alpha<span>=</span><span>0.3</span>)</span>
<span id="cb834-4">plt.title(<span>"Data projected on first principal component"</span>)</span>
<span id="cb834-5">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-485-output-1.png" width="590" height="431"></p>
</figure>
</div>
</div>
<p>This collapses data into 1D, keeping the most variance.</p>
<ol start="5" type="1">
<li>Visualize principal axes</li>
</ol>
<div id="fd596086" data-execution_count="485">
<div id="cb835"><pre><code><span id="cb835-1">plt.scatter(X_centered[:,<span>0</span>], X_centered[:,<span>1</span>], alpha<span>=</span><span>0.3</span>)</span>
<span id="cb835-2"><span>for</span> length, vector <span>in</span> <span>zip</span>(S, Vt):</span>
<span id="cb835-3">    plt.plot([<span>0</span>, vector[<span>0</span>]<span>*</span>length], [<span>0</span>, vector[<span>1</span>]<span>*</span>length], <span>'r-'</span>, linewidth<span>=</span><span>3</span>)</span>
<span id="cb835-4">plt.title(<span>"Principal components (directions of max variance)"</span>)</span>
<span id="cb835-5">plt.axis(<span>"equal"</span>)</span>
<span id="cb835-6">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-486-output-1.png" width="578" height="431"></p>
</figure>
</div>
</div>
<p>The red arrows show where the data spreads most.</p>
<ol start="6" type="1">
<li>PCA on real data (digits)</li>
</ol>
<div id="b6906858" data-execution_count="486">
<div id="cb836"><pre><code><span id="cb836-1">digits <span>=</span> load_digits()</span>
<span id="cb836-2">X <span>=</span> digits.data  <span># 1797 samples, 64 features</span></span>
<span id="cb836-3">X_centered <span>=</span> X <span>-</span> X.mean(axis<span>=</span><span>0</span>)</span>
<span id="cb836-4"></span>
<span id="cb836-5">U, S, Vt <span>=</span> np.linalg.svd(X_centered, full_matrices<span>=</span><span>False</span>)</span>
<span id="cb836-6"></span>
<span id="cb836-7">explained_variance <span>=</span> (S<span>**</span><span>2</span>) <span>/</span> np.<span>sum</span>(S<span>**</span><span>2</span>)</span>
<span id="cb836-8"><span>print</span>(<span>"Explained variance ratio (first 5):"</span>, explained_variance[:<span>5</span>])</span></code></pre></div>
<div>
<pre><code>Explained variance ratio (first 5): [0.14890594 0.13618771 0.11794594 0.08409979 0.05782415]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-83">
<h4 data-anchor-id="try-it-yourself-83">Try It Yourself</h4>
<ol type="1">
<li>Reduce digits dataset to 2D using the top 2 components and plot. Do digit clusters separate?</li>
<li>Compare explained variance ratio for top 10 components.</li>
<li>Add noise to data and check if PCA filters it out when projecting to fewer dimensions.</li>
</ol>
</section>
<section id="the-takeaway-67">
<h4 data-anchor-id="the-takeaway-67">The Takeaway</h4>
<ul>
<li>PCA finds directions of maximum variance using SVD.</li>
<li>By projecting onto top components, you compress data with minimal information loss.</li>
<li>PCA is the backbone of dimensionality reduction, visualization, and preprocessing in machine learning.</li>
</ul>
</section>
</section>
<section id="pseudoinverse-moorepenrose-and-solving-ill-posed-systems">
<h3 data-anchor-id="pseudoinverse-moorepenrose-and-solving-ill-posed-systems">86. Pseudoinverse (Moore–Penrose) and Solving Ill-Posed Systems</h3>
<p>The Moore–Penrose pseudoinverse <span>\(A^+\)</span> generalizes the inverse of a matrix. It allows solving systems <span>\(Ax = b\)</span> even when:</p>
<ul>
<li><span>\(A\)</span> is not square, or</li>
<li><span>\(A\)</span> is singular (non-invertible).</li>
</ul>
<p>The solution given by the pseudoinverse is the least-squares solution with minimum norm:</p>
<p><span>\[
x = A^+ b
\]</span></p>
<p>If <span>\(A = U \Sigma V^T\)</span>, then:</p>
<p><span>\[
A^+ = V \Sigma^+ U^T
\]</span></p>
<p>where <span>\(\Sigma^+\)</span> is obtained by taking reciprocals of nonzero singular values.</p>
<section id="set-up-your-lab-85">
<h4 data-anchor-id="set-up-your-lab-85">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-85">
<h4 data-anchor-id="step-by-step-code-walkthrough-85">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Solve an overdetermined system (more equations than unknowns)</li>
</ol>
<div id="440b7514" data-execution_count="488">
<div id="cb839"><pre><code><span id="cb839-1">A <span>=</span> np.array([[<span>1</span>,<span>1</span>],</span>
<span id="cb839-2">              [<span>1</span>,<span>2</span>],</span>
<span id="cb839-3">              [<span>1</span>,<span>3</span>]])  <span># 3x2 system</span></span>
<span id="cb839-4">b <span>=</span> np.array([<span>1</span>,<span>2</span>,<span>2</span>])</span>
<span id="cb839-5"></span>
<span id="cb839-6">x_ls, <span>*</span>_ <span>=</span> np.linalg.lstsq(A, b, rcond<span>=</span><span>None</span>)</span>
<span id="cb839-7"><span>print</span>(<span>"Least-squares solution:"</span>, x_ls)</span></code></pre></div>
<div>
<pre><code>Least-squares solution: [0.66666667 0.5       ]</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Compute with pseudoinverse</li>
</ol>
<div id="658fedc6" data-execution_count="489">
<div id="cb841"><pre><code><span id="cb841-1">A_pinv <span>=</span> np.linalg.pinv(A)</span>
<span id="cb841-2">x_pinv <span>=</span> A_pinv <span>@</span> b</span>
<span id="cb841-3"><span>print</span>(<span>"Pseudoinverse solution:"</span>, x_pinv)</span></code></pre></div>
<div>
<pre><code>Pseudoinverse solution: [0.66666667 0.5       ]</code></pre>
</div>
</div>
<p>Both match → pseudoinverse gives least-squares solution.</p>
<ol start="3" type="1">
<li>Solve an underdetermined system (fewer equations than unknowns)</li>
</ol>
<div id="7e95044d" data-execution_count="490">
<div id="cb843"><pre><code><span id="cb843-1">A <span>=</span> np.array([[<span>1</span>,<span>2</span>,<span>3</span>]])  <span># 1x3</span></span>
<span id="cb843-2">b <span>=</span> np.array([<span>1</span>])</span>
<span id="cb843-3"></span>
<span id="cb843-4">x_pinv <span>=</span> np.linalg.pinv(A) <span>@</span> b</span>
<span id="cb843-5"><span>print</span>(<span>"Minimum norm solution:"</span>, x_pinv)</span></code></pre></div>
<div>
<pre><code>Minimum norm solution: [0.07142857 0.14285714 0.21428571]</code></pre>
</div>
</div>
<p>Here, infinitely many solutions exist. The pseudoinverse picks the one with smallest norm.</p>
<ol start="4" type="1">
<li>Compare with singular matrix</li>
</ol>
<div id="e452326e" data-execution_count="491">
<div id="cb845"><pre><code><span id="cb845-1">A <span>=</span> np.array([[<span>1</span>,<span>2</span>],</span>
<span id="cb845-2">              [<span>2</span>,<span>4</span>]])  <span># rank deficient</span></span>
<span id="cb845-3">b <span>=</span> np.array([<span>1</span>,<span>2</span>])</span>
<span id="cb845-4"></span>
<span id="cb845-5">x_pinv <span>=</span> np.linalg.pinv(A) <span>@</span> b</span>
<span id="cb845-6"><span>print</span>(<span>"Solution with pseudoinverse:"</span>, x_pinv)</span></code></pre></div>
<div>
<pre><code>Solution with pseudoinverse: [0.2 0.4]</code></pre>
</div>
</div>
<p>Even when <span>\(A\)</span> is singular, pseudoinverse provides a solution.</p>
<ol start="5" type="1">
<li>Manual pseudoinverse via SVD</li>
</ol>
<div id="987376d8" data-execution_count="492">
<div id="cb847"><pre><code><span id="cb847-1">A <span>=</span> np.array([[<span>1</span>,<span>2</span>],</span>
<span id="cb847-2">              [<span>3</span>,<span>4</span>]])</span>
<span id="cb847-3">U, S, Vt <span>=</span> np.linalg.svd(A)</span>
<span id="cb847-4">S_inv <span>=</span> np.zeros((Vt.shape[<span>0</span>], U.shape[<span>0</span>]))</span>
<span id="cb847-5"><span>for</span> i <span>in</span> <span>range</span>(<span>len</span>(S)):</span>
<span id="cb847-6">    <span>if</span> S[i] <span>&gt;</span> <span>1e-10</span>:</span>
<span id="cb847-7">        S_inv[i,i] <span>=</span> <span>1</span><span>/</span>S[i]</span>
<span id="cb847-8"></span>
<span id="cb847-9">A_pinv_manual <span>=</span> Vt.T <span>@</span> S_inv <span>@</span> U.T</span>
<span id="cb847-10"><span>print</span>(<span>"Manual pseudoinverse:</span><span>\n</span><span>"</span>, A_pinv_manual)</span>
<span id="cb847-11"><span>print</span>(<span>"NumPy pseudoinverse:</span><span>\n</span><span>"</span>, np.linalg.pinv(A))</span></code></pre></div>
<div>
<pre><code>Manual pseudoinverse:
 [[-2.   1. ]
 [ 1.5 -0.5]]
NumPy pseudoinverse:
 [[-2.   1. ]
 [ 1.5 -0.5]]</code></pre>
</div>
</div>
<p>They match.</p>
</section>
<section id="try-it-yourself-84">
<h4 data-anchor-id="try-it-yourself-84">Try It Yourself</h4>
<ol type="1">
<li>Create an overdetermined system with noise and see how pseudoinverse smooths the solution.</li>
<li>Compare pseudoinverse with direct inverse (<code>np.linalg.inv</code>) on a square nonsingular matrix.</li>
<li>Zero out small singular values manually and see how solution changes.</li>
</ol>
</section>
<section id="the-takeaway-68">
<h4 data-anchor-id="the-takeaway-68">The Takeaway</h4>
<ul>
<li>The pseudoinverse solves any linear system, square or not.</li>
<li>It provides the least-squares solution in overdetermined cases and the minimum-norm solution in underdetermined cases.</li>
<li>Built on SVD, it is a cornerstone of regression, optimization, and numerical methods.</li>
</ul>
</section>
</section>
<section id="conditioning-and-sensitivity-how-errors-amplify">
<h3 data-anchor-id="conditioning-and-sensitivity-how-errors-amplify">87. Conditioning and Sensitivity (How Errors Amplify)</h3>
<p>Conditioning tells us how sensitive a system is to small changes. For a linear system <span>\(Ax = b\)</span>:</p>
<ul>
<li>If <span>\(A\)</span> is well-conditioned, small changes in <span>\(b\)</span> or <span>\(A\)</span> → small changes in <span>\(x\)</span>.</li>
<li>If <span>\(A\)</span> is ill-conditioned, tiny changes can cause huge swings in <span>\(x\)</span>.</li>
</ul>
<p>The condition number is defined as:</p>
<p><span>\[
\kappa(A) = \|A\| \cdot \|A^{-1}\|
\]</span></p>
<p>For SVD:</p>
<p><span>\[
\kappa(A) = \frac{\sigma_{\max}}{\sigma_{\min}}
\]</span></p>
<p>where <span>\(\sigma_{\max}\)</span> and <span>\(\sigma_{\min}\)</span> are the largest and smallest singular values.</p>
<ul>
<li>Large <span>\(\kappa(A)\)</span> → unstable system.</li>
<li>Small <span>\(\kappa(A)\)</span> → stable system.</li>
</ul>
<section id="set-up-your-lab-86">
<h4 data-anchor-id="set-up-your-lab-86">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-86">
<h4 data-anchor-id="step-by-step-code-walkthrough-86">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Well-conditioned system</li>
</ol>
<div id="4128faff" data-execution_count="494">
<div id="cb850"><pre><code><span id="cb850-1">A <span>=</span> np.array([[<span>2</span>,<span>0</span>],</span>
<span id="cb850-2">              [<span>0</span>,<span>1</span>]])</span>
<span id="cb850-3">b <span>=</span> np.array([<span>1</span>,<span>1</span>])</span>
<span id="cb850-4"></span>
<span id="cb850-5">x <span>=</span> np.linalg.solve(A, b)</span>
<span id="cb850-6">cond <span>=</span> np.linalg.cond(A)</span>
<span id="cb850-7"><span>print</span>(<span>"Solution:"</span>, x)</span>
<span id="cb850-8"><span>print</span>(<span>"Condition number:"</span>, cond)</span></code></pre></div>
<div>
<pre><code>Solution: [0.5 1. ]
Condition number: 2.0</code></pre>
</div>
</div>
<p>Condition number = ratio of singular values → moderate size.</p>
<ol start="2" type="1">
<li>Ill-conditioned system</li>
</ol>
<div id="e826c157" data-execution_count="495">
<div id="cb852"><pre><code><span id="cb852-1">A <span>=</span> np.array([[<span>1</span>, <span>1.0001</span>],</span>
<span id="cb852-2">              [<span>1</span>, <span>1.0000</span>]])</span>
<span id="cb852-3">b <span>=</span> np.array([<span>2</span>,<span>2</span>])</span>
<span id="cb852-4"></span>
<span id="cb852-5">x <span>=</span> np.linalg.lstsq(A, b, rcond<span>=</span><span>None</span>)[<span>0</span>]</span>
<span id="cb852-6">cond <span>=</span> np.linalg.cond(A)</span>
<span id="cb852-7"><span>print</span>(<span>"Solution:"</span>, x)</span>
<span id="cb852-8"><span>print</span>(<span>"Condition number:"</span>, cond)</span></code></pre></div>
<div>
<pre><code>Solution: [ 2.00000000e+00 -5.73526099e-13]
Condition number: 40002.000075017124</code></pre>
</div>
</div>
<p>Condition number is very large → instability.</p>
<ol start="3" type="1">
<li>Perturb the right-hand side</li>
</ol>
<div id="22ddf53f" data-execution_count="496">
<div id="cb854"><pre><code><span id="cb854-1">b2 <span>=</span> np.array([<span>2</span>, <span>2.001</span>])  <span># tiny change</span></span>
<span id="cb854-2">x2 <span>=</span> np.linalg.lstsq(A, b2, rcond<span>=</span><span>None</span>)[<span>0</span>]</span>
<span id="cb854-3"><span>print</span>(<span>"Solution after tiny change:"</span>, x2)</span></code></pre></div>
<div>
<pre><code>Solution after tiny change: [ 12.001 -10.   ]</code></pre>
</div>
</div>
<p>The solution changes drastically → shows sensitivity.</p>
<ol start="4" type="1">
<li>Relation to singular values</li>
</ol>
<div id="112eeb2a" data-execution_count="497">
<div id="cb856"><pre><code><span id="cb856-1">U, S, Vt <span>=</span> np.linalg.svd(A)</span>
<span id="cb856-2"><span>print</span>(<span>"Singular values:"</span>, S)</span>
<span id="cb856-3"><span>print</span>(<span>"Condition number (SVD):"</span>, S[<span>0</span>]<span>/</span>S[<span>-</span><span>1</span>])</span></code></pre></div>
<div>
<pre><code>Singular values: [2.000050e+00 4.999875e-05]
Condition number (SVD): 40002.00007501713</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Scaling experiment</li>
</ol>
<div id="1c3cf41d" data-execution_count="498">
<div id="cb858"><pre><code><span id="cb858-1"><span>for</span> scale <span>in</span> [<span>1</span>,<span>1e-2</span>,<span>1e-4</span>,<span>1e-6</span>]:</span>
<span id="cb858-2">    A <span>=</span> np.array([[<span>1</span>,<span>0</span>],[<span>0</span>,scale]])</span>
<span id="cb858-3">    <span>print</span>(<span>f"Scale=</span><span>{</span>scale<span>}</span><span>, condition number=</span><span>{</span>np<span>.</span>linalg<span>.</span>cond(A)<span>}</span><span>"</span>)</span></code></pre></div>
<div>
<pre><code>Scale=1, condition number=1.0
Scale=0.01, condition number=100.0
Scale=0.0001, condition number=10000.0
Scale=1e-06, condition number=1000000.0</code></pre>
</div>
</div>
<p>As scale shrinks, condition number explodes.</p>
</section>
<section id="try-it-yourself-85">
<h4 data-anchor-id="try-it-yourself-85">Try It Yourself</h4>
<ol type="1">
<li>Generate random matrices and compute their condition numbers. Which are stable?</li>
<li>Compare condition numbers of Hilbert matrices (notoriously ill-conditioned).</li>
<li>Explore how rounding errors grow with high condition numbers.</li>
</ol>
</section>
<section id="the-takeaway-69">
<h4 data-anchor-id="the-takeaway-69">The Takeaway</h4>
<ul>
<li>Condition number = measure of problem sensitivity.</li>
<li><span>\(\kappa(A) = \sigma_{\max}/\sigma_{\min}\)</span>.</li>
<li>Ill-conditioned problems amplify errors and are numerically unstable → why scaling, regularization, and good formulations matter.</li>
</ul>
</section>
</section>
<section id="matrix-norms-and-singular-values-measuring-size-properly">
<h3 data-anchor-id="matrix-norms-and-singular-values-measuring-size-properly">88. Matrix Norms and Singular Values (Measuring Size Properly)</h3>
<p>Matrix norms measure the size or strength of a matrix. They extend the idea of vector length to matrices. Norms are crucial for analyzing stability, error growth, and performance of algorithms.</p>
<p>Some important norms:</p>
<ul>
<li>Frobenius norm:</li>
</ul>
<p><span>\[
\|A\|_F = \sqrt{\sum_{i,j} |a_{ij}|^2}
\]</span></p>
<p>Equivalent to treating the matrix as a big vector.</p>
<ul>
<li>Spectral norm (operator 2-norm):</li>
</ul>
<p><span>\[
\|A\|_2 = \sigma_{\max}
\]</span></p>
<p>The largest singular value - tells how much <span>\(A\)</span> can stretch a vector.</p>
<ul>
<li>1-norm: maximum absolute column sum.</li>
<li>∞-norm: maximum absolute row sum.</li>
</ul>
<section id="set-up-your-lab-87">
<h4 data-anchor-id="set-up-your-lab-87">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-87">
<h4 data-anchor-id="step-by-step-code-walkthrough-87">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Build a test matrix</li>
</ol>
<div id="235cb189" data-execution_count="500"><pre><code><span id="cb861-1">A <span>=</span> np.array([[<span>1</span>, <span>-</span><span>2</span>, <span>3</span>],</span>
<span id="cb861-2">              [<span>0</span>,  <span>4</span>, <span>5</span>],</span>
<span id="cb861-3">              [<span>-</span><span>1</span>, <span>2</span>, <span>1</span>]])</span></code></pre></div>
<ol start="2" type="1">
<li>Compute different norms</li>
</ol>
<div id="6649f4bd" data-execution_count="501">
<div id="cb862"><pre><code><span id="cb862-1">fro <span>=</span> np.linalg.norm(A, <span>'fro'</span>)</span>
<span id="cb862-2">spec <span>=</span> np.linalg.norm(A, <span>2</span>)</span>
<span id="cb862-3">one_norm <span>=</span> np.linalg.norm(A, <span>1</span>)</span>
<span id="cb862-4">inf_norm <span>=</span> np.linalg.norm(A, np.inf)</span>
<span id="cb862-5"></span>
<span id="cb862-6"><span>print</span>(<span>"Frobenius norm:"</span>, fro)</span>
<span id="cb862-7"><span>print</span>(<span>"Spectral norm:"</span>, spec)</span>
<span id="cb862-8"><span>print</span>(<span>"1-norm:"</span>, one_norm)</span>
<span id="cb862-9"><span>print</span>(<span>"Infinity norm:"</span>, inf_norm)</span></code></pre></div>
<div>
<pre><code>Frobenius norm: 7.810249675906654
Spectral norm: 6.813953458914004
1-norm: 9.0
Infinity norm: 9.0</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Compare spectral norm with largest singular value</li>
</ol>
<div id="4863af1b" data-execution_count="502">
<div id="cb864"><pre><code><span id="cb864-1">U, S, Vt <span>=</span> np.linalg.svd(A)</span>
<span id="cb864-2"><span>print</span>(<span>"Largest singular value:"</span>, S[<span>0</span>])</span>
<span id="cb864-3"><span>print</span>(<span>"Spectral norm:"</span>, spec)</span></code></pre></div>
<div>
<pre><code>Largest singular value: 6.813953458914004
Spectral norm: 6.813953458914004</code></pre>
</div>
</div>
<p>They match → spectral norm = largest singular value.</p>
<ol start="4" type="1">
<li>Frobenius norm from singular values</li>
</ol>
<p><span>\[
\|A\|_F = \sqrt{\sigma_1^2 + \sigma_2^2 + \dots}
\]</span></p>
<div id="669c5bb0" data-execution_count="503">
<div id="cb866"><pre><code><span id="cb866-1">fro_from_svd <span>=</span> np.sqrt(np.<span>sum</span>(S<span>**</span><span>2</span>))</span>
<span id="cb866-2"><span>print</span>(<span>"Frobenius norm (from SVD):"</span>, fro_from_svd)</span></code></pre></div>
<div>
<pre><code>Frobenius norm (from SVD): 7.810249675906654</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Stretching effect demonstration</li>
</ol>
<p>Pick a random vector and see how much it grows:</p>
<div id="22f1ca82" data-execution_count="504">
<div id="cb868"><pre><code><span id="cb868-1">x <span>=</span> np.random.randn(<span>3</span>)</span>
<span id="cb868-2">stretch <span>=</span> np.linalg.norm(A <span>@</span> x) <span>/</span> np.linalg.norm(x)</span>
<span id="cb868-3"><span>print</span>(<span>"Stretch factor:"</span>, stretch)</span>
<span id="cb868-4"><span>print</span>(<span>"Spectral norm (max possible stretch):"</span>, spec)</span></code></pre></div>
<div>
<pre><code>Stretch factor: 2.7537463268177698
Spectral norm (max possible stretch): 6.813953458914004</code></pre>
</div>
</div>
<p>The stretch ≤ spectral norm, always.</p>
</section>
<section id="try-it-yourself-86">
<h4 data-anchor-id="try-it-yourself-86">Try It Yourself</h4>
<ol type="1">
<li>Compare norms for diagonal matrices - do they match the largest diagonal entry?</li>
<li>Generate random matrices and see how norms differ.</li>
<li>Compute Frobenius vs spectral norm for a rank-1 matrix.</li>
</ol>
</section>
<section id="the-takeaway-70">
<h4 data-anchor-id="the-takeaway-70">The Takeaway</h4>
<ul>
<li>Frobenius norm = overall energy of the matrix.</li>
<li>Spectral norm = maximum stretching power (largest singular value).</li>
<li>Other norms (1-norm, ∞-norm) capture row/column dominance.</li>
<li>Singular values unify all these views of “matrix size.”</li>
</ul>
</section>
</section>
<section id="regularization-ridgetikhonov-to-tame-instability">
<h3 data-anchor-id="regularization-ridgetikhonov-to-tame-instability">89. Regularization (Ridge/Tikhonov to Tame Instability)</h3>
<p>When solving <span>\(Ax = b\)</span>, if <span>\(A\)</span> is ill-conditioned (large condition number), small errors in data can cause huge errors in the solution. Regularization stabilizes the problem by adding a penalty term that discourages extreme solutions.</p>
<p>The most common form: ridge regression (a.k.a. Tikhonov regularization):</p>
<p><span>\[
x_\lambda = \arg\min_x \|Ax - b\|^2 + \lambda \|x\|^2
\]</span></p>
<p>Closed form:</p>
<p><span>\[
x_\lambda = (A^T A + \lambda I)^{-1} A^T b
\]</span></p>
<p>Here <span>\(\lambda &gt; 0\)</span> controls the amount of regularization:</p>
<ul>
<li>Small <span>\(\lambda\)</span>: solution close to least-squares.</li>
<li>Large <span>\(\lambda\)</span>: smaller coefficients, more stability.</li>
</ul>
<section id="set-up-your-lab-88">
<h4 data-anchor-id="set-up-your-lab-88">Set Up Your Lab</h4>
<div id="9049a165" data-execution_count="505"><pre><code><span id="cb870-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb870-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-88">
<h4 data-anchor-id="step-by-step-code-walkthrough-88">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Build an ill-conditioned system</li>
</ol>
<div id="acc8155f" data-execution_count="506"><pre><code><span id="cb871-1">A <span>=</span> np.array([[<span>1</span>, <span>1.001</span>],</span>
<span id="cb871-2">              [<span>1</span>, <span>0.999</span>]])</span>
<span id="cb871-3">b <span>=</span> np.array([<span>2</span>, <span>2</span>])</span></code></pre></div>
<ol start="2" type="1">
<li>Solve without regularization</li>
</ol>
<div id="36b4f819" data-execution_count="507">
<div id="cb872"><pre><code><span id="cb872-1">x_ls, <span>*</span>_ <span>=</span> np.linalg.lstsq(A, b, rcond<span>=</span><span>None</span>)</span>
<span id="cb872-2"><span>print</span>(<span>"Least squares solution:"</span>, x_ls)</span></code></pre></div>
<div>
<pre><code>Least squares solution: [ 2.00000000e+00 -2.84186735e-14]</code></pre>
</div>
</div>
<p>The result may be unstable.</p>
<ol start="3" type="1">
<li>Apply ridge regularization</li>
</ol>
<div id="d838a6e0" data-execution_count="508">
<div id="cb874"><pre><code><span id="cb874-1">lam <span>=</span> <span>0.1</span></span>
<span id="cb874-2">x_ridge <span>=</span> np.linalg.inv(A.T <span>@</span> A <span>+</span> lam<span>*</span>np.eye(<span>2</span>)) <span>@</span> A.T <span>@</span> b</span>
<span id="cb874-3"><span>print</span>(<span>"Ridge solution (λ=0.1):"</span>, x_ridge)</span></code></pre></div>
<div>
<pre><code>Ridge solution (λ=0.1): [0.97561927 0.97559976]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Compare effect of different λ</li>
</ol>
<div id="b2f022a5" data-execution_count="509">
<div id="cb876"><pre><code><span id="cb876-1">lambdas <span>=</span> np.logspace(<span>-</span><span>4</span>, <span>2</span>, <span>20</span>)</span>
<span id="cb876-2">solutions <span>=</span> []</span>
<span id="cb876-3"><span>for</span> lam <span>in</span> lambdas:</span>
<span id="cb876-4">    x_reg <span>=</span> np.linalg.inv(A.T <span>@</span> A <span>+</span> lam<span>*</span>np.eye(<span>2</span>)) <span>@</span> A.T <span>@</span> b</span>
<span id="cb876-5">    solutions.append(np.linalg.norm(x_reg))</span>
<span id="cb876-6"></span>
<span id="cb876-7">plt.semilogx(lambdas, solutions, <span>'o-'</span>)</span>
<span id="cb876-8">plt.xlabel(<span>"λ (regularization strength)"</span>)</span>
<span id="cb876-9">plt.ylabel(<span>"Solution norm"</span>)</span>
<span id="cb876-10">plt.title(<span>"Effect of ridge regularization"</span>)</span>
<span id="cb876-11">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-510-output-1.png" width="589" height="451"></p>
</figure>
</div>
</div>
<p>As <span>\(\lambda\)</span> increases, the solution becomes smaller and more stable.</p>
<ol start="5" type="1">
<li>Connection to SVD</li>
</ol>
<p>If <span>\(A = U \Sigma V^T\)</span>:</p>
<p><span>\[
x_\lambda = \sum_i \frac{\sigma_i}{\sigma_i^2 + \lambda} (u_i^T b) v_i
\]</span></p>
<p>Small singular values (causing instability) get damped by <span>\(\frac{\sigma_i}{\sigma_i^2 + \lambda}\)</span>.</p>
</section>
<section id="try-it-yourself-87">
<h4 data-anchor-id="try-it-yourself-87">Try It Yourself</h4>
<ol type="1">
<li>Experiment with larger and smaller <span>\(\lambda\)</span>. What happens to the solution?</li>
<li>Add random noise to <span>\(b\)</span>. Compare least-squares vs ridge stability.</li>
<li>Plot how each coefficient changes with λ.</li>
</ol>
</section>
<section id="the-takeaway-71">
<h4 data-anchor-id="the-takeaway-71">The Takeaway</h4>
<ul>
<li>Regularization controls instability in ill-conditioned problems.</li>
<li>Ridge regression balances fit vs.&nbsp;stability using λ.</li>
<li>In SVD terms, regularization damps small singular values that cause wild solutions.</li>
</ul>
</section>
</section>
<section id="rank-revealing-qr-and-practical-diagnostics-what-rank-really-is">
<h3 data-anchor-id="rank-revealing-qr-and-practical-diagnostics-what-rank-really-is">90. Rank-Revealing QR and Practical Diagnostics (What Rank Really Is)</h3>
<p>In practice, we often need to determine the numerical rank of a matrix - not just the theoretical rank, but how many directions carry meaningful information beyond round-off errors or noise. A useful tool for this is the Rank-Revealing QR (RRQR) factorization.</p>
<p>For a matrix <span>\(A\)</span>:</p>
<p><span>\[
A P = Q R
\]</span></p>
<ul>
<li><span>\(Q\)</span>: orthogonal matrix</li>
<li><span>\(R\)</span>: upper triangular matrix</li>
<li><span>\(P\)</span>: column permutation matrix</li>
</ul>
<p>By reordering columns smartly, the diagonal of <span>\(R\)</span> reveals which directions are significant.</p>
<section id="set-up-your-lab-89">
<h4 data-anchor-id="set-up-your-lab-89">Set Up Your Lab</h4>
<div id="8ba6e248" data-execution_count="510"><pre><code><span id="cb877-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb877-2"><span>from</span> scipy.linalg <span>import</span> qr</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-89">
<h4 data-anchor-id="step-by-step-code-walkthrough-89">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Build a nearly rank-deficient matrix</li>
</ol>
<div id="e3d441f0" data-execution_count="511"><pre><code><span id="cb878-1">A <span>=</span> np.array([[<span>1</span>, <span>2</span>, <span>3</span>],</span>
<span id="cb878-2">              [<span>2</span>, <span>4.001</span>, <span>6</span>],</span>
<span id="cb878-3">              [<span>3</span>, <span>6</span>, <span>9.001</span>]])</span>
<span id="cb878-4"><span>print</span>(<span>"Rank (theoretical):"</span>, np.linalg.matrix_rank(A))</span></code></pre></div>
<p>This matrix is almost rank 2 but with small perturbations.</p>
<ol start="2" type="1">
<li>QR with column pivoting</li>
</ol>
<div id="436f11a4" data-execution_count="512">
<div id="cb880"><pre><code><span id="cb880-1">Q, R, P <span>=</span> qr(A, pivoting<span>=</span><span>True</span>)</span>
<span id="cb880-2"><span>print</span>(<span>"R:</span><span>\n</span><span>"</span>, R)</span>
<span id="cb880-3"><span>print</span>(<span>"Column permutation:"</span>, P)</span></code></pre></div>
<div>
<pre><code>R:
 [[-1.12257740e+01 -7.48384925e+00 -3.74165738e+00]
 [ 0.00000000e+00 -1.20185042e-03 -1.84886859e-04]
 [ 0.00000000e+00  0.00000000e+00 -7.41196374e-05]]
Column permutation: [2 1 0]</code></pre>
</div>
</div>
<p>The diagonal entries of <span>\(R\)</span> decrease rapidly → numerical rank is determined where they become tiny.</p>
<ol start="3" type="1">
<li>Compare with SVD</li>
</ol>
<div id="036777be" data-execution_count="513">
<div id="cb882"><pre><code><span id="cb882-1">U, S, Vt <span>=</span> np.linalg.svd(A)</span>
<span id="cb882-2"><span>print</span>(<span>"Singular values:"</span>, S)</span></code></pre></div>
<div>
<pre><code>Singular values: [1.40009286e+01 1.00000000e-03 7.14238341e-05]</code></pre>
</div>
</div>
<p>The singular values tell the same story: one is very small → effective rank ≈ 2.</p>
<ol start="4" type="1">
<li>Thresholding for rank</li>
</ol>
<div id="ba3b4c4e" data-execution_count="514"><pre><code><span id="cb884-1">tol <span>=</span> <span>1e-3</span></span>
<span id="cb884-2">rank_est <span>=</span> np.<span>sum</span>(S <span>&gt;</span> tol)</span>
<span id="cb884-3"><span>print</span>(<span>"Estimated rank:"</span>, rank_est)</span></code></pre></div>
<ol start="5" type="1">
<li>Diagnostics on a noisy matrix</li>
</ol>
<div id="f410e44c" data-execution_count="515">
<div id="cb886"><pre><code><span id="cb886-1">np.random.seed(<span>0</span>)</span>
<span id="cb886-2">B <span>=</span> np.random.randn(<span>50</span>, <span>10</span>) <span>@</span> np.random.randn(<span>10</span>, <span>10</span>)  <span># rank ≤ 10</span></span>
<span id="cb886-3">B[:, <span>-</span><span>1</span>] <span>+=</span> <span>1e-6</span> <span>*</span> np.random.randn(<span>50</span>)  <span># tiny noise</span></span>
<span id="cb886-4"></span>
<span id="cb886-5">U, S, Vt <span>=</span> np.linalg.svd(B)</span>
<span id="cb886-6">plt.semilogy(S, <span>'o-'</span>)</span>
<span id="cb886-7">plt.title(<span>"Singular values (log scale)"</span>)</span>
<span id="cb886-8">plt.xlabel(<span>"Index"</span>)</span>
<span id="cb886-9">plt.ylabel(<span>"Value"</span>)</span>
<span id="cb886-10">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-516-output-1.png" width="591" height="449"></p>
</figure>
</div>
</div>
<p>The drop in singular values shows effective rank.</p>
</section>
<section id="try-it-yourself-88">
<h4 data-anchor-id="try-it-yourself-88">Try It Yourself</h4>
<ol type="1">
<li>Change the perturbation in <span>\(A\)</span> from 0.001 to 0.000001. Does the numerical rank change?</li>
<li>Test QR with pivoting on random rectangular matrices.</li>
<li>Compare rank estimates from QR vs SVD for large noisy matrices.</li>
</ol>
</section>
<section id="the-takeaway-72">
<h4 data-anchor-id="the-takeaway-72">The Takeaway</h4>
<ul>
<li>Rank-revealing QR is a practical tool to detect effective rank in real-world data.</li>
<li>SVD gives the most precise picture (singular values), but QR with pivoting is faster.</li>
<li>Understanding numerical rank is crucial for diagnostics, stability, and model complexity control.</li>
</ul>
</section>
</section>
</section>
<section id="chapter-10.-applications-and-computation">
<h2 data-anchor-id="chapter-10.-applications-and-computation">Chapter 10. Applications and computation</h2>
<section id="d3d-geometry-pipelines-cameras-rotations-and-transforms">
<h3 data-anchor-id="d3d-geometry-pipelines-cameras-rotations-and-transforms">91. 2D/3D Geometry Pipelines (Cameras, Rotations, and Transforms)</h3>
<p>Linear algebra powers the geometry pipelines in computer graphics and robotics.</p>
<ul>
<li>2D transforms: rotation, scaling, translation.</li>
<li>3D transforms: same ideas, but with an extra dimension.</li>
<li>Homogeneous coordinates let us unify all transforms (even translations) into matrix multiplications.</li>
</ul>
<section id="set-up-your-lab-90">
<h4 data-anchor-id="set-up-your-lab-90">Set Up Your Lab</h4>
<div id="df7c2c91" data-execution_count="516"><pre><code><span id="cb887-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb887-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-90">
<h4 data-anchor-id="step-by-step-code-walkthrough-90">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Rotation in 2D</li>
</ol>
<p><span>\[
R(\theta) =
\begin{bmatrix}
\cos\theta &amp; -\sin\theta \\
\sin\theta &amp; \cos\theta
\end{bmatrix}
\]</span></p>
<div id="e8f530c7" data-execution_count="517">
<div id="cb888"><pre><code><span id="cb888-1">theta <span>=</span> np.pi<span>/</span><span>4</span>  <span># 45 degrees</span></span>
<span id="cb888-2">R <span>=</span> np.array([[np.cos(theta), <span>-</span>np.sin(theta)],</span>
<span id="cb888-3">              [np.sin(theta),  np.cos(theta)]])</span>
<span id="cb888-4"></span>
<span id="cb888-5">point <span>=</span> np.array([<span>1</span>, <span>0</span>])</span>
<span id="cb888-6">rotated <span>=</span> R <span>@</span> point</span>
<span id="cb888-7"></span>
<span id="cb888-8"><span>print</span>(<span>"Original:"</span>, point)</span>
<span id="cb888-9"><span>print</span>(<span>"Rotated:"</span>, rotated)</span></code></pre></div>
<div>
<pre><code>Original: [1 0]
Rotated: [0.70710678 0.70710678]</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Translation using homogeneous coordinates</li>
</ol>
<p>In 2D:</p>
<p><span>\[
T(dx, dy) =
\begin{bmatrix}
1 &amp; 0 &amp; dx \\
0 &amp; 1 &amp; dy \\
0 &amp; 0 &amp; 1
\end{bmatrix}
\]</span></p>
<div id="539612c3" data-execution_count="518">
<div id="cb890"><pre><code><span id="cb890-1">T <span>=</span> np.array([[<span>1</span>,<span>0</span>,<span>2</span>],</span>
<span id="cb890-2">              [<span>0</span>,<span>1</span>,<span>1</span>],</span>
<span id="cb890-3">              [<span>0</span>,<span>0</span>,<span>1</span>]])</span>
<span id="cb890-4"></span>
<span id="cb890-5">p_h <span>=</span> np.array([<span>1</span>,<span>1</span>,<span>1</span>])  <span># homogeneous (x=1,y=1)</span></span>
<span id="cb890-6">translated <span>=</span> T <span>@</span> p_h</span>
<span id="cb890-7"><span>print</span>(<span>"Translated point:"</span>, translated)</span></code></pre></div>
<div>
<pre><code>Translated point: [3 2 1]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Combine rotation + translation</li>
</ol>
<p>Transformations compose by multiplying matrices.</p>
<div id="83b56b24" data-execution_count="519">
<div id="cb892"><pre><code><span id="cb892-1">M <span>=</span> T <span>@</span> np.block([[R, np.zeros((<span>2</span>,<span>1</span>))],</span>
<span id="cb892-2">                  [np.zeros((<span>1</span>,<span>2</span>)), <span>1</span>]])</span>
<span id="cb892-3">combined <span>=</span> M <span>@</span> p_h</span>
<span id="cb892-4"><span>print</span>(<span>"Combined transform (rotation+translation):"</span>, combined)</span></code></pre></div>
<div>
<pre><code>Combined transform (rotation+translation): [2.         2.41421356 1.        ]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>3D rotation (around z-axis)</li>
</ol>
<p><span>\[
R_z(\theta) =
\begin{bmatrix}
\cos\theta &amp; -\sin\theta &amp; 0 \\
\sin\theta &amp;  \cos\theta &amp; 0 \\
0          &amp; 0           &amp; 1
\end{bmatrix}
\]</span></p>
<div id="e2a3a04f" data-execution_count="520">
<div id="cb894"><pre><code><span id="cb894-1">theta <span>=</span> np.pi<span>/</span><span>3</span></span>
<span id="cb894-2">Rz <span>=</span> np.array([[np.cos(theta), <span>-</span>np.sin(theta), <span>0</span>],</span>
<span id="cb894-3">               [np.sin(theta),  np.cos(theta), <span>0</span>],</span>
<span id="cb894-4">               [<span>0</span>,              <span>0</span>,             <span>1</span>]])</span>
<span id="cb894-5"></span>
<span id="cb894-6">point3d <span>=</span> np.array([<span>1</span>,<span>0</span>,<span>0</span>])</span>
<span id="cb894-7">rotated3d <span>=</span> Rz <span>@</span> point3d</span>
<span id="cb894-8"><span>print</span>(<span>"3D rotated point:"</span>, rotated3d)</span></code></pre></div>
<div>
<pre><code>3D rotated point: [0.5       0.8660254 0.       ]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Camera projection (3D → 2D)</li>
</ol>
<p>Simple pinhole model:</p>
<p><span>\[
\begin{bmatrix}
x' \\
y'
\end{bmatrix}
=
\begin{bmatrix}
f \cdot x / z \\
f \cdot y / z
\end{bmatrix}
\]</span></p>
<div id="d8f93d16" data-execution_count="521">
<div id="cb896"><pre><code><span id="cb896-1">f <span>=</span> <span>1.0</span>  <span># focal length</span></span>
<span id="cb896-2">P <span>=</span> np.array([[f,<span>0</span>,<span>0</span>],</span>
<span id="cb896-3">              [<span>0</span>,f,<span>0</span>],</span>
<span id="cb896-4">              [<span>0</span>,<span>0</span>,<span>1</span>]])  <span># projection matrix</span></span>
<span id="cb896-5"></span>
<span id="cb896-6">point3d <span>=</span> np.array([<span>2</span>,<span>3</span>,<span>5</span>])</span>
<span id="cb896-7">p_proj <span>=</span> P <span>@</span> point3d</span>
<span id="cb896-8">p_proj <span>=</span> p_proj[:<span>2</span>] <span>/</span> p_proj[<span>2</span>]  <span># divide by z</span></span>
<span id="cb896-9"><span>print</span>(<span>"Projected 2D point:"</span>, p_proj)</span></code></pre></div>
<div>
<pre><code>Projected 2D point: [0.4 0.6]</code></pre>
</div>
</div>
</section>
<section id="try-it-yourself-89">
<h4 data-anchor-id="try-it-yourself-89">Try It Yourself</h4>
<ol type="1">
<li>Rotate a square in 2D, then translate it. Plot before/after.</li>
<li>Rotate a 3D point cloud around x, y, and z axes.</li>
<li>Project a cube into 2D using the pinhole camera model.</li>
</ol>
</section>
<section id="the-takeaway-73">
<h4 data-anchor-id="the-takeaway-73">The Takeaway</h4>
<ul>
<li>Geometry pipelines = sequences of linear transforms.</li>
<li>Homogeneous coordinates unify rotation, scaling, and translation.</li>
<li>Camera projection links 3D world to 2D images - a cornerstone of graphics and vision.</li>
</ul>
</section>
</section>
<section id="computer-graphics-and-robotics-homogeneous-tricks-in-action">
<h3 data-anchor-id="computer-graphics-and-robotics-homogeneous-tricks-in-action">92. Computer Graphics and Robotics (Homogeneous Tricks in Action)</h3>
<p>Computer graphics and robotics both rely on homogeneous coordinates to unify rotations, translations, scalings, and projections into a single framework. With <span>\(4 \times 4\)</span> matrices in 3D, entire transformation pipelines can be built as matrix products.</p>
<section id="set-up-your-lab-91">
<h4 data-anchor-id="set-up-your-lab-91">Set Up Your Lab</h4>
<div id="80c11f86" data-execution_count="522"><pre><code><span id="cb898-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb898-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-91">
<h4 data-anchor-id="step-by-step-code-walkthrough-91">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Homogeneous representation of a point</li>
</ol>
<p>In 3D:</p>
<p><span>\[
(x, y, z) \mapsto (x, y, z, 1)
\]</span></p>
<div id="22c8a6b5" data-execution_count="523"><pre><code><span id="cb899-1">p <span>=</span> np.array([<span>1</span>,<span>2</span>,<span>3</span>,<span>1</span>])  <span># homogeneous point</span></span></code></pre></div>
<ol start="2" type="1">
<li>Define translation, rotation, and scaling matrices</li>
</ol>
<ul>
<li>Translation by <span>\((dx,dy,dz)\)</span>:</li>
</ul>
<div id="9dbaf916" data-execution_count="524"><pre><code><span id="cb900-1">T <span>=</span> np.array([[<span>1</span>,<span>0</span>,<span>0</span>,<span>2</span>],</span>
<span id="cb900-2">              [<span>0</span>,<span>1</span>,<span>0</span>,<span>1</span>],</span>
<span id="cb900-3">              [<span>0</span>,<span>0</span>,<span>1</span>,<span>3</span>],</span>
<span id="cb900-4">              [<span>0</span>,<span>0</span>,<span>0</span>,<span>1</span>]])</span></code></pre></div>
<ul>
<li>Scaling by factors <span>\((sx, sy, sz)\)</span>:</li>
</ul>
<div id="48d5939f" data-execution_count="525"><pre><code><span id="cb901-1">S <span>=</span> np.diag([<span>2</span>, <span>0.5</span>, <span>1.5</span>, <span>1</span>])</span></code></pre></div>
<ul>
<li>Rotation about z-axis (<span>\(\theta = 90^\circ\)</span>):</li>
</ul>
<div id="35ea8743" data-execution_count="526"><pre><code><span id="cb902-1">theta <span>=</span> np.pi<span>/</span><span>2</span></span>
<span id="cb902-2">Rz <span>=</span> np.array([[np.cos(theta), <span>-</span>np.sin(theta), <span>0</span>, <span>0</span>],</span>
<span id="cb902-3">               [np.sin(theta),  np.cos(theta), <span>0</span>, <span>0</span>],</span>
<span id="cb902-4">               [<span>0</span>,              <span>0</span>,             <span>1</span>, <span>0</span>],</span>
<span id="cb902-5">               [<span>0</span>,              <span>0</span>,             <span>0</span>, <span>1</span>]])</span></code></pre></div>
<ol start="3" type="1">
<li>Combine transforms into a pipeline</li>
</ol>
<div id="f413813e" data-execution_count="527">
<div id="cb903"><pre><code><span id="cb903-1">M <span>=</span> T <span>@</span> Rz <span>@</span> S  <span># first scale, then rotate, then translate</span></span>
<span id="cb903-2">p_transformed <span>=</span> M <span>@</span> p</span>
<span id="cb903-3"><span>print</span>(<span>"Transformed point:"</span>, p_transformed)</span></code></pre></div>
<div>
<pre><code>Transformed point: [1.  3.  7.5 1. ]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Robotics: forward kinematics of a 2-link arm</li>
</ol>
<p>Each joint is a rotation + translation.</p>
<div id="a322ad7c" data-execution_count="528">
<div id="cb905"><pre><code><span id="cb905-1"><span>def</span> link(theta, length):</span>
<span id="cb905-2">    <span>return</span> np.array([[np.cos(theta), <span>-</span>np.sin(theta), <span>0</span>, length<span>*</span>np.cos(theta)],</span>
<span id="cb905-3">                     [np.sin(theta),  np.cos(theta), <span>0</span>, length<span>*</span>np.sin(theta)],</span>
<span id="cb905-4">                     [<span>0</span>,              <span>0</span>,             <span>1</span>, <span>0</span>],</span>
<span id="cb905-5">                     [<span>0</span>,              <span>0</span>,             <span>0</span>, <span>1</span>]])</span>
<span id="cb905-6"></span>
<span id="cb905-7">theta1, theta2 <span>=</span> np.pi<span>/</span><span>4</span>, np.pi<span>/</span><span>6</span></span>
<span id="cb905-8">L1, L2 <span>=</span> <span>2</span>, <span>1.5</span></span>
<span id="cb905-9"></span>
<span id="cb905-10">M1 <span>=</span> link(theta1, L1)</span>
<span id="cb905-11">M2 <span>=</span> link(theta2, L2)</span>
<span id="cb905-12"></span>
<span id="cb905-13">end_effector <span>=</span> M1 <span>@</span> M2 <span>@</span> np.array([<span>0</span>,<span>0</span>,<span>0</span>,<span>1</span>])</span>
<span id="cb905-14"><span>print</span>(<span>"End effector position:"</span>, end_effector[:<span>3</span>])</span></code></pre></div>
<div>
<pre><code>End effector position: [1.80244213 2.8631023  0.        ]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Graphics: simple 3D camera projection</li>
</ol>
<div id="532ad0dc" data-execution_count="529">
<div id="cb907"><pre><code><span id="cb907-1">f <span>=</span> <span>2.0</span></span>
<span id="cb907-2">P <span>=</span> np.array([[f,<span>0</span>,<span>0</span>,<span>0</span>],</span>
<span id="cb907-3">              [<span>0</span>,f,<span>0</span>,<span>0</span>],</span>
<span id="cb907-4">              [<span>0</span>,<span>0</span>,<span>1</span>,<span>0</span>]])</span>
<span id="cb907-5"></span>
<span id="cb907-6">cube <span>=</span> np.array([[x,y,z,<span>1</span>] <span>for</span> x <span>in</span> [<span>0</span>,<span>1</span>] <span>for</span> y <span>in</span> [<span>0</span>,<span>1</span>] <span>for</span> z <span>in</span> [<span>0</span>,<span>1</span>]])</span>
<span id="cb907-7">proj <span>=</span> (P <span>@</span> cube.T).T</span>
<span id="cb907-8">proj2d <span>=</span> proj[:,:<span>2</span>] <span>/</span> proj[:,<span>2</span>:<span>3</span>]</span>
<span id="cb907-9"></span>
<span id="cb907-10">plt.scatter(proj2d[:,<span>0</span>], proj2d[:,<span>1</span>])</span>
<span id="cb907-11">plt.title(<span>"Projected cube"</span>)</span>
<span id="cb907-12">plt.show()</span></code></pre></div>
<div>
<pre><code>/var/folders/_g/lq_pglm508df70x751kkxrl80000gp/T/ipykernel_31637/2038614107.py:8: RuntimeWarning: divide by zero encountered in divide
  proj2d = proj[:,:2] / proj[:,2:3]
/var/folders/_g/lq_pglm508df70x751kkxrl80000gp/T/ipykernel_31637/2038614107.py:8: RuntimeWarning: invalid value encountered in divide
  proj2d = proj[:,:2] / proj[:,2:3]</code></pre>
</div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-530-output-2.png" width="579" height="431"></p>
</figure>
</div>
</div>
</section>
<section id="try-it-yourself-90">
<h4 data-anchor-id="try-it-yourself-90">Try It Yourself</h4>
<ol type="1">
<li>Change order of transforms (<code>Rz @ S @ T</code>). How does the result differ?</li>
<li>Add a third joint to the robotic arm and compute new end-effector position.</li>
<li>Project the cube with different focal lengths <span>\(f\)</span>.</li>
</ol>
</section>
<section id="the-takeaway-74">
<h4 data-anchor-id="the-takeaway-74">The Takeaway</h4>
<ul>
<li>Homogeneous coordinates unify all transformations.</li>
<li>Robotics uses this framework for forward kinematics.</li>
<li>Graphics uses it for camera and projection pipelines.</li>
<li>Both fields rely on the same linear algebra tricks - just applied differently.</li>
</ul>
</section>
</section>
<section id="graphs-adjacency-and-laplacians-networks-via-matrices">
<h3 data-anchor-id="graphs-adjacency-and-laplacians-networks-via-matrices">93. Graphs, Adjacency, and Laplacians (Networks via Matrices)</h3>
<p>Graphs can be studied with linear algebra by encoding them into matrices. Two of the most important:</p>
<ul>
<li><p>Adjacency matrix <span>\(A\)</span>:</p>
<p><span>\[
A_{ij} =
\begin{cases}
1 &amp; \text{if edge between i and j exists} \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p></li>
<li><p>Graph Laplacian <span>\(L\)</span>:</p>
<p><span>\[
L = D - A
\]</span></p>
<p>where <span>\(D\)</span> is the degree matrix ($D_{ii} = $ number of neighbors of node <span>\(i\)</span>).</p></li>
</ul>
<p>These matrices let us analyze connectivity, diffusion, and clustering.</p>
<section id="set-up-your-lab-92">
<h4 data-anchor-id="set-up-your-lab-92">Set Up Your Lab</h4>
<div id="cdd35208" data-execution_count="530"><pre><code><span id="cb909-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb909-2"><span>import</span> networkx <span>as</span> nx</span>
<span id="cb909-3"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-92">
<h4 data-anchor-id="step-by-step-code-walkthrough-92">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Build a simple graph</li>
</ol>
<div id="84a2eebf" data-execution_count="531">
<div id="cb910"><pre><code><span id="cb910-1">G <span>=</span> nx.Graph()</span>
<span id="cb910-2">G.add_edges_from([(<span>0</span>,<span>1</span>), (<span>1</span>,<span>2</span>), (<span>2</span>,<span>3</span>), (<span>3</span>,<span>0</span>), (<span>0</span>,<span>2</span>)])  <span># square with diagonal</span></span>
<span id="cb910-3"></span>
<span id="cb910-4">nx.draw(G, with_labels<span>=</span><span>True</span>, node_color<span>=</span><span>"lightblue"</span>, node_size<span>=</span><span>800</span>)</span>
<span id="cb910-5">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-532-output-1.png" width="691" height="499"></p>
</figure>
</div>
</div>
<ol start="2" type="1">
<li>Adjacency matrix</li>
</ol>
<div id="501a09ad" data-execution_count="532">
<div id="cb911"><pre><code><span id="cb911-1">A <span>=</span> nx.to_numpy_array(G)</span>
<span id="cb911-2"><span>print</span>(<span>"Adjacency matrix:</span><span>\n</span><span>"</span>, A)</span></code></pre></div>
<div>
<pre><code>Adjacency matrix:
 [[0. 1. 1. 1.]
 [1. 0. 1. 0.]
 [1. 1. 0. 1.]
 [1. 0. 1. 0.]]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Degree and Laplacian matrices</li>
</ol>
<div id="21c0739e" data-execution_count="533">
<div id="cb913"><pre><code><span id="cb913-1">D <span>=</span> np.diag(A.<span>sum</span>(axis<span>=</span><span>1</span>))</span>
<span id="cb913-2">L <span>=</span> D <span>-</span> A</span>
<span id="cb913-3"><span>print</span>(<span>"Degree matrix:</span><span>\n</span><span>"</span>, D)</span>
<span id="cb913-4"><span>print</span>(<span>"Graph Laplacian:</span><span>\n</span><span>"</span>, L)</span></code></pre></div>
<div>
<pre><code>Degree matrix:
 [[3. 0. 0. 0.]
 [0. 2. 0. 0.]
 [0. 0. 3. 0.]
 [0. 0. 0. 2.]]
Graph Laplacian:
 [[ 3. -1. -1. -1.]
 [-1.  2. -1.  0.]
 [-1. -1.  3. -1.]
 [-1.  0. -1.  2.]]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Eigenvalues of Laplacian (connectivity check)</li>
</ol>
<div id="725c9809" data-execution_count="534">
<div id="cb915"><pre><code><span id="cb915-1">eigvals, eigvecs <span>=</span> np.linalg.eigh(L)</span>
<span id="cb915-2"><span>print</span>(<span>"Laplacian eigenvalues:"</span>, eigvals)</span></code></pre></div>
<div>
<pre><code>Laplacian eigenvalues: [1.11022302e-16 2.00000000e+00 4.00000000e+00 4.00000000e+00]</code></pre>
</div>
</div>
<ul>
<li>The number of zero eigenvalues = number of connected components.</li>
</ul>
<ol start="5" type="1">
<li>Spectral embedding (clustering)</li>
</ol>
<p>Use Laplacian eigenvectors to embed nodes in low dimensions.</p>
<div id="51f42d71" data-execution_count="535">
<div id="cb917"><pre><code><span id="cb917-1">coords <span>=</span> eigvecs[:,<span>1</span>:<span>3</span>]  <span># skip the trivial first eigenvector</span></span>
<span id="cb917-2">plt.scatter(coords[:,<span>0</span>], coords[:,<span>1</span>], c<span>=</span><span>range</span>(<span>len</span>(coords)), cmap<span>=</span><span>"tab10"</span>, s<span>=</span><span>200</span>)</span>
<span id="cb917-3"><span>for</span> i, (x,y) <span>in</span> <span>enumerate</span>(coords):</span>
<span id="cb917-4">    plt.text(x, y, <span>str</span>(i), fontsize<span>=</span><span>12</span>, ha<span>=</span><span>"center"</span>, va<span>=</span><span>"center"</span>, color<span>=</span><span>"white"</span>)</span>
<span id="cb917-5">plt.title(<span>"Spectral embedding of graph"</span>)</span>
<span id="cb917-6">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-536-output-1.png" width="582" height="431"></p>
</figure>
</div>
</div>
</section>
<section id="try-it-yourself-91">
<h4 data-anchor-id="try-it-yourself-91">Try It Yourself</h4>
<ol type="1">
<li>Remove one edge from the graph and see how Laplacian eigenvalues change.</li>
<li>Add a disconnected node - does an extra zero eigenvalue appear?</li>
<li>Try a random graph and compare adjacency vs Laplacian spectra.</li>
</ol>
</section>
<section id="the-takeaway-75">
<h4 data-anchor-id="the-takeaway-75">The Takeaway</h4>
<ul>
<li>Adjacency matrices describe direct graph structure.</li>
<li>Laplacians capture connectivity and diffusion.</li>
<li>Eigenvalues of <span>\(L\)</span> reveal graph properties like connectedness and clustering - bridging networks with linear algebra.</li>
</ul>
</section>
</section>
<section id="data-preprocessing-as-linear-ops-centering-whitening-scaling">
<h3 data-anchor-id="data-preprocessing-as-linear-ops-centering-whitening-scaling">94. Data Preprocessing as Linear Ops (Centering, Whitening, Scaling)</h3>
<p>Many machine learning and data analysis workflows begin with preprocessing, and linear algebra provides the tools.</p>
<ul>
<li>Centering: subtract the mean → move data to origin.</li>
<li>Scaling: divide by standard deviation → normalize feature ranges.</li>
<li>Whitening: decorrelate features → make covariance matrix the identity.</li>
</ul>
<p>Each step can be written as a matrix operation.</p>
<section id="set-up-your-lab-93">
<h4 data-anchor-id="set-up-your-lab-93">Set Up Your Lab</h4>
<div id="792dc3fc" data-execution_count="536"><pre><code><span id="cb918-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb918-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-93">
<h4 data-anchor-id="step-by-step-code-walkthrough-93">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Generate correlated data</li>
</ol>
<div id="45baa38e" data-execution_count="537">
<div id="cb919"><pre><code><span id="cb919-1">np.random.seed(<span>0</span>)</span>
<span id="cb919-2">X <span>=</span> np.random.randn(<span>200</span>, <span>2</span>) <span>@</span> np.array([[<span>3</span>,<span>1</span>],[<span>1</span>,<span>0.5</span>]])</span>
<span id="cb919-3">plt.scatter(X[:,<span>0</span>], X[:,<span>1</span>], alpha<span>=</span><span>0.4</span>)</span>
<span id="cb919-4">plt.title(<span>"Original correlated data"</span>)</span>
<span id="cb919-5">plt.axis(<span>"equal"</span>)</span>
<span id="cb919-6">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-538-output-1.png" width="569" height="431"></p>
</figure>
</div>
</div>
<ol start="2" type="1">
<li>Centering (subtract mean)</li>
</ol>
<div id="ac953bbe" data-execution_count="538">
<div id="cb920"><pre><code><span id="cb920-1">X_centered <span>=</span> X <span>-</span> X.mean(axis<span>=</span><span>0</span>)</span>
<span id="cb920-2"><span>print</span>(<span>"Mean after centering:"</span>, X_centered.mean(axis<span>=</span><span>0</span>))</span></code></pre></div>
<div>
<pre><code>Mean after centering: [ 8.88178420e-18 -1.22124533e-17]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Scaling (normalize features)</li>
</ol>
<div id="5446af64" data-execution_count="539">
<div id="cb922"><pre><code><span id="cb922-1">X_scaled <span>=</span> X_centered <span>/</span> X_centered.std(axis<span>=</span><span>0</span>)</span>
<span id="cb922-2"><span>print</span>(<span>"Std after scaling:"</span>, X_scaled.std(axis<span>=</span><span>0</span>))</span></code></pre></div>
<div>
<pre><code>Std after scaling: [1. 1.]</code></pre>
</div>
</div>
<ol start="4" type="1">
<li>Whitening via eigen-decomposition</li>
</ol>
<p>Covariance of centered data:</p>
<div id="b3f5265b" data-execution_count="540"><pre><code><span id="cb924-1">C <span>=</span> np.cov(X_centered.T)</span>
<span id="cb924-2">eigvals, eigvecs <span>=</span> np.linalg.eigh(C)</span>
<span id="cb924-3"></span>
<span id="cb924-4">W <span>=</span> eigvecs <span>@</span> np.diag(<span>1</span><span>/</span>np.sqrt(eigvals)) <span>@</span> eigvecs.T</span>
<span id="cb924-5">X_white <span>=</span> X_centered <span>@</span> W</span></code></pre></div>
<p>Check covariance:</p>
<div id="31827587" data-execution_count="541">
<div id="cb925"><pre><code><span id="cb925-1"><span>print</span>(<span>"Whitened covariance:</span><span>\n</span><span>"</span>, np.cov(X_white.T))</span></code></pre></div>
<div>
<pre><code>Whitened covariance:
 [[1.00000000e+00 2.54402864e-15]
 [2.54402864e-15 1.00000000e+00]]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Compare scatter plots</li>
</ol>
<div id="60d5737a" data-execution_count="542">
<div id="cb927"><pre><code><span id="cb927-1">plt.subplot(<span>1</span>,<span>3</span>,<span>1</span>)</span>
<span id="cb927-2">plt.scatter(X[:,<span>0</span>], X[:,<span>1</span>], alpha<span>=</span><span>0.4</span>)</span>
<span id="cb927-3">plt.title(<span>"Original"</span>)</span>
<span id="cb927-4"></span>
<span id="cb927-5">plt.subplot(<span>1</span>,<span>3</span>,<span>2</span>)</span>
<span id="cb927-6">plt.scatter(X_scaled[:,<span>0</span>], X_scaled[:,<span>1</span>], alpha<span>=</span><span>0.4</span>)</span>
<span id="cb927-7">plt.title(<span>"Scaled"</span>)</span>
<span id="cb927-8"></span>
<span id="cb927-9">plt.subplot(<span>1</span>,<span>3</span>,<span>3</span>)</span>
<span id="cb927-10">plt.scatter(X_white[:,<span>0</span>], X_white[:,<span>1</span>], alpha<span>=</span><span>0.4</span>)</span>
<span id="cb927-11">plt.title(<span>"Whitened"</span>)</span>
<span id="cb927-12"></span>
<span id="cb927-13">plt.tight_layout()</span>
<span id="cb927-14">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-543-output-1.png" width="662" height="470"></p>
</figure>
</div>
</div>
<ul>
<li>Original: elongated ellipse.</li>
<li>Scaled: axis-aligned ellipse.</li>
<li>Whitened: circular cloud (uncorrelated, unit variance).</li>
</ul>
</section>
<section id="try-it-yourself-92">
<h4 data-anchor-id="try-it-yourself-92">Try It Yourself</h4>
<ol type="1">
<li>Add a third feature and apply centering, scaling, whitening.</li>
<li>Compare whitening with PCA - they use the same eigen-decomposition.</li>
<li>Test what happens if you skip centering before whitening.</li>
</ol>
</section>
<section id="the-takeaway-76">
<h4 data-anchor-id="the-takeaway-76">The Takeaway</h4>
<ul>
<li>Centering → mean zero.</li>
<li>Scaling → unit variance.</li>
<li>Whitening → features uncorrelated, variance = 1. Linear algebra provides the exact matrix operations to make preprocessing systematic and reliable.</li>
</ul>
</section>
</section>
<section id="linear-regression-and-classification-from-model-to-matrix">
<h3 data-anchor-id="linear-regression-and-classification-from-model-to-matrix">95. Linear Regression and Classification (From Model to Matrix)</h3>
<p>Linear regression and classification problems can be written neatly in matrix form. This unifies data, models, and solutions under the framework of least squares and linear decision boundaries.</p>
<section id="linear-regression-model">
<h4 data-anchor-id="linear-regression-model">Linear Regression Model</h4>
<p>For data <span>\((x_i, y_i)\)</span>:</p>
<p><span>\[
y \approx X \beta
\]</span></p>
<ul>
<li><span>\(X\)</span>: design matrix (rows = samples, columns = features).</li>
<li><span>\(\beta\)</span>: coefficients to solve for.</li>
<li>Solution (least squares):</li>
</ul>
<p><span>\[
\hat{\beta} = (X^T X)^{-1} X^T y
\]</span></p>
</section>
<section id="set-up-your-lab-94">
<h4 data-anchor-id="set-up-your-lab-94">Set Up Your Lab</h4>
<div id="e8b6b4df" data-execution_count="543"><pre><code><span id="cb928-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb928-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb928-3"><span>from</span> sklearn.datasets <span>import</span> make_classification</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-94">
<h4 data-anchor-id="step-by-step-code-walkthrough-94">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Linear regression example</li>
</ol>
<div id="5194d201" data-execution_count="544"><pre><code><span id="cb929-1">np.random.seed(<span>0</span>)</span>
<span id="cb929-2">X <span>=</span> np.linspace(<span>0</span>, <span>10</span>, <span>30</span>).reshape(<span>-</span><span>1</span>,<span>1</span>)</span>
<span id="cb929-3">y <span>=</span> <span>3</span><span>*</span>X.squeeze() <span>+</span> <span>5</span> <span>+</span> np.random.randn(<span>30</span>)<span>*</span><span>2</span></span></code></pre></div>
<p>Construct design matrix with bias term:</p>
<div id="fabf5d4e" data-execution_count="545">
<div id="cb930"><pre><code><span id="cb930-1">X_design <span>=</span> np.column_stack([np.ones_like(X), X])</span>
<span id="cb930-2">beta_hat, <span>*</span>_ <span>=</span> np.linalg.lstsq(X_design, y, rcond<span>=</span><span>None</span>)</span>
<span id="cb930-3"><span>print</span>(<span>"Fitted coefficients:"</span>, beta_hat)</span></code></pre></div>
<div>
<pre><code>Fitted coefficients: [6.65833151 2.84547628]</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Visualize regression line</li>
</ol>
<div id="0dcf8422" data-execution_count="546">
<div id="cb932"><pre><code><span id="cb932-1">y_pred <span>=</span> X_design <span>@</span> beta_hat</span>
<span id="cb932-2"></span>
<span id="cb932-3">plt.scatter(X, y, label<span>=</span><span>"Data"</span>)</span>
<span id="cb932-4">plt.plot(X, y_pred, <span>'r-'</span>, label<span>=</span><span>"Fitted line"</span>)</span>
<span id="cb932-5">plt.legend()</span>
<span id="cb932-6">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-547-output-1.png" width="566" height="411"></p>
</figure>
</div>
</div>
<ol start="3" type="1">
<li>Logistic classification with linear decision boundary</li>
</ol>
<div id="691d7b6a" data-execution_count="547">
<div id="cb933"><pre><code><span id="cb933-1">Xc, yc <span>=</span> make_classification(n_features<span>=</span><span>2</span>, n_redundant<span>=</span><span>0</span>, n_informative<span>=</span><span>2</span>,</span>
<span id="cb933-2">                             n_clusters_per_class<span>=</span><span>1</span>, n_samples<span>=</span><span>100</span>, random_state<span>=</span><span>0</span>)</span>
<span id="cb933-3"></span>
<span id="cb933-4">plt.scatter(Xc[:,<span>0</span>], Xc[:,<span>1</span>], c<span>=</span>yc, cmap<span>=</span><span>"bwr"</span>, alpha<span>=</span><span>0.7</span>)</span>
<span id="cb933-5">plt.title(<span>"Classification data"</span>)</span>
<span id="cb933-6">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-548-output-1.png" width="569" height="431"></p>
</figure>
</div>
</div>
<ol start="4" type="1">
<li>Logistic regression via gradient descent</li>
</ol>
<div id="9af00f2e" data-execution_count="548">
<div id="cb934"><pre><code><span id="cb934-1"><span>def</span> sigmoid(z):</span>
<span id="cb934-2">    <span>return</span> <span>1</span><span>/</span>(<span>1</span><span>+</span>np.exp(<span>-</span>z))</span>
<span id="cb934-3"></span>
<span id="cb934-4">X_design <span>=</span> np.column_stack([np.ones(<span>len</span>(Xc)), Xc])</span>
<span id="cb934-5">y <span>=</span> yc</span>
<span id="cb934-6"></span>
<span id="cb934-7">w <span>=</span> np.zeros(X_design.shape[<span>1</span>])</span>
<span id="cb934-8">lr <span>=</span> <span>0.1</span></span>
<span id="cb934-9"></span>
<span id="cb934-10"><span>for</span> _ <span>in</span> <span>range</span>(<span>2000</span>):</span>
<span id="cb934-11">    preds <span>=</span> sigmoid(X_design <span>@</span> w)</span>
<span id="cb934-12">    grad <span>=</span> X_design.T <span>@</span> (preds <span>-</span> y) <span>/</span> <span>len</span>(y)</span>
<span id="cb934-13">    w <span>-=</span> lr <span>*</span> grad</span>
<span id="cb934-14"></span>
<span id="cb934-15"><span>print</span>(<span>"Learned weights:"</span>, w)</span></code></pre></div>
<div>
<pre><code>Learned weights: [-2.10451116  0.70752542  4.13295129]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Plot decision boundary</li>
</ol>
<div id="1085020b" data-execution_count="549">
<div id="cb936"><pre><code><span id="cb936-1">xx, yy <span>=</span> np.meshgrid(np.linspace(Xc[:,<span>0</span>].<span>min</span>()<span>-</span><span>1</span>, Xc[:,<span>0</span>].<span>max</span>()<span>+</span><span>1</span>, <span>200</span>),</span>
<span id="cb936-2">                     np.linspace(Xc[:,<span>1</span>].<span>min</span>()<span>-</span><span>1</span>, Xc[:,<span>1</span>].<span>max</span>()<span>+</span><span>1</span>, <span>200</span>))</span>
<span id="cb936-3"></span>
<span id="cb936-4">grid <span>=</span> np.c_[np.ones(xx.size), xx.ravel(), yy.ravel()]</span>
<span id="cb936-5">probs <span>=</span> sigmoid(grid <span>@</span> w).reshape(xx.shape)</span>
<span id="cb936-6"></span>
<span id="cb936-7">plt.contourf(xx, yy, probs, levels<span>=</span>[<span>0</span>,<span>0.5</span>,<span>1</span>], alpha<span>=</span><span>0.3</span>, cmap<span>=</span><span>"bwr"</span>)</span>
<span id="cb936-8">plt.scatter(Xc[:,<span>0</span>], Xc[:,<span>1</span>], c<span>=</span>yc, cmap<span>=</span><span>"bwr"</span>, edgecolor<span>=</span><span>"k"</span>)</span>
<span id="cb936-9">plt.title(<span>"Linear decision boundary"</span>)</span>
<span id="cb936-10">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-550-output-1.png" width="569" height="431"></p>
</figure>
</div>
</div>
</section>
<section id="try-it-yourself-93">
<h4 data-anchor-id="try-it-yourself-93">Try It Yourself</h4>
<ol type="1">
<li>Add polynomial features to regression and refit. Does the line bend into a curve?</li>
<li>Change learning rate in logistic regression - what happens?</li>
<li>Generate data that is not linearly separable. Can a linear model still classify well?</li>
</ol>
</section>
<section id="the-takeaway-77">
<h4 data-anchor-id="the-takeaway-77">The Takeaway</h4>
<ul>
<li>Regression and classification fit naturally into linear algebra with matrix formulations.</li>
<li>Least squares solves regression directly; logistic regression requires optimization.</li>
<li>Linear models are simple, interpretable, and still form the foundation of modern machine learning.</li>
</ul>
</section>
</section>
<section id="pca-in-practice-dimensionality-reduction-workflow">
<h3 data-anchor-id="pca-in-practice-dimensionality-reduction-workflow">96. PCA in Practice (Dimensionality Reduction Workflow)</h3>
<p>Principal Component Analysis (PCA) is widely used to reduce dimensions, compress data, and visualize high-dimensional datasets. Here, we’ll walk through a full PCA workflow: centering, computing components, projecting, and visualizing.</p>
<section id="set-up-your-lab-95">
<h4 data-anchor-id="set-up-your-lab-95">Set Up Your Lab</h4>
<div id="371a6791" data-execution_count="550"><pre><code><span id="cb937-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb937-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb937-3"><span>from</span> sklearn.datasets <span>import</span> load_digits</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-95">
<h4 data-anchor-id="step-by-step-code-walkthrough-95">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Load dataset (digits)</li>
</ol>
<div id="bdc48c42" data-execution_count="551"><pre><code><span id="cb938-1">digits <span>=</span> load_digits()</span>
<span id="cb938-2">X <span>=</span> digits.data  <span># shape (1797, 64)</span></span>
<span id="cb938-3">y <span>=</span> digits.target</span>
<span id="cb938-4"><span>print</span>(<span>"Data shape:"</span>, X.shape)</span></code></pre></div>
<p>Each sample is an 8×8 grayscale image flattened into 64 features.</p>
<ol start="2" type="1">
<li>Center the data</li>
</ol>
<div id="815b2e39" data-execution_count="552"><pre><code><span id="cb940-1">X_centered <span>=</span> X <span>-</span> X.mean(axis<span>=</span><span>0</span>)</span></code></pre></div>
<ol start="3" type="1">
<li>Compute PCA via SVD</li>
</ol>
<div id="e4b8509b" data-execution_count="553"><pre><code><span id="cb941-1">U, S, Vt <span>=</span> np.linalg.svd(X_centered, full_matrices<span>=</span><span>False</span>)</span>
<span id="cb941-2">explained_variance <span>=</span> (S<span>**</span><span>2</span>) <span>/</span> (<span>len</span>(X) <span>-</span> <span>1</span>)</span>
<span id="cb941-3">explained_ratio <span>=</span> explained_variance <span>/</span> explained_variance.<span>sum</span>()</span></code></pre></div>
<ol start="4" type="1">
<li>Plot explained variance ratio</li>
</ol>
<div id="6546ba1f" data-execution_count="554">
<div id="cb942"><pre><code><span id="cb942-1">plt.plot(np.cumsum(explained_ratio[:<span>30</span>]), <span>'o-'</span>)</span>
<span id="cb942-2">plt.xlabel(<span>"Number of components"</span>)</span>
<span id="cb942-3">plt.ylabel(<span>"Cumulative explained variance"</span>)</span>
<span id="cb942-4">plt.title(<span>"PCA explained variance"</span>)</span>
<span id="cb942-5">plt.grid(<span>True</span>)</span>
<span id="cb942-6">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-555-output-1.png" width="590" height="449"></p>
</figure>
</div>
</div>
<p>This shows how many components are needed to capture most variance.</p>
<ol start="5" type="1">
<li>Project onto top 2 components for visualization</li>
</ol>
<div id="1064dba5" data-execution_count="555">
<div id="cb943"><pre><code><span id="cb943-1">X_pca2 <span>=</span> X_centered <span>@</span> Vt[:<span>2</span>].T</span>
<span id="cb943-2">plt.scatter(X_pca2[:,<span>0</span>], X_pca2[:,<span>1</span>], c<span>=</span>y, cmap<span>=</span><span>"tab10"</span>, alpha<span>=</span><span>0.6</span>, s<span>=</span><span>15</span>)</span>
<span id="cb943-3">plt.colorbar()</span>
<span id="cb943-4">plt.title(<span>"Digits dataset (PCA 2D projection)"</span>)</span>
<span id="cb943-5">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-556-output-1.png" width="536" height="431"></p>
</figure>
</div>
</div>
<ol start="6" type="1">
<li>Reconstruct images from reduced dimensions</li>
</ol>
<div id="4adf8ea5" data-execution_count="556">
<div id="cb944"><pre><code><span id="cb944-1">k <span>=</span> <span>20</span></span>
<span id="cb944-2">X_pca20 <span>=</span> X_centered <span>@</span> Vt[:k].T</span>
<span id="cb944-3">X_reconstructed <span>=</span> X_pca20 <span>@</span> Vt[:k]</span>
<span id="cb944-4"></span>
<span id="cb944-5">fig, axes <span>=</span> plt.subplots(<span>2</span>, <span>10</span>, figsize<span>=</span>(<span>10</span>,<span>2</span>))</span>
<span id="cb944-6"><span>for</span> i <span>in</span> <span>range</span>(<span>10</span>):</span>
<span id="cb944-7">    axes[<span>0</span>,i].imshow(X[i].reshape(<span>8</span>,<span>8</span>), cmap<span>=</span><span>"gray"</span>)</span>
<span id="cb944-8">    axes[<span>0</span>,i].axis(<span>"off"</span>)</span>
<span id="cb944-9">    axes[<span>1</span>,i].imshow(X_reconstructed[i].reshape(<span>8</span>,<span>8</span>), cmap<span>=</span><span>"gray"</span>)</span>
<span id="cb944-10">    axes[<span>1</span>,i].axis(<span>"off"</span>)</span>
<span id="cb944-11">plt.suptitle(<span>"Original (top) vs PCA reconstruction (bottom, 20 comps)"</span>)</span>
<span id="cb944-12">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-557-output-1.png" width="763" height="184"></p>
</figure>
</div>
</div>
<p>Even with only 20/64 components, the digits remain recognizable.</p>
</section>
<section id="try-it-yourself-94">
<h4 data-anchor-id="try-it-yourself-94">Try It Yourself</h4>
<ol type="1">
<li>Change <span>\(k\)</span> to 5, 10, 30 - how do reconstructions change?</li>
<li>Use top 2 PCA components to classify digits with k-NN. How does accuracy compare to full 64 features?</li>
<li>Try PCA on your own dataset (images, tabular data).</li>
</ol>
</section>
<section id="the-takeaway-78">
<h4 data-anchor-id="the-takeaway-78">The Takeaway</h4>
<ul>
<li>PCA reduces dimensions while keeping maximum variance.</li>
<li>In practice: center → decompose → select top components → project/reconstruct.</li>
<li>PCA enables visualization, compression, and denoising in real-world workflows.</li>
</ul>
</section>
</section>
<section id="recommender-systems-and-low-rank-models-fill-the-missing-entries">
<h3 data-anchor-id="recommender-systems-and-low-rank-models-fill-the-missing-entries">97. Recommender Systems and Low-Rank Models (Fill the Missing Entries)</h3>
<p>Recommender systems often deal with incomplete matrices - rows are users, columns are items, entries are ratings. Most entries are missing, but the matrix is usually close to low-rank (because user preferences depend on only a few hidden factors). SVD and low-rank approximations are powerful tools to fill in these missing values.</p>
<section id="set-up-your-lab-96">
<h4 data-anchor-id="set-up-your-lab-96">Set Up Your Lab</h4>
<div id="97ca48fe" data-execution_count="557"><pre><code><span id="cb945-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb945-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-96">
<h4 data-anchor-id="step-by-step-code-walkthrough-96">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Simulate a user–item rating matrix</li>
</ol>
<div id="b02cb992" data-execution_count="558"><pre><code><span id="cb946-1">np.random.seed(<span>0</span>)</span>
<span id="cb946-2">true_users <span>=</span> np.random.randn(<span>10</span>, <span>3</span>)   <span># 10 users, 3 latent features</span></span>
<span id="cb946-3">true_items <span>=</span> np.random.randn(<span>3</span>, <span>8</span>)    <span># 8 items</span></span>
<span id="cb946-4">R_full <span>=</span> true_users <span>@</span> true_items      <span># true low-rank ratings</span></span></code></pre></div>
<ol start="2" type="1">
<li>Hide some ratings (simulate missing data)</li>
</ol>
<div id="a149f707" data-execution_count="559">
<div id="cb947"><pre><code><span id="cb947-1">mask <span>=</span> np.random.rand(<span>*</span>R_full.shape) <span>&gt;</span> <span>0.3</span>  <span># keep 70% of entries</span></span>
<span id="cb947-2">R_obs <span>=</span> np.where(mask, R_full, np.nan)</span>
<span id="cb947-3"></span>
<span id="cb947-4"><span>print</span>(<span>"Observed ratings:</span><span>\n</span><span>"</span>, R_obs)</span></code></pre></div>
<div>
<pre><code>Observed ratings:
 [[-1.10781465         nan -3.56526968         nan -2.1729387   1.43510077
   1.46641178  0.79023284]
 [ 0.84819453         nan         nan         nan         nan         nan
   2.30434358  3.03008138]
 [        nan  0.32479187 -0.51818422         nan  0.02013802         nan
   1.29874918  1.33053637]
 [-1.81407786  1.24241182         nan -1.32723907         nan         nan
  -0.31110699         nan]
 [-0.48527696         nan -1.51957106         nan -0.86984941  0.52807989
          nan  0.33771451]
 [-0.26997359 -0.48498966         nan -2.73891459 -2.48167957  2.88740609
  -0.24614835         nan]
 [ 3.57769701 -1.608339    4.73789234  1.13583164  3.63451505 -2.60495928
   2.12453635  3.76472563]
 [ 0.69623809 -0.59117353 -0.28890188 -2.36431192         nan  1.50136796
   0.74268078         nan]
 [ 0.85768141  1.33357168         nan         nan  1.65089037 -2.46456289
   3.51030491  3.31220347]
 [-2.463496    0.60826298 -3.81241599 -2.11839267 -3.86597359  3.52934055
  -1.76203083 -2.63130953]]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li>Simple mean imputation (baseline)</li>
</ol>
<div id="208ac586" data-execution_count="560"><pre><code><span id="cb949-1">R_mean <span>=</span> np.where(np.isnan(R_obs), np.nanmean(R_obs), R_obs)</span></code></pre></div>
<ol start="4" type="1">
<li>Apply SVD for low-rank approximation</li>
</ol>
<div id="c39b13e2" data-execution_count="561"><pre><code><span id="cb950-1"><span># Replace NaNs with zeros for SVD step</span></span>
<span id="cb950-2">R_filled <span>=</span> np.nan_to_num(R_obs, nan<span>=</span><span>0.0</span>)</span>
<span id="cb950-3"></span>
<span id="cb950-4">U, S, Vt <span>=</span> np.linalg.svd(R_filled, full_matrices<span>=</span><span>False</span>)</span>
<span id="cb950-5"></span>
<span id="cb950-6">k <span>=</span> <span>3</span>  <span># latent dimension</span></span>
<span id="cb950-7">R_approx <span>=</span> U[:, :k] <span>@</span> np.diag(S[:k]) <span>@</span> Vt[:k, :]</span></code></pre></div>
<ol start="5" type="1">
<li>Compare filled matrix with ground truth</li>
</ol>
<div id="67904f97" data-execution_count="562">
<div id="cb951"><pre><code><span id="cb951-1">error <span>=</span> np.nanmean((R_full <span>-</span> R_approx)<span>**</span><span>2</span>)</span>
<span id="cb951-2"><span>print</span>(<span>"Approximation error (MSE):"</span>, error)</span></code></pre></div>
<div>
<pre><code>Approximation error (MSE): 1.4862378490976202</code></pre>
</div>
</div>
<ol start="6" type="1">
<li>Visualize original vs reconstructed</li>
</ol>
<div id="6e6cf3f3" data-execution_count="563">
<div id="cb953"><pre><code><span id="cb953-1">fig, axes <span>=</span> plt.subplots(<span>1</span>, <span>2</span>, figsize<span>=</span>(<span>8</span>,<span>4</span>))</span>
<span id="cb953-2">axes[<span>0</span>].imshow(R_full, cmap<span>=</span><span>"viridis"</span>)</span>
<span id="cb953-3">axes[<span>0</span>].set_title(<span>"True ratings"</span>)</span>
<span id="cb953-4">axes[<span>1</span>].imshow(R_approx, cmap<span>=</span><span>"viridis"</span>)</span>
<span id="cb953-5">axes[<span>1</span>].set_title(<span>"Low-rank approximation"</span>)</span>
<span id="cb953-6">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-564-output-1.png" width="598" height="357"></p>
</figure>
</div>
</div>
</section>
<section id="try-it-yourself-95">
<h4 data-anchor-id="try-it-yourself-95">Try It Yourself</h4>
<ol type="1">
<li>Vary <span>\(k\)</span> (2, 3, 5). Does error go down?</li>
<li>Mask more entries (50%, 80%) - how does SVD reconstruction perform?</li>
<li>Use iterative imputation: alternate filling missing entries with low-rank approximations.</li>
</ol>
</section>
<section id="the-takeaway-79">
<h4 data-anchor-id="the-takeaway-79">The Takeaway</h4>
<ul>
<li>Recommender systems rely on low-rank structure of user–item matrices.</li>
<li>SVD provides a natural way to approximate and fill missing ratings.</li>
<li>This low-rank modeling idea underpins modern collaborative filtering systems like Netflix and Spotify recommenders.</li>
</ul>
</section>
</section>
<section id="pagerank-and-random-walks-ranking-with-eigenvectors">
<h3 data-anchor-id="pagerank-and-random-walks-ranking-with-eigenvectors">98. PageRank and Random Walks (Ranking with Eigenvectors)</h3>
<p>The PageRank algorithm, made famous by Google, uses linear algebra and random walks on graphs to rank nodes (webpages, people, items). The idea: importance flows through links - being linked by important nodes makes you important.</p>

<section id="set-up-your-lab-97">
<h4 data-anchor-id="set-up-your-lab-97">Set Up Your Lab</h4>
<div id="a01f2545" data-execution_count="564"><pre><code><span id="cb954-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb954-2"><span>import</span> networkx <span>as</span> nx</span>
<span id="cb954-3"><span>import</span> matplotlib.pyplot <span>as</span> plt</span></code></pre></div>
</section>
<section id="step-by-step-code-walkthrough-97">
<h4 data-anchor-id="step-by-step-code-walkthrough-97">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Build a small directed graph</li>
</ol>
<div id="b6e2fe96" data-execution_count="565">
<div id="cb955"><pre><code><span id="cb955-1">G <span>=</span> nx.DiGraph()</span>
<span id="cb955-2">G.add_edges_from([</span>
<span id="cb955-3">    (<span>0</span>,<span>1</span>), (<span>1</span>,<span>2</span>), (<span>2</span>,<span>0</span>),  <span># cycle among 0–1–2</span></span>
<span id="cb955-4">    (<span>2</span>,<span>3</span>), (<span>3</span>,<span>2</span>),         <span># back-and-forth 2–3</span></span>
<span id="cb955-5">    (<span>1</span>,<span>3</span>), (<span>3</span>,<span>4</span>), (<span>4</span>,<span>1</span>)   <span># small loop with 1–3–4</span></span>
<span id="cb955-6">])</span>
<span id="cb955-7">nx.draw_circular(G, with_labels<span>=</span><span>True</span>, node_color<span>=</span><span>"lightblue"</span>, node_size<span>=</span><span>800</span>, arrowsize<span>=</span><span>15</span>)</span>
<span id="cb955-8">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-566-output-1.png" width="691" height="499"></p>
</figure>
</div>
</div>
<ol start="2" type="1">
<li>Build adjacency and transition matrix</li>
</ol>
<div id="65893da4" data-execution_count="566"><pre><code><span id="cb956-1">n <span>=</span> G.number_of_nodes()</span>
<span id="cb956-2">A <span>=</span> nx.to_numpy_array(G, nodelist<span>=</span><span>range</span>(n))</span>
<span id="cb956-3">P <span>=</span> A <span>/</span> A.<span>sum</span>(axis<span>=</span><span>1</span>, keepdims<span>=</span><span>True</span>)  <span># row-stochastic transition matrix</span></span></code></pre></div>
<ol start="3" type="1">
<li>Add teleportation (Google matrix)</li>
</ol>
<div id="d7e0f57a" data-execution_count="567"><pre><code><span id="cb957-1">alpha <span>=</span> <span>0.85</span>  <span># damping factor</span></span>
<span id="cb957-2">G_matrix <span>=</span> alpha <span>*</span> P <span>+</span> (<span>1</span> <span>-</span> alpha) <span>*</span> np.ones((n,n)) <span>/</span> n</span></code></pre></div>
<ol start="4" type="1">
<li>Power iteration to compute PageRank</li>
</ol>
<div id="13505b0b" data-execution_count="568">
<div id="cb958"><pre><code><span id="cb958-1">r <span>=</span> np.ones(n) <span>/</span> n  <span># start uniform</span></span>
<span id="cb958-2"><span>for</span> _ <span>in</span> <span>range</span>(<span>100</span>):</span>
<span id="cb958-3">    r <span>=</span> r <span>@</span> G_matrix</span>
<span id="cb958-4">r <span>/=</span> r.<span>sum</span>()</span>
<span id="cb958-5"><span>print</span>(<span>"PageRank vector:"</span>, r)</span></code></pre></div>
<div>
<pre><code>PageRank vector: [0.13219034 0.25472358 0.24044787 0.24044787 0.13219034]</code></pre>
</div>
</div>
<ol start="5" type="1">
<li>Compare with NetworkX built-in</li>
</ol>
<div id="d71f3c41" data-execution_count="569">
<div id="cb960"><pre><code><span id="cb960-1">pr <span>=</span> nx.pagerank(G, alpha<span>=</span>alpha)</span>
<span id="cb960-2"><span>print</span>(<span>"NetworkX PageRank:"</span>, pr)</span></code></pre></div>
<div>
<pre><code>NetworkX PageRank: {0: 0.13219008157546333, 1: 0.2547244023837789, 2: 0.24044771723264727, 3: 0.24044771723264727, 4: 0.13219008157546333}</code></pre>
</div>
</div>
<ol start="6" type="1">
<li>Visualize node importance</li>
</ol>
<div id="0bced649" data-execution_count="570">
<div id="cb962"><pre><code><span id="cb962-1">sizes <span>=</span> [<span>5000</span> <span>*</span> r_i <span>for</span> r_i <span>in</span> r]</span>
<span id="cb962-2">nx.draw_circular(G, with_labels<span>=</span><span>True</span>, node_size<span>=</span>sizes, node_color<span>=</span><span>"lightblue"</span>, arrowsize<span>=</span><span>15</span>)</span>
<span id="cb962-3">plt.title(<span>"PageRank visualization (node size ~ importance)"</span>)</span>
<span id="cb962-4">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-571-output-1.png" width="691" height="519"></p>
</figure>
</div>
</div>
</section>
<section id="try-it-yourself-96">
<h4 data-anchor-id="try-it-yourself-96">Try It Yourself</h4>
<ol type="1">
<li>Change <span>\(\alpha\)</span> (e.g., 0.6 vs 0.95). Does ranking change?</li>
<li>Add a “dangling node” with no outlinks. How does teleportation handle it?</li>
<li>Try PageRank on a larger graph (like a random graph with 50 nodes).</li>
</ol>
</section>
<section id="the-takeaway-80">
<h4 data-anchor-id="the-takeaway-80">The Takeaway</h4>
<ul>
<li>PageRank is a random-walk steady state problem.</li>
<li>It reduces to finding the dominant eigenvector of the Google matrix.</li>
<li>This method generalizes beyond webpages - to influence ranking, recommendation, and network analysis.</li>
</ul>
</section>
</section>
<section id="numerical-linear-algebra-essentials-floating-point-blaslapack">
<h3 data-anchor-id="numerical-linear-algebra-essentials-floating-point-blaslapack">99. Numerical Linear Algebra Essentials (Floating Point, BLAS/LAPACK)</h3>
<p>When working with linear algebra on computers, numbers are not exact. They live in floating-point arithmetic, and computations rely on highly optimized libraries like BLAS and LAPACK. Understanding these essentials is crucial to doing linear algebra at scale.</p>
<section id="floating-point-basics">
<h4 data-anchor-id="floating-point-basics">Floating Point Basics</h4>
<ul>
<li><p>Numbers are stored in base-2 scientific notation:</p>
<p><span>\[
x = \pm (1.b_1b_2b_3\ldots) \times 2^e
\]</span></p></li>
<li><p>Limited precision means rounding errors.</p></li>
<li><p>Two key constants:</p>
<ul>
<li>Machine epsilon ($<span>\(): smallest difference detectable (\)</span>^{-16}$ for double).</li>
<li>Overflow/underflow: too large or too small to represent.</li>
</ul></li>
</ul>
</section>
<section id="set-up-your-lab-98">
<h4 data-anchor-id="set-up-your-lab-98">Set Up Your Lab</h4>

</section>
<section id="step-by-step-code-walkthrough-98">
<h4 data-anchor-id="step-by-step-code-walkthrough-98">Step-by-Step Code Walkthrough</h4>
<ol type="1">
<li>Machine epsilon</li>
</ol>
<div id="0aad442b" data-execution_count="572">
<div id="cb964"><pre><code><span id="cb964-1">eps <span>=</span> np.finfo(<span>float</span>).eps</span>
<span id="cb964-2"><span>print</span>(<span>"Machine epsilon:"</span>, eps)</span></code></pre></div>
<div>
<pre><code>Machine epsilon: 2.220446049250313e-16</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>Round-off error demo</li>
</ol>
<div id="15e367d8" data-execution_count="573"><pre><code><span id="cb966-1">a <span>=</span> <span>1e16</span></span>
<span id="cb966-2">b <span>=</span> <span>1.0</span></span>
<span id="cb966-3"><span>print</span>(<span>"a + b - a:"</span>, (a <span>+</span> b) <span>-</span> a)  <span># may lose b due to precision limits</span></span></code></pre></div>
<ol start="3" type="1">
<li>Stability of matrix inversion</li>
</ol>
<div id="5bbb754b" data-execution_count="574">
<div id="cb968"><pre><code><span id="cb968-1">A <span>=</span> np.array([[<span>1</span>, <span>1.0001</span>], [<span>1.0001</span>, <span>1</span>]])</span>
<span id="cb968-2">b <span>=</span> np.array([<span>2</span>, <span>2.0001</span>])</span>
<span id="cb968-3"></span>
<span id="cb968-4">x_direct <span>=</span> np.linalg.solve(A, b)</span>
<span id="cb968-5">x_via_inv <span>=</span> np.linalg.inv(A) <span>@</span> b</span>
<span id="cb968-6"></span>
<span id="cb968-7"><span>print</span>(<span>"Solve:"</span>, x_direct)</span>
<span id="cb968-8"><span>print</span>(<span>"Inverse method:"</span>, x_via_inv)</span></code></pre></div>
<div>
<pre><code>Solve: [1.499975 0.499975]
Inverse method: [1.499975 0.499975]</code></pre>
</div>
</div>
<p>Notice: using <code>np.linalg.inv</code> can be less stable - better to solve directly.</p>
<ol start="4" type="1">
<li>Conditioning of a matrix</li>
</ol>
<div id="484c8a2c" data-execution_count="575">
<div id="cb970"><pre><code><span id="cb970-1">cond <span>=</span> np.linalg.cond(A)</span>
<span id="cb970-2"><span>print</span>(<span>"Condition number:"</span>, cond)</span></code></pre></div>
<div>
<pre><code>Condition number: 20001.00000000417</code></pre>
</div>
</div>
<ul>
<li>Large condition number → small input changes cause big output changes.</li>
</ul>
<ol start="5" type="1">
<li>BLAS/LAPACK under the hood</li>
</ol>
<div id="d05bdc3f" data-execution_count="576"><pre><code><span id="cb972-1">A <span>=</span> np.random.randn(<span>500</span>, <span>500</span>)</span>
<span id="cb972-2">B <span>=</span> np.random.randn(<span>500</span>, <span>500</span>)</span>
<span id="cb972-3"></span>
<span id="cb972-4"><span># Matrix multiplication (calls optimized BLAS under the hood)</span></span>
<span id="cb972-5">C <span>=</span> A <span>@</span> B</span></code></pre></div>
<p>This <code>@</code> operator is not a naive loop - it calls a highly optimized C/Fortran routine.</p>
</section>
<section id="try-it-yourself-97">
<h4 data-anchor-id="try-it-yourself-97">Try It Yourself</h4>
<ol type="1">
<li>Compare solving <code>Ax = b</code> with <code>np.linalg.solve</code> vs <code>np.linalg.inv(A) @ b</code> for larger, ill-conditioned systems.</li>
<li>Use <code>np.linalg.svd</code> on a nearly singular matrix. How stable are the singular values?</li>
<li>Check performance: time <code>A @ B</code> for sizes 100, 500, 1000.</li>
</ol>
</section>
<section id="the-takeaway-81">
<h4 data-anchor-id="the-takeaway-81">The Takeaway</h4>
<ul>
<li>Numerical linear algebra = math + floating-point reality.</li>
<li>Always prefer stable algorithms (<code>solve</code>, <code>qr</code>, <code>svd</code>) over naive inversion.</li>
<li>Libraries like BLAS/LAPACK make large computations fast, but understanding precision and conditioning prevents nasty surprises.</li>
</ul>
</section>
</section>
<section id="capstone-problem-sets-and-next-steps-a-roadmap-to-mastery">
<h3 data-anchor-id="capstone-problem-sets-and-next-steps-a-roadmap-to-mastery">100. Capstone Problem Sets and Next Steps (A Roadmap to Mastery)</h3>
<p>This final section ties everything together. Instead of introducing a new topic, it provides capstone labs that combine multiple ideas from the book. Working through them will give you confidence that you can apply linear algebra to real problems.</p>
<section id="problem-set-1---image-compression-with-svd">
<h4 data-anchor-id="problem-set-1---image-compression-with-svd">Problem Set 1 - Image Compression with SVD</h4>
<p>Take an image, treat it as a matrix, and approximate it with low-rank SVD.</p>
<div id="23b4a029" data-execution_count="577">
<div id="cb973"><pre><code><span id="cb973-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb973-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb973-3"><span>from</span> skimage <span>import</span> data, color</span>
<span id="cb973-4"></span>
<span id="cb973-5"><span># Load grayscale image</span></span>
<span id="cb973-6">img <span>=</span> color.rgb2gray(data.astronaut())</span>
<span id="cb973-7">U, S, Vt <span>=</span> np.linalg.svd(img, full_matrices<span>=</span><span>False</span>)</span>
<span id="cb973-8"></span>
<span id="cb973-9"><span># Approximate with rank-k</span></span>
<span id="cb973-10">k <span>=</span> <span>50</span></span>
<span id="cb973-11">img_approx <span>=</span> U[:, :k] <span>@</span> np.diag(S[:k]) <span>@</span> Vt[:k, :]</span>
<span id="cb973-12"></span>
<span id="cb973-13">plt.subplot(<span>1</span>,<span>2</span>,<span>1</span>)</span>
<span id="cb973-14">plt.imshow(img, cmap<span>=</span><span>"gray"</span>)</span>
<span id="cb973-15">plt.title(<span>"Original"</span>)</span>
<span id="cb973-16">plt.axis(<span>"off"</span>)</span>
<span id="cb973-17"></span>
<span id="cb973-18">plt.subplot(<span>1</span>,<span>2</span>,<span>2</span>)</span>
<span id="cb973-19">plt.imshow(img_approx, cmap<span>=</span><span>"gray"</span>)</span>
<span id="cb973-20">plt.title(<span>f"Rank-</span><span>{</span>k<span>}</span><span> Approximation"</span>)</span>
<span id="cb973-21">plt.axis(<span>"off"</span>)</span>
<span id="cb973-22"></span>
<span id="cb973-23">plt.show()</span></code></pre></div>
<div>
<figure>
<p><img src="https://little-book-of.github.io/linear-algebra/books/en-US/lab_files/figure-html/cell-578-output-1.png" width="540" height="276"></p>
</figure>
</div>
</div>
<p>Try different <span>\(k\)</span> values (5, 20, 100). How does quality vs.&nbsp;compression trade off?</p>
</section>
<section id="problem-set-2---predictive-modeling-with-pca-regression">
<h4 data-anchor-id="problem-set-2---predictive-modeling-with-pca-regression">Problem Set 2 - Predictive Modeling with PCA + Regression</h4>
<p>Combine PCA for dimensionality reduction with linear regression for prediction.</p>
<div id="77a0cb02" data-execution_count="578">
<div id="cb974"><pre><code><span id="cb974-1"><span>from</span> sklearn.datasets <span>import</span> load_diabetes</span>
<span id="cb974-2"><span>from</span> sklearn.model_selection <span>import</span> train_test_split</span>
<span id="cb974-3"><span>from</span> sklearn.linear_model <span>import</span> LinearRegression</span>
<span id="cb974-4"><span>from</span> sklearn.decomposition <span>import</span> PCA</span>
<span id="cb974-5"></span>
<span id="cb974-6"><span># Load dataset</span></span>
<span id="cb974-7">X, y <span>=</span> load_diabetes(return_X_y<span>=</span><span>True</span>)</span>
<span id="cb974-8">X_train, X_test, y_train, y_test <span>=</span> train_test_split(X, y, random_state<span>=</span><span>0</span>)</span>
<span id="cb974-9"></span>
<span id="cb974-10"><span># PCA reduce features</span></span>
<span id="cb974-11">pca <span>=</span> PCA(n_components<span>=</span><span>5</span>)</span>
<span id="cb974-12">X_train_pca <span>=</span> pca.fit_transform(X_train)</span>
<span id="cb974-13">X_test_pca <span>=</span> pca.transform(X_test)</span>
<span id="cb974-14"></span>
<span id="cb974-15"><span># Regression on reduced space</span></span>
<span id="cb974-16">model <span>=</span> LinearRegression().fit(X_train_pca, y_train)</span>
<span id="cb974-17"><span>print</span>(<span>"R^2 on test set:"</span>, model.score(X_test_pca, y_test))</span></code></pre></div>
<div>
<pre><code>R^2 on test set: 0.3691398497153573</code></pre>
</div>
</div>
<p>Does reducing dimensions improve or hurt accuracy?</p>
</section>

<section id="problem-set-4---solving-differential-equations-with-eigen-decomposition">
<h4 data-anchor-id="problem-set-4---solving-differential-equations-with-eigen-decomposition">Problem Set 4 - Solving Differential Equations with Eigen Decomposition</h4>
<p>Use eigenvalues/eigenvectors to solve a linear dynamical system.</p>
<div id="d2ae3bd6" data-execution_count="580">
<div id="cb977"><pre><code><span id="cb977-1">A <span>=</span> np.array([[<span>0</span>,<span>1</span>],[<span>-</span><span>2</span>,<span>-</span><span>3</span>]])</span>
<span id="cb977-2">eigvals, eigvecs <span>=</span> np.linalg.eig(A)</span>
<span id="cb977-3"></span>
<span id="cb977-4"><span>print</span>(<span>"Eigenvalues:"</span>, eigvals)</span>
<span id="cb977-5"><span>print</span>(<span>"Eigenvectors:</span><span>\n</span><span>"</span>, eigvecs)</span></code></pre></div>
<div>
<pre><code>Eigenvalues: [-1. -2.]
Eigenvectors:
 [[ 0.70710678 -0.4472136 ]
 [-0.70710678  0.89442719]]</code></pre>
</div>
</div>
<p>Predict long-term behavior: will the system decay, oscillate, or grow?</p>
</section>
<section id="problem-set-5---least-squares-for-overdetermined-systems">
<h4 data-anchor-id="problem-set-5---least-squares-for-overdetermined-systems">Problem Set 5 - Least Squares for Overdetermined Systems</h4>
<div id="7ed4e2da" data-execution_count="581">
<div id="cb979"><pre><code><span id="cb979-1">np.random.seed(<span>0</span>)</span>
<span id="cb979-2">X <span>=</span> np.random.randn(<span>100</span>, <span>3</span>)</span>
<span id="cb979-3">beta_true <span>=</span> np.array([<span>2</span>, <span>-</span><span>1</span>, <span>0.5</span>])</span>
<span id="cb979-4">y <span>=</span> X <span>@</span> beta_true <span>+</span> np.random.randn(<span>100</span>)<span>*</span><span>0.1</span></span>
<span id="cb979-5"></span>
<span id="cb979-6">beta_hat, <span>*</span>_ <span>=</span> np.linalg.lstsq(X, y, rcond<span>=</span><span>None</span>)</span>
<span id="cb979-7"><span>print</span>(<span>"Estimated coefficients:"</span>, beta_hat)</span></code></pre></div>
<div>
<pre><code>Estimated coefficients: [ 1.99371939 -1.00708947  0.50661857]</code></pre>
</div>
</div>
<p>Compare estimated vs.&nbsp;true coefficients. How close are they?</p>
</section>
<section id="try-it-yourself-98">
<h4 data-anchor-id="try-it-yourself-98">Try It Yourself</h4>
<ol type="1">
<li>Combine SVD and recommender systems - build a movie recommender with synthetic data.</li>
<li>Implement Gram–Schmidt by hand and test it against <code>np.linalg.qr</code>.</li>
<li>Write a mini “linear algebra toolkit” with your favorite helper functions.</li>
</ol>
</section>
<section id="the-takeaway-82">
<h4 data-anchor-id="the-takeaway-82">The Takeaway</h4>
<ul>
<li>You’ve practiced vectors, matrices, systems, eigenvalues, SVD, PCA, PageRank, and more.</li>
<li>Real problems often combine multiple concepts - the labs show how everything fits together.</li>
<li>Next steps: dive deeper into numerical linear algebra, explore machine learning applications, or study advanced matrix factorizations (Jordan form, tensor decompositions).</li>
</ul>
<p>This concludes the hands-on journey. By now, you don’t just know the theory - you can use linear algebra as a working tool in Python for data, science, and engineering.</p>


</section>
</section>
</section>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pop OS is getting beta (336 pts)]]></title>
            <link>https://system76.com/pop/pop-beta/</link>
            <guid>45384481</guid>
            <pubDate>Fri, 26 Sep 2025 09:20:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://system76.com/pop/pop-beta/">https://system76.com/pop/pop-beta/</a>, See on <a href="https://news.ycombinator.com/item?id=45384481">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-layout-id="6bc17e76-3469-4287-8921-4550dfa88066" data-content-region="page_builder_content" id="main-content" role="main" data-currency-code="USD">       

       <div data-widget-id="a264662f-cf34-4cbf-992d-72a78e2efeb6" data-placement-id="90d557ef-1109-413f-8873-6b3ae5c0b6e0" data-placement-status="ACTIVE" data-sub-layout="dc33d788-37ce-47b8-a265-0cbf5778f73d" data-sub-layout-container="cff3ccd7-27a9-4746-8d17-0b58801be577" data-layout-name="Layout">
<nav aria-label="Breadcrumb">
    <ol>
        <li>
            <a href="https://system76.com/">Home</a>
            <i></i>
        </li>
        <li>
            <a href="https://system76.com/pop">
                pop
            </a>
            <i></i>
        </li>
        <li>
                pop beta
            <i></i>
        </li>
    </ol>
</nav>

</div>

       <div data-widget-id="52f95cef-c0af-4f12-b825-386a4ed36fa0" data-placement-id="7a3347f3-0299-4db5-b99c-b2a8eb557068" data-placement-status="ACTIVE" data-sub-layout="74a45637-d62e-41c6-b003-62be72c1780d" data-sub-layout-container="acf8cb71-3917-46ba-9c3a-9733ba83f201" data-layout-name="Layout">
        <picture>
            <source media="(min-width: 1280px)" srcset="https://cdn11.bigcommerce.com/s-pywjnxrcr2/images/stencil/original/image-manager/pop-hero-beta-xl.jpg?t=1758317155" type="image/jpeg">
            <source media="(min-width: 920px)" srcset="https://cdn11.bigcommerce.com/s-pywjnxrcr2/images/stencil/original/image-manager/pop-hero-beta-l.jpg?t=1758578673" type="image/jpeg">
            <source media="(min-width: 730px)" srcset="https://cdn11.bigcommerce.com/s-pywjnxrcr2/images/stencil/original/image-manager/pop-hero-beta-xl.jpg?t=1758317155" type="image/jpeg">
            <source media="(min-width: 480px)" srcset="https://cdn11.bigcommerce.com/s-pywjnxrcr2/images/stencil/original/image-manager/pop-hero-betam.jpg?t=1758578940" type="image/jpeg">
            <source media="(max-width: 479px)" srcset="https://cdn11.bigcommerce.com/s-pywjnxrcr2/images/stencil/original/image-manager/pop-hero-beta-s.jpg?t=1758578956" type="image/jpeg">
            <img alt="Background" src="https://cdn11.bigcommerce.com/s-pywjnxrcr2/images/stencil/original/image-manager/pop-hero-beta-xl.jpg?t=1758317155">
        </picture>
        
    </div>

       <div data-widget-id="14237f5e-6e70-48b0-be16-b116b49ccc6e" data-placement-id="7a0c1966-1a0e-4562-b0b7-82e56c80e3da" data-placement-status="ACTIVE" data-sub-layout="6ad4ab1f-27bf-4f0d-96a2-df5a14165654" data-sub-layout-container="2d07d15d-b327-4b43-b651-3e2a79a41396" data-layout-name="Layout">
            <h2>Pop!_OS is Getting Beta</h2>
        <p id="markdown-area-14237f5e-6e70-48b0-be16-b116b49ccc6e">Pop!_OS 24.04 LTS with the new COSMIC DE, developed by System76, is coming with many new features to explore and discover. Test out the beta as we fine-tune for release.</p>

        
    </div>

       <div data-sub-layout="48969a82-1252-4012-a165-cdb0fd351d57" data-sub-layout-container="fd5706fc-d8c3-4d83-b727-4158050ba6e8" data-layout-name="Layout">
        
        <div data-widget-id="0f21ac44-11f1-4ec3-823b-cc79cf335835" data-placement-id="a535560b-e379-4c90-960b-6a68e7fb8e03" data-placement-status="ACTIVE">
        <p><img src="https://cdn11.bigcommerce.com/s-pywjnxrcr2/images/stencil/original/image-manager/pop-beta-panels.png?t=1758745926" alt="Alt Text">
        </p>
        <div>
                <h3>Upgrading from Pop!_OS 22.04 LTS to Pop!_OS 24.04 LTS Beta</h3>


            <p>- Read the release notes below before upgrading. This is a beta release and some bugs are expected.
- If you wish to upgrade, backup your current install and run **```pop-upgrade release upgrade -f```** from the terminal.

- Favorites in the Pop!_OS 22.04 LTS dock are not saved after upgrade. Adding apps to the COSMIC dock is called “Pin to app tray”. Pin to the app tray by right-click on them in the Dock or Application Library.

- PPA’s are disabled during upgrade as they can lead to errors. Re-enable PPAs after upgrading.</p>


        </div>
    </div>
        <div data-widget-id="e7e09db3-165b-4b1e-a34b-359cf1031708" data-placement-id="ed299a89-f2b6-4da1-b501-636e61d2d817" data-placement-status="ACTIVE">
        <p><img src="https://cdn11.bigcommerce.com/s-pywjnxrcr2/images/stencil/original/image-manager/servw14-composite-cos-ic.jpg?t=1758812879" alt="Alt Text">
        </p>
        <div>
                <h3>Pop!_OS 24.04 LTS Beta</h3>

            <p>
                <strong>Filesize:</strong> <span id="pop-filesize-e7e09db3-165b-4b1e-a34b-359cf1031708"></span> GB
            </p>

            <p>**Recommended:**
    4 GB RAM, 16 GB storage, 64-bit processor

**Classes:** Computers with Intel or AMD graphics or 10 series NVIDIA and older (e.g. GTX 1060 and previous)

Disable Secure Boot in your BIOS to install Pop!_OS.

Use the following  image checksum to verify the file once downloaded: </p>

            

            
        </div>
    </div>
        <div data-widget-id="25b35bac-3fbd-44a0-ac32-8c70104855ba" data-placement-id="321aff79-583d-47a6-af54-5ad251180802" data-placement-status="ACTIVE">
        <p><img src="https://cdn11.bigcommerce.com/s-pywjnxrcr2/images/stencil/original/image-manager/display-cosmic.png?t=1758812888" alt="Alt Text">
        </p>
        <div>
                <h3>Pop!_OS 24.04 LTS Beta with NVIDIA</h3>

            <p>
                <strong>Filesize:</strong> <span id="pop-filesize-25b35bac-3fbd-44a0-ac32-8c70104855ba"></span> GB
            </p>

            <p>**Recommended:**
    4 GB RAM, 16 GB storage, 64-bit processor

**Class:** Computers with 16 series NVIDIA Graphics or newer (e.g. GTX 16xx to RTX 5xxx)

Disable Secure Boot in your BIOS to install Pop!_OS.

Use the following  image checksum to verify the file once downloaded:</p>

            

            
        </div>
    </div>
        <div data-widget-id="db74b53c-e1fd-41ab-9e8c-a4d4447b8f74" data-placement-id="edeacad9-d9fd-45da-9a0c-3fdd47eca748" data-placement-status="ACTIVE">
            <h2>Release Notes</h2>
        <p id="markdown-area-db74b53c-e1fd-41ab-9e8c-a4d4447b8f74">- Pop!_OS 24.04 LTS Beta includes the new COSMIC Desktop Environment designed and developed by System76. COSMIC DE is largely feature complete for the first release and development focus has turned to bug fixes for the final release.

- This is a beta release and some bugs are expected.

- On occasion, the installer does not start in a virtual machine. Press Super to activate the Launcher and search for "Installer".

- Some GNOME apps are replaced by COSMIC apps
  - GNOME Files (Nautilus) &gt; COSMIC Files
  - GNOME Terminal &gt; COSMIC Terminal
  - GNOME Text Editor &gt; COSMIC Text Editor
  - GNOME Media Player (Totem) &gt; COSMIC Media Player

- Pop!_Shop is replaced by COSMIC Store

- Key components
  - COSMIC Epoch 1 Beta
  - Linux kernel 6.16.3
  - Mesa 25.1.5-1
  - NVIDIA Driver 580
  - libwayland/libwayland-client 1.23.1-3
  - libdrm 2.4.125-1

- Dragging and Dropping files from Wayland apps to X11 apps is not currently supported. For instance dragging files from COSMIC Files to Slack. Use the applications upload option as a work-around until the feature is added.
- On distributions other than Pop!_OS, Firefox may need a configuration flag set to match COSMIC theming

  - Go to **```about:config```** and set **```widget.gtk.libadwaita-colors.enabled```** to **```false```**
- Google Chrome based browsers

  - As of Google Chrome version 140, no configuration is necessary for Wayland
  - For versions prior to 140 and other Chrome based browsers that aren’t updated, setting the **```ozone-platform-hint```** is necessary. Go to chrome://flags in a tab, search for **```ozone-platform-hint```** and change the setting to “auto”. Restart the browser.

- Gaming is working well but we expect to need more fixes in our xwayland implementation for the Release Candidate.

  - Some games may start partially off screen. Press F11 or Super+F11 to full screen the game (Goat Simulator is one example)

- Display toggle hotkeys and an on-screen-display is not supported yet.

- The “Accent hint” around windows doesn’t match the roundness style setting in Appearance. This is expected for at least COSMIC apps for the Release Candidate.

- COSMIC has a built-in screenshot tool. If you require annotations, we recommend Flameshot which can be installed from Flathub via COSMIC Store. Version 13.1 or higher is required for COSMIC.

- COSMIC Store doesn’t currently display Flatpak suggested addons for apps. This is planned for the Release Candidate.

- Accessibility: The screen reader may not read all COSMIC apps widgets or may read them in an unintuitive direction. We’re working on screen reader flow and navigation for the Release Candidate.

- Some application indicators do not appear in the Notification Tray applet.

- Switching to an application using its application indicator does not currently work.

- Printing support in COSMIC Text Editor is planned for the release candidate

- Additional features and bugs expected to be fixed are triaged in the RC column on the 
[project board](https://github.com/orgs/pop-os/projects/23/views/1)
</p>

    </div>
    </div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Translating a Fortran F-16 Simulator to Unity3D (191 pts)]]></title>
            <link>https://vazgriz.com/762/f-16-flight-sim-in-unity-3d/</link>
            <guid>45383637</guid>
            <pubDate>Fri, 26 Sep 2025 07:06:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vazgriz.com/762/f-16-flight-sim-in-unity-3d/">https://vazgriz.com/762/f-16-flight-sim-in-unity-3d/</a>, See on <a href="https://news.ycombinator.com/item?id=45383637">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
			
<p>I recently purchased the textbook “Aircraft Control and Simulation” by Brian L. Stevens, Frank L. Lewis, and Eric N. Johnson<sup data-fn="043b6f3b-03c5-49af-a847-588dcf74793a"><a href="#043b6f3b-03c5-49af-a847-588dcf74793a" id="043b6f3b-03c5-49af-a847-588dcf74793a-link">1</a></sup>. This book covers the control and simulation of aircraft. It’s really dense and frankly hard to understand. As far as aerodynamics texts go, it’s pretty typical.</p>



<figure><img fetchpriority="high" decoding="async" width="638" height="1000" src="https://vazgriz.com/wp-content/uploads/2024/05/81j0kPR-upL._AC_UF10001000_QL80_.jpg" alt=""></figure>



<p>One interesting item in the appendices of the book is the source code for the simulation of an F-16. It has a flight model, based on scale model wind tunnel data. The flight model consists of a dozen lookup tables and the math equations to make it fly.</p>



<p>The only problem: it’s written entirely in Fortran.</p>



<figure><img decoding="async" width="1280" height="720" src="https://vazgriz.com/wp-content/uploads/2024/04/cover.jpg" alt="" srcset="https://vazgriz.com/wp-content/uploads/2024/04/cover.jpg 1280w, https://vazgriz.com/wp-content/uploads/2024/04/cover-1024x576.jpg 1024w, https://vazgriz.com/wp-content/uploads/2024/04/cover-768x432.jpg 768w" sizes="(max-width: 1280px) 100vw, 1280px"></figure>



<p>The source code is available on <a href="https://github.com/vazgriz/FlightSim_F16">Github</a>.</p>



<p>You can play the finished project right now on itch.io</p>







<p>Or watch the demo on Youtube:</p>



<figure><p>
<iframe loading="lazy" title="Fortran F-16 Simulator Demo" width="825" height="464" src="https://www.youtube.com/embed/2HZQnnxdISM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p></figure>



<p>While I am a professional software engineer and I have worked in the aerospace industry, that doesn’t mean that I understand what I’m doing.</p>


<div id="toc_container"><p>Table of Contents</p><ul><li><a href="#Introduction"><span>1</span> Introduction</a></li><li><a href="#Aerospace_Conventions"><span>2</span> Aerospace Conventions</a><ul><li><a href="#Coordinate_System"><span>2.1</span> Coordinate System</a></li><li><a href="#Units"><span>2.2</span> Units</a></li><li><a href="#Terminology"><span>2.3</span> Terminology</a></li></ul></li><li><a href="#Air_Data"><span>3</span> Air Data</a></li><li><a href="#Table_Interpolation"><span>4</span> Table Interpolation</a><ul><li><a href="#1D_Lookup_Table"><span>4.1</span> 1D Lookup Table</a></li><li><a href="#2D_Lookup_Table"><span>4.2</span> 2D Lookup Table</a></li></ul></li><li><a href="#Engine"><span>5</span> Engine</a><ul><li><a href="#Power"><span>5.1</span> Power</a></li><li><a href="#Thrust"><span>5.2</span> Thrust</a></li></ul></li><li><a href="#Forces"><span>6</span> Forces</a><ul><li><a href="#Lift_force_vs_Normal_force"><span>6.1</span> Lift force vs Normal force</a></li></ul></li><li><a href="#Moments"><span>7</span> Moments</a><ul><li><a href="#Damping"><span>7.1</span> Damping</a></li></ul></li><li><a href="#Complete_Flight_Model"><span>8</span> Complete Flight Model</a></li><li><a href="#Stability"><span>9</span> Stability</a></li><li><a href="#Flight_Control_System"><span>10</span> Flight Control System</a><ul><li><a href="#PID_Controllers"><span>10.1</span> PID Controllers</a></li><li><a href="#G_and_AOA_Limiter"><span>10.2</span> G and AOA Limiter</a></li><li><a href="#Stick_Pusher"><span>10.3</span> Stick Pusher</a></li></ul></li><li><a href="#Testing"><span>11</span> Testing</a><ul><li><a href="#Unit_Testing"><span>11.1</span> Unit Testing</a></li><li><a href="#Flight_Testing"><span>11.2</span> Flight Testing</a></li></ul></li><li><a href="#Limitations"><span>12</span> Limitations</a></li><li><a href="#Conclusion"><span>13</span> Conclusion</a></li></ul></div>




<h2><span id="Introduction">Introduction</span></h2>



<p>In previous posts on this blog<sup data-fn="4bf1b8e1-c40d-43b7-b010-d8b1043a2423"><a href="#4bf1b8e1-c40d-43b7-b010-d8b1043a2423" id="4bf1b8e1-c40d-43b7-b010-d8b1043a2423-link">2</a></sup> <sup data-fn="b896ed6f-b50c-4b43-9d20-113b9f6b1497"><a href="#b896ed6f-b50c-4b43-9d20-113b9f6b1497" id="b896ed6f-b50c-4b43-9d20-113b9f6b1497-link">3</a></sup> <sup data-fn="f8f5c94c-2a47-40ee-9825-f1e6583752ae"><a href="#f8f5c94c-2a47-40ee-9825-f1e6583752ae" id="f8f5c94c-2a47-40ee-9825-f1e6583752ae-link">4</a></sup>, I covered the development of a flight simulator based on the lift equation and hand-tuned parameters. This gives the game designer direct control over a lot of flight parameters. For example, you can directly choose the turn rate and the G-limit of the aircraft, allowing the designer to easily tune the corner speed. This works well for game development, since the designer, and ultimately the player, care more about these high-level parameters.</p>



<p>But real aircraft are designed from the other direction, starting from low-level parameters such as the size, shape, and position of airfoils. Engineers tune every aspect of the aircraft in order to reach those high-level behaviors. But every design decision has trade offs and reaching the goal for one parameter means compromising another. An airliner is designed very differently from a fighter jet because of this.</p>



<p>Simulating all of the low-level parameters is difficult. It’s possible to simulate air flow over the vehicle using <em>computational fluid dynamics</em> (CFD), but this kind of software is difficult to write and even more difficult to verify.</p>



<p>The F-16 flight model from the textbook does not simulate the low-level parameters, but it also doesn’t simulate the high-level parameters either. It sits somewhere in between, so it serves as a useful stepping stone from my previous projects. This project will explore more advanced flight dynamics and explain the limitations of the old flight model as well as the new one.</p>



<h2><span id="Aerospace_Conventions">Aerospace Conventions</span></h2>



<h2><span id="Coordinate_System">Coordinate System</span></h2>



<p>Before we can write any code, we need to understand the conventions used for mathematically modeling aircraft that are used in the aerospace industry. The textbook uses aerospace conventions and to use them in this project, we must convert them to Unity conventions.</p>



<p>The first convention is the coordinate system axes. If you’ve ever visited a graphics programming forum, you might have seen people arguing over how the X, Y, and Z axes should be arranged in their game. Especially whether to use a right handed or left handed, and Y-up or Z-up coordinate system.</p>



<p>This chart by Freya Holmer<sup data-fn="149b4841-9ef4-469f-8a98-22bc8a941e7f"><a href="#149b4841-9ef4-469f-8a98-22bc8a941e7f" id="149b4841-9ef4-469f-8a98-22bc8a941e7f-link">5</a></sup> shows the axis choices made by a variety of 3D software tools.</p>



<figure><img loading="lazy" decoding="async" width="2048" height="2048" src="https://vazgriz.com/wp-content/uploads/2025/05/EmVSW5AW8AAoDD9-2048x2048.jpg" alt="" srcset="https://vazgriz.com/wp-content/uploads/2025/05/EmVSW5AW8AAoDD9-2048x2048.jpg 2048w, https://vazgriz.com/wp-content/uploads/2025/05/EmVSW5AW8AAoDD9-1024x1024.jpg 1024w, https://vazgriz.com/wp-content/uploads/2025/05/EmVSW5AW8AAoDD9-150x150.jpg 150w, https://vazgriz.com/wp-content/uploads/2025/05/EmVSW5AW8AAoDD9-768x768.jpg 768w, https://vazgriz.com/wp-content/uploads/2025/05/EmVSW5AW8AAoDD9-1536x1536.jpg 1536w" sizes="(max-width: 2048px) 100vw, 2048px"><figcaption>Mathematically speaking, these are all equally valid. But we all know that Unreal made the worst possible choice</figcaption></figure>



<p>The aerospace industry takes a different path. The most common coordinate system for aircraft is right handed, X forward, Y right, and Z down. This is completely different from every tool shown above. The textbook defines all of it’s math using this convention.</p>



<figure><img loading="lazy" decoding="async" width="1920" height="1080" src="https://vazgriz.com/wp-content/uploads/2025/06/Image_006.jpg" alt="" srcset="https://vazgriz.com/wp-content/uploads/2025/06/Image_006.jpg 1920w, https://vazgriz.com/wp-content/uploads/2025/06/Image_006-1024x576.jpg 1024w, https://vazgriz.com/wp-content/uploads/2025/06/Image_006-768x432.jpg 768w, https://vazgriz.com/wp-content/uploads/2025/06/Image_006-1536x864.jpg 1536w" sizes="(max-width: 1920px) 100vw, 1920px"></figure>



<p>Luckily, translating between two coordinate systems is easy. You just swap the components around and then add or remove minus signs until it all works. Every calculation made by the textbook’s code can be easily translated into Unity’s coordinate system and vice versa.</p>



<p>Writing functions to do this is simple:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">public static Vector3 ConvertVectorToAerospace(Vector3 vector) {
    return new Vector3(vector.z, vector.x, -vector.y);
}

public static Vector3 ConvertVectorToUnity(Vector3 vector) {
    return new Vector3(vector.y, -vector.z, vector.x);
}</pre>



<p>When translating euler angles, torque, or other angular values, one additional negation is needed:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">public static Vector3 ConvertAngleToAerospace(Vector3 angle) {
    // negate when switching handedness
    return -ConvertVectorToAerospace(angle);
}

public static Vector3 ConvertAngleToUnity(Vector3 angle) {
    // negate when switching handedness
    return -ConvertVectorToUnity(angle);
}</pre>



<h2><span id="Units">Units</span></h2>



<p>For completely inscrutable reasons, American aerospace texts (and the industry!) insist on using US customary units for everything. All math is defined with these units. Distance is measured in feet. Mass is measured in slugs.</p>



<p>What the hell is a slug? A slug is the unit of mass in the US system. This is the equivalent unit of the kilogram in the metric system. Remember that weight and mass are not the same thing.</p><p>



\(1 \, \text{kg} * 9.81 \, \text{m/s}^2 = 9.81 \, \text{N}\)



</p><p>



\(1 \, \text{slug} * 32.17 \, \text{ft/s}^2 = 32.17 \, \text{lb}\)



</p><p><em>Mass</em> is the measure of how much an object resists linear force. <em>Moment of inertia</em> is how much the object resists rotational force or <em>torque</em>. The unit for moment of inertia in metric is kg-m<sup>2</sup>. Thus, the equivalent unit in customary is slug-ft<sup>2</sup>.</p>



<p>You want to measure how much air mass is in a given volume? That’s gonna be slugs/ft<sup>3</sup>.</p>



<figure><img loading="lazy" decoding="async" width="459" height="183" src="https://vazgriz.com/wp-content/uploads/2025/05/Slug_parts.png" alt=""><figcaption>A slug’s foot</figcaption></figure>



<p>Speed is mostly measured in feet per second, unless you want to know the speed of the aircraft. Then you use <em>knots</em>, which means nautical miles per hour. Importantly, a nautical mile is not the same as a regular mile. A regular mile is 5,280 feet. A nautical mile is ~6,076 feet or exactly 1,852 meters (???).</p>



<p>Do you want to know how fast your ship is sailing? Just throw out this piece of wood tied to a spool of rope. The rope has <em>knots</em> tied at regular intervals. Count the number of knots that unspool in a given time frame. That’s how many <em>knots</em> your ship is making.</p>



<figure><img loading="lazy" decoding="async" width="2048" height="1023" src="https://vazgriz.com/wp-content/uploads/2025/05/Speyer_Handlog-2048x1023.jpg" alt="" srcset="https://vazgriz.com/wp-content/uploads/2025/05/Speyer_Handlog-2048x1023.jpg 2048w, https://vazgriz.com/wp-content/uploads/2025/05/Speyer_Handlog-1024x511.jpg 1024w, https://vazgriz.com/wp-content/uploads/2025/05/Speyer_Handlog-768x384.jpg 768w, https://vazgriz.com/wp-content/uploads/2025/05/Speyer_Handlog-1536x767.jpg 1536w" sizes="(max-width: 2048px) 100vw, 2048px"><figcaption>A captain’s log<sup data-fn="dbd8aeb2-20e9-45d2-8b13-f967c36c6393"><a href="#dbd8aeb2-20e9-45d2-8b13-f967c36c6393" id="dbd8aeb2-20e9-45d2-8b13-f967c36c6393-link">6</a></sup></figcaption></figure>



<p>Finally, temperature is measured in degrees Rankine. You know how the Kelvin scale is just the Celsius scale adjusted so that 0 Kelvin equals absolute zero? Well Rankine is the same concept applied to Fahrenheit.</p><p>



\(0 \, \text{R} = \text{absolute zero} \\ 534 \, \text{R} = 75 \, \text{F} = \text{room temperature}\)



</p><p>How the fuck did we ever build the SR-71? 💀</p>



<p>Unity by convention uses metric for all physics units. The flight model in the textbook uses US customary units, so every input and output of this system has to be converted. So we have to add functions to handle converting to and from US customary units. This is easy enough since conversion is just a multiplication or division operation.</p>



<h2><span id="Terminology">Terminology</span></h2>



<p>There are several terms used in aerospace that I need to define. I have used equivalent terms in the previous project, but I will clarify them here.</p>



<p><em>Alpha</em> (α) refers to the angle of attack.</p>



<p><em>Beta</em> (β) refers to the angle of side slip.</p>



<p><em>Longitudinal axis</em> is the X-axis, from tail to nose.</p>



<p><em>Normal axis</em> is the Z- axis, the vertical axis pointing downwards.</p>



<p><em>Lateral axis</em> is the Y-axis, or side axis, pointing right.</p>



<p><em>Phi</em> (φ) is the aircraft’s roll around the X axis.</p>



<p><em>Theta</em> (θ) is the aircraft’s pitch around the Y axis.</p>



<p><em>Psi</em> (ψ) is the aircraft’s yaw around the Z axis.</p>



<p><em>P</em>, <em>Q</em>, and <em>R</em> refer to the angular velocity around the X, Y, and Z axes respectively.</p>



<p>In general you will find that aerodynamics texts are allergic to good variable names. I suspect this is a form of gatekeeping. Or perhaps the authors have to pay by the letter to publish.</p>



<h2><span id="Air_Data">Air Data</span></h2>



<p>Airplanes need air to fly [citation needed]. Every behavior of a plane is determined by the movement of air. Therefore it is critically important, for real and simulated planes, to be able to measure the air flowing around it.</p>



<p>Real planes need to measure static and dynamic air pressure to determine how fast the plane is moving. Static pressure is measured by a static pressure port. It’s the pressure that you would measure if you just lifted a pressure meter to the same altitude as the plane. Static pressure decreases with altitude.</p>



<p>Dynamic pressure measures the pressure added by the plane’s forward motion. This requires a pitot tube to measure. As the plane moves forward it rams air into the pitot tube and increases the pressure above the static pressure. The pitot measures the total pressure of the air. By subtracting the static pressure, we can obtain the dynamic pressure.</p>



<p>The static and dynamic pressures can then be used to calculate many of the variables the pilot needs to fly. Most important are the airspeed and altitude of the aircraft. Specifically, these values can be used to calculate the indicated airspeed of the aircraft. Indicated airspeed is calculated directly from the dynamic pressure.</p>



<p>At most subsonic speeds, the dynamic pressure of air flowing over the wings is the most important variable in flight. A plane’s performance can be defined in terms of indicated airspeed. For example, a plane may have a stall speed of 100 knots indicated airspeed. This means that no matter what altitude the plane is at, the indicated airspeed will be 100 when the plane stalls.</p>



<p>This is important since it gives a consistent number for the stall speed regardless of atmospheric conditions. The pressure and density of the air can vary based on weather, temperature, and other factors. So the true airspeed when a stall occurs can be very inconsistent. But as long as the pilot knows the indicated airspeed, they know how their plane will behave.</p>



<p>For this simulator, the calculation has to work backwards. We know the true airspeed and altitude of the plane from the velocity and position of the rigidbody. From that, we can calculate the dynamic pressure. This dynamic pressure is then used for later calculations in the flight model. Additionally, the plane’s speed in mach is calculated here as well.</p>



<p>The original Fortran source code is given:</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">  	SUBROUTINE ADC(VT,ALT,AMACH,QBAR)
  	DATA R0/2.377E-3/
  	TFAC = 1.0 - 0.703E-5 * ALT
  	T = 519.0 * TFAC
  	IF (ALT .GE. 35000.0) T= 390.0
  	RHO = R0 * (TFAC**4.14)
  	AMACH= VT/SQRT(1.4*1716.3*T)
  	QBAR = 0.5*RHO*VT*VT
C 	PS = 1715.0 * RHO * T
  	RETURN
  	END</pre>



<p>It turns out, Fortran is actually pretty good at translating formulas. So this code is not as difficult to read as I expected.</p>



<p>To translate this to Unity, we create a class AirDataComputer to perform these calculations. The output of the calculation is the AirData struct.</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">public struct AirData {
	public float altitudeMach;
	public float qBar;
}

public class AirDataComputer {
    /// &lt;summary&gt;
    /// Density in slugs/ft^3
    /// &lt;/summary&gt;
    public const float SeaLevelDensity = 2.377e-3f;
    public const float MaxAltitude = 35000.0f;

    /// &lt;summary&gt;
    /// Calculates air data based on velocity and altitude
    /// &lt;/summary&gt;
    /// &lt;param name="velocity"&gt;Velocity in ft/s&lt;/param&gt;
    /// &lt;param name="altitude"&gt;Altitude in ft&lt;/param&gt;
    /// &lt;returns&gt;Air data&lt;/returns&gt;
    public AirData CalculateAirData(float velocity, float altitude) {
        ...
    }
}</pre>



<p>Here we can see where the US customary units are used. The density of air at sea level is defined in slugs/ft<sup>3</sup>. The altitude is defined in feet. Theoretically, these values could be defined using metric. But the implementation of the function depends on even more values defined in customary.</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">const float baseTemperature = 519.0f;    	// sea level temp in R
const float minTemperature = 390.0f;     	// minimum temp in R
const float temperatureGradient = 0.703e-5f; // gradient in R / ft

altitude = Mathf.Clamp(altitude, 0, MaxAltitude);

// calculate temperature in Rankine
float temperatureFactor = 1.0f - (temperatureGradient * altitude);
float T = Mathf.Max(minTemperature, baseTemperature * temperatureFactor);</pre>



<p>These calculations simulate the change in atmospheric conditions at different altitudes. Particularly important is how the temperature drops at higher altitudes. The temperature gradient approximates the decreases in temperature (in Rankine) as altitude increases.</p>



<p>This flight model supports altitudes up to 35,000 ft. Altitudes above this are not supported. At any altitude above this, the plane will behave as if it were at 35,000 ft. This is because the temperatures at this altitude no longer consistently decrease, as it does in the lower atmosphere. A more advanced atmosphere model would need to be used.</p>



<p>Temperature factor does not drop below about 0.75 in this range, so the resulting temperature T does not fall below 390 R.</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">const float gamma = 1.4f; // ratio of specific heats
const float gasConstant = 1716.3f;
float speedOfSound = Mathf.Sqrt(gamma * gasConstant * T);
float altitudeMach = velocity / speedOfSound;</pre>



<p>Now we can calculate the speed of sound at the plane’s current altitude and use it to find the plane’s Mach number. The speed of sound varies with density, which varies with temperature. The speed of sound is equal to the square root of the ratio of specific heat, called gamma, times the gas constant, gasConstant, times the absolute temperature, T.<sup data-fn="09917b2e-68b9-4e51-880e-4cfe4f9dfc5a"><a href="#09917b2e-68b9-4e51-880e-4cfe4f9dfc5a" id="09917b2e-68b9-4e51-880e-4cfe4f9dfc5a-link">7</a></sup></p>



<p>Once the speed of sound is known, calculating the Mach number is just a simple division.</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">const float densityPower = 4.14f;

float rho = SeaLevelDensity * Mathf.Pow(temperatureFactor, densityPower);
float qBar = 0.5f * rho * velocity * velocity;</pre>



<p>And finally the dynamic pressure is calculated from the temperature factor. I’ll admit, I don’t understand why exactly the formula is designed this way. It seems to calculate a density factor, called rho, based solely on the temperature factor, raised to an arbitrary value, densityPower.</p>



<p>The NASA reference provides a similar formula using metric units and using a different arbitrary power. I guess this value is just what results from using customary🤷‍♂️</p>



<p>In any case, this gives us the two air data values we need for the rest of the simulation, dynamic pressure and mach number.</p>



<h2><span id="Table_Interpolation">Table Interpolation</span></h2>



<p>Throughout this flight model, various forms of table lookups are used to determine the aircraft’s behavior. Lookup tables are commonly used in flight simulators to represent complex curves and functions. In fact, Unity’s AnimationCurve class in the previous project is used to define a few lookup tables, such as lift coefficient.</p>



<figure><img loading="lazy" decoding="async" width="449" height="500" src="https://vazgriz.com/wp-content/uploads/2025/05/aoa_curve.png" alt=""></figure>



<p>This animation curve serves as a 1 dimensional lookup table. The input dimension is AOA and the output value is lift coefficient.</p>



<p>Fortran code doesn’t have the luxury of using AnimationCurves, but a simple table of values with an interpolation function is almost as powerful.</p>



<h2><span id="1D_Lookup_Table">1D Lookup Table</span></h2>



<p>The interpolation functions provided by the textbook look something like this:</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">FUNCTION LOOKUP(ALPHA, RESULT)
    REAL A(-2:9)
C
    DATA A /  .770,.241,-.100,-.416,-.731,-1.053,
    &amp;     	-1.366,-1.646,-1.917,-2.120,-2.248,-2.229 /
C
    S = 0.2*ALPHA
    K = INT(S)
    IF(K.LE.-2) K=-1
    IF(K.GE.9) K=8
    DA = S - FLOAT(K)
    L = K + INT(SIGN(1.1,DA))
    RESULT = A(K) + ABS(DA)*(A(L)-A(K))
    RETURN
END</pre>



<p>This function takes alpha (AOA) and uses it to lookup a value from the table. Alpha is a float that can have any value from [-10, 45]. The table “A” represents values for every 5 degree increment of alpha. Note that Fortran supports arrays with an arbitrary starting index, in this case -2. So this table supports indices in the range [-2, 9].</p>



<figure><img loading="lazy" decoding="async" width="500" height="256" src="https://vazgriz.com/wp-content/uploads/2025/05/interpolation_index.png" alt=""></figure>



<p>This first step is multiplying alpha by a scaling value to create a float S, which maps alpha to the range [-2, 9]. An integer index K is created from S and then clamped to values <em>one less</em> than the table’s index range. The value DA is calculated as the difference between S and K.</p>



<figure><img loading="lazy" decoding="async" width="500" height="256" src="https://vazgriz.com/wp-content/uploads/2025/05/interpolation_scaling.png" alt=""></figure>



<p>The value L is calculated to be one index away from K, in the same direction as S. So now we have two indices to the table, K and L, which we use to read two values from the table, A(K) and A(L). DA is then used to blend between these table values and produce the final result.</p>



<figure><img loading="lazy" decoding="async" width="500" height="256" src="https://vazgriz.com/wp-content/uploads/2025/05/interpolation_scaling2.png" alt=""></figure>



<p>This has two effects. The first is the simplest to understand. If alpha falls within the input range of the table, L and K are selected as the closest table values. For example, if alpha is 12, the two indices would be 2 and 3. The difference between S and K would be less than 1. The values A(2) and A(3) can be read from the table and then interpolated based on the value of DA. This is a fairly normal interpolation calculation.</p>



<p>The other effect is what happens when alpha is outside of the input range of the table. K is guaranteed to not be the first or last index, and L is allowed to be one index off of K. L and K are still valid indices, but the value of DA may be larger than 1. This means when we interpolate between A(L) and A(K), we can extrapolate values for inputs beyond the range of the table.</p>



<figure><img loading="lazy" decoding="async" width="550" height="256" src="https://vazgriz.com/wp-content/uploads/2025/05/interpolation_scaling3.png" alt=""></figure>



<p>This means our lookup table can handle values outside of it’s input range. But there is still a limitation. As the input value gets further away from the input range, the extrapolated values will become more and more unrealistic. This allows our plane to fly slightly outside the flight envelope of the lookup tables.</p>



<p>I translated this function into C# like this:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">public static float ReadTable(float[] table, int i, int start) {
    return table[i - start];
}

public static (int k0, int k1, float t) GetLookUpIndex(float value, float scale, int min, int max) {
    float scaled = value * scale;
    int K0 = Mathf.Clamp((int)scaled, min, max);
    float T = scaled - K0;
    int K1 = K0 + (int)Mathf.Sign(T);

    return (K0, K1, T);
}

public static float LinearLookup(float value, float scale, float[] table, int min, int max) {
    (int k0, int k1, float kT) = GetLookUpIndex(value, scale, min + 1, max - 1);
    float T = ReadTable(table, k0, min);
    float U = ReadTable(table, k1, min);
    float result = T + Math.Abs(kT) * (U - T);
    return result;
}</pre>



<p>GetLookUpIndex calculates K, L, and DA. These variables are renamed to k0, k1, and kT respectively.</p>



<p>ReadTable is a function that maps array indices to a new range, to support arbitrary starting indices like Fortran. (C# surprisingly supports this feature natively, but who actually uses that?)</p>



<p>LinearLookup reads the k0 and k1 values from the array and performs the interpolation. This allows us to calculate values for any input to the lookup table.</p>



<p>Note that the expression “T + Math.Abs(kT) * (U – T)” is effectively equivalent to Mathf.LerpUnclamped.</p>



<h2><span id="2D_Lookup_Table">2D Lookup Table</span></h2>



<p>All of the above code is needed to perform a one dimensional table lookup. Performing this kind of table lookup with two input dimensions is called a <em>bilinear interpolation</em>. Extending this to two dimensions is not that much more complicated.</p>



<p>The two input values to the table form a two dimensional space. Our input values form a two dimensional point. Instead of selecting two array indices K and L, we need to select four array indices. These four indices form a box around our input point. We simply perform 2 one dimensional lookups, and then interpolate between them to produce the final value.</p>



<figure><img loading="lazy" decoding="async" width="1280" height="1209" src="https://vazgriz.com/wp-content/uploads/2025/05/bilinear_interpolation.png" alt="" srcset="https://vazgriz.com/wp-content/uploads/2025/05/bilinear_interpolation.png 1280w, https://vazgriz.com/wp-content/uploads/2025/05/bilinear_interpolation-1024x967.png 1024w, https://vazgriz.com/wp-content/uploads/2025/05/bilinear_interpolation-768x725.png 768w" sizes="(max-width: 1280px) 100vw, 1280px"><figcaption>The points marked with Q are the entries from the table. The point P is our supplied point. Since the point is closer to Q<sub>12</sub> than to Q<sub>11</sub>, the output has more influence from Q<sub>12</sub>. (From wikipedia<sup data-fn="af27d837-c397-472b-ab56-f366283b8cab"><a href="#af27d837-c397-472b-ab56-f366283b8cab" id="af27d837-c397-472b-ab56-f366283b8cab-link">8</a></sup>)</figcaption></figure>



<p>The 2 one dimensional lookups are marked in red. The final interpolation is marked in blue.</p>



<p>Implementing this in C# is a simple extension of the LinearLookup function:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">public static float BilinearLookup(float xValue, float xScale, float yValue, float yScale, float[,] table, int xMin, int xMax, int yMin, int yMax) {
    (int x0, int x1, float xT) = GetLookUpIndex(xValue, xScale, xMin + 1, xMax - 1);
    (int y0, int y1, float yT) = GetLookUpIndex(yValue, yScale, yMin + 1, yMax - 1);
    float T = ReadTable(table, x0, y0, xMin, yMin);
    float U = ReadTable(table, x0, y1, xMin, yMin);
    float V = T + Math.Abs(xT) * (ReadTable(table, x1, y0, xMin, yMin) - T);
    float W = U + Math.Abs(xT) * (ReadTable(table, x1, y1, xMin, yMin) - U);

    float result = V + (W - V) * Math.Abs(yT);
    return result;
}</pre>



<p>A bilinear interpolation is a very common operation in computer graphics. This is how textures are sampled when placed on 3D geometry.</p>



<p>In the next section, we will see that the engine thrust calculation interpolates between the output of 2 two dimensional tables. Adding this third interpolation means this calculation is now a <em>trilinear interpolation</em>. Interpolating between two tables is how mipmaps are blended together in computer graphics. How neat is that?</p>



<figure><img loading="lazy" decoding="async" width="1600" height="1412" src="https://vazgriz.com/wp-content/uploads/2025/05/trilinear_interpolation.png" alt="" srcset="https://vazgriz.com/wp-content/uploads/2025/05/trilinear_interpolation.png 1600w, https://vazgriz.com/wp-content/uploads/2025/05/trilinear_interpolation-1024x904.png 1024w, https://vazgriz.com/wp-content/uploads/2025/05/trilinear_interpolation-768x678.png 768w, https://vazgriz.com/wp-content/uploads/2025/05/trilinear_interpolation-1536x1356.png 1536w" sizes="(max-width: 1600px) 100vw, 1600px"><figcaption>Trilinear Interpolation😵‍ (From Wikipedia<sup data-fn="6e07b8fb-275e-41db-a382-9fe2007f962f"><a href="#6e07b8fb-275e-41db-a382-9fe2007f962f" id="6e07b8fb-275e-41db-a382-9fe2007f962f-link">9</a></sup>)</figcaption></figure>



<h2><span id="Engine">Engine</span></h2>



<p>The next system we’re going to add is the engine. In my previous project, the engine was dead simple. The player selected a throttle value from [0, 1], which is multiplied by the plane’s total thrust. This works fine for that simulation and even gives us the ability to reduce thrust to zero, so the plane becomes a glider.</p>



<p>However, it is not a realistic simulation of how a jet engine works. In reality, a jet engine still produces some thrust at idle throttle. And there are more factors that affect thrust output than just the throttle setting.</p>



<p>The thrust output of a jet engine decreases with altitude and increases with speed. As altitude increases, the air gets thinner and the jet engine becomes weaker. But as speed increases, dynamic pressure, and thus pressure in the engine, increases and the engine becomes stronger. These two effects need to be considered at the same time to find the thrust output at any given moment.</p>



<figure><img loading="lazy" decoding="async" width="2048" height="1177" src="https://vazgriz.com/wp-content/uploads/2025/05/engine_thrust-2048x1177.png" alt="" srcset="https://vazgriz.com/wp-content/uploads/2025/05/engine_thrust-2048x1177.png 2048w, https://vazgriz.com/wp-content/uploads/2025/05/engine_thrust-1024x588.png 1024w, https://vazgriz.com/wp-content/uploads/2025/05/engine_thrust-768x441.png 768w, https://vazgriz.com/wp-content/uploads/2025/05/engine_thrust-1536x883.png 1536w" sizes="(max-width: 2048px) 100vw, 2048px"><figcaption>A 3D surface chart of the engine thrust. The horizontal axes are altitude and mach. The vertical axis is engine thrust in lbf</figcaption></figure>



<p>Additionally, we have to consider how jet engines behave in terms of RPM. Just like piston engines (like in a typical car), jet engines have rotating components whose speed increases with throttle. The max RPM of a jet is much higher than a piston engine, however the range of possible RPM is smaller.</p>



<p>The engine in an F-16 has a maximum RPM of about 14,000. This is at the maximum non-afterburner power, called <em>military power</em>. When throttle is reduced to the lowest setting, <em>idle</em>, the RPM falls to about 8,400 RPM or about 60% of the max. Planes of course do not have a transmission like a car does, so this range of RPM also covers the range of thrust needed at all stages of flight.</p>



<p>At idle throttle, the engine runs at 60% max RPM, but only produces 8% of max thrust. At military power, the engine runs at 100% RPM and produces 100% thrust.</p>



<p>Military power is selected when the pilot moves the throttle lever to 77% of it’s max setting. Pushing the throttle beyond that engages the afterburner and produces even more thrust.&nbsp; Setting the throttle lever to 100% is called <em>max power</em>. Max power provides about 57% more thrust than military power. Engine RPM does not increase when using afterburner.</p>



<p>A significant difference between a piston engine and a jet engine is how fast the engine can change RPM. In a car, you can put the transmission in neutral and rev the engine up and down very quickly. But a jet engine is much slower to respond to changes in throttle, regardless of how fast the pilot moves the throttle lever. Generally, it can take several seconds to go from idle to military power or vice versa.</p>



<p>The reasons why jet engines are slower to change RPM are complicated. The change in throttle is managed by a computer to avoid <em>compressor stall</em>, which can cause damage or shut down of the engine. This computer will change engine parameters slowly to avoid compressor stall or any other problems that might be caused by moving the throttle too quickly.</p>



<h2><span id="Power">Power</span></h2>



<p>The behavior of the jet engine is included in the textbook’s flight model. RPM is not explicitly modeled, but is abstracted as power. The pilot chooses a commanded power level and the engine’s current power setting will move towards this over time. This behavior is spring-like, thus a larger difference will cause the current power setting to change faster. It takes about 2 seconds to increase from idle to military power in this flight model.</p>



<p>The first step is to translate the player’s throttle setting into engine power. This is a fairly simple function that maps military power, or 77% throttle, to 50% power. Full afterburner, or 100%, is mapped to 100% power. This is called the “throttle gearing”, but don’t confuse that with a car’s gearing. It’s much simpler.</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">FUNCTION TGEAR(THTL) ! Power command v. thtl. relationship
    IF(THTL.LE.0.77) THEN
        TGEAR = 64.94*THTL
    ELSE
        TGEAR = 217.38*THTL-117.38
    END IF
    RETURN
END</pre>



<p>In C#, this is translated as:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">public static float CalculateThrottleGear(float throttle) {
    	// maps throttle 0 - 0.77   to power 0% - 50%
    	// maps throttle 0.77 - 1.0 to power 50% - 100%

    	float power;

    	if (throttle &lt;= militaryPowerThrottle) {
        	power = 64.94f * throttle;
    	} else {
        	power = 217.38f * throttle - 117.38f;
    	}

    	return power;
}</pre>



<p>Those constants might seem weird, but they just define two lines with different slopes. The two lines intersect when the throttle is 0.77.</p>



<figure><img loading="lazy" decoding="async" width="2048" height="1146" src="https://vazgriz.com/wp-content/uploads/2025/05/throttle_gear_chart-2048x1146.png" alt="" srcset="https://vazgriz.com/wp-content/uploads/2025/05/throttle_gear_chart-2048x1146.png 2048w, https://vazgriz.com/wp-content/uploads/2025/05/throttle_gear_chart-1024x573.png 1024w, https://vazgriz.com/wp-content/uploads/2025/05/throttle_gear_chart-768x430.png 768w, https://vazgriz.com/wp-content/uploads/2025/05/throttle_gear_chart-1536x860.png 1536w" sizes="(max-width: 2048px) 100vw, 2048px"><figcaption>The output of CalculateThrottleGear</figcaption></figure>



<p>The player’s throttle setting is used to calculate the commanded power level. The rate of change of engine power also depends on the current power level. This rate is calculated in the functions PDOT and RTAU:</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">FUNCTION PDOT(P3,P1) ! PDOT= rate of change of power
      IF (P1.GE.50.0) THEN ! P3= actual power, P1= power command
        IF (P3.GE.50.0) THEN
            T=5.0
            P2=P1
        ELSE
            P2=60.0
            T=RTAU(P2-P3)
        END IF
      ELSE
        IF (P3.GE.50.0) THEN
            T=5.0
            P2=40.0
        ELSE
            P2=P1
            T=RTAU(P2-P3)
        END IF
      END IF
      PDOT=T*(P2-P3)
      RETURN
END

FUNCTION RTAU(DP) 
    ! used by function PDOT
    IF (DP.LE.25.0) THEN
        RTAU=1.0
        ! reciprocal time constant
    ELSE IF (DP.GE.50.0)THEN
        RTAU=0.1
    ELSE
        RTAU=1.9-.036*DP
    END IF
    RETURN
END</pre>



<p>PDOT means power rate of change. In C#, this is translated as:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">float CalculatePowerRateOfChange(float actualPower, float commandPower) {
    	// calculates how fast power output should change based on commanded power
    	float T;
    	float p2;

    	if (commandPower &gt;= 50.0) {
        	if (actualPower &gt;= 50.0) {
            	T = 5.0f;
            	p2 = commandPower;
        	} else {
            	p2 = 60.0f;
            	T = CalculateRTau(p2 - actualPower);
        	}
    	} else {
        	if (actualPower &gt;= 50.0) {
            	T = 5.0f;
            	p2 = 40.0f;
        	} else {
            	p2 = commandPower;
            	T = CalculateRTau(p2 - actualPower);
        	}
    	}

    	float pdot = T * (p2 - actualPower);

    	return pdot;
}

float CalculateRTau(float deltaPower) {
    	float rTau;

    	if (dp &lt;= 25.0) {
        	rTau = 1.0f;
    	} else if (dp &gt;= 50.0) {
        	rTau = 0.1f;
    	} else {
        	rTau = 1.9f - 0.036f * dp;
    	}

    	return rTau;
}</pre>



<p>Power rate of change is the velocity of the power level. The most important line is this:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">float pdot = T * (p2 - actualPower);</pre>



<p>The velocity depends on the quantity (p2 – actualPower). Let’s call this value deltaPower. A larger deltaPower means a larger velocity. This is scaled by the factor T. The complexity comes from selecting the values for p2 and T. p2 is <em>sometimes</em> the commandPower value. T is <em>sometimes</em> the result of calling CalculateRTau.</p>



<p>These values are selected by the if statements above. These check for two conditions, the commandedPower being above 50%, and the actualPower being above 50%. This is checking whether the afterburner is being requested, and whether the afterburner is currently active. Remember that afterburner starts at 77% throttle, but 50% power.</p>



<p>If the afterburner is not active, then the T is given the value of CalculateRTau. If it is active, then T is given the constant value of 5.0. This matches with our expectation of how the engine’s RPM changes. When not in afterburner, the engine RPM should change slowly, thus power changes slowly. When in afterburner, fuel flow into the afterburner can change quickly, thus power changes quickly.</p>



<p>If we look at the function CalculateRTau, we can see that T can vary in the range [0.1, 1.0]. This depends on deltaPower. When the engine is not in afterburner, T can be at most 1.0. In afterburner, T is 5.0. That means power can change about 5 times faster when in afterburner. When multiplied with deltaPower, pdot can be as large as 250% per second.</p>



<p>The smallest value of T occurs when deltaPower is 50 or greater. This occurs when actualPower is 0 and commanded power is 50%, for example. This will cause the power rate of change to be quite small at only 6% per second. Note that this is simply the instantaneous rate of change. As the actual power rises, T will become larger and the rate of change will increase.</p>



<p>Now the reason why p2 is used instead of commandedPower is to handle the case where commandedPower is over 50% and actualPower is below 50%, or vice versa. The pilot is requesting afterburner, but the engine has not reached military power yet. In that case, deltaPower would become very large and the simulation would change power levels too quickly. To avoid this, an arbitrary constant is chosen that is on the opposite side of 50%, but not very far.</p>



<p>So if the actualPower is 0%, but commandedPower is 100%, p2 is set to the value of 60. This limits deltaPower to a maximum value of 60, instead of 100. And in the case where actualPower is 100% and commandedPower is 0%, deltaPower is limited to -60.</p>



<p>Another behavior of this code is that CalculateRTau does not handle cases where deltaPower is negative. In this case, the function returns 1, the highest value it can return. This means that the power can decrease 10 times faster than it can increase, in the most extreme case.</p>



<p>I don’t know if this is an intentional effect. This may match the behavior of real jet engines, or it may be an oversight by the authors. You can play with the behavior by adding a few calls to Mathf.Abs().</p>



<p>The practical effect of all this is that the plane’s power will lag behind the player’s throttle setting. The pilot needs to make sure that they provide enough time for the power level to change when moving the throttle.</p>



<p>The HUD for this project is mostly reused from the previous flight sim project. But the throttle indicator must be updated, since it can’t show the difference between commanded power and current power.</p>



<p>Previously, the red bar used to show the player’s throttle setting. This worked fine since power lag was not modeled. In this project, the red bar shows the engine’s current power level. I added a triangle marker to show the commanded power setting.</p>



<figure><img loading="lazy" decoding="async" width="337" height="309" src="https://vazgriz.com/wp-content/uploads/2025/05/throttle_lag.png" alt=""></figure>



<p>As you move the throttle, you’ll see that current power level changes quickly when there is a large difference from commanded power, and it slows down as it approaches. And when the engine enters afterburner, the power level changes very quickly.</p>



<h2><span id="Thrust">Thrust</span></h2>



<p>Engine power is a fairly abstract variable in this flight model. It doesn’t really correspond to any physical variable. Once we calculate the current power, we use it to find the <em>thrust</em> generated by the engine. Thrust in this flight model is defined in terms of pounds-force (lbf).</p>



<p>Thrust is defined by a group of look up tables. Each table has two dimensions as input, mach number and altitude, and the output is thrust. This gives us different thrust values in different flight conditions. Mach is input as 0.0 to 1.0 mach, in increments of 0.2 mach. Altitude is input as 0 to 50,000 ft, in increments of 10,000 ft. In other words, the table has dimensions 6×6.</p>



<p>The lookup tables in this flight model correspond to idle power, military power, and max power (full afterburner). The engine’s power value is used to perform a third interpolation between the output values of these tables. This makes the thrust calculation a <em>trilinear interpolation</em>.</p>



<p>At idle throttle, the thrust output has 100% influence from the idle table. When the throttle is halfway to military power, the output has 50% influence from the idle table and 50% influence from the military table. Above military power, the output will have some influence from the military power table and the max power table.</p>



<p>The code to read one table in Fortran is given:</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">DATA A/  [IDLE TABLE OMITTED]
DATA B/  [MIL TABLE OMITTED]
DATA C/  [MAX TABLE OMITTED]
H=0.0001*ALT
I=INT(H)
IF (I.GE.5) I=4
DH=H-FLOAT(I)
RM=5.*RMACH
M=INT(RM)
IF (M.GE.5) M=4
DM=RM-FLOAT(M)
CDH=1.0-DH</pre>



<p>These parameters are used to perform the table lookups:</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">TMIL= S + (T-S)*DM
IF (POW.LT.50.0) THEN
    S= A(I,M)*CDH + A(I+1,M)*DH
    T= A(I,M+1)*CDH + A(I+1,M+1)*DH
    TIDL= S + (T-S)*DM
    THRUST= TIDL + (TMIL-TIDL)*POW/50.0
ELSE
    S= C(I,M)*CDH + C(I+1,M)*DH
    T= C(I,M+1)*CDH + C(I+1,M+1)*DH
    TMAX= S + (T-S)*DM
    THRUST= TMIL + (TMAX-TMIL)*(POW-50.0)*0.02
END IF</pre>



<p>The output of the military power table, TMIL, is always calculated. If the power level is under 50, then the idle table is calculated as well, TIDL. Otherwise the max table is calculated, TMAX. The output of the two table lookups is then interpolated again to calculate the final thrust value, THRUST.</p>



<p>Altogether, this forms a <em>trilinear lookup</em>. To translate this to C#, we call BilinearLookup twice. Then those two results are interpolated based on the power level:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">float InterpolateThrust(float thrust1, float thrust2, float power) {
    float result = Mathf.LerpUnclamped(thrust1, thrust2, power * 0.02f);
    return result;
}

float CalculateThrust(float power, float altitude, float rMach) {
	float a = Mathf.Max(0, altitude);
	float m = Mathf.Max(0, rMach);

	float thrust;
	float thrustMilitary = Table.BilinearLookup(a, 0.0001f, m, 5, militaryPowerTable, 0, 6, 0, 6);

	// perform trilinear interpolation
	if (power &lt; 50.0) {
    	float thrustIdle = Table.BilinearLookup(a, 0.0001f, m, 5, idlePowerTable, 0, 6, 0, 6);
    	thrust = InterpolateThrust(thrustIdle, thrustMilitary, power);
	} else {
    	float thrustMax = Table.BilinearLookup(a, 0.0001f, m, 5, maxPowerTable, 0, 6, 0, 6);
    	thrust = InterpolateThrust(thrustMilitary, thrustMax, power - 50.0f);
	}

	return thrust;
}</pre>



<p>The output of this calculation is the plane’s thrust in pounds-force. A simple unit conversion allows us to apply it in newtons to a Unity rigidbody:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">void UpdateThrust(float dt) {
    engine.ThrottleCommand = Throttle;
    engine.Mach = Mach;
    engine.Altitude = AltitudeFeet;

    engine.Update(dt);

    Rigidbody.AddRelativeForce(new Vector3(0, 0, engine.Thrust * poundsForceToNewtons));
}</pre>



<h2><span id="Forces">Forces</span></h2>



<h2><span id="Lift_force_vs_Normal_force">Lift force vs Normal force</span></h2>



<p>In the previous flight sim project, we calculated a plane’s lift force using the angle of attack and an AnimationCurve. This is the very core of the flight simulator and is what enables flight. The flight model from the textbook <em>does not</em> calculate lift force.</p>



<p>Instead what this flight model calculates is <em>normal force</em>. Recall that lift force is perpendicular to the aircraft’s velocity vector. Normal force is perpendicular to the aircraft’s nose. This distinction is subtle at a low angle of attack, but it becomes significant at a high angle of attack.</p>



<figure><img loading="lazy" decoding="async" width="1920" height="1080" src="https://vazgriz.com/wp-content/uploads/2025/06/Image_004_Text.jpg" alt="" srcset="https://vazgriz.com/wp-content/uploads/2025/06/Image_004_Text.jpg 1920w, https://vazgriz.com/wp-content/uploads/2025/06/Image_004_Text-1024x576.jpg 1024w, https://vazgriz.com/wp-content/uploads/2025/06/Image_004_Text-768x432.jpg 768w, https://vazgriz.com/wp-content/uploads/2025/06/Image_004_Text-1536x864.jpg 1536w" sizes="(max-width: 1920px) 100vw, 1920px"></figure>



<p>There are two more analogous forces to consider, drag and axial force. Drag is always exactly opposite to the aircraft’s velocity vector while axial force is opposite the aircraft’s nose. Lift and drag are perpendicular to each other and form one set of forces. Normal and axial form another perpendicular set. It’s important to understand that these two sets of forces are equally valid. In fact, they are simply the consequence of choosing different basis vectors for measuring force.</p>



<p>And of course there is the side force that points to the right. These forces are applied on the normal, side, and longitudinal (axial) axes, which are equivalent to the X, Y, and Z axes.</p>



<figure><img loading="lazy" decoding="async" width="1920" height="1080" src="https://vazgriz.com/wp-content/uploads/2025/06/Image_005_Text.jpg" alt="" srcset="https://vazgriz.com/wp-content/uploads/2025/06/Image_005_Text.jpg 1920w, https://vazgriz.com/wp-content/uploads/2025/06/Image_005_Text-1024x576.jpg 1024w, https://vazgriz.com/wp-content/uploads/2025/06/Image_005_Text-768x432.jpg 768w, https://vazgriz.com/wp-content/uploads/2025/06/Image_005_Text-1536x864.jpg 1536w" sizes="(max-width: 1920px) 100vw, 1920px"></figure>



<p>Imagine all of the forces being produced by the aircraft are summed into a single force vector. This vector would be strongly vertical, because the plane is generating enough lift to support it’s own weight, and somewhat backwards because of drag. When this vector is projected onto the lift vector, the result is the lift force. When it’s projected onto the normal vector, the result is the normal force.</p>



<p>Choosing to represent these forces as lift/drag or normal/axial is arbitrary. The textbook flight model only deals with normal/axial force. I suspect that’s because it’s easier to measure the physical forces when using normal/axial forces in a wind tunnel, since those are always aligned with the plane’s local axes.</p>



<figure><img loading="lazy" decoding="async" width="518" height="363" src="https://vazgriz.com/wp-content/uploads/2025/05/cl-cn.png" alt=""><figcaption>Lift force in red vs normal force in blue. The stall condition is visible on both forces at 15 degrees AOA. (From aerospaceweb.org<sup data-fn="a8954d70-ab22-4f44-91ba-707ddd1807a0"><a href="#a8954d70-ab22-4f44-91ba-707ddd1807a0" id="a8954d70-ab22-4f44-91ba-707ddd1807a0-link">10</a></sup>)</figcaption></figure>



<p>The normal force is very similar to lift for low angles of attack. Lift force peaks at the stall AOA and then declines. Normal force similarly peaks at stall AOA, but it then increases again to peak at 90 AOA, with an even higher force. 90 degrees AOA means the plane is falling downwards belly first, so it’s no longer producing lift over the wings. Instead the normal vector and the drag vector are now aligned. All of the drag force projected onto the normal vector results in a large normal force.</p>



<p>We can calculate the lift force from the normal and axial force. Both normal and axial force may contribute to the lift force, so a complete projection needs to use both. This is the formula:</p><p>



\(\text{Lift} = \text{normal} * \cos{(\text{alpha})} – \text{axial} * \sin{(\text{alpha})}\)



</p><p>When we apply this formula to the normal force from the textbook, this is the result:</p>



<figure><img loading="lazy" decoding="async" width="2048" height="1030" src="https://vazgriz.com/wp-content/uploads/2025/05/NormalLiftChart-2048x1030.png" alt="" srcset="https://vazgriz.com/wp-content/uploads/2025/05/NormalLiftChart-2048x1030.png 2048w, https://vazgriz.com/wp-content/uploads/2025/05/NormalLiftChart-1024x515.png 1024w, https://vazgriz.com/wp-content/uploads/2025/05/NormalLiftChart-768x386.png 768w, https://vazgriz.com/wp-content/uploads/2025/05/NormalLiftChart-1536x772.png 1536w" sizes="(max-width: 2048px) 100vw, 2048px"></figure>



<p>Oh wait, that’s upside down. Recall that the Z axis points downward in this coordinate system. So a negative Z value is an upwards force. Still though, the chart is a little confusing. I inverted the values below to make it more intuitive.</p>



<figure><img loading="lazy" decoding="async" width="2048" height="875" src="https://vazgriz.com/wp-content/uploads/2025/05/NormalLiftChart_inverted-2048x875.png" alt="" srcset="https://vazgriz.com/wp-content/uploads/2025/05/NormalLiftChart_inverted-2048x875.png 2048w, https://vazgriz.com/wp-content/uploads/2025/05/NormalLiftChart_inverted-1024x438.png 1024w, https://vazgriz.com/wp-content/uploads/2025/05/NormalLiftChart_inverted-768x328.png 768w, https://vazgriz.com/wp-content/uploads/2025/05/NormalLiftChart_inverted-1536x657.png 1536w" sizes="(max-width: 2048px) 100vw, 2048px"></figure>



<p>We can see at 90 degrees AOA, the normal force stays relatively high while the lift force drops to zero. This roughly matches with the chart from aerospaceweb.org above.</p>



<p>Also note that the textbook only provides table values up to 45 degrees AOA. The extrapolation of the table lookup function is what allows us to have normal force values up to 90 degrees AOA. Additionally, the table only goes down to -10 degrees AOA. We can extrapolate further, but the data will be inaccurate by -30 degrees AOA. Large negative AOA values will quickly become inaccurate. So when you’re flying, don’t do that.</p>



<p>Anyways, adding these forces to our simulator is easy. The functions are fairly simple. They are called CZ, CY, and CX. These calculate the coefficients of force on the Z, Y, and X axes respectively. Note that these functions are the <em>coefficients</em>, not the force values themselves. They are used to calculate the force later on.</p>



<p>CZ or the normal coefficient is calculated like this:</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">FUNCTION CZ(ALPHA,BETA,EL)
    REAL A(-2:9)
C
    DATA A/ [TABLE OMITTED]
C
    S = 0.2*ALPHA
    K = INT(S)
    IF(K.LE.-2) K=-1
    IF(K.GE.9) K=8
    DA = S - FLOAT(K)
    L = K + INT(SIGN(1.1,DA))
    S = A(K) + ABS(DA)*(A(L)-A(K))
    CZ = S*(1-(BETA/57.3)**2) - .19*(EL/25.0)
C
    RETURN
END</pre>



<p>The bulk of this code is just the table interpolation function. The table only depends on ALPHA and the output is S. The only new part here is the last line, where CZ is assigned a value. S is reduced based on the value of BETA and another term is subtracted based on EL, the elevator angle.</p>



<p>This is very easy to translate to C#:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">float GetZAxisForceCoefficient(float alpha, float beta, float elevator) {
    	float S = Table.LinearLookup(alpha, 0.2f, zAxisTable, -2, 10);
    	float CZ = S * (1 - Mathf.Pow(beta * Mathf.Deg2Rad, 2)) - 0.19f * (elevator / 25.0f);
    	return CZ;
}</pre>



<p>CY or the side coefficient is even simpler. It doesn’t even have a lookup table. Side force is perpendicular to both normal and axial force.</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">FUNCTION CY(BETA,AIL,RDR)
    CY = -.02*BETA + .021*(AIL/20.0) + .086*(RDR/30.0)
C
    RETURN
END</pre>



<p>Side coefficient depends solely on beta, aileron angle, and rudder angle.</p>



<p>In C#:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">float GetYAxisForceCoefficient(float beta, float aileron, float rudder) {
    float CY = -0.02f * beta + 0.021f * (aileron / 20.0f) + 0.086f * (rudder / 30.0f);
    return CY;
}</pre>



<p>CX or the axial coefficient is basically what creates drag on the aircraft. This function is a little more complicated since it performs a bilinear interpolation, with alpha and elevator angle as the inputs.</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">FUNCTION CX(ALPHA,EL)
    REAL A(-2:9,-2:2)
C
    DATA A/ [TABLE OMITTED]
C
    S = 0.2*ALPHA
    K = INT(S)
    IF(K.LE.-2) K=-1
    IF(K.GE.9) K=8
    DA = S - FLOAT(K)
    L = K + INT(SIGN(1.1,DA))
    S = EL/12.0
    M = INT(S)
    IF(M.LE.-2) M=-1
    IF(M.GE.2) M=1
    DE = S - FLOAT(M)
    N = M + INT(SIGN(1.1,DE))
    V = A(K,M) + ABS(DA)*(A(L,M)-A(K,M))
    W = A(K,N) + ABS(DA)*(A(L,N)-A(K,N))
    CX = V + (W-V)*ABS(DE)
C
    RETURN
END</pre>



<p>Thanks to the table lookup functions, this is easy to translate to C#:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">float GetXAxisForceCoefficient(float alpha, float elevator) {
    float result = Table.BilinearLookup(alpha, 0.2f, elevator, 1f / 12f, xAxisTable, -2, 9, -2, 2);
    return result;
}</pre>



<p>These three functions define all of the linear force coefficients applied to the aircraft during flight. None of these will rotate the aircraft. That is handled by a different and more complicated set of calculations.</p>



<h2><span id="Moments">Moments</span></h2>



<p>Moment is another word for torque. (There is a subtle difference, but who cares?🤓) The F-16 flight model uses another set of look up tables to compute the moment for the aircraft.</p>



<p>In the previous flight sim, torque was not actually calculated. Instead, the flight model calculates the angular acceleration directly. This ignores the mass of the plane when applying the torque. This is a simplification. A more realistic flight model would take into account the mass of the aircraft when applying torque.</p>



<p>Note that mass is not sufficient to model rotations. When it comes to rotation, the analogy to mass is called <em>moment of inertia</em>. Just like mass is the property that measures an object’s resistance to force, moment of inertia is the resistance to torque. But unlike mass, moment of inertia can differ on all 3 axes. This means a torque on the X axis will result in a different angular acceleration than the same torque on the Y axis, for example.</p>



<p>Moment of inertia is a four-dimensional value, called AXX, AYY, AZZ, and AXZ in the textbook code. This flight model contains it’s own calculations for angular velocity using these moment of inertia values.</p>



<p>The flight model contains several functions that calculate the moment of the aircraft. The three basic functions are called CM, CL, and CN. These calculate the moments around the Y, X, and Z axes, AKA pitch, roll, and yaw, respectively. (L stands for longitudinal, which is the X axis. N stands for normal, which is the Z axis. M stands for… something)</p>



<p>These functions are all simple look up tables. CM (pitch) uses alpha and elevator angle as the input. CL (roll) and CN (yaw) use alpha and beta as the input. CM is basically the same as the other lookup table functions. CL and CN are similar to each other since they both use a symmetric table. This is because the plane is symmetric on the lateral axis, so a single table can represent the left and right sides. Their final output is then multiplied by the sign of beta.</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">FUNCTION CL(ALPHA,BETA)
    REAL A(-2:9,0:6)
    DATA A/    [DATA OMITTED]

    S = 0.2*ALPHA
    K = INT(S)
    IF(K.LE.-2) K=-1
    IF(K.GE.9) K=8
    DA = S - FLOAT(K)
    L = K + INT(SIGN(1.1,DA))
    S = .2*ABS(BETA)
    M = INT(S)
    IF(M.EQ.0) M=1
    IF(M.GE.6) M=5
    DB = S - FLOAT(M)
    N = M + INT(SIGN(1.1,DB))
    T = A(K,M)
    U = A(K,N)
    V = T + ABS(DA)*(A(L,M) - T)
    W = U + ABS(DA)*(A(L,N) - U)
    DUM = V + (W - V) * ABS(DB)
    CL = DUM + SIGN(1.0,BETA)

    RETURN
END</pre>



<p>Note that there is an error in the textbook code. The final operation “CL = DUM + SIGN(…)” should use multiplication instead of addition. Otherwise this operation doesn’t make any sense.</p>



<p>When translated into C#:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">float GetYAxisMoment(float alpha, float elevator) {
    float result = Table.BilinearLookup(alpha, 0.2f, elevator, 1f / 12f, yMomentTable, -2, 9, -2, 2);
    return result;
}

float GetXAxisMoment(float alpha, float beta) {
    float DUM = Table.BilinearLookup(alpha, 0.2f, Mathf.Abs(beta), 0.2f, xMomentTable, -2, 9, 0, 7);
    float CL = DUM * Mathf.Sign(beta);

    return CL;
}

float GetZAxisMoment(float alpha, float beta) {
    float DUM = Table.BilinearLookup(alpha, 0.2f, Mathf.Abs(beta), 0.2f, zMomentTable, -2, 9, 0, 7);
    float CL = DUM * Mathf.Sign(beta);

    return CL;
}</pre>



<p>Notice that CM takes “elevator” as an argument, so this is where the elevator’s turning effect is calculated. But CL and CN do not take any control surface as an argument. These functions only apply moment based on alpha and beta. For example, at high angles of sideslip, the plane tends to roll. On real planes, this is caused by wing sweep. In this flight model, it’s caused by the CL function.</p>



<p>Elevators are applied in CM, but rudder and ailerons are not. Those are actually handled by four more functions, called DLDA, DLDR, DNDA, and DNDR. The names are cryptic, but it just means which axis is affected from which control surface.</p>



<p>The “L” stands for longitudinal, so DLDA is the longitudinal moment from the ailerons, A. DLDR is the longitudinal moment from the rudder, R. The “N” stands for normal, so those functions are the normal axis moment from aileron and rudders.</p>



<p>These four functions are eventually summed with the CL and CN functions above. These functions mean that roll is affected by aileron and rudder, and yaw is affected by aileron and rudder.</p>



<h2><span id="Damping">Damping</span></h2>



<p>There is one more set of coefficients that must be calculated. These are the damping coefficients and they depend solely on alpha. These values are stored in 9 distinct 1D lookup tables. The code for these lookups is the same as the other lookup code.</p>



<p>These values are stored in an array of length 9 called D.</p>



<p>Damping is the moment that opposes the angular velocity of an aircraft, essentially angular drag. They affect the other moment values in somewhat complex ways. For example, some of them are combined with the plane’s current bank value to affect the roll moment.</p>



<p>What isn’t clear is what the damping values actually represent. In the C# code, I added these comments explaining their meaning:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">//  D[0] = CXq
//  D[1] = CYr
//  D[2] = CYp
//  D[3] = CZq
//  D[4] = Clr
//  D[5] = Clp
//  D[6] = Cmq
//  D[7] = Cnr
//  D[8] = Cnp</pre>



<p>Hope this helps!</p>



<p>The best I can tell is that “CXq” is the damping moment on the X axis relative to <em>q</em>, which is the angular velocity around the Y axis. The other damping values follow this naming scheme.</p>



<p>This is yet another example of aerodynamics texts with poor variable names.</p>



<h2><span id="Complete_Flight_Model">Complete Flight Model</span></h2>



<p>With all of the individual coefficients defined, we can now implement the complete flight model for the F-16. This flight model actually contains it’s own physics integrator. The text provides it’s own code for calculating velocity and angular velocity from the aerodynamic forces.</p>



<p>Strictly speaking, we don’t need to use this code since Unity allows us to provide those same forces and then performs the physics calculation for us. Setting the mass is easy enough, we just have to convert slugs to kilograms. The textbook code calculates acceleration by dividing the force by the aircraft mass. We simply omit this division, convert the forces to newtons, and apply it to the rigidbody.</p>



<p>However the moment of inertia is more complicated. The textbook provides the 4 dimensional MOI values, but Unity expects a 3 dimensional inertia tensor. That inertia tensor is then rotated by a quaternion called “inertiaTensorRotation”. I have no idea how to calculate this quaternion from the textbook’s provided value.</p>



<p>Therefore, we continue to use the textbook’s code for applying moment and simply apply the resulting angular acceleration to the rigidbody.</p>



<p>The Fortran code for the flight model is concise, yet scrutable. The first step is to read the plane’s current state from the input state vector X. This is simply an array that contains all of the relevant data for this frame.</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">VT= X(1); ALPHA= X(2)*RTOD; BETA= X(3)*RTOD
PHI=X(4); THETA= X(5); PSI= X(6)
P= X(7); Q= X(8); R= X(9); ALT= X(12); POW= X(13)</pre>



<p>VT is the plane’s velocity in feet per second.</p>



<p>ALPHA and BETA are the plane’s AOA and AOS in degrees. RTOD is the constant to convert from radians to degrees.</p>



<p>PHI, THETA, and PSI are the plane’s roll, pitch, and yaw in radians.</p>



<p>P, Q, and R are the plane’s angular velocities (or roll rate, pitch rate, and yaw rate) in radians per second.</p>



<p>ALT is the altitude in feet.</p>



<p>POW is the current power level of the engine (0 – 100).</p>



<p>The air data computer (ADC) and engine model are then called using these variables:</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">CALL ADC(VT,ALT,AMACH,QBAR); CPOW= TGEAR(THTL)
XD(13) = PDOT(POW,CPOW); T= THRUST(POW,ALT,AMACH)</pre>



<p>The ADC function populates the variables AMACH and QBAR, which are the altitude mach and dynamic pressure.</p>



<p>CPOW is the pilot’s commanded power setting. That is, the power level returned by calling the throttle gear function, TGEAR, on the throttle lever position, THTL.</p>



<p>The array XD is the output state vector. Specifically, it holds the calculated derivative for every input value. XD(13) is set to the value calculated by PDOT, which is the velocity of the power level.</p>



<p>T is the thrust in pounds-force output by the engine, calculated using the power level, altitude, and altitude mach.</p>



<p>Then the aerodynamic coefficients are calculated using the force and moment functions:</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">  CXT = CX (ALPHA,EL)
  CYT = CY (BETA,AIL,RDR)
  CZT = CZ (ALPHA,BETA,EL)
  DAIL= AIL/20.0; DRDR= RDR/30.0
  CLT = CL(ALPHA,BETA) + DLDA(ALPHA,BETA)*DAIL
 &amp;                 	+ DLDR(ALPHA,BETA)*DRDR
  CMT = CM(ALPHA,EL)
  CNT = CN(ALPHA,BETA) + DNDA(ALPHA,BETA)*DAIL
 &amp;                 	+ DNDR(ALPHA,BETA)*DRDR</pre>



<p>The values CXT, CYT, and CZT are the coefficients on the X, Y, and Z axes, calculated by calling their respective coefficient functions.</p>



<p>EL, AIL, and RDR are the current position of the elevators, ailerons, and rudder in degrees. DAIL and DRDR are simply the angle of these surfaces divided by the max angle. Their range is [-1, 1].</p>



<p>The values CLT, CMT, and CNT are the moment coefficients on the longitudinal, m’lateral, and normal axes. Note that CM calculates moment caused by the elevator position. The effects of the other control surfaces are calculated in the DLDA, DLDR, DNDA, and DNDR functions.</p>



<p>Then some other values are calculated and the damping coefficients are added to the above values:</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">TVT= 0.5/VT; B2V= B*TVT; CQ= CBAR*Q*TVT
CALL DAMP(ALPHA,D)
CXT= CXT + CQ * D(1)
CYT= CYT + B2V * ( D(2)*R + D(3)*P )
CZT= CZT + CQ * D(4)
CLT= CLT + B2V * ( D(5)*R + D(6)*P )
CMT= CMT + CQ * D(7) + CZT * (XCGR-XCG)
CNT= CNT + B2V*(D(8)*R + D(9)*P) - CYT*(XCGR-XCG) * CBAR/B</pre>



<p>I’ll be honest, I straight up don’t know what any of these values are or why they are being applied like this. The effect appears to be angular damping (AKA angular drag) which opposes the plane’s angular velocity.</p>



<p>The value (XCGR – XCG) is the center of gravity reference minus the current center of gravity. This allows us to alter the center of gravity of the aircraft and see how that affects stability.</p>



<p>XCGR is 0.35 for this flight model. XCG is 0.35 by default. XCG is the normalized position of the center of gravity, with a possible range of [0, 1]. This means that when XCG is 0.35, the term (XCGR – XCG) becomes zero and the aircraft is balanced around it’s center of gravity.</p>



<p>The center of gravity term affects CMT and CNT, which are the pitch and yaw axes. The roll axis is not affected.</p>



<p>The next block of code is fun:</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">CBTA = COS(X(3)); U=VT*COS(X(2))*CBTA
V= VT * SIN(X(3)); W=VT*SIN(X(2))*CBTA
STH = SIN(THETA); CTH= COS(THETA); SPH= SIN(PHI)
CPH = COS(PHI) ; SPSI= SIN(PSI); CPSI= COS(PSI)
QS = QBAR * S ; QSB= QS * B; RMQS= QS/MASS
GCTH = GD * CTH ; QSPH= Q * SPH
AY = RMQS*CYT ; AZ= RMQS * CZT</pre>



<p>This writhing mass of arithmetic is simply pre-calculating a lot of the values that are used to calculate the aerodynamic forces. Some of these values are used in multiple places, so to avoid repeating them, they are pulled out of those equations and placed here.</p>



<p>This is essentially the “common subexpression” optimization pass of a compiler, but applied manually.</p>



<p>The important variables are U, V, and W, which is the plane’s velocity on the X, Y, and Z axes respectively.</p>



<p>QS is QBAR (dynamic pressure) times S (wing area).</p>



<p>Now the aerodynamic forces are calculated:</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">UDOT = R*V - Q*W - GD*STH + (QS * CXT + T)/MASS
VDOT = P*W - R*U + GCTH * SPH + AY
WDOT = Q*U - P*V + GCTH * CPH + AZ
DUM = (U*U + W*W)
xd(1) = (U*UDOT + V*VDOT + W*WDOT)/VT
xd(2) = (U*WDOT - W*UDOT) / DUM
xd(3) = (VT*VDOT- V*XD(1)) * CBTA / DUM</pre>



<p>Once again, I don’t actually understand what I’m reading. UDOT etc are the accelerations on each axis. These values are then used to update the output state vector xd(1), xd(2), and xd(3), which are the VT, ALPHA, and BETA that will be used in the next frame.</p>



<p>It appears that this flight model is calculating the change in alpha and beta directly from the change in velocity. This is not necessary in C#, since we can calculate alpha and beta fresh in each frame.</p>



<p>But I don’t fully understand how UDOT is calculated. R and Q are angular velocities, so multiplying them with linear velocity doesn’t make any sense. Perhaps this is some physics equation that I’m not familiar with.</p>



<p>GD * STH is the gravity acceleration times sin(theta). This is simply how gravity is applied. When the plane is level (theta = 0), sin(theta) is 0. The plane experiences no gravity acceleration on the X axis (the forward axis). When the plane is pointed straight down, sin(theta) = 1, so the plane experiences the full force of gravity pulling on the X axis.</p>



<p>A similar calculation is made for every axis.</p>



<p>For UDOT, the final term is (QS * CXT + T) / MASS. This is the coefficient CXT plus the thrust from the engine, divided by mass. VDOT and WDOT have similar final terms, made more difficult to read by the common subexpression optimization.</p>



<p>Ignoring the other terms, the 3 accelerations can be written:</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">UDOT = (QS * CXT + T)/MASS
VDOT = AY
WDOT = AZ</pre>



<p>Then the variables can be expanded and rewritten:</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">UDOT = (QBAR * S * CXT + T) / MASS
VDOT = (QBAR * S * CYT)     / MASS
WDOT = (QBAR * S * CZT)     / MASS</pre>



<p>This is simply the force coefficient times QBAR (dynamic pressure) times S (wing area). Then thrust is added to the X axis. This is how all forces are applied to the aircraft.</p>



<p>Recall the lift equation from my previous project:</p><p>



\(L=\frac12\times A\times\rho\times C_L\times v^2\)



</p><ul>
<li>L is the resulting lift force</li>



<li>A is the surface area</li>



<li>ρ (rho) is the air density</li>



<li>C<sub>L</sub> is the coefficient of lift</li>



<li>v is the velocity</li>
</ul>



<p>The surface area A is equivalent to the wing area S in the Fortran code. C<sub>L</sub> is equivalent to the variables CXT, CYT, or CZT. The factor ρ * v<sup>2</sup> is equivalent to QBAR. Thus we are essentially calculating a lift force on all three axes. But remember that we are specifically calculating normal force, not lift force.</p>



<p>The roll, pitch, and yaw state vectors are then updated:</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">xd(4) = P + (STH/CTH)*(QSPH + R*CPH)
xd(5) = Q*CPH - R*SPH
xd(6) = (QSPH + R*CPH)/CTH</pre>



<p>Once again, these equations make zero sense to me🤷‍♂️. It’s important for the Fortran code, but we will be calculating roll, pitch, and yaw differently in C#.</p>



<p>Aerodynamic moment is about to be calculated. However this depends on the moment of inertia values and some more values derived from those:</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">PARAMETER (AXX=9496.0, AYY= 55814.0, AZZ=63100.0, AXZ= 982.0)
PARAMETER (AXZS=AXZ**2, XPQ=AXZ*(AXX-AYY+AZZ),GAM=AXX*AZZ-AXZ**2)
PARAMETER (XQR= AZZ*(AZZ-AYY)+AXZS, ZPQ=(AXX-AYY)*AXX+AXZS)
PARAMETER ( YPR= AZZ - AXX )</pre>



<p>Now the aerodynamic moment is calculated:</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">ROLL  = QSB*CLT
PITCH = QS *CBAR*CMT
YAW   = QSB*CNT
PQ    = p*Q
QR    = Q*R
QHX   = Q*HX
xd(7) = ( XPQ*PQ - XQR*QR + AZZ*ROLL + AXZ*(YAW + QHX) )/GAM
xd(8) = ( YPR*P*R - AXZ*(P**2 - R**2) + PITCH - R*HX )/AYY
xd(9) = ( ZPQ*PQ - XPQ*QR + AXZ*ROLL + AXX*(YAW + QHX) )/GAM</pre>



<p>There’s a lot of stuff going on here. The output state vectors are updated using the moment of inertia values as well as HX, which is the angular momentum of the spinning engine mass. I don’t know enough about physics to fully understand why these equations are defined like this.</p>



<p>But we can at least see how the moment coefficients are used if we expand the ROLL, PITCH, and YAW variables:</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">ROLL  = QBAR * S * B    * CLT
PITCH = QBAR * S * CBAR * CMT
YAW   = QBAR * S * B    * CNT</pre>



<p>ROLL and YAW depend on B, the wingspan of the plane. Pitch depends on CBAR, the mean aerodynamic chord.</p>



<p>The final step is to calculate the world space position of the aircraft. Since we are using Unity rigidbodies to implement the flight model, this step is not translated to C#. But for reference:</p>



<pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">T1= SPH * CPSI; T2= CPH * STH; T3= SPH * SPSI
S1= CTH * CPSI; S2= CTH * SPSI; S3= T1 * STH - CPH * SPSI
S4= T3 * STH + CPH * CPSI; S5= SPH * CTH; S6= T2*CPSI + T3
S7= T2 * SPSI - T1; S8= CPH * CTH

xd(10) = U * S1 + V * S3 + W * S6 ! North speed
xd(11) = U * S2 + V * S4 + W * S7 ! East speed
xd(12) = U * STH -V * S5 - W * S8 ! Vertical speed

AN = -AZ/GD; ALAT= AY/GD;</pre>



<p>Now we can start translating this into C# using Unity’s physics engine to replace some parts.</p>



<p>A lot of the code can be reused from the previous flight sim project. Using it for this new flight model only requires some conversion into customary units and back. The main class that controls everything is Plane. This class contains instances of the AirDataComputer, Engine, and Aerodynamics, which is where the translated Fortran code lives.</p>



<p>One simplification can be made since we are using Unity physics. We do not need to calculate the acceleration of the aircraft manually. That can be done automatically by the physics engine. However, the moment calculation needs to be copied more or less directly from the textbook.</p>



<p>The air data computer needs to be called:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">void UpdateAirData() {
    float speed = LocalVelocity.magnitude;  // m/s
    float speedFeet = speed * metersToFeet;
    AltitudeFeet = Rigidbody.position.y * metersToFeet;

    airData = airDataComputer.CalculateAirData(speedFeet, AltitudeFeet);
}</pre>



<p>Then the engine needs to be updated and the thrust force applied:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">void UpdateThrust(float dt) {
    engine.ThrottleCommand = Throttle;
    engine.Mach = Mach;
    engine.Altitude = AltitudeFeet;

    engine.Update(dt);

    Rigidbody.AddRelativeForce(new Vector3(0, 0, engine.Thrust * poundsForceToNewtons));
}</pre>



<p>For the aerodynamics class, a struct with all relevant aerodynamic state is passed, similar to the state vector in the Fortran code.</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">public struct AerodynamicState {
    public Vector4 inertiaTensor;
    public Vector3 velocity;
    public Vector3 angularVelocity;
    public AirData airData;
    public float altitude;
    public float alpha;
    public float beta;
    public float xcg;
    public ControlSurfaces controlSurfaces;
}</pre>



<p>This is populated by the Plane class, which also handles unit conversions:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">AerodynamicState currentState = new AerodynamicState {
    inertiaTensor = inertiaTensor,
    velocity = ConvertVectorToAerospace(LocalVelocity) * metersToFeet,
    angularVelocity = ConvertAngleToAerospace(LocalAngularVelocity),
    airData = airData,
    alpha = alpha,
    beta = beta,
    xcg = centerOfGravityPosition,
    controlSurfaces = ControlSurfaces
};

var newState = aerodynamics.CalculateAerodynamics(currentState);</pre>



<p>All of the flight model code is located inside the Aerodynamics class.</p>



<p>First step is to call the aerodynamic coefficient functions from above:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">Vector3 GetForceCoefficient(float alpha, float beta, float aileron, float rudder, float elevator) {
    return new Vector3(
        GetXAxisForceCoefficient(alpha, elevator),
        GetYAxisForceCoefficient(beta, aileron, rudder),
        GetZAxisForceCoefficient(alpha, beta, elevator)
    );
}

Vector3 GetMomentCoefficient(float alpha, float beta, float elevator) {
    return new Vector3(
        GetXAxisMomentCoefficient(alpha, beta),
        GetYAxisMomentCoefficient(alpha, elevator),
        GetZAxisMomentCoefficient(alpha, beta)
    );
}

...

public AerodynamicForces CalculateAerodynamics(AerodynamicState currentState) {
    Vector3 forceCoefficient = GetForceCoefficient(
        currentState.alpha, currentState.beta,
        currentState.controlSurfaces.aileron, currentState.controlSurfaces.rudder, currentState.controlSurfaces.elevator
    );

    Vector3 momentCoefficient = GetMomentCoefficient(
        currentState.alpha, currentState.beta, currentState.controlSurfaces.elevator
    );
}</pre>



<p>Then we calculate the damping values. This function simply performs the 9 table lookups.</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">void CalculateDampingValues(float alpha) {
    float S = 0.2f * alpha;
    int K = Mathf.Clamp((int)S, -1, 8);

    float DA = S - K;
    int L = K + (int)Mathf.Sign(DA);

    for (int i = 0; i &lt; 9; i++) {
        dampingTable[i] = ReadDampTable(dampTable, K, i) + Math.Abs(DA) * (ReadDampTable(dampTable, L, i) - ReadDampTable(dampTable, K, i));
    }
}</pre>



<p>Then the variables we need later are calculated:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">// calculate variables
float P = currentState.angularVelocity.x;   // roll rate
float Q = currentState.angularVelocity.y;   // pitch rate
float R = currentState.angularVelocity.z;   // yaw rate

float airspeed = Mathf.Max(1, currentState.velocity.magnitude);
float TVT = 0.5f / airspeed;

float B2V = wingSpanFt * TVT;
float CQ = CBAR * Q * TVT;

float DAIL = currentState.controlSurfaces.aileron / 20.0f;
float DRDR = currentState.controlSurfaces.rudder / 30.0f;

float QS = currentState.airData.qBar * wingAreaFtSquared;
float QSB = QS * wingSpanFt;</pre>



<p>Then damping is applied to the force and moment coefficients:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">// damping
float CXT = forceCoefficient.x + CQ * dampingTable[0];
float CYT = forceCoefficient.y + B2V * (dampingTable[1] * R + dampingTable[2] * P);
float CZT = forceCoefficient.z + CQ * dampingTable[3];

float CLT = momentCoefficient.x + B2V * (dampingTable[4] * R + dampingTable[5] * P);
CLT += GetDLDA(currentState.alpha, currentState.beta) * DAIL;
CLT += GetDLDR(currentState.alpha, currentState.beta) * DRDR;
float CMT = momentCoefficient.y + CQ * dampingTable[6] + CZT * (XCGR - currentState.xcg);
float CNT = momentCoefficient.z + B2V * (dampingTable[7] * R + dampingTable[8] * P) - CYT * (XCGR - currentState.xcg) * CBAR / wingSpanFt;
CNT += GetDNDA(currentState.alpha, currentState.beta) * DAIL;
CNT += GetDNDR(currentState.alpha, currentState.beta) * DRDR;</pre>



<p>Note that the damping array in Fortran is 1 based, while the same array in C# is 0 based.</p>



<p>Forces are calculated from the force coefficients. Since we are using Unity’s physics to apply gravity, the gravity terms are not included here. The force from engine thrust is applied outside of this class. And force is applied to a rigidbody, so acceleration does not need to be calculated manually. So the force calculations are now very simple:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">// forces
// Acceleration in original text. Need to calculate force instead of acceleration
float UDOT = QS * CXT;
float VDOT = QS * CYT;
float WDOT = QS * CZT;</pre>



<p>Moments are calculated using largely the same code as the textbook:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">// moments
float ROLL = QSB * CLT;
float PITCH = QS * CBAR * CMT;
float YAW = QSB * CNT;
float PQ = P * Q;
float QR = Q * R;
float QHX = Q * HX;

// calculate inertia values
float AXX = currentState.inertiaTensor.x;
float AYY = currentState.inertiaTensor.y;
float AZZ = currentState.inertiaTensor.z;
float AXZ = currentState.inertiaTensor.w;

float AXZS = AXZ * AXZ;
float XPQ = AXZ * (AXX - AYY + AZZ);
float GAM = AXX * AZZ - AXZS;
float XQR = AZZ * (AZZ - AYY) + AXZS;
float ZPQ = (AZZ - AYY) * AXX + AXZS;
float YPR = AZZ - AXX;

float rollAccel  = ((XPQ * PQ)	- (XQR * QR) + (AZZ * ROLL) + (AXZ * (YAW + QHX))) / GAM;
float pitchAccel = ((YPR * P * R) - (AXZ * (P * P - R * R))   + PITCH - (R * HX))	/ AYY;
float yawAccel   = ((ZPQ * PQ)	- (XPQ * QR) + (AXZ * ROLL) + (AXX * (YAW + QHX))) / GAM;</pre>



<p>Finally, the force and angular acceleration is returned:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">public AerodynamicForces CalculateAerodynamics(AerodynamicState currentState) {
    ...
    result.force = new Vector3(UDOT, VDOT, WDOT);
    result.angularAcceleration = new Vector3(rollAccel, pitchAccel, yawAccel);

    return result;
}</pre>



<p>Then in the Plane class, the force and angular acceleration can be applied to the rigidbody:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">// aeroForces in pounds
var forces = ConvertVectorToUnity(aeroForces) * poundsForceToNewtons;
Rigidbody.AddRelativeForce(forces);

// aeroAngularAcceleration changes angular velocity directly
Vector3 avCorrection = ConvertAngleToUnity(aeroAngularAcceleration);
Rigidbody.AddRelativeTorque(avCorrection, ForceMode.Acceleration);
lastAngularAcceleration = avCorrection;</pre>



<p>The plane is now able to fly. But if you try flying it right now, you will quickly find that it is impossible to fly by hand.</p>



<h2><span id="Stability">Stability</span></h2>



<p>One important aerodynamic effect not modeled in my previous flight sim project is <em>stability</em>. Stability is the behavior of an aircraft when it’s disturbed from it’s flight path. More specifically, it’s how the aircraft behaves when it’s nose vector doesn’t match it’s velocity vector. Stability is the force that pulls the nose vector back towards the velocity vector.</p>



<p>For most aircraft, stability is created by the stabilizers in the tail. A stabilizer is simply a small airfoil (wing). Even without the pilot giving input, the stabilizers act like the fins of a dart. As the plane increases it’s Angle of Attack, the horizontal stabilizer will produce a lift force at the rear of the plane. This creates a torque that pulls the plane’s nose back towards the velocity vector, thus reducing the AOA. Likewise, the vertical stabilizers will create a torque that reduces Angle of Slip.</p>



<figure><img loading="lazy" decoding="async" width="1920" height="1080" src="https://vazgriz.com/wp-content/uploads/2025/06/Image-Sequence_001.jpg" alt="" srcset="https://vazgriz.com/wp-content/uploads/2025/06/Image-Sequence_001.jpg 1920w, https://vazgriz.com/wp-content/uploads/2025/06/Image-Sequence_001-1024x576.jpg 1024w, https://vazgriz.com/wp-content/uploads/2025/06/Image-Sequence_001-768x432.jpg 768w, https://vazgriz.com/wp-content/uploads/2025/06/Image-Sequence_001-1536x864.jpg 1536w" sizes="(max-width: 1920px) 100vw, 1920px"></figure>



<p>Keep in mind that stability depends on AOA, just as lift does. When the aircraft has a large AOA, the wings produce lift, which brings the velocity vector towards the nose vector. The stabilizers create torque which brings the nose vector towards the velocity vector. These two forces balance out somewhere and the aircraft will take a new attitude with a new velocity.</p>



<figure><img loading="lazy" decoding="async" width="1920" height="1080" src="https://vazgriz.com/wp-content/uploads/2025/06/Image-Sequence_002.jpg" alt="" srcset="https://vazgriz.com/wp-content/uploads/2025/06/Image-Sequence_002.jpg 1920w, https://vazgriz.com/wp-content/uploads/2025/06/Image-Sequence_002-1024x576.jpg 1024w, https://vazgriz.com/wp-content/uploads/2025/06/Image-Sequence_002-768x432.jpg 768w, https://vazgriz.com/wp-content/uploads/2025/06/Image-Sequence_002-1536x864.jpg 1536w" sizes="(max-width: 1920px) 100vw, 1920px"></figure>



<p>However, the aircraft needs to maintain a non-zero AOA to create enough lift to fly straight and level. How does it maintain this AOA when stability works to reduce AOA to zero? The stabilizers can be <em>trimmed</em> to hold a specific AOA. This means that the stabilizers produce zero torque at this particular non-zero AOA. In some planes this must be done manually by the pilot, but in the F-16 this is done automatically by the FCS.</p>



<figure><img loading="lazy" decoding="async" width="1920" height="1080" src="https://vazgriz.com/wp-content/uploads/2025/06/Image-Sequence_003.jpg" alt="" srcset="https://vazgriz.com/wp-content/uploads/2025/06/Image-Sequence_003.jpg 1920w, https://vazgriz.com/wp-content/uploads/2025/06/Image-Sequence_003-1024x576.jpg 1024w, https://vazgriz.com/wp-content/uploads/2025/06/Image-Sequence_003-768x432.jpg 768w, https://vazgriz.com/wp-content/uploads/2025/06/Image-Sequence_003-1536x864.jpg 1536w" sizes="(max-width: 1920px) 100vw, 1920px"></figure>



<p>The two forces of lift and stability combine to produce the “feel” of an aircraft’s controls. The tendency for the nose to be pulled towards the velocity vector is called <em>positive stability</em>. Most non-fighter aircraft are designed to have positive stability to maximize safety and ease of flying.</p>



<p>But fighter aircraft like the F-16 are different. These aircraft are often designed to have <em>neutral</em> or even <em>negative stability</em>. Neutral stability means that the aircraft will hold it’s current attitude. Negative stability means that the aircraft will rotate even further away from the velocity vector, at an increasing rate.</p>



<p>The previous flight sim does not model this at all. There is no torque that changes the plane’s attitude except for the steering force. So the behavior is best described as neutrally stable.</p>



<p>This F-16 flight model does include stability. But keep in mind that the real F-16 was designed to have <em>relaxed static stability</em>. This means that it is positively stable, but weakly so. This makes the aircraft more maneuverable and better at retaining energy while turning. But flying an aircraft like this is difficult or even impossible for a human pilot. The plane will depart from steady flight from the smallest stick input or wind gust. The only way a human can handle an aircraft like this during long and stressful missions is with a computerized <em>flight control system</em>.</p>



<h2><span id="Flight_Control_System">Flight Control System</span></h2>



<p>A <em>flight control system</em> (FCS) is a computer located between the flight stick and the control surfaces. This computer translates the pilot’s input on the flight stick into control surface movement. It can react to disturbances in the plane’s attitude more quickly and precisely than a human can.</p>



<p>In my previous flight sim project, the control surfaces were purely cosmetic. The actual method used to turn the vehicle was by applying torque directly to the center of mass. That torque was calculated to create a certain amount of angular acceleration without exceeding the plane’s turn rate limit.</p>



<p>For example, the plane had a turn rate on the pitch axis of 60 degrees per second and an acceleration of 120 degrees per second per second. The plane’s turn rate never leaves the range [-60, 60]. Actually, no torque is ever calculated. Unity provides a function to apply angular acceleration directly, ignoring moment of inertia. I chose this behavior to make it easy to both understand the code and to fly the plane.</p>



<p>But this F-16 simulator does depend on the position of the control surfaces. Instead of specifying the acceleration directly, this simulator specifies the torque (moment) and calculates the resulting acceleration. This is more accurate to how serious simulators work and how real planes fly, but this makes controlling the plane more difficult.</p>



<p>The steering system in the previous flight sim project was essentially a perfect FCS that could always achieve the turn rate chosen by the pilot. This is helped by the fact that that simulator does not model aerodynamic stability or instability at all. Spinning out of control was simply not possible.</p>



<p>This F-16 simulator is more difficult to control both because of the more accurate control surfaces and because of the modeled stability. You can actually try to fly this F-16 manually, by disabling the FCS in the config menu.</p>



<p>You will quickly find that the F-16 is almost impossible to fly manually. Every small disturbance from straight and level flight will create small torques that turn your plane unexpectedly. If you try to correct it with the control stick, you will almost certainly overcorrect and send the plane into a new and exciting attitude. This is called <em>pilot induced oscillation</em>.</p>



<figure><img loading="lazy" decoding="async" width="599" height="400" src="https://vazgriz.com/wp-content/uploads/2025/05/f-16_pio.jpg" alt=""><figcaption>PIO was a problem on the real F-16. Photo of an F-16 test flight (<a href="https://www.youtube.com/watch?v=qAp4RtGKbHE">Source</a>)</figcaption></figure>



<p>It simply isn’t possible for a human to react quickly and precisely enough to fly this aircraft. You may be able to fly straight and level with some effort, but you will quickly lose control if you attempt any maneuver. This is indeed a property of the real F-16.</p>



<p>The textbook provides no Fortran code for the FCS. From here on out, it’s my own original code.</p>



<h2><span id="PID_Controllers">PID Controllers</span></h2>



<p>The steering system from the previous project cannot be reused. The solution is to use PID controllers, a topic I’ve covered on this blog before.<sup data-fn="7c868a94-be19-4150-96d5-52380d04525c"><a href="#7c868a94-be19-4150-96d5-52380d04525c" id="7c868a94-be19-4150-96d5-52380d04525c-link">11</a></sup></p>



<p>To be more specific, steering in the previous flight sim was easy because we could read the angular velocity of the aircraft and apply a torque that directly countered any undesired movement. This F-16 flight model does not allow us to apply torques directly. We can only set the angle of the control surfaces. This is the problem that PID controllers are good at solving.</p>



<p>Adding the PID controllers is simple. The pilot’s control input is used to select a target angular velocity for the plane, for the 3 axes of rotation. This is given to three independent PID controllers. The output of the PID controllers set the target position for the control surface.</p>



<p>The control surface positions are then passed into the flight model inside AerodynamicState.</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">Vector3 targetAV = Vector3.Scale(controlInput, steeringSpeed * steeringSpeedFactor);

var accel = lastAngularAcceleration * Mathf.Rad2Deg * dt;

controlSurfaceTarget = new Vector3(
    pitchController.Calculate(dt, av.x, accel.x, targetAV.x),
    -yawController.Calculate(dt, av.y, accel.y, targetAV.y),
    rollController.Calculate(dt, av.z, accel.z, targetAV.z)
);

var current = ControlSurfaces;

ControlSurfaces = new ControlSurfaces(
    Utilities.MoveTo(current.elevator, controlSurfaceTarget.x, elevatorSpeed, dt, -elevatorRange, elevatorRange),
    Utilities.MoveTo(current.rudder,   controlSurfaceTarget.y, rudderSpeed,   dt, -rudderRange, rudderRange),
    Utilities.MoveTo(current.aileron,  controlSurfaceTarget.z, aileronSpeed,  dt, -aileronRange, aileronRange)
);

...

AerodynamicState currentState = new AerodynamicState {
    controlSurfaces = ControlSurfaces
};

var newState = aerodynamics.CalculateAerodynamics(currentState);</pre>



<p>Here the PIDs are named “pitchController”, “yawController”, and “rollController”. They are all tuned separately to handle a single axis.</p>



<p>When the player releases the stick, the PID controllers will attempt to hold an angular velocity of zero. This makes the aircraft feel like it’s neutrally stable. This also acts as a way to trim the aircraft, so that level flight can be maintained without needing to constantly pull the stick. The PID controller will detect an undesired rotation and move the elevators at a slight angle to counter it.</p>



<p>These PID controllers only add a small amount of complexity to the code, but they achieve similar results as the perfect FCS from the previous project. But there are still limitations that prevent it from being a perfect FCS.</p>



<p>First, the PID controllers must be tuned. The output has to be strong enough to quickly respond to pilot inputs, while avoiding oscillation. This is of course a limitation of any PID control system.</p>



<p>Second, the control surfaces move at a finite speed. This means that it will take some time for the control surface to match the FCS’s commands. So the commands will be imperfectly applied to the aircraft.</p>



<p>Third, unlike the previous flight sim, the three axes of rotation are not independent. For example, a large angle of slip will cause the plane to roll. This is due to the swept wings of the F-16. The roll controller will cancel this out somewhat, but a large enough AOS will result in an uncommanded roll.</p>



<p>Even with these limitations, the PID controllers work fairly well at keeping the plane in control.</p>



<p>Additionally, I use a technique called <em>gain scheduling</em> to change the gain parameters of the roll controller. Because roll performance increases with airspeed, we need a way to limit the amount of aileron movement at high speed. I add two animation curves, which take speed as input, and give the P and D gain of the roll controller as output.</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">rollController.P = rollPSchedule.Evaluate(Mathf.Max(0, LocalVelocity.z));
rollController.D = rollDSchedule.Evaluate(Mathf.Max(0, LocalVelocity.z));

Vector3 fcsTarget = new Vector3(
    pitchController.Calculate(dt, av.x, accel.x, targetAV.x),
    -yawController.Calculate(dt, av.y, accel.y, targetAV.y),
    rollController.Calculate(dt, av.z, accel.z, targetAV.z)
);</pre>



<p>This allows us to change the strength of the roll controller at different speeds. A more advanced FCS might have a gain schedule for each controller, possibly using more inputs than just airspeed. In fact, if there were multiple inputs, we would need a 2D lookup table to calculate the gain schedule.</p>



<p>Because the flight model is a complete description of how the aircraft will respond at different combinations of AOA, AOS, and control input, it is theoretically possible to design an FCS system that perfectly counters all of the unwanted tendencies. However, that is beyond my understanding of aerodynamics and control theory.</p>



<h2><span id="G_and_AOA_Limiter">G and AOA Limiter</span></h2>



<p>The weakness of PID controllers is that they only control the angular velocity of the plane. This is not sufficient to control the plane. The previous project has a G limiter, which is simple since steering torque is applied directly to the aircraft. Adding a G limiter is more complicated with this F-16 flight model.</p>



<p>Additionally, a critical part of the FCS on a real F-16 is the AOA limiter. Just like the G limiter prevents the pilot from creating excessive G-forces while maneuvering, the AOA limiter prevents excessive AOA. This is because the aircraft becomes so unstable at about 28 degrees AOA that even the FCS can not compensate. And importantly, our flight model only supports a limited range of AOA (up to 45 degrees), so if the pilot goes beyond that, the behavior of the simulator becomes nonsensical. So limiting the AOA to about 25 degrees is important for maintaining stable flight.</p>



<p>The previous flight sim project did not have anything like an AOA limiter. I simply tuned the steering strength so that AOA would not exceed about 15 degrees (unless stalling). And even then, there is no instability caused by high AOA, so nothing bad happens if the pilot exceeds that.</p>



<p>We need a system that prevents the pilot from exceeding 25 degrees AOA. This would be implemented as a multiplier on the pilot’s stick input, just like a G limiter. Since there are two limits, we simply select the more restrictive limit using <em>min()</em>. So if the G limiter says to limit input to 0.75 and the AOA limiter says to limit input to 0.5, the value 0.5 is chosen.</p>



<p>Because this flight model uses lookup tables, there is no simple formula for calculating either the G limiter or AOA limiter. The G limiter from the previous project won’t work here. Additionally, the relationship between steering input and AOA is not simple. There is a feedback loop between AOA and lift. As AOA increases, lift increases. But as lift increases, AOA decreases since lift pulls the plane onto a new velocity vector. Not to mention lift also depends on airspeed and altitude.</p>



<p>Luckily, the F-16 flight model is completely disconnected from Unity’s physics system. We can actually run the flight model as much as we want with any inputs, and use the outputs for any purpose. There is the “main” flight model that syncs with a Unity rigidbody. But we can create “side” flight models to predict future behavior of the plane.</p>



<p>I chose to implement the G and AOA limiters by running a side flight model. This side model takes the pilot’s inputs and simulates the aircraft in a simplified world state. In a single physics update, the main flight model runs once, but the side flight model runs multiple times to predict movement several seconds into the future. Because running the flight model is a few lookups and math operations, running multiple times per frame is dirt cheap.</p>



<p>By running this side model, we can determine how the plane would behave if it flew without any limiters. So if the plane is flying fast enough to pull 16 Gs, the side model will report that. We can use that information to calculate the G limiter for the main model.</p>



<p>The side model is contained in the class SimpleTrimmer. The main function Trim looks like this:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">public SimulatedState Trim(float dt, float timeMax, SimulatedState initialState) {
    float time = 0;

    while (time &lt; timeMax) {
        AerodynamicState aeroState = new AerodynamicState() {
            ...
        };

        var aeroForces = aerodynamics.CalculateAerodynamics(aeroState);

        …

        time += dt;
    }

    return state;
}</pre>



<p>It just calls CalculateAerodynamics in a loop with it’s own time variable. The timestep can also be different from the main FixedUpdate loop time step. The variable timeMax controls how far into the future the prediction runs. For example, this side model can run at 0.1 second time steps for 5 seconds total.</p>



<p>After one step of the simulation is run, the state variables are updated and fed back into the next step. The maximum G force and AOA of the whole simulation is recorded.</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">// rotate velocity by pitchDelta
Quaternion newRotation = Quaternion.Euler(0, pitchDelta, 0);
Vector3 newVelocity = newRotation * state.velocity;
newVelocity.y = 0;
newVelocity.z += gravity * dt;
Vector3 velNormalized = newVelocity.normalized;

// assume airspeed magnitude does not change (no drag, no thrust)
state.velocity = velNormalized * airspeed;

state.alpha = Mathf.Atan2(velNormalized.z, velNormalized.x) * Mathf.Rad2Deg;

state.maxAlpha = Mathf.Max(state.maxAlpha, state.alpha);
state.maxAccelerationZ = Mathf.Min(state.maxAccelerationZ, state.acceleration.z);</pre>



<p>This simulation is highly simplified compared to the main flight model. It ignores the pilot’s input except pitch. It ignores angular velocity except for pitch rate. It does not apply drag or any other force that changes airspeed or altitude. It ignores any change to the aircraft’s pitch. Note that the flight model does not care about the orientation of the aircraft to begin with.</p>



<p>The Trim function assumes the pilot is giving a full pitch up or pitch down input and takes the pitch PID controller as a parameter. So this side flight model uses the same PID values as the main model, to prevent the simulation from turning faster than the max turn rate. Since the I term is not used on the PID controller, we can use it without worrying about state.</p>



<p>Gravity as a single float value is also passed as a parameter. This allows the simulation to know how much gravity is affecting the turn on the pitch axis. If the plane is level, this value is 1. If the plane is rolled 90 degrees to the side, this value is 0. If upside down, this value is -1. Gravity on the other axes is ignored.</p>



<p>The larger time step and reduced complexity of simulation means that the side model is not completely accurate to how the plane will fly. But that’s acceptable since we are only using this to estimate the maximum G force and AOA that a turn might create.</p>



<p>After running through a few seconds of simulation on the flight model, the Trim function returns with the max G force and AOA. The FCS then uses these values to calculate the limiting factors for the pilot’s pitch input.</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">SimpleTrimmer.SimulatedState state = simpleTrimmer.Trim(
    trimmerTimeStep,
    trimmerTime,
    initialState,
    maxAV.x * Mathf.Deg2Rad,
    gravityFactor * metersToFeet,
    pitchController,
    centerOfGravityPosition
);

float predictedAlpha = state.maxAlpha;

float predictedG = -state.maxAccelerationZ * feetToMeters;

float aoaPitchMult = CalculateAOALimiter(predictedAlpha);
float gLimit = gLimitPitch; // pitch up limit (ie 8G)

if (controlInput.x &gt; 0) {
    gLimit = this.gLimit; // pitch down limit (ie 4G)
}

float gPitchMult = CalculateGLimiter(predictedG, gLimit);</pre>



<p>The limiting factor for AOA and G force is calculated with a simple function:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">float ApplyLimiter(float value, float limit, float limitStrength) {
    if (limit &lt;= 0) return 1;
    if (value &lt; limit) return 1;

    float error = value - limit;
    error *= limitStrength;

    return limit / (limit + error);
}</pre>



<p>ApplyLimiter returns a factor in the range [0, 1], which is eventually multiplied with the pilot’s control input. This function then used in the limiter functions:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">float CalculateGLimiter(float predictedG, float gLimit) {
    float gForce = predictedG / 9.81f;
    float gPitchMult = ApplyLimiter(gForce, gLimit, gLimitStrength);

    return gPitchMult;
}</pre>



<p>The variable gForce is the predicted max G force from the side model. gLimit is the value chosen as the max G force, for example, 8. If the predicted value is 12, then the variable error will be 12 – 8 = 4. The returned factor would be 8 / (8 + 4) = 8 / 12 = 0.66. limitStrength is used to tune how strongly the error affects the returned limit factor.</p>



<p>If the value is below the limit, the returned factor is 1.</p>



<p>The AOA limiter uses the same function to calculate two limiting factors which are combined:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">float CalculateAOALimiter(float predictedAlpha) {
    float aoaPitchMult = 1.0f;

    aoaPitchMult *= ApplyLimiter(predictedAlpha, predictedAoaLimitMax, predictedAoaLimitStrength);

    float realAOA = AngleOfAttack * Mathf.Rad2Deg;
    aoaPitchMult *= ApplyLimiter(realAOA, feedbackAoaLimitMax, feedbackAoaLimitStrength);

    return aoaPitchMult;
}</pre>



<p>One limit factor depends on the predicted alpha from the SimpleTrimmer class. The other factor depends on the actual alpha value the plane currently has. This can handle cases where the real alpha is much larger than the predicted value, such as when the plane is already stalling.</p>



<p>Then the AOA and G limiter factors are applied to the pilot’s input:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">float aoaPitchMult = CalculateAOALimiter(predictedAlpha);
float gPitchMult = CalculateGLimiter(predictedG, gLimitPitch);

float pitchMult = Mathf.Min(aoaPitchMult, gPitchMult); // select whichever limiter is stronger
float rollMult = rollPitchFactor.Evaluate(Mathf.Abs(controlInput.x)) * rollAOAFactor.Evaluate(AngleOfAttack * Mathf.Rad2Deg);

Vector3 limitedInput = Vector3.Scale(controlInput, new Vector3(pitchMult, 1, rollMult));
Vector3 targetAV = Vector3.Scale(limitedInput, steeringSpeed * steeringSpeedFactor);</pre>



<p>The min() function is used to select whichever limiter factor is strongest. Since I am designing these systems myself, I can tell you there is not a strong reason why I chose min() instead of another multiplication. This is just the formula that felt right when I was testing it.</p>



<p>In fact there are many different ways that the limiting factors could be calculated and combined. I designed the ApplyLimiter function primarily to be easy to tune. These allow me to have separate variables for tuning predicted G, predicted AOA, and feedback AOA limiters.</p>



<p>There is one final limiter above, rollMult. This is controlled by two AnimationCurves, rollPitchFactor and rollAOAFactor. These curves reduce the strength of roll input when the pilot is commanding a strong pitch rotation and when the plane has a high AOA. I added this because rolls felt too sensitive when in a high G or high AOA turn. Tune these to your own taste.</p>



<h2><span id="Stick_Pusher">Stick Pusher</span></h2>



<p>The final system to add is a stick pusher. A stick pusher is a device some aircraft have that physically pushes the stick forward to avoid a stall. This doesn’t exist in the real F-16, even digitally, but who cares? It was quick and easy to write.</p>



<p>If the AOA exceeds some threshold, a bias value is added to the pilot’s stick input to push the nose down. This is different from the AOA limiter above, which multiplies the input by a factor [0, 1]. If the pilot is giving an input of 0, then the AOA limiter has no effect. The stick pusher adds the bias value to the pilot’s input, so it will work even when the pilot gives no input.</p>



<p>The stick pusher will apply when the plane is stalling or if the AOA limiter fails to keep the AOA in the safe range.</p>



<p>The code for this is incredibly simple in concept and implementation:</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">float CalculateAOAPusher() {
    float bias = 0.0f;

    float aoa = AngleOfAttack * Mathf.Rad2Deg;

    if (aoa &gt; stickPusherThreshold) {
        float error = aoa - stickPusherThreshold;
        bias = stickPusherCurve.Evaluate(error);
    }

    return Mathf.Min(stickPusherMax, bias);
}</pre>



<p>If the AOA is over the stickPushThreshold, add a bias to the player’s input. The more it exceeds the threshold, the stronger the bias. At max strength, the stick pusher can give a full nose down input that can’t be overridden by the pilot.</p>



<p>This value is summed with the pilot’s input before running the PID controllers.</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">Vector3 stickPusher = new Vector3(CalculateAOAPusher(), 0, 0);

Vector3 limitedInput = Vector3.Scale(controlInput, new Vector3(pitchMult, 1, rollMult)) + stickPusher;
Vector3 targetAV = Vector3.Scale(limitedInput, steeringSpeed * steeringSpeedFactor);</pre>



<p>With all of these systems added to the FCS, the plane should be very stable to fly now. Since the side model simulation is simplified, the G and AOA limiters are not perfect. They will sometimes result in those parameters being limited at a value too high or too low. But these systems do work accurately enough to keep the plane stable.</p>



<h2><span id="Testing">Testing</span></h2>



<p>Of course any implementation can have bugs. We need to test the flight model to make sure it works. This includes the translation of the Fortran flight model, and the code that implements all of this in Unity.</p>



<h2><span id="Unit_Testing">Unit Testing</span></h2>



<p>Because the flight model is separate from Unity’s physics engine, we can actually test it using normal unit testing techniques. Unity provides a unit testing framework based on NUnit, so testing is pretty typical for C#.</p>



<p>The authors of the textbook also helpfully provide a test case to use. They give the inputs to the model (airspeed, control surface position, throttle, etc) and the expected output of the model (forces, moment, angular velocity, etc). This lets us validate that the model is implemented correctly by running a single step of simulation.</p>



<pre data-enlighter-language="csharp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">// Textbook provides a table of input values and the expected output
// Index    Param       Input           State (X)       Output (XD)
//     1    0.4 (XCG)   0.9 (throttle)  500 (vt)        -75.23724
//     2                 20 (elevator)   0.5 (alpha)    -0.8813419
//     3                -15 (aileron)   -0.2 (beta)     -0.4759990
//     4                -20 (rudder)    -1 (phi)        
//     5                                 1 (theta)      
//     6                                -1 (psi)        
//     7                                 0.7 (P)        12.62679
//     8                                -0.8 (Q)        0.9649671
//     9                                 0.9 (R)        0.5809759
//    10                                1000 (north)    
//    11                                 900 (east)     
//    12                                10000 (alt)     248.1241
//    13                                90 (power)      -58.68999</pre>



<p>Note that the output values for roll, pitch, and yaw, and north, east, and altitude, are not checked in this test. We are using the Unity rigidbody to handle these, so these values are not even calculated in C#.</p>



<p>Additionally, the table lookup operations are fairly simple C# code, so these functions can also be unit tested. I caught a few bugs in the flight model by adding these tests.</p>



<p>All of the tests are in the ModelTestCase class.</p>



<h2><span id="Flight_Testing">Flight Testing</span></h2>



<p>Of course unit testing can only cover so much. The whole point of this project is to create a flight simulator. The only way to know how the flight model really feels is to fly it. So get out there and start flying it!</p>



<p>I have caught a few bugs in the implementation just by flying it and realizing that some aspect felt weird.</p>



<p>In the aerospace industry, test flights are thoroughly instrumented to gather as much data as possible. Force on every axis, angular velocity, pilot input, GPS track, etc is all recorded and stored for future analysis. It’s possible to write automated tests that read this data and check that values stay within expected bounds.</p>



<p>I have done none of that here. Just have fun flying 🙂</p>



<h2><span id="Limitations">Limitations</span></h2>



<p>The flight model defined in the textbook has several limitations.</p>



<p>The effects of alpha on the flight model is only modeled for the range [-10, 45]. Beta is only modeled for the range [-30, 30]. The flight model supports extrapolating data tables beyond their defined ranges, but the returned values will quickly become nonsensical. This means that if you manage to fly the F-16 beyond the provided ranges for alpha and beta, the flight model will break down and begin behaving non-physically.</p>



<p>In some cases, the aircraft will eventually return to controlled flight. But in other cases, one bad data value used to query the tables will cause increasingly bad data to be stored to the plane’s state. These bad values will quickly grow until the plane is thrown to infinity.</p>



<figure><img loading="lazy" decoding="async" width="793" height="542" src="https://vazgriz.com/wp-content/uploads/2025/06/image.png" alt="" srcset="https://vazgriz.com/wp-content/uploads/2025/06/image.png 793w, https://vazgriz.com/wp-content/uploads/2025/06/image-768x525.png 768w" sizes="(max-width: 793px) 100vw, 793px"><figcaption>Pilots call this the “impossible turn”</figcaption></figure>



<p>Hopefully, this is not possible when using FCS that I’ve written. But I encourage any readers to try breaking it themselves.</p>



<p>You can also turn off parts of the FCS using the config menu in the top left corner. This allows you to fly the plane completely manually, turn the engine off, or alter the center of gravity.</p>



<figure><img loading="lazy" decoding="async" width="161" height="106" src="https://vazgriz.com/wp-content/uploads/2025/06/f16_config_menu.png" alt=""></figure>



<p>If the flight model doesn’t bug out from extreme values, then you can actually perform a backflip or a “Kulbit” maneuver with the F-16. I recommend turning off only the pitch axis FCS if you want to try that.</p>



<p>Another limitation is that flaps and slats are not defined in the flight model. The real F-16 uses a single control surface called a “flaperon” that works as both a flap and an aileron. When more lift is needed at low speeds, both flaperons deflect downwards like traditional flaps. Leading edge slats also deflect downwards to increase lift.</p>



<p>The textbook flight model only considers these control surfaces to be ailerons. That is, they always deflect in opposite directions in order to create a roll moment. Only a single “aileron” value is used to represent both left and right, so they cannot be used as flaps. If they were to be used as flaps, then there would need to be a left aileron and right aileron value and the Z axis force coefficient would depend on flaperon position.</p>



<p>The effect of slat position is blended into the existing tables, so there is some effect of slats on Z axis force. But the slat position cannot be animated on the plane’s 3D model since no variable for it exists.</p>



<p>This means that there are reduced high lift devices on the aircraft. The extra lift from flaps cannot be modeled. So the plane’s takeoff speed is much higher than you might expect from the F-16. The textbook only defines a model for <em>flight</em>, not for taxiing or takeoff. Landings feel quite bad because of this.</p>



<p>Another limitation is the lack of landing gear simulation. The landing gear is implemented exactly the same as the previous project: three capsule colliders. There is no simulation of wheel, tire, or suspension behavior. Again, this makes takeoff and landing feel kind of weird. But I have no idea how to write a system like that and it’s out of scope for this project anyways.🤷‍♂️</p>



<p>Another limitation of the flight model is the inaccuracy when flying super sonic. With real planes, lift and drag forces change drastically as you approach Mach 1. Air accelerates as it passes over the wing. Even while the plane remains subsonic, some parts of the air flow are forced to accelerate above Mach 1. When this air reaches supersonic speeds, shockwaves form over the wing which alters the way air flows around it.</p>



<figure><img loading="lazy" decoding="async" width="1920" height="1571" src="https://vazgriz.com/wp-content/uploads/2025/06/Transonic_flow_patterns.jpg" alt="" srcset="https://vazgriz.com/wp-content/uploads/2025/06/Transonic_flow_patterns.jpg 1920w, https://vazgriz.com/wp-content/uploads/2025/06/Transonic_flow_patterns-1024x838.jpg 1024w, https://vazgriz.com/wp-content/uploads/2025/06/Transonic_flow_patterns-768x628.jpg 768w, https://vazgriz.com/wp-content/uploads/2025/06/Transonic_flow_patterns-1536x1257.jpg 1536w" sizes="(max-width: 1920px) 100vw, 1920px"><figcaption>FAA Airplane Flying Handbook</figcaption></figure>



<p>This region, where some air is supersonic and some is not, is called the transonic region. This has a drastic effect on the aircraft’s performance and handling. In particular, the coefficient of drag increases, creating the “sound barrier” effect. The position of lift force on the wing changes, which will change how the plane handles.</p>



<p>None of these effects are included in the textbook’s flight model. These could be modeled by adding another input dimension to the force and moment tables. I suspect these were omitted to keep the flight model simple.</p>



<p>The practical effect is that the flight model only works up to about Mach 0.7. Above that, all of the forces on the aircraft become unrealistic. The behavior of the plane continues to increase smoothly with airspeed as if supersonic effects don’t exist.</p>



<h2><span id="Conclusion">Conclusion</span></h2>



<p>I started this project after I got a job in the aerospace industry. The textbook was recommended by my manager, since I was working on real flight control systems. In a way, this article is a summary of everything I’ve learned about flying and software engineering in that job.</p>



<p>The way this F-16 flight model is implemented is very different from my previous project. It is actually close to how professional level sims are written, though simplified to fit in a textbook. Even so, there are still plenty of limitations in the flight model which means the simulation will behave unrealistically beyond the intended flight envelope.</p>



<p>The authors of the textbook based their flight model on a NASA paper<sup data-fn="39ede4de-9768-4f6e-a777-e5f706c2c887"><a href="#39ede4de-9768-4f6e-a777-e5f706c2c887" id="39ede4de-9768-4f6e-a777-e5f706c2c887-link">12</a></sup> which measured the aerodynamic properties of a scale model in a wind tunnel. The Nasa paper provides 50 lookup tables. The textbook simplified, approximated, and combined these into only 13 lookup tables.</p>



<p>With only a little more effort, you could write a simulator that uses many more tables to cover a larger flight envelope with more detail. The only limit is the data you have access to and your understanding of aerodynamics.</p>



<p>The FCS I’ve written is much simpler than the real FCS. Theoretically, it would be possible to write code that models the real F-16 FCS and apply it to this flight simulator. But how could you even get that information and who would be crazy enough to try that?</p>



<figure><img loading="lazy" decoding="async" width="750" height="1000" src="https://vazgriz.com/wp-content/uploads/2025/06/51M06JIVaLL._AC_UF10001000_QL80_.jpg" alt=""></figure>


<ol><li id="043b6f3b-03c5-49af-a847-588dcf74793a">“Aircraft Control and Simulation” by Brian L. Stevens, Frank L. Lewis, and Eric N. Johnson <a href="#043b6f3b-03c5-49af-a847-588dcf74793a-link" aria-label="Jump to footnote reference 1">↩︎</a></li><li id="4bf1b8e1-c40d-43b7-b010-d8b1043a2423"><a href="https://vazgriz.com/346/flight-simulator-in-unity3d-part-1/">https://vazgriz.com/346/flight-simulator-in-unity3d-part-1/</a> <a href="#4bf1b8e1-c40d-43b7-b010-d8b1043a2423-link" aria-label="Jump to footnote reference 2">↩︎</a></li><li id="b896ed6f-b50c-4b43-9d20-113b9f6b1497"><a href="https://vazgriz.com/467/flight-simulator-in-unity3d-part-2/">https://vazgriz.com/467/flight-simulator-in-unity3d-part-2/</a> <a href="#b896ed6f-b50c-4b43-9d20-113b9f6b1497-link" aria-label="Jump to footnote reference 3">↩︎</a></li><li id="f8f5c94c-2a47-40ee-9825-f1e6583752ae"><a href="https://vazgriz.com/503/creating-a-flight-simulator-in-unity3d-part-3/">https://vazgriz.com/503/creating-a-flight-simulator-in-unity3d-part-3/</a> <a href="#f8f5c94c-2a47-40ee-9825-f1e6583752ae-link" aria-label="Jump to footnote reference 4">↩︎</a></li><li id="149b4841-9ef4-469f-8a98-22bc8a941e7f"><a href="https://x.com/FreyaHolmer/status/1325556229410861056">https://twitter.com/FreyaHolmer/status/1325556229410861056</a> <a href="#149b4841-9ef4-469f-8a98-22bc8a941e7f-link" aria-label="Jump to footnote reference 5">↩︎</a></li><li id="dbd8aeb2-20e9-45d2-8b13-f967c36c6393"><a href="https://commons.wikimedia.org/wiki/File:Speyer_Handlog.jpg">https://commons.wikimedia.org/wiki/File:Speyer_Handlog.jpg</a> <a href="#dbd8aeb2-20e9-45d2-8b13-f967c36c6393-link" aria-label="Jump to footnote reference 6">↩︎</a></li><li id="09917b2e-68b9-4e51-880e-4cfe4f9dfc5a"><a href="https://www.grc.nasa.gov/www/k-12/VirtualAero/BottleRocket/airplane/sound.html">https://www.grc.nasa.gov/www/k-12/VirtualAero/BottleRocket/airplane/sound.html</a> <a href="#09917b2e-68b9-4e51-880e-4cfe4f9dfc5a-link" aria-label="Jump to footnote reference 7">↩︎</a></li><li id="af27d837-c397-472b-ab56-f366283b8cab"><a href="https://en.wikipedia.org/wiki/Bilinear_interpolation">https://en.wikipedia.org/wiki/Bilinear_interpolation</a> <a href="#af27d837-c397-472b-ab56-f366283b8cab-link" aria-label="Jump to footnote reference 8">↩︎</a></li><li id="6e07b8fb-275e-41db-a382-9fe2007f962f"><a href="https://en.wikipedia.org/wiki/Trilinear_interpolation">https://en.wikipedia.org/wiki/Trilinear_interpolation</a> <a href="#6e07b8fb-275e-41db-a382-9fe2007f962f-link" aria-label="Jump to footnote reference 9">↩︎</a></li><li id="a8954d70-ab22-4f44-91ba-707ddd1807a0"><a href="https://aerospaceweb.org/question/aerodynamics/q0194.shtml">https://aerospaceweb.org/question/aerodynamics/q0194.shtml</a> <a href="#a8954d70-ab22-4f44-91ba-707ddd1807a0-link" aria-label="Jump to footnote reference 10">↩︎</a></li><li id="7c868a94-be19-4150-96d5-52380d04525c"><a href="https://vazgriz.com/621/pid-controllers/">https://vazgriz.com/621/pid-controllers/</a> <a href="#7c868a94-be19-4150-96d5-52380d04525c-link" aria-label="Jump to footnote reference 11">↩︎</a></li><li id="39ede4de-9768-4f6e-a777-e5f706c2c887">Simulator study of stall/post-stall characteristics of a fighter airplane with relaxed longitudinal static stability, Nyugen et al, 1979 (<a href="https://ntrs.nasa.gov/citations/19800005879">https://ntrs.nasa.gov/citations/19800005879</a>) <a href="#39ede4de-9768-4f6e-a777-e5f706c2c887-link" aria-label="Jump to footnote reference 12">↩︎</a></li></ol>		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[No reachable chess position with more than 218 moves (294 pts)]]></title>
            <link>https://lichess.org/@/Tobs40/blog/there-is-no-reachable-chess-position-with-more-than-218-moves/a5xdxeqs</link>
            <guid>45382755</guid>
            <pubDate>Fri, 26 Sep 2025 04:47:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lichess.org/@/Tobs40/blog/there-is-no-reachable-chess-position-with-more-than-218-moves/a5xdxeqs">https://lichess.org/@/Tobs40/blog/there-is-no-reachable-chess-position-with-more-than-218-moves/a5xdxeqs</a>, See on <a href="https://news.ycombinator.com/item?id=45382755">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Created by the author using GIMP and freely available images.</p><p><strong>Stop searching, we had it right for 60 years.</strong></p><div><p>Ever since <a href="https://de.wikipedia.org/wiki/Nenad_Petrovi%C4%87">Nenad Petrović</a>, grandmaster of chess composition, published his 218 move composition in 1964, people have tried to come up with a better one. Last month, I joined the hunt and, being a computer scientist, I decided to settle this question once and for all, using computers. You can give it a try yourself. Try to find a position with more moves than the one below.</p>
<p><strong>Spoiler</strong>: You won't.</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=tobs40:ublogBody:RmcRmpJOEzQR:rIwLr8BO.png&amp;w=800&amp;sig=003f79dea9024998ea4db46c4c764ae26cec4efb" alt="image.png"><br>Reachable chess position with 218 moves for White, published by Petrović in 1964.</p>
<p>...but how can we know for sure?</p>
<p>By checking all <a href="https://github.com/lechmazur/ChessCounter">approximately 8.7x10^45</a> reachable chess positions?<br>Yeah that's not gonna happen...</p>
<p>That's 8.7 billion billion billion billion billion and it's enough to scare even the mightiest of supercomputers. In fact, cracking AES-128 encryption would be easier.</p>
<p>Fortunately, we can use the power of Math!</p>
<p>Follow me along and perhaps you can compute yourself a world record afterwards :)</p>
<h2><a href="#using-the-power-of-math" id="using-the-power-of-math"></a>Using the power of Math</h2>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=tobs40:ublogBody:AZJCGj9etF4L:wQdEg0Vn.png&amp;w=800&amp;sig=05a7ba2531a44db54a10d356bdf45846175b6f5f" alt="image.png"><br>Red is the official color of Math, at least in my elementary school.</p>
<p>We're gonna tackle this problem from the white side, i.e., it is White to move. Except for rare exceptions regarding the reachability of positions, this is equivalent.</p>
<p>Since proving that a position is reachable is complicated, we're gonna search through all ways of placing pieces on the board and filter out the non-reachable ones later on, if needed.</p>
<p>Obviously, 99.9% of the positions suck at having lots of moves, they are not even close to 218. We just need a way to skip them and pray that there exists enough electricity to check the rest.</p>
<p>Let's begin with a couple <strong>of</strong> useful observations.</p>
<h4><a href="#useless-pieces" id="useless-pieces"></a>Useless pieces</h4>
<p>A black piece does not improve the number of moves, most of the time. Its existence only benefits the number of moves if it increases White's number of moves, i.e., at least one of the following is true:</p>
<ul>
<li>It can be taken by a white pawn, giving said pawn more moves</li>
<li>It protects the black king from check, making an otherwise illegal position with lots of moves legal</li>
<li>It frees a white piece from being pinned to the white king, thus giving it more moves</li>
</ul>
<p>Otherwise, it is useless, at best, and thus can be removed.</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=tobs40:ublogBody:WasmQmLOrBL6:ULIsaqYx.png&amp;w=800&amp;sig=4156d91f382695716b3739afbac196155f423aad" alt="image.png"><br>I inserted this picture for the sole purpose of making this article look less text-heavy.</p>
<h4><a href="#too-powerful-pieces" id="too-powerful-pieces"></a>Too powerful pieces</h4>
<p>Next, we observe that, if piece counts permit, we can always replace a black piece, with the exception of the black king, with a strictly less powerful one. That is, a piece that has a subset of the moves of the original piece. For example, a queen with a pawn/bishop/rook or a bishop with a pawn. Except if a pawn is on the seventh rank, since in that case, each of its moves <strong>to the promotion square</strong> counts as 4 separata moves. The only way for Black's moves to affect White's number of moves is by pinning White's pieces or preventing the white king from stepping on a certain square. Both of these things can't get worse if the black piece has less moves than before.</p>
<h4><a href="#too-weak-pieces" id="too-weak-pieces"></a>Too weak pieces?</h4>
<p>The other way around, it is not so easy, however. You'd think that you can just replace white rooks and white bishops with white queens if counts permit, but the problem is this: How do you ensure that you cannot capture the black king, making the position illegal? Maybe the optimal solution has a rook instead of a queen to avoid just that?</p>
<p>Well, you might say "Let's just place a black piece in between then".</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=tobs40:ublogBody:siS3rUkB5pwZ:DCkdccwl.png&amp;w=800&amp;sig=0976813b03991d42ecf4a80d300aaf2bc111cb02" alt="Untitled.png"></p>
<p>And you would be wrong unless you can tell me why you haven't just blocked some other white piece and thus reduced the number of moves in total. Or what your plan is, if there is no space to put something in between. You just made the position illegal, duh!</p>
<h4><a href="#no-checks-thank-you" id="no-checks-thank-you"></a>No checks, thank you</h4>
<p>Finally, we can get rid of checks. If the black king is in check, it means that the position is illegal, since it is White's turn to move and they could just capture the black king. Not okay! So that can't be it. On the other hand, if the white king is in check, then the number of White's moves is severely restricted and we can easily prove that we cannot reach 218 moves. There are three ways to get out of check:</p>
<ul>
<li>Move the king</li>
<li>Capture the attacker</li>
<li>Block the attack</li>
</ul>
<p>Moving the king gives 8 moves at best. Since any square can be reached by at most 16 pieces at the same time (8 knights and 8 other pieces straight or diagonally), capturing gives 16 moves at best. We can have at most 6 squares to block an attack, so that's an additional 6 x 16 = 96 moves. So at best 8+16+96 = 120 moves, far less than 218, and thus we do not need to consider positions in which either king is in check.</p>
<p>So is this it Tobi? Can we now call NASA and ask for their supercomputer?</p>
<p>Nope, it's still absolutely hopeless, there are waaaaay too many positions left.<br>We have to skip even more positions.</p>
<h2><a href="#introducing-chess-with-cheating-partial-pieces-and-moves" id="introducing-chess-with-cheating-partial-pieces-and-moves"></a>Introducing Chess with Cheating: Partial Pieces and Moves</h2>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=tobs40:ublogBody:mZvNWBPGH05p:UggWM1vO.png&amp;w=800&amp;sig=07c189e3deb15f59cf05794ae1041d3aa89ac872" alt="image.png"><br>Reminder: Replace rook pictures by something more interesting.</p>
<p>While searching through all possible piece configurations, we would ideally like to have some provably correct way of telling whether we can still reach 218 moves, so we can stop trying and save ourselves an astronomically large amount of work. The better the method is, the more work we save. But it also needs to be fast, since we need to run it millions and billions of times.</p>
<p>A common technique in optimization is to allow fractional decisions. Instead of a piece being either on e4 or not, it can be 27.3% there and 72.7% not there. This enables us to just "swim" through the solution space towards the optimal solution instead of trying all combinations. The drawback is that we usually end up with a way too good solution with most decisions being fractional. But if that doesn't get us beyond 218 moves, we know we can stop trying.</p>
<p>Obviously, these kinds of algorithms are already implemented in state-of-the-art solvers like Gurobi, so all we need to do is model our problem to its likings (as a so-called integer programming problem), tell it to maximize the number of moves and pray. Finally, after ~55 000 seconds, it crashed.<br>Not enough memory, but also hopelessly far away from completing the proof. Extrapolating the runtime led to an estimated runtime of ~6 years. Yeah, let's not go there. Going back to making more observations:</p>
<h4><a href="#checking-more-positions-to-make-things-less-slow" id="checking-more-positions-to-make-things-less-slow"></a>Checking more positions to make things less slow</h4>
<p>To reduce the size of the model, I bent some chess rules, simplifying the search space:</p>
<ul>
<li>I allowed castling if king and rook were on the right squares, not requiring anything else</li>
<li>I stopped caring about pieces moving despite being pinned</li>
<li>I stopped caring whether the white king is in check or walks into check</li>
<li>I allowed white pawns to always capture when standing on the fifth rank so I'd not have to check en passant.</li>
</ul>
<p>All of these things are unlikely to happen in 218+ move positions and after a solution has been found, I can still check and discard it if it has less moves than it claims to have.</p>
<p>I started again and this time, memory wasn't a problem, but the progress was awfully slow with ~29 days remaining. The world could not wait this long and neither could I. I pressed cancel.</p>
<h4><a href="#preventing-white-magic" id="preventing-white-magic"></a>Preventing white magic</h4>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=tobs40:ublogBody:iShBvrU9VQCw:ArUe8Px1.png&amp;w=800&amp;sig=c35b91d1fcfcf2956ddf3aa013c8b34a03026e49" alt="image.png"><br>The optimal fractional solution for the empty board. Most pieces are spread out over multiple squares and have fractional numbers of moves available that sum to ~305. This does prove that the real solution can't have more than 305 moves. Still quite a bit from proving 218 to be the optimum.</p>
<p>Our current way of cheating is too different from reality, causing the solver to have to search through way too many board configurations. In our case, the optimal solution to the easy problem flooded the center with half queens and half knights sharing the same squares, having half pieces move through other half pieces with half moves. Gurobi thinks that the above position has 305 moves in total, which is far away from 218 and a bad upper bound.</p>
<p>In order to cut off this crazy solution, I added a "redundant" constraint, saying that at most one piece in total can move from one direction onto a particular square. Which got us a new hallucination...</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=tobs40:ublogBody:OmGeWp49e01E:kNej345t.png&amp;w=800&amp;sig=ac767d5042739c64da4dd768503de05253fb3043" alt="image.png"><br>The second try. This one has 271 2/3 moves which proves that there is no solution with more than 271 moves.</p>
<p>Now we have 271 2/3 moves, which is much better. It's a bit like having to try all passwords with 53 characters vs. all passwords with 87 characters, the latter being many magnitudes harder.</p>
<p>Wait a minute, White can't have 4 kings.</p>
<p>White does not have four kings. White has 3 times 24.6% kings and one 26.2% king. Also, the queen on g3 barely exists, it's 0.8% but apparently it somehow contributes to the total sum of moves :)</p>
<p>With this improved model, I tried again and after ~23 000 seconds, Gurobi solved it to optimality!</p>
<h2><a href="#results" id="results"></a>Results</h2>
<p>Sadly, instead of finding me a fancy position with 219 moves and making my name immortal, Gurobi gave me the following 12 representative positions (of 40,000 in total) with 218 moves each:</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=tobs40:ublogBody:M7Cw0smh4jQ9:DNPmvada.png&amp;w=800&amp;sig=95ebffed9f88146a5632c919b55f27654667793c" alt="image.png"><br><a href="https://lichess.org/study/PLtuv3v5">Link to the positions</a></p>
<p>All 12 positions seem trivially reachable. I only constructed a proof game for one of the positions, since that is sufficient for proving our claim. If you don't believe that this is possible, keep in mind that White's last or second last move might have been a capture. Or click on the link :)</p>
<p>Sadly not a world record, but at least we now know for certain that 218 is the limit.</p>
<p>And you smart chess move 3.7 bit compression people and chess engine developers, you can finally stop worrying. 256 moves will be enough. You're welcome :-)</p>
<p>Except if you allow non-reachable positions, in which case you might want to read on :P</p>
<h2><a href="#other-stuff-solved-along-the-way" id="other-stuff-solved-along-the-way"></a>Other stuff solved along the way</h2>
<p>I also confirmed the optimality of the 144 move record without promotions. Since Gurobi did not find any position with more than 144 moves, that means that there also is no reachable position with more than 144 moves. Hence, 144 moves is the best we can do and "Jenő Bán", a chess composer from Hungary, found one in 1960 already:</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=tobs40:ublogBody:pa72BRyqBqjr:OdXsn3pg.png&amp;w=800&amp;sig=e3af55f6ed060b75e69e4abe96137a36227315c" alt="image.png"><br>144 moves for White, no promotions. Created by Hungarian chess composer "Jenő Bán". Here is a <a href="https://lichess.org/study/hOSxwpyB/PUHHb9cr">proof game</a>, demonstrating that the position is reachable.</p>
<p>Also, I confirmed the optimality of the following illegal position, which has 288 moves for White.</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=tobs40:ublogBody:2lRWfWA2zNcC:vfeNowSo.png&amp;w=800&amp;sig=db69bc7ef724e8eb3bca5581f6e7ba980eebcb13" alt="image.png"><br>Illegal position with 288 moves for White. Corner queens can be replaced with bishops.</p>
<p>Now have a guess at what the best non-reachable legal position looks like ;)<br>Yep, you're right, cramming the kings into the corners for 271 moves.</p>
<p><img src="https://image.lichess1.org/display?h=0&amp;op=resize&amp;path=tobs40:ublogBody:D3fh622moE3N:zMvHxcKQ.png&amp;w=800&amp;sig=9509e500782247e9ce8499588941f25c6f268c95" alt="image.png"><br>Legal but non-reachable position with 271 moves for White. Corner queens can be replaced with bishops.</p>
<h2><a href="#future-plans" id="future-plans"></a>Future plans</h2>
<p>My code snippet is freely available at <a href="https://github.com/Tobs40/chess218">Github</a>. If you manage to do something cool based on it, please let the world know :)</p>
<p>Fun problems enthusiasts could try to tackle next using similar techniques:</p>
<ul>
<li>Most captures</li>
<li>Most stalemates</li>
<li>Most checks</li>
<li>Most checkmates</li>
<li>Most mates in two</li>
<li>...</li>
<li>Most ... under &lt;condition&gt;</li>
</ul>
<p>Some of these might be hard, extremely hard or practically impossible to solve with current technology. I don't think that integer linear programming is a suitable approach for all of them, one likely has to develop a custom algorithm for computing good upper bounds, based on creative mathematical insights.</p>
<p>Good luck to those who dare to try solving one of these ^^</p>
<p><img src="https://image.lichess1.org/display?fmt=png&amp;h=0&amp;op=resize&amp;path=ublogBody:Bfk358pYOCZw:nElJr2rj.png&amp;w=800&amp;sig=873b091c67b8b8ad17652c34b9640b24e0e5d777" alt="image.png"></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A platform-jumping prince – History of Prince of Persia's 1990s Ports (149 pts)]]></title>
            <link>https://www.jordanmechner.com/en/latest-news/#a-platform-jumping-prince</link>
            <guid>45382645</guid>
            <pubDate>Fri, 26 Sep 2025 04:29:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jordanmechner.com/en/latest-news/#a-platform-jumping-prince">https://www.jordanmechner.com/en/latest-news/#a-platform-jumping-prince</a>, See on <a href="https://news.ycombinator.com/item?id=45382645">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="content" role="main" aria-label="Page Main Content">

		<article>
			<div>
				<h3>A platform-jumping prince</h3>
				<p><time datetime="2025-09-25">⌛ September 25, 2025</time>
			</p></div>
			
		
			<div>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/pop/sega-pop.jpg" alt="">
				</figure>

				<p>"Which is your favorite/definitive version of the original <cite>Prince of Persia</cite> game?"</p>

				<p>I get this question surprisingly often, considering it's been 35 years. I figured it deserves a blog post.</p>

				<h4>Apple&nbsp;II</h4>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/pop/apple2-pop.jpg" alt="">
				</figure>

				<p>The Apple&nbsp;II version was the original. It's the only version I programmed myself; <cite>Prince of Persia</cite>'s gameplay, graphics, animation and music were all created on the Apple&nbsp;II. I spent three years sweating over every byte (from 1986 to 1989), so it's close to my heart in a way no other version can be. That said...</p>

				<h4>DOS/Windows</h4>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/pop/dos-pop.jpg" alt="">
				</figure>

				<p>The 1990 PC version, developed in parallel with the Apple&nbsp;II and shipped a few months later, took advantage of the PC's improved graphics and sound capabilities to deliver the <cite>Prince of Persia</cite> most players remember (in CGA, EGA, or VGA). My dad, Francis Mechner, re-orchestrated his music (previously limited by the Apple&nbsp;II's tinny built-in speaker) for MIDI synthesizers. The Broderbund in-house team, led by programmer Lance Groody, with Leila Joslyn on art, Tom Rettig on sound, and me as director, stayed faithful to the Apple game while upping the quality in every dimension. The digitized spike and slicer sound effects that traumatized many an elementary-school gamer originated with the PC version. If someone asked me the best way to play old-school PoP online today, I'd likely recommend the <a href="https://archive.org/details/msdos_Prince_of_Persia_1990">DOS version</a>.</p>

				<p>In 1990, C-family programming languages were the future, 6502 machine language the past. For good reasons, nearly all subsequent ports of PoP took the PC version as their starting point, rather than the Apple&nbsp;II.</p>

				<h4>Amiga</h4>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/pop/amiga-pop.jpg" alt="">
				</figure>

				<p>The Amiga port was developed by Dan Gorlin (of <a href="https://en.wikipedia.org/wiki/Choplifter">Choplifter</a> fame), in parallel with the PC version, using the graphics and sound assets developed by the Broderbund team.</p>

				<p>Danny was one of my game-author heroes. Playing Choplifter, as a 17-year-old college freshman in 1982, blew me away and set me on the creative path that would lead to <a href="https://www.jordanmechner.com/en/games-movies/karateka/">Karateka</a>. I was star-struck that he agreed to port PoP to Amiga. He did an impeccable job, working alone at home, using the state-of-the-art development system he'd built for his games Airheart and Typhoon Thompson.</p>

				<p>In a detail perhaps mainly interesting to lawyers, Amiga was one of three PoP versions (Apple&nbsp;II and Macintosh were the others) that I was contractually responsible for delivering to Broderbund, rather than their doing the development. This meant me driving to Danny's house for meetings instead of to Broderbund, and that I was on the hook in case the project fell behind schedule or something went wrong. Fortunately, with Danny, all was smooth sailing.</p>

				<h4>Commodore 64</h4>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/pop/c64-pop.jpg" alt="">
				</figure>

				<p>One port that didn't get greenlit was the Commodore 64. Like the Apple&nbsp;II, the C64 had its heyday in the mid-1980s. By 1990, Broderbund (and most U.S. retailers) considered the C64 and Apple&nbsp;II outdated platforms; sales numbers were dwindling by the month. Broderbund couldn't escape publishing PoP on Apple, since it was the lead platform I created the game on, but they had little interest in a C64 version. It would have been a tough port in any case. To fit PoP into 64K of memory, with the Commodore's technical limitations, needed an ace 6502 programmer.</p>

				<p>In a twist I'd never have predicted, an unofficial, fan-made C64 port was finally done in 2011, over 20 years later, and a <a href="https://plus4world.powweb.com/software/Prince_of_Persia">Commodore Plus/4 port</a> just last year. I hope my Apple&nbsp;II source code was helpful.</p>

				<h4>Macintosh</h4>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/pop/mac-pop.jpg" alt="">
				</figure>

				<p>In 1984, Apple unveiled the Macintosh computer (with a now-legendary <a href="https://www.youtube.com/watch?v=2zfqw8nhUwA">Super Bowl ad</a>). Still in college, and flush with <cite>Karateka</cite> royalties, I took advantage of the student discount to purchase a 128K Mac — keeping my Apple&nbsp;IIe for games. (A computer with no lowercase, and enough RAM to hold four pages of text, isn't ideal for writing term papers.) I loved my Mac, and faithfully upgraded my system every time they did: Mac Plus, SE, II, IIci, LC. By 1990, I was proudly Mac-only.</p>

				<p>But the games market was overwhelmingly PC. Broderbund estimated Mac's games market share as 5% of DOS/Windows. Since I believed in the Mac more than they did, it made sense for me to take on the port, as I'd done with Amiga. I subcontracted it to Presage Software, a group of ex-Broderbund programmers I'd known since <cite>Karateka</cite> days.</p>

				<blockquote>
					<p><b>Fun fact:</b> the previous occupant of Presage's San Rafael office was George Lucas's Industrial Light &amp; Magic.</p>
				</blockquote>
	
			</div>
			<div>

				<p>Presage had an excellent, seasoned lead Mac programmer in Scott Shumway; but whereas Danny met his Amiga milestones promptly, Scott's Mac milestones receded like the horizon as they approached. With each new Mac model release — black-and-white, then color, then a different-sized screen — Presage had to redo the bit-mapped PoP graphics for the new configuration. While <cite>Prince of Persia</cite>'s Apple, Amiga and PC versions languished on store shelves (the game wasn't a hit in its first two years), the Mac release date slipped from 1990 to 1991, then to 1992.</p>

				<blockquote>
					<p><b>Fun fact #2:</b> the young graphic artist who up-rezzed the Mac sprites, Mike Kennedy, went on to found the comics imprint Magnetic Press. We met again in 2024, when Magnetic published my graphic novel <a href="https://www.jordanmechner.com/en/books/monte-cristo/">Monte Cristo</a>.</p>
				</blockquote>

				<p>Ironically, the Mac delays turned out to be a blessing in disguise. By the time the port was finally finished, almost two years late, Broderbund marketing had noticed that despite PoP's lackluster U.S. sales, its overseas and console versions were doing surprisingly well. Maybe the game had untapped potential?</p>

				<p>Broderbund took the gamble of combining PoP's Mac release with a PC re-release in a bigger, hourglass-shaped "candy box" designed by the San Francisco firm Wong &amp; Yeo. The dual Mac-PC release in the new box turned the prince's fortunes around. PoP not only became the #1-selling Mac game, it went from ice-cold to hot on PC as well. To 1992 Mac owners who'd been using their machines mainly for work, a game like PoP was a welcome diversion.</p>

				<p>The Mac port was terrific. A sign of its quality is that we adopted its revamped prince (sporting a vest, turban and shoes) for the sequel, <cite>Prince of Persia 2: The Shadow and The Flame</cite>.</p>

				<p>But I still think the original Apple and PC graphics play best. The CRT blur and fat pixels smoothed over animated glitches, enhancing the illusion of life. Higher resolution leaves less to the imagination. (The same can be said of photography and cinema.)</p>

				<h4>Other ports</h4>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/pop/large/pop-ports.jpg"></a>
				</figure>

				<p>Between 1990 and 1993, more computer and console ports of PoP than I can list — Nintendo NES, Game Boy, SEGA Game Gear, Genesis, Master System, Amstrad CPC, Atari ST, NEC PC-9801, FM Towns, Sam Coupé — were developed by teams in Japan, Europe, and elsewhere. Usually, by the time someone handed me a controller to playtest a build, it was too late for my feedback to matter, so I rarely played beyond the first level or two. I don't remember enough specifics of those versions to compare them; I'll leave that to players who know them better.</p>

				<p>There is one unforgettable exception.</p>

				<h4>Super Nintendo</h4>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/pop/snes-pop.jpg" alt="">
				</figure>

				<p>In March 1992, I moved to Paris for a year (to learn French and 16mm filmmaking). Soon after my arrival, a colleague at Activision invited me to visit their office. They showed me the Super Nintendo version of PoP, developed by Arsys and published by NCS in Japan. Activision was lobbying Broderbund for the rights to publish it in Europe and the U.S. It wasn't my call, but they hoped I'd put in a word.</p>

				<p>I wrote in <a href="https://www.jordanmechner.com/en/books/journals/">my journal</a> that day:</p>

				<p>"Wow! It was like a brand new game. For the first time I felt what it's really like to play <cite>Prince of Persia</cite>, when you're not the author and don't already know by rote what's lurking around every corner."</p>

				<p>Arsys had done more than a straight port; they'd expanded the game from 12 levels to 20, adding new enemies, traps, setpieces, and new music. I didn't play all the way through — a half-hour in Activision's office only scratched the surface — but I'll never forget the delighted thrill of being surprised playing my own game. You can see and play it in your browser <a href="https://online.oldgames.sk/play/snes/prince-of-persia/9108">here</a>.</p>

				<p>Elaborate production values and doubled playtime helped make SNES PoP a huge hit. I especially loved the fantastic box artwork by Katsuya Terada.</p>

				<p>A recent feature article in <a href="https://www.timeextension.com/features/like-a-completely-new-game-the-untold-story-behind-prince-of-persias-impressive-snes-port">Time Extension</a> revealed behind-the-scenes details about the SNES development that I hadn't known — including that game producer Keiichi Onogi traveled to the U.S. to visit Broderbund in 1991, hoping to get my feedback. (I missed his visit.) The article is a fascinating time capsule and testament to how special that port was.</p>

				<h4>...And onwards</h4>

				<p>The SNES, so different from the original Apple/DOS version, gave me my first taste of a feeling I would grow used to in decades to come: playing and enjoying new Prince of Persia games that were made by others. With the exception of <a href="https://www.jordanmechner.com/en/games-movies/the-sands-of-time/">The Sands of Time</a> (2003), where I was part of a Ubisoft Montreal team, the more recent modern PoP games don't have my fingerprints on them.</p>

				<p>I suspect that for many reading this post, your answer to "Which is your favorite PoP?" will be the same as mine: Whichever version we played, for hours on end, at a formative age when playing and finishing a game mattered intensely. The real value is in the ingenuity and imagination you brought to the effort, and in your own memories tied to that time.</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/pop/large/jm-c-1988-pop-broderbund.jpg"></a>
				</figure>

				<p>Thanks for reading this post. If you'd like a deeper dive into the story behind <cite>Prince of Persia</cite>'s creation, I've published two books on the subject: my old journals (1985-1993), and my new graphic novel <cite>Replay</cite>. You can check them out <a href="https://www.jordanmechner.com/en/books/">here</a>. Archival materials about PoP (including the Apple&nbsp;II source code) can be found in this website's <a href="https://www.jordanmechner.com/en/library/#pop">Library</a>.</p>

			</div>
			
		
		
		</article>

		<article>
			<div>
				<h3>A Good Old Game</h3>
				<p><time datetime="2025-09-17">⌛ September 17, 2025</time>
			</p></div>
			
		
			<div>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/tle/tatvass.jpg" alt="">
				</figure>

				<p>My 1997 adventure game <a href="https://www.jordanmechner.com/en/games-movies/the-last-express/">The Last Express</a> is 80% off at <a href="https://www.gog.com/game/last_express_the">GOG.com</a> this week (September 17-25) in their Historical Games sale. You can grab the original PC version for just over a buck.</p>
				<p>Eurogamer has a good behind-the-scenes retrospective article, <a href="https://www.eurogamer.net/the-making-of-the-last-express-how-prince-of-persias-jordan-mechner-created-one-of-the-last-great-classic-adventure-games">"The Making of The Last Express"</a>.</p>
				<p>If you want to read deeper, my <a href="https://www.jordanmechner.com/en/library/1993-journals/">ongoing 1990s game-dev journal</a> continues after the Stripe Press book <a href="https://www.jordanmechner.com/en/books/journals/">The Making of Prince of Persia</a> ends. I post a new batch of entries every Wednesday. Today's installment reminds me that 30 years ago in September 1995, our San Francisco team was desperately crunching to meet <cite>The Last Express</cite> target date (hint: we wouldn't), while I postponed my nervous breakdown by mixing the game's cinematics soundtracks, like this one:</p>

				<figure>
					<audio src="https://www.jordanmechner.com/videos/extras/tle-cinematic.mp3" preload="none" controls="controls"></audio>
				</figure>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/tle/yell.jpg" alt="">
				</figure>
	
			</div>
			<div>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/artworks/saturday-1979.jpg"></a>
				</figure>

				<p>Four copies of my signed, limited-edition Apple&nbsp;II-themed art print "Saturday 1979" are still available (as of this moment—prints can go fast). My <cite>Last Express</cite>-inspired artworks "Departure" and "Anna Wolff" are also not yet sold out. If you're looking for a special gift for a retro-gaming collector in your life, check out <a href="https://www.jordanmechner.com/en/artworks/">my artwork gallery</a> here.</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/artworks/departure.jpg"></a>
				</figure>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/artworks/large/anna-wolff.jpg"></a>
				</figure>

			</div>
			
		
		
		</article>

		<article>
			<div>
				<h3>Roguelite September</h3>
				<p><time datetime="2025-09-02">⌛ September 2, 2025</time>
			</p></div>
			
		
			<div>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/roguepop/large/rpop-oasis.jpg"></a>
				</figure>

				<p>A prince who started his career by climbing a garden wall for unauthorized trysts with a princess must be a bit of a rogue, so it makes sense that Evil Empire (developer of <cite>Dead Cells</cite>) stealth-dropped <cite>The Rogue Prince of Persia</cite> on August 20 without alerting the palace guards.</p>
				<p>The game is out now for PC, Xbox and PlayStation, and getting good reviews (<a href="https://www.player2.net.au/2025/08/the-rogue-prince-of-persia-review-a-rogue-like-expedition-worth-taking/">here's one</a>). You can check out the Rogue Prince on <a href="https://princeofpersia.com/">Ubisoft's PoP page</a>, where he coexists alongside the very different, award-winning 2D Metroidvania <cite>Prince of Persia: The Lost Crown</cite> (<a href="https://www.gameshedge.com/prince-of-persia-the-lost-crown-dominates-the-2025-pegases-awards/">4 Pegasus awards</a> including Best Game of the Year!)</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/events/large/celsius232.jpg"></a>
				</figure>

				<p>While the Bordeaux team was crunching to ship their anime-styled game, I've been drawing comics pages the old-fashioned way by hand. July took me to the Festival de Cine in Sax, <a href="https://museoarcadevintage.com/">Vintage Video Arcade Museum</a> in Ibi, and <a href="https://www.youtube.com/watch?v=1UVpb4dB6eY">Celsius 232 festival</a> in Aviles, Spain, where I had the pleasure to discuss my graphic novel <a href="https://www.jordanmechner.com/en/books/replay/">Replay</a> (recently released in Spanish), Prince of Persia, and related topics. Here's <a href="https://www.youtube.com/watch?v=xia9KWkrLF4">one interview</a> (questions in Spanish, my answers in English).</p>
	
			</div>
			<div>

				<h4>Upcoming events</h4>

				<ul>
					<li><strong>London, September 6</strong> — I'll be at the <a href="https://ukftweekendfestival.live.ft.com/">Financial Times Weekend Festival</a> at Kenwood House Gardens, on a video games panel with Holly Gramazio, Jon Ingold, and Tom Faber.</li>
					<li><strong>Blois, October 11</strong> — At Rendez-vous de l'Histoire book festival, discussing my new historical graphic novel trilogy <a href="https://www.jordanmechner.com/en/books/liberty/">Liberty!</a> with artists Etienne Le Roux and Loic Chevallier (Book Three was released in France last week).</li>
					<li><strong>Trieste, October 21</strong> — "Reinventing Monte Cristo — From Dumas to Wall Street" is the title of my talk with artist Mario Alberti at IVIPRO Days. Mario will be there in person, I'll join by video link. Our <a href="https://www.jordanmechner.com/en/books/monte-cristo/">Monte Cristo</a> graphic novel is out now in English, French and Italian, coming soon in Dutch.</li>
					<li><strong>St. Malo, October 24-26</strong> — "Marian," my current graphic novel adventure in collaboration with the great Olivier Vatine, continues with Chapter 2 in this month's issue of <a href="https://editions-blackandwhite.com/">Black &amp; White Stories</a>. If you're at Quai des Bulles BD festival, be sure to catch Olivier at the B&amp;W booth and get your copy signed. We're pre-publishing chapters in serial form, with the full book due in hardcover once the tale is complete.</li>
				</ul>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/marian/large/marian.jpg"></a>
				</figure>

				<p>Now, back to work. More soon!</p>
	
			</div>
			
		
		
		</article>

		<article>
			<div>
				<h3>Liberty! Graphic novel trilogy is complete</h3>
				<p><time datetime="2025-08-26">⌛ August 26, 2025</time>
			</p></div>
			
		
			<div>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/liberty/large/jordan-etienne.jpg"></a>
				</figure>
	
				<p>For those already up to speed, I'll cut to the chase: <cite>Liberty!</cite> Book 3 lands in French comic bookstores tomorrow, August 27. And if you haven't yet discovered my newest graphic novel trilogy—read on! I hope fans of my previous adventures (books, video games, or both) will enjoy this one.</p>
				<p>As an American who lives in France, I cherish the special bond between our two countries. It's a love affair that goes back 250 years, to when the USA was fighting for its survival as a brand-new nation. Outnumbered, outgunned, about to be crushed by an English imperial armada, the American rebels sent a Connecticut businessman named Silas Deane on a secret mission to ask France for support.</p>
				<p>King Louis XVI refused, but Parisian playwright Pierre Caron de Beaumarchais (best known as the author of <cite>The Marriage of Figaro</cite>, less well known as an entrepreneur with a zest for real-life intrigue) was listening. The result was an unlikely odd-couple friendship and an extraordinary covert operation that would change history.</p>
				<p>When I began researching and writing <a href="https://www.jordanmechner.com/en/books/liberty/">LIBERTY!</a>, in collaboration with the wonderful French illustrators Etienne Le Roux, Loic Chevallier, and colorist Elvire DeCock, we didn't foresee how powerfully our graphic novel would resonate with current events. We were at work on Book Two when Russia's army invaded Ukraine in 2022. Our 18th-century characters became agonizingly relatable as we saw their struggles echoed in today's news headlines.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/liberty/panels11.jpg" alt="">
				</figure>
	
			</div>
			<div>
	
				<p>From 1775 to 1778, American citizens lived in a limbo of war and occupation. The English king's refusal to recognize their nation's existence, and scorched-earth military retaliation against people he considered his disloyal subjects, put the rebels in a win-or-die situation. Desperately short of ammunition, the American army had no hope of resisting without aid from Europe. But the French ministers hesitated—constrained by politics, by fear of provoking England, by the whims of their king. If this sounds familiar, it goes to show: History might not repeat itself, but it rhymes.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/liberty/panels12.jpg" alt="">
				</figure>

				<p>I wrote this graphic novel because Beaumarchais and Deane deserve to be remembered. Their story holds inspiration and experience that the world needs now as much as ever. As an entertaining adventure of fascinating personalities in tumultuous times, I hope readers of all ages and nationalities will enjoy and identify with it today. Etienne, Loic and Elvire's artwork brings the world of 18th-century France and the American Revolution to life with spectacular cinematic scope and historical fidelity.</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/liberty/large/panels13.jpg"></a>
				</figure>

				<p><cite>Liberty!</cite> is a complete story in three volumes. The full trilogy, Book 1 - <cite>The Insurgents</cite>, Book 2 - <cite>The Traffickers</cite>, and Book 3 - <cite>The Ambassadors</cite> (all three subtitles describe the same protagonists) can now be purchased in French comic bookstores or online.</p>
				<p>English and other language editions of <a href="https://www.jordanmechner.com/en/books/liberty/">LIBERTY!</a> are in the works. Watch this space for updates and news about future releases.</p>
	
			</div>
			
		
		
		</article>

		<article>
			<div>
				<h3>The Last Newsletter</h3>
				<p><time datetime="2025-08-25">⌛ August 25, 2025</time>
			</p></div>
			
		
			<div>

				<p>The internet evolves fast. For several reasons — including the increasing tendency of mass-mailing platforms (and email providers) to track user data, which I don't want my website to do — I've decided to retire my monthly email newsletter. If you're a subscriber, you should receive a farewell thank-you email from me tomorrow, then I'll leave you in peace.</p>
				<p>Going forward, I'll post announcements and updates about my creative projects here in Latest News. You can stay notified by subscribing to the jordanmechner.com <a href="https://www.jordanmechner.com/en/feed/">RSS feed</a>. (If you're reading this in your RSS reader or email inbox right now, congratulations — you're using the cleanest, most efficient, algorithm-free, privacy-friendly content delivery method I know of.)</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/technology/large/rss.jpg"></a>
				</figure>

				<p>Social media users can also follow me on the <a href="#social">platforms listed in the footer</a> of the jordanmechner.com home page. I sometimes (when the mood strikes) post on Bluesky, Mastodon, Instagram, Facebook, Youtube, and/or the platform formerly known as Twitter.</p>

			</div>
			<div>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/technology/large/news.jpg"></a>
				</figure>

				<p>Most importantly and most consistently, you can always find my current news and updates here at jordanmechner.com. For the past 15 years we've built and maintained this website by hand, the old-school way, to keep it clean, fast-loading, and free of third-party plug-ins, ads or cookies. It's packed with both new and archival content about Prince of Persia and my other projects, past and present. I encourage you to explore it at your leisure.</p>
				<p>A reminder for Francophones that we have a parallel site, jordanmechner.com/fr (just click the French flag in upper right), with customized content, hand-translated by a bilingual French speaker on our team (not AI). Tomorrow, I'll have a new announcement especially for French readers.</p>
				<p>Watch this space. À demain!</p>

			</div>
		
		
		</article>

		<article>
			<div>
				<h3>Five artworks</h3>
				<p><time datetime="2025-01-27">⌛ January 27, 2025</time>
			</p></div>
			
		
			
			<div>

				<p>Last spring, I received an unexpected invitation. Jules Maeght, a printer and gallerist in Paris, had seen my sketchbooks online, and wondered if I'd ever tried copperplate etching?</p>
				<p>I hadn't. For my 60th birthday, I blocked off four days in the calendar, and took up Jules' kind offer to experiment in his family's <a href="https://www.maeght.com/imprimerie-maeght/" target="_blank">printing shop</a> on the Left Bank, at 13 rue Daguerre. Looking up the metro route to get there, I thought the address sounded familiar— but from where?</p>
				<p>As readers of my graphic novel <a href="https://www.jordanmechner.com/en/books/replay/">Replay</a> know, my grandfather Papi (a doctor) left a voluminous family memoir. His sister, Else Mechner, had been a painter in Paris in the 1920s. I checked the manuscript. Else had lived at 11 rue Daguerre, literally next door to the print shop I was now headed for.</p>
				<p>Over our first coffee at Imprimerie ARTE, Jules and his crew showed me the view from their upstairs window. Across the courtyard (shared with a boulangerie, on a street bustling with food markets) stood the building from whose windows my great-aunt Else must have looked out at theirs a century earlier.</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/artworks/jordan-rue11.jpg"></a>
				</figure>

				<p>That morning, I sat in the courtyard and drew my view of Else's atelier, with a stylus directly on a copper plate. In the afternoon, we added two shades of aquatint (a process in which the printer turns a hand crank in a wooden box, letting a cloud of powder settle onto the varnished plate). The result was my first etching, <a href="https://www.jordanmechner.com/en/artworks/11-rue-daguerre/">11 rue Daguerre</a>.</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/artworks/rue11.jpg"></a>
				</figure>

				<p>Jules' instinct had been right. I was hooked.</p>

				<p>For my second print, I chose a panel from <cite>Replay</cite> depicting Else as a schoolgirl, surreptitiously sketching passengers on the streetcar in her hometown of Czernowitz. (The sketching habit runs in the family; my daughter Jane and I both inherited it.) My cartoon is based on a <a href="https://www.jordanmechner.com/en/library/replay-annex/chapter4/#page-112-streetcar">more famous one</a> by the great French caricaturist Daumier (1808-1879). A laser-engraving machine transferred a JPEG scan of my line art to a copper plate, hopping from 19th- to 21st-century technology and back again. I titled this homage to Else and Daumier <a href="https://www.jordanmechner.com/en/artworks/streetcar-1910/">Streetcar 1910</a>.</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/artworks/copperplates.jpg"></a>
				</figure>

			</div>
			<div>

				<p>My third artwork depicts a young woman walking down an Orient Express train corridor, in the Belle Epoque period when Else was an art student in Vienna. Players of my 1997 game <cite>The Last Express</cite> will know why I titled this print <a href="https://www.jordanmechner.com/en/artworks/anna-wolff/">Anna Wolff</a>. The grid layout of rectangular panels evokes the rotoscoping process our team used to create the animation thirty years ago, as well as the French comics and Art Nouveau lithographs that inspired us. The ARTE workshop, where craftsmen hand-ink copper plates and roll them through iron printing presses, was a perfect place to bring the game's mix of old and new technology full circle, back to ink on paper.</p>
				<p>Print number four is inspired by another video game moment. In <a href="https://www.jordanmechner.com/en/artworks/shadow/">Shadow</a>, I've pictured a climactic confrontation between <cite>Prince of Persia</cite>'s young hero and his mysterious nemesis. The computer memory limitations that led to Shadowman's birth in 1988 (as described in this <a href="https://www.youtube.com/watch?v=6ozxnrs0BP4" target="_blank">ArsTechnica video</a>, and in <cite>Replay</cite>) are now history, but the insight that technical constraints can spark creative breakthroughs stayed with me. This copperplate etching with aquatint is adapted from a watercolor I did last year. (The original hangs on the wall of game developer Neil Druckmann, who co-created <cite>The Last of Us</cite>.)</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/artworks/shadow-copperplate.jpg"></a>
				</figure>

				<p><a href="https://www.jordanmechner.com/en/artworks/dome-house/">Chappaqua — Dome House</a> (drawn directly on copperplate) shows my childhood home, where I played and programmed my first computer games on a 1970s Apple&nbsp;II, in the woods north of New York City. It's where <cite>Prince of Persia</cite> (and many of my other projects) began.</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/artworks/dome.jpg"></a>
				</figure>

				<p>These five prints will be shown at <a href="https://www.maeght.com/galerie-maeght/" target="_blank">Galerie Maeght</a> (42 rue du Bac) in Paris next Thursday evening, 6 February 2025, along with my sketchbooks and other recent work. I'll be there from 5-8 p.m. to chat (and sign books) in a very pleasant setting, with French wine on hand. If you're in the neighborhood, please stop by and say hello.</p>
				<p>For those unable to come on Thursday, the artworks can be seen (and purchased) online at Galerie Maeght, and <a href="https://www.jordanmechner.com/en/artworks/#etchings">at my website</a>. The limited editions of 35 or 40 are split between the two sites; if you find a print sold out at one, try the other. Both ship internationally.</p>
				<p>Else Mechner's artistic career was cut short when she returned to Czernowitz in 1931, leaving her paintings behind. Hitler, Stalin and World War II ensued; she never saw Paris again. Thirty years later, after Else's death, Papi tracked down her former landlord at rue Daguerre, who agreed to ship the canvases to New York in exchange for the unpaid rent.</p>
				<p>One of her paintings, entitled "<a href="https://www.jordanmechner.com/en/library/replay-annex/chapter1/#page-24-else-mechner-s-paintings">The Prince</a>," impressed me particularly as a child; Papi often showed it to me when I visited. His love and pride in his sister's work was evident. I think he'd have been happy about the unexpected chain of events that led his grandson back to her Paris atelier and inspired new artworks in her honor. I'm sure he'll be there in spirit next Thursday.</p>

			</div>
			
		
		
		</article>

		<article>
			<div>
				<h3>Prince of Persia turns 35</h3>
				<p><time datetime="2024-09-26">⌛ September 26, 2024</time>
			</p></div>
			
		
			<div>

				<p>Next week marks the 35th anniversary of <cite>Prince of Persia</cite>'s release in 1989. PoP players have a lot to celebrate this year: two brand-new games released (<cite>The Lost Crown</cite> and <cite>The Rogue Prince of Persia</cite>), <cite>The Sands of Time</cite> remake confirmed and announced for 2026, and a <a href="https://www.ubisoft.com/en-us/game/prince-of-persia/the-lost-crown">new DLC for <cite>The Lost Crown</cite></a> that dropped last week.</p>
				<p>Today, I have something personal to share with old-school PoP fans. 35 years ago, artist Robert Florczak painted a lovely oil illustration for <cite>Prince of Persia</cite>'s original "red box." My fondness for that first box art inspired me to draw this tribute in my own style.</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/artworks/redbox.jpg"></a>
				</figure>

				<p>"Red Box" is the 8th in a series of limited-edition artworks I've created as a companion to <a href="https://www.jordanmechner.com/en/books/replay/">my graphic novel memoir REPLAY</a> (published earlier this year by First Second Books), in which I recount my game-development adventures and family story. <a href="https://www.jordanmechner.com/en/artworks/redbox/">Signed prints are available</a> exclusively from my website.</p>

			</div>
			<div>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/artworks/jordan-and-david.jpg"></a>
				</figure>

				<p>Happy 35th birthday to the prince — and happy 54th to my brother David, who modeled the game's rotoscoped animation at age 15, and is celebrating his birthday today!</p>

				<figure>
					<iframe width="100%" height="251" data-src="https://www.youtube.com/embed/AbbX_Mq-gdg?rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe>
				</figure>

				<p>I was 25 when PoP shipped on Apple&nbsp;II. I couldn't have imagined then that such an incredible, enduring community of players and teams would share the adventure for decades to come. My heartfelt thanks to all Prince of Persia fans for keeping the flame alive. Your dedication and skill has saved the kingdom, time after time. I'm deeply grateful.</p>

			</div>
			
		
		
		</article>

		<article>
			<div>
				<h3>Monte Cristo coming in English!</h3>
				<p><time datetime="2024-03-26">⌛ March 26, 2024</time>
			</p></div>
			
		
			<div>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/mc/mc.jpg" alt="">
				</figure>

				<p>I'm excited to announce that my graphic novel <cite>MONTE CRISTO</cite> — a modern update of Alexandre Dumas' classic novel transposed to post-9/11 America, brilliantly illustrated by Mario Alberti — will be published in English by Magnetic Press later this year.</p>
				<p><strong>Our Kickstarter begins today</strong> and will run through April 12. You can preorder the English edition (hardcover or digital, 224 pages), comprising the complete trilogy originally published by Glénat in French, along with various signed, limited-edition add-ons and bundles, from the <a href="https://www.kickstarter.com/projects/neurobellum/418331324?ref=1s8b6x&amp;token=c19c37fc">Kickstarter page</a>.</p>
				<p>The French website Les Sentiers de l'Imaginaire, in its review, wrote: "Mario Alberti's work is remarkable in every way... Resolutely modern... Romantic, captivating and truly original. Jordan Mechner appropriates this masterpiece by Alexandre Dumas to create an intelligent and daring adaptation."</p>
				<p>Last week, <a href="https://www.jordanmechner.com/en/books/replay/">my graphic novel memoir <cite>Replay</cite></a> was published in English by First Second Books. I'm delighted to now be able to share <a href="https://www.jordanmechner.com/en/books/monte-cristo/">Monte Cristo</a>, as well, with English-speaking fans who've enjoyed my video games like <cite>Prince of Persia: The Sands of Time</cite> and <cite>The Last Express</cite>. I'm excited for you to discover what I've been up to lately.</p>

			</div>
			
		
		
		</article>

		<article>
			<div>
				<h3>Replay is here!</h3>
				<p><time datetime="2024-03-19">⌛ March 19, 2024</time>
			</p></div>
			
		
			<div>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/replay/large/replay-open.jpg"></a>
				</figure>

				<p>Four years ago, I embarked on the greatest creative adventure of my life — writing and drawing all 320 pages of my graphic novel memoir, <cite>REPLAY</cite>. Today, I'm thrilled to announce <cite>REPLAY</cite>'s arrival in U.S. bookstores, in a beautiful hardcover English edition from First Second.</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/replay/replay-here.jpg"></a>
				</figure>
				
				<p>I'll be celebrating the book launch tonight (Tuesday 3/19, 5:30 pm) at Book Passage in San Francisco.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/replay-jordan.jpg" alt="">
				</figure>

			</div>
			<div>

				<p>You can order <cite>Replay</cite> now (and see a preview and trailer) on my website's <a href="https://www.jordanmechner.com/en/books/replay/">Replay page</a>.</p>
				<p>If you're in the neighborhood of San Francisco (or Los Angeles, or New York), please come say hello at one of my upcoming events! I'll be delighted to personally sign your book.</p>
				<p>Upcoming events (Register free online):</p>
				<ul>
					<li>San Francisco — <a href="https://www.bookpassage.com/event/jordan-mechner-replay-memoir-uprooted-family-ferry-building-store">Book Passage</a> — Tuesday 3/19</li>
					<li>Los Angeles — <a href="https://www.dieselbookstore.com/event/Jordan-Mechner-Author-signing">Diesel Bookstore</a> — Tuesday 3/26</li>
					<li>New York — <a href="https://www.greenlightbookstore.com/event/jordan-mechner-burkhard-bilger">Greenlight Bookstore</a> — Thursday 4/4</li>
				</ul>
				<p>All three bookstores also offer signed copy preorders online. (Greenlight can ship within the U.S., Book Passage and Diesel internationally.)</p>
				<p>And if you're attending the <a href="https://schedule.gdconf.com/session/classic-game-postmortem-karateka/899233">Game Developers Conference</a>, I'll be giving a talk on Thursday, followed by GDC bookstore signing at 1 pm. I'll also be speaking in coming weeks at USC (3/25) and <a href="https://tisch.nyu.edu/game-center/events/2024/JordanMechnerBookTalk">NYU</a> (4/2).</p>
				<p>No matter where you are, you can join me for an <a href="https://www.eventbrite.com/e/book-talk-replay-tickets-852639165697">online book talk</a> (Wednesday 3/27, 10 am PT / 1 pm ET) with gaming historian Chris Kohler, hosted by the Internet Archive and Authors Alliance.</p>

				<h4>Share the journey</h4>

				<p><cite>Replay</cite> interweaves my life as a game developer with my dad's flight from Vienna as a child refugee in 1938-41 through Nazi-occupied France, and my grandfather's back story as an Austrian teenage soldier in World War&nbsp;I. I've made three short video trailers (evoking the three bands on the book's cover) to offer a glimpse inside. Here's #3:</p>

				<figure>
					<iframe width="100%" height="251" data-src="https://www.youtube.com/embed/a9tLFxzHNnM?rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe>
				</figure>

				<p>I'm excited for you to discover <cite>Replay</cite>. I hope it will speak to you. Please feel free to share any of the content on my <a href="https://www.jordanmechner.com/en/books/replay/">Replay</a> page so that others can discover it too, and don't forget to tag me (@jmechner) so I can thank you! I can't wait to see <cite>Replay</cite> arrive in your hands — the culmination of its creative journey — and to hear what you think.</p>

			</div>
			
		
		
		</article>

		<article>
			<div>
				<h3>Replay by the bay</h3>
				<p><time datetime="2024-02-22">⌛ February 22, 2024</time>
			</p></div>
			
		
			
			<div>

				<p>I'm excited to announce that my graphic novel memoir <a href="https://www.jordanmechner.com/en/books/replay/">Replay</a> will make its U.S. debut on Tuesday, March 19, at Book Passage in the San Francisco Ferry Building. I'll be there at 5:30 pm for a talk and <a href="https://www.bookpassage.com/event/jordan-mechner-replay-memoir-uprooted-family-ferry-building-store">book signing</a>. If you're near the neighborhood, please stop by!</p>
				<p>For those attending the Game Developers Conference that week, I'll be giving <a href="https://schedule.gdconf.com/session/classic-game-postmortem-karateka/899233">a retrospective talk</a> about the making of my first game, <cite>Karateka</cite>, on Thursday at 11:30. (Digital Eclipse's wonderful in-depth interactive, <a href="https://www.jordanmechner.com/en/games-movies/karateka/">playable documentary</a> on that subject is nominated for a GDC Innovation Award.) <cite>Replay</cite> book signing will follow at the GDC bookstore starting at 1 pm.</p>
				<p>And if you're far from San Francisco, you can still <a href="https://www.jordanmechner.com/en/books/replay/">pre-order <cite>Replay</cite></a> and receive your copy by launch day, March 19. (The French edition is already in bookstores and available online.) My next events will be in Los Angeles and New York; I'll post details on social media as soon as those are set.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/replay-la-panel1.gif" alt="">
				</figure>

				<h4>Memoir of a family</h4>

				<p>Since <cite>Replay</cite>'s release last year in France (where it's received awards including the 2023 "Chateau de Cheverny" graphic novel prize), I've been eagerly counting down the months — now weeks — until I can share it with my friends, family and readers in English.</p>
				<p><cite>Replay</cite> interweaves the story of my life as a game developer (making <cite>Prince of Persia</cite>, <cite>Karateka</cite> and <cite>The Last Express</cite>) with my dad's flight from Vienna as a child refugee in 1938-41 through Nazi-occupied France, and my grandfather's back story as an Austrian teenage soldier in World War&nbsp;I. It's a very special, personal work for me, my first graphic novel as writer-artist. You can read more about it, reviews and excerpts, on my website's <a href="https://www.jordanmechner.com/en/books/replay/">Replay</a> page.</p>
				<p>The U.S. edition is essentially identical to the French one, but in hardcover. (It's not a translation; I wrote and drew it first in English.)</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/replay/replay-hardcover.jpg"></a>
				</figure>

			</div>
			<div>

				<p>For those who'd like to get a signed copy, but can't make it to an in-person event in San Francisco, New York or L.A. next month, you can still early-order a signed copy of <cite>Replay</cite> through March 1. After that, signed copies will be available from bookstores where I do in-person signings, but no longer from the website.</p>

				<h4>Share the journey</h4>

				<p>If you're excited about checking out <cite>Replay</cite>, I'd like to ask you a favor. Pre-orders are a critical part of a book's success, and it really does matter when you</p>

				<ul>
					<li><a href="https://www.jordanmechner.com/en/books/replay/">Pre-order the book</a> (Links to various sellers are here)</li>
					<li><a href="https://www.jordanmechner.com/en/books/replay/#share">Share the news</a> (using one of the images below, or in your own way)</li>
					<li><a href="https://www.amazon.com/Replay-Memoir-Uprooted-Jordan-Mechner/dp/1250873754?tag=jordan-25">Leave a review</a> on Amazon.</li>
				</ul>

				<figure>
					<img src="https://www.jordanmechner.com/images/pages/replay/share/replay-ordered.jpg" alt="">
				</figure>
				<figure>
					<img src="https://www.jordanmechner.com/images/pages/replay/share/replay-dream.jpg" alt="">
				</figure>
				<figure>
					<img src="https://www.jordanmechner.com/images/pages/replay/share/replay-apple.jpg" alt="">
				</figure>
				<figure>
					<img src="https://www.jordanmechner.com/images/pages/replay/share/replay-bones.jpg" alt="">
				</figure>

				<p>Your support makes a huge difference to the launch; I'm truly grateful. Most importantly, I hope you'll discover <cite>Replay</cite>, enjoy it and find it meaningful — and perhaps, that it will speak to you personally and resonate with your own memories.</p>

			</div>
			
		
		
		</article>

		<article>
			<div>
				<h3>Saturday 1979</h3>
				<p><time datetime="2024-02-20">⌛ February 20, 2024</time>
			</p></div>
			
		
			<div>

				<p>Today, I'm releasing a new artwork, "Saturday 1979," depicting the living room of my childhood home and a very special computer — the Apple&nbsp;II, where my earliest gaming memories were created. This is the machine on which I learned to program in BASIC, then went on to develop my first games, <cite>Karateka</cite> and <cite>Prince of Persia</cite>.</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/artworks/saturday-1979.jpg"></a>
				</figure>

				<p>I drew this scene remembering the endless weekends and after-school hours I spent playing games like <cite>Breakout</cite>, <cite>Star Trek</cite>, and <cite>Space Invaders</cite>, and trying to unlock the fascinating new machine's powers. Readers of <a href="https://www.jordanmechner.com/en/books/replay/">my graphic novel memoir <cite>Replay</cite></a> will recognize the kid sitting next to me as my brother David, whom I drafted into service to <a href="https://www.youtube.com/watch?v=AbbX_Mq-gdg">model the <cite>Prince of Persia</cite> animation six years later</a>, in 1985.</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/artworks/saturday-1979-preparing.jpg"></a>
				</figure>

			</div>
			<div>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/artworks/saturday-1979-closeup.jpg"></a>
				</figure>

				<p>"Saturday 1979" is available as a high-quality giclée print in a signed, numbered and hand-stamped limited edition of 20, <a href="https://www.jordanmechner.com/en/artworks/saturday-1979/">exclusively here</a>. As with previous prints, once the edition is sold out, it will not be reprinted; this protects its value for collectors.</p>

				<h4>Four weeks to <cite>Replay</cite></h4>

				<p>My video-game inspired <a href="https://www.jordanmechner.com/en/artworks/">artwork series</a> is a companion project to my new graphic novel memoir <cite>Replay</cite>, which interweaves my four decades in the game industry with three generations of my family's story. The nostalgic retro-gaming moment of "Saturday 1979" is also evoked in the book (pages 5 and 26).</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/artworks/replay-arriving.jpg"></a>
				</figure>

				<p>The English edition of <cite>Replay</cite> will be released by First Second Books on March 19, 2024 (in four weeks). You can pre-order it now, read reviews, or early-order a signed edition, <a href="https://www.jordanmechner.com/en/books/replay/">on the <cite>Replay</cite> book page</a>.</p>

			</div>
			
		
		
		</article>

		<article>
			<div>
				<h3>Game on! The Lost Crown awaits</h3>
				<p><time datetime="2024-01-16">⌛ January 16, 2024</time>
			</p></div>
			
		
			<div>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/poptlc/poptlc.jpg" alt="">
				</figure>

				<p>At last, it's here! <a href="https://www.princeofpersia.com/">Prince of Persia: The Lost Crown</a> releases this Thursday, January 18 for Nintendo Switch, PlayStation, PC, and Xbox.</p>
				<p>I started playing the early-access Deluxe edition when it dropped 48 hours ago, and I'm already hooked and irrevocably immersed. I've been eagerly looking forward to this adventure for many reasons:</p>

				<ul>
					<li>It's a long-awaited new beginning for a franchise that's been close to my heart for almost four decades. (It <a href="https://www.jordanmechner.com/en/games-movies/prince-of-persia/">started in 1985</a> with me videotaping my 15-year-old brother <a href="https://www.youtube.com/watch?v=AbbX_Mq-gdg">running and jumping in his pajamas</a> in our high school parking lot.)</li>
					<li><cite>The Lost Crown</cite> is made by a wonderful, talented team of friends and colleagues in Montpellier, France, where I live. I've worked with several of them on previous POP projects, and know their passion and dedication to POP first-hand.</li>
				</ul>

				<p>Usually when I hear reviewers remark that a game is "deep" and "challenging," I experience a slight bit of terror. As much as I delight in conceiving devious puzzles for others, my own gamer skills are far from hardcore. (Ask anyone who's ever been on a development team with me.) But within the first hour of taking controller in hand as young warrior Sargon, I found myself enthralled by this team's fresh vision of ancient Persia, and determined to fulfill my epic mission to save a prince. Challenge accepted! The glowing reviews are justified:</p>

				<blockquote>
					<p><strong>IGN:</strong> "It's such a great fit that I'm scratching my head wondering how this franchise and genre never got together before... A surprisingly deep, no-nonsense Metroidvania that looks set to get our gaming year of 2024 off to a good start."</p>
				</blockquote>

				<blockquote>
					<p><strong>WellPlayed.com:</strong> "In a beautiful melding of Prince of Persia's competing gameplay identities, <cite>The Lost Crown</cite> invokes the Metroidvania genre, allowing it to craft a challenging 2.5D platformer and compelling combat experience all at once... It was endlessly fun, something unlocking in the back of my mind that finally allowed me to understand the innate appeal of this genre."</p>
				</blockquote>

				<blockquote>
					<p><strong>EuroGamer:</strong> "It might just be my favorite thing the publisher has released in the last ten years... <cite>The Lost Crown</cite> begins a new chapter in the storied history of Prince of Persia — and for Ubisoft Montpellier, the studio behind the game."</p>
				</blockquote>

			</div>
			<div>

				<blockquote>
					<p><strong>DigitalTrends:</strong> "It's just so pleasurable to actually play... The platforming is a smooth callback to Prince of Persia's 2D roots while peppering in more complex movements... It nails everything that a good Metroidvania needs to succeed and goes even further with more story and gameplay systems. All of this gives <cite>The Lost Crown</cite> the ingredients it needs to be a genre classic."</p>
				</blockquote>

				<figure>
					<iframe width="100%" height="300" data-src="https://www.youtube.com/embed/5wh2r7kKKjM?rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe>
					<!-- <a href="/images/news/poptlc/poptlc-forestwarriors.jpg"><img src="/images/news/poptlc/poptlc-forestwarriors.jpg" class="illustration image-popup" alt="" /></a> -->
					<a href="https://www.jordanmechner.com/images/news/poptlc/poptlc-platforming.jpg"></a>
				</figure>

				<p>I'm especially glad that <cite>The Lost Crown</cite> has such an authentically Persian flavor — from the stylish visuals and music to the mythological underpinnings. (I'm playing it in Persian, of course, with subtitles.) Timeless elements of Persian culture like Simurgh, Mount Qaf, and Athra that I've long wished to see in a Prince of Persia game are woven into the universe within a coherent gameplay and story framework. I can't wait to discover everything the team has put into it.</p>
				<p>And for well-informed POP fans and readers who may be wondering: No, <cite>The Lost Crown</cite> is not the 2D Prince of Persia game whose development I recount in my graphic novel memoir <a href="https://www.jordanmechner.com/en/books/replay/">Replay</a>. That was a different, ultimately cancelled project, also based in Montpellier. <cite>The Lost Crown</cite> rose from its ashes, as befits a Persian phoenix.</p>
				<p>For those curious about the full story — and back story — of my personal involvement with Prince of Persia through its 35 years and many iterations, <cite>Replay</cite> will be released in English on March 19, 2024. You can <a href="https://www.jordanmechner.com/en/books/replay/">pre-order it here</a> (including a limited signed edition). For French readers, <cite>Replay</cite> is already in bookstores!</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/poptlc/replay-pop-combat-en.gif"></a>
				</figure>

				<p>Now it's time for me to sign off, pick up the controller and continue developing my skills to see Sargon through the adventures that await. See you on Mount Qaf!</p>

			</div>
			
		
		
		</article>

		<article>
			<div>
				<h3>Happy Holidays!</h3>
				<p><time datetime="2023-12-07">⌛ December 7, 2023</time>
			</p></div>
			
		
			<div>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/artworks/happyholidays-fullres.jpg"><img src="https://www.jordanmechner.com/images/news/artworks/happyholidays.jpg" alt=""></a>
				</figure>

			</div>
			<div>

				<p>A huge thanks to everyone who has followed my creative projects and supported me in 2023. It's been a rare delight and privilege to share so many new releases in a single year.</p>
				<p>As an author and game developer, I'm used to spending most of my working life behind the scenes on not-yet-announced projects. When the time comes to unveil the results, and meet and talk with the people I make things for, it's an occasion to celebrate. I've been especially touched by the heartfelt sentiments shared by readers of <cite>Replay</cite> and fans of <cite>Prince of Persia</cite> this year, sometimes including personal childhood memories that remind me how far our actions can ripple into the future.</p>
				<p>I hope the coming year will let you be together with the people you love, and share things that bring you happiness. I wish you the best for 2024, including the chance to pursue your own creative work, to enjoy the games, films, books and art that inspire you, and to spend time with your friends and family.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/jm-dedicaces-narbonne-cropped.jpg" alt="">
				</figure>

				<p>This year's French release of my graphic novel memoir <a href="https://www.jordanmechner.com/en/books/replay/">Replay</a> — my first book as both writer and visual artist, and the most personal project I've done — led to memorable encounters at book signings, talks and festivals throughout France. I've been moved by the overwhelming warm response, including the great honor of the 2023 "Chateau de Cheverny" history graphic novel prize awarded to <cite>Replay</cite>, and a second printing of the book. <cite>Replay</cite> is especially meaningful to me because it tells the story of three generations of my family (including my father's odyssey as a child refugee in World War II Europe), interwoven with my own story making video games like <cite>Prince of Persia</cite> and <cite>The Last Express</cite>. I can't wait to start sharing it with readers worldwide this spring, when First Second/Macmillan will publish the English edition (on March 19, 2024).</p>
				<p>In 2023, I also had the pleasure of signing my two new French graphic-novel action-adventure trilogies, <a href="https://www.jordanmechner.com/en/books/monte-cristo/">Monte Cristo</a> and <a href="https://www.jordanmechner.com/en/books/liberty/">Liberty! The Insurgents</a>, alongside my wonderful artist collaborators. Books 1 and 2 of <cite>Monte Cristo</cite> (with Mario Alberti) and Book 1 of <cite>Liberty</cite> (with Etienne Le Roux and Loic Chevallier) are now available; the next volumes will be released in spring. (English editions to be announced in 2024.)</p>
				<p>Exciting news for Prince of Persia fans: this year brought the announcement of a brand-new title, <a href="https://www.jordanmechner.com/en/games-movies/the-sands-of-time/">Prince of Persia: The Lost Crown</a> (coming in January 2024), along with the 30th anniversary celebration of <cite>The Shadow and the Flame</cite>, and the 20th anniversary of <cite>Prince of Persia: The Sands of Time</cite>. In parallel, the Ubisoft Montreal team working on the <cite>Sands of Time</cite> remake announced that the development has passed its latest internal milestone and is proceeding full steam ahead.</p>

			</div>
			<div>

				<p>Prince of Persia has been very much present in my creative focus this year: it's central to my graphic novel <cite>Replay</cite>, and is featured in a new series of <a href="https://www.jordanmechner.com/en/artworks/">original artworks</a> I've drawn paying tribute to the games. The artworks are available as signed, limited-edition physical art prints and as free downloadable <a href="https://www.jordanmechner.com/en/library/#replay">desktop and mobile backgrounds</a>.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/artworks/a-faithful-friend-closeup.jpg" alt="">
					<img src="https://www.jordanmechner.com/images/news/artworks/dagger.jpg" alt="">
				</figure>

				<p>As if this weren't enough to fill one year, in August, Digital Eclipse released <a href="https://www.jordanmechner.com/en/games-movies/karateka/">The Making of Karateka</a> for consoles and PC, an in-depth playable, interactive documentary about the Apple&nbsp;II game I made before <cite>Prince of Persia</cite>. <cite>The Making of Karateka</cite> is earning acclaim as a new template for game history preservation, and has become one of 2023's best-reviewed titles. Considering their raw material was work I did more than 40 years ago, that's a real accomplishment.</p>
				<p>For fans of <a href="https://www.jordanmechner.com/en/games-movies/the-last-express/">The Last Express</a> and those interested in behind-the-scenes, I've launched a new ongoing feature on my website: <a href="https://www.jordanmechner.com/en/library/1993-journals/">My game development journal</a> "30 years ago this week" continues from January 1993, where the book <cite>The Making of Prince of Persia</cite> ends. As of this week in 1993, <cite>Prince of Persia&nbsp;2</cite> is on shelves for Christmas, Smoking Car Productions has moved into new San Francisco offices, and <cite>The Last Express</cite> production is getting under way.</p>
				<p>If my end-of-year greeting card (above) brings good feelings, you can <a href="https://www.jordanmechner.com/images/news/artworks/happyholidays-fullres.jpg">download it here</a>. And if you're looking for a gift for someone in your life who appreciates graphic novels, books about making video games, real-life 20th-century history, or a multigenerational family epic, <a href="https://www.jordanmechner.com/en/books/replay/">Replay</a> is an engrossing, hefty 320-page combination of all of the above. You can find it in French bookstores, or pre-order it in English.</p>
				<p>Happy Holidays!</p>

			</div>
			
		
		
		</article>

		<article>
			<div>
				<h3>Cliffhanger</h3>
				<p><time datetime="2023-09-12">⌛ September 12, 2023</time>
			</p></div>
			
		
			<div>

				<p>Earlier this year, as a companion project to <a href="https://www.jordanmechner.com/en/books/replay/">my graphic memoir <cite>Replay</cite></a>, I started creating "author's tribute" artworks inspired by my past video games. (You can <a href="https://www.jordanmechner.com/en/library/#replay">download them as wallpapers</a> from the Library, or see the original series of art prints <a href="https://www.jordanmechner.com/en/artworks/#artworks">on the Artworks page</a>.)</p>
				<p>I've been blown away by the enthusiastic response both to the artworks, and to <cite>Replay</cite> (now in French bookstores; English edition will be released in March 2024). The outpouring of love for these games so many decades after their release is amazing to me.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/artworks/cliffhanger.jpg" alt="">
				</figure>

				<p>Today, I'm releasing a new print: "Cliffhanger" — which is where <cite>Prince of Persia 2: The Shadow and the Flame</cite> ended in 1993. For those who remember the game, here's a short refresher video of the ending. (If you didn't finish it: Spoiler alert!)</p>

				<figure>
					<iframe width="100%" height="300" data-src="https://www.youtube.com/embed/Ct20XCOU414?rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe>
				</figure>

				<p>Fans and colleagues have been asking me for the past 30 years: Why not do a third game to complete the 2D Prince of Persia trilogy? And who is that mysterious sorceress, anyway?</p>

				<h4>Prince of Persia 3</h4>

				<p>My graphic memoir <cite>Replay</cite> addresses the first question. In parallel to my grandfather's experience as a young soldier in World War I, and my father's as a child refugee during World War II, the book tells my own story of how <cite>Prince of Persia 3</cite> got green-lit, then cancelled — twice: first in 1993, then in 2019.</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/replay/cliffhanger-en.gif"></a>
				</figure>

				<h4>The Sorceress</h4>

				<p>As to the second question, my intention was always to reveal the sorceress's identity and back story through game play, not in a blog post. Short of making the game, the best answer I can give now is this artwork. Images can suggest things in ways words can't.</p>

			</div>
			<div>

				<p>I composed "Cliffhanger" to evoke the final image of <cite>POP2</cite>, and to depart from it. The prince, princess, and sorceress in my drawing don't exactly match the characters in the 1993 PC game, nor do they literally represent the 2019 team's work-in-progress at the point development was cancelled. My goal was to create an artwork that embraces both my evolving vision of <cite>POP3</cite>, and fans' enduring curiosity for the past 30 years about this mysterious sorceress and the game that never was.</p>
				<p>In the book <cite>Replay</cite>, I use a yellow two-color palette for the present-day story, blue for my 1980s and 90s game-development days. "Cliffhanger" combines both palettes. The tower glimpsed in the crystal ball behind the prince and princess's flying horse is not a Persian palace, but the medieval Gothic cathedral of Montpellier, where the 2019 game development was based.</p>
				<p>It was a disappointment to me and the team when <cite>POP3</cite> got cancelled four years ago, but I'm grateful for the silver lining. It gave me time and space to create my recent graphic novels <cite>Monte Cristo</cite>, <cite>Replay</cite>, and <cite>Liberty</cite>, and other team members the opportunity to bring their top-notch talents to exciting new projects — notably <cite>Prince of Persia: The Lost Crown</cite>, a fresh 2D Metroidvania take on the POP universe, slated for January 2024 release.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/artworks/cliffhanger-closeup.jpg" alt="">
				</figure>

				<p>In the meantime, here's "Cliffhanger" — my personal tribute to the POP teams, the great work they've done for three decades and counting, and to the fans who've kept the <cite>Prince of Persia</cite> flame alive in our collective imaginations.</p>

				<h4>Time-Limited Edition</h4>

				<p>My first two <cite>POP</cite> art prints sold out their editions of 40 within hours of announcement, leaving some people wishing they'd been able to get one. So rather than try to guess the right number in advance for "Cliffhanger," I'm doing a time-limited release. (The concept feels appropriate for <cite>POP</cite>.) Here's how it works:</p>
				<p>For the first 48 hours following this announcement (ending at 9 a.m. Thursday, 14 September), everyone who orders can get a print (in either the 30x40 or 40x60 size; take your pick). The editions will then be sized based on the number of orders received in that 48-hour window. Whether the edition ends up being 10, 40, or more, we'll produce and I'll individually hand-number and sign that many. Once the sale closes, no more prints of this artwork will be made — that's the nature of a limited edition, and protects its value for collectors. If you'd like one, you can <a href="https://www.jordanmechner.com/en/artworks/cliffhanger/">place your order here</a>.</p>
				<p>And for those who read French, I urge you to get to a bookstore and check out <cite>Replay</cite>. It provides insight into the personal and creative roots of my games, including <cite>Prince of Persia</cite>, that I can only communicate in a graphic novel. For readers curious about the connections between the book and real-life events, I've also posted an online <a href="https://www.jordanmechner.com/en/library/replay-annex/">Replay Annex</a> with chapter-by-chapter commentary and resources.</p>
				<p>The English edition of <cite>Replay</cite> will be released by First Second Books in March 2024. You can pre-order it, or early-order a signed edition, <a href="https://www.jordanmechner.com/en/books/replay/">on the Replay page</a>.</p>

			</div>
			
		
		
		</article>

		<article>
			<div>
				<h3>Liberty!</h3>
				<p><time datetime="2023-08-22">⌛ August 22, 2023</time>
			</p></div>
			
		
			<div>

				<p>I'm thrilled to announce the launch of my new graphic novel trilogy LIBERTY — an epic historical adventure, spectacularly drawn by Étienne LeRoux and Loïc Chevallier. It's the true story of an unlikely friendship that changed history in 1776, when a Parisian playwright teamed up with a Yankee merchant from Connecticut to smuggle desperately-needed arms to the American rebel army. From the moment I learned about this 18th-century "black ops" — a little-known and fascinating chapter of the American revolution and of Pierre Caron de Beaumarchais' colorful life, powerfully relevant to today's world — I knew I had to write it.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/liberty/poster.jpg" alt="">
				</figure>

				<p>If you've read my previous graphic novels <cite>Templar</cite> or <cite>Monte Cristo</cite> (or played <cite>The Last Express</cite>), you know I love stories of adventure and intrigue set against a backdrop of real historical events. (Those three take place in the 14th, 21st, and early 20th centuries, respectively.) I connected with this one immediately.</p>
				<p>In school, I was always bored by American history. Benjamin Franklin, George Washington... yawn. But the odd-couple pairing of bon vivant Beaumarchais (the author of <cite>The Marriage of Figaro</cite>, whom Voltaire called the wittiest writer in France) and straitlaced Silas Deane (an American secret agent sent to Paris in a classic fish-out-of-water setup) grabbed my imagination. Deane and Beaumarchais are unsung heroes. They have no statues or streets named in their honor (and when you read LIBERTY, you'll understand why). Yet their contribution was critical.</p>
				<p>I felt personal empathy with both characters. As an American expat in France myself (see <a href="https://www.jordanmechner.com/en/books/replay/">my graphic memoir <cite>Replay</cite></a>), I identified with Silas Deane's sense of being an outsider in Paris, "lost in translation" far from home. It must have been daunting for a guy from a small East Coast town to plunge into Parisian politics and dealmaking at the glittering pinnacle of an older, sophisticated European society. And although video game development has little in common with gun running, I could vividly relate to Silas's partner, Pierre Caron de Beaumarchais. He earned fame and fortune by writing a hit play (<cite>The Barber of Seville</cite>), staked it all on a startup with an idealistic premise but questionable business model, ran through his funding too fast, and wound up at the mercy of his backers and creditors. To be sure, I gambled my <cite>Prince of Persia</cite> royalties in 1993 on a much less important venture; but it helped me imagine the predicament of a playwright-turned-entrepreneur facing bankruptcy in 1776.</p>
				<p>The purpose that brought Deane and Beaumarchais together — an underdog struggle by a new nation against a vastly more powerful and better-equipped empire, who sends a huge army to crush resistance and lay waste to their homes and towns rather than accept their right to self-government — resonated with me on multiple levels. As an American who grew up with democracy as an enshrined ideal; as a child of refugees who fled Europe to escape dictatorship in the 20th century; and as a European citizen today. A true story about a people fighting back more resolutely and effectively than anyone expected, enduring horrific losses and reprisals, urban warfare and occupation, while sympathetic but self-interested great powers dispassionately calibrate the degree and timing of support to offer, feels worth telling in 2023.</p>

			</div>
			<div>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/liberty/panels2.jpg"></a>
				</figure>

				<p>Benjamin Franklin, George Washington, Lafayette, and other well-known historical characters appear in this story, too, but even they turned out to be full of surprises. Seeing those great figures through Beaumarchais' and Deane's eyes made them human and brought them to life for me in ways I'd missed at school. Researching episodes like the battle for New York City in 1776, I was able for the first time to vividly picture the devastation once inflicted on my home town.</p>
				<p>With its cast of compelling characters, sweeping scope that blends the personal and epic, and international action spanning two continents and an ocean, I couldn't have created LIBERTY in any other medium than a graphic novel. I wrote it on a grand scale, knowing that to find an artist capable of doing justice to the project, and willing to dedicate the years it would take, was a tall order. In Etienne, Loïc, and colorist Elvire De Cock, I found LIBERTY's dream team.</p>
				<p>Four years ago, I met Etienne and Loïc in Tours, discussed Silas Deane, Beaumarchais and their world, and sealed our partnership over a bottle of Bordeaux. A new Franco-American collaboration was born. Tomorrow, August 23, 2023, <a href="https://www.jordanmechner.com/en/books/liberty/">LIBERTY Book 1: The Insurgents</a> arrives in French bookstores.</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/liberty/panels3.jpg"></a>
				</figure>

				<p>LIBERTY is a complete story in three volumes. <cite>Book 2: The Traffickers</cite> will be released in January 2024; <cite>Book 3: The Ambassadors</cite> in September. I'll post as soon as I know release dates for English and other language editions. If you'd like to be sure to be notified, you can subscribe to my e-mail newsletter or RSS feed on the home page.</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/liberty/panels1.jpg"></a>
				</figure>

				<p>In a quirk of timing, <cite>Liberty</cite>'s release makes my fifth major project announcement within six months — along with my autobiographical graphic novel <a href="https://www.jordanmechner.com/en/latest-news/#announcing-replay">REPLAY</a> (also from Delcourt), Book 2 of <a href="https://www.jordanmechner.com/en/latest-news/#meet-victor-sirin">Monte Cristo</a> (from Glénat), and back-to-back video-game announcements (a <a href="https://www.jordanmechner.com/en/latest-news/#prince-of-persia-takes-a-mighty-new-leap">new Prince of Persia</a> from Ubisoft and a <a href="https://www.jordanmechner.com/en/latest-news/#karateka-climbs-again">Karateka retrospective</a> from Digital Eclipse). It might seem like I've been doing some insane multitasking, but in reality, all of these have been in development for years. The announcements and releases landing so clustered together is just an oddity of 2023.</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/liberty/panels4.jpg"></a>
				</figure>

				<p>I'm very proud of the work we've done on <cite>Liberty</cite>. If you read French or enjoy graphic novels, I hope you'll check it out — and <cite>Replay</cite>, and <cite>Monte Cristo</cite>. <a href="https://www.jordanmechner.com/en/books/">All three titles are now in French bookstores</a>.</p>

			</div>
			
		
		
		</article>

		<article>
			<div>
				<h3>Replay Signed Edition</h3>
				<p><time datetime="2023-08-15">⌛ August 15, 2023</time>
			</p></div>
			
		
			<div>

				<p>For everyone who's asked if it will be possible to order a signed copy of REPLAY in English: I have good news, and thank you for the idea! It took a bit of organizing, but we've solved the logistics. Starting now, you can <a href="https://www.jordanmechner.com/en/books/replay/">order signed books from the Replay page</a>, via the "Signed Edition" button.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/replay-us-cover.jpg" alt="">
				</figure>

				<p>The signed edition package will include the Macmillan hardcover English edition of <cite>Replay: Memoir of an Uprooted Family</cite>, signed by me, and a set of two collector's postcards (French readers may have seen these at my in-person book signings). Books will be signed and shipped worldwide from the U.S. in March 2024, when they'll arrive in the Macmillan warehouse. I'll make a special day trip to sign them all, and they'll go out to you along with the postcards, well-protected in premium packaging (bubble wrap and cardboard box).</p>

				<figure>
					<a href="https://www.jordanmechner.com/images/news/replay/large/replay-postcards-templar.jpg"></a>
				</figure>

				<p>I took this photo of the <cite>Replay</cite> postcards alongside the French edition (which is softcover, with different cover art), and my previous First Second graphic novel, TEMPLAR, to help imagine how the soon-to-be-printed hardcover edition of REPLAY will look and feel. I've just signed off on the 320-page interior mechanical. I'm counting down the months until I can hold a physical copy in my hands.</p>
				<p><a href="https://www.jordanmechner.com/en/books/replay/">The Replay page</a> also has a button to pre-order regular unsigned books from Macmillan, Amazon, or your favorite bookseller. I hope as many of you as possible will do that, as well. Publishers and booksellers watch pre-order numbers as an indicator of a book's potential, and make their own ordering and marketing decisions accordingly. So even though <cite>Replay</cite>'s March 2024 release is months away, your pre-orders already help support the launch, and increase the chance that more people will discover the book.</p>
				<blockquote>
					<p><strong>Note:</strong></p>
					<p>I'll cut off orders on the day we need to tell the warehouse how many books to ship, or if the number of books reaches the limit of what I can comfortably sign. (This would be a great problem to have.) At that point, we'll remove the "Signed Edition" button from the website. As long as the button is there, you'll know orders are open.</p>
				</blockquote>

			</div>
			<div>

				<h4>If you're in France...</h4>

				<p>The signed English books will ship worldwide, including to France. The French edition is in bookstores now (you can also <a href="https://www.jordanmechner.com/en/books/replay/">order it online</a> from the <cite>Replay</cite> page). If you come to any of my upcoming book signings or talks, I'll be delighted to sign your book and say hello. My September/October schedule is below.</p>
				<p>If you're in France but our schedules don't line up, you can also order from most bookstores before the event. The bookseller will gladly set your book aside for me to sign, and ship it to you.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/jm-dedicaces-narbonne.jpg" alt="">
				</figure>

				<h4>Upcoming events for September/October 2023</h4>

				<p>Bookstores:</p>

				<ul>
					<li>August 26 - Viols-le-Fort / La Bestiole</li>
					<li>September 21 - Paris / Vignettes (19th arr.)</li>
					<li>September 22 - Paris / Bulles en tête (17th arr.)</li>
					<li>September 23 - Reims / Bédérama</li>
					<li>September 30 - Annecy / BD Fugue</li>
					<li>October 4 - Nice / Librairie Massena</li>
					<li>October 14 - Le Touquet / Maison de la Presse La Touquettoise</li>
					<li>October 20 - Nantes / La Mysteriéuse Librairie Nantaise</li>
					<li>October 24 - Les Sables d'Olonne / Médiatheque</li>
				</ul>

				<p>Festivals:</p>

				<ul>
					<li>September 30-October 1 - Annecy / Savoie Retro Games</li>
					<li>October 6-8 - Mouans-Sartoux / Festival du livre</li>
					<li>October 21-22 - La Vendée / Histoire(s) de BD</li>
					<li>October 27-29 - Saint-Malo / Quai des Bulles</li>
				</ul>

				<p>I'll post updates and details via social media as the events approach. You can also see <a href="https://www.editions-delcourt.fr/auteurs/mechner-jordan">my agenda on Delcourt's website</a>.</p>

			</div>
			
		
		
		</article>

		<article>
			<div>
				<h3>Karateka Climbs Again</h3>
				<p><time datetime="2023-07-11">⌛ July 11, 2023</time>
			</p></div>
			
		
			<div>

				<p>When the <a href="https://www.digitaleclipse.com/">Digital Eclipse</a> team told me they wanted to give my early game <cite>Karateka</cite> "the Criterion treatment" and re-release it in a deluxe remastered edition, I couldn't quite picture exactly what they had in mind. Their enthusiasm and evident passion for video game history inspired confidence, so I said yes. I never in my wildest dreams imagined how far they'd take it.</p>
				<p>Fast-forward to April 2023: I'm sitting with my dad and family in New York. Our jaws drop as we watch Chris Kohler demo an almost-final build of "The Making of Karateka" (announced today for digital release on Xbox, PlayStation, PC and Nintendo Switch). What they've built around my 1984 kicking-punching debut is so much more than a game remaster, I'm still trying to wrap my mind around it.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/karateka/francis.jpg" alt="">
					<img src="https://www.jordanmechner.com/images/news/karateka/screenshot1.jpg" alt="">
				</figure>

				<p>The photo above captures my dad's reaction as (age 92) he watches himself climbing up onto the hood of our family car forty years earlier. He's wearing a karate gi at my request, in a Super 8 film I shot at age 18 to create rotoscoped animation for <cite>Karateka</cite>. (This was three years before I pressed my 15-year-old brother into service as the model for my next game, <cite>Prince of Persia</cite>.)</p>
				<p>Digital Eclipse has reconstructed my Super 8 rotoscoping process — from film to pencil tracings to pixelated game character — in their interactive, hands-on "Rotoscope Theater." And that's just one element of "The Making of Karateka." It's packed with audio and video interviews with me, my dad, and game-industry luminaries; a podcast about <cite>Karateka</cite>'s music (which my dad composed); rare original design documents; excerpts from my journals; and 14 playable games — including not only the final Apple&nbsp;II, Commodore, and Atari versions of <cite>Karateka</cite>, but also work-in-progress builds I submitted to Broderbund along the way, tracking its development from prototype to gold master. All the games are playable on a choose-your-own nostalgic menu of period monitors and TVs, with optional audio commentary and a "watch/play" mode that the Dagger of Time would envy.</p>

				<figure>
					<iframe width="100%" height="251" data-src="https://www.youtube.com/embed/IrS2pipZaH0?rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe>
				</figure>

			</div>
			<div>

				<p>As a bonus, they've salvaged and resurrected my never-before-published arcade shoot-em-up <cite>Deathbounce</cite> (the game I made before <cite>Karateka</cite>, which teenage me hoped would be my ticket to software success in 1982)… and the one I did before <em>that</em>, an unauthorized Apple&nbsp;II clone of the arcade hit <cite>Asteroids</cite>. Incredibly, they've not only remastered <cite>Karateka</cite>, but also remade <cite>Deathbounce</cite>, using today's technology to reimagine my 1982 prototype as a jazzy twin-stick shooter. All these are included and playable in "The Making of Karateka."</p>
				<p>I'm mind-boggled that the Digital Eclipse team has poured so much hard work, love and fidelity into reconstructing my journey as a fledgling game developer. From a shoebox of 5.25" floppy disks I stashed in my closet 40 years ago — each disk a step along the 7-year path that led me from high-school BASIC to <cite>Prince of Persia</cite> — they've excavated work I never expected to see again, brought it to life, and placed it in historical context.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/karateka/screenshot2.jpg" alt="">
				</figure>

				<p>If "The Making of Karateka" were an interactive exhibition in the Strong Museum of Play (from whose collection many of the archival materials came), it would require several rooms and a full afternoon to explore. Now, when they release the full package (date TBA later this summer), you'll be able to download, play and discover it at your leisure.</p>
				<p>As of today, you can wishlist it on Steam. Details and links are on the <a href="https://www.digitaleclipse.com/games/karateka">Digital Eclipse game page</a>.</p>
				<p>With this release, Digital Eclipse has set a new bar for game-development history preservation. I'm touched and honored that they chose <cite>Karateka</cite> as the first title in their planned <a href="https://www.digitaleclipse.com/media/goldmasterseries">Gold Master series</a>. I can't wait to see what comes next.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/karateka/makinggames.gif" alt="">
				</figure>

				<blockquote>
					<p>The "Karateka" Super 8 rotoscoping process as depicted on page 30 of my graphic-novel memoir <a href="https://www.jordanmechner.com/en/books/replay/">Replay</a>.</p>
				</blockquote>

			</div>
			
		
		
		</article>

		<article>
			<div>
				<h3>Prince of Persia Takes a Mighty New Leap</h3>
				<p><time datetime="2023-06-08">⌛ June 8, 2023</time>
			</p></div>
			
		
			<div>

				<p>I've been eagerly awaiting this moment for so long, I can't believe it's here at last. The first trailer for Ubisoft's new <cite>Prince of Persia: The Lost Crown</cite> was unveiled today at Summer Game Fest. After over a decade since the last major game release, Prince of Persia fans once again have a wondrous universe to discover and embark on an exciting new adventure.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/poptlc/releasetrailer-leap.jpg" alt="">
					<img src="https://www.jordanmechner.com/images/news/poptlc/releasetrailer.jpg" alt="">
				</figure>

				<figure>
					<iframe width="100%" height="251" data-src="https://www.youtube.com/embed/MmX7a_e65uU?rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe>
				</figure>

				<p>For anyone wondering: <cite>The Lost Crown</cite> is not a continuation of either the <cite>Sands of Time</cite> or the retro-2D storyline, it's a fresh beginning. I didn't write or have a direct role in this one — which means I'll get to enjoy its surprises as a gamer. I know the talented POP team at Ubisoft Montpellier well, I've watched them pour their hearts and passion into this project over three years from pre-conception to full beta, and I couldn't be more excited. This is the Prince of Persia game I've been wishing for.</p>
				<p>If you've read <a href="https://www.jordanmechner.com/en/books/replay/">my graphic memoir <cite>Replay</cite></a> (released in France last month; English edition coming in March 2024), you might wonder whether there's a link between the unannounced, cancelled Prince of Persia project that brought me to Montpellier in 2017 (as told in <cite>Replay</cite>) and <cite>The Lost Crown</cite>. Is it really a coincidence that both projects were launched in the same small city in the south of France?</p>

			</div>
			<div>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/makinggames-en.gif" alt="">
				</figure>

				<p>The link is the talent. Ubisoft's storied Montpellier studio and "French touch" were a big part of what drew me in 2001, when we first joined forces <a href="https://www.jordanmechner.com/en/games-movies/the-sands-of-time/">to reboot Prince of Persia (<cite>The Sands of Time</cite>)</a>, and again in 2017. A number of Montpellier hands have also worked in Montreal, including some of the best talent I've had the privilege to work with anywhere. I've seen their dedication and love for POP at close range; we've immersed ourselves together in Persian mythology and gameplay on past projects. It's no coincidence that this group of people is the one to finally crack the challenge of reinventing POP for a new generation of gamers. I'd call it destiny. I'm thrilled and delighted to see their hard work come to fruition — and I can't wait to play it.</p>
				<p><cite>Prince of Persia: The Lost Crown</cite> will be released in January 2024 for PlayStation, Xbox, Nintendo Switch and PC. Ubisoft will share more details in coming days; you can find up-to-date info on <a href="https://www.princeofpersia.com/">the official Prince of Persia game page</a>.</p>
				<p>If you're curious about the back story of Prince of Persia's original creation, its 35-year legacy, and the multiple (sometimes uncanny) echoes through time that intertwine the prince's adventures with my own family story, check out <a href="https://www.jordanmechner.com/en/books/replay/">my graphic memoir <cite>Replay</cite></a>. It will add new dimensions to your appreciation of past POP games, and of why it's fitting that Montpellier — home to so much video game creativity — is the place where the prince's Lost Crown was finally found.</p>
				<p>Now to start counting down the months till January…</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/pop-en.gif" alt="">
				</figure>

			</div>
			
		
		
		</article>

		<article>
			<div>
				<h3>A New Departure</h3>
				<p><time datetime="2023-05-16">⌛ May 16, 2023</time>
			</p></div>
			
		
		<div>

			<p>A big thank you to everyone who purchased limited-edition prints of my first three <cite>Prince of Persia</cite>-inspired game tribute artworks, "A Faithful Friend," "Bones," and "Dagger." Today, I'm releasing a fourth: "Departure." As the title hints, this one isn't <cite>Prince of Persia</cite>.</p>

			<figure>
				<img src="https://www.jordanmechner.com/images/news/artworks/departure.jpg">
				<img src="https://www.jordanmechner.com/images/news/artworks/departure-closeup.jpg">
			</figure>

			<figure>
				<iframe width="100%" height="300" data-src="https://www.youtube.com/embed/U7k3_CX6NII?rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe>
			</figure>

			<p>Players of a certain 1997 point-and-click adventure game may recognize the characters (and even the time on the station clock). It depicts a moment on the Gare de l'Est platform just before <cite>The Last Express</cite> begins and the train leaves the station. I'd often imagined seeing our characters boarding the train; now at last I get to draw them!</p>
			<p>If you've read my graphic novel <a href="https://www.jordanmechner.com/en/books/replay/">Replay</a> (released last week in France) you'll understand that this series of artworks isn't only about the games, but also about my personal journey making them. <cite>The Last Express</cite>, in particular, has echoes of my own family's story of 20th century Europe. While writing and drawing <cite>Replay</cite>'s chapter 7 and 8, whose dual timelines recount my struggle to complete the game's production in 1993-97 in parallel with my dad and his young aunt's flight from Nazi-occupied France in 1940-41, I almost felt as if I was drawing scenes from a sequel to the game.</p>

		</div>
		<div>

			<figure>
				<img src="https://www.jordanmechner.com/images/news/replay/train-en.gif">
			</figure>

			<p>It's probably no coincidence that, out of all my games, <cite>The Last Express</cite> is the one that feels closest to a graphic novel. As an American, I'd been unfamiliar with the incredible legacy of European comics until my French friends Patrick and Sandrine introduced me to them in the 1990s. To discover, in my twenties, masters like Pratt, Bilal, and Tardi was a revelation and a formative influence in creating <cite>Last Express</cite>. (That's Patrick below, helping me with train research in 1993. He appears as a character in both <cite>Replay</cite>'s blue and yellow timelines.)</p>

			<figure>
				<img src="https://www.jordanmechner.com/images/news/replay/1993-en.gif">
			</figure>

			<p>"Departure" is my homage to Smoking Car Productions; to the European comics authors and filmmakers whose work inspired us; and to the fans who embraced <cite>The Last Express</cite> and have kept its world and characters alive for 25 years. It's also an homage to my grandfather, my father and his aunt Lisa, whose real-life adventures reverberate both in <cite>Replay</cite> and in <cite>The Last Express</cite>'s fictional story.</p>
			<p>Along with "Departure", I'm releasing a second Replay-linked print today. "Promenade des Anglais" depicts the storied boardwalk in Nice, France, where my dad spent a year of his childhood as a refugee in 1940. Readers of <cite>Replay</cite> (chapters 7 and 8 especially) will understand the personal resonance this setting has for my family, and why I chose it to pair with <cite>The Last Express</cite>.</p>

			<figure>
				<img src="https://www.jordanmechner.com/images/news/artworks/promenade-des-anglais.jpg">
			</figure>

			<p>"<a href="https://www.jordanmechner.com/en/artworks/departure/">Departure</a>" and "<a href="https://www.jordanmechner.com/en/artworks/promenade-des-anglais/">Promenade des Anglais</a>" are available exclusively here in signed and numbered limited editions of 40. By popular demand, I'm also releasing "Departure" as a limited edition of 10 in a larger format, as I did last month with "Dagger." Details and links to purchase are on the <a href="https://www.jordanmechner.com/en/artworks/">Artworks</a> page.</p>
			<p>My graphic memoir <cite>Replay</cite> is now available in French bookstores, and from the <a href="https://www.jordanmechner.com/en/books/replay/">Replay book page</a> (where you can pre-order the English edition). And if you'd like to play <a href="https://www.jordanmechner.com/en/games-movies/the-last-express/">The Last Express</a>, it's available on Steam and mobile from Dotemu.</p>
			<p>Thank you for supporting my creative endeavors in all these forms, over all these years!</p>

		</div>
		
		
		
		</article>

		<article>
			<div>
				<h3>Prince of Persia 2 turns 30!</h3>
				<p><time datetime="2023-05-03">⌛ May 3, 2023</time>
			</p></div>
			
		
			<div>

				<p>30 years ago today in 1993, <cite>Prince of Persia 2: The Shadow and the Flame</cite> went gold master.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/pop2/prince2-title.jpg" alt="">
					<img src="https://www.jordanmechner.com/images/news/pop2/prince2-rooftops.jpg" alt="">
					<img src="https://www.jordanmechner.com/images/news/pop2/prince2-streets.jpg" alt="">
					<img src="https://www.jordanmechner.com/images/news/pop2/prince2-arrival.jpg" alt="">
				</figure>

				<p>My journal reminds me that the Broderbund team and I celebrated the completion of two years of work at Pasha's, our Persian restaurant in San Francisco (and I suffered the aftereffects of my overindulgence the next morning, on a long flight to Paris).</p>
				<p>Thirty years later almost to the day, I find myself once again in Paris, fresh off a plane — this time from New York, where I celebrated the release of <a href="https://www.jordanmechner.com/en/books/replay/">my newly published graphic novel Replay</a>, and gave my dad a copy for his 92nd birthday. The book tells the story of his childhood, and of our family. The tales of the creation of <cite>Prince of Persia</cite> 1 and 2 (and my other games) are nested inside it, like "1001 Nights" episodes.</p>

			</div>
			<div>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/tleresearch-en.gif" alt="">
				</figure>

				<p>The reason I went to Paris in May 1993 was to do research for my next game, <cite>The Last Express</cite>. I visited train stations, train yards and archives, including the Gare de l'Est basement meeting depicted in the <cite>Replay</cite> panel above (which I recorded in my journal). I hungrily collected comics by European masters like Pratt, Bilal, Tardi, and Giardino — at that time difficult to find in the U.S. — that would inspire the game's story line and visuals.</p>
				<p>Arriving in France today in May 2023, I'm not visiting, but returning home. (I moved here from Los Angeles in 2016.) In 1993, I was discovering comics as a fan; this week, I became a French comics author. (REPLAY is the first book I've both drawn and scripted.) Reading this week's batch of journal entries makes me feel that in many ways, I've come full circle, and closed a 30-year loop.</p>
				<p>You can follow the making of <cite>Prince of Persia 2</cite> and <cite>The Last Express</cite> on <a href="https://www.jordanmechner.com/en/library/1993-journals/">my website's new 1993 journals page</a> (continuing where my published journal <cite>The Making of Prince of Persia</cite> leaves off, in January 1993).</p>
				<p>The bigger story (including the 100-year loop of how my family's 20th-century survival story relates to the creation of <cite>Prince of Persia</cite>, <cite>Prince&nbsp;2</cite>, and <cite>The Last Express</cite>) is told in <a href="https://www.jordanmechner.com/en/books/replay/">my book REPLAY</a>. You can find it in French comics stores this week, or <a href="https://www.jordanmechner.com/en/books/replay/">pre-order the English version</a> here.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/jordan-replay.jpg" alt="">
				</figure>

			</div>
			
		
		
		</article>

		<article>
			<div>
				<h3>Replay in English!</h3>
				<p><time datetime="2023-05-02">⌛ May 2, 2023</time>
			</p></div>
			
		
			<p>I'm happy to share the newly-completed artwork for the English cover of my new graphic novel memoir <cite>Replay</cite> — slated for release by First Second Books/Macmillan on March 19, 2024. You can <a href="https://www.jordanmechner.com/en/books/replay/">pre-order it now</a> from your favorite bookseller (links are on the <a href="https://www.jordanmechner.com/en/books/replay/"><cite>Replay</cite> book page</a>).</p>
			<div>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/cover-en.jpg" alt="">
				</figure>

			</div>
			<div>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/cover-fr.gif" alt="">
				</figure>

			</div>
			<div>

				<p>The English edition has a different cover design from the French edition (released by Delcourt last week, April 26), and will be in hardcover rather than paperback, but inside, they're the same book. I wrote and drew chapters first in English (my native language), worked closely with my editor Lewis Trondheim on the French translation, and brought the art to final for both editions nearly simultaneously. So both are "the original" edition.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/expat-en.gif" alt="">
					<img src="https://www.jordanmechner.com/images/news/replay/expat-fr.gif" alt="">
				</figure>

				<p>Why such a long gap between the French and English releases (almost eleven months)? The short answer is that U.S. and French publishers work differently. In the U.S., longer lead times for printing, marketing, and distribution mean that publishers usually schedule a graphic novel release 12-18 months after an author delivers final art. In France (where books are printed locally), it's more like 3 months.</p>
				<p>If you're bilingual and wondering which edition to get, my answer is: you can't go wrong! Whichever version of <cite>Replay</cite> you pick up, you'll be getting a beautifully designed, printed and bound edition that I'm deeply proud of. I love the way the softcover French book feels in my hands — it's just the right size, thickness and flexibility. And the First Second hardcover edition will be a different and equally gorgeous tactile pleasure, worth the wait.</p>
				<p>Details and links to purchase both editions are on the <a href="https://www.jordanmechner.com/en/books/replay/">Replay book page</a>. If you're in France, or read French, there's no need to wait; <cite>Replay</cite> is in bookstores now!</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/apple-en.gif" alt="">
				</figure>

			</div>
		
		
		</article>

		<article>
			<div>
				<h3>Announcing Replay</h3>
				<p><time datetime="2023-04-25">⌛ April 25, 2023</time>
			</p></div>
			
		
			<div>

				<p>I'm excited to finally share the project I've been deeply immersed in for the past two years. It's an adventure that will have special meaning for game fans who've enjoyed <cite>Prince of Persia</cite> or <cite>The Last Express</cite>, yet it's very different from anything I've done before. It unites in a new way three crafts and lifelong passions that have animated my work: storytelling, visual art, and history.</p>
				<p><cite>Replay</cite> is a graphic novel memoir of three generations. It interweaves my father's childhood odyssey as a Jewish refugee in Nazi-occupied France; my grandfather's experience as a teenage soldier on the Russian front in World War I; and my own youth as a videogame-obsessed American kid, from a 1978 Apple&nbsp;II through four decades in the fast-evolving game industry. The games, books, and films I've spent my career making were born out of those formative events.</p>
				<p>Some readers may already know that my dad composed the music for <cite>Karateka</cite> and <cite>Prince of Persia</cite> and that my younger brother David was the rotoscoped animation model. That's just the tip of the iceberg of all the ways my family's story underlies my past and present creative efforts. In <cite>Replay</cite>, I share the larger human and personal context of those games' creation.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/cover-fr.gif" alt="">
				</figure>

				<p><cite>Replay</cite> is my first graphic novel as a "complete author" — meaning I've drawn as well as written it. It's 320 pages in color, so you can understand why I've been somewhat quiet through 2021-22. Making 1,500 drawings takes time. (If you've seen my recent game tribute artworks, from "A Faithful Friend" to "Dagger", you'll appreciate their kinship with <cite>Replay</cite>.)</p>
				<p>The French edition of <cite>Replay</cite> will be in bookstores tomorrow, April 26. It's published by Delcourt. You can purchase it online (and read a free 25-page preview) at the <a href="https://www.jordanmechner.com/en/books/replay/">Replay book page</a>. <cite>Replay</cite> will be published in English by First Second Books in early 2024.</p>

				<h4>Rivers of Time</h4>
				<p>To make it easy for readers to follow <cite>Replay</cite>'s intersecting storylines, I've used three distinct palettes.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/suitcases.gif" alt="">
				</figure>

				<p><cite>Replay</cite>'s "blue" timeline covers my career in game development, from programming my first Apple&nbsp;II arcade games as a teenager, through the 1990s and 2000s with ever-bigger teams, budgets, and stakes on <cite>The Last Express</cite> and <cite>The Sands of Time</cite>. If you've read my published game-dev journals or viewed the ArsTechnica video, you'll appreciate the destiny-altering moment in 1988 when my then-girlfriend Tomi persuaded me that <cite>Prince of Persia</cite> would be more fun if it had sword-fighting.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/blue-en.gif" alt="">
				</figure>

				<p><cite>Replay</cite>'s second, "sepia" timeline depicts my dad's childhood flight through occupied France from 1938-41, as he and his young aunt Lisa tried to outrun the rapidly expanding Nazi regime to reunite with their family across the Atlantic. I grew up hearing their stories. Like many second-generation immigrants, I've often felt that the challenges of my own life were undramatic compared to the last generation's heroic survival. Forty years before little Franzi composed the music for his son's Apple&nbsp;II games, he had bigger things to worry about.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/ww2-en.gif" alt="">
				</figure>

				<p>This sepia timeline also holds the back story of my dad's odyssey of family separation and reunion. A quarter-century earlier, in 1914, my grandfather saw his own idyllic childhood shattered by World War I. (His hometown of Czernowitz, now in Ukraine, was a thriving Jewish capital of the Austro-Hungarian Empire.) He was conscripted and sent to the Russian and Italian fronts, where he spent three years in the trenches on the losing side.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/ww1-en.gif" alt="">
				</figure>

				<p>Linking both timelines is <cite>Replay</cite>'s third, "yellow" present-day frame, recounting my move to France for a video game project in 2016, as an American with two teenage kids. It's a story of today's game industry, when multimillion-dollar productions involving hundreds of people can be greenlit, morph, change direction, and get cancelled.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/yellow-en.gif" alt="">
				</figure>

				<p>(By serendipity, <cite>Replay</cite>'s release coincides with the 30th anniversary of <cite>Prince of Persia 2: The Shadow and the Flame</cite> signing out of QA in 1993. For fans who've wondered why the 2D <cite>Prince of Persia</cite> trilogy never got its third game, you'll find part of the answer in <cite>Replay</cite>.)</p>
				<p>I chose the title <cite>Replay</cite> because it resonates with both the video-game and historical threads of this book. I've often had the sensation that in my life, I'm unintentionally or unconsciously echoing past events. Like my grandfather, I uprooted and resettled my family across the Atlantic — but in the opposite direction, under significantly more favorable circumstances. "Replay" also evokes my mental habit of rehashing past decisions, as if by doing that I might somehow magically undo the past and obtain a better outcome. Which, of course, is only possible in a video game.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/flyinghorse.gif" alt="">
				</figure>

				<p>The French edition of <cite>Replay</cite> will be in bookstores tomorrow, April 26. It's published by Delcourt. You can purchase it online (and read a free 25-page preview) at the <a href="https://www.jordanmechner.com/en/books/replay/">Replay book page</a>.</p>
				<p><cite>Replay</cite> will be published in English by First Second Books in early 2024. (French readers are getting it ten months sooner.) I'll share more details about the English release in my next post. As of today, you can already <a href="https://www.jordanmechner.com/en/books/replay/">pre-order it</a>.</p>
				<p>If you're a fan of graphic novels, video games, or are interested in twentieth-century history — or all three — I hope <cite>Replay</cite> will speak to you and resonate on multiple levels. It's the great origin story I've spent my life preparing to tell. I can't wait for you to discover it.</p>

				<figure>
					<img src="https://www.jordanmechner.com/images/news/replay/books.jpg" alt="">
				</figure>

			</div>
		
		
		</article>

		<article>
			<div>
				<h3>Dagger of Time</h3>
				<p><time datetime="2023-04-04">⌛ April 4, 2023</time>
			</p></div>
			
		
		<div>

			<p>Today, I'm releasing the third in a series of Prince of Persia tribute artworks — my homage to the fans and teams that shaped the prince's destiny (and mine). This one is dedicated to the incredibly talented Ubisoft team I had the privilege to work with to make The Sands of Time, and to the fans whose loyalty has kept the flame alive these past 20 years.</p>

			<figure>
				<img src="https://www.jordanmechner.com/images/news/artworks/dagger.jpg">
			</figure>

			<p>In 2003, the Montreal team and I had no way of knowing whether Prince of Persia would appeal to a new generation of console gamers. The original 2D game series had fizzled out a decade earlier, when my planned third game of the trilogy was cancelled. A 1999 3D reboot from Red Orb had flopped. We felt sure we had something special with Sands of Time, but no one was counting on it to be a hit. The enthusiasm and warm embrace with which you greeted the new Prince of Persia, Farah, and the Sands of Time universe surpassed our dreams.</p>
			<p>Like my two preceding artworks, "A Faithful Friend" and "Bones," "Dagger" is a personal expression as a visual artist and graphic novelist of what Sands of Time has meant to me, looking back over two decades of memorable experiences and adventures since that game's release.</p>

		</div>
		<div>

			<p>"Dagger" is available as a limited edition of 40 signed and numbered giclée prints, exclusively <a href="https://www.jordanmechner.com/art-prints/dagger/">here</a>. This time, I've also created a limited edition of 5 prints in a larger format (60 x 40 cm).</p>

			<figure>
				<img src="https://www.jordanmechner.com/images/news/artworks/dagger-sizes.jpg">
			</figure>

			<p>My 30-years-ago journal reminds me that this week in April, 1993, I was in San Francisco with a Broderbund team in the final weeks of playtesting and debugging POP 2: The Shadow and the Flame. Retro-gaming fans and time-travelers can follow the journal <a href="https://www.jordanmechner.com/library/#1993journals">in the Library</a>. Time is an ocean in a storm...</p>
			<p>In my next post, I'll have something very special to announce. It's the main project that's consumed most of my working hours and creative passion for the past two and a half years. I feel confident in saying that it's not anything you've been expecting. It's not a game, but I believe it will be of great interest to game fans — and not only to game fans. It will be released in France on April 26, only three weeks from now. I can't wait to share it with you.</p>
			<p>Until then, I hope you'll discover and enjoy my last few weeks' releases: the 1993 journals, the exciting new adventure of <a href="https://www.jordanmechner.com/books/monte-cristo/">Monte-Cristo</a> (Books 1 and 2 now in French comic book stores), and Dagger!</p>

		</div>

			
		
		
		</article>

		<article>
			<div>
				<h3>Meet Victor Sirin</h3>
				<p><time datetime="2023-03-21">⌛ March 21, 2023</time>
			</p></div>
			
		
		<div>

			<p>I'm excited to announce that the second volume of my new graphic-novel trilogy <a href="https://www.jordanmechner.com/books/monte-cristo/">Monte-Cristo</a> arrives in bookstores tomorrow (March 22) in France. It's the story of Sam Castillo, an innocent young man unjustly accused and imprisoned for 17 years, who returns as mysterious mega-billionaire Victor Sirin to take his revenge on the three men who stole his youth.</p>
			<p>In 2005, post-9/11 America (Book One), 24-year-old <strong>SAM CASTILLO</strong> has every reason to be happy—promoted to foreman of his company's Iraq reconstruction project, engaged to his high-school sweetheart <strong>ABBY</strong>—until he's framed as a terrorist and rendered to a "black site" prison an ocean away.</p>
			<p>Three men put him there: Sam's supervisor <strong>EDDIE DALGLEISH</strong>, who's been skimming money in a boondoggle Sam's promotion threatens to expose; FBI agent <strong>WALTER FARRELL</strong>, who makes a devil's bargain to conceal his Army general father-in-law's corrupt dealings with military contractor Greendale; and Abby's best friend <strong>ANDREW McCLANE</strong>, who betrays Sam to clear the way for his own courtship of Abby.</p>
			<p>Over the next 15 years, cut off from the world, Sam forms a deep friendship with fellow detainee <strong>FARHAD</strong>—a brilliant, multi-lingual master of intrigue, who bequeaths to Sam the bank codes of his late Russian-oligarch employer's hidden fortune... and by his own death enables Sam's escape.</p>

			<figure>
				<img src="https://www.jordanmechner.com/images/news/mc/victor.jpg">
			</figure>

			<p>In Book Two (our present day), Sam arrives in the U.S. with a new identity as mysterious mega-billionaire emigré <strong>VICTOR SIRIN</strong>, owner of the offshore shell <strong>MONTE-CRISTO CORPORATION</strong>. The three men who separated him from Abby and shattered his life have risen in the world. Dalgleish is a hedge fund billionaire, McClane is a Congressman running for governor, and Farrell is U.S. Deputy Attorney General. Abby, now a public defender, is married to McClane with two children. They have no idea what's coming.</p>

			<figure>
				<img src="https://www.jordanmechner.com/images/news/mc/mccorp.jpg">
			</figure>

		</div>
		<div>

			<figure>
				<img src="https://www.jordanmechner.com/images/news/mc/danica.jpg">
			</figure>

			<p>Victor skillfully plays on his enemies' greed and ambition, using his wealth to insinuate himself into their world of power and privilege while he methodically lays the groundwork of an elaborate plot that he hopes will destroy them. Only young FBI agent <strong>DANICA JORJEVIC</strong> suspects him. Convinced that Victor's elegant international façade masks a criminal identity, she lobbies her boss to investigate him. Victor appreciates Danica's integrity and determination, even as he frustrates her attempts to learn the truth. Their ensuing battle of wits will test Danica's trust in the legal justice system she's sworn to uphold... and reawaken Victor's frozen heart.</p>

			<figure>
				<img src="https://www.jordanmechner.com/images/news/mc/fireworks.jpg">
			</figure>

			<p>My collaborator, the supremely talented Italian illustrator Mario Alberti, has done incredible work bringing Sam, Victor, Abby, Danica, and their rich universe to life. I love these characters, and hope you will too. You can check out the <a href="https://www.jordanmechner.com/books/monte-cristo/">first 10 pages of both volumes</a> online here, and in French comic book stores starting tomorrow.</p>
			<p>Meanwhile, my 30-years-ago journal continues this week on this website's <a href="https://www.jordanmechner.com/library/#1993journals">Library</a> page. On 22 March 1993, I was a 28-year-old American in Paris, discovering the world of European comics for the first time. Hugo Pratt, Jacques Tardi, and Enki Bilal (along with Alexandre Dumas) became key inspirations as I researched and developed the story for my next game, <cite>The Last Express</cite>. <cite>Prince of Persia 2: The Shadow and the Flame</cite> was in its final weeks of playtesting and debugging.</p>
			<p>I couldn't have dreamt then that 30 years later, I'd be back in France and once again immersed in comics, this time not just as a reader, but as an author. I hope fans of <cite>Prince of Persia</cite>, <cite>The Last Express</cite> and my other games will join me in rooting for Sam Castillo and Danica Jorjevic as they fight for justice, each in their way, against enemies so powerful that they seem untouchable. <cite>Monte-Cristo</cite> is my first adventure story set not in a historical or fantastic past, but in our own world of today. I can't wait for you to discover it.</p>
			
		</div>

			
		
		
		</article>

		<article>
			<div>
				<h3>The 1993 Journals: POP2 and The Last Express</h3>
				<p><time datetime="2023-03-16">⌛ March 16, 2023</time>
			</p></div>
			
		
		<div>

			<p>When I launched this website in 2008, I began transcribing and posting daily entries from my old handwritten journals as a "blog from the past," documenting my game-development odyssey making the first version of Prince of Persia in 1985-1993. Later, I released the collected journals as a book, <a href="https://www.jordanmechner.com/books/journals/">The Making of Prince of Persia</a>, followed by a prequel, <a href="https://www.jordanmechner.com/books/the-making-of-karateka/">The Making of Karateka</a> (my even older journals from 1982-1985, when I was in college trying to break into the game industry with my first Apple II games). The response was more enthusiastic than I imagined. <cite>The Making of POP</cite> has since been re-published twice, in a beautiful illustrated hardcover edition from Stripe Press (and in French, from Third Editions).</p>
			
			<figure>
				<img src="https://www.jordanmechner.com/images/news/journals/makjournals.jpg">
			</figure>
			
			<p><cite>The Making of POP</cite> ends in January 1993, at Las Vegas CES, a few months before the release of POP 2: The Shadow and the Flame. I stopped there because, as I wrote in the afterword: "After that, my attention (and what I wrote in my journal) focused more and more on Smoking Car Productions and making The Last Express. Nearly a decade would go by before I'd be hands-on again in the creation of a Prince of Persia title [Sands of Time in 2003]."</p>
			
			<figure>
				<img src="https://www.jordanmechner.com/images/news/journals/lastexpress-onset.jpg">
			</figure>
			
			<p>I'm sometimes asked by people who enjoyed those journals whether I plan to publish a third volume about The Last Express. I've always answered no. The Last Express development was too complex and involved too many people. My journal tells only a small part of the story. There are gaps where I went weeks or even months without writing (I barely found time to sleep). Although it's a fascinating read for me personally, I don't think my 1993-1997 journal in itself would be enough for a stand-alone book entitled <cite>The Making of The Last Express</cite>.</p>
			
		</div>
		<div>

			<p>That said, there is a lot in the journal that I think retro-gaming fans and developers would find interesting. When I set out to make The Last Express in 1993 at age 28, I was in a rare and fortunate position, thanks to the success of Karateka and Prince of Persia. Few creative artists ever get an opportunity to write their own ticket in the ways that were offered to me then. How I navigated those choices — my ongoing struggle to reconcile values of art, business, and life; mistakes I made, things I was blind to, things that miraculously went right — makes for a valuable post-mortem.</p>
			<p>Rereading my journal, seeing my steps and missteps exposed in merciless real-time day by day, I know this is the kind of story I would have loved to read at that juncture in my life. (Hungry to learn from others' hard-won experience, I devoured Steven Soderbergh's and Eleanor Coppola's production diaries of <cite>Sex, lies and videotape</cite> and <cite>Apocalypse Now</cite>.) Non-fiction first-person narratives featuring protagonists with grandiose artistic ambitions who are mature in some ways, painfully immature in others, and spoiled for choice are not so numerous.</p>
			
			<figure>
				<img src="https://www.jordanmechner.com/images/news/journals/lastexpress-directing.jpg">
			</figure>
			
			<p>On February 1, 2023, I posted my first batch of "30 years ago this week" journal entries on Instagram, Facebook, Twitter and Mastodon. Since this year marks the 30th anniversary of POP2's release and Last Express's beginning (and the cancellation of POP3, featuring the mysterious sorceress glimpsed at the end of POP2), I thought it would be a good moment to continue the "making of" narrative. Even if my 1993-96 journals don't make a book, they deserve at least a dedicated page in this website's <a href="https://www.jordanmechner.com/library/">Library</a> section. So here it is: <a href="https://www.jordanmechner.com/library/1993-journals/">The 1993 Journals: Prince of Persia 2 and The Last Express</a>.</p>
			<p>I'll do my best to keep up the weekly Wednesday posts, staying exactly 30 years ahead. That's a pace I think I can confidently handle on top of my other workload. It took the team four years to finish <a href="https://www.jordanmechner.com/games-movies/the-last-express/">The Last Express</a>; I've got enough journal to take us through 2026.</p>
			<p>You can follow my weekly old-journal postings on social media (links are at the bottom of this page; take your pick). Or via this site's RSS feed and/or my monthly e-mail newsletter, which are ad-free and cookie-free.</p>
			<p>See you back in 1993!</p>

		</div>

			
		
		
		</article>

		<article>
			<div>
				<h3>Prince of Packaging</h3>
				<p><time datetime="2023-02-28">⌛ February 28, 2023</time>
			</p></div>
			
		
		<div>
			
			<p>So many video games, films, and music albums I "own" now live in the cloud, and I'm nostalgic for the days when they existed as physical objects on a bookshelf. The tactile quality, size and shape, and cover art of every game box was linked to memories of how I'd acquired it—new, second-hand, or as a gift?—and of hours spent playing.</p>
			<p>For a game developer, a shrink-wrapped box that holds the thing we've been working on for years brings home the reality that our game is truly done. In the pre-internet 1980s and early 90s, before downloadable updates and patches, shipped meant shipped.</p>
			<p>Last month, the sale at auction of American painter Robert Florczak's original artwork for my game Prince of Persia (the Broderbund "red box" edition) triggered memories of the in-house drama surrounding its creation.</p>
			<p>That summer of 1989, I was in the throes of trying to finish and ship Prince of Persia on Apple II, its first platform. I didn't know if it would be a hit or a flop. Thanks to the journal I kept then (a habit since age 17), I can now recall dates and details I'd have otherwise forgotten—like these pencil sketches I did at the end of April to show Broderbund's art director my ideas for the package:</p>
			
			<figure>
				<img src="https://www.jordanmechner.com/images/news/popboxart/popcoversketch1.gif">
			</figure>
			<figure>
				<img src="https://www.jordanmechner.com/images/news/popboxart/popcoversketch2.gif">
			</figure>
			<figure>
				<img src="https://www.jordanmechner.com/images/news/popboxart/popcoversketch3.gif">
			</figure>
		
			<p>As a rule, a game programmer can expect marketing to receive creative suggestions about package design with about as much delight as a surgeon getting advice from a patient on how to operate. My pitch to do a painting in the spirit of old-school Hollywood swashbuckling film posters like Robin Hood (1938) or Raiders of the Lost Ark (1981) earned a "meh." But I had a staunch ally in my product manager Brian Eheler. He made sure I was invited to the marketing meeting. Nine color comps were considered; this one won.</p>
			
			<figure>
				<img src="https://www.jordanmechner.com/images/news/popboxart/popcoverrough.gif">
			</figure>
			
			<p>Florczak, our first-choice artist, developed the idea into a detailed sketch (which he sent by fax—this was before e-mail).</p>
			
			<figure>
				<img src="https://www.jordanmechner.com/images/news/popboxart/popcoverlineart.gif">
			</figure>
			
			<p>Things went smoothly until the head of marketing balked at the $5500 price to execute it. My June 7 <a href="https://www.jordanmechner.com/books/journals/">journal entry</a> records my angst: "After making the rounds and lobbying everyone, I think they'll OK it, but the whole thing was a really disturbing vote of no confidence in POP."</p>
			<p>While I crunched to ship the game I'd been working on for three years, the general feeling at Broderbund was that it wouldn't sell. Foolishly, I'd built Prince of Persia on the Apple II, a decade-old machine that even Apple had stopped supporting. My game had fans at the top and bottom of the company but not in the middle, where the actual marketing got done. Apart from Brian, the QA testers who were playing Prince of Persia daily, and Broderbund's CEO-founder Doug Carlston, few people believed in it.</p>
			<p>In the next four weeks, while Florczak painted (his friend Kevin Nealon, an actor and Saturday Night Live comedian, posed for the vizier Jaffar), I fixed bugs, added features, and spent four days in New York with my dad, adding his newly-composed music to the game.</p>
			<p>In July, Florczak delivered a lovely painting in 1980s movie-poster style—exactly what Brian and I had hoped for.</p>
		
			<figure>
				<img src="https://www.jordanmechner.com/images/news/popboxart/popcovercolor.jpg">
			</figure>
			
			<p>But seeing the finished work, marketing thought it was too pulp-sexy. Broderbund had started as a game publisher; by 1989, its emphasis had shifted to educational and productivity software like The Print Shop and Carmen Sandiego. Prince of Persia was out of sync with the company's new family-friendly direction.</p>
			<p>Marketing sent the painting back to Florczak for revision. I can imagine with what enthusiasm he duly added a green Persian sports bra to the princess's decolletage. Personally, I preferred the original; but as I wrote in my <a href="https://www.jordanmechner.com/books/journals/">journal</a> on July 25: "There are battles you win and battles you lose, and in the big picture, this one is pretty meaningless."</p>
			<p>Then the whole thing nearly crashed at the final hurdle. The box was shown at a company-wide meeting. A group of employees wrote to the CEO, saying the package condoned violence against women and requesting that it be scrapped. Doug gave a balanced two-page reply, acknowledging their valid concerns ("We don't want Broderbund ever to be seen in such a light"), but defending Jaffar's threatening gesture as nonetheless appropriate for a villain in a game whose hero could be "impaled, sliced in two, squashed and otherwise discomforted for relatively minor lapses in behavior." After a tense week of debate, the box was approved.</p>
			
			<figure>
				<img src="https://www.jordanmechner.com/images/news/popboxart/popbox.jpg">
			</figure>

			<p>The rest is history... sort of. Prince of Persia shipped on Apple II in September 1989, PC in April 1990, then Amiga. It got rave reviews on all three platforms. And it was a flop.</p>
			<p>By July 1990—ten months after launch, three months after the much-anticipated (by me) PC release—fewer than 10,000 red boxes had found their way into gamers' homes. I recorded in my journal: "POP sold 500 units last month on PC, 48 on Apple. That's about as dead as can be." In August, the major chain Electronics Boutique de-listed Prince of Persia due to lack of sales. Chilled, I visited the local mall where my game could no longer be found and was told by a saleswoman: "It's a great game, but the box was horrible."</p>
			<p>Over the next two years, in a miraculous turnaround that would scarcely be possible today, Prince of Persia was gradually, then suddenly, saved by a confluence of events. First, foreign and console versions, which Broderbund had sublicensed in a dozen different countries on platforms like Nintendo NES, Sega Master System, and NEC 9800, began to ship. There was no coordination; it was the Wild West. Each sublicensee did its own packaging, marketing launch, PR, and distribution, not overseen by Broderbund. The U.S. release flopped, but some of those overseas and console ports became hits.</p>
		
			<figure>
				<img src="https://www.jordanmechner.com/images/news/popboxart/popcovers.jpg">
			</figure>
			
			<p>Some licensees used the red-box artwork, others created their own. For the most part, I didn't see packages until they shipped. Domark's box art for the UK Sega version made me wince; I still find it offensive, even by that epoch's standards. It was too late for them to redo the package, but Brian made them promise never to use it outside the UK. (They promised, but forgot.) At the opposite extreme, I loved Katsuya Terada's gorgeous illustration for the Japanese Nintendo Super FamiCom version. It's a fan favorite as well; French book publisher Third Editions used Terada's artwork for the cover of their <a href="https://www.thirdeditions.com/81_jordan-mechner" target="_blank">deluxe collectors' edition</a> of my old journals.</p>
			
			<figure>
				<img src="https://www.jordanmechner.com/images/news/popboxart/popterada.jpg">
			</figure>
			
			<p>The second unanticipated factor that saved Prince of Persia was that the Mac port—which I'd subcontracted to friends at Presage Software—ran two years over schedule. Between 1989 and 1992, Apple released a series of new Mac models: black-and-white and color, with different-sized screens. The Presage team, wanting to take advantage of the latest capabilities, went back to the drawing board and redid the graphics sprites three times. (Each time, I tore my hair out.)</p>
			<p>By the time the Mac version was finally ready, Prince of Persia's overseas successes had given Brian and me ammunition to persuade Broderbund marketing that the game had untapped potential. Doug okayed our proposal to combine the Mac release with a PC re-release in a bigger, solidly constructed 1990s-style "candy box," which we hoped retailers and customers would perceive as denoting a higher-quality product than the flip-top, flimsy-cardboard red box (even though the .exe file on the PC disks hadn't changed).</p>
			<p>San Francisco designer Hock Yeo, of Wong &amp; Yeo, designed a two-piece candy box with an unusual shape reminiscent of an hourglass. If you're a PC or Mac gamer who played Prince of Persia in the U.S. in the 1990s, this is the box you most likely remember.</p>
			
			<figure>
				<img src="https://www.jordanmechner.com/images/news/popboxart/popmacbox.jpg">
			</figure>
		
			<p>The dual Mac-PC release in the oddly-shaped box turned the prince's fortunes around. A previously untapped cohort of gamers—among them, journalists and editors who used Macs for desktop publishing—were excited to have a game they could play on their new color screens. Prince of Persia became the #1-selling Mac game at a time when most game publishers considered the Mac market too small to bother with. Prince of Persia went from ice-cold to hot on PC as well. Two years after its failed first PC launch, Prince of Persia became a hit.</p>
			<p>I was reminded of all this when Florczak's artwork popped up on an auction website in December. (Doom co-creator John Romero, an Apple II aficionado, spotted it and sent me the link.) The last time I'd seen the full painting unobscured by a title, logo and stickers, it was propped on a desk in Broderbund's marketing office. It hung for 33 years on Kevin Nealon's wall, a thank-you from the artist for modeling the Vizier.</p>
			<p>Seeing it again, now that its role in the drama of that summer of 1989 is ancient history, I can appreciate the painting as an artwork in its own right. The green stripe still bugs me. But a flaw in a Persian carpet only makes the whole more beautiful. And if there's one thing video games have taught us, it's that timing is everything. (The collector whose $63,000 bid won last week's auction would surely agree.) Florczak's painting joins the ever-expanding collection of diverse physical objects, of all sizes and shapes, that form the tangible record of a video game character's intangible digital existence.</p>
		
		</div>
		
		
		</article>

		<article>
			<div>
				<h3>A Prisoner Escapes</h3>
				<p><time datetime="2023-02-21">⌛ February 21, 2023</time>
			</p></div>
			
		
		<div>

			<p>A huge thank you to everyone who bought a print of "A Faithful Friend" last month! I was really touched by the warm response from Prince of Persia players who remembered the princess's brave little companion. I hoped my drawing would evoke fond memories; I didn't expect the entire edition of 40 prints to sell out in less than 24 hours.</p>
			<p>A number of people wrote to say they wished they'd heard about the release sooner. I cannot print more of "A Faithful Friend" (that's the nature of a limited edition), but I've gone ahead and drawn a second author's tribute artwork, inspired by a gameplay moment in the Prince of Persia dungeon. I'm calling this one "Bones." If you've played level 3, I'm sure you can guess the reason.</p>

			<figure>
				<img src="https://www.jordanmechner.com/images/news/artworks/bones.jpg">
			</figure>
			
			<figure>
				<iframe width="100%" height="300" data-src="https://www.youtube.com/embed/Q3M4R42d3qY?rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe>
			</figure>
			
			<p>I've enjoyed creating these artworks. In the past, when I've put pen on paper to draw the world of Prince of Persia, the purpose was to clarify an idea in my head or communicate it to the team during development. To revisit that universe now as a visual artist, seeing the games through the lens of decades of personal memories, is a wonderfully pleasant experience for me.</p>

		</div>
		<div>

			<p>"Bones" is available as a giclée print in a signed and numbered limited edition of 40, exclusively <a href="https://www.jordanmechner.com/art-prints/bones/">here</a>.</p>
			<p>I'm excited to make a second announcement especially for French readers. Book Two of Monte-Cristo, my new graphic novel trilogy with the wonderful illustrator Mario Alberti, will be in bookstores in France on March 22. It's a tale of thwarted love, unjust imprisonment, and a daring escape — a modern update of the Alexandre Dumas classic transposed to post-9/11 America.</p>
			<p>In 2005, Sam Castillo is a happy young man—promoted to contractor, engaged to his sweetheart Abby—until three enemies conspire to frame him as a terrorist. Rendered to a black-site prison an ocean away, Sam befriends a brilliant, multi-lingual fellow detainee who educates him in the ways of the world... and bestows on him the key to a secret fortune. 17 years later, Sam resurfaces with a new identity as enigmatic billionaire Victor Sirin, and a plan to take revenge against the three men who stole his life.</p>
			<p>Monte Cristo T2: The Island will be released in France by Editions Glénat on March 22. You can read about it, preview it live, and pre-order it online <a href="https://www.jordanmechner.com/books/monte-cristo/">here</a>.</p>
			
			<figure>
				<img src="https://www.jordanmechner.com/images/news/mc/book2-cover.jpg">
			</figure>

		</div>

			
		
		
		</article>

		<article>
			<div>
				<h3>A Faithful Friend</h3>
				<p><time datetime="2023-01-24">⌛ January 24, 2023</time>
			</p></div>
			
		
		<div>

			<p>Happy New Year! 2023 will be an exciting year, with new releases and announcements lined up.</p>
			<p>To start off January, I want to share a nostalgic artwork that I was recently inspired to create. It's a tribute to a delightful moment Prince of Persia fans may remember from the original 2D game. I've titled it "A Faithful Friend."</p>

			<figure>
				<img src="https://www.jordanmechner.com/images/news/artworks/a-faithful-friend.jpg">
			</figure>

			<p>The following video clip (from Level 8) shows why the little white mouse — sent by the princess to help the prince in a dark dungeon moment — is one of my favorite characters. I added the mouse to the game in August 1989, when Prince of Persia was already well into beta testing. Today, no publisher would let a developer slip in a feature like that at the last minute.</p>

		</div>
		<div>

			<figure>
				<iframe width="100%" height="300" data-src="https://www.youtube.com/embed/Ag_yDAPsFD4?rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe>
			</figure>

			<p>I drew "A Faithful Friend" as an author's tribute, not just to a memorable moment in a game that's meant so much to me, but to the teams, collaborators, and fans who have supported and kept its legacy vibrant for 33 years. Without you, there'd be no Prince of Persia.</p>
			<p>If you'd like to own a hand-signed limited edition giclée print of this original artwork, "A Faithful Friend" is <a href="https://www.jordanmechner.com/art-prints/a-faithful-friend/">available here</a>. Tomoe, my local fine-art printer in Montpellier, printed 40 in total. I've stamped, signed and numbered them. Once they're sold out, the edition won't be reprinted; this protects its value for collectors.</p>
			<p>Having spent most of the past four decades creating digitally, I appreciate more and more the tactile qualities of handmade physical objects. My ink line these days is finer than was possible on a 280 x 192 computer, but I've respected the restricted Apple II color palette. As for the 8-bit hand stamp (my personal logo), I expect old-school gamers will quickly recognize its source.</p>
			<p>I'll share next month's announcements here in this space, and in my monthly newsletter. As subscribers already know, I've also recently joined Mastodon, and will be tooting there as well. Thanks for following!</p>

		</div>
		
		
		</article>

		<article>
			<div>
				<h3>On Graphic Novel Writing</h3>
				<p><time datetime="2022-07-05">⌛ July 5, 2022</time>
			</p></div>
			
		
		<div>

			<p>Thanks to all the early adopters who showed up for the French launch of Monte Cristo, Book One! Our May festival and bookstore signings were a great occasion for illustrator Mario Alberti and me to see each other in person. (Mario lives and works in Trieste, I'm in Montpellier.)</p>
			<p>Here's a sneak peek at our work-in-progress on Book Two of the trilogy: "The Island," on track for early 2023 release. Mario has drawn the first 19 pages (only 51 to go!), working in B&amp;W grayscale, with color to be added at the end. He began by storyboarding the full 70-page book, working from my script; we thrashed out details via Slack and Zoom.</p>

			<figure>
				<img src="https://www.jordanmechner.com/images/news/mc/mct2-bw-ex.gif">
			</figure>

			<p>For those curious about the graphic-novel collaborative process, here are the script and rough storyboard excerpts for the panels above:</p>

			<figure>
				<img src="https://www.jordanmechner.com/images/news/mc/mct2-script-ex.gif">
			</figure>

		</div>
		<div>

			<figure>
				<img src="https://www.jordanmechner.com/images/news/mc/mct2-sb-ex.gif">
			</figure>

			<p>Every project, and writer-artist pairing, is unique. Sometimes writers dictate page layouts and panel compositions in detail. For me to do that with an artist of Mario's caliber would feel like telling a film D.P. what lens to use. I "see" panels in my mind's eye as I write, but that doesn't mean Mario needs to draw them exactly that way. A comics artist's job is like a film director, D.P., set designer, cast and crew rolled into one. In writing, whether for a graphic novel or film/TV, I try to suggest my ideas for panels and shots (and casting, and actors' performances...) indirectly through word choice and phrasing, rather than "do it this way." I want the script to be specific enough to make scenes and moments come alive in the reader's (director's, illustrator's, actor's) imagination — then leave them enough room to create those moments anew as only they can.</p>
			<p>That said, to fit a dense, complex story into 70 large-format pages is a writer's, not an illustrator's, job. In my script for Monte-Cristo, I do specify page breaks. (I knew the panels above would be near the bottom of page 2, and that it would be a left-hand page.) But again, every project is different. For an in-depth look at the creative process on another graphic novel — Templar, with illustrators LeUyen Pham and Alex Puvilland — check out this <a href="https://www.jordanmechner.com/books/templar/#free-downloads">free 86-page e-book</a>.</p>
			<p>Now, back to work — Mario on pages 20-21, and me (since I've already written the scripts for the three books) on projects not yet announced. Monte Cristo T1: "The Prisoner" is in French comic book stores now. You can read about it (and read reviews, and download color PDF excerpts) <a href="https://www.jordanmechner.com/books/monte-cristo/">here</a>.</p>

		</div>

			
		
		
		</article>

		<article>
			<div>
				<h3>A Last Express Milestone</h3>
				<p><time datetime="2022-04-19">⌛ April 19, 2022</time>
			</p></div>
			
		
		<div>

			<p>The Last Express left the station 25 years ago this month, on 3 CD-ROMs — shipping on PC and Mac in April 1997 after an intense 4-year development. It's a game that will always be close to my heart. In honor of the anniversary, Dotemu is <a href="https://www.jordanmechner.com/projects/the-last-express/">offering the game at 75% discount</a> on iOS and Steam from April 19-May 3 (on Android until April 26).</p>
			<p>To mark the occasion, I'm adding two new items to the Last Express section on this site's <a href="https://www.jordanmechner.com/library/">library page</a>: The original 1993 game script that Tomi Pierce and I wrote for the game production, and the movie adaptation I wrote for director Paul Verhoeven in 2010 (three years after his Black Book, which I loved). We never got to make the movie, so you'll have to judge for yourself whether it would have worked on screen.</p>

		</div>
		
		
		
		</article>

		<article>
			<div>
				<h3>French Dispatch</h3>
				<p><time datetime="2022-03-28">⌛ March 28, 2022</time>
			</p></div>
			
		
		<div>

			<p>I've had my French driver's license and mobile phone for two years now, but it took me a little longer to get this website switched over. As of today, <a href="https://www.jordanmechner.com/">jordanmechner.com</a> is bilingual. (To change languages, click on the icon in the upper right.)</p>
			<p>Now, I get to make my first announcement in French and English: In May 2022, Glénat/Comixburo will release Volume 1 of my new graphic novel trilogy, Monte Cristo. It's a modern update of Alexandre Dumas' timeless tale of betrayed love, revenge, and redemption, transposed to post-9/11 America and today's globalized world. The illustrator is the wonderful Mario Alberti (The Wall).</p>
			<p>Monte Cristo will be published first in France as three 72-page hardcover volumes. Volume 1, "The Prisoner," will be in bookstores in early May; it's now available for pre-order. Volumes 2 and 3, "The Island" and "The Storm", will be released next year. If you'd like to be notified when an English version is announced, you can subscribe to my monthly e-mail newsletter <a href="https://www.jordanmechner.com/">here</a>.</p>

		</div>
		<div>

			<p>My last two book launches (Samak the Ayyar and The Making of Prince of Persia) were virtual, for 2020-21 reasons. I'm happy to say that for Monte Cristo, Mario Alberti and I will be doing in-person book signings in Paris, Montpellier, and a few other cities. I hope to see some of you there. I can't wait for readers to discover this new adventure.</p>
			<p>Details on Monte Cristo (and link to order online) are <a href="https://www.jordanmechner.com/books/monte-cristo/">here</a>.</p>

			<figure>
				<img src="https://www.jordanmechner.com/images/news/mc/montecristo.jpg">
			</figure>

		</div>
		
		
		</article>

		<article>
			<div>
				<h3>Summer Reading</h3>
				<p><time datetime="2021-07-26">⌛ July 26, 2021</time>
			</p></div>
			
		
		<div>

			<p>This week, I'm excited to share two new book releases.</p>
			<p>
				<cite>Samak the Ayyar</cite> is a wonderful, thousand-year-old Persian adventure saga that I've had the honor to adapt in its first English-language translation. It's the source material my Prince of Persia games (and movie) always wanted but never had. You can <a href="https://www.jordanmechner.com/projects/samak/">read about the project's origins (and download a free sample chapter) here</a>.
			</p>
			<p>You can order <cite>Samak</cite> on Amazon, or direct from Columbia University Press, via the <a href="https://www.jordanmechner.com/store/samak-the-ayyar">store</a>. If you order from the publisher, enter the code CUP20 for a 20% discount.</p>
			<p>My other July book release is the third in a trilogy: <cite>Year 3 in France</cite>, 166 pages of my sketchbook journal from 2018-19, the third year after I moved to France from L.A. for a video game project. Like the first two volumes, it's a small, high-quality print run from local publisher Tomoe.</p>
			<p>I've signed a stack of books, so the first 30 people to order Year 3 from the online <a href="https://www.jordanmechner.com/store/sketch-journal-year-3-in-france">store</a> will receive signed copies.</p>
			<p><cite>Year 2</cite> has sold out its print run, but you can get signed copies of all three books at <a href="https://www.chicagogamespace.com/exhibitions/sketchjournalofjordanmechner" target="_blank">Chicago Gamespace</a>, where my sketch art is on exhibit thru August 23. It's a unique space dedicated to video game culture and art, well worth a visit if you're in the Chicago area.</p>

		</div>
		<div>

			<p>The Chicago show also includes a new print from <cite>Year 3</cite>: "Les Beatnik Modernes", in a <a href="https://www.jordanmechner.com/store/beatnik-modernes">signed and numbered limited edition of 10</a>. It's a sketch I did in May 2019 at a café just up the street. I've missed sketching in cafés, and can't wait to rekindle the habit.</p>

			<figure>
				<img src="https://www.jordanmechner.com/images/news/artworks/rassouli.jpg">
				<figcaption>Rassouli in his studio, as we started our collaboration to translate and adapt <cite>Samak</cite>. (From my sketch journal <cite>Year 2 in France</cite>.)</figcaption>
			</figure>

		</div>
		
		
		</article>

		<article>
			<div>
				<h3>A Tale of Ancient Persia</h3>
				<p><time datetime="2021-02-16">⌛ February 16, 2021</time>
			</p></div>
			
		
		<div>

			<p>I'm excited to share a very special project. It's been my honor to adapt a wonderful, thousand-year-old Persian adventure saga in its first English-language edition — <cite>Samak the Ayyar</cite>.</p>
			<p>Despite having spent a certain number of the past 30 years delving into Persian culture and lore for video game and film development-related purposes, I'd never heard of Samak (or ayyars) until the day my translator and collaborator Freydoon Rassouli took down a dusty out-of-print volume from his shelf and said: "This is what you've been looking for."</p>
			<p>As he began reading to me from its pages, translating on the fly from archaic Persian, shivers ran down my spine. Here was a fantastic adventure set in ninth-century Persia, featuring a treacherous vizier, a star-crossed romance between a noble prince and princess, kings, warriors, and an agile trickster hero who scales walls and sneaks into palaces. It was the source material my Prince of Persia games (and movie) had always wanted but never had. But since I don't read Persian (and even most Persians don't read 900-year-old manuscripts), I couldn't read it.</p>
			<p>I really, really needed to read that book. So... we wrote it. <cite>Samak the Ayyar</cite> will be released in paperback this August from Columbia University Press.</p>
			<p>What are ayyars? A concept as specific to Persia as ronin and samurai are to Japan, and as universal. Samak is a hero and bandit, a man of the people with the skills of a ninja and the ideals of a knight. You could call him a Persian Robin Hood, but he and his band of male and female ayyars have a unique and compelling spirit all their own. Armed with a dagger, a lasso, and his wits, he accomplishes things even kings can't.</p>

		</div>
		<div>

			<figure>
				<img src="https://www.jordanmechner.com/images/news/samak/bodleian.jpg">
			</figure>
			<p>If you appreciate the <cite>1001 Nights</cite>, or classic tales of world folklore, I hope you'll be as enchanted by Samak's adventures as I am. You can read more about the book (and pre-order it, once it becomes available in your territory) <a href="https://www.jordanmechner.com/store/samak-the-ayyar">here</a>.</p>

		</div>
		
		
		</article>

		<article>
			<div>
				<h3>Why I Keep a Journal</h3>
				<p><time datetime="2020-08-25">⌛ August 25, 2020</time>
			</p></div>
			
		
		<div>

			<p>
				I've always been fascinated by other people's stories. In my twenties — dreaming of making video games, books, movies — I devoured memoirs and interviews with my role models, hungering for insight on how they'd done it. Published journals were most rare and valuable of all, because they were a raw record of experience: written in the heat of the moment, not shaped and burnished into a smooth narrative with hindsight.
			</p>
			<p>
				I was 17 when I started keeping my own journal. I kept up the habit, filling dozens of spiral-bound notebooks over the years. I thought I'd never show them to anybody.
			</p>
			<p>
				The cumulative power of daily practice is well known but still amazes me. Ten years ago, my brother David picked up a ukulele and started strumming. Now he's a ukulele player. A behavior becomes a routine, a habit, and finally a trait. The things we do every day shape us, literally: We become a guitarist, a smoker, a programmer or athlete or stoner, by doing something for the first time, then keeping it up.
			</p>
			<p>
				I'm a journal-keeper. With over a hundred notebooks filled since 1982, it's become part of who I am. I couldn't have expected or anticipated all the ways my new habit would enrich my life.
			</p>
			<p>
				Even if we never reread what we write in our journals, the act of writing changes us. It shapes our perceptions and memory. Over time, opening the notebook and picking up the pen becomes like resuming a long-running conversation with a friend. We develop a voice, even though there's no one on the other end to hear it — or rather, our self is listening.
			</p>
			<p>
				I decided to publish my own 1980s journals — begun as a Yale college freshman, while I was making my first video games, Karateka and Prince of Persia — when enough years had passed that their value as a time capsule outweighed my embarrassment. I still cringe rereading certain entries, but I'm glad the journals exist. They contain hard-won experience I wish I could have had the benefit of when I was 20.
			</p>
			<p>
				Keeping a journal has special value for anyone engaged in a creative project. Reading pages written a year ago, or five, or twenty, can help reveal the big arc of our lives, and illuminate the present. Past journal entries remind us of intentions, resolves, lessons forgotten. They bring home how much of our worries, schemes and plans are transient, even quaint in retrospect.
			</p>
			<figure>
				<img src="https://www.jordanmechner.com/images/news/artworks/why-journal.jpg">
			</figure>

		</div>
		<div>

			<p>In the four years it took me to make the first Prince of Persia game on the Apple II, my journal did more than record my creative process: it was part of it. I used my notebook as a sounding board — wrestling with design challenges, discarding ideas and sparking new ones in the act of writing. In dark moments I poured out my angst, questioned whether I was on the right path, if the game was even worth finishing. More than once, my journal brought me back from the brink and helped me find the clarity and confidence to continue. Some entries capture the exact moment of illumination when I hit upon a solution I'd been groping toward in the dark. For all the digital and technological advancement of the past half-century, pen and paper may still be the tool that comes closest to being able to record thought.</p>
			<p>
				For every entry that makes me feel smart, there's a youthful wise reflection like this one: "The games business is drying up. There's no guarantee there will even be a computer games market a couple of years from now." (July 1985) Or: "I've grown middle-aged these last few years. Roland is 23 but he's still young at heart." (Written when I was 22.) Rereading such passages is a joy that only journal-keepers know.
			</p>
			<p>
				The final PC version of Prince of Persia that shipped thirty years ago, in April 1990, is so familiar now it feels inevitable. It's easy to forget that it was once a fragile thing in flux. My journal reminds me of roads not taken, of how easily things could have turned out differently.
			</p>
			<p>
				These days, I keep my journal in a Hobonichi Techo — a compact format that reinforces the practice of one page a day, neatly fitting a year into the palm of my hand, a decade in a shoebox. I've found poignant solace in this month of confinement, April 2020, flipping back a few dozen pages to see how many of my concerns and decisions of February have been rendered irrelevant, while a few mattered more than I knew.
			</p>
			<p>
				A journal keeps us honest and tethers us to truth. In George Orwell's 1984, the protagonist's first act of resistance to brainwashing is to start secretly keeping a diary — a crime not explicitly forbidden, but punishable by death, because it threatens a totalitarian state's power to retroactively rewrite history.
			</p>
			<p>
				Like a yardstick calibrated in millimeters, a journal holds both the detail and the scope of life. Our human condition is to live one moment at a time; we're never given more than that. Of all the gifts journaling gives us, maybe that's the greatest: The simple practice of making daily marks on paper, like mental push-ups, can strengthen the part of us that tries to rise above the timeline, to see a pattern and bigger picture — and, paradoxically, also strengthen the part of us that can learn to treasure the present moment.
			</p>
			<p>Originally published in <a href="https://medium.com/swlh/why-i-keep-a-journal-b3ef4b33cbd3" target="_blank">Medium</a>, May 2020.</p>

		</div>

			
		
		
		</article>
	</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Flock Reinstalls Cameras Without City Approval After Unlawful Govt Access (289 pts)]]></title>
            <link>https://evanstonroundtable.com/2025/09/24/flock-safety-reinstalls-evanston-cameras/</link>
            <guid>45382434</guid>
            <pubDate>Fri, 26 Sep 2025 03:51:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://evanstonroundtable.com/2025/09/24/flock-safety-reinstalls-evanston-cameras/">https://evanstonroundtable.com/2025/09/24/flock-safety-reinstalls-evanston-cameras/</a>, See on <a href="https://news.ycombinator.com/item?id=45382434">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		
		

<p>Private surveillance vendor Flock Safety reinstalled all of its stationary license plate cameras in Evanston that had previously been removed, apparently doing so without authorization from the city, which sent the company a cease-and-desist order Tuesday afternoon demanding that the cams be taken back down.</p>

<p>The city previously <strong><a href="https://evanstonroundtable.com/2025/08/26/evanston-shuts-down-license-plate-cameras-terminates-contract-with-flock-safety/" target="_blank" rel="noreferrer noopener">ordered Flock to shut down</a></strong> 19 automated license plate readers (18 stationary and one flex camera that can be attached to a squad car) provided by the company and put its contract with Flock on a 30-day termination notice on Aug. 26. </p>

<p>This decision came after Illinois Secretary of State Alexi Giannoulias discovered that Flock had allowed U.S. Customs and Border Protection to access Illinois cameras in a “pilot program” against state law, and after the RoundTable reported in June that out-of-state law enforcement agencies were able to search Flock’s data for assistance in immigration cases.</p>

	<div data-posts="" data-current-post-id="507141">
							<h2>
					<span>previous coverage</span>
				</h2>
						
	<article data-post-id="490536">
							<figure>
								
				
							</figure><!-- .featured-image -->
		
		<!-- .entry-wrapper -->
	</article>

		
	<article data-post-id="492240">
							<figure>
								
				
							</figure><!-- .featured-image -->
		
		<!-- .entry-wrapper -->
	</article>

		
	<article data-post-id="494616">
							<figure>
								
				
							</figure><!-- .featured-image -->
		
		<!-- .entry-wrapper -->
	</article>

				</div>
	
<p>Flock had removed 15 of the 18 stationary cameras by Sept. 8, only to reinstall each one at or near its prior location by Tuesday. City spokesperson Cynthia Vargas said in a written statement that the city has not deviated from or made any changes to its policies “since the earlier contract termination, meaning Flock reinstalled the cameras without the city’s permission.”</p>

<p>“Recently, we became aware that Flock has reinstalled the physical cameras that they had previously taken down,” Vargas wrote. “We immediately issued a cease-and-desist order to Flock. Earlier this afternoon, Flock committed to promptly removing the cameras.”</p>

<p>Flock did not immediately respond to a request for comment from the RoundTable on Tuesday night.</p>

<p>The city first installed Flock cameras in late 2022 and early 2023 as part of two separate one-year contracts, and City Council later approved a single five-year contract extension in January 2024. </p>

<p>The city has paid the first two years of that extension but would still owe $145,500 for the final three years if the contract is upheld. The city intends to terminate the contract on Sept. 26 under its notice to Flock, but the company is challenging that termination, and the dispute could escalate to litigation.</p>


<h4>Same spots, with some different models</h4>

<p>The RoundTable mapped and photographed each of the 18 stationary cameras in June, and site visits on Sept. 8 confirmed that all but three had been removed by Flock. The last three, which appear to have never been removed, are the north-facing cameras at Howard Street’s intersections with Chicago, Ridge and Dodge avenues.</p>



<p>Further site visits Tuesday confirmed that the 15 removed cameras had been replaced at the same locations. Most of them were banded back onto public streetlight fixtures where they were placed before, while five located on east-west streets along McCormick Boulevard had individual poles reinstalled into the ground. Near three of these pole mounts were freshly spray painted lines, the word “FLOCK” and numbers appearing to designate the cameras individually.</p>

<p>A Reddit user posted a photo to <strong><a href="https://www.reddit.com/r/evanston/comments/1no1gby/flock_cameras_apparently_being_reinstalled/" target="_blank" rel="noreferrer noopener">the r/Evanston subreddit</a></strong> on Monday evening showing a worker installing one of these pole mounts and its camera earlier that morning at the corner of McCormick and Main Street.</p>

<p>The worker is seen on a ladder holding the camera’s solar panel in front of the pole mount, and behind them is an Enterprise-branded rental van parked on the sidewalk in front of the sign for the Skokie Northshore Channel Park. Although this camera and the one at McCormick and Oakton Street are installed outside of Evanston’s city limits, they both fall under Evanston’s contract with Flock, rather than Skokie’s.</p>



<p><em>Click on the images in the gallery above to see them full screen.</em></p>


<p>Additionally, not all of the reinstalled cameras were “Falcon” models — the long, oval-shaped camera with a solar panel and battery packs that was previously used in every location.</p>

<p>At five locations, there was instead a stubbier camera that looks similar to the “Standard” model currently advertised on Flock’s website, except with an extra attachment under the main body. These five also appear to lack solar panels, instead attaching to several previously unseen boxes, and at least one camera is attached to a wire connected to the city-owned light post it’s mounted to, suggesting it may draw power from the city’s grid.</p>



<p><em>Click on the images in the gallery above to see them full screen.</em></p>

<h4>Analysis: Flock’s data suggests cams could be active</h4>

<p>Even before any cameras were initially removed, none of them were supposed to be collecting any data. The city wrote in its Aug. 26 announcement that the 19 cameras were “no longer collecting or providing license plate reader data to the Flock network,” and EPD Cmdr. Scott Sophier reconfirmed this to the RoundTable on Sept. 8.</p>

<p>“The last read on an Evanston Flock camera was logged shortly before 1:00 p.m. on August 26th, which is consistent with the City’s request for de-activation,” Sophier said at the time.</p>


<p>However, Flock’s own publicly available data suggests that may not be the case. </p>

<p>The company maintains a “transparency portal” webpage for Evanston that updates daily with basic data on the cameras’ operations, including “Number of LPR [license plate readers] and other cameras” and “Vehicles detected in the last 30 days.” The RoundTable has tracked this page since shortly after the city’s shutdown order, logging the data and archiving updates on most days.</p>

<figure><img decoding="async" width="1024" height="707" data-attachment-id="514470" data-permalink="https://evanstonroundtable.com/screenshot-2025-09-24-at-2-17-30-am/" data-orig-file="https://evanstonroundtable.com/wp-content/uploads/2025/09/Screenshot-2025-09-24-at-2.17.30-AM.png" data-orig-size="1149,793" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="flock evanston transparency portal 25.09.23" data-image-description="" data-image-caption="<p>A screenshot shows the data listed on Flock’s transparency portal for Evanston on Sept. 23, 2025.</p>
" data-medium-file="https://evanstonroundtable.com/wp-content/uploads/2025/09/Screenshot-2025-09-24-at-2.17.30-AM-300x207.png" data-large-file="https://evanstonroundtable.com/wp-content/uploads/2025/09/Screenshot-2025-09-24-at-2.17.30-AM-1024x707.png" src="https://evanstonroundtable.com/wp-content/uploads/2025/09/Screenshot-2025-09-24-at-2.17.30-AM-1024x707.png" alt="" srcset="https://evanstonroundtable.com/wp-content/uploads/2025/09/Screenshot-2025-09-24-at-2.17.30-AM-1024x707.png 1024w, https://evanstonroundtable.com/wp-content/uploads/2025/09/Screenshot-2025-09-24-at-2.17.30-AM-300x207.png 300w, https://evanstonroundtable.com/wp-content/uploads/2025/09/Screenshot-2025-09-24-at-2.17.30-AM-768x530.png 768w, https://evanstonroundtable.com/wp-content/uploads/2025/09/Screenshot-2025-09-24-at-2.17.30-AM-780x538.png 780w, https://evanstonroundtable.com/wp-content/uploads/2025/09/Screenshot-2025-09-24-at-2.17.30-AM-400x276.png 400w, https://evanstonroundtable.com/wp-content/uploads/2025/09/Screenshot-2025-09-24-at-2.17.30-AM-706x487.png 706w, https://evanstonroundtable.com/wp-content/uploads/2025/09/Screenshot-2025-09-24-at-2.17.30-AM.png 1149w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>A screenshot shows the data listed on Flock’s transparency portal for Evanston on Sept. 23, 2025. <span><span>Credit:</span> Flock Safety</span></figcaption></figure>

<p>The “Number of LPR and other cameras” figure was at 19 when the shutdown was ordered, matching Evanston’s 19 cameras, but it later dropped to 10 on Aug. 30. Rather than falling to zero, however, the figure stayed at 10 until Sept. 16, when it increased to 12, eventually returning to 19 on Sept. 23, matching the reinstallation of all the cameras.</p>

<p>Meanwhile, the “Vehicles detected in the last 30 days” number has steadily decreased since the shutdown order, with each passing update rolling off another day when the cameras were known to be active. However, the figure has not decreased enough over time to actually reach zero once 30 days have passed.</p>

<p>When the RoundTable began tracking this figure on Aug. 28, it stood at 439,542 vehicles detected over approximately 28 days of active cameras. To reach zero by 30 days post-shutdown, the figure would need to drop by an average of around 15,700 each day, because every new day added to the data should have included zero new vehicles detected.</p>


<p>Based on the city’s Aug. 26 termination notice, there should only be two full days’ worth of vehicle detections left on Flock’s data portal as of late Tuesday, Sept. 23. But the page still reports 155,507 vehicles detected in the last 30 days, yielding a reduction of 284,035 vehicles over 26 days, or around 10,924 per day — well below the reduction rate needed to reach zero.</p>

<p>This trend means that on Friday, Sept. 26, when more than 30 days will have passed since the city’s cameras were supposed to be shut down, Flock will still report some number of vehicles as being detected in the prior 30 days. That suggests some number of cameras may have remained active and logging vehicles after Aug. 26, in violation of the city’s order and without the city’s knowledge, as indicated by Sophier’s response to the RoundTable on Sept. 8.</p>

<p>“Flock has not indicated to the City in direct communications that any ALPR’s are active or have been re-activated,” Sophier wrote. “There is no indication that Flock did not honor/fulfill the City’s request and also no indication on the City’s end to show any plate reads since the aforementioned date/time.”</p>

<p>Flock did not answer questions about this data sent by the RoundTable on Sept. 8. Site visits by the RoundTable that day confirmed that the 15 aforementioned cameras had been removed by that time.</p>

<p><strong><em>Update: </em></strong><em>The City of Evanston has <strong><a href="https://evanstonroundtable.com/2025/09/25/city-covers-up-flock-cameras-while-waiting-for-removal/">covered up the Flock cameras</a></strong> while waiting for their removal</em>.</p>




	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My Deus Ex lipsyncing fix mod (242 pts)]]></title>
            <link>https://www.joewintergreen.com/my-deus-ex-lipsyncing-fix-mod-making-of/</link>
            <guid>45382397</guid>
            <pubDate>Fri, 26 Sep 2025 03:45:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.joewintergreen.com/my-deus-ex-lipsyncing-fix-mod-making-of/">https://www.joewintergreen.com/my-deus-ex-lipsyncing-fix-mod-making-of/</a>, See on <a href="https://news.ycombinator.com/item?id=45382397">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Back in 2021 I made a mod for Deus Ex 1 that fixes the lipsyncing and blinking, which, I betcha didn’t know, was broken since ship. Everything I wrote about it is on Twitter, and it oughta be somewhere else, so here’s a post about it. The mod itself can be downloaded <a href="https://www.moddb.com/mods/joe-wintergreens-deus-ex-lipsync-mod" data-type="link" data-id="https://www.moddb.com/mods/joe-wintergreens-deus-ex-lipsync-mod">here</a>.</p>



<figure><p>
<iframe title="Joe Wintergreen's Deus Ex Lipsync Mod v1" width="500" height="281" src="https://www.youtube.com/embed/oxTWU2YgzfQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p></figure>



<p>I guess I was playing DX1 and thinking, geez, was this lipsync always this bad? In a weird way? It’s insta-snapping mouth shapes, but they’re not always the same mouth shapes. Is this broken? I couldn’t find anything online about it, but I did find <a href="https://www.gamedeveloper.com/design/classic-tools-retrospective-the-tools-that-built-deus-ex-with-chris-norden" target="_blank" rel="noreferrer noopener">this article</a>: an interview with Chris Norden, a coder on DX, where he goes into the lipsyncing and how it was, at one point, super elaborate and amazing, and they had to pare it back for performance reasons. I thought I’d check how much of this was done in Unrealscript (since the C++ source for DX is nowhere) and whether I could un-pare it. It turns out it was an extremely simple fix to get it as good as I got it, and I think that’s as good as you can get it until someone leaks the source code.</p>



<p>I’d messed around with lipsyncing stuff before and was familiar with the broad strokes of how it tends to work via my intense familiarity with Half-Life 2: you figure out, hopefully automatically, the sounds (phonemes) present in a sound file (“oo”, “ah”, whatever) and map those to mouth shapes (visemes), then when the audio plays, move the mouth into the right shape for the phoneme we’re in at this moment. The figuring-out process is called “phoneme extraction”, at least by Valve, and Valve do this offline, because it takes a sec. In Valve’s case they append this phoneme information to the end of the .wav file, and it looks like this:</p>



<hr>



<pre><code>PLAINTEXT
{
Okay, I don't blame you for hesitating, but if we're gonna do this thing, then let's just get through it. 
}
WORDS
{
WORD Okay 0.064 0.224
{
111 ow 0.014 0.096 1.000
107 k 0.096 0.142 1.000
101 ey 0.142 0.220 1.000
}
WORD I 0.224 0.352
{
593 ay 0.220 0.310 1.000
105 iy 0.310 0.364 1.000
}
WORD don't 0.352 0.496
{
100 d 0.364 0.396 1.000
111 ow 0.396 0.456 1.000
110 n 0.456 0.496 1.000
}
</code></pre>



<p>, etc. Phonemes, start times, end times. Easy!</p>



<p>My assumption is that the reason Deus Ex’s super cool lipsyncing was too expensive to ship was, they don’t seem to save this information anywhere, so I guess they were figuring out the phonemes in realtime. If correct, this is sort of a bummer – doing what Valve did would have scooped the whole cost out. Maybe there was more to it.</p>



<p>Anyway, the Unrealscript. Deus Ex is pre-Unreal having skeletal animation, it’s all vertex animation. The character heads have a few: relevant here, 7 visemes and a blink. <code>nextphoneme</code> is set from somewhere outside this code (probably a cpp audio system I can’t access) to A, E, F, M, O, T or U, which it doesn’t matter which is which and I don’t remember, or X, which is nothing (close mouth). Then this Unrealscript on the character sets the head’s anim sequence to the appropriate pose. This all happens on tick, but only if <code>IsSpeaking</code>. We have a <code>tweentime</code> we’re using to blend between these poses, so we <em>should</em> be seeing nice smooth blending, the lack of which is why I’m here in the first place! So what’s the problem?</p>



<p>The main thing is a dodgy frame rate check:</p>



<pre><code>// update the animation timers that we are using
	animTimer[0] += deltaTime;
	animTimer[1] += deltaTime;
	animTimer[2] += deltaTime;

	if (bIsSpeaking)
	{
		// if our framerate is high enough (&gt;20fps), tween the lips smoothly
		if (Level.TimeSeconds - animTimer[3]  &lt; 0.05)
			tweentime = 0;
		else
			tweentime = 0.1;
</code></pre>



<p>“tweentime” is how long it takes to blend to the next viseme in seconds; if 0, it’s an instant snap. The intent here is to skip blending entirely if our framerate is so low that it looks better snapping the lips around than showing any in-between poses, only it doesn’t work. The code is keeping <code>Level.TimeSeconds</code> from the previous frame and subtracting that from the current <code>Level.TimeSeconds</code> to get deltatime, which if it’s less than 0.05, we’re assumed to be getting less than 20fps. So it’s flipped.</p>



<p>Also, 0.1 is just way too fast a value, which I suspect a reason for that I’ll come back to*. I increased it to 0.35 to make the blends take long enough to really see.</p>



<p>With that fixed, the lipsync is smooth! Hooray! But it’s not perfect: at the end of a line, when the audio finishes, we don’t smoothly close the mouth; we snap the mouth shut instantly. This is because we’re only doing any blending if <code>bIsSpeaking=true</code>, which it suddenly isn’t. The perf hit of this function no longer matters at all, so I just skip that check too: every character always gets to run lipsync. Tweentime is also local to this function and initialises at 0, so I had to set it to 0.3 to get blending even when we have no phoneme.</p>



<p>Blinking was also way too fast, so fast as to be invisible, so I slowed it down a ton. Now you can see ’em blinkin’.</p>



<p>So now we have nice blinking and smooth mouth movement, but there’s one thing that still sucks: presumably as part of the optimisation that made this ship at all, <code>nextphoneme</code> does not update every tick, or anywhere near every tick. It doesn’t even update at a fixed rate – sometimes you’ll get a good amount of updates in a sentence, sometimes one or two. This means that all the smooth blending in the world won’t get you a correct result unless you happen to get lucky: JC can be speaking the M in “a bomb” and you’re still back on the “a”. As far as I can tell there’s no way to fix this right now – the code that updates the phonemes just needs to do it every tick, and it don’t, and it’s not Unrealscript so I can’t touch it. If the time between phoneme updates was at least consistent, you could set tweentime to that duration and make your blend take as long as it takes for a new phoneme to show up, but it ain’t. So close!</p>



<p>*In the interview where Norden alludes to this amazing lipsync demo they had going on before they optimised it down, I assume it was initially getting a new phoneme every tick, and <em>that</em> is probably when they set 0.1 seconds as a blend duration. If you’re getting constant new phonemes, blending super fast to the next one makes sense; it’s only when you’re not that a slower blend time looks good.</p>



<p>There’s a lot of jank to this code. The silliest thing about it might be that it lives in <code>ScriptedPawn</code>, Deus Ex’s NPC class, which does not share an immediate parent with the player character, so this whole function is just duplicated between the two classes.</p>



<p>Anyway, here’s the whole function after I futzed with it.</p>



<pre><code>// lip synching support - DEUS_EX CNN
//
function LipSynch(float deltaTime)
{
	local name animseq;
	local float rnd;
	local float tweentime;

	// update the animation timers that we are using
	animTimer[0] += deltaTime;
	animTimer[1] += deltaTime;
	animTimer[2] += deltaTime;

	if (bIsSpeaking)
	{
		// if our framerate is high enough (&gt;20fps), tween the lips smoothly
		
//JOE CHANGE: 
//This used to set tweentime to 0 (no blend) if it thought FPS was low, else 0.1. It was 
//backwards though, the result was the opposite. 
//Even 0.1 is too fast to look good though. Anyway, skip the check, we don't care
//
//		if (Level.TimeSeconds - animTimer[3]  &lt; 0.05)
//			tweentime = 0.4;
//		else
			tweentime = 0.36;

//Also, ideally tweentime would be the duration until the next time we get a phoneme update?
//But I don't know where that update comes from at the moment

		// the last animTimer slot is used to check framerate
		animTimer[3] = Level.TimeSeconds;

		if (nextPhoneme == "A")
			animseq = 'MouthA';
		else if (nextPhoneme == "E")
			animseq = 'MouthE';
		else if (nextPhoneme == "F")
			animseq = 'MouthF';
		else if (nextPhoneme == "M")
			animseq = 'MouthM';
		else if (nextPhoneme == "O")
			animseq = 'MouthO';
		else if (nextPhoneme == "T")
			animseq = 'MouthT';
		else if (nextPhoneme == "U")
			animseq = 'MouthU';
		else if (nextPhoneme == "X")
			animseq = 'MouthClosed';

		if (animseq != '')
		{
					if (lastPhoneme != nextPhoneme)
			{
				lastPhoneme = nextPhoneme;
				TweenBlendAnim(animseq, tweentime);
				TimeLastPhoneme = Level.TimeSeconds;
			}
		
		}
		

//		if ((Level.TimeSeconds - TimeLastPhoneme) &gt;= tweentime*0.8 &amp;&amp; TimeLastPhoneme != 0)
//		{
//		TweenBlendAnim('MouthClosed', 0.2);
//		nextPhoneme = "X";
//		lastPhoneme = "A";
//		TimeLastPhoneme = Level.TimeSeconds;
//		}
	}
	else
	if (bWasSpeaking)
	{
		bWasSpeaking = false;
		
//JOE: I added this tweentime set. Without it it was 0 as initialised, so the jaw snapped shut

		tweentime = 0.3;
		TweenBlendAnim('MouthClosed', tweentime);
	}

	// blink randomly
	if (animTimer[0] &gt; 0.5)
	{
		animTimer[0] = 0;
		if (FRand() &lt; 0.4)
			PlayBlendAnim('Blink', 0.2, 0.1, 1);
	}

	LoopHeadConvoAnim();
	LoopBaseConvoAnim();
}
</code></pre>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[All British adults to require a digital ID 'Brit Card' (138 pts)]]></title>
            <link>https://news.sky.com/video/all-british-adults-to-require-a-digital-id-brit-card-13438041</link>
            <guid>45381810</guid>
            <pubDate>Fri, 26 Sep 2025 02:09:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.sky.com/video/all-british-adults-to-require-a-digital-id-brit-card-13438041">https://news.sky.com/video/all-british-adults-to-require-a-digital-id-brit-card-13438041</a>, See on <a href="https://news.ycombinator.com/item?id=45381810">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="article-wrap">
        
        
               
                
                
               
                    <div data-testid="article-header">
                    <div>
                            <p data-testid="article-header-sub-title">The proposals are the government's latest bid to tackle illegal immigration, with the new ID being a form of proof of a citizen's right to live and work in the UK.</p>
                        </div>
            
            
            
            
                    <div>
                        <div data-testid="article-date"><p data-testid="article-date-time">Thursday 25 September 2025 16:22, UK</p>
</div>
            
                            <div data-align="right" data-testid="article-tags">
        
        <ul><li data-testid="article-tags-item"><a href="https://news.sky.com/topic/keir-starmer-6820">Keir Starmer</a></li></ul>
      </div>
                    </div>
                </div>
    <div data-account-id="6058004172001" data-amp-iframe-embed="" data-asset-id="55ec3b4e-1565-4ccf-a626-6a0953606057" data-asset-path="" data-asset-version="" data-autoplay="false" data-auth-config="" data-caption="The Prime Minister is set to announce that all adults in the UK will be required to own a new form of government-issued digital ID." data-clip-type="" data-component-name="sdc-site-video" data-component-name-alias="ui-sitewide-video" data-competition="" data-copy-url-text="URL copied to clipboard" data-fn="sdc-site-video" data-id="id_55ec3b4e-1565-4ccf-a626-6a0953606057" data-is-live-stream="false" data-lite="true" data-options="" data-originator-id="-1" data-originator-handle="brightcove-news-gb" data-package-name="" data-provider="brightcove" data-player-id="yjBoKQ5XA" data-playsinline="" data-sensitive="" data-sdc-id="7031478" data-sdc-video-id="55ec3b4e-1565-4ccf-a626-6a0953606057" data-sport-category="" data-state="loading" data-token-state="none" data-video-ad-unit="" data-video-blacklisted-originator-ids="" data-video-id="ref:55ec3b4e-1565-4ccf-a626-6a0953606057" data-video-type="" data-auto-pause-on-not-visible="true" tabindex="-1" data-closed-captions-position="middle-third" data-show-closed-captions="true" data-show-pip="true" data-show-live-stream-scrubber="true" data-autoload="false" data-testid="article-top-media">
            <div data-role="bridge-controller">
                <p data-role="accessibility-message">Please use Chrome browser for a more accessible video player</p>
              
              <video id="id_55ec3b4e-1565-4ccf-a626-6a0953606057" data-embed="default" data-application-id="" controls="" playsinline="" data-testid="sitewide-video-video"></video>
            </div>
        
            <figcaption>
              
              <span data-role="caption-text" data-testid="sitewide-video-caption">The Prime Minister is set to announce that all adults in the UK will be required to own a new form of government-issued digital ID.</span>
            </figcaption>
          </div>
        
        
                    
        
            <p data-testid="trust-project">
                <a href="https://news.sky.com/info/policies-and-standards"><svg height="18" viewBox="0 0 19 18" width="19"><g fill="currentColor"><path d="m15.129 0h-12.247a2.89 2.89 0 0 0 -2.882 2.88v12.24a2.89 2.89 0 0 0 2.882 2.88h12.247a2.89 2.89 0 0 0 2.881-2.88v-12.24a2.89 2.89 0 0 0 -2.88-2.88zm1.8 15.12c0 .99-.81 1.8-1.8 1.8h-12.247c-.99 0-1.801-.81-1.801-1.8v-12.24c0-.99.81-1.8 1.8-1.8h12.249c.99 0 1.8.81 1.8 1.8v12.24z" fill-rule="nonzero"></path><path d="m10.788 13.824h-3.548a.123.123 0 0 1 -.126-.126v-6.048a.161.161 0 0 0 -.162-.162h-3.17a.123.123 0 0 1 -.126-.126v-3.114c0-.072.054-.126.126-.126h10.446c.072 0 .126.054.126.126v3.132a.123.123 0 0 1 -.126.126h-3.188a.123.123 0 0 0 -.126.126v6.066a.123.123 0 0 1 -.126.126z"></path></g></svg>Why you can trust Sky News<span></span></a>
            </p>
        
        
        
                  <!-- footer -->
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Investigating a Forged PDF (264 pts)]]></title>
            <link>https://mjg59.dreamwidth.org/73317.html</link>
            <guid>45381010</guid>
            <pubDate>Fri, 26 Sep 2025 00:14:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mjg59.dreamwidth.org/73317.html">https://mjg59.dreamwidth.org/73317.html</a>, See on <a href="https://news.ycombinator.com/item?id=45381010">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I had to rent a house for a couple of months recently, which is long enough in California that it pushes you into proper tenant protection law. As landlords tend to do, they failed to return my security deposit within the 21 days <a href="https://leginfo.legislature.ca.gov/faces/codes_displaySection.xhtml?sectionNum=1950.5&amp;lawCode=CIV">required by law</a>, having already failed to provide the required notification that I was entitled to an inspection before moving out. Cue some tedious argumentation with the letting agency, and eventually me threatening to take them to small claims court.</p><p>This post is not about that.</p><p>Now, under Californian law, the onus is on the <em>landlord</em> to hold and return the security deposit - the agency has no role in this. The only reason I was talking to them is that my lease didn't mention the name or address of the landlord (another <a href="https://leginfo.legislature.ca.gov/faces/codes_displaySection.xhtml?sectionNum=1962&amp;lawCode=CIV">legal violation</a>, but the outcome is just that you get to serve the landlord via the agency). So it was a bit surprising when I received an email from the owner of the agency informing me that they did not hold the deposit and so were not liable - I already knew this.</p><p>The odd bit about this, though, is that they sent me another copy of the contract, asserting that it made it clear that the landlord held the deposit. I read it, and instead found a clause reading <q>SECURITY: The security deposit will secure the performance of Tenant’s obligations. IER may, but will not be obligated to, apply all portions of said deposit on account of Tenant’s obligations. Any balance remaining upon termination will be returned to Tenant. Tenant will not have the right to apply the security deposit in payment of the last month’s rent. Security deposit held at IER Trust Account.</q>, where IER is <a href="https://www.iersf.com/">International Executive Rentals</a>, the agency in question. Why send me a contract that says you hold the money while you're telling me you don't? And then I read further down and found this:<br><img src="https://codon.org.uk/~mjg59/pictures/addendum.png" alt="Text reading ENTIRE AGREEMENT: The foregoing constitutes the entire agreement between the parties and may bemodified only in writing signed by all parties. This agreement and any modifications, including anyphotocopy or facsimile, may be signed in one or more counterparts, each of which will be deemed anoriginal and all of which taken together will constitute one and the same instrument. The followingexhibits, if checked, have been made a part of this Agreement before the parties’ execution:۞Exhibit 1:Lead-Based Paint Disclosure (Required by Law for Rental Property Built Prior to 1978)۞Addendum 1 The security deposit will be held by (name removed) and applied, refunded, or forfeited in accordance with the terms of this lease agreement."><br>Ok, fair enough, there's an addendum that says the landlord has it (I've removed the landlord's name, it's present in the original).</p><p>Except. I had no recollection of that addendum. I went back to the copy of the contract I had and discovered:<br><img src="https://codon.org.uk/~mjg59/pictures/no-addendum.png" alt="The same text as the previous picture, but addendum 1 is empty"><br>Huh! But obviously I could just have edited that to remove it (there's no obvious reason for me to, but whatever), and then it'd be my word against theirs. However, I'd been sent the document via <a href="https://www.sharefile.com/rightsignature">RightSignature</a>, an online document signing platform, and they'd added a certification page that looked like this:<br><img src="https://codon.org.uk/~mjg59/pictures/contract-certificate.png" alt="A Signature Certificate, containing a bunch of data about the document including a checksum or the original"><br>Interestingly, the certificate page was identical in both documents, including the checksums, despite the content being different. So, how do I show which one is legitimate? You'd think given this certificate page this would be trivial, but RightSignature provides no documented mechanism whatsoever for anyone to verify any of the fields in the certificate, which is annoying but let's see what we can do anyway.</p><p>First up, let's look at the PDF metadata. </p><a href="https://www.pdflabs.com/tools/pdftk-the-pdf-toolkit/"><tt>pdftk</tt></a><p> has a </p><tt>dump_data</tt><p> command that dumps the metadata in the document, including the creation date and the modification date. My file had both set to identical timestamps in June, both listed in UTC, corresponding to the time I'd signed the document. The file containing the addendum? The same creation time, but a modification time of this Monday, shortly before it was sent to me. This time, the modification timestamp was in Pacific Daylight Time, the timezone currently observed in California. In addition, the data included two ID fields, ID0 and ID1. In my document both were identical, in the one with the addendum ID0 matched mine but ID1 was different.</p><p>These ID tags are intended to be some form of representation (such as a hash) of the document. ID0 is set when the document is created and should not be modified afterwards - ID1 initially identical to ID0, but changes when the document is modified. This is intended to allow tooling to identify whether two documents are modified versions of the same document. The identical ID0 indicated that the document with the addendum was originally identical to mine, and the different ID1 that it had been modified.</p><p>Well, ok, that seems like a pretty strong demonstration. I had the "I have a very particular set of skills" conversation with the agency and pointed these facts out, that they were an extremely strong indication that my copy was authentic and their one wasn't, and they responded that the document was "re-sealed" every time it was downloaded from RightSignature and that would explain the modifications. This doesn't seem plausible, but it's an argument. Let's go further.</p><p>My next move was <a href="https://pypi.org/project/pdfalyzer/">pdfalyzer</a>, which allows you to pull a PDF apart into its component pieces. This revealed that the documents were identical, other than page 3, the one with the addendum. This page included tags entitled "touchUp_TextEdit", evidence that the page had been modified using Acrobat. But in itself, that doesn't prove anything - obviously it had been edited at some point to insert the landlord's name, it doesn't prove whether it happened before or after the signing.</p><p>But in the process of editing, Acrobat appeared to have renamed all the font references on that page into a different format. Every other page had a consistent naming scheme for the fonts, and they matched the scheme in the page 3 I had. Again, that doesn't tell us whether the renaming happened before or after the signing. Or does it?</p><p>You see, when I completed my signing, RightSignature inserted my name into the document, and did so using a font that wasn't otherwise present in the document (Courier, in this case). That font was named identically throughout the document, except on page 3, where it was named in the same manner as every other font that Acrobat had renamed. Given the font wasn't present in the document until after I'd signed it, this is proof that the page was edited <em>after</em> signing.</p><p>But eh this is all very convoluted. Surely there's an easier way? Thankfully yes, although I hate it. RightSignature had sent me a link to view my signed copy of the document. When I went there it presented it to me as the original PDF with my signature overlaid on top. Hitting F12 gave me the network tab, and I could see a reference to a </p><tt>base.pdf</tt><p>. Downloading that gave me the original PDF, pre-signature. Running </p><tt>sha256sum</tt><p> on it gave me an identical hash to the "Original checksum" field. Needless to say, it did not contain the addendum.</p><p>Why do this? The only explanation I can come up with (and I am obviously guessing here, I may be incorrect!) is that International Executive Rentals realised that they'd sent me a contract which could mean that they <em>were</em> liable for the return of my deposit, even though they'd already given it to my landlord, and after realising this added the addendum, sent it to me, and assumed that I just wouldn't notice (or that, if I did, I wouldn't be able to prove anything). In the process they went from an extremely unlikely possibility of having civil liability for a few thousand dollars (even if they were holding the deposit it's still the landlord's legal duty to return it, as far as I can tell) to doing something that looks extremely like <a href="https://leginfo.legislature.ca.gov/faces/codes_displayText.xhtml?lawCode=PEN&amp;division=&amp;title=13.&amp;part=1.&amp;chapter=4.&amp;article=">forgery</a>.</p><p>There's a hilarious followup. After this happened, the agency offered to do a screenshare with me showing them logging into RightSignature and showing the signed file with the addendum, and then proceeded to do so. One minor problem - the "Send for signature" button was still there, just below a field saying "Uploaded: 09/22/25". I asked them to search for my name, and it popped up two hits - one marked draft, one marked completed. The one marked completed? Didn't contain the addendum.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Redis is fast – I'll cache in Postgres (293 pts)]]></title>
            <link>https://dizzy.zone/2025/09/24/Redis-is-fast-Ill-cache-in-Postgres/</link>
            <guid>45380699</guid>
            <pubDate>Thu, 25 Sep 2025 23:34:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dizzy.zone/2025/09/24/Redis-is-fast-Ill-cache-in-Postgres/">https://dizzy.zone/2025/09/24/Redis-is-fast-Ill-cache-in-Postgres/</a>, See on <a href="https://news.ycombinator.com/item?id=45380699">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>There are <a href="https://www.manning.com/books/just-use-postgres">books</a> &amp; many articles online, like <a href="https://www.amazingcto.com/postgres-for-everything/">this one</a> arguing for using Postgres for everything. I thought I’d take a look at one use case - using Postgres instead of Redis for caching. I work with APIs quite a bit, so I’d build a super simple HTTP server that responds with data from that cache. I’d start from Redis as this is something I frequently encounter at work, switch it out to Postgres using unlogged tables and see if there’s a difference.</p>
<h2 id="the-setup">The setup</h2>
<p>I’ll run the experiment on my <a href="https://dizzy.zone/2025/03/10/State-of-my-Homelab-2025/">homelab’s</a> k8s cluster. The idea is to run Postgres or Redis on one node, limiting it to 2CPUs via k8s limits, as well as 8GiB of memory. On another node, I’ll run the web server itself and then spin a pod for the benchmark executed via <a href="https://k6.io/">k6</a> on the third.</p>
<p>Both postgres and redis are used with the out of the box settings for the following images:</p>
<ul>
<li>Postgres - <code>postgres:17.6</code></li>
<li>Redis - <code>redis:8.2</code></li>
</ul>
<p>I wrote a simple webserver, with 2 endpoints, a cache and a “Session” struct which we’ll store in the cache:</p>
<div><pre tabindex="0"><code data-lang="go"><span><span><span>var</span> <span>ErrCacheMiss</span> = <span>errors</span>.<span>New</span>(<span>"cache miss"</span>)
</span></span><span><span>
</span></span><span><span><span>type</span> <span>Cache</span> <span>interface</span> {
</span></span><span><span>	<span>Get</span>(<span>ctx</span> <span>context</span>.<span>Context</span>, <span>key</span> <span>string</span>) (<span>string</span>, <span>error</span>)
</span></span><span><span>	<span>Set</span>(<span>ctx</span> <span>context</span>.<span>Context</span>, <span>key</span> <span>string</span>, <span>value</span> <span>string</span>) <span>error</span>
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>type</span> <span>Session</span> <span>struct</span> {
</span></span><span><span>	<span>ID</span> <span>string</span>
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>func</span> <span>serveHTTP</span>(<span>c</span> <span>Cache</span>) {
</span></span><span><span>	<span>http</span>.<span>HandleFunc</span>(<span>"/get"</span>, <span>getHandler</span>(<span>c</span>))
</span></span><span><span>	<span>http</span>.<span>HandleFunc</span>(<span>"/set"</span>, <span>setHandler</span>(<span>c</span>))
</span></span><span><span>
</span></span><span><span>	<span>port</span> <span>:=</span> <span>os</span>.<span>Getenv</span>(<span>"PORT"</span>)
</span></span><span><span>	<span>if</span> <span>port</span> <span>==</span> <span>""</span> {
</span></span><span><span>		<span>port</span> = <span>"8080"</span>
</span></span><span><span>	}
</span></span><span><span>
</span></span><span><span>	<span>fmt</span>.<span>Println</span>(<span>"Server starting on http://0.0.0.0:"</span> <span>+</span> <span>port</span>)
</span></span><span><span>
</span></span><span><span>	<span>server</span> <span>:=</span> <span>&amp;</span><span>http</span>.<span>Server</span>{<span>Addr</span>: <span>"0.0.0.0:"</span> <span>+</span> <span>port</span>}
</span></span><span><span>
</span></span><span><span>	<span>go</span> <span>func</span>() {
</span></span><span><span>		<span>if</span> <span>err</span> <span>:=</span> <span>server</span>.<span>ListenAndServe</span>(); <span>err</span> <span>!=</span> <span>nil</span> <span>&amp;&amp;</span> <span>err</span> <span>!=</span> <span>http</span>.<span>ErrServerClosed</span> {
</span></span><span><span>			<span>fmt</span>.<span>Println</span>(<span>"Error starting server:"</span>, <span>err</span>)
</span></span><span><span>		}
</span></span><span><span>	}()
</span></span><span><span>
</span></span><span><span>	<span>quit</span> <span>:=</span> make(<span>chan</span> <span>os</span>.<span>Signal</span>, <span>1</span>)
</span></span><span><span>	<span>signal</span>.<span>Notify</span>(<span>quit</span>, <span>os</span>.<span>Interrupt</span>)
</span></span><span><span>	<span>&lt;-</span><span>quit</span>
</span></span><span><span>
</span></span><span><span>	<span>fmt</span>.<span>Println</span>(<span>"Shutting down server..."</span>)
</span></span><span><span>
</span></span><span><span>	<span>if</span> <span>err</span> <span>:=</span> <span>server</span>.<span>Close</span>(); <span>err</span> <span>!=</span> <span>nil</span> {
</span></span><span><span>		<span>fmt</span>.<span>Println</span>(<span>"Error shutting down server:"</span>, <span>err</span>)
</span></span><span><span>	}
</span></span><span><span>}</span></span></code></pre></div>
<p>For redis, I’ve implemented the cache using <code>github.com/redis/go-redis/v9</code> as follows:</p>
<div><pre tabindex="0"><code data-lang="go"><span><span><span>type</span> <span>RedisCache</span> <span>struct</span> {
</span></span><span><span>	<span>client</span> <span>*</span><span>redis</span>.<span>Client</span>
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>func</span> <span>NewRedisCache</span>() <span>*</span><span>RedisCache</span> {
</span></span><span><span>	<span>redisURL</span> <span>:=</span> <span>os</span>.<span>Getenv</span>(<span>"REDIS_URL"</span>)
</span></span><span><span>	<span>if</span> <span>redisURL</span> <span>==</span> <span>""</span> {
</span></span><span><span>		<span>redisURL</span> = <span>"localhost:6379"</span>
</span></span><span><span>	}
</span></span><span><span>
</span></span><span><span>	<span>fmt</span>.<span>Println</span>(<span>"Connecting to Redis at"</span>, <span>redisURL</span>)
</span></span><span><span>
</span></span><span><span>	<span>client</span> <span>:=</span> <span>redis</span>.<span>NewClient</span>(<span>&amp;</span><span>redis</span>.<span>Options</span>{
</span></span><span><span>		<span>Addr</span>:     <span>redisURL</span>,
</span></span><span><span>		<span>Password</span>: <span>""</span>,
</span></span><span><span>		<span>DB</span>:       <span>0</span>,
</span></span><span><span>	})
</span></span><span><span>
</span></span><span><span>	<span>return</span> <span>&amp;</span><span>RedisCache</span>{
</span></span><span><span>		<span>client</span>: <span>client</span>,
</span></span><span><span>	}
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>func</span> (<span>r</span> <span>*</span><span>RedisCache</span>) <span>Get</span>(<span>ctx</span> <span>context</span>.<span>Context</span>, <span>key</span> <span>string</span>) (<span>string</span>, <span>error</span>) {
</span></span><span><span>	<span>val</span>, <span>err</span> <span>:=</span> <span>r</span>.<span>client</span>.<span>Get</span>(<span>ctx</span>, <span>key</span>).<span>Result</span>()
</span></span><span><span>	<span>if</span> <span>err</span> <span>==</span> <span>redis</span>.<span>Nil</span> {
</span></span><span><span>		<span>return</span> <span>""</span>, <span>ErrCacheMiss</span>
</span></span><span><span>	}
</span></span><span><span>	<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
</span></span><span><span>		<span>return</span> <span>""</span>, <span>err</span>
</span></span><span><span>	}
</span></span><span><span>	<span>return</span> <span>val</span>, <span>nil</span>
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>func</span> (<span>r</span> <span>*</span><span>RedisCache</span>) <span>Set</span>(<span>ctx</span> <span>context</span>.<span>Context</span>, <span>key</span> <span>string</span>, <span>value</span> <span>string</span>) <span>error</span> {
</span></span><span><span>	<span>return</span> <span>r</span>.<span>client</span>.<span>Set</span>(<span>ctx</span>, <span>key</span>, <span>value</span>, <span>0</span>).<span>Err</span>()
</span></span><span><span>}</span></span></code></pre></div>
<p>The postgres cache is implemented using the <code>github.com/jackc/pgx/v5</code> library:
</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>type</span> <span>PostgresCache</span> <span>struct</span> {
</span></span><span><span>	<span>db</span> <span>*</span><span>pgxpool</span>.<span>Pool</span>
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>func</span> <span>NewPostgresCache</span>() (<span>*</span><span>PostgresCache</span>, <span>error</span>) {
</span></span><span><span>	<span>pgDSN</span> <span>:=</span> <span>os</span>.<span>Getenv</span>(<span>"POSTGRES_DSN"</span>)
</span></span><span><span>	<span>if</span> <span>pgDSN</span> <span>==</span> <span>""</span> {
</span></span><span><span>		<span>pgDSN</span> = <span>"postgres://user:password@localhost:5432/mydb"</span>
</span></span><span><span>	}
</span></span><span><span>
</span></span><span><span>	<span>cfg</span>, <span>err</span> <span>:=</span> <span>pgxpool</span>.<span>ParseConfig</span>(<span>pgDSN</span>)
</span></span><span><span>	<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
</span></span><span><span>		<span>return</span> <span>nil</span>, <span>err</span>
</span></span><span><span>	}
</span></span><span><span>
</span></span><span><span>	<span>cfg</span>.<span>MaxConns</span> = <span>50</span>
</span></span><span><span>	<span>cfg</span>.<span>MinConns</span> = <span>10</span>
</span></span><span><span>
</span></span><span><span>	<span>pool</span>, <span>err</span> <span>:=</span> <span>pgxpool</span>.<span>NewWithConfig</span>(<span>context</span>.<span>Background</span>(), <span>cfg</span>)
</span></span><span><span>	<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
</span></span><span><span>		<span>return</span> <span>nil</span>, <span>err</span>
</span></span><span><span>	}
</span></span><span><span>
</span></span><span><span>	<span>_</span>, <span>err</span> = <span>pool</span>.<span>Exec</span>(<span>context</span>.<span>Background</span>(), <span>`
</span></span></span><span><span><span>		CREATE UNLOGGED TABLE IF NOT EXISTS cache (
</span></span></span><span><span><span>			key VARCHAR(255) PRIMARY KEY,
</span></span></span><span><span><span>			value TEXT
</span></span></span><span><span><span>		);
</span></span></span><span><span><span>	`</span>)
</span></span><span><span>	<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
</span></span><span><span>		<span>return</span> <span>nil</span>, <span>err</span>
</span></span><span><span>	}
</span></span><span><span>
</span></span><span><span>	<span>return</span> <span>&amp;</span><span>PostgresCache</span>{
</span></span><span><span>		<span>db</span>: <span>pool</span>,
</span></span><span><span>	}, <span>nil</span>
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>func</span> (<span>p</span> <span>*</span><span>PostgresCache</span>) <span>Get</span>(<span>ctx</span> <span>context</span>.<span>Context</span>, <span>key</span> <span>string</span>) (<span>string</span>, <span>error</span>) {
</span></span><span><span>	<span>var</span> <span>content</span> <span>string</span>
</span></span><span><span>	<span>err</span> <span>:=</span> <span>p</span>.<span>db</span>.<span>QueryRow</span>(<span>ctx</span>, <span>`SELECT value FROM cache WHERE key = $1`</span>, <span>key</span>).<span>Scan</span>(<span>&amp;</span><span>content</span>)
</span></span><span><span>	<span>if</span> <span>err</span> <span>==</span> <span>pgx</span>.<span>ErrNoRows</span> {
</span></span><span><span>		<span>return</span> <span>""</span>, <span>ErrCacheMiss</span>
</span></span><span><span>	}
</span></span><span><span>	<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
</span></span><span><span>		<span>return</span> <span>""</span>, <span>err</span>
</span></span><span><span>	}
</span></span><span><span>	<span>return</span> <span>content</span>, <span>nil</span>
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>func</span> (<span>p</span> <span>*</span><span>PostgresCache</span>) <span>Set</span>(<span>ctx</span> <span>context</span>.<span>Context</span>, <span>key</span> <span>string</span>, <span>value</span> <span>string</span>) <span>error</span> {
</span></span><span><span>	<span>_</span>, <span>err</span> <span>:=</span> <span>p</span>.<span>db</span>.<span>Exec</span>(<span>ctx</span>, <span>`INSERT INTO cache (key, value) VALUES ($1, $2) ON CONFLICT (key) DO UPDATE SET value = $2`</span>, <span>key</span>, <span>value</span>)
</span></span><span><span>	<span>return</span> <span>err</span>
</span></span><span><span>}</span></span></code></pre></div>
<p>I’ll seed the redis and postgres with 30 million entries each, keeping record of the inserted uuids. From there, I’ll generate a subset of existing uuids to use while benchmarking. This allows for simulating both hits and misses.</p>
<p>I’ll do a few runs of benchmarks for gets first, then sets and then a mixed run. Each run will execute for 2 minutes. I’ll look at the number of operations per second, latencies as well as memory and CPU usage during those times.</p>
<p>To simulate a somewhat real scenario where only a subset of keys exist in the cache the set benchmark will have a 10% chance to update an existing key, whereas the get will have an 80% chance of picking an existing key. The mixed workload will have a 20% chance to execute a set scenario and 80% for the get scenario.</p>
<h2 id="the-results">The results</h2>
<h3 id="getting-values-from-cache">Getting values from cache</h3>
















<p>
    
        Requests per second – higher is better
    
</p>






<p>Redis performed better than Postgres, which did not surprise me at all. The bottleneck was actually the HTTP server. The machine running the http server maxed out on CPU, with redis running comfortably with ~1280mCPU - short of the 2000mCPU limit imposed. Redis used ~3800MiB of RAM, which stayed flat across the runs.</p>
<p>For postgres, the bottleneck was the CPU on postgres side. It consistently maxed out the 2 cores dedicated to it, while also using ~5000MiB of RAM.</p>
<p>Redis also did better when it comes to latencies of the HTTP responses:</p>
















<p>
    
        Latency, milliseconds – lower is better
    
</p>






<h3 id="setting-values-in-cache">Setting values in cache</h3>
















<p>
    
        Requests per second – higher is better
    
</p>






<p>Once again Redis performed better. The CPU usage stayed roughly the same as in the case of the GET experiment, with the RAM usage growing to ~4300MiB due to the new keys being inserted. The bottleneck stayed on the HTTP server side, with Redis using ~1280mCPU once again.</p>
<p>Postgres once again was bottlenecked by the CPU, constantly using 100% of the 2 cores it was limited to. During the course of the run, the memory usage grew to ~5500MiB.</p>
<p>During the test, the endpoints with the Redis cache implementation also had better latencies:</p>
















<p>
    
        Latency, milliseconds – lower is better
    
</p>






<h3 id="readwrite-performance">Read/write performance</h3>
















<p>
    
        Requests per second – higher is better
    
</p>






<p>The mixed benchmark also returned the predictable result of Redis reigning superior. As has been the story so far, the CPU stayed put at ~1280mCPU, RAM usage grew a bit due to the new keys being inserted.</p>
<p>Postgres maxed out the two cores and reached around 6GiB of memory used.</p>
<p>Latencies once again were better when using redis:</p>
















<p>
    
        Latency, milliseconds – lower is better
    
</p>






<h3 id="unlogged-tables">Unlogged tables</h3>
<p>In the benchmark, I’ve used an unlogged table for postgres but this has not seemed to help, or has it? If I rerun the same benchmark with a normal(logged) table we can look at the numbers.</p>
















<p>
    
        Requests per second – higher is better
    
</p>






<p>The unlogged table makes a huge difference for the write benchmark and a somewhat smaller but still significant one for the mixed workload. This is because the unlogged tables skip the write ahead log making them a lot faster for writes. There’s very little difference for the read performance though and I expect more runs would show the two test cases converging.</p>
<h3 id="conclusion">Conclusion</h3>
<p>Redis is faster than postgres when it comes to caching, there’s no doubt about it. It conveniently comes with a bunch of other useful functionality that one would expect from a cache, such as TTLs. It was also bottlenecked by the hardware, my service or a combination of both and could definitely show better numbers. Surely, we should all use Redis for our caching needs then, right? Well, I think I’ll still use postgres. Almost always, my projects need a database. Not having to add another dependency comes with its own benefits. If I need my keys to expire, I’ll add a column for it, and a cron job to remove those keys from the table. As far as speed goes - 7425 requests per second is still a lot. That’s more than half a billion requests per day. All on hardware that’s 10 years old and using laptop CPUs. Not many projects will reach this scale and if they do I can just upgrade the postgres instance or if need be spin up a redis then. Having an interface for your cache so you can easily switch out the underlying store is definitely something I’ll keep doing exactly for this purpose.</p>
<p>Thanks for reading!</p>


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RedoxFS is the default filesystem of Redox OS, inspired by ZFS (180 pts)]]></title>
            <link>https://doc.redox-os.org/book/redoxfs.html</link>
            <guid>45379325</guid>
            <pubDate>Thu, 25 Sep 2025 21:25:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://doc.redox-os.org/book/redoxfs.html">https://doc.redox-os.org/book/redoxfs.html</a>, See on <a href="https://news.ycombinator.com/item?id=45379325">Hacker News</a></p>
<div id="readability-page-1" class="page">
        <!-- Provide site root to javascript -->
        

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        

        <!-- Set the theme before any content is loaded, prevents flash -->
        

        <!-- Hide / unhide sidebar before it is displayed -->
        

        <nav id="sidebar" aria-label="Table of contents">
            
            
        </nav>

        <div id="page-wrapper">

            <div class="page">
                                
                <div id="menu-bar">
                    

                    <h2>The Redox Operating System</h2>

                    
                </div>

                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                

                <div id="content">
                    <main>
                        <h2 id="redoxfs"><a href="#redoxfs">RedoxFS</a></h2>
<p>This is the default filesystem of Redox OS, inspired by ZFS and adapted to a microkernel architecture.</p>
<p>Redox had a read-only ZFS driver but it was abandoned because of the monolithic nature of ZFS that created problems with the Redox microkernel design.</p>
<p>(It's a replacement for <a href="https://gitlab.redox-os.org/redox-os/tfs">TFS</a>)</p>
<p>Current features:</p>
<ul>
<li>Compatible with Redox and Linux (FUSE)</li>
<li>Copy-on-write</li>
<li>Data/metadata checksums</li>
<li>Transparent encryption</li>
<li>Standard Unix file attributes</li>
<li>File/directory size limit up to 193TiB (212TB)</li>
<li>File/directory quantity limit up to 4 billion per 193TiB (2^32 - 1 = 4294967295)</li>
<li>Disk encryption fully supported by the Redox bootloader, letting it load the kernel off an encrypted partition.</li>
<li>MIT licensed</li>
</ul>
<p>Being MIT licensed, RedoxFS can be bundled on GPL-licensed operating systems (Linux, for example).</p>

<p>RedoxFS tooling can be used to create, mount and edit contents of an <code>.img</code> file containing RedoxFS. It can be installed with:</p>
<pre><code>cargo install redoxfs
</code></pre>
<p>If you found errors while installing it, make sure to install <code>fuse3</code>.</p>
<h3 id="create-a-disk"><a href="#create-a-disk">Create a disk</a></h3>
<p>You can create an empty, non bootable RedoxFS by allocating an empty file with <code>fallocate</code> then run <code>redoxfs-mkfs</code> to initialize the whole image as <code>RedoxFS</code>.</p>
<pre><code>fallocate -l 1G redox.img
</code></pre>
<pre><code>redoxfs-mkfs redox.img
</code></pre>
<h3 id="mount-a-disk"><a href="#mount-a-disk">Mount a disk</a></h3>
<p>To mount the disk, run <code>redoxfs [image] [directory]</code>:</p>
<pre><code>mkdir ./redox-img
</code></pre>
<pre><code>redoxfs redox.img ./redox-img
</code></pre>
<p>It will mount the disk using FUSE underneath.</p>
<h3 id="unmount"><a href="#unmount">Unmount</a></h3>
<p>Unmount the disk using FUSE unmount binary:</p>
<pre><code>fusermount3 ./redox-img
</code></pre>

                    </main>

                    <nav aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="https://doc.redox-os.org/book/drivers.html" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i></i>
                            </a>

                            <a rel="next" href="https://doc.redox-os.org/book/graphics-windowing.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i></i>
                            </a>

                        
                    </nav>
                </div>
            </div>

            <nav aria-label="Page navigation">
                    <a rel="prev" href="https://doc.redox-os.org/book/drivers.html" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i></i>
                    </a>

                    <a rel="next" href="https://doc.redox-os.org/book/graphics-windowing.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i></i>
                    </a>
            </nav>

        </div>




        


        
        
        

        
        
        

        <!-- Custom JS scripts -->


    

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. once again hits new low in World Happiness Report (209 pts)]]></title>
            <link>https://www.axios.com/2025/03/20/us-new-low-world-happiness-report</link>
            <guid>45378896</guid>
            <pubDate>Thu, 25 Sep 2025 20:52:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.axios.com/2025/03/20/us-new-low-world-happiness-report">https://www.axios.com/2025/03/20/us-new-low-world-happiness-report</a>, See on <a href="https://news.ycombinator.com/item?id=45378896">Hacker News</a></p>
Couldn't get https://www.axios.com/2025/03/20/us-new-low-world-happiness-report: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Can a model trained on satellite data really find brambles on the ground? (162 pts)]]></title>
            <link>https://toao.com/blog/can-we-really-see-brambles-from-space</link>
            <guid>45377748</guid>
            <pubDate>Thu, 25 Sep 2025 19:28:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://toao.com/blog/can-we-really-see-brambles-from-space">https://toao.com/blog/can-we-really-see-brambles-from-space</a>, See on <a href="https://news.ycombinator.com/item?id=45377748">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
		
		<p>Over the summer <a href="https://gabrielmahler.org/">Gabriel Mahler</a> has been conducting research on <a href="https://gabrielmahler.org/environment/ai/ml/%F0%9F%A6%94/2025/07/04/hedgehogs01.html">hedgehog habitat mapping</a> using <a href="https://gabrielmahler.org/environment/ai/ml/%F0%9F%A6%94/2025/07/13/hedgehogs02.html">Agent Based Models (ABMs)</a> and remote sensing. Hedgehogs seem to like <a href="https://en.wikipedia.org/wiki/Rubus">brambles</a> and so as part of his work he has produced a <a href="https://gabrielmahler.org/environment/ai/ml/%F0%9F%A6%94/2025/08/07/summer5.html">bramble map</a>. He did this by combining the <a href="https://arxiv.org/abs/2506.20380">TESSERA earth representation embeddings</a> (using the <a href="https://github.com/ucam-eo/geotessera">geotessera</a> library) with data from <a href="https://www.inaturalist.org/">iNaturalist</a>. The current model is an ensemble of <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">logistic regression</a> and a <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">knn</a> classifier.</p>
<p>Can we really see brambles from space? What better way to test the model than a quick field trip around Cambridge. Gabriel, <a href="https://anil.recoil.org/">Anil</a>, <a href="https://www.shaneweisz.com/">Shane</a> and I did just that today.</p>
<figure>
<img src="https://toao.com/static/20250924_140445.jpg" alt="Gabriel's model overlaid on a map - which was quite hard to read in outdoor sunlight">
<figcaption>Gabriel's model overlaid on a map - which was quite hard to read in outdoor sunlight</figcaption>
</figure>

<p>We started at Milton Community Centre, as the model was relatively confident there were brambles near the car park and along the path to Milton Park. It took us about 20 seconds to find the first one in an area indicated by the model.</p>
<figure>
<img src="https://toao.com/static/20250924_130422.jpg" alt="Our first bramble!">
<figcaption>Our first bramble!</figcaption>
</figure>

<p>So it turns out that there's a lot of bramble between the community center and entrance to Milton Country Park. We stopped six or seven times before reaching the park entrance. While the model predicted we'd find brambles all over the park, we went for the few areas of very high confidence near the entrance. In every place we checked, we found pretty significant amounts of bramble.</p>
<figure>
<img src="https://toao.com/static/20250924_131517.jpg" alt="Some bramble on both sides of the path near the park entrance">
<figcaption>Some bramble on both sides of the path near the park entrance</figcaption>
</figure>

<figure>
<img src="https://toao.com/static/4bad2b6d-8b50-476d-9626-108ae1533a90_1_105_c.jpeg" alt="Trying to navigate using the model">
<figcaption>Trying to navigate using the model</figcaption>
</figure>

<figure>
<img src="https://toao.com/static/b268335a-6128-4ffa-8895-d1eb4eabc224_1_105_c.jpeg" alt="I was fairly sure it was that way">
<figcaption>I was fairly sure it was in that direction (and thankfully it was)</figcaption>
</figure>

<p>We collected photos of all the places we stopped, as well as recording our GPS location. One thought while out exploring is that the model did a great job predicting where we would find very large quantities of bramble without any cover. It didn't have high confidence in other areas where we found smaller brambles under partial cover. Since TESSERA is learned representation from remote sensing data (Sentinel 1 and 2), it would make sense that bramble partially obscured from above might be harder to spot. This is something we can potentially tease apart when we have more validation data.</p>
<figure>
<img src="https://toao.com/static/20250924_131651.jpg" alt="And more bramble">
<figcaption>And more bramble</figcaption>
</figure>

<figure>
<img src="https://toao.com/static/20250924_130606.jpg" alt="Stopping to check the model">
<figcaption>Stopping to check the model</figcaption>
</figure>

<figure>
<img src="https://toao.com/static/20250924_134801.jpg" alt="Stopping to take a photo of a very photogenic bee">
<figcaption>Stopping to take a photo of a very photogenic bee</figcaption>
</figure>

<p>Finally, we were satisfied the model was doing a good job in the park area and decided to pick a hotspot the model was predicting in part of a residential street. We drove over to find an empty plot that did indeed have a lot of bramble!</p>
<figure>
<img src="https://toao.com/static/20250924_141728.jpg" alt="Empty area that had a lot of bramble">
<figcaption>Empty area that had a lot of bramble</figcaption>
</figure>

<p>Another hotspot was on Fen Road and we stopped by to find this absolute unit:</p>
<figure>
<img src="https://toao.com/static/20250924_140247.jpg" alt="Absolute unit of bramble">
<figcaption>Absolute unit of bramble</figcaption>
</figure>

<p>Finally, we headed back in to Cambridge to see what one of the big hotspots in North Cambridge was like. To our amusement we ended up at the local nature reserve Bramblefields, which, true to its name, has a lot of bramble.</p>
<figure>
<img src="https://toao.com/static/20250924_143514.jpg" alt="Bramblefields">
<figcaption>Bramblefields</figcaption>
</figure>

<p>I was pleasantly surprised by how good Gabriel's model was for its simplicity. Great work!</p>
<p>We had hoped to actually re-run the model based on the data we were gathering but that proved tricky on a laptop, in a park. Given the richness of the TESSERA embeddings and the simplicity of the classifiers being used, a mobile phone-based human-in-the-loop active learning setup could be practical..</p>

		
	</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ollama Web Search (329 pts)]]></title>
            <link>https://ollama.com/blog/web-search</link>
            <guid>45377641</guid>
            <pubDate>Thu, 25 Sep 2025 19:21:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ollama.com/blog/web-search">https://ollama.com/blog/web-search</a>, See on <a href="https://news.ycombinator.com/item?id=45377641">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
      <p><a href="https://ollama.com/signup"><img src="https://files.ollama.com/ollama_web_search.png" alt="Ollama’s web search"></a></p>

<p>A new web search API is now available in Ollama. Ollama provides a generous free tier of web searches for individuals to use, and higher rate limits are available via <a href="https://ollama.com/cloud">Ollama’s cloud</a>.</p>

<p>This web search capability can augment models with the latest information from the web to reduce hallucinations and improve accuracy.</p>

<p>Web search is provided as a REST API with deeper tool integrations in Ollama’s Python and JavaScript libraries. This also enables models such as OpenAI’s <a href="https://ollama.com/library/gpt-oss"><code>gpt-oss</code></a> models to conduct long-running research tasks.</p>

<h3 id="get-started">Get started</h3>

<p>Create an API key from your <a href="https://ollama.com/settings/keys">Ollama account</a>.</p>

<pre><code>export OLLAMA_API_KEY="your_api_key"
</code></pre>

<h4 id="curl">cURL</h4>

<pre><code>curl https://ollama.com/api/web_search \
  --header "Authorization: Bearer $OLLAMA_API_KEY" \
  -d '{
    "query": "what is ollama?"
  }'
</code></pre>

<p><strong>Example output</strong></p>

<pre><code>{
  "results": [
    {
      "title": "Ollama",
      "url": "https://ollama.com/",
      "content": "Cloud models are now available..."
    },
    {
      "title": "What is Ollama? Introduction to the AI model management tool",
      "url": "https://www.hostinger.com/tutorials/what-is-ollama",
      "content": "Ariffud M. 6min Read..."
    },
    {
      "title": "Ollama Explained: Transforming AI Accessibility and Language ...",
      "url": "https://www.geeksforgeeks.org/artificial-intelligence/ollama-explained-transforming-ai-accessibility-and-language-processing/",
      "content": "Data Science Data Science Projects Data Analysis..."
    }
  ]
}
</code></pre>

<h4 id="python">Python</h4>

<p>Install and run Ollama’s Python library</p>

<pre><code>pip install 'ollama&gt;=0.6.0'
</code></pre>

<p>Then make a request using <code>ollama.web_search</code>:</p>

<pre><code>import ollama
response = ollama.web_search("What is Ollama?")
print(response)
</code></pre>

<p><strong>Example output</strong></p>

<pre><code>results = [
    {
        "title": "Ollama",
        "url": "https://ollama.com/",
        "content": "Cloud models are now available in Ollama..."
    },
    {
        "title": "What is Ollama? Features, Pricing, and Use Cases - Walturn",
        "url": "https://www.walturn.com/insights/what-is-ollama-features-pricing-and-use-cases",
        "content": "Our services..."
    },
    {
        "title": "Complete Ollama Guide: Installation, Usage &amp; Code Examples",
        "url": "https://collabnix.com/complete-ollama-guide-installation-usage-code-examples",
        "content": "Join our Discord Server..."
    }
]
</code></pre>

<h4 id="javascript">JavaScript</h4>

<p>Install and run Ollama’s JavaScript library</p>

<pre><code>npm install 'ollama@&gt;=0.6.0'
</code></pre>

<pre><code>import { Ollama } from "ollama";

const client = new Ollama();
const results = await client.webSearch({ query: "what is ollama?" });
console.log(JSON.stringify(results, null, 2));
</code></pre>

<p><strong>Example output</strong></p>

<pre><code>{
  "results": [
    {
      "title": "Ollama",
      "url": "https://ollama.com/",
      "content": "Cloud models are now available..."
    },
    {
      "title": "What is Ollama? Introduction to the AI model management tool",
      "url": "https://www.hostinger.com/tutorials/what-is-ollama",
      "content": "Ollama is an open-source tool..."
    },
    {
      "title": "Ollama Explained: Transforming AI Accessibility and Language Processing",
      "url": "https://www.geeksforgeeks.org/artificial-intelligence/ollama-explained-transforming-ai-accessibility-and-language-processing/",
      "content": "Ollama is a groundbreaking..."
    }
  ]
}

</code></pre>

<h3 id="building-a-search-agent">Building a search agent</h3>

<p>Use Ollama’s web search as a tool to build a mini search agent.</p>

<p>The example uses Alibaba’s Qwen 3 model with 4B parameters.</p>

<pre><code>ollama pull qwen3:4b
</code></pre>

<pre><code>from ollama import chat, web_fetch, web_search

available_tools = {'web_search': web_search, 'web_fetch': web_fetch}

messages = [{'role': 'user', 'content': "what is ollama's new engine"}]

while True:
  response = chat(
    model='qwen3:4b',
    messages=messages,
    tools=[web_search, web_fetch],
    think=True
    )
  if response.message.thinking:
    print('Thinking: ', response.message.thinking)
  if response.message.content:
    print('Content: ', response.message.content)
  messages.append(response.message)
  if response.message.tool_calls:
    print('Tool calls: ', response.message.tool_calls)
    for tool_call in response.message.tool_calls:
      function_to_call = available_tools.get(tool_call.function.name)
      if function_to_call:
        args = tool_call.function.arguments
        result = function_to_call(**args)
        print('Result: ', str(result)[:200]+'...')
        # Result is truncated for limited context lengths
        messages.append({'role': 'tool', 'content': str(result)[:2000 * 4], 'tool_name': tool_call.function.name})
      else:
        messages.append({'role': 'tool', 'content': f'Tool {tool_call.function.name} not found', 'tool_name': tool_call.function.name})
  else:
    break
</code></pre>

<p><strong>Result</strong></p>

<pre><code>Thinking:  Okay, the user is asking about Ollama's new engine. I need to figure out what they're referring to. Ollama is a company that develops large language models, so maybe they've released a new model or an updated version of their existing engine...

Tool calls:  [ToolCall(function=Function(name='web_search', arguments={'max_results': 3, 'query': 'Ollama new engine'}))]
Result:  results=[WebSearchResult(content='# New model scheduling\n\n## September 23, 2025\n\nOllama now includes a significantly improved model scheduling system. Ahead of running a model, Ollama’s new engine...

Thinking:  Okay, the user asked about Ollama's new engine. Let me look at the search results.

First result is from September 23, 2025, talking about new model scheduling. It mentions improved memory management, reduced crashes, better GPU utilization, and multi-GPU performance. Examples show speed improvements and accurate memory reporting. Supported models include gemma3, llama4, qwen3, etc...

Content:  Ollama has introduced two key updates to its engine, both released in 2025:

1. **Enhanced Model Scheduling (September 23, 2025)**
   - **Precision Memory Management**: Exact memory allocation reduces out-of-memory crashes and optimizes GPU utilization.
   - **Performance Gains**: Examples show significant speed improvements (e.g., 85.54 tokens/s vs 52.02 tokens/s) and full GPU layer utilization.
   - **Multi-GPU Support**: Improved efficiency across multiple GPUs, with accurate memory reporting via tools like `nvidia-smi`.
   - **Supported Models**: Includes `gemma3`, `llama4`, `qwen3`, `mistral-small3.2`, and more.

2. **Multimodal Engine (May 15, 2025)**
   - **Vision Support**: First-class support for vision models, including `llama4:scout` (109B parameters), `gemma3`, `qwen2.5vl`, and `mistral-small3.1`.
   - **Multimodal Tasks**: Examples include identifying animals in multiple images, answering location-based questions from videos, and document scanning.

These updates highlight Ollama's focus on efficiency, performance, and expanded capabilities for both text and vision tasks.
</code></pre>

<p><strong>Recommended models:</strong></p>

<p>These models have great tool-use capabilities and are able to have multi-turn interactions with the user and tools to get to a final result.</p>

<ul>
<li><code>qwen3</code></li>
<li><code>gpt-oss</code></li>
</ul>

<p><strong>Recommended cloud models:</strong></p>

<ul>
<li><code>qwen3:480b-cloud</code></li>
<li><code>gpt-oss:120b-cloud</code></li>
<li><code>deepseek-v3.1-cloud</code></li>
</ul>

<p>The <code>web_search</code> and <code>web_fetch</code> tools can return thousands of tokens. It is recommended to increase the context length of the model to ~32000 tokens for reasonable performance. Search agents work best with full context length.</p>

<h3 id="fetching-page-results">Fetching page results</h3>

<p>To fetch individual pages (e.g. when a user provides a url in the prompt), use the new web fetch API.</p>

<h4 id="python-library">Python library</h4>

<pre><code>from ollama import web_fetch

result = web_fetch('https://ollama.com')
print(result)
</code></pre>

<p><strong>Result</strong></p>

<pre><code>WebFetchResponse(
    title='Ollama',
    content='[Cloud models](https://ollama.com/blog/cloud-models) are now available in Ollama\n\n**Chat &amp; build
with open models**\n\n[Download](https://ollama.com/download) [Explore
models](https://ollama.com/models)\n\nAvailable for macOS, Windows, and Linux',
    links=['https://ollama.com/', 'https://ollama.com/models', 'https://github.com/ollama/ollama']
)
</code></pre>

<p>Example Python code is available on <a href="https://github.com/ollama/ollama-python/blob/main/examples/web-search.py">GitHub</a>.</p>

<h4 id="javascript-library">JavaScript library</h4>

<pre><code>import { Ollama } from "ollama";

const client = new Ollama();
const fetchResult = await client.webFetch({ url: "https://ollama.com" });
console.log(JSON.stringify(fetchResult, null, 2));
</code></pre>

<p><strong>Result</strong></p>

<pre><code>{
  "title": "Ollama",
  "content": "[Cloud models](https://ollama.com/blog/cloud-models) are now available in Ollama...",
  "links": [
    "https://ollama.com/",
    "https://ollama.com/models",
    "https://github.com/ollama/ollama"
  ]
}
</code></pre>

<p>Example JavaScript code is available on <a href="https://github.com/ollama/ollama-js/blob/main/examples/websearch/websearch-tools.ts">GitHub</a>.</p>

<h4 id="curl-1">cURL</h4>

<pre><code>curl --request POST \
  --url https://ollama.com/api/web_fetch \
  --header "Authorization: Bearer $OLLAMA_API_KEY" \
  --header 'Content-Type: application/json' \
  --data '{
      "url": "ollama.com"
}'
</code></pre>

<p><strong>Result</strong></p>

<pre><code>{
  "title": "Ollama",
  "content": "[Cloud models](https://ollama.com/blog/cloud-models) are now available in Ollama...",
  "links": [
    "http://ollama.com/",
    "http://ollama.com/models",
    "https://github.com/ollama/ollama"
  ]
}
</code></pre>

<h3 id="integrations">Integrations</h3>

<h3 id="mcp-server-model-context-protocol-server">MCP Server (Model Context Protocol server)</h3>

<p>You can enable web search in any MCP client through the <a href="https://github.com/ollama/ollama-python/blob/main/examples/web-search-mcp.py">Python MCP server</a>.</p>

<h4 id="cline">Cline</h4>

<p>To integrate with Cline, configure MCP servers in its settings.</p>

<ul>
<li>Manage MCP Servers &gt; Configure MCP Servers &gt; Add the configuration below</li>
</ul>

<pre><code>{
  "mcpServers": {
    "web_search_and_fetch": {
      "type": "stdio",
      "command": "uv",
      "args": ["run", "path/to/web-search-mcp.py"],
      "env": { "OLLAMA_API_KEY": "your_api_key_here" }
    }
  }
}
</code></pre>

<p><a href="https://ollama.com/download"><img src="https://files.ollama.com/cline_mcp_web_search.png" alt="Cline"></a></p>

<h4 id="codex">Codex</h4>

<p>Add the following configuration to <code>~/.codex/config.toml</code></p>

<pre><code>[mcp_servers.web_search]
command = "uv"
args = ["run", "path/to/web-search-mcp.py"]
env = { "OLLAMA_API_KEY" = "your_api_key_here" }
</code></pre>

<p><a href="https://ollama.com/download"><img src="https://files.ollama.com/codex_demo.png" alt="Codex"></a></p>

<h4 id="goose">Goose</h4>

<p>You can integrate with Ollama via Goose’s extensions.</p>

<p><a href="https://ollama.com/download"><img src="https://files.ollama.com/goose_web1.png" alt="Goose"></a></p>

<p><a href="https://ollama.com/download"><img src="https://files.ollama.com/goose_web2.png" alt="Goose"></a></p>

<h3 id="get-started-1">Get started</h3>

<p>Web search is included with a free Ollama account, with much higher rate limits available by <a href="https://ollama.com/cloud">upgrading your Ollama subscription</a>.</p>

<p>To get started, <a href="https://ollama.com/signup">sign up for an Ollama account</a>!</p>

    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Electron-based apps cause system-wide lag on macOS 26 Tahoe (281 pts)]]></title>
            <link>https://github.com/electron/electron/issues/48311</link>
            <guid>45376977</guid>
            <pubDate>Thu, 25 Sep 2025 18:36:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/electron/electron/issues/48311">https://github.com/electron/electron/issues/48311</a>, See on <a href="https://news.ycombinator.com/item?id=45376977">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="issue-body-viewer" data-team-hovercards-enabled="true" data-turbolinks="false" id="issue-body-viewer"><h3 dir="auto">Maintainer update</h3>
<p dir="auto">From <a data-hovercard-type="user" data-hovercard-url="/users/MarshallOfSound/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/MarshallOfSound">@MarshallOfSound</a> (<a data-error-text="Failed to load title" data-id="3412185866" data-permission-text="Title is private" data-url="https://github.com/electron/electron/issues/48311" data-hovercard-type="issue" data-hovercard-url="/electron/electron/issues/48311/hovercard?comment_id=3304577581&amp;comment_type=issue_comment" href="https://github.com/electron/electron/issues/48311#issuecomment-3304577581">#48311 (comment)</a>):</p>
<blockquote>
<p dir="auto">Hey folks, anyone experiencing this issue can you please raise a Feedback (via Feedback Assistant) with Apple. Make sure you send it while the issue is occurring and ensure you include a sysdiagnose with your report (I think that's automatic now, but check the box if there's a box).</p>
<p dir="auto">We need a lot more to go on and this is likely a macOS issue.</p>
</blockquote>
<h3 dir="auto">Preflight Checklist</h3>
<ul>
<li> I have read the <a href="https://github.com/electron/electron/blob/main/CONTRIBUTING.md">Contributing Guidelines</a> for this project.</li>
<li> I agree to follow the <a href="https://github.com/electron/electron/blob/main/CODE_OF_CONDUCT.md">Code of Conduct</a> that this project adheres to.</li>
<li> I have searched the <a href="https://www.github.com/electron/electron/issues">issue tracker</a> for a bug report that matches the one I want to file, without success.</li>
</ul>
<h3 dir="auto">Electron Version</h3>
<p dir="auto">37.3.1</p>
<h3 dir="auto">What operating system(s) are you using?</h3>
<p dir="auto">macOS</p>
<h3 dir="auto">Operating System Version</h3>
<p dir="auto">macOS 26 Tahoe RC</p>
<h3 dir="auto">What arch are you using?</h3>
<p dir="auto">arm64 (including Apple Silicon)</p>
<h3 dir="auto">Last Known Working Electron version</h3>
<p dir="auto">N/A, issue only persists since macOS 26</p>
<h3 dir="auto">Does the issue also appear in Chromium / Google Chrome?</h3>
<p dir="auto">No</p>
<h3 dir="auto">Expected Behavior</h3>
<p dir="auto">Smooth 120fps experience even when Electron-apps are open or not minimized</p>
<h3 dir="auto">Actual Behavior</h3>
<p dir="auto">Using an M1 Max MacBook Pro, having Electron-based apps open / not minimized causes a huge lag.<br>
CPU and GPU usage remains low, but if I have Discord and VS Code open, moving windows, scrolling is stuttery. It happens even when only Discord is open but it gets worse if I open a second Electron app.<br>
This is kind of weird because while having Discord open and I'm in Chrome, the lag still occurs, but it's fixed if I minimize Discord (even though Chrome is fully in focus and maximized). This happens since upgrading to macOS 26 RC, macOS 15 didn't have this issue.<br>
There is a similar lag if I open Settings - Wallpapers, moving the Settings window is laggy then (looks like 60fps instead of 120).</p>
<h3 dir="auto">Testcase Gist URL</h3>
<p dir="auto"><em>No response</em></p>
<h3 dir="auto">Additional Information</h3>
<p dir="auto"><em>No response</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Athlon 64: How AMD turned the tables on Intel (313 pts)]]></title>
            <link>https://dfarq.homeip.net/athlon-64-how-amd-turned-the-tables-on-intel/</link>
            <guid>45376605</guid>
            <pubDate>Thu, 25 Sep 2025 18:09:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dfarq.homeip.net/athlon-64-how-amd-turned-the-tables-on-intel/">https://dfarq.homeip.net/athlon-64-how-amd-turned-the-tables-on-intel/</a>, See on <a href="https://news.ycombinator.com/item?id=45376605">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>22 years ago, on September 23, 2003, AMD changed the game for x86 once and for all. They released the Athlon 64 CPU, a chip that did something Intel didn’t want. Intel didn’t want to extend x86 to 64 bits. But when AMD did it, it forced Intel to clone AMD, rather than the other way around.</p><h2>Why Intel didn’t want to go 64-bit</h2><figure id="attachment_35916" aria-describedby="caption-attachment-35916"><a href="https://dfarq.homeip.net/athlon-64-how-amd-turned-the-tables-on-intel/amd_athlon64_badge/" rel="attachment wp-att-35916"><img data-recalc-dims="1" fetchpriority="high" decoding="async" src="https://i0.wp.com/dfarq.homeip.net/wp-content/uploads/2024/09/AMD_Athlon64_Badge.png?resize=266%2C300&amp;ssl=1" alt="AMD Athlon 64" width="266" height="300" srcset="https://i0.wp.com/dfarq.homeip.net/wp-content/uploads/2024/09/AMD_Athlon64_Badge.png?resize=266%2C300&amp;ssl=1 266w, https://i0.wp.com/dfarq.homeip.net/wp-content/uploads/2024/09/AMD_Athlon64_Badge.png?w=531&amp;ssl=1 531w" sizes="(max-width: 266px) 100vw, 266px"></a><figcaption id="caption-attachment-35916">With the Athlon 64, AMD pulled x86 kicking and screaming into the world of 64-bit, dragging Intel along with it.</figcaption></figure><p>Even in 2001, x86 had decades of baggage attached to it. It was a 32-bit architecture that had been extended from a 16-bit architecture. But that in turn had been extended from an 8-bit CPU design from 1972 that, believe it or not, originated at <a href="https://dfarq.homeip.net/first-desktop-computer/">Datapoint</a>, not Intel.</p><p>This was great for backward compatibility. 8-bit applications were very easy to port to x86 in the early 1980s, and those early DOS applications still ran flawlessly on modern systems 30 years later. For that matter, it’s not impossible to get them running even today.</p><p>Removal of the ability to run 16-bit applications in 64-bit Windows was a design decision, not a technical limitation.</p><p>Intel wanted to start over to go 64-bit. Without having to worry about backward compatibility, they could design something that would be faster and more efficient. In theory at least, it would be able to scale higher in clock speed. And there was no question a new design would outperform a theoretical 64-bit x86 when running at the same speed because of efficiency.</p><p>And if you are cynical, there was one more motivation. If Intel could start over, they wouldn’t have to worry about competing CPU designs, at least not for a very long time. The new design would be encumbered with so many patents, it might be 20 years before someone could clone it.</p><p>Keep in mind that in 2003, not only was AMD in the picture, but <a href="https://dfarq.homeip.net/what-happened-to-transmeta-the-last-big-dotcom-ipo/">Transmeta</a> was still in the picture, and <a href="https://dfarq.homeip.net/cyrix-processor-chips/">Cyrix</a> was fading but not completely gone.</p><p>Starting over with a new CPU architecture outright was massively attractive to Intel.</p><p>This new 64-bit architecture wasn’t theoretical, either. Intel was producing it. It was called Itanium, and Intel first released it in June 2001.</p><h2>AMD’s risky bet and why they made it</h2><p>AMD was well aware of the shortcomings of extending x86 to 64 bits. And they did it anyway. For them, the stakes were completely different.</p><p>AMD knew that if Itanium caught on, that would be the end for them as a CPU company, unless maybe they wanted to become just another ARM licensee. Being just another ARM licensee is more attractive in 2025 than it was in 2003.</p><p>But they could see Itanium wasn’t catching on. It had its uses, and it was doing well enough in those niches, but Windows on Itanium was a non-starter. So much so, <em>The Register</em> called it “Itanic.”</p><p>AMD bet that there would be appeal in a 64-bit architecture that was fully backward compatible with x86 and natively ran 32-bit applications at full speed. People would be able to run 32-bit Windows and 32-bit applications on it if they needed to, and then when they were ready for 64-bit software, the hardware was there and ready to go. And they could continue to run 32-bit apps in 64-bit operating systems as long as needed to ease the transition.</p><p>The transition to 32 bits took a decade. AMD reasoned more people would be willing to upgrade to 64 bits if they made that transition as similar as the transition from the <a href="https://dfarq.homeip.net/286-vs-386sx/">286 to the 386</a> as possible.</p><p>They believed the market would willingly trade lower 64-bit performance in the long term for better 32-bit performance right away. They also believed that if Microsoft was willing to build Windows on Itanium, they would be willing to take a chance on 64-bit x86 as well.</p><p>So on September 23, 2003, AMD launched its Athlon 64, the first 64-bit x86 CPU.</p><h2>Why the Athlon 64 was a hit</h2><p>AMD64 was everything AMD hoped it would be. It was backward compatible with 32-bit x86. The 64-bit builds of Windows weren’t available immediately, and they didn’t catch on immediately, but you cannot say nobody used them. People did, in fact, use them. In late 2005, I was in charge of administering the complimentary antivirus software that Charter Communications provided to its subscribers. I’m not going to say say someone called me every day wanting 64-bit antivirus for 64-bit Windows. But it did happen once a week.</p><p>The transition took at least as long as AMD expected. <a href="https://dfarq.homeip.net/64-bits-or-bust/">When I finally bought an Athlon 64 in 2011</a>, I found native 64-bit software was still scarce. I’m an outspoken Firefox fan; the reason I briefly switched to <a href="https://dfarq.homeip.net/google-chrome-launched-september-2-2008/">Google Chrome</a> was to get a 64-bit web browser.</p><h3>The Athlon 64 in the enterprise</h3><p>A few months later, I got a better job with more pay and better growth potential. I can’t talk a lot about the job, but I was administering a mission critical system that ran on Windows, mostly on Dell hardware. I mention <a href="https://dfarq.homeip.net/history-of-dell-computers/">Dell</a> because they were exclusively an Intel vendor for years. Cofounder and longtime AMD CEO <a href="https://dfarq.homeip.net/jerry-sanders-cofounder-of-amd/">Jerry Sanders</a> once said of Michael Dell, “I can’t sell him a[n AMD] <a href="https://dfarq.homeip.net/amd-k6-released-april-2-1997/">K6</a> no matter what I do.”</p><p>It was the Athlon 64 that made Dell relent and finally start using AMD CPUs. Not only were they using them on desktop systems, but they were putting AMD CPUs in servers, an idea that would have been extremely controversial 5 years before. At least in the circles I ran in.</p><p>The Athlon 64 caught on because, in spite of its name, it was an outstanding 32-bit CPU. It was faster than an Intel CPU running at the same clock rate, and it used less power as well. The power consumption was the key to getting into the data center. The Intel name was a security blanket, even though AMD had been making x86 CPUs exactly as long as Intel. But certain decision makers bought Intel marketing and saw AMD as a second tier brand.</p><p>The thing is, when you have a data center with hundreds of systems in it, the money you save on a more efficient CPU really talks.</p><p>Replacing Intel Prescott-based servers with AMD64 servers was not a universally popular idea. But you could tell a difference when you were standing behind a rack full of Intel-based servers versus a rack full of AMD based servers. The Intels ran hotter.</p><p>From an uptime perspective, we couldn’t see a difference. The performance metrics I collected showed there was a slight difference, and that difference was in AMD’s favor. So the AMD critics quickly ate their words.</p><h3>Intel giving in and cloning AMD64</h3><p>In 2004, Intel wrote off the Itanium and cloned AMD64. They called it Intel64, but it was a blatant copy of the AMD implementation. A quirk in the agreements that allowed AMD to use the x86 instruction set also gave Intel the rights to use the AMD64 instructions. So there was nothing illegal about what Intel did. Itanium continued to see use in specialized applications, but Intel quietly discontinued it in 2020.</p><p>AMD and Intel have been chasing and catching each other ever since. One of them will pass the other for a CPU generation or two, and then they will change positions. It’s not terribly different from the situation in 1999 with the <a href="https://dfarq.homeip.net/amd-athlon-amds-game-changing-cpu-from-1999/">original Athlon</a>, when AMD outperformed Intel for the first time. The question in everyone’s mind was whether they would do it a second time. The Athlon 64 was the second time.</p><p>It was a big step forward. Eight years before, AMD was trying to pass off a high-clocked <a href="https://dfarq.homeip.net/amd-5x86-announced-september-1995/">486 as a Pentium equivalent</a>. With the Athlon 64, AMD was innovating.</p><div itemtype="http://schema.org/Person" itemscope="" itemprop="author"><p><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/dfarq.homeip.net/wp-content/uploads/2017/06/dave_farquhar_181px.jpg?resize=100%2C100&amp;ssl=1" data-src="https://i0.wp.com/dfarq.homeip.net/wp-content/uploads/2017/06/dave_farquhar_181px.jpg?resize=100%2C100&amp;ssl=1" width="100" height="100" alt="" itemprop="image"></p><div><p>David Farquhar is a computer security professional, entrepreneur, and author. He has written professionally about computers since 1991, so he was writing about retro computers when they were still new. He has been working in IT professionally since 1994 and has specialized in vulnerability management since 2013. He holds Security+ and CISSP certifications. Today he blogs five times a week, mostly about retro computers and retro gaming covering the time period from 1975 to 2000.</p></div></div></div></div>]]></description>
        </item>
    </channel>
</rss>