<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 06 Nov 2024 11:30:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[New images of Jupiter (112 pts)]]></title>
            <link>https://www.missionjuno.swri.edu/junocam/processing?source=all&amp;ob_from=2024-10-01&amp;ob_to=2024-11-01&amp;phases%5B%5D=PERIJOVE+66&amp;perpage=16</link>
            <guid>42057851</guid>
            <pubDate>Wed, 06 Nov 2024 07:30:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.missionjuno.swri.edu/junocam/processing?source=all&#x26;ob_from=2024-10-01&#x26;ob_to=2024-11-01&#x26;phases%5B%5D=PERIJOVE+66&#x26;perpage=16">https://www.missionjuno.swri.edu/junocam/processing?source=all&#x26;ob_from=2024-10-01&#x26;ob_to=2024-11-01&#x26;phases%5B%5D=PERIJOVE+66&#x26;perpage=16</a>, See on <a href="https://news.ycombinator.com/item?id=42057851">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="section_info" tabindex="0">

						<h2>IMAGE PROCESSING GALLERY</h2>

<section>
	<nav>
			<a role="button" href="#Welcome" id="labelfor-Welcome">Welcome!</a>
			<a role="button" href="#PJ1Images" id="labelfor-PJ1Images">PJ‚Äì1 Images</a>
			<a role="button" href="#GalleryOrganization" id="labelfor-GalleryOrganization">Gallery Organization</a>
			<a role="button" href="#AboutJunoCamImages" id="labelfor-AboutJunoCamImages">About JunoCam Images</a>
	</nav>
	
	<section aria-labelledby="labelfor-Welcome" id="Welcome" tabindex="0">	<span role="button"><p>Welcome!</p></span>
	<div><p>This is where we post raw images from <a href="https://www.missionjuno.swri.edu/media-gallery/instruments?show=fig_562e2fa248b496f704cf3f8a&amp;m=179" target="">JunoCam</a>. We invite you to
download them, do your own image processing, and we encourage you to upload
your creations for us to enjoy and share. The types of image processing
we‚Äôd love to see range from simply cropping an image to highlighting a
particular atmospheric feature, as well as adding your own color enhancements,
creating collages and adding advanced color reconstruction.</p><p>One of the biggest challenges for Juno is <a href="https://www.missionjuno.swri.edu/jupiter/magnetosphere?show=hs_jupiter_magnetosphere_story_radiation-belts">Jupiter's intense radiation belts</a>, which are expected to limit the lifetime of both Juno‚Äôs engineering and science subsystems. <i>JunoCam is now showing the effects of that radiation on some of its parts</i>.&nbsp; <a href="https://www.missionjuno.swri.edu/junocam/processing?source=junocam&amp;phases[]=PERIJOVE+56" target="">PJ56 images</a> show a reduction in our dynamic range and an increase in background and noise. We invite citizen scientists to explore new ways to process these images to continue to bring out the beauty and mysteries of Jupiter and its moons. </p><p>For those of you who have contributed ‚Äì thank you! Your labors of love have illustrated articles
about Juno, Jupiter and JunoCam. Your
products show up in all sorts of places.&nbsp; We have used them to report to the scientific community. We are writing papers for scientific journals
and using your contributions ‚Äì always with appropriate attribution of
course. Some creations are works of art
and we are working out ways to showcase them as art.</p></div>
</section><section aria-labelledby="labelfor-PJ1Images" id="PJ1Images" tabindex="0">	<span role="button"><p>PJ‚Äì1 Images</p></span>
	<div><p>The first perijove pass of Jupiter was a test run for
JunoCam. The set of 28 images taken
were designed to find optimal viewing geometries and camera settings. For example, we took 4 images of the north
pole. We used two different settings for
the time-delayed-integration (TDI), which determines the integration time, to
see which would be best for the polar region and a very high TDI level (long
exposure) to try to detect Jupiter‚Äôs aurora. We imaged at two different geometries, looking directly down at the pole
and looking at closest range at a more oblique angle, to see which would give
us the best results. We ran through a
similar set of tests for the south pole. Another comparison we made was to test different compression
settings.</p><p>

We
have a methane filter, included for the polar science investigation, that is
almost at the limits of our detector‚Äôs wavelength range. To get enough photons for an image we need to
use a very long exposure. In some images
this results in scattered light in the image.&nbsp;
For science purposes we will simply crop out the portions of the image
that include this artifact. Work is in
progress to determine exactly what conditions cause stray light problems so
that this can be minimized for future imaging.</p></div>
</section><section aria-labelledby="labelfor-GalleryOrganization" id="GalleryOrganization" tabindex="0">	<span role="button"><p>Gallery Organization</p></span>
	<div><p>The gallery displays images from JunoCam itself, as well as uploads from the community.&nbsp;</p><p>The&nbsp;JunoCam images&nbsp;are identified by a small spacecraft icon. You will see both raw and processed versions of the images as they become available. The JunoCam movie posts have too many images to post individually, so we are making &nbsp;them available for download in batches as zip files.</p><p>You can&nbsp;filter&nbsp;the gallery by many different characteristics, including by Perijove Pass, Points of Interest and Mission Phase. If you have a favorite ‚Äúartist‚Äù you can create your own gallery.&nbsp; Click on ‚ÄúSubmitted by‚Äù on the left, select your favorite artist(s), and then click on ‚ÄúFilter‚Äù.</p><p><i>A special note about the&nbsp;<i>Earth Flyby mission phase</i>&nbsp;images</i><i>: these were acquired in 2013 when Juno flew past Earth.&nbsp;Examples of processed images are shown; most contributions are from amateurs.</i></p></div>
</section><section aria-labelledby="labelfor-AboutJunoCamImages" id="AboutJunoCamImages" tabindex="0">	<span role="button"><p>About JunoCam Images</p></span>
	<div><p>Like previous MSSS cameras (e.g., Mars Reconnaissance Orbiter‚Äôs Mars Color Imager) Junocam is a "pushframe" imager. The detector has multiple filter strips, each with a different bandpass, bonded directly to its photoactive surface. Each strip extends the entire width of the detector, but only a fraction of its height; Junocam's filter strips are 1600 pixels wide and about 155 rows high. The filter strips are scanned across the target by spacecraft rotation. At the nominal spin rate of 2 RPM, frames are acquired about every 400 milliseconds. Junocam has four filters: three visible (red/green/blue) and a narrowband "methane" filter centered at about 890 nm.&nbsp;</p><p>The spacecraft spin rate would cause more than a pixel's worth of image blurring for exposures longer than about 3.2 milliseconds. For the illumination conditions at Jupiter such short exposures would result in unacceptably low SNR, so the camera provides Time-Delayed-Integration (TDI). TDI vertically shifts the image one row each 3.2 milliseconds over the course of the exposure, cancelling the scene motion induced by rotation. Up to about 100 TDI steps can be used for the orbital timing case while still maintaining the needed frame rate for frame-to-frame overlap. For Earth Flyby the light levels are high enough that TDI is not needed except for the methane band and for nightside imaging. &nbsp;</p><p>Junocam pixels are 12 bits deep from the camera but are converted to 8 bits inside the instrument using a lossless "companding" table, a process similar to gamma correction, to reduce their size.&nbsp; All Junocam products on the missionjuno website are in this 8-bit form as received on Earth.&nbsp; Scientific users interested in radiometric analysis should use the "RDR" data products archived with the Planetary Data System, which have been converted back to a linear 12-bit scale.</p></div>
</section></section>
						

					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Former President Trump is projected to win the presidency (325 pts)]]></title>
            <link>https://thehill.com/homenews/campaign/4969061-trump-wins-presidential-election/</link>
            <guid>42057647</guid>
            <pubDate>Wed, 06 Nov 2024 06:49:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thehill.com/homenews/campaign/4969061-trump-wins-presidential-election/">https://thehill.com/homenews/campaign/4969061-trump-wins-presidential-election/</a>, See on <a href="https://news.ycombinator.com/item?id=42057647">Hacker News</a></p>
Couldn't get https://thehill.com/homenews/campaign/4969061-trump-wins-presidential-election/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Useful built-in macOS command-line utilities (150 pts)]]></title>
            <link>https://weiyen.net/articles/useful-macos-cmd-line-utilities</link>
            <guid>42057431</guid>
            <pubDate>Wed, 06 Nov 2024 05:51:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://weiyen.net/articles/useful-macos-cmd-line-utilities">https://weiyen.net/articles/useful-macos-cmd-line-utilities</a>, See on <a href="https://news.ycombinator.com/item?id=42057431">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-mdx-content="true"><article><p>Sometimes when I'm bored, I like to look at the list of <a href="https://ss64.com/mac/">macOS Bash commands</a>. Here's some commands that I found interesting:</p><h2>Access your Keychain programmatically</h2><p>If you store your secrets in the Keychain (and you should!), you can access them programmatically using <code>security</code>.</p><pre><p><span>security find-internet-password </span><span>-s</span><span> </span><span>"https://example.com"</span></p></pre><p>I found this useful for writing automated scripts that used locally-stored credentials.</p><p>Link: <a href="https://ss64.com/mac/security.html">https://ss64.com/mac/security.html</a></p><p>Bonus tip: If you are using 1Password, there is a <a href="https://developer.1password.com/docs/ssh/get-started#install-the-1password-cli">1Password CLI</a> that you can use to access your 1Password items from the command line.</p><h2>Open files from the terminal</h2><p>If you want to open a file from the terminal, you can use the <code>open</code> command.</p><pre><p><span>open</span><span> file.txt</span></p></pre><p>This will open the file in the default application for that file type, as if you had double-clicked it in the Finder.</p><p>Link: <a href="https://ss64.com/mac/open.html">https://ss64.com/mac/open.html</a></p><h2>Copy and paste</h2><p><code>pbcopy</code> and <code>pbpaste</code> are command-line utilities that allow you to copy and paste text to the pasteboard (what other operating systems might call the "clipboard").</p><p><code>pbcopy</code> takes whatever was given in the standard input, and places it in the pasteboard.</p><pre><p><span>echo</span><span> </span><span>"Hello, world!"</span><span> </span><span>|</span><span> pbcopy</span><span>;</span></p></pre><p><code>pbpaste</code> takes whatever is in the pasteboard and prints it to the standard output.</p><pre><p><span>pbpaste</span></p><p><span></span><span>&gt;&gt;</span><span> Hello, world</span><span>!</span></p></pre><p>This is very useful for getting data from files into the browser, or other GUI applications.</p><h4>Links:</h4><ul><li><a href="https://ss64.com/mac/pbcopy.html">https://ss64.com/mac/pbcopy.html</a></li><li><a href="https://ss64.com/mac/pbpaste.html">https://ss64.com/mac/pbpaste.html</a></li></ul><h2>UTC date</h2><p>If you work with servers a lot, it can be useful to know the current time in UTC, when e.g. looking at<!-- --> <!-- -->server logs.</p><p>This is a one-liner in the terminal:</p><pre><p><span>date</span><span> </span><span>-u</span></p></pre><p>Alternatively, you can use</p><pre><p><span>TZ</span><span>=</span><span>UTC </span><span>date</span></p></pre><p>Link: <a href="https://ss64.com/mac/date.html">https://ss64.com/mac/date.html</a></p><h2>Internet speedtest</h2><p>If you want to run an Internet speedtest, you can run one directly from the terminal with</p><pre><p><span>networkQuality  </span></p></pre><p>Link: <a href="https://ss64.com/mac/networkquality.html">https://ss64.com/mac/networkquality.html</a></p><h2>Prevent your Mac from sleeping</h2><p>If you are want to keep your Mac from sleeping, you can run <code>caffeinate</code> in the terminal.</p><pre><p><span>caffeinate</span></p></pre><p><code>caffeinate</code> will keep your Mac awake until you stop it, e.g. by pressing Ctrl+C. <code>caffeinate</code> used to<!-- --> <!-- -->be a third-party tool, but it is now built-in to macOS.</p><p>I use this mostly to prevent my Mac from sleeping when I am running a server.</p><p>Link: <a href="https://ss64.com/mac/caffeinate.html">https://ss64.com/mac/caffeinate.html</a></p><h2>Generate UUIDs</h2><p>If you need to generate a UUID, you can use the <code>uuidgen</code> command.</p><pre><p><span>uuidgen</span></p></pre><p>By default <code>uuidgen</code> outputs a UUID in uppercase. You can combine this with <code>tr</code> and <code>pbcopy</code> to copy the UUID to the clipboard in lowercase.</p><pre><p><span>uuidgen </span><span>|</span><span> </span><span>tr</span><span> </span><span>'[:upper:]'</span><span> </span><span>'[:lower:]'</span><span> </span><span>|</span><span> pbcopy</span></p></pre><p>I use this a lot when writing unit tests that require IDs.</p><p>Link: <a href="https://ss64.com/mac/uuidgen.html">https://ss64.com/mac/uuidgen.html</a></p><h2>Honourable mentions</h2><ul><li><code>mdfind</code>: Spotlight search, but in the terminal. I generally use Spotlight itself (or rather the excellent <a href="https://www.raycast.com/">Raycast</a>). <a href="https://ss64.com/mac/mdfind.html">Link</a></li><li><code>say</code>: This command makes your Mac speak the text you give it. <a href="https://ss64.com/mac/say.html">Link</a></li><li><code>screencapture</code>: This command allows you to take screenshots and save them to a file. I prefer using <code>cmd-shift-5</code> for this. <a href="https://ss64.com/mac/screencapture.html">Link</a></li><li><code>networksetup</code>: This command allows you to configure your network settings programmatically. I found its API very intimidating, and so I haven't really used it much. <a href="https://ss64.com/mac/networksetup.html">Link</a></li></ul></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What if your computer beeps each time it sends data to Google? (320 pts)]]></title>
            <link>https://berthub.eu/articles/posts/tracker-beeper/</link>
            <guid>42057036</guid>
            <pubDate>Wed, 06 Nov 2024 03:21:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://berthub.eu/articles/posts/tracker-beeper/">https://berthub.eu/articles/posts/tracker-beeper/</a>, See on <a href="https://news.ycombinator.com/item?id=42057036">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><nav id="main-menu" aria-label="Main Menu">
  
</nav>

      

      <main id="content">


<article>
  

  
  

  <div>
  <p>A week ago, I finally got round to implementing an idea I‚Äôd been toying with for years: what if your computer made a little bit of noise every time it sent data to Google?</p>
<p>From studying logs, I‚Äôd long known just how many sites send all your visits and clicks to (at least) Google, but a log that you have to manually create first and then analyze is not very dramatic. You need to work on it and finally you think ‚Äúwell yeah that is a lot‚Äù.</p>
<center>
<video controls="" width="90%">
    <source src="https://berthub.eu/articles/trackerbeeper.mp4" type="video/mp4">
    Sorry, your browser doesn't support embedded videos.
</video>
</center>
<p>The video above beeps only on Google, and it shows how the <a href="https://werkenvoornederland.nl/">official Dutch government jobs site</a> (which also advertises for the intelligence and security services) sends your every click to Google - despite never asking for your permission to do so. It also reports to Google if you clicked the button ‚Äúapply for this job‚Äù, or even ‚Äúcall us for information‚Äù. Nice.</p>
<p>I announced the tool in a tweet:</p>
<p><img loading="lazy" src="https://berthub.eu/articles/beeper-tweet.png"></p>
<p>And within a week, the video received a million views. This spurred me on to add support for Facebook and dozens of the other trackers that infest our sites. Behold the noise when you visit some well known news sites:</p>
<center>
<video controls="" width="90%">
    <source src="https://berthub.eu/articles/trackerbeeper-wow.mp4" type="video/mp4">
    Sorry, your browser doesn't support embedded videos.
</video>
</center>

<ul>
<li><a href="https://www.rtlnieuws.nl/tech/artikel/5329774/google-tracking-klikker-googerteller-bert-hubert-privacy-online">RTLNieuws.nl</a></li>
<li><a href="https://9to5google.com/2022/08/22/app-beeps-send-data-google/">9to5Google</a></li>
<li><a href="https://www.it-daily.net/shortnews/google-teller-browser-plugin-macht-ein-geraeusch-wenn-google-daten-erhaelt">it-daily.net</a></li>
<li><a href="https://stadt-bremerhaven.de/googerteller-app-piept-jedes-mal-wenn-der-rechner-daten-an-google-uebertraegt/">Stadt Bremerhaven</a></li>
<li><a href="https://tarnkappe.info/artikel/datenschutz/googerteller-dem-datenkraken-auf-der-spur-254630.html">Tarnkappe.info</a></li>
</ul>
<h2 id="status-of-the-software">Status of the software</h2>
<p>For now, <a href="https://github.com/berthubert/googerteller">it is still pretty rough stuff</a>, suitable only for Linux, OSX and BSD users comfortable entering command lines. The goals are:</p>
<ol>
<li>Continue development on Linux until the necessary features are implemented and stable</li>
<li>Perhaps simultaneously make an Apple / OSX version available that runs with a single click</li>
<li>Create a Windows version</li>
<li>Perhaps perhaps try to implement something similar on iOS and Android, which will not be easy: phones prefer to snitch on you in full privacy</li>
</ol>
<h2 id="live-demo-installation">Live demo installation</h2>
<p>I would also <strong>love</strong> to turn this into a live demo for use on phones and tablets. The idea would then be to have a low power WiFi network. There‚Äôs a big QR code (on a poster or a big screen). If you scan that, your phone asks you if you want to join the demo WiFi.</p>
<p>And when you do and use your phone, big speakers make the tracker noises. For extra points, make one speaker per tracker, so a huge Google speaker, one for Facebook and dozens of smaller ones.</p>
<p>Especially Android phones leak information 24/7 so this should be a pretty convincing demo.</p>
<p>If anyone wants to help make this happen, let me know. All it requires is a Raspberry Pi and another phone to deliver internet connectivity.</p>
<h2 id="further-goals">Further Goals</h2>
<ul>
<li>Support all popular trackers</li>
<li>Configurable which ones you want to hear about</li>
<li>With configurable sounds (also in stereo, so ‚Äúgoogle‚Äù in the middle, ‚ÄúFacebook‚Äù on the right speaker)</li>
</ul>

</div>

  



</article>

<nav>
  
</nav>




      </main>

      

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Title drops in movies (223 pts)]]></title>
            <link>https://www.titledrops.net/</link>
            <guid>42056923</guid>
            <pubDate>Wed, 06 Nov 2024 02:48:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.titledrops.net/">https://www.titledrops.net/</a>, See on <a href="https://news.ycombinator.com/item?id=42056923">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p data-svelte-h="svelte-re3d2b">I'm sure you all know the part of the movie where one of the characters says <i>the actual title of the movie</i>
		and you're like
		<img loading="lazy" src="https://www.titledrops.net/img/leonardo_pointing.jpg"></p> <p>The overall meta-ness of this is - of course - nothing new. And filmmakers and scriptwriters
		have been doing it since the dawn of the medium itself<span data-svelte-h="svelte-1lmooii">*</span>. It's known in film speak as a <strong data-svelte-h="svelte-u51ftf">title drop</strong>.</p> <p data-svelte-h="svelte-yuztv1">Consequently, there's tons of examples throughout movie history that range from the iconic (see
		Back to the Future's above)<br> via the eccentric,</p> <p><img loading="lazy" src="https://www.titledrops.net/img/drops/eternal_sunshine_drop.jpg"></p><p data-svelte-h="svelte-3rfu4h">the very much self-aware</p> <p><img loading="lazy" src="https://www.titledrops.net/img/drops/hot_tub_drop.jpg"></p><p data-svelte-h="svelte-1c1pags">to the downright cringe.</p> <p><img loading="lazy" src="https://www.titledrops.net/img/drops/suicide_squad_drop.jpg"></p><p data-svelte-h="svelte-1b81rdb">But how common are these title drops really? Has this phenomenon gained momentum over time with
		our postmodern culture becoming ever more meta? Can we predict anything about the quality of a
		film based on how many times its title is mentioned? And what does a movie title mean, anyway?</p> <p>There have been <a href="https://datanaut.blog/posts/title-drops-in-movies/" target="_blank" data-svelte-h="svelte-6w8ywg">analyses</a>
		and
		<a href="https://www.buzzfeed.com/jeremyhayes1/movie-title-in-the-movie-moments" target="_blank" data-svelte-h="svelte-a9m0z6">oh</a> <a href="https://www.newyorker.com/humor/daily-shouts/the-thirty-greatest-titular-lines-in-movie-history" target="_blank" data-svelte-h="svelte-10ewxdq">so</a> <a href="https://www.watchmojo.com/articles/top-10-movie-title-name-drops#:~:text=%231%3A%20%E2%80%9CBack%20to%20the%20Future%E2%80%9D%20(1985)&amp;text=It%20would%20top%20this%20podium,%2Ddrops%20go%2C%20it's%20exemplary!" target="_blank" data-svelte-h="svelte-125zz3k">so</a> <a href="https://screenrant.com/worst-title-drops-movies-reddit/" target="_blank" data-svelte-h="svelte-143sq03">many</a> <a href="https://www.gamesradar.com/50-greatest-movie-title-drops/" target="_blank" data-svelte-h="svelte-1ozhrnj">listicles</a>
		of the title drop phenomenon before, but they are small and anecdotal. Here's the first extensive
		analysis of title drops for a dataset of <strong data-svelte-h="svelte-1b0xmtx">73,921 movies</strong> that amount to
		<strong data-svelte-h="svelte-1r2vhv">roughly 61% of movies on IMDb</strong> with at least 100 user votes<span data-svelte-h="svelte-1lmooii">*</span>. I'm looking at <strong data-svelte-h="svelte-1dmwuha">movies released between 1940 and 2023</strong>.
		<img loading="lazy" src="https://www.titledrops.net/img/charts/movie_histogram.svg"> Special thanks go to my friends at
		<a href="https://opensubtitles.com/" target="_blank" data-svelte-h="svelte-6ld4nw">OpenSubtitles.com</a> for providing this data!</p> <h3>Let's talk data</h3> <p><img src="https://www.titledrops.net/img/deco/scribble-scratchy.svg"></p><p data-svelte-h="svelte-1b16of4">I started out with two datasets: 89,242 (English) movie subtitles from <a href="https://opensubtitles.com/" target="_blank">OpenSubtitles.com</a>
		and metadata for 121,797 movies from
		<a href="https://developer.imdb.com/non-commercial-datasets/" target="_blank">IMDb</a>. After
		joining them and filtering them for broken subtitle files I was left with a total of 73,921
		subtitled movies. With that out of the way, I realized that the tougher task was still ahead of
		me: answering the question what even <em>was</em> a title drop?</p> <p>The na√Øve approach is - of course - to simply look for the movie's name anywhere in the
		subtitles. Which is a fantastic approach for movies like <a href="https://imdb.com/title/tt0088763" target="_blank"><img src="https://www.titledrops.net/img/film-strip-fill.svg">Back to the Future</a> with a nice unique title:</p>  <p>But this quickly breaks down if we look at movies like <a href="https://imdb.com/title/tt0910869" target="_blank"><img src="https://www.titledrops.net/img/film-strip-fill.svg">E</a> or <a href="https://imdb.com/title/tt2302966" target="_blank"><img src="https://www.titledrops.net/img/film-strip-fill.svg">I</a> <span data-svelte-h="svelte-1lmooii">*</span>, which lead to way too many matches.</p> <p data-svelte-h="svelte-1cgcrje">We also run into problems with every movie that is a sequel (Rocky III, Hot Tub Time Machine 2)
		since none of the characters will add the sequel number to character names/oversized bathing
		equipment. Similarly, the <a href="https://www.thewrap.com/2016-movie-titles-colon-record-captain-america-rogue-one/" target="_blank">rise of the colon</a>
		in movie titles would make for some very awkward dialogue (LUKE: "Gosh Mr. Kenobi, it's almost like
		we're in the middle of some <i>Star Wars Episode Four: A New Hope</i>!").
		<img loading="lazy" src="https://www.titledrops.net/img/dead_poets_fake.jpeg">
		(See also the
		<a href="https://knowyourmeme.com/memes/he-didnt-say-that-movie-titles-in-movie-lines" target="_blank">He Didn't Say That</a>
		meme.)</p> <p>So I applied a few rules to my title matching in the dialogue. Leading 'The', 'An' and 'A's and
		special characters like dashes are ignored, sequel numbers both Arabic and Roman are dropped
		(along with 'Episode...', 'Part...' etc.) and titles containing a colon are split and either
		side counts as a title drop. So for <a href="https://imdb.com/title/tt0120737" target="_blank"><img src="https://www.titledrops.net/img/film-strip-fill.svg">The Lord of the Rings: The Fellowship of the Ring</a>
		either "Lord of the Rings" or "Fellowship of the Ring" would count as title drops (feel free to hover
		over the visualizations to explore the matches)!</p>  <p data-svelte-h="svelte-1byeszk">With the data cleaning out of the way, let's get down to business!</p> <h3>Stats</h3> <p><img src="https://www.titledrops.net/img/deco/scribble-scratchy.svg"></p><div data-svelte-h="svelte-jpwz7q"><p>Alright, so here's the number you've all been waiting for (drumroll):
		</p><p> <strong>36.5% - so about a third - of movies have at least one title drop during their runtime.</strong></p><p>
		Also, there's a total of 277,668 title drops for all 26,965 title-dropping movies which means that
		there's an
		<strong>average of 10.3 title drops per movie that title drops</strong>. If they do it, they
		really go for it.</p></div> <p>So who are the most excessive offenders in mentioning their titles over the course of the film?
		The overall star when it comes to fiction only came out last year: it's <a href="https://imdb.com/title/tt1517268" target="_blank"><img src="https://www.titledrops.net/img/film-strip-fill.svg">Barbie</a> by Greta Gerwig with an impressive
		<strong data-svelte-h="svelte-14d7fe0">267 title drops within its 1 hour and 54 minutes runtime</strong>, clocking in at a
		whopping <strong data-svelte-h="svelte-dufrow">2.34 BPM (Barbies Per Minute)</strong>.</p> <p>On the non-fiction side of documentaries the winner is <a href="https://imdb.com/title/tt11178402" target="_blank"><img src="https://www.titledrops.net/img/film-strip-fill.svg">Mickey: The Story of a Mouse</a>
		with <strong data-svelte-h="svelte-1rvxecz">309 title drops in only 90 minutes</strong>, so
		<strong data-svelte-h="svelte-covmcq">3.43 Mickeys Per Minute</strong>!</p> <div><h4>Top ten number of title drops in one movie</h4> <p><img src="https://www.titledrops.net/img/deco/scribble-wavy.svg"> </p></div> <div><p data-svelte-h="svelte-1b6n1rq">Fiction only</p>   <p data-svelte-h="svelte-1teau28">Fiction + Documentaries</p></div>  <p data-svelte-h="svelte-t9zag4">What's interesting about the (Fiction) list here is that it's pretty international: only two of
		the top ten movies come from Hollywood, 6 are from India, one from Indonesia and one from
		Turkey. So it's definitely an international phenomenon.</p> <h3>Names in titles</h3> <p><img src="https://www.titledrops.net/img/deco/scribble-scratchy.svg"></p><p>Looking at the top ten list you might have noticed this little icon <img src="https://www.titledrops.net/img/person-circle-outline-svgrepo-com.svg">
		signifying a movie where the data says it's named after one of its characters<span data-svelte-h="svelte-1lmooii">*</span>.</p> <p data-svelte-h="svelte-1sdzjv0">Unsurprisingly, movies named after one of their characters have <strong>an average of 24.7 title drops</strong>, more than twice as much as the usual 10.3. Protagonists have a tendency to pop up repeatedly
		in a film, so their names usually do the same.
		<br>
		Similarly, movies named after a protagonist have a <strong>title drop rate of 88.5%</strong>
		while only 34.2% of other movies drop their titles.
		<img loading="lazy" src="https://www.titledrops.net/img/charts/named_vs_unnamed.svg"></p> <details><summary data-svelte-h="svelte-qu4307">A note on the data here</summary> <hr>
			This is the more experimental part of the analysis. To figure out if a movie was named after its
			protagonist I've used
			<a href="https://datasets.imdbws.com/" target="_blank" data-svelte-h="svelte-xvznn2">IMDb's Principals Dataset</a>
			that lists character names for the first couple of actors and compared that to the movie's title.<br>
			This approach yields reliable results, but of course misses movies when the character the movie
			is named after does not appear on that list. So you might find movies that miss the
			<img loading="lazy" src="https://www.titledrops.net/img/person-circle-outline-svgrepo-com.svg"> 'Named'
			icon even though they're clearly named after a character.

			<p>

			Special characters in the title and character name are also challenging: for example, <a href="https://imdb.com/title/tt0253828" target="_blank"><img src="https://www.titledrops.net/img/film-strip-fill.svg">Tosun Pasa</a> which actually has a ≈ü character in its title - wrong on IMDb (Pasa) as well as the subtitles
			(Pasha) - or <a href="https://imdb.com/title/tt0910970" target="_blank"><img src="https://www.titledrops.net/img/film-strip-fill.svg">WALL¬∑E</a> with the challenging ¬∑ in the middle: Even
			though there are mentions of "Wall-E" in the subtitles, the script - looking for "WALL¬∑E" - wouldn't
			detect it. (I've fixed both of these films manually - but there might be more!)

			</p><p>
			Titles or surnames also usually prevent being counted as title drops according to our definitions.
			<a href="https://imdb.com/title/tt0066078" target="_blank"><img src="https://www.titledrops.net/img/film-strip-fill.svg">Michael The Brave</a>,
			<a href="https://imdb.com/title/tt0064553" target="_blank"><img src="https://www.titledrops.net/img/film-strip-fill.svg">King Lear</a> or <a href="https://imdb.com/title/tt0072684" target="_blank"><img src="https://www.titledrops.net/img/film-strip-fill.svg">Barry Lyndon</a> might mention a character's name ('Michael', 'Lear', 'Barry') but leave out the title or surname
			- so zero drops.

			</p><p>
			Nevertheless, there do exist named films where you would expect a title drop which doesn't come!
			Examples are:
			</p>

			Anyway - back to the analysis!
			<hr></details> <p data-svelte-h="svelte-1yzg0zi">An interesting category are movies named after a character that only have a single title drop -
		making it all the more meaningful?</p> <div><h4>Movies named after a character with single title drops</h4> <p><img src="https://www.titledrops.net/img/deco/scribble-wavy.svg"> </p></div>  <h3>"Real" title drops</h3> <p><img src="https://www.titledrops.net/img/deco/scribble-scratchy.svg"></p><p data-svelte-h="svelte-1h25k9n">Title-drop connoisseurs might sneer at this point and well-actually us that a "real" title drop
		should only happen once in a film. That there's this one memorable (or cringe-y) scene where the
		protagonist looks directly at the camera and declares the title of the film with as much pathos
		as they can muster. Or as a nice send-off in the last spoken line.</p> <p><img loading="lazy" src="https://www.titledrops.net/img/little_women_moe_side_by_side.jpg"></p><div data-svelte-h="svelte-1uojp5d"><p>Such single drops happen surprisingly often:<br> <strong>11.3% of all movies do EXACTLY ONE title drop during their runtime.</strong></p><p>
		Which means that there's about twice as many movies having multiple title drops than single ones.</p></div> <p><img loading="lazy" src="https://www.titledrops.net/img/charts/count_percentage.svg"></p><p data-svelte-h="svelte-hkq436">In the single drop case it is more likely that the filmmakers were adding a title drop very
		consciously.</p> <div><h4>Highest rated single drop movies</h4> <p><img src="https://www.titledrops.net/img/deco/scribble-wavy.svg"> </p></div> <div><p data-svelte-h="svelte-1b6n1rq">Fiction only</p>   <p data-svelte-h="svelte-1teau28">Fiction + Documentaries</p></div>  <p data-svelte-h="svelte-1t9sgt8">Single drops often happen in a key scene and explain the movie's title: what mysterious
		fellowship the first Lord of the Rings is named after. Or that the audience waiting for some
		dark knight to show up must simply accept that it's been the Batman all along.</p> <h3>Title drops over the years</h3> <p><img src="https://www.titledrops.net/img/deco/scribble-scratchy.svg"></p><p data-svelte-h="svelte-uw6c0s">One suspicion I had was that the very meta act of having a character speak the name of the movie
		they're in would be something gaining more and more traction over the last two or three decades.</p> <p data-svelte-h="svelte-5yxcef">And indeed, if we look at the average number of movies with title drops over the decades we can
		see that there's a certain upwards trend. The 1960s and 1970s seemed to be most averse to
		mentioning their title in the film, while it's become more common-place over the last years.

		<img loading="lazy" src="https://www.titledrops.net/img/charts/deviation_by_decade.svg"></p> <div><h4>Highest title drops by decade</h4> <p><img src="https://www.titledrops.net/img/deco/scribble-wavy.svg"> </p></div>  <div><p data-svelte-h="svelte-zrarij">Most drops</p>   <p data-svelte-h="svelte-1lsvmm9">Best rated (at least 1 drop)</p></div>  <p data-svelte-h="svelte-1o5djl7">If we dig deeper, this growth over the decades comes with a clearer explanation: splitting up
		movies by single- and multi-title drops shows that while the tendency of movies to drop their
		title exactly once keeps more or less steady, the number of multi-drop films is on the rise.</p> <p><img loading="lazy" src="https://www.titledrops.net/img/charts/decs_single_vs_multi.svg"></p><p data-svelte-h="svelte-xxse9f">Your explanation for this (More movies are being named after their protagonists? Movies are more
		productified so brand recognition becomes an important concern?) is probably as good as mine ü§∑</p> <h3>A sign of quality?</h3> <p><img src="https://www.titledrops.net/img/deco/scribble-scratchy.svg"></p><p data-svelte-h="svelte-zyvhke">Another question I wanted to answer was if a high number of title drops was a sign of a bad
		movie. Think of all the trashy slasher and horror movies about Meth Marmots and Killer
		Ballerinas - wouldn't their characters in the sparse dialogues constantly mention the title for
		brand recognition and all that?</p> <p><img loading="lazy" src="https://www.titledrops.net/img/charts/rating_vs_drops.svg"></p><p data-svelte-h="svelte-btkys1">Interestingly though, there's no strong connection between film quality (expressed as IMDb
		rating (YMMV)) and the probability of title-dropping.</p> <h3>Genres and title drops</h3> <p><img src="https://www.titledrops.net/img/deco/scribble-scratchy.svg"></p><p data-svelte-h="svelte-5mtp2k">An aspect that certainly <em>does</em> have an impact on the probability of a title drop though is
		the genre of a film.</p> <p data-svelte-h="svelte-12p4e2m">If you think back to the discussion about names in titles from earlier, genres like Biography
		and other non-fiction genres like Sport and History - almost by definition - mention their
		subject in both the title and throughout the film.</p> <p data-svelte-h="svelte-z5f9mt">Accordingly, the probability of a title drop varies wildly by genre. Non-fiction films have a
		strong tendency towards title-dropping, while more fiction-oriented genres like Crime, Romance
		and War don't.</p> <p data-svelte-h="svelte-89tmax"><img loading="lazy" src="https://www.titledrops.net/img/charts/deviation_by_genre_styled_anno.svg"></p> <h3>What does a movie title mean?</h3> <p><img src="https://www.titledrops.net/img/deco/scribble-scratchy.svg"></p><p data-svelte-h="svelte-xmexhq">Finally, we can ask the question: what even <em>is</em> a movie title?
		</p> <p data-svelte-h="svelte-xsiy58">I couldn't find a complete classification in the scientific literature (<a href="https://www.tandfonline.com/doi/abs/10.1080/02666286.2015.1053037" target="_blank">"What's in a name? The art of movie titling"</a>
		by Ingrid Haidegger comes the closest). Movie titles are an interesting case, since they have to
		work as a description of a product, a marketing instrument, but also as the title of a piece of art.<br>
		Consequently, it's a field ripe with
		<a href="https://www.michigandaily.com/arts/film/good-movie-titles-or-how-i-learned-to-stop-worrying-and-love-the-overly-complex-naming-scheme/" target="_blank">opinions</a>,
		<a href="https://variety.com/2023/film/features/how-are-movie-titles-decided-hollywood-1235554998/" target="_blank">science and experimentation</a>
		and
		<a href="https://www.cleveland.com/movies/2017/10/50_coolest_movie_titles_ever.html" target="_blank">listicles</a>.</p> <p data-svelte-h="svelte-qw1d0v">The most extensive classification of media titles in general I could find is
		<a href="https://tvtropes.org/pmwiki/pmwiki.php/Main/TitleTropes" target="_blank">TVTropes' Title Tropes list</a>
		which lists over 180 (!) different types of tropes alone. Some of those tropes are:</p>  <p data-svelte-h="svelte-1dt3wh9">While naming a movie is a very creative task and pretty successfully defies classification, we
		can still look at the overall shape of movie titles and see if that has any impact on the number
		of title drops.</p> <p>One such simple aspect is the length of the title itself. As you would expect there's a negative
		correlation (if only a slight one<span data-svelte-h="svelte-1lmooii">*</span>)
		between the length of a title and the number of title drops it does.
		<img loading="lazy" src="https://www.titledrops.net/img/charts/title_length.svg"></p> <p data-svelte-h="svelte-10d3gzs">Still, there are some fun examples for <em>reaaaaally</em>
		long movie titles that nevertheless do at least one title drop:</p>  <p data-svelte-h="svelte-1l1h6zn">And while these previous examples only drops parts from before or after the colon, this next
		specimen actually does an impressive full title drop:</p>  <p><img loading="lazy" src="https://www.titledrops.net/img/deco/scribble1.svg"></p><p data-svelte-h="svelte-vvmi2w">And with that, we're done with the overarching analysis! Feel free to drop us an <a href="mailto:do@minik.us" target="_blank">e-mail</a>
		or follow up on <a href="https://twitter.com/dominikus" target="_blank">X</a>/<a href="https://twitter.com/al_ice_t" target="_blank">X</a>,
		<a href="https://bsky.app/profile/do.minik.us" target="_blank">Bluesky</a>
		or
		<a href="https://vis.social/@dominikus" target="_blank">Mastodon</a>

		if you have comments, questions, praise ‚ù§Ô∏è</p> <p data-svelte-h="svelte-4xjs5i">Oh, and one more thing:<br>
		If you're curious, here's the full dataset for you to explore!</p> <div><h4>Explore all movies!</h4> <p><img src="https://www.titledrops.net/img/deco/scribble-wavy.svg"> </p></div>  <p data-svelte-h="svelte-aoox60">Analysis + development by <a href="https://do.minik.us/" target="_blank">Dominikus Baur</a><br>
		Design by <a href="https://www.alicethudt.de/" target="_blank">Alice Thudt</a><br></p> <p data-svelte-h="svelte-sqhboi">Datasets provided by <a href="https://opensubtitles.com/" target="_blank">OpenSubtitles.com</a>
		and
		<a href="https://developer.imdb.com/non-commercial-datasets/" target="_blank">IMDb</a>.</p> <p data-svelte-h="svelte-p8xh42">Data: <a href="https://github.com/dominikus/titledrops.net" target="_blank">https://github.com/dominikus/titledrops.net</a></p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[More Oracle Layoffs Started Nov. 1, Cloud Unit Impacted (103 pts)]]></title>
            <link>https://www.channelfutures.com/channel-business/more-oracle-layoffs-started-nov-1-cloud-unit-impacted#</link>
            <guid>42056330</guid>
            <pubDate>Wed, 06 Nov 2024 00:04:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.channelfutures.com/channel-business/more-oracle-layoffs-started-nov-1-cloud-unit-impacted#">https://www.channelfutures.com/channel-business/more-oracle-layoffs-started-nov-1-cloud-unit-impacted#</a>, See on <a href="https://news.ycombinator.com/item?id=42056330">Hacker News</a></p>
Couldn't get https://www.channelfutures.com/channel-business/more-oracle-layoffs-started-nov-1-cloud-unit-impacted#: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Mozilla is eliminating its advocacy division (187 pts)]]></title>
            <link>https://www.theverge.com/2024/11/5/24289124/mozilla-foundation-layoffs-advocacy-global-programs</link>
            <guid>42055979</guid>
            <pubDate>Tue, 05 Nov 2024 23:04:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/11/5/24289124/mozilla-foundation-layoffs-advocacy-global-programs">https://www.theverge.com/2024/11/5/24289124/mozilla-foundation-layoffs-advocacy-global-programs</a>, See on <a href="https://news.ycombinator.com/item?id=42055979">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The Mozilla Foundation laid off 30 percent of its workforce and completely eliminated its advocacy and global programs divisions, <a href="https://techcrunch.com/2024/11/05/mozilla-foundation-lays-off-30-staff-drops-advocacy-division/"><em>TechCrunch </em>reports</a>.&nbsp;</p><p>While Mozilla is best known for its Firefox web browser, the Mozilla Foundation ‚Äî the parent of the Mozilla Corporation ‚Äî describes itself as standing up ‚Äúfor the health of the internet.‚Äù With its advocacy and global programs divisions gone, its impact may be lessened going forward.</p><p>‚ÄúFighting for a free and open internet will always be core to our mission, and advocacy continues to be a critical tool in that work. We‚Äôre revisiting how we pursue that work, not stopping it,‚Äù Brandon Borrman, the Mozilla Foundation‚Äôs communications chief, said in an email to <em>The Verge. </em>Borrman declined to confirm exactly how many people were laid off, but said it was about ‚Äú30% of the current team.‚Äù</p><p>This is Mozilla‚Äôs <a href="https://www.theverge.com/2024/2/13/24072184/mozilla-is-laying-off-around-60-workers-and-scaling-back-its-mastodon-instance">second round of layoffs this year</a>. In February, the Mozilla Corporation laid off around 60 workers said it would be making a ‚Äústrategic correction‚Äù that would involve involve cutting back its work on a Mastodon instance. Mozilla shut down its virtual 3D platform and refocused its efforts on Firefox and AI. The Mozilla Foundation had around 120 employees before this more recent round of layoffs, according to <em>TechCrunch</em>.</p><p>In an email sent to all employees on October 30th, Nabhia Syed, the foundation‚Äôs executive director, said that the advocacy and global programs divisions ‚Äúare no longer part of our structure.‚Äù</p><p>‚ÄúNavigating this topsy-turvy, distracting time requires laser focus ‚Äî and sometimes saying goodbye to the excellent work that has gotten us this far because it won‚Äôt get us to the next peak,‚Äù wrote Syed, who previously worked as the chief executive of <em>The Markup</em>, an investigative news site. ‚ÄúLofty goals demand hard choices.‚Äù&nbsp;</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unix Programmer's Manual Third Edition [pdf] (1973) (142 pts)]]></title>
            <link>https://dspinellis.github.io/unix-v3man/v3man.pdf</link>
            <guid>42055644</guid>
            <pubDate>Tue, 05 Nov 2024 22:12:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dspinellis.github.io/unix-v3man/v3man.pdf">https://dspinellis.github.io/unix-v3man/v3man.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=42055644">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[What Shapes Do Matrix Multiplications Like? (106 pts)]]></title>
            <link>https://www.thonking.ai/p/what-shapes-do-matrix-multiplications</link>
            <guid>42055616</guid>
            <pubDate>Tue, 05 Nov 2024 22:08:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thonking.ai/p/what-shapes-do-matrix-multiplications">https://www.thonking.ai/p/what-shapes-do-matrix-multiplications</a>, See on <a href="https://news.ycombinator.com/item?id=42055616">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>A while back, Karpathy tweeted that </span><em>increasing</em><span> the size of his matmul made it run faster. Surprisingly, it‚Äôs not just </span><em>relatively</em><span> faster, it takes less </span><em>absolute</em><span> time. In other words, despite doing more work, it is executing in less time.</span></p><p>This may seem intuitively quite strange. Is cuBLAS just messing up somehow? Why doesn‚Äôt the matrix multiplication kernel just pad it to a larger shape? </p><p>It has become tribal knowledge that the particular shapes chosen for matmuls has a surprisingly large effect on their performance. But ‚Ä¶ why? Can this be understood by mere mortals?</p><p>Let‚Äôs take a crack at it.</p><p>First, let‚Äôs plot FLOPs achieved for square matmuls. By the end of this article, I will aim to explain all the strange squiggly lines. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda2b6db5-95ca-47c9-adb5-6f5ca85c92f0_2418x1602.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda2b6db5-95ca-47c9-adb5-6f5ca85c92f0_2418x1602.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda2b6db5-95ca-47c9-adb5-6f5ca85c92f0_2418x1602.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda2b6db5-95ca-47c9-adb5-6f5ca85c92f0_2418x1602.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda2b6db5-95ca-47c9-adb5-6f5ca85c92f0_2418x1602.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda2b6db5-95ca-47c9-adb5-6f5ca85c92f0_2418x1602.jpeg" width="1456" height="965" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/da2b6db5-95ca-47c9-adb5-6f5ca85c92f0_2418x1602.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:965,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:&quot;Image&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Image" title="Image" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda2b6db5-95ca-47c9-adb5-6f5ca85c92f0_2418x1602.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda2b6db5-95ca-47c9-adb5-6f5ca85c92f0_2418x1602.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda2b6db5-95ca-47c9-adb5-6f5ca85c92f0_2418x1602.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda2b6db5-95ca-47c9-adb5-6f5ca85c92f0_2418x1602.jpeg 1456w" sizes="100vw"></picture></div></a></figure></div><p>There are 3 general concepts to understand that explain the majority of performance variation among matmul shapes.</p><ol><li><p>Compute Intensity/Parallelization: This explains the general upward trend</p></li><li><p>Tiling: This explains the multiple tiers of lines.</p></li><li><p>Wave Quantization: This explains the strange striped lines.</p></li></ol><p>First of all, as we move along the x-axis, the matrix multiplications generally get more performant. There‚Äôs two primary reasons for this. </p><p>The first one is simply ‚Äúmore work/more parallelism‚Äù. There are a large number of fixed overheads that come with launching a kernel (e.g. creating new SMs, waiting for all SMs to finish, etc.), and so, the more work we have to do, the less important those fixed overheads are. Along with more work comes more parallelism, and since GPUs have a ton of parallel cores, you need a surprising amount of work in order to fill a GPU up with enough parallelism.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5b2ae02-19be-4af7-8fc4-037f7476761b_2196x1668.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5b2ae02-19be-4af7-8fc4-037f7476761b_2196x1668.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5b2ae02-19be-4af7-8fc4-037f7476761b_2196x1668.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5b2ae02-19be-4af7-8fc4-037f7476761b_2196x1668.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5b2ae02-19be-4af7-8fc4-037f7476761b_2196x1668.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5b2ae02-19be-4af7-8fc4-037f7476761b_2196x1668.png" width="478" height="363.09615384615387" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c5b2ae02-19be-4af7-8fc4-037f7476761b_2196x1668.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1106,&quot;width&quot;:1456,&quot;resizeWidth&quot;:478,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5b2ae02-19be-4af7-8fc4-037f7476761b_2196x1668.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5b2ae02-19be-4af7-8fc4-037f7476761b_2196x1668.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5b2ae02-19be-4af7-8fc4-037f7476761b_2196x1668.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5b2ae02-19be-4af7-8fc4-037f7476761b_2196x1668.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Data movement is expensive!</figcaption></figure></div><p><span>The second one is ‚Äúarithmetic intensity‚Äù. As I‚Äôve </span><a href="https://horace.io/brrr_intro.html" rel="">written about before</a><span>, memory accesses are much more expensive than compute. So, since a square matmul performs 3N^2 memory accesses and 2N^3 FLOPs, at a very minimum, N needs to be in the hundreds before we start spending more time on compute than memory!</span></p><p><span>The desire for sufficient Arithmetic Intensity and Parallelism also compound. For example, let‚Äôs say you have your output matrix is </span><code>1024 x 1024</code><span>. If you let each SM compute a </span><code>128 x 128 </code><span>slice of the output, that‚Äôs only 64 pieces of ‚Äúwork‚Äù for your GPU, not even enough for each one of an A100‚Äôs 108 SMs ! If you decrease your output slice size to 64 x 64, we now have 256 pieces of ‚Äúwork‚Äù for our GPU, but our arithmetic intensity has also decreased by a factor of 4.</span></p><p>With smaller matrix sizes, you need to worry about problems like this that don‚Äôt show up with larger matrices. </p><p>Now that we understand the overall structure of the plot, the next question is: why is the plot all over the place? Why, even for very large matrices, do the TFLOPS jumping between &gt;250 and &lt;100?</p><p>To give a hint, let‚Äôs color-code each dot by the highest power of 2 it‚Äôs divisible by.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F245e1a67-e758-4561-9521-72191ef5992f_1513x1130.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F245e1a67-e758-4561-9521-72191ef5992f_1513x1130.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F245e1a67-e758-4561-9521-72191ef5992f_1513x1130.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F245e1a67-e758-4561-9521-72191ef5992f_1513x1130.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F245e1a67-e758-4561-9521-72191ef5992f_1513x1130.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F245e1a67-e758-4561-9521-72191ef5992f_1513x1130.jpeg" width="1456" height="1087" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/245e1a67-e758-4561-9521-72191ef5992f_1513x1130.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1087,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Image" title="Image" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F245e1a67-e758-4561-9521-72191ef5992f_1513x1130.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F245e1a67-e758-4561-9521-72191ef5992f_1513x1130.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F245e1a67-e758-4561-9521-72191ef5992f_1513x1130.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F245e1a67-e758-4561-9521-72191ef5992f_1513x1130.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>As it turns out, the multiple ‚Äúlevels‚Äù of FLOPS are due to their shapes‚Äô divisibility. For example, when the shape is odd, the matmul performs significantly worse than when the shape is even. The matmul performs even better when the shape is divisible by 8, with even more performance gains when it‚Äôs divisible by 16 or 32.</p><p><span>Now, merely knowing about this effect is very practically useful, but what actually causes this effect? As it turns out, the answer is tiling. But, what even </span><em>is</em><span> tiling? And why does it cause such substantial performance issues?</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5939a8fd-8356-45a6-8e83-db6fd48c014d_2116x660.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5939a8fd-8356-45a6-8e83-db6fd48c014d_2116x660.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5939a8fd-8356-45a6-8e83-db6fd48c014d_2116x660.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5939a8fd-8356-45a6-8e83-db6fd48c014d_2116x660.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5939a8fd-8356-45a6-8e83-db6fd48c014d_2116x660.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5939a8fd-8356-45a6-8e83-db6fd48c014d_2116x660.png" width="1456" height="454" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5939a8fd-8356-45a6-8e83-db6fd48c014d_2116x660.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:454,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:110115,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5939a8fd-8356-45a6-8e83-db6fd48c014d_2116x660.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5939a8fd-8356-45a6-8e83-db6fd48c014d_2116x660.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5939a8fd-8356-45a6-8e83-db6fd48c014d_2116x660.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5939a8fd-8356-45a6-8e83-db6fd48c014d_2116x660.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Taken from https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#tile-quant</figcaption></figure></div><p><span>Some online have mentioned </span><a href="https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#tile-quant" rel="">tile quantization</a><span> as the culprit. Tile quantization certainly can impact performance, but </span><em>only at tile boundary sizes</em><span>. Basically, tile quantization occurs when the size of your matrix multiplication increases such that the GPU needs to launch another ‚Äúchunk‚Äù of work. For example, imagine that you could multiply 8 elements at a time with a SIMD instruction. Now, if you went from 32 elements to 33 elements (a 3% increase in problem size), you go from needing 4 SIMD instructions to 5 (a 25% increase). Note that crucially, when tile quantization is the culprit, your absolute runtime still grows monotonically, although your efficiency may drop.</span></p><p><span>However, in our above plot, we see much more drastic performance drops! Moreover, like in Karpathy‚Äôs original example, we see that the </span><em>absolute runtime decreases despite problem size increasing</em><span>. So, tile quantization cannot be the explanation here.</span></p><p>The true cause is that tiling is just fundamentally worse for certain memory layouts. In other words, by the time we‚Äôre trying to execute the matmul, you‚Äôve already lost. The memory layout is poor and your performance will suffer.</p><p>Let‚Äôs look at some examples!</p><p>First, let‚Äôs think about how our matrix‚Äôs memory layout looks like when our size is a multiple of the cache line (pretend it‚Äôs 4 elements). </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F819f8836-c521-4363-b170-49ee039569c6_1170x836.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F819f8836-c521-4363-b170-49ee039569c6_1170x836.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F819f8836-c521-4363-b170-49ee039569c6_1170x836.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F819f8836-c521-4363-b170-49ee039569c6_1170x836.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F819f8836-c521-4363-b170-49ee039569c6_1170x836.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F819f8836-c521-4363-b170-49ee039569c6_1170x836.png" width="520" height="371.55555555555554" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/819f8836-c521-4363-b170-49ee039569c6_1170x836.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:836,&quot;width&quot;:1170,&quot;resizeWidth&quot;:520,&quot;bytes&quot;:624255,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F819f8836-c521-4363-b170-49ee039569c6_1170x836.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F819f8836-c521-4363-b170-49ee039569c6_1170x836.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F819f8836-c521-4363-b170-49ee039569c6_1170x836.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F819f8836-c521-4363-b170-49ee039569c6_1170x836.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>I choose to show 3 ‚Äúcache lines‚Äù per row because our matrix logically has 12 elements per row.</figcaption></figure></div><p><span>We see that each row starts on a cache line</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-142904770" href="https://www.thonking.ai/p/what-shapes-do-matrix-multiplications#footnote-1-142904770" target="_self" rel="">1</a></span><span>. Among other advantages, this means that we don‚Äôt need to perform any ‚Äúunnecessary‚Äù loads to load all yellow elements. We can just load the 3 cache lines that the yellow elements are part of.</span></p><p>However, what happens if we increase the number of elements per row from 12 to 13? </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57fffe15-4659-44e1-b516-0944867b9f96_1374x838.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57fffe15-4659-44e1-b516-0944867b9f96_1374x838.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57fffe15-4659-44e1-b516-0944867b9f96_1374x838.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57fffe15-4659-44e1-b516-0944867b9f96_1374x838.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57fffe15-4659-44e1-b516-0944867b9f96_1374x838.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57fffe15-4659-44e1-b516-0944867b9f96_1374x838.png" width="520" height="317.14701601164484" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/57fffe15-4659-44e1-b516-0944867b9f96_1374x838.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:838,&quot;width&quot;:1374,&quot;resizeWidth&quot;:520,&quot;bytes&quot;:759750,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57fffe15-4659-44e1-b516-0944867b9f96_1374x838.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57fffe15-4659-44e1-b516-0944867b9f96_1374x838.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57fffe15-4659-44e1-b516-0944867b9f96_1374x838.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57fffe15-4659-44e1-b516-0944867b9f96_1374x838.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Each logical row (which have 13 elements) no longer starts aligned with a cache line.</figcaption></figure></div><p><span>With an unaligned layout, each row is misaligned relative to our cache line. In other words, if we start loading the beginning of the green row, we </span><em>must</em><span> redundantly load the last element of the blue row as well.</span></p><p>Now, let‚Äôs look at what happens when we actually try to load an entire ‚Äútile‚Äù from these memory layouts.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf2a2033-e9bf-44b8-8961-5b50c25eda91_2032x870.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf2a2033-e9bf-44b8-8961-5b50c25eda91_2032x870.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf2a2033-e9bf-44b8-8961-5b50c25eda91_2032x870.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf2a2033-e9bf-44b8-8961-5b50c25eda91_2032x870.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf2a2033-e9bf-44b8-8961-5b50c25eda91_2032x870.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf2a2033-e9bf-44b8-8961-5b50c25eda91_2032x870.jpeg" width="1456" height="623" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cf2a2033-e9bf-44b8-8961-5b50c25eda91_2032x870.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:623,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Image" title="Image" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf2a2033-e9bf-44b8-8961-5b50c25eda91_2032x870.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf2a2033-e9bf-44b8-8961-5b50c25eda91_2032x870.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf2a2033-e9bf-44b8-8961-5b50c25eda91_2032x870.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf2a2033-e9bf-44b8-8961-5b50c25eda91_2032x870.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Shaded regions = elements we‚Äôre trying to load. Crossed out regions = elements we don‚Äôt need but must load due to it being in the same cache line.</figcaption></figure></div><p>With the aligned layout, this is very clean! We issue one load per row. One for the 4 blue elements, one for the 4 green elements, one for the 4 yellow elements, and one for the 4 pink elements. </p><p>With the unaligned layout, things are much messier. For example, in order to load the first 4 green elements, we must issue 2 loads! One that gets the last blue element + the first 3 green elements, and one that gets the 4th green element. A similar pattern occurs with loading the 4 yellow elements as well as the 4 pink elements.</p><p><span>So, when our matrix size is divisible by the cache line (which is 32 elements on a GPU), tiling fits nicely within the cache line, and our memory loads are maximally efficient. When it‚Äôs not‚Ä¶ the kernel needs many more workarounds in order to end up the proper alignment.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-142904770" href="https://www.thonking.ai/p/what-shapes-do-matrix-multiplications#footnote-2-142904770" target="_self" rel="">2</a></span></p><p>This is why even very small changes in our matrix size can lead to substantially worsened performance.</p><p>Ok, so we‚Äôve understood most of the variation in matmul performance. But what about these strange stripes up here? All of these points are with matmuls that are divisible by 32 already. Seeing that the peaks are separated by 256, our first guess might be that this is also memory-layout related, just at a larger scale.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F040453ca-4ac6-4275-bc84-7765a0bc70c4_844x466.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F040453ca-4ac6-4275-bc84-7765a0bc70c4_844x466.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F040453ca-4ac6-4275-bc84-7765a0bc70c4_844x466.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F040453ca-4ac6-4275-bc84-7765a0bc70c4_844x466.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F040453ca-4ac6-4275-bc84-7765a0bc70c4_844x466.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F040453ca-4ac6-4275-bc84-7765a0bc70c4_844x466.png" width="844" height="466" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/040453ca-4ac6-4275-bc84-7765a0bc70c4_844x466.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:466,&quot;width&quot;:844,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Image" title="Image" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F040453ca-4ac6-4275-bc84-7765a0bc70c4_844x466.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F040453ca-4ac6-4275-bc84-7765a0bc70c4_844x466.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F040453ca-4ac6-4275-bc84-7765a0bc70c4_844x466.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F040453ca-4ac6-4275-bc84-7765a0bc70c4_844x466.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Some truly mysterious patterns‚Ä¶.</figcaption></figure></div><p><span>However, as it turns out, these peaks (2944 and 3120) do </span><em>not </em><span>occur when the matrix shapes are divisible by 256, but instead they‚Äôre at 128 mod 256! </span></p><p><span>As it turns out, these peaks are not caused by poor memory-layouts, they‚Äôre instead caused by a (neatly-named) phenomenon called </span><em>wave quantization</em><span>.</span></p><p>The main idea behind wave quantization is quite simple. </p><p><span>Let‚Äôs say we have N parallel tasks (which each take a second) and N CPUs. </span><br><span>Q: How long does it perform to take all tasks?</span><br><span>A: 1 second</span></p><p><span>Q: What about if we have (N+1) parallel tasks, and N CPUs?</span><br><span>A: 2 seconds(!) Now, one CPU must perform two tasks, taking a total of 2 seconds.</span></p><p>So, despite adding just one task, we‚Äôve doubled our overall latency.</p><p>This is exactly what wave quantization is, except with CPUs =&gt; SMs and tasks =&gt; thread blocks.</p><p>As your matrix size increases, the total number of tiles/blocks increases. When this crosses a multiple of the # of SMs, your perf drops since you need to execute an additional "wave".</p><p>Now, let's apply our newfound knowledge to actually explain these curves! Let‚Äôs try looking at this sudden drop in performance around 1792 first.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98d21d15-54c6-44ad-a522-1fa775badc59_434x764.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98d21d15-54c6-44ad-a522-1fa775badc59_434x764.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98d21d15-54c6-44ad-a522-1fa775badc59_434x764.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98d21d15-54c6-44ad-a522-1fa775badc59_434x764.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98d21d15-54c6-44ad-a522-1fa775badc59_434x764.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98d21d15-54c6-44ad-a522-1fa775badc59_434x764.png" width="244" height="429.5299539170507" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/98d21d15-54c6-44ad-a522-1fa775badc59_434x764.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:764,&quot;width&quot;:434,&quot;resizeWidth&quot;:244,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Image" title="Image" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98d21d15-54c6-44ad-a522-1fa775badc59_434x764.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98d21d15-54c6-44ad-a522-1fa775badc59_434x764.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98d21d15-54c6-44ad-a522-1fa775badc59_434x764.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98d21d15-54c6-44ad-a522-1fa775badc59_434x764.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Since wave quantization depends a lot on the actual kernel parameters, we must look at what kernels are actually being run.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81f276b8-3a39-418f-b586-8c016abb4285_2532x300.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81f276b8-3a39-418f-b586-8c016abb4285_2532x300.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81f276b8-3a39-418f-b586-8c016abb4285_2532x300.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81f276b8-3a39-418f-b586-8c016abb4285_2532x300.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81f276b8-3a39-418f-b586-8c016abb4285_2532x300.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81f276b8-3a39-418f-b586-8c016abb4285_2532x300.png" width="1456" height="173" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/81f276b8-3a39-418f-b586-8c016abb4285_2532x300.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:173,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Image" title="Image" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81f276b8-3a39-418f-b586-8c016abb4285_2532x300.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81f276b8-3a39-418f-b586-8c016abb4285_2532x300.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81f276b8-3a39-418f-b586-8c016abb4285_2532x300.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81f276b8-3a39-418f-b586-8c016abb4285_2532x300.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Using the profiler, we see that we're running a CUTLASS-based matmul with a tile size of 256x128. Note that our matmul kernel *doesn't change at all*, but our perf drops from 60+ TF/s at N=1791 to 43 TF/s at N=1793. </p><p>Now, some basic arithmetic. Our tile grid has dimensions 1792/256 = 7 and 1792/128 = 14. That gives us 7 * 14 = 98 tiles. Since an A100 has 108 SMs, that's still one wave. However, with N=1793 we need to increase the size of our grid. (7+1)*(14+1) = 120 tiles, or 2 waves!</p><p>Now, let‚Äôs look at the previous (mysterious) stripes. Specifically, we‚Äôll look at N=3200.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c110a2b-9845-4305-b9f0-40c2d61b7831_844x466.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c110a2b-9845-4305-b9f0-40c2d61b7831_844x466.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c110a2b-9845-4305-b9f0-40c2d61b7831_844x466.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c110a2b-9845-4305-b9f0-40c2d61b7831_844x466.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c110a2b-9845-4305-b9f0-40c2d61b7831_844x466.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c110a2b-9845-4305-b9f0-40c2d61b7831_844x466.png" width="476" height="262.81516587677726" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0c110a2b-9845-4305-b9f0-40c2d61b7831_844x466.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:466,&quot;width&quot;:844,&quot;resizeWidth&quot;:476,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Image" title="Image" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c110a2b-9845-4305-b9f0-40c2d61b7831_844x466.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c110a2b-9845-4305-b9f0-40c2d61b7831_844x466.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c110a2b-9845-4305-b9f0-40c2d61b7831_844x466.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c110a2b-9845-4305-b9f0-40c2d61b7831_844x466.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Mysterious no more!</figcaption></figure></div><p><span>Profiling it, we see that the </span><em>proximal</em><span> cause is not actually wave quantization. Instead, CuBLAS decided to  change algorithms. But, why did CuBLAS decide to change algorithms?</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccbbbd2-2f3f-46a1-b542-e575decf2f6d_1792x300.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccbbbd2-2f3f-46a1-b542-e575decf2f6d_1792x300.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccbbbd2-2f3f-46a1-b542-e575decf2f6d_1792x300.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccbbbd2-2f3f-46a1-b542-e575decf2f6d_1792x300.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccbbbd2-2f3f-46a1-b542-e575decf2f6d_1792x300.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccbbbd2-2f3f-46a1-b542-e575decf2f6d_1792x300.png" width="1456" height="244" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4ccbbbd2-2f3f-46a1-b542-e575decf2f6d_1792x300.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:244,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:&quot;Image&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Image" title="Image" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccbbbd2-2f3f-46a1-b542-e575decf2f6d_1792x300.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccbbbd2-2f3f-46a1-b542-e575decf2f6d_1792x300.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccbbbd2-2f3f-46a1-b542-e575decf2f6d_1792x300.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccbbbd2-2f3f-46a1-b542-e575decf2f6d_1792x300.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Well, (3200/128) * (3200/128) = 625. 625/108 = 5.8 waves. Thus, at N=3232 we would create another wave.</p><p>In this case, though, it seems that 160x128 still isn't a great tile size. Since the resulting grid (26x21) results in 5.05 waves...</p><p>Well, CuBLAS isn't perfect!</p><p>Beyond the obvious matrix multiplication shape issues, performance loss due to wave quantization often ends up being tricky to find, since it depends upon things like the batch size as well. However, if you take a closer look at each matmul, you might find that there‚Äôs another 10-15% performance you can squeeze out of it by choosing the shapes more carefully!</p><p><span>I will note that it‚Äôs possible that wave quantization effects may soon be a thing of the past. New matrix multiplication technology like </span><a href="https://arxiv.org/abs/2301.03598" rel="">stream-k</a><span> allow us to completely bypass wave quantization effects. Perhaps I‚Äôll explain the basic idea behind matmul implementation strategies someday.</span></p><p><span>As it turns out, torch.compile does try and pad your matmuls to have the right shape! See the code </span><a href="https://github.com/pytorch/pytorch/blob/6b1f13ea2f3b1bcd575620eecd7d84a4d2e3eb76/torch/_inductor/fx_passes/pad_mm.py#L90" rel="">here</a><span>, or try this benchmark.</span></p><pre><code>import torch
torch.set_default_device('cuda')
from triton.testing import do_bench

def f(a, b):
    return torch.mm(a, b)

a = torch.randn(4096, 4096, dtype=torch.bfloat16)
b = torch.randn(4096, 4095, dtype=torch.bfloat16)
print("eager: ", do_bench(lambda: f(a, b)))
cf = torch.compile(f)
print("compiled: ", do_bench(lambda: cf(a, b)))
&gt;&gt; eager: 1.4077268838882446
&gt;&gt; compiled: 0.6021425127983093</code></pre><p>However, there are still limitations that mean it makes sense for users to manually pad their shapes. </p><p>For one, padding requires a full copy! Although torch.compile can often fuse this into a preceding op, in the case where the matrix being padded comes from the input (like a weight matrix), there‚Äôs no way to avoid this copy.</p><p>Second, resolving wave quantization is far more difficult.</p><p><span>Overall, I hope the topic of "how do I squeeze the most out of my matmuls" is an interesting one. There's still many more intricacies in matmul perf that I didn‚Äôt have the time to get to, as well (I‚Äôm sure) many more intricacies that I don‚Äôt know! Here‚Äôs the </span><a href="https://gist.github.com/Chillee/f86675147366a7a0c6e244eaa78660f7#file-4-matmul-bench-py" rel="">main code</a><span> to replicate the results.</span></p><p>Also, here‚Äôs some quiz questions to test your understanding! I will publish a brief explanation of the answers at some later point.</p><p><strong>1:</strong><span> Let's say I have a </span><code>[M x K] @ [K x N]</code><span> matmul. Which one of these configurations will have the best perf? Think about the actual ramifications of tiling! Both matrices are in row-major layout (i.e. K and N are the innermost dimensions)</span><br><span>A: M=2047, K=N=2048 </span><br><span>B: K=2047, M=N=2048 </span><br><span>C: N=2047, M=K=2048</span></p><p><strong>2: </strong><span>Let‚Äôs say I have an A100 with 108 SMs, and I want to benchmark a number of matmuls with no wave quantization. How would I go about constructing the shapes for these matmuls?</span></p><p><strong>3: </strong><span>Based off this post, would you expect that making your batch size a power of 2 leads to more efficient performance?</span></p><div><p><strong>4: </strong><span>Similar to Question 1, let‚Äôs say we have a A: [M x K] @ B: [K x N] matmul.  However, now, A is in column-major (i.e. </span><code>torch.randn(K, M).t()</code><span>) while B is still row-major. What is the best configuration now?</span></p><p><span>A: M=2047, K=N=2048 </span><br><span>B: K=2047, M=N=2048 </span><br><span>C: N=2047, M=K=2048</span></p></div><p><strong>5: </strong><span>Let‚Äôs say that we have this code.</span></p><pre><code>A = torch.randn(4096, 4096)
B = torch.randn(4096, 4096)
B = B[:, :4095] # B now has shape [4096, 4095]</code></pre><p>Would you expect that we have good performance on a matmul between A and B?</p><p><span>Solutions can be found below</span><br></p><div data-component-name="DigestPostEmbed"><a href="https://www.thonking.ai/p/answer-key-what-shapes-do-matrix" target="_blank" rel="noopener"></a><div><a href="https://www.thonking.ai/p/answer-key-what-shapes-do-matrix" target="_blank" rel="noopener"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,h_212,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bd02621-bbe2-44c9-8fa3-4809a5087400_1828x600.png 424w, https://substackcdn.com/image/fetch/w_848,h_424,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bd02621-bbe2-44c9-8fa3-4809a5087400_1828x600.png 848w, https://substackcdn.com/image/fetch/w_1272,h_636,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bd02621-bbe2-44c9-8fa3-4809a5087400_1828x600.png 1272w, https://substackcdn.com/image/fetch/w_1300,h_650,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bd02621-bbe2-44c9-8fa3-4809a5087400_1828x600.png 1300w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1300,h_650,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bd02621-bbe2-44c9-8fa3-4809a5087400_1828x600.png" sizes="100vw" alt="Answer Key: What Shapes Do Matrix Multiplications Like?" srcset="https://substackcdn.com/image/fetch/w_424,h_212,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bd02621-bbe2-44c9-8fa3-4809a5087400_1828x600.png 424w, https://substackcdn.com/image/fetch/w_848,h_424,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bd02621-bbe2-44c9-8fa3-4809a5087400_1828x600.png 848w, https://substackcdn.com/image/fetch/w_1272,h_636,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bd02621-bbe2-44c9-8fa3-4809a5087400_1828x600.png 1272w, https://substackcdn.com/image/fetch/w_1300,h_650,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bd02621-bbe2-44c9-8fa3-4809a5087400_1828x600.png 1300w" width="1300" height="650"></picture></a></div></div></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tracking down a mysterious skateboarder from 1979 (174 pts)]]></title>
            <link>https://www.ncrabbithole.com/p/tony-hawk-fayetteville-nc-girl-skateboarder-1979</link>
            <guid>42055558</guid>
            <pubDate>Tue, 05 Nov 2024 22:00:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ncrabbithole.com/p/tony-hawk-fayetteville-nc-girl-skateboarder-1979">https://www.ncrabbithole.com/p/tony-hawk-fayetteville-nc-girl-skateboarder-1979</a>, See on <a href="https://news.ycombinator.com/item?id=42055558">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>Here‚Äôs a really cool picture:</p><div data-attrs="{&quot;instagram_id&quot;:&quot;DB42tOlymvY&quot;,&quot;title&quot;:&quot;A post shared by @tonyhawk&quot;,&quot;author_name&quot;:&quot;tonyhawk&quot;,&quot;thumbnail_url&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/__ss-rehost__IG-meta-DB42tOlymvY.jpg&quot;,&quot;timestamp&quot;:null,&quot;belowTheFold&quot;:false}" data-component-name="InstagramToDOM"><p><a href="https://instagram.com/p/DB42tOlymvY" target="_blank" rel=""><img src="https://substackcdn.com/image/fetch/w_640,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F__ss-rehost__IG-meta-DB42tOlymvY.jpg"></a></p></div><p><span>A former colleague from my TV days, Michelle Chernicoff, sent me this picture of a young girl skateboarding underneath an umbrella on a rainy day. She saw it after Tony Hawk </span><a href="https://www.instagram.com/p/DB42tOlymvY/" rel="">posted it to his Instagram page</a><span> on Saturday. According to his caption, Tony </span><em>really</em><span> wanted to know who it was:</span></p><blockquote><p>New fav mystery skater unlocked: from Fayetteville Observer (NC), 1973. Style, grace, confidence, and‚Ä¶ goofy footed, in the rain!! I hope she‚Äôs still around.</p></blockquote><p><span>Folks, we love two things around here: </span><a href="https://www.ncrabbithole.com/p/tony-hawk-went-to-some-coffee-shops" rel="">Tony Hawk</a><span> and </span><a href="https://www.ncrabbithole.com/p/steven-spielberg-indiana-jones-nc-highway-patrol" rel="">decades-old mysteries</a><span>. </span></p><p><span>There were two initial clues: The date and the newspaper. I plugged the picture into Google Image Search to see if I could find the original post. Back on September 21, blackarchives.co had </span><a href="https://www.instagram.com/blackarchives.co/p/DALpuCIur9i/?img_index=1" rel="">posted a collection of images</a><span> taken by </span><em>Fayetteville Observer</em><span> photographers between 1973 and 1987. The image of the skateboarder in the rain was the first one in the carousel. Tony Hawk himself commented the same day: ‚ÄúNew fav skater unlocked.‚Äù</span></p><p><span>Next, I did some searches on the </span><em>Observer</em><span>‚Äôs website. The image showed up in </span><a href="https://www.fayobserver.com/picture-gallery/news/2019/06/21/from-the-archives-fayetteville-observer/69320732007/" rel="">a monthly roundup of old ‚Äò70s photos</a><span> that staff photographer </span><a href="https://www.instagram.com/aacraft/" rel="">Andrew Craft</a><span> had pulled from the paper‚Äôs negative archive in 2019 (‚ÄúAnytime I come across a skateboarding image I always scan it,‚Äù he said later in an Instagram comment). This photo also included a date: January 20, 1979 (not 1973, as Hawk‚Äôs post said). But that was it. No location. No name. No back story.</span></p><p>So I turned to the people you turn to when times get tough: Librarians.</p><p><a href="https://x.com/TarHeelFoodways" rel="">John O‚ÄôConnor</a><span> is the manager of the Robinson-Spangler Carolina Room at the Charlotte Mecklenburg Library. He‚Äôs also a </span><em>Rabbit Hole</em><span> reader, and he‚Äôs helped me try to find archival material in the past. I sent him a message on Sunday morning to see if his library had </span><em>Fayetteville Observer</em><span> on microfilm. No, he said, but he‚Äôd call the Cumberland County Public Library as soon as they opened in the afternoon. There, library associate Matt Morgan got the film out, found the January 21, 1979 edition of the </span><em>Fayetteville Observer</em><span>, and sent over what he found:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png" width="1048" height="1254" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1254,&quot;width&quot;:1048,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1852844,&quot;alt&quot;:&quot;scan of 1979 newspaper&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="scan of 1979 newspaper" title="scan of 1979 newspaper" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d8188e-8522-4222-959a-46ed2463aeea_1048x1254.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>There, on page 1B, was the image and the caption that had the rest of the details:</p><blockquote><p>Shaundra Shane didn‚Äôt want to get wet, but the idea of spending a Saturday without her skateboard was to much to handle. So she got her umbrella and wheeled down Dinsmore Drive.</p></blockquote><p><span>From there, I tracked down the 10-year-old girl in the picture. She‚Äôs 56 now, still lives in Fayetteville, and her name is Shaunda, not Shaundra (the caption misspelled it). I reached out Sunday night, and we talked after she got off of work on Monday afternoon. She remembers the day the picture was taken. An </span><em>Observer</em><span> photographer, the late Steve Aldridge, lived across the street from her on Dinsmore Drive. ‚ÄúMr. Steve saw me out there riding my skateboard in the rain,‚Äù she says. He asked her if she could get permission from her grandparents to take a picture. He also asked if she could come back with a prop. ‚ÄúHe wanted me to have an umbrella, so I got an umbrella,‚Äù she says. ‚ÄúHe took that picture and that was it.‚Äù</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b72356c-16c8-453b-948f-77042873849b_2076x1312.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b72356c-16c8-453b-948f-77042873849b_2076x1312.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b72356c-16c8-453b-948f-77042873849b_2076x1312.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b72356c-16c8-453b-948f-77042873849b_2076x1312.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b72356c-16c8-453b-948f-77042873849b_2076x1312.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b72356c-16c8-453b-948f-77042873849b_2076x1312.png" width="1456" height="920" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9b72356c-16c8-453b-948f-77042873849b_2076x1312.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:920,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4286930,&quot;alt&quot;:&quot;Google map street view of Dinsmore Drive in 2023&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Google map street view of Dinsmore Drive in 2023" title="Google map street view of Dinsmore Drive in 2023" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b72356c-16c8-453b-948f-77042873849b_2076x1312.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b72356c-16c8-453b-948f-77042873849b_2076x1312.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b72356c-16c8-453b-948f-77042873849b_2076x1312.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b72356c-16c8-453b-948f-77042873849b_2076x1312.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>A 2023 view from the area where the original picture was taken on Dinsmore Drive in Fayetteville (via Google Street View)</figcaption></figure></div><p>Shaunda lived with her grandparents in a neighborhood that she loved. Family and friends were all close by. She‚Äôd been a skater for about a year, ever since her uncle brought her a little pink board with metal wheels. Her friends were more into roller skating back then, but going to the rink was kind of boring, she thought. It was limiting. Skateboarding allowed her to explore. ‚ÄúI was free,‚Äù Shaunda says. ‚ÄúYou‚Äôre outside. There‚Äôs the wind. There‚Äôs just, like, a freeness, you know?‚Äù</p><p>Shaunda was the only member of her group that skateboarded, and she kept doing it for years. She got faster. Started learning tricks. But she stopped skateboarding after she got her first boyfriend in middle school. As a teenager, she started spending more time with him and less time on the board. Plus, she didn‚Äôt need to skateboard to get around‚Äîhe had a car.</p><p>After that, skateboarding and the image of her doing it faded from her life. ‚ÄúIt was just a picture in the paper,‚Äù she says. At some point, a car backed over her little pink board.</p><p data-attrs="{&quot;url&quot;:&quot;https://www.ncrabbithole.com/p/tony-hawk-fayetteville-nc-girl-skateboarder-1979?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.ncrabbithole.com/p/tony-hawk-fayetteville-nc-girl-skateboarder-1979?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p>Shaunda went on living. She moved to a new place in Fayetteville. She got married and divorced. She had three kids. She started working as a patient access specialist at Cape Fear Valley Health.</p><p>Then, back in September, she got a call from a woman who still lived in her old neighborhood. She told Shaunda that she had the old picture of her skateboarding in the rain and gave it to her. A week later, the same picture started to show up on Facebook. Shaunda called up her friend, who swore that she didn‚Äôt put the image online. ‚ÄúI said, well, did your mom do it? And she was like, girl, my mom don‚Äôt know how to post nothing on Facebook,‚Äù she says, laughing.</p><p>It was just a coincidence. Someone online had rediscovered the picture at the same time as Shaunda‚Äôs friend. And for a while, that was it. Shaunda made it her profile picture. A few friends tagged her when they saw the image online. They made comments. They liked her ponytails. They dropped fire emojis.</p><p>Then Tony Hawk posted the picture. And then I called.</p><p><span>‚ÄúIt's amazing,‚Äù Shaunda says. ‚ÄúI kind of have butterflies, you know? I definitely do know who Tony Hawk is. I have watched Tony Hawk.‚Äù The </span><em>Fayetteville Observer</em><span> got in touch, and they‚Äôre having her re-create the photo, 45 years later (UPDATE: </span><a href="https://www.fayobserver.com/story/lifestyle/2024/11/04/who-is-the-mystery-skater-shared-by-tony-hawk-shares-fayetteville-observer-photo/76047957007/?taid=672975236877af0001c06d8e" rel="">Here‚Äôs the picture</a><span>). A local skate shop has been in touch. Her phone is blowing up.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic" width="400" height="533.2417582417582" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1941,&quot;width&quot;:1456,&quot;resizeWidth&quot;:400,&quot;bytes&quot;:1193953,&quot;alt&quot;:&quot;Shaunda Shane, modern day pic&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Shaunda Shane, modern day pic" title="Shaunda Shane, modern day pic" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd319cf1d-eb81-453a-af17-f81cd522a979_2316x3088.heic 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Shaunda Shane, today.</figcaption></figure></div><p><span>Also, she had to learn what it meant to ride ‚Äúgoofy footed.‚Äù It‚Äôs when you ride a board with your right foot forward instead of your left. ‚ÄúOh,‚Äù Shaunda says. ‚ÄúWell, I </span><em>am</em><span> left-handed.‚Äù</span></p><p>Shaunda said she hasn‚Äôt quite been able to re-create the freedom she felt on a board all those years ago, although riding a motorcycle comes pretty close. In any event, she doesn‚Äôt really have the time to skateboard now anyway. ‚ÄúI mean, literally all I do is work,‚Äù she jokes. ‚ÄúTrying to pay these bills.‚Äù</p><p>Her son skateboards, though. And so, months ago, she got back up on a board for the first time in years. Just for a little bit. She‚Äôs a little afraid of falling, but the picture really brings back memories of a free-ranging childhood in a nice little neighborhood. ‚ÄúI probably would be skateboarding now,‚Äù she says, ‚Äúif I really thought I could.‚Äù</p><p><strong>MANY UPDATES (11/5/24, 9:06 a.m.):</strong><span> First up, Tony Hawk has seen this article and </span><a href="https://x.com/deftlyinane/status/1853651377840374245" rel="">name-dropped Shaunda in an Instagram story</a><span>, and Shaunda is aware that Tony Hawk has name-dropped her, but she hasn‚Äôt read this story yet because she was on her way to work. We‚Äôre all leading busy lives, people!</span></p><div data-attrs="{&quot;instagram_id&quot;:&quot;DB_l10pPUn5&quot;,&quot;title&quot;:&quot;A post shared by @tonyhawk&quot;,&quot;author_name&quot;:&quot;tonyhawk&quot;,&quot;thumbnail_url&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/__ss-rehost__IG-meta-DB_l10pPUn5.jpg&quot;,&quot;timestamp&quot;:null,&quot;belowTheFold&quot;:true}" data-component-name="InstagramToDOM"><p><a href="https://instagram.com/p/DB_l10pPUn5" target="_blank" rel=""><img src="https://substackcdn.com/image/fetch/w_640,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F__ss-rehost__IG-meta-DB_l10pPUn5.jpg" loading="lazy"></a></p></div><p><span>Also, the </span><em>Fayetteville Observer</em><span> posted its version of this story last night, which included the new picture of Shaunda riding a skateboard with an umbrella (</span><a href="https://www.fayobserver.com/story/lifestyle/2024/11/04/who-is-the-mystery-skater-shared-by-tony-hawk-shares-fayetteville-observer-photo/76047957007/?taid=672975236877af0001c06d8e" rel="">you can go to their website to see it</a><span>). The phenomenally talented </span><a href="https://www.instagram.com/aacraft" rel="">Andrew Craft</a><span> took that photo. He‚Äôs also the one who scanned the original picture of Shaunda from 1979 and posted it online, and really, there‚Äôs no story here without him. He also </span><a href="https://www.fayobserver.com/picture-gallery/news/local/2024/11/04/from-the-archives-fayetteville-nc-skateboarders-in-the-70s-and-80s/76054303007/?taid=67296e172eec1a0001fac113" rel="">posted even </a><em><a href="https://www.fayobserver.com/picture-gallery/news/local/2024/11/04/from-the-archives-fayetteville-nc-skateboarders-in-the-70s-and-80s/76054303007/?taid=67296e172eec1a0001fac113" rel="">more </a></em><a href="https://www.fayobserver.com/picture-gallery/news/local/2024/11/04/from-the-archives-fayetteville-nc-skateboarders-in-the-70s-and-80s/76054303007/?taid=67296e172eec1a0001fac113" rel="">historical photos of skateboarders</a><span> around Fayetteville, including another archival shot of Shaunda skateboarding with two friends. Check it out.</span></p><p><span>Lastly, a correction: I originally said that riding goofy foot meant riding with your left foot forward. It‚Äôs actually the opposite, which I would have known had I been paying attention TO THE PICTURE THAT I BASED THIS ENTIRE STORY ON. Anyhow, I regret the error. My penance shall be listening to the </span><a href="https://tonyhawkgames.fandom.com/wiki/Tony_Hawk%27s_Pro_Skater_2_Soundtrack" rel="">Tony Hawk‚Äôs Pro Skater 2 soundtrack</a><span> for the rest of the day (which I was gonna do anyway).</span></p><p data-attrs="{&quot;url&quot;:&quot;https://www.ncrabbithole.com/p/tony-hawk-fayetteville-nc-girl-skateboarder-1979/comments&quot;,&quot;text&quot;:&quot;Leave a comment&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.ncrabbithole.com/p/tony-hawk-fayetteville-nc-girl-skateboarder-1979/comments" rel=""><span>Leave a comment</span></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mozilla Foundation lays off 30% staff, drops advocacy division (174 pts)]]></title>
            <link>https://techcrunch.com/2024/11/05/mozilla-foundation-lays-off-30-staff-drops-advocacy-division/</link>
            <guid>42054867</guid>
            <pubDate>Tue, 05 Nov 2024 20:29:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/11/05/mozilla-foundation-lays-off-30-staff-drops-advocacy-division/">https://techcrunch.com/2024/11/05/mozilla-foundation-lays-off-30-staff-drops-advocacy-division/</a>, See on <a href="https://news.ycombinator.com/item?id=42054867">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">The Mozilla Foundation, the nonprofit arm of the Firefox browser maker Mozilla, has laid off 30% of its employees as the organization says it faces a ‚Äúrelentless onslaught of change.‚Äù</p>

<p>When reached by TechCrunch, Mozilla Foundation‚Äôs communications chief Brandon Borrman confirmed the layoffs in an email.</p>







<p>‚ÄúThe Mozilla Foundation is reorganizing teams to increase agility and impact as we accelerate our work to ensure a more open and equitable technical future for us all. That unfortunately means ending some of the work we have historically pursued and eliminating associated roles to bring more focus going forward,‚Äù read the statement shared with TechCrunch.</p>

<p>According to its annual tax filings, the Mozilla Foundation <a href="https://projects.propublica.org/nonprofits/organizations/200097189" target="_blank" rel="noreferrer noopener nofollow">reported having 60 employees</a> during the 2022 tax year. The number of employees at the time of the layoffs was closer to 120 people, according to a person with knowledge. When asked by TechCrunch, Mozilla‚Äôs spokesperson did not dispute the figure.</p>

<p>This is the second layoff at Mozilla this year, the first <a href="https://techcrunch.com/2024/02/13/mozilla-downsizes-as-it-refocuses-on-firefox-and-ai-read-the-memo/">affecting dozens of employees</a> who work on the side of the organization that builds the popular Firefox browser.</p>

<p>Mozilla is <a href="https://www.mozilla.org/en-US/about/governance/organizations/" target="_blank" rel="noreferrer noopener nofollow">made up of several organizations</a>, one of which is the Mozilla Corporation, which develops Firefox and other technologies, and another is its nonprofit and tax-exempt Foundation, which oversees Mozilla‚Äôs corporate governance structure and sets the browser maker‚Äôs policies.</p>

<p>Much of Mozilla‚Äôs work focused on advocating for privacy, inclusion, and decentralization of technologies, and ‚Äúto create safer, more transparent online experiences for everyone,‚Äù which ultimately benefit the browser maker and its users.</p>

<p>Announcing the layoffs in an email to all employees on October 30, the Mozilla Foundation‚Äôs executive director Nabiha Syed confirmed that two of the foundation‚Äôs major divisions ‚Äî <a href="https://blog.mozilla.org/en/mozilla/mozilla-welcomes-ashley-boyd-vp-of-advocacy/" target="_blank" rel="noreferrer noopener nofollow">advocacy</a> and <a href="https://foundation.mozilla.org/en/blog/welcoming-j-bob-alotta-mozilla-foundations-new-vp-global-programs/" target="_blank" rel="noreferrer noopener nofollow">global programs</a> ‚Äî are ‚Äúno longer a part of our structure.‚Äù </p>

<p>After publication, Borrman told TechCrunch that ‚Äúadvocacy is still a central tenet of Mozilla Foundation‚Äôs work and will be embedded in all the other functional areas,‚Äù without providing specifics.</p>

<p>The move, according to Syed, is in part to produce a ‚Äúunified, powerful narrative from the Foundation,‚Äù including revamping the foundation‚Äôs strategic communications.</p>







<p>‚ÄúOur mission at Mozilla is more high-stakes than ever,‚Äù wrote Syed in an email to staff, a copy of which was shared with TechCrunch. ‚ÄúWe find ourselves in a relentless onslaught of change in the technology (and broader) world, and the idea of putting people before profit feels increasingly radical.‚Äù&nbsp;</p>

<p>‚ÄúNavigating this topsy-turvy, distracting time requires laser focus ‚Äî and sometimes saying goodbye to the excellent work that has gotten us this far because it won‚Äôt get us to the next peak. Lofty goals demand hard choices,‚Äù wrote Syed.</p>

<p>Syed, who joined <a href="https://foundation.mozilla.org/en/blog/mozilla-foundation-welcomes-nabiha-syed-as-executive-director/" target="_blank" rel="noreferrer noopener nofollow">the Mozilla Foundation in February</a>, previously served as chief executive at data journalism and investigative news site The Markup.</p>

<p><em>Updated with comment from Mozilla.</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Traceroute Isn't Real (130 pts)]]></title>
            <link>https://gekk.info/articles/traceroute.htm</link>
            <guid>42054835</guid>
            <pubDate>Tue, 05 Nov 2024 20:22:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gekk.info/articles/traceroute.htm">https://gekk.info/articles/traceroute.htm</a>, See on <a href="https://news.ycombinator.com/item?id=42054835">Hacker News</a></p>
<div id="readability-page-1" class="page">
<p><strong><a href="https://gekk.info/">gekk.info</a> ¬´ <a href="https://gekk.info/articles/index.html">articles</a></strong></p>




<h2>Traceroute isn't real, or: Whoops! Everyone Was Wrong Forever</h2>



<p>There is no such thing as traceroute.</p>
<p>I used to deliver network training at work. It was freeform, I was given wide latitude to design it as I saw fit, so I focused on things that I had seen people struggling with - clearly explaining VLANs in a less abstract manner than most literature, for instance, as well as actually explaining how QoS queuing works, which very few people understand properly.</p>
<p>One of the "chapters" in my presentation was about traceroute, and it more or less said <em>"Don't use it, because you don't know how, and almost nobody you'll talk to does either, so try your best to ignore them."</em> This is not just my opinion, it's backed up by people much more experienced than me. For a good summary I highly recommend <a href="https://www.slideshare.net/RichardSteenbergen/a-practical-guide-to-correctly-troubleshooting-with-traceroute" target="_blank" rel="nofollow noopener noreferrer">this presentation</a>.</p>
<p>But as good as that deck is, I always felt it left out a crucial piece of information: Traceroute, <em>as far as the industry is concerned</em>, does not exist.</p>
<p>Look it up. There is no RFC. There are no ports for traceroute, no rules in firewalls to accommodate it, no best practices for network operators. Why is that?</p>
<h3>Traceroute has no history</h3>
<p>First off: Yes, there <em>is</em> a traceroute RFC. It's <a href="https://datatracker.ietf.org/doc/html/rfc1393" target="_blank" rel="nofollow noopener noreferrer">RFC1393</a>, it's 31 years old, and to my knowledge nothing supports it. The RFCs are jam-packed with brilliant ideas nobody implemented. This is one of them. The traceroute we have is completely unrelated to this.</p>
<p>Unsurprisingly however, it's a good description of how a traceroute protocol <em>should</em> work. You send a packet to a given destination, with a special flag set, and any machine it passes through observes the flag and says "oh, this packet is meant to be traced," so it generates an ICMP Traceroute response and sends it back to the originating host.</p>
<p>The host, then, sends a single packet and receives a flood of responses describing the path that packet took, definitively. Great! Or, I mean, it would be, if anything supported it. And if it was 1993.</p>
<p>As the linked presentation explains, traceroute simply no longer works in the modern world, at least not "as designed" - and it no longer <em>can</em> work that way, for several reasons not the least that networks have been abstracted in ways it did not anticipate.</p>
<p>There are now things like MPLS, which operate by encapsulating IP - in other words, putting a bag over a packets head, throwing it in the back of a van, driving it across town and letting it loose so it has no idea how far it's traveled. Without getting much further into how that works: It is completely impossible for it to satisfy the expectations of traceroute.</p>
<p>This "tool" works purely at layer 3, so it's impossible for it to adapt to the sort of "layer 12-dimensional-chess" type shenanigan that MPLS does - and there are other problems, but they're all getting ahead of reality, since traceroute never even worked correctly as intended, and there's no reason it would.</p>
<p>Traceroute, you see, is "clever," which is an engineering term that means "fragile." When programmers discover something "clever," any ability they may have had to assess its sustainability or purpose-fit often goes out the window, because it's far more important to embrace the "cleverness" than to solve a problem reliably.</p>
<p>The RFC process is likely not perfect - it's basically an enormous committee system, so, that's troubling - but it does at least constitute a review and consensus process. Had someone written out a spec for traceroute, and then vendors had agreed to implement it, that would be one thing. But that is not what happened.</p>
<h3>Traceroute is a filthy hack</h3>
<p>From the traceroute man page (1987):</p>
<pre>Implemented by Van Jacobson from a suggestion by Steve Deering.
Debugged by a cast of thousands with particularly cogent
suggestions or fixes from C. Philip Wood, Tim Seaver and Ken Adelman.</pre>
<p>I can't find any proper history of the tool, but my <em>impression</em> and my <em>assumption</em> is that it is simply a behavior that someone <em>noticed was possible</em>. Engineers did not get together and design a system for this; some people just realized that it was <em>a side effect of other network behavior not intended to accomplish this goal.</em></p>
<p>In other words, it's an exploit, and that is really the best way to describe both how it works, and why it's a bad idea.</p>
<hr>
<p><strong>Here's how traceroute works:</strong></p>
<p>When you send a packet to a destination, it often has to go through multiple routers, or "hops."</p>
<p>To prevent packets from cycling indefinitely in a network due to routing loops (router A points to router B which points to router A...) they include a Time-To-Live field, which is set to a reasonably high value when a packet is created, and each machine that the packet passes through decrements that field by one.</p>
<p>When the field hits zero, the packet gets thrown away. <em>As a courtesy,</em> the router that's dropping the packet has the <em>option</em> to generate a new packet, using the ICMP protocol, with the subtype <em>"TTL Exceeded,"</em> and send it back to the originating machine, to let it know there's something wrong with the network path.</p>
<p>These clever fellows in 1987 realized that by manipulating the TTL value, you can choose which router will send that ICMP message.</p>
<p>Send a packet with the TTL set to 1. The first router you hit will decrement it to zero. The packet is now "dead", so it drops the packet, and sends back TTL Exceeded. That response will originate from the router's own IP address - congratulations, you now have the IP of the first hop.</p>
<p>Now send another with the TTL set to 2. The first router will decrement it to 1 and pass it, and the <em>second</em> one will decrement to zero and drop it. Now you have <em>its</em> IP address.</p>
<p>Repeat, increasing TTL each time, until the final hop responds. You now have your complete path.</p>
<hr>
<p>This is indeed quite clever, but don't lose sight of what is going on here. TTL Exceeded is simply <em>not meant for this.</em> It is a message meant to diagnose a specific, unrelated kind of network malfunction. It's not intended for tracing paths, and for reasons I'll explain, it's also exactly the kind of feature that may exist in a lab, and in the first few experimental networks, but gets abandoned as soon as money enters the picture.</p>
<h3>DJ Shadow - Why Hip Hop Sucks In 96 (It's The Money)</h3>
<p><em>TTL Exceeded</em> is not a "feature."</p>
<p>Features are things that enable functionality. It doesn't do that.</p>
<p>Features are things that affect end-user experience. It doesn't do that.</p>
<p>TTL Exceeded is purely informational. It's useful to exactly one person: a network engineer. It would be absolutely untenable to report this sort of error to an end user, since there's positively nothing they can do about it, so no application will ever do this.</p>
<p>Not only were these messages not intended for end users, they weren't even intended for network operators as we know them now.</p>
<p>In 1987, virtually every network admin could get an email address for the admin of pretty much any other network, worldwide, with a couple phone calls or a whois lookup. That meant it was practical to troubleshoot <em>other peoples networks</em>, which are often where these errors are seen. Nowadays? Forget about it. Hah. Wow. <em>No way.</em></p>
<p>If you get a TTL Exceeded while trying to reach another host through the <em>internet</em>, there is a zero percent probability that you can get traction on your problem unless you are a Fortune 500 - and even then it will be tough. At least half the companies that are likely to be involved simply do not provide any form of support for problems involving less than millions of hosts.</p>
<p>It is, generally speaking, not possible to call AT&amp;T and say "Hey, when I try to ping one of your subscribers in California from a Level3 circuit in New York, I'm hitting a routing loop." I have worked for an ISP with direct peering with those networks and that simply never worked. We got incompetent, consumer-grade support techs and the issue went nowhere, if we even had a contact at all.</p>
<p>It's even harder to call the exchange partners, the network providers that may sit in between AT&amp;T and Level3 in this equation. Nobody will even tell you who they are, and if they did, <em>there is simply nobody to call.</em> Those phone numbers don't exist unless you're a network engineer at one of their direct partners who is calling to report that a fiber port is down.</p>
<p>No, AT&amp;T is not going to push your complaint up the line to XO. Haha. No.</p>
<p>Problems like this are fairly rare these days, which makes it even less likely that anyone will be on hand to work on them. Most of the time, IME, they get resolved through Brownian Troubleshooting: large scale network maintenance happens for unrelated reasons and <em>incidentally</em> fixes the problem.</p>
<p>So traceroute, on an internet scale, has been useless for ages. You think you see a routing problem? So what? There's absolutely nothing you can do about it.</p>
<p>With that information, go ahead and ask yourself if you think anyone, at any network hardware company, has given a shit about implementing TTL Exceeded since the 90s. The answer is obvious: No. Without a doubt, this is not on anyone's priority list.</p>
<p>If you're at Juniper, nobody is clamoring for this. You do not have ISPs threatening to switch to Cisco (lmao!) just because you didn't implement TTL Exceeded correctly, because they aren't using it. The kind of problems ISPs care about are "we lost the entire US northeast" or "we can't reach Comcast, at all." The NOCs involved at that point may use traceroute, but they will get by without it. Nobody is going to make a C-level escalation with Juniper over it.</p>
<p>So, as a network hardware vendor, with a certain budget and a whole galaxy of internet standards to implement, are you going to put time into this? Absolutely not.</p>
<p>Academics, perhaps. People doing research and experiments at universities 35 years ago might have stuck to the specs religiously, but there is <em>no financial reason whatsoever to implement this correctly.</em></p>
<p>But then, we're ahead of ourselves again. Because what is a "correct" implementation?</p>
<h3>Nothing involving a router is "correct", but</h3>
<p>RFC 792, <em>"INTERNET CONTROL MESSAGE PROTOCOL",</em> explains how to implement TTL Exceeded (which I believe is technically called "Time Exceeded"):</p>
<pre>   Description

      If the gateway processing a datagram finds the time to live field
      is zero it must discard the datagram.  The gateway <em>may</em> also notify
      the source host via the time exceeded message.

      If a host reassembling a fragmented datagram cannot complete the
      reassembly due to missing fragments within its time limit it
      discards the datagram, and it <em>may</em> send a time exceeded message.</pre>
<p>Anyone who knows how to read an RFC understands the crushing solemnity of MAY.</p>
<p>Something that you MAY do is something that WILL NOT be done when it counts. MAY, in RFC terminology, means exactly what the dictionary says it should: The implementer can do it if they want.</p>
<p>That means that it is "standards-compliant" to create a router that has absolutely no implementation of ICMP TTL Exceeded. "May" can mean "never." It is completely up to the vendor.</p>
<p>And wouldn't you know it: vendors <em>do</em>, in fact, choose to <em>never</em> send these messages, and for good reason: It's hard!</p>
<p>The linked presentation (seriously! read it! please! you will benefit!) addresses why this is, and in short it comes down to the fact that routers are basically supercomputers. Consider that a core router at an ISP is potentially handling <em>billions</em> of packets per second. Running all that through a conventional CPU is absurd, and nobody has done this in decades.</p>
<p>Instead, routers contain custom, purpose-built hardware - called the <em>data plane</em> - consisting of dedicated silicon with the sole ability to look at <em>the parts of a packet that matter for routing purposes</em> and ask a couple very simple questions, e.g. <em>"Do I have a way to get this to its next hop?"</em> and <em>"does it still have time to live?"</em></p>
<p>There are probably other details, but you get my point - it's <em>highly</em> optimized.</p>
<p>99.99% of the packets that pass through such a device simply come in one port, get glanced at, and are then hurled out of another port so the silicon can get on to the next packet. During all of this, the actual <em>computer</em> part of the router, the thing that can make complex decisions, is idle.</p>
<p>Yes, routers do contain general-purpose computers; they're pitiful little things. From the linked presentation (read it!!):</p>
<pre>A 320-640+ Gbps router may only have a 600MHz CPU
ICMP generation is <em>NOT</em> a priority for the router.
</pre>
<p>Yeah. The CPU is... Not Fast.</p>
<p>As I implied, this is how supercomputers often work: you have a <em>massive</em> array of extremely fast processors, that can only solve certain, very well defined kinds of problems, and then off to the side you have some horrible little Core i3 Ideapad Yoga whose sole job is to feed program and data into the thing and then pull the string on its back.</p>
<p>With a supercomputer, feeding it invalid data will simply crash the process and you'll have to start all over. That's not an option with networks, since you can't control the incoming data, so routers need a way to handle exception scenarios. That's where the computer - known as the <em>control plane</em> - comes in.</p>
<p>In addition to feeding configs to the data plane, the control plane CPU is responsible for handling unexpected situations. If an interface goes down, the data plane simply starts dropping packets (if it has no other paths.) It takes no other actions; it's the control plane's job to notice this event and do something about it, e.g. sending SNMP traps so someone in a NOC can investigate.</p>
<p>I don't know how many <em>TTLs</em> get <em>Exceeded</em> these days, but even if a router sees tons of them every day, there's nothing it can do to fix the problem, and sending TTL Exceeded is a <em>MAY</em>, not a <em>MUST</em>, so no vendor is going to spend an extra $100,000 to design circuitry to generate and return those responses. That means that any packet that runs out of TTL will have to get forwarded to the control plane, which will decide if and when to send a response.</p>
<p>It goes without saying that the control plane is a very busy little bee. It's bad enough that it has to handle all the "exceptions", which are going to be <em>plentiful</em> in a carrier network with millions of hosts passing through it, but it <em>also</em> has to handle any actual self-destined traffic.</p>
<p>In addition to the millions of hosts that an internet router has to arbitrate between, it also has its own IP addresses, which people rudely try to interact with all day long. When you ping a router, you're making that poor little 600MHz ARM chip find time to deal with your traffic, <em>not</em> the terabit-per-second monster that it's married to. Same goes for SNMP queries, regular config backups, and other forms of management access.</p>
<p>Other than traceroute, <em>TTL Exceeded</em> serves very little purpose in the modern world, and with traceroutes being such a tiny percentage of traffic, it is perfectly reasonable for network admins to not care if it works or not. When you put all this together, it becomes apparent that most network providers are never going to spend a second thinking about this.</p>
<p>You can easily confirm this is true. Run a traceroute... anywhere. Yahoo dot com. You will see nodes that never respond, 9 times out of 10.</p>
<h3>The Worst Diagnostics In The World</h3>
<p>I cannot even guess how many times I have seen network techs see one hop not respond and say "well it looks like hop 5 is down, so that's your problem," even though hop 6 is responding.</p>
<p>It is impossible for me to imagine how they think the internet works, but they're playing against a stacked deck, because traceroute is just <em>the worst diagnostic tool imaginable.</em></p>
<p>A good tool gives you a go, a no-go, or information. That is, it tells you something is working, or broken, or provides data you can interpret.</p>
<p>Traceroute does provide a single "go" outcome: If you see a trace get <em>all the way through to the last node</em>, well, okay, that's a success. The path is probably fine.</p>
<p>However, it also only provides a single "no-go" outcome, and it's not the one people think. Lack of response from hosts is not a failure. The <em>sole</em> failure you can identify reliably from traceroute is a network loop. If you see the same pair of nodes respond over and over, then you have a loop.</p>
<p>...and that information is almost completely useless, because this is the exact problem that TTL Exceeded is meant to diagnose, so you can just use it as intended. Just ping the target, and you'll see a TTL Exceeded response from one of the two routers that is looping the packet, identifying the failure point. Admittedly, traceroute does tell you <em>both</em> of those names, which is convenient.</p>
<p>Inadvertent routing loops are incredibly rare however, and 99% of the ones that I have seen were actually caused by network interfaces being down, and would have been discovered and resolved through ordinary, thorough network review.</p>
<p>As far as information? Well, read the presentation. The information provided by traceroute is limited, objectively incorrect and misleading in many cases, and fiendishly hard to interpret.</p>
<p>Lack of response from a node means nothing.</p>
<p>Even if <em>all</em> the nodes past a certain point aren't responding, <em>that also means nothing.</em></p>
<p>If the nodes have high latency, <em>that also means nothing.</em></p>
<p>If they respond on some probes and not others, <em>that also means nothing.</em></p>
<p><em>Nothing you see in a traceroute means anything, because it is all accidental.</em></p>
<p>You are sending a packet through a network that <em>did not plan for it</em>. Nobody has taken steps to ensure your traceroute should succeed. There are no "Traceroute" checkboxes or statements in router configs. There's a really spicy reason for that, too: Traceroute does not even meet the most minimal definitions of a network protocol.</p>
<p>It's not a special kind of ICMP message, or a UDP or TCP packet that uses a defined port. You cannot "permit traceroute" in a firewall, because it has no standard characteristics. A lot of people think traceroute sends pings - this is an option, but never the default behavior AFAIK.</p>
<p>By default, traceroute simply sends <em>a gibberish UDP packet on a random pair of ephemeral ports.</em> The entire point is to be thrown away before a host even gets a <em>chance</em> to consume it, so the contents are irrelevant.</p>
<p>That means that if you were <em>trying</em> to prepare a network to handle traceroute, you wouldn't be able to. From a network perspective, traceroute does not exist.</p>
<p>It's simply an exploit, a trick someone discovered, so it's to be expected that it has no defined qualities. It's just random junk being thrown at a host, hoping that everything along the paths responds in a way that they are <em>explicitly not required to.</em> Is it any surprise that the resulting signal to noise ratio is awful?</p>
<h3>So What Does All This Mean</h3>
<p>It means that you can't run a traceroute unless you <em>know what you expect to see.</em></p>
<p>When you're tracing inside a network that you control - such as a large enterprise WAN, multiple sites connected with VPNs, or an ISP that you work for - you can guess what each hop will look like, or at least look at the results and suss out whether they looks like they "should."</p>
<p>If you trace from, say, a server at one business location to one at another, you might see your local prem router, then a network edge router, a few core routers, another edge and then another prem router.</p>
<p>From this you can guess, pretty reliably, that you made it all the way to the destination, but either had trouble reaching the specific host (investigate the local router/firewall) or that the host is ACLed or doesn't send ICMP responses (do packet captures on the host.)</p>
<p>If you're tracing through a network you don't control, you have <em>no idea</em> how it's supposed to work. If you're a seasoned network tech who's seen some shit (and, ideally, worked on provider-scale networks) then you can run a traceroute over an unknown network and <em>maybe, possibly,</em> suss out something, but there are no guidelines, it's pure gut feeling: <em>does this look right?</em></p>
<p>If you aren't that experienced however, you should avoid it, because you are not immune to propaganda. When you see high latency, hops not responding or whatever, that information will stick in your head. Despite your best efforts, it will affect the course of your troubleshooting even though you would not be able to say, if asked, what those results <em>meant</em> and what should be <em>done with them.</em></p>
<p>As a diagnostician, you should ask yourself one question before performing any test: <em>"What would I do if the outcome was x? And what if it was y?"</em></p>
<p>Can you fill in x and y? Can you answer either question? If not - why run the test?</p>
<p>And if you <em>do</em> run the traceroute anyway, <em>god forbid</em> you mention it to someone else. Do not write down the results unless you think you actually know what they mean, because no matter how offhandedly you do it, whoever comes across it is <em>guaranteed</em> to see it as a lifeline.</p>
<p>Network techs are mostly incompetent. It is a sad truth, and it's not their fault. People get pressed into jobs that they are told are far less complicated than they actually are. It has been my experience that <em>easily</em> 75% of people working networking jobs are operating in a state of absolute terror, trying to keep their head above water with problems they don't really understand at all.</p>
<p>If you say "hop five isn't responding," congratulations - you just identified "the cause of the problem" as far as all those folks are concerned, and there's no way to get that piss out of the pool.</p>
<p>Whoever you said it in front of is going to refuse, <em>aggressively</em>, to do a lick of additional troubleshooting until "hop five" starts responding. If that's clearly a node that nobody on the conf call or email thread has access to, then everyone's going to throw up their hands and say "Well I Guess We Just Have To Hope It Starts Working." I have seen this countless times.</p>
<p>It happens because, fundamentally, troubleshooting networks <em>sucks.</em></p>
<p>If you don't have total control of the entire path, end to end, with admin access <em>and</em> expertise on every node along the way, there is no way to get a complete picture of what's going on. That kind of access is extremely rare; you're probably a high-ranking network architect if you have it; and <em>even with all that access</em> there are still plenty of cases where you simply cannot see what's wrong, because it's happening either too fast, or in a place that's impossible to inspect.</p>
<p>As a result, networking is full of superstition. People casting spells, executing words of power, trying to read tea leaves and declaring that the end times are coming, not because the hard info isn't available, but because it's incredibly difficult to obtain and interpret.</p>
<h3>The Thanksgiving Uncle Problem</h3>
<p>Read the presentation. It does a better job than this messy post at illustrating the problem. Even if you don't understand networking, by the time you're done, you will be convinced that this is too complicated for <em>most people</em>, full stop. There are just too many unknowns.</p>
<p>You will hate me for making you read this. You will regret it, because you will now be the only person in every conversation who understands these things, and the knowledge is damning. You will have to sit, silently, as everyone around you makes egregious errors in diagnostics that lead them down completely incorrect paths. This is the <em>Thanksgiving Uncle Problem.</em></p>
<p>That's the situation where you, a gay leftist, go to Thanksgiving dinner with the family, and a shitty uncle sits across from you and begins telling lies about society, about people of color, about gay marriage, and so on. If you're self-destructive, you engage him. It will not go well.</p>
<p>The reason for this is that, in order for him to accept <em>anything</em> you say, he needs to accept that many of his foundational beliefs about the world are wrong. Ideas like "the police protect us" and "children need a mom and a dad" have been part of his worldview for so long that he has, without question, made <em>millions</em> of decisions based on these assumptions.</p>
<p>In order for him to discard them, he has to admit that he has been making a fool of himself, doing incredibly wrong and often <em>harmful</em> things, for his entire life. That is too much guilt to handle, and he - and most people - will do anything possible to avoid accepting it. Certainly, this is not a door he's willing to open when he's on his fourth mimosa and doing his best not to think about the goddamn insurance adjuster job he has to go back to on Monday.</p>
<p>So you will read this slideshow, and then you will sit on conference calls thinking, "My god. They are all wrong. And they've always been wrong. And <em>I can't help them,</em> because they will fight me tooth and claw to continue being wrong."</p>
<p>I have no advice on how to deal with this, but it's better to be correct than to be comfortable.</p>
<p><small>Footnote: Ironically, it seems very possible to me that the systems that most consistently enable this are the cheapest routers on the market. Every single home "gateway" ever sold runs Linux, where the ICMP implementation is a core kernel feature, not a user provided daemon.</small></p><small>
<p>I would not be surprised at all if the Linux kernel devs actually <em>have</em> made sure that TTL Exceeded is implemented and enabled by default - and since most Linux-based routers do everything in pure software, there is no data/control plane split to worry about, so sending an Exceeded is more or less "free."</p>
</small><p><small>This would only make a difference for traceroute if Linux was used for anything other than the cheapest endpoint routers, but it's still very funny.</small></p>
<hr>
<h2>Addendum #1</h2>
<p>I reviewed that slide deck again and learned that I conflated a couple concepts.</p>
<p>Yes, the control plane <em>may</em> be responsible for handling exceptions, including ICMP generation, but it is apparently more likely (at least, at the scale of equipment that I am discussing) that the data plane has a <em>fast path</em> and <em>slow path</em>, both located in the data plane, and the slowpath is responsible for handling this work. The control plane, in such a device, <em>only</em> handles data destined to the router's own IP.</p>
<p>However, the slowpath is (per Richard Steenbergen, the author of that presentation; we will trust his research is valid) <em>still</em> a general-purpose CPU instead of custom silicon, so functionally, the point I was making is still valid: There is a very slow computer handling these packets.</p>
<p>Steenbergen uses this fact to make the point that, because these slow-path CPUs <em>are</em> so slow, they are usually rate-limited. Yes, this means that <em>some number of TTL Exceeded messages will simply be thrown away, even if they are enabled.</em></p>
<p>The example given is that a handful of users running MTR (do not get me started on this bastard program) can actually hit this rate limit. This is an outstanding example because I have seen something similar in practice.</p>
<p>Consider what that would look like, and how common it would be: If you have a NOC full of people who think they know what they're doing, but don't, that only enhances the probability that everyone is trying to troubleshoot on their own instead of doing a screenshare and coordinating their efforts - thus, you have six guys running MTR to the same IP.</p>
<p>If they hit that rate limit, what do they see? Nodes suddenly not responding! Randomly, in fact - sometimes responding, sometimes not! That means it's not just a hop that doesn't respond to traceroutes, but <em>packet loss!</em> Wow! We found the problem!</p>
<p>Of course, if four of them hit Ctrl+C, the PL would mysteriously vanish. Huh! Weird! Well, it must be an intermittent issue in <em>squints at resolved hostname</em> Hurricane Electric's network. I'm <em>sure</em> they have a flapping port they haven't noticed (lol.) Just send them a trouble ticket!</p>
<p>By the time this useless waste of effort has resolved (e.g. HE has received, acked, investigated, and declared the ticket No Trouble Found and rejected it) the problem has probably gone away due to unrelated network weather effects. The NOC guys all tell each other that HE was lying about their broken network, slap each other on the back for being smarter than the other bastards, and go out for beers.</p>
<p>How do I know this? Because I've been part of it!</p>
<p>My employer used to have an unholy number of customer sites terminated with little Linux shitboxes - you know the sort, they used to be common as dirt. Tiny Soekris-esque SBCs in folded sheet metal boxes with 12V power supplies, running horrible little SoCs and a copy of Busybox from before the fall of Rome. We had reasons for it that I won't go into.</p>
<p>These things were underpowered to put it mildly. They could route maybe 30 mbps, and if you turned on any firewall functionality that dropped to 10. This was at a time when a <em>tremendous</em> number of customers were on connections no faster than 5 mbps, so, this wasn't a huge problem. We got rid of them all when bandwidths skyrocketed.</p>
<p>But what used to happen is that you'd have three or four people looking at one of these things at once, and you'd start seeing packet loss. And there you go, the customer has a bad connection. Kick it to the ISP and close the ticket, right?</p>
<p>I can't count how many times this happened, but I do remember after about four years of doing this, I had come up with a method for getting more accurate latency stats: just ping -i .1. Absolutely <em>hammer</em> the thing with pings while you have the customer test their usual business processes, and it'll be easier to see latency spikes if something is eating up too much bandwidth.</p>
<p>What I discovered is that running two of these in parallel would produce exactly 50% packet loss, with total reliability. I then tested and found that if I just fired up three or four <em>normal</em> pings, at the default interval, it would do the same thing. 30% or 40% packet loss.</p>
<p>There is no telling how many issues we prolonged because everyone was running their own pings simultaneously and the kernel was getting overloaded and throwing some of them out. This is a snapshot of every network support center, everywhere. It is a bad scene.</p>

	<p><a href="https://gekk.info/articles/index.html">List of Articles</a></p>

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Companies Are Ditching the Cloud: The Rise of Cloud Repatriation (154 pts)]]></title>
            <link>https://thenewstack.io/why-companies-are-ditching-the-cloud-the-rise-of-cloud-repatriation/</link>
            <guid>42054813</guid>
            <pubDate>Tue, 05 Nov 2024 20:19:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thenewstack.io/why-companies-are-ditching-the-cloud-the-rise-of-cloud-repatriation/">https://thenewstack.io/why-companies-are-ditching-the-cloud-the-rise-of-cloud-repatriation/</a>, See on <a href="https://news.ycombinator.com/item?id=42054813">Hacker News</a></p>
Couldn't get https://thenewstack.io/why-companies-are-ditching-the-cloud-the-rise-of-cloud-repatriation/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Fisker EVs Hired an IT Spy Who Funneled Millions to N. Korea's Missile Program (132 pts)]]></title>
            <link>https://www.torquenews.com/1084/fisker-evs-hired-it-spy-who-funneled-millions-north-koreas-missile-program-case-fbi</link>
            <guid>42054791</guid>
            <pubDate>Tue, 05 Nov 2024 20:16:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.torquenews.com/1084/fisker-evs-hired-it-spy-who-funneled-millions-north-koreas-missile-program-case-fbi">https://www.torquenews.com/1084/fisker-evs-hired-it-spy-who-funneled-millions-north-koreas-missile-program-case-fbi</a>, See on <a href="https://news.ycombinator.com/item?id=42054791">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
    <strong><span>Follow us today...</span></strong>
    <a href="https://www.facebook.com/sharer.php?u=https://www.torquenews.com/1084/fisker-evs-hired-it-spy-who-funneled-millions-north-koreas-missile-program-case-fbi" target="_blank">
      <img src="https://www.torquenews.com/profiles/torquenews/facebook.png" loading="lazy" width="64" height="64" alt="Facebook icon">
    </a>&nbsp;
    <a href="https://x.com/share?text=Fisker+EVs+Hired+An+IT+Spy+Who+Funneled+Millions+To+North+Korea%E2%80%99s+Missile+Program%2C+CEO+Henrik+Fisker%2C+Says+%E2%80%9CThe+Case+Is+With+the+FBI%E2%80%9D%20-%C2%A0via%C2%A0@torquenewsauto&amp;url=https://www.torquenews.com/1084/fisker-evs-hired-it-spy-who-funneled-millions-north-koreas-missile-program-case-fbi">
      <img src="https://www.torquenews.com/profiles/torquenews/x.png" loading="lazy" width="64" height="64" alt="X icon">
    </a>&nbsp;
    <a href="https://t.me/teslaev" target="_blank" title="Join us on Telegram!">
      <img src="https://www.torquenews.com/profiles/torquenews/telegram.png" loading="lazy" width="64" height="64" alt="Telegram icon">
    </a>&nbsp;
    <a href="https://www.reddit.com/submit?url=https://www.torquenews.com/1084/fisker-evs-hired-it-spy-who-funneled-millions-north-koreas-missile-program-case-fbi&amp;title=Fisker+EVs+Hired+An+IT+Spy+Who+Funneled+Millions+To+North+Korea%E2%80%99s+Missile+Program%2C+CEO+Henrik+Fisker%2C+Says+%E2%80%9CThe+Case+Is+With+the+FBI%E2%80%9D" target="_blank" title="Join us on Reddit!">
      <img src="https://www.torquenews.com/profiles/torquenews/reddit.png" loading="lazy" width="64" height="64" alt="Reddit icon">
    </a>
  </p><div><p>Welcome to the shocking world of automotive espionage.&nbsp;</p>
<p>It appears North Korea is targeting U.S. automakers. A report from <a href="https://www.autonews.com/manufacturing/an-fisker-hired-north-korean-spy/" target="_blank">Automotive News</a> (by subscription) reveals that <a href="https://www.torquenews.com/fisker" target="_blank">Fisker Inc.</a>, the <a href="https://www.fiskerinc.com/" target="_blank">Ocean SUV electric car</a> manufacturer located in Manhattan Beach, California, hired an IT worker who was a spy for the North Korean government to steal money for its missile program.&nbsp;</p>
<blockquote><p>
AN says, "A remote information technology employee hired by&nbsp;Fisker Inc.&nbsp;turned out to be a spy for the North Korean government."
</p></blockquote>
<p>North Korea targeted Fisker and other automakers.</p>
<p>The report from the Danish magazine The Engineer said Fisker was among numerous U.S. automobile companies targeted by a money laundering scheme that funneled more than a staggering $6 million to North Korea's ballistic missile program.</p>
<blockquote><p>
AN says, "The story, based on documents filed by the U.S. Department of Justice, says Fisker hired a remote IT worker named Kou Thao in October 2021. But Thao's purported address in Arizona belonged to a woman named Christina Chapman, who set up laptop computers that the North Koreans accessed through Russia and China."
</p></blockquote>
<p>Fisker filed for bankruptcy shortly after firing the spy.&nbsp;</p>
<p>Fisker terminated Thao in September 2023 after the Justice Department notified the electric vehicle maker that it was being scammed. The financial strain caused by the espionage activities could have led to <a href="https://www.reuters.com/business/autos-transportation/ev-startup-fisker-files-bankruptcy-2024-06-18/" target="_blank">Fisker&nbsp;filing for bankruptcy</a>&nbsp;nine months later, in June 2024.</p>
<p>The report says Fisker wasn't the only automaker targeted by the spy. The Justice Department's April indictment of Chapman identifies one of her co-conspirators as "Frank C.," a contractor who worked for "a Fortune 500 iconic American automotive manufacturer located in Detroit, Michigan," starting in April 2022.&nbsp;</p>
<p>The Justice Department document doesn't name the company. Two American Fortune 500 automakers are in Detroit, Michigan: General Motors and Ford Motor Company.&nbsp;</p>
<p>The <a href="https://www.wsj.com/tech/north-korean-spies-are-infiltrating-u-s-companies-through-it-jobs-e45a1be8" target="_blank">Wall Street Journal</a> (by subscription) report says that North Korean spies are infiltrating U.S. companies through IT jobs, and companies unknowingly hire North Koreans for hundreds of low-level jobs, "giving Pyongyang access to cash and IP."</p>
<p>The FBI is aware of the problem.</p>
<p>The FBI warned companies about North Korea's highly tailored, difficult-to-detect social engineering campaigns against employees of decentralized finance ("DeFi"), cryptocurrency, and similar businesses to deploy malware and steal company cryptocurrency.</p>
<p>The FBI issued this public service announcement on September 3, 2024.</p>
<p><em>The Democratic People's Republic of Korea ("DPRK" aka North Korea) is conducting highly tailored, difficult-to-detect social engineering campaigns against employees of decentralized finance ("DeFi"), cryptocurrency, and similar businesses to deploy malware and steal company cryptocurrency.</em></p>
<p><em>North Korean social engineering schemes are not just complex, but also elaborate, often compromising victims with sophisticated technical acumen. Given the scale and persistence of this malicious activity, even those well-versed in cybersecurity practices can be vulnerable to North Korea's determination to compromise networks connected to cryptocurrency assets.</em></p>
<p><em>North Korean malicious cyber actors researched various targets connected to cryptocurrency exchange-traded funds (ETFs) over the last several months. This research included pre-operational preparations suggesting North Korean actors may attempt malicious cyber activities against companies associated with cryptocurrency ETFs or other cryptocurrency-related financial products.</em></p>
<p><em>For companies active in or associated with the cryptocurrency sector, the FBI emphasizes North Korea employs sophisticated tactics to steal cryptocurrency funds. It is a persistent threat to organizations with access to large quantities of cryptocurrency-related assets or products.</em></p>
<p>Regarding it being a target of a North Korean spy, CEO Henrik Fisker told The Engineer that the case "is with the FBI" and declined to comment further.</p>
<p>I am&nbsp;<a href="https://www.torquenews.com/users/denis-flierl" target="_blank">Denis Flierl</a>, a Senior Torque News Writer since 2012. I‚Äôve invested over 13 years in the automotive industry in a consulting role, working with every major car brand. I am an experienced Rocky Mountain Automotive Press member. You'll find my expert Subaru analysis <a href="https://www.torquenews.com/subaru" target="_blank">here</a>. Follow me on my X&nbsp;<a href="https://twitter.com/SubaruReport" target="_blank"><strong>SubaruReport</strong></a><strong>,&nbsp;</strong><a href="https://twitter.com/AllSubaru" target="_blank"><strong>All&nbsp;Subaru</strong></a><strong>, </strong><a href="https://twitter.com/WRX_STI_News" target="_blank"><strong>WRXSTI</strong></a>, <a href="https://twitter.com/DenisFlierl" target="_blank"><strong>@DenisFlierl</strong></a>,&nbsp;<a href="https://www.facebook.com/SubaruReport" target="_blank">Facebook</a>, and&nbsp;<a href="https://www.instagram.com/subaru.report/" target="_blank">Instagram</a>.</p>
<p>Photo credit: Denis Flierl via Fisker Inc.</p>
</div><p>
    <strong><span>Follow us today...</span></strong>
    <a href="https://www.facebook.com/sharer.php?u=https://www.torquenews.com/1084/fisker-evs-hired-it-spy-who-funneled-millions-north-koreas-missile-program-case-fbi" target="_blank">
      <img src="https://www.torquenews.com/profiles/torquenews/facebook.png" loading="lazy" width="64" height="64" alt="Facebook icon">
    </a>&nbsp;
    <a href="https://x.com/share?text=Fisker+EVs+Hired+An+IT+Spy+Who+Funneled+Millions+To+North+Korea%E2%80%99s+Missile+Program%2C+CEO+Henrik+Fisker%2C+Says+%E2%80%9CThe+Case+Is+With+the+FBI%E2%80%9D%20-%C2%A0via%C2%A0@torquenewsauto&amp;url=https://www.torquenews.com/1084/fisker-evs-hired-it-spy-who-funneled-millions-north-koreas-missile-program-case-fbi">
      <img src="https://www.torquenews.com/profiles/torquenews/x.png" loading="lazy" width="64" height="64" alt="X icon">
    </a>&nbsp;
    <a href="https://t.me/teslaev" target="_blank" title="Join us on Telegram!">
      <img src="https://www.torquenews.com/profiles/torquenews/telegram.png" loading="lazy" width="64" height="64" alt="Telegram icon">
    </a>&nbsp;
    <a href="https://www.reddit.com/submit?url=https://www.torquenews.com/1084/fisker-evs-hired-it-spy-who-funneled-millions-north-koreas-missile-program-case-fbi&amp;title=Fisker+EVs+Hired+An+IT+Spy+Who+Funneled+Millions+To+North+Korea%E2%80%99s+Missile+Program%2C+CEO+Henrik+Fisker%2C+Says+%E2%80%9CThe+Case+Is+With+the+FBI%E2%80%9D" target="_blank" title="Join us on Reddit!">
      <img src="https://www.torquenews.com/profiles/torquenews/reddit.png" loading="lazy" width="64" height="64" alt="Reddit icon">
    </a>
  </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. chip revival plan chooses sites (141 pts)]]></title>
            <link>https://spectrum.ieee.org/nstc</link>
            <guid>42054779</guid>
            <pubDate>Tue, 05 Nov 2024 20:14:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/nstc">https://spectrum.ieee.org/nstc</a>, See on <a href="https://news.ycombinator.com/item?id=42054779">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="U.S. Chip Revival Plan Chooses Sites"><p>Last week the organization tasked with running the the <a href="https://www.semiconductors.org/chips-rd-programs/" target="_blank">biggest chunk</a> of <a href="https://spectrum.ieee.org/tag/chips-act" target="_blank">U.S. CHIPS Act‚Äôs</a> US $13 billion R&amp;D program made some significant strides: The <a href="https://www.nist.gov/chips/research-development-programs/national-semiconductor-technology-center" target="_blank">National Semiconductor Technology Center (NSTC)</a>  released a strategic plan and selected the sites of two of three planned facilities and released a new strategic plan. The locations of the two sites‚Äîa <a href="https://www.nist.gov/news-events/news/2024/11/biden-harris-administration-announces-sunnyvale-ca-expected-location-second" target="_blank"><u>‚Äúdesign and collaboration‚Äù center</u></a> in Sunnyvale, Calif., and a lab devoted to advancing the <a href="https://www.nist.gov/news-events/news/2024/10/biden-harris-administration-announces-ny-creates-albany-nanotech-complex" target="_blank"><u>leading edge of chipmaking</u></a>, in Albany, N.Y.‚Äîbuild on an existing ecosystem at each location, experts say. The location of the third planned center‚Äîa chip prototyping and packaging site that could be especially <a href="https://spectrum.ieee.org/power-electronics" target="_self"><u>critical for speeding semiconductor startups</u></a>‚Äîis still a matter of speculation.</p><p>‚ÄúThe NSTC represents a once-in-a-generation opportunity for the U.S. to accelerate the pace of innovation in semiconductor technology,‚Äù <a href="https://natcast.org/leadership" target="_blank"><u>Deirdre Hanford</u></a>, CEO of Natcast, the nonprofit that runs the NSTC centers, said in a <a href="https://natcast.org/biden-harris-announces-second-chips-flagship-facility" rel="noopener noreferrer" target="_blank"><u>statement</u></a>. According to the strategic plan, which covers 2025 to 2027, the NSTC is meant to accomplish three goals: extend U.S. technology leadership, reduce the time and cost to prototype, and build and sustain a semiconductor workforce development ecosystem. The three centers are meant to do a mix of all three. </p><h2>New York gets extreme ultraviolet lithography</h2><p>NSTC plans to direct $825 million into the Albany project. The site will be dedicated to extreme ultraviolet lithography, a technology that‚Äôs essential to making the most advanced logic chips. The <a href="https://ny-creates.org/ny-creates-campus-complex/" rel="noopener noreferrer" target="_blank"><u>Albany Nanotech Complex</u></a>, which has already seen more than $25 billion in investments from the state and industry partners over two decades, will form the heart of the future NSTC center. It already has an<a href="https://spectrum.ieee.org/tag/euv" target="_self"><u> EUV lithography</u></a> machine on site and has begun an expansion to install a next-generation version, called <a href="https://spectrum.ieee.org/high-na-euv" target="_self"><u>high-NA EUV</u></a>, which promises to produce even finer chip features. Working with a tool recently installed in Europe, <a href="https://spectrum.ieee.org/tag/ibm">IBM</a>, a long-time tenant of the Albany research facility, reported record yields of <a href="https://research.ibm.com/blog/new-euv-patterning-yield-benchmarks" rel="noopener noreferrer" target="_blank"><u>copper interconnects built every 21 nanometers</u></a>, a pitch several nanometers tighter than possible with ordinary <a href="https://spectrum.ieee.org/high-na-euv">EUV</a>. </p><p>‚ÄúIt‚Äôs fulfilling to see that this ecosystem can be taken to the national and global level through CHIPS Act funding,‚Äù said <a href="https://research.ibm.com/people/mukesh-khare" rel="noopener noreferrer" target="_blank"><u>Mukesh Khare</u></a>, general manager of IBM‚Äôs <a href="https://spectrum.ieee.org/topic/semiconductors/">semiconductors</a> division, speaking from the future site of the NSTC EUV center. ‚ÄúIt‚Äôs the right time, and we have all the ingredients.‚Äù</p><p>While only a few companies are capable of manufacturing cutting edge logic using EUV, the impact of the NSTC center will be much broader, Khare argues. It will extend down as far as early-stage startups with ideas or materials for improving the chipmaking process ‚ÄúAn EUV R&amp;D center doesn‚Äôt mean just one machine,‚Äù says Khare. ‚ÄúIt needs so many machines around it‚Ä¶ It‚Äôs a very large ecosystem.‚Äù</p><h2>Silicon Valley lands the design center</h2><p>The design center is tasked with conducting advanced research in <a href="https://spectrum.ieee.org/tag/chip-design" target="_self"><u>chip design</u></a>, electronic design automation (EDA), chip and system architectures, and<a href="https://spectrum.ieee.org/tag/security" target="_self"><u> hardware security</u></a>. It will also host the NSTC‚Äôs design enablement gateway‚Äîa program that provides NSTC members with a secure, cloud-based access to design tools, reference processes and designs, and shared data sets, with the goal of reducing the time and cost of design. Additionally, it will house workforce development, member convening, and administration functions. </p><p>Situating the design center in Silicon Valley, with its concentration of research universities, venture capital, and workforce, seems like the obvious choice to many experts. ‚ÄúI can‚Äôt think of a better place,‚Äù says Patrick Soheili, co-founder of interconnect technology startup Eliyan, which is based in Santa Clara, Calif.</p><p><a href="https://www.linkedin.com/in/abhijeet-chakraborty-1a21931/" rel="noopener noreferrer" target="_blank"><u>Abhijeet Chakraborty</u></a>, vice president of engineering in the technology and product group at Silicon Valley-based <a href="https://www.synopsys.com/" rel="noopener noreferrer" target="_blank"><u>Synopsys</u></a>, a leading maker of EDA software, sees Silicon Valley‚Äôs expansive tech ecosystem as one of its main advantages in landing the NSTC‚Äôs design center. The region concentrates companies and researchers involved in the whole spectrum of the industry from semiconductor process technology to cloud software.</p><p>Access to such a broad range of industries is increasingly important for <a href="https://spectrum.ieee.org/tag/chip-design">chip design</a> startups, he says. ‚ÄúTo design a chip or component these days you need to go from concept to design to validation in an environment that takes care of the entire stack,‚Äù he says. It‚Äôs prohibitively expensive for a startup to do that alone, so one of Chakraborty‚Äôs hopes for the design center is that it will help startups access the design kits and other data needed to operate in this new environment.</p><h2>Packaging and prototyping still to come</h2><p>A third promised center for prototyping and packaging is still to come. ‚ÄúThe big question is where does the packaging and prototyping go?‚Äù says Mark Granahan, cofounder and CEO of Pennsylvania-based <a href="https://spectrum.ieee.org/power-electronics" target="_blank">power semiconductor startup Ideal Semiconductor</a>. ‚ÄúTo me that‚Äôs a great opportunity.‚Äù He points out that because there is so little packaging technology infrastructure in the United States, any ambitious state or region should have a shot at hosting such a center. One of the original intentions of the act, after all, was to expand the number of regions of the country that are involved in the semiconductor industry.</p><p>But that hasn‚Äôt stopped some already tech-heavy regions from wanting it. ‚ÄúOregon offers the strongest ecosystem for such a facility,‚Äù a spokesperson for <a href="https://www.intel.com/content/www/us/en/foundry/overview.html" rel="noopener noreferrer" target="_blank"><u>Intel</u></a>, whose technology development is done there. ‚ÄúThe state is uniquely positioned to contribute to the success of the NSTC and help drive technological advancements in the U.S. semiconductor industry.‚Äù</p><p>As NSTC makes progress, Granahan‚Äôs concern is that bureaucracy will expand with it and slow efforts to boost the U.S. chip industry. Already the layers of control are multiplying. The <a href="https://www.nist.gov/chips" rel="noopener noreferrer" target="_blank"><u>Chips Office</u></a> at the National Institute of Standards and Technology executes the Act. The NSTC is administered by the nonprofit <a href="https://natcast.org/" rel="noopener noreferrer" target="_blank"><u>Natcast</u></a>, which directs the EUV center, which is in a facility run by another nonprofit, <a href="https://ny-creates.org/" rel="noopener noreferrer" target="_blank"><u>NY CREATES</u></a>. ‚ÄúWe want these things to be agile and make local decisions.‚Äù</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[First time ever, AMD outsells Intel in the datacenter space (417 pts)]]></title>
            <link>https://www.tomshardware.com/pc-components/cpus/for-the-first-time-ever-amd-outsells-intel-in-the-datacenter-space</link>
            <guid>42054449</guid>
            <pubDate>Tue, 05 Nov 2024 19:27:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/pc-components/cpus/for-the-first-time-ever-amd-outsells-intel-in-the-datacenter-space">https://www.tomshardware.com/pc-components/cpus/for-the-first-time-ever-amd-outsells-intel-in-the-datacenter-space</a>, See on <a href="https://news.ycombinator.com/item?id=42054449">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-body">
<p>For well more than two decades, Intel has been the undisputed leader in the market for datacenter CPUs. Intel's Xeon processors powered the vast majority of servers, whereas AMD's processors commanded a single-digit market share just some seven or eight years ago. However, the situation has changed drastically. While Intel's Xeon CPUs still power the majority of servers, the most expensive machines now use AMD's EPYC processors. This is why AMD's datacenter business unit now outsells Intel's datacenter and AI business group, as observed by&nbsp;<a data-analytics-id="inline-link" href="https://x.com/SKundojjala/status/1853041284157682063" data-url="https://x.com/SKundojjala/status/1853041284157682063" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">SemiAnalysis</a>.</p><p>Indeed, AMD's datacenter segment revenue reached&nbsp;<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/amd-rakes-in-cash-with-best-quarterly-revenue-ever-amid-datacenter-business-rise-but-gaming-business-craters" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/cpus/amd-rakes-in-cash-with-best-quarterly-revenue-ever-amid-datacenter-business-rise-but-gaming-business-craters">$3.549 billion</a>&nbsp;in the third quarter, whereas Intel's datacenter and AI group's earnings were&nbsp;<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/intel-lost-usd16-6-billion-in-q3-reports-usd13-3-billion-in-revenue" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/cpus/intel-lost-usd16-6-billion-in-q3-reports-usd13-3-billion-in-revenue">$3.3 billion</a>&nbsp;in Q3 2024. Just two years ago, Intel's DCAI group earned $5 billion - $6 billion per quarter. But as AMD's EPYC processors have gained competitive advantages over Intel's Xeon CPUs, Intel has had to sell its server chips at significant discounts, which has reduced the company's revenue and profit margins.</p><p>It is noteworthy that Intel's flagship&nbsp;<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/intels-latest-flagship-128-core-xeon-cpu-costs-usd17-800-granite-rapids-sets-a-new-high-watermark" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/cpus/intels-latest-flagship-128-core-xeon-cpu-costs-usd17-800-granite-rapids-sets-a-new-high-watermark">128-core Xeon 6980P 'Granite Rapids' processor costs $17,800</a>, making it the company's most expensive standard CPU ever. By contrast, AMD's most expensive 96-core EPYC 6979P processor costs $11,805. If demand for Intel's Xeon 6900-series processors remains high and the company can supply these CPUs in decent volumes, then Intel's datacenter revenue will likely get back on track and surpass AMD's datacenter sales. However, Intel still has to ramp up production of its Granite Rapids products.&nbsp;</p><p>While both Intel and AMD now earn around $3-3.5 billion per quarter selling datacenter CPUs, Nvidia earns much more from its datacenter GPUs and networking chips, which are required to make AI processors work in concert in datacenters. In fact, sales of Nvidia's networking products totaled&nbsp;<a data-analytics-id="inline-link" href="https://s201.q4cdn.com/141608511/files/doc_financials/2025/Q225/Q2FY25-CFO-Commentary.pdf" data-url="https://s201.q4cdn.com/141608511/files/doc_financials/2025/Q225/Q2FY25-CFO-Commentary.pdf" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">$3.668 billion</a>&nbsp;in the company's second quarter of fiscal 2025. Meanwhile, compute GPU sales reached $22.604 billion in Q2 FY2025, which far surpasses the combined sales of Intel and AMD datacenter hardware. Altogether, Nvidia sold nearly $42 billion worth of AI and HPC GPUs in the first half of this year, and it is likely that the company will sell even more datacenter processors in the second half.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-gRFTK4rNSThvy54UVmstUM"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tencent Hunyuan-Large (135 pts)]]></title>
            <link>https://github.com/Tencent/Tencent-Hunyuan-Large</link>
            <guid>42054186</guid>
            <pubDate>Tue, 05 Nov 2024 18:52:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Tencent/Tencent-Hunyuan-Large">https://github.com/Tencent/Tencent-Hunyuan-Large</a>, See on <a href="https://news.ycombinator.com/item?id=42054186">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
    <a href="https://github.com/Tencent/Tencent-Hunyuan-Large/blob/main/README_CN.md">‰∏≠Êñá</a>&nbsp; ÔΩú English
</p>
<p dir="auto">
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4f7f1dcd99f1e24cb8787dc695f3fb708e2f9799fe3559366a3d58139e74491a/68747470733a2f2f647363616368652e74656e63656e742d636c6f75642e636e2f75706c6f61642f75706c6f616465722f68756e7975616e2d363462343138666430353263303333623232386530346263373762626334623534666437663562632e706e67"><img src="https://camo.githubusercontent.com/4f7f1dcd99f1e24cb8787dc695f3fb708e2f9799fe3559366a3d58139e74491a/68747470733a2f2f647363616368652e74656e63656e742d636c6f75642e636e2f75706c6f61642f75706c6f616465722f68756e7975616e2d363462343138666430353263303333623232386530346263373762626334623534666437663562632e706e67" width="400" data-canonical-src="https://dscache.tencent-cloud.cn/upload/uploader/hunyuan-64b418fd052c033b228e04bc77bbc4b54fd7f5bc.png"></a> <br>
</p>
<p dir="auto">
    ü´£&nbsp;<a href="https://huggingface.co/tencent/Tencent-Hunyuan-Large" rel="nofollow"><b>Hugging Face</b></a>&nbsp;&nbsp; |  &nbsp;&nbsp;üñ•Ô∏è&nbsp;&nbsp;<a href="https://llm.hunyuan.tencent.com/" rel="nofollow"><b>official website</b></a>&nbsp;&nbsp;ÔΩú&nbsp;&nbsp;üïñ&nbsp;&nbsp; <a href="https://cloud.tencent.com/product/hunyuan" rel="nofollow"><b>HunyuanAPI</b></a>
</p><p dir="auto">
    <a href="https://arxiv.org/abs/2411.02265" rel="nofollow"><b>Technical Report</b></a>&nbsp;&nbsp;ÔΩú&nbsp;&nbsp; <a href="https://huggingface.co/spaces/tencent/Hunyuan-Large" rel="nofollow"><b>Demo</b></a>&nbsp;&nbsp;&nbsp;ÔΩú&nbsp;&nbsp; <a href="https://cloud.tencent.com/document/product/851/112032" rel="nofollow"><b>Tencent Cloud TI</b></a>&nbsp;&nbsp;&nbsp;</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Model Introduction</h2><a id="user-content-model-introduction" aria-label="Permalink: Model Introduction" href="#model-introduction"></a></p>
<p dir="auto">With the rapid development of artificial intelligence technology, large language models (LLMs) have made significant progress in fields such as natural language processing, computer vision, and scientific tasks. However, as the scale of these models increases, optimizing resource consumption while maintaining high performance has become a key challenge. To address this challenge, we have explored Mixture of Experts (MoE) models. The currently unveiled Hunyuan-Large (Hunyuan-MoE-A52B) model is the largest open-source Transformer-based MoE model in the industry, featuring a total of 389 billion parameters and 52 billion active parameters. This is currently the largest open-source Transformer-based MoE model in the industry, featuring a total of 389 billion parameters and 52 billion active parameters.</p>
<p dir="auto">By open-sourcing the Hunyuan-Large model and revealing related technical details, we hope to inspire more researchers with innovative ideas and collectively advance the progress and application of AI technology. We welcome you to join our open-source community to explore and optimize future AI models together!</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Introduction to Technical Advantages</h3><a id="user-content-introduction-to-technical-advantages" aria-label="Permalink: Introduction to Technical Advantages" href="#introduction-to-technical-advantages"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Model</h4><a id="user-content-model" aria-label="Permalink: Model" href="#model"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>High-Quality Synthetic Data</strong>: By enhancing training with synthetic data, Hunyuan-Large can learn richer representations, handle long-context inputs, and generalize better to unseen data.</p>
</li>
<li>
<p dir="auto"><strong>KV Cache Compression</strong>: Utilizes Grouped Query Attention (GQA) and Cross-Layer Attention (CLA) strategies to significantly reduce memory usage and computational overhead of KV caches, improving inference throughput.</p>
</li>
<li>
<p dir="auto"><strong>Expert-Specific Learning Rate Scaling</strong>: Sets different learning rates for different experts to ensure each sub-model effectively learns from the data and contributes to overall performance.</p>
</li>
<li>
<p dir="auto"><strong>Long-Context Processing Capability</strong>: The pre-trained model supports text sequences up to 256K, and the Instruct model supports up to 128K, significantly enhancing the ability to handle long-context tasks.</p>
</li>
<li>
<p dir="auto"><strong>Extensive Benchmarking</strong>: Conducts extensive experiments across various languages and tasks to validate the practical effectiveness and safety of Hunyuan-Large.</p>
</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">inference Framework</h4><a id="user-content-inference-framework" aria-label="Permalink: inference Framework" href="#inference-framework"></a></p>
<ul dir="auto">
<li>This open-source release offers two inference backend options tailored for the Hunyuan-Large model: the popular vLLM-backend and the TRT-LLM-backend. Both solutions include optimizations for enhanced performance. For instance, the introduction of a new CLA structure significantly reduces GPU memory usage, achieving a 50% savings in the KV-Cache portion, which ensures efficient handling of long text scenarios. Additionally, by employing FP8 quantization, we achieve a 50% reduction in memory usage compared to traditional FP16/BF16 quantization, while maintaining precision and resulting in a 70% increase in throughput. Meanwhile, by leveraging the efficient operators at the core of TRT-LLM, the performance of the TRT-LLM solution surpasses that of vLLM by over 30%. The TRT-LLM solution is widely used in Tencent's Hunyuan project. In this release, we are initially open-sourcing the vLLM solution, with plans to release the TRT-LLM solution in the near future.</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Training Framework</h4><a id="user-content-training-framework" aria-label="Permalink: Training Framework" href="#training-framework"></a></p>
<ul dir="auto">
<li>The Hunyuan-Large open-source model is fully compatible with the Hugging Face format, enabling researchers and developers to perform model fine-tuning using the hf-deepspeed framework. Additionally, we support training acceleration through the use of flash attention. To further assist in the adoption process, we have made the corresponding training scripts and model implementations publicly available to the community through this release, facilitating subsequent model training and fine-tuning operations based on these resources.</li>
</ul>

<p dir="auto"><h2 tabindex="-1" dir="auto">Related News</h2><a id="user-content-related-news" aria-label="Permalink: Related News" href="#related-news"></a></p>
<ul dir="auto">
<li>2024.11.5 <a href="https://cloud.tencent.com/product/ti" rel="nofollow">TI Platform</a> has integrated Hunyuan-Large model already, you can easily train and deploy it in just a few steps. Visit <a href="https://console.cloud.tencent.com/tione/v2/aimarket/detail/hunyuan_series?PublicAlgoGroupId=hunyuan-large-chat&amp;detailTab=demo" rel="nofollow">Chat with Hunyuan-Large</a> to experience real-time conversations with the model, and explore <a href="https://cloud.tencent.com/document/product/851/112032" rel="nofollow">Hunyuan-Large Best Practice on TI</a> to create your own customized Hunyuan-Large model.</li>
<li>2024.11.5 We have open-sourced <strong>Hunyuan-A52B-Pretrain</strong>, <strong>Hunyuan-A52B-Instruct</strong>, and <strong>Hunyuan-A52B-Instruct-FP8</strong> on Hugging Face. We also released a technical report and a training and inference operations manual, providing detailed information on the model's capabilities and the procedures for training and inference.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Benchmark Evaluation</h2><a id="user-content-benchmark-evaluation" aria-label="Permalink: Benchmark Evaluation" href="#benchmark-evaluation"></a></p>
<p dir="auto"><strong>Hunyuan-Large pre-trained model</strong> achieves the best overall performance compared to both Dense and MoE based
competitors having similar activated parameter sizes.  For aggregated benchmarks such as MMLU, MMLU-Pro, and CMMLU,
Hunyuan-Large consistently achieves the best performance, confirming its comprehensive abilities on aggregated tasks.
Hunyuan-Large also shows superior performance in commonsense understanding and reasoning, and classical NLP tasks
such as QA and reading comprehension tasks (e.g., CommonsenseQA, PIQA and TriviaQA).<br>
For the mathematics capability, Hunyuan-Large outperforms all baselines in math datasets of GSM8K and MATH,
and also gains the best results on CMATH in Chinese.We also observe that Hunyuan-Large achieves the overall
best performance in all Chinese tasks (e.g., CMMLU, C-Eval).</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>LLama3.1-405B</th>
<th>LLama3.1-70B</th>
<th>Mixtral-8x22B</th>
<th>DeepSeek-V2</th>
<th>Hunyuan-Large</th>
</tr>
</thead>
<tbody>
<tr>
<td>MMLU</td>
<td>85.2</td>
<td>79.3</td>
<td>77.8</td>
<td>78.5</td>
<td><strong>88.4</strong></td>
</tr>
<tr>
<td>MMLU-Pro</td>
<td><strong>61.6</strong></td>
<td>53.8</td>
<td>49.5</td>
<td>-</td>
<td>60.2</td>
</tr>
<tr>
<td>BBH</td>
<td>85.9</td>
<td>81.6</td>
<td>78.9</td>
<td>78.9</td>
<td><strong>86.3</strong></td>
</tr>
<tr>
<td>HellaSwag</td>
<td>-</td>
<td>-</td>
<td><strong>88.7</strong></td>
<td>87.8</td>
<td>86.8</td>
</tr>
<tr>
<td>CommonsenseQA</td>
<td>85.8</td>
<td>84.1</td>
<td>82.4</td>
<td>-</td>
<td><strong>92.9</strong></td>
</tr>
<tr>
<td>WinoGrande</td>
<td>86.7</td>
<td>85.3</td>
<td>85.0</td>
<td>84.9</td>
<td><strong>88.7</strong></td>
</tr>
<tr>
<td>PIQA</td>
<td>-</td>
<td>-</td>
<td>83.6</td>
<td>83.7</td>
<td><strong>88.3</strong></td>
</tr>
<tr>
<td>NaturalQuestions</td>
<td>-</td>
<td>-</td>
<td>39.6</td>
<td>38.7</td>
<td><strong>52.8</strong></td>
</tr>
<tr>
<td>DROP</td>
<td>84.8</td>
<td>79.6</td>
<td>80.4</td>
<td>80.1</td>
<td><strong>88.9</strong></td>
</tr>
<tr>
<td>ARC-C</td>
<td><strong>96.1</strong></td>
<td>92.9</td>
<td>91.2</td>
<td>92.4</td>
<td>95.0</td>
</tr>
<tr>
<td>TriviaQA</td>
<td>-</td>
<td>-</td>
<td>82.1</td>
<td>79.9</td>
<td><strong>89.2</strong></td>
</tr>
<tr>
<td>CMMLU</td>
<td>-</td>
<td>-</td>
<td>60.0</td>
<td>84.0</td>
<td><strong>90.2</strong></td>
</tr>
<tr>
<td>C-Eval</td>
<td>-</td>
<td>-</td>
<td>59.6</td>
<td>81.7</td>
<td><strong>91.9</strong></td>
</tr>
<tr>
<td>C3</td>
<td>-</td>
<td>-</td>
<td>71.4</td>
<td>77.4</td>
<td><strong>82.3</strong></td>
</tr>
<tr>
<td>GSM8K</td>
<td>89.0</td>
<td>83.7</td>
<td>83.7</td>
<td>79.2</td>
<td><strong>92.8</strong></td>
</tr>
<tr>
<td>MATH</td>
<td>53.8</td>
<td>41.4</td>
<td>42.5</td>
<td>43.6</td>
<td><strong>69.8</strong></td>
</tr>
<tr>
<td>CMATH</td>
<td>-</td>
<td>-</td>
<td>72.3</td>
<td>78.7</td>
<td><strong>91.3</strong></td>
</tr>
<tr>
<td>HumanEval</td>
<td>61.0</td>
<td>58.5</td>
<td>53.1</td>
<td>48.8</td>
<td><strong>71.4</strong></td>
</tr>
<tr>
<td>MBPP</td>
<td><strong>73.4</strong></td>
<td>68.6</td>
<td>64.2</td>
<td>66.6</td>
<td>72.6</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><strong>Hunyuan-Large-Instruct</strong> achieves consistent improvements on most types of tasks compared to LLMs having similar
activated parameters, indicating the effectiveness of our post-training.    Delving into the model performance
in different categories of benchmarks, we find that our instruct model achieves the best performance on MMLU and MATH dataset.<br>
Notably, on the MMLU dataset, our model demonstrates a significant improvement, outperforming the LLama3.1-405B model by 2.6%.<br>
This enhancement is not just marginal but indicative of the Hunyuan-Large-Instruct‚Äôs superior understanding and reasoning
capabilities across a wide array of language understanding tasks. The model‚Äôs prowess is further underscored in its performance
on the MATH dataset, where it surpasses the LLama3.1-405B by a notable margin of 3.6%.<br>
Remarkably, this leap in accuracy is achieved with only 52 billion activated parameters, underscoring the efficiency of our model.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>LLama3.1 405B Inst.</th>
<th>LLama3.1 70B Inst.</th>
<th>Mixtral 8x22B Inst.</th>
<th>DeepSeekV2.5 Chat</th>
<th>Hunyuan-Large Inst.</th>
</tr>
</thead>
<tbody>
<tr>
<td>MMLU</td>
<td>87.3</td>
<td>83.6</td>
<td>77.8</td>
<td>80.4</td>
<td><strong>89.9</strong></td>
</tr>
<tr>
<td>CMMLU</td>
<td>-</td>
<td>-</td>
<td>61.0</td>
<td>-</td>
<td><strong>90.4</strong></td>
</tr>
<tr>
<td>C-Eval</td>
<td>-</td>
<td>-</td>
<td>60.0</td>
<td>-</td>
<td><strong>88.6</strong></td>
</tr>
<tr>
<td>BBH</td>
<td>-</td>
<td>-</td>
<td>78.4</td>
<td>84.3</td>
<td><strong>89.5</strong></td>
</tr>
<tr>
<td>HellaSwag</td>
<td>-</td>
<td>-</td>
<td>86.0</td>
<td><strong>90.3</strong></td>
<td>88.5</td>
</tr>
<tr>
<td>ARC-C</td>
<td><strong>96.9</strong></td>
<td>94.8</td>
<td>90.0</td>
<td>-</td>
<td>94.6</td>
</tr>
<tr>
<td>GPQA_diamond</td>
<td><strong>51.1</strong></td>
<td>46.7</td>
<td>-</td>
<td>-</td>
<td>42.4</td>
</tr>
<tr>
<td>MATH</td>
<td>73.8</td>
<td>68.0</td>
<td>49.8</td>
<td>74.7</td>
<td><strong>77.4</strong></td>
</tr>
<tr>
<td>HumanEval</td>
<td>89.0</td>
<td>80.5</td>
<td>75.0</td>
<td>89.0</td>
<td><strong>90.0</strong></td>
</tr>
<tr>
<td>AlignBench</td>
<td>6.0</td>
<td>5.9</td>
<td>6.2</td>
<td>8.0</td>
<td><strong>8.3</strong></td>
</tr>
<tr>
<td>MT-Bench</td>
<td>9.1</td>
<td>8.8</td>
<td>8.1</td>
<td>9.0</td>
<td><strong>9.4</strong></td>
</tr>
<tr>
<td>IFEval strict-prompt</td>
<td><strong>86.0</strong></td>
<td>83.6</td>
<td>71.2</td>
<td>-</td>
<td>85.0</td>
</tr>
<tr>
<td>Arena-Hard</td>
<td>69.3</td>
<td>55.7</td>
<td>-</td>
<td>76.2</td>
<td><strong>81.8</strong></td>
</tr>
<tr>
<td>AlpacaEval-2.0</td>
<td>39.3</td>
<td>34.3</td>
<td>30.9</td>
<td>50.5</td>
<td><strong>51.8</strong></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto"><h2 tabindex="-1" dir="auto">Data</h2><a id="user-content-data" aria-label="Permalink: Data" href="#data"></a></p>
<p dir="auto">Hunyuan-Large provides model training-related processes. In this section, you can process the training data format for use in model training.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Training Data Format and Processing</h3><a id="user-content-training-data-format-and-processing" aria-label="Permalink: Training Data Format and Processing" href="#training-data-format-and-processing"></a></p>
<p dir="auto">Training data is processed into a messages format as follows. The default system prompt for training and inference is "You are a helpful assistant." Below are examples of single-turn and multi-turn data:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Single-Turn Data Format

messages = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Êµ∑Ê∞¥‰∏∫‰ªÄ‰πàÊòØÂí∏ÁöÑ&quot; },
    {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Êµ∑Ê∞¥ÊòØÂí∏ÁöÑ‰∏ªË¶ÅÊòØÂõ†‰∏∫ÂÖ∂‰∏≠Âê´ÊúâËÆ∏Â§öÊ∫∂Ëß£Âú®Ê∞¥‰∏≠ÁöÑÁõêÁ±ªÂíåÁüøÁâ©Ë¥®„ÄÇËøô‰∫õÁõêÁ±ªÂíåÁüøÁâ©Ë¥®Êù•Ëá™‰∫éÂú∞ÁêÉË°®Èù¢ÁöÑÂ≤©Áü≥ÂíåÂúüÂ£§‰∏≠ÁöÑÂåñÂ≠¶Áâ©Ë¥®ÔºåÈöèÁùÄÊó∂Èó¥ÁöÑÊé®ÁßªÔºåÂÆÉ‰ª¨Ë¢´Â∏¶Âà∞‰∫ÜÊµ∑Ê¥ã‰∏≠„ÄÇÂΩìÊµ∑Ê∞¥Ëí∏ÂèëÊó∂ÔºåÊ∞¥ÂàÜËí∏ÂèëÊéâ‰∫ÜÔºå‰ΩÜÁõêÁ±ªÂíåÁüøÁâ©Ë¥®‰ªçÁÑ∂ÁïôÂú®Ê∞¥‰∏≠ÔºåÂØºËá¥Êµ∑Ê∞¥ÂèòÂæóÊõ¥Âä†Âí∏Âë≥„ÄÇÂõ†Ê≠§ÔºåÊµ∑Ê∞¥ÁöÑÂí∏Â∫¶ÊòØÁî±ÂÖ∂‰∏≠ÁöÑÁõêÁ±ªÂíåÁüøÁâ©Ë¥®ÁöÑÂê´ÈáèÂÜ≥ÂÆöÁöÑ„ÄÇ&quot;}
]

# Multi-Turn Data Format
messages = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Áü•ÈÅìÈáçÂ∫ÜÊ£ÆÊûóËøôÈÉ®ÁîµÂΩ±ÂêóÔºü&quot; },
    {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Áü•ÈÅìÂëÄÔºåÊòØ‰∏ÄÈÉ®Áî±ÁéãÂÆ∂Âç´ÂØºÊºîÁöÑÁâáÂ≠ê„ÄÇ&quot; },
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot; ËÄå‰∏ªÊºîÈáåÊõ¥ÊòØÊúâÁéãËè≤Ôºå‰∏Ä‰∏äÊò†‰æøÂèóÂà∞ËøΩÊçß„ÄÇ&quot; },
    {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;ÊâÄ‰ª•Ê≠§ÁâáËé∑Âæó‰∫ÜÁ¨¨14Â±äÈ¶ôÊ∏ØÁîµÂΩ±ÈáëÂÉèÂ•ñÊúÄ‰Ω≥ÂΩ±ÁâáÂ•ñ„ÄÇ&quot;}
]

from tokenizers import ByteLevelBPETokenizer
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;./models&quot;, use_fast=False, trust_remote_code=True)

ids = tokenizer.apply_chat_template(messages)
"><pre><span># Single-Turn Data Format</span>

<span>messages</span> <span>=</span> [
    {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>"You are a helpful assistant."</span>},
    {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>"Êµ∑Ê∞¥‰∏∫‰ªÄ‰πàÊòØÂí∏ÁöÑ"</span> },
    {<span>"role"</span>: <span>"assistant"</span>, <span>"content"</span>: <span>"Êµ∑Ê∞¥ÊòØÂí∏ÁöÑ‰∏ªË¶ÅÊòØÂõ†‰∏∫ÂÖ∂‰∏≠Âê´ÊúâËÆ∏Â§öÊ∫∂Ëß£Âú®Ê∞¥‰∏≠ÁöÑÁõêÁ±ªÂíåÁüøÁâ©Ë¥®„ÄÇËøô‰∫õÁõêÁ±ªÂíåÁüøÁâ©Ë¥®Êù•Ëá™‰∫éÂú∞ÁêÉË°®Èù¢ÁöÑÂ≤©Áü≥ÂíåÂúüÂ£§‰∏≠ÁöÑÂåñÂ≠¶Áâ©Ë¥®ÔºåÈöèÁùÄÊó∂Èó¥ÁöÑÊé®ÁßªÔºåÂÆÉ‰ª¨Ë¢´Â∏¶Âà∞‰∫ÜÊµ∑Ê¥ã‰∏≠„ÄÇÂΩìÊµ∑Ê∞¥Ëí∏ÂèëÊó∂ÔºåÊ∞¥ÂàÜËí∏ÂèëÊéâ‰∫ÜÔºå‰ΩÜÁõêÁ±ªÂíåÁüøÁâ©Ë¥®‰ªçÁÑ∂ÁïôÂú®Ê∞¥‰∏≠ÔºåÂØºËá¥Êµ∑Ê∞¥ÂèòÂæóÊõ¥Âä†Âí∏Âë≥„ÄÇÂõ†Ê≠§ÔºåÊµ∑Ê∞¥ÁöÑÂí∏Â∫¶ÊòØÁî±ÂÖ∂‰∏≠ÁöÑÁõêÁ±ªÂíåÁüøÁâ©Ë¥®ÁöÑÂê´ÈáèÂÜ≥ÂÆöÁöÑ„ÄÇ"</span>}
]

<span># Multi-Turn Data Format</span>
<span>messages</span> <span>=</span> [
    {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>"You are a helpful assistant."</span>},
    {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>"Áü•ÈÅìÈáçÂ∫ÜÊ£ÆÊûóËøôÈÉ®ÁîµÂΩ±ÂêóÔºü"</span> },
    {<span>"role"</span>: <span>"assistant"</span>, <span>"content"</span>: <span>"Áü•ÈÅìÂëÄÔºåÊòØ‰∏ÄÈÉ®Áî±ÁéãÂÆ∂Âç´ÂØºÊºîÁöÑÁâáÂ≠ê„ÄÇ"</span> },
    {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>" ËÄå‰∏ªÊºîÈáåÊõ¥ÊòØÊúâÁéãËè≤Ôºå‰∏Ä‰∏äÊò†‰æøÂèóÂà∞ËøΩÊçß„ÄÇ"</span> },
    {<span>"role"</span>: <span>"assistant"</span>, <span>"content"</span>: <span>"ÊâÄ‰ª•Ê≠§ÁâáËé∑Âæó‰∫ÜÁ¨¨14Â±äÈ¶ôÊ∏ØÁîµÂΩ±ÈáëÂÉèÂ•ñÊúÄ‰Ω≥ÂΩ±ÁâáÂ•ñ„ÄÇ"</span>}
]

<span>from</span> <span>tokenizers</span> <span>import</span> <span>ByteLevelBPETokenizer</span>
<span>from</span> <span>transformers</span> <span>import</span> <span>AutoTokenizer</span>

<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>"./models"</span>, <span>use_fast</span><span>=</span><span>False</span>, <span>trust_remote_code</span><span>=</span><span>True</span>)

<span>ids</span> <span>=</span> <span>tokenizer</span>.<span>apply_chat_template</span>(<span>messages</span>)</pre></div>
<p dir="auto">For more usage references, see the <code>./models/test.py</code> file.</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto">You can quickly get started by referring to the content in the <a href="https://github.com/Tencent/Tencent-Hunyuan-Large/blob/main/examples/README.md">Quick Start Guide</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Model Training</h2><a id="user-content-model-training" aria-label="Permalink: Model Training" href="#model-training"></a></p>
<p dir="auto">To simplify the Training process, HunyuanLLM provides a pre-built Docker image:</p>
<p dir="auto"><a href="https://hub.docker.com/repository/docker/hunyuaninfer/hunyuan-large/general" rel="nofollow">hunyuaninfer/hunyuan-large</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hardware Requirements</h3><a id="user-content-hardware-requirements" aria-label="Permalink: Hardware Requirements" href="#hardware-requirements"></a></p>
<p dir="auto">Tested on H20, without enabling <code>make_moe_param_leaf_module</code> and using <code>zero3+offload</code>, with a <code>max_seq_length</code> of 2048, full fine-tuning requires at least 32 GPUs, and LoRA fine-tuning requires at least 8 GPUs.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Training Performance</h3><a id="user-content-training-performance" aria-label="Permalink: Training Performance" href="#training-performance"></a></p>
<p dir="auto">With the minimum configuration (8 GPUs for LoRA fine-tuning), <code>per_device_train_batch_size</code> is set to 1, and <code>gradient_accumulation_steps</code> is set to 1, resulting in approximately 35 seconds per iteration.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Launch Method</h3><a id="user-content-launch-method" aria-label="Permalink: Launch Method" href="#launch-method"></a></p>
<p dir="auto">Refer to: <a href="https://huggingface.co/docs/transformers/v4.19.2/en/main_classes/trainer" rel="nofollow">HuggingFace Transformers Trainer</a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Single-Machine Training</h4><a id="user-content-single-machine-training" aria-label="Permalink: Single-Machine Training" href="#single-machine-training"></a></p>
<p dir="auto">In the <code>train</code> directory, execute:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt
bash train.sh"><pre>pip install -r requirements.txt
bash train.sh</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Multi-Machine Training</h4><a id="user-content-multi-machine-training" aria-label="Permalink: Multi-Machine Training" href="#multi-machine-training"></a></p>
<p dir="auto">To start training on multiple machines, follow the steps below and ensure that all machines are within the same cluster.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Configure Passwordless SSH Login Between Machines</h5><a id="user-content-configure-passwordless-ssh-login-between-machines" aria-label="Permalink: Configure Passwordless SSH Login Between Machines" href="#configure-passwordless-ssh-login-between-machines"></a></p>
<p dir="auto">The following steps use two machines as an example, with their IPs represented as <code>${ip1}</code> and <code>${ip2}</code>. These operations are performed within a Docker container.</p>
<p dir="auto">First, configure passwordless SSH between containers on each machine.</p>
<div dir="auto" data-snippet-clipboard-copy-content="ssh-keygen			# Generate id_rsa and id_rsa.pub for passwordless login
ssh-keygen -t rsa -A    # Generate /etc/ssh/ssh_host_rsa_key and ssh_host_ecdsa_key for starting 'SSH listen' later
/usr/sbin/sshd -p 36005 -o ListenAddress=0.0.0.0        # Start SSH listen
echo &quot;Port 36005&quot; > ~/.ssh/config   # Change SSH connection port to 36005
passwd root    # Set root password to avoid alerts from monitoring platforms"><pre>ssh-keygen			<span><span>#</span> Generate id_rsa and id_rsa.pub for passwordless login</span>
ssh-keygen -t rsa -A    <span><span>#</span> Generate /etc/ssh/ssh_host_rsa_key and ssh_host_ecdsa_key for starting 'SSH listen' later</span>
/usr/sbin/sshd -p 36005 -o ListenAddress=0.0.0.0        <span><span>#</span> Start SSH listen</span>
<span>echo</span> <span><span>"</span>Port 36005<span>"</span></span> <span>&gt;</span> <span>~</span>/.ssh/config   <span><span>#</span> Change SSH connection port to 36005</span>
passwd root    <span><span>#</span> Set root password to avoid alerts from monitoring platforms</span></pre></div>
<p dir="auto">Note: The <code>36005</code> here is an example. You can choose any port, but ensure that the port is <strong>open</strong> and <strong>not occupied by other processes</strong>.</p>
<p dir="auto">Next, within the container on each machine, execute:</p>

<p dir="auto"><strong>Copy the output SSH public key and paste it into the <code>~/.ssh/authorized_keys</code> file, with one public key per line. This must be done on every machine.</strong> Ultimately, the <code>~/.ssh/authorized_keys</code> file on each machine should be identical and contain the public keys of all machines.</p>
<p dir="auto">It's important to note that during multi-node training, the code executed on each node must be consistent. It is recommended to mount a shared network drive. If mounting a shared drive is not possible, you need to manually copy the dataset, scripts, and code to the same directory on all machines.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Start Multi-Machine Training</h5><a id="user-content-start-multi-machine-training" aria-label="Permalink: Start Multi-Machine Training" href="#start-multi-machine-training"></a></p>
<p dir="auto">Once the preparation steps are completed and dependencies are confirmed to be installed (if not, execute <code>pip install -r requirements.txt</code> to install), you can add the following configuration at the beginning of <code>train.sh</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export HOST_GPU_NUM=8
# Current machine IP
export LOCAL_IP=${ip1}
# Multi-node machine IPs, separated by commas
export NODE_IP_LIST=&quot;${ip1}:8,${ip2}:8&quot;
# Number of machine nodes
export NODES=2
export NODE_NUM=$((${NODES} * ${HOST_GPU_NUM}))"><pre><span>export</span> HOST_GPU_NUM=8
<span><span>#</span> Current machine IP</span>
<span>export</span> LOCAL_IP=<span>${ip1}</span>
<span><span>#</span> Multi-node machine IPs, separated by commas</span>
<span>export</span> NODE_IP_LIST=<span><span>"</span><span>${ip1}</span>:8,<span>${ip2}</span>:8<span>"</span></span>
<span><span>#</span> Number of machine nodes</span>
<span>export</span> NODES=2
<span>export</span> NODE_NUM=<span><span>$((</span><span>${NODES}</span> <span>*</span> <span>${HOST_GPU_NUM}</span><span>))</span></span></pre></div>
<p dir="auto">Note: Replace <code>${ip1}</code> and <code>${ip2}</code> with the actual IP addresses!</p>
<p dir="auto">Then, on the machine with <code>${ip1}</code>, execute <code>bash train.sh</code> in the <code>train/</code> directory. Note that on the first run, you might see the following output:</p>
<div data-snippet-clipboard-copy-content="The authenticity of host '[ip]:36005 ([ip]:36005)' can't be established.
ECDSA key fingerprint is xxxxxx.
ECDSA key fingerprint is MD5:xxxxxx.
Are you sure you want to continue connecting (yes/no)?"><pre lang="ssh"><code>The authenticity of host '[ip]:36005 ([ip]:36005)' can't be established.
ECDSA key fingerprint is xxxxxx.
ECDSA key fingerprint is MD5:xxxxxx.
Are you sure you want to continue connecting (yes/no)?
</code></pre></div>
<p dir="auto">At this point, type <code>yes</code> to continue.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Key Parameters</h5><a id="user-content-key-parameters" aria-label="Permalink: Key Parameters" href="#key-parameters"></a></p>
<p dir="auto">The key parameters in the script are as follows:</p>
<ul dir="auto">
<li><code>--deepspeed</code>: This parameter should point to a DeepSpeed configuration file. The <code>train</code> folder provides three default DeepSpeed configuration files: <code>ds_zero2_no_offload.json</code>, <code>ds_zero3_no_offload.json</code>, <code>ds_zero3_offload.json</code>. The required GPU memory decreases in this order.</li>
<li><code>--model_name_or_path</code>: The path to the HF pre-trained model. Ensure this path contains the <code>modeling_hunyuan.py</code> and <code>configuration_hunyuan.py</code> files; otherwise, it cannot be loaded.</li>
<li><code>--tokenizer_name_or_path</code>: The path to the tokenizer folder. Ensure this path contains the <code>tokenization_hy.py</code> file; otherwise, it cannot be loaded.</li>
<li><code>--train_data_file</code>: The path to the training file, which should be a JSONL file.</li>
<li><code>--output_dir</code>: The output directory where logs, tensorboard files, and model weights will be stored.</li>
<li><code>--per_device_train_batch_size</code>: The batch size per GPU.</li>
<li><code>--gradient_accumulation_steps</code>: The number of gradient accumulation steps. The global batch size is <code>per_device_train_batch_size * gradient_accumulation_steps * dp_size</code>.</li>
<li><code>--max_steps</code>: The total number of training steps.</li>
<li><code>--save_steps</code>: The number of steps between saving checkpoints.</li>
<li><code>--use_lora</code>: Whether to use LoRA for training. This also accepts <code>--lora_rank</code>, <code>--lora_alpha</code>, and <code>--lora_dropout</code> parameters. LoRA is applied by default to the 'q_proj', 'k_proj', 'v_proj', 'o_proj' parameters. If you need to change this, modify it in the code. Note: <strong>When using LoRA for training, only the LoRA weights are saved, not the base model weights</strong>. If you need to merge LoRA weights, see the "LoRA Weight Merging" section below.</li>
<li><code>--make_moe_param_leaf_module</code>: When using zero3 and MoE training, treat the MoE module as a leaf module, meaning its parameters are not split by zero3. This option is expected to significantly increase memory usage.</li>
<li><code>--gradient_checkpointing</code>: Enable gradient checkpointing.</li>
<li><code>--train_attention_params_only</code>: Whether to train only the attention parameters.</li>
<li><code>--learning_rate</code>: The maximum learning rate during training.</li>
<li><code>--min_lr</code>: The minimum learning rate during training.</li>
<li><code>--use_flash_attn</code>: ÂºÄÂêØ flash-attention ËøõË°åËÆ≠ÁªÉÂä†ÈÄü</li>
</ul>
<p dir="auto"><strong>Note:</strong></p>
<ul dir="auto">
<li>If you want to continue training from a previously saved checkpoint instead of loading pre-trained weights, specify <code>--resume_from_checkpoint</code> with the path to the checkpoint from the previous training. Do not specify <code>--model_name_or_path</code>, as this will only load the weights and not the training state.</li>
<li>When continuing training from a checkpoint, there might be slight deviations in loss due to randomness introduced by some non-deterministic algorithms, which is considered normal. Refer to: <a href="https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#randomness" rel="nofollow">HuggingFace Transformers Trainer Randomness</a></li>
<li>When <code>--model_name_or_path</code> is specified, all model-related parameters will be ignored.</li>
<li>Samples within a batch will be padded to align with the longest sample in the batch, with each sample having a maximum length of <code>max_seq_length</code>. Any excess will be truncated.</li>
<li>If you encounter warnings about bias weights not being loaded, you can ignore them, as biases are not used in Hunyuan-Large.</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">What to Do If Out of Memory?</h4><a id="user-content-what-to-do-if-out-of-memory" aria-label="Permalink: What to Do If Out of Memory?" href="#what-to-do-if-out-of-memory"></a></p>
<p dir="auto">Refer to: <a href="https://www.deepspeed.ai/docs/config-json/" rel="nofollow">DeepSpeed Configuration</a></p>
<p dir="auto">You can try modifying the DeepSpeed configuration by removing the auto attribute from these parameters and reducing their values:</p>
<ul dir="auto">
<li><code>stage3_param_persistence_threshold</code></li>
<li><code>stage3_prefetch_bucket_size</code></li>
<li><code>stage3_max_reuse_distance</code></li>
<li><code>stage3_max_reuse_distance</code></li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Merging LoRA Models</h4><a id="user-content-merging-lora-models" aria-label="Permalink: Merging LoRA Models" href="#merging-lora-models"></a></p>
<p dir="auto">The saved LoRA weights cannot be merged into the zero3 model during training because, with zero3 enabled, model weights are split across different data parallel ranks. If you want to merge LoRA weights into the base model, you can do so offline to obtain the merged weight file. Execute <code>merge_lora_weight.sh</code> to merge the LoRA weights with the base model weights. The parameters include:</p>
<ul dir="auto">
<li><code>--base_model_path</code>: Directory of the base model weights</li>
<li><code>--adapter_model_path</code>: Directory of the LoRA weights</li>
<li><code>--output_path</code>: Directory to save the merged weights</li>
<li><code>--save_dtype</code>: Data format for storing the merged weights, available options include: fp16, bf16, fp32</li>
</ul>

<p dir="auto"><h2 tabindex="-1" dir="auto">Inference and Deployment</h2><a id="user-content-inference-and-deployment" aria-label="Permalink: Inference and Deployment" href="#inference-and-deployment"></a></p>
<p dir="auto">HunyuanLLM uses TRT-LLM and vLLM for deployment. We are open sourcing the vLLM deployment (see Reasoning with vLLM), and the TRT-LLM deployment (see Reasoning with TRT-LLM) will be available in the near future.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Using TRT-LLM for Inference</h2><a id="user-content-using-trt-llm-for-inference" aria-label="Permalink: Using TRT-LLM for Inference" href="#using-trt-llm-for-inference"></a></p>
<p dir="auto">To be opened</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Using vLLM for Inference</h2><a id="user-content-using-vllm-for-inference" aria-label="Permalink: Using vLLM for Inference" href="#using-vllm-for-inference"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Docker:</h3><a id="user-content-docker" aria-label="Permalink: Docker:" href="#docker"></a></p>
<p dir="auto">To simplify the deployment process, HunyuanLLM provides a pre-built Docker image:</p>
<p dir="auto"><a href="https://hub.docker.com/repository/docker/hunyuaninfer/hunyuan-large/general" rel="nofollow">hunyuaninfer/hunyuan-large</a>. You only need to download the model files and start the Docker container using the code below to begin model inference.</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker run --name hunyuanLLM_infer -itd --privileged --user root --net=host --ipc=host --gpus=8 hunyuaninfer/hunyuan-large:infer-open-source"><pre>docker run --name hunyuanLLM_infer -itd --privileged --user root --net=host --ipc=host --gpus=8 hunyuaninfer/hunyuan-large:infer-open-source</pre></div>
<p dir="auto">Note: Docker container privilege management. The above code uses privileged mode (<code>--privileged</code>) to start the Docker container, which grants the container higher privileges, increasing the risk of data leakage and cluster security threats. It is recommended to avoid using privileged mode unless necessary to reduce security risks. For scenarios where privileged mode is required, conduct a thorough security assessment and implement appropriate security monitoring and hardening measures.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configure Passwordless SSH Login Between Machines</h3><a id="user-content-configure-passwordless-ssh-login-between-machines-1" aria-label="Permalink: Configure Passwordless SSH Login Between Machines" href="#configure-passwordless-ssh-login-between-machines-1"></a></p>
<p dir="auto">The following steps use two machines as an example, with their IPs represented as <code>${ip1}</code> and <code>${ip2}</code>. These operations are performed within a Docker container.</p>
<p dir="auto">First, run <code>passwd</code> on both machines to set a password, for example: <code>Tmp123,./</code></p>
<p dir="auto">Copy <code>inference/login_ssh.py</code> into the container and execute the following command, ensuring the IP and password are correctly entered.</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 login_ssh.py --ips ${ip1},${ip2} --port 36000 --password=Tmp123,./"><pre>python3 login_ssh.py --ips <span>${ip1}</span>,<span>${ip2}</span> --port 36000 --password=Tmp123,./</pre></div>
<p dir="auto"><strong>Note üì¢: Before starting, be sure to verify multi-machine communication using VLLM's debugging script: <a href="https://docs.vllm.ai/en/latest/getting_started/debugging.html" rel="nofollow">https://docs.vllm.ai/en/latest/getting_started/debugging.html</a></strong></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">BF16 Deployment</h3><a id="user-content-bf16-deployment" aria-label="Permalink: BF16 Deployment" href="#bf16-deployment"></a></p>
<p dir="auto">BF16 requires 16 H800 or H20 GPUs for deployment. After verifying that multi-machine communication is correct, execute the following steps:</p>
<p dir="auto">Before running the commands, set the following environment variables:</p>
<div dir="auto" data-snippet-clipboard-copy-content="${LOCAL_IP}: The IP corresponding to bond1 on the current machine
${MODEL_PATH}: Path to the Hunyuan LLM model"><pre><span>${LOCAL_IP}</span>: The IP corresponding to bond1 on the current machine
<span>${MODEL_PATH}</span>: Path to the Hunyuan LLM model</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Step 1: Start Ray</h4><a id="user-content-step-1-start-ray" aria-label="Permalink: Step 1: Start Ray" href="#step-1-start-ray"></a></p>
<p dir="auto">Ray is an open-source library for parallel and distributed Python. In this section, we use Ray to achieve multi-machine communication.</p>
<p dir="auto">Ray Component Configuration Hardening: The default configuration of Ray components does not enable authentication mechanisms for service ports (e.g., 6379, 8265), posing risks of unauthorized access and command execution. It is recommended to deploy Ray components only in trusted internal network environments or ensure strict access control list (ACL) policies are implemented for these ports to prevent unauthorized network access.</p>
<p dir="auto">First, start Ray on each node (either in the background or by keeping the terminal running):</p>
<p dir="auto">On the head node:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export VLLM_HOST_IP=${LOCAL_IP}
export NCCL_SOCKET_IFNAME=bond1
export GLOO_SOCKET_IFNAME=bond1
ray start --block --head --node-ip-address=${LOCAL_IP} --port=6379"><pre><span>export</span> VLLM_HOST_IP=<span>${LOCAL_IP}</span>
<span>export</span> NCCL_SOCKET_IFNAME=bond1
<span>export</span> GLOO_SOCKET_IFNAME=bond1
ray start --block --head --node-ip-address=<span>${LOCAL_IP}</span> --port=6379</pre></div>
<p dir="auto">On all worker nodes:</p>
<p dir="auto">Note: Replace <code>{HEAD NODE $LOCAL_IP}</code> with the actual <code>${LOCAL_IP}</code> of the head node.</p>
<div dir="auto" data-snippet-clipboard-copy-content="export VLLM_HOST_IP=${LOCAL_IP}
export NCCL_SOCKET_IFNAME=bond1
export GLOO_SOCKET_IFNAME=bond1
ray start --block --address={HEAD NODE $LOCAL_IP}:6379 --node-ip-address=${LOCAL_IP}"><pre><span>export</span> VLLM_HOST_IP=<span>${LOCAL_IP}</span>
<span>export</span> NCCL_SOCKET_IFNAME=bond1
<span>export</span> GLOO_SOCKET_IFNAME=bond1
ray start --block --address={HEAD NODE <span>$LOCAL_IP</span>}:6379 --node-ip-address=<span>${LOCAL_IP}</span></pre></div>
<p dir="auto">If Ray fails to start, execute <code>ray stop</code> and then run the above commands again.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Step 2: Execute Inference</h4><a id="user-content-step-2-execute-inference" aria-label="Permalink: Step 2: Execute Inference" href="#step-2-execute-inference"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Method 1: Command Line Inference</h4><a id="user-content-method-1-command-line-inference" aria-label="Permalink: Method 1: Command Line Inference" href="#method-1-command-line-inference"></a></p>
<p dir="auto">Below is a code snippet demonstrating how to quickly request the chat model using <code>vLLM</code>:</p>
<p dir="auto">Note: vLLM Component Remote Code Execution Protection. In the code below, if the <code>trust-remote-code</code> configuration option of the vLLM component is enabled, it will allow loading and executing code from remote model repositories, which may lead to the execution of malicious code. Unless explicitly required by business needs, it is recommended to keep this configuration option disabled to reduce potential security threats.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import os
from vllm import LLM, SamplingParams

model_path=os.environ.get('MODEL_PATH')

llm = LLM(model=model_path,
        tokenizer=model_path,
        trust_remote_code=True,
        max_model_len=10240,
        dtype='bfloat16',
        tensor_parallel_size=16,
        pipeline_parallel_size=1,
        disable_log_stats=False,
        gpu_memory_utilization=0.98,
        disable_custom_all_reduce=True,
        #distributed_executor_backend='ray',
        enforce_eager=True,
        max_num_seqs=8,
        use_v2_block_manager=True,
        quantization=None)

prompts = [&quot;Êµ∑Ê∞¥‰∏∫‰ªÄ‰πàÊòØÂí∏ÁöÑ&quot;]

sampling_params = SamplingParams(
    temperature=0.7, top_p=0.6, max_tokens=200, top_k=20, repetition_penalty=1.05)

outputs = llm.generate(prompts, sampling_params)

# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f&quot;Prompt: {prompt!r}, Generated text: {generated_text!r}&quot;)"><pre><span>import</span> <span>os</span>
<span>from</span> <span>vllm</span> <span>import</span> <span>LLM</span>, <span>SamplingParams</span>

<span>model_path</span><span>=</span><span>os</span>.<span>environ</span>.<span>get</span>(<span>'MODEL_PATH'</span>)

<span>llm</span> <span>=</span> <span>LLM</span>(<span>model</span><span>=</span><span>model_path</span>,
        <span>tokenizer</span><span>=</span><span>model_path</span>,
        <span>trust_remote_code</span><span>=</span><span>True</span>,
        <span>max_model_len</span><span>=</span><span>10240</span>,
        <span>dtype</span><span>=</span><span>'bfloat16'</span>,
        <span>tensor_parallel_size</span><span>=</span><span>16</span>,
        <span>pipeline_parallel_size</span><span>=</span><span>1</span>,
        <span>disable_log_stats</span><span>=</span><span>False</span>,
        <span>gpu_memory_utilization</span><span>=</span><span>0.98</span>,
        <span>disable_custom_all_reduce</span><span>=</span><span>True</span>,
        <span>#distributed_executor_backend='ray',</span>
        <span>enforce_eager</span><span>=</span><span>True</span>,
        <span>max_num_seqs</span><span>=</span><span>8</span>,
        <span>use_v2_block_manager</span><span>=</span><span>True</span>,
        <span>quantization</span><span>=</span><span>None</span>)

<span>prompts</span> <span>=</span> [<span>"Êµ∑Ê∞¥‰∏∫‰ªÄ‰πàÊòØÂí∏ÁöÑ"</span>]

<span>sampling_params</span> <span>=</span> <span>SamplingParams</span>(
    <span>temperature</span><span>=</span><span>0.7</span>, <span>top_p</span><span>=</span><span>0.6</span>, <span>max_tokens</span><span>=</span><span>200</span>, <span>top_k</span><span>=</span><span>20</span>, <span>repetition_penalty</span><span>=</span><span>1.05</span>)

<span>outputs</span> <span>=</span> <span>llm</span>.<span>generate</span>(<span>prompts</span>, <span>sampling_params</span>)

<span># Print the outputs.</span>
<span>for</span> <span>output</span> <span>in</span> <span>outputs</span>:
    <span>prompt</span> <span>=</span> <span>output</span>.<span>prompt</span>
    <span>generated_text</span> <span>=</span> <span>output</span>.<span>outputs</span>[<span>0</span>].<span>text</span>
    <span>print</span>(<span>f"Prompt: <span><span>{</span><span>prompt</span>!r<span>}</span></span>, Generated text: <span><span>{</span><span>generated_text</span>!r<span>}</span></span>"</span>)</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Method 2: Service-Based Inference</h4><a id="user-content-method-2-service-based-inference" aria-label="Permalink: Method 2: Service-Based Inference" href="#method-2-service-based-inference"></a></p>
<p dir="auto">Below we demonstrate how to deploy the model using <code>vLLM</code> in a service-based manner and make requests.</p>
<p dir="auto">Run the following on the head node:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export VLLM_HOST_IP=${LOCAL_IP}
export NCCL_SOCKET_IFNAME=bond1
export GLOO_SOCKET_IFNAME=bond1"><pre><span>export</span> VLLM_HOST_IP=<span>${LOCAL_IP}</span>
<span>export</span> NCCL_SOCKET_IFNAME=bond1
<span>export</span> GLOO_SOCKET_IFNAME=bond1</pre></div>
<p dir="auto">Next, start the service by running:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd inference
sh run_server.sh"><pre><span>cd</span> inference
sh run_server.sh</pre></div>
<p dir="auto"><em>Tips</em>: Troubleshooting, if you encounter the following error:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ray.exceptions.RaySystemError: System error: No module named 'transformers_modules' traceback: Traceback (most recent call last):
ModuleNotFoundError: No module named 'transformers_modules'"><pre><span>ray</span>.<span>exceptions</span>.<span>RaySystemError</span>: <span>System</span> <span>error</span>: <span>No</span> <span>module</span> <span>named</span> <span>'transformers_modules'</span> <span>traceback</span>: <span>Traceback</span> (<span>most</span> <span>recent</span> <span>call</span> <span>last</span>):
<span>ModuleNotFoundError</span>: <span>No</span> <span>module</span> <span>named</span> <span>'transformers_modules'</span></pre></div>
<p dir="auto">Copy the <code>~/.cache/huggingface/modules/</code> directory from the head node to the corresponding path on all worker nodes.</p>
<p dir="auto">After successfully running <code>run_server.sh</code>, execute the request script:</p>

<p dir="auto">Be sure to modify <code>${LOCAL_IP}</code> and <code>${MODEL_PATH}</code> in <code>openapi.sh</code> to values match the corresponding service.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quantized Model Deployment:</h3><a id="user-content-quantized-model-deployment" aria-label="Permalink: Quantized Model Deployment:" href="#quantized-model-deployment"></a></p>
<p dir="auto">This section describes the process of deploying a quantized model using vLLM.</p>
<p dir="auto">Image: The deployment image is the same as for BF16.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Int8 Quantized Model Deployment:</h4><a id="user-content-int8-quantized-model-deployment" aria-label="Permalink: Int8 Quantized Model Deployment:" href="#int8-quantized-model-deployment"></a></p>
<p dir="auto">To deploy the Int8-weight-only version of the Hunyuan-L model, simply set the environment variables in <code>run_server_int8.sh</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="${MODEL_PATH}: Path to the BF16 model
${LOCAL_IP}: The IP corresponding to bond1 on the current machine"><pre><span>${MODEL_PATH}</span>: Path to the BF16 model
<span>${LOCAL_IP}</span>: The IP corresponding to bond1 on the current machine</pre></div>
<p dir="auto">Then, start the Int8 service by running:</p>

<p dir="auto">After successfully running <code>run_server_int8.sh</code>, execute the request script:</p>

<p dir="auto"><h4 tabindex="-1" dir="auto">FP8 Quantized Model Deployment:</h4><a id="user-content-fp8-quantized-model-deployment" aria-label="Permalink: FP8 Quantized Model Deployment:" href="#fp8-quantized-model-deployment"></a></p>
<p dir="auto">To deploy the W8A8C8 version of the Hunyuan-L model, simply set the environment variables in <code>run_server_fp8.sh</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="${MODEL_PATH}: Path to the FP8 model
${LOCAL_IP}: The IP corresponding to bond1 on the current machine"><pre><span>${MODEL_PATH}</span>: Path to the FP8 model
<span>${LOCAL_IP}</span>: The IP corresponding to bond1 on the current machine</pre></div>
<p dir="auto">Then, start the FP8 service by running:</p>

<p dir="auto">After successfully running <code>run_server_fp8.sh</code>, execute the request script:</p>

<p dir="auto"><h4 tabindex="-1" dir="auto">FP8 BENCHMARK</h4><a id="user-content-fp8-benchmark" aria-label="Permalink: FP8 BENCHMARK" href="#fp8-benchmark"></a></p>
<p dir="auto">This part introduces the Benchmark of Hunyuan Large Instruct FP8 quantitative model.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Dataset</th>
<th>BF16</th>
<th>W8A8C8-FP8</th>
</tr>
</thead>
<tbody>
<tr>
<td>ARC-C</td>
<td>94.6</td>
<td>94.2</td>
</tr>
<tr>
<td>C-Eval</td>
<td>88.6</td>
<td>89.2</td>
</tr>
<tr>
<td>CMMLU</td>
<td>90.4</td>
<td>89.8</td>
</tr>
<tr>
<td>MMLU</td>
<td>89.9</td>
<td>88.9</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Inference Performance</h3><a id="user-content-inference-performance" aria-label="Permalink: Inference Performance" href="#inference-performance"></a></p>
<p dir="auto">This section presents the efficiency test results of deploying various models (original and quantized) using vLLM, including inference speed (tokens/s) under different batch sizes.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Inference Framework</th>
<th>Model</th>
<th>Number of GPUs (H20)</th>
<th>input_length</th>
<th>batch=1</th>
<th>batch=4</th>
</tr>
</thead>
<tbody>
<tr>
<td>vLLM</td>
<td>Hunyuan-Large</td>
<td>16</td>
<td>2048</td>
<td>20.2</td>
<td>75.5</td>
</tr>
<tr>
<td>vLLM</td>
<td>Hunyuan-Large(int8 weight only)</td>
<td>8</td>
<td>2048</td>
<td>19.3</td>
<td>73.6</td>
</tr>
<tr>
<td>vLLM</td>
<td>Hunyuan-Large(W8A8C8-FP8)</td>
<td>8</td>
<td>2048</td>
<td>19.8</td>
<td>74.9</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tokenizer</h2><a id="user-content-tokenizer" aria-label="Permalink: Tokenizer" href="#tokenizer"></a></p>
<p dir="auto">The tokenizer used in the HunYuan-Large model balances compression rate and effectiveness, ensuring that embeddings are sufficiently trained. The vocabulary includes 100K tokens integrated from tiktoken. Additionally, we trained an extra 29K Chinese tokens using a large amount of high-quality Chinese training data to enhance the model's Chinese capabilities and the tokenizer's compression rate. Combined, our new tokenizer improves the compression rate compared to the LLaMA3 tokenizer, increasing from 2.78 characters/token to 3.13 characters/token.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hunyuan API</h2><a id="user-content-hunyuan-api" aria-label="Permalink: Hunyuan API" href="#hunyuan-api"></a></p>
<p dir="auto">You can experience our Hunyuan-Large model on Tencent Cloud. For details, please visit: <a href="https://cloud.tencent.com/document/product/1729/97730" rel="nofollow">https://cloud.tencent.com/document/product/1729/97730</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Interactive Demo Web</h2><a id="user-content-interactive-demo-web" aria-label="Permalink: Interactive Demo Web" href="#interactive-demo-web"></a></p>
<p dir="auto">The Hunyuan-Large web demo is now open. Visit <a href="https://huggingface.co/spaces/tencent/Hunyuan-Large" rel="nofollow">https://huggingface.co/spaces/tencent/Hunyuan-Large</a> to easily experience our model.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Training/Inference on TI</h2><a id="user-content-traininginference-on-ti" aria-label="Permalink: Training/Inference on TI" href="#traininginference-on-ti"></a></p>
<p dir="auto">Tencent Cloud's <a href="https://cloud.tencent.com/product/ti" rel="nofollow">TI Platform</a> is a comprehensive machine learning platform tailored for AI engineers. With the Hunyuan-Large model already integrated, you can easily train and deploy it in just a few steps. Visit <a href="https://console.cloud.tencent.com/tione/v2/aimarket/detail/hunyuan_series?PublicAlgoGroupId=hunyuan-large-chat&amp;detailTab=demo" rel="nofollow">Chat with Hunyuan-Large</a> to experience real-time conversations with the model, and explore <a href="https://cloud.tencent.com/document/product/851/112032" rel="nofollow">Hunyuan-Large Best Practice on TI</a> to create your own customized Hunyuan-Large model.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you find our work helpful, feel free to give us a cite.</p>
<div data-snippet-clipboard-copy-content="@misc{sun2024hunyuanlargeopensourcemoemodel,
      title={Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent}, 
      author={Xingwu Sun and Yanfeng Chen and Yiqing Huang and Ruobing Xie and Jiaqi Zhu and Kai Zhang and Shuaipeng Li and Zhen Yang and Jonny Han and Xiaobo Shu and Jiahao Bu and Zhongzhi Chen and Xuemeng Huang and Fengzong Lian and Saiyong Yang and Jianfeng Yan and Yuyuan Zeng and Xiaoqin Ren and Chao Yu and Lulu Wu and Yue Mao and Tao Yang and Suncong Zheng and Kan Wu and Dian Jiao and Jinbao Xue and Xipeng Zhang and Decheng Wu and Kai Liu and Dengpeng Wu and Guanghui Xu and Shaohua Chen and Shuang Chen and Xiao Feng and Yigeng Hong and Junqiang Zheng and Chengcheng Xu and Zongwei Li and Xiong Kuang and Jianglu Hu and Yiqi Chen and Yuchi Deng and Guiyang Li and Ao Liu and Chenchen Zhang and Shihui Hu and Zilong Zhao and Zifan Wu and Yao Ding and Weichao Wang and Han Liu and Roberts Wang and Hao Fei and Peijie She and Ze Zhao and Xun Cao and Hai Wang and Fusheng Xiang and Mengyuan Huang and Zhiyuan Xiong and Bin Hu and Xuebin Hou and Lei Jiang and Jiajia Wu and Yaping Deng and Yi Shen and Qian Wang and Weijie Liu and Jie Liu and Meng Chen and Liang Dong and Weiwen Jia and Hu Chen and Feifei Liu and Rui Yuan and Huilin Xu and Zhenxiang Yan and Tengfei Cao and Zhichao Hu and Xinhua Feng and Dong Du and Tinghao She and Yangyu Tao and Feng Zhang and Jianchen Zhu and Chengzhong Xu and Xirui Li and Chong Zha and Wen Ouyang and Yinben Xia and Xiang Li and Zekun He and Rongpeng Chen and Jiawei Song and Ruibin Chen and Fan Jiang and Chongqing Zhao and Bo Wang and Hao Gong and Rong Gan and Winston Hu and Zhanhui Kang and Yong Yang and Yuhong Liu and Di Wang and Jie Jiang},
      year={2024},
      eprint={2411.02265},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.02265}, 
}"><pre><code>@misc{sun2024hunyuanlargeopensourcemoemodel,
      title={Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent}, 
      author={Xingwu Sun and Yanfeng Chen and Yiqing Huang and Ruobing Xie and Jiaqi Zhu and Kai Zhang and Shuaipeng Li and Zhen Yang and Jonny Han and Xiaobo Shu and Jiahao Bu and Zhongzhi Chen and Xuemeng Huang and Fengzong Lian and Saiyong Yang and Jianfeng Yan and Yuyuan Zeng and Xiaoqin Ren and Chao Yu and Lulu Wu and Yue Mao and Tao Yang and Suncong Zheng and Kan Wu and Dian Jiao and Jinbao Xue and Xipeng Zhang and Decheng Wu and Kai Liu and Dengpeng Wu and Guanghui Xu and Shaohua Chen and Shuang Chen and Xiao Feng and Yigeng Hong and Junqiang Zheng and Chengcheng Xu and Zongwei Li and Xiong Kuang and Jianglu Hu and Yiqi Chen and Yuchi Deng and Guiyang Li and Ao Liu and Chenchen Zhang and Shihui Hu and Zilong Zhao and Zifan Wu and Yao Ding and Weichao Wang and Han Liu and Roberts Wang and Hao Fei and Peijie She and Ze Zhao and Xun Cao and Hai Wang and Fusheng Xiang and Mengyuan Huang and Zhiyuan Xiong and Bin Hu and Xuebin Hou and Lei Jiang and Jiajia Wu and Yaping Deng and Yi Shen and Qian Wang and Weijie Liu and Jie Liu and Meng Chen and Liang Dong and Weiwen Jia and Hu Chen and Feifei Liu and Rui Yuan and Huilin Xu and Zhenxiang Yan and Tengfei Cao and Zhichao Hu and Xinhua Feng and Dong Du and Tinghao She and Yangyu Tao and Feng Zhang and Jianchen Zhu and Chengzhong Xu and Xirui Li and Chong Zha and Wen Ouyang and Yinben Xia and Xiang Li and Zekun He and Rongpeng Chen and Jiawei Song and Ruibin Chen and Fan Jiang and Chongqing Zhao and Bo Wang and Hao Gong and Rong Gan and Winston Hu and Zhanhui Kang and Yong Yang and Yuhong Liu and Di Wang and Jie Jiang},
      year={2024},
      eprint={2411.02265},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.02265}, 
}
</code></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Contact Us</h2><a id="user-content-contact-us" aria-label="Permalink: Contact Us" href="#contact-us"></a></p>
<p dir="auto">If you would like to leave a message for our R&amp;D and product teams, Welcome to contact our open-source team . You can also contact us via email (<a href="mailto:hunyuan_opensource@tencent.com">hunyuan_opensource@tencent.com</a>).</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Whirlwind ‚Äì Async concurrent hashmap for Rust (130 pts)]]></title>
            <link>https://github.com/fortress-build/whirlwind</link>
            <guid>42053747</guid>
            <pubDate>Tue, 05 Nov 2024 18:02:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/fortress-build/whirlwind">https://github.com/fortress-build/whirlwind</a>, See on <a href="https://news.ycombinator.com/item?id=42053747">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">üåÄ Whirlwind</h2><a id="user-content--whirlwind" aria-label="Permalink: üåÄ Whirlwind" href="#-whirlwind"></a></p>
<p dir="auto"><a href="https://github.com/yourusername/shardmap/actions"><img src="https://camo.githubusercontent.com/3d4342abdadc99b07e57b7ca52055dc301b3653c031e4c2aea260634adc71ad9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f666f7274726573732d6275696c642f776869726c77696e642f727573742e796d6c3f6272616e63683d6d61696e" alt="Build Status" data-canonical-src="https://img.shields.io/github/actions/workflow/status/fortress-build/whirlwind/rust.yml?branch=main"></a>
<a href="https://crates.io/crates/whirlwind" rel="nofollow"><img src="https://camo.githubusercontent.com/969ebd9c3027f52c2839134225f7037dc1fb82a0203a8d8e9cae1d66df8f0f32/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f776869726c77696e64" alt="Crates.io" data-canonical-src="https://img.shields.io/crates/v/whirlwind"></a>
<a href="https://docs.rs/whirlwind" rel="nofollow"><img src="https://camo.githubusercontent.com/76ace1e89bfc0634727a66633ac4a07f80da861497a48251bcb0fa59e266c7b7/68747470733a2f2f646f63732e72732f776869726c77696e642f62616467652e737667" alt="Docs.rs" data-canonical-src="https://docs.rs/whirlwind/badge.svg"></a>
<a href="https://github.com/fortress-build/whirlwind/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/940e860108f5b2c58bdd7f1551e99c79b31000843e59249a5e33ca94dcbbd173/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f6c2f776869726c77696e64" alt="License" data-canonical-src="https://img.shields.io/crates/l/whirlwind"></a></p>
<p dir="auto">An asynchronous, sharded <code>HashMap</code> for high-performance concurrent data access
in Rust.</p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">This crate is in development, and breaking changes may be made up until a 1.0 release.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">üìñ Table of Contents</h2><a id="user-content--table-of-contents" aria-label="Permalink: üìñ Table of Contents" href="#-table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#-features">Features</a></li>
<li><a href="#-installation">Installation</a></li>
<li><a href="#-usage">Usage</a></li>
<li><a href="#-examples">Examples</a></li>
<li><a href="#-benchmarks">Benchmark</a></li>
<li><a href="#-contributing">Contributing</a></li>
<li><a href="#license">License</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">‚ú® Features</h2><a id="user-content--features" aria-label="Permalink: ‚ú® Features" href="#-features"></a></p>
<ul dir="auto">
<li><strong>Async Ready</strong>: Seamless integration with Rust's <code>async</code>/<code>await</code> syntax.</li>
<li><strong>High Performance</strong>: Sharding minimizes lock contention in concurrent environments.</li>
<li><strong>Thread-safe</strong>: Safe for use across multiple threads without fear of data races.</li>
<li><strong>Familiar API</strong>: Intuitive <code>HashMap</code>-like interface for ease of adoption.</li>
<li><strong>Customizable Shards</strong>: Configure the number of shards to optimize for your workload.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">üì¶ Installation</h2><a id="user-content--installation" aria-label="Permalink: üì¶ Installation" href="#-installation"></a></p>
<p dir="auto">Add <code>whirlwind</code> to your <code>Cargo.toml</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[dependencies]
whirlwind = &quot;0.1.1&quot;"><pre>[<span>dependencies</span>]
<span>whirlwind</span> = <span><span>"</span>0.1.1<span>"</span></span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">üîß Usage</h2><a id="user-content--usage" aria-label="Permalink: üîß Usage" href="#-usage"></a></p>
<p dir="auto">Here's a quick example to get you started:</p>
<div dir="auto" data-snippet-clipboard-copy-content="use whirlwind::ShardMap;

#[tokio::main]
async fn main() {
    let map = ShardMap::new();

    map.insert(&quot;apple&quot;, 3).await;
    map.insert(&quot;banana&quot;, 5).await;

    if let Some(quantity) = map.get(&amp;&quot;apple&quot;).await {
        println!(&quot;We have {} apples!&quot;, quantity);
    }

    map.remove(&amp;&quot;banana&quot;).await;
}"><pre><span>use</span> whirlwind<span>::</span><span>ShardMap</span><span>;</span>

<span>#<span>[</span>tokio<span>::</span>main<span>]</span></span>
<span>async</span> <span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> map = <span>ShardMap</span><span>::</span><span>new</span><span>(</span><span>)</span><span>;</span>

    map<span>.</span><span>insert</span><span>(</span><span>"apple"</span><span>,</span> <span>3</span><span>)</span><span>.</span><span>await</span><span>;</span>
    map<span>.</span><span>insert</span><span>(</span><span>"banana"</span><span>,</span> <span>5</span><span>)</span><span>.</span><span>await</span><span>;</span>

    <span>if</span> <span>let</span> <span>Some</span><span>(</span>quantity<span>)</span> = map<span>.</span><span>get</span><span>(</span><span>&amp;</span><span>"apple"</span><span>)</span><span>.</span><span>await</span> <span>{</span>
        <span>println</span><span>!</span><span>(</span><span>"We have {} apples!"</span>, quantity<span>)</span><span>;</span>
    <span>}</span>

    map<span>.</span><span>remove</span><span>(</span><span>&amp;</span><span>"banana"</span><span>)</span><span>.</span><span>await</span><span>;</span>
<span>}</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">üìö Examples</h2><a id="user-content--examples" aria-label="Permalink: üìö Examples" href="#-examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Concurrent Inserts</h3><a id="user-content-concurrent-inserts" aria-label="Permalink: Concurrent Inserts" href="#concurrent-inserts"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="use whirlwind::ShardMap;
use tokio::task::JoinSet;

#[tokio::main]
async fn main() {
    let map = ShardMap::new();
    let tasks: JoinSet<_> = (0..1000).map(|i| {
        let map = map.clone();
        tokio::spawn(async move {
            map.insert(i, i * 2).await;
        })
    }).collect();

    tasks.join_all().await.ok();

    assert_eq!(map.len().await, 1000);
}"><pre><span>use</span> whirlwind<span>::</span><span>ShardMap</span><span>;</span>
<span>use</span> tokio<span>::</span>task<span>::</span><span>JoinSet</span><span>;</span>

<span>#<span>[</span>tokio<span>::</span>main<span>]</span></span>
<span>async</span> <span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> map = <span>ShardMap</span><span>::</span><span>new</span><span>(</span><span>)</span><span>;</span>
    <span>let</span> tasks<span>:</span> <span>JoinSet</span><span>&lt;</span><span>_</span><span>&gt;</span> = <span>(</span><span>0</span>..<span>1000</span><span>)</span><span>.</span><span>map</span><span>(</span>|i| <span>{</span>
        <span>let</span> map = map<span>.</span><span>clone</span><span>(</span><span>)</span><span>;</span>
        tokio<span>::</span><span>spawn</span><span>(</span><span>async</span> <span>move</span> <span>{</span>
            map<span>.</span><span>insert</span><span>(</span>i<span>,</span> i <span>*</span> <span>2</span><span>)</span><span>.</span><span>await</span><span>;</span>
        <span>}</span><span>)</span>
    <span>}</span><span>)</span><span>.</span><span>collect</span><span>(</span><span>)</span><span>;</span>

    tasks<span>.</span><span>join_all</span><span>(</span><span>)</span><span>.</span><span>await</span><span>.</span><span>ok</span><span>(</span><span>)</span><span>;</span>

    <span>assert_eq</span><span>!</span><span>(</span>map.len<span>(</span><span>)</span>.<span>await</span>, <span>1000</span><span>)</span><span>;</span>
<span>}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Custom Shard Count</h3><a id="user-content-custom-shard-count" aria-label="Permalink: Custom Shard Count" href="#custom-shard-count"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="use whirlwind::ShardMap;

#[tokio::main]
async fn main() {
    let map = ShardMap::with_shards(64); // Initialize with 64 shards
    // Use the map as needed
}"><pre><span>use</span> whirlwind<span>::</span><span>ShardMap</span><span>;</span>

<span>#<span>[</span>tokio<span>::</span>main<span>]</span></span>
<span>async</span> <span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> map = <span>ShardMap</span><span>::</span><span>with_shards</span><span>(</span><span>64</span><span>)</span><span>;</span> <span>// Initialize with 64 shards</span>
    <span>// Use the map as needed</span>
<span>}</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">üìä Benchmarks</h2><a id="user-content--benchmarks" aria-label="Permalink: üìä Benchmarks" href="#-benchmarks"></a></p>
<p dir="auto">Benchmarks were run in a asyncified version of <a href="https://github.com/xacrimon/conc-map-bench">this benchmark</a>. You can
find it <a href="https://github.com/willothy/conc-map-bench">here</a>. Since the benchmarks use <a href="https://github.com/jonhoo/bustle"><code>jonhoo/bustle</code></a>,
an asyncified fork of that library (<a href="https://github.com/willothy/bustle">here</a>) is required.</p>
<p dir="auto">Machine: Apple M3 Max (2023 16-inch MacBook Pro, 36GB RAM)</p>
<p dir="auto">OS: macOS 15.0</p>
<p dir="auto">See the <code>results/</code> directory.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Read Heavy (std hasher)</h3><a id="user-content-read-heavy-std-hasher" aria-label="Permalink: Read Heavy (std hasher)" href="#read-heavy-std-hasher"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/ReadHeavy.std.throughput.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/ReadHeavy.std.throughput.svg" alt=""></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/ReadHeavy.std.latency.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/ReadHeavy.std.latency.svg" alt=""></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exchange (std hasher)</h3><a id="user-content-exchange-std-hasher" aria-label="Permalink: Exchange (std hasher)" href="#exchange-std-hasher"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/Exchange.std.throughput.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/Exchange.std.throughput.svg" alt=""></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/Exchange.std.latency.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/Exchange.std.latency.svg" alt=""></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Rapid Grow (std hasher)</h3><a id="user-content-rapid-grow-std-hasher" aria-label="Permalink: Rapid Grow (std hasher)" href="#rapid-grow-std-hasher"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/RapidGrow.std.throughput.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/RapidGrow.std.throughput.svg" alt=""></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/RapidGrow.std.latency.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/RapidGrow.std.latency.svg" alt=""></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Read Heavy (ahash)</h3><a id="user-content-read-heavy-ahash" aria-label="Permalink: Read Heavy (ahash)" href="#read-heavy-ahash"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/ReadHeavy.ahash.throughput.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/ReadHeavy.ahash.throughput.svg" alt=""></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/ReadHeavy.ahash.latency.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/ReadHeavy.ahash.latency.svg" alt=""></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exchange (ahash)</h3><a id="user-content-exchange-ahash" aria-label="Permalink: Exchange (ahash)" href="#exchange-ahash"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/Exchange.ahash.throughput.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/Exchange.ahash.throughput.svg" alt=""></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/Exchange.ahash.latency.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/Exchange.ahash.latency.svg" alt=""></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Rapid Grow (ahash)</h3><a id="user-content-rapid-grow-ahash" aria-label="Permalink: Rapid Grow (ahash)" href="#rapid-grow-ahash"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/RapidGrow.ahash.throughput.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/RapidGrow.ahash.throughput.svg" alt=""></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/fortress-build/whirlwind/blob/main/results/RapidGrow.ahash.latency.svg"><img src="https://github.com/fortress-build/whirlwind/raw/main/results/RapidGrow.ahash.latency.svg" alt=""></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">ü§ù Contributing</h2><a id="user-content--contributing" aria-label="Permalink: ü§ù Contributing" href="#-contributing"></a></p>
<p dir="auto">Contributions are welcome! Please follow these steps:</p>
<ol dir="auto">
<li>Fork the repository.</li>
<li>Create a new branch: <code>git checkout -b feature/your-feature</code>.</li>
<li>Commit your changes: <code>git commit -am 'Add your feature'</code>.</li>
<li>Push to the branch: <code>git push origin feature/your-feature</code>.</li>
<li>Open a pull request.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Running Tests</h3><a id="user-content-running-tests" aria-label="Permalink: Running Tests" href="#running-tests"></a></p>
<p dir="auto">Ensure all tests pass before submitting a PR:</p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Code Style</h3><a id="user-content-code-style" aria-label="Permalink: Code Style" href="#code-style"></a></p>
<p dir="auto">We use <code>rustfmt</code> for code formatting:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Copyright 2024 Will Hopkins</p>
<p dir="auto">Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at</p>
<p dir="auto"><a href="http://www.apache.org/licenses/LICENSE-2.0" rel="nofollow">http://www.apache.org/licenses/LICENSE-2.0</a></p>
<p dir="auto">Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.</p>
<hr>
<p dir="auto">Made with üíñ and Rust.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I wrote an open-source browser alternative for Computer Use for any LLM (156 pts)]]></title>
            <link>https://github.com/gregpr07/browser-use</link>
            <guid>42052432</guid>
            <pubDate>Tue, 05 Nov 2024 15:51:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/gregpr07/browser-use">https://github.com/gregpr07/browser-use</a>, See on <a href="https://news.ycombinator.com/item?id=42052432">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<p dir="auto"><h2 tabindex="-1" dir="auto">üåê Browser-Use</h2><a id="user-content--browser-use" aria-label="Permalink: üåê Browser-Use" href="#-browser-use"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Open-Source Web Automation with LLMs</h3><a id="user-content-open-source-web-automation-with-llms" aria-label="Permalink: Open-Source Web Automation with LLMs" href="#open-source-web-automation-with-llms"></a></p>

<p dir="auto"><a href="https://github.com/gregpr07/browser-use/stargazers"><img src="https://camo.githubusercontent.com/0f19403bd69bc9dd8ab794fa4cce28d24300bfd37881d117c0b68e001bc86418/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f67726567707230372f62726f777365722d7573653f7374796c653d736f6369616c" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/gregpr07/browser-use?style=social"></a>
<a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/6cd0120cc4c5ac11d28b2c60f76033b52db98dac641de3b2644bb054b449d60c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"></a>
<a href="https://www.python.org/downloads/" rel="nofollow"><img src="https://camo.githubusercontent.com/1e5852941fcfe768cdba62e1ef6b1db0d9c87c4f9017432c39ad06853f6d4df9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e31312b2d626c75652e737667" alt="Python 3.11+" data-canonical-src="https://img.shields.io/badge/python-3.11+-blue.svg"></a></p>
<p dir="auto"><em>Let LLMs interact with websites naturally</em></p>
<p dir="auto"><a href="#-key-features">Key Features</a> ‚Ä¢
<a href="#-live-demos">Live Demos</a> ‚Ä¢
<a href="#-quick-start">Quick Start</a> ‚Ä¢
<a href="#-examples">Examples</a> ‚Ä¢
<a href="#-supported-models">Models</a></p>
</div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">üé• Live Demos</h2><a id="user-content--live-demos" aria-label="Permalink: üé• Live Demos" href="#-live-demos"></a></p>
<p dir="auto">Watch Browser-Use tackle real-world tasks:</p>
<div dir="auto">
  <div dir="auto">
    <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/gregpr07/browser-use/blob/main/static/kayak.gif"><img src="https://github.com/gregpr07/browser-use/raw/main/static/kayak.gif" alt="Kayak flight search demo" data-animated-image=""></a></p><p dir="auto"><i>Prompt: Go to kayak.com and find a one-way flight from Z√ºrich to San Francisco on 12 January 2025.</i></p>
  </div>
  <div dir="auto">
    <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/gregpr07/browser-use/blob/main/static/photos.gif"><img src="https://github.com/gregpr07/browser-use/raw/main/static/photos.gif" alt="Photos search demo" data-animated-image=""></a></p><p dir="auto"><i>Prompt: Opening new tabs and searching for images for these people: Albert Einstein, Oprah Winfrey, Steve Jobs. Then ask me for further instructions.</i></p>
  </div>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">üöÄ Key Features</h2><a id="user-content--key-features" aria-label="Permalink: üöÄ Key Features" href="#-key-features"></a></p>
<ul dir="auto">
<li>ü§ñ <strong>Universal LLM Support</strong> - Works with any Language Model</li>
<li>üéØ <strong>Smart Element Detection</strong> - Automatically finds interactive elements</li>
<li>üìë <strong>Multi-Tab Management</strong> - Seamless handling of browser tabs</li>
<li>üîç <strong>XPath Extraction</strong> - No more manual DevTools inspection</li>
<li>üëÅÔ∏è <strong>Vision Model Support</strong> - Process visual page information</li>
<li>üõ†Ô∏è <strong>Customizable Actions</strong> - Add your own browser interactions</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">üíª Quick Start</h2><a id="user-content--quick-start" aria-label="Permalink: üíª Quick Start" href="#-quick-start"></a></p>
<p dir="auto">Create a virtual environment and install the dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# I recommend using uv
pip install -r requirements.txt"><pre><span><span>#</span> I recommend using uv</span>
pip install -r requirements.txt</pre></div>
<p dir="auto">Add your API keys to the <code>.env</code> file.</p>

<p dir="auto">You can use any LLM model that is supported by LangChain by adding correct environment variables. Head over to the <a href="https://python.langchain.com/docs/integrations/chat/" rel="nofollow">langchain models</a> page to see all available models.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">üìù Examples</h2><a id="user-content--examples" aria-label="Permalink: üìù Examples" href="#-examples"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="from src import Agent
from langchain_openai import ChatOpenAI

# Initialize browser agent
agent = Agent(
	task='Find cheapest flight from London to Kyrgyzstan and return the url.',
	llm=ChatOpenAI(model='gpt-4o'),
)

# Let it work its magic
await agent.run()"><pre><span>from</span> <span>src</span> <span>import</span> <span>Agent</span>
<span>from</span> <span>langchain_openai</span> <span>import</span> <span>ChatOpenAI</span>

<span># Initialize browser agent</span>
<span>agent</span> <span>=</span> <span>Agent</span>(
	<span>task</span><span>=</span><span>'Find cheapest flight from London to Kyrgyzstan and return the url.'</span>,
	<span>llm</span><span>=</span><span>ChatOpenAI</span>(<span>model</span><span>=</span><span>'gpt-4o'</span>),
)

<span># Let it work its magic</span>
<span>await</span> <span>agent</span>.<span>run</span>()</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Chain of Agents</h3><a id="user-content-chain-of-agents" aria-label="Permalink: Chain of Agents" href="#chain-of-agents"></a></p>
<p dir="auto">You can persist the browser across multiple agents and chain them together.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from langchain_anthropic import ChatAnthropic
from src import Agent, Controller

# Persist the browser state across agents
controller = Controller()

# Initialize browser agent
agent1 = Agent(
	task='Open 5 VCs websites in the New York area.',
	llm=ChatAnthropic(model_name='claude-3-sonnet', timeout=25, stop=None, temperature=0.3),
	controller=controller,
)
agent2 = Agent(
	task='Give me the names of the founders of the companies in all tabs.',
	llm=ChatAnthropic(model_name='claude-3-sonnet', timeout=25, stop=None, temperature=0.3),
	controller=controller,
)

# Let it work its magic
await agent1.run()
founders, history = await agent2.run()

print(founders)"><pre><span>from</span> <span>langchain_anthropic</span> <span>import</span> <span>ChatAnthropic</span>
<span>from</span> <span>src</span> <span>import</span> <span>Agent</span>, <span>Controller</span>

<span># Persist the browser state across agents</span>
<span>controller</span> <span>=</span> <span>Controller</span>()

<span># Initialize browser agent</span>
<span>agent1</span> <span>=</span> <span>Agent</span>(
	<span>task</span><span>=</span><span>'Open 5 VCs websites in the New York area.'</span>,
	<span>llm</span><span>=</span><span>ChatAnthropic</span>(<span>model_name</span><span>=</span><span>'claude-3-sonnet'</span>, <span>timeout</span><span>=</span><span>25</span>, <span>stop</span><span>=</span><span>None</span>, <span>temperature</span><span>=</span><span>0.3</span>),
	<span>controller</span><span>=</span><span>controller</span>,
)
<span>agent2</span> <span>=</span> <span>Agent</span>(
	<span>task</span><span>=</span><span>'Give me the names of the founders of the companies in all tabs.'</span>,
	<span>llm</span><span>=</span><span>ChatAnthropic</span>(<span>model_name</span><span>=</span><span>'claude-3-sonnet'</span>, <span>timeout</span><span>=</span><span>25</span>, <span>stop</span><span>=</span><span>None</span>, <span>temperature</span><span>=</span><span>0.3</span>),
	<span>controller</span><span>=</span><span>controller</span>,
)

<span># Let it work its magic</span>
<span>await</span> <span>agent1</span>.<span>run</span>()
<span>founders</span>, <span>history</span> <span>=</span> <span>await</span> <span>agent2</span>.<span>run</span>()

<span>print</span>(<span>founders</span>)</pre></div>
<p dir="auto">You can use the <code>history</code> to run the agents again deterministically.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Simple Run</h2><a id="user-content-simple-run" aria-label="Permalink: Simple Run" href="#simple-run"></a></p>
<p dir="auto">You can run any of the examples using the command line interface:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python examples/try.py &quot;Your query here&quot; --provider [openai|anthropic]"><pre>python examples/try.py <span><span>"</span>Your query here<span>"</span></span> --provider [openai<span>|</span>anthropic]</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Anthropic</h3><a id="user-content-anthropic" aria-label="Permalink: Anthropic" href="#anthropic"></a></p>
<p dir="auto">You need to add <code>ANTHROPIC_API_KEY</code> to your environment variables. Example usage:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python examples/try.py &quot;Find cheapest flight from London to Paris&quot; --provider anthropic"><pre>python examples/try.py <span><span>"</span>Find cheapest flight from London to Paris<span>"</span></span> --provider anthropic</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">OpenAI</h3><a id="user-content-openai" aria-label="Permalink: OpenAI" href="#openai"></a></p>
<p dir="auto">You need to add <code>OPENAI_API_KEY</code> to your environment variables. Example usage:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python examples/try.py &quot;Search for top AI companies&quot; --provider openai"><pre>python examples/try.py <span><span>"</span>Search for top AI companies<span>"</span></span> --provider openai</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">ü§ñ Supported Models</h2><a id="user-content--supported-models" aria-label="Permalink: ü§ñ Supported Models" href="#-supported-models"></a></p>
<p dir="auto">All LangChain chat models are supported.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Tested</h3><a id="user-content-tested" aria-label="Permalink: Tested" href="#tested"></a></p>
<ul dir="auto">
<li>GPT-4o</li>
<li>GPT-4o Mini</li>
<li>Claude 3.5 Sonnet</li>
<li>LLama 3.1 405B</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">ü§ù Contributing</h2><a id="user-content--contributing" aria-label="Permalink: ü§ù Contributing" href="#-contributing"></a></p>
<p dir="auto">Contributions are welcome! Also feel free to open issues for any bugs or feature requests.</p>
<hr>
<p><b>Star ‚≠ê this repo if you find it useful!</b><br>
  Made with ‚ù§Ô∏è by the Browser-Use team
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Future Roadmap</h2><a id="user-content-future-roadmap" aria-label="Permalink: Future Roadmap" href="#future-roadmap"></a></p>
<ul>
<li> Save agent actions and execute them deterministically (for QA testing etc)</li>
<li> Pydantic forced output</li>
<li> Third party SERP API for faster Google Search results</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hacking 700M Electronic Arts Accounts (778 pts)]]></title>
            <link>https://battleda.sh/blog/ea-account-takeover</link>
            <guid>42052143</guid>
            <pubDate>Tue, 05 Nov 2024 15:18:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://battleda.sh/blog/ea-account-takeover">https://battleda.sh/blog/ea-account-takeover</a>, See on <a href="https://news.ycombinator.com/item?id=42052143">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[New documentary reveals that 21,000 laborers have died working Saudi Vision 2030 (245 pts)]]></title>
            <link>https://www.archpaper.com/2024/10/documentary-reveals-21000-workers-killed-saudi-vision-2030-neom/</link>
            <guid>42052105</guid>
            <pubDate>Tue, 05 Nov 2024 15:14:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.archpaper.com/2024/10/documentary-reveals-21000-workers-killed-saudi-vision-2030-neom/">https://www.archpaper.com/2024/10/documentary-reveals-21000-workers-killed-saudi-vision-2030-neom/</a>, See on <a href="https://news.ycombinator.com/item?id=42052105">Hacker News</a></p>
Couldn't get https://www.archpaper.com/2024/10/documentary-reveals-21000-workers-killed-saudi-vision-2030-neom/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Hacker Fab (383 pts)]]></title>
            <link>https://docs.hackerfab.org/hacker-fab-space</link>
            <guid>42051968</guid>
            <pubDate>Tue, 05 Nov 2024 14:59:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.hackerfab.org/hacker-fab-space">https://docs.hackerfab.org/hacker-fab-space</a>, See on <a href="https://news.ycombinator.com/item?id=42051968">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><!--$?--><template id="B:0"></template><!--/$--><p>Last updated <time data-visual-test="transparent" datetime="2024-11-05T18:11:11.169Z" title="11/5/2024, 6:11:11 PM">0 minutes ago</time></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Netflix Europe offices raided in tax fraud probe (322 pts)]]></title>
            <link>https://www.bbc.co.uk/news/articles/cwy1vze09wwo</link>
            <guid>42051643</guid>
            <pubDate>Tue, 05 Nov 2024 14:20:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.co.uk/news/articles/cwy1vze09wwo">https://www.bbc.co.uk/news/articles/cwy1vze09wwo</a>, See on <a href="https://news.ycombinator.com/item?id=42051643">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="text-block"><p><b>Offices of streaming giant Netflix in Paris and Amsterdam have been raided by the French and Dutch authorities as part of an investigation into tax fraud, French judicial sources say.</b></p><p>Officials from the two countries have been co-operating on the case since the investigation was opened in November 2022.</p><p>Netflix has not as yet made any specific comment on the raids, but insists it complies with tax laws wherever it operates.</p><p>The Amsterdam office is the headquarters of the company's operations in Europe, the Middle East and Africa.</p></div><div data-component="text-block"><p>The French investigation is being carried out by the National Financial Prosecutor's office (PNF), a special unit used for investigations into high-profile white-collar crime.</p><p>It relates to suspicions of "covering up serious tax fraud and off-the-books work", according to the PNF.</p><p>The company is also under investigation for tax filings for 2019, 2020 and 2021.</p><p>The French sources said authorities in the Netherlands were conducting simultaneous searches, and that co-operation between the two countries had been going on for "many months".</p><p>Last year, French media outlet La Lettre reported that until 2021, Netflix in France minimised its tax payments by declaring its turnover generated in France to the Netherlands.</p><p>After it abandoned this arrangement, La Lettre said, its annual declared turnover in France  jumped from ‚Ç¨47.1m ($51.3m; ¬£39.6m) in 2020 to ‚Ç¨1.2bn in 2021.</p><p>However, the outlet says investigators are trying to determine whether Netflix continued to attempt to minimise its profits after 2021.</p><p>Netflix arrived in France more than 10 years ago, opening its Paris office in 2020. It has some 10 million subscribers in the country, according to AFP news agency.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Failure Analysis of the Arecibo 305 Meter Telescope Collapse (211 pts)]]></title>
            <link>https://nap.nationalacademies.org/read/26982/chapter/1</link>
            <guid>42051368</guid>
            <pubDate>Tue, 05 Nov 2024 13:37:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nap.nationalacademies.org/read/26982/chapter/1">https://nap.nationalacademies.org/read/26982/chapter/1</a>, See on <a href="https://news.ycombinator.com/item?id=42051368">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="openbook-ocr">
          <p>Below is the uncorrected machine-read text of this chapter, intended to provide our own search engines and external engines with highly rich, chapter-representative searchable text of each book. Because it is UNCORRECTED material, please consider the following text as a useful but insufficient proxy for the authoritative book pages.</p>
                      <p>Failure Analysis of the
Arecibo Observatory 305-Meter
Telescope Collapse




Committee on Analysis of Causes of Failure
and Collapse of the 305-Meter Telescope at
the Arecibo Observatory

Board on Infrastructure and the Constructed
Environment

Division on Engineering and Physical Sciences




                                                 Consensus Study Report
            PREPUBLICATION COPY√¢¬Ä¬îSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>NATIONAL ACADEMIES PRESS 500 Fifth Street, NW Washington, DC 20001

This activity was supported by Grant CMMI- 2135084 from the National Science Foundation to the National
Academy of Sciences. Any opinions, findings, conclusions, or recommendations expressed in this publication do
not necessarily reflect the views of any organization or agency that provided support for the project.

International Standard Book Number-13: 978-0-309-XXXXX-X
International Standard Book Number-10: 0-309-XXXXX-X
Digital Object Identifier: https://doi.org/10.17226/26982

Cover: Photo courtesy of the Arecibo Observatory, a facility of the National Science Foundation.

This publication is available from the National Academies Press, 500 Fifth Street, NW, Keck 360, Washington,
DC 20001; (800) 624-6242 or (202) 334-3313; http://www.nap.edu.

Copyright 2024 by the National Academy of Sciences. National Academies of Sciences, Engineering, and
Medicine and National Academies Press and the graphical logos for each are all trademarks of the National
Academy of Sciences. All rights reserved.

Printed in the United States of America.

Suggested citation: National Academies of Sciences, Engineering, and Medicine. 2024. Failure Analysis of the
Arecibo Observatory 305-Meter Telescope Collapse. Washington, DC: The National Academies Press. https://
doi.org/10.17226/26982.




      PREPUBLICATION COPY√¢¬Ä¬îSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>The National Academy of Sciences was established in 1863 by an Act of Congress, signed by President
Lincoln, as a private, nongovernmental institution to advise the nation on issues related to science and
technology. Members are elected by their peers for outstanding contributions to research. Dr. Marcia McNutt is
president.

The National Academy of Engineering was established in 1964 under the charter of the National Academy
of Sciences to bring the practices of engineering to advising the nation. Members are elected by their peers for
extraordinary contributions to engineering. Dr. John L. Anderson is president.

The National Academy of Medicine (formerly the Institute of Medicine) was established in 1970 under the
charter of the National Academy of Sciences to advise the nation on medical and health issues. Members are
elected by their peers for distinguished contributions to medicine and health. Dr. Victor J. Dzau is president.

The three Academies work together as the National Academies of Sciences, Engineering, and Medicine
to provide independent, objective analysis and advice to the nation and conduct other activities to solve complex
problems and inform public policy decisions. The National Academies also encourage education and research,
recognize outstanding contributions to knowledge, and increase public understanding in matters of science,
engineering, and medicine.

Learn more about the National Academies of Sciences, Engineering, and Medicine at
www.nationalacademies.org.




      PREPUBLICATION COPY√¢¬Ä¬îSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>Consensus Study Reports published by the National Academies of Sciences, Engineering, and
Medicine document the evidence-based consensus on the study√¢¬Ä¬ôs statement of task by an authoring committee of
experts. Reports typically include findings, conclusions, and recommendations based on information gathered by
the committee and the committee√¢¬Ä¬ôs deliberations. Each report has been subjected to a rigorous and independent
peer-review process and it represents the position of the National Academies on the statement of task.

Proceedings published by the National Academies of Sciences, Engineering, and Medicine chronicle the
presentations and discussions at a workshop, symposium, or other event convened by the National Academies.
The statements and opinions contained in proceedings are those of the participants and are not endorsed by
other participants, the planning committee, or the National Academies.

Rapid Expert Consultations published by the National Academies of Sciences, Engineering, and Medicine are
authored by subject-matter experts on narrowly focused topics that can be supported by a body of evidence. The
discussions contained in rapid expert consultations are considered those of the authors and do not contain policy
recommendations. Rapid expert consultations are reviewed by the institution before release.

For information about other products and activities of the National Academies, please visit
www.nationalacademies.org/about/whatwedo.




      PREPUBLICATION COPY√¢¬Ä¬îSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>               COMMITTEE ON ANALYSIS OF CAUSES OF FAILURE AND COLLAPSE OF
                  THE 305-METER TELESCOPE AT THE ARECIBO OBSERVATORY


ROGER L. McCARTHY (NAE), McCarthy Engineering, Chair
RAM√É¬ìN L. CARRASQUILLO,1 Carrasquillo Associates
DIANNE CHONG (NAE), Boeing Engineering, Operations &amp; Technology (retired)
ROBERT B. GILBERT (NAE), The University of Texas at Austin
W. ALLEN MARR, JR. (NAE), Geocomp, Inc.
JOHN R. SCULLY, University of Virginia
SAWTEEN SEE, See Robertson Structural Engineers
HABIB TABATABAI, University of Wisconsin√¢¬Ä¬ìMilwaukee


Study Staff
CAMERON OSKVIG, Board Director, Study Director
JAYDA WADE, Research Associate (until July 31, 2023)
JOSEPH PALMER, SR., Program Assistant
RADAKA LIGHTFOOT, Financial Business Partner (until March 20, 2023)
DONAVAN THOMAS, Financial Business Partner (from March 20, 2023)




 1 Deceased   on February 2, 2024.

                                                v



      PREPUBLICATION COPY√¢¬Ä¬îSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>           BOARD ON INFRASTRUCTURE AND THE CONSTRUCTED ENVIRONMENT


JESUS M. DE LA GARZA, Clemson University, Chair
BURCU AKINCI, Carnegie Mellon University
STEPHEN AYERS, Ayers Group
BURCIN BECERIK-GERBER, University of Southern California
LEAH BROOKS, The George Washington University
MIKHAIL V. CHESTER, Arizona State University
JAMES (JACK) DEMPSEY, Asset Management Partnership, LLC
LEONARDO DUENAS-OSORIO, Rice University
DEVIN K. HARRIS, University of Virginia
DAVID J. HAUN, Haun Consulting, Inc.
CHRISTOPHER J. MOSSEY, Fermi National Accelerator Laboratory
ANDREW PERSILY, National Institute of Standards and Technology
ROBERT B. RAINES, Atkins Nuclear Secured
JAMES RISPOLI, North Carolina State University
DOROTHY ROBYN, Boston University
SHOSHANNA D. SAXE, University of Toronto


Staff
CAMERON OSKVIG, Board Director
JAMES MYSKA, Senior Program Officer
BRITTANY SEGUNDO, Program Officer
JOSEPH PALMER, SR., Senior Program Assistant
DONAVAN THOMAS, Finance Business Partner




                                               vi



        PREPUBLICATION COPY√¢¬Ä¬îSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>                                                   Dedication




     This report is dedicated to committee member Dr. Ram√É¬≥n L. Carrasquillo, who unexpectedly passed away
before this report√¢¬Ä¬ôs release. His pragmatic and insightful contributions strengthened the report. In addition to his
extensive engineering and materials science expertise, his deep connection to Puerto Rico helped the commit-
tee develop a nuanced understanding of the community and culture surrounding the Arecibo Observatory. He is
remembered by the committee as a thoughtful, warm, and generous colleague.




NOTE: Image courtesy of Carrasquillo Associates.


                                                        vii



       PREPUBLICATION COPY√¢¬Ä¬îSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>PREPUBLICATION COPY√¢¬Ä¬îSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>                                               Reviewers




     This Consensus Study Report was reviewed in draft form by individuals chosen for their diverse perspec-
tives and technical expertise. The purpose of this independent review is to provide candid and critical comments
that will assist the National Academies of Sciences, Engineering, and Medicine in making each published report
as sound as possible and to ensure that it meets the institutional standards for quality, objectivity, evidence, and
responsiveness to the study charge. The review comments and draft manuscript remain confidential to protect the
integrity of the deliberative process.
     We thank the following individuals for their review of this report:

    DONALD CAMPBELL, Cornell University
    GREGORY G. DEIERLEIN (NAE), Stanford University
    LENNARD FISK (NAS), University of Michigan
    DAVID GOODYEAR (NAE), Independent Consultant
    MARTHA HAYNES (NAS), Cornell University
    LT. COL. (RET.) CLARENCE (BART) KEMPER, Kemper Engineering Services, LLC
    MATTHYS LEVY (NAE), Thornton Tomasetti
    MOHAMMAD MODARRES, University of Maryland
    JANINE PARDEE, Independent Consultant
    RANDALL POSTON (NAE), Pivot Engineers

     Although the reviewers listed above provided many constructive comments and suggestions, they were not
asked to endorse the conclusions or recommendations of this report nor did they see the final draft before its
release. The review of this report was overseen by WILLIAM F. BAKER, Skidmore Owings and Merrill, LLP,
and STEVE BATTEL (NAE), Battel Engineering. They were responsible for making certain that an independent
examination of this report was carried out in accordance with the standards of the National Academies and that all
review comments were carefully considered. Responsibility for the final content rests entirely with the authoring
committee and the National Academies.




                                                         ix



      PREPUBLICATION COPY√¢¬Ä¬îSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>PREPUBLICATION COPY√¢¬Ä¬îSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>                                            Contents




PREFACE                                                                      xiii

SUMMARY                                                                        1

1   INTRODUCTION                                                               7
    History of the Arecibo Telescope, 7
    Arecibo Telescope Cable System, 12
    Statement of Task, 16

2   THE COLLAPSE: WHAT HAPPENED                                               18
    Arecibo Telescope Failure Sequence, 18
    Hurricane Maria Hits Arecibo Telescope, 18
    Post-Maria Arecibo Telescope Inspections, 22
    The Hurricane Maria Aftermath, 26
    Bureaucratic Delays in Funding Arecibo Telescope Hurricane Repairs, 28
    Sequence of Cable Failure Events, 30

3   ANALYSIS                                                                  35
    Cable Socket Zinc Creep Failure, 35
    Cable End Sockets, 48
    Wire Breaks, 49
    Earthquake, 55
    Wind Speed Consideration in the Arecibo Telescope√¢¬Ä¬ôs Design, 56
    Governing Cable Design Standards, 56
    Arecibo Telescope Cable Load, 58
    Risk Considerations, 61
    Structural Robustness, 63
    Monitoring, 65



                                                    xi



     PREPUBLICATION COPY√¢¬Ä¬îSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>xii                                                                            CONTENTS



4     ARECIBO TELESCOPE√¢¬Ä¬ôS MANAGEMENT AND OVERSIGHT                                   70

5     OTHER LESSONS LEARNED                                                          74
      State of Knowledge, 74
      Continued Research, 75

BIBLIOGRAPHY                                                                         76

APPENDIXES

A     Committee Member Biographical Information                                      81
B     Information-Gathering Activities                                               84
C     Arecibo Telescope Cable Failure Mechanisms Considered by the Committee         86
D     Arecibo Telescope Design Issues Considered by the Committee                    94
E     Acronyms and Abbreviations                                                     96




       PREPUBLICATION COPY√¢¬Ä¬îSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>                                                  Preface




     It has been my privilege to chair this committee of distinguished subject-matter experts in its investigation
and final probable cause determination of one of the most publicized and baffling failures of the modern era. It
became clear shortly after the Arecibo Telescope√¢¬Ä¬ôs collapse that the zinc used to anchor the steel supporting cable
wires into their sockets had allowed the failed cables to slip out of their sockets, known as spelter sockets. The
sockets slowly lost their grip on a critical number of the cable wires via slow zinc √¢¬Ä¬úcreep,√¢¬Ä¬ù a process where the
zinc deformed slowly at a load below half the socket√¢¬Ä¬ôs nominal strength. Although the committee agrees with
the conclusions from other forensic reports regarding zinc creep at the connection being the failure mechanism,
the baffling question was, √¢¬Ä¬úWhy was there excessive zinc creep at such loading?√¢¬Ä¬ù Such a failure had never been
reported previously in over a century of widespread zinc spelter socket successful use.
     Fortunately, the committee had the benefit of the detailed analysis and well-documented reports from NASA;
Wiss, Janney, Elstner Associates, Inc.; and Thornton Tomasetti, Inc., without which it could not have completed its
task. Building on their work, the committee presents a clear and plausible explanation of why the telescope√¢¬Ä¬ôs sockets
failed when no such sockets have ever been reported to have failed before. Unfortunately, there was not enough
data available to prove our explanation. It is simply the most plausible hypothesis based on the data we do have.
     Without the depth and breadth of expertise on the committee, its task would remain uncompleted. I think I
speak for everyone on the committee when I say that none of us could have done this alone. I want to thank my
colleagues for their unwavering dedication to the task. Their professionalism and competence made my job an
enjoyable one.

                                                                                     Roger L. McCarthy, Chair
                                                        Committee on Analysis of Causes of Failure and Collapse
                                                          of the 305-Meter Telescope at the Arecibo Observatory




                                                        xiii



      PREPUBLICATION COPY√¢¬Ä¬îSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                      <p>PREPUBLICATION COPY√¢¬Ä¬îSUBJECT TO FURTHER EDITORIAL CORRECTION
</p>
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[State of Python 3.13 performance: Free-threading (144 pts)]]></title>
            <link>https://codspeed.io/blog/state-of-python-3-13-performance-free-threading</link>
            <guid>42051197</guid>
            <pubDate>Tue, 05 Nov 2024 13:06:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://codspeed.io/blog/state-of-python-3-13-performance-free-threading">https://codspeed.io/blog/state-of-python-3-13-performance-free-threading</a>, See on <a href="https://news.ycombinator.com/item?id=42051197">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>CPython 3.13 was released two weeks ago and this release is the most
performance-oriented in some time. After a quick read of the release notes, a
few things stand out for the impact they can have on the performance:</p>
<ul>
<li>CPython can now run in <strong>free-threaded mode</strong>, with the global interpreter
lock (GIL) disabled</li>
<li>a brand new <strong>just-in-time</strong> (JIT) compiler has been added</li>
<li>CPython now bundles the <code>mimalloc</code> allocator out of the box</li>
</ul>
<p>Let's focus on the free-threaded mode in this article to see how to leverage
this change and how it can impact the performance of Python applications by
measuring performance with CodSpeed.</p>
<blockquote>
<p>‚è≠Ô∏è The JIT and <code>mimalloc</code> performance will be covered in the next post. Stay
tuned!</p>
</blockquote>
<h2 id="free-threaded-cpython"><a href="#free-threaded-cpython">Free-threaded CPython<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h2>
<p>Free-threading is an experimental feature in Python 3.13 that allows CPython to
run without the Global Interpreter Lock (GIL). The GIL is a mutex preventing
multiple threads from executing Python bytecode simultaneously. This design
choice has simplified CPython's memory management and made the C API easier to
work with. However, it has also been one of the most significant barriers to
utilizing modern multi-core processors effectively.</p>
<h4 id="the-multiprocessing-workaround"><a href="#the-multiprocessing-workaround">The Multiprocessing Workaround<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h4>
<p>The traditional solution has been to use the <code>multiprocessing</code> module, which
spawns separate Python processes instead of threads and while this approach
works, it comes with significant limitations:</p>
<ol>
<li>
<p><strong>Memory Overhead</strong>: Each process requires its own Python interpreter
instance and memory space. For data-intensive applications, this can quickly
become a bottleneck.</p>
</li>
<li>
<p><strong>Communication Cost</strong>: Processes can't share memory directly. Data must be
serialized and deserialized when passed between processes, which adds
overhead and complexity.</p>
</li>
<li>
<p><strong>Startup Time</strong>: Creating new processes is significantly slower than
creating threads, making it impractical for tasks that require frequent
spawning of workers.</p>
</li>
</ol>

<p>To illustrate these limitations, let's consider implementing PageRank, the
algorithm that powered Google's early search engine. PageRank is an ideal
example because it:</p>
<ol>
<li>Is compute-intensive (matrix operations)</li>
<li>Works with large datasets (the web graph)</li>
<li>Can benefit significantly from parallelization</li>
</ol>
<p>A naive multithreaded implementation in Python 3.12 or earlier would be
bottlenecked by the GIL during matrix operations, while a multiprocessing
version would struggle with:</p>
<ul>
<li>The memory overhead of copying the graph to each process</li>
<li>The cost of transferring partial results between processes</li>
<li>The complexity of managing shared state</li>
</ul>
<p>Before we proceed, it's important to clarify that our focus here isn't on the
specifics of the PageRank algorithm itself but rather on the parallelization so
we won't go into the details of the algorithm itself here.</p>
<p>Let's look at how we would implement this with those different concurrency
models.</p>
<h3 id="textbook-implementation-single-threaded-"><a href="#textbook-implementation-single-threaded-">Textbook Implementation (Single-Threaded)<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h3>
<pre tabindex="0"><code><span><span>def</span><span> pagerank_single</span><span>(</span><span>matrix</span><span>: np.ndarray, </span><span>num_iterations</span><span>: </span><span>int</span><span>) -&gt; np.ndarray:</span></span>
<span><span>    """Single-threaded PageRank implementation"""</span></span>
<span><span>    size </span><span>=</span><span> matrix.shape[</span><span>0</span><span>]</span></span>
<span><span>    # Initialize scores</span></span>
<span><span>    scores </span><span>=</span><span> np.</span><span>ones</span><span>(size) </span><span>/</span><span> size</span></span>
<span></span>
<span><span>    for</span><span> _ </span><span>in</span><span> range</span><span>(num_iterations):</span></span>
<span><span>        new_scores </span><span>=</span><span> np.</span><span>zeros</span><span>(size)</span></span>
<span><span>        for</span><span> i </span><span>in</span><span> range</span><span>(size):</span></span>
<span><span>            # Get nodes that point to current node</span></span>
<span><span>            incoming </span><span>=</span><span> np.</span><span>where</span><span>(matrix[:, i])[</span><span>0</span><span>]</span></span>
<span><span>            for</span><span> j </span><span>in</span><span> incoming:</span></span>
<span><span>                # Add score contribution from incoming node</span></span>
<span><span>                new_scores[i] </span><span>+=</span><span> scores[j] </span><span>/</span><span> np.</span><span>sum</span><span>(matrix[j]) </span></span>
<span><span>        # Apply damping factor</span></span>
<span><span>        scores </span><span>=</span><span> (</span><span>1</span><span> -</span><span> DAMPING</span><span>) </span><span>/</span><span> size </span><span>+</span><span> DAMPING</span><span> *</span><span> new_scores </span></span>
<span></span>
<span><span>    return</span><span> scores</span></span></code></pre>
<p>The highlighted lines show the two most computationally intensive parts of the
algorithm. The first computes the score contribution from incoming nodes, while
the second applies the damping factor, incorporating the new scores into the
final result.</p>
<p>Parallelizing the first part will be the most beneficial and easy to implement
since we can divide the range and thus efficiently use multiple threads to
compute the <code>new_scores</code> array.</p>
<h3 id="multithreaded-implementation"><a href="#multithreaded-implementation">Multithreaded Implementation<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h3>
<p>For the multithreaded implementation, we'll start by dividing the matrix into
multiple chunks:</p>
<pre tabindex="0"><code><span><span>chunk_size </span><span>=</span><span> size </span><span>//</span><span> num_threads</span></span>
<span><span>chunks </span><span>=</span><span> [(i, </span><span>min</span><span>(i </span><span>+</span><span> chunk_size, size)) </span><span>for</span><span> i </span><span>in</span><span> range</span><span>(</span><span>0</span><span>, size, chunk_size)]</span></span></code></pre>
<p>Each thread will then work on a different chunk of the matrix, updating the new
scores:</p>
<pre tabindex="0"><code><span><span>def</span><span> _thread_worker</span><span>(</span></span>
<span><span>    matrix</span><span>: np.ndarray,</span></span>
<span><span>    scores</span><span>: np.ndarray,</span></span>
<span><span>    new_scores</span><span>: np.ndarray,</span></span>
<span><span>    start_idx</span><span>: </span><span>int</span><span>,</span></span>
<span><span>    end_idx</span><span>: </span><span>int</span><span>,</span></span>
<span><span>    lock</span><span>: threading.Lock,</span></span>
<span><span>):</span></span>
<span><span>    size </span><span>=</span><span> matrix.shape[</span><span>0</span><span>]</span></span>
<span><span>    local_scores </span><span>=</span><span> np.</span><span>zeros</span><span>(size)</span></span>
<span></span>
<span><span>    for</span><span> i </span><span>in</span><span> range</span><span>(start_idx, end_idx):</span></span>
<span><span>        incoming </span><span>=</span><span> np.</span><span>where</span><span>(matrix[:, i])[</span><span>0</span><span>]</span></span>
<span><span>        for</span><span> j </span><span>in</span><span> incoming:</span></span>
<span><span>            local_scores[i] </span><span>+=</span><span> scores[j] </span><span>/</span><span> np.</span><span>sum</span><span>(matrix[j])</span></span>
<span></span>
<span><span>    with</span><span> lock: </span></span>
<span><span>        new_scores </span><span>+=</span><span> local_scores </span></span></code></pre>
<p>It's important to note that updating the <code>new_scores</code> array is done behind a
lock to prevent race conditions. This could potentially become a bottleneck if
the lock is held for too long, but in practice, parallelizing the first part of
the algorithm should provide a significant speedup already.</p>
<p>Finally, we'll feed the chunks to each of the threads:</p>
<pre tabindex="0"><code><span><span>new_scores </span><span>=</span><span> np.</span><span>zeros</span><span>(size)</span></span>
<span><span>lock </span><span>=</span><span> threading.</span><span>Lock</span><span>() </span></span>
<span><span>with</span><span> concurrent.futures.</span><span>ThreadPoolExecutor</span><span>(</span><span>max_workers</span><span>=</span><span>num_threads) </span><span>as</span><span> executor: </span></span>
<span><span>    # Process chunks in parallel</span></span>
<span><span>    futures </span><span>=</span><span> executor.</span><span>map</span><span>( </span></span>
<span><span>        lambda</span><span> args</span><span>: </span><span>_thread_worker</span><span>(*args), </span><span># starmap isn't available on ThreadPoolExecutor</span></span>
<span><span>        [ </span></span>
<span><span>            (matrix, scores, new_scores, start_idx, end_idx, lock) </span></span>
<span><span>            for</span><span> start_idx, end_idx </span><span>in</span><span> chunks </span></span>
<span><span>        ], </span></span>
<span><span>    ) </span></span>
<span><span>new_scores </span><span>=</span><span> (</span><span>1</span><span> -</span><span> DAMPING</span><span>) </span><span>/</span><span> size </span><span>+</span><span> DAMPING</span><span> *</span><span> new_scores</span></span>
<span><span>scores </span><span>=</span><span> new_scores</span></span></code></pre>
<h3 id="multiprocessing-implementation"><a href="#multiprocessing-implementation">Multiprocessing Implementation<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h3>
<p>Essentially, the <code>multiprocessing</code> implementation is extremely similar to the
<code>threading</code> one, let's focus on the differences:</p>
<ul>
<li>
<p>Each worker, will now return the <code>local_scores</code> array instead of updating a
shared <code>new_scores</code> array since processes can't share memory directly. The
local scores will then be aggregated in the main process:</p>
<pre tabindex="0"><code><span><span># Combine results</span></span>
<span><span>new_scores </span><span>=</span><span> sum</span><span>(chunk_results)</span></span></code></pre>
<p>While this should be faster than the threading version, it still incurs the
overhead of the inter-process communication which can become very significant
for large datasets.</p>
</li>
<li>
<p>Instead of using a <code>ThreadPoolExecutor</code>, it will use a <code>multiprocessing.Pool</code>.
The APIs are very similar, but <code>multiprocessing.Pool</code> will spawn a pool of
processes instead of threads:</p>
<pre tabindex="0"><code><span><span>with</span><span> multiprocessing.</span><span>Pool</span><span>(</span><span>processes</span><span>=</span><span>num_processes) </span><span>as</span><span> pool: </span></span>
<span><span>    # Process chunks in parallel</span></span>
<span><span>    chunk_results </span><span>=</span><span> pool.</span><span>starmap</span><span>(_process_chunk, chunks) </span></span>
<span><span>    # Combine results</span></span>
<span><span>    new_scores </span><span>=</span><span> sum</span><span>(chunk_results)</span></span>
<span><span>    new_scores </span><span>=</span><span> (</span><span>1</span><span> -</span><span> DAMPING</span><span>) </span><span>/</span><span> size </span><span>+</span><span> DAMPING</span><span> *</span><span> new_scores</span></span>
<span><span>    scores </span><span>=</span><span> new_scores</span></span></code></pre>
</li>
</ul>
<h2 id="measuring-performance"><a href="#measuring-performance">Measuring Performance<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h2>
<p>In order to measure the actual performance changes, let's build a performance
test. First things first, we need to generate some testing data:</p>
<pre tabindex="0"><code><span><span>def</span><span> create_test_graph</span><span>(</span><span>size</span><span>: </span><span>int</span><span>) -&gt; np.ndarray:</span></span>
<span><span>    # Fixed seed</span></span>
<span><span>    np.random.</span><span>seed</span><span>(</span><span>0</span><span>) </span></span>
<span><span>    # Create random adjacency matrix with ~5 outgoing edges per node</span></span>
<span><span>    matrix </span><span>=</span><span> np.random.</span><span>choice</span><span>([</span><span>0</span><span>, </span><span>1</span><span>], </span><span>size</span><span>=</span><span>(size, size), </span><span>p</span><span>=</span><span>[</span><span>1</span><span> -</span><span> 5</span><span>/</span><span>size, </span><span>5</span><span>/</span><span>size])</span></span>
<span><span>    # Find nodes with no outgoing edges</span></span>
<span><span>    zero_outdegree </span><span>=</span><span> ~</span><span>matrix.</span><span>any</span><span>(</span><span>axis</span><span>=</span><span>1</span><span>)</span></span>
<span><span>    zero_indices </span><span>=</span><span> np.</span><span>where</span><span>(zero_outdegree)[</span><span>0</span><span>]</span></span>
<span><span>    # For each node with no outgoing edges, add a random edge</span></span>
<span><span>    if</span><span> len</span><span>(zero_indices) </span><span>&gt;</span><span> 0</span><span>:</span></span>
<span><span>        random_targets </span><span>=</span><span> np.random.</span><span>randint</span><span>(</span><span>0</span><span>, size, </span><span>size</span><span>=len</span><span>(zero_indices))</span></span>
<span><span>        matrix[zero_indices, random_targets] </span><span>=</span><span> 1</span></span>
<span></span>
<span><span>    return</span><span> matrix</span></span></code></pre>
<p>As highlighted, we're using a fixed seed to ensure reproducibility from one run
to the next. This is important when comparing the performance of different
implementations. Here we're building some fake connections between pages to
build a realistic graph but the mathematical operations would be exactly the
same with an empty matrix as long as the size is the same.</p>
<p>Next, we'll use
<a target="blank" href="https://github.com/CodSpeedHQ/pytest-codspeed"><code>pytest-codspeed</code></a>, a <code>pytest</code>
plugin to measure the performance of the different implementations with various
parameter and with multiple builds/versions of CPython.</p>
<p>First let's define the benchmark cases:</p>
<pre tabindex="0"><code><span><span>@pytest</span><span>.</span><span>mark</span><span>.</span><span>parametrize</span><span>(</span></span>
<span><span>    "pagerank"</span><span>,</span></span>
<span><span>    [</span></span>
<span><span>        pagerank_single</span><span>,</span></span>
<span><span>        partial</span><span>(</span><span>pagerank_multiprocess</span><span>,</span><span> num_processes</span><span>=</span><span>8</span><span>),</span></span>
<span><span>        partial</span><span>(</span><span>pagerank_multithread</span><span>,</span><span> num_threads</span><span>=</span><span>8</span><span>),</span></span>
<span><span>    ],</span></span>
<span><span>    ids</span><span>=</span><span>[</span><span>"single"</span><span>,</span><span> "8-processes"</span><span>,</span><span> "8-threads"</span><span>],</span></span>
<span><span>)</span></span>
<span><span>@pytest</span><span>.</span><span>mark</span><span>.</span><span>parametrize</span><span>(</span></span>
<span><span>    "graph"</span><span>,</span></span>
<span><span>    [</span></span>
<span><span>        create_test_graph</span><span>(</span><span>100</span><span>),</span></span>
<span><span>        create_test_graph</span><span>(</span><span>1000</span><span>),</span></span>
<span><span>        create_test_graph</span><span>(</span><span>2000</span><span>),</span></span>
<span><span>    ],</span></span>
<span><span>    ids</span><span>=</span><span>[</span><span>"XS"</span><span>,</span><span> "L"</span><span>,</span><span> "XL"</span><span>],</span></span>
<span><span>)</span></span>
<span><span>def</span><span> test_pagerank</span><span>(</span></span>
<span><span>    benchmark</span><span>: BenchmarkFixture,</span></span>
<span><span>    pagerank</span><span>: PagerankFunc,</span></span>
<span><span>    graph</span><span>: np.ndarray,</span></span>
<span><span>):</span></span>
<span><span>    benchmark</span><span>(pagerank, graph, </span><span>num_iterations</span><span>=</span><span>10</span><span>)</span></span></code></pre>
<p>Here we're testing the 3 implementations with 3 different graph sizes. The
<code>benchmark</code> fixture is provided by <code>pytest-codspeed</code> and will measure the
execution time of the <code>pagerank</code> function with the given args.</p>
<p>Then, let's write a GitHub Actions workflow to measure the performance with
various builds of CPython on CodSpeed's infrastructure:</p>
<pre tabindex="0"><code><span><span>on</span><span>:</span></span>
<span><span>  push</span><span>:</span></span>
<span><span>jobs</span><span>:</span></span>
<span><span>  codspeed</span><span>:</span></span>
<span><span>    runs-on</span><span>: </span><span>codspeed-macro</span><span> # requests a CodSpeed Macro runner for the jobs</span></span>
<span><span>    strategy</span><span>:</span></span>
<span><span>      matrix</span><span>:</span></span>
<span><span>        python-version</span><span>: [</span><span>"3.12"</span><span>, </span><span>"3.13"</span><span>]</span></span>
<span><span>        include</span><span>:</span></span>
<span><span>          - { </span><span>python-version</span><span>: </span><span>"3.13t"</span><span>, </span><span>gil</span><span>: </span><span>"1"</span><span> } </span></span>
<span><span>          - { </span><span>python-version</span><span>: </span><span>"3.13t"</span><span>, </span><span>gil</span><span>: </span><span>"0"</span><span> } </span></span>
<span><span>    env</span><span>:</span></span>
<span><span>      UV_PYTHON</span><span>: </span><span>${{ matrix.python-version }}</span></span>
<span><span>    steps</span><span>:</span></span>
<span><span>      - </span><span>uses</span><span>: </span><span>actions/checkout@v4</span></span>
<span><span>      - </span><span>name</span><span>: </span><span>Install uv</span></span>
<span><span>        uses</span><span>: </span><span>astral-sh/setup-uv@v3</span></span>
<span><span>      - </span><span>name</span><span>: </span><span>Install CPython &amp; dependencies</span></span>
<span><span>        run</span><span>: </span><span>uv sync --all-extras</span></span>
<span><span>      - </span><span>name</span><span>: </span><span>Run benchmarks</span></span>
<span><span>        uses</span><span>: </span><span>CodSpeedHQ/action@v3</span></span>
<span><span>        env</span><span>:</span></span>
<span><span>          PYTHON_GIL</span><span>: </span><span>${{ matrix.gil }}</span></span>
<span><span>        with</span><span>:</span></span>
<span><span>          run</span><span>: </span><span>uv run pytest --codspeed --codspeed-max-time 10 -vs src/tests.py</span></span></code></pre>
<p>Here we're running the benchmarks with Python 3.12, 3.13, and 3.13 with free
threading support (<code>3.13t</code>), both with and without the GIL. Running 3.13 both
with and without free-threading support will allow us to see its impact on the
performance even when the GIL is enabled.</p>
<p>The python builds used are pulled by <code>uv</code> directly from
<a target="blank" href="https://github.com/indygreg/python-build-standalone">python-build-standalone</a>
built with <code>GCC 6.3.0 20170516</code></p>
<p>The jobs will run on
<a target="blank" href="https://docs.codspeed.io/instruments/walltime/">CodSpeed Macro</a> runners, which
are ARM64 bare-metal instances with 16 cores and 32GB of RAM, dedicated to each
job.</p>
<h2 id="results"><a href="#results">Results<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h2>
<figure><div><p>Measured by</p></div><figcaption><p>Time (in s) for the L graph size, <strong>less is better</strong></p></figcaption></figure>
<!-- -->
<ul>
<li>Without enabling new build options both <b>3.12</b> and
<b>3.13</b> perform very similarly.
We also clearly see the limitation of the <code>multiprocessing</code> implementation being even
slower than the single-threaded one due to the overhead of the inter-process communication.</li>
</ul>
<!-- -->
<ul>
<li>As expected the <code>threading</code> based implementation is the fastest when running
<b>3.13t with no GIL</b> and the GIL is effectively not limiting the
parallel execution of the threads anymore.</li>
</ul>
<!-- -->
<ul>
<li>Still, when running with the free threaded build both <b>with</b> and
<b>without</b> the GIL, we see a significant
slowdown for all other implementations. This is mostly because the free-threaded build
requires the specializing adaptive interpreter to be disabled, thus clearly decreasing
the performance of the other implementations. This overhead should be reduced in the
3.14 release where the specializing adaptive interpreter will be thread-safe and thus
will be re-enabled. At that point, migrating to the free-threaded build should be a
no-brainer for a lot of parallel applications and it will be interesting to measure
the performance changes.</li>
</ul>
<p>For all other graph sizes, the results are very similar and the conclusions are
the same. From this measurement, we can see that the new free-threaded build of
CPython 3.13 can have a significant impact on the performance of parallel
applications, bringing a very relevant alternative to <code>multiprocessing</code>. Still
it's experimental and not yet ready for production use because of the overall
slowdown it introduces, but it's a very promising step in the right direction!</p>
<h4 id="note"><a href="#note">Note<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h4>
<p>This benchmark doesn't include subinterpreters, which is another way to run
Python code parallelly without the GIL introduced in Python 3.12. Subintepreters
proved to be slower than other approaches in most cases, mostly because where
data sharing and inter-worker communication has not been fully solved yet. But
when it's there, it might definitely be a nice alternative to <code>multiprocessing</code>.</p>
<h2 id="resources"><a href="#resources">Resources<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h2>
<ul>
<li>
<p><a target="blank" href="https://github.com/CodSpeedHQ/python-parallel-pagerank/">Repository with the full code</a></p>
</li>
<li>
<p><a target="blank" href="https://docs.python.org/3.13/whatsnew/3.13.html">What's New In Python 3.13?</a></p>
</li>
<li>
<p><a target="blank" href="https://docs.python.org/3/howto/free-threading-python.html">Python experimental support for free threading</a></p>
</li>
<li>
<p><a target="blank" href="https://py-free-threading.github.io/">py-free-threading</a>: a centralized
collection of documentation and trackers around compatibility with
free-threaded CPython</p>
</li>
<li>
<p><a target="blank" href="https://peps.python.org/pep-0659/">PEP 659 - Specializing Adaptive Interpreter</a></p>
</li>
<li>
<p><a target="blank" href="https://docs.google.com/document/d/1hsV25JSwDb08c6-aHrI_YBHyl_uQKlys0ODihSx_aSw">Docs on having the Specializing Adaptive Interpreter without the GIL</a></p>
</li>
</ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: rallyup ‚Äì Lightweight Wake-on-LAN Scheduler (128 pts)]]></title>
            <link>https://github.com/darwindarak/rallyup</link>
            <guid>42050862</guid>
            <pubDate>Tue, 05 Nov 2024 12:04:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/darwindarak/rallyup">https://github.com/darwindarak/rallyup</a>, See on <a href="https://news.ycombinator.com/item?id=42050862">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto"><code>rallyup</code></h2><a id="user-content-rallyup" aria-label="Permalink: rallyup" href="#rallyup"></a></p>
<p dir="auto"><code>rallyup</code> is a lightweight Wake-On-LAN (WOL) scheduler and dependency manager designed for small businesses and homelabs. It ensures that infrastructure services like firewalls, storage, and hypervisors are brought online in the correct order, particularly after events like power outages.</p>
<p dir="auto">A typical setup involves configuring most of the infrastructure for WOL but not for Wake-On-Power, and setting <code>rallyup</code> to run on startup on a low-power device like a Raspberry Pi. When you need to bring the entire environment online, simply power on the device running <code>rallyup</code>, and the rest of the infrastructure will automatically follow in the correct order.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/darwindarak/rallyup/actions/workflows/tests.yml/badge.svg"><img src="https://github.com/darwindarak/rallyup/actions/workflows/tests.yml/badge.svg" alt="Tests"></a>
<a href="https://crates.io/crates/rallyup" rel="nofollow"><img src="https://camo.githubusercontent.com/d06930b3b5f4b41abbd2ec7ab3bc8e39ba0efc33a314dd6961304cfc0d37d75c/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f72616c6c7975702e737667" alt="Crates.io" data-canonical-src="https://img.shields.io/crates/v/rallyup.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul>
<li> <em>VLAN Support</em>: Send WOL packets to devices across different VLANs.</li>
<li> <em>YAML Configuration</em>: Easily define server boot sequences, dependencies, and status checks.</li>
<li> <em>Service Status Checks</em>: Verify that a service is up using built-in status checks (HTTP health checks, NFS, SMB, custom shell commands).
<ul>
<li> HTTP</li>
<li> Open port</li>
<li> Shell</li>
<li> NFS (might just use open port check)</li>
<li> SMB (might just use open port check)</li>
</ul>
</li>
<li> <em>Plugin-Friendly</em>: Users can write their own custom status check plugins.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">The dependencies between servers, along with the methods for validating that they are online, are defined in a YAML configuration file.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Servers Configuration</h2><a id="user-content-servers-configuration" aria-label="Permalink: Servers Configuration" href="#servers-configuration"></a></p>
<p dir="auto"><strong>Fields</strong>:</p>
<ul dir="auto">
<li><strong>name</strong>: The name of the server, used for identification when defining dependencies between servers</li>
<li><strong>mac</strong>: The MAC address of the server we want to wake up</li>
<li><strong>interface</strong>: The network interface to use when sending the WOL packet</li>
<li><strong>vlan</strong>: The VLAN ID (optional) that the server is on</li>
<li><strong>depends</strong>: A list of other server names that this server depends on</li>
<li><strong>check</strong>: A list of health checks that must pass before this server is considered fully online</li>
</ul>
<p dir="auto"><strong>Example</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="- name: &quot;firewall&quot;
  mac: &quot;00:11:22:33:44:55&quot;
  interface: &quot;eth0&quot;
  vlan: 100
  depends:
    - &quot;storage&quot;
  check: [... see below]"><pre>- <span>name</span>: <span><span>"</span>firewall<span>"</span></span>
  <span>mac</span>: <span><span>"</span>00:11:22:33:44:55<span>"</span></span>
  <span>interface</span>: <span><span>"</span>eth0<span>"</span></span>
  <span>vlan</span>: <span>100</span>
  <span>depends</span>:
    - <span><span>"</span>storage<span>"</span></span>
  <span>check</span>: <span>[... see below]</span></pre></div>
<ul dir="auto">
<li></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Health Check Configurations</h2><a id="user-content-health-check-configurations" aria-label="Permalink: Health Check Configurations" href="#health-check-configurations"></a></p>
<p dir="auto">Each server can have multiple health checks to ensure the server is fully online before the next device starts up.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Common Fields</h3><a id="user-content-common-fields" aria-label="Permalink: Common Fields" href="#common-fields"></a></p>
<ul dir="auto">
<li><strong>retry</strong>: The interval, defined in human readable string (e.g. 1s, 1 minute, etc.) to wait between retrying this health check</li>
<li><strong>timeout</strong>: The timeout interval after which the check, and subsequently the entire boot sequence, will fail</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Built-in Health Checks</h3><a id="user-content-built-in-health-checks" aria-label="Permalink: Built-in Health Checks" href="#built-in-health-checks"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">HTTP Health Checks</h4><a id="user-content-http-health-checks" aria-label="Permalink: HTTP Health Checks" href="#http-health-checks"></a></p>
<p dir="auto">The HTTP health check verifies whether a specified endpoint responds as expected.</p>
<p dir="auto"><strong>Fields</strong></p>
<ul dir="auto">
<li><strong>type</strong>: should be <code>http</code> for an HTTP health check.</li>
<li><strong>url</strong>:  The URL to perform the HTTP health check against</li>
<li><strong>status</strong>: Expected HTTP status code</li>
<li><strong>regex</strong>: Regex to match in the response body</li>
</ul>
<blockquote>
<p dir="auto">Note: You must provide either <code>status</code> or <code>regex</code>, or both.</p>
</blockquote>
<p dir="auto"><strong>Example</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="- type: http
  url: &quot;http://192.168.1.1/health&quot;
  status: 200
  retry: 5s
  timeout: 30s"><pre>- <span>type</span>: <span>http</span>
  <span>url</span>: <span><span>"</span>http://192.168.1.1/health<span>"</span></span>
  <span>status</span>: <span>200</span>
  <span>retry</span>: <span>5s</span>
  <span>timeout</span>: <span>30s</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Port Health Check</h4><a id="user-content-port-health-check" aria-label="Permalink: Port Health Check" href="#port-health-check"></a></p>
<p dir="auto">The port health check verifies whether a specified TCP port on a server is open and accessible.
This is really a stand-in for verifying NFS and SMB ports until I can figure out how to check if those services are online.</p>
<p dir="auto"><strong>Fields</strong></p>
<ul dir="auto">
<li><strong>type</strong>: should be <code>port</code> for a port health check</li>
<li><strong>ip</strong>: the IP address to check</li>
<li><strong>port</strong>: the port number to check</li>
</ul>
<p dir="auto"><strong>Example</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="- type: port
  ip: &quot;192.168.1.1&quot;
  port: 22
  retry: &quot;10s&quot;
  timeout: &quot;1m&quot;"><pre>- <span>type</span>: <span>port</span>
  <span>ip</span>: <span><span>"</span>192.168.1.1<span>"</span></span>
  <span>port</span>: <span>22</span>
  <span>retry</span>: <span><span>"</span>10s<span>"</span></span>
  <span>timeout</span>: <span><span>"</span>1m<span>"</span></span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Shell Health Checks</h4><a id="user-content-shell-health-checks" aria-label="Permalink: Shell Health Checks" href="#shell-health-checks"></a></p>
<p dir="auto">The shell health check executes a shell command checks the result.
This is to provide the option of user-defined health checks.</p>
<p dir="auto"><strong>Fields</strong></p>
<ul dir="auto">
<li><strong>type</strong>: should be <code>shell</code> for a shell health check.</li>
<li><strong>command</strong>:  he shell command to execute</li>
<li><strong>status</strong>: Expected exit code</li>
<li><strong>regex</strong>: Regex to match in the standard output</li>
</ul>
<blockquote>
<p dir="auto">Note: You must provide either <code>status</code> or <code>regex</code>, or both.</p>
</blockquote>
<p dir="auto"><strong>Example</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="- type: shell
  command: ping -c 1 192.168.1.1
  status: 0
  retry: 5s
  timeout: 20s"><pre>- <span>type</span>: <span>shell</span>
  <span>command</span>: <span>ping -c 1 192.168.1.1</span>
  <span>status</span>: <span>0</span>
  <span>retry</span>: <span>5s</span>
  <span>timeout</span>: <span>20s</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Full Example</h3><a id="user-content-full-example" aria-label="Permalink: Full Example" href="#full-example"></a></p>
<blockquote>
<p dir="auto">TODO:</p>
<ul>
<li> Need to test in the lab and post the actual sample</li>
</ul>
</blockquote>
<div dir="auto" data-snippet-clipboard-copy-content="- name: &quot;Firewall&quot;
  mac: &quot;00:1A:2B:3C:4D:5E&quot;
  interface: eth0
  vlan: 10
  depends: []
  check:
    - type: http
      url: &quot;http://192.168.1.1/health&quot;
      status: 200
      regex: 'ok'

- name: &quot;Storage Server 1&quot;
  mac: &quot;00:1A:2B:3C:4D:5F&quot;
  interface: eth0
  vlan: 100
  depends:
    - &quot;Firewall&quot;
  check:
    - type: port
      ip: 192.168.100.101
      port: 2049
      timeout: 5 minutes

- name: &quot;Storage Server 2&quot;
  mac: &quot;00:1A:2B:3C:4D:5G&quot;
  vlan: 100
  depends:
    - &quot;Firewall&quot;
  check:
    - type: port
      ip: 192.168.100.102
      port: 445
      retry: 5s

- name: &quot;VM Host&quot;
  mac: &quot;00:1A:2B:3C:4D:60&quot;
  vlan: 200
  depends:
    - &quot;Storage Server 1&quot;
    - &quot;Storage Server 2&quot;
  check:
    - type: command
      command: &quot;ping -c 192.168.200.10&quot;
      status: 0"><pre>- <span>name</span>: <span><span>"</span>Firewall<span>"</span></span>
  <span>mac</span>: <span><span>"</span>00:1A:2B:3C:4D:5E<span>"</span></span>
  <span>interface</span>: <span>eth0</span>
  <span>vlan</span>: <span>10</span>
  <span>depends</span>: <span>[]</span>
  <span>check</span>:
    - <span>type</span>: <span>http</span>
      <span>url</span>: <span><span>"</span>http://192.168.1.1/health<span>"</span></span>
      <span>status</span>: <span>200</span>
      <span>regex</span>: <span><span>'</span>ok<span>'</span></span>

- <span>name</span>: <span><span>"</span>Storage Server 1<span>"</span></span>
  <span>mac</span>: <span><span>"</span>00:1A:2B:3C:4D:5F<span>"</span></span>
  <span>interface</span>: <span>eth0</span>
  <span>vlan</span>: <span>100</span>
  <span>depends</span>:
    - <span><span>"</span>Firewall<span>"</span></span>
  <span>check</span>:
    - <span>type</span>: <span>port</span>
      <span>ip</span>: <span>192.168.100.101</span>
      <span>port</span>: <span>2049</span>
      <span>timeout</span>: <span>5 minutes</span>

- <span>name</span>: <span><span>"</span>Storage Server 2<span>"</span></span>
  <span>mac</span>: <span><span>"</span>00:1A:2B:3C:4D:5G<span>"</span></span>
  <span>vlan</span>: <span>100</span>
  <span>depends</span>:
    - <span><span>"</span>Firewall<span>"</span></span>
  <span>check</span>:
    - <span>type</span>: <span>port</span>
      <span>ip</span>: <span>192.168.100.102</span>
      <span>port</span>: <span>445</span>
      <span>retry</span>: <span>5s</span>

- <span>name</span>: <span><span>"</span>VM Host<span>"</span></span>
  <span>mac</span>: <span><span>"</span>00:1A:2B:3C:4D:60<span>"</span></span>
  <span>vlan</span>: <span>200</span>
  <span>depends</span>:
    - <span><span>"</span>Storage Server 1<span>"</span></span>
    - <span><span>"</span>Storage Server 2<span>"</span></span>
  <span>check</span>:
    - <span>type</span>: <span>command</span>
      <span>command</span>: <span><span>"</span>ping -c 192.168.200.10<span>"</span></span>
      <span>status</span>: <span>0</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under either of the following licenses, at your option:</p>
<ul dir="auto">
<li><a href="https://github.com/darwindarak/rallyup/blob/master/LICENSE-MIT">MIT License</a></li>
<li><a href="https://github.com/darwindarak/rallyup/blob/master/LICENSE-APACHE">Apache License 2.0</a></li>
</ul>
<p dir="auto">You may choose to use this project under the terms of either license.</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>