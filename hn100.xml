<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 12 Dec 2024 20:30:13 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Android XR (171 pts)]]></title>
            <link>https://blog.google/products/android/android-xr/</link>
            <guid>42400556</guid>
            <pubDate>Thu, 12 Dec 2024 16:26:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/products/android/android-xr/">https://blog.google/products/android/android-xr/</a>, See on <a href="https://news.ycombinator.com/item?id=42400556">Hacker News</a></p>
Couldn't get https://blog.google/products/android/android-xr/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[The FDA Hasn't Inspected This Drug Factory After 7 Recalls for the Same Flaw (118 pts)]]></title>
            <link>https://www.propublica.org/article/glenmark-pharmaceuticals-recalls-fda-oversight</link>
            <guid>42399530</guid>
            <pubDate>Thu, 12 Dec 2024 14:45:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.propublica.org/article/glenmark-pharmaceuticals-recalls-fda-oversight">https://www.propublica.org/article/glenmark-pharmaceuticals-recalls-fda-oversight</a>, See on <a href="https://news.ycombinator.com/item?id=42399530">Hacker News</a></p>
Couldn't get https://www.propublica.org/article/glenmark-pharmaceuticals-recalls-fda-oversight: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Fermat's Last Theorem – how it's going (190 pts)]]></title>
            <link>https://xenaproject.wordpress.com/2024/12/11/fermats-last-theorem-how-its-going/</link>
            <guid>42399397</guid>
            <pubDate>Thu, 12 Dec 2024 14:30:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xenaproject.wordpress.com/2024/12/11/fermats-last-theorem-how-its-going/">https://xenaproject.wordpress.com/2024/12/11/fermats-last-theorem-how-its-going/</a>, See on <a href="https://news.ycombinator.com/item?id=42399397">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						
<p>So I’m two months into trying to teach a proof of Fermat’s Last Theorem (FLT) to a computer. Most of “how it’s going” is quite tedious and technical to explain: to cut a long story short, Wiles proved an “R=T” theorem and much of the work so far has gone into teaching the computer what R and T are; we still have not finished either definition. However my PhD student Andrew Yang has already proved the abstract commutative algebra result which we need (“if abstract rings R and T satisfy lots of technical conditions then they’re equal”), and this is an exciting first step. The current state of the write-up is <a href="https://imperialcollegelondon.github.io/FLT/blueprint/">here</a>, and the system we are using is <a href="https://lean-fro.org/">Lean</a> and its mathematical library <a href="https://github.com/leanprover-community/mathlib4">mathlib</a>, maintained by the <a href="https://leanprover-community.github.io/">Lean prover community</a>. If you know a bit of Lean and a bit of number theory then feel free to read the <a href="https://github.com/ImperialCollegeLondon/FLT/blob/main/CONTRIBUTING.md">contribution guidelines</a>, checkout the <a href="https://github.com/orgs/ImperialCollegeLondon/projects/102">project dashboard</a> and claim an <a href="https://github.com/ImperialCollegeLondon/FLT/issues">issue</a>. As I say, we’re two months in. However we already have one interesting story, which I felt was worth sharing. Who knows if it’s an indication of what is to come.</p>



<p>We’re not formalising the old-fashioned 1990s proof of FLT. Since then, work of many people (Diamond/Fujiwara, Kisin, Taylor, Scholze and several others) led to the proof being generalised and simplified, and part of the motivation of my work is not to just get FLT over the line but to prove these more general and powerful results. Why? Because if the AI mathematics revolution actually happens (which it might) and if Lean turns out to be an important component (which it might) then computers will be in a better position to start helping humans to push back the boundaries of modern number theory because of this formalization work, as they’ll have access to key modern definitions in a form which they understand. One concept which was not used in Wiles’ original proof but which is being used in the proof we’re formalizing, is crystalline cohomology, a theory developed in the 60s and 70s in Paris, with the foundations laid down by Berthelot following ideas of Grothendieck. The basic idea here is that the classical exponential and logarithm functions play a key role in differential geometry (relating Lie algebras and Lie groups, for example), and in particular in understanding de Rham cohomology, but they do not work in more arithmetic situations (for example in characteristic p); the theory of “divided power structures”, developed in the 1960s in a series of beautiful papers by Roby, play a crucial role in constructing an analogue of these functions which can be used in the arithmetic case. tl;dr: we need to teach the computer crystalline cohomology, so first we need to teach it divided powers.</p>



<p>Antoine Chambert-Loir and Maria Ines de Frutos Fernandez have been teaching the theory of divided powers to Lean, and over the summer Lean did that irritating thing which it sometimes does: it complained about the human presentation of an argument in the standard literature, and on closer inspection it turned out that the human argument left something to be desired. In particular a key lemma in Roby’s work seems to be incorrect. When Antoine told me this in a DM, he remarked that he would suppose I thought this was funny, and indeed the very long string of laughing emojis which he got back as a response to his message confirmed this. However Antoine, being rather more professional than me, argued that instead of me tweeting about the issue (which I can’t do anyway because I left Twitter and joined <a href="https://bsky.app/profile/xenaproject.bsky.social">bluesky</a> yesterday), we should in fact attempt to fix the problem. We went about this in rather different ways. Antoine put it on his job list to look at, and I completely ignored the problem and just started occasionally mentioning to people that the proof was in trouble, in a weak sense. I say “in a weak sense” because this observation has to be put into some context. According to the way I currently view mathematics (as a formalist), the entire theory of crystalline cohomology vanished from the literature at the moment Antoine discovered the issue, with massive collateral damage (for example huge chunks of Scholze’s work just disappeared, entire books and papers vaporised etc). But this disappearance is only <em>temporary</em>. Crystalline cohomology is in no practical sense “wrong”. The theorems were still undoubtedly <em>correct</em>, it’s just that the proofs were as far as I am concerned <em>incomplete</em> (or at least, the ones Antoine and Maria Ines were following were), and unfortunately it is now our job to fix them. The thing I want to stress is that it was absolutely clear to both me and Antoine that the proofs of the main results were of course going to be <em>fixable</em>, even if an intermediate lemma was false, because crystalline cohomology has been used so much since the 1970s that if there were a problem with it, it would have come to light a long time ago. Every expert I’ve spoken to is in complete agreement on this point (and several even went so far as to claim that I’m making a fuss about nothing, but perhaps they don’t understand what formalization actually means in practice: you can’t just say “I’m sure it’s fixable” — you have to <strong>actually fix it</strong>). One added twist is that Roby, Grothendieck and Berthelot have all died, so we could not go back to the original experts and ask directly for help.</p>



<p>[For those that are interested in more technical details, here they are: Berthelot’s thesis does not develop the theory of divided powers from scratch, he uses Roby’s “Les algebres a puissances divisees”, published in Bull Sci Math, 2ieme serie, 89, 1965, pages 75-91. Lemme 8 (on p86) of that paper seems to be false and it’s not obvious how to repair the proof; the proof of the lemma misquotes another lemma of Roby from his 1963 Ann Sci ENS paper; the correct statement is Gamma_A(M) tensor_A R = Gamma_R(M tensor_A R) but one of the tensor products accidentally falls off in the application. This breaks Roby’s proof that the divided power algebra of a module has divided powers, and thus stops us from defining the ring A_{cris}.]</p>



<p>So as I say, Antoine worked on fixing the problem, whereas I just worked on gossiping about it to the experts, and I made the mistake of telling Tadashi Tokieda about it in a <a href="https://www.instagram.com/coffeezeeholloway/">coffeeshop in Islington</a>, he duly went back to Stanford and mentioned it to Brian Conrad, and the next thing I knew Conrad was in my inbox asking me what was all this about crystalline cohomology being wrong. I explained the technical details of the issue, Conrad agreed that there seemed to be a problem and he went off to think about it. Several hours later he got back to me and pointed out that another, different, proof of the claim that the universal divided power algebra of a module had divided powers was in the appendix to the Berthelot-Ogus book on crystalline cohomology, and that as far as Conrad was concerned this approach should be fine. The proof was back on! </p>



<p>And that is pretty much the end of the story, other than the fact that last month I visited Berkeley and I had lunch with Arthur Ogus, who I’ve known since I was a post-doc there in the 90s. I’d promised Arthur a story of how he’d saved Fermat’s Last Theorem, and over the meal I told him about how his appendix had dug me out of a hole. His response was “Oh! That appendix has several errors in it! But it’s OK, I think I know how to fix them.”</p>



<p>This story really highlights, to me, the poor job which humans do of documenting modern mathematics. There appear to be so many things which are “known to the experts” but not correctly documented. The experts are in agreement that the important ideas are robust enough to withstand knocks like this, but the <em>details</em> of what is actually going on might not actually be where you expect them to be. For me, this is just one of many reasons why humans might want to consider getting mathematics written down <em>properly</em>, i.e. in a formal system, where the chances of error are orders of magnitude smaller. However most mathematicians are not formalists, and for those people I need to justify my work in a different way. For those mathematicians, I argue that teaching machines our arguments is a crucial step towards getting machines to do it themselves. Until then, we seemed to be doomed to fix up human errors manually.</p>



<p>The story does have a happy ending though — two weeks ago Maria Ines gave <a href="https://talks.cam.ac.uk/talk/index/222778">a talk</a> about formalization of divided powers in the Cambridge Formalization of Mathematics seminar (which was started by Angeliki Koutsoukou-Argyraki a couple of years ago — thanks Angeliki!), and my understanding of Maria Ines’ talk is that these issues have now been sorted out. So we are actually back on track. Until the next time the literature lets us down…</p>

			
														</div><div id="entry-author-info">
						<p><img alt="" src="https://2.gravatar.com/avatar/e7ccfd42722fdb89c0fdea2f473152c14ab978bc1974acc4d67944906653f36e?s=60&amp;d=identicon&amp;r=G" srcset="https://2.gravatar.com/avatar/e7ccfd42722fdb89c0fdea2f473152c14ab978bc1974acc4d67944906653f36e?s=60&amp;d=identicon&amp;r=G 1x, https://2.gravatar.com/avatar/e7ccfd42722fdb89c0fdea2f473152c14ab978bc1974acc4d67944906653f36e?s=90&amp;d=identicon&amp;r=G 1.5x, https://2.gravatar.com/avatar/e7ccfd42722fdb89c0fdea2f473152c14ab978bc1974acc4d67944906653f36e?s=120&amp;d=identicon&amp;r=G 2x, https://2.gravatar.com/avatar/e7ccfd42722fdb89c0fdea2f473152c14ab978bc1974acc4d67944906653f36e?s=180&amp;d=identicon&amp;r=G 3x, https://2.gravatar.com/avatar/e7ccfd42722fdb89c0fdea2f473152c14ab978bc1974acc4d67944906653f36e?s=240&amp;d=identicon&amp;r=G 4x" height="60" width="60" loading="lazy" decoding="async">						</p><!-- #author-avatar -->
						<div id="author-description">
							<h2>
							About xenaproject							</h2><p>
							The Xena Project aims to get mathematics undergraduates (at Imperial College and beyond) trained in the art of formalising mathematics on a computer. Why? Because I have this feeling that digitising mathematics will be really important one day.							</p><!-- #author-link	-->
						</div><!-- #author-description -->
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gukesh becomes the youngest chess world champion in history (610 pts)]]></title>
            <link>https://lichess.org/@/Lichess/blog/wcc-2024-round-14-gukesh-becomes-the-youngest-world-champion-in-history/cDggdNZw</link>
            <guid>42398952</guid>
            <pubDate>Thu, 12 Dec 2024 13:29:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lichess.org/@/Lichess/blog/wcc-2024-round-14-gukesh-becomes-the-youngest-world-champion-in-history/cDggdNZw">https://lichess.org/@/Lichess/blog/wcc-2024-round-14-gukesh-becomes-the-youngest-world-champion-in-history/cDggdNZw</a>, See on <a href="https://news.ycombinator.com/item?id=42398952">Hacker News</a></p>
Couldn't get https://lichess.org/@/Lichess/blog/wcc-2024-round-14-gukesh-becomes-the-youngest-world-champion-in-history/cDggdNZw: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[BlenderGPT (270 pts)]]></title>
            <link>https://www.blendergpt.org/</link>
            <guid>42398913</guid>
            <pubDate>Thu, 12 Dec 2024 13:23:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.blendergpt.org/">https://www.blendergpt.org/</a>, See on <a href="https://news.ycombinator.com/item?id=42398913">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>BLENDERGPT<span>®</span></h2><p>is an advanced artificial intelligence program that creates 3D models from text or image prompts in ~20 seconds.</p><p>It lets you synthesise fully textured meshes, then import directly to Blender with a shortcut or download the source files for use in any compatible software.</p><p>we think its really good, try it out for free now.</p><p><img src="https://www.blendergpt.org/supercharging.jpg" alt="Supercharging Artists"></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Quantus – LeetCode for Financial Modeling (159 pts)]]></title>
            <link>https://quantus.finance/</link>
            <guid>42398471</guid>
            <pubDate>Thu, 12 Dec 2024 12:04:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://quantus.finance/">https://quantus.finance/</a>, See on <a href="https://news.ycombinator.com/item?id=42398471">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Parkinson's Law: It’s real, so use it (101 pts)]]></title>
            <link>https://theengineeringmanager.substack.com/p/parkinsons-law-its-real-so-use-it</link>
            <guid>42397781</guid>
            <pubDate>Thu, 12 Dec 2024 09:48:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theengineeringmanager.substack.com/p/parkinsons-law-its-real-so-use-it">https://theengineeringmanager.substack.com/p/parkinsons-law-its-real-so-use-it</a>, See on <a href="https://news.ycombinator.com/item?id=42397781">Hacker News</a></p>
Couldn't get https://theengineeringmanager.substack.com/p/parkinsons-law-its-real-so-use-it: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Timemap.org – Interactive Map of History (584 pts)]]></title>
            <link>https://www.oldmapsonline.org/en/history/regions</link>
            <guid>42397550</guid>
            <pubDate>Thu, 12 Dec 2024 09:12:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.oldmapsonline.org/en/history/regions">https://www.oldmapsonline.org/en/history/regions</a>, See on <a href="https://news.ycombinator.com/item?id=42397550">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div><div><p><img alt="Old Maps Online Logo – Explore Historical Maps" fetchpriority="high" width="104.7" height="32.89" decoding="async" data-nimg="1" src="https://www.oldmapsonline.org/images/header_logo_light_mode.svg"><img alt="Old Maps Online Logo – Explore Historical Maps" fetchpriority="high" width="104.7" height="32.89" decoding="async" data-nimg="1" src="https://www.oldmapsonline.org/images/header_logo_dark_mode.svg"></p></div><div><p><span>​</span></p></div></div><ul><li><a href="https://www.oldmapsonline.org/en/project">Project</a></li><li><a href="https://www.oldmapsonline.org/en/community">Community</a></li><li><a href="https://www.oldmapsonline.org/en/news">News</a></li><li><a href="https://www.oldmapsonline.org/en/app">My App</a></li></ul><div><p><a tabindex="0" href="https://www.oldmapsonline.org/en/user/maps">My maps</a></p></div></div><header><div><h6>Regions</h6></div></header><div><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><div><a href="https://www.oldmapsonline.org/en/history/regions"><p><span>History</span></p></a></div><div><a href="https://www.oldmapsonline.org/en"><p><span>Maps</span></p></a></div><div><a href="https://www.maptiler.com/story/oldmapsonline/" target="_blank"><div><p><img alt="Maptiler" src="https://www.oldmapsonline.org/images/maptiler/icon.svg"></p></div></a></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Canada euthanasia now accounts for nearly one in 20 deaths (160 pts)]]></title>
            <link>https://www.bbc.com/news/articles/c0j1z14p57po</link>
            <guid>42396733</guid>
            <pubDate>Thu, 12 Dec 2024 06:23:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/c0j1z14p57po">https://www.bbc.com/news/articles/c0j1z14p57po</a>, See on <a href="https://news.ycombinator.com/item?id=42396733">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="text-block"><p>Medically-assisted dying – also known as voluntary euthanasia – accounted for 4.7% of deaths in Canada in 2023, new government data shows.<!-- --></p><p>The country's fifth annual report since euthanasia was legalised in 2016 showed around 15,300 people underwent assisted dying last year after being successful in their applications.<!-- --></p><p>The median age of this group was more than 77. The vast majority – around 96% - had a death deemed "reasonably foreseeable", due to severe medical conditions such as cancer.<!-- --></p><p>In the small minority of other cases, patients may not have been terminally ill, but sought an assisted death due to a long and complicated illness that had significantly impacted their quality of life.<!-- --></p></div><div data-component="text-block"><p>Canada is among a few countries that have introduced assisted dying laws in the past decade. Others include Australia, New Zealand, Spain and Austria.<!-- --></p><p>In Canada, consenting adults can request medical assistance in dying from a healthcare provider if they have a serious and irremediable medical condition.<!-- --></p><p>Some provisions are in place, including a requirement of having two independent healthcare providers confirm that the patient is eligible before their request is approved.<!-- --></p><p>More than 320,000 people died in Canada in 2023, and 15,300 of those deaths - about one in 20 - were medically assisted.<!-- --></p><p>The figures released on Wednesday by Health Canada show that the rate of assisted dying in Canada increased by nearly 16% in 2023. This number is a sharp drop from the average increase of 31% in previous years.<!-- --></p><p>The report cautioned that it is too early to determine what caused the rate to slacken.<!-- --></p><p>For the first time, the report delved into race and ethnic data of those who died by euthanasia. <!-- --></p><p>Around 96% of recipients identified as white people, who account for about 70% of Canada's population. It is unclear what caused this disparity.<!-- --></p><p>The second most reported ethnic group was east Asians (1.8%), who account for about 5.7% of Canadians.<!-- --></p><p>Assisted dying continued to have the highest usage rate in Quebec, which accounted for nearly 37% of all euthanasia deaths, despite the province holding just 22% of Canada's population.<!-- --></p><p>Quebec's government launched a study earlier this year to examine why its euthanasia rate was so high.<!-- --></p></div><div data-component="text-block"><p>While the number of assisted deaths in Canada is growing, the country still falls behind the Netherlands, where euthanasia accounted for around 5% of total deaths last year.<!-- --></p><p>UK MPs voted late last month to pass a similar bill that gives terminally ill adults in England and Wales the right to have an assisted death, though it will face months of further scrutiny before it could become law.<!-- --></p><p>As British MPs debated the legislation, Canada was cited by some as a cautionary tale due to its perceived lack of safeguards.<!-- --></p><p>Like the UK, Canada initially only legalised assisted dying for those whose death was "reasonably foreseeable".<!-- --></p><p>However, Canada expanded access in 2021 to people who may not have a terminal diagnosis, but want to end their life because of a chronic, debilitating condition.<!-- --></p><p>It was set to broaden access once again to people with mental illnesses earlier this year. <!-- --></p><p>But that was delayed for the second time after concerns were raised by Canadian provinces, which oversee healthcare delivery, about whether the system could cope with such an expansion.<!-- --></p><p>On Wednesday, Health Canada defended the procedure, saying that the criminal code sets out "strict eligibility" criteria.<!-- --></p><p>But Cardus, a Christian think tank, said the latest figures were "alarming" and showed Canada has one of the fastest growing euthanasia programmes in the world.<!-- --></p><p>A report released in October by Ontario - Canada's most populous province - has since shed some light on controversial cases where people were granted assisted dying when they were not nearing their natural death.<!-- --></p><p>One example included a woman in her 50s with a history of depression and suicidal thoughts who had a severe sensitivity to chemicals.<!-- --></p><p>Her request for euthanasia was granted after she failed to secure housing that could have met her medical needs.<!-- --></p><p>Another case made headlines in recent months of a Nova Scotia cancer patient who said she was asked if she was aware of assisted dying as an option twice as she underwent mastectomy surgeries.<!-- --></p><p>The question "came up in completely inappropriate places", she told the National Post.<!-- --></p><p>Canadian news outlets have also reported on cases where people with disabilities have considered assisted dying due to lack of housing or disability benefits.<!-- --></p><p><i id="clarification-12-december:-this-article's-introduction-has-been-amended-to-be-clearer-about-the-fact-that-these-figures-relate-to-voluntary-euthanasia-and-to-more-clearly-and-prominently-explain-the-background-to-the-data-for-two-groups-who-successfully-sought-medically-assisted-dying.">Clarification 12 December: This article's introduction has been amended to be clearer about the fact that these figures relate to voluntary euthanasia and to more clearly and prominently explain the background to the data for two groups who successfully sought medically assisted dying.<!-- --></i></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A ChatGPT clone, in 3000 bytes of C, backed by GPT-2 (2023) (332 pts)]]></title>
            <link>https://nicholas.carlini.com/writing/2023/chat-gpt-2-in-c.html</link>
            <guid>42396372</guid>
            <pubDate>Thu, 12 Dec 2024 05:01:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nicholas.carlini.com/writing/2023/chat-gpt-2-in-c.html">https://nicholas.carlini.com/writing/2023/chat-gpt-2-in-c.html</a>, See on <a href="https://news.ycombinator.com/item?id=42396372">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>
              This program is a dependency-free implementation of GPT-2. It loads
              the weight matrix and BPE file out of the original TensorFlow files,
              tokenizes the input with a simple byte-pair encoder,
              implements a basic linear algebra package with matrix math operations,
              defines the transformer architecture, performs transformer inference,
              and un-tokenizes the output with the BPE decoder.
              All in ~3000 bytes of C.
            </p>
            
      
            <p>
              It's optimized efficiently enough so that GPT-2 Small takes a few
              seconds per reply on any modern machine. To do this I've implemented
              KV caching and an efficient matrix multiplication algorithm,
              with optional OMP parallelism.
            </p>
      
            <p>
              You can then use this to create something like Chat GPT---just so long
              as you don't care about the quality of the output. (It's actually
              pretty terrible output, objectively speaking... But it does run.)
              There are a
              few quirks (especially with handling UTF-8 characters), and running
              the XL size model at long context length can require ~100GB of RAM.
              But if you're just typing with ASCII using GPT2-Small it should run
              just about anywhere.
            </p>
      
            <p>
              I've uploaded <a href="https://github.com/carlini/c-chat-gpt-2">the code to GitHub</a>, so feel free to try and use it there.
            </p>
            
      
            
        
            
            <div id="mine"><p>
            This program is made up of the following main blocks (hover over each to see the coresponding code):        
                <a href="#linalg" id="show0">Basic matrix math library (700 bytes)</a>
                <a href="#matmul" id="show1">Fast matrix multiplication (300 bytes)</a>
                <a href="#nn" id="show2">Neural network layers (300 bytes)</a>
                <a href="#gpt" id="show3">Transformer model (600 bytes)</a>
                <a href="#bpe" id="show5">Byte pair encoding (400 bytes)</a>
                <a href="#z" id="show6">I/O (200 bytes)</a>
                <a href="#loadweight" id="show8">Weight loading (300 bytes)</a>
                <a href="#loadbpe" id="show7">Byte pair encoding loading (300 bytes)</a></p><div id="main">
      <p><span>#include</span><span>&lt;stdio.h&gt;</span></p>
      <p><span>#include</span><span>&lt;stdlib.h&gt;</span></p>
      <p><span>#include</span><span>&lt;string.h&gt;</span></p>
      <p><span>#include</span><span>&lt;math.h&gt;</span></p>
      <p><span>int</span><span> </span><span>U</span><span>,</span><span>C</span><span>,</span><span>K</span><span>,</span><span>c</span><span>,</span><span>d</span><span>,</span><span>S</span><span>,</span><span>zz</span><span>;</span><span>char</span><span>*</span><span>bpe</span><span>;</span><span>typedef</span><span> </span><span>struct</span><span>{</span><span>float</span><span>*</span><span>i</span><span>;</span><span>int</span><span> </span><span>j</span><span>,</span><span>k</span><span>;} </span><span>A</span><span>;</span><span>void</span><span>*</span><span>E</span><span>,*</span><span>n</span><span>;</span><span>A</span><span>*</span><span>f</span><span>;</span><span>FILE</span><span>*</span><span>fp</span><span>;</span></p>
      <p><span>#define</span><span> </span><span>N</span><span>(</span><span>i</span><span>,</span><span>j</span><span>)</span><span>for</span><span>(</span><span>int</span><span> </span><span>i</span><span>=0; i&lt;j; i++)</span></p>
      <p><span>
      <p><span>A</span><span> </span><span>o</span><span>(</span><span>int</span><span> </span><span>j</span><span>,</span><span>int</span><span> </span><span>k</span><span>,</span><span>int</span><span> </span><span>i</span><span>){</span><span>float</span><span>*</span><span>a</span><span>=E;E+=S=4*j*k;memset(a,0,S*i);</span><span>A</span><span> </span><span>R</span><span>={ a,j,k} ;</span><span>return</span><span> R;}</span></p>
      <p><span>#define</span><span> </span><span>I</span><span>(</span><span>R</span><span>,</span><span>B</span><span>)</span><span>A</span><span> </span><span>R</span><span>(</span><span>A</span><span> </span><span>a</span><span>,</span><span>float</span><span> </span><span>k</span><span>){ N(i,a.j*a.k){ </span><span>float</span><span> </span><span>b</span><span>=a.i[i]; a.i[i]=B; } </span><span>return</span><span> a; }</span></p>
      <p><span>I</span><span>(l,b/k)</span><span>I</span><span>(q,b+k)</span><span>I</span><span>(u,1./sqrt(</span><span>b</span><span>))</span><span>I</span><span>(z,</span><span>exp</span><span>(</span><span>b</span><span>))</span><span>I</span><span>(r,a.i[(i/a.k)*a.k])</span><span>I</span><span>(P,(i/k&lt;i%(</span><span>int</span><span>)k)?0:exp(b/8))</span><span>I</span><span>(Q,b/2*(1+tanh(.7978845*(b+.044715*b*b*b))))</span></p>
      <p><span>#define</span><span> </span><span>F</span><span>(</span><span>R</span><span>,</span><span>B</span><span>)</span><span>A</span><span> </span><span>R</span><span>(</span><span>A</span><span> </span><span>a</span><span>,</span><span>A</span><span> </span><span>b</span><span>){ N(i,a.j*a.k){ a.i[i]=a.i[i]B b.i[i]; } </span><span>return</span><span> a; }</span></p>
      <p><span>F</span><span>(V,+)</span><span>F</span><span>(v,*)</span><span>F</span><span>(H,/)</span><span>F</span><span>(at,+b.i[i%a.k];)</span><span>F</span><span>(mt,*b.i[i%a.k];)</span><span>A</span><span> </span><span>X</span><span>(</span><span>A</span><span> </span><span>a</span><span>){</span><span>A</span><span> </span><span>R</span><span>=o(a.j,a.k,1);N(i,a.j*a.k)R.i[(i/a.k)*a.k]+=a.i[i];r(R,0);</span><span>return</span><span> R;}</span><span>A</span><span> </span><span>p</span><span>(</span><span>A</span><span> </span><span>a</span><span>){</span><span>A</span><span> </span><span>R</span><span>=o(a.k,a.j,1);N(i,a.j*a.k)R.i[i%a.k*a.j+i/a.k]=a.i[i];</span><span>return</span><span> R;}</span></p>
      </span>
      <span>
        <p><span>A</span><span> </span><span>g</span><span>(</span><span>A</span><span> </span><span>a</span><span>,</span><span>A</span><span> </span><span>b</span><span>){</span><span>A</span><span> </span><span>R</span><span>=o(a.j,b.j,!c);</span><span>{</span><span>for</span><span>(</span><span>int</span><span> </span><span>i</span><span>=c;i&lt;d;i++){</span><span>for</span><span>(</span><span>int</span><span> </span><span>j</span><span>=0;j&lt;b.j;j+=4){</span><span>for</span><span>(</span><span>int</span><span> </span><span>k</span><span>=0;k&lt;a.k;k+=4){N(k2,4)N(j2,4)R.i[i*b.j+j+j2]+=a.i[i*a.k+k+k2]*b.i[(j+j2)*b.k+k+k2];}}}}</span><span>return</span></p>
      </span>
      <span>
      <p><span> V(o(R.j,R.k,1),R);}</span><span>A</span><span> </span><span>J</span><span>(</span><span>A</span><span> </span><span>a</span><span>,</span><span>int</span><span> </span><span>b</span><span>,</span><span>int</span><span> </span><span>j</span><span>,</span><span>int</span><span> </span><span>k</span><span>){</span><span>A</span><span> </span><span>R</span><span>={ a.i+b*j,j,k} ;</span><span>return</span><span> R;}</span><span>A</span><span> </span><span>s</span><span>(</span><span>A</span><span> </span><span>a</span><span>,</span><span>int</span><span> </span><span>i</span><span>){</span><span>A</span><span> </span><span>b</span><span>=V(a,l(X(a),-a.k));</span><span>A</span><span> </span><span>k</span><span>=l(X(v(V(o(b.j,b.k,1),b),b)),b.k-1);</span><span>A</span><span> </span><span>R</span><span>=at(mt(v(V(o(b.j,b.k,1),b),u(q(k,1e-5),0)),f[i+1]),f[i]);</span><span>return</span><span> R;}</span></p>
      <p><span>#define</span><span> </span><span>G</span><span>(</span><span>a</span><span>,</span><span>i</span><span>)at(g(a,f[i+1]),f[i])</span></p>
      </span>
      <span>
        <p><span>A</span><span> </span><span>m</span><span>(</span><span>int</span><span> </span><span>j</span><span>,</span><span>int</span><span> </span><span>k</span><span>){j+=!j;k+=!k;</span><span>A</span><span> </span><span>a</span><span>=o(j,k,1);fread(a.i,S,1,fp);</span><span>return</span><span> p(a);}</span></p>
      </span>
      <span>
        <p><span>int</span><span> </span><span>t</span><span>;</span><span>int</span><span> </span><span>Y</span><span>(</span><span>char</span><span>*</span><span>R</span><span>){</span><span>if</span><span>(!*R)</span><span>return</span><span> 0;</span><span>int</span><span> </span><span>B</span><span>=1e9,</span><span>r</span><span>;N(i,5e4){</span><span>if</span><span>(bpe[999*i]&amp;&amp;strncmp(bpe+999*i,R,S=strlen(bpe+999*i))==0){</span><span>int</span><span> </span><span>k</span><span>=Y(R+S)+i+1e7;</span><span>if</span><span>(k&lt;B){B=k;r=i;}}}t=r;</span><span>return</span><span> B;}</span><span>int</span><span> *</span><span>w</span><span>(</span><span>char</span><span>*</span><span>q</span><span>,</span><span>int</span><span>*</span><span>B</span><span>){</span><span>char</span><span> </span><span>R</span><span>[1000];</span><span>int</span><span> </span><span>i</span><span>=0;</span><span>while</span><span>(q[i]){</span><span>int</span><span> </span><span>j</span><span>=i++;</span><span>while</span><span>(47&lt;q[i]&amp;&amp;q[i]&lt;58||64&lt;q[i]){fflush(stdout);i++;}strcpy(R,q+j);R[i-j]=0;fflush(stdout);</span><span>int</span><span> </span><span>k</span><span>=0;</span><span>while</span><span>(R[k]){Y(R+k);</span><span>char</span><span>*</span><span>M</span><span>=bpe+t*999;k+=strlen(M);*B++=t;}}</span><span>return</span><span> B;}</span></p>
      </span>
      <span>
        <p><span>int</span><span> </span><span>main</span><span>(</span><span>int</span><span> </span><span>S</span><span>,</span><span>char</span><span>**</span><span>D</span><span>){S=D[1][5]+3*D[1][7]+3&amp;3;K=12+4*S+(S&gt;2);U=K*64;C=12*S+12;zz=atoi(D[4]);E=malloc(2LL*U*U*C*zz);</span></p>
      </span>
      <span>
        <p><span>bpe=malloc(1e9);fp=fopen(D[2],</span><span>"r"</span><span>);</span><span>unsigned</span><span> </span><span>char</span><span> </span><span>a</span><span>[S=999],</span><span>b</span><span>[S];N(i,5e4){</span><span>int</span><span> </span><span>k</span><span>=i*S;</span><span>if</span><span>(i&lt;93){bpe[k]=i+33;bpe[k+1]=0;} </span><span>else</span><span> </span><span>if</span><span>(i&gt;254){fscanf(fp,</span><span>"%s %s"</span><span>,a,b);strcat((</span><span>char</span><span>*)a,(</span><span>char</span><span>*)b);</span><span>int</span><span> </span><span>j</span><span>=0;N(i,a[i])bpe[k+j++]=a[i]^196?a[i]:a[++i]-128;bpe[k+j++]=0;} </span><span>else</span><span> </span><span>if</span><span>(i&gt;187){bpe[k]=i-188;bpe[k+1]=0;}}</span><span>int</span><span> </span><span>e</span><span>[1024];d=w(D[3],e)-e;</span><span>int</span><span> </span><span>h</span><span>;N(i,d){</span><span>if</span><span>(e[i]==18861)h=i+1;}printf(</span><span>"AI"</span><span>);N(i,d-h)printf(</span><span>"%s"</span><span>,bpe+e[i+h]*999);</span></p>
      </span>
      <span>
        <p><span>fp=fopen(D[1],</span><span>"r"</span><span>);</span><span>A</span><span>\</span></p>
      <p><span><span>&nbsp;</span></span><span>x</span><span>[999];</span><span>A</span><span>*</span><span>R</span><span>=x;N(i,C){N(j,12)*R++=m(U+U*(j?j^8?j^11?0:3:3:2),U*((j%8==3)+3*(j%8==1)+(j==9)));}*R++=m(U,1);*R++=m(U,1);</span><span>A</span><span> </span><span>QA</span><span>=m(1024,U),</span><span>Z</span><span>=p(m(5e4,U));</span></p>
      </span>
      <span>
        <p><span>while</span><span>(1){</span><span>char</span><span> </span><span>W</span><span>[1000]={ 0} ;</span><span>int</span><span> </span><span>T</span><span>;strcat(W,</span><span>"\nAlice: "</span><span>);printf(</span><span>"\n%s: "</span><span>,bpe+20490*999);fflush(stdout);fgets(W+8,1000,stdin);printf(</span><span>"AI:"</span><span>);strcat(W,</span><span>"\nBob:"</span><span>);d=w(W,e+d)-e;n=E;c=0;</span></p>
      </span>
      <span>
        <p><span>while</span><span>(1){E=n;T=d+32-d%32;c*=!!(d%32);</span><span>A</span><span> </span><span>O</span><span>=o(T,U,1);N(i,d){N(j,U)O.i[i*U+j]=Z.i[e[i]*U+j]+QA.i[j*1024+i];}N(i,C){</span><span>int</span><span> </span><span>y</span><span>;S=0;N(j,10){</span><span>if</span><span>(j==i)y=S;S++;N(k,10*(j&gt;0)){</span><span>if</span><span>(j*10+k&lt;C&amp;&amp;S++&amp;&amp;i==j*10+k)y=S;}}f=x+12*y;</span><span>A</span><span> </span><span>QB</span><span>=p(J(G(s(O,4),0),0,T*3,U));</span><span>A</span><span> </span><span>B</span><span>=o(U,T,1);N(k,K){</span><span>A</span><span> </span><span>L</span><span>=p(J(QB,k*3,64*T,3)),</span><span>a</span><span>=P(g(p(J(L,0,64,T)),p(J(L,T,64,T))),T),</span><span>R</span><span>=p(g(H(a,X(a)),J(L,T*2,64,T)));memcpy(B.i+64*T*k,R.i,64*T*4);}O=V(O,G(p(B),2));O=V(O,G(Q(G(s(O,6),8),0),10));}f=x;O=s(O,12*C);c=0;</span><span>int</span><span> </span><span>S</span><span>=d;d=1;</span><span>A</span><span> </span><span>B</span><span>=g(p(J(O,S-1,U,1)),Z);c=d=S;S=0;N(i,5e4){</span><span>if</span><span>(B.i[i]&gt;B.i[S])S=i;}</span><span>if</span><span>(d==zz){memcpy(e,e+zz/2,S*2);d-=zz/2;c=0;}e[d++]=S;</span></p>
      </span>
      <span>
        <p><span>if</span><span>(bpe[S*999]==10)</span><span>break</span><span>;printf(</span><span>"%s"</span><span>,bpe+S*999);fflush(stdout);}}}</span></p>
        </span>
            </p></div>
      
            </div>
      
            
            
      
            <br>
            <h2>Background: ChatGPT and transformers</h2>
      
            <p>
              In case you've been living under a rock for the past few months,
              ChatGPT is an application where you can talk to a type of machine learning
              model called a "language model" as if it was another person. It responds remarkably well,
              and GPT-4, the latest model that powers ChatGPT, is incredibly impressive.
            </p>
      
            <p>
              This C program implements the behavior of ChatGPT using a much
              weaker model from 2019: GPT-2. Despite being just 2 smaller than GPT-4,
              it has no where near the same capabilities---but it is open source.
              So it has that going for it.
            </p>
      
            <p>
              <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>
              is a type of machine learning model called a <a href="https://arxiv.org/abs/1706.03762">"transformer"</a>.
              These neural networks take a fixed-size sequence of words as input,
              and predict the next word that will occur. By repeating the procedure
              over and over, you can use them to generate arbitrary-length sequences.
            </p>
      
            <p>
              This post isn't meant to be an introduction to all the machine learning
              you'll need to know <i>why</i> a transformer is designed the way it is,
              but the rest of this post will be dedicated to describing how the above
              C code works.
            </p>
      
            <br>
            <h2>Walkthrough the C Code</h2>
      
            <h3 id="linalg">Getting started: Matrix Math (700 bytes)</h3>
      
            <p>
              Seeing as neural networks are just matrix operations. So we're going to need to get
              started by building a matrix library in as few bytes as possible.
            </p>
      
            <p>
              My definition of a matrix is completely minimal:
            </p>
      
      
            <div>
            <p><span>typedef</span><span> </span><span>struct</span><span> {</span></p>
      <p><span><span>&nbsp; </span></span><span>float</span><span>* </span><span>dat</span><span>;</span></p>
            <p><span><span>&nbsp; </span></span><span>int</span><span> </span><span>rows</span><span>, </span><span>cols</span><span>;</span></p>
      <p><span>} Matrix;</span></p>
            </div>
      
            <p>
              We'll begin by by observing that while there are a bunch of different operations
              we'll need to implement, there are basically two "types" of operations"
              </p><ol>
                <li>
                  Matrix-constant operations (e.g., add 7 to each entry of a matrix)
                </li>
                <li>
                  Matrix-matrix operations (e.g., add corresponding matrix entries)
                </li>
              </ol>
            
      
            <p>
              This similarity allows us to use macros to pull out a bunch of the common logic
              into a meta-routine that knows how to operate on, for example, pairs
              of matrices and just leaves the specific operator implementation defined.
            </p>
      
            <p>
              To do this in C, I'll define the function
            </p>
      
      
            <p><span>#define</span><span> </span><span>BINARY</span><span>(</span><span>function</span><span>, </span><span>operation</span><span>)</span></p>
      
            <p>
              as the following:
            </p>
            
            <div>
              <p><span>Matrix FUNCTION(</span><span>Matrix</span><span> </span><span>a</span><span>, </span><span>Matrix</span><span> </span><span>b</span><span>) {</span></p>
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; i &lt; a.rows; i++) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>j</span><span> = 0; j &lt; a.cols; j++) {</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span>a[i*a.cols + j] = a[i*a.cols + j] OPERATION b[i*a.cols+j];</span></p>
      <p><span><span>&nbsp; &nbsp; </span>}</span></p>
      <p><span><span>&nbsp; </span>}</span></p>
      <p><span><span>&nbsp; </span></span><span>return</span><span> a;</span></p>
      <p><span>}</span></p>
            </div>
      
            <p>
              And so for example this lets us just write
            </p>
      
            <div>
              <p><span>BINARY(matrix_elementwise_add, +);</span></p>
      <p><span>BINARY</span><span>(matrix_elementwise_multiply, *);</span></p>
            </div>
      
            <p>
            and have it automatically expand to the full operation that perform
            elementwise addition or multiplication of two matrices. I define a few
            other easy to understand operations as well:
            </p>
      
            <p>
            Now the thing about C's #defines is they're basically just glorified regexs.
            So when we actually run this, what's going to happen is we're going to take
            the line
            </p>
      
            <p><span>a[i*a.cols + j] = a[i*a.cols + j] OPERATION b[i*a.cols+j];</span></p><p>
            
            and expand for the case of multiplication expand it to
            
            </p><p><span>a[i*a.cols + j] = a[i*a.cols + j] * b[i*a.cols+j];</span></p>
      
            <p>
              But this replacement is almost literally just a regular expression replace.
              We could have put anything in place of OPERATION.
              This allows us to define a function like
            </p>
      
            <p><span>BINARY(add_tile, + b.dat[i%a.cols] ; )</span></p>
      
            <p>
              Which at first glance looks rather confusing---what is that semi-colon doing there?---but
              if you just do a regular expression replace on it, you'll see that it expands to
            </p>
      
            <p><span><span>&nbsp; </span>a[i*a.cols + j] = a[i*a.cols + j] + b.dat[i%a.cols] ; </span><span>b</span><span>[i*a.cols+j];</span></p>
      
            <p>
              where because the second expression doesn't do anything this is just equivalent to
            </p>
            
            <p><span>a[i*a.cols + j] = a[i*a.cols + j] + b.dat[i%a.cols] ; </span><span>b</span><span>[i*a.cols+j];</span></p>
      
            <p>
              (TAKE THAT LANGUAGES WITH PROPER MACROS. LISP ISN'T ALWAYS BETTER THAN C!)
            </p>
            
            <h3 id="matmul">Fast matrix multiplication (300 bytes)</h3>
      
            <p>
              The basic implementation of matrix multiplication is entirely straightforward:
              we just implement the naive cubic-time three loops:
              (There's nothing intelligent about my matrix multiplication. If you know how to make
              matrix multiplication fast you can just move along.)
            </p>
      
      
            <div>
              <p><span>Matrix matmul(</span><span>Matrix</span><span> </span><span>a</span><span>, </span><span>Matrix</span><span> </span><span>b</span><span>) {</span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>out</span><span> = NewMatrix(a.rows, b.rows);</span></p>
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; i &lt; a.rows; i++)</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>j</span><span> = 0; j &lt; b.rows; j++)</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>k</span><span> = 0; k &lt; a.cols; k++)</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span>out.dat[i * b.rows + j] += a.dat[i * a.cols + k+k2] * b.dat[(j+j2) * b.cols + k];</span></p>
      
      <p><span><span>&nbsp; </span></span><span>return</span><span> out;</span></p>
      <p><span>}</span></p>
            </div>
            
            <p>
              Fortunately we can make it much faster with just a few bits of intelligence.
              Because of the way memory and caches work on most computers, it's (much!) faster
              to read and write to the same piece of memory over and over.
            </p>
            
            <div>
              <p><span>Matrix matmul_t_fast(</span><span>Matrix</span><span> </span><span>a</span><span>, </span><span>Matrix</span><span> </span><span>b</span><span>) {</span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>out</span><span> = NewMatrix(a.rows, b.rows);</span></p>
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; i &lt; a.rows; i++)</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>j</span><span> = 0; j &lt; b.rows; j += 4)</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>k</span><span> = 0; k &lt; a.cols; k += 4)</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>k2</span><span> = 0; k2 &lt; 4; k2 += 1)</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>j2</span><span> = 0; j2 &lt; 4; j2 += 1)</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>out.dat[i * b.rows + j+j2] += a.dat[i * a.cols + k+k2] * b.dat[(j+j2) * b.cols + k+k2];</span></p>
      
      <p><span><span>&nbsp; </span></span><span>return</span><span> out;</span></p>
      <p><span>}</span></p>
            </div>
      
            <p>
              Later we're going to make one more change to the way we do inference and add a new
              parameter to the matrix multiply that instead allows us to only multiply part of Matrix A
              by Matrix B, which is useful when we've already pre-computed part of the product.
            </p>
            
            <h3 id="nn">Neural network layers (300 bytes)</h3>
      
            <p>
              In order to write a transformer I'll need to define a few special neural-network specific layers.
              One of these is the <a href="https://arxiv.org/abs/1606.08415">GELU</a> activation function,
              which you can just think of as magic.
            </p>
            <p><span>UNARY(GELU, b / 2 * (1 + tanh(.7978845 * (b + .044715 * b * b * b))))</span></p>
      
            <p>
              I also implement a function to set the lower-diagonal of a matrix
              (after exponentiating the values). This is useful for what's called <i>causal attention</i>:
              we only want to attend to the past, not the future, and so we set
              the lower diagonal of the attention matrix to zero with this function.
            </p>
      
            <p><span>UNARY(tril, (i/k&lt;i%(</span><span>int</span><span>)k) ? 0 : exp(b/8))</span></p>
      
            <p>
              And finally we need a layer normalization function.
              (Again another piece of magic that you can look up if you want.
              Basically what it does is normalize the mean and variance of each layer.)
            </p>
            <div>
              <p><span>Matrix LayerNorm(</span><span>Matrix</span><span> </span><span>a</span><span>, </span><span>int</span><span> </span><span>i</span><span>) {</span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>b</span><span> = add(a, divide_const(sum(a), -a.cols));</span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>k</span><span> = divide_const(sum(multiply(</span></p>
      <p><span><span>&nbsp; &nbsp; </span>add(NewMatrix(b.rows,b.cols,1),b), b)), b.cols-1);</span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>out</span><span> = add_tile(multiply_tile(</span></p>
      <p><span><span>&nbsp; &nbsp; </span>multiply(add(NewMatrix(b.rows,b.cols,1),b),</span></p>
      <p><span><span>&nbsp; &nbsp; </span>mat_isqrt(add_const(k, 1e-5),0)), layer_weights[i+1]),</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>layer_weights[i]);</span></p>
      
      <p><span><span>&nbsp; </span></span><span>return</span><span> out;</span></p>
      <p><span>}</span></p>
            </div>
      
            <p>
              The final piece of the model is the Linear function that
              just performs a matrix multiplication and adds (with tiling) a bias.
            </p>
      
            <p><span>#define</span><span> </span><span>Linear</span><span>(</span><span>a</span><span>, </span><span>i</span><span>) add_tile(matmul_t_fast(a, layer_weights[i+1]), layer_weights[i])</span></p>
      
            <h3 id="gpt">Transformer architecture (600 bytes)</h3>
      
            <p>
              With all of this out of the way, we can finally implement our transformer
              in just 600 bytes. 
              
            </p>
            
            <div>
              <p><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; i &lt; NLAYER; i++) {</span></p>
      <p><span><span>&nbsp; </span>layer_weights = weights + 12*permute;</span></p>
      
      <p><span><span>&nbsp; </span></span><span>// Compute the keys, queries, and values all at once with a big multiply </span><span></span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>qkv</span><span> = transpose(slice(Linear(LayerNorm(line, 4), 0), 0, T*3, DIM));</span></p>
      
      <p><span><span>&nbsp; </span></span><span>// Make space for the output of the computation<span>&nbsp;</span></span><span> </span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>result</span><span> = NewMatrix(DIM, T, 1);</span></p>
      
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>k</span><span> = 0; k &lt; NHEAD; k++) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>// Split the qkv into each of the heads<span>&nbsp;</span></span><span> </span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>Matrix</span><span> </span><span>merge</span><span> = transpose(slice(qkv, k*3, 64*T, 3)),</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span></span><span>// perform the product of the queries and keys and then exponentiate </span><span></span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span></span><span>a</span><span> = tril(matmul_t_fast(transpose(slice(merge, 0, 64, T)),</span></p>
      <p><span><span>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>transpose(slice(merge, T, 64, T))), T),</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span></span><span>// finally multiply the softmax output (a/sum(a)) with the values matrix </span><span></span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span></span><span>out</span><span> = transpose(matmul_t_fast(divide(a, sum(a)), slice(merge, T*2, 64, T)));</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>// and copy the output to the proper location in the result matrix </span><span></span></p>
      <p><span><span>&nbsp; &nbsp; </span>memcpy(result.dat+64*T*k, out.dat, 64*T*4);</span></p>
      <p><span><span>&nbsp; </span>}</span></p>
      
      <p><span><span>&nbsp; </span></span><span>// Residual connection </span><span></span></p>
      <p><span><span>&nbsp; </span>line = add(line,Linear(transpose(result), 2));</span></p>
      
      <p><span><span>&nbsp; </span></span><span>// Activation function and residual connection </span><span></span></p>
      <p><span><span>&nbsp; </span>line = add(line, Linear(GELU(Linear(LayerNorm(line, 6), 8), 0), 10));</span></p>
      <p><span>}</span></p>
      
      <p><span>// Reset layer weights so we can do the last layer norm<span>&nbsp;</span></span><span> </span></p>
      <p><span>layer_weights = weights;</span></p>
      <p><span>line = LayerNorm(line, 12*NLAYER);</span></p>
      
      <p><span>Matrix</span><span> </span><span>result</span><span> = matmul_t_fast(transpose(slice(line, tmp-1, DIM, 1)), wte);</span></p>
      
            </div>
            
            <p>
              Now here's a fact that might not be completely obvious about transformer
              inference: once you've called the model to generate one token, you don't
              actually have to re-compute the entire function to generate the next token.
              In fact, you only need to do a small amount of additional work to generate
              each additional token.
            </p>
      
            <p>
              This is because once you've computed the output of the transformer on the
              output of all the tokens up to the Nth token, you can re-use almost all of
              this output to compute the N+1st token (with a little bit more work.)
            </p>
      
            <p>
              To actually implement this, I make all allocations in my code occur
              sequentially within the same block of memory, to guarantee that each
              matrix multiply will always use exactly the same memory. Then, at each
              iteration of the loop, I can just not zero-out the memory before using
              it for the next iteration, and the memory will already contain the
              results of the previous iteration. I just need to run the computation
              for the N+1st row.
            </p>
            
            
            <h3 id="bpe">Byte pair encoding (400 bytes)</h3>
      
            <p>          
              The simplest way to build a language model is on a sequence of words.
              But because the total number of words is essentially unbounded,
              and language models need to have fixed-size inputs,
              it would be necessary to replace sufficiently rare words with a special
              [OUT OF DISTRIBUTION] token. This is no good.
            </p>
      
            <p>
              While a simple “fix” for this would be to just use character-level
              language models that only know about individual letters, this would
              be a problem because it would mean that the model would have to learn
              the meaning of every word from scratch, and also reduces the effective
              context size of the language model by a factor of the average word length.
            </p>
      
            <p>
              So to fix this, language models like GPT-2 work by creating tokens out
              of "word pieces". Some words might be tokens all by them-self, but
              rare words are broken up into smaller pieces. For example, the word
              “nicholas” might be broken up into “nich” “o” “las”.
            </p>
            
            <p>
              The general algorithm for this is rather easy to implement:
              given a word we want to tokenize, we first split it into individual
              characters. Then, we look for pairs of adjacent tokens that should
              be merged, and merge them together. We repeat this until there are
              no more possible merges.
            </p>
      
            <p>
              This algorithm is simple but unfortunately rather hard to implement
              in C because it requires a bunch of allocations, and requires keeping
              track of a tree-like structure of the tokens.
            </p>
      
            <p>
              So instead, we'll turn the rather simple linear time algorithm into a
              potentially exponential time algorithm but save a bunch of code.
              Our basic idea will work like this in C-like pseudocode:
            </p>
      
            <div>
              <p><span>word_tokenize(word) {</span></p>
      <p><span><span>&nbsp; </span></span><span>if</span><span> len(word) == 0 { </span><span>return</span><span> (0, 0); }</span></p>
      <p><span><span>&nbsp; </span>result = (1e9, -1);</span></p>
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; i &lt; VOCAB_LEN; i++) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>if</span><span> (is_prefix(bpe[i]), word) {</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span>sub_cost = word_tokenize(word+len(bpe[i]))[0] + i + 1e7;</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span>result = min(result, (sub_cost, i));</span></p>
      <p><span><span>&nbsp; &nbsp; </span>}</span></p>
      <p><span><span>&nbsp; </span>}</span></p>
      <p><span><span>&nbsp; </span></span><span>return</span><span> result;</span></p>
      <p><span>}</span></p>
            </div>
      
            <p>
              That is, to tokenize a word, we try each possible word in the vocabulary
              to see if it's a prefix of the current word. If so, we try to use this
              as the first token, and then recursively try to tokenize the rest of the
              word. We keep track of the best tokenization we've seen so far (as judged
              by the length, breaking ties by the index of the token in the vocab), and
              return that.
            </p>
              
            
            <h3 id="loadweight">Weight loading (300 bytes)</h3>
      
            <p>
              We're almost done! The last thing we need to do is load the actual weights
              of the neural network off disk. This is actually pretty easy, because
              the weights are stored in a simple binary format that's easy to read
              in C: it's just a completely flat serialization of 32-bit floats.
            </p>
      
            <p>
              The only thing we need to know is how big the various matrices are.
              And fortunately, this is also easy to figure out. Each of the GPT-2
              model sizes have the same architecture, and the weights are saved in the
              same order, so all we need to do is read read the correctly-shaped
              matrices off of disk.
            </p>
      
            <p>
              There's one final annoying thing. The layers of the neural network are
              not stored on disk in the order you might expect, with layer 0 first,
              then layer 1, then layer 2. Instead, the first layer is layer 0, then
              layer 1, and then layer .... TEN! (and then layer 11, and then layer 12.)
              This is because weights are stored when sorted lexicographically.
              And lexicographically, “0” comes before “1”, but “10” comes before
              “2”. So we have to do a bit of work to permute the weights into the
              correct order with the following code
            </p>
      
            <div>
              <p><span>int</span><span> permute;</span></p>
      <p><span>tmp=0;</span></p>
      <p><span>for</span><span> (</span><span>int</span><span> </span><span>j</span><span> = 0; j &lt; 10; j++) {</span></p>
      <p><span><span>&nbsp; </span></span><span>if</span><span> (j == i) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span>permute = tmp;</span></p>
      <p><span><span>&nbsp; </span>}</span></p>
      <p><span><span>&nbsp; </span>tmp++;</span></p>
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>k</span><span> = 0; k &lt; 10*(j&gt;0); k++) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>if</span><span> (j*10+k &lt; NLAYER &amp;&amp; tmp++ &amp;&amp; i == j*10+k) {</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span>permute = tmp;</span></p>
      <p><span><span>&nbsp; &nbsp; </span>}</span></p>
      <p><span><span>&nbsp; </span>}</span></p>
      <p><span>}</span></p>
            </div>
      
            
            <h3 id="loadbpe">Byte pair encoding loading (300 bytes)</h3>
      
            <p>
              In order to actually perform byte-pair encoding, we need to first load
              the byte-pair encoding vocabulary off disk. In an ideal world we'd
              actually just have a list of all the words in the vocabulary stored
              in some reasonable C-readable format, but because the original file
              was (a) meant for reading in Python, and (b) not meant to make it
              easy to parse in as few bytes as possible, we'll have to do some work here.
            </p>
      
            <p>
              You might expect the file format to just be a list of words one after
              the other, but it's actually instead a list of the byte-pair encodings.
              What this means is instead of being able to read “Hello” as one token,
              the line is “H” “ello” which means we should be merging the tokens
              “H” and “ello” into a single token “Hello”.
            </p>
      
            <p>
              The other challenge is that the file is encoded in smoothing-like
              UTF-8 (but not quite exactly that) for ... reasons.
              All of the printable ascii characters are encoded as themselves,
              but the non-printable characters from 0-31 are encoded as the value
              188+the character. So for example, a space is encoded as the token “Ġ”.
              But now the problem is that the UTF8 encoding of “Ġ” is 0xc4 0xa0
              when on disk, and so when reading it we have to do just some ugly work
              to convert this back to a space.
            </p>
      
            <p>
              And while none of this is actually that hard to do, it still requires
              a fair amount of code which is annoying when you're trying to compress
              everything to be small.
            </p>
      
            <div>
      <p><span>unsigned</span><span> </span><span>char</span><span> a[tmp=999],b[tmp];</span></p>
      <p><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; i &lt; 5e4; i++) {</span></p>
      <p><span><span>&nbsp; </span></span><span>int</span><span> </span><span>k</span><span> = i*tmp;</span></p>
      <p><span><span>&nbsp; </span></span><span>if</span><span> (i &lt; 93) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>// The first 92 tokens are just the printable ascii characters </span><span></span></p>
      <p><span><span>&nbsp; &nbsp; </span>bpe[k] = i + 33;</span></p>
      <p><span><span>&nbsp; &nbsp; </span>bpe[k+1] = 0;</span></p>
      <p><span><span>&nbsp; </span>} </span><span>else</span><span> </span><span>if</span><span> (i &gt; 254) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>// Ones above 254 are from the BPE file. Load those<span>&nbsp;</span></span><span> </span></p>
      <p><span><span>&nbsp; &nbsp; </span>fscanf(fp, </span><span>"%s %s"</span><span>, a, b);</span></p>
      <p><span><span>&nbsp; &nbsp; </span>strcat((</span><span>char</span><span>*)a, (</span><span>char</span><span>*)b);</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>int</span><span> </span><span>j</span><span> = 0;</span></p>
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; a[i]; i++) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>// UTF8 encoding makes life hard so handle that here </span><span></span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span>bpe[k+j++] = a[i] ^ 196 ? a[i] : a[++i]-128;</span></p>
      <p><span><span>&nbsp; &nbsp; </span>}</span></p>
      <p><span><span>&nbsp; &nbsp; </span>bpe[k+j++] = 0;</span></p>
      <p><span><span>&nbsp; </span>} </span><span>else</span><span> </span><span>if</span><span> (i &gt; 187) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>// Tokens above 187 are the nonprintable asii character from 0-32<span>&nbsp;</span></span><span> </span></p>
      <p><span><span>&nbsp; &nbsp; </span>bpe[k] = i-188;</span></p>
      <p><span><span>&nbsp; &nbsp; </span>bpe[k+1] = 0;</span></p>
      <p><span><span>&nbsp; </span>}</span></p>
      <p><span>}</span></p>
      </div>        
                        
            
      
            <h2>Conclusion</h2>
            
            <p>
              It's really remarkable how you can distill so many
              decades of progress in machine learning to just a few thousand bytes.
              There is essentially nothing missing here from everything you need to run
              any state-of-the-art neural network (except for the actual model weights).
              While I mostly put this together for fun,
              it's a nice demonstration how <i>simple</i> neural networks actually are.
            </p>
      
            
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mouseless – fast mouse control with the keyboard (160 pts)]]></title>
            <link>https://mouseless.click/</link>
            <guid>42396336</guid>
            <pubDate>Thu, 12 Dec 2024 04:53:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mouseless.click/">https://mouseless.click/</a>, See on <a href="https://news.ycombinator.com/item?id=42396336">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[QEMU with VirtIO GPU Vulkan Support (229 pts)]]></title>
            <link>https://gist.github.com/peppergrayxyz/fdc9042760273d137dddd3e97034385f</link>
            <guid>42392802</guid>
            <pubDate>Wed, 11 Dec 2024 20:48:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gist.github.com/peppergrayxyz/fdc9042760273d137dddd3e97034385f">https://gist.github.com/peppergrayxyz/fdc9042760273d137dddd3e97034385f</a>, See on <a href="https://news.ycombinator.com/item?id=42392802">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="file-qemu-vulkan-virtio-md">
    <article itemprop="text">
<p dir="auto">With its latest reales qemu added the Venus patches so that virtio-gpu now support venus encapsulation for vulkan. This is one more piece to the puzzle towards full Vulkan support.</p>
<p dir="auto">An outdated blog post on <a href="https://www.collabora.com/news-and-blog/blog/2021/11/26/venus-on-qemu-enabling-new-virtual-vulkan-driver/" rel="nofollow">clollabora</a> described in 2021 how to enable 3D acceleration of Vulkan applications in QEMU through the Venus experimental Vulkan driver for VirtIO-GPU with a local development environment. Following up on the outdated write up, this is how its done today.</p>
<p dir="auto"><h2 dir="auto">Definitions</h2><a id="user-content-definitions" aria-label="Permalink: Definitions" href="#definitions"></a></p>
<p dir="auto">Let's start with the brief description of the projects mentioned in the post &amp; extend them:</p>
<ul dir="auto">
<li>QEMU is a machine emulator</li>
<li>VirGL is an OpenGL driver for VirtIO-GPU, available in Mesa.</li>
<li>Venus is an experimental Vulkan driver for VirtIO-GPU, also available in Mesa.</li>
<li>Virglrenderer is a library that enables hardware acceleration to VM guests, effectively translating commands from the two drivers just mentioned to either OpenGL or Vulkan.</li>
<li>libvirt is an API for managing platform virtualization</li>
<li>virt-manager is a desktop user interface for managing virtual machines through libvirt</li>
</ul>
<p dir="auto">Merged Patches:</p>
<ul dir="auto">
<li>2024-08-14 <a href="https://gitlab.freedesktop.org/mesa/mesa/-/commit/087e9a96d13155e26987befae78b6ccbb7ae242b" rel="nofollow">venus: make cross-device optional</a> merged in <a href="https://www.phoronix.com/news/Mesa-24.2-Released" rel="nofollow">mesa 24.2</a></li>
<li>2024-11-25 <a href="https://lore.kernel.org/all/20240726235234.228822-1-seanjc@google.com/" rel="nofollow">KVM: Stop grabbing references to PFNMAP'd pages</a> merged in <a href="https://www.phoronix.com/news/Linux-6.13-KVM" rel="nofollow">linux 6.13</a></li>
<li>2024-11-12 <a href="https://lists.gnu.org/archive/html/qemu-devel/2024-08/msg03288.html" rel="nofollow">Support blob memory and venus on qemu</a> merged in <a href="https://www.phoronix.com/news/QEMU-9.2-Released" rel="nofollow">qemu 9.2.0</a></li>
</ul>
<p dir="auto">Work in progress:</p>
<ul dir="auto">
<li>libvirt <a href="https://gitlab.com/libvirt/libvirt/-/issues/638" rel="nofollow">Add support for more virtio-vga-gl arguments #638</a></li>
<li>virt-manager <a href="https://github.com/virt-manager/virt-manager/issues/362" data-hovercard-type="issue" data-hovercard-url="/virt-manager/virt-manager/issues/362/hovercard">Add support for Venus / Vulkan VirtIO-GPU driver #362</a></li>
</ul>
<p dir="auto"><h2 dir="auto">Prerequisites</h2><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<p dir="auto">Make sure you have the proper version installed on the host:</p>
<ul dir="auto">
<li>linux kernel &gt;= 6.13 built with CONFIG_UDMABUF</li>
<li>working Vulkan and kvm setup</li>
<li>qemu &gt;= 9.2.0</li>
</ul>
<p dir="auto">You can verify this like so:</p>
<pre><code>$ uname -r
6.13.0
$ ls /dev/udmabuf
/dev/udmabuf
$ ls /dev/kvm
/dev/kvm
$ qemu-system-x86_64 --version
QEMU emulator version 9.2.0
Copyright (c) 2003-2024 Fabrice Bellard and the QEMU Project developers
</code></pre>
<p dir="auto">For Vulkan to work you need the proper drivers to be installed for your graphics card. To verfiy your setup, install <code>vulkan-tools</code>:</p>
<pre><code>$ vulkaninfo --summary
==========
VULKANINFO
==========

Vulkan Instance Version: ...
...
$ vkcube
Selected GPU x: ..., type: ...
</code></pre>
<p dir="auto"><h4 dir="auto">Building qemu</h4><a id="user-content-building-qemu" aria-label="Permalink: Building qemu" href="#building-qemu"></a></p>
<p dir="auto">If your distro doesn't (yet) ship and updated version of qemu, you can build it yourself from source:</p>
<pre><code>wget https://download.qemu.org/qemu-9.2.0.tar.xz
tar xvJf qemu-9.2.0.tar.xz
cd qemu-9.2.0
mkdir build &amp;&amp; cd build
../configure --target-list=x86_64-softmmu  \
  --enable-kvm                 \
  --enable-opengl              \
  --enable-virglrenderer       \
  --enable-gtk                 \
  --enable-sdl
make -j4
</code></pre>
<p dir="auto">The configuration step will throgh errors if packages are missing. Check the qemu wiki for further info what to install: <a href="https://wiki.qemu.org/Hosts/Linux" rel="nofollow">https://wiki.qemu.org/Hosts/Linux</a></p>
<p dir="auto"><h2 dir="auto">Create and run an image for QEMU</h2><a id="user-content-create-and-run-an-image-for-qemu" aria-label="Permalink: Create and run an image for QEMU" href="#create-and-run-an-image-for-qemu"></a></p>
<p dir="auto">Create an image &amp; fetch the distro of your choice:</p>
<p dir="auto"><h3 dir="auto">Host</h3><a id="user-content-host" aria-label="Permalink: Host" href="#host"></a></p>
<div dir="auto"><pre>ISO=ubuntu-24.10-desktop-amd64.iso  
wget https://releases.ubuntu.com/oracular/ubuntu-24.10-desktop-amd64.iso  

IMG=ubuntu-24-10.qcow2
qemu-img create -f qcow2 <span>$IMG</span> 16G</pre></div>
<p dir="auto">Run a live version or install the distro</p>
<pre><code>qemu-system-x86_64                                               \
    -enable-kvm                                                  \
    -M q35                                                       \
    -smp 4                                                       \
    -m 4G                                                        \
    -cpu host                                                    \
    -net nic,model=virtio                                        \
    -net user,hostfwd=tcp::2222-:22                              \
    -device virtio-vga-gl,hostmem=4G,blob=true,venus=true        \
    -vga none                                                    \
    -display gtk,gl=on,show-cursor=on                            \
    -usb -device usb-tablet                                      \
    -object memory-backend-memfd,id=mem1,size=4G                 \
    -machine memory-backend=mem1                                 \
    -hda $IMG                                                    \
    -cdrom $ISO                                                  
</code></pre>
<p dir="auto">Adjust the parameters accordingly:</p>
<ul dir="auto">
<li>smp: number of cpu cores</li>
<li>m: RAM</li>
<li>hostmem,size: VRAM</li>
</ul>
<p dir="auto"><h3 dir="auto">Guest</h3><a id="user-content-guest" aria-label="Permalink: Guest" href="#guest"></a></p>
<p dir="auto">Install <code>mesa-utilites</code> and <code>vulkan-tools</code> to test the setup:</p>
<pre><code>$ glxinfo -B
</code></pre>
<pre><code>$ vkcube
Selected GPU x: ..., type: ...
</code></pre>
<p dir="auto">If the deive is <code>llvmpipe</code> somehting is wrong. The device should be <code>virgl (...)</code>.</p>
<p dir="auto"><h4 dir="auto">Troubleshooting</h4><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<ul dir="auto">
<li>(host) add <code>-d guest_errors</code> to show error messages from the guest</li>
<li>(guest) try installing vulkan virtio drivers and mesa</li>
<li>check the original <a href="https://www.collabora.com/news-and-blog/blog/2021/11/26/venus-on-qemu-enabling-new-virtual-vulkan-driver/" rel="nofollow">blog post</a></li>
</ul>
<p dir="auto"><h2 dir="auto">virt-manager</h2><a id="user-content-virt-manager" aria-label="Permalink: virt-manager" href="#virt-manager"></a></p>
<p dir="auto">-- work in progress --</p>
<p dir="auto">Currently this is work in progress, so there is no option to add vulkan support in virt-manager. There are no fields to configure this. Also xml doesnt work, because libvirt doesn't know about these options either, so xml validation fails. There is however an option for <a href="https://libvirt.org/kbase/qemu-passthrough-security.html" rel="nofollow">QEMU command-line passthrough</a> which bypasses the validation.</p>
<p dir="auto">If you setup a default machine with 4G of memory, you can do this:</p>
<div dir="auto"><pre>  &lt;<span>qemu</span><span>:</span><span>commandline</span>&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>-device<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>virtio-vga-gl,hostmem=4G,blob=true,venus=true<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>-object<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>memory-backend-memfd,id=mem1,size=4G<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>-machine<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>memory-backend=mem1<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>-vga<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>none<span>"</span></span>/&gt;
  &lt;/<span>qemu</span><span>:</span><span>commandline</span>&gt;</pre></div>
<p dir="auto">Which gives this error:</p>
<pre><code>qemu-system-x86_64: virgl could not be initialized: -1
</code></pre>
<p dir="auto">Changing the number from 4G to 4194304k (same as memory) leds to this error:</p>
<pre><code>qemu-system-x86_64: Spice: ../spice-0.15.2/server/red-qxl.cpp:435:spice_qxl_gl_scanout: condition `qxl_state-&gt;gl_draw_cookie == GL_DRAW_COOKIE_INVALID' failed
</code></pre>
<p dir="auto">to be further investigated.</p>
</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Instant macOS install on Proxmox including AMD patches (159 pts)]]></title>
            <link>https://github.com/luchina-gabriel/OSX-PROXMOX</link>
            <guid>42392660</guid>
            <pubDate>Wed, 11 Dec 2024 20:36:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/luchina-gabriel/OSX-PROXMOX">https://github.com/luchina-gabriel/OSX-PROXMOX</a>, See on <a href="https://news.ycombinator.com/item?id=42392660">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">OSX-PROXMOX - Run macOS on ANY Computer - AMD &amp; Intel</h2><a id="user-content-osx-proxmox---run-macos-on-any-computer---amd--intel" aria-label="Permalink: OSX-PROXMOX - Run macOS on ANY Computer - AMD &amp; Intel" href="#osx-proxmox---run-macos-on-any-computer---amd--intel"></a></p>
<p dir="auto">Install <code>** FRESH/CLEAN **</code> Proxmox VE v7.0.XX ~ 8.2.XX - Next, Next &amp; Finish (NNF).</p>
<p dir="auto">Open Proxmox Web Console -&gt; Datacenter &gt; NAME OF YOUR HOST &gt; Shell.</p>
<p dir="auto">Copy, paste and execute (code below).</p>
<p dir="auto">Voilà, install macOS! This is really and magic <strong>easiest way</strong>!
<a target="_blank" rel="noopener noreferrer" href="https://github.com/luchina-gabriel/OSX-PROXMOX/blob/main/Artefacts/proxmox-screen.png"><img src="https://github.com/luchina-gabriel/OSX-PROXMOX/raw/main/Artefacts/proxmox-screen.png" alt="overview"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">COPY &amp; PASTE - in shell of Proxmox (for Install or Update this solution)</h2><a id="user-content-copy--paste---in-shell-of-proxmox-for-install-or-update-this-solution" aria-label="Permalink: COPY &amp; PASTE - in shell of Proxmox (for Install or Update this solution)" href="#copy--paste---in-shell-of-proxmox-for-install-or-update-this-solution"></a></p>
<div data-snippet-clipboard-copy-content="/bin/bash -c &quot;$(curl -fsSL https://install.osx-proxmox.com)&quot;"><pre><code>/bin/bash -c "$(curl -fsSL https://install.osx-proxmox.com)"
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">For install EFI Package in macOS, first disable Gatekeeper</h2><a id="user-content-for-install-efi-package-in-macos-first-disable-gatekeeper" aria-label="Permalink: For install EFI Package in macOS, first disable Gatekeeper" href="#for-install-efi-package-in-macos-first-disable-gatekeeper"></a></p>
<div data-snippet-clipboard-copy-content="sudo spctl --master-disable"><pre><code>sudo spctl --master-disable
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Versions of macOS Supported</h2><a id="user-content-versions-of-macos-supported" aria-label="Permalink: Versions of macOS Supported" href="#versions-of-macos-supported"></a></p>
<ul dir="auto">
<li>macOS High Sierra - 10.13</li>
<li>macOS Mojave - 10.14</li>
<li>macOS Catalina - 10.15</li>
<li>macOS Big Sur - 11</li>
<li>macOS Monterey - 12</li>
<li>macOS Ventura - 13</li>
<li>macOS Sonoma - 14</li>
<li>macOS Sequoia - 15</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Versions of Proxmox VE Supported</h2><a id="user-content-versions-of-proxmox-ve-supported" aria-label="Permalink: Versions of Proxmox VE Supported" href="#versions-of-proxmox-ve-supported"></a></p>
<ul dir="auto">
<li>v7.0.XX ~ 8.2.XX</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Opencore version</h2><a id="user-content-opencore-version" aria-label="Permalink: Opencore version" href="#opencore-version"></a></p>
<ul dir="auto">
<li>Oct/2024 - 1.0.2 Added support to macOS Sequoia</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cloud Support (Yes, install your Hackintosh in Cloud Environment)</h2><a id="user-content-cloud-support-yes-install-your-hackintosh-in-cloud-environment" aria-label="Permalink: Cloud Support (Yes, install your Hackintosh in Cloud Environment)" href="#cloud-support-yes-install-your-hackintosh-in-cloud-environment"></a></p>
<ul dir="auto">
<li><a href="https://www.vultr.com/?ref=9035565-8H" rel="nofollow">VultR</a></li>
<li><a href="https://youtu.be/8QsMyL-PNrM" rel="nofollow">Vídeo/Tutorial</a>, please activate captions!</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Disclaimer</h2><a id="user-content-disclaimer" aria-label="Permalink: Disclaimer" href="#disclaimer"></a></p>
<ul dir="auto">
<li>FOR DEV/STUDENT/TEST ONLY PURPOSES.</li>
<li>I'm not responsible for any problem and/or equipment damage or loss of files.</li>
<li>Always back up everything before any changes to your computer.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<p dir="auto">Since Monterey, your host must have a working TSC (timestamp counter), because otherwise if you give the VM more than one core, macOS will observe the skew between cores and <strong>kernel/memory panic</strong> when it sees time ticking backwards. To check this, on Proxmox run:</p>
<div data-snippet-clipboard-copy-content="dmesg | grep -i -e tsc -e clocksource
...
# for working host must be:
...
clocksource: Switched to clocksource tsc
...

# for broken host could be:
tsc: Marking TSC unstable due to check_tsc_sync_source failed
clocksource: Switched to clocksource hpet"><pre><code>dmesg | grep -i -e tsc -e clocksource
...
# for working host must be:
...
clocksource: Switched to clocksource tsc
...

# for broken host could be:
tsc: Marking TSC unstable due to check_tsc_sync_source failed
clocksource: Switched to clocksource hpet
</code></pre></div>
<p dir="auto">Below is a possible workaround from here: <a href="https://www.nicksherlock.com/2022/10/installing-macos-13-ventura-on-proxmox/comment-page-1/#comment-55532" rel="nofollow">https://www.nicksherlock.com/2022/10/installing-macos-13-ventura-on-proxmox/comment-page-1/#comment-55532</a></p>
<ol dir="auto">
<li>Try to turn off “ErP mode” or any C state power saving modes your BIOS supports and poweroff/poweron device (including physical cable). It could help host OS to init TSC correctly, but no guarantee.</li>
<li>Or try to activate TSC force in GRUB by adding boot flags <code>clocksource=tsc tsc=reliable</code> in the <code>GRUB_CMDLINE_LINUX_DEFAULT</code> and call <code>update-grub</code>. In this case host OS probably could work unstable in some cases.</li>
<li>Check the current TSC by call <code>cat /sys/devices/system/clocksource/clocksource0/current_clocksource</code> must be <code>tsc</code>.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">High Siearra and below installation issues</h3><a id="user-content-high-siearra-and-below-installation-issues" aria-label="Permalink: High Siearra and below installation issues" href="#high-siearra-and-below-installation-issues"></a></p>
<p dir="auto">To solve error <em>The Recovery Server Could Not Be Contacted</em> you need to change the protocol from <code>https://</code> to <code>http://</code>. To do this, follow:</p>
<ul dir="auto">
<li>start installation and get error <em>The Recovery Server Could Not Be Contacted</em>, hold the window with error opened</li>
<li>open Window -&gt; Installer Log</li>
<li>search for the line "Failed to load catalog" -&gt; select line in log windows -&gt; Edit -Copy</li>
<li>close the error message and return to <code>macOS Utilities</code> window</li>
<li>open Utilities -&gt; Terminal, right click -&gt; paste</li>
<li>edit the pasted data, remove everything except URL, like <code>https://blablabla.sucatalog</code></li>
<li>change https -&gt; http</li>
<li>adjust the command to be like: nvram IASUCatalogURL=""</li>
<li>press enter, quit Terminal and try to start installation again</li>
</ul>
<p dir="auto">After this, no additional ISO needed, HighSierra must be installed well from recovey.</p>
<p dir="auto">Here a sample how need to change the error message to the final URL:</p>
<p dir="auto"><code>nIUvram IASUCatalogURL="http://swscan.apple.com/content/catalogs/others/index-10.13-10.12-10.11-10.10-10.9-mountainlion-lion-snowleopard-leopard.merged-1.sucatalog"</code></p>
<p dir="auto">The solution took from here: <a href="https://mrmacintosh.com/how-to-fix-the-recovery-server-could-not-be-contacted-error-high-sierra-recovery-is-still-online-but-broken/" rel="nofollow">https://mrmacintosh.com/how-to-fix-the-recovery-server-could-not-be-contacted-error-high-sierra-recovery-is-still-online-but-broken/</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demonstration (in Portuguese/Brazil)</h2><a id="user-content-demonstration-in-portuguesebrazil" aria-label="Permalink: Demonstration (in Portuguese/Brazil)" href="#demonstration-in-portuguesebrazil"></a></p>
<p dir="auto"><a href="https://youtu.be/dil6iRWiun0" rel="nofollow">https://youtu.be/dil6iRWiun0</a></p>
<p dir="auto">* Please use CC with Auto Translate to English for your convenience.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<ul dir="auto">
<li>Opencore/Acidanthera Team</li>
<li>Corpnewt for Applications (ProperTree, genSMBIOS, etc)</li>
<li>Apple for macOS</li>
<li>Proxmox - Excelent and better documentation for Virtualization</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Discord - Universo Hackintosh</h2><a id="user-content-discord---universo-hackintosh" aria-label="Permalink: Discord - Universo Hackintosh" href="#discord---universo-hackintosh"></a></p>
<ul dir="auto">
<li><a href="https://discord.universohackintosh.com.br/" rel="nofollow">Discord</a></li>
</ul>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>