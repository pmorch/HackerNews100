<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 01 Jun 2025 16:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Cinematography of "Andor" (147 pts)]]></title>
            <link>https://www.pushing-pixels.org/2025/05/20/cinematography-of-andor-interview-with-christophe-nuyens.html</link>
            <guid>44149718</guid>
            <pubDate>Sun, 01 Jun 2025 09:44:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pushing-pixels.org/2025/05/20/cinematography-of-andor-interview-with-christophe-nuyens.html">https://www.pushing-pixels.org/2025/05/20/cinematography-of-andor-interview-with-christophe-nuyens.html</a>, See on <a href="https://news.ycombinator.com/item?id=44149718">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		        	<div id="hero"><p><img src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still01.jpg"></p>
                 
		        	<p><span>Cinematography of "Andor" by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>
		        	</div>		        	<p><span>Cinematography of "Andor" by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p><p>Continuing the ongoing series of interviews with creative artists working on various aspects of movie and TV productions, it is my pleasure to welcome <a href="https://www.christophenuyens.com/"><strong>Christophe Nuyens</strong></a>. In this interview, he talks about the transition of this creative field from film to digital, bridging the gap between feature films and episodic productions, learning from different cultures, and what advice he’d give to his younger self. Between all these and more, Christophe dives deep into his work on the second season of “Andor”.</p>
<p><img fetchpriority="high" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-sets01.jpg" alt="" width="3200" height="2133"><br>
<span>Christophe Nuyens on the set of “Andor”. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: Please tell us about yourself and the path that took you to where you are today.</em></p>
<p><span>Christophe</span>: I finished a trade school as a general electrician, but I wanted to do something more, and I went to film school. During your first year you can choose between editing, sound and image – which is light and camera. So we had our first workshop, and I had the camera in my hands, and I knew this was it. I really loved the mix of technical and creative.</p>
<p><em><span>Kirill</span>: Do you feel that you can teach the technical part, but the artistic part comes from within a person, and if one doesn’t have it, it can’t be learned?</em></p>
<p><span>Christophe</span>: No, I think you can teach both. When I was growing up, I didn’t have a lot of cultural influences in my life, at home or at school. It is something that I grew over the years. When I started at the film school, I noticed that I needed to catch up on it. I spent a lot of evenings around that time watching movies with my friends, and it grew on me.</p>
<p>You can cultivate it the same way you cultivate the technical skills. There are also people who are more artistic than technical. Maybe I am more naturally inclined to be better at the technical side, but I grew and worked on my creative side over the years. I really believe you can grow the creative part of your brain.</p>
<p><em><span>Kirill</span>: Is there such a thing as universally good art vs universally bad art, or is it all subjective?</em></p>
<p><span>Christophe</span>: It’s subjective. There’s so many forms and styles of art. And that is good, because there’s something for everybody. Everything can be art, and people with different taste can find things that they appreciate.</p>
<p><img decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-sets03.jpg" alt="" width="3200" height="2133"><br>
<span>Christophe Nuyens on the set of “Andor”. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: Was film still a thing when you were in film school?</em></p>
<p><span>Christophe</span>: We did most of our projects on 16mm, either Bolex or Arriflex SR2. We did a few things on video, but it was really basic at the time. I remember those assignments to film something and edit it ourselves, and it was a nightmare. The computers were slow, the Video cards didn’t work, the software was basic. It’s incredible to see how all of that progressed since then. These days I teach at that same school, and the difference is night and day. They can edit it in DaVinci, they can grade it, and it’s so accessible. Sometimes I’m a bit jealous to see that [laughs].</p>
<p><em><span>Kirill</span>: How was the transition from film to digital for you after you finished the film school?</em></p>
<p><span>Christophe</span>: When I graduated, most of the productions were still on film. I was exposed to both mediums, and I’m happy about it. I know how to light for film. I still have an analog still camera, and I use it a lot.</p>
<p>But at the same time, I’m so happy that the digital revolution happened. It’s a bigger toolbox for your creativity, especially for night scenes. It’s much easier to light something natural, and to do something with less. I started my career in Belgium, and it’s a smaller market with smaller budgets for TV shows and films – but you still want to make good things. I did a TV show called “Cordon” about 10 years ago. It was an ambitious project for its small budget, and that project started my international career. I don’t think it would have been possible to make that project on film. We had a lot of night scenes on it, and it’s so much different to light a night scene on a digital camera.</p>
<p><img decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still06.jpg" alt="" width="3840" height="1608"><br>
<span>Cinematography of “Andor” by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>

<p><em><span>Kirill</span>: There’s a lot of ongoing technological advances in the field, from lights to lenses to sensors. Is it hard to stay up to date on all the latest developments?</em></p>
<p><span>Christophe</span>: Not for me, because that’s my technical side that I’m interested in. I follow everything, and if there’s something new, I want to test it. But at the same time, I’m a fan of old glass. I like to dig into old equipment and to test new equipment.</p>
<p>Right now there’s a lot of interesting things happening with lights. Camera sensors are getting bigger and we’re getting more pixels, but I don’t think it matters that much. And the camera sensibility has been at a good level for a while now. The latest breakthroughs are all in the LED light space. I used a lot of LED lights on “Andor”. All the lights are RGBW (red, green, blue, white), and you can choose any color you want. There’s someone next to you controlling those lights on their iPad, and you’re almost painting the scene with these controllable lights. You can control the color of each one, you can control the intensity of each one, and you can do it all in real time.</p>
<p>When I graduated, it was all with gel filters, tungsten lights and HMIs. Those lights were shifting their color as they aged, and it was a more time consuming process to tweak the colors. Now you have such fine grained control over LEDs, and it’s the biggest positive change for me in the last few years. Sometimes I still use tungsten lights, but my first preference is LED.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still12.jpg" alt="" width="3840" height="1588"><br>
<span>Cinematography of “Andor” by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: Is there anything today that is a big gap on the technology side?</em></p>
<p><span>Christophe</span>: Right now everything is going wireless. Video is wireless. Lights are wireless. Sound is wireless. It’s all good, but there’s a lot of congestion on sets with all those things combined. Sometimes those nice tools don’t work because there’s too much technology on set [laughs]. That’s my only complaint.</p>
<p><em><span>Kirill</span>: Getting closer to “Andor”, you’ve spent around 15 years now working on various episodic productions. How do you see audience expectations and production ambitions evolving over that period of time? I look back at how it was in the ’90s, where we had the feature world and the TV world, and there was an almost unbridgeable gap between the two. And now in 2025 that gap is pretty much gone.</em></p>
<p><span>Christophe</span>: It’s nice that you say that, because it’s something I fought against for years. Up until about 5 years ago people would say that if you do TV shows, you can’t do feature film work. If you wanted to do something creative on a show, with lights or with a camera, they would push back on it. They would say that it’s a TV show and that it doesn’t need that.</p>
<p>Right now, the audiences watching episodic shows accept and expect a higher level of quality. It’s such a good transition. There are TV shows that look better than films. I worked on some French TV shows where they gave us a lot of freedom even as the budget was not that big, and I’m so happy with what we were able to do there. I’ve been fighting against it for a long time in discussions with agents. They used to push me to go into the feature film world, and I’m happy with where these episodic productions are these days.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-sets02.jpg" alt="" width="3200" height="2133"><br>
<span>On the set of “Andor”. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: Getting to “Andor”, what brought you to it?</em></p>
<p><span>Christophe</span>: Thanks to David Meanti who is a producer on “Andor”. I knew him from the show “Riviera” that we did a few years ago in the south of France. He was an assistant director on that show, and we had a great working relationship. We kept in touch after that show, as he moved to the UK and started working as a producer. He tried to introduce me to the producers on the first season of “Andor”, but it didn’t work back then. But he kept on trying for the second season, and I’m grateful to David for that. At first I was offered the first 3 episodes, and after that it was extended to the next 3 episodes.</p>
<p><em><span>Kirill</span>: How did you approach bridging this arc between Season 1 and “Rogue One”?</em></p>
<p><span>Christophe</span>: “Rogue One” is one of the best Star Wars films, and because of that I was so happy when they called me to work on “Andor”. “Rogue One” has a great story and great visuals.</p>
<p>I wanted to elevate “Andor”. The first season was shot on Panavision C lenses on VENICE camera with a cropped sensor. I wanted to use a full frame sensor with a full frame anamorphic lens to get a bit closer to “Rogue One” which was shot on Alexa 65 with anamorphic lenses. A bigger sensor gives you a different feeling, and you see it when you watch a movie in IMAX. With lighting, I wanted to have a natural approach to it.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still08.jpg" alt="" width="3840" height="1592"><br>
<span>Cinematography of “Andor” by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: How much time did you have in pre-production to talk about ideas, visuals and inspirations?</em></p>
<p><span>Christophe</span>: We had a lot of time, and it’s a rare thing. The director Ariel Kleiman and I went through the same process for each episode. We were reading the scripts together, and throwing ideas and brainstorming. We did that twice for each episode, and then we started making moodboards. After that we did another read through, and then we started blocking the scenes. We had a lot of 3D pre-viz with ILM, with our camera and lenses in those virtual sets. That allowed us to start looking for shots and to refine everything.</p>
<p><em><span>Kirill</span>: How difficult is it to stay practical, to capture as much as you can in camera on sets that are literally out of this world?</em></p>
<p><span>Christophe</span>: On some sets it was more difficult, especially the ones with a lot of green screens. You have to imagine what’s behind as you’re lighting it and trying to see through it. There are limitations when working with green screens. You can’t use too much smoke or haze. You can’t use flares. It becomes less natural for me.</p>
<p>This is why for some sets we used either LED walls or painted backdrops. The wedding scene in episode 3 was time-lapsed. Every time we come back to the wedding, the light was slightly different as the Sun was getting lower and we were getting atmospheric effects. We wanted to create a feeling of an estate with the views on the mountains. Eventually we decided to not use green screens. They painted a nice backdrop of the mountains we used in Barcelona, and that was great. You see the final result in camera, and you can light it more naturally. Another set where we used painted backdrops was when Krennic gives his speech to take control of Ghorman. We had a painted backdrop of snowy mountains, and it worked well.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still04.jpg" alt="" width="3840" height="1608"><br>
<span>Cinematography of “Andor” by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: How was your collaboration with VFX? Was it mainly in post-production, or an ongoing process?</em></p>
<p><span>Christophe</span>: It was ongoing. We were all in one office, including the art department and VFX. We had a lot of meetings to discuss how this planet should look, what we can do in pre-viz, etc. We had full VFX shots of the TIE fighter flying integrated into the larger sequences. Mohen Leo was the VFX supervisor, and he wanted every VFX shot to be physically grounded. You see that in every VFX shot, including the lighting. Some shots started full VFX and then became sets. Luke Hull, our production designer, was always in those meetings. It was a close collaboration between Ariel the director, myself, the art department and the VFX crew from ILM.</p>
<p><em><span>Kirill</span>: In terms of sets, how much was built on stage and how much was built on location?</em></p>
<p><span>Christophe</span>: There was a lot built on the stage and back lots at Pinewood and Longcross. Yavin, for example, was all at Longcross. It’s an old military test track, with some buildings and a lot of forest. We used that forest to build Yavin’s landing areas for the fighters. The Ghorman city and plaza were built on a back lot at Pinewood, including most of the interiors.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still09.jpg" alt="" width="3840" height="1608"><br>
<span>Cinematography of “Andor” by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: How much time did you spend shooting the wedding ceremony?</em></p>
<p><span>Christophe</span>: It was about a week and a half, but we also had a set strike as we were getting close to the finish line. The last bit when they all start dancing like crazy was filmed six months later.</p>
<p><em><span>Kirill</span>: How many countries did this show take you to?</em></p>
<p><span>Christophe</span>: We were in Barcelona where we shot the Coruscant Senate, and in London where we shot at Barbican and Lloyd’s building.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still02.jpg" alt="" width="3840" height="1610"><br>
<span>Cinematography of “Andor” by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: For some of the bigger sets, is it a combination of a physical build and then digital extensions?</em></p>
<p><span>Christophe</span>: Some of them were fully physical, and some were extended. The Plaza at Ghorman was built physically as one story, and everything above that is a digital extension. I remember how astonished I was the first time I arrived on that set. It was so big. Another example of combining the two was the Ghorman heist sequence. They built quite a bit of those streets, and then above it we had green screens for extensions. The VFX crew was so good at making extensions look so natural.</p>
<p>We had a great collaboration on this show. We knew how we wanted these places to look before we started shooting. I could plan my lighting ahead of time, including the VFX pieces.</p>
<p><em><span>Kirill</span>: The show spends a lot of time at the apartment with Bix. How big was it?</em></p>
<p><span>Christophe</span>: The nice thing about that set is that everything you see is captured in camera, including the view outside which was an LED wall. That wall allowed us to be more creative and find some extra shots. You have shots from the outside where you see the city below in reflection with raindrops on the window. You see cars passing and light coming in.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still07.jpg" alt="" width="3840" height="1608"><br>
<span>Cinematography of “Andor” by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: Did you want to use different colors for different planets or different sets?</em></p>
<p><span>Christophe</span>: We wanted every arc of three episodes to feel like its own movie. Our first block was episodes 4 through 6, and we wanted that block to feel cold. Our visual reference for it was the city of Turin in Italy, which is a mountain city. We wanted the feeling of that winter light when the Sun is already behind the mountains, but there’s still light, and it’s all blue. And if the Sun is out, it should be low, with a warm magenta feel to it. We have a lot of rain in these three episodes as well. And then when we shot the next block of the first three episodes, we wanted it to be sunny. It all has a summer feel, with lots of sunlight, including Dedra’s apartment.</p>
<p>Then, within each block you have different planets. Some places are more cold with not a lot of colors. The wedding had a more classical look, and I used tungsten lights there to create that look. Yavin feels like an old Star Wars movie. It was such a nice exercise to play with different looks and moods. It’s a gift to be able to do that.</p>
<p><em><span>Kirill</span>: Was there any color you wanted to stay away from?</em></p>
<p><span>Christope</span>: Yes for certain sets, but not for the whole show. We used a mix of cold moonlight and warm sources on Yavin for consistency, for example.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still10.jpg" alt="" width="3840" height="1608"><br>
<span>Cinematography of “Andor” by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: What was your most memorable moment, and what was your most challenging moment on this show?</em></p>
<p><span>Christophe</span>: The most challenging was Mina-Rau and its grain fields. They planted the grain fields, and we had a window of 3-4 weeks to shoot it. And then, just before we started shooting, we had a set strike and we lost half of our actors. We decided to keep on shooting with non-SAG actors, but pretty much every scene was a mix of SAG and non-SAG. So we started shooting bits and pieces of the scenes, also fighting with the elements [laughs], and then we completed the reverse shots six months later, on a stage during winter time.</p>
<p>Our greens department was cutting all those grain stalks to put on stage, but my main challenge was to keep the lighting consistent and natural. Back when we were shooting in the field, I was already thinking about what would happen later on. I was really meticulous, measuring the light of the field, the light of the Sun, the light of the clouds, and noting all the color temperatures. Then when we came to that stage, we had lots of LED fixtures on the ceiling, and we were able to match those color temperatures everywhere. Another interesting challenge was to recreate the feeling of the air with all of the field particles.</p>
<p>It’s easy when you have one scene outside and then another on the stage, but here we were matching shots within the same scenes. I like those kinds of challenges.</p>
<p>As for the most memorable scene, I loved shooting Yavin. I like being outside, and it also reminded me of the old Star Wars movies. Another one was the heist on the streets of Ghorman. We spent about four weeks, mostly during the night, in the UK winter. It was freezing. We had rain machines. It was hard, but it paid off in the end. It was also my first big set to light. It was a big challenge, and I was so happy with the end result.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still11.jpg" alt="" width="3779" height="1606"><br>
<span>Cinematography of “Andor” by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: How does it feel to see these weeks and months of work compressed into a few hours on the screen?</em></p>
<p><span>Christophe</span>: Before the episode is shown to the public, I see multiple cuts of it. That’s when I see all the faults. That’s when I think of all the things that I could have done better. And then some months later it hits the streaming network, and now I can see it from a distance – and it’s really nice to see it in its final form.</p>
<p><em><span>Kirill</span>: How was Covid for you and how is it today?</em></p>
<p><span>Christophe</span>: I’m so happy that it’s all behind me. We still had Covid restrictions when we started working on this season. It was a lot of testing, and it was cumbersome for the entire production. Also, when you work in a mask, it takes away a big part of the human interaction on set. I want to see people that I work with. When you talk with someone and you have a mask on, you lose half of the conversation. From the moment we could work without a mask, it really opened up. Everything became more relaxed. Communication became easier and warmer. It was hard for me to not have the same human interaction with the restrictions in place.</p>
<p><em><span>Kirill</span>: What are your top three favorite movies of all time?</em></p>
<p><span>Christophe</span>: One of my favorites is “Apocalypse Now”. It’s incredible, and it’s also maybe another reason why I loved shooting Yavin – it’s the same atmosphere. I love “No Country for Old Men” and “Children of Men” with its incredible camera work.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still13.jpg" alt="" width="3840" height="1594"><br>
<span>Cinematography of “Andor” by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: If you had a time machine, and you could go back in time to give your younger self a piece of advice, what would it be?</em></p>
<p><span>Christophe</span>: Be patient. I was really impatient with everything, as it was so difficult to get into doing fiction. Everything else comes along. I don’t regret any twists and turns of the path that I took.</p>
<p><em><span>Kirill</span>: What keeps you going? You get to travel the world and work on these great productions, but it also involves long hours and sleepless nights, and being away from your family and friends for long periods of time.</em></p>
<p><span>Christophe</span>: It’s really hard for the family, and it’s a demanding job. As long as I feel that I keep on learning, that I keep on evolving, that I keep on meeting really nice people I work with, I’m happy. When it starts to feel that it’s becoming a routine and that I’m doing the same things all over again, then I’ll stop and see what’s next. There are several other things I would love to do in my life outside of movies, but for now I still love it.</p>
<p>I’m getting to know new people. I’m learning a lot, be it on a bigger project like “Andor” or on smaller ones. When you travel, you get exposed to other cultures. I was on a production that took us to Ukraine and Kazakhstan, and the locals have a different way of working. I worked a lot in the UK, and their way is different from how they do it in France. I’m about to do two French movies, and I’m looking forward to that. The French way is the artists’ way. They spend the whole day talking about it. In the UK it’s a bit more efficient, maybe [laughs].</p>
<p>As long as I have this feeling, it’s good for me. But if I feel that I’m doing the same thing over and over again, I’d want to stop.</p>
<p><em>Kirill: From all the places you’ve traveled to so far, what’s your favorite cuisine?</em></p>
<p>Christophe: French. When I work in France, I gain 5 kilos every time. The food is so good, and they take a full hour to eat. If you don’t take an hour to eat and production decides to do continuous days, they will strike. It’s too much [laughs]. The UK crews don’t like to stop. They keep on going so they can go home on time to spend at least a bit of time with the family. As far as the food goes, I prefer to work in France.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-sets04.jpg" alt="" width="3200" height="2133"><br>
<span>On the set of “Andor”. Courtesy of Lucasfilm/Disney+.</span></p>
<p>And here I’d like to thank <a href="https://www.christophenuyens.com/"><strong>Christophe Nuyens </strong></a>for taking the time to talk with me about the art and craft of cinematography. I also want to thank Nathalie Retana and Jamie Miller for making this interview happen. “Andor” is <a href="https://www.disneyplus.com/browse/entity-faba988a-a9f5-45f2-a074-0775a7d6f67a">streaming on Disney+</a>. Finally, if you want to know more about how films and TV shows are made, <a href="https://www.pushing-pixels.org/inmotion/">click here</a> for additional in-depth interviews in this series.</p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why DeepSeek is cheap at scale but expensive to run locally (147 pts)]]></title>
            <link>https://www.seangoedecke.com/inference-batching-and-deepseek/</link>
            <guid>44149238</guid>
            <pubDate>Sun, 01 Jun 2025 07:31:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.seangoedecke.com/inference-batching-and-deepseek/">https://www.seangoedecke.com/inference-batching-and-deepseek/</a>, See on <a href="https://news.ycombinator.com/item?id=44149238">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header></header><section><p>Why is DeepSeek-V3 supposedly fast and cheap to serve at scale, but too slow and expensive to run locally? Why are some AI models slow to respond but fast once they get going?</p>
<p>AI inference providers often talk about a fundamental tradeoff between <em>throughput</em> and <em>latency</em>: for any given model, you can either serve it at high-throughput high-latency, or low-throughput low-latency. In fact, some models are so naturally GPU-inefficient that in practice they must be served at high-latency to have any workable throughput at all (for instance, DeepSeek-V3).</p>
<p>This tradeoff comes from the <strong>batch size</strong> the inference provider chooses for the model: not batching inference inside an individual request<sup id="fnref-1"><a href="#fn-1">1</a></sup>, but batching inference across tens or hundreds of concurrent user requests. It’s a peculiar feature of transformer-based LLMs that computing a batch of completions at the same time is almost as fast as computing a single completion. Why is that?</p>
<h3>What is batch inference?</h3>
<p>GPUs are good at doing big matrix multiplications (GEMMs, or “general matrix multiplications”). Say you have a single token that you want to pass through a model (i.e. by multiplying against all its weights - other architecture details aren’t relevant). You express that as a vector that matches the dimension (or hidden size) of the model (i.e. 1 x the width of its big weights matrices) and multiply it through. That’s 1 GEMM. But if you want to pass ten tokens through in a batch, that’s still only one GEMM, because you can stack the tokens into one matrix (10 x the model dimension). That’s a <em>lot</em> faster than doing ten slightly smaller GEMMs. So an inference server implementation might look something like this:</p>
<ol>
<li>A request comes in with a prompt</li>
<li>That prompt is pre-filled (passed through attention - we’ll see later how that can be batched as well<sup id="fnref-2"><a href="#fn-2">2</a></sup>), forming a KV cache and a token-sized matrix (1 x model-size) that will eventually become the predicted token<sup id="fnref-3"><a href="#fn-3">3</a></sup></li>
<li>That token-sized matrix goes into a queue</li>
<li>A GPU server pulls batches (e.g. of 128) off that queue, stacks them up into a 128 x model-size matrix, and multiplies them through the feed-forward model weights</li>
<li>The end result is then split into 128 separate tokens</li>
<li>The one for the original request is streamed back to the user</li>
<li>Assuming that token isn’t an end-of-sequence token, return to step 2 to continue generating the next token in the response</li>
</ol>
<p>Note that <em>the server decides</em> how big a batch size to pull. It’s a tradeoff between throughput and latency. If you do no batching and just process tokens one by one, no user ever waits in a queue (step 3 above), so latency is low (assuming you have enough GPUs). However, if you do a lot of batching, latency is high because users will be waiting until the batch size fills up, but throughput will be much higher because the GPUs are being used more efficiently.</p>
<p>Why are GPUs faster at multiplying large matrices once than small matrices many times? Two reasons. First, there’s some overhead involved in issuing each command to the GPU, and one big multiplication can be launched with a single command. Second, each new GPU command involves fetching weights from memory, which can be expensive for large weights. If you run lots of small GEMMs, you can end up spending most of your time shipping weights in and out of memory instead of computing.</p>
<h3>Why are some models tuned for high batch sizes?</h3>
<p>Typically an inference server will have a “collection window” where user requests come in and are queued. Chat servers typically aim for 5-10ms, but very high-batch backends might go as wide as 200ms. If a new request comes in at the start of the window, it might wait the entire window duration before being processed<sup id="fnref-4"><a href="#fn-4">4</a></sup>. When the window closes, all the queued requests are batched up (i.e. all the 1xmodel-size matrices are concatenated into a single 128xmodel-size matrix) and that batch is sent through the pipeline. Running a batch like this is sometimes called a “tick”.</p>
<p>As the explanation above suggests, you can run any model at any batch size. There’s nothing inherently about the batching process that would rule out some types of model. However, it is possible to build a model so GPU-inefficiently that it effectively <em>needs</em> batching in order to be practical.</p>
<h4>Why mixture of experts requires higher batch sizes</h4>
<p>For instance, take a mixture-of-experts model (like DeepSeek-V3 or supposedly the original GPT-4). You can get a strong model by training it to have hundreds and hundreds of “experts”: separate blocks of feed-forward weights, from which a routing layer picks a subset that’s used on each token. But a model like this is really GPU-inefficient. We can see why: GPUs want to do a small number of really big matrix multiplications, but if you have many experts you’re forced into many small multiplications. Unless you do your inference in batches, that’s going to mean low throughput.</p>
<p>Let’s think through how a “collection window” of 5ms and 200ms would perform for a large mixture-of-experts model. Suppose you pick up ten user requests in that 5ms window. If you have many experts, some experts might end up only running against one or two tokens (i.e. the batch size <em>for each expert</em> will be much lower than the total set of requests you’ve picked up in your window). If, however, you wait for 200ms and pick up 4000 user requests, you are much more likely to saturate all your experts. At the cost of some latency, you’re making sure that your GEMMs are large and your GPUs are constantly utilized at their maximum capacity.</p>
<h4>Why large pipelines require high batch sizes to avoid pipeline bubbles</h4>
<p>For large models, it can be a challenge to keep the GPUs active at all. Large models typically have many transformer layers: i.e. hundreds of matrices of weights that make up the feed-forward network. The only way to do fast inference here is to <em>pipeline</em> those layers by having one GPU handle the first ten layers, another handle the next ten, and so on. Otherwise you just won’t be able to fit all the weights in a single GPU’s memory, so you’ll spend a ton of time swapping weights in and out of memory and it’ll end up being really slow. During inference, each token (typically in a “micro batch” of a few tens of tokens each) passes sequentially through that pipeline of GPUs.</p>
<p>How efficient your pipeline is depends on the number of layers you have and the size of your collection window. When you’re processing the tokens in a window during a “tick”, you’ll get some idle GPUs at the start (because GPUs in later layers won’t have anything to work on yet) and some more idle GPUs at the end (when there’s no more tokens in the queue, GPUs in early layers will have to wait for the next “tick”). These periods of idleness are sometimes called “warmup” and “drain”. If you have many small windows, you’re going to spend more GPU time in warmup and drain than if you have fewer large windows. By picking your window size, you’re thus directly trading off between throughput and latency.</p>
<p>If you have a ton of layers and your collection window is really short, you might sometimes end up with fewer tokens to process than layers. This is called a “pipeline bubble” - in effect the “drain” stage starts earlier than usual. You can’t eliminate warmup and drain (for reasons discussed below, inference has to operate in sequential “ticks”), but you can eliminate pipeline bubbles by making your collection window long enough. Pipeline bubbles can be absolutely brutal for model throughput, so inference providers always set their windows wide enough to avoid them. That adds noticeable latency for models with many layers.</p>
<h4>Can’t you just keep the queue full?</h4>
<p>Why couldn’t inference providers eliminate warmup and drain entirely by keeping the GPU queue full of tokens? In other words, couldn’t you do away with ticks altogether and just keep the token micro-batches flowing? Of course each user’s inference has to be sequential (since you can’t start generating the next token until the current token is done), but large inference providers should have enough concurrent traffic to keep the queue full of separate user requests.</p>
<p>I’ll confess I struggle to see why this shouldn’t be possible in theory. As far as I can tell the practical barrier is how the <em>attention</em> step is batched: if you want to batch up attention GEMMs, they need to all be the same shape (i.e. the same number of prior tokens in the sequence). So you have to run groups of the same shape at the same time, instead of being able to just maintain a single queue. There’s at least <a href="https://arxiv.org/abs/2403.02310">some public research</a> on this front, but I wouldn’t be surprised if there were more clever tricks for doing this that I haven’t seen.</p>
<p>Another idea: if you need ticks for the attention step, why not just have a tick-based attention inference system and a more efficient continuous system for the FFN? As I understand it, the reason is <em>memory overhead</em>:</p>
<ol>
<li>Since the attention output is needed for the FFN, you’d need to have some place in-memory to park it while it waits for its slot in the FFN queue, which would quickly become too expensive.</li>
<li>Modern inference stacks are able to combine the attention and FFN step into a couple of large GEMMs in a single “operation”. If you’re doing these on different GPUs, you have to run different operations and shuttle the weights in and out of memory.</li>
</ol>
<h3>Summary</h3>
<ul>
<li>GPUs are most efficient on <em>large</em> GEMMs, so stacking many tokens into a single matrix multiply gives far higher token throughput than processing them one-by-one</li>
<li>
<p>During decoding, attention can only be batched for tokens at the <strong>same step</strong>, forcing schedulers to run in short “ticks”. How many tokens you pack into a single “tick” (i.e. how long you wait to collect tokens) is your batch size</p>
<ul>
<li>These are tokens <em>from different users</em>. You can’t batch tokens from the same user because you need previous tokens to generate the next one, so batching requires a high volume of traffic from different users</li>
</ul>
</li>
<li>Bigger batches raise latency because user tokens might be waiting up to 200ms before the batch is full enough to run, but they boost throughput by allowing larger (and thus more efficient) GEMMs in the feed-forward step</li>
<li>Models with many layers (e.g. long pipelines) need larger batches to avoid <strong>pipeline bubbles</strong> (by ensuring each tick contains more batches than pipeline steps) </li>
<li>Mixture-of-Experts models need to be served with high-latency to be efficient: each expert sees only the tokens routed to it, so you need larger global batches to keep every expert busy.  </li>
<li>Inference providers pick a batch size/window that clears pipeline bubbles and saturates experts. High batch sizes buy you more throughput at the cost of higher latency as tokens wait to fill up the tick</li>
<li>Some models (like DeepSeek’s) that are mixture-of-experts with many layers thus <em>require</em> large batch sizes and high latency, otherwise throughput drops off a cliff. That’s why it’s commonly said that you can’t easily run DeepSeek for personal use: because with a single user running one inference at a time, it runs at very low efficiency/throughput</li>
<li>
<p>The fact that OpenAI and Anthropic’s models are quick to respond suggests that either:</p>
<ul>
<li>Their models have a more efficient architecture (non-MoE, fewer layers), or</li>
<li>OpenAI/Anthropic have some very clever tricks for serving inference, or</li>
<li>they’re paying through the nose for way more GPUs than they strictly need</li>
</ul>
</li>
</ul>
<p><sup id="fnref-1"><a href="#fn-1">1</a></sup> One commonly-observed strength of transformers is that they can batch <em>prefill</em> within a single user request. When you pass them a long prompt, they can process that prompt all at once because of how the attention mechanism works. Previous recurrent models had to go token-by-token, which was much slower (because it involved many more GEMMs). <strong>This has nothing to do with the kind of batching I’m talking about in this post</strong>. I’m talking about how you can efficiently batch <em>inference</em> across many different user requests once the prefilling is complete.</p>
<p><sup id="fnref-2"><a href="#fn-2">2</a></sup> This can also be batched, so long as you’re only batching attention operations with the same number of tokens in the sequence (i.e. every sequence predicting the fourth token can be batched together). Otherwise the size of the KV cache matrices are different, so you can’t easily combine them into a single batch. More on that later.</p>
<p><sup id="fnref-3"><a href="#fn-3">3</a></sup> Technically it’s not a token being generated, but the “logits” (a probability distribution across all possible tokens). I’ll say “token” here and later on to keep it simpler.</p>
<p><sup id="fnref-4"><a href="#fn-4">4</a></sup> Note that in practice modern inference stacks will use “continuous batching”, where a batch is sent off as soon as it’s full instead of waiting for the entire length of the fixed time window. However, the inference is still done in batches, to the core tradeoff between throughput and latency is the same.</p></section><p>If you liked this post, consider <a href="https://buttondown.com/seangoedecke" target="_blank">subscribing</a> to email updates about my new posts.</p><p>June 1, 2025<!-- -->&nbsp;│ Tags: <a href="https://www.seangoedecke.com/tags/ai/">ai</a>, <a href="https://www.seangoedecke.com/tags/explainers/">explainers</a>, <a href="https://www.seangoedecke.com/tags/deepseek/">deepseek</a></p><hr></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Figma Slides Is a Beautiful Disaster (245 pts)]]></title>
            <link>https://allenpike.com/2025/figma-slides-beautiful-disaster</link>
            <guid>44148933</guid>
            <pubDate>Sun, 01 Jun 2025 05:59:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://allenpike.com/2025/figma-slides-beautiful-disaster">https://allenpike.com/2025/figma-slides-beautiful-disaster</a>, See on <a href="https://news.ycombinator.com/item?id=44148933">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Some highlights and lowlights.</p><div>
<p>I think of presentation slides as having 3 main jobs:</p>

<ol>
  <li><strong>Emphasize key points</strong>, so people remember</li>
  <li><strong>Break up complex concepts</strong>, so people learn</li>
  <li><strong>Entertain</strong>, so people pay attention</li>
</ol>

<p>This calls for slides that are mostly images or very short phrases. A minority of slides justify designing something to match the vibe or illustrate a point – a diagram, for example. This can mean going back and forth between Keynote and, say, Figma, but it’s worth the effort.</p>

<p>This year I’ve been spending a lot of time in Figma, so when I was recently asked to speak at an event I thought, “Why not try <a href="https://www.figma.com/slides/">Figma Slides</a>?” Figma Slides launched a year back, and <a href="https://www.linkedin.com/posts/paigecostello_figma-slides-came-out-of-beta-today-30-activity-7308211241049591811-Z_-U/">graduated from beta in March</a>. I’ve used Keynote for almost 20 years now – it seemed like time for me to finally upgrade my presentation tool.</p>

<p>So I gave it a whirl.</p>

<p>Pretty quickly, I liked building slides right in Figma. The Grid view made it easy for me to structure my ideas. Features like Auto Layout and Components made building slides that adapt to different text and images a snap.</p>

<p>For example, a key point in my talk was that selecting a JavaScript framework can be overwhelming. It was easy to build a quick visual of this right in the deck.</p>

<p><img src="https://allenpike.com/images/2025/figma-smash.jpg" alt="A Choose Your Fighter screen of JS frameworks.">
Components and Auto Layout made this 10x faster to build in Figma than Keynote.
</p>

<p>Admittedly, Figma Slides is missing some Keynote features that I think of as pretty essential.</p>

<p>For example, Keynote has long had a toggle to “Autosize Text”, which will set an object’s font size to just big enough to fill its container. There’s no way to do this in Figma’s auto layout because Figma only wants to support layouts that CSS Grid can implement. That constraint makes sense when designing for the web, but shows how hard it is to expand a design tool to other domains.</p>

<p>Another odd omission is that you can’t easily create a slide where items (e.g. bullets, sections of a diagram) appear with each click. The closest you can do is to split them into different layers, add a “Fade” animation of 1 millisecond on each, then <em>reorder the animation order to match the order they appear on the slide</em> since it will default to the order the layers were created. 🥴</p>

<p>I’m not a fan of bullet-laden decks, but sometimes you just want to make 4 words appear one by one, okay?</p>

<p>Whining aside, building in Figma Slides got me excited. It was powerful, it was fun, and it gave me hope that Figma would successfully make the transition from single-product company about to be acquired by Adobe, to multi-product company <a href="https://www.cnbc.com/2025/04/15/figma-confidentially-files-for-ipo-a-year-after-ditching-adobe-deal.html">ready to IPO</a>.</p>

<p>Then I made a fatal mistake: I tried to actually give the presentation.</p>

<p><img src="https://allenpike.com/images/2025/figma-offline.jpg" alt="Figma error -106">
</p>

<p>I’ve given enough talks that I know to do at least one rehearsal with representative conditions: on an external display, clicker in hand, cued by the presenter view. I know to make sure it works offline, save a backup copy of the deck, and so on.</p>

<p>It was during this dry run I noticed some bad omens:</p>

<ol>
  <li>You can “Save Local Copy” of the presentation, but that doesn’t allow you to present it locally.</li>
  <li>Just because you have a presentation open and loaded, doesn’t mean you can present it. If you are offline when you actually click Present, it will barf.</li>
  <li>Once you are presenting, you can click to “download” the presentation to be available offline – but be careful not to close the tab or it will undownload!</li>
  <li>When you do click Present, you will not get a full-screen audience view, nor a <a href="https://support.apple.com/en-ca/guide/keynote/tanfde4a3e6d/mac">keyboard shortcut to swap which display the audience can see</a>. You just get a pop-up window you need to manually drag to the projector – hope it’s somewhere intuitive! – then maximize it. At a pro event you can do this with the AV crew, but at a meetup this is just visible to the audience.</li>
  <li>Make sure you then move your mouse to the edge of the screen. Otherwise, it will stay there on top of your slides like it’s 1999.</li>
  <li>Even beyond this, the functionality around managing Present and the Audience view is just a little… flaky?</li>
</ol>

<div>
<video src="https://allenpike.com/images/2025/figma-x.mp4" autoplay="" loop="" muted="" playsinline="">
  Your browser doesn't support HTML5 video.
</video><p>
Figma will decide when your presentation is over.
</p></div>

<p>This all made me uneasy, so I practiced the flow more often than usual. I made sure I clicked the right things to make it work offline. Then, I went to go present.</p>

<p>At the venue, I immediatley noticed something once the slides were up on their giant 40 foot display: I needed to click twice to advance each slide. Odd, but I’m an adult. I can click twice for each slide.</p>

<p>Unfortunately, halfway into my talk I noticed a much bigger problem.</p>

<p>The animations – which I had used only on my most complex slides, which needed to be explained in parts – weren’t advancing at all. Even if I clicked twice, the audience display would continue to show a blank slide.</p>

<p><a href="https://www.youtube.com/watch?v=j7_o-YiwGwo">
<img src="https://allenpike.com/images/2025/clicker-fail.jpg" alt="A man looks in confusion at his own clicker.">
</a>
Looking at my clicker in desperation: the exact moment I realized that I’d been hoisted on my own petard.
</p>

<p>After some exploratory clicking, I found that once all the builds on a blank slide had been triggered, it would then jump to the next slide. At that point, you could click back to show the fully built skipped slide.</p>

<p>So that’s what I did: on a slide with 7 builds, I would dutifully click 14 times to get Figma through to the next slide, then then click back, and attempt to explain all seven parts at once.</p>

<p>I stumbled through. The audience was gracious – thankfully it was only 100 people and they were friendly. But it stung a little.</p>

<p>A silver lining was that the core point of my talk was that <a href="https://boringtechnology.club/">boring technology is underrated</a>. Watching me struggle with Figma Slides helped prove the point. <strong>Here lies Allen, murdered by not-boring technology</strong>.</p>

<p>While I was able to reproduce the animation bug the next day, I haven’t been able to since restarting Figma. Their forums have stories of <a href="https://forum.figma.com/ask-the-community-7/fixed-figma-slides-show-smart-animate-transition-only-in-presenter-view-and-not-in-audience-view-39081?utm_source=chatgpt.com">other</a> <a href="https://forum.figma.com/ask-the-community-7/fixed-figma-slides-show-smart-animate-transition-only-in-presenter-view-and-not-in-audience-view-39081?utm_source=chatgpt.com">similar</a> <a href="https://forum.figma.com/ask-the-community-7/figma-slides-with-multiple-videos-in-present-mode-have-to-click-arrow-key-loads-before-next-slide-28477?utm_source=chatgpt.com">bugs</a>, none quite the same. So maybe it’s fixed.</p>

<p>Even if this specific bug is fixed, though, I get the strong impression that Figma doesn’t treat presenting Slides as mission-critical. A team that uses Figma Slides at meetups or conferences wouldn’t let clicking Present offline just vomit “Error -106”.</p>

<p>While Keynote is old, and pasting things in from other design tools feels janky, the OG still benefits from Apple institutionally <a href="https://allenpike.com/2022/giving-a-shit">giving a shit</a> about giving a presentation to an audience. You can tell they feel it should be bulletproof.</p>

<p>So, in the end, I learned the lesson I was trying to teach. Boring technology is good, actually.</p>

<p>Here’s to Keynote: it works. 🍻</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Father Ted Kilnettle Shrine Tape Dispenser (158 pts)]]></title>
            <link>https://stephencoyle.net/kilnettle</link>
            <guid>44148853</guid>
            <pubDate>Sun, 01 Jun 2025 05:31:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stephencoyle.net/kilnettle">https://stephencoyle.net/kilnettle</a>, See on <a href="https://news.ycombinator.com/item?id=44148853">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><meta property="og:image" content="/media/kilnettle1.jpg">
<img src="https://stephencoyle.net/media/kilnettle1.jpg"></p>

<p>A couple of years ago I wrote <a href="https://stephencoyle.net/father-ted-tape-dispenser">this</a> post, wherein I detail how I built a working version of the tape dispenser from Father Ted (<a href="https://youtu.be/c_ojhPi7lH4">the one that says</a> <em>you have used two inches of sticky tape, god bless you</em>). While it worked surprisingly well, that version had a number of flaws. It used unnecessarily overpowered hardware, was fragile, and didn’t sound very good.</p>

<p>Since making that one, I’d toyed with the idea of creating a better, more repeatable design. So, in very procrastinate-y fits and starts over the last few months, I’ve been doing just that.</p>

<p>This new one is much smaller, sounds better, and I daresay looks a touch more <em>professional</em> than its predecessor. Read on to find out how to get one of your own!</p>

<p>
        <iframe width="420pt" height="315pt" src="//www.youtube.com/embed/zKlZgHTyWA4" frameborder="0" allowfullscreen="">
            </iframe>
        </p>

<p><img src="https://stephencoyle.net/media/kilnettle3.jpg"></p>

<p>More importantly, it’s also much easier to make and I’ve got actual documentation for doing so. The case is 3D-printable, and all the parts print with no supports. Instead of a rotary encoder to measure the tape rotation, it now uses an IR led and sensor. The logic runs on an ESP8266 microcontroller instead of a Raspberry Pi Zero, all of which means the electronics inside can be purchased for less than €10.</p>

<p>It was a lot of fun improving on my 3D modelling, electronics, design and general planning skills in the process of making it. The initial body design was more complex, as was the circuitry, and I enjoyed iterating on and simplifying those significantly by the final product.</p>

<p>I did nurture the idea of actually selling these, but ultimately I think I’ve decided against it for now, for a few reasons:</p>

<ul>
<li>Making one is fun, and a nice evening or weekend project. Making lots is more effort than I have time for at the moment.</li>
<li>I’ve got several earmarked for people already, and when I subtract those from the dozen I’ve made, I’m left with only a tiny batch to sell.</li>
<li>Factoring in the cost of components, time to assemble, and shipping, I don’t think there’s a compelling price point that doesn’t run at a loss. In which case I’d rather just give them away on my own terms.</li>
</ul>

<h2>How can you get one?</h2>

<p><img src="https://stephencoyle.net/media/kilnettle4.jpg"></p>

<p>The good news, though, is that you can build one yourself! I’ve made the software, 3D-printable models, and instructions public on <a href="https://github.com/stephenjdc/TapeDispenser">GitHub</a>. You can also view the print files on <a href="https://www.printables.com/model/935093-kilnettle-shrine-commemorative-talking-tape-dispen">Printables</a>. It’s not a super complex build, so if you can manage basic soldering and have a 3D printer it’s probably a one-day project. I could potentially make kits, which contain all the required parts, that folks could assemble by themselves. If that's something you'd be interested in, do let me know. And if you do make one, or make some kind of remix, please send me pics!</p>

<p>Finally, and perhaps most importantly, if you do make one for yourself I'd appreciate if you made a donation to a charity that supports trans people. It's an awful shame that the creator of Father Ted is doing so much damage to an already vulnerable group, so anything that can help offset that is appreciated.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RenderFormer: Neural rendering of triangle meshes with global illumination (216 pts)]]></title>
            <link>https://microsoft.github.io/renderformer/</link>
            <guid>44148524</guid>
            <pubDate>Sun, 01 Jun 2025 03:43:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://microsoft.github.io/renderformer/">https://microsoft.github.io/renderformer/</a>, See on <a href="https://news.ycombinator.com/item?id=44148524">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><nav><div><p><img src="https://microsoft.github.io/renderformer/imgs/rf.png" alt="RF"></p></div></nav><header></header><div id="intro"><h2>Introduction</h2><p>We present RenderFormer, a neural rendering pipeline that directly renders an image from a triangle-based representation of a scene with full global illumination effects and that <strong>does not require per-scene training or fine-tuning</strong>.</p><h3>Mesh to Image, End to End</h3><p>Instead of taking a physics-centric approach to rendering, we formulate rendering as a <strong>sequence-to-sequence transformation</strong> where a sequence of tokens representing triangles with reflectance properties is converted to a sequence of output tokens representing small patches of pixels.</p><h3>Simple Transformer Architecture with Minimal Prior Constraints</h3><p>RenderFormer follows a two stage pipeline: a view-independent stage that models triangle-to-triangle light transport, and a view-dependent stage that transforms a token representing a bundle of rays to the corresponding pixel values guided by the triangle-sequence from the the view-independent stage. Both stages are based on the transformer architecture and are learned with minimal prior constraints. <strong>No rasterization, no ray tracing.</strong></p><p><img src="https://microsoft.github.io/renderformer/imgs/model-arch.jpg" alt="Model architecture"></p></div><div id="gallery"><h2>Rendering Gallery</h2><p>Examples of scenes rendered with RenderFormer demonstrating various lighting conditions, materials, and geometric complexity, without any per-scene training or fine-tuning. Check out the <a href="https://microsoft.github.io/renderformer/error-images">reference images</a> for more details.</p><div><div><p><img src="https://microsoft.github.io/renderformer/gallery/cbox.png" alt="Cornell Box"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/cbox-bunny.png" alt="Cornell Box with Bunny"></p><div><p>Stanford Bunny in Cornell Box</p></div></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/cbox-lucy.png" alt="Cornell Box with Lucy"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/cbox-teapot.png" alt="Cornell Box with Teapot"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/compose-scene.png" alt="Composed Scene"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/constant-width.png" alt="Constant Width Scene"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/crystals.png" alt="Crystal Scene"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/fox-in-the-wild.png" alt="Fox Scene"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/horse-and-heart.png" alt="Horse and Heart"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/renderformer-logo.png" alt="RenderFormer Logo"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/room.png" alt="Room Scene"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/shader-ball.png" alt="Shader Ball"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/tree.png" alt="Tree Scene"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/veach-mis.png" alt="Veach MIS Scene"></p></div></div></div><div id="videos"><h2>Videos</h2><p>Check out extra video results including <a href="https://microsoft.github.io/renderformer/uncompressed-videos">uncompressed videos</a> and <a href="https://microsoft.github.io/renderformer/reference-videos">some reference videos</a>.</p><div><h3>Teaser Scenes</h3><p>Dynamic demonstrations of RenderFormer's capabilities, showing object rotations, lighting changes, and material adjustments.</p><div><div><video autoplay="" muted="" loop="" playsinline=""><source src="https://microsoft.github.io/renderformer/videos/teaser-scenes/cbox-roughness.mp4" type="video/mp4"></video><div><p>Cornell Box Roughness Adjustment</p></div></div><div><video autoplay="" muted="" loop="" playsinline=""><source src="https://microsoft.github.io/renderformer/videos/teaser-scenes/cbox-bunny-roughness.mp4" type="video/mp4"></video><div><p>Bunny Roughness Adjustment</p></div></div><div><video autoplay="" muted="" loop="" playsinline=""><source src="https://microsoft.github.io/renderformer/videos/teaser-scenes/compose-change-light.mp4" type="video/mp4"></video><div><p>Composed Scene View Change</p></div></div></div></div><div><h3>Animations</h3><p>RenderFormer can render animations of scenes.</p></div><div><h3>Physical-Based Simulations</h3><p>RenderFormer can render physically simulated scenes with complex dynamics and interactions.</p><div><div><video autoplay="" muted="" loop="" playsinline=""><source src="https://microsoft.github.io/renderformer/videos/simulations/bowling.mp4" type="video/mp4"></video><div><p>Bowling Ball Physics Simulation</p></div></div><div><video autoplay="" muted="" loop="" playsinline=""><source src="https://microsoft.github.io/renderformer/videos/simulations/constant-width-sim.mp4" type="video/mp4"></video><div><p>Constant Width Body Simulation</p></div></div></div></div></div><div id="BibTeX"><h2>BibTeX</h2><div><pre><code>@inproceedings {zeng2025renderformer,
    title      = {RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination},
    author     = {Chong Zeng and Yue Dong and Pieter Peers and Hongzhi Wu and Xin Tong},
    booktitle  = {ACM SIGGRAPH 2025 Conference Papers},
    year       = {2025}
}</code></pre></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stepping Back (107 pts)]]></title>
            <link>https://rjp.io/blog/2025-05-31-stepping-back</link>
            <guid>44147966</guid>
            <pubDate>Sun, 01 Jun 2025 01:04:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rjp.io/blog/2025-05-31-stepping-back">https://rjp.io/blog/2025-05-31-stepping-back</a>, See on <a href="https://news.ycombinator.com/item?id=44147966">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        

<section>
<p>The other day I was playing around with Claude Code, experimenting with porting
some C code to Rust - not for any particular reason, just because I was curious
how well it could do. As these things happen, I got more and more invested in
the process, instead of just letting Claude do something and noting what
happened, I kept poking in and adjusting the behavior, "no you can't pretend the
result is 0, try again" - the usual stuff when you're trying to get an LLM to do
something large or novel.</p>
<p>But I got so invested in seeing the port of the module work that I forgot that
my original goal was to see how well the LLM could do on its own. Instead of
testing a potentially interesting hypothesis, I was poorly porting a module of a
project that I had no interest in continuing. I was only rescued in this case
from my pains by the LLM itself: Claude hit some kind of rate-limiting
error, forcing me to take a break from the project.</p>
<p>Once I was forced away, I found myself posessed with a sudden clarity: why on
earth had I 5 minutes ago become so invested in this particular task?  Given the
forced break, my brain "reset" and I could step away from it, and go back to
thinking about the original problem and what my goals and conclusions were. <em>But
there was no way in the moment I could do so.</em> I just couldn't let it go.</p>
<p>This isn't of course the first time that this has happened. Sometimes the breaks
are imposed: my wife and I are off to get lunch, and while walking to the door I
realize the last 2 hours spent feverishly trying to get some RPC from A to B to
work are actually pointless and there's some much simpler solution. Other times
I find myself sitting back and catching myself getting too deep into a problem.</p>
<p>Of course, in any big enough problem, you're never sure what to do initially.
You have to just... explore, and correct yourself if you're off. There's an
unresolvable tension between trying to get through the problem at hand and
contemplating "maybe this isn't the right direction after all". At least I can't
seem to hold both of them in my head at the same time: if you're trying to debug
some subtle problem involving 10 layers of software and hardware, you can't
afford the brain space for questioning whether you'd be better off trying a
different approach. But even more, the tenancity that makes you stick on
problems, it's almost the thing that makes you a good engineer. You need to be
stubborn and willing to push through stupid problems in order to get things
done.</p>
<p>I don't think there's an easy solution: how do you know when you're in too deep?
In RL we'd probably characterize this as a variation on the
<a href="https://en.wikipedia.org/wiki/Exploration%E2%80%93exploitation_dilemma">Exploration/Explotation dilemma</a></p>
<ul>
<li>how much energy should we invest into getting "just this next bug fixed and
working" versus taking a sip of tea and then you should change your approach or change your job?</li>
</ul>
<p>Spending too much time fixating on one thing leads you towards a sort of
monomanical obsession with it: "functional programming is the only way" sort of
thing. On the other hand, you can fixate the other way and spend all your time
contemplating what the best thing is to do, leading to a weird sort of nihilism
as you question everything while you watch life go by. And yet stepping away
from our problems can be such a powerful tool for us, giving us the space to
understand if we're doing the right thing: you can't see the forest for the
trees and all that.</p>
<p>Recently I've found myself adopting a sort of ritual for stepping back and
reflecting. I find if I align it to natural time boundaries: hours, days, weeks,
months, years, I can sort of set a mental schedule for myself and it's not too
much of a distraction. When I take a break, I can ask myself the usual
questions: "what am I doing?", "why am I doing it?", "what could I be doing
instead?". I find this can be cathartic as well, letting me note progress as
well as struggles.</p>
<p>(It should be taken for given that of course it's really hard to switch your
mindset from "debugging this thing is really important" to "should I debug this
thing at all", <em>while you're in the middle of debugging said thing</em>. But you
know, you just try your best, sometimes your brain refuses to let go, and other
times, maybe you get enough space to decide going off for a beer is the better
bet.)</p>
<p>Of course, the amount of time deliberating and the scope of the questions
changes with the time duration: I don't want to question why I'm working at my
job 8 times at day, but maybe once a week is okay? Conversely, I can afford 1
minute an hour to jot down what I'm trying to do: I'm not contemplating throwing
away a project at that point, but maybe the scope of a feature or how I'm
debugging an issue.</p>
<p>But once a year, well then maybe it's worth taking a day off from everything and
contemplating where you're going in life: are you happy with your decisions? In
a way you're paying 1% of your time and energy as a sort of insurance against
wandering too far in a bad direction. Not a bad trade.</p>

</section>

    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Progressive JSON (373 pts)]]></title>
            <link>https://overreacted.io/progressive-json/</link>
            <guid>44147945</guid>
            <pubDate>Sun, 01 Jun 2025 00:58:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://overreacted.io/progressive-json/">https://overreacted.io/progressive-json/</a>, See on <a href="https://news.ycombinator.com/item?id=44147945">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Do you know about Progressive JPEGs? Here’s a <a target="_blank" href="https://www.liquidweb.com/blog/what-is-a-progressive-jpeg/">nice explanation</a> of what a Progressive JPEG is. The idea is that instead of loading the image top to bottom, the image instead is fuzzy at first and then progressively becomes more crisp.</p>
<p>What if we apply the same idea to transferring JSON?</p>
<p>Suppose you have a JSON tree with some data:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>'</span><span>Welcome to my blog</span><span>'</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>{</span></span>
<span data-line=""><span>    content: </span><span>'</span><span>This is my article</span><span>'</span><span>,</span></span>
<span data-line=""><span>    comments: </span><span>[</span></span>
<span data-line=""><span>      '</span><span>First comment</span><span>'</span><span>,</span></span>
<span data-line=""><span>      '</span><span>Second comment</span><span>'</span><span>,</span></span>
<span data-line=""><span>      // ...</span></span>
<span data-line=""><span>    ]</span></span>
<span data-line=""><span>  },</span></span>
<span data-line=""><span>  footer: </span><span>'</span><span>Hope you like it</span><span>'</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>Now imagine you want to transfer it over the wire. Because the format is JSON, you’re not going to have a valid object tree until the last byte loads. You have to wait for the <em>entire</em> thing to load, then call <code>JSON.parse</code>, and then process it.</p>
<p>The client can’t do <em>anything</em> with JSON until the server sends the <em>last</em> byte. If a part of the JSON was slow to generate on the server (e.g. loading <code>comments</code> took a slow database trip), <strong>the client can’t <em>start any</em> work until the server <em>finishes all</em> the work.</strong></p>
<p>Would you call that good engineering? And yet it’s the status quo—that’s how 99.9999%<sup>*</sup> of apps send and process JSON. Do we dare to improve on that?</p>
<p><small>* I made it up</small></p>
<hr>
<h3 id="streaming-json"><a target="_self" href="#streaming-json">Streaming JSON</a></h3>
<p>We can try to improve this by implementing a <em>streaming</em> JSON parser. A streaming JSON parser would be able to produce an object tree from an incomplete input:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>'</span><span>Welcome to my blog</span><span>'</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>{</span></span>
<span data-line=""><span>    content: </span><span>'</span><span>This is my article</span><span>'</span><span>,</span></span>
<span data-line=""><span>    comments: </span><span>[</span></span>
<span data-line=""><span>      '</span><span>First comment</span><span>'</span><span>,</span></span>
<span data-line=""><span>      '</span><span>Second comment</span><span>'</span></span></code></pre></figure>
<p>If you ask for the result at this point, a streaming parser would hand you this:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>'</span><span>Welcome to my blog</span><span>'</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>{</span></span>
<span data-line=""><span>    content: </span><span>'</span><span>This is my article</span><span>'</span><span>,</span></span>
<span data-line=""><span>    comments: </span><span>[</span></span>
<span data-line=""><span>      '</span><span>First comment</span><span>'</span><span>,</span></span>
<span data-line=""><span>      '</span><span>Second comment</span><span>'</span></span>
<span data-line=""><span>      // (The rest of the comments are missing)</span></span>
<span data-line=""><span>    ]</span></span>
<span data-line=""><span>  }</span></span>
<span data-line=""><span>  // (The footer property is missing)</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>However, this isn’t too great either.</p>
<p>One downside of this approach is that the objects are kind of malformed. For example, the top-level object was supposed to have three properties (<code>header</code>, <code>post</code>, and <code>footer</code>), but the <code>footer</code> is missing because it hasn’t appeared in the stream yet. The <code>post</code> was supposed to have three <code>comments</code>, but you <em>can’t actually tell</em> whether more <code>comments</code> are coming or if this was the last one.</p>
<p>In a way, this is inherent to streaming—didn’t we <em>want</em> to get incomplete data?—but <strong>this makes it very difficult to actually <em>use</em> this data on the client.</strong> None of the types “match up” due to missing fields. We don’t know what’s complete and what’s not. That’s why streaming JSON isn’t popular aside from niche use cases. It’s just too hard to actually take advantage of it in the application logic which generally assumes the types are correct, “ready” means “complete”, and so on.</p>
<p>In the analogy with JPEG, this naïve approach to streaming matches the default “top-down” loading mechanism. The picture you see is crisp but you only see the top 10%. So despite the high fidelity, you don’t actually see <em>what’s</em> on the picture.</p>
<p>Curiously, this is also how streaming <em>HTML itself</em> works by default. If you load an HTML page on a slow connection, it will be streamed in the document order:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>&lt;</span><span>html</span><span>&gt;</span></span>
<span data-line=""><span>  &lt;</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;</span><span>header</span><span>&gt;</span><span>Welcome to my blog</span><span>&lt;/</span><span>header</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;</span><span>article</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>p</span><span>&gt;</span><span>This is my article</span><span>&lt;/</span><span>p</span><span>&gt;</span></span>
<span data-line=""><span>        &lt;</span><span>ul</span><span> class=</span><span>"</span><span>comments</span><span>"</span><span>&gt;</span></span>
<span data-line=""><span>          &lt;</span><span>li</span><span>&gt;</span><span>First comment</span><span>&lt;/</span><span>li</span><span>&gt;</span></span>
<span data-line=""><span>          &lt;</span><span>li</span><span>&gt;</span><span>Second comment</span><span>&lt;/</span><span>li</span><span>&gt;</span></span></code></pre></figure>
<p>This has some upsides—the browser is able to display the page partially—but it has the same issues. The cutoff point is arbitrary and can be visually jarring or even mess up the page layout. It’s unclear if more content is coming. Whatever’s below—like the footer—is cut off, even if it <em>was</em> ready on the server and <em>could</em> have been sent earlier. When we stream data <em>in order</em>, <strong>one slow part delays <em>everything</em>.</strong></p>
<p>Let’s repeat that: when we stream things in order they appear, a <em>single</em> slow part delays <em>everything</em> that comes after it. Can you think of some way to fix this?</p>
<hr>
<h3 id="progressive-json"><a target="_self" href="#progressive-json">Progressive JSON</a></h3>
<p>There is another way to approach streaming.</p>
<p>So far we’ve been sending things <em>depth-first</em>. We start with the top-level object’s properties, we go into that object’s <code>post</code> property, then we go into <em>that</em> object’s <code>comments</code> property, and so on. If something is slow, everything else gets held up.</p>
<p>However, we could also send data <em>breadth-first</em>.</p>
<p>Suppose we send the top-level object like this:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>$1</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>"</span><span>$2</span><span>"</span><span>,</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>$3</span><span>"</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>Here, <code>"$1"</code>, <code>"$2"</code>, <code>"$3"</code> refer to pieces of information that <em>have not been sent yet</em>. These are placeholders that can progressively be filled in later in the stream.</p>
<p>For example, suppose the server sends a few more rows of data to the stream:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>$1</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>"</span><span>$2</span><span>"</span><span>,</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>$3</span><span>"</span></span>
<span data-line=""><span>}</span></span>
<span data-line="" data-highlighted-line=""><span>/* $1 */</span></span>
<span data-line="" data-highlighted-line=""><span>"</span><span>Welcome to my blog</span><span>"</span></span>
<span data-line="" data-highlighted-line=""><span>/* $3 */</span></span>
<span data-line="" data-highlighted-line=""><span>"</span><span>Hope you like it</span><span>"</span></span></code></pre></figure>
<p>Notice that we’re not obligated to send the rows in any particular order. In the above example, we’ve just sent both <code>$1</code> and <code>$3</code>—but the <code>$2</code> row is still pending!</p>
<p>If the client tried to reconstruct the tree at this point, it could look like this:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>Welcome to my blog</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>new</span><span> Promise</span><span>(</span><span>/* ... not yet resolved ... */</span><span>),</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>We’ll represent the parts that haven’t loaded yet as Promises.</p>
<p>Then suppose the server could stream in a few more rows:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>$1</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>"</span><span>$2</span><span>"</span><span>,</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>$3</span><span>"</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""><span>/* $1 */</span></span>
<span data-line=""><span>"</span><span>Welcome to my blog</span><span>"</span></span>
<span data-line=""><span>/* $3 */</span></span>
<span data-line=""><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line="" data-highlighted-line=""><span>/* $2 */</span></span>
<span data-line="" data-highlighted-line=""><span>{</span></span>
<span data-line="" data-highlighted-line=""><span>  content: </span><span>"</span><span>$4</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>  comments: </span><span>"</span><span>$5</span><span>"</span></span>
<span data-line="" data-highlighted-line=""><span>}</span></span>
<span data-line="" data-highlighted-line=""><span>/* $4 */</span></span>
<span data-line="" data-highlighted-line=""><span>"</span><span>This is my article</span><span>"</span></span></code></pre></figure>
<p>This would “fill in” some of the missing pieces from the client’s perspective:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>Welcome to my blog</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>  post: </span><span>{</span></span>
<span data-line="" data-highlighted-line=""><span>    content: </span><span>"</span><span>This is my article</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>    comments: </span><span>new</span><span> Promise</span><span>(</span><span>/* ... not yet resolved ... */</span><span>),</span></span>
<span data-line="" data-highlighted-line=""><span>  },</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>The Promise for the <code>post</code> would now resolve to an object. However, we still don’t know what’s inside the <code>comments</code>, so now <em>those</em> are represented as a Promise.</p>
<p>Finally, the comments could stream in:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>$1</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>"</span><span>$2</span><span>"</span><span>,</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>$3</span><span>"</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""><span>/* $1 */</span></span>
<span data-line=""><span>"</span><span>Welcome to my blog</span><span>"</span></span>
<span data-line=""><span>/* $3 */</span></span>
<span data-line=""><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>/* $2 */</span></span>
<span data-line=""><span>{</span></span>
<span data-line=""><span>  content: </span><span>"</span><span>$4</span><span>"</span><span>,</span></span>
<span data-line=""><span>  comments: </span><span>"</span><span>$5</span><span>"</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""><span>/* $4 */</span></span>
<span data-line=""><span>"</span><span>This is my article</span><span>"</span></span>
<span data-line="" data-highlighted-line=""><span>/* $5 */</span></span>
<span data-line="" data-highlighted-line=""><span>[</span><span>"</span><span>$6</span><span>"</span><span>,</span><span> "</span><span>$7</span><span>"</span><span>,</span><span> "</span><span>$8</span><span>"</span><span>]</span></span>
<span data-line="" data-highlighted-line=""><span>/* $6 */</span></span>
<span data-line="" data-highlighted-line=""><span>"</span><span>This is the first comment</span><span>"</span></span>
<span data-line="" data-highlighted-line=""><span>/* $7 */</span></span>
<span data-line="" data-highlighted-line=""><span>"</span><span>This is the second comment</span><span>"</span></span>
<span data-line="" data-highlighted-line=""><span>/* $8 */</span></span>
<span data-line="" data-highlighted-line=""><span>"</span><span>This is the third comment</span><span>"</span></span></code></pre></figure>
<p>Now, from the client’s perspective, the entire tree would be complete:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>Welcome to my blog</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>{</span></span>
<span data-line=""><span>    content: </span><span>"</span><span>This is my article</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>    comments: </span><span>[</span></span>
<span data-line="" data-highlighted-line=""><span>      "</span><span>This is the first comment</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>      "</span><span>This is the second comment</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>      "</span><span>This is the third comment</span><span>"</span></span>
<span data-line="" data-highlighted-line=""><span>    ]</span></span>
<span data-line=""><span>  },</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>By sending data breadth-first in chunks, we gained the ability to progressively handle it on the client. As long as the client can deal with some parts being “not ready” (represented as Promises) and process the rest, this is an improvement!</p>
<hr>
<h3 id="inlining"><a target="_self" href="#inlining">Inlining</a></h3>
<p>Now that we have the basic mechanism, we’ll adjust it for more efficient output. Let’s have another look at the entire streaming sequence from the last example:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>$1</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>"</span><span>$2</span><span>"</span><span>,</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>$3</span><span>"</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""><span>/* $1 */</span></span>
<span data-line=""><span>"</span><span>Welcome to my blog</span><span>"</span></span>
<span data-line=""><span>/* $3 */</span></span>
<span data-line=""><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>/* $2 */</span></span>
<span data-line=""><span>{</span></span>
<span data-line=""><span>  content: </span><span>"</span><span>$4</span><span>"</span><span>,</span></span>
<span data-line=""><span>  comments: </span><span>"</span><span>$5</span><span>"</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""><span>/* $4 */</span></span>
<span data-line=""><span>"</span><span>This is my article</span><span>"</span></span>
<span data-line=""><span>/* $5 */</span></span>
<span data-line=""><span>[</span><span>"</span><span>$6</span><span>"</span><span>,</span><span> "</span><span>$7</span><span>"</span><span>,</span><span> "</span><span>$8</span><span>"</span><span>]</span></span>
<span data-line=""><span>/* $6 */</span></span>
<span data-line=""><span>"</span><span>This is the first comment</span><span>"</span></span>
<span data-line=""><span>/* $7 */</span></span>
<span data-line=""><span>"</span><span>This is the second comment</span><span>"</span></span>
<span data-line=""><span>/* $8 */</span></span>
<span data-line=""><span>"</span><span>This is the third comment</span><span>"</span></span></code></pre></figure>
<p>We may have gone a <em>little</em> too far with streaming here. Unless generating some parts actually <em>is</em> slow, we don’t gain anything from sending them as separate rows.</p>
<p>Suppose that we have two different slow operations: loading a post and loading a post’s comments. In that case, it would make sense to send three chunks in total.</p>
<p>First, we would send the outer shell:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>Welcome to my blog</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>"</span><span>$1</span><span>"</span><span>,</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>On the client, this would immediately become:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>Welcome to my blog</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>new</span><span> Promise</span><span>(</span><span>/* ... not yet resolved ... */</span><span>),</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>Then we’d send the <code>post</code> data (but without the <code>comments</code>):</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>Welcome to my blog</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>"</span><span>$1</span><span>"</span><span>,</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>}</span></span>
<span data-line="" data-highlighted-line=""><span>/* $1 */</span></span>
<span data-line="" data-highlighted-line=""><span>{</span></span>
<span data-line="" data-highlighted-line=""><span>  content: </span><span>"</span><span>This is my article</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>  comments: </span><span>"</span><span>$2</span><span>"</span></span>
<span data-line="" data-highlighted-line=""><span>}</span></span></code></pre></figure>
<p>From the client’s perspective:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>Welcome to my blog</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>  post: </span><span>{</span></span>
<span data-line="" data-highlighted-line=""><span>    content: </span><span>"</span><span>This is my article</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>    comments: </span><span>new</span><span> Promise</span><span>(</span><span>/* ... not yet resolved ... */</span><span>),</span></span>
<span data-line="" data-highlighted-line=""><span>  },</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>Finally, we’d send the comments in a single chunk:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>Welcome to my blog</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>"</span><span>$1</span><span>"</span><span>,</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""><span>/* $1 */</span></span>
<span data-line=""><span>{</span></span>
<span data-line=""><span>  content: </span><span>"</span><span>This is my article</span><span>"</span><span>,</span></span>
<span data-line=""><span>  comments: </span><span>"</span><span>$2</span><span>"</span></span>
<span data-line=""><span>}</span></span>
<span data-line="" data-highlighted-line=""><span>/* $2 */</span></span>
<span data-line="" data-highlighted-line=""><span>[</span></span>
<span data-line="" data-highlighted-line=""><span>  "</span><span>This is the first comment</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>  "</span><span>This is the second comment</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>  "</span><span>This is the third comment</span><span>"</span></span>
<span data-line="" data-highlighted-line=""><span>]</span></span></code></pre></figure>
<p>That would give us the whole tree on the client:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>Welcome to my blog</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>{</span></span>
<span data-line=""><span>    content: </span><span>"</span><span>This is my article</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>    comments: </span><span>[</span></span>
<span data-line="" data-highlighted-line=""><span>      "</span><span>This is the first comment</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>      "</span><span>This is the second comment</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>      "</span><span>This is the third comment</span><span>"</span></span>
<span data-line="" data-highlighted-line=""><span>    ]</span></span>
<span data-line=""><span>  },</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>This is more compact and achieves the same purpose.</p>
<p>In general, this format gives us leeway to decide when to send things as a single chunks vs. multiple chunks. As long as the client is resilient to chunks arriving out-of-order, the server can pick different batching and chunking heuristics.</p>
<hr>
<h3 id="outlining"><a target="_self" href="#outlining">Outlining</a></h3>
<p>One interesting consequence of this approach is that it <em>also</em> gives us a natural way to reduce repetition in the output stream. If we’re serializing an object we’ve already seen before, we can just outline it as a separate row, and reuse it.</p>
<p>For example, suppose we have an object tree like this:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>const </span><span>userInfo</span><span> =</span><span> {</span><span> name</span><span>:</span><span> '</span><span>Dan</span><span>'</span><span> };</span></span>
<span data-line=""> </span>
<span data-line=""><span>[</span></span>
<span data-line=""><span>  {</span><span> type</span><span>:</span><span> '</span><span>header</span><span>'</span><span>,</span><span> user</span><span>:</span><span> userInfo </span><span>},</span></span>
<span data-line=""><span>  {</span><span> type</span><span>:</span><span> '</span><span>sidebar</span><span>'</span><span>,</span><span> user</span><span>:</span><span> userInfo </span><span>},</span></span>
<span data-line=""><span>  {</span><span> type</span><span>:</span><span> '</span><span>footer</span><span>'</span><span>,</span><span> user</span><span>:</span><span> userInfo </span><span>}</span></span>
<span data-line=""><span>]</span></span></code></pre></figure>
<p>If we were to serialize it to plain JSON, we’d end up repeating <code>{ name: 'Dan' }</code>:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>[</span></span>
<span data-line=""><span>  {</span><span> type</span><span>:</span><span> '</span><span>header</span><span>'</span><span>,</span><span> user</span><span>:</span><span> {</span><span> name</span><span>:</span><span> '</span><span>Dan</span><span>'</span><span> }</span><span> },</span></span>
<span data-line=""><span>  {</span><span> type</span><span>:</span><span> '</span><span>sidebar</span><span>'</span><span>,</span><span> user</span><span>:</span><span> {</span><span> name</span><span>:</span><span> '</span><span>Dan</span><span>'</span><span> }</span><span> },</span></span>
<span data-line=""><span>  {</span><span> type</span><span>:</span><span> '</span><span>footer</span><span>'</span><span>,</span><span> user</span><span>:</span><span> {</span><span> name</span><span>:</span><span> '</span><span>Dan</span><span>'</span><span> }</span><span> }</span></span>
<span data-line=""><span>]</span></span></code></pre></figure>
<p>However, if we’re serving JSON progressively, we could choose to outline it:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>[</span></span>
<span data-line=""><span>  {</span><span> type</span><span>:</span><span> '</span><span>header</span><span>'</span><span>,</span><span> user</span><span>:</span><span> "</span><span>$1</span><span>"</span><span> },</span></span>
<span data-line=""><span>  {</span><span> type</span><span>:</span><span> '</span><span>sidebar</span><span>'</span><span>,</span><span> user</span><span>:</span><span> "</span><span>$1</span><span>"</span><span> },</span></span>
<span data-line=""><span>  {</span><span> type</span><span>:</span><span> '</span><span>footer</span><span>'</span><span>,</span><span> user</span><span>:</span><span> "</span><span>$1</span><span>"</span><span> }</span></span>
<span data-line=""><span>]</span></span>
<span data-line="" data-highlighted-line=""><span>/* $1 */</span></span>
<span data-line="" data-highlighted-line=""><span>{</span><span> name: </span><span>"</span><span>Dan</span><span>"</span><span> }</span></span></code></pre></figure>
<p>We could also pursue a more balanced strategy—for example, to inline objects by default (for compactness) until we see some object being used two or more times, at which point we’ll emit it separately and dedupe the rest of them in the stream.</p>
<p>This also means that, unlike with plain JSON, we can support serializing cyclic objects. A cyclic object just has a property that points to its own stream “row”.</p>
<hr>
<h3 id="streaming-data-vs-streaming-ui"><a target="_self" href="#streaming-data-vs-streaming-ui">Streaming Data vs Streaming UI</a></h3>
<p>The approach described above is essentially how React Server Components work.</p>
<p>Suppose you write a page with React Server Components:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>function</span><span> Page</span><span>()</span><span> {</span></span>
<span data-line=""><span>  return</span><span> (</span></span>
<span data-line=""><span>    &lt;</span><span>html</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>        &lt;</span><span>header</span><span>&gt;</span><span>Welcome to my blog</span><span>&lt;/</span><span>header</span><span>&gt;</span></span>
<span data-line=""><span>        &lt;</span><span>Post</span><span> /&gt;</span></span>
<span data-line=""><span>        &lt;</span><span>footer</span><span>&gt;</span><span>Hope you like it</span><span>&lt;/</span><span>footer</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;/</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;/</span><span>html</span><span>&gt;</span></span>
<span data-line=""><span>  );</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>async</span><span> function</span><span> Post</span><span>()</span><span> {</span></span>
<span data-line=""><span>  const </span><span>post</span><span> =</span><span> await </span><span>loadPost</span><span>();</span></span>
<span data-line=""><span>  return</span><span> (</span></span>
<span data-line=""><span>    &lt;</span><span>article</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>p</span><span>&gt;{</span><span>post</span><span>.</span><span>text</span><span>}&lt;/</span><span>p</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>Comments</span><span> /&gt;</span></span>
<span data-line=""><span>    &lt;/</span><span>article</span><span>&gt;</span></span>
<span data-line=""><span>  );</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>async</span><span> function</span><span> Comments</span><span>()</span><span> {</span></span>
<span data-line=""><span>  const </span><span>comments</span><span> =</span><span> await </span><span>loadComments</span><span>();</span></span>
<span data-line=""><span>  return</span><span> &lt;</span><span>ul</span><span>&gt;{</span><span>comments</span><span>.</span><span>map</span><span>(</span><span>c </span><span>=&gt;</span><span> &lt;</span><span>li</span><span> key={</span><span>c</span><span>.</span><span>id</span><span>}&gt;{</span><span>c</span><span>.</span><span>text</span><span>}&lt;/</span><span>li</span><span>&gt;)}&lt;/</span><span>ul</span><span>&gt;;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>React will serve the output of the <code>Page</code> as a progressive JSON stream. On the client, it will be reconstructed as a progressively loaded React tree.</p>
<p>Initially, the React tree on the client will appear like this:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>&lt;</span><span>html</span><span>&gt;</span></span>
<span data-line=""><span>  &lt;</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;</span><span>header</span><span>&gt;</span><span>Welcome to my blog</span><span>&lt;/</span><span>header</span><span>&gt;</span></span>
<span data-line=""><span>    {new</span><span> Promise</span><span>(</span><span>/* ... not resolved yet */</span><span>)}</span></span>
<span data-line=""><span>    &lt;</span><span>footer</span><span>&gt;</span><span>Hope you like it</span><span>&lt;/</span><span>footer</span><span>&gt;</span></span>
<span data-line=""><span>  &lt;/</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>&lt;/</span><span>html</span><span>&gt;</span></span></code></pre></figure>
<p>Then, as <code>loadPost</code> resolves on the server, more will stream in:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>&lt;</span><span>html</span><span>&gt;</span></span>
<span data-line=""><span>  &lt;</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;</span><span>header</span><span>&gt;</span><span>Welcome to my blog</span><span>&lt;/</span><span>header</span><span>&gt;</span></span>
<span data-line="" data-highlighted-line=""><span>    &lt;</span><span>article</span><span>&gt;</span></span>
<span data-line="" data-highlighted-line=""><span>      &lt;</span><span>p</span><span>&gt;</span><span>This is my post</span><span>&lt;/</span><span>p</span><span>&gt;</span></span>
<span data-line="" data-highlighted-line=""><span>      {new</span><span> Promise</span><span>(</span><span>/* ... not resolved yet */</span><span>)}</span></span>
<span data-line="" data-highlighted-line=""><span>    &lt;/</span><span>article</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;</span><span>footer</span><span>&gt;</span><span>Hope you like it</span><span>&lt;/</span><span>footer</span><span>&gt;</span></span>
<span data-line=""><span>  &lt;/</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>&lt;/</span><span>html</span><span>&gt;</span></span></code></pre></figure>
<p>Finally, when <code>loadComments</code> resolves on the server, the client receives the rest:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>&lt;</span><span>html</span><span>&gt;</span></span>
<span data-line=""><span>  &lt;</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;</span><span>header</span><span>&gt;</span><span>Welcome to my blog</span><span>&lt;/</span><span>header</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;</span><span>article</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>p</span><span>&gt;</span><span>This is my post</span><span>&lt;/</span><span>p</span><span>&gt;</span></span>
<span data-line="" data-highlighted-line=""><span>      &lt;</span><span>ul</span><span>&gt;</span></span>
<span data-line="" data-highlighted-line=""><span>        &lt;</span><span>li</span><span> key=</span><span>"</span><span>1</span><span>"</span><span>&gt;</span><span>This is the first comment</span><span>&lt;/</span><span>li</span><span>&gt;</span></span>
<span data-line="" data-highlighted-line=""><span>        &lt;</span><span>li</span><span> key=</span><span>"</span><span>2</span><span>"</span><span>&gt;</span><span>This is the second comment</span><span>&lt;/</span><span>li</span><span>&gt;</span></span>
<span data-line="" data-highlighted-line=""><span>        &lt;</span><span>li</span><span> key=</span><span>"</span><span>3</span><span>"</span><span>&gt;</span><span>This is the third comment</span><span>&lt;/</span><span>li</span><span>&gt;</span></span>
<span data-line="" data-highlighted-line=""><span>      &lt;/</span><span>ul</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;/</span><span>article</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;</span><span>footer</span><span>&gt;</span><span>Hope you like it</span><span>&lt;/</span><span>footer</span><span>&gt;</span></span>
<span data-line=""><span>  &lt;/</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>&lt;/</span><span>html</span><span>&gt;</span></span></code></pre></figure>
<p>However, here’s the kicker.</p>
<p>You don’t actually <em>want</em> the page to jump arbitrarily as the data streams in. For example, maybe you never want to show the page <em>without</em> the post’s content.</p>
<p><strong>This is why React doesn’t display “holes” for pending Promises. Instead, it displays the closest declarative loading state, indicated by <a target="_blank" href="https://react.dev/reference/react/Suspense"><code>&lt;Suspense&gt;</code></a>.</strong></p>
<p>In the above example, there are no <code>&lt;Suspense&gt;</code> boundaries in the tree. This means that, although React will receive the <em>data</em> as a stream, it will not actually display a “jumping” page to the user. It will wait for the <em>entire</em> page to be ready.</p>
<p>However, you can <em>opt into</em> a progressively revealed loading state by wrapping a part of the UI tree into <code>&lt;Suspense&gt;</code>. This doesn’t change how the data is sent (it’s still as “streaming” as possible), but it changes <em>when</em> React reveals it to the user.</p>
<p>For example:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line="" data-highlighted-line=""><span>import</span><span> {</span><span> Suspense </span><span>}</span><span> from</span><span> '</span><span>react</span><span>'</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>function</span><span> Page</span><span>()</span><span> {</span></span>
<span data-line=""><span>  return</span><span> (</span></span>
<span data-line=""><span>    &lt;</span><span>html</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>        &lt;</span><span>header</span><span>&gt;</span><span>Welcome to my blog</span><span>&lt;/</span><span>header</span><span>&gt;</span></span>
<span data-line=""><span>        &lt;</span><span>Post</span><span> /&gt;</span></span>
<span data-line=""><span>        &lt;</span><span>footer</span><span>&gt;</span><span>Hope you like it</span><span>&lt;/</span><span>footer</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;/</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;/</span><span>html</span><span>&gt;</span></span>
<span data-line=""><span>  );</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>async</span><span> function</span><span> Post</span><span>()</span><span> {</span></span>
<span data-line=""><span>  const </span><span>post</span><span> =</span><span> await </span><span>loadPost</span><span>();</span></span>
<span data-line=""><span>  return</span><span> (</span></span>
<span data-line=""><span>    &lt;</span><span>article</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>p</span><span>&gt;{</span><span>post</span><span>.</span><span>text</span><span>}&lt;/</span><span>p</span><span>&gt;</span></span>
<span data-line="" data-highlighted-line=""><span>      &lt;</span><span>Suspense</span><span> fallback={&lt;</span><span>CommentsGlimmer</span><span> /&gt;}&gt;</span></span>
<span data-line=""><span>        &lt;</span><span>Comments</span><span> /&gt;</span></span>
<span data-line="" data-highlighted-line=""><span>      &lt;/</span><span>Suspense</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;/</span><span>article</span><span>&gt;</span></span>
<span data-line=""><span>  );</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>async</span><span> function</span><span> Comments</span><span>()</span><span> {</span></span>
<span data-line=""><span>  const </span><span>comments</span><span> =</span><span> await </span><span>loadComments</span><span>();</span></span>
<span data-line=""><span>  return</span><span> &lt;</span><span>ul</span><span>&gt;{</span><span>comments</span><span>.</span><span>map</span><span>(</span><span>c </span><span>=&gt;</span><span> &lt;</span><span>li</span><span> key={</span><span>c</span><span>.</span><span>id</span><span>}&gt;{</span><span>c</span><span>.</span><span>text</span><span>}&lt;/</span><span>li</span><span>&gt;)}&lt;/</span><span>ul</span><span>&gt;;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>Now the user will perceive the loading sequence in two stages:</p>
<ul>
<li>First, the post “pops in” together with the header, the footer, and a glimmer for comments. The header and the footer never appear on their own.</li>
<li>Then, the comments “pop in” on their own.</li>
</ul>
<p><strong>In other words, the stages in which the UI gets revealed are decoupled from how the data arrives. The data is streamed as it becomes available, but we only want to <em>reveal</em> things to the user according to intentionally designed loading states.</strong></p>
<p>In a way, you can see those Promises in the React tree acting almost like a <code>throw</code>, while <code>&lt;Suspense&gt;</code> acts almost like a <code>catch</code>. The data arrives as fast as it can in whatever order the server is ready to send it, but React takes care to present the loading sequence gracefully and let the developer control the visual reveal.</p>
<p>Note that what I described so far has nothing to do with “SSR” or HTML. I was describing a general mechanism for streaming a UI tree represented as JSON. You can <em>turn</em> that JSON tree into progressively revealed HTML (and <a target="_blank" href="https://gal.hagever.com/posts/out-of-order-streaming-from-scratch">React can do that</a>), but the idea is broader than HTML and applies to SPA-like navigations as well.</p>
<hr>
<h3 id="in-conclusion"><a target="_self" href="#in-conclusion">In Conclusion</a></h3>
<p>In this post, I’ve sketched out one of the core innovations of RSC. Instead of sending data as a single big chunk, it sends the props for your component tree outside-in. As a result, as soon as there’s an intentional loading state to display, React can do that while the rest of the data for your page is being streamed in.</p>
<p>I’d like to challenge more tools to adopt progressive streaming of data. If you have a situation where you can’t <em>start</em> doing something on the client until the server <em>stops</em> doing something, that’s a clear example of where streaming can help. If a <em>single slow thing</em> can slow down <em>everything after it,</em> that’s another warning sign.</p>
<p>Like I showed in this post, streaming <em>alone</em> is not enough—you also need a programming model that can <em>take advantage</em> of streaming and gracefully handle incomplete information. React solves that with intentional <code>&lt;Suspense&gt;</code> loading states. If you know systems that solve this differently, I’d love to hear about them!</p><p><a href="https://ko-fi.com/gaearon" target="_blank"><span></span>Pay what you like</a></p><hr><p><a target="_blank" href="https://bsky.app/search?q=https%3A%2F%2Foverreacted.io%2Fprogressive-json%2F">Discuss on Bluesky</a>&nbsp;&nbsp;·&nbsp;&nbsp;<a target="_blank" href="https://github.com/gaearon/overreacted.io/edit/main/public/progressive-json/index.md">Edit on GitHub</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Patio – Rent tools, learn DIY, reduce waste (158 pts)]]></title>
            <link>https://patio.so</link>
            <guid>44147803</guid>
            <pubDate>Sun, 01 Jun 2025 00:17:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://patio.so">https://patio.so</a>, See on <a href="https://news.ycombinator.com/item?id=44147803">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The NFS 4 Freezer Spacer In Science Fiction Sets (101 pts)]]></title>
            <link>https://kolektiva.social/@beka_valentine/114600567753999701</link>
            <guid>44147631</guid>
            <pubDate>Sat, 31 May 2025 23:23:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kolektiva.social/@beka_valentine/114600567753999701">https://kolektiva.social/@beka_valentine/114600567753999701</a>, See on <a href="https://news.ycombinator.com/item?id=44147631">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[New adaptive optics shows details of our star's atmosphere (127 pts)]]></title>
            <link>https://nso.edu/press-release/new-adaptive-optics-shows-stunning-details-of-our-stars-atmosphere/</link>
            <guid>44147573</guid>
            <pubDate>Sat, 31 May 2025 23:08:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nso.edu/press-release/new-adaptive-optics-shows-stunning-details-of-our-stars-atmosphere/">https://nso.edu/press-release/new-adaptive-optics-shows-stunning-details-of-our-stars-atmosphere/</a>, See on <a href="https://news.ycombinator.com/item?id=44147573">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2><i>Scientists develop new optical system that removes blur over fine-structure in the Sun’s corona, revealing clearest images to date</i></h2>
<p><b>BOULDER, CO, Tuesday, May 27, 2025 </b><b><i>– </i></b>The Sun’s corona—the outermost layer of its atmosphere, visible only during a total solar eclipse—has long intrigued scientists due to its extreme temperatures, violent eruptions, and large prominences. However, turbulence in the Earth’s atmosphere has caused image blur and hindered observations of the corona. A ground-breaking recent development by scientists from the U.S. National Science Foundation (NSF) National Solar Observatory (NSO), and New Jersey Institute of Technology (NJIT), is changing that by using adaptive optics to remove the blur.</p>
<p>As published in <a href="https://www.nature.com/articles/s41550-025-02564-0" target="_blank" rel="noopener"><i>Nature Astronomy</i></a>, this pioneering ‘coronal adaptive optics’ technology has produced the most astonishing, clearest images and videos of fine-structure in the corona to date. This development will open the door for deeper insights into the corona’s enigmatic behavior and the processes driving space weather.</p>
<h3><b>Most Detailed Coronal Images to Date Revealed</b><b></b></h3>
<p>Funded by the NSF and installed at the 1.6-meter Goode Solar Telescope (GST), operated by NJIT’s Center for Solar-Terrestrial Research (CSTR) at Big Bear Solar Observatory (BBSO) in California, “Cona”—the adaptive optics system responsible for these new images—compensates for the blur caused by air turbulence in the Earth’s atmosphere —similar to the bumpy air passengers feel during a flight.</p>
<p><i>“The turbulence in the air severely degrades images of objects in space, like our Sun, seen through our telescopes. But we can correct for that,”</i> says Dirk Schmidt, NSO Adaptive Optics Scientist who led the development.</p>
<p>Among the team’s remarkable observations is a movie of a quickly restructuring solar prominence unveiling fine, turbulent internal flows. Solar prominences are large, bright features, often appearing as arches or loops, extending outward from the Sun’s surface.</p>
<p><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->
<video id="video-1392942-1" width="1080" height="1080" preload="metadata" controls="controls"><source type="video/mp4" src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Video.mp4?_=1"><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Video.mp4">https://nso.edu/wp-content/uploads/2025/05/Prominence-1-Video.mp4</a></video></p>
<p>This image of a prominence above the solar surface is a snapshot of a 4-minute time-lapse movie that reveals its rapid, fine, and turbulent restructuring with unprecedented detail. The Sun’s fluffy-looking surface is covered by “spicules”, short-lived plasma jets, whose creation is still the subject of scientific debate. The streaks on the right of this image are coronal rain falling down onto the Sun’s surface. This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light. Credit: Schmidt et al./NJIT/NSO/AURA/NSF</p>
<hr>

<p>A second movie replays the rapid formation and collapse of a finely structured plasma stream. <i>“These are by far the most detailed observations of this kind, showing features not previously observed, and it’s not quite clear what they are,”</i> says Vasyl Yurchyshyn, co-author of the study and NJIT-CSTR research professor. “<i>It is super exciting to build an instrument that shows us the Sun like never before,</i>” Schmidt adds.</p>
<p><video id="video-1392942-2" width="1080" height="739" preload="metadata" controls="controls"><source type="video/mp4" src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid-Video.mp4?_=2"><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid-Video.mp4">https://nso.edu/wp-content/uploads/2025/05/Plasmoid-Video.mp4</a></video></p>
<p>This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light.<br>
<b>Credit:</b> Schmidt et al./NJIT/NSO/AURA/NSF</p>
<hr>

<p>A third movie shows fine strands of coronal rain—a phenomenon where cooling plasma condenses and falls back toward the Sun’s surface. “<i>Raindrops in the Sun’s corona can be narrower than 20 kilometers,” </i>NSO Astronomer Thomas Schad concludes from the most detailed images of coronal rain to date, “<i>These findings offer new invaluable observational insight that is vital to test computer models of coronal processes.</i>”</p>
<p><video id="video-1392942-3" width="1080" height="1080" preload="metadata" controls="controls"><source type="video/mp4" src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain-Video.mp4?_=3"><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain-Video.mp4">https://nso.edu/wp-content/uploads/2025/05/Coronal-Rain-Video.mp4</a></video></p>
<p>Coronal rain forms when hotter plasma in the Sun’s corona cools down and becomes denser. Like raindrops on Earth, coronal rain is pulled down to the surface by gravity. Because the plasma is electrically charged, it follows the magnetic field lines, which make huge arches/loops, instead of falling in a straight line.</p>
<p>This image is a snapshot from a 23-minute time-lapse video which is comprised of the highest resolution images ever made of coronal rain. The scientists show in the paper that the strands can be narrower than 20 kilometers.</p>
<p>This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light.<br>
<b>Credit: </b>Schmidt et al./NJIT/NSO/AURA/NSF</p>
<hr>
<p>Another movie shows the dramatic motion of a solar prominence being shaped by the Sun’s magnetism.</p>
<p><video id="video-1392942-4" width="1080" height="1080" preload="metadata" controls="controls"><source type="video/mp4" src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2-Video.mp4?_=4"><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2-Video.mp4">https://nso.edu/wp-content/uploads/2025/05/Prominence-2-Video.mp4</a></video></p>
<p>This image of a solar prominence is a snapshot of a 19-minute time-lapse movie showing how plasma “dances” and twists with the Sun’s magnetic field.</p>
<p>This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light.<br>
<b>Credit:</b> Schmidt et al./NJIT/NSO/AURA/NSF</p>
<hr>
<h3><b>A Breakthrough in Solar Adaptive Optics</b><b></b></h3>
<p>The corona is heated to millions of degrees–much hotter than the Sun’s surface–by mechanisms unknown to scientists. It is also home to dynamic phenomena of much cooler solar plasma that appears reddish-pink during eclipses. Scientists believe that resolving the structure and dynamics of the cooler plasma at small scales holds a key to answering the coronal heating mystery and improving our understanding of eruptions that eject plasma into space driving space weather—i.e., the conditions in Earth’s near-space environment primarily influenced by the Sun’s activity (e.g.,&nbsp;solar flares,&nbsp;coronal mass ejections,&nbsp;and the solar wind) that can impact technology and systems on Earth and in space. The precision required demands large telescopes and adaptive optics systems like the one developed by this team.</p>
<p>The GST system Cona uses a mirror that continuously reshapes itself 2,200 times per second to counteract the image degradation caused by turbulent air. “<i>Adaptive optics is like a pumped-up autofocus and optical image stabilization in your smartphone camera, but correcting for the errors in the atmosphere rather than the user’s shaky hands</i>,” says BBSO Optical Engineer and Chief Observer, Nicolas Gorceix.</p>
<p>Since the early 2000s, adaptive optics have been used in large solar telescopes to restore images of the Sun’s surface to their full potential, enabling telescopes to reach their theoretical diffraction limits—i.e., the theoretical maximum resolution of an optical system. These systems have since revolutionized observing the Sun’s surface, but until now, have not been useful for observations in the corona; and the resolution of features beyond the solar limb stagnated at an order of 1,000 kilometers or worse—levels achieved 80 years ago.</p>
<p>“<i>The new coronal adaptive optics system closes this decades-old gap and delivers images of coronal features at 63 kilometers resolution—the theoretical limit of the 1.6-meter Goode Solar Telescope,</i>” says Thomas Rimmele, NSO Chief Technologist who built the first operational adaptive optics for the Sun’s surface, and motivated the development.</p>
<h3><b>Implications for the Future</b><b><br>
</b></h3>
<p>Coronal adaptive optics is now available at the GST. <i>“This technological advancement is a game-changer, there is a lot to discover when you boost your resolution by a factor of 10</i>,” Schmidt says.</p>
<p>The team now knows how to overcome the resolution limit imposed by the Earth’s lowest region of the atmosphere—i.e., the troposphere—on observations beyond the solar limb and is working to apply the technology at the 4-meter NSF Daniel K. Inouye Solar Telescope, built and operated by the NSO in Maui, Hawaiʻi. The world’s largest solar telescope would see even smaller details in the Sun’s atmosphere.</p>
<p>“<i>This transformative technology, which is likely to be adopted at observatories world-wide, is poised to reshape ground-based solar astronomy,</i>” says Philip R. Goode, distinguished research professor of physics at NJIT-CSTR and former director at BBSO, who co-authored the study. “<i>With coronal adaptive optics now in operation, this marks the beginning of a new era in solar physics, promising many more discoveries in the years and decades to come.</i>”</p>
<p>The <a href="https://www.nature.com/articles/s41550-025-02564-0">paper</a> describing this study, titled<i> “Observations of fine coronal structures with high-order solar adaptive optics,”</i> is now available in <i>Nature Astronomy</i>.</p>
<p>The authors are: Dirk Schmidt (NSO), Thomas A. Schad (NSO), Vasyl Yurchyshyn (NJIT), Nicolas Gorceix (NJIT), Thomas R. Rimmele (NSO), and Philip R. Goode (NJIT).</p>
<p><iframe loading="lazy" title="“Raindrops in the Sun’s Corona”: New Adaptive Optics Shows Stunning Details of our Star’s Atmosphere" src="about:blank" width="1080" height="608" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media" data-rocket-lazyload="fitvidscompatible" data-lazy-src="https://player.vimeo.com/video/1088466060?h=12164cb284&amp;dnt=1&amp;app_id=122963"></iframe><br>
Credit: Dirk Schmidt</p>

<h3>About the U.S. NSF National Solar Observatory</h3>
<p>The mission of the NSF National Solar Observatory (NSO) is to advance knowledge of the Sun, both as an astronomical object and as the dominant external influence on Earth, by providing forefront observational opportunities to the research community.</p>
<p>NSO built and operates the world’s most extensive collection of ground-based optical and infrared solar telescopes and auxiliary instrumentation— including the NSF GONG network of six stations around the world, and the world’s largest solar telescope, the NSF Daniel K. Inouye Solar Telescope—allowing solar physicists to probe all aspects of the Sun, from the deep solar interior to the photosphere, chromosphere, the outer corona, and out into the interplanetary medium. These assets also provide data for heliospheric modeling, space weather forecasting, and stellar astrophysics research, putting our Sun in the context of other stars and their environments.</p>
<p>Besides the operation of cutting-edge facilities, the mission includes the continued development of advanced instrumentation both in-house and through partnerships, conducting solar research, and educational and public outreach. NSO is managed by the Association of Universities for Research in Astronomy, Inc. (AURA) under a cooperative agreement with NSF. For more information, visit <a href="https://nso.edu/">nso.edu</a>.</p>
<h3>About the Big Bear Solar Observatory</h3>
<p>The Center for Solar-Terrestrial Research (CSTR) at New Jersey Institute of Technology (NJIT) is an international leader in ground- and space-based solar and terrestrial physics, with interest in understanding the effects of the Sun on the geospace environment. CSTR operates, along with a number of other observatories, the Big Bear Solar Observatory (BBSO).</p>
<p>BBSO is located on the north side of Big Bear Lake in the San Bernardino Mountains of southwestern San Bernardino County, California, approximately 75 miles East of downtown Los Angeles. BBSO has a 1.6-meter clear-aperture Goode Solar Telescope (GST), which has no obscuration in the optical train. The telescopes and instruments at the observatory are designed and employed specifically for studying the activities and phenomena of the Sun. GST was the largest and highest-resolution solar telescope in the world for ten years, and is a leading facility for solar physics research.</p>
<p>The NSF National Solar Observatory and BBSO have collaborated for over two decades to develop and to advance adaptive optics technologies for solar observations. The GST has been a critical facility to develop and test prototypes for the 4-meter NSF Daniel K. Inouye Solar Telescope, which took over as the world’s largest solar telescope in 2022. GST’s first-ever coronal adaptive optics system Cona is the latest product of this successful and pioneering collaboration. For more information, visit <a href="http://bbso.njit.edu/">bbso.njit.edu</a>.</p>
<h3>Contact</h3>
<p>For media inquiries, please contact:</p>
<p>Evan Pascual<br>
Communications Specialist<br>
U.S. NSF National Solar Observatory<br>
<a href="mailto:media@nso.edu">media@nso.edu</a></p>
<p>For technical inquiries, please contact:</p>
<p>Dirk Schmidt<br>
U.S. NSF National Solar Observatory<br>
<a href="mailto:dschmidt@nso.edu">dschmidt@nso.edu</a></p></div><div>
				
				
				
				
				<div><h4>Related Images</h4>
<p><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Raindrops-in-the-Suns-Corona.zip" target="_blank">Download Complete Media Package</a></p><div id="attachment_1392975"><picture fetchpriority="high" decoding="async" aria-describedby="caption-attachment-1392975">
<source type="image/webp" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-300x300.jpg.webp 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-1024x1024.jpg.webp 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-150x150.jpg.webp 150w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-768x768.jpg.webp 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-1536x1536.jpg.webp 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-610x610.jpg.webp 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-1080x1080.jpg.webp 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-440x440.jpg.webp 440w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized.jpg.webp 1786w" sizes="(max-width: 300px) 100vw, 300px">
<img fetchpriority="high" decoding="async" aria-describedby="caption-attachment-1392975" src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-300x300.jpg" alt="This image of a prominence above the solar surface is a snapshot of a 4-minute time-lapse movie that reveals its rapid, fine, and turbulent restructuring with unprecedented detail. The Sun’s fluffy-looking surface is covered by “spicules”, short-lived plasma jets, whose creation is still the subject of scientific debate. The streaks on the right of this image are coronal rain falling down onto the Sun’s surface. This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light. Credit: Schmidt et al./NJIT/NSO/AURA/NSF" width="300" height="300" srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-300x300.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-1024x1024.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-150x150.jpg 150w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-768x768.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-1536x1536.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-610x610.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-1080x1080.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-440x440.jpg 440w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized.jpg 1786w" sizes="(max-width: 300px) 100vw, 300px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-300x300.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-1024x1024.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-150x150.jpg 150w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-768x768.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-1536x1536.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-610x610.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-1080x1080.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-440x440.jpg 440w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized.jpg 1786w" data-lazy-src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-300x300.jpg">
</picture>
<p id="caption-attachment-1392975">This image of a prominence above the solar surface is a snapshot of a 4-minute time-lapse movie that reveals its rapid, fine, and turbulent restructuring with unprecedented detail. The Sun’s fluffy-looking surface is covered by “spicules”, short-lived plasma jets, whose creation is still the subject of scientific debate. The streaks on the right of this image are coronal rain falling down onto the Sun’s surface. This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light. Credit: Schmidt et al./NJIT/NSO/AURA/NSF</p></div>
<ul>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-1.png" target="_blank">Download Hi-res Version</a> (Right-mouse click and “Save Image As.”)</li>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2-Scale.png" target="_blank">Download Hi-res Version with Scale</a> (Right-mouse click and “Save Image As.”)</li>
</ul>

<div id="attachment_1392972"><picture decoding="async" aria-describedby="caption-attachment-1392972">
<source type="image/webp" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-300x205.jpg.webp 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-1024x700.jpg.webp 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-768x525.jpg.webp 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-1536x1050.jpg.webp 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-610x417.jpg.webp 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-1080x738.jpg.webp 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized.jpg.webp 1786w" sizes="(max-width: 300px) 100vw, 300px">
<img decoding="async" aria-describedby="caption-attachment-1392972" src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-300x205.jpg" alt="This image is a snapshot from a 16-minute time-lapse movie showing the formation and collapse of a complexly shaped plasma stream traveling at almost 100 kilometers per seconds in front of a coronal loop system. This is likely the first time such a stream, which the scientists refer to as “plasmoid”, has been observed, leaving them wondering about the physical explanation of the observed dynamics. This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light. Credit: Schmidt et al./NJIT/NSO/AURA/NSF" width="300" height="205" srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-300x205.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-1024x700.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-768x525.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-1536x1050.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-610x417.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-1080x738.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized.jpg 1786w" sizes="(max-width: 300px) 100vw, 300px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20205'%3E%3C/svg%3E" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-300x205.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-1024x700.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-768x525.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-1536x1050.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-610x417.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-1080x738.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized.jpg 1786w" data-lazy-src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-300x205.jpg">
</picture>
<p id="caption-attachment-1392972">This image is a snapshot from a 16-minute time-lapse movie showing the formation and collapse of a complexly shaped plasma stream traveling at almost 100 kilometers per seconds in front of a coronal loop system. This is likely the first time such a stream, which the scientists refer to as “plasmoid”, has been observed, leaving them wondering about the physical explanation of the observed dynamics. This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light. Credit: Schmidt et al./NJIT/NSO/AURA/NSF</p></div>

<ul>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid.png" target="_blank">Download Hi-res Version</a> (Right-mouse click and “Save Image As.”)</li>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid-Scale.png" target="_blank">Download Hi-res Version with Scale</a> (Right-mouse click and “Save Image As.”)</li>
</ul>

<div id="attachment_1392966"><picture decoding="async" aria-describedby="caption-attachment-1392966">
<source type="image/webp" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-300x300.jpg.webp 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-1024x1024.jpg.webp 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-150x150.jpg.webp 150w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-768x768.jpg.webp 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-1536x1536.jpg.webp 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-610x610.jpg.webp 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-1080x1080.jpg.webp 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-440x440.jpg.webp 440w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized.jpg.webp 1786w" sizes="(max-width: 300px) 100vw, 300px">
<img decoding="async" aria-describedby="caption-attachment-1392966" src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-300x300.jpg" alt="Coronal rain forms when hotter plasma in the Sun’s corona cools down and becomes denser. Like raindrops on Earth, coronal rain is pulled down to the surface by gravity. Because the plasma is electrically charged, it follows the magnetic field lines, which make huge arches/loops, instead of falling in a straight line. This image is a snapshot from a 23-minute time-lapse video which is comprised of the highest resolution images ever made of coronal rain. The scientists show in the paper that the strands can be narrower than 20 kilometers. This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light. Credit: Schmidt et al./NJIT/NSO/AURA/NSF" width="300" height="300" srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-300x300.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-1024x1024.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-150x150.jpg 150w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-768x768.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-1536x1536.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-610x610.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-1080x1080.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-440x440.jpg 440w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized.jpg 1786w" sizes="(max-width: 300px) 100vw, 300px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-300x300.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-1024x1024.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-150x150.jpg 150w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-768x768.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-1536x1536.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-610x610.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-1080x1080.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-440x440.jpg 440w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized.jpg 1786w" data-lazy-src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-300x300.jpg">
</picture>
<p id="caption-attachment-1392966">Coronal rain forms when hotter plasma in the Sun’s corona cools down and becomes denser. Like raindrops on Earth, coronal rain is pulled down to the surface by gravity. Because the plasma is electrically charged, it follows the magnetic field lines, which make huge arches/loops, instead of falling in a straight line. This image is a snapshot from a 23-minute time-lapse video which is comprised of the highest resolution images ever made of coronal rain. The scientists show in the paper that the strands can be narrower than 20 kilometers. This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light. Credit: Schmidt et al./NJIT/NSO/AURA/NSF</p></div>

<ul>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain.png" target="_blank">Download Hi-res Version</a> (Right-mouse click and “Save Image As.”) </li>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain-2.png" target="_blank">Download Hi-res Version with Scale</a> (Right-mouse click and “Save Image As.”) </li>
</ul>

<div id="attachment_1392951"><picture decoding="async" aria-describedby="caption-attachment-1392951">
<source type="image/webp" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-300x300.jpg.webp 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-1024x1024.jpg.webp 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-150x150.jpg.webp 150w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-768x768.jpg.webp 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-1536x1536.jpg.webp 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-610x610.jpg.webp 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-1080x1080.jpg.webp 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-440x440.jpg.webp 440w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized.jpg.webp 1786w" sizes="(max-width: 300px) 100vw, 300px">
<img decoding="async" aria-describedby="caption-attachment-1392951" src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-300x300.jpg" alt="This image of a solar prominence is a snapshot of a 19-minute time-lapse movie showing how plasma “dances” and twists with the Sun’s magnetic field. This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light. Credit: Schmidt et al./NJIT/NSO/AURA/NSF" width="300" height="300" srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-300x300.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-1024x1024.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-150x150.jpg 150w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-768x768.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-1536x1536.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-610x610.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-1080x1080.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-440x440.jpg 440w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized.jpg 1786w" sizes="(max-width: 300px) 100vw, 300px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-300x300.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-1024x1024.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-150x150.jpg 150w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-768x768.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-1536x1536.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-610x610.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-1080x1080.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-440x440.jpg 440w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized.jpg 1786w" data-lazy-src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-300x300.jpg">
</picture>
<p id="caption-attachment-1392951">This image of a solar prominence is a snapshot of a 19-minute time-lapse movie showing how plasma “dances” and twists with the Sun’s magnetic field. This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light. Credit: Schmidt et al./NJIT/NSO/AURA/NSF</p></div>

<ul>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2.png" target="_blank">Download Hi-res Version</a> (Right-mouse click and “Save Image As.”)</li>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2-Scale.png">Download Hi-res Version with Scale</a> (Right-mouse click and “Save Image As.”)</li>
</ul>

<div id="attachment_1392968"><picture decoding="async" aria-describedby="caption-attachment-1392968">
<source type="image/webp" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-300x240.jpg.webp 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-1024x819.jpg.webp 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-768x614.jpg.webp 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-1536x1229.jpg.webp 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-610x488.jpg.webp 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-1080x864.jpg.webp 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized.jpg.webp 1920w" sizes="(max-width: 300px) 100vw, 300px">
<img decoding="async" aria-describedby="caption-attachment-1392968" src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-300x240.jpg" alt="The 1.6-meter Goode Solar Telescope (GST), located in Big Bear Lake, California, observing the Sun. The steady temperature of the water surface helps keep the air around the telescope calm, reducing the optical effects of turbulent air that degrades the telescope’s images of the Sun and that the adaptive optics further removes to achieve the maximum image detail. The GST is the second-largest solar telescope in the world and home to several instruments that scientists use to analyze the light from the Sun to infer physical processes in the Sun. The NSF National Solar Observatory and the Big Bear Solar Observatory have collaborated for over two decades to develop and advance adaptive optics technologies for solar observations. The GST has been a critical facility to develop and test prototypes for the U.S. National Science Foundation’s 4-meter Daniel K. Inouye Solar Telescope, which took over as the world’s largest solar telescope in 2019. GST’s first-ever coronal adaptive optics system is the latest product of this successful and pioneering collaboration. Credit: Sergey Shumko" width="300" height="240" srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-300x240.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-1024x819.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-768x614.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-1536x1229.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-610x488.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-1080x864.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized.jpg 1920w" sizes="(max-width: 300px) 100vw, 300px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20240'%3E%3C/svg%3E" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-300x240.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-1024x819.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-768x614.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-1536x1229.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-610x488.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-1080x864.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized.jpg 1920w" data-lazy-src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-300x240.jpg">
</picture>
<p id="caption-attachment-1392968">The 1.6-meter Goode Solar Telescope (GST), located in Big Bear Lake, California, observing the Sun. The steady temperature of the water surface helps keep the air around the telescope calm, reducing the optical effects of turbulent air that degrades the telescope’s images of the Sun and that the adaptive optics further removes to achieve the maximum image detail. The GST is the second-largest solar telescope in the world and home to several instruments that scientists use to analyze the light from the Sun to infer physical processes in the Sun. The NSF National Solar Observatory and the Big Bear Solar Observatory have collaborated for over two decades to develop and advance adaptive optics technologies for solar observations. The GST has been a critical facility to develop and test prototypes for the U.S. National Science Foundation’s 4-meter Daniel K. Inouye Solar Telescope, which took over as the world’s largest solar telescope in 2019. GST’s first-ever coronal adaptive optics system is the latest product of this successful and pioneering collaboration. Credit: Sergey Shumko</p></div>

<ul>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST.jpg" target="_blank">Download Hi-res Version</a>(Right-mouse click and “Save Image As.”)</li>
</ul>

<div id="attachment_1392959"><picture decoding="async" aria-describedby="caption-attachment-1392959">
<source type="image/webp" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-300x200.jpg.webp 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-1024x683.jpg.webp 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-768x512.jpg.webp 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-1536x1024.jpg.webp 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-610x407.jpg.webp 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-1080x720.jpg.webp 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized.jpg.webp 1920w" sizes="(max-width: 300px) 100vw, 300px">
<img decoding="async" aria-describedby="caption-attachment-1392959" src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-300x200.jpg" alt="The coronal adaptive optics system Cona at the Goode Solar Telescope. The black square box in the center illuminated and reflecting the sunlight is the adaptive mirror that corrects the images of the Sun. Credit: Dirk Schmidt" width="300" height="200" srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-300x200.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-1024x683.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-768x512.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-1536x1024.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-610x407.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-1080x720.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized.jpg 1920w" sizes="(max-width: 300px) 100vw, 300px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20200'%3E%3C/svg%3E" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-300x200.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-1024x683.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-768x512.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-1536x1024.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-610x407.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-1080x720.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized.jpg 1920w" data-lazy-src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-300x200.jpg">
</picture>
<p id="caption-attachment-1392959">The coronal adaptive optics system Cona at the Goode Solar Telescope. The black square box in the center illuminated and reflecting the sunlight is the adaptive mirror that corrects the images of the Sun. Credit: Dirk Schmidt</p></div>

<ul>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System.jpg" target="_blank">Download Hi-res Version</a>(Right-mouse click and “Save Image As.”)</li>
</ul>
</div><div><h4>Related Video</h4>
<p><strong>Instructions:</strong> To download, right mouse-click, and select “Save Link As” or “Download Video As.”</p>
<ul>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid-Video.mp4">Plasmoid Video</a></li>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Video.mp4">Prominence 1 Video</a></li>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain-Video.mp4">Coronal Rain</a></li>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain-Video.mp4">Prominence 2</a></li>
</ul></div><div><h3>Credits</h3><p>
Schmidt et al./NJIT/NSO/AURA/NSF</p></div><div>
				
				
				
				
				<p><h3>Contacts</h3>
</p>
			</div><div>
				
				
				
				
				<p><strong>Media Contact:</strong><b></b><br>Evan Pascual<br>National Solar Observatory Communications Officer<br><a href="mailto:media@nso.edu">media@nso.edu<b></b><b></b></a></p>
			</div>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[YOLO-World: Real-Time Open-Vocabulary Object Detection (132 pts)]]></title>
            <link>https://arxiv.org/abs/2401.17270</link>
            <guid>44146858</guid>
            <pubDate>Sat, 31 May 2025 20:54:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2401.17270">https://arxiv.org/abs/2401.17270</a>, See on <a href="https://news.ycombinator.com/item?id=44146858">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2401.17270">View PDF</a>
    <a href="https://arxiv.org/html/2401.17270v3">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>The You Only Look Once (YOLO) series of detectors have established themselves as efficient and practical tools. However, their reliance on predefined and trained object categories limits their applicability in open scenarios. Addressing this limitation, we introduce YOLO-World, an innovative approach that enhances YOLO with open-vocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets. Specifically, we propose a new Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information. Our method excels in detecting a wide range of objects in a zero-shot manner with high efficiency. On the challenging LVIS dataset, YOLO-World achieves 35.4 AP with 52.0 FPS on V100, which outperforms many state-of-the-art methods in terms of both accuracy and speed. Furthermore, the fine-tuned YOLO-World achieves remarkable performance on several downstream tasks, including object detection and open-vocabulary instance segmentation.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Tianheng Cheng [<a href="https://arxiv.org/show-email/d19800c9/2401.17270" rel="nofollow">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2401.17270v1" rel="nofollow">[v1]</a></strong>
        Tue, 30 Jan 2024 18:59:38 UTC (5,276 KB)<br>
            <strong><a href="https://arxiv.org/abs/2401.17270v2" rel="nofollow">[v2]</a></strong>
        Fri, 2 Feb 2024 10:06:24 UTC (5,276 KB)<br>
    <strong>[v3]</strong>
        Thu, 22 Feb 2024 13:05:52 UTC (5,277 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Oniux: Kernel-level Tor isolation for any Linux app (167 pts)]]></title>
            <link>https://blog.torproject.org/introducing-oniux-tor-isolation-using-linux-namespaces/</link>
            <guid>44146830</guid>
            <pubDate>Sat, 31 May 2025 20:46:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.torproject.org/introducing-oniux-tor-isolation-using-linux-namespaces/">https://blog.torproject.org/introducing-oniux-tor-isolation-using-linux-namespaces/</a>, See on <a href="https://news.ycombinator.com/item?id=44146830">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>When launching privacy-critical apps and services, developers want to make sure that every packet really only goes through Tor. One mistyped proxy setting–or a single system-call outside the SOCKS wrapper–and your data is suddenly on the line.</p>
<p>That's why today, we are excited to introduce <em>oniux</em>: a small command-line utility providing Tor network isolation for third-party applications using Linux namespaces. Built on Arti, and onionmasq, oniux drop-ships any Linux program into its own network namespace to route it through Tor and strips away the potential for data leaks. If your work, activism, or research demands rock-solid traffic isolation, oniux delivers it.</p>
<h2>What are Linux namespaces? 🐧</h2>
<p>Namespaces are an isolation feature found in the Linux kernel that were introduced around the year 2000. They provide a secure way of isolating a certain part of an application from the rest of the system. Namespaces come in various forms and shapes. Some examples include network namespaces, mount namespaces, process namespaces, and a few more; each of them isolating a certain amount of system resources from an application.</p>
<p>What do we mean by <strong>system resources</strong>? In Linux, system resources are available globally by all applications on the system. The most notable example of this is probably your operating system clock, but there are many other areas as well, such as the list of all processes, the file system, and the list of users.</p>
<p>Namespaces <em>containerize</em> a certain part of an application from the rest of the operating system; this is exactly what Docker uses in order to provide its isolation primitives.</p>
<h2>Tor + Namespaces = ❤️</h2>
<p>As outlined above, namespaces are a powerful feature that gives us the ability to isolate Tor network access of an arbitrary application. We put each application in a network namespace that doesn't provide access to system-wide network interfaces (such as eth0), and instead provides a custom network interface onion0.</p>
<p>This allows us to isolate an arbitrary application over Tor in the most secure way possible software-wise, namely by relying on a security primitive offered by the operating system kernel. Unlike SOCKS, the application cannot accidentally leak data by failing to make some connection via the configured SOCKS, which may happen due to a mistake by the developer.</p>
<h2>oniux vs. torsocks</h2>
<p>You may have also heard of a tool with a similar goal, known as
<a href="https://gitlab.torproject.org/tpo/core/torsocks"><code>torsocks</code></a>, which works by
overwriting all network-related libc functions in a way to route traffic over a
SOCKS proxy offered by Tor.  While this approach is a bit more cross-platform,
it has the notable downside that applications making system calls <em>not</em> through
a dynamically linked libc, either with malicious intent or not, will leak data.
Most notably, this excludes support for purely static binaries and applications
from the Zig ecosystem.</p>
<p>The following provides a basic comparison on <em>oniux</em> vs <em>torsocks</em>:</p>
<table>
<thead><tr>
<th><em>oniux</em></th>
<th><em>torsocks</em></th>
</tr>
</thead>
<tbody>
<tr>
<td>Standalone application</td>
<td>Requires running Tor daemon</td>
</tr>
<tr>
<td>Uses Linux namespaces</td>
<td>Uses an ld.so preload hack</td>
</tr>
<tr>
<td>Works on all applications</td>
<td>Only works on applications making system calls through libc</td>
</tr>
<tr>
<td>Malicious application cannot leak</td>
<td>Malicious application can leak by making a system call through raw assembly</td>
</tr>
<tr>
<td>Linux only</td>
<td>Cross-platform</td>
</tr>
<tr>
<td>New and experimental</td>
<td>Battle-proven for over 15 years</td>
</tr>
<tr>
<td>Uses Arti as its engine</td>
<td>Uses CTor as its engine</td>
</tr>
<tr>
<td>Written in Rust</td>
<td>Written in C</td>
</tr>
</tbody>
</table>
<h2>How can I use <em>oniux</em>? 🧅</h2>
<p>First, you need a Linux system with a Rust toolchain installed.
Afterwards, you can install <em>oniux</em> with the following command:</p>
<div><pre><span></span>$ cargo install --git https://gitlab.torproject.org/tpo/core/oniux oniux@0.4.0
</pre></div>
<p>Once that is done, you are ready to go for using <em>oniux</em>!  🙂</p>
<p>Using <em>oniux</em> is straightforward:</p>
<div><pre><span></span><span># Perform a simple HTTPS query using oniux!</span>
$ oniux curl https://icanhazip.com
&lt;A TOR EXIT NODE IP ADDRESS&gt;

<span># oniux also supports IPv6 of course!</span>
$ oniux curl -6 https://ipv6.icanhazip.com
&lt;A TOR EXIT NODE IPv6 ADDRESS&gt;

<span># Tor without onion services is like a car without an engine ...</span>
$ oniux curl http://2gzyxa5ihm7nsggfxnu52rck2vv4rvmdlkiu3zzui5du4xyclen53wid.onion/index.html

<span># You can also enable logging if you are a nerd. 🤓</span>
$ <span>RUST_LOG</span><span>=</span>debug oniux curl https://icanhazip.com

<span># If you want, you can "torify" your entire shell, isolating all processes within!</span>
$ oniux bash

<span># If you are in a desktop environment, you can isolate graphical applications too!</span>
$ oniux hexchat
</pre></div>
<h2>How does this work internally? ⚙️</h2>
<p><em>oniux</em> works by immediately spawning a child process using the <code>clone(2)</code>
system call, which is isolated in its own network, mount, PID, and user
namespace.  This process then mounts its own copy of <code>/proc</code> followed by UID
and GID mappings to the respective UID and GID of the parent process.</p>
<p>Afterwards, it creates a temporary file with nameserver entries which will then
be bind mounted onto <code>/etc/resolv.conf</code>, so that applications running within
will use a custom name resolver that supports resolving through Tor.</p>
<p>Next, the child process utilizes
<a href="https://gitlab.torproject.org/tpo/core/onionmasq">onionmasq</a> to create a TUN
interface named <code>onion0</code> followed by some <code>rtnetlink(7)</code> operations required to
set up the interface, such as assigning IP addresses.</p>
<p>Then, the child process sends the file descriptor of the TUN interface over
a Unix Domain socket to the parent process, who has been waiting for this
message ever since executing the initial <code>clone(2)</code>.</p>
<p>Once that is done, the child process drops all of its capabilities which
were acquired as part of being the root process in the user namespace.</p>
<p>Finally, the command supplied by the user is executed using facilities
provided by the Rust standard library.</p>
<h2><em>oniux</em> is experimental ⚠️</h2>
<p>Although this section should not discourage you from using <em>oniux</em>, you should
keep in mind that this is a relatively new feature which uses new Tor software,
such as <em>Arti</em> and <em>onionmasq</em>.</p>
<p>While things are already working as expected at the moment, tools such as
<em>torsocks</em> have been around for over 15 years, giving them more experience on
the battlefield.</p>
<p>But we do want to reach a similar state with oniux, so please go ahead and
check it out!</p>
<h2>Credits</h2>
<p>Many thanks to the developers of
<a href="https://github.com/smoltcp-rs/smoltcp"><code>smoltcp</code></a>, which is a Rust crate that
implements a full IP stack in Rust -- something, we make heavy use of.</p>
<p>Also many thanks go to <code>7ppKb5bW</code>, who taught us on how this can implemented
without the use of <code>capabilities(7)</code> by using <code>user_namespaces(7)</code> properly.</p>
<p>Last but not least, many thanks to all people and organizations who support Tor financially. The Tor Project, Inc. is a 501(c)(3) nonprofit advancing human rights and defending privacy online through free software and open networks. The oniux release is powered by a community of supporters. Please consider donating today to continue advancing our work that makes privacy possible.</p>
<p><a href="https://torproject.org/donate/donate-bp2-sc2025"><img src="https://blog.torproject.org/introducing-oniux-tor-isolation-using-linux-namespaces/button-large-black.png" alt="Donate Button"></a></p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CCD co-inventor George E. Smith dies at 95 (131 pts)]]></title>
            <link>https://www.nytimes.com/2025/05/30/science/george-e-smith-dead.html</link>
            <guid>44146619</guid>
            <pubDate>Sat, 31 May 2025 19:56:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/05/30/science/george-e-smith-dead.html">https://www.nytimes.com/2025/05/30/science/george-e-smith-dead.html</a>, See on <a href="https://news.ycombinator.com/item?id=44146619">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/05/30/science/george-e-smith-dead.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[A Lean companion to Analysis I (250 pts)]]></title>
            <link>https://terrytao.wordpress.com/2025/05/31/a-lean-companion-to-analysis-i/</link>
            <guid>44145517</guid>
            <pubDate>Sat, 31 May 2025 16:55:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://terrytao.wordpress.com/2025/05/31/a-lean-companion-to-analysis-i/">https://terrytao.wordpress.com/2025/05/31/a-lean-companion-to-analysis-i/</a>, See on <a href="https://news.ycombinator.com/item?id=44145517">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>
 Almost 20 years ago, I wrote a textbook in real analysis called “<a href="https://terrytao.wordpress.com/books/analysis-i/">Analysis I</a>“. It was intended to complement the many good available analysis textbooks out there by focusing more on foundational issues, such as the construction of the natural numbers, integers, rational numbers, and reals, as well as providing enough set theory and logic to allow students to develop proofs at high levels of rigor.
</p><p>
While some proof assistants such as Coq or Agda were well established when the book was written, formal verification was not on my radar at the time. However, now that I have had some experience with this subject, I realize that the content of this book is in fact very compatible with such proof assistants; in particular, the ‘naive type theory’ that I was implicitly using to do things like construct the standard number systems, dovetails well with the dependent type theory of Lean (which, among other things, has excellent support for quotient types).
</p><p>
I have therefore decided to launch a <a href="https://github.com/teorth/analysis">Lean companion to “Analysis I”</a>, which is a “translation” of many of the definitions, theorems, and exercises of the text into Lean. In particular, this gives an alternate way to perform the exercises in the book, by instead filling in the corresponding “sorries” in the Lean code. (I do not however plan on hosting “official” solutions to the exercises in this companion; instead, feel free to create forks of the repository in which these sorries are filled in.)
</p><p>
Currently, the following sections of the text have been translated into Lean: 

</p><ul> <li> <a href="https://teorth.github.io/estimate_tools/docs/Analysis/Section_2_1.html">Section 2.1: The natural numbers</a> </li><li> <a href="https://teorth.github.io/estimate_tools/docs/Analysis/Section_2_2.html">Section 2.2: Addition</a> </li><li> <a href="https://teorth.github.io/estimate_tools/docs/Analysis/Section_2_3.html">Section 2.3: Multiplication</a> </li><li> <a href="https://teorth.github.io/estimate_tools/docs/Analysis/Section_2_epilogue.html">Chapter 2 epilogue: Isomorphism with the Mathlib natural numbers</a> </li><li> <a href="https://teorth.github.io/estimate_tools/docs/Analysis/Section_3_1.html">Section 3.1: Basic set theory</a> </li><li> <a href="https://teorth.github.io/estimate_tools/docs/Analysis/Section_4_1.html">Section 4.1: The integers</a> 
</li></ul>


<p>
The formalization has been deliberately designed to be separate from the standard Lean math library <a href="https://leanprover-community.github.io/mathlib4_docs/">Mathlib</a> at some places, but reliant on it at others. For instance, Mathlib already has a standard notion of the natural numbers <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{{\bf N}}">. In the Lean formalization, I first develop “by hand” an alternate construction <code>Chapter2.Nat</code> of the natural numbers (or just <code>Nat</code>, if one is working in the <code>Chapter2</code> namespace), setting up many of the basic results about these alternate natural numbers which parallel similar lemmas about <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{{\bf N}}"> that are already in Mathlib (but with many of these lemmas set as exercises to the reader, with the proofs currently replaced with “sorries”). Then, in an epilogue section, isomorphisms between these alternate natural numbers and the Mathlib natural numbers are established (or more precisely, set as exercises). From that point on, the Chapter 2 natural numbers are deprecated, and the Mathlib natural numbers are used instead. I intend to continue this general pattern throughout the book, so that as one advances into later chapters, one increasingly relies on Mathlib’s definitions and functions, rather than directly referring to any counterparts from earlier chapters. As such, this companion could also be used as an introduction to Lean and Mathlib as well as to real analysis (somewhat in the spirit of the “<a href="https://adam.math.hhu.de/">Natural number game</a>“, which in fact has significant thematic overlap with Chapter 2 of my text).
</p><p>
The code in this repository compiles in Lean, but I have not tested whether all of the (numerous) “sorries” in the code can actually be filled (i.e., if all the exercises can actually be solved in Lean). I would be interested in having volunteers “playtest” the companion to see if this can actually be done (and if the helper lemmas or “API” provided in the Lean files are sufficient to fill in the sorries in a conceptually straightforward manner without having to rely on more esoteric Lean programming techniques). Any other feedback will of course also be welcome.
</p><p>
[UPDATE, May 31: moved the companion to a standalone repository.]


</p>	</div></div>]]></description>
        </item>
    </channel>
</rss>