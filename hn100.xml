<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 02 Sep 2025 18:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Anthropic raises $13B Series F at $183B post-money valuation (256 pts)]]></title>
            <link>https://www.anthropic.com/news/anthropic-raises-series-f-at-usd183b-post-money-valuation</link>
            <guid>45104907</guid>
            <pubDate>Tue, 02 Sep 2025 16:04:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/news/anthropic-raises-series-f-at-usd183b-post-money-valuation">https://www.anthropic.com/news/anthropic-raises-series-f-at-usd183b-post-money-valuation</a>, See on <a href="https://news.ycombinator.com/item?id=45104907">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>Anthropic has completed a Series F fundraising of $13 billion led by ICONIQ. This financing values Anthropic at $183 billion post-money. Along with ICONIQ, the round was co-led by Fidelity Management &amp; Research Company and Lightspeed Venture Partners. The investment reflects Anthropic’s continued momentum and reinforces our position as the leading intelligence platform for enterprises, developers, and power users.</p><p>Significant investors in this round include Altimeter, Baillie Gifford, affiliated funds of BlackRock, Blackstone, Coatue, D1 Capital Partners, General Atlantic, General Catalyst, GIC, Growth Equity at Goldman Sachs Alternatives, Insight Partners, Jane Street, Ontario Teachers' Pension Plan, Qatar Investment Authority, TPG, T. Rowe Price Associates, Inc., T. Rowe Price Investment Management, Inc., WCM Investment Management, and XN.</p><p>“From Fortune 500 companies to AI-native startups, our customers rely on Anthropic’s frontier models and platform products for their most important, mission-critical work,” said Krishna Rao, Chief Financial Officer of Anthropic. “We are seeing exponential growth in demand across our entire customer base. This financing demonstrates investors’ extraordinary confidence in our financial performance and the strength of their collaboration with us to continue fueling our unprecedented growth.”</p><p>Anthropic has seen rapid growth since the launch of Claude in March 2023. At the beginning of 2025, less than two years after launch, Anthropic’s run-rate revenue had grown to approximately $1 billion. By August 2025, just eight months later, our run-rate revenue reached over $5 billion—making Anthropic one of the fastest-growing technology companies in history.</p><p>Anthropic’s trajectory has been driven by our leading technical talent, our focus on safety, and our frontier research, including pioneering alignment and interpretability work, all of which underpin the performance and reliability of our models. Every day more businesses, developers, and consumer power users are trusting Claude to help them solve their most challenging problems. Anthropic now serves over 300,000 business customers, and our number of large accounts—customers that each represent over $100,000 in run-rate revenue—has grown nearly 7x in the past year.</p><p>This growth spans the entire Anthropic platform, with advancements for businesses, developers, and consumers. For businesses, our API and <a href="https://www.anthropic.com/news/claude-for-financial-services">industry-specific products</a> make it easy to add powerful AI to their critical applications without complex integration work. Developers have made Claude Code their tool of choice since its full launch in May 2025. Claude Code has quickly taken off—already generating over $500 million in run-rate revenue with usage growing more than 10x in just three months. For individual users, the Pro and Max plans for Claude deliver enhanced AI capabilities for everyday tasks and specialized projects.</p><p>“Anthropic is on an exceptional trajectory, combining research excellence, technological leadership, and relentless focus on customers. We’re honored to partner with Dario and the team, and our lead investment in their Series F reflects our belief in their values and their ability to shape the future of responsible AI,” said Divesh Makan, Partner at ICONIQ. “Enterprise leaders tell us what we’re seeing firsthand—Claude is reliable, built on a trustworthy foundation, and guided by leaders truly focused on the long term.”</p><p>The Series F investment will expand our capacity to meet growing enterprise demand, deepen our safety research, and support international expansion as we continue building reliable, interpretable, and steerable AI systems.</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[X(Twitter) Shadow Bans Turkish Presidential Candidate (246 pts)]]></title>
            <link>https://utkusen.substack.com/p/xtwitter-secretly-shadow-bans-turkish</link>
            <guid>45104597</guid>
            <pubDate>Tue, 02 Sep 2025 15:42:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://utkusen.substack.com/p/xtwitter-secretly-shadow-bans-turkish">https://utkusen.substack.com/p/xtwitter-secretly-shadow-bans-turkish</a>, See on <a href="https://news.ycombinator.com/item?id=45104597">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!3grz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb559ac01-f8c8-4a15-b72d-4abbe075938c_2816x1536.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!3grz!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb559ac01-f8c8-4a15-b72d-4abbe075938c_2816x1536.png 424w, https://substackcdn.com/image/fetch/$s_!3grz!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb559ac01-f8c8-4a15-b72d-4abbe075938c_2816x1536.png 848w, https://substackcdn.com/image/fetch/$s_!3grz!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb559ac01-f8c8-4a15-b72d-4abbe075938c_2816x1536.png 1272w, https://substackcdn.com/image/fetch/$s_!3grz!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb559ac01-f8c8-4a15-b72d-4abbe075938c_2816x1536.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!3grz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb559ac01-f8c8-4a15-b72d-4abbe075938c_2816x1536.png" width="1456" height="794" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b559ac01-f8c8-4a15-b72d-4abbe075938c_2816x1536.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:794,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:6168245,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://utkusen.substack.com/i/172556114?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb559ac01-f8c8-4a15-b72d-4abbe075938c_2816x1536.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!3grz!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb559ac01-f8c8-4a15-b72d-4abbe075938c_2816x1536.png 424w, https://substackcdn.com/image/fetch/$s_!3grz!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb559ac01-f8c8-4a15-b72d-4abbe075938c_2816x1536.png 848w, https://substackcdn.com/image/fetch/$s_!3grz!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb559ac01-f8c8-4a15-b72d-4abbe075938c_2816x1536.png 1272w, https://substackcdn.com/image/fetch/$s_!3grz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb559ac01-f8c8-4a15-b72d-4abbe075938c_2816x1536.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Istanbul Mayor Ekrem İmamoğlu’s university diploma was annulled on March 18, 2025, and he was jailed for corruption on March 23, 2025. Many people believe he did nothing wrong and that the current Turkish president wants to remove his strongest rival.</p><p>What happened afterward supports this view. His photo was banned from billboards in Istanbul, and his X (formerly Twitter) account with 9.7 million followers was restricted in Turkey.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!okXG!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd3cb70b-6e2d-4bad-88c2-ab4081f7324d_1166x690.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!okXG!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd3cb70b-6e2d-4bad-88c2-ab4081f7324d_1166x690.png 424w, https://substackcdn.com/image/fetch/$s_!okXG!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd3cb70b-6e2d-4bad-88c2-ab4081f7324d_1166x690.png 848w, https://substackcdn.com/image/fetch/$s_!okXG!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd3cb70b-6e2d-4bad-88c2-ab4081f7324d_1166x690.png 1272w, https://substackcdn.com/image/fetch/$s_!okXG!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd3cb70b-6e2d-4bad-88c2-ab4081f7324d_1166x690.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!okXG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd3cb70b-6e2d-4bad-88c2-ab4081f7324d_1166x690.png" width="1166" height="690" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/dd3cb70b-6e2d-4bad-88c2-ab4081f7324d_1166x690.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:690,&quot;width&quot;:1166,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:593226,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://utkusen.substack.com/i/172556114?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd3cb70b-6e2d-4bad-88c2-ab4081f7324d_1166x690.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!okXG!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd3cb70b-6e2d-4bad-88c2-ab4081f7324d_1166x690.png 424w, https://substackcdn.com/image/fetch/$s_!okXG!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd3cb70b-6e2d-4bad-88c2-ab4081f7324d_1166x690.png 848w, https://substackcdn.com/image/fetch/$s_!okXG!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd3cb70b-6e2d-4bad-88c2-ab4081f7324d_1166x690.png 1272w, https://substackcdn.com/image/fetch/$s_!okXG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd3cb70b-6e2d-4bad-88c2-ab4081f7324d_1166x690.png 1456w" sizes="100vw"></picture></div></a></figure></div><p><span>Elon Musk didn’t say anything about the situation and X didn’t defend freedom of speech. They only </span><a href="https://x.com/GlobalAffairs/status/1920426409358455081" rel="">said</a><span> there was a court order and they couldn’t do anything. But many people believe they should have defended free speech.</span></p><p><span>What happened next made people wonder about Elon Musk’s position on the issue. Ekrem İmamoğlu opened a new X account called </span><a href="https://x.com/CBAdayOfisi" rel="">“Cumhurbaşkanlığı Aday Ofisi” (Office of the Presidential Candidate)</a><span>.</span></p><p>The account was promoted by the opposition party leader (3.7M followers) and the mayor of Ankara (7.8M followers). In a short time, it reached 200K followers. But after that, the growth slowed down sharply. A week later, it was around 210K, and the following week, 230K. Considering his original account had 9.7M followers, these numbers seemed unusually low. </p><p><span>Then, people started noticing they weren’t seeing İmamoğlu’s posts on their timelines. Even though he posted regularly, the algorithm didn’t promote his content. I noticed the same. I had 200 mutual friends with him, I followed his new account, visited his profile, and liked all his posts. But still didn’t see any of his tweets in the following days. Now, he has only 318K followers while his wife </span><a href="https://x.com/dk_imamoglu" rel="">Dilek İmamoğlu</a><span> has 600K.</span></p><p>For the last two weeks, I was busy with personal matters but still checked my X timeline daily. Yesterday, I realized I hadn’t seen any posts from İmamoğlu in a long time. I wondered why he wasn’t posting. When I visited his profile, I saw that he was actually tweeting regularly, but X wasn’t showing his posts to me.</p><p><span>This started a discussion in the Turkish community. Many people said they follow İmamoğlu but don’t see his tweets on their homepages. I </span><a href="https://x.com/utkusen/status/1962801872458129536" rel="">created</a><span> a poll and 715 people voted:</span></p><ul><li><p>56% said they never see his posts</p></li><li><p>34% said they see them rarely</p></li><li><p>6% said they see an average amount</p></li><li><p>3% said they see them constantly</p></li></ul><p>We don’t have solid proof, but it strongly suggests that X is secretly shadow banning İmamoğlu. I don’t think Elon Musk will change this, but I’m writing this article to show the political power he holds.</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Americans Lose Faith That Hard Work Leads to Economic Gains, WSJ-NORC Poll Finds (132 pts)]]></title>
            <link>https://www.wsj.com/economy/wsj-norc-economic-poll-73bce003</link>
            <guid>45104082</guid>
            <pubDate>Tue, 02 Sep 2025 15:07:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/economy/wsj-norc-economic-poll-73bce003">https://www.wsj.com/economy/wsj-norc-economic-poll-73bce003</a>, See on <a href="https://news.ycombinator.com/item?id=45104082">Hacker News</a></p>
Couldn't get https://www.wsj.com/economy/wsj-norc-economic-poll-73bce003: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[You don't want to hire "the best engineers" (319 pts)]]></title>
            <link>https://www.otherbranch.com/shared/blog/no-you-dont-want-to-hire-the-best-engineers</link>
            <guid>45103646</guid>
            <pubDate>Tue, 02 Sep 2025 14:32:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.otherbranch.com/shared/blog/no-you-dont-want-to-hire-the-best-engineers">https://www.otherbranch.com/shared/blog/no-you-dont-want-to-hire-the-best-engineers</a>, See on <a href="https://news.ycombinator.com/item?id=45103646">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-framer-name="Content" data-framer-component-type="RichTextContainer"><blockquote><p>“We only want to hire the <em>best</em> engineers”</p></blockquote><p>I hear this from almost every client I speak to. So does every other recruiter.</p><p>Seriously - just say those eight words to any room full of recruiting people, and everyone will give a wry chuckle and roll their eyes. We've all heard it a million times.</p><blockquote><p>“We only want to hire the <em>best </em>engineers.”</p></blockquote><p><strong>No. No, you do not.</strong></p><p>The <strong>best</strong> engineers make more than your entire payroll. They have opinions on tech debt and timelines. They <!--$--><a href="https://www.otherbranch.com/shared/blog/quantifying-the-cost-of-rto">have remote jobs</a><!--/$-->, if they want them. They don’t go “oh, well, this is your third company, so I guess I’ll defer to you on all product decisions”. They care about comp, a trait you consider disqualifying. They can care about work-life balance, because they’re not desperate enough to feel the need not to. And however successful your company has been so far, they have other options they like better.</p><p>You’re not stupid. If I asked you, point blank, “do you actually think the best engineers in the world would give your company a second thought,” I bet you could say “well, no, obviously not”.</p><p>But you don’t act like it.</p><p>You lock in the same set of criteria as every other startup. Experience at early stage. Highly independent. In-office in the Bay Area. Not too “salary motivated”. Don’t even apply if you want a 40h/week job - we work hard and play hard.</p><p><img alt="" width="250" height="299" src="https://framerusercontent.com/images/z67Wlg1c4azHjpvYkRal5sHvYqw.png?width=500&amp;height=598" srcset="https://framerusercontent.com/images/z67Wlg1c4azHjpvYkRal5sHvYqw.png?width=500&amp;height=598 500w" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (min-width: 810px) and (max-width: 1199.98px) 100vw, (max-width: 809.98px) 100vw"></p><p>Four months later, you haven’t found a good founding engineer. Do you know how long four months is in the life of a young startup? That’s an <strong>eternity</strong>, and you’ve spent it in stasis.</p><p>Hiring is a negotiation, and you’re acting like you’re holding all the cards when you aren't. You’re looking for a highly competitive candidate pool, and you’re not being competitive: you’re just checking the same baselines as everybody else. <strong>You're acting like a replacement-level employer and expecting more than replacement-level candidates.</strong></p><p>Would you rather spend four months in stasis waiting for a senior candidate who hits the ground running on day one, or hire a skilled midlevel hacker who will be at full capacity in two weeks immediately?</p><p>Would you rather spend four months in stasis waiting for a 50h/week candidate, or have a 40h/week candidate now?</p><p>Would you rather be a green bar in this chart, or a red one?</p><p><img alt="" width="982" height="817" src="https://framerusercontent.com/images/I4qkE65BZtaiuPKM5iA6Vx9d1Bk.png?width=1964&amp;height=1634" srcset="https://framerusercontent.com/images/I4qkE65BZtaiuPKM5iA6Vx9d1Bk.png?scale-down-to=512&amp;width=1964&amp;height=1634 512w,https://framerusercontent.com/images/I4qkE65BZtaiuPKM5iA6Vx9d1Bk.png?scale-down-to=1024&amp;width=1964&amp;height=1634 1024w,https://framerusercontent.com/images/I4qkE65BZtaiuPKM5iA6Vx9d1Bk.png?width=1964&amp;height=1634 1964w" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (min-width: 810px) and (max-width: 1199.98px) 100vw, (max-width: 809.98px) 100vw"></p><p>You don’t ask yourself these questions. You say “I want a candidate with these traits,” and sit on your hands until one materializes, until you run out of money, or - more likely - until someone manages to worm through your unrealistic expectations and convince you to compromise <em>for them</em>. </p><p>If you had accepted compromise, you could have opened the floodgates on day one and had your pick of ten great-but-not-perfect candidates. Instead, you waited months and settled for one.</p><p>When you accept that you need a <strong>great</strong> engineer, and not the <strong>best</strong> engineer, you can deal with the trade-offs consciously. What traits are actually important? How much are you willing to give up to get them? What’s the dollar value of a hire this month versus next month? <strong>“What actually matters today?” is the most important question a startup can ask</strong>, and you haven't applied it to one of the most important aspects of running a company!</p><blockquote><p>"Well, we're a little different from other companies, because we have really high standards."</p></blockquote><p>Does it <strong>sound</strong> like you're different?</p><blockquote><p>"We just raised a very exciting Series A!"</p></blockquote><p>So did literally a thousand other companies. There was <!--$--><a href="https://news.crunchbase.com/venture/state-of-startups-q2-h1-2025-ai-ma-charts-data/" target="_blank" rel="noopener">$26B in early stage venture investment last quarter</a><!--/$-->, and you can do the math as to how much of that your $10M raise occupies. The hires you need aren't looking at your company as the slam-dunk success that a founder necessarily needs to believe that it is. Maybe they will once they talk to you, but that's later - at the top of your funnel, you're just another face in the crowd, and you need to act like it.</p><p>I’m not telling you to hire people who aren’t good. I’m not even telling you that the traits you want aren’t good things to look for. I’m not telling you to actually compromise on quality. I’m telling you that <strong>trying to hire </strong>the best engineers is the enemy of<strong> actually hiring </strong>great ones. You’re going to have to give up something (possibly time, possibly comp, possibly workplace policy) to make the hire you want.</p><p>The longer you aren’t thinking about <em>what</em> to give up, the more you’re implicitly choosing to give up time, the thing startups treasure more than anything else. And you’re giving up time to - what, play it safe?</p><p>The default outcome for a startup is always failure. You took a risk by even starting one. You ship things that might be broken all the time, <strong>because you know that speed is more important than perfection</strong>. You take moonshots, because you know that big wins matter more than small losses. And then you give up months of time because you refuse to apply the same philosophy to hiring!</p><p>I run a recruiting company. It’s no skin off my back if you want to be irrational about hiring. Please, by all means, continue. You’re leaving a thousand great engineers to sit in my database instead of your ATS, and I would much rather you pay me 40 grand to find them than find them yourself.</p><p>Or you can act like the scrappy realist you probably like to think you are, stop insisting on perfection, and move fast.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Little Book of Linear Algebra (125 pts)]]></title>
            <link>https://github.com/the-litte-book-of/linear-algebra</link>
            <guid>45103436</guid>
            <pubDate>Tue, 02 Sep 2025 14:17:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/the-litte-book-of/linear-algebra">https://github.com/the-litte-book-of/linear-algebra</a>, See on <a href="https://news.ycombinator.com/item?id=45103436">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">The Little Book of Linear Algebra</h2><a id="user-content-the-little-book-of-linear-algebra" aria-label="Permalink: The Little Book of Linear Algebra" href="#the-little-book-of-linear-algebra"></a></p>
<p dir="auto">A concise, beginner-friendly introduction to the core ideas of linear algebra.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Formats</h2><a id="user-content-formats" aria-label="Permalink: Formats" href="#formats"></a></p>
<ul dir="auto">
<li><a href="https://github.com/the-litte-book-of/linear-algebra/blob/main/book.pdf">Download PDF</a> – print-ready version</li>
<li><a href="https://github.com/the-litte-book-of/linear-algebra/blob/main/book.epub">Download EPUB</a> – e-reader friendly</li>
<li><a href="https://github.com/the-litte-book-of/linear-algebra/blob/main/book.tex">View LaTeX</a> – Latex source</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Chapter 1. Vectors</h2><a id="user-content-chapter-1-vectors" aria-label="Permalink: Chapter 1. Vectors" href="#chapter-1-vectors"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">1.1 Scalars and Vectors</h2><a id="user-content-11-scalars-and-vectors" aria-label="Permalink: 1.1 Scalars and Vectors" href="#11-scalars-and-vectors"></a></p>
<p dir="auto">A scalar is a single numerical quantity, most often taken from the real numbers, denoted by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}$</math-renderer>. Scalars are
the fundamental building blocks of arithmetic: they can be added, subtracted, multiplied, and, except in the case of
zero, divided. In linear algebra, scalars play the role of coefficients, scaling factors, and entries of larger
structures such as vectors and matrices. They provide the weights by which more complex objects are measured and
combined. A vector is an ordered collection of scalars, arranged either in a row or a column. When the scalars are real
numbers, the vector is said to belong to <em>real</em> <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer>-dimensional space, written</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbb{R}^n = { (x_1, x_2, \dots, x_n) \mid x_i \in \mathbb{R} }.
$$</math-renderer></p>
<p dir="auto">An element of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer> is called a vector of dimension <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> or an <em>n</em>-vector. The number <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> is called the
dimension of the vector space. Thus <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer> is the space of all ordered pairs of real numbers, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer> the
space of all ordered triples, and so on.</p>
<p dir="auto">Example 1.1.1.</p>
<ul dir="auto">
<li>A 2-dimensional vector: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(3, -1) \in \mathbb{R}^2$</math-renderer>.</li>
<li>A 3-dimensional vector: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(2, 0, 5) \in \mathbb{R}^3$</math-renderer>.</li>
<li>A 1-dimensional vector: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(7) \in \mathbb{R}^1$</math-renderer>, which corresponds to the scalar <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$7$</math-renderer> itself.</li>
</ul>
<p dir="auto">Vectors are often written vertically in column form, which emphasizes their role in matrix multiplication:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{v} = \begin{bmatrix} 2 \ 0 \ 5 \end{bmatrix} \in \mathbb{R}^3.
$$</math-renderer></p>
<p dir="auto">The vertical layout makes the structure clearer when we consider linear combinations or multiply matrices by vectors.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation"></a></p>
<p dir="auto">In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>, a vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x_1, x_2)$</math-renderer> can be visualized as an arrow starting at the origin <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(0,0)$</math-renderer> and ending at the
point <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x_1, x_2)$</math-renderer>. Its length corresponds to the distance from the origin, and its orientation gives a direction in the
plane. In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, the same picture extends into three dimensions: a vector is an arrow from the origin
to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x_1, x_2, x_3)$</math-renderer>. Beyond three dimensions, direct visualization is no longer possible, but the algebraic rules of
vectors remain identical. Even though we cannot draw a vector in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^{10}$</math-renderer>, it behaves under addition, scaling,
and transformation exactly as a 2- or 3-dimensional vector does. This abstract point of view is what allows linear
algebra to apply to data science, physics, and machine learning, where data often lives in very high-dimensional spaces.
Thus a vector may be regarded in three complementary ways:</p>
<ol dir="auto">
<li>As a point in space, described by its coordinates.</li>
<li>As a displacement or arrow, described by a direction and a length.</li>
<li>As an abstract element of a vector space, whose properties follow algebraic rules independent of geometry.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Notation</h3><a id="user-content-notation" aria-label="Permalink: Notation" href="#notation"></a></p>
<ul dir="auto">
<li>Vectors are written in boldface lowercase letters: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}, \mathbf{w}, \mathbf{x}$</math-renderer>.</li>
<li>The <em>i</em>-th entry of a vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer> is written <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$v_i$</math-renderer>, where indices begin at 1.</li>
<li>The set of all <em>n</em>-dimensional vectors over <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}$</math-renderer> is denoted <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>.</li>
<li>Column vectors will be the default form unless otherwise stated.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why begin here?</h3><a id="user-content-why-begin-here" aria-label="Permalink: Why begin here?" href="#why-begin-here"></a></p>
<p dir="auto">Scalars and vectors form the atoms of linear algebra. Every structure we will build-vector spaces, linear
transformations, matrices, eigenvalues-relies on the basic notions of number and ordered collection of numbers. Once
vectors are understood, we can define operations such as addition and scalar multiplication, then generalize to
subspaces, bases, and coordinate systems. Eventually, this framework grows into the full theory of linear algebra, with
powerful applications to geometry, computation, and data.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 1.1</h3><a id="user-content-exercises-11" aria-label="Permalink: Exercises 1.1" href="#exercises-11"></a></p>
<ol dir="auto">
<li>Write three different vectors in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer> and sketch them as arrows from the origin. Identify their coordinates
explicitly.</li>
<li>Give an example of a vector in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^4$</math-renderer>. Can you visualize it directly? Explain why high-dimensional
visualization is challenging.</li>
<li>Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (4, -3, 2)$</math-renderer>. Write <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer> in column form and state <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$v_1, v_2, v_3$</math-renderer>.</li>
<li>In what sense is the set <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^1$</math-renderer> both a line and a vector space? Illustrate with examples.</li>
<li>Consider the vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (1,1,\dots,1) \in \mathbb{R}^n$</math-renderer>. What is special about this vector when <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> is
large? What might it represent in applications?</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">1.2 Vector Addition and Scalar Multiplication</h2><a id="user-content-12-vector-addition-and-scalar-multiplication" aria-label="Permalink: 1.2 Vector Addition and Scalar Multiplication" href="#12-vector-addition-and-scalar-multiplication"></a></p>
<p dir="auto">Vectors in linear algebra are not static objects; their power comes from the operations we can perform on them. Two
fundamental operations define the structure of vector spaces: addition and scalar multiplication. These operations
satisfy simple but far-reaching rules that underpin the entire subject.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Vector Addition</h3><a id="user-content-vector-addition" aria-label="Permalink: Vector Addition" href="#vector-addition"></a></p>
<p dir="auto">Given two vectors of the same dimension, their sum is obtained by adding corresponding entries. Formally, if</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} = (u_1, u_2, \dots, u_n), \quad
\mathbf{v} = (v_1, v_2, \dots, v_n),
$$</math-renderer></p>
<p dir="auto">then their sum is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} + \mathbf{v} = (u_1+v_1, u_2+v_2, \dots, u_n+v_n).
$$</math-renderer></p>
<p dir="auto">Example 1.2.1.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (2, -1, 3)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (4, 0, -5)$</math-renderer>. Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} + \mathbf{v} = (2+4, -1+0, 3+(-5)) = (6, -1, -2).
$$</math-renderer></p>
<p dir="auto">Geometrically, vector addition corresponds to the <em>parallelogram rule</em>. If we draw both vectors as arrows from the
origin, then placing the tail of one vector at the head of the other produces the sum. The diagonal of the parallelogram
they form represents the resulting vector.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Scalar Multiplication</h3><a id="user-content-scalar-multiplication" aria-label="Permalink: Scalar Multiplication" href="#scalar-multiplication"></a></p>
<p dir="auto">Multiplying a vector by a scalar stretches or shrinks the vector while preserving its direction, unless the scalar is
negative, in which case the vector is also reversed. If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c \in \mathbb{R}$</math-renderer> and</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{v} = (v_1, v_2, \dots, v_n),
$$</math-renderer></p>
<p dir="auto">then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
c \mathbf{v} = (c v_1, c v_2, \dots, c v_n).
$$</math-renderer></p>
<p dir="auto">Example 1.2.2.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (3, -2)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c = -2$</math-renderer>. Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
c\mathbf{v} = -2(3, -2) = (-6, 4).
$$</math-renderer></p>
<p dir="auto">This corresponds to flipping the vector through the origin and doubling its length.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Linear Combinations</h3><a id="user-content-linear-combinations" aria-label="Permalink: Linear Combinations" href="#linear-combinations"></a></p>
<p dir="auto">The interaction of addition and scalar multiplication allows us to form <em>linear combinations</em>. A linear combination of
vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k$</math-renderer> is any vector of the form</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_k \mathbf{v}_k, \quad c_i \in \mathbb{R}.
$$</math-renderer></p>
<p dir="auto">Linear combinations are the mechanism by which we generate new vectors from existing ones. The span of a set of
vectors-the collection of all their linear combinations-will later lead us to the idea of a subspace.</p>
<p dir="auto">Example 1.2.3.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_1 = (1,0)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_2 = (0,1)$</math-renderer>. Then any vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(a,b)\in\mathbb{R}^2$</math-renderer> can be expressed as</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
a\mathbf{v}_1 + b\mathbf{v}_2.
$$</math-renderer></p>
<p dir="auto">Thus <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(0,1)$</math-renderer> form the basic building blocks of the plane.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Notation</h3><a id="user-content-notation-1" aria-label="Permalink: Notation" href="#notation-1"></a></p>
<ul dir="auto">
<li>Addition: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} + \mathbf{v}$</math-renderer> means component-wise addition.</li>
<li>Scalar multiplication: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c\mathbf{v}$</math-renderer> scales each entry of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer> by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c$</math-renderer>.</li>
<li>Linear combination: a sum of the form <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c_1 \mathbf{v}_1 + \cdots + c_k \mathbf{v}_k$</math-renderer>.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters" aria-label="Permalink: Why this matters" href="#why-this-matters"></a></p>
<p dir="auto">Vector addition and scalar multiplication are the defining operations of linear algebra. They give structure to vector
spaces, allow us to describe geometric phenomena like translation and scaling, and provide the foundation for solving
systems of equations. Everything that follows-basis, dimension, transformations-builds on these simple but profound
rules.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 1.2</h3><a id="user-content-exercises-12" aria-label="Permalink: Exercises 1.2" href="#exercises-12"></a></p>
<ol dir="auto">
<li>Compute <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} + \mathbf{v}$</math-renderer> where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (1,2,3)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (4, -1, 0)$</math-renderer>.</li>
<li>Find <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3\mathbf{v}$</math-renderer> where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (-2,5)$</math-renderer>. Sketch both vectors to illustrate the scaling.</li>
<li>Show that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(5,7)$</math-renderer> can be written as a linear combination of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(0,1)$</math-renderer>.</li>
<li>Write <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(4,4)$</math-renderer> as a linear combination of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,-1)$</math-renderer>.</li>
<li>Prove that if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$</math-renderer>,
then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(c+d)(\mathbf{u}+\mathbf{v}) = c\mathbf{u} + c\mathbf{v} + d\mathbf{u} + d\mathbf{v}$</math-renderer> for
scalars <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c,d \in \mathbb{R}$</math-renderer>.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">1.3 Dot Product, Norms, and Angles</h2><a id="user-content-13-dot-product-norms-and-angles" aria-label="Permalink: 1.3 Dot Product, Norms, and Angles" href="#13-dot-product-norms-and-angles"></a></p>
<p dir="auto">The dot product is the fundamental operation that links algebra and geometry in vector spaces. It allows us to measure
lengths, compute angles, and determine orthogonality. From this single definition flow the notions of <em>norm</em> and
<em>angle</em>, which give geometry to abstract vector spaces.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Dot Product</h3><a id="user-content-the-dot-product" aria-label="Permalink: The Dot Product" href="#the-dot-product"></a></p>
<p dir="auto">For two vectors in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>, the dot product (also called the inner product) is defined by</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} \cdot \mathbf{v} = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n.
$$</math-renderer></p>
<p dir="auto">Equivalently, in matrix notation:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} \cdot \mathbf{v} = \mathbf{u}^T \mathbf{v}.
$$</math-renderer></p>
<p dir="auto">Example 1.3.1.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (2, -1, 3)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (4, 0, -2)$</math-renderer>. Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} \cdot \mathbf{v} = 2\cdot 4 + (-1)\cdot 0 + 3\cdot (-2) = 8 - 6 = 2.
$$</math-renderer></p>
<p dir="auto">The dot product outputs a single scalar, not another vector.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Norms (Length of a Vector)</h3><a id="user-content-norms-length-of-a-vector" aria-label="Permalink: Norms (Length of a Vector)" href="#norms-length-of-a-vector"></a></p>
<p dir="auto">The <em>Euclidean norm</em> of a vector is the square root of its dot product with itself:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
|\mathbf{v}| = \sqrt{\mathbf{v} \cdot \mathbf{v}} = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}.
$$</math-renderer></p>
<p dir="auto">This generalizes the Pythagorean theorem to arbitrary dimensions.</p>
<p dir="auto">Example 1.3.2.
For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (3, 4)$</math-renderer>,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
|\mathbf{v}| = \sqrt{3^2 + 4^2} = \sqrt{25} = 5.
$$</math-renderer></p>
<p dir="auto">This is exactly the length of the vector as an arrow in the plane.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Angles Between Vectors</h3><a id="user-content-angles-between-vectors" aria-label="Permalink: Angles Between Vectors" href="#angles-between-vectors"></a></p>
<p dir="auto">The dot product also encodes the angle between two vectors. For nonzero vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}, \mathbf{v}$</math-renderer>,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} \cdot \mathbf{v} = |\mathbf{u}| , |\mathbf{v}| \cos \theta,
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\theta$</math-renderer> is the angle between them. Thus,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\cos \theta = \frac{\mathbf{u} \cdot \mathbf{v}}{|\mathbf{u}||\mathbf{v}|}.
$$</math-renderer></p>
<p dir="auto">Example 1.3.3.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (1,0)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (0,1)$</math-renderer>. Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} \cdot \mathbf{v} = 0, \quad |\mathbf{u}| = 1, \quad |\mathbf{v}| = 1.
$$</math-renderer></p>
<p dir="auto">Hence</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\cos \theta = \frac{0}{1\cdot 1} = 0 \quad \Rightarrow \quad \theta = \frac{\pi}{2}.
$$</math-renderer></p>
<p dir="auto">The vectors are perpendicular.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Orthogonality</h3><a id="user-content-orthogonality" aria-label="Permalink: Orthogonality" href="#orthogonality"></a></p>
<p dir="auto">Two vectors are said to be orthogonal if their dot product is zero:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} \cdot \mathbf{v} = 0.
$$</math-renderer></p>
<p dir="auto">Orthogonality generalizes the idea of perpendicularity from geometry to higher dimensions.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Notation</h3><a id="user-content-notation-2" aria-label="Permalink: Notation" href="#notation-2"></a></p>
<ul dir="auto">
<li>Dot product: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} \cdot \mathbf{v}$</math-renderer>.</li>
<li>Norm (length): <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\mathbf{v}|$</math-renderer>.</li>
<li>Orthogonality: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} \perp \mathbf{v}$</math-renderer> if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} \cdot \mathbf{v} = 0$</math-renderer>.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-1" aria-label="Permalink: Why this matters" href="#why-this-matters-1"></a></p>
<p dir="auto">The dot product turns vector spaces into geometric objects: vectors gain lengths, angles, and notions of
perpendicularity. This foundation will later support the study of orthogonal projections, Gram–Schmidt
orthogonalization, eigenvectors, and least squares problems.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 1.3</h3><a id="user-content-exercises-13" aria-label="Permalink: Exercises 1.3" href="#exercises-13"></a></p>
<ol dir="auto">
<li>Compute <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} \cdot \mathbf{v}$</math-renderer> for <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (1,2,3)$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (4,5,6)$</math-renderer>.</li>
<li>Find the norm of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (2, -2, 1)$</math-renderer>.</li>
<li>Determine whether <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (1,1,0)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (1,-1,2)$</math-renderer> are orthogonal.</li>
<li>Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (3,4)$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (4,3)$</math-renderer>. Compute the angle between them.</li>
<li>Prove that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\mathbf{u} + \mathbf{v}|^2 = |\mathbf{u}|^2 + |\mathbf{v}|^2 + 2\mathbf{u}\cdot \mathbf{v}$</math-renderer>. This
identity is the algebraic version of the Law of Cosines.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">1.4 Orthogonality</h2><a id="user-content-14-orthogonality" aria-label="Permalink: 1.4 Orthogonality" href="#14-orthogonality"></a></p>
<p dir="auto">Orthogonality captures the notion of perpendicularity in vector spaces. It is one of the most important geometric ideas
in linear algebra, allowing us to decompose vectors, define projections, and construct special bases with elegant
properties.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Definition</h3><a id="user-content-definition" aria-label="Permalink: Definition" href="#definition"></a></p>
<p dir="auto">Two vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$</math-renderer> are said to be orthogonal if their dot product is zero:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} \cdot \mathbf{v} = 0.
$$</math-renderer></p>
<p dir="auto">This condition ensures that the angle between them is <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\pi/2$</math-renderer> radians (90 degrees).</p>
<p dir="auto">Example 1.4.1.
In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>, the vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,2)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(2,-1)$</math-renderer> are orthogonal since</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
(1,2) \cdot (2,-1) = 1\cdot 2 + 2\cdot (-1) = 0.
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Orthogonal Sets</h3><a id="user-content-orthogonal-sets" aria-label="Permalink: Orthogonal Sets" href="#orthogonal-sets"></a></p>
<p dir="auto">A collection of vectors is called orthogonal if every distinct pair of vectors in the set is orthogonal. If, in
addition, each vector has norm 1, the set is called orthonormal.</p>
<p dir="auto">Example 1.4.2.
In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, the standard basis vectors</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{e}_1 = (1,0,0), \quad \mathbf{e}_2 = (0,1,0), \quad \mathbf{e}_3 = (0,0,1)
$$</math-renderer></p>
<p dir="auto">form an orthonormal set: each has length 1, and their dot products vanish when the indices differ.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Projections</h3><a id="user-content-projections" aria-label="Permalink: Projections" href="#projections"></a></p>
<p dir="auto">Orthogonality makes possible the decomposition of a vector into two components: one parallel to another vector, and one
orthogonal to it. Given a nonzero vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer> and any vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer>, the projection of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer>
onto <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer> is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{\mathbf{u} \cdot \mathbf{v}}{\mathbf{u} \cdot \mathbf{u}} \mathbf{u}.
$$</math-renderer></p>
<p dir="auto">The difference</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})
$$</math-renderer></p>
<p dir="auto">is orthogonal to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer>. Thus every vector can be decomposed uniquely into a parallel and perpendicular part with
respect to another vector.</p>
<p dir="auto">Example 1.4.3.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (1,0)$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (2,3)$</math-renderer>. Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{(1,0)\cdot(2,3)}{(1,0)\cdot(1,0)} (1,0)
= \frac{2}{1}(1,0) = (2,0).
$$</math-renderer></p>
<p dir="auto">Thus</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{v} = (2,3) = (2,0) + (0,3),
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(2,0)$</math-renderer> is parallel to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(0,3)$</math-renderer> is orthogonal to it.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Orthogonal Decomposition</h3><a id="user-content-orthogonal-decomposition" aria-label="Permalink: Orthogonal Decomposition" href="#orthogonal-decomposition"></a></p>
<p dir="auto">In general, if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} \neq \mathbf{0}$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} \in \mathbb{R}^n$</math-renderer>, then</p>
<p dir="auto">$$
\mathbf{v} = \text{proj}<em>{\mathbf{u}}(\mathbf{v}) + \big(\mathbf{v} - \text{proj}</em>{\mathbf{u}}(\mathbf{v})\big),
$$</p>
<p dir="auto">where the first term is parallel to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer> and the second term is orthogonal. This decomposition underlies methods
such as least squares approximation and the Gram–Schmidt process.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Notation</h3><a id="user-content-notation-3" aria-label="Permalink: Notation" href="#notation-3"></a></p>
<ul dir="auto">
<li>
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} \perp \mathbf{v}$</math-renderer>: vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer> are orthogonal.</li>
<li>An orthogonal set: vectors pairwise orthogonal.</li>
<li>An orthonormal set: pairwise orthogonal, each of norm 1.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-2" aria-label="Permalink: Why this matters" href="#why-this-matters-2"></a></p>
<p dir="auto">Orthogonality gives structure to vector spaces. It provides a way to separate independent directions cleanly, simplify
computations, and minimize errors in approximations. Many powerful algorithms in numerical linear algebra and data
science (QR decomposition, least squares regression, PCA) rely on orthogonality.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 1.4</h3><a id="user-content-exercises-14" aria-label="Permalink: Exercises 1.4" href="#exercises-14"></a></p>
<ol dir="auto">
<li>Verify that the vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,2,2)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(2,0,-1)$</math-renderer> are orthogonal.</li>
<li>Find the projection of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(3,4)$</math-renderer> onto <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1)$</math-renderer>.</li>
<li>Show that any two distinct standard basis vectors in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer> are orthogonal.</li>
<li>Decompose <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(5,2)$</math-renderer> into components parallel and orthogonal to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(2,1)$</math-renderer>.</li>
<li>Prove that if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}, \mathbf{v}$</math-renderer> are orthogonal and nonzero,
then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v}) = 0$</math-renderer>.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Chapter 2. Matrices</h2><a id="user-content-chapter-2-matrices" aria-label="Permalink: Chapter 2. Matrices" href="#chapter-2-matrices"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">2.1 Definition and Notation</h2><a id="user-content-21-definition-and-notation" aria-label="Permalink: 2.1 Definition and Notation" href="#21-definition-and-notation"></a></p>
<p dir="auto">Matrices are the central objects of linear algebra, providing a compact way to represent and manipulate linear
transformations, systems of equations, and structured data. A matrix is a rectangular array of numbers arranged in rows
and columns.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Formal Definition</h3><a id="user-content-formal-definition" aria-label="Permalink: Formal Definition" href="#formal-definition"></a></p>
<p dir="auto">An <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$m \times n$</math-renderer> matrix is an array with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$m$</math-renderer> rows and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> columns, written</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A =
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Each entry <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$a_{ij}$</math-renderer> is a scalar, located in the <em>i</em>-th row and <em>j</em>-th column. The size (or dimension) of the matrix is
denoted by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$m \times n$</math-renderer>.</p>
<ul dir="auto">
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$m = n$</math-renderer>, the matrix is square.</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$m = 1$</math-renderer>, the matrix is a row vector.</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n = 1$</math-renderer>, the matrix is a column vector.</li>
</ul>
<p dir="auto">Thus, vectors are simply special cases of matrices.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Examples</h3><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto">Example 2.1.1. A <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$2 \times 3$</math-renderer> matrix:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
1 &amp; -2 &amp; 4 \\
0 &amp; 3 &amp; 5
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Here, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$a_{12} = -2$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$a_{23} = 5$</math-renderer>, and the matrix has 2 rows, 3 columns.</p>
<p dir="auto">Example 2.1.2. A <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3 \times 3$</math-renderer> square matrix:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
B = \begin{bmatrix}
2 &amp; 0 &amp; 1 \\
-1 &amp; 3 &amp; 4 \\
0 &amp; 5 &amp; -2
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">This will later serve as the representation of a linear transformation on <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Indexing and Notation</h3><a id="user-content-indexing-and-notation" aria-label="Permalink: Indexing and Notation" href="#indexing-and-notation"></a></p>
<ul dir="auto">
<li>Matrices are denoted by uppercase bold letters: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A, B, C$</math-renderer>.</li>
<li>Entries are written as <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$a_{ij}$</math-renderer>, with the row index first, column index second.</li>
<li>The set of all real <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$m \times n$</math-renderer> matrices is denoted <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^{m \times n}$</math-renderer>.</li>
</ul>
<p dir="auto">Thus, a matrix is a function <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A: {1,\dots,m} \times {1,\dots,n} \to \mathbb{R}$</math-renderer>, assigning a scalar to each row-column
position.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-3" aria-label="Permalink: Why this matters" href="#why-this-matters-3"></a></p>
<p dir="auto">Matrices generalize vectors and give us a language for describing linear operations systematically. They encode systems
of equations, rotations, projections, and transformations of data. With matrices, algebra and geometry come together: a
single compact object can represent both numerical data and functional rules.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 2.1</h3><a id="user-content-exercises-21" aria-label="Permalink: Exercises 2.1" href="#exercises-21"></a></p>
<ol dir="auto">
<li>Write a <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3 \times 2$</math-renderer> matrix of your choice and identify its entries <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$a_{ij}$</math-renderer>.</li>
<li>Is every vector a matrix? Is every matrix a vector? Explain.</li>
<li>Which of the following are square
matrices: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{4\times4}$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$B \in \mathbb{R}^{3\times5}$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C \in \mathbb{R}^{1\times1}$</math-renderer>?</li>
<li>Let $D = \begin{bmatrix} 1 &amp; 0 \ 0 &amp; 1 \end{bmatrix}$. What kind of matrix is this?</li>
<li>Consider the matrix $E = \begin{bmatrix} a &amp; b \ c &amp; d \end{bmatrix}$. Express <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$e_{11}, e_{12}, e_{21}, e_{22}$</math-renderer>
explicitly.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">2.2 Matrix Addition and Multiplication</h2><a id="user-content-22-matrix-addition-and-multiplication" aria-label="Permalink: 2.2 Matrix Addition and Multiplication" href="#22-matrix-addition-and-multiplication"></a></p>
<p dir="auto">Once matrices are defined, the next step is to understand how they combine. Just as vectors gain meaning through
addition and scalar multiplication, matrices become powerful through two operations: addition and multiplication.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Matrix Addition</h3><a id="user-content-matrix-addition" aria-label="Permalink: Matrix Addition" href="#matrix-addition"></a></p>
<p dir="auto">Two matrices of the same size are added by adding corresponding entries. If</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = [a_{ij}] \in \mathbb{R}^{m \times n}, \quad
B = [b_{ij}] \in \mathbb{R}^{m \times n},
$$</math-renderer></p>
<p dir="auto">then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A + B = [a_{ij} + b_{ij}] \in \mathbb{R}^{m \times n}.
$$</math-renderer></p>
<p dir="auto">Example 2.2.1.
Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}, \quad
B = \begin{bmatrix}
-1 &amp; 0 \\
5 &amp; 2
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">$$
A + B = \begin{bmatrix}
1 + (-1) &amp; 2 + 0 \
3 + 5 &amp; 4 + 2
\end{bmatrix}</h2><a id="user-content-a--b--beginbmatrix1---1--2--0-3--5--4--2endbmatrix" aria-label="Permalink: $$
A + B = \begin{bmatrix}
1 + (-1) &amp; 2 + 0 \
3 + 5 &amp; 4 + 2
\end{bmatrix}" href="#a--b--beginbmatrix1---1--2--0-3--5--4--2endbmatrix"></a></p>
<p dir="auto">\begin{bmatrix}
0 &amp; 2 \
8 &amp; 6
\end{bmatrix}.
$$</p>
<p dir="auto">Matrix addition is commutative (<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A+B = B+A$</math-renderer>) and associative ($(A+B)+C = A+(B+C)$). The zero matrix, with all entries 0,
acts as the additive identity.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Scalar Multiplication</h3><a id="user-content-scalar-multiplication-1" aria-label="Permalink: Scalar Multiplication" href="#scalar-multiplication-1"></a></p>
<p dir="auto">For a scalar <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c \in \mathbb{R}$</math-renderer> and a matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A = [[a_{ij}]$</math-renderer>, we define</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
cA = [c \cdot a_{ij}].
$$</math-renderer></p>
<p dir="auto">This stretches or shrinks all entries of the matrix uniformly.</p>
<p dir="auto">Example 2.2.2.
If</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
2 &amp; -1 \\
0 &amp; 3
\end{bmatrix}, \quad c = -2,
$$</math-renderer></p>
<p dir="auto">then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
cA = \begin{bmatrix}
-4 &amp; 2 \\
0 &amp; -6
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Matrix Multiplication</h3><a id="user-content-matrix-multiplication" aria-label="Permalink: Matrix Multiplication" href="#matrix-multiplication"></a></p>
<p dir="auto">The defining operation of matrices is multiplication. If</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A \in \mathbb{R}^{m \times n}, \quad B \in \mathbb{R}^{n \times p},
$$</math-renderer></p>
<p dir="auto">then their product is the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$m \times p$</math-renderer> matrix</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
AB = C = [c_{ij}], \quad c_{ij} = \sum_{k=1}^n a_{ik} b_{kj}.
$$</math-renderer></p>
<p dir="auto">Thus, the entry in the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$i$</math-renderer>-th row and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$j$</math-renderer>-th column of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$AB$</math-renderer> is the dot product of the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$i$</math-renderer>-th row of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> with the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$j$</math-renderer>-th
column of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$B$</math-renderer>.</p>
<p dir="auto">Example 2.2.3.
Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
1 &amp; 2 \\
0 &amp; 3
\end{bmatrix}, \quad
B = \begin{bmatrix}
4 &amp; -1 \\
2 &amp; 5
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">$$
AB = \begin{bmatrix}
1\cdot4 + 2\cdot2 &amp; 1\cdot(-1) + 2\cdot5 \
0\cdot4 + 3\cdot2 &amp; 0\cdot(-1) + 3\cdot5
\end{bmatrix}</h2><a id="user-content-ab--beginbmatrix1cdot4--2cdot2--1cdot-1--2cdot5-0cdot4--3cdot2--0cdot-1--3cdot5endbmatrix" aria-label="Permalink: $$
AB = \begin{bmatrix}
1\cdot4 + 2\cdot2 &amp; 1\cdot(-1) + 2\cdot5 \
0\cdot4 + 3\cdot2 &amp; 0\cdot(-1) + 3\cdot5
\end{bmatrix}" href="#ab--beginbmatrix1cdot4--2cdot2--1cdot-1--2cdot5-0cdot4--3cdot2--0cdot-1--3cdot5endbmatrix"></a></p>
<p dir="auto">\begin{bmatrix}
8 &amp; 9 \
6 &amp; 15
\end{bmatrix}.
$$</p>
<p dir="auto">Notice that matrix multiplication is not commutative in general: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$AB \neq BA$</math-renderer>. Sometimes <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$BA$</math-renderer> may not even be defined if
dimensions do not align.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Meaning</h3><a id="user-content-geometric-meaning" aria-label="Permalink: Geometric Meaning" href="#geometric-meaning"></a></p>
<p dir="auto">Matrix multiplication corresponds to the composition of linear transformations. If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> transforms vectors
in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$B$</math-renderer> transforms vectors in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^p$</math-renderer>, then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$AB$</math-renderer> represents applying <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$B$</math-renderer> first, then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>. This
makes matrices the algebraic language of transformations.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Notation</h3><a id="user-content-notation-4" aria-label="Permalink: Notation" href="#notation-4"></a></p>
<ul dir="auto">
<li>Matrix sum: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A+B$</math-renderer>.</li>
<li>Scalar multiple: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$cA$</math-renderer>.</li>
<li>Product: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$AB$</math-renderer>, defined only when the number of columns of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> equals the number of rows of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$B$</math-renderer>.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-4" aria-label="Permalink: Why this matters" href="#why-this-matters-4"></a></p>
<p dir="auto">Matrix multiplication is the core mechanism of linear algebra: it encodes how transformations combine, how systems of
equations are solved, and how data flows in modern algorithms. Addition and scalar multiplication make matrices into a
vector space, while multiplication gives them an algebraic structure rich enough to model geometry, computation, and
networks.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 2.2</h3><a id="user-content-exercises-22" aria-label="Permalink: Exercises 2.2" href="#exercises-22"></a></p>
<ol dir="auto">
<li>Compute <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A+B$</math-renderer> for</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; 3 \ -1 &amp; 0 \end{bmatrix}, \quad
B = \begin{bmatrix} 4 &amp; -2 \ 5 &amp; 7 \end{bmatrix}.
$$</math-renderer></p>
<ol start="2" dir="auto">
<li>Find <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3A$</math-renderer> where</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 1 &amp; -4 \ 2 &amp; 6 \end{bmatrix}.
$$</math-renderer></p>
<ol start="3" dir="auto">
<li>Multiply</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 1 &amp; 0 &amp; 2 \ -1 &amp; 3 &amp; 1 \end{bmatrix}, \quad
B = \begin{bmatrix} 2 &amp; 1 \ 0 &amp; -1 \ 3 &amp; 4 \end{bmatrix}.
$$</math-renderer></p>
<ol start="4" dir="auto">
<li>Verify with an explicit example that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$AB \neq BA$</math-renderer>.</li>
<li>Prove that matrix multiplication is distributive: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A(B+C) = AB + AC$</math-renderer>.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">2.3 Transpose and Inverse</h2><a id="user-content-23-transpose-and-inverse" aria-label="Permalink: 2.3 Transpose and Inverse" href="#23-transpose-and-inverse"></a></p>
<p dir="auto">Two special operations on matrices-the transpose and the inverse-give rise to deep algebraic and geometric properties.
The transpose rearranges a matrix by flipping it across its main diagonal, while the inverse, when it exists, acts as
the undo operation for matrix multiplication.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Transpose</h3><a id="user-content-the-transpose" aria-label="Permalink: The Transpose" href="#the-transpose"></a></p>
<p dir="auto">The transpose of an <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$m \times n$</math-renderer> matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A = [a_{ij}]$</math-renderer> is the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n \times m$</math-renderer> matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^T = [a_{ji}]$</math-renderer>, obtained by swapping
rows and columns.</p>
<p dir="auto">Formally,</p>
<p dir="auto">$$
(A^T)<em>{ij} = a</em>{ji}.
$$</p>
<p dir="auto">Example 2.3.1.
If</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
1 &amp; 4 &amp; -2 \\
0 &amp; 3 &amp; 5
\end{bmatrix},
$$</math-renderer></p>
<p dir="auto">then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A^T = \begin{bmatrix}
1 &amp; 0 \\
4 &amp; 3 \\
-2 &amp; 5
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Properties of the Transpose.</p>
<ol dir="auto">
<li>$ (A^T)^T = A$.</li>
<li>$ (A+B)^T = A^T + B^T$.</li>
<li>$ (cA)^T = cA^T$, for scalar <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c$</math-renderer>.</li>
<li>$ (AB)^T = B^T A^T$.</li>
</ol>
<p dir="auto">The last rule is crucial: the order reverses.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Inverse</h3><a id="user-content-the-inverse" aria-label="Permalink: The Inverse" href="#the-inverse"></a></p>
<p dir="auto">A square matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{n \times n}$</math-renderer> is said to be invertible (or nonsingular) if there exists another
matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^{-1}$</math-renderer> such that</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
AA^{-1} = A^{-1}A = I_n,
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$I_n$</math-renderer> is the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n \times n$</math-renderer> identity matrix. In this case, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^{-1}$</math-renderer> is called the inverse of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>.</p>
<p dir="auto">Not every matrix is invertible. A necessary condition is that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) \neq 0$</math-renderer>, a fact that will be developed in Chapter
6.</p>
<p dir="auto">Example 2.3.2.
Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Its determinant is <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) = (1)(4) - (2)(3) = -2 \neq 0$</math-renderer>. The inverse is</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">$$
A^{-1} = \frac{1}{\det(A)} \begin{bmatrix}
4 &amp; -2 \
-3 &amp; 1
\end{bmatrix}</h2><a id="user-content-a-1--frac1deta-beginbmatrix4---2--3--1endbmatrix" aria-label="Permalink: $$
A^{-1} = \frac{1}{\det(A)} \begin{bmatrix}
4 &amp; -2 \
-3 &amp; 1
\end{bmatrix}" href="#a-1--frac1deta-beginbmatrix4---2--3--1endbmatrix"></a></p>
<p dir="auto">\begin{bmatrix}
-2 &amp; 1 \
1.5 &amp; -0.5
\end{bmatrix}.
$$</p>
<p dir="auto">Verification:</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">$$
AA^{-1} = \begin{bmatrix}
1 &amp; 2 \
3 &amp; 4
\end{bmatrix}
\begin{bmatrix}
-2 &amp; 1 \
1.5 &amp; -0.5
\end{bmatrix}</h2><a id="user-content-aa-1--beginbmatrix1--2-3--4endbmatrixbeginbmatrix-2--1-15---05endbmatrix" aria-label="Permalink: $$
AA^{-1} = \begin{bmatrix}
1 &amp; 2 \
3 &amp; 4
\end{bmatrix}
\begin{bmatrix}
-2 &amp; 1 \
1.5 &amp; -0.5
\end{bmatrix}" href="#aa-1--beginbmatrix1--2-3--4endbmatrixbeginbmatrix-2--1-15---05endbmatrix"></a></p>
<p dir="auto">\begin{bmatrix}
1 &amp; 0 \
0 &amp; 1
\end{bmatrix}.
$$</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Meaning</h3><a id="user-content-geometric-meaning-1" aria-label="Permalink: Geometric Meaning" href="#geometric-meaning-1"></a></p>
<ul dir="auto">
<li>The transpose corresponds to reflecting a linear transformation across the diagonal. For vectors, it switches between
row and column forms.</li>
<li>The inverse, when it exists, corresponds to reversing a linear transformation. For example, if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> scales and rotates
vectors, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^{-1}$</math-renderer> rescales and rotates them back.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Notation</h3><a id="user-content-notation-5" aria-label="Permalink: Notation" href="#notation-5"></a></p>
<ul dir="auto">
<li>Transpose: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^T$</math-renderer>.</li>
<li>Inverse: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^{-1}$</math-renderer>, defined only for invertible square matrices.</li>
<li>Identity: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$I_n$</math-renderer>, acts as the multiplicative identity.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-5" aria-label="Permalink: Why this matters" href="#why-this-matters-5"></a></p>
<p dir="auto">The transpose allows us to define symmetric and orthogonal matrices, central to geometry and numerical methods. The
inverse underlies the solution of linear systems, encoding the idea of undoing a transformation. Together, these
operations set the stage for determinants, eigenvalues, and orthogonalization.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 2.3</h3><a id="user-content-exercises-23" aria-label="Permalink: Exercises 2.3" href="#exercises-23"></a></p>
<ol dir="auto">
<li>Compute the transpose of</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; -1 &amp; 3 \ 0 &amp; 4 &amp; 5 \end{bmatrix}.
$$</math-renderer></p>
<ol start="2" dir="auto">
<li>Verify that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(AB)^T = B^T A^T$</math-renderer> for</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 1 &amp; 2 \ 0 &amp; 1 \end{bmatrix}, \quad
B = \begin{bmatrix} 3 &amp; 4 \ 5 &amp; 6 \end{bmatrix}.
$$</math-renderer></p>
<ol start="3" dir="auto">
<li>Determine whether</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
C = \begin{bmatrix} 2 &amp; 1 \ 4 &amp; 2 \end{bmatrix}
$$</math-renderer></p>
<p dir="auto">is invertible. If so, find <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C^{-1}$</math-renderer>.</p>
<ol start="4" dir="auto">
<li>Find the inverse of</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
D = \begin{bmatrix} 0 &amp; 1 \ -1 &amp; 0 \end{bmatrix},
$$</math-renderer></p>
<p dir="auto">and explain its geometric action on vectors in the plane.</p>
<ol start="5" dir="auto">
<li>Prove that if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is invertible, then so is <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^T$</math-renderer>, and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(A^T)^{-1} = (A^{-1})^T$</math-renderer>.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">2.4 Special Matrices</h2><a id="user-content-24-special-matrices" aria-label="Permalink: 2.4 Special Matrices" href="#24-special-matrices"></a></p>
<p dir="auto">Certain matrices occur so frequently in theory and applications that they are given special names. Recognizing their
properties allows us to simplify computations and understand the structure of linear transformations more clearly.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Identity Matrix</h3><a id="user-content-the-identity-matrix" aria-label="Permalink: The Identity Matrix" href="#the-identity-matrix"></a></p>
<p dir="auto">The identity matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$I_n$</math-renderer> is the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n \times n$</math-renderer> matrix with ones on the diagonal and zeros elsewhere:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
I_n = \begin{bmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 1 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 1
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">It acts as the multiplicative identity:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
AI_n = I_nA = A, \quad \text{for all } A \in \mathbb{R}^{n \times n}.
$$</math-renderer></p>
<p dir="auto">Geometrically, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$I_n$</math-renderer> represents the transformation that leaves every vector unchanged.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Diagonal Matrices</h3><a id="user-content-diagonal-matrices" aria-label="Permalink: Diagonal Matrices" href="#diagonal-matrices"></a></p>
<p dir="auto">A diagonal matrix has all off-diagonal entries zero:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
D = \begin{bmatrix}
d_{11} &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; d_{22} &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; d_{nn}
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Multiplication by a diagonal matrix scales each coordinate independently:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
D\mathbf{x} = (d_{11}x_1, d_{22}x_2, \dots, d_{nn}x_n).
$$</math-renderer></p>
<p dir="auto">Example 2.4.1.
Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
D = \begin{bmatrix} 2 &amp; 0 &amp; 0 \ 0 &amp; 3 &amp; 0 \ 0 &amp; 0 &amp; -1 \end{bmatrix}, \quad
\mathbf{x} = \begin{bmatrix} 1 \ 4 \ -2 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
D\mathbf{x} = \begin{bmatrix} 2 \ 12 \ 2 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Permutation Matrices</h3><a id="user-content-permutation-matrices" aria-label="Permalink: Permutation Matrices" href="#permutation-matrices"></a></p>
<p dir="auto">A permutation matrix is obtained by permuting the rows of the identity matrix. Multiplying a vector by a permutation
matrix reorders its coordinates.</p>
<p dir="auto">Example 2.4.2.
Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
P = \begin{bmatrix}
0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
P\begin{bmatrix} a \ b \ c \end{bmatrix} =
\begin{bmatrix} b \ a \ c \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Thus, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer> swaps the first two coordinates.</p>
<p dir="auto">Permutation matrices are always invertible; their inverses are simply their transposes.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Symmetric and Skew-Symmetric Matrices</h3><a id="user-content-symmetric-and-skew-symmetric-matrices" aria-label="Permalink: Symmetric and Skew-Symmetric Matrices" href="#symmetric-and-skew-symmetric-matrices"></a></p>
<p dir="auto">A matrix is symmetric if</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A^T = A,
$$</math-renderer></p>
<p dir="auto">and skew-symmetric if</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A^T = -A.
$$</math-renderer></p>
<p dir="auto">Symmetric matrices appear in quadratic forms and optimization, while skew-symmetric matrices describe rotations and
cross products in geometry.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Orthogonal Matrices</h3><a id="user-content-orthogonal-matrices" aria-label="Permalink: Orthogonal Matrices" href="#orthogonal-matrices"></a></p>
<p dir="auto">A square matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$Q$</math-renderer> is orthogonal if</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q^T Q = QQ^T = I.
$$</math-renderer></p>
<p dir="auto">Equivalently, the rows (and columns) of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$Q$</math-renderer> form an orthonormal set. Orthogonal matrices preserve lengths and angles;
they represent rotations and reflections.</p>
<p dir="auto">Example 2.4.3.
The rotation matrix in the plane:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
R(\theta) = \begin{bmatrix}
\cos\theta &amp; -\sin\theta \\
\sin\theta &amp; \cos\theta
\end{bmatrix}
$$</math-renderer></p>
<p dir="auto">is orthogonal, since</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
R(\theta)^T R(\theta) = I_2.
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-6" aria-label="Permalink: Why this matters" href="#why-this-matters-6"></a></p>
<p dir="auto">Special matrices serve as the building blocks of linear algebra. Identity matrices define the neutral element, diagonal
matrices simplify computations, permutation matrices reorder data, symmetric and orthogonal matrices describe
fundamental geometric structures. Much of modern applied mathematics reduces complex problems to operations involving
these simple forms.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 2.4</h3><a id="user-content-exercises-24" aria-label="Permalink: Exercises 2.4" href="#exercises-24"></a></p>
<ol dir="auto">
<li>Show that the product of two diagonal matrices is diagonal, and compute an example.</li>
<li>Find the permutation matrix that cycles <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(a,b,c)$</math-renderer> into <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(b,c,a)$</math-renderer>.</li>
<li>Prove that every permutation matrix is invertible and its inverse is its transpose.</li>
<li>Verify that</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q = \begin{bmatrix} 0 &amp; 1 \ -1 &amp; 0 \end{bmatrix}
$$</math-renderer></p>
<p dir="auto">is orthogonal. What geometric transformation does it represent?
5. Determine whether</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; 3 \ 3 &amp; 2 \end{bmatrix}, \quad
B = \begin{bmatrix} 0 &amp; 5 \ -5 &amp; 0 \end{bmatrix}
$$</math-renderer></p>
<p dir="auto">are symmetric, skew-symmetric, or neither.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Chapter 3. Systems of Linear Equations</h2><a id="user-content-chapter-3-systems-of-linear-equations" aria-label="Permalink: Chapter 3. Systems of Linear Equations" href="#chapter-3-systems-of-linear-equations"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">3.1 Linear Systems and Solutions</h2><a id="user-content-31-linear-systems-and-solutions" aria-label="Permalink: 3.1 Linear Systems and Solutions" href="#31-linear-systems-and-solutions"></a></p>
<p dir="auto">One of the central motivations for linear algebra is solving systems of linear equations. These systems arise naturally
in science, engineering, and data analysis whenever multiple constraints interact. Matrices provide a compact language
for expressing and solving them.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Linear Systems</h3><a id="user-content-linear-systems" aria-label="Permalink: Linear Systems" href="#linear-systems"></a></p>
<p dir="auto">A linear system consists of equations where each unknown appears only to the first power and with no products between
variables. A general system of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$m$</math-renderer> equations in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> unknowns can be written as:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{aligned}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &amp;= b_1, \\
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &amp;= b_2, \\
&amp;\vdots \\
a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &amp;= b_m.
\end{aligned}
$$</math-renderer></p>
<p dir="auto">Here the coefficients <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$a_{ij}$</math-renderer> and constants <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$b_i$</math-renderer> are scalars, and the unknowns are <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$x_1, x_2, \dots, x_n$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Matrix Form</h3><a id="user-content-matrix-form" aria-label="Permalink: Matrix Form" href="#matrix-form"></a></p>
<p dir="auto">The system can be expressed compactly as:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A\mathbf{x} = \mathbf{b},
$$</math-renderer></p>
<p dir="auto">where</p>
<ul dir="auto">
<li>
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{m \times n}$</math-renderer> is the coefficient matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$[a_{ij}]$</math-renderer>,</li>
<li>
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x} \in \mathbb{R}^n$</math-renderer> is the column vector of unknowns,</li>
<li>
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{b} \in \mathbb{R}^m$</math-renderer> is the column vector of constants.</li>
</ul>
<p dir="auto">This formulation turns the problem of solving equations into analyzing the action of a matrix.</p>
<p dir="auto">Example 3.1.1.
The system</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{cases}
x + 2y = 5, \\
3x - y = 4
\end{cases}
$$</math-renderer></p>
<p dir="auto">can be written as</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">$$
\begin{bmatrix} 1 &amp; 2 \ 3 &amp; -1 \end{bmatrix}
\begin{bmatrix} x \ y \end{bmatrix}</h2><a id="user-content-beginbmatrix-1--2--3---1-endbmatrixbeginbmatrix-x--y-endbmatrix" aria-label="Permalink: $$
\begin{bmatrix} 1 &amp; 2 \ 3 &amp; -1 \end{bmatrix}
\begin{bmatrix} x \ y \end{bmatrix}" href="#beginbmatrix-1--2--3---1-endbmatrixbeginbmatrix-x--y-endbmatrix"></a></p>
<p dir="auto">\begin{bmatrix} 5 \ 4 \end{bmatrix}.
$$</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Types of Solutions</h3><a id="user-content-types-of-solutions" aria-label="Permalink: Types of Solutions" href="#types-of-solutions"></a></p>
<p dir="auto">A linear system may have:</p>
<ol dir="auto">
<li>
<p dir="auto">No solution (inconsistent): The equations conflict.
Example:
$
\begin{cases}
x + y = 1 \
x + y = 2
\end{cases}
$
has no solution.</p>
</li>
<li>
<p dir="auto">Exactly one solution (unique): The system’s equations intersect at a single point.
Example: The above system with coefficient matrix $
\begin{bmatrix} 1 &amp; 2 \ 3 &amp; -1 \end{bmatrix}
$ has a unique solution.</p>
</li>
<li>
<p dir="auto">Infinitely many solutions: The equations describe overlapping constraints (e.g., multiple equations representing the
same line or plane).</p>
</li>
</ol>
<p dir="auto">The nature of the solution depends on the rank of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> and its relation to the augmented matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(A|\mathbf{b})$</math-renderer>, which
we will study later.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-1" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-1"></a></p>
<ul dir="auto">
<li>In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>, each linear equation represents a line. Solving a system means finding intersection points of
lines.</li>
<li>In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, each equation represents a plane. A system may have no solution (parallel planes), one solution (a
unique intersection point), or infinitely many (a line of intersection).</li>
<li>In higher dimensions, the picture generalizes: solutions form intersections of hyperplanes.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-7" aria-label="Permalink: Why this matters" href="#why-this-matters-7"></a></p>
<p dir="auto">Linear systems are the practical foundation of linear algebra. They appear in balancing chemical reactions, circuit
analysis, least-squares regression, optimization, and computer graphics. Understanding how to represent and classify
their solutions is the first step toward systematic solution methods like Gaussian elimination.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 3.1</h3><a id="user-content-exercises-31" aria-label="Permalink: Exercises 3.1" href="#exercises-31"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Write the following system in matrix form:
$
\begin{cases}
2x + 3y - z = 7, \
x - y + 4z = 1, \
3x + 2y + z = 5
\end{cases}
$</p>
</li>
<li>
<p dir="auto">Determine whether the system
$
\begin{cases}
x + y = 1, \
2x + 2y = 2
\end{cases}
$
has no solution, one solution, or infinitely many solutions.</p>
</li>
<li>
<p dir="auto">Geometrically interpret the system
$
\begin{cases}
x + y = 3, \
x - y = 1
\end{cases}
$
in the plane.</p>
</li>
<li>
<p dir="auto">Solve the system
$
\begin{cases}
2x + y = 1, \
x - y = 4
\end{cases}
$
and check your solution.</p>
</li>
<li>
<p dir="auto">In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, describe the solution set of
$
\begin{cases}
x + y + z = 0, \
2x + 2y + 2z = 0
\end{cases}
$.
What geometric object does it represent?</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">3.2 Gaussian Elimination</h2><a id="user-content-32-gaussian-elimination" aria-label="Permalink: 3.2 Gaussian Elimination" href="#32-gaussian-elimination"></a></p>
<p dir="auto">To solve linear systems efficiently, we use Gaussian elimination: a systematic method of transforming a system into a
simpler equivalent one whose solutions are easier to see. The method relies on elementary row operations that preserve
the solution set.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Elementary Row Operations</h3><a id="user-content-elementary-row-operations" aria-label="Permalink: Elementary Row Operations" href="#elementary-row-operations"></a></p>
<p dir="auto">On an augmented matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(A|\mathbf{b})$</math-renderer>, we are allowed three operations:</p>
<ol dir="auto">
<li>Row swapping: interchange two rows.</li>
<li>Row scaling: multiply a row by a nonzero scalar.</li>
<li>Row replacement: replace one row by itself plus a multiple of another row.</li>
</ol>
<p dir="auto">These operations correspond to re-expressing equations in different but equivalent forms.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Row Echelon Form</h3><a id="user-content-row-echelon-form" aria-label="Permalink: Row Echelon Form" href="#row-echelon-form"></a></p>
<p dir="auto">A matrix is in row echelon form (REF) if:</p>
<ol dir="auto">
<li>All nonzero rows are above any zero rows.</li>
<li>Each leading entry (the first nonzero number from the left in a row) is to the right of the leading entry in the row
above.</li>
<li>All entries below a leading entry are zero.</li>
</ol>
<p dir="auto">Further, if each leading entry is 1 and is the only nonzero entry in its column, the matrix is in reduced row echelon
form (RREF).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Algorithm of Gaussian Elimination</h3><a id="user-content-algorithm-of-gaussian-elimination" aria-label="Permalink: Algorithm of Gaussian Elimination" href="#algorithm-of-gaussian-elimination"></a></p>
<ol dir="auto">
<li>Write the augmented matrix for the system.</li>
<li>Use row operations to create zeros below each pivot (the leading entry in a row).</li>
<li>Continue column by column until the matrix is in echelon form.</li>
<li>Solve by back substitution: starting from the last pivot equation and working upward.</li>
</ol>
<p dir="auto">If we continue to RREF, the solution can be read off directly.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example</h3><a id="user-content-example" aria-label="Permalink: Example" href="#example"></a></p>
<p dir="auto">Example 3.2.1. Solve</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{cases}
x + 2y - z = 3, \\
2x + y + z = 7, \\
3x - y + 2z = 4.
\end{cases}
$$</math-renderer></p>
<p dir="auto">Step 1. Augmented matrix</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\left[\begin{array}{ccc|c}
1 &amp; 2 &amp; -1 &amp; 3 \\
2 &amp; 1 &amp; 1 &amp; 7 \\
3 &amp; -1 &amp; 2 &amp; 4
\end{array}\right].
$$</math-renderer></p>
<p dir="auto">Step 2. Eliminate below the first pivot</p>
<p dir="auto">Subtract 2 times row 1 from row 2, and 3 times row 1 from row 3:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\left[\begin{array}{ccc|c}
1 &amp; 2 &amp; -1 &amp; 3 \\
0 &amp; -3 &amp; 3 &amp; 1 \\
0 &amp; -7 &amp; 5 &amp; -5
\end{array}\right].
$$</math-renderer></p>
<p dir="auto">Step 3. Pivot in column 2</p>
<p dir="auto">Divide row 2 by -3:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\left[\begin{array}{ccc|c}
1 &amp; 2 &amp; -1 &amp; 3 \\
0 &amp; 1 &amp; -1 &amp; -\tfrac{1}{3} \\
0 &amp; -7 &amp; 5 &amp; -5
\end{array}\right].
$$</math-renderer></p>
<p dir="auto">Add 7 times row 2 to row 3:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\left[\begin{array}{ccc|c}
1 &amp; 2 &amp; -1 &amp; 3 \\
0 &amp; 1 &amp; -1 &amp; -\tfrac{1}{3} \\
0 &amp; 0 &amp; -2 &amp; -\tfrac{22}{3}
\end{array}\right].
$$</math-renderer></p>
<p dir="auto">Step 4. Pivot in column 3</p>
<p dir="auto">Divide row 3 by -2:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\left[\begin{array}{ccc|c}
1 &amp; 2 &amp; -1 &amp; 3 \\
0 &amp; 1 &amp; -1 &amp; -\tfrac{1}{3} \\
0 &amp; 0 &amp; 1 &amp; \tfrac{11}{3}
\end{array}\right].
$$</math-renderer></p>
<p dir="auto">Step 5. Back substitution</p>
<p dir="auto">From the last row:
$
z = \tfrac{11}{3}.
$</p>
<p dir="auto">Second row:
$
y - z = -\tfrac{1}{3} \implies y = -\tfrac{1}{3} + \tfrac{11}{3} = \tfrac{10}{3}.
$</p>
<p dir="auto">First row:
$
x + 2y - z = 3 \implies x + 2\cdot\tfrac{10}{3} - \tfrac{11}{3} = 3.
$</p>
<p dir="auto">So
$
x + \tfrac{20}{3} - \tfrac{11}{3} = 3 \implies x + 3 = 3 \implies x = 0.
$</p>
<p dir="auto">Solution:
$
(x,y,z) = \big(0, \tfrac{10}{3}, \tfrac{11}{3}\big).
$</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-8" aria-label="Permalink: Why this matters" href="#why-this-matters-8"></a></p>
<p dir="auto">Gaussian elimination is the foundation of computational linear algebra. It reduces complex systems to a form where
solutions are visible, and it forms the basis for algorithms used in numerical analysis, scientific computing, and
machine learning.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 3.2</h3><a id="user-content-exercises-32" aria-label="Permalink: Exercises 3.2" href="#exercises-32"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Solve by Gaussian elimination:
$
\begin{cases}
x + y = 2, \
2x - y = 0.
\end{cases}
$</p>
</li>
<li>
<p dir="auto">Reduce the following augmented matrix to REF:
$
\left[\begin{array}{ccc|c}
1 &amp; 1 &amp; 1 &amp; 6 \
2 &amp; -1 &amp; 3 &amp; 14 \
1 &amp; 4 &amp; -2 &amp; -2
\end{array}\right].
$</p>
</li>
<li>
<p dir="auto">Show that Gaussian elimination always produces either:</p>
<ul dir="auto">
<li>a unique solution,</li>
<li>infinitely many solutions, or</li>
<li>a contradiction (no solution).</li>
</ul>
</li>
<li>
<p dir="auto">Use Gaussian elimination to find all solutions of
$
\begin{cases}
x + y + z = 0, \
2x + y + z = 1.
\end{cases}
$</p>
</li>
<li>
<p dir="auto">Explain why pivoting (choosing the largest available pivot element) is useful in numerical computation.</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">3.3 Rank and Consistency</h2><a id="user-content-33-rank-and-consistency" aria-label="Permalink: 3.3 Rank and Consistency" href="#33-rank-and-consistency"></a></p>
<p dir="auto">Gaussian elimination not only provides solutions but also reveals the structure of a linear system. Two key ideas are
the rank of a matrix and the consistency of a system. Rank measures the amount of independent information in the
equations, while consistency determines whether the system has at least one solution.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Rank of a Matrix</h3><a id="user-content-rank-of-a-matrix" aria-label="Permalink: Rank of a Matrix" href="#rank-of-a-matrix"></a></p>
<p dir="auto">The rank of a matrix is the number of leading pivots in its row echelon form. Equivalently, it is the maximum number of
linearly independent rows or columns.</p>
<p dir="auto">Formally,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{rank}(A) = \dim(\text{row space of } A) = \dim(\text{column space of } A).
$$</math-renderer></p>
<p dir="auto">The rank tells us the effective dimension of the space spanned by the rows (or columns).</p>
<p dir="auto">Example 3.3.1.
For</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
1 &amp; 2 &amp; 3 \\
2 &amp; 4 &amp; 6 \\
3 &amp; 6 &amp; 9
\end{bmatrix},
$$</math-renderer></p>
<p dir="auto">row reduction gives</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Thus, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{rank}(A) = 1$</math-renderer>, since all rows are multiples of the first.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Consistency of Linear Systems</h3><a id="user-content-consistency-of-linear-systems" aria-label="Permalink: Consistency of Linear Systems" href="#consistency-of-linear-systems"></a></p>
<p dir="auto">Consider the system <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A\mathbf{x} = \mathbf{b}$</math-renderer>.
The system is consistent (has at least one solution) if and only if</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{rank}(A) = \text{rank}(A|\mathbf{b}),
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(A|\mathbf{b})$</math-renderer> is the augmented matrix.
If the ranks differ, the system is inconsistent.</p>
<ul dir="auto">
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{rank}(A) = \text{rank}(A|\mathbf{b}) = n$</math-renderer> (number of unknowns), the system has a unique solution.</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{rank}(A) = \text{rank}(A|\mathbf{b}) &amp;lt; n$</math-renderer>, the system has infinitely many solutions.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example</h3><a id="user-content-example-1" aria-label="Permalink: Example" href="#example-1"></a></p>
<p dir="auto">Example 3.3.2.
Consider</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{cases}
x + y + z = 1, \\
2x + 2y + 2z = 2, \\
x + y + z = 3.
\end{cases}
$$</math-renderer></p>
<p dir="auto">The augmented matrix is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\left[\begin{array}{ccc|c}
1 &amp; 1 &amp; 1 &amp; 1 \\
2 &amp; 2 &amp; 2 &amp; 2 \\
1 &amp; 1 &amp; 1 &amp; 3
\end{array}\right].
$$</math-renderer></p>
<p dir="auto">Row reduction gives</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\left[\begin{array}{ccc|c}
1 &amp; 1 &amp; 1 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 2
\end{array}\right].
$$</math-renderer></p>
<p dir="auto">Here, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{rank}(A) = 1$</math-renderer>, but <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{rank}(A|\mathbf{b}) = 2$</math-renderer>. Since the ranks differ, the system is inconsistent: no
solution exists.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example with Infinite Solutions</h3><a id="user-content-example-with-infinite-solutions" aria-label="Permalink: Example with Infinite Solutions" href="#example-with-infinite-solutions"></a></p>
<p dir="auto">Example 3.3.3.
For</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{cases}
x + y = 2, \\
2x + 2y = 4,
\end{cases}
$$</math-renderer></p>
<p dir="auto">the augmented matrix reduces to</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\left[\begin{array}{cc|c}
1 &amp; 1 &amp; 2 \\
0 &amp; 0 &amp; 0
\end{array}\right].
$$</math-renderer></p>
<p dir="auto">Here, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{rank}(A) = \text{rank}(A|\mathbf{b}) = 1 &amp;lt; 2$</math-renderer>. Thus, infinitely many solutions exist, forming a line.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-9" aria-label="Permalink: Why this matters" href="#why-this-matters-9"></a></p>
<p dir="auto">Rank is a measure of independence: it tells us how many truly distinct equations or directions are present. Consistency
explains when equations align versus when they contradict. These concepts connect linear systems to vector spaces and
prepare for the ideas of dimension, basis, and the Rank–Nullity Theorem.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 3.3</h3><a id="user-content-exercises-33" aria-label="Permalink: Exercises 3.3" href="#exercises-33"></a></p>
<ol dir="auto">
<li>Compute the rank of</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
1 &amp; 2 &amp; 1 \\
0 &amp; 1 &amp; -1 \\
2 &amp; 5 &amp; -1
\end{bmatrix}.
$$</math-renderer></p>
<ol start="2" dir="auto">
<li>Determine whether the system</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{cases}
x + y + z = 1, \\
2x + 3y + z = 2, \\
3x + 5y + 2z = 3
\end{cases}
$$</math-renderer></p>
<p dir="auto">is consistent.</p>
<ol start="3" dir="auto">
<li>
<p dir="auto">Show that the rank of the identity matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$I_n$</math-renderer> is <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Give an example of a system in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer> with infinitely many solutions, and explain why it satisfies the rank
condition.</p>
</li>
<li>
<p dir="auto">Prove that for any matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{m \times n}$</math-renderer>,
$
\text{rank}(A) \leq \min(m,n).
$</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">3.4 Homogeneous Systems</h2><a id="user-content-34-homogeneous-systems" aria-label="Permalink: 3.4 Homogeneous Systems" href="#34-homogeneous-systems"></a></p>
<p dir="auto">A homogeneous system is a linear system in which all constant terms are zero:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A\mathbf{x} = \mathbf{0},
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{m \times n}$</math-renderer>, and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{0}$</math-renderer> is the zero vector in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^m$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Trivial Solution</h3><a id="user-content-the-trivial-solution" aria-label="Permalink: The Trivial Solution" href="#the-trivial-solution"></a></p>
<p dir="auto">Every homogeneous system has at least one solution:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{x} = \mathbf{0}.
$$</math-renderer></p>
<p dir="auto">This is called the trivial solution. The interesting question is whether <em>nontrivial solutions</em> (nonzero vectors) exist.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Existence of Nontrivial Solutions</h3><a id="user-content-existence-of-nontrivial-solutions" aria-label="Permalink: Existence of Nontrivial Solutions" href="#existence-of-nontrivial-solutions"></a></p>
<p dir="auto">Nontrivial solutions exist precisely when the number of unknowns exceeds the rank of the coefficient matrix:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{rank}(A) &lt; n.
$$</math-renderer></p>
<p dir="auto">In this case, there are infinitely many solutions, forming a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>. The dimension of this solution
space is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\dim(\text{null}(A)) = n - \text{rank}(A),
$$</math-renderer></p>
<p dir="auto">where null(A) is the set of all solutions to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A\mathbf{x} = 0$</math-renderer>. This set is called the null space or kernel of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example</h3><a id="user-content-example-2" aria-label="Permalink: Example" href="#example-2"></a></p>
<p dir="auto">Example 3.4.1.
Consider</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{cases}
x + y + z = 0, \\
2x + y - z = 0.
\end{cases}
$$</math-renderer></p>
<p dir="auto">The augmented matrix is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\left[\begin{array}{ccc|c}
1 &amp; 1 &amp; 1 &amp; 0 \\
2 &amp; 1 &amp; -1 &amp; 0
\end{array}\right].
$$</math-renderer></p>
<p dir="auto">Row reduction:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\left[\begin{array}{ccc|c}
1 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; -1 &amp; -3 &amp; 0
\end{array}\right]
\quad\to\quad
\left[\begin{array}{ccc|c}
1 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 3 &amp; 0
\end{array}\right].
$$</math-renderer></p>
<p dir="auto">So the system is equivalent to:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{cases}
x + y + z = 0, \\
y + 3z = 0.
\end{cases}
$$</math-renderer></p>
<p dir="auto">From the second equation, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$y = -3z$</math-renderer>. Substituting into the first:
$
x - 3z + z = 0 \implies x = 2z.
$</p>
<p dir="auto">Thus solutions are:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
(x,y,z) = z(2, -3, 1), \quad z \in \mathbb{R}.
$$</math-renderer></p>
<p dir="auto">The null space is the line spanned by the vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(2, -3, 1)$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-2" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-2"></a></p>
<p dir="auto">The solution set of a homogeneous system is always a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>.</p>
<ul dir="auto">
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{rank}(A) = n$</math-renderer>, the only solution is the zero vector.</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{rank}(A) = n-1$</math-renderer>, the solution set is a line through the origin.</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{rank}(A) = n-2$</math-renderer>, the solution set is a plane through the origin.</li>
</ul>
<p dir="auto">More generally, the null space has dimension <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n - \text{rank}(A)$</math-renderer>, known as the nullity.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-10" aria-label="Permalink: Why this matters" href="#why-this-matters-10"></a></p>
<p dir="auto">Homogeneous systems are central to understanding vector spaces, subspaces, and dimension. They lead directly to the
concepts of kernel, null space, and linear dependence. In applications, homogeneous systems appear in equilibrium
problems, eigenvalue equations, and computer graphics transformations.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 3.4</h3><a id="user-content-exercises-34" aria-label="Permalink: Exercises 3.4" href="#exercises-34"></a></p>
<ol dir="auto">
<li>Solve the homogeneous system</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{cases}
x + 2y - z = 0, \\
2x + 4y - 2z = 0.
\end{cases}
$$</math-renderer></p>
<p dir="auto">What is the dimension of its solution space?</p>
<ol start="2" dir="auto">
<li>Find all solutions of</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{cases}
x - y + z = 0, \\
2x + y - z = 0.
\end{cases}
$$</math-renderer></p>
<ol start="3" dir="auto">
<li>
<p dir="auto">Show that the solution set of any homogeneous system is a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Suppose <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is a <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3 \times 3$</math-renderer> matrix with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{rank}(A) = 2$</math-renderer>. What is the dimension of the null space of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>?</p>
</li>
<li>
<p dir="auto">For</p>
</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 1 &amp; 2 &amp; -1 \ 0 &amp; 1 &amp; 3 \end{bmatrix},
$$</math-renderer></p>
<p dir="auto">compute a basis for the null space of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Chapter 4. Vector Spaces</h2><a id="user-content-chapter-4-vector-spaces" aria-label="Permalink: Chapter 4. Vector Spaces" href="#chapter-4-vector-spaces"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">4.1 Definition of a Vector Space</h2><a id="user-content-41-definition-of-a-vector-space" aria-label="Permalink: 4.1 Definition of a Vector Space" href="#41-definition-of-a-vector-space"></a></p>
<p dir="auto">Up to now we have studied vectors and matrices concretely in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>. The next step is to move beyond coordinates
and define vector spaces in full generality. A vector space is an abstract setting where the familiar rules of addition
and scalar multiplication hold, regardless of whether the elements are geometric vectors, polynomials, functions, or
other objects.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Formal Definition</h3><a id="user-content-formal-definition-1" aria-label="Permalink: Formal Definition" href="#formal-definition-1"></a></p>
<p dir="auto">A vector space over the real numbers <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}$</math-renderer> is a set <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> equipped with two operations:</p>
<ol dir="auto">
<li>Vector addition: For any <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}, \mathbf{v} \in V$</math-renderer>, there is a vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} + \mathbf{v} \in V$</math-renderer>.</li>
<li>Scalar multiplication: For any scalar <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c \in \mathbb{R}$</math-renderer> and any <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} \in V$</math-renderer>, there is a
vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c\mathbf{v} \in V$</math-renderer>.</li>
</ol>
<p dir="auto">These operations must satisfy the following axioms (for all <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$</math-renderer> and all
scalars <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$a,b \in \mathbb{R}$</math-renderer>):</p>
<ol dir="auto">
<li>Commutativity of addition: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$</math-renderer>.</li>
<li>Associativity of addition: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$</math-renderer>.</li>
<li>Additive identity: There exists a zero vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{0} \in V$</math-renderer> such that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} + \mathbf{0} = \mathbf{v}$</math-renderer>.</li>
<li>Additive inverses: For each <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} \in V$</math-renderer>, there exists <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(-\mathbf{v} \in V$</math-renderer> such
that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} + (-\mathbf{v}) = \mathbf{0}$</math-renderer>.</li>
<li>Compatibility of scalar multiplication: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$a(b\mathbf{v}) = (ab)\mathbf{v}$</math-renderer>.</li>
<li>Identity element of scalars: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$1 \cdot \mathbf{v} = \mathbf{v}$</math-renderer>.</li>
<li>Distributivity over vector addition: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$a(\mathbf{u} + \mathbf{v}) = a\mathbf{u} + a\mathbf{v}$</math-renderer>.</li>
<li>Distributivity over scalar addition: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(a+b)\mathbf{v} = a\mathbf{v} + b\mathbf{v}$</math-renderer>.</li>
</ol>
<p dir="auto">If a set <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> with operations satisfies all eight axioms, we call it a vector space.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Examples</h3><a id="user-content-examples-1" aria-label="Permalink: Examples" href="#examples-1"></a></p>
<p dir="auto">Example 4.1.1. Standard Euclidean space
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer> with ordinary addition and scalar multiplication is a vector space. This is the model case from which the
axioms are abstracted.</p>
<p dir="auto">Example 4.1.2. Polynomials
The set of all polynomials with real coefficients, denoted <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}[x]$</math-renderer>, forms a vector space. Addition and scalar
multiplication are defined term by term.</p>
<p dir="auto">Example 4.1.3. Functions
The set of all real-valued functions on an interval, e.g. <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$f: [0,1] \to \mathbb{R}$</math-renderer>, forms a vector space, since
functions can be added and scaled pointwise.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Non-Examples</h3><a id="user-content-non-examples" aria-label="Permalink: Non-Examples" href="#non-examples"></a></p>
<p dir="auto">Not every set with operations qualifies. For instance, the set of positive real numbers under usual addition is not a
vector space, because additive inverses (negative numbers) are missing. The axioms must all hold.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-3" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-3"></a></p>
<p dir="auto">In familiar cases like <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer> or <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, vector spaces provide the stage for geometry: vectors can be
added, scaled, and combined to form lines, planes, and higher-dimensional structures. In abstract settings like function
spaces, the same algebraic rules let us apply geometric intuition to infinite-dimensional problems.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-11" aria-label="Permalink: Why this matters" href="#why-this-matters-11"></a></p>
<p dir="auto">The concept of vector space unifies seemingly different mathematical objects under a single framework. Whether dealing
with forces in physics, signals in engineering, or data in machine learning, the common language of vector spaces allows
us to use the same techniques everywhere.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 4.1</h3><a id="user-content-exercises-41" aria-label="Permalink: Exercises 4.1" href="#exercises-41"></a></p>
<ol dir="auto">
<li>Verify that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer> with standard addition and scalar multiplication satisfies all eight vector space axioms.</li>
<li>Show that the set of integers <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{Z}$</math-renderer> with ordinary operations is not a vector space over <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}$</math-renderer>. Which
axiom fails?</li>
<li>Consider the set of all polynomials of degree at most 3. Show it forms a vector space over <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}$</math-renderer>. What is its
dimension?</li>
<li>Give an example of a vector space where the vectors are not geometric objects.</li>
<li>Prove that in any vector space, the zero vector is unique.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">4.2 Subspaces</h2><a id="user-content-42-subspaces" aria-label="Permalink: 4.2 Subspaces" href="#42-subspaces"></a></p>
<p dir="auto">A subspace is a smaller vector space living inside a larger one. Just as lines and planes naturally sit inside
three-dimensional space, subspaces generalize these ideas to higher dimensions and more abstract settings.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Definition</h3><a id="user-content-definition-1" aria-label="Permalink: Definition" href="#definition-1"></a></p>
<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> be a vector space. A subset <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W \subseteq V$</math-renderer> is called a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> if:</p>
<ol dir="auto">
<li>
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{0} \in W$</math-renderer> (contains the zero vector),</li>
<li>For all <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}, \mathbf{v} \in W$</math-renderer>, the sum <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} + \mathbf{v} \in W$</math-renderer> (closed under addition),</li>
<li>For all scalars <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c \in \mathbb{R}$</math-renderer> and vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} \in W$</math-renderer>, the product <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c\mathbf{v} \in W$</math-renderer> (closed under
scalar multiplication).</li>
</ol>
<p dir="auto">If these hold, then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W$</math-renderer> is itself a vector space with the inherited operations.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Examples</h3><a id="user-content-examples-2" aria-label="Permalink: Examples" href="#examples-2"></a></p>
<p dir="auto">Example 4.2.1. Line through the origin in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>
The set</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
W = { (t, 2t) \mid t \in \mathbb{R} }
$$</math-renderer></p>
<p dir="auto">is a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>. It contains the zero vector, is closed under addition, and is closed under scalar
multiplication.</p>
<p dir="auto">Example 4.2.2. The x–y plane in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>
The set</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
W = { (x, y, 0) \mid x,y \in \mathbb{R} }
$$</math-renderer></p>
<p dir="auto">is a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>. It is the collection of all vectors lying in the plane through the origin parallel to
the x–y plane.</p>
<p dir="auto">Example 4.2.3. Null space of a matrix
For a matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{m \times n}$</math-renderer>, the null space</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
{ \mathbf{x} \in \mathbb{R}^n \mid A\mathbf{x} = \mathbf{0} }
$$</math-renderer></p>
<p dir="auto">is a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>. This subspace represents all solutions to the homogeneous system.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Non-Examples</h3><a id="user-content-non-examples-1" aria-label="Permalink: Non-Examples" href="#non-examples-1"></a></p>
<p dir="auto">Not every subset is a subspace.</p>
<ul dir="auto">
<li>The set <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${ (x,y) \in \mathbb{R}^2 \mid x \geq 0 }$</math-renderer> is not a subspace: it is not closed under scalar multiplication (a
negative scalar breaks the condition).</li>
<li>Any line in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer> that does not pass through the origin is not a subspace, because it does not
contain <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{0}$</math-renderer>.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-4" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-4"></a></p>
<p dir="auto">Subspaces are the linear structures inside vector spaces.</p>
<ul dir="auto">
<li>In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>, the subspaces are: the zero vector, any line through the origin, or the entire plane.</li>
<li>In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, the subspaces are: the zero vector, any line through the origin, any plane through the origin, or
the entire space.</li>
<li>In higher dimensions, the same principle applies: subspaces are the flat linear pieces through the origin.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-12" aria-label="Permalink: Why this matters" href="#why-this-matters-12"></a></p>
<p dir="auto">Subspaces capture the essential structure of linear problems. Column spaces, row spaces, and null spaces are all
subspaces. Much of linear algebra consists of understanding how these subspaces intersect, span, and complement each
other.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 4.2</h3><a id="user-content-exercises-42" aria-label="Permalink: Exercises 4.2" href="#exercises-42"></a></p>
<ol dir="auto">
<li>Prove that the set <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W = { (x,0) \mid x \in \mathbb{R} } \subseteq \mathbb{R}^2$</math-renderer> is a subspace.</li>
<li>Show that the line <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${ (1+t, 2t) \mid t \in \mathbb{R} }$</math-renderer> is not a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>. Which condition fails?</li>
<li>Determine whether the set of all vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x,y,z) \in \mathbb{R}^3$</math-renderer> satisfying <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$x+y+z=0$</math-renderer> is a subspace.</li>
<li>For the matrix</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 1 &amp; 2 &amp; 3 \ 4 &amp; 5 &amp; 6 \end{bmatrix},
$$</math-renderer></p>
<p dir="auto">describe the null space of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> as a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>.
5. List all possible subspaces of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">4.3 Span, Basis, Dimension</h2><a id="user-content-43-span-basis-dimension" aria-label="Permalink: 4.3 Span, Basis, Dimension" href="#43-span-basis-dimension"></a></p>
<p dir="auto">The ideas of span, basis, and dimension provide the language for describing the size and structure of subspaces.
Together, they tell us how a vector space is generated, how many building blocks it requires, and how those blocks can
be chosen.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Span</h3><a id="user-content-span" aria-label="Permalink: Span" href="#span"></a></p>
<p dir="auto">Given a set of vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k} \subseteq V$</math-renderer>, the span is the collection of
all linear combinations:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{span}{\mathbf{v}_1, \dots, \mathbf{v}_k} = { c_1\mathbf{v}_1 + \cdots + c_k\mathbf{v}_k \mid c_i \in \mathbb{R} }.
$$</math-renderer></p>
<p dir="auto">The span is always a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer>, namely the smallest subspace containing those vectors.</p>
<p dir="auto">Example 4.3.1.
In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>, $ \text{span}{(1,0)} = {(x,0) \mid x \in \mathbb{R}},$ the x-axis.
Similarly, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{span}{(1,0),(0,1)} = \mathbb{R}^2.$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basis</h3><a id="user-content-basis" aria-label="Permalink: Basis" href="#basis"></a></p>
<p dir="auto">A basis of a vector space <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> is a set of vectors that:</p>
<ol dir="auto">
<li>Span <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer>.</li>
<li>Are linearly independent (no vector in the set is a linear combination of the others).</li>
</ol>
<p dir="auto">If either condition fails, the set is not a basis.</p>
<p dir="auto">Example 4.3.2.
In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, the standard unit vectors</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{e}_1 = (1,0,0), \quad \mathbf{e}_2 = (0,1,0), \quad \mathbf{e}_3 = (0,0,1)
$$</math-renderer></p>
<p dir="auto">form a basis. Every vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x,y,z)$</math-renderer> can be uniquely written as</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
x\mathbf{e}_1 + y\mathbf{e}_2 + z\mathbf{e}_3.
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Dimension</h3><a id="user-content-dimension" aria-label="Permalink: Dimension" href="#dimension"></a></p>
<p dir="auto">The dimension of a vector space <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer>, written <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\dim(V)$</math-renderer>, is the number of vectors in any basis of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer>. This number is
well-defined: all bases of a vector space have the same cardinality.</p>
<p dir="auto">Examples 4.3.3.</p>
<ul dir="auto">
<li>
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\dim(\mathbb{R}^2) = 2$</math-renderer>, with basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0), (0,1)$</math-renderer>.</li>
<li>
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\dim(\mathbb{R}^3) = 3$</math-renderer>, with basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0,0), (0,1,0), (0,0,1)$</math-renderer>.</li>
<li>The set of polynomials of degree at most 3 has dimension 4, with basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1, x, x^2, x^3)$</math-renderer>.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-5" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-5"></a></p>
<ul dir="auto">
<li>The span is like the reach of a set of vectors.</li>
<li>A basis is the minimal set of directions needed to reach everything in the space.</li>
<li>The dimension is the count of those independent directions.</li>
</ul>
<p dir="auto">Lines, planes, and higher-dimensional flats can all be described in terms of span, basis, and dimension.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-13" aria-label="Permalink: Why this matters" href="#why-this-matters-13"></a></p>
<p dir="auto">These concepts classify vector spaces and subspaces in terms of size and structure. Many theorems in linear algebra-such
as the Rank–Nullity Theorem-are consequences of understanding span, basis, and dimension. In practical terms, bases are
how we encode data in coordinates, and dimension tells us how much freedom a system truly has.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 4.3</h3><a id="user-content-exercises-43" aria-label="Permalink: Exercises 4.3" href="#exercises-43"></a></p>
<ol dir="auto">
<li>Show that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0,0)$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(0,1,0)$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1,0)$</math-renderer> span the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$xy$</math-renderer>-plane in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>. Are they a basis?</li>
<li>Find a basis for the line <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${(2t,-3t,t) : t \in \mathbb{R}}$</math-renderer> in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>.</li>
<li>Determine the dimension of the subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer> defined by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$x+y+z=0$</math-renderer>.</li>
<li>Prove that any two different bases of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer> must contain exactly <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> vectors.</li>
<li>Give a basis for the set of polynomials of degree <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\leq 2$</math-renderer>. What is its dimension?</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">4.4 Coordinates</h2><a id="user-content-44-coordinates" aria-label="Permalink: 4.4 Coordinates" href="#44-coordinates"></a></p>
<p dir="auto">Once a basis for a vector space is chosen, every vector can be expressed uniquely as a linear combination of the basis
vectors. The coefficients in this combination are called the coordinates of the vector relative to that basis.
Coordinates allow us to move between the abstract world of vector spaces and the concrete world of numbers.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Coordinates Relative to a Basis</h3><a id="user-content-coordinates-relative-to-a-basis" aria-label="Permalink: Coordinates Relative to a Basis" href="#coordinates-relative-to-a-basis"></a></p>
<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> be a vector space, and let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathcal{B} = {\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n}
$$</math-renderer></p>
<p dir="auto">be an ordered basis for <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer>. Every vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} \in V$</math-renderer> can be written uniquely as</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} = c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_n \mathbf{v}_n.
$$</math-renderer></p>
<p dir="auto">The scalars <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(c_1, c_2, \dots, c_n)$</math-renderer> are the coordinates of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer> relative to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B}$</math-renderer>, written</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[\mathbf{u}]_{\mathcal{B}} = \begin{bmatrix} c_1 \ c_2 \ \vdots \ c_n \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>
</h3><a id="user-content-example-in-mathbbr2" aria-label="Permalink: Example in $\mathbb{R}^2$" href="#example-in-mathbbr2"></a></p>
<p dir="auto">Example 4.4.1.
Let the basis be</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathcal{B} = { (1,1), (1,-1) }.
$$</math-renderer></p>
<p dir="auto">To find the coordinates of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (3,1)$</math-renderer> relative to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B}$</math-renderer>, solve</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
(3,1) = c_1(1,1) + c_2(1,-1).
$$</math-renderer></p>
<p dir="auto">This gives the system</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{cases}
c_1 + c_2 = 3, \\
c_1 - c_2 = 1.
\end{cases}
$$</math-renderer></p>
<p dir="auto">Adding: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$2c_1 = 4 \implies c_1 = 2$</math-renderer>. Then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c_2 = 1$</math-renderer>.</p>
<p dir="auto">So,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[\mathbf{u}]_{\mathcal{B}} = \begin{bmatrix} 2 \ 1 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Standard Coordinates</h3><a id="user-content-standard-coordinates" aria-label="Permalink: Standard Coordinates" href="#standard-coordinates"></a></p>
<p dir="auto">In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>, the standard basis is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{e}_1 = (1,0,\dots,0), \quad \mathbf{e}_2 = (0,1,0,\dots,0), \dots, \mathbf{e}_n = (0,\dots,0,1).
$$</math-renderer></p>
<p dir="auto">Relative to this basis, the coordinates of a vector are simply its entries. Thus, column vectors are coordinate
representations by default.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Change of Basis</h3><a id="user-content-change-of-basis" aria-label="Permalink: Change of Basis" href="#change-of-basis"></a></p>
<p dir="auto">If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B} = {\mathbf{v}_1, \dots, \mathbf{v}_n}$</math-renderer> is a basis of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>, the change of basis matrix is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
P = \begin{bmatrix} \mathbf{v}_1 &amp; \mathbf{v}_2 &amp; \cdots &amp; \mathbf{v}_n \end{bmatrix},
$$</math-renderer></p>
<p dir="auto">with basis vectors as columns. For any vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer>,</p>
<p dir="auto">$$
\mathbf{u} = P[\mathbf{u}]<em>{\mathcal{B}}, \qquad [\mathbf{u}]</em>{\mathcal{B}} = P^{-1}\mathbf{u}.
$$</p>
<p dir="auto">Thus, switching between bases reduces to matrix multiplication.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-6" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-6"></a></p>
<p dir="auto">Coordinates are the address of a vector relative to a chosen set of directions. Different bases are like different
coordinate systems: Cartesian, rotated, skewed, or scaled. The same vector may look very different numerically depending
on the basis, but its geometric identity is unchanged.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-14" aria-label="Permalink: Why this matters" href="#why-this-matters-14"></a></p>
<p dir="auto">Coordinates turn abstract vectors into concrete numerical data. Changing basis is the algebraic language for rotations
of axes, diagonalization of matrices, and principal component analysis in data science. Mastery of coordinates is
essential for moving fluidly between geometry, algebra, and computation.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 4.4</h3><a id="user-content-exercises-44" aria-label="Permalink: Exercises 4.4" href="#exercises-44"></a></p>
<ol dir="auto">
<li>Express <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(4,2)$</math-renderer> in terms of the basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1), (1,-1)$</math-renderer>.</li>
<li>Find the coordinates of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,2,3)$</math-renderer> relative to the standard basis of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>.</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B} = {(2,0), (0,3)}$</math-renderer>, compute <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$[ (4,6) ]_{\mathcal{B}}$</math-renderer>.</li>
<li>Construct the change of basis matrix from the standard basis of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer> to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B} = {(1,1), (1,-1)}$</math-renderer>.</li>
<li>Prove that coordinate representation with respect to a basis is unique.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Chapter 5. Linear Transformations</h2><a id="user-content-chapter-5-linear-transformations" aria-label="Permalink: Chapter 5. Linear Transformations" href="#chapter-5-linear-transformations"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">5.1 Functions that Preserve Linearity</h2><a id="user-content-51-functions-that-preserve-linearity" aria-label="Permalink: 5.1 Functions that Preserve Linearity" href="#51-functions-that-preserve-linearity"></a></p>
<p dir="auto">A central theme of linear algebra is understanding linear transformations: functions between vector spaces that preserve
their algebraic structure. These transformations generalize the idea of matrix multiplication and capture the essence of
linear behavior.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Definition</h3><a id="user-content-definition-2" aria-label="Permalink: Definition" href="#definition-2"></a></p>
<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W$</math-renderer> be vector spaces over <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}$</math-renderer>. A function</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
T : V \to W
$$</math-renderer></p>
<p dir="auto">is called a linear transformation (or linear map) if for all vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}, \mathbf{v} \in V$</math-renderer> and all
scalars <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c \in \mathbb{R}$</math-renderer>:</p>
<ol dir="auto">
<li>
<p dir="auto">Additivity:</p>
<p dir="auto">$$
T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v}),
$$</p>
</li>
<li>
<p dir="auto">Homogeneity:</p>
<p dir="auto">$$
T(c\mathbf{u}) = cT(\mathbf{u}).
$$</p>
</li>
</ol>
<p dir="auto">If both conditions hold, then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T$</math-renderer> automatically respects linear combinations:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
T(c_1\mathbf{v}_1 + \cdots + c_k\mathbf{v}_k) = c_1 T(\mathbf{v}_1) + \cdots + c_k T(\mathbf{v}_k).
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Examples</h3><a id="user-content-examples-3" aria-label="Permalink: Examples" href="#examples-3"></a></p>
<p dir="auto">Example 5.1.1. Scaling in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T:\mathbb{R}^2 \to \mathbb{R}^2$</math-renderer> be defined by</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
T(x,y) = (2x, 2y).
$$</math-renderer></p>
<p dir="auto">This doubles the length of every vector, preserving direction. It is linear.</p>
<p dir="auto">Example 5.1.2. Rotation.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$R_\theta: \mathbb{R}^2 \to \mathbb{R}^2$</math-renderer> be</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
R_\theta(x,y) = (x\cos\theta - y\sin\theta, ; x\sin\theta + y\cos\theta).
$$</math-renderer></p>
<p dir="auto">This rotates vectors by angle <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\theta$</math-renderer>. It satisfies additivity and homogeneity, hence is linear.</p>
<p dir="auto">Example 5.1.3. Differentiation.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$D: \mathbb{R}[x] \to \mathbb{R}[x]$</math-renderer> be differentiation: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$D(p(x)) = p'(x)$</math-renderer>. Since derivatives respect addition and
scalar multiples, differentiation is a linear transformation.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Non-Example</h3><a id="user-content-non-example" aria-label="Permalink: Non-Example" href="#non-example"></a></p>
<p dir="auto">The map <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$S:\mathbb{R}^2 \to \mathbb{R}^2$</math-renderer> defined by</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
S(x,y) = (x^2, y^2)
$$</math-renderer></p>
<p dir="auto">is not linear, because <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$S(\mathbf{u} + \mathbf{v}) \neq S(\mathbf{u}) + S(\mathbf{v})$</math-renderer> in general.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-7" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-7"></a></p>
<p dir="auto">Linear transformations are exactly those that preserve the origin, lines through the origin, and proportions along those
lines. They include familiar operations: scaling, rotations, reflections, shears, and projections. Nonlinear
transformations bend or curve space, breaking these properties.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-15" aria-label="Permalink: Why this matters" href="#why-this-matters-15"></a></p>
<p dir="auto">Linear transformations unify geometry, algebra, and computation. They explain how matrices act on vectors, how data can
be rotated or projected, and how systems evolve under linear rules. Much of linear algebra is devoted to understanding
these transformations, their representations, and their invariants.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 5.1</h3><a id="user-content-exercises-51" aria-label="Permalink: Exercises 5.1" href="#exercises-51"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Verify that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T(x,y) = (3x-y, 2y)$</math-renderer> is a linear transformation on <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Show that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T(x,y) = (x+1, y)$</math-renderer> is not linear. Which axiom fails?</p>
</li>
<li>
<p dir="auto">Prove that if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$S$</math-renderer> are linear transformations, then so is <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T+S$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Give an example of a linear transformation from <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer> to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T:\mathbb{R}[x] \to \mathbb{R}[x]$</math-renderer> be integration:</p>
<p dir="auto">$$
T(p(x)) = \int_0^x p(t),dt.
$$</p>
<p dir="auto">Prove that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T$</math-renderer> is a linear transformation.</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">5.2 Matrix Representation of Linear Maps</h2><a id="user-content-52-matrix-representation-of-linear-maps" aria-label="Permalink: 5.2 Matrix Representation of Linear Maps" href="#52-matrix-representation-of-linear-maps"></a></p>
<p dir="auto">Every linear transformation between finite-dimensional vector spaces can be represented by a matrix. This correspondence
is one of the central insights of linear algebra: it lets us use the tools of matrix arithmetic to study abstract
transformations.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">From Linear Map to Matrix</h3><a id="user-content-from-linear-map-to-matrix" aria-label="Permalink: From Linear Map to Matrix" href="#from-linear-map-to-matrix"></a></p>
<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T: \mathbb{R}^n \to \mathbb{R}^m$</math-renderer> be a linear transformation. Choose the standard
basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${ \mathbf{e}_1, \dots, \mathbf{e}_n }$</math-renderer> of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>, where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{e}_i$</math-renderer> has a 1 in the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$i$</math-renderer>-th position
and 0 elsewhere.</p>
<p dir="auto">The action of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T$</math-renderer> on each basis vector determines the entire transformation:</p>
<p dir="auto">$$
T(\mathbf{e}<em>j) = \begin{bmatrix} a</em>{1j} \ a_{2j} \ \vdots \ a_{mj} \end{bmatrix}.
$$</p>
<p dir="auto">Placing these outputs as columns gives the matrix of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T$</math-renderer>:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[T] = A = \begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then for any vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x} \in \mathbb{R}^n$</math-renderer>:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
T(\mathbf{x}) = A\mathbf{x}.
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Examples</h3><a id="user-content-examples-4" aria-label="Permalink: Examples" href="#examples-4"></a></p>
<p dir="auto">Example 5.2.1. Scaling in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T(x,y) = (2x, 3y)$</math-renderer>. Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
T(\mathbf{e}_1) = (2,0), \quad T(\mathbf{e}_2) = (0,3).
$$</math-renderer></p>
<p dir="auto">So the matrix is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[T] = \begin{bmatrix}
2 &amp; 0 \\
0 &amp; 3
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Example 5.2.2. Rotation in the plane.
The rotation transformation <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$R_\theta(x,y) = (x\cos\theta - y\sin\theta, ; x\sin\theta + y\cos\theta)$</math-renderer> has matrix</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[R_\theta] = \begin{bmatrix}
\cos\theta &amp; -\sin\theta \\
\sin\theta &amp; \cos\theta
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Example 5.2.3. Projection onto the x-axis.
The map <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P(x,y) = (x,0)$</math-renderer> corresponds to</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[P] = \begin{bmatrix}
1 &amp; 0 \\
0 &amp; 0
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Change of Basis</h3><a id="user-content-change-of-basis-1" aria-label="Permalink: Change of Basis" href="#change-of-basis-1"></a></p>
<p dir="auto">Matrix representations depend on the chosen basis. If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B}$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{C}$</math-renderer> are bases of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>
and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^m$</math-renderer>, then the matrix of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T: \mathbb{R}^n \to \mathbb{R}^m$</math-renderer> with respect to these bases is obtained by
expressing <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T(\mathbf{v}_j)$</math-renderer> in terms of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{C}$</math-renderer> for each <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_j \in \mathcal{B}$</math-renderer>. Changing bases
corresponds to conjugating the matrix by the appropriate change-of-basis matrices.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-8" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-8"></a></p>
<p dir="auto">Matrices are not just convenient notation-they <em>are</em> linear maps once a basis is fixed. Every rotation, reflection,
projection, shear, or scaling corresponds to multiplying by a specific matrix. Thus, studying linear transformations
reduces to studying their matrices.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-16" aria-label="Permalink: Why this matters" href="#why-this-matters-16"></a></p>
<p dir="auto">Matrix representations make linear transformations computable. They connect abstract definitions to explicit
calculations, enabling algorithms for solving systems, finding eigenvalues, and performing decompositions. Applications
from graphics to machine learning depend on this translation.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 5.2</h3><a id="user-content-exercises-52" aria-label="Permalink: Exercises 5.2" href="#exercises-52"></a></p>
<ol dir="auto">
<li>Find the matrix representation of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T:\mathbb{R}^2 \to \mathbb{R}^2$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T(x,y) = (x+y, x-y)$</math-renderer>.</li>
<li>Determine the matrix of the linear transformation <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T:\mathbb{R}^3 \to \mathbb{R}^2$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T(x,y,z) = (x+z, y-2z)$</math-renderer>.</li>
<li>What matrix represents reflection across the line <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$y=x$</math-renderer> in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>?</li>
<li>Show that the matrix of the identity transformation on <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer> is <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$I_n$</math-renderer>.</li>
<li>For the differentiation map <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$D:\mathbb{R}_2[x] \to \mathbb{R}_1[x]$</math-renderer>, where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}_k[x]$</math-renderer> is the space of
polynomials of degree at most <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$k$</math-renderer>, find the matrix of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$D$</math-renderer> relative to the bases <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${1,x,x^2}$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${1,x}$</math-renderer>.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">5.3 Kernel and Image</h2><a id="user-content-53-kernel-and-image" aria-label="Permalink: 5.3 Kernel and Image" href="#53-kernel-and-image"></a></p>
<p dir="auto">To understand a linear transformation deeply, we must examine what it kills and what it produces. These ideas are
captured by the kernel and the image, two fundamental subspaces associated with any linear map.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Kernel</h3><a id="user-content-the-kernel" aria-label="Permalink: The Kernel" href="#the-kernel"></a></p>
<p dir="auto">The kernel (or null space) of a linear transformation <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T: V \to W$</math-renderer> is the set of all vectors in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> that map to the zero
vector in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W$</math-renderer>:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\ker(T) = { \mathbf{v} \in V \mid T(\mathbf{v}) = \mathbf{0} }.
$$</math-renderer></p>
<p dir="auto">The kernel is always a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer>. It measures the degeneracy of the transformation-directions that collapse to
nothing.</p>
<p dir="auto">Example 5.3.1.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T:\mathbb{R}^3 \to \mathbb{R}^2$</math-renderer> be defined by</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
T(x,y,z) = (x+y, y+z).
$$</math-renderer></p>
<p dir="auto">In matrix form,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[T] = \begin{bmatrix}
1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 1
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">To find the kernel, solve</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{bmatrix}
1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 1
\end{bmatrix}
\begin{bmatrix} x \ y \ z \end{bmatrix}
= \begin{bmatrix} 0 \ 0 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">This gives the equations <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$x + y = 0$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$y + z = 0$</math-renderer>. Hence <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$x = -y, z = -y$</math-renderer>. The kernel is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\ker(T) = { (-t, t, -t) \mid t \in \mathbb{R} },
$$</math-renderer></p>
<p dir="auto">a line in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Image</h3><a id="user-content-the-image" aria-label="Permalink: The Image" href="#the-image"></a></p>
<p dir="auto">The image (or range) of a linear transformation <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T: V \to W$</math-renderer> is the set of all outputs:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{im}(T) = { T(\mathbf{v}) \mid \mathbf{v} \in V } \subseteq W.
$$</math-renderer></p>
<p dir="auto">Equivalently, it is the span of the columns of the representing matrix. The image is always a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W$</math-renderer>.</p>
<p dir="auto">Example 5.3.2.
For the same transformation as above,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[T] = \begin{bmatrix}
1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 1
\end{bmatrix},
$$</math-renderer></p>
<p dir="auto">the columns are <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0)$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1)$</math-renderer>, and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(0,1)$</math-renderer>. Since <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1) = (1,0) + (0,1)$</math-renderer>, the image is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{im}(T) = \text{span}{ (1,0), (0,1) } = \mathbb{R}^2.
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Dimension Formula (Rank–Nullity Theorem)</h3><a id="user-content-dimension-formula-ranknullity-theorem" aria-label="Permalink: Dimension Formula (Rank–Nullity Theorem)" href="#dimension-formula-ranknullity-theorem"></a></p>
<p dir="auto">For a linear transformation <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T: V \to W$</math-renderer> with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> finite-dimensional,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\dim(\ker(T)) + \dim(\text{im}(T)) = \dim(V).
$$</math-renderer></p>
<p dir="auto">This fundamental result connects the lost directions (kernel) with the achieved directions (image).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-9" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-9"></a></p>
<ul dir="auto">
<li>The kernel describes how the transformation flattens space (e.g., projecting a 3D object onto a plane).</li>
<li>The image describes the target subspace reached by the transformation.</li>
<li>The rank–nullity theorem quantifies the tradeoff: the more dimensions collapse, the fewer remain in the image.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-17" aria-label="Permalink: Why this matters" href="#why-this-matters-17"></a></p>
<p dir="auto">Kernel and image capture the essence of a linear map. They classify transformations, explain when systems have unique or
infinite solutions, and form the backbone of important results like the Rank–Nullity Theorem, diagonalization, and
spectral theory.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 5.3</h3><a id="user-content-exercises-53" aria-label="Permalink: Exercises 5.3" href="#exercises-53"></a></p>
<ol dir="auto">
<li>Find the kernel and image of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T:\mathbb{R}^2 \to \mathbb{R}^2$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T(x,y) = (x-y, x+y)$</math-renderer>.</li>
<li>Let $A = \begin{bmatrix} 1 &amp; 2 &amp; 3 \ 0 &amp; 1 &amp; 4 \end{bmatrix}$. Find bases for <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\ker(A)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{im}(A)$</math-renderer>.</li>
<li>For the projection map <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P(x,y,z) = (x,y,0)$</math-renderer>, describe the kernel and image.</li>
<li>Prove that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\ker(T)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{im}(T)$</math-renderer> are always subspaces.</li>
<li>Verify the Rank–Nullity Theorem for the transformation in Example 5.3.1.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">5.4 Change of Basis</h2><a id="user-content-54-change-of-basis" aria-label="Permalink: 5.4 Change of Basis" href="#54-change-of-basis"></a></p>
<p dir="auto">Linear transformations can look very different depending on the coordinate system we use. The process of rewriting
vectors and transformations relative to a new basis is called a change of basis. This concept lies at the heart of
diagonalization, orthogonalization, and many computational techniques.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Coordinate Change</h3><a id="user-content-coordinate-change" aria-label="Permalink: Coordinate Change" href="#coordinate-change"></a></p>
<p dir="auto">Suppose <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> is an <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer>-dimensional vector space, and let $\mathcal{B} = {\mathbf{v}_1, \dots, \mathbf{v}<em>n}$ be a
basis. Every vector $\mathbf{x} \in V$ has a coordinate vector $[\mathbf{x}]</em>{\mathcal{B}} \in \mathbb{R}^n$.</p>
<p dir="auto">If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer> is the change-of-basis matrix from <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B}$</math-renderer> to the standard basis, then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{x} = P [\mathbf{x}]_{\mathcal{B}}.
$$</math-renderer></p>
<p dir="auto">Equivalently,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[\mathbf{x}]_{\mathcal{B}} = P^{-1} \mathbf{x}.
$$</math-renderer></p>
<p dir="auto">Here, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer> has the basis vectors of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B}$</math-renderer> as its columns:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
P = \begin{bmatrix}
\mathbf{v}_1 &amp; \mathbf{v}_2 &amp; \cdots &amp; \mathbf{v}_n
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Transformation of Matrices</h3><a id="user-content-transformation-of-matrices" aria-label="Permalink: Transformation of Matrices" href="#transformation-of-matrices"></a></p>
<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T: V \to V$</math-renderer> be a linear transformation. Suppose its matrix in the standard basis is <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>. In the
basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B}$</math-renderer>, the representing matrix becomes</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[T]_{\mathcal{B}} = P^{-1} A P.
$$</math-renderer></p>
<p dir="auto">Thus, changing basis corresponds to a similarity transformation of the matrix.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example</h3><a id="user-content-example-3" aria-label="Permalink: Example" href="#example-3"></a></p>
<p dir="auto">Example 5.4.1.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T:\mathbb{R}^2 \to \mathbb{R}^2$</math-renderer> be given by</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
T(x,y) = (3x + y, x + y).
$$</math-renderer></p>
<p dir="auto">In the standard basis, its matrix is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
3 &amp; 1 \\
1 &amp; 1
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Now consider the basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B} = { (1,1), (1,-1) }$</math-renderer>. The change-of-basis matrix is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
P = \begin{bmatrix}
1 &amp; 1 \\
1 &amp; -1
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[T]_{\mathcal{B}} = P^{-1} A P.
$$</math-renderer></p>
<p dir="auto">Computing gives</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[T]_{\mathcal{B}} =
\begin{bmatrix}
4 &amp; 0 \\
0 &amp; 0
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">In this new basis, the transformation is diagonal: one direction is scaled by 4, the other collapsed to 0.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-10" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-10"></a></p>
<p dir="auto">Change of basis is like rotating or skewing your coordinate grid. The underlying transformation does not change, but its
description in numbers becomes simpler or more complicated depending on the basis. Finding a basis that simplifies a
transformation (often a diagonal basis) is a key theme in linear algebra.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-18" aria-label="Permalink: Why this matters" href="#why-this-matters-18"></a></p>
<p dir="auto">Change of basis connects the abstract notion of similarity to practical computation. It is the tool that allows us to
diagonalize matrices, compute eigenvalues, and simplify complex transformations. In applications, it corresponds to
choosing a more natural coordinate system-whether in geometry, physics, or machine learning.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 5.4</h3><a id="user-content-exercises-54" aria-label="Permalink: Exercises 5.4" href="#exercises-54"></a></p>
<ol dir="auto">
<li>Let $A = \begin{bmatrix} 2 &amp; 1 \ 0 &amp; 2 \end{bmatrix}$. Compute its representation in the basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${(1,0),(1,1)}$</math-renderer>.</li>
<li>Find the change-of-basis matrix from the standard basis of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer> to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${(2,1),(1,1)}$</math-renderer>.</li>
<li>Prove that similar matrices (related by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P^{-1}AP$</math-renderer>) represent the same linear transformation under different bases.</li>
<li>Diagonalize the matrix $A = \begin{bmatrix} 1 &amp; 0 \ 0 &amp; -1 \end{bmatrix}$ in the basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${(1,1),(1,-1)}$</math-renderer>.</li>
<li>In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B} = {(1,0,0),(1,1,0),(1,1,1)}$</math-renderer>. Construct the change-of-basis matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer> and
compute <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P^{-1}$</math-renderer>.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Chapter 6. Determinants</h2><a id="user-content-chapter-6-determinants" aria-label="Permalink: Chapter 6. Determinants" href="#chapter-6-determinants"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">6.1 Motivation and Geometric Meaning</h2><a id="user-content-61-motivation-and-geometric-meaning" aria-label="Permalink: 6.1 Motivation and Geometric Meaning" href="#61-motivation-and-geometric-meaning"></a></p>
<p dir="auto">Determinants are numerical values associated with square matrices. At first they may appear as a complicated formula,
but their importance comes from what they measure: determinants encode scaling, orientation, and invertibility of linear
transformations. They bridge algebra and geometry.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Determinants of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$2 \times 2$</math-renderer> Matrices</h3><a id="user-content-determinants-of-2-times-2-matrices" aria-label="Permalink: Determinants of $2 \times 2$ Matrices" href="#determinants-of-2-times-2-matrices"></a></p>
<p dir="auto">For a <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$2 \times 2$</math-renderer> matrix</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} a &amp; b \ c &amp; d \end{bmatrix},
$$</math-renderer></p>
<p dir="auto">the determinant is defined as</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(A) = ad - bc.
$$</math-renderer></p>
<p dir="auto">Geometric meaning: If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> represents a linear transformation of the plane, then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\det(A)|$</math-renderer> is the area scaling factor.
For example, if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) = 2$</math-renderer>, areas of shapes are doubled. If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) = 0$</math-renderer>, the transformation collapses the plane to
a line: all area is lost.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Determinants of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3 \times 3$</math-renderer> Matrices</h3><a id="user-content-determinants-of-3-times-3-matrices" aria-label="Permalink: Determinants of $3 \times 3$ Matrices" href="#determinants-of-3-times-3-matrices"></a></p>
<p dir="auto">For</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
a &amp; b &amp; c \\
d &amp; e &amp; f \\
g &amp; h &amp; i
\end{bmatrix},
$$</math-renderer></p>
<p dir="auto">the determinant can be computed as</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(A) = a(ei - fh) - b(di - fg) + c(dh - eg).
$$</math-renderer></p>
<p dir="auto">Geometric meaning: In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\det(A)|$</math-renderer> is the volume scaling factor. If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) &amp;lt; 0$</math-renderer>, orientation is
reversed (a handedness flip), such as turning a right-handed coordinate system into a left-handed one.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">General Case</h3><a id="user-content-general-case" aria-label="Permalink: General Case" href="#general-case"></a></p>
<p dir="auto">For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{n \times n}$</math-renderer>, the determinant is a scalar that measures how the linear transformation given
by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> scales n-dimensional volume.</p>
<ul dir="auto">
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) = 0$</math-renderer>: the transformation squashes space into a lower dimension, so <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is not invertible.</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) &amp;gt; 0$</math-renderer>: volume is scaled by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A)$</math-renderer>, orientation preserved.</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) &amp;lt; 0$</math-renderer>: volume is scaled by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\det(A)|$</math-renderer>, orientation reversed.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Visual Examples</h3><a id="user-content-visual-examples" aria-label="Permalink: Visual Examples" href="#visual-examples"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Shear in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>:
$A = \begin{bmatrix} 1 &amp; 1 \ 0 &amp; 1 \end{bmatrix}$.
Then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) = 1$</math-renderer>. The transformation slants the unit square into a parallelogram but preserves area.</p>
</li>
<li>
<p dir="auto">Projection in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>:
$A = \begin{bmatrix} 1 &amp; 0 \ 0 &amp; 0 \end{bmatrix}$.
Then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) = 0$</math-renderer>. The unit square collapses into a line segment: area vanishes.</p>
</li>
<li>
<p dir="auto">Rotation in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>:
$R_\theta = \begin{bmatrix} \cos\theta &amp; -\sin\theta \ \sin\theta &amp; \cos\theta \end{bmatrix}$.
Then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(R_\theta) = 1$</math-renderer>. Rotations preserve area and orientation.</p>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-19" aria-label="Permalink: Why this matters" href="#why-this-matters-19"></a></p>
<p dir="auto">The determinant is not just a formula-it is a measure of transformation. It tells us whether a matrix is invertible, how
it distorts space, and whether it flips orientation. This geometric insight makes the determinant indispensable in
analysis, geometry, and applied mathematics.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 6.1</h3><a id="user-content-exercises-61" aria-label="Permalink: Exercises 6.1" href="#exercises-61"></a></p>
<ol dir="auto">
<li>Compute the determinant of $\begin{bmatrix} 2 &amp; 3 \ 1 &amp; 4 \end{bmatrix}$. What area scaling factor does it
represent?</li>
<li>Find the determinant of the shear matrix $\begin{bmatrix} 1 &amp; 2 \ 0 &amp; 1 \end{bmatrix}$. What happens to the area of
the unit square?</li>
<li>For the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3 \times 3$</math-renderer> matrix
$\begin{bmatrix} 1 &amp; 0 &amp; 0 \ 0 &amp; 2 &amp; 0 \ 0 &amp; 0 &amp; 3 \end{bmatrix}$, compute the determinant. How does it scale
volume in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>?</li>
<li>Show that any rotation matrix in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer> has determinant <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$1$</math-renderer>.</li>
<li>Give an example of a <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$2 \times 2$</math-renderer> matrix with determinant <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$-1$</math-renderer>. What geometric action does it represent?</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">6.2 Properties of Determinants</h2><a id="user-content-62-properties-of-determinants" aria-label="Permalink: 6.2 Properties of Determinants" href="#62-properties-of-determinants"></a></p>
<p dir="auto">Beyond their geometric meaning, determinants satisfy a collection of algebraic rules that make them powerful tools in
linear algebra. These properties allow us to compute efficiently, test invertibility, and understand how determinants
behave under matrix operations.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basic Properties</h3><a id="user-content-basic-properties" aria-label="Permalink: Basic Properties" href="#basic-properties"></a></p>
<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A, B \in \mathbb{R}^{n \times n}$</math-renderer>, and let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c \in \mathbb{R}$</math-renderer>. Then:</p>
<ol dir="auto">
<li>
<p dir="auto">Identity:</p>
<p dir="auto">$$
\det(I_n) = 1.
$$</p>
</li>
<li>
<p dir="auto">Triangular matrices:
If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is upper or lower triangular, then</p>
<p dir="auto">$$
\det(A) = a_{11} a_{22} \cdots a_{nn}.
$$</p>
</li>
<li>
<p dir="auto">Row/column swap:
Interchanging two rows (or columns) multiplies the determinant by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$-1$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Row/column scaling:
Multiplying a row (or column) by a scalar <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c$</math-renderer> multiplies the determinant by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Row/column addition:
Adding a multiple of one row to another does not change the determinant.</p>
</li>
<li>
<p dir="auto">Transpose:</p>
<p dir="auto">$$
\det(A^T) = \det(A).
$$</p>
</li>
<li>
<p dir="auto">Multiplicativity:</p>
<p dir="auto">$$
\det(AB) = \det(A)\det(B).
$$</p>
</li>
<li>
<p dir="auto">Invertibility:
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is invertible if and only if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) \neq 0$</math-renderer>.</p>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example Computations</h3><a id="user-content-example-computations" aria-label="Permalink: Example Computations" href="#example-computations"></a></p>
<p dir="auto">Example 6.2.1.
For</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; 0 &amp; 0 \ 1 &amp; 3 &amp; 0 \ -1 &amp; 4 &amp; 5 \end{bmatrix},
$$</math-renderer></p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is lower triangular, so</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(A) = 2 \cdot 3 \cdot 5 = 30.
$$</math-renderer></p>
<p dir="auto">Example 6.2.2.
Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
B = \begin{bmatrix} 1 &amp; 2 \ 3 &amp; 4 \end{bmatrix}, \quad
C = \begin{bmatrix} 0 &amp; 1 \ 1 &amp; 0 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(B) = 1\cdot 4 - 2\cdot 3 = -2, \quad \det(C) = -1.
$$</math-renderer></p>
<p dir="auto">Since <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$CB$</math-renderer> is obtained by swapping rows of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$B$</math-renderer>,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(CB) = -\det(B) = 2.
$$</math-renderer></p>
<p dir="auto">This matches the multiplicativity rule: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(CB) = \det(C)\det(B) = (-1)(-2) = 2.$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Insights</h3><a id="user-content-geometric-insights" aria-label="Permalink: Geometric Insights" href="#geometric-insights"></a></p>
<ul dir="auto">
<li>Row swaps: flipping orientation of space.</li>
<li>Scaling a row: stretching space in one direction.</li>
<li>Row replacement: sliding hyperplanes without altering volume.</li>
<li>Multiplicativity: performing two transformations multiplies their scaling factors.</li>
</ul>
<p dir="auto">These properties make determinants both computationally manageable and geometrically interpretable.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-20" aria-label="Permalink: Why this matters" href="#why-this-matters-20"></a></p>
<p dir="auto">Determinant properties connect computation with geometry and theory. They explain why Gaussian elimination works, why
invertibility is equivalent to nonzero determinant, and why determinants naturally arise in areas like volume
computation, eigenvalue theory, and differential equations.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 6.2</h3><a id="user-content-exercises-62" aria-label="Permalink: Exercises 6.2" href="#exercises-62"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Compute the determinant of</p>
<p dir="auto">$$
A = \begin{bmatrix} 1 &amp; 2 &amp; 3 \ 0 &amp; 1 &amp; 4 \ 0 &amp; 0 &amp; 2 \end{bmatrix}.
$$</p>
</li>
<li>
<p dir="auto">Show that if two rows of a square matrix are identical, then its determinant is zero.</p>
</li>
<li>
<p dir="auto">Verify <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A^T) = \det(A)$</math-renderer> for</p>
<p dir="auto">$$
A = \begin{bmatrix} 2 &amp; -1 \ 3 &amp; 4 \end{bmatrix}.
$$</p>
</li>
<li>
<p dir="auto">If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is invertible, prove that</p>
<p dir="auto">$$
\det(A^{-1}) = \frac{1}{\det(A)}.
$$</p>
</li>
<li>
<p dir="auto">Suppose <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is a <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3\times 3$</math-renderer> matrix with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) = 5$</math-renderer>. What is <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(2A)$</math-renderer>?</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">6.3 Cofactor Expansion</h2><a id="user-content-63-cofactor-expansion" aria-label="Permalink: 6.3 Cofactor Expansion" href="#63-cofactor-expansion"></a></p>
<p dir="auto">While determinants of small matrices can be computed directly from formulas, larger matrices require a systematic
method. The cofactor expansion (also known as Laplace expansion) provides a recursive way to compute determinants by
breaking them into smaller ones.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Minors and Cofactors</h3><a id="user-content-minors-and-cofactors" aria-label="Permalink: Minors and Cofactors" href="#minors-and-cofactors"></a></p>
<p dir="auto">For an <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n \times n$</math-renderer> matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A = [a_{ij}]$</math-renderer>:</p>
<ul dir="auto">
<li>The minor <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$M_{ij}$</math-renderer> is the determinant of the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(n-1) \times (n-1)$</math-renderer> matrix obtained by deleting the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$i$</math-renderer>-th row and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$j$</math-renderer>
-th column of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>.</li>
<li>The cofactor <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C_{ij}$</math-renderer> is defined by</li>
</ul>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
C_{ij} = (-1)^{i+j} M_{ij}.
$$</math-renderer></p>
<p dir="auto">The sign factor <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(-1)^{i+j}$</math-renderer> alternates in a checkerboard pattern:</p>
<p dir="auto">$$
\begin{bmatrix}</p>
<ul dir="auto">
<li>&amp; - &amp; + &amp; - &amp; \cdots \</li>
</ul>
<ul dir="auto">
<li>&amp; + &amp; - &amp; + &amp; \cdots \</li>
</ul>
<ul dir="auto">
<li>&amp; - &amp; + &amp; - &amp; \cdots \
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots
\end{bmatrix}.
$$</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Cofactor Expansion Formula</h3><a id="user-content-cofactor-expansion-formula" aria-label="Permalink: Cofactor Expansion Formula" href="#cofactor-expansion-formula"></a></p>
<p dir="auto">The determinant of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> can be computed by expanding along any row or any column:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(A) = \sum_{j=1}^n a_{ij} C_{ij} \quad \text{(expansion along row (i))},
$$</math-renderer></p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(A) = \sum_{i=1}^n a_{ij} C_{ij} \quad \text{(expansion along column (j))}.
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example</h3><a id="user-content-example-4" aria-label="Permalink: Example" href="#example-4"></a></p>
<p dir="auto">Example 6.3.1.
Compute</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
1 &amp; 2 &amp; 3 \\
0 &amp; 4 &amp; 5 \\
1 &amp; 0 &amp; 6
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Expand along the first row:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(A) = 1 \cdot C_{11} + 2 \cdot C_{12} + 3 \cdot C_{13}.
$$</math-renderer></p>
<ul dir="auto">
<li>For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C_{11}$</math-renderer>:
$M_{11} = \det \begin{bmatrix} 4 &amp; 5 \ 0 &amp; 6 \end{bmatrix} = 24$, so <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C_{11} = (+1)(24) = 24$</math-renderer>.</li>
<li>For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C_{12}$</math-renderer>:
$M_{12} = \det \begin{bmatrix} 0 &amp; 5 \ 1 &amp; 6 \end{bmatrix} = 0 - 5 = -5$, so <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C_{12} = (-1)(-5) = 5$</math-renderer>.</li>
<li>For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C_{13}$</math-renderer>:
$M_{13} = \det \begin{bmatrix} 0 &amp; 4 \ 1 &amp; 0 \end{bmatrix} = 0 - 4 = -4$, so <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C_{13} = (+1)(-4) = -4$</math-renderer>.</li>
</ul>
<p dir="auto">Thus,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(A) = 1(24) + 2(5) + 3(-4) = 24 + 10 - 12 = 22.
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Properties of Cofactor Expansion</h3><a id="user-content-properties-of-cofactor-expansion" aria-label="Permalink: Properties of Cofactor Expansion" href="#properties-of-cofactor-expansion"></a></p>
<ol dir="auto">
<li>Expansion along any row or column yields the same result.</li>
<li>The cofactor expansion provides a recursive definition of determinant: a determinant of size <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> is expressed in
terms of determinants of size <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n-1$</math-renderer>.</li>
<li>Cofactors are fundamental in constructing the adjugate matrix, which gives a formula for inverses:</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A^{-1} = \frac{1}{\det(A)} , \text{adj}(A), \quad \text{where adj}(A) = [C_{ji}].
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-11" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-11"></a></p>
<p dir="auto">Cofactor expansion breaks down the determinant into contributions from sub-volumes defined by fixing one row or column
at a time. Each cofactor measures how that row/column influences the overall volume scaling.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-21" aria-label="Permalink: Why this matters" href="#why-this-matters-21"></a></p>
<p dir="auto">Cofactor expansion generalizes the small-matrix formulas and provides a conceptual definition of determinants. While not
the most efficient way to compute determinants for large matrices, it is essential for theory, proofs, and connections
to adjugates, Cramer’s rule, and classical geometry.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 6.3</h3><a id="user-content-exercises-63" aria-label="Permalink: Exercises 6.3" href="#exercises-63"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Compute the determinant of</p>
<p dir="auto">$$
\begin{bmatrix}
2 &amp; 0 &amp; 1 \
3 &amp; -1 &amp; 4 \
1 &amp; 2 &amp; 0
\end{bmatrix}
$$</p>
<p dir="auto">by cofactor expansion along the first column.</p>
</li>
<li>
<p dir="auto">Verify that expanding along the second row of Example 6.3.1 gives the same determinant.</p>
</li>
<li>
<p dir="auto">Prove that expansion along any row gives the same value.</p>
</li>
<li>
<p dir="auto">Show that if a row of a matrix is zero, then its determinant is zero.</p>
</li>
<li>
<p dir="auto">Use cofactor expansion to prove that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) = \det(A^T)$</math-renderer>.</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">6.4 Applications (Volume, Invertibility Test)</h2><a id="user-content-64-applications-volume-invertibility-test" aria-label="Permalink: 6.4 Applications (Volume, Invertibility Test)" href="#64-applications-volume-invertibility-test"></a></p>
<p dir="auto">Determinants are not merely algebraic curiosities; they have concrete geometric and computational uses. Two of the most
important applications are measuring volumes and testing invertibility of matrices.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Determinants as Volume Scalers</h3><a id="user-content-determinants-as-volume-scalers" aria-label="Permalink: Determinants as Volume Scalers" href="#determinants-as-volume-scalers"></a></p>
<p dir="auto">Given vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n \in \mathbb{R}^n$</math-renderer>, arrange them as columns of a matrix:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
| &amp; | &amp; &amp; | \\
\mathbf{v}_1 &amp; \mathbf{v}_2 &amp; \cdots &amp; \mathbf{v}_n \\
| &amp; | &amp; &amp; |
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\det(A)|$</math-renderer> equals the volume of the parallelepiped spanned by these vectors.</p>
<ul dir="auto">
<li>In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\det(A)|$</math-renderer> gives the area of the parallelogram spanned by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_1, \mathbf{v}_2$</math-renderer>.</li>
<li>In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\det(A)|$</math-renderer> gives the volume of the parallelepiped spanned
by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3$</math-renderer>.</li>
<li>In higher dimensions, it generalizes to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer>-dimensional volume (hypervolume).</li>
</ul>
<p dir="auto">Example 6.4.1.
Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{v}_1 = (1,0,0), \quad \mathbf{v}_2 = (1,1,0), \quad \mathbf{v}_3 = (1,1,1).
$$</math-renderer></p>
<p dir="auto">Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
1 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 1 \\
0 &amp; 0 &amp; 1
\end{bmatrix}, \quad \det(A) = 1.
$$</math-renderer></p>
<p dir="auto">So the parallelepiped has volume <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$1$</math-renderer>, even though the vectors are not orthogonal.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Invertibility Test</h3><a id="user-content-invertibility-test" aria-label="Permalink: Invertibility Test" href="#invertibility-test"></a></p>
<p dir="auto">A square matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is invertible if and only if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) \neq 0$</math-renderer>.</p>
<ul dir="auto">
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) = 0$</math-renderer>: the transformation collapses space into a lower dimension (area/volume is zero). No inverse exists.</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) \neq 0$</math-renderer>: the transformation scales volume by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\det(A)|$</math-renderer>, and is reversible.</li>
</ul>
<p dir="auto">Example 6.4.2.
The matrix</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
B = \begin{bmatrix} 2 &amp; 4 \ 1 &amp; 2 \end{bmatrix}
$$</math-renderer></p>
<p dir="auto">has determinant <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(B) = 2 \cdot 2 - 4 \cdot 1 = 0$</math-renderer>.
Thus, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$B$</math-renderer> is not invertible. Geometrically, the two column vectors are collinear, spanning only a line
in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Cramer’s Rule</h3><a id="user-content-cramers-rule" aria-label="Permalink: Cramer’s Rule" href="#cramers-rule"></a></p>
<p dir="auto">Determinants also provide an explicit formula for solving systems of linear equations when the matrix is invertible.
For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A\mathbf{x} = \mathbf{b}$</math-renderer> with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{n \times n}$</math-renderer>:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
x_i = \frac{\det(A_i)}{\det(A)},
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A_i$</math-renderer> is obtained by replacing the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$i$</math-renderer>-th column of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{b}$</math-renderer>.
While inefficient computationally, Cramer’s rule highlights the determinant’s role in solutions and uniqueness.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Orientation</h3><a id="user-content-orientation" aria-label="Permalink: Orientation" href="#orientation"></a></p>
<p dir="auto">The sign of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A)$</math-renderer> indicates whether a transformation preserves or reverses orientation. For example, a reflection in
the plane has determinant <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$-1$</math-renderer>, flipping handedness.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-22" aria-label="Permalink: Why this matters" href="#why-this-matters-22"></a></p>
<p dir="auto">Determinants condense key information: they measure scaling, test invertibility, and track orientation. These insights
are indispensable in geometry (areas and volumes), analysis (Jacobian determinants in calculus), and computation (
solving systems and checking singularity).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 6.4</h3><a id="user-content-exercises-64" aria-label="Permalink: Exercises 6.4" href="#exercises-64"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Compute the area of the parallelogram spanned by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(2,1)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,3)$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Find the volume of the parallelepiped spanned by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0,0), (1,1,0), (1,1,1)$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Determine whether the matrix $\begin{bmatrix} 1 &amp; 2 \ 3 &amp; 6 \end{bmatrix}$ is invertible. Justify using
determinants.</p>
</li>
<li>
<p dir="auto">Use Cramer’s rule to solve</p>
<p dir="auto">$$
\begin{cases}
x + y = 3, \
2x - y = 0.
\end{cases}
$$</p>
</li>
<li>
<p dir="auto">Explain geometrically why a determinant of zero implies no inverse exists.</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Chapter 7. Inner Product Spaces</h2><a id="user-content-chapter-7-inner-product-spaces" aria-label="Permalink: Chapter 7. Inner Product Spaces" href="#chapter-7-inner-product-spaces"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">7.1 Inner Products and Norms</h2><a id="user-content-71-inner-products-and-norms" aria-label="Permalink: 7.1 Inner Products and Norms" href="#71-inner-products-and-norms"></a></p>
<p dir="auto">To extend the geometric ideas of length, distance, and angle beyond <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, we introduce
inner products. Inner products provide a way of measuring similarity between vectors, while norms derived from them
measure length. These concepts are the foundation of geometry inside vector spaces.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Inner Product</h3><a id="user-content-inner-product" aria-label="Permalink: Inner Product" href="#inner-product"></a></p>
<p dir="auto">An inner product on a real vector space <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> is a function</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\langle \cdot, \cdot \rangle : V \times V \to \mathbb{R}
$$</math-renderer></p>
<p dir="auto">that assigns to each pair of vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(\mathbf{u}, \mathbf{v})$</math-renderer> a real number, subject to the following properties:</p>
<ol dir="auto">
<li>
<p dir="auto">Symmetry:
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\langle \mathbf{u}, \mathbf{v} \rangle = \langle \mathbf{v}, \mathbf{u} \rangle.$</math-renderer></p>
</li>
<li>
<p dir="auto">Linearity in the first argument:
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\langle a\mathbf{u} + b\mathbf{w}, \mathbf{v} \rangle = a \langle \mathbf{u}, \mathbf{v} \rangle + b \langle \mathbf{w}, \mathbf{v} \rangle.$</math-renderer></p>
</li>
<li>
<p dir="auto">Positive-definiteness:
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\langle \mathbf{v}, \mathbf{v} \rangle \geq 0$</math-renderer>, and equality holds if and only if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = \mathbf{0}$</math-renderer>.</p>
</li>
</ol>
<p dir="auto">The standard inner product on <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer> is the dot product:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\langle \mathbf{u}, \mathbf{v} \rangle = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n.
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Norms</h3><a id="user-content-norms" aria-label="Permalink: Norms" href="#norms"></a></p>
<p dir="auto">The norm of a vector is its length, defined in terms of the inner product:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
|\mathbf{v}| = \sqrt{\langle \mathbf{v}, \mathbf{v} \rangle}.
$$</math-renderer></p>
<p dir="auto">For the dot product in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
|(x_1, x_2, \dots, x_n)| = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}.
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Angles Between Vectors</h3><a id="user-content-angles-between-vectors-1" aria-label="Permalink: Angles Between Vectors" href="#angles-between-vectors-1"></a></p>
<p dir="auto">The inner product allows us to define the angle <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\theta$</math-renderer> between two nonzero vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}, \mathbf{v}$</math-renderer> by</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\cos \theta = \frac{\langle \mathbf{u}, \mathbf{v} \rangle}{|\mathbf{u}| , |\mathbf{v}|}.
$$</math-renderer></p>
<p dir="auto">Thus, two vectors are orthogonal if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\langle \mathbf{u}, \mathbf{v} \rangle = 0$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Examples</h3><a id="user-content-examples-5" aria-label="Permalink: Examples" href="#examples-5"></a></p>
<p dir="auto">Example 7.1.1.
In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>, with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (1,2)$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (3,4)$</math-renderer>:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\langle \mathbf{u}, \mathbf{v} \rangle = 1\cdot 3 + 2\cdot 4 = 11.
$$</math-renderer></p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
|\mathbf{u}| = \sqrt{1^2 + 2^2} = \sqrt{5}, \quad |\mathbf{v}| = \sqrt{3^2 + 4^2} = 5.
$$</math-renderer></p>
<p dir="auto">So,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\cos \theta = \frac{11}{\sqrt{5}\cdot 5}.
$$</math-renderer></p>
<p dir="auto">Example 7.1.2.
In the function space <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C[0,1]$</math-renderer>, the inner product</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\langle f, g \rangle = \int_0^1 f(x) g(x), dx
$$</math-renderer></p>
<p dir="auto">defines a length</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
|f| = \sqrt{\int_0^1 f(x)^2 dx}.
$$</math-renderer></p>
<p dir="auto">This generalizes geometry to infinite-dimensional spaces.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-12" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-12"></a></p>
<ul dir="auto">
<li>Inner product: measures similarity between vectors.</li>
<li>Norm: length of a vector.</li>
<li>Angle: measure of alignment between two directions.</li>
</ul>
<p dir="auto">These concepts unify algebraic operations with geometric intuition.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-23" aria-label="Permalink: Why this matters" href="#why-this-matters-23"></a></p>
<p dir="auto">Inner products and norms allow us to extend geometry into abstract vector spaces. They form the basis of orthogonality,
projections, Fourier series, least squares approximation, and many applications in physics and machine learning.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 7.1</h3><a id="user-content-exercises-71" aria-label="Permalink: Exercises 7.1" href="#exercises-71"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Compute <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\langle (2,-1,3), (1,4,0) \rangle$</math-renderer>. Then find the angle between them.</p>
</li>
<li>
<p dir="auto">Show that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|(x,y)| = \sqrt{x^2+y^2}$</math-renderer> satisfies the properties of a norm.</p>
</li>
<li>
<p dir="auto">In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, verify that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1,0)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,-1,0)$</math-renderer> are orthogonal.</p>
</li>
<li>
<p dir="auto">In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C[0,1]$</math-renderer>, compute <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\langle f,g \rangle$</math-renderer> for <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$f(x)=x$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$g(x)=1$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Prove the Cauchy–Schwarz inequality:</p>
<p dir="auto">$$
|\langle \mathbf{u}, \mathbf{v} \rangle| \leq |\mathbf{u}| , |\mathbf{v}|.
$$</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">7.2 Orthogonal Projections</h2><a id="user-content-72-orthogonal-projections" aria-label="Permalink: 7.2 Orthogonal Projections" href="#72-orthogonal-projections"></a></p>
<p dir="auto">One of the most useful applications of inner products is the notion of orthogonal projection. Projection allows us to
approximate a vector by another lying in a subspace, minimizing error in the sense of distance. This idea underpins
geometry, statistics, and numerical analysis.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Projection onto a Line</h3><a id="user-content-projection-onto-a-line" aria-label="Permalink: Projection onto a Line" href="#projection-onto-a-line"></a></p>
<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} \in \mathbb{R}^n$</math-renderer> be a nonzero vector. The line spanned by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer> is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
L = { c\mathbf{u} \mid c \in \mathbb{R} }.
$$</math-renderer></p>
<p dir="auto">Given a vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer>, the projection of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer> onto <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer> is the vector in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$L$</math-renderer> closest
to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer>. Geometrically, it is the shadow of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer> on the line.</p>
<p dir="auto">The formula is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{\langle \mathbf{v}, \mathbf{u} \rangle}{\langle \mathbf{u}, \mathbf{u} \rangle} , \mathbf{u}.
$$</math-renderer></p>
<p dir="auto">The error vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})$</math-renderer> is orthogonal to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example 7.2.1</h3><a id="user-content-example-721" aria-label="Permalink: Example 7.2.1" href="#example-721"></a></p>
<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (1,2)$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (3,1)$</math-renderer>.</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\langle \mathbf{v}, \mathbf{u} \rangle = 3\cdot 1 + 1\cdot 2 = 5, \quad
\langle \mathbf{u}, \mathbf{u} \rangle = 1^2 + 2^2 = 5.
$$</math-renderer></p>
<p dir="auto">So</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{5}{5}(1,2) = (1,2).
$$</math-renderer></p>
<p dir="auto">The error vector is <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(3,1) - (1,2) = (2,-1)$</math-renderer>, which is orthogonal to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,2)$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Projection onto a Subspace</h3><a id="user-content-projection-onto-a-subspace" aria-label="Permalink: Projection onto a Subspace" href="#projection-onto-a-subspace"></a></p>
<p dir="auto">Suppose <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W \subseteq \mathbb{R}^n$</math-renderer> is a subspace with orthonormal basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${ \mathbf{w}_1, \dots, \mathbf{w}_k }$</math-renderer>. The
projection of a vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer> onto <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W$</math-renderer> is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{proj}_{W}(\mathbf{v}) = \langle \mathbf{v}, \mathbf{w}_1 \rangle \mathbf{w}_1 + \cdots + \langle \mathbf{v}, \mathbf{w}_k \rangle \mathbf{w}_k.
$$</math-renderer></p>
<p dir="auto">This is the unique vector in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W$</math-renderer> closest to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer>. The difference <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} - \text{proj}_{W}(\mathbf{v})$</math-renderer> is
orthogonal to all of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Least Squares Approximation</h3><a id="user-content-least-squares-approximation" aria-label="Permalink: Least Squares Approximation" href="#least-squares-approximation"></a></p>
<p dir="auto">Orthogonal projection explains the method of least squares. To solve an overdetermined
system <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A\mathbf{x} \approx \mathbf{b}$</math-renderer>, we seek the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x}$</math-renderer> that makes <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A\mathbf{x}$</math-renderer> the projection
of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{b}$</math-renderer> onto the column space of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>. This gives the normal equations</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A^T A \mathbf{x} = A^T \mathbf{b}.
$$</math-renderer></p>
<p dir="auto">Thus, least squares is just projection in disguise.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-13" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-13"></a></p>
<ul dir="auto">
<li>Projection finds the closest point in a subspace to a given vector.</li>
<li>It minimizes distance (error) in the sense of Euclidean norm.</li>
<li>Orthogonality ensures the error vector points directly away from the subspace.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-24" aria-label="Permalink: Why this matters" href="#why-this-matters-24"></a></p>
<p dir="auto">Orthogonal projection is central in both pure and applied mathematics. It underlies the geometry of subspaces, the
theory of Fourier series, regression in statistics, and approximation methods in numerical linear algebra. Whenever we
fit data with a simpler model, projection is at work.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 7.2</h3><a id="user-content-exercises-72" aria-label="Permalink: Exercises 7.2" href="#exercises-72"></a></p>
<ol dir="auto">
<li>Compute the projection of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(2,3)$</math-renderer> onto the vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1)$</math-renderer>.</li>
<li>Show that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})$</math-renderer> is orthogonal to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer>.</li>
<li>Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W = \text{span}{(1,0,0), (0,1,0)} \subseteq \mathbb{R}^3$</math-renderer>. Find the projection of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,2,3)$</math-renderer> onto <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W$</math-renderer>.</li>
<li>Explain why least squares fitting corresponds to projection onto the column space of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>.</li>
<li>Prove that projection onto a subspace <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W$</math-renderer> is unique: there is exactly one closest vector in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W$</math-renderer> to a
given <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer>.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">7.3 Gram–Schmidt Process</h2><a id="user-content-73-gramschmidt-process" aria-label="Permalink: 7.3 Gram–Schmidt Process" href="#73-gramschmidt-process"></a></p>
<p dir="auto">The Gram–Schmidt process is a systematic way to turn any linearly independent set of vectors into an orthonormal basis.
This is especially useful because orthonormal bases simplify computations: inner products become simple coordinate
comparisons, and projections take clean forms.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Idea</h3><a id="user-content-the-idea" aria-label="Permalink: The Idea" href="#the-idea"></a></p>
<p dir="auto">Given a linearly independent set of vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n}$</math-renderer> in an inner product
space, we want to construct an orthonormal set <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n}$</math-renderer> that spans the same
subspace.</p>
<p dir="auto">We proceed step by step:</p>
<ol dir="auto">
<li>Start with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_1$</math-renderer>, normalize it to get <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}_1$</math-renderer>.</li>
<li>Subtract from <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_2$</math-renderer> its projection onto <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}_1$</math-renderer>, leaving a vector orthogonal to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}_1$</math-renderer>.
Normalize to get <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}_2$</math-renderer>.</li>
<li>For each <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_k$</math-renderer>, subtract projections onto all previously
constructed $\mathbf{u}<em>1, \dots, \mathbf{u}</em>{k-1}$, then normalize.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Algorithm</h3><a id="user-content-the-algorithm" aria-label="Permalink: The Algorithm" href="#the-algorithm"></a></p>
<p dir="auto">For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$k = 1, 2, \dots, n$</math-renderer>:</p>
<p dir="auto">$$
\mathbf{w}_k = \mathbf{v}<em>k - \sum</em>{j=1}^{k-1} \langle \mathbf{v}_k, \mathbf{u}_j \rangle \mathbf{u}_j,
$$</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u}_k = \frac{\mathbf{w}_k}{|\mathbf{w}_k|}.
$$</math-renderer></p>
<p dir="auto">The result <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${\mathbf{u}_1, \dots, \mathbf{u}_n}$</math-renderer> is an orthonormal basis of the span of the original vectors.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example 7.3.1</h3><a id="user-content-example-731" aria-label="Permalink: Example 7.3.1" href="#example-731"></a></p>
<p dir="auto">Take <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_1 = (1,1,0), \ \mathbf{v}_2 = (1,0,1), \ \mathbf{v}_3 = (0,1,1)$</math-renderer> in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>.</p>
<ol dir="auto">
<li>Normalize <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_1$</math-renderer>:</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u}_1 = \frac{1}{\sqrt{2}}(1,1,0).
$$</math-renderer></p>
<ol start="2" dir="auto">
<li>Subtract projection of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_2$</math-renderer> on <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}_1$</math-renderer>:</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{w}_2 = \mathbf{v}_2 - \langle \mathbf{v}_2,\mathbf{u}_1 \rangle \mathbf{u}_1.
$$</math-renderer></p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\langle \mathbf{v}_2,\mathbf{u}_1 \rangle = \frac{1}{\sqrt{2}}(1\cdot 1 + 0\cdot 1 + 1\cdot 0) = \tfrac{1}{\sqrt{2}}.
$$</math-renderer></p>
<p dir="auto">So</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{w}_2 = (1,0,1) - \tfrac{1}{\sqrt{2}}\cdot \tfrac{1}{\sqrt{2}}(1,1,0)
= (1,0,1) - \tfrac{1}{2}(1,1,0)
= \left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right).
$$</math-renderer></p>
<p dir="auto">Normalize:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u}_2 = \frac{1}{\sqrt{\tfrac{1}{4}+\tfrac{1}{4}+1}} \left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right)
= \frac{1}{\sqrt{\tfrac{3}{2}}}\left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right).
$$</math-renderer></p>
<ol start="3" dir="auto">
<li>Subtract projections from <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_3$</math-renderer>:</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{w}_3 = \mathbf{v}_3 - \langle \mathbf{v}_3,\mathbf{u}_1 \rangle \mathbf{u}_1 - \langle \mathbf{v}_3,\mathbf{u}_2 \rangle \mathbf{u}_2.
$$</math-renderer></p>
<p dir="auto">After computing, normalize to obtain <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}_3$</math-renderer>.</p>
<p dir="auto">The result is an orthonormal basis of the span of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3}$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-14" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-14"></a></p>
<p dir="auto">Gram–Schmidt is like straightening out a set of vectors: you start with the original directions and adjust each new
vector to be perpendicular to all previous ones. Then you scale to unit length. The process ensures orthogonality while
preserving the span.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-25" aria-label="Permalink: Why this matters" href="#why-this-matters-25"></a></p>
<p dir="auto">Orthonormal bases simplify inner products, projections, and computations in general. They make coordinate systems easier
to work with and are crucial in numerical methods, QR decomposition, Fourier analysis, and statistics (orthogonal
polynomials, principal component analysis).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 7.3</h3><a id="user-content-exercises-73" aria-label="Permalink: Exercises 7.3" href="#exercises-73"></a></p>
<ol dir="auto">
<li>Apply Gram–Schmidt to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0), (1,1)$</math-renderer> in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>.</li>
<li>Orthogonalize <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1,1), (1,0,1)$</math-renderer> in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>.</li>
<li>Prove that each step of Gram–Schmidt yields a vector orthogonal to all previous ones.</li>
<li>Show that Gram–Schmidt preserves the span of the original vectors.</li>
<li>Explain how Gram–Schmidt leads to the QR decomposition of a matrix.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">7.4 Orthonormal Bases</h2><a id="user-content-74-orthonormal-bases" aria-label="Permalink: 7.4 Orthonormal Bases" href="#74-orthonormal-bases"></a></p>
<p dir="auto">An orthonormal basis is a basis of a vector space in which all vectors are both orthogonal to each other and have unit
length. Such bases are the most convenient possible coordinate systems: computations involving inner products,
projections, and norms become exceptionally simple.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Definition</h3><a id="user-content-definition-3" aria-label="Permalink: Definition" href="#definition-3"></a></p>
<p dir="auto">A set of vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n}$</math-renderer> in an inner product space <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> is called an
orthonormal basis if</p>
<ol dir="auto">
<li>
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\langle \mathbf{u}_i, \mathbf{u}_j \rangle = 0$</math-renderer> whenever <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$i \neq j$</math-renderer> (orthogonality),</li>
<li>
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\mathbf{u}_i| = 1$</math-renderer> for all <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$i$</math-renderer> (normalization),</li>
<li>The set spans <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer>.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Examples</h3><a id="user-content-examples-6" aria-label="Permalink: Examples" href="#examples-6"></a></p>
<p dir="auto">Example 7.4.1. In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>, the standard basis</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{e}_1 = (1,0), \quad \mathbf{e}_2 = (0,1)
$$</math-renderer></p>
<p dir="auto">is orthonormal under the dot product.</p>
<p dir="auto">Example 7.4.2. In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, the standard basis</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{e}_1 = (1,0,0), \quad \mathbf{e}_2 = (0,1,0), \quad \mathbf{e}_3 = (0,0,1)
$$</math-renderer></p>
<p dir="auto">is orthonormal.</p>
<p dir="auto">Example 7.4.3. Fourier basis on functions:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
{1, \cos x, \sin x, \cos 2x, \sin 2x, \dots}
$$</math-renderer></p>
<p dir="auto">is an orthogonal set in the space of square-integrable functions on <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$[-\pi,\pi]$</math-renderer> with inner product</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\langle f,g \rangle = \int_{-\pi}^{\pi} f(x) g(x), dx.
$$</math-renderer></p>
<p dir="auto">After normalization, it becomes an orthonormal basis.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Properties</h3><a id="user-content-properties" aria-label="Permalink: Properties" href="#properties"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Coordinate simplicity: If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${\mathbf{u}_1,\dots,\mathbf{u}_n}$</math-renderer> is an orthonormal basis of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer>, then any
vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}\in V$</math-renderer> has coordinates</p>
<p dir="auto">$$
[\mathbf{v}] = \begin{bmatrix} \langle \mathbf{v}, \mathbf{u}_1 \rangle \ \vdots \ \langle \mathbf{v}, \mathbf{u}_n \rangle \end{bmatrix}.
$$</p>
<p dir="auto">That is, coordinates are just inner products.</p>
</li>
<li>
<p dir="auto">Parseval’s identity:
For any <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} \in V$</math-renderer>,</p>
<p dir="auto">$$
|\mathbf{v}|^2 = \sum_{i=1}^n |\langle \mathbf{v}, \mathbf{u}_i \rangle|^2.
$$</p>
</li>
<li>
<p dir="auto">Projections:
The orthogonal projection onto the span of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${\mathbf{u}_1,\dots,\mathbf{u}_k}$</math-renderer> is</p>
<p dir="auto">$$
\text{proj}(\mathbf{v}) = \sum_{i=1}^k \langle \mathbf{v}, \mathbf{u}_i \rangle \mathbf{u}_i.
$$</p>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Constructing Orthonormal Bases</h3><a id="user-content-constructing-orthonormal-bases" aria-label="Permalink: Constructing Orthonormal Bases" href="#constructing-orthonormal-bases"></a></p>
<ul dir="auto">
<li>Start with any linearly independent set, then apply the Gram–Schmidt process to obtain an orthonormal set spanning the
same subspace.</li>
<li>In practice, orthonormal bases are often chosen for numerical stability and simplicity of computation.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-15" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-15"></a></p>
<p dir="auto">An orthonormal basis is like a perfectly aligned and equally scaled coordinate system. Distances and angles are computed
directly using coordinates without correction factors. They are the ideal rulers of linear algebra.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-26" aria-label="Permalink: Why this matters" href="#why-this-matters-26"></a></p>
<p dir="auto">Orthonormal bases simplify every aspect of linear algebra: solving systems, computing projections, expanding functions,
diagonalizing symmetric matrices, and working with Fourier series. In data science, principal component analysis
produces orthonormal directions capturing maximum variance.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 7.4</h3><a id="user-content-exercises-74" aria-label="Permalink: Exercises 7.4" href="#exercises-74"></a></p>
<ol dir="auto">
<li>Verify that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1/\sqrt{2})(1,1)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1/\sqrt{2})(1,-1)$</math-renderer> form an orthonormal basis of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>.</li>
<li>Express <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(3,4)$</math-renderer> in terms of the orthonormal basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${(1/\sqrt{2})(1,1), (1/\sqrt{2})(1,-1)}$</math-renderer>.</li>
<li>Prove Parseval’s identity for <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer> with the dot product.</li>
<li>Find an orthonormal basis for the plane <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$x+y+z=0$</math-renderer> in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>.</li>
<li>Explain why orthonormal bases are numerically more stable than arbitrary bases in computations.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Chapter 8. Eigenvalues and eigenvectors</h2><a id="user-content-chapter-8-eigenvalues-and-eigenvectors" aria-label="Permalink: Chapter 8. Eigenvalues and eigenvectors" href="#chapter-8-eigenvalues-and-eigenvectors"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">8.1 Definitions and Intuition</h2><a id="user-content-81-definitions-and-intuition" aria-label="Permalink: 8.1 Definitions and Intuition" href="#81-definitions-and-intuition"></a></p>
<p dir="auto">The concepts of eigenvalues and eigenvectors reveal the most fundamental behavior of linear transformations. They
identify the special directions in which a transformation acts by simple stretching or compressing, without rotation or
distortion.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Definition</h3><a id="user-content-definition-4" aria-label="Permalink: Definition" href="#definition-4"></a></p>
<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T: V \to V$</math-renderer> be a linear transformation on a vector space <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer>. A nonzero vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} \in V$</math-renderer> is called an
eigenvector of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T$</math-renderer> if</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
T(\mathbf{v}) = \lambda \mathbf{v}
$$</math-renderer></p>
<p dir="auto">for some scalar <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda \in \mathbb{R}$</math-renderer> (or <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{C}$</math-renderer>). The scalar <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda$</math-renderer> is the eigenvalue corresponding
to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer>.</p>
<p dir="auto">Equivalently, if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is the matrix of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T$</math-renderer>, then eigenvalues and eigenvectors satisfy</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A\mathbf{v} = \lambda \mathbf{v}.
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basic Examples</h3><a id="user-content-basic-examples" aria-label="Permalink: Basic Examples" href="#basic-examples"></a></p>
<p dir="auto">Example 8.1.1.
Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; 0 \ 0 &amp; 3 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A(1,0)^T = 2(1,0)^T, \quad A(0,1)^T = 3(0,1)^T.
$$</math-renderer></p>
<p dir="auto">So <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0)$</math-renderer> is an eigenvector with eigenvalue <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$2$</math-renderer>, and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(0,1)$</math-renderer> is an eigenvector with eigenvalue <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3$</math-renderer>.</p>
<p dir="auto">Example 8.1.2.
Rotation matrix in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
R_\theta = \begin{bmatrix} \cos\theta &amp; -\sin\theta \ \sin\theta &amp; \cos\theta \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\theta \neq 0, \pi$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$R_\theta$</math-renderer> has no real eigenvalues: every vector is rotated, not scaled. Over <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{C}$</math-renderer>,
however, it has eigenvalues <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$e^{i\theta}, e^{-i\theta}$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Algebraic Formulation</h3><a id="user-content-algebraic-formulation" aria-label="Permalink: Algebraic Formulation" href="#algebraic-formulation"></a></p>
<p dir="auto">Eigenvalues arise from solving the characteristic equation:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(A - \lambda I) = 0.
$$</math-renderer></p>
<p dir="auto">This polynomial in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda$</math-renderer> is the characteristic polynomial. Its roots are the eigenvalues.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Intuition</h3><a id="user-content-geometric-intuition" aria-label="Permalink: Geometric Intuition" href="#geometric-intuition"></a></p>
<ul dir="auto">
<li>Eigenvectors are directions that remain unchanged in orientation under a transformation; only their length is scaled.</li>
<li>Eigenvalues tell us the scaling factor along those directions.</li>
<li>If a matrix has many independent eigenvectors, it can often be simplified (diagonalized) by changing basis.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Applications in Geometry and Science</h3><a id="user-content-applications-in-geometry-and-science" aria-label="Permalink: Applications in Geometry and Science" href="#applications-in-geometry-and-science"></a></p>
<ul dir="auto">
<li>Stretching along principal axes of an ellipse (quadratic forms).</li>
<li>Stable directions of dynamical systems.</li>
<li>Principal components in statistics and machine learning.</li>
<li>Quantum mechanics, where observables correspond to operators with eigenvalues.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-27" aria-label="Permalink: Why this matters" href="#why-this-matters-27"></a></p>
<p dir="auto">Eigenvalues and eigenvectors are a bridge between algebra and geometry. They provide a lens for understanding linear
transformations in their simplest form. Nearly every application of linear algebra-differential equations, statistics,
physics, computer science-relies on eigen-analysis.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 8.1</h3><a id="user-content-exercises-81" aria-label="Permalink: Exercises 8.1" href="#exercises-81"></a></p>
<ol dir="auto">
<li>Find the eigenvalues and eigenvectors of
$\begin{bmatrix} 4 &amp; 0 \ 0 &amp; -1 \end{bmatrix}$.</li>
<li>Show that every scalar multiple of an eigenvector is again an eigenvector for the same eigenvalue.</li>
<li>Verify that the rotation matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$R_\theta$</math-renderer> has no real eigenvalues unless <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\theta = 0$</math-renderer> or <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\pi$</math-renderer>.</li>
<li>Compute the characteristic polynomial of
$\begin{bmatrix} 1 &amp; 2 \ 2 &amp; 1 \end{bmatrix}$.</li>
<li>Explain geometrically what eigenvectors and eigenvalues represent for the shear matrix
$\begin{bmatrix} 1 &amp; 1 \ 0 &amp; 1 \end{bmatrix}$.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">8.2 Diagonalization</h2><a id="user-content-82-diagonalization" aria-label="Permalink: 8.2 Diagonalization" href="#82-diagonalization"></a></p>
<p dir="auto">A central goal in linear algebra is to simplify the action of a matrix by choosing a good basis. Diagonalization is the
process of rewriting a matrix so that it acts by simple scaling along independent directions. This makes computations
such as powers, exponentials, and solving differential equations far easier.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Definition</h3><a id="user-content-definition-5" aria-label="Permalink: Definition" href="#definition-5"></a></p>
<p dir="auto">A square matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{n \times n}$</math-renderer> is diagonalizable if there exists an invertible matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer> such that</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
P^{-1} A P = D,
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$D$</math-renderer> is a diagonal matrix.</p>
<p dir="auto">The diagonal entries of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$D$</math-renderer> are eigenvalues of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>, and the columns of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer> are the corresponding eigenvectors.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">When is a Matrix Diagonalizable?</h3><a id="user-content-when-is-a-matrix-diagonalizable" aria-label="Permalink: When is a Matrix Diagonalizable?" href="#when-is-a-matrix-diagonalizable"></a></p>
<ul dir="auto">
<li>A matrix is diagonalizable if it has <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> linearly independent eigenvectors.</li>
<li>Equivalently, the sum of the dimensions of its eigenspaces equals <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer>.</li>
<li>Symmetric matrices (over <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}$</math-renderer>) are always diagonalizable, with an orthonormal basis of eigenvectors.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example 8.2.1</h3><a id="user-content-example-821" aria-label="Permalink: Example 8.2.1" href="#example-821"></a></p>
<p dir="auto">Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 4 &amp; 1 \ 0 &amp; 2 \end{bmatrix}.
$$</math-renderer></p>
<ol dir="auto">
<li>Characteristic polynomial:</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(A - \lambda I) = (4-\lambda)(2-\lambda).
$$</math-renderer></p>
<p dir="auto">So eigenvalues are <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda_1 = 4$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda_2 = 2$</math-renderer>.</p>
<ol start="2" dir="auto">
<li>Eigenvectors:</li>
</ol>
<ul dir="auto">
<li>For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda = 4$</math-renderer>, solve <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(A-4I)\mathbf{v}=0$</math-renderer>:
$\begin{bmatrix} 0 &amp; 1 \ 0 &amp; -2 \end{bmatrix}\mathbf{v} = 0$, giving <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_1 = (1,0)$</math-renderer>.</li>
<li>For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda = 2$</math-renderer>: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(A-2I)\mathbf{v}=0$</math-renderer>, giving <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_2 = (1,-2)$</math-renderer>.</li>
</ul>
<ol start="3" dir="auto">
<li>Construct $P = \begin{bmatrix} 1 &amp; 1 \ 0 &amp; -2 \end{bmatrix}$. Then</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
P^{-1} A P = \begin{bmatrix} 4 &amp; 0 \ 0 &amp; 2 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Thus, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is diagonalizable.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why Diagonalize?</h3><a id="user-content-why-diagonalize" aria-label="Permalink: Why Diagonalize?" href="#why-diagonalize"></a></p>
<ul dir="auto">
<li>
<p dir="auto">Computing powers:
If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A = P D P^{-1}$</math-renderer>, then</p>
<p dir="auto">$$
A^k = P D^k P^{-1}.
$$</p>
<p dir="auto">Since <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$D$</math-renderer> is diagonal, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$D^k$</math-renderer> is easy to compute.</p>
</li>
<li>
<p dir="auto">Matrix exponentials:
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$e^A = P e^D P^{-1}$</math-renderer>, useful in solving differential equations.</p>
</li>
<li>
<p dir="auto">Understanding geometry:
Diagonalization reveals the directions along which a transformation stretches or compresses space independently.</p>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Non-Diagonalizable Example</h3><a id="user-content-non-diagonalizable-example" aria-label="Permalink: Non-Diagonalizable Example" href="#non-diagonalizable-example"></a></p>
<p dir="auto">Not all matrices can be diagonalized.</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 1 &amp; 1 \ 0 &amp; 1 \end{bmatrix}
$$</math-renderer></p>
<p dir="auto">has only one eigenvalue <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda = 1$</math-renderer>, with eigenspace dimension 1. Since <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n=2$</math-renderer> but we only have 1 independent
eigenvector, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is not diagonalizable.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-16" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-16"></a></p>
<p dir="auto">Diagonalization means we have found a basis of eigenvectors. In this basis, the matrix acts by simple scaling along each
coordinate axis. It transforms complicated motion into independent 1D motions.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-28" aria-label="Permalink: Why this matters" href="#why-this-matters-28"></a></p>
<p dir="auto">Diagonalization is a cornerstone of linear algebra. It simplifies computation, reveals structure, and is the starting
point for the spectral theorem, Jordan form, and many applications in physics, engineering, and data science.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 8.2</h3><a id="user-content-exercises-82" aria-label="Permalink: Exercises 8.2" href="#exercises-82"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Diagonalize</p>
<p dir="auto">$$
A = \begin{bmatrix} 2 &amp; 0 \ 0 &amp; 3 \end{bmatrix}.
$$</p>
</li>
<li>
<p dir="auto">Determine whether</p>
<p dir="auto">$$
A = \begin{bmatrix} 1 &amp; 1 \ 0 &amp; 1 \end{bmatrix}
$$</p>
<p dir="auto">is diagonalizable. Why or why not?</p>
</li>
<li>
<p dir="auto">Find <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^5$</math-renderer> for</p>
<p dir="auto">$$
A = \begin{bmatrix} 4 &amp; 1 \ 0 &amp; 2 \end{bmatrix}
$$</p>
<p dir="auto">using diagonalization.</p>
</li>
<li>
<p dir="auto">Show that any <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n \times n$</math-renderer> matrix with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> distinct eigenvalues is diagonalizable.</p>
</li>
<li>
<p dir="auto">Explain why real symmetric matrices are always diagonalizable.</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">8.3 Characteristic Polynomials</h2><a id="user-content-83-characteristic-polynomials" aria-label="Permalink: 8.3 Characteristic Polynomials" href="#83-characteristic-polynomials"></a></p>
<p dir="auto">The key to finding eigenvalues is the characteristic polynomial of a matrix. This polynomial encodes the values
of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda$</math-renderer> for which the matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A - \lambda I$</math-renderer> fails to be invertible.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Definition</h3><a id="user-content-definition-6" aria-label="Permalink: Definition" href="#definition-6"></a></p>
<p dir="auto">For an <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n \times n$</math-renderer> matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>, the characteristic polynomial is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
p_A(\lambda) = \det(A - \lambda I).
$$</math-renderer></p>
<p dir="auto">The roots of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$p_A(\lambda)$</math-renderer> are the eigenvalues of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Examples</h3><a id="user-content-examples-7" aria-label="Permalink: Examples" href="#examples-7"></a></p>
<p dir="auto">Example 8.3.1.
Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; 1 \ 1 &amp; 2 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
p_A(\lambda) = \det!\begin{bmatrix} 2-\lambda &amp; 1 \ 1 &amp; 2-\lambda \end{bmatrix}
= (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3.
$$</math-renderer></p>
<p dir="auto">Thus eigenvalues are <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda = 1, 3$</math-renderer>.</p>
<p dir="auto">Example 8.3.2.
For</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 0 &amp; -1 \ 1 &amp; 0 \end{bmatrix}
$$</math-renderer></p>
<p dir="auto">(rotation by 90°),</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
p_A(\lambda) = \det!\begin{bmatrix} -\lambda &amp; -1 \ 1 &amp; -\lambda \end{bmatrix}
= \lambda^2 + 1.
$$</math-renderer></p>
<p dir="auto">Eigenvalues are <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda = \pm i$</math-renderer>. No real eigenvalues exist, consistent with pure rotation.</p>
<p dir="auto">Example 8.3.3.
For a triangular matrix</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; 1 &amp; 0 \ 0 &amp; 3 &amp; 5 \ 0 &amp; 0 &amp; 4 \end{bmatrix},
$$</math-renderer></p>
<p dir="auto">the determinant is simply the product of diagonal entries minus <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda$</math-renderer>:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
p_A(\lambda) = (2-\lambda)(3-\lambda)(4-\lambda).
$$</math-renderer></p>
<p dir="auto">So eigenvalues are <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$2, 3, 4$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Properties</h3><a id="user-content-properties-1" aria-label="Permalink: Properties" href="#properties-1"></a></p>
<ol dir="auto">
<li>
<p dir="auto">The characteristic polynomial of an <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n \times n$</math-renderer> matrix has degree <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer>.</p>
</li>
<li>
<p dir="auto">The sum of the eigenvalues (counted with multiplicity) equals the trace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>:</p>
<p dir="auto">$$
\text{tr}(A) = \lambda_1 + \cdots + \lambda_n.
$$</p>
</li>
<li>
<p dir="auto">The product of the eigenvalues equals the determinant of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>:</p>
<p dir="auto">$$
\det(A) = \lambda_1 \cdots \lambda_n.
$$</p>
</li>
<li>
<p dir="auto">Similar matrices have the same characteristic polynomial, hence the same eigenvalues.</p>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-17" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-17"></a></p>
<p dir="auto">The characteristic polynomial captures when <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A - \lambda I$</math-renderer> collapses space: its determinant is zero precisely when the
transformation <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A - \lambda I$</math-renderer> is singular. Thus, eigenvalues mark the critical scalings where the matrix loses
invertibility.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-29" aria-label="Permalink: Why this matters" href="#why-this-matters-29"></a></p>
<p dir="auto">Characteristic polynomials provide the computational tool to extract eigenvalues. They connect matrix invariants (trace
and determinant) with geometry, and form the foundation for diagonalization, spectral theorems, and stability analysis
in dynamical systems.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 8.3</h3><a id="user-content-exercises-83" aria-label="Permalink: Exercises 8.3" href="#exercises-83"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Compute the characteristic polynomial of</p>
<p dir="auto">$$
A = \begin{bmatrix} 4 &amp; 2 \ 1 &amp; 3 \end{bmatrix}.
$$</p>
</li>
<li>
<p dir="auto">Verify that the sum of the eigenvalues of
$\begin{bmatrix} 5 &amp; 0 \ 0 &amp; -2 \end{bmatrix}$
equals its trace, and their product equals its determinant.</p>
</li>
<li>
<p dir="auto">Show that for any triangular matrix, the eigenvalues are just the diagonal entries.</p>
</li>
<li>
<p dir="auto">Prove that if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$B$</math-renderer> are similar matrices, then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$p_A(\lambda) = p_B(\lambda)$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Compute the characteristic polynomial of
$\begin{bmatrix} 1 &amp; 1 &amp; 0 \ 0 &amp; 1 &amp; 1 \ 0 &amp; 0 &amp; 1 \end{bmatrix}$.</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">8.4 Applications (Differential Equations, Markov Chains)</h2><a id="user-content-84-applications-differential-equations-markov-chains" aria-label="Permalink: 8.4 Applications (Differential Equations, Markov Chains)" href="#84-applications-differential-equations-markov-chains"></a></p>
<p dir="auto">Eigenvalues and eigenvectors are not only central to the theory of linear algebra-they are indispensable tools across
mathematics and applied science. Two classic applications are solving systems of differential equations and analyzing
Markov chains.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Linear Differential Equations</h3><a id="user-content-linear-differential-equations" aria-label="Permalink: Linear Differential Equations" href="#linear-differential-equations"></a></p>
<p dir="auto">Consider the system</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\frac{d\mathbf{x}}{dt} = A \mathbf{x},
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is an <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n \times n$</math-renderer> matrix and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x}(t)$</math-renderer> is a vector-valued function.</p>
<p dir="auto">If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer> is an eigenvector of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> with eigenvalue <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda$</math-renderer>, then the function</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{x}(t) = e^{\lambda t}\mathbf{v}
$$</math-renderer></p>
<p dir="auto">is a solution.</p>
<ul dir="auto">
<li>
<p dir="auto">Eigenvalues determine the growth or decay rate:</p>
<ul dir="auto">
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda &amp;lt; 0$</math-renderer>, solutions decay (stable).</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda &amp;gt; 0$</math-renderer>, solutions grow (unstable).</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda$</math-renderer> is complex, oscillations occur.</li>
</ul>
</li>
</ul>
<p dir="auto">By combining eigenvector solutions, we can solve general initial conditions.</p>
<p dir="auto">Example 8.4.1.
Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; 0 \ 0 &amp; -1 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then eigenvalues are <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$2, -1$</math-renderer> with eigenvectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0)$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(0,1)$</math-renderer>. Solutions are</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{x}(t) = c_1 e^{2t}(1,0) + c_2 e^{-t}(0,1).
$$</math-renderer></p>
<p dir="auto">Thus one component grows exponentially, the other decays.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Markov Chains</h3><a id="user-content-markov-chains" aria-label="Permalink: Markov Chains" href="#markov-chains"></a></p>
<p dir="auto">A Markov chain is described by a stochastic matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer>, where each column sums to 1 and entries are nonnegative.
If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x}_k$</math-renderer> represents the probability distribution after <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$k$</math-renderer> steps, then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{x}_{k+1} = P \mathbf{x}_k.
$$</math-renderer></p>
<p dir="auto">Iterating gives</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{x}_k = P^k \mathbf{x}_0.
$$</math-renderer></p>
<p dir="auto">Understanding long-term behavior reduces to analyzing powers of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer>.</p>
<ul dir="auto">
<li>The eigenvalue <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda = 1$</math-renderer> always exists. Its eigenvector gives the steady-state distribution.</li>
<li>All other eigenvalues satisfy <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\lambda| \leq 1$</math-renderer>. Their influence decays as <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$k \to \infty$</math-renderer>.</li>
</ul>
<p dir="auto">Example 8.4.2.
Consider</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
P = \begin{bmatrix} 0.9 &amp; 0.5 \ 0.1 &amp; 0.5 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Eigenvalues are <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda_1 = 1$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda_2 = 0.4$</math-renderer>. The eigenvector for <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda = 1$</math-renderer> is proportional to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(5,1)$</math-renderer>.
Normalizing gives the steady state</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\pi = \left(\tfrac{5}{6}, \tfrac{1}{6}\right).
$$</math-renderer></p>
<p dir="auto">Thus, regardless of the starting distribution, the chain converges to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\pi$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-18" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-18"></a></p>
<ul dir="auto">
<li>In differential equations, eigenvalues determine the time evolution: exponential growth, decay, or oscillation.</li>
<li>In Markov chains, eigenvalues determine the long-term equilibrium of stochastic processes.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-30" aria-label="Permalink: Why this matters" href="#why-this-matters-30"></a></p>
<p dir="auto">Eigenvalue methods turn complex iterative or dynamical systems into tractable problems. In physics, engineering, and
finance, they describe stability and resonance. In computer science and statistics, they power algorithms from Google’s
PageRank to modern machine learning.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 8.4</h3><a id="user-content-exercises-84" aria-label="Permalink: Exercises 8.4" href="#exercises-84"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Solve $\tfrac{d}{dt}\mathbf{x} = \begin{bmatrix} 3 &amp; 0 \ 0 &amp; -2 \end{bmatrix}\mathbf{x}$.</p>
</li>
<li>
<p dir="auto">Show that if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> has a complex eigenvalue <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\alpha \pm i\beta$</math-renderer>, then solutions
of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\tfrac{d}{dt}\mathbf{x} = A\mathbf{x}$</math-renderer> involve oscillations of frequency <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\beta$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Find the steady-state distribution of</p>
<p dir="auto">$$
P = \begin{bmatrix} 0.7 &amp; 0.2 \ 0.3 &amp; 0.8 \end{bmatrix}.
$$</p>
</li>
<li>
<p dir="auto">Prove that for any stochastic matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$1$</math-renderer> is always an eigenvalue.</p>
</li>
<li>
<p dir="auto">Explain why all eigenvalues of a stochastic matrix satisfy <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\lambda| \leq 1$</math-renderer>.</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Chapter 9. Quadratic Forms and Spectral Theorems</h2><a id="user-content-chapter-9-quadratic-forms-and-spectral-theorems" aria-label="Permalink: Chapter 9. Quadratic Forms and Spectral Theorems" href="#chapter-9-quadratic-forms-and-spectral-theorems"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">9.1 Quadratic Forms</h2><a id="user-content-91-quadratic-forms" aria-label="Permalink: 9.1 Quadratic Forms" href="#91-quadratic-forms"></a></p>
<p dir="auto">A quadratic form is a polynomial of degree two in several variables, expressed neatly using matrices. Quadratic forms
appear throughout mathematics: in optimization, geometry of conic sections, statistics (variance), and physics (energy
functions).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Definition</h3><a id="user-content-definition-7" aria-label="Permalink: Definition" href="#definition-7"></a></p>
<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> be an <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n \times n$</math-renderer> symmetric matrix and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x} \in \mathbb{R}^n$</math-renderer>. The quadratic form associated with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q(\mathbf{x}) = \mathbf{x}^T A \mathbf{x}.
$$</math-renderer></p>
<p dir="auto">Expanded,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q(\mathbf{x}) = \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j.
$$</math-renderer></p>
<p dir="auto">Because <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is symmetric (<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$a_{ij} = a_{ji}$</math-renderer>), the cross-terms can be grouped naturally.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Examples</h3><a id="user-content-examples-8" aria-label="Permalink: Examples" href="#examples-8"></a></p>
<p dir="auto">Example 9.1.1.
For</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; 1 \ 1 &amp; 3 \end{bmatrix}, \quad \mathbf{x} = \begin{bmatrix} x \ y \end{bmatrix},
$$</math-renderer></p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q(x,y) = \begin{bmatrix} x &amp; y \end{bmatrix}
\begin{bmatrix} 2 &amp; 1 \ 1 &amp; 3 \end{bmatrix}
\begin{bmatrix} x \ y \end{bmatrix}
= 2x^2 + 2xy + 3y^2.
$$</math-renderer></p>
<p dir="auto">Example 9.1.2.
The quadratic form</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q(x,y) = x^2 + y^2
$$</math-renderer></p>
<p dir="auto">corresponds to the matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A = I_2$</math-renderer>. It measures squared Euclidean distance from the origin.</p>
<p dir="auto">Example 9.1.3.
The conic section equation</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
4x^2 + 2xy + 5y^2 = 1
$$</math-renderer></p>
<p dir="auto">is described by the quadratic form <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x}^T A \mathbf{x} = 1$</math-renderer> with</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 4 &amp; 1 \ 1 &amp; 5 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Diagonalization of Quadratic Forms</h3><a id="user-content-diagonalization-of-quadratic-forms" aria-label="Permalink: Diagonalization of Quadratic Forms" href="#diagonalization-of-quadratic-forms"></a></p>
<p dir="auto">By choosing a new basis consisting of eigenvectors of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>, we can rewrite the quadratic form without cross terms.
If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A = PDP^{-1}$</math-renderer> with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$D$</math-renderer> diagonal, then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q(\mathbf{x}) = \mathbf{x}^T A \mathbf{x} = (P^{-1}\mathbf{x})^T D (P^{-1}\mathbf{x}).
$$</math-renderer></p>
<p dir="auto">Thus quadratic forms can always be expressed as a sum of weighted squares:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q(\mathbf{y}) = \lambda_1 y_1^2 + \cdots + \lambda_n y_n^2,
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda_i$</math-renderer> are the eigenvalues of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-19" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-19"></a></p>
<p dir="auto">Quadratic forms describe geometric shapes:</p>
<ul dir="auto">
<li>In 2D: ellipses, parabolas, hyperbolas.</li>
<li>In 3D: ellipsoids, paraboloids, hyperboloids.</li>
<li>In higher dimensions: generalizations of ellipsoids.</li>
</ul>
<p dir="auto">Diagonalization aligns the coordinate axes with the principal axes of the shape.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-31" aria-label="Permalink: Why this matters" href="#why-this-matters-31"></a></p>
<p dir="auto">Quadratic forms unify geometry and algebra. They are central in optimization (minimizing energy functions), statistics (
covariance matrices and variance), mechanics (kinetic energy), and numerical analysis. Understanding quadratic forms
leads directly to the spectral theorem.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 9.1</h3><a id="user-content-exercises-91" aria-label="Permalink: Exercises 9.1" href="#exercises-91"></a></p>
<ol dir="auto">
<li>Write the quadratic form <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$Q(x,y) = 3x^2 + 4xy + y^2$</math-renderer> as <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x}^T A \mathbf{x}$</math-renderer> for some symmetric matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>.</li>
<li>For $A = \begin{bmatrix} 1 &amp; 2 \ 2 &amp; 1 \end{bmatrix}$, compute <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$Q(x,y)$</math-renderer> explicitly.</li>
<li>Diagonalize the quadratic form <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$Q(x,y) = 2x^2 + 2xy + 3y^2$</math-renderer>.</li>
<li>Identify the conic section given by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$Q(x,y) = x^2 - y^2$</math-renderer>.</li>
<li>Show that if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is symmetric, quadratic forms defined by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^T$</math-renderer> are identical.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">9.2 Positive Definite Matrices</h2><a id="user-content-92-positive-definite-matrices" aria-label="Permalink: 9.2 Positive Definite Matrices" href="#92-positive-definite-matrices"></a></p>
<p dir="auto">Quadratic forms are especially important when their associated matrices are positive definite, since these guarantee
positivity of energy, distance, or variance. Positive definiteness is a cornerstone in optimization, numerical analysis,
and statistics.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Definition</h3><a id="user-content-definition-8" aria-label="Permalink: Definition" href="#definition-8"></a></p>
<p dir="auto">A symmetric matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{n \times n}$</math-renderer> is called:</p>
<ul dir="auto">
<li>
<p dir="auto">Positive definite if</p>
<p dir="auto">$$
\mathbf{x}^T A \mathbf{x} &gt; 0 \quad \text{for all nonzero } \mathbf{x} \in \mathbb{R}^n.
$$</p>
</li>
<li>
<p dir="auto">Positive semidefinite if</p>
<p dir="auto">$$
\mathbf{x}^T A \mathbf{x} \geq 0 \quad \text{for all } \mathbf{x}.
$$</p>
</li>
</ul>
<p dir="auto">Similarly, negative definite (always &lt; 0) and indefinite (can be both &lt; 0 and &gt; 0) matrices are defined.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Examples</h3><a id="user-content-examples-9" aria-label="Permalink: Examples" href="#examples-9"></a></p>
<p dir="auto">Example 9.2.1.</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; 0 \ 0 &amp; 3 \end{bmatrix}
$$</math-renderer></p>
<p dir="auto">is positive definite, since</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q(x,y) = 2x^2 + 3y^2 &gt; 0
$$</math-renderer></p>
<p dir="auto">for all <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x,y) \neq (0,0)$</math-renderer>.</p>
<p dir="auto">Example 9.2.2.</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 1 &amp; 2 \ 2 &amp; 1 \end{bmatrix}
$$</math-renderer></p>
<p dir="auto">has quadratic form</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q(x,y) = x^2 + 4xy + y^2.
$$</math-renderer></p>
<p dir="auto">This matrix is not positive definite, since <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$Q(1,-1) = -2 &amp;lt; 0$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Characterizations</h3><a id="user-content-characterizations" aria-label="Permalink: Characterizations" href="#characterizations"></a></p>
<p dir="auto">For a symmetric matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>:</p>
<ol dir="auto">
<li>
<p dir="auto">Eigenvalue test: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is positive definite if and only if all eigenvalues of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> are positive.</p>
</li>
<li>
<p dir="auto">Principal minors test (Sylvester’s criterion): <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is positive definite if and only if all leading principal minors (
determinants of top-left <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$k \times k$</math-renderer> submatrices) are positive.</p>
</li>
<li>
<p dir="auto">Cholesky factorization: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is positive definite if and only if it can be written as</p>
<p dir="auto">$$
A = R^T R,
$$</p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$R$</math-renderer> is an upper triangular matrix with positive diagonal entries.</p>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-20" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-20"></a></p>
<ul dir="auto">
<li>Positive definite matrices correspond to quadratic forms that define ellipsoids centered at the origin.</li>
<li>Positive semidefinite matrices define flattened ellipsoids (possibly degenerate).</li>
<li>Indefinite matrices define hyperbolas or saddle-shaped surfaces.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Applications</h3><a id="user-content-applications" aria-label="Permalink: Applications" href="#applications"></a></p>
<ul dir="auto">
<li>Optimization: Hessians of convex functions are positive semidefinite; strict convexity corresponds to positive
definite Hessians.</li>
<li>Statistics: Covariance matrices are positive semidefinite.</li>
<li>Numerical methods: Cholesky decomposition is widely used to solve systems with positive definite matrices efficiently.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-32" aria-label="Permalink: Why this matters" href="#why-this-matters-32"></a></p>
<p dir="auto">Positive definiteness provides stability and guarantees in mathematics and computation. It ensures energy functions are
bounded below, optimization problems have unique solutions, and statistical models are meaningful.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 9.2</h3><a id="user-content-exercises-92" aria-label="Permalink: Exercises 9.2" href="#exercises-92"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Use Sylvester’s criterion to check whether</p>
<p dir="auto">$$
A = \begin{bmatrix} 2 &amp; -1 \ -1 &amp; 2 \end{bmatrix}
$$</p>
<p dir="auto">is positive definite.</p>
</li>
<li>
<p dir="auto">Determine whether</p>
<p dir="auto">$$
A = \begin{bmatrix} 0 &amp; 1 \ 1 &amp; 0 \end{bmatrix}
$$</p>
<p dir="auto">is positive definite, semidefinite, or indefinite.</p>
</li>
<li>
<p dir="auto">Find the eigenvalues of</p>
<p dir="auto">$$
A = \begin{bmatrix} 4 &amp; 2 \ 2 &amp; 3 \end{bmatrix},
$$</p>
<p dir="auto">and use them to classify definiteness.</p>
</li>
<li>
<p dir="auto">Prove that all diagonal matrices with positive entries are positive definite.</p>
</li>
<li>
<p dir="auto">Show that if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is positive definite, then so is <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P^T A P$</math-renderer> for any invertible matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer>.</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">9.3 Spectral Theorem</h2><a id="user-content-93-spectral-theorem" aria-label="Permalink: 9.3 Spectral Theorem" href="#93-spectral-theorem"></a></p>
<p dir="auto">The spectral theorem is one of the most powerful results in linear algebra. It states that symmetric matrices can always
be diagonalized by an orthogonal basis of eigenvectors. This links algebra (eigenvalues), geometry (orthogonal
directions), and applications (stability, optimization, statistics).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Statement of the Spectral Theorem</h3><a id="user-content-statement-of-the-spectral-theorem" aria-label="Permalink: Statement of the Spectral Theorem" href="#statement-of-the-spectral-theorem"></a></p>
<p dir="auto">If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{n \times n}$</math-renderer> is symmetric (<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^T = A$</math-renderer>), then:</p>
<ol dir="auto">
<li>
<p dir="auto">All eigenvalues of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> are real.</p>
</li>
<li>
<p dir="auto">There exists an orthonormal basis of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer> consisting of eigenvectors of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Thus, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> can be written as</p>
<p dir="auto">$$
A = Q \Lambda Q^T,
$$</p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$Q$</math-renderer> is an orthogonal matrix (<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$Q^T Q = I$</math-renderer>) and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\Lambda$</math-renderer> is diagonal with eigenvalues of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> on the diagonal.</p>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Consequences</h3><a id="user-content-consequences" aria-label="Permalink: Consequences" href="#consequences"></a></p>
<ul dir="auto">
<li>Symmetric matrices are always diagonalizable, and the diagonalization is numerically stable.</li>
<li>Quadratic forms <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x}^T A \mathbf{x}$</math-renderer> can be expressed in terms of eigenvalues and eigenvectors, showing
ellipsoids aligned with eigen-directions.</li>
<li>Positive definiteness can be checked by confirming that all eigenvalues are positive.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example 9.3.1</h3><a id="user-content-example-931" aria-label="Permalink: Example 9.3.1" href="#example-931"></a></p>
<p dir="auto">Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; 1 \ 1 &amp; 2 \end{bmatrix}.
$$</math-renderer></p>
<ol dir="auto">
<li>Characteristic polynomial:</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
p(\lambda) = (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3.
$$</math-renderer></p>
<p dir="auto">Eigenvalues: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda_1 = 1, \ \lambda_2 = 3$</math-renderer>.</p>
<ol start="2" dir="auto">
<li>Eigenvectors:</li>
</ol>
<ul dir="auto">
<li>For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda=1$</math-renderer>: solve <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(A-I)\mathbf{v} = 0$</math-renderer>, giving <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,-1)$</math-renderer>.</li>
<li>For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda=3$</math-renderer>: solve <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(A-3I)\mathbf{v} = 0$</math-renderer>, giving <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1)$</math-renderer>.</li>
</ul>
<ol start="3" dir="auto">
<li>Normalize eigenvectors:</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u}_1 = \tfrac{1}{\sqrt{2}}(1,-1), \quad \mathbf{u}_2 = \tfrac{1}{\sqrt{2}}(1,1).
$$</math-renderer></p>
<ol start="4" dir="auto">
<li>Then</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q = \begin{bmatrix} \tfrac{1}{\sqrt{2}} &amp; \tfrac{1}{\sqrt{2}} [6pt] -\tfrac{1}{\sqrt{2}} &amp; \tfrac{1}{\sqrt{2}} \end{bmatrix}, \quad
\Lambda = \begin{bmatrix} 1 &amp; 0 \ 0 &amp; 3 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">So</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = Q \Lambda Q^T.
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-21" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-21"></a></p>
<p dir="auto">The spectral theorem says every symmetric matrix acts like independent scaling along orthogonal directions. In geometry,
this corresponds to stretching space along perpendicular axes.</p>
<ul dir="auto">
<li>Ellipses, ellipsoids, and quadratic surfaces can be fully understood via eigenvalues and eigenvectors.</li>
<li>Orthogonality ensures directions remain perpendicular after transformation.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Applications</h3><a id="user-content-applications-1" aria-label="Permalink: Applications" href="#applications-1"></a></p>
<ul dir="auto">
<li>Optimization: The spectral theorem underlies classification of critical points via eigenvalues of the Hessian.</li>
<li>PCA (Principal Component Analysis): Data covariance matrices are symmetric, and PCA finds orthogonal directions of
maximum variance.</li>
<li>Differential equations &amp; physics: Symmetric operators correspond to measurable quantities with real eigenvalues (
stability, energy).</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-33" aria-label="Permalink: Why this matters" href="#why-this-matters-33"></a></p>
<p dir="auto">The spectral theorem guarantees that symmetric matrices are as simple as possible: they can always be analyzed in terms
of real, orthogonal eigenvectors. This provides both deep theoretical insight and powerful computational tools.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 9.3</h3><a id="user-content-exercises-93" aria-label="Permalink: Exercises 9.3" href="#exercises-93"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Diagonalize</p>
<p dir="auto">$$
A = \begin{bmatrix} 4 &amp; 2 \ 2 &amp; 3 \end{bmatrix}
$$</p>
<p dir="auto">using the spectral theorem.</p>
</li>
<li>
<p dir="auto">Prove that all eigenvalues of a real symmetric matrix are real.</p>
</li>
<li>
<p dir="auto">Show that eigenvectors corresponding to distinct eigenvalues of a symmetric matrix are orthogonal.</p>
</li>
<li>
<p dir="auto">Explain geometrically how the spectral theorem describes ellipsoids defined by quadratic forms.</p>
</li>
<li>
<p dir="auto">Apply the spectral theorem to the covariance matrix</p>
<p dir="auto">$$
\Sigma = \begin{bmatrix} 2 &amp; 1 \ 1 &amp; 2 \end{bmatrix},
$$</p>
<p dir="auto">and interpret the eigenvectors as principal directions of variance.</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">9.4 Principal Component Analysis (PCA)</h2><a id="user-content-94-principal-component-analysis-pca" aria-label="Permalink: 9.4 Principal Component Analysis (PCA)" href="#94-principal-component-analysis-pca"></a></p>
<p dir="auto">Principal Component Analysis (PCA) is a widely used technique in data science, machine learning, and statistics. At its
core, PCA is an application of the spectral theorem to covariance matrices: it finds orthogonal directions (principal
components) that capture the maximum variance in data.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Idea</h3><a id="user-content-the-idea-1" aria-label="Permalink: The Idea" href="#the-idea-1"></a></p>
<p dir="auto">Given a dataset of vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_m \in \mathbb{R}^n$</math-renderer>:</p>
<ol dir="auto">
<li>
<p dir="auto">Center the data by subtracting the mean vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\bar{\mathbf{x}}$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Form the covariance matrix</p>
<p dir="auto">$$
\Sigma = \frac{1}{m} \sum_{i=1}^m (\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})^T.
$$</p>
</li>
<li>
<p dir="auto">Apply the spectral theorem: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\Sigma = Q \Lambda Q^T$</math-renderer>.</p>
<ul dir="auto">
<li>Columns of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$Q$</math-renderer> are orthonormal eigenvectors (principal directions).</li>
<li>Eigenvalues in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\Lambda$</math-renderer> measure variance explained by each direction.</li>
</ul>
</li>
</ol>
<p dir="auto">The first principal component is the eigenvector corresponding to the largest eigenvalue; it is the direction of maximum
variance.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example 9.4.1</h3><a id="user-content-example-941" aria-label="Permalink: Example 9.4.1" href="#example-941"></a></p>
<p dir="auto">Suppose we have two-dimensional data points roughly aligned along the line <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$y = x$</math-renderer>. The covariance matrix is
approximately</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\Sigma = \begin{bmatrix} 2 &amp; 1.9 \ 1.9 &amp; 2 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Eigenvalues are about <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3.9$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$0.1$</math-renderer>. The eigenvector for <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda = 3.9$</math-renderer> is approximately <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1)/\sqrt{2}$</math-renderer>.</p>
<ul dir="auto">
<li>First principal component: the line <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$y = x$</math-renderer>.</li>
<li>Most variance lies along this direction.</li>
<li>Second component is nearly orthogonal (<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$y = -x$</math-renderer>), but variance there is tiny.</li>
</ul>
<p dir="auto">Thus PCA reduces the data to essentially one dimension.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Applications of PCA</h3><a id="user-content-applications-of-pca" aria-label="Permalink: Applications of PCA" href="#applications-of-pca"></a></p>
<ol dir="auto">
<li>Dimensionality reduction: Represent data with fewer features while retaining most variance.</li>
<li>Noise reduction: Small eigenvalues correspond to noise; discarding them filters data.</li>
<li>Visualization: Projecting high-dimensional data onto top 2 or 3 principal components reveals structure.</li>
<li>Compression: PCA is used in image and signal compression.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Connection to the Spectral Theorem</h3><a id="user-content-connection-to-the-spectral-theorem" aria-label="Permalink: Connection to the Spectral Theorem" href="#connection-to-the-spectral-theorem"></a></p>
<p dir="auto">The covariance matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\Sigma$</math-renderer> is always symmetric and positive semidefinite. Hence by the spectral theorem, it has an
orthonormal basis of eigenvectors and nonnegative real eigenvalues. PCA is nothing more than re-expressing data in this
eigenbasis.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-34" aria-label="Permalink: Why this matters" href="#why-this-matters-34"></a></p>
<p dir="auto">PCA demonstrates how abstract linear algebra directly powers modern applications. Eigenvalues and eigenvectors give a
practical method for simplifying data, revealing patterns, and reducing complexity. It is one of the most important
algorithms derived from the spectral theorem.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 9.4</h3><a id="user-content-exercises-94" aria-label="Permalink: Exercises 9.4" href="#exercises-94"></a></p>
<ol dir="auto">
<li>Show that the covariance matrix is symmetric and positive semidefinite.</li>
<li>Compute the covariance matrix of the dataset <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,2), (2,3), (3,4)$</math-renderer>, and find its eigenvalues and eigenvectors.</li>
<li>Explain why the first principal component captures the maximum variance.</li>
<li>In image compression, explain how PCA can reduce storage by keeping only the top <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$k$</math-renderer> principal components.</li>
<li>Prove that the sum of the eigenvalues of the covariance matrix equals the total variance of the dataset.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Chapter 10. Linear Algebra in Practice</h2><a id="user-content-chapter-10-linear-algebra-in-practice" aria-label="Permalink: Chapter 10. Linear Algebra in Practice" href="#chapter-10-linear-algebra-in-practice"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">10.1 Computer Graphics (Rotations, Projections)</h2><a id="user-content-101-computer-graphics-rotations-projections" aria-label="Permalink: 10.1 Computer Graphics (Rotations, Projections)" href="#101-computer-graphics-rotations-projections"></a></p>
<p dir="auto">Linear algebra is the language of modern computer graphics. Every image rendered on a screen, every 3D model rotated or
projected, is ultimately the result of applying matrices to vectors. Rotations, reflections, scalings, and projections
are all linear transformations, making matrices the natural tool for manipulating geometry.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Rotations in 2D</h3><a id="user-content-rotations-in-2d" aria-label="Permalink: Rotations in 2D" href="#rotations-in-2d"></a></p>
<p dir="auto">A counterclockwise rotation by an angle <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\theta$</math-renderer> in the plane is represented by</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
R_\theta =
\begin{bmatrix}
\cos\theta &amp; -\sin\theta \\
\sin\theta &amp; \cos\theta
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">For any vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} \in \mathbb{R}^2$</math-renderer>, the rotated vector is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{v}' = R_\theta \mathbf{v}.
$$</math-renderer></p>
<p dir="auto">This preserves lengths and angles, since <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$R_\theta$</math-renderer> is orthogonal with determinant <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$1$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Rotations in 3D</h3><a id="user-content-rotations-in-3d" aria-label="Permalink: Rotations in 3D" href="#rotations-in-3d"></a></p>
<p dir="auto">In three dimensions, rotations are represented by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3 \times 3$</math-renderer> orthogonal matrices with determinant <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$1$</math-renderer>. For example, a
rotation about the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$z$</math-renderer>-axis is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
R_z(\theta) =
\begin{bmatrix}
\cos\theta &amp; -\sin\theta &amp; 0 \\
\sin\theta &amp; \cos\theta &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Similar formulas exist for rotations about the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$x$</math-renderer>- and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$y$</math-renderer>-axes.</p>
<p dir="auto">More general 3D rotations can be described by axis–angle representation or quaternions, but the underlying idea is still
linear transformations represented by matrices.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Projections</h3><a id="user-content-projections-1" aria-label="Permalink: Projections" href="#projections-1"></a></p>
<p dir="auto">To display 3D objects on a 2D screen, we use projections:</p>
<ol dir="auto">
<li>
<p dir="auto">Orthogonal projection: drops the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$z$</math-renderer>-coordinate, mapping <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x,y,z) \mapsto (x,y)$</math-renderer>.</p>
<p dir="auto">$$
P = \begin{bmatrix}
1 &amp; 0 &amp; 0 \
0 &amp; 1 &amp; 0
\end{bmatrix}.
$$</p>
</li>
<li>
<p dir="auto">Perspective projection: mimics the effect of a camera. A point <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x,y,z)$</math-renderer> projects to</p>
<p dir="auto">$$
\left(\frac{x}{z}, \frac{y}{z}\right),
$$</p>
<p dir="auto">capturing how distant objects appear smaller.</p>
</li>
</ol>
<p dir="auto">These operations are linear (orthogonal projection) or nearly linear (perspective projection becomes linear in
homogeneous coordinates).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Homogeneous Coordinates</h3><a id="user-content-homogeneous-coordinates" aria-label="Permalink: Homogeneous Coordinates" href="#homogeneous-coordinates"></a></p>
<p dir="auto">To unify translations and projections with linear transformations, computer graphics uses homogeneous coordinates. A 3D
point <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x,y,z)$</math-renderer> is represented as a 4D vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x,y,z,1)$</math-renderer>. Transformations are then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$4 \times 4$</math-renderer> matrices, which can
represent rotations, scalings, and translations in a single framework.</p>
<p dir="auto">Example: Translation by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(a,b,c)$</math-renderer>:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
T = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; a \\
0 &amp; 1 &amp; 0 &amp; b \\
0 &amp; 0 &amp; 1 &amp; c \\
0 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-22" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-22"></a></p>
<ul dir="auto">
<li>Rotations preserve shape and size, only changing orientation.</li>
<li>Projections reduce dimension: from 3D world space to 2D screen space.</li>
<li>Homogeneous coordinates allow us to combine multiple transformations (rotation + translation + projection) into a
single matrix multiplication.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-35" aria-label="Permalink: Why this matters" href="#why-this-matters-35"></a></p>
<p dir="auto">Linear algebra enables all real-time graphics: video games, simulations, CAD software, and movie effects. By chaining
simple matrix operations, complex transformations are applied efficiently to millions of points per second.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 10.1</h3><a id="user-content-exercises-101" aria-label="Permalink: Exercises 10.1" href="#exercises-101"></a></p>
<ol dir="auto">
<li>Write the rotation matrix for a 90° counterclockwise rotation in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>. Apply it to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0)$</math-renderer>.</li>
<li>Rotate the point <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1,0)$</math-renderer> about the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$z$</math-renderer>-axis by 180°.</li>
<li>Show that the determinant of any 2D or 3D rotation matrix is 1.</li>
<li>Derive the orthogonal projection matrix from <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer> to the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$xy$</math-renderer>-plane.</li>
<li>Explain how homogeneous coordinates allow translations to be represented as matrix multiplications.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">10.2 Data Science (Dimensionality Reduction, Least Squares)</h2><a id="user-content-102-data-science-dimensionality-reduction-least-squares" aria-label="Permalink: 10.2 Data Science (Dimensionality Reduction, Least Squares)" href="#102-data-science-dimensionality-reduction-least-squares"></a></p>
<p dir="auto">Linear algebra provides the foundation for many data science techniques. Two of the most important are dimensionality
reduction, where high-dimensional datasets are compressed while preserving essential information, and the least squares
method, which underlies regression and model fitting.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Dimensionality Reduction</h3><a id="user-content-dimensionality-reduction" aria-label="Permalink: Dimensionality Reduction" href="#dimensionality-reduction"></a></p>
<p dir="auto">High-dimensional data often contains redundancy: many features are correlated, meaning the data essentially lies near a
lower-dimensional subspace. Dimensionality reduction identifies these subspaces.</p>
<ul dir="auto">
<li>
<p dir="auto">PCA (Principal Component Analysis):
As introduced earlier, PCA diagonalizes the covariance matrix of the data.</p>
<ul dir="auto">
<li>Eigenvectors (principal components) define orthogonal directions of maximum variance.</li>
<li>Eigenvalues measure how much variance lies along each direction.</li>
<li>Keeping only the top <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$k$</math-renderer> components reduces data from <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer>-dimensional space to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$k$</math-renderer>-dimensional space while
retaining most variability.</li>
</ul>
</li>
</ul>
<p dir="auto">Example 10.2.1. A dataset of 1000 images, each with 1024 pixels, may have most variance captured by just 50 eigenvectors
of the covariance matrix. Projecting onto these components compresses the data while preserving essential features.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Least Squares</h3><a id="user-content-least-squares" aria-label="Permalink: Least Squares" href="#least-squares"></a></p>
<p dir="auto">Often, we have more equations than unknowns-an overdetermined system:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A\mathbf{x} \approx \mathbf{b}, \quad A \in \mathbb{R}^{m \times n}, \ m &gt; n.
$$</math-renderer></p>
<p dir="auto">An exact solution may not exist. Instead, we seek <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x}$</math-renderer> that minimizes the error</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
|A\mathbf{x} - \mathbf{b}|^2.
$$</math-renderer></p>
<p dir="auto">This leads to the normal equations:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A^T A \mathbf{x} = A^T \mathbf{b}.
$$</math-renderer></p>
<p dir="auto">The solution is the orthogonal projection of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{b}$</math-renderer> onto the column space of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example 10.2.2</h3><a id="user-content-example-1022" aria-label="Permalink: Example 10.2.2" href="#example-1022"></a></p>
<p dir="auto">Fit a line <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$y = mx + c$</math-renderer> to data points <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x_i, y_i)$</math-renderer>.</p>
<p dir="auto">Matrix form:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
x_1 &amp; 1 \\
x_2 &amp; 1 \\
\vdots &amp; \vdots \\
x_m &amp; 1
\end{bmatrix},
\quad
\mathbf{b} = \begin{bmatrix} y_1 \ y_2 \ \vdots \ y_m \end{bmatrix},
\quad
\mathbf{x} = \begin{bmatrix} m \ c \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Solve <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^T A \mathbf{x} = A^T \mathbf{b}$</math-renderer>. This yields the best-fit line in the least squares sense.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Geometric Interpretation</h3><a id="user-content-geometric-interpretation-23" aria-label="Permalink: Geometric Interpretation" href="#geometric-interpretation-23"></a></p>
<ul dir="auto">
<li>Dimensionality reduction: Find the best subspace capturing most variance.</li>
<li>Least squares: Project the target vector onto the subspace spanned by predictors.</li>
</ul>
<p dir="auto">Both are projection problems, solved using inner products and orthogonality.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-36" aria-label="Permalink: Why this matters" href="#why-this-matters-36"></a></p>
<p dir="auto">Dimensionality reduction makes large datasets tractable, filters noise, and reveals structure. Least squares fitting
powers regression, statistics, and machine learning. Both rely directly on eigenvalues, eigenvectors, and
projections-core tools of linear algebra.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 10.2</h3><a id="user-content-exercises-102" aria-label="Permalink: Exercises 10.2" href="#exercises-102"></a></p>
<ol dir="auto">
<li>Explain why PCA reduces noise in datasets by discarding small eigenvalue components.</li>
<li>Compute the least squares solution to fitting a line through <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(0,0), (1,1), (2,2)$</math-renderer>.</li>
<li>Show that the least squares solution is unique if and only if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^T A$</math-renderer> is invertible.</li>
<li>Prove that the least squares solution minimizes the squared error by projection arguments.</li>
<li>Apply PCA to the data points <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0), (2,1), (3,2)$</math-renderer> and find the first principal component.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">10.3 Networks and Markov Chains</h2><a id="user-content-103-networks-and-markov-chains" aria-label="Permalink: 10.3 Networks and Markov Chains" href="#103-networks-and-markov-chains"></a></p>
<p dir="auto">Graphs and networks provide a natural setting where linear algebra comes to life. From modeling flows and connectivity
to predicting long-term behavior, matrices translate network structure into algebraic form. Markov chains, already
introduced in Section 8.4, are a central example of networks evolving over time.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adjacency Matrices</h3><a id="user-content-adjacency-matrices" aria-label="Permalink: Adjacency Matrices" href="#adjacency-matrices"></a></p>
<p dir="auto">A network (graph) with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> nodes can be represented by an adjacency matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{n \times n}$</math-renderer>:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A_{ij} =
\begin{cases}
1 &amp; \text{if there is an edge from node (i) to node (j)} \\
0 &amp; \text{otherwise.}
\end{cases}
$$</math-renderer></p>
<p dir="auto">For weighted graphs, entries may be positive weights instead of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$0/1$</math-renderer>.</p>
<ul dir="auto">
<li>The number of walks of length <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$k$</math-renderer> from node <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$i$</math-renderer> to node <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$j$</math-renderer> is given by the entry <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(A^k)_{ij}$</math-renderer>.</li>
<li>Powers of adjacency matrices thus encode connectivity over time.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Laplacian Matrices</h3><a id="user-content-laplacian-matrices" aria-label="Permalink: Laplacian Matrices" href="#laplacian-matrices"></a></p>
<p dir="auto">Another important matrix is the graph Laplacian:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
L = D - A,
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$D$</math-renderer> is the diagonal degree matrix ($D_{ii} = \text{degree}(i)$).</p>
<ul dir="auto">
<li>
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$L$</math-renderer> is symmetric and positive semidefinite.</li>
<li>The smallest eigenvalue is always <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$0$</math-renderer>, with eigenvector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1,\dots,1)$</math-renderer>.</li>
<li>The multiplicity of eigenvalue <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$0$</math-renderer> equals the number of connected components in the graph.</li>
</ul>
<p dir="auto">This connection between eigenvalues and connectivity forms the basis of spectral graph theory.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Markov Chains on Graphs</h3><a id="user-content-markov-chains-on-graphs" aria-label="Permalink: Markov Chains on Graphs" href="#markov-chains-on-graphs"></a></p>
<p dir="auto">A Markov chain can be viewed as a random walk on a graph. If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer> is the transition matrix where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P_{ij}$</math-renderer> is the
probability of moving from node <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$i$</math-renderer> to node <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$j$</math-renderer>, then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{x}_{k+1} = P \mathbf{x}_k
$$</math-renderer></p>
<p dir="auto">describes the distribution of positions after <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$k$</math-renderer> steps.</p>
<ul dir="auto">
<li>The steady-state distribution is given by the eigenvector of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer> with eigenvalue <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$1$</math-renderer>.</li>
<li>The speed of convergence depends on the gap between the largest eigenvalue (which is always <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$1$</math-renderer>) and the second
largest eigenvalue.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example 10.3.1</h3><a id="user-content-example-1031" aria-label="Permalink: Example 10.3.1" href="#example-1031"></a></p>
<p dir="auto">Consider a simple 3-node cycle graph:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
P = \begin{bmatrix}
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">This Markov chain cycles deterministically among the nodes. Eigenvalues are the cube roots of
unity: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$1, e^{2\pi i/3}, e^{4\pi i/3}$</math-renderer>. The eigenvalue <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$1$</math-renderer> corresponds to the steady state, which is the uniform
distribution <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1/3,1/3,1/3)$</math-renderer>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Applications</h3><a id="user-content-applications-2" aria-label="Permalink: Applications" href="#applications-2"></a></p>
<ul dir="auto">
<li>Search engines: Google’s PageRank algorithm models the web as a Markov chain, where steady-state probabilities rank
pages.</li>
<li>Network analysis: Eigenvalues of adjacency or Laplacian matrices reveal communities, bottlenecks, and robustness.</li>
<li>Epidemiology and information flow: Random walks model how diseases or ideas spread through networks.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-37" aria-label="Permalink: Why this matters" href="#why-this-matters-37"></a></p>
<p dir="auto">Linear algebra transforms network problems into matrix problems. Eigenvalues and eigenvectors reveal connectivity, flow,
stability, and long-term dynamics. Networks are everywhere-social media, biology, finance, and the internet-so these
tools are indispensable.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 10.3</h3><a id="user-content-exercises-103" aria-label="Permalink: Exercises 10.3" href="#exercises-103"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Write the adjacency matrix of a square graph with 4 nodes. Compute <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^2$</math-renderer> and interpret the entries.</p>
</li>
<li>
<p dir="auto">Show that the Laplacian of a connected graph has exactly one zero eigenvalue.</p>
</li>
<li>
<p dir="auto">Find the steady-state distribution of the Markov chain with</p>
<p dir="auto">$$
P = \begin{bmatrix} 0.5 &amp; 0.5 \ 0.4 &amp; 0.6 \end{bmatrix}.
$$</p>
</li>
<li>
<p dir="auto">Explain how eigenvalues of the Laplacian can detect disconnected components of a graph.</p>
</li>
<li>
<p dir="auto">Describe how PageRank modifies the transition matrix of the web graph to ensure a unique steady-state distribution.</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">10.4 Machine Learning Connections</h2><a id="user-content-104-machine-learning-connections" aria-label="Permalink: 10.4 Machine Learning Connections" href="#104-machine-learning-connections"></a></p>
<p dir="auto">Modern machine learning is built on linear algebra. From the representation of data as matrices to the optimization of
large-scale models, nearly every step relies on concepts such as vector spaces, projections, eigenvalues, and matrix
decompositions.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Data as Matrices</h3><a id="user-content-data-as-matrices" aria-label="Permalink: Data as Matrices" href="#data-as-matrices"></a></p>
<p dir="auto">A dataset with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$m$</math-renderer> examples and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> features is represented as a matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$X \in \mathbb{R}^{m \times n}$</math-renderer>:</p>
<p dir="auto">$$
X =
\begin{bmatrix}</p>
<ul dir="auto">
<li>&amp; \mathbf{x}_1^T &amp; - \</li>
<li>&amp; \mathbf{x}_2^T &amp; - \
&amp; \vdots &amp; \</li>
<li>&amp; \mathbf{x}_m^T &amp; -
\end{bmatrix},
$$</li>
</ul>
<p dir="auto">where each row <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x}_i \in \mathbb{R}^n$</math-renderer> is a feature vector. Linear algebra provides tools to analyze, compress,
and transform this data.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Linear Models</h3><a id="user-content-linear-models" aria-label="Permalink: Linear Models" href="#linear-models"></a></p>
<p dir="auto">At the heart of machine learning are linear predictors:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\hat{y} = X\mathbf{w},
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{w}$</math-renderer> is the weight vector. Training often involves solving a least squares problem or a regularized
variant such as ridge regression:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\min_{\mathbf{w}} |X\mathbf{w} - \mathbf{y}|^2 + \lambda |\mathbf{w}|^2.
$$</math-renderer></p>
<p dir="auto">This is solved efficiently using matrix factorizations.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Singular Value Decomposition (SVD)</h3><a id="user-content-singular-value-decomposition-svd" aria-label="Permalink: Singular Value Decomposition (SVD)" href="#singular-value-decomposition-svd"></a></p>
<p dir="auto">The SVD of a matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$X$</math-renderer> is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
X = U \Sigma V^T,
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$U, V$</math-renderer> are orthogonal and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\Sigma$</math-renderer> is diagonal with nonnegative entries (singular values).</p>
<ul dir="auto">
<li>Singular values measure the importance of directions in feature space.</li>
<li>SVD is used for dimensionality reduction (low-rank approximations), topic modeling, and recommender systems.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Eigenvalues in Machine Learning</h3><a id="user-content-eigenvalues-in-machine-learning" aria-label="Permalink: Eigenvalues in Machine Learning" href="#eigenvalues-in-machine-learning"></a></p>
<ul dir="auto">
<li>PCA (Principal Component Analysis): diagonalization of the covariance matrix identifies directions of maximal
variance.</li>
<li>Spectral clustering: uses eigenvectors of the Laplacian to group data points into clusters.</li>
<li>Stability analysis: eigenvalues of Hessian matrices determine whether optimization converges to a minimum.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Neural Networks</h3><a id="user-content-neural-networks" aria-label="Permalink: Neural Networks" href="#neural-networks"></a></p>
<p dir="auto">Even deep learning, though nonlinear, uses linear algebra at its core:</p>
<ul dir="auto">
<li>Each layer is a matrix multiplication followed by a nonlinear activation.</li>
<li>Training requires computing gradients, which are expressed in terms of matrix calculus.</li>
<li>Backpropagation is essentially repeated applications of the chain rule with linear algebra.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why this matters</h3><a id="user-content-why-this-matters-38" aria-label="Permalink: Why this matters" href="#why-this-matters-38"></a></p>
<p dir="auto">Machine learning models often involve datasets with millions of features and parameters. Linear algebra provides the
algorithms and abstractions that make training and inference possible. Without it, large-scale computation in AI would
be intractable.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Exercises 10.4</h3><a id="user-content-exercises-104" aria-label="Permalink: Exercises 10.4" href="#exercises-104"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Show that ridge regression leads to the normal equations</p>
<p dir="auto">$$
(X^T X + \lambda I)\mathbf{w} = X^T \mathbf{y}.
$$</p>
</li>
<li>
<p dir="auto">Explain how SVD can be used to compress an image represented as a matrix of pixel intensities.</p>
</li>
<li>
<p dir="auto">For a covariance matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\Sigma$</math-renderer>, show why its eigenvalues represent variances along principal components.</p>
</li>
<li>
<p dir="auto">Give an example of how eigenvectors of the Laplacian matrix can be used for clustering a small graph.</p>
</li>
<li>
<p dir="auto">In a neural network with one hidden layer, write the forward pass in matrix form.</p>
</li>
</ol>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Imgur's Community Is in Full Revolt Against Its Owner (119 pts)]]></title>
            <link>https://www.404media.co/imgurs-community-is-in-full-revolt-against-its-owner/</link>
            <guid>45102905</guid>
            <pubDate>Tue, 02 Sep 2025 13:33:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/imgurs-community-is-in-full-revolt-against-its-owner/">https://www.404media.co/imgurs-community-is-in-full-revolt-against-its-owner/</a>, See on <a href="https://news.ycombinator.com/item?id=45102905">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>
          <div>
              
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<p>The front page of Imgur, a popular image hosting and social media site, is full of pictures of <a href="https://imgur.com/gallery/you-business-daddy-OWhCH26?ref=404media.co"><u>John Oliver</u></a> raising his middle finger and telling MediaLab AI, the site’s parent company, “fuck you.” Imgurians, as the site’s users call themselves, telling their <a href="https://deadline.com/2023/04/john-oliver-dings-business-daddy-warner-bros-discovery-content-purge-hbo-max-rebrand-1235328069/?ref=404media.co"><u>business daddy</u></a> to go to hell is the end result of a years-long degradation of the website. The Imgur story is one a classic case of <a href="https://doctorow.medium.com/my-mcluhan-lecture-on-enshittification-ea343342b9bc?ref=404media.co"><u>enshitification</u></a>,</p>
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<p>Imgur began life in 2009 when Ohio University student Alan Schaaf got tired of how hard it was to upload and host images on the internet. He created Imgur as a simple one stop shop for image hosting and the service took off. It was a place where people could host images they wanted to share across multiple services and became ubiquitous on sites like Reddit.</p><p>As the internet evolved, most of the rest of the internet got its act together and platforms built their own image sharing infrastructure and people used Imgur less. But the site still had a community of millions of people who shared images to the site every day. It was a social media&nbsp; based around images and upvotes, with its own in-jokes, memes, and norms.</p><p>In 2021, a media holding company called <a href="https://www.theverge.com/2021/9/28/22697957/imgur-acquisition-medialab-kik-genius-whisper-worldstarhiphop?ref=404media.co"><u>MediaLab AI acquired Imgur</u></a> and Schaaf left. MediaLab AI also owns Genius and World Star and on its website, the company <a href="https://www.medialab.la/?ref=404media.co"><u>bills itself as a place</u></a> where advertisers can “reach audiences at scale, on platforms that build community and influence culture.”</p><p>The community and culture of Imgur, which MedialLab AI <a href="https://www.medialab.la/brands?ref=404media.co"><u>claims is 41 million strong</u></a>, is pissed.</p><p>For the last few days, the front page of Imgur (which cultivates the day’s “most viral posts”) has been full of anti MediaLab AI sentiment. Imgurian <a href="https://imgur.com/gallery/know-meme-WCZM0EP?ref=404media.co"><u>VoidForScreaming posted </u></a>the first instance of the John Oliver meme several days ago, and it’s become a favorite of the community, but there are also calls to <a href="https://imgur.com/gallery/techs-business-daddys-server-room-when-yet-another-intergalactic-quality-gif-is-posted-within-minutes-of-each-other-MYbeZNw?ref=404media.co"><u>flood the servers and crash the site</u></a>, and a <a href="https://imgur.com/gallery/grievances-indictment-SGiEwIE?ref=404media.co"><u>list of grievances</u></a> Imgurians broadly agree brought them to the place they’re in now.</p><p>GhostTater, a longtime Imgurian, told me that the protest was about a confluence of things including a breakdown of the basic features of the site and the disappearance of human moderators.&nbsp;</p><p>“The moderators on Imgur have always been active members of the community. Many were effectively public figures, and their sudden group absence was immediately noticed,” he said. “Several very well-known mods posted generic departure messages, smelling strongly of Legal Department approval. These mods had many friends and acquaintances on the site, and while some are still visiting the site as users, they have gone completely silent.”</p><p>A former Imgur employee who spoke with 404 Media on the condition that we preserve their anonymity because they’re afraid of retaliation from MediaLab AI said that several people on the Imgur team were laid off without notice. Others were moved to MediaLab’s internal teams. “To the best of my knowledge, no employees are remaining solely focused on Imgur. Imgur's social media has been silent for a month,” the employee said. “As far as I am aware, the dedicated part-time moderation team was laid off sometime in the last 8 months, including the full-time moderation manager.”</p><p>Imgurians are convinced that MediaLab AI has replaced those moderators with unreliable AI systems. <a href="https://www.medialab.la/community-content-policy?ref=404media.co"><u>The Community &amp; Content Policy</u></a> on MediaLab AI’s website says it employs human&nbsp; moderators but also uses AI technologies. A common post in the past few days is Imgurians sharing the weird things they’ve been banned for, including one <a href="https://imgur.com/gallery/Y3dR5i0/comment/2472538895?ref=404media.co"><u>who made the comment</u></a> “tell me more” under a post and others who’ve seen their John Olivers removed.</p><p>“There were no humans responding to appeals or concerns,” GhostTater said. “Once the protest started, many users complained about posts being deleted and suspensions or bans being handed out when those posts were critical of MediaLab but not in violation of the written rules.”</p><p>But this isn’t just about bad moderation. Multiple posts on Imgur also called out the breakdown of the site’s basic functionality. GhostTater told me he’d personally experienced the broken notification system and repeated failures of images to upload. “The big one (to me) is the fact that hosted video wouldn’t play for viewers who were not logged in to Imgur,” he said. “The site began as an image hosting site, a place to upload your images and get a link, so that one could share images.”</p><p>MediaLab AI did not respond to 404 Media’s request for comment. “MediaLab’s presence has seemed to many users to fall somewhere between casual institutional indifference and ruthless mechanization. Many report, and resent, feeling explicitly harvested for profit,” GhostTater said.</p><p>Like all companies, MediaLab AI is driven by profit. It makes money as a media holding company, scooping up popular websites and plastering them with ads. It also owns the lyrics sharing site Genius and the once-influential WorldStarHipHop. It’s also being <a href="https://www.forbes.com.au/news/innovation/medialab-bought-up-imgur-genius-and-amino-why-are-they-suing/?ref=404media.co"><u>sued by many</u></a> of the people it bought these sites from, including Imgur’s founder. Schaaf and others have accused MediaLab AI of withholding payments owed to them as part of the sales deals they made.</p><p>The John Olivers and other protest memes keep flowing. Some have set up alternative image sharing sites. “There is a movement rattling around in User Submitted calling for a boycott day, suggesting that all users stay off the site on September first,” GhostTater said. “It has some steam, but we will have to see if it gets enough buy-in to make an impact.”</p>
<!--kg-card-begin: html-->

<!--kg-card-end: html-->

                    <div>
    <div>
      <p>About the author</p>
      <p>Matthew Gault is a writer covering weird tech, nuclear war, and video games. He’s worked for Reuters, Motherboard, and the New York Times.</p>
      
    </div>
      <p><img data-src="https://www.gravatar.com/avatar/87e07bd5bb3d003b0b135303a3e7f8b9?s=250&amp;r=x&amp;d=mp" alt="Matthew Gault" src="https://www.gravatar.com/avatar/87e07bd5bb3d003b0b135303a3e7f8b9?s=250&amp;r=x&amp;d=mp">  
      </p>
  </div>
          </div>
        </article>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Take something you don’t like and try to like it (104 pts)]]></title>
            <link>https://dynomight.net/liking/</link>
            <guid>45102512</guid>
            <pubDate>Tue, 02 Sep 2025 12:56:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dynomight.net/liking/">https://dynomight.net/liking/</a>, See on <a href="https://news.ycombinator.com/item?id=45102512">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <p>Here’s one possible hobby:</p>

<ol>
  <li>Take something you don’t like.</li>
  <li><em>Try</em> to like it.</li>
</ol>

<p>It could be food or music or people or just the general situation you’re in. I recommend this hobby, partly because it’s nice to enjoy things, but mostly as an instrument for probing human nature.</p>

<h2 id="1">1.</h2>

<p>I was in Paris once. By coincidence, I wandered past a bunch of places that were playing Michael Jackson. I thought to myself, “Huh. The French sure do like Michael Jackson.” Gradually I decided, “You know what? They’re right! Michael Jackson is good.” Later, I saw a guy driving around blasting <em>Billy Jean</em> while hanging a hand outside his car with a sparkly white Michael Jackson glove. Again, I thought, “Huh.” That day was <a href="https://en.wikipedia.org/wiki/Death_of_Michael_Jackson">June 25, 2009</a>.</p>

<h2 id="2">2.</h2>

<p>I don’t like cooked spinach. But if I eat some and try to forget that I hate it, it seems OK. Why?</p>

<p>Well, as a child, I was subjected to some misguided spinach-related parental interventions. (“You cannot leave this table until you’ve finished this extremely small portion”, etc.) I hated this, but looking back, it wasn’t the innate qualities of spinach the bothered me, so much as that being forced to put something inside my body felt like a violation of my autonomy.</p>

<p>When I encountered spinach as an adult, instead of tasting a vegetable, I tasted a grueling battle of will. Spinach was dangerous—if I liked it, that would teach my parents that they were right to control my diet.</p>

<p>So I tried telling myself little stories: I’m hiking in the mountains in Japan when suddenly the temperature drops, and it starts pouring rain. Freezing and desperate, I spot a monastery and knock on the door. The monks warm me up and offer me <em>hōrensō no ohitashi</em>, made from some exotic vegetable I’ve never seen before. Presumably, I’d think it was amazing.</p>

<p>I can’t fully access that mind-space. But just knowing it exists seems to make a big difference. Using similar techniques, I’ve successfully made myself like (or less dislike) white wine, disco, yoga, non-spicy food, Ezra Klein, Pearl Jam, and Studio Ghibli movies.</p>

<p>Lesson: Sometimes we dislike things simply because we have a <em>concept</em> of ourselves as not liking them.</p>

<h2 id="3">3.</h2>

<p>Meanwhile, I’ve failed to make myself like country music. I mean, I like <a href="https://www.youtube.com/watch?v=WOHPuY88Ry4"><em>A Boy Named Sue</em></a>. Who doesn’t? But what about <em>Stand By Your Man</em> or <em>Dust on the Bottle</em>? I listen to these, and I appreciate what they’re doing. I admire that they aren’t entirely oriented around the concerns of teenagers. But I can’t seem to actually enjoy them.</p>

<p>Of course, it seems unlikely that this is unrelated to the fact that no one in my peer group thinks country music is cool. On the other hand, I’m constantly annoyed that my opinions aren’t more unique or interesting. And I subscribe to the idea that what’s <em>really</em> cool is to be a cultural omnivore who appreciates everything.</p>

<p>It doesn’t matter. I still can’t like country music. I <em>think</em> the problem is that I don’t actually want to like country music. I only <em>want</em> to want to like country music. The cultural programming is in too deep.</p>

<p>Lesson: Certain levels of the subconscious are easier to screw around with than others.</p>

<h2 id="4">4.</h2>

<p>For years, a friend and I would go on week-long hikes. Before we started, we’d go make our own trail mix, and I’d always insist on adding raisins. Each year, my friend would object more loudly that I don’t actually like raisins. But I do like raisins. So I’d scoff. But after several cycles, I had to admit that while I “liked raisins”, there never came a time that I actually wanted to <em>eat</em> raisins, ever.</p>

<p>Related: Once every year or two, I’ll have a rough day, and I’ll say to myself, “OK, screw it. Liking Oasis is the lamest thing that has ever been done by anyone. But the dirty truth is that I love Oasis. So I will listen to Oasis and thereby be comforted.” Then I listen to Oasis, and it just isn’t that good.</p>

<p>Lesson: You can have an incorrect concept of self.</p>

<h2 id="5">5.</h2>

<p>I don’t like this about myself, but I’m a huge snob regarding television. I believe TV can be true art, as high as any other form. (How does <em>My Brilliant Friend</em> only have an 89 on Metacritic?) But even after pretentiously filtering for critical acclaim, I usually feel that most shows are slop and can’t watch them.</p>

<p>At first glance, this seems just like country music—I don’t like it because of status-driven memetic desire or whatever. But there’s a difference. Not liking country music is fine (neurotic self-flagellation aside) because there’s an infinite amount of other music. But not liking most TV is really annoying, because often I <em>want</em> to watch TV, but can’t find anything acceptable.</p>

<p>I see three possible explanations:</p>

<ol>
  <li>
    <p>Almost all TV is, in fact, bad.</p>
  </li>
  <li>
    <p>Lots of TV is fine, but just doesn’t appeal to me.</p>
  </li>
  <li>
    <p>Lots of TV is fine, but it’s hard to tell yourself stories where you’re hiking in the mountains and a bunch of Japanese monks show you, like, <em>Big Bang Theory</em>.</p>
  </li>
</ol>

<p>Whatever it is, it seems hard to change.</p>

<p>Lesson: Some things are hard to change.</p>

<h2 id="6">6.</h2>

<p>On planes, the captain will often invite you to, “sit back and enjoy the ride”. This is confusing. Enjoy the ride? Enjoy being trapped in a pressurized tube and jostled by all the passengers lining up to relieve themselves because your company decided to cram in a few more seats instead of having an adequate number of toilets? Aren’t flights supposed to be endured?</p>

<p>At the same time, those invitations seem like a glimpse of a parallel universe. Are there members of my species who sit back and enjoy flights?</p>

<p>I have no hard data. But it’s a good heuristic that there are people “who actually X” for approximately all values of X. If one in nine people enjoy <a href="https://doi.org/10.1111/joor.13305">going to the dentist</a>, surely at least that many enjoy being on planes.</p>

<p>What I think the captain is trying to say is, “While you can’t always control your situation, you have tremendous power over how you <em>experience</em> that situation. You may find a cramped flight to be a torture. But the torture happens inside your head. Some people like your situation. You too, perhaps could like it.”</p>

<p>That’s an important message. Though one imagines that giving it as an in-flight announcement would cause more confusion, not less. So the captain does what they can.</p>

  </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RubyMine is now free for non-commercial use (139 pts)]]></title>
            <link>https://blog.jetbrains.com/ruby/2025/09/rubymine-is-now-free-for-non-commercial-use/</link>
            <guid>45102186</guid>
            <pubDate>Tue, 02 Sep 2025 12:25:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.jetbrains.com/ruby/2025/09/rubymine-is-now-free-for-non-commercial-use/">https://blog.jetbrains.com/ruby/2025/09/rubymine-is-now-free-for-non-commercial-use/</a>, See on <a href="https://news.ycombinator.com/item?id=45102186">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <h2 id="major-updates">RubyMine Is Now Free for Non-Commercial Use</h2>                    <div><p data-nosnippet="">Read this post in other languages:</p></div>
                    
<p>Hold on to your helper methods – RubyMine is now FREE for non-commercial use! Whether you’re learning Ruby and Rails, pushing open-source forward, creating dev content, or building your passion project, we want to make sure you have the tools to enjoy what you do even more… for free.</p>


    







<h2>Another chapter in the story</h2>



<p>We recently introduced a <a href="https://blog.jetbrains.com/blog/2024/10/24/webstorm-and-rider-are-now-free-for-non-commercial-use/">new licensing model</a> for WebStorm, RustRover, Rider, and CLion – making them free for non-commercial use. RubyMine is now joining the party! For commercial use, our existing licensing model still applies.</p>



<h2>Why are we doing this?</h2>



<p>We believe developers do their best work when the right tools are accessible. We’ve been listening closely to the Ruby and Rails community – their feedback, success, challenges, and passion for building with joy. Now, we’re making a change that reflects what we’ve heard.</p>



<p>By making RubyMine free for non-commercial use, we hope to lower the barrier to starting and help more people write clean, confident Ruby code from day one. It’s our way of supporting the unique Ruby community – from those who choose Ruby for their projects to maintainers of gems and frameworks who contribute to the Ruby ecosystem. Whether you’re debugging at midnight, crafting clever DSLs, or launching your first Rails app, RubyMine is here to help you build smarter (and crash less).</p>



<h2>Commercial vs. non-commercial use</h2>



<p>As defined in the <a href="https://www.jetbrains.com/legal/docs/toolbox/license_non-commercial/" target="_blank" rel="noopener">Toolbox Subscription Agreement for Non-Commercial Use</a>, commercial use means developing products and earning commercial benefits from your activities. However, certain categories are explicitly excluded from this definition. Common examples of non-commercial uses include learning and self-education, open-source contributions without earning commercial benefits, any form of content creation, and hobby development.</p>



<p>It’s important to note that, if you’re using a non-commercial license, you cannot opt out of the collection of anonymous usage statistics. We use this information to improve our products. The data we collect is exclusively that of anonymous feature usages of our IDEs. It is focused on what actions are performed and what types of functionality of the IDE are used. We do not collect any other data. This is similar to our Early Access Program (EAP) and is in compliance with our Privacy Policy.</p>



<h2>FAQ</h2>



<p>Below are answers to the most common questions. Check out the <a href="https://sales.jetbrains.com/hc/en-gb/articles/18950890312210-The-free-non-commercial-licensing-FAQ" target="_blank" rel="noopener"><strong>full FAQ</strong></a> for more information.</p>



<h3>Licensing</h3>



<h4>What features are included under the free license?</h4>



<p>With the new non-commercial license type, you can enjoy a full-featured IDE that is identical to its paid version. The only difference is in the Code With Me feature – you get <a href="https://www.jetbrains.com/code-with-me/buy/?section=personal&amp;billing=monthly" target="_blank" rel="noopener">Code With Me Community</a> with your free license.</p>



<h4>Which license should I choose if I want to use RubyMine for both non-commercial and commercial projects?</h4>



<p>If you intend to use RubyMine for commercial development for which you will receive direct or indirect commercial advantage or monetary compensation within the meaning of the definitions provided in the <a href="https://www.jetbrains.com/legal/docs/toolbox/license_non-commercial/" target="_blank" rel="noopener">Toolbox Subscription Agreement for Non-Commercial Use</a>, you will need to purchase a commercial subscription (either individual or organizational). This license can then also be used for non-commercial development.</p>



<h4>How do renewals and upgrades work now?</h4>



<p>Non-commercial subscriptions are issued for one year and will automatically renew after that. However, for the renewal to happen, you must have used the assigned license at least once during the last 6 months of the subscription period. If it has been more than 6 months since you last used an IDE activated with this type of license and the renewal did not occur automatically, you can request a new non-commercial subscription again at any time.</p>



<h4>Am I eligible for a refund if I’ve already bought a paid subscription but do non-commercial development?</h4>



<p>If you’re unsure whether you qualify for a refund, you’ll find full details of our policy <a href="https://sales.jetbrains.com/hc/en-gb/articles/115000913704-How-can-I-get-a-refund" target="_blank" rel="noopener">here</a>. Please note that if you also work on projects that qualify as commercial usage, you can’t use the free license for them.</p>



<h3>Anonymous data collection&nbsp;</h3>



<h4>Does my IDE send any data to JetBrains?</h4>



<p>The terms of the non-commercial agreement assume that the product may also electronically send JetBrains anonymized statistics (IDE telemetry) related to your usage of the product’s features. This information may include but is not limited to frameworks, file templates used in the product, actions invoked, and other interactions with the product’s features. This information does not contain personal data.</p>



<h4>Is there a way to opt out of sending anonymized statistics?</h4>



<p>We appreciate that this might not be convenient for everyone, but there is unfortunately no way to opt out of sending anonymized statistics to JetBrains under the terms of the Toolbox agreement for non-commercial use. The only way to opt out is by switching to either a paid subscription or one of the complimentary options mentioned <a href="https://www.jetbrains.com/store/?section=students&amp;billing=yearly" target="_blank" rel="noopener">here</a>.</p>



<h3>Getting a non-commercial subscription&nbsp;</h3>



<h4>What should I do to apply for this subscription?&nbsp;</h4>



<p>It can be easily done right inside your IDE:</p>



<ol>
<li>Install RubyMine and run it.</li>



<li>Upon startup, there will be a license dialog box where you can choose the <em>Non-commercial use </em>option.</li>



<li>Log in to your JetBrains account or create a new one.&nbsp;</li>



<li>Accept the <a href="https://www.jetbrains.com/legal/docs/toolbox/license_non-commercial/" target="_blank" rel="noopener">Toolbox Subscription Agreement for Non-Commercial Use</a>.</li>



<li>Enjoy development in your IDE.</li>
</ol>



<p>If you’ve already started a trial period or have activated your IDE using a paid license, you still can switch to a non-commercial subscription by following these steps:</p>



<ol>
<li>Go to <em>Help | Register.</em></li>



<li>In the window that opens, click on the <em>Deactivate License</em> button.</li>



<li>Choose <em>Non-commercial use.</em></li>



<li>Log in to your JetBrains account or create a new one.&nbsp;</li>



<li>Accept the <a href="https://www.jetbrains.com/legal/docs/toolbox/license_non-commercial/" target="_blank" rel="noopener">Toolbox Subscription Agreement for Non-Commercial Use</a>.</li>



<li>Enjoy development in your IDE.</li>
</ol>



<h4>I don’t see the <em>Non-commercial use</em> option in my IDE. What should I do?&nbsp;</h4>



<p>The most likely explanation for this is that you’re using an older version of RubyMine. Unfortunately, we don’t support obtaining the non-commercial license for any releases prior to RubyMine 2025.2.1.</p>



<p>That’s it for today! If you don’t find an answer to your question, feel free to leave a comment or contact us at <a href="https://www.jetbrains.com/support/sales/#email-sales" target="_blank" rel="noopener">sales@jetbrains.com</a>.</p>



<p>The RubyMine team</p>



<p><em>JetBrains</em></p>



<p>Make it happen. With code.</p>
                    
                                                                                                                                                                                                                            <div>
                                <div>
                                                                            <h4>Subscribe to RubyMine Blog updates</h4>
                                                                                                            
                                </div>
                                
                                <p><img src="https://blog.jetbrains.com/wp-content/themes/jetbrains/assets/img/img-form.svg" alt="image description">
                                                                    </p>
                            </div>
                                                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What's New with Firefox 142 (157 pts)]]></title>
            <link>https://www.mozilla.org/en-US/firefox/142.0.1/whatsnew/?oldversion=139.0.4&amp;utm_medium=firefox-desktop&amp;utm_source=update&amp;utm_campaign=142</link>
            <guid>45101300</guid>
            <pubDate>Tue, 02 Sep 2025 10:50:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mozilla.org/en-US/firefox/142.0.1/whatsnew/?oldversion=139.0.4&#x26;utm_medium=firefox-desktop&#x26;utm_source=update&#x26;utm_campaign=142">https://www.mozilla.org/en-US/firefox/142.0.1/whatsnew/?oldversion=139.0.4&#x26;utm_medium=firefox-desktop&#x26;utm_source=update&#x26;utm_campaign=142</a>, See on <a href="https://news.ycombinator.com/item?id=45101300">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="outer-wrapper">
  <main>
    <!-- Hero Section -->
    <div>
            <p>What’s New | Firefox 142</p>
            <h2>Keep your email address to yourself.</h2>
            <p>Firefox Relay generates secure email masks when you sign up for new online accounts, so you can stay anonymous and get less spam in your inbox.</p>
            
          </div>


<!-- Section 2: Headlines / Sub Features -->
    <div aria-labelledby="features-heading">
        <h2 id="features-heading">Know before you click. Stay in control as you browse.</h2>
        <p>Firefox helps you preview links and keep tabs tidy so nothing slows you down.</p>

        <div>
            <p>Link Previews</p>
            <h3>See what's behind the link before you click.</h3>
            <p>Link Previews show a snapshot of a page before you open it, helping you decide what’s worth your time. Just long press any link to preview and reduce distractions.</p>
            
          </div>

        <div>
            <p>AI-Enhanced Tab Groups</p>
            <h3>Your tabs, automatically grouped by topic.</h3>
            <p>A local AI model identifies similar tabs, automatically organizes them into groups, and even suggests group names, helping you stay organized. Everything happens on your device to respect your privacy.</p>
            
          </div>

        <p>
          <a href="https://www.mozilla.org/firefox/142.0.1/releasenotes/" data-cta-text="See full release notes">See Full Release Notes</a>
        </p>
      </div>



    <!-- Section 3: Desktop Feature Panels -->
    <div aria-labelledby="highlights-heading">
        <h2 id="highlights-heading">Tailor Firefox to fit your flow.</h2>
        <p>Stay organized, find what you need, and browse your way.</p>
        
      </div>

    <!-- Subscribe Banner -->
    <div aria-labelledby="subscribe-heading">
        <h2 id="subscribe-heading">Keep up with all things Firefox</h2>
        <p>Get monthly how-tos, advice and news to make your Firefox experience work best for you.</p>
        






  <form id="newsletter-form" action="https://basket.mozilla.org/news/subscribe/" method="post" novalidate="">
    
      
    
    

    <label for="wnp-email">Your email address</label>
    
      
    
    

    

    
    
    
    <div id="newsletter-errors" data-testid="newsletter-error-message">
      <ul>
        <li>Please enter a valid email address</li>
        <li>We are sorry, but there was a problem with our system. Please try again later!</li>
      </ul>
    </div>
  </form>
  


      </div>

    <!-- Section 4: Mobile Features -->
    <div aria-labelledby="wnp-mobile-heading">
        <h2 id="wnp-mobile-heading">What’s New on Firefox Mobile?</h2>
        <p>New tools for focus, privacy, and smoother mobile browsing.</p>
        <div>
          <article>
            <p><span>Privacy</span><span>Android</span>
            </p>
            <h3>Private tabs that stay private</h3>
            <p>Your private tabs lock automatically when you step away — and only unlock with your face, fingerprint, or PIN.</p>
          </article>
          <article>
            <p><span>Language</span><span>Android</span>
            </p>
            <h3>Getting even more multilingual</h3>
            <p>Now translate web pages into Japanese, Chinese, Korean and more, so you can browse in your preferred language.</p>
          </article>
          <article>
            <p><span>Security</span><span>iOS</span>
            </p>
            <h3>Smarter passwords, fewer hassles</h3>
            <p>Firefox suggests strong passwords when you’re creating a new account on any site, and keeps them secure, ready on any device when you sync.</p>
          </article>
          <article>
            <p><span>Design</span><span>iOS</span>
            </p>
            <h3>A cleaner look with sharper focus</h3>
            <p>A streamlined UI and upgraded dark mode on Firefox for iOS bring clarity and calm to everything you browse.</p>
          </article>
        </div>
      </div>

    <!-- QR Code Banner -->
    <div aria-labelledby="wnp-qr-qr-heading">
                <h2 id="wnp-qr-qr-heading">Take Firefox with you</h2>
                <p>Scan the QR code to get Firefox Mobile and browse with calm, focus, and control — wherever you go.</p>
              </div>

  </main>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Europol said ChatControl doesn't go far enough; they want to retain data forever (120 pts)]]></title>
            <link>https://old.reddit.com/r/europe/comments/1n6cjw1/europol_said_chat_control_doesnt_go_far_enough/</link>
            <guid>45101127</guid>
            <pubDate>Tue, 02 Sep 2025 10:21:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/europe/comments/1n6cjw1/europol_said_chat_control_doesnt_go_far_enough/">https://old.reddit.com/r/europe/comments/1n6cjw1/europol_said_chat_control_doesnt_go_far_enough/</a>, See on <a href="https://news.ycombinator.com/item?id=45101127">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Palantir CEO, Alex Karp.</p>

<p><a href="https://www.reddit.com/r/singularity/s/7mziZTnxWS">https://www.reddit.com/r/singularity/s/7mziZTnxWS</a></p>

<p>And the owner is even more unhinged than that.</p>

<p><a href="https://youtu.be/qqHueZNEzig?si=Lz7f-H34UrZ05eFB">https://youtu.be/qqHueZNEzig?si=Lz7f-H34UrZ05eFB</a></p>

<p>In the UK, Palantir is used in the NHS.</p>

<p><a href="https://www.theguardian.com/society/2023/nov/21/patient-privacy-fears-us-spy-tech-firm-palantir-wins-nhs-contract">https://www.theguardian.com/society/2023/nov/21/patient-privacy-fears-us-spy-tech-firm-palantir-wins-nhs-contract</a></p>

<p><a href="https://pt.m.wikipedia.org/wiki/Palantir_Technologies">https://pt.m.wikipedia.org/wiki/Palantir_Technologies</a></p>

<p><a href="https://www.theguardian.com/technology/2025/aug/04/tech-trillion-dollars-palantir-techscape">https://www.theguardian.com/technology/2025/aug/04/tech-trillion-dollars-palantir-techscape</a></p>

<p>Peter Thiel, co-founder of PayPal, owner of the military technology companies Palantir and Anduril, an early investor in Facebook, the man who has openly declared since the 2000s that democracy is a mistake and that he wants to destroy the United States and European countries to establish city-states, each governed by a different corporation (fiefdoms), on our ruins, has always chosen his projects based on a clear political vision. Unlike other businessmen, Thiel is more of an activist than an investor. For example, PayPal was initially intended as an alternative to the American banking system to undermine and eventually overthrow the government; this is what attracted Thiel's attention to the project in the first place. Palantir, it seems to me, is no different.</p>

<p><a href="https://youtu.be/mfXbyQ9KFdg?si=3x9G8Py8naetTP8J">The Gravedigger of Democracy</a></p>

<p><a href="https://en.m.wikipedia.org/wiki/Peter_Thiel">https://en.m.wikipedia.org/wiki/Peter_Thiel</a></p>

<p><a href="https://www.praxisnation.com/">https://www.praxisnation.com/</a></p>

<p><a href="https://en.m.wikipedia.org/wiki/Praxis_(proposed_city)">https://en.m.wikipedia.org/wiki/Praxis_(proposed_city)</a></p>

<p><a href="https://en.m.wikipedia.org/wiki/Balaji_Srinivasan">https://en.m.wikipedia.org/wiki/Balaji_Srinivasan</a></p>

<p>"Gandalf is the madman who wants to start a war... Mordor is a technological civilization based on reason and science. Outside of Mordor, everything is kind of mystical and environmental, and nothing works.", Peter Thiel</p>

<p><a href="https://en.m.wikipedia.org/wiki/Dark_Enlightenment">https://en.m.wikipedia.org/wiki/Dark_Enlightenment</a></p>

<p>The fact that JD Vance, a creature of Peter's, is Vice-President clearly demonstrates the power of influence he has achieved to introduce his beliefs into the White House policies.</p>

<p><a href="https://www.google.com/amp/s/www.cbsnews.com/amp/news/jd-vance-trump-vp-peter-thiel-billionaire/">https://www.google.com/amp/s/www.cbsnews.com/amp/news/jd-vance-trump-vp-peter-thiel-billionaire/</a></p>

<p>And by the way, keep an eye on this new meeting between Trump and Putin. Since the 90s, we've been aware of the Russian elites' declared ambitions,</p>

<p><a href="https://en.m.wikipedia.org/wiki/Foundations_of_Geopolitics">https://en.m.wikipedia.org/wiki/Foundations_of_Geopolitics</a></p>

<p>Which now apparently partially overlaps with the ambitions of the American ones,</p>

<p><a href="https://en.m.wikipedia.org/wiki/Project_2025">https://en.m.wikipedia.org/wiki/Project_2025</a></p>

<p><a href="https://www.nationalreview.com/corner/why-the-heritage-foundation-keeps-popping-up-in-russian-media/">https://www.nationalreview.com/corner/why-the-heritage-foundation-keeps-popping-up-in-russian-media/</a></p>

<p><a href="https://www.youtube.com/live/aqgfhZpRZhg?si=S-SQUragmZxjMiJs">https://www.youtube.com/live/aqgfhZpRZhg?si=S-SQUragmZxjMiJs</a></p>

<p><a href="https://crisismagazine.com/opinion/the-final-conversations-of-a-dying-priest">https://crisismagazine.com/opinion/the-final-conversations-of-a-dying-priest</a></p>

<p><a href="https://www.politico.eu/article/russia-plot-against-the-west-vladimir-putin-donald-trump-europe/">https://www.politico.eu/article/russia-plot-against-the-west-vladimir-putin-donald-trump-europe/</a></p>

<p><a href="https://www.businessinsider.com/fbi-peter-thiel-vladimir-putin-russia-christian-angermayer-daniil-bisslinger-2023-11">https://www.businessinsider.com/fbi-peter-thiel-vladimir-putin-russia-christian-angermayer-daniil-bisslinger-2023-11</a></p>

<p><a href="https://www.wired.com/story/palantir-doge-irs-mega-api-data/">https://www.wired.com/story/palantir-doge-irs-mega-api-data/</a></p>

<p><a href="https://www.theverge.com/2023/10/19/23923759/peter-thiel-fbi-informant-foreign-influence-report">https://www.theverge.com/2023/10/19/23923759/peter-thiel-fbi-informant-foreign-influence-report</a></p>

<p><a href="https://en.m.wikipedia.org/wiki/Steve_Bannon">https://en.m.wikipedia.org/wiki/Steve_Bannon</a></p>

<p><a href="https://www.scielo.br/j/se/a/WQCvpNry8vCKWVXmDpCxp3m/">https://www.scielo.br/j/se/a/WQCvpNry8vCKWVXmDpCxp3m/</a></p>

<p><a href="https://www.newstatesman.com/international-politics/2025/02/steve-bannon-interview-godfather-of-maga-right">https://www.newstatesman.com/international-politics/2025/02/steve-bannon-interview-godfather-of-maga-right</a></p>

<p><a href="https://www.newsweek.com/ivana-trump-letitia-james-new-york-attorney-general-testimony-ivanka-donald-1725005">https://www.newsweek.com/ivana-trump-letitia-james-new-york-attorney-general-testimony-ivanka-donald-1725005</a></p>

<p><a href="https://en.m.wikipedia.org/wiki/Ivana_Trump">https://en.m.wikipedia.org/wiki/Ivana_Trump</a></p>

<p><a href="https://www.theguardian.com/us-news/2018/oct/29/czechoslovakia-spied-on-trump-to-exploit-ties-to-highest-echelons-of-us-power">https://www.theguardian.com/us-news/2018/oct/29/czechoslovakia-spied-on-trump-to-exploit-ties-to-highest-echelons-of-us-power</a></p>

<p><a href="https://www.project2025.observer/">https://www.project2025.observer/</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unfortunately, the ICEBlock app is activism theater (124 pts)]]></title>
            <link>https://micahflee.com/unfortunately-the-iceblock-app-is-activism-theater/</link>
            <guid>45101117</guid>
            <pubDate>Tue, 02 Sep 2025 10:19:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://micahflee.com/unfortunately-the-iceblock-app-is-activism-theater/">https://micahflee.com/unfortunately-the-iceblock-app-is-activism-theater/</a>, See on <a href="https://news.ycombinator.com/item?id=45101117">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article>
                    <header>
                        
                            <figure>
        <img srcset="https://micahflee.com/content/images/size/w300/2025/08/vlcsnap-2025-08-31-10h16m03s490.png 300w,
                    https://micahflee.com/content/images/size/w720/2025/08/vlcsnap-2025-08-31-10h16m03s490.png 720w,
                    https://micahflee.com/content/images/size/w960/2025/08/vlcsnap-2025-08-31-10h16m03s490.png 960w,
                    https://micahflee.com/content/images/size/w1200/2025/08/vlcsnap-2025-08-31-10h16m03s490.png 1200w,
                    https://micahflee.com/content/images/size/w2000/2025/08/vlcsnap-2025-08-31-10h16m03s490.png 2000w,
                    https://micahflee.com/content/images/2025/08/vlcsnap-2025-08-31-10h16m03s490.png" sizes="(max-width: 1200px) 100vw, 1200px" src="https://micahflee.com/content/images/size/w1200/2025/08/vlcsnap-2025-08-31-10h16m03s490.png" alt="Unfortunately, the ICEBlock app is activism theater">
            <figcaption><span>ICEBlock developer Joshua Aaron speaking remotely at Hackers on Planet Earth on August 17</span></figcaption>
    </figure>
                    </header>

                <section>
                    <p>At this summer's HOPE conference, Joshua Aaron spoke about <a href="https://www.iceblock.app/" rel="noreferrer">ICEBlock</a>, his iPhone app that allows users to anonymously report ICE sightings within a 5 mile radius, and to get notifications when others report ICE sightings near them. You can see the full talk, and the lively/infuriating Q&amp;A, <a href="https://www.youtube.com/live/Pm4tC2Mnabs?t=22330s" rel="noreferrer">here</a>, starting at 6:12:10.</p><p>Thanks to repression from the highest levels of the Trump administration, his app has gone viral and garnered over a million downloads from the App Store. Karoline Leavitt <a href="https://www.foxnews.com/media/karoline-leavitt-accuses-cnn-encouraging-violence-against-agents-reports-new-ice-tracking-app" rel="noreferrer">called it</a> "an incitement of further violence against our ICE officers." Tom Homan <a href="https://xcancel.com/bennyjohnson/status/1939722973373497632" rel="noreferrer">said</a>, "DOJ needs to look at this and see if they're crossing that line." Kristi Noem <a href="https://www.newsweek.com/kirsti-noem-iceblock-deportation-immigration-app-2092878" rel="noreferrer">called the app</a> "obstruction of justice." Pam Bondi <a href="https://www.foxnews.com/media/attorney-general-pam-bondi-warns-iceblock-app-developer-watch-out-says-doj-looking-him" rel="noreferrer">announced</a> "we are looking at it, we are looking at him, and he better watch out, because that's not a protected speech." (Notifying people about ICE sightings is protected speech, no matter what the fascist Attorney General says.) Joshua and his family have been receiving threats.</p><p>But unfortunately, despite the app’s goal of protecting people from ICE, its viral success, and the state repression against it, ICEBlock has serious issues:</p><p>Most importantly, it wasn’t developed with input from people who actually defend immigrants from deportation. As a result, it doesn’t provide people with what they need to stay safe.</p><p>Because ICE sightings in the app aren’t verified in any way, it's likely that most reports in the app aren't actually ICE, even if they’re posted by people who mean well – as I describe below, the vast majority of ICE reports are false positives.</p><p>And judging by the App Store reviews, it’s clear that not everyone means well. One review says: “This is a great app for safety information. Unfortunately MAGA is now posting false information on there and making racist comments in the comment section.”</p><p>Joshua makes strong claims about the security and privacy of his app without backing any of them up with technical details. Many of his claims are false. He also chose to target only iOS, and not Android, because of a <a href="https://bsky.app/profile/grapheneos.org/post/3lt2prfb2vk2r" rel="noreferrer">misunderstanding</a> about how Android push notifications work. And even worse, during the Q&amp;A, he made it clear that he didn't understand terms like “warrant canary,” "reverse engineering," or “security through obscurity,” which doesn't inspire confidence.</p><h2 id="privacy-promises-without-the-evidence">Privacy promises without the evidence</h2><p>When I first heard about ICEBlock, I liked the idea, but I – and others in various group chats I'm part of – were skeptical.</p><p>Joshua promises that ICE reports are "completely anonymous," that the app doesn't store any personal data, and that it's "impossible to trace reports back to individual users." These are bold claims that he hasn't backed up with evidence. Unlike reputable privacy tools, ICEBlock isn't open source (in the talk, he explicitly rejected the idea of open sourcing it or allowing the security community to help him improve it), and Joshua hasn't published a threat model or technical documentation explaining how his app keeps these promises.</p><p>My friend Cooper Quintin, a security researcher at EFF, was also skeptical of ICEBlock, and so he <a href="https://bsky.app/profile/cooperq.com/post/3ltzl5a7y5c2d" rel="noreferrer">reverse engineered it</a>, and spoke to 404 Media about <a href="https://www.404media.co/immigration-raid-tracking-app-ice-block-keeps-your-data-private-researcher-finds/" rel="noreferrer">his findings</a>. He largely confirmed Joshua's claims:</p><figure><blockquote data-bluesky-uri="at://did:plc:t3zkzmz2242yi4rj4lwj7ux4/app.bsky.feed.post/3ltzl5bqmrs2d" data-bluesky-cid="bafyreiaadwar2akdnjfqolubdu4zhbfxxqvc6zz6ygxcxzdsbhz5pp7mji"><p lang="en">The TL;DR is that I didn't find anything suspicious, the app doesn't talk to any third parties, and it doesn't send your location to the developer. Neither your phone ID or iCloud account are associated with the requests the app sends to the apple cloud servers to run. (2/11)</p>— <a href="https://bsky.app/profile/did:plc:t3zkzmz2242yi4rj4lwj7ux4?ref_src=embed">Exploit Code Not People (@cooperq.com)</a> <a href="https://bsky.app/profile/did:plc:t3zkzmz2242yi4rj4lwj7ux4/post/3ltzl5bqmrs2d?ref_src=embed">2025-07-15T18:52:15.697Z</a></blockquote></figure><p>This is great, and it's the reason that (despite his hostility towards transparency) I really do think that Joshua means well.</p><p>Even if we can trust that Joshua isn't collecting data himself, it's difficult to discern what Apple would be able to hand over if it got subpoenaed for data related to his app. The website simply says it's "completely anonymous," without any caveats.</p><p>But ignoring the lack of transparency, there's an even larger problem.</p><h2 id="iceblock-spreads-unverified-information-making-it-useless-for-defending-immigrants">ICEBlock spreads unverified information, making it useless for defending immigrants</h2><p>Local immigrant defense groups around the country have been defending people from deportation for the last decade or more. In a training with <a href="https://www.norcalresist.org/" rel="noreferrer">NorCal Resist</a>, I learned that when people post (and repost) unverified reports of ICE sightings on social media, <em>it does more harm than good</em>.</p><p>Millions of people are living in a state of fear. From my experience working with NorCal Resist, <em>most ICE sightings that people hear about aren't real, even when the person reporting it believes that they are.</em> It's common for someone to see a bunch of dudes in uniforms, or sketchy looking vans, and assume it's ICE, when it's actually something else. If I had to guess, I'd say about 98% of reports are false positives.</p><p><em>False reports encourage panic, which doesn't help anyone.</em> Meanwhile, what people actually need are <em>legal observers</em> – people to document the behavior of federal agents, and provide this evidence to their lawyers. They also need help with connecting families of kidnapped people with information and lawyers, and they need communities coming out to defend their neighbors.</p><p>When I asked Joshua about this during the Q&amp;A of his talk, he didn't answer the question. Here's my question and his non-answers:</p><figure data-kg-thumbnail="https://micahflee.com/content/media/2025/09/iceblock-clip1_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://micahflee.com/content/media/2025/09/iceblock-clip1.mp4" poster="https://img.spacergif.org/v1/1920x1080/0a/spacer.png" width="1920" height="1080" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>4:46</span>
                        </p>
                        </div>
            </div>
            <figcaption><p><span>Joshua's non-answer to my question about false positives and user research</span></p></figcaption>
        <img src="https://micahflee.com/content/media/2025/09/iceblock-clip1_thumb.jpg"></figure><p>Specifically, I asked:</p><blockquote>With my local group, they put a whole lot of energy into verifying every single report before spreading information about it. My question is, how do you know that ICEBlock isn't just full of false positives? And have you done any user research, or worked with local immigration groups to figure out how reliable this is, how much it's actually helping people versus causing panic?</blockquote><p>In an attempt to answer the question about user research, Joshua said, "No, we do not do any user data or metrics." He misunderstood the question, apparently thinking that I meant collecting data from users rather than talking to humans who know more than he does and incorporating their feedback into the design of the app.</p><p>He then explained what ICEBlock does to prevent <em>malicious</em> people from making false reports — including falsely claiming that it's "not possible" to make tons of simultaneous fake reports (more on this below).</p><p>ICEBlock doesn't verify anything, and instead <em>only</em> spreads unverified rumors. To be fair, verification is a <em>very hard problem</em>. In my local group, we have <a href="https://micahflee.com/using-signal-groups-for-activism/" rel="noreferrer">announcement-only Signal groups</a> full of volunteers who physically verify every single ICE sighting that's reported to our rapid response hotline. The vast majority of reports are false positives. There might be several reports a day, but actual ICE or CBP activity is much more rare. I've personally gone to check out maybe 10 to 15 different ICE sightings, only one of which turned out to be actual immigration enforcement (though by the time I got to the location, ICE had already left the area). None of these false reports were malicious: they were simply scared people who saw a bunch of vehicles and people in uniforms and reported an ICE sighting, when it was actually something else.</p><p>Another person in the audience asked a similar question:</p><blockquote>I'm wondering, I think someone asked earlier, if in the design of ICEBlock, or even now, are you currently working with immigrant communities to figure out what resources they need?</blockquote><figure data-kg-thumbnail="https://micahflee.com/content/media/2025/09/iceblock-clip2_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://micahflee.com/content/media/2025/09/iceblock-clip2.mp4" poster="https://img.spacergif.org/v1/1920x1080/0a/spacer.png" width="1920" height="1080" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>3:18</span>
                        </p>
                        </div>
            </div>
            <figcaption><p><span>Another question about if Joshua has engaged with community groups</span></p></figcaption>
        <img src="https://micahflee.com/content/media/2025/09/iceblock-clip2_thumb.jpg"></figure><p>His answer was that ICEBlock has been translated into many different languages. And that the community organizers he's spoken with told him that ICEBlock doesn't meet their needs. So, he decided to not worry about their feedback and do his own thing instead.</p><p><strong>If you want to support people who are actually protecting immigrants from deportation, please </strong><a href="https://www.norcalresist.org/make-a-donation.html" rel="noreferrer"><strong>donate to NorCal Resist</strong></a><strong> or your local community rapid response networks.</strong></p><h2 id="whats-gps-spoofing">What's GPS spoofing?</h2><p>When Joshua explained the safeguards against abuse in the app, he claimed that it's "not possible" to make 100 fake reports in a single morning, in part because you can only make reports within a 5 mile radius of your location. But apparently, Joshua has never heard of GPS spoofing.</p><p>Even though I'm sitting at my house in California right now, here's a screenshot I just took of the ICEBlock app from the Eiffel Tower in Paris. While I won't go into details of the masterful hacking skills that this took, I'll give you a hint: it's the same technique kids use to cheat at Pokemon Go.</p><figure><img src="https://micahflee.com/content/images/2025/09/IMG_0006.PNG" alt="" loading="lazy" width="1170" height="2532" srcset="https://micahflee.com/content/images/size/w600/2025/09/IMG_0006.PNG 600w, https://micahflee.com/content/images/size/w1000/2025/09/IMG_0006.PNG 1000w, https://micahflee.com/content/images/2025/09/IMG_0006.PNG 1170w" sizes="(min-width: 720px) 720px"><figcaption><span>Screenshot of ICEBlock app, with GPS location spoofed to make it think I'm in Paris</span></figcaption></figure><h2 id="make-iceblock-open-source-absolutely-not">Make ICEBlock open source? "Absolutely not."</h2><p>Someone asked whether Joshua would be interested in collaborating with the hacker community on ICEBlock, so they could provide him with advice and help him with feature development.</p><p>Joshua rejected the idea, saying that he believes that he'd need to completely trust anyone he collaborated with. "Believe me when I say I would love help. I'm supporting over a million users myself. There's not some giant company behind this," he said. "But it's really really hard for me to put my trust in somebody, and share the source code, and share the access to this."</p><figure data-kg-thumbnail="https://micahflee.com/content/media/2025/09/iceblock-clip4_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://micahflee.com/content/media/2025/09/iceblock-clip4.mp4" poster="https://img.spacergif.org/v1/1920x1080/0a/spacer.png" width="1920" height="1080" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>1:49</span>
                        </p>
                        </div>
            </div>
            <figcaption><p><span>Joshua explaining that he's building ICEBlock all on his own because he can't trust outside contributors</span></p></figcaption>
        <img src="https://micahflee.com/content/media/2025/09/iceblock-clip4_thumb.jpg"></figure><p>This is, of course, not how secure software development works. The most widely trusted security and privacy tools that exist, like Signal and Tor, are open source, and they accept peer review and code contributions from the public.</p><p>The thing that makes this perfectly reasonable and safe is <em>code review</em>. If Joshua published the ICEBlock source code, experts in the hacker community could add features or fix bugs for him, and make pull requests with their changes. He could then carefully review the changes before merging them into his codebase. He could reject whatever changes he wants. You don't actually need to trust – or even know the identity of – hackers who help you develop software. This is a solved a problem, and Joshua seems utterly unaware of it.</p><p>My friend Jen Helsby, the CTO of Freedom of the Press Foundation and a SecureDrop developer, explicitly asked if he would be open to making ICEBlock open source. Here's the clip:</p><figure data-kg-thumbnail="https://micahflee.com/content/media/2025/09/iceblock-clip5_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://micahflee.com/content/media/2025/09/iceblock-clip5.mp4" poster="https://img.spacergif.org/v1/1920x1080/0a/spacer.png" width="1920" height="1080" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>1:40</span>
                        </p>
                        </div>
            </div>
            <figcaption><p><span>Joshua will not release ICEBlock as open source because he doesn't believe in reverse engineering and thinks keeping the implementation details of his app obscure makes it more secure</span></p></figcaption>
        <img src="https://micahflee.com/content/media/2025/09/iceblock-clip5_thumb.jpg"></figure><p>Jen asked:</p><blockquote>There's a lot of secure software, that probably people in this room work on, that is developed in the open, and that is used primarily by at-risk users, including things like Tor, Signal, SecureDrop. That's great, because it makes it easy for folks to contribute. Maybe you don't want that, I understand that can be hard. But it also makes it easier for people to audit and gain assurance that the app is doing what you claim without having to have, you know, EFF reverse engineer it. Would you be open to making the app open source?</blockquote><p>His answer: "Absolutely not."</p><p>Why? "I don't want anybody from the government to have their hooks in how I'm doing what I'm doing. Once you go open source, everybody has access to it. So I'm just going to keep the codebase private at this time."</p><p>He also claimed that the government can't learn everything about how an app works by reverse engineering it, which isn't true.</p><p>I agree with Jen. His answers are very concerning.</p><h2 id="whats-security-through-obscurity">What's security through obscurity?</h2><p>Another person asked specifically how concealing the details of how the app works from the government is distinguishable from security through obscurity, Joshua agreed that security through obscurity is terrible... and denied that he's doing it?</p><figure data-kg-thumbnail="https://micahflee.com/content/media/2025/09/iceblock-clip6_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://micahflee.com/content/media/2025/09/iceblock-clip6.mp4" poster="https://img.spacergif.org/v1/1920x1080/0a/spacer.png" width="1920" height="1080" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>0:37</span>
                        </p>
                        </div>
            </div>
            <figcaption><p><span>Joshua falsely claiming he doesn't do security through obscurity</span></p></figcaption>
        <img src="https://micahflee.com/content/media/2025/09/iceblock-clip6_thumb.jpg"></figure><p>In case you're not aware of this term, the first sentence of the Wikipedia <a href="https://en.wikipedia.org/wiki/Security_through_obscurity" rel="noreferrer">article</a> on security through obscurity has a concise definition:</p><blockquote>In security engineering, <strong>security through obscurity</strong> is the practice of concealing the details or mechanisms of a system to enhance its security.</blockquote><p>NIST's <a href="https://csrc.nist.gov/pubs/sp/800/123/final" rel="noreferrer">General Guide to Server Security</a> lists "Open Design" as a core security principle, saying that, "System security should not depend on the secrecy of the implementation or its components."</p><p>Minutes before this, Joshua had just finishing explaining that he definitely won't open source his app because, "I don't want anybody from the government to have their hooks in how I'm doing what I'm doing."</p><p>He's implying that his code includes some "secret sauce" that, if it were made public, would make the app less secure, so he can't risk letting anyone discover how it works. This is the <em>definition</em> of security through obscurity.</p><h2 id="my-server-is-highly-secure-he-says-to-a-room-full-of-hackers">My server is "HIGHLY secure," he says to a room full of hackers</h2><p>Throughout the Q&amp;A, Joshua kept referencing the security of his server. At one point, he even said that he built it himself and it's "<em>HIGHLY</em> secure." He also assured the audience, "Trust me when I tell you, I think about <em>EVERYTHING</em> to the Nth degree."</p><p>It took about 20 minutes of digging around to discover that the server that hosts the iceblock.app website is running on Linode and also hosts the websites of several of Joshua's other projects, going back decades. This includes a website for his IT consulting business, his band, etc. If any one of those old websites gets hacked, it's possible that the hacker could more easily access ICEBlock data that's stored on the same server.</p><p>Without providing more details, I also discovered that his server is running outdated software with known vulnerabilities.</p><h2 id="whats-a-warrant-canary">What's a warrant canary?</h2><p>At one point, a lawyer asked some excellent legal questions:</p><blockquote>I'm curious if ICEBlock either currently or has intentions to implement something like a warrant canary or other method. And more generally, whether you have received anything like search warrants, or All Writs Act requests, or anything else. Things like more intrusive means of obtaining information from ICEBlock. Things like requests for live interception, which would be authorized under a search warrant. And if you have a response plan in place already for those.</blockquote><figure data-kg-thumbnail="https://micahflee.com/content/media/2025/09/iceblock-clip3_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://micahflee.com/content/media/2025/09/iceblock-clip3.mp4" poster="https://img.spacergif.org/v1/1920x1080/0a/spacer.png" width="1920" height="1080" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>3:56</span>
                        </p>
                        </div>
            </div>
            <figcaption><p><span>A lawyer asking Joshua about warrant canaries and data requests</span></p></figcaption>
        <img src="https://micahflee.com/content/media/2025/09/iceblock-clip3_thumb.jpg"></figure><p>If you're not familiar with <a href="https://en.wikipedia.org/wiki/Warrant_canary" rel="noreferrer">warrant canaries</a>, these are basically public notices that say, "I've never been forced to give up user data." If the notice ever gets taken down, the public can infer that the service was in fact forced to hand over user data.</p><p>Joshua said, "No on the warrant canary, because it would probably require some sort of user data to do that." He seemed to think that a warrant canary would be a new feature in the app (that's uh, not what a warrant canary is), and he completely ignored the legal questions, instead opting to talk about why it's important to keep the app design simple.</p><p>When the lawyer asked again what he would do if the government tried to compel him to spy on his users, Joshua simply said, "I'd just tell them to go fuck themselves." It's a good answer, but it's also naive. Government requests can include gag orders, preventing him from telling anyone that he has received them, and punishment for disobeying them can include threats of jail time. It's good to plan ahead. Luckily, he has EFF and ACLU offering him legal support, in case he ever actually has to face something like this.</p><h2 id="its-not-too-late">It's not too late</h2><p>Despite everything, I do think that Joshua's heart is in the right place and that he genuinely wants to help people. He's sticking his neck out to fight fascism, and the far right is harassing him and his family for it.</p><p>This is why I, and several other hackers who attended his HOPE talk, spent so much time and energy (both during his talk and in the days after it) trying to encourage him to open things up so that ICEBlock, and its million-strong userbase, might yet become a helpful tool in defending immigrants against Trump's fascist plans. He has rejected our offers.</p><p>It's possible for him to turn things around, but sadly, I'm not holding my breath.</p>
                    
                </section>

                    

                
            </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Run Erlang/Elixir on Microcontrollers and Embedded Linux (168 pts)]]></title>
            <link>https://www.grisp.org/software</link>
            <guid>45100499</guid>
            <pubDate>Tue, 02 Sep 2025 08:40:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.grisp.org/software">https://www.grisp.org/software</a>, See on <a href="https://news.ycombinator.com/item?id=45100499">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-1428d186="" id="VPContent" data-v-5d98c3a5=""><div id="software-hero-section" data-v-613a8312=""><p data-v-f21f4e85=""><!----><header data-v-f21f4e85=""><h2 data-v-f21f4e85="" data-v-982d6d9a="">GRiSP Software</h2><h2 data-v-f21f4e85="" data-v-ea2c30f1="">Run Erlang/Elixir on Microcontrollers and Embedded Linux</h2></header></p><!----><p>Deploy Erlang and Elixir on embedded systems with three purpose-built software stacks. From microcontrollers to enterprise Linux builds, GRiSP provides deterministic, real-time runtime environments that boot directly into the BEAM. Manage your deployments at scale with GRiSP-io cloud platform.</p><!----><!----><!----><!----><!----><!----><!----><!----></div><main><!--[--><div id="grisp-software-section"><!--[--><!--[--><div id="grisp-software-grid" invert="false"><p data-v-f21f4e85=""><!----><header data-v-f21f4e85=""><h2 data-v-f21f4e85="" data-v-982d6d9a="">GRiSP Software Stacks</h2><h2 data-v-f21f4e85="" data-v-ea2c30f1="">Bring Erlang/Elixir all the way to the edge. Deterministic, fault-tolerant, and production-ready</h2></header></p><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><div id="grisp-software-content-grid" invert="false"><!--[--><!--[--><div id="grisp-software-content-col__1"><!--[--><div><p data-v-f21f4e85=""><!----><header data-v-f21f4e85=""><h2 data-v-f21f4e85="" data-v-982d6d9a="">GRiSP Metal</h2><h2 data-v-f21f4e85="" data-v-ea2c30f1="">Erlang/Elixir on RTEMS. <br>Tiny BEAM for devices.</h2></header></p><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><div><!----><!----><p>GRiSP Metal, formerly just GRiSP, boots straight into the Erlang/Elixir VM on RTEMS for deterministic, real‑time behavior with a minimal footprint. It runs on microcontrollers, and we've made the full stack fit in 16 MB of RAM, ideal when every byte and millisecond matter.</p><ul><!--[--><li><span>Boots directly to the BEAM (Erlang/Elixir) on RTEMS</span></li><li><span>MCU-class footprint (fits in 16 MB RAM)</span></li><li><span>Real-time scheduling with predictable I/O</span></li><li><span>Direct, low-overhead access to hardware interfaces</span></li><li><span>Supervision trees bring BEAM reliability to the edge</span></li><!--]--></ul><!----><!----><!----><!----><!----><!----><!----></div><!--]--></div><div id="grisp-software-content-col__2"><!--[--><div><p data-v-f21f4e85=""><!----><header data-v-f21f4e85=""><h2 data-v-f21f4e85="" data-v-982d6d9a="">GRiSP Alloy</h2><h2 data-v-f21f4e85="" data-v-ea2c30f1="">Erlang/Elixir on Linux RT. <br>Buildroot edition.</h2></header></p><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><div><!----><!----><p>GRiSP Alloy boots directly into the Erlang/Elixir VM atop a lean, Buildroot-based real-time Linux. Run multiple Erlang/Elixir VMs with distinct priorities and/or pinned to different cores, connected via efficient, secure distributed Erlang links.</p><ul><!--[--><li><span>Minimal Linux RT image (Buildroot) with BEAM-first boot path</span></li><li><span>Multiple BEAM instances with priority separation and core affinity</span></li><li><span>Local, secure node-to-node links via distributed Erlang</span></li><li><span>Full access to Linux drivers, filesystems, and networking</span></li><li><span>Fast boot, small attack surface, production-ready</span></li><!--]--></ul><!----><!----><!----><!----><!----><!----><!----></div><!--]--></div><div id="grisp-software-content-col__3"><!--[--><div><p data-v-f21f4e85=""><!----><header data-v-f21f4e85=""><h2 data-v-f21f4e85="" data-v-982d6d9a="">GRiSP Forge</h2><h2 data-v-f21f4e85="" data-v-ea2c30f1="">Erlang/Elixir on Linux RT.<br>Yocto edition.</h2></header></p><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><div><!----><!----><p>GRiSP Forge applies the same architecture as GRiSP Alloy to Yocto, for teams requiring long-term, customizable Linux stacks and BSP integration. Boots directly into the Erlang/Elixir VM with the same multi-VM model and secure local links via distributed Erlang.</p><ul><!--[--><li><span>Yocto-based builds with reproducible, customizable images</span></li><li><span>Multiple Erlang/Elixir VMs by design (priorities and/or core pinning)</span></li><li><span>Efficient, secure local links via distributed Erlang</span></li><li><span>Industrial Linux ecosystem compatibility and tooling</span></li><li><span>Built for long lifecycles and enterprise requirements</span></li><!--]--></ul><!----><!----><!----><!----><!----><!----><!----></div><!--]--></div><!--]--><!--]--></div><!--]--><!--]--></div><div id="grisp-io-platform-section"><!--[--><div><p data-v-f21f4e85=""><!----><header data-v-f21f4e85=""><h2 data-v-f21f4e85="" data-v-982d6d9a="">GRiSP-io: Manage Embedded Systems at Scale</h2><!----></header></p><!----><p>GRiSP-io is our cloud and edge platform for deploying, monitoring, and managing distributed embedded systems built with GRiSP stacks. From remote updates to real-time system insights, it helps you stay in control—whether you're testing a prototype or scaling a fleet.</p><!----><!----><!----><!----><!----><!----><!----><!----></div><div><!----><!----><!----><ul><!--[--><li><span>Deploy and manage GRiSP-based devices remotely</span></li><li><span>Monitor system performance and health in real-time</span></li><li><span>Perform over-the-air updates with confidence</span></li><li><span>Integrate cloud and edge control into your workflows</span></li><!--]--></ul><!----><!----><!----><!----><!----><!----><!----></div><!--]--></div><section id="why-use-grisp-section"><!--[--><!--[--><div id="why-use-grisp-grid"><p data-v-f21f4e85=""><!----><header data-v-f21f4e85=""><h2 data-v-f21f4e85="" data-v-982d6d9a="">Why Use GRiSP Software?</h2><!----></header></p><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><div id="why-use-grisp-content-grid"><!--[--><!--[--><div id="why-use-grisp-content-col__1"><p data-v-f21f4e85=""><!----><header data-v-f21f4e85=""><!----><h2 data-v-f21f4e85="" data-v-ea2c30f1="">GRiSP brings the power of Erlang/Elixir to embedded systems, making development efficient, scalable, and fault-tolerant</h2></header></p><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div><div id="why-use-grisp-content-col__2"><!--[--><div><p data-v-f21f4e85=""><!----><header data-v-f21f4e85=""><!----><h2 data-v-f21f4e85="" data-v-ea2c30f1="">For Developers</h2></header></p><!----><p>GRiSP enables developers to run Erlang and Elixir on bare metal or embedded Linux, reducing complexity with minimal overhead and real-time capabilities. With GRiSP stacks and GRiSP-io, they can build and deploy robust, distributed applications optimized for embedded environments.</p><!----><!----><!----><!----><!----><!----><!----><!----></div><div><p data-v-f21f4e85=""><!----><header data-v-f21f4e85=""><!----><h2 data-v-f21f4e85="" data-v-ea2c30f1="">For IoT &amp; Industrial Systems</h2></header></p><!----><p>From prototyping to production, GRiSP provides open-source tools that scale with project needs. Its real-time execution supports automation, robotics, and connected devices, while GRiSP-io enables remote management and monitoring of deployments.</p><!----><!----><!----><!----><!----><!----><!----><!----></div><!--]--></div><!--]--><!--]--></div><!--]--><!--]--></section><!--]--></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Next.js Is Infuriating (839 pts)]]></title>
            <link>https://blog.meca.sh/3lxoty3shjc2z</link>
            <guid>45099922</guid>
            <pubDate>Tue, 02 Sep 2025 06:57:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.meca.sh/3lxoty3shjc2z">https://blog.meca.sh/3lxoty3shjc2z</a>, See on <a href="https://news.ycombinator.com/item?id=45099922">Hacker News</a></p>
Couldn't get https://blog.meca.sh/3lxoty3shjc2z: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Collecting All Causal Knowledge (200 pts)]]></title>
            <link>https://causenet.org/</link>
            <guid>45099418</guid>
            <pubDate>Tue, 02 Sep 2025 05:26:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://causenet.org/">https://causenet.org/</a>, See on <a href="https://news.ycombinator.com/item?id=45099418">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h2>Collecting All Causal Knowledge</h2>
            <p>
                    CauseNet aims at creating a causal knowledge base that comprises all human causal knowledge and to separate it from mere causal beliefs, with the goal of enabling large-scale research into causal inference.
                </p>
        </div><div>

    

    <p>Causal knowledge is seen as one of the key ingredients to advance artificial intelligence. Yet, few knowledge bases comprise causal knowledge to date, possibly due to significant efforts required for validation. Notwithstanding this challenge, we compile CauseNet, a large-scale knowledge base of <em>claimed</em> causal relations between causal concepts. By extraction from different semi- and unstructured web sources, we collect more than 11 million causal relations with an estimated extraction precision of 83\% and construct the first large-scale and open-domain causality graph. We analyze the graph to gain insights about causal beliefs expressed on the web and we demonstrate its benefits in basic causal question answering. Future work may use the graph for causal reasoning, computational argumentation, multi-hop question answering, and more.</p>

    <h2 id="download">Download</h2>

    <p>We provide three versions of our causality graph CauseNet:</p>
    <ul>
      <li><a href="https://groups.uni-paderborn.de/wdqa/causenet/causality-graphs/causenet-full.jsonl.bz2">CauseNet-Full</a>: The complete dataset</li>
      <li><a href="https://groups.uni-paderborn.de/wdqa/causenet/causality-graphs/causenet-precision.jsonl.bz2">CauseNet-Precision</a>: A subset of CauseNet-Full with higher precision</li>
      <li><a href="https://groups.uni-paderborn.de/wdqa/causenet/causality-graphs/causenet-sample.json">CauseNet-Sample</a>: A small sample dataset for first explorations and experiments without provenance data</li>
    </ul>

    <h2 id="statistics">Statistics</h2>

    <table>
      <thead>
        <tr>
          <th>&nbsp;</th>
          <th>Relations</th>
          <th>Concepts</th>
          <th>File Size</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>CauseNet-Full</td>
          <td>11,609,890</td>
          <td>12,186,195</td>
          <td>1.8GB</td>
        </tr>
        <tr>
          <td>CauseNet-Precision</td>
          <td>199,806</td>
          <td>80,223</td>
          <td>135MB</td>
        </tr>
        <tr>
          <td>CauseNet-Sample</td>
          <td>264</td>
          <td>524</td>
          <td>54KB</td>
        </tr>
      </tbody>
    </table>

    <h2 id="data-model">Data Model</h2>

    <p>The core of CauseNet consists of causal concepts which are connected by causal relations. Each causal relation has comprehensive provenance data on where and how it was extracted.</p>

    <p><img src="https://causenet.org/img/data-model.svg" alt="drawing" width="600"></p>

    <h2 id="examples-of-causal-relations">Examples of Causal Relations</h2>

    <p>Causal relations are represented as shown in the following example. Provenance data is omitted.</p>

    <div><pre><code><span>{</span><span>
    </span><span>"causal_relation"</span><span>:</span><span> </span><span>{</span><span>
        </span><span>"cause"</span><span>:</span><span> </span><span>{</span><span>
            </span><span>"concept"</span><span>:</span><span> </span><span>"disease"</span><span>
        </span><span>},</span><span>
        </span><span>"effect"</span><span>:</span><span> </span><span>{</span><span>
            </span><span>"concept"</span><span>:</span><span> </span><span>"death"</span><span>
        </span><span>}</span><span>
    </span><span>}</span><span>
</span><span>}</span><span>
</span></code></pre></div>

    <p>For CauseNet-Full and CauseNet-Precision, we include comprehensive provenance data. In the following, we give one example per source.</p>

    <p>For relations extracted from natural language sentences we provide:</p>
    <ul>
      <li><code>surface</code>: the surface form of the sentence, i.e., the original string</li>
      <li><code>path_pattern</code>: the linguistic path pattern used for extraction</li>
    </ul>

    <h3 id="clueweb12-sentences">ClueWeb12 Sentences</h3>

    <ul>
      <li><code>clueweb12_page_id</code>: page id as provided in the ClueWeb12 corpus</li>
      <li><code>clueweb12_page_reference</code>: page reference as provided in the ClueWeb12 corpus</li>
      <li><code>clueweb12_page_timestamp</code>: page access data as stated in the ClueWeb12 corpus</li>
    </ul>

    <div><pre><code><span>{</span><span>
    </span><span>"causal_relation"</span><span>:{</span><span>
        </span><span>"cause"</span><span>:{</span><span>
            </span><span>"concept"</span><span>:</span><span>"smoking"</span><span>
        </span><span>},</span><span>
        </span><span>"effect"</span><span>:{</span><span>
            </span><span>"concept"</span><span>:</span><span>"disability"</span><span>
        </span><span>}</span><span>
    </span><span>},</span><span>
    </span><span>"sources"</span><span>:[</span><span>
        </span><span>{</span><span>
            </span><span>"type"</span><span>:</span><span>"clueweb12_sentence"</span><span>,</span><span>
            </span><span>"payload"</span><span>:{</span><span>
                </span><span>"clueweb12_page_id"</span><span>:</span><span>"urn:uuid:4cbae00e-8c7f-44b1-9f02-d797f53d448a"</span><span>,</span><span>
                </span><span>"clueweb12_page_reference"</span><span>:</span><span>"http://atlas.nrcan.gc.ca/site/english/maps/health/healthbehaviors/smoking"</span><span>,</span><span>
                </span><span>"clueweb12_page_timestamp"</span><span>:</span><span>"2012-02-23T21:10:45Z"</span><span>,</span><span>
                </span><span>"sentence"</span><span>:</span><span> </span><span>"In Canada, smoking is the most important cause of preventable illness, disability and premature death."</span><span>,</span><span>
                </span><span>"path_pattern"</span><span>:</span><span>"[[cause]]/N</span><span>\t</span><span>-nsubj</span><span>\t</span><span>cause/NN</span><span>\t</span><span>+nmod:of</span><span>\t</span><span>[[effect]]/N"</span><span>
            </span><span>}</span><span>
        </span><span>}</span><span>
    </span><span>]</span><span>
</span><span>}</span><span>
</span></code></pre></div>

    <h3 id="wikipedia-sentences">Wikipedia Sentences</h3>

    <ul>
      <li><code>wikipedia_page_id</code>: the Wikipedia page id</li>
      <li><code>wikipedia_page_title</code>: the Wikipedia page title</li>
      <li><code>wikipedia_revision_id</code>: the Wikipedia revision id of the last edit</li>
      <li><code>wikipedia_revision_timestamp</code>: the timestamp of the Wikipedia revision id of the last edit</li>
      <li><code>sentence_section_heading</code>: the section heading where the sentence comes from</li>
      <li><code>sentence_section_level</code>: the level where the section heading comes from</li>
    </ul>

    <div><pre><code><span>{</span><span>
    </span><span>"causal_relation"</span><span>:{</span><span>
        </span><span>"cause"</span><span>:{</span><span>
            </span><span>"concept"</span><span>:</span><span>"human_activity"</span><span>
        </span><span>},</span><span>
        </span><span>"effect"</span><span>:{</span><span>
            </span><span>"concept"</span><span>:</span><span>"climate_change"</span><span>
        </span><span>}</span><span>
    </span><span>},</span><span>
    </span><span>"sources"</span><span>:[</span><span>
        </span><span>{</span><span>
            </span><span>"type"</span><span>:</span><span>"wikipedia_sentence"</span><span>,</span><span>
            </span><span>"payload"</span><span>:{</span><span>
                </span><span>"wikipedia_page_id"</span><span>:</span><span>"13109"</span><span>,</span><span>
                </span><span>"wikipedia_page_title"</span><span>:</span><span>"Global warming controversy"</span><span>,</span><span>
                </span><span>"wikipedia_revision_id"</span><span>:</span><span>"860220175"</span><span>,</span><span>
                </span><span>"wikipedia_revision_timestamp"</span><span>:</span><span>"2018-09-19T04:52:18Z"</span><span>,</span><span>
                </span><span>"sentence_section_heading"</span><span>:</span><span>"Global warming controversy"</span><span>,</span><span>
                </span><span>"sentence_section_level"</span><span>:</span><span>"1"</span><span>,</span><span>
                </span><span>"sentence"</span><span>:</span><span> </span><span>"The controversy is, by now, political rather than scientific: there is a scientific consensus that climate change is happening and is caused by human activity."</span><span>,</span><span>
                </span><span>"path_pattern"</span><span>:</span><span>"[[cause]]/N</span><span>\t</span><span>-nmod:agent</span><span>\t</span><span>caused/VBN</span><span>\t</span><span>+nsubjpass</span><span>\t</span><span>[[effect]]/N"</span><span>
            </span><span>}</span><span>
        </span><span>}</span><span>
    </span><span>]</span><span>
</span><span>}</span><span>
</span></code></pre></div>

    <h3 id="wikipedia-lists">Wikipedia Lists</h3>

    <ul>
      <li><code>list_toc_parent_title</code>: The heading of the parent section the list appears in</li>
      <li><code>list_toc_section_heading</code>: The heading of the section the list appears in</li>
      <li><code>list_toc_section_level</code>: The nesting level of the section within the table of content (toc)</li>
    </ul>

    <div><pre><code><span>{</span><span>
    </span><span>"causal_relation"</span><span>:{</span><span>
        </span><span>"cause"</span><span>:{</span><span>
            </span><span>"concept"</span><span>:</span><span>"separation_from_parents"</span><span>
        </span><span>},</span><span>
        </span><span>"effect"</span><span>:{</span><span>
            </span><span>"concept"</span><span>:</span><span>"stress_in_early_childhood"</span><span>
        </span><span>}</span><span>
    </span><span>},</span><span>
    </span><span>"sources"</span><span>:[</span><span>
        </span><span>{</span><span>
            </span><span>"type"</span><span>:</span><span>"wikipedia_list"</span><span>,</span><span>
            </span><span>"payload"</span><span>:{</span><span>
                </span><span>"wikipedia_page_id"</span><span>:</span><span>"33096801"</span><span>,</span><span>
                </span><span>"wikipedia_page_title"</span><span>:</span><span>"Stress in early childhood"</span><span>,</span><span>
                </span><span>"wikipedia_revision_id"</span><span>:</span><span>"859225864"</span><span>,</span><span>
                </span><span>"wikipedia_revision_timestamp"</span><span>:</span><span>"2018-09-12T16:22:05Z"</span><span>,</span><span>
                </span><span>"list_toc_parent_title"</span><span>:</span><span>"Stress in early childhood"</span><span>,</span><span>
                </span><span>"list_toc_section_heading"</span><span>:</span><span>"Causes"</span><span>,</span><span>
                </span><span>"list_toc_section_level"</span><span>:</span><span>"2"</span><span>
            </span><span>}</span><span>
        </span><span>}</span><span>
    </span><span>]</span><span>
</span><span>}</span><span>
</span></code></pre></div>

    <h3 id="wikipedia-infoboxes">Wikipedia Infoboxes</h3>

    <ul>
      <li><code>infobox_template</code>: The Wikipedia template of the infobox</li>
      <li><code>infobox_title</code>: The title of the Wikipedia infobox</li>
      <li><code>infobox_argument</code>: The argument of the infobox (the key of the key-value pair)</li>
    </ul>

    <div><pre><code><span>{</span><span>
    </span><span>"causal_relation"</span><span>:{</span><span>
        </span><span>"cause"</span><span>:{</span><span>
            </span><span>"concept"</span><span>:</span><span>"alcohol"</span><span>
        </span><span>},</span><span>
        </span><span>"effect"</span><span>:{</span><span>
            </span><span>"concept"</span><span>:</span><span>"cirrhosis"</span><span>
        </span><span>}</span><span>
    </span><span>},</span><span>
    </span><span>"sources"</span><span>:[</span><span>
        </span><span>{</span><span>
            </span><span>"type"</span><span>:</span><span>"wikipedia_infobox"</span><span>,</span><span>
            </span><span>"payload"</span><span>:{</span><span>
                </span><span>"wikipedia_page_id"</span><span>:</span><span>"21365918"</span><span>,</span><span>
                </span><span>"wikipedia_page_title"</span><span>:</span><span>"Cirrhosis"</span><span>,</span><span>
                </span><span>"wikipedia_revision_id"</span><span>:</span><span>"861860835"</span><span>,</span><span>
                </span><span>"wikipedia_revision_timestamp"</span><span>:</span><span>"2018-09-30T15:40:21Z"</span><span>,</span><span>
                </span><span>"infobox_template"</span><span>:</span><span>"Infobox medical condition (new)"</span><span>,</span><span>
                </span><span>"infobox_title"</span><span>:</span><span>"Cirrhosis"</span><span>,</span><span>
                </span><span>"infobox_argument"</span><span>:</span><span>"causes"</span><span>
            </span><span>}</span><span>
        </span><span>}</span><span>
    </span><span>]</span><span>
</span><span>}</span><span>
</span></code></pre></div>

    <h2 id="loading-causenet-into-neo4j">Loading CauseNet into Neo4j</h2>

    <p>We provide <a href="https://causenet.org/data/load-into-neo4j.ipynb">sample code</a> to load CauseNet into the graph database <a href="https://neo4j.com/">Neo4j</a>.</p>

    <p>The following figure shows an excerpt of CauseNet within Neo4j (showing a coronavirus causing the disease SARS):</p>

    <p><img src="https://causenet.org/img/graph.svg" alt="drawing" width="600"></p>

    <h2 id="concept-spotting-datasets">Concept Spotting Datasets</h2>

    <p>For the construction of CauseNet, we employ a causal concept spotter as a causal concept can be composed of multiple words (e.g., “global warming”, “human activity”, or “lack of exercise”). We determine the exact start and end of a causal
concept in a sentence with a sequence tagger. Our training and evaluation data is available as part of our <a href="https://groups.uni-paderborn.de/wdqa/causenet/concept-spotting">concept spotting datasets</a>: one for Wikipedia infoboxes, Wikipedia lists, and ClueWeb sentences. We split each dataset into 80% training, 10% development and 10% test set</p>

    <h2 id="paper">Paper</h2>

    <p>CauseNet forms the basis for our CIKM 2020 paper <a href="https://papers.dice-research.org/2020/CIKM-20/heindorf_2020a_public.pdf">CauseNet: Towards a Causality Graph Extracted from the Web</a>. Please make sure to refer to it as follows:</p>

    <div><pre><code>@inproceedings<span>{</span>heindorf2020causenet,
  author    = <span>{</span>Stefan Heindorf and
               Yan Scholten and
               Henning Wachsmuth and
               Axel-Cyrille Ngonga Ngomo and
               Martin Potthast<span>}</span>,
  title     = <span>{</span>CauseNet: Towards a Causality Graph Extracted from the Web<span>}</span>,
  booktitle = <span>{{</span>CIKM<span>}}</span>,
  publisher = <span>{{</span>ACM<span>}}</span>,
  year      = <span>{</span>2020<span>}</span>
<span>}</span>
</code></pre></div>

    

    <p>For questions and feedback please contact:</p>

    <p>Stefan Heindorf, Paderborn University<br>
Yan Scholten, Technical University of Munich<br>
Henning Wachsmuth, Paderborn University<br>
Axel-Cyrille Ngonga Ngomo, Paderborn University<br>
Martin Potthast, Leipzig University</p>

    <h2 id="licenses">Licenses</h2>

    <p>The code is licensed under a <a href="https://opensource.org/licenses/MIT">MIT license</a>. The data is licensed under a <a href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International license</a>.</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Keyboards from my collection (2023) (109 pts)]]></title>
            <link>https://aresluna.org/50-keyboards-from-my-collection/</link>
            <guid>45099192</guid>
            <pubDate>Tue, 02 Sep 2025 04:38:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aresluna.org/50-keyboards-from-my-collection/">https://aresluna.org/50-keyboards-from-my-collection/</a>, See on <a href="https://news.ycombinator.com/item?id=45099192">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <header>
      <p>
        Marcin Wichary
      </p>
      <p>
        12&nbsp;February&nbsp;2023&nbsp;/ 50&nbsp;posts&nbsp;/ 60&nbsp;photos
      </p>
    </header>
    
    
    <p>
      This is an archive of a Mastodon thread from 2023. You can still <a target="_blank" href="https://mastodon.online/@mwichary/109853938591676347">read the thread (and all the replies) at its original location</a>, however the photo quality is much better on this page.
    </p>
    
    <hr>
    
    <!-- start tweets -->


<div>
<p>To celebrate the Kickstarter for Shift Happens going well, I thought I would show you 50 keyboards from my collection of really strange/esoteric/meaningful keyboards that I gathered over the years. (It might be the world’s strangest keyboard collection!)</p>


</div>

<div><p>This is technically a bit of a spoiler for the book, but a) a lot of them are not in the book, and b) the book comes out in half a year, and we’ll all forget by then! </p><p>Let’s start!</p><p><a target="_blank" href="https://www.kickstarter.com/projects/mwichary/shift-happens">Shift Happens on Kickstarter</a></p></div>

<div>
<p>1.<br>I have a SafeType, thanks to a friend who noticed one about to be thrown away. This is among the most notable and interesting “ergonomic” keyboards, complete with mirrors that help you orient yourself when you’re starting out.</p>
<figure>
<img data-width="1678" data-height="1235" src="https://aresluna.org/images/50-keyboards-from-my-collection/3-1.avif">
</figure>

</div>

<div>
<p>2.<br>The Comfort System keyboard is another “ergonomic“ device that is honestly pretty frightening to look at (explaining the challenge of making keyboards like these). You can reposition and reorient each of the three parts independently.</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/4-1.avif">
</figure>

</div>

<div>
<p>3.<br>I love these DataDesk Little Fingers keyboards with smaller keys because you can see exactly when iMac was introduced and how the company tried to “redesign”&nbsp;the keyboard to fit the new style.</p>


</div>

<div>
<p>4.<br>This is another Mac “alternate universe“ keyboard - an Adesso ergonomic keyboard that feels like “what if Apple Adjustable still existed when iMac came around”?</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/6-1.avif">
</figure>

</div>

<div>
<p>5.<br>This strange “medical” keyboard is more mechanical than you’d expect! I wrote more about it here: <a target="_blank" href="https://newsletter.shifthappens.site/archive/a-tale-of-three-skeuomorphs">A tale of three skeuomorphs</a>. Cleaning required when flashing!</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/7-1.avif">
</figure>

</div>

<div>
<p>6.<br>Once you’re done with your shift (no pun intended) at the hospital, how about some Pizza? This is i-Opener, one of the many shortlived internet appliances, this one with a gimmick that keeps on gimmicking.</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/8-1.avif">
</figure>

</div>

<div>
<p>7.<br>Speaking of spacebar-adjacent gimmicks, I am mildly obsessed with how beautiful is this first NeXT keyboard from 1987, with a bunch of cool subtle things including a Command *bar* underneath the spacebar. As a matter of fact, I just finished writing an essay on it today!</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/9-1.avif">
</figure>

</div>

<div>
<p>8.<br>This is Olivetti Praxis 48: perhaps one of the most beautiful among the most beautiful typewriters, and strangely similar in palette to the above NeXT keyboard. You could turn on this (electric) typewriter just by pressing any key. That’s pretty wild.</p>


</div>

<div>
<p>9.<br>This Olympia Reporter typewriter is not beautiful, but it has a lot of POWER THIS and POWER THAT keys that celebrate its marriage with electricity? Why is X and some other keys red? Those are the ones that auto repeat! </p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/11-1.avif">
</figure>

</div>

<div>
<p>10.<br>This is another typewriter, so proud of a functioning (erasing!) Backspace that it gives this a treatment I have never seen before or after.</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/12-1.avif">
</figure>

</div>



<div>
<p>12.<br>This keypad… is so bad.</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/14-1.avif">
</figure>

</div>

<div>
<p>13.<br>This was meant to be mounted atop Commodore 64 (which I don’t have), an interesting reversal from the early typewriters being nothing more than repurposed music keyboards.</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/15-1.avif">
</figure>

</div>

<div>
<p>14.<br>These two are taking this idea even further – mount these overlays on regular keyboards to turn them into new kinds of interfaces.</p>


</div>

<div>
<p>15.<br>There’s also professional gaming. It was cheaper for me to buy QSENN keyboards and replicate what professional StarCraft gamers were doing in the 1990s, than to find a good existing photo of one of these keyboards.</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/17-1.avif">
</figure>

</div>

<div>
<p>16.<br>And speaking of gaming – we’re all used to the thumb style of typing from the first photo that it was fun to discover the short moment where the gaming keyboards looked like the one in the second photo.</p>


</div>

<div>
<p>17.<br>And a bit earlier, some game consoles tried to reinvent themselves as home computers with keyboard accessories. This is among the strangest of them: a “keyboard” to add BASIC to the Atari 2600.</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/19-1.avif">
</figure>

</div>

<div>
<p>18.<br>I commissioned this “joystick” from <span translate="no"><a target="_blank" href="https://mastodon.social/@benjedwards">@benjedwards</a></span> and I am so happy with how it turned out. It’s technically a joystick without a stick, but software turned it into a one-key keyboard. It’s F11, currently mapped to muting/unmuting in Zoom. It’s *incredibly* rewarding to press.</p>


</div>

<div>
<p>19.<br>Speaking of strange keyboards, this is my “space cadet” keyboard – a mini keyboard that outputs only spaces, and instead of legends, each key *feels* different. Wrote about it more here: <a target="_blank" href="https://newsletter.shifthappens.site/archive/stop-me-if-youve-seen-this-one-before/">Stop me if you’ve seen this one before</a></p>


</div>

<div>
<p>20.<br>And here is a keyboard I built and hid in my shoes, made for one very specific reason. Are you interested what it is? Check out the whole story here:  <a target="_blank" href="https://newsletter.shifthappens.site/archive/to-walk-among-keyboard-magicians/">To walk among keyboard magicians</a></p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/22-1.avif">
</figure>

</div>

<div>
<p>21.<br>This is one of the most rare keyboards I have –&nbsp;the strange abKey Evolution imported through a friend from Singapore – a keyboard that tried to reinvent perhaps one thing too many. Wrote more about it here: <a target="_blank" href="https://newsletter.shifthappens.site/archive/the-worst-keyboard-ever-made-3/">The worst keyboard ever made</a></p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/23-1.avif">
</figure>

</div>

<div>
<p>22. <br>And this one from Commodore is not really that unique, except it has this fun property –&nbsp;it reverses the usual beige colour scheme making the keys inside darker. It’s kinda neat!</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/24-1.avif">
</figure>

</div>

<div>
<p>23.<br>This is a really cheap Bulgarian keyboard with such a poor build quality it cannot be unseen! I wrote more about it here: <a target="_blank" href="https://newsletter.shifthappens.site/archive/the-worst-keyboard-ever-made-3/">The worst keyboard ever made</a></p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/25-1.avif">
</figure>

</div>

<div>
<p>24.<br>Oh, it gets worse. This calculator keyboard is so cheap it’s not a keyboard at all – just an exposed PCB with a pen to complete the circuit. More about it here: <a target="_blank" href="https://newsletter.shifthappens.site/archive/the-worst-keyboard-ever-made-2/">The worst keyboard ever made</a></p>


</div>

<div>
<p>25.<br>And this is the opposite, an incredibly well-built IBM Model F banking typewriter with an enclosure made out of zinc. Hefty enough to stop a bank robbery? Perhaps. More here: <a target="_blank" href="https://newsletter.shifthappens.site/archive/to-save-a-keyboard-pt-2/">To save a keyboard, pt. 2</a></p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/27-1.avif">
</figure>

</div>

<div>
<p>Halfway through! I need a bit of a break. Is this interesting? Should I keep going!?</p>

</div>

<div>
<p>26.<br>If your bank robbery goes poorly, you probably end up typing on this Swintec, transparent so that no contraband could be hidden inside. More about transparent tech for prisons in this Techmoan video: <a target="_blank" href="https://www.youtube.com/watch?v=O3PfsndsihY">YouTube</a></p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/29-1.avif">
</figure>

</div>

<div>
<p>27.<br>This simple braille keyboard – Tellatouch – was gorgeous and important. Type a key on one side, and the right braille letter assembles itself on the other.</p>
<figure>
<img data-width="1592" data-height="1303" src="https://aresluna.org/images/50-keyboards-from-my-collection/30-1.avif">
</figure>

</div>

<div>
<p>28.<br>This is a more modern version of an adjacent idea. Connect this device to a phone line, and you can speak even if you cannot talk. (Also, I just love any time a keyboard lands itself next to a segmented display.)</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/31-1.avif">
</figure>

</div>

<div>
<p>29.<br>The creators of this Seiko keyboard recognized a watch with a keyboard wouldn’t make sense – so you could dock your watch and type this way. (I don’t have the watch itself. Too expensive!)</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/32-1.avif">
</figure>

</div>

<div>
<p>30.<br>Just kidding! Here’s a keyboard on another Seiko watch. It’s an index keyboard – you don’t touch the keys directly, just move the cursor left and right like on Apple TV –&nbsp;since the keys are smaller than 1mm.</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/33-1.avif">
</figure>

</div>

<div>
<p>31.<br>This TI calculator for school use has tiny keys… in between other keys. What a strange thing.</p>


</div>

<div>
<p>32.<br>This calculator went… a different way.</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/35-1.avif">
</figure>

</div>

<div>
<p>33.<br>I love hybrid things and in-betweeners. This tiny Panasonic Toughbook asks a question: what if a BlackBerry keyboard, but twice the width?</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/36-1.avif">
</figure>

</div>

<div>
<p>34.<br>This one, for TermiFlex, is a one-hand operation, inspired by phone keypads. There are three shifts under your long fingers!</p>


</div>

<div>
<p>35.<br>Speaking of complex shortcuts, look at this Apple keyboard with Avid software keycaps. The icon on Z is my favourite. I don’t even wanna know what this function does.</p>
<figure>
<img data-width="1716" data-height="1208" src="https://aresluna.org/images/50-keyboards-from-my-collection/38-1.avif">
</figure>

</div>

<div>
<p>36.<br>One among many foldable keyboards – this one for Palm devices (RIP).</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/39-1.avif">
</figure>

</div>

<div>
<p>37.<br>This Sony remote had a built-in keyboard for typing in MiniDisc titles.</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/40-1.avif">
</figure>

</div>

<div>
<p>38.<br>And *this* Sony keyboard had two numeric keypads going in two different directions! One for typical calculator use, and one inspired by mobile phones to allow to chat as easily for people who got used to chatting that way.</p>
<figure>
<img data-width="1617" data-height="1283" src="https://aresluna.org/images/50-keyboards-from-my-collection/41-1.avif">
</figure>

</div>

<div>
<p>39.<br>Very happy (and also maybe also a little concerned) to report I am in possession of the entire ProHance lineup of the strange pointing device/keyboard hybrids!</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/42-1.avif">
</figure>

</div>

<div>
<p>40.<br>But it’s amazing how rarely the graphical user interfaces and keyboards intersect. This here – an old AT&amp;T terminal keyboard – is an exception, providing dedicated keys for window management.</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/43-1.avif">
</figure>

</div>

<div>
<p>41.<br>I had to get this keyboard for a now-obscure Harris word processor, just because LOOK AT THE SHAPE OF THIS ENTER KEY.</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/44-1.avif">
</figure>

</div>

<div>
<p>42.<br>I have seen so many keyboards, but only this one – from a strange titling device meant to be connected to your TV –&nbsp;treats uppercase and lowercase exactly like all the other shifted and unshifted symbols. (With the exception of keyboards for kids, I assume!)</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/45-1.avif">
</figure>

</div>

<div>
<p>43.<br>Back in the day, keyboards were so expensive that you often started on a “training” keyboard that came without the machine connected to it. Here’s a training keyboard for a Linotype, which is itself a fascinating machine.</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/46-1.avif">
</figure>

</div>

<div>
<p>44.<br>Here’s another one for the first popular line of desk calculators that predates a 10-key keypad.</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/47-1.avif">
</figure>

</div>

<div>
<p>45.<br>(I also have the actual calculator, called a Comptometer. It’s beautiful, really fun to use, and honestly a work of art. A truly impressive machine from the bygone era. I bought it because I was so impressed reading what it can do.)</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/48-1.avif">
</figure>

</div>

<div>
<p>46.<br>Here’s another practice keyboard, with a record to play to teach you how to type!</p>
<figure>
<img data-width="1681" data-height="1234" src="https://aresluna.org/images/50-keyboards-from-my-collection/49-1.avif">
</figure>

</div>

<div>
<p>47.<br>And here’s the most modern version of a practice keyboard I know of – itself a small computer. After that, the likes of Mavis Beacon took over teaching typing in software.</p>
<figure>
<img data-width="1681" data-height="1234" src="https://aresluna.org/images/50-keyboards-from-my-collection/50-1.avif">
</figure>

</div>

<div>
<p>48.<br>Speaking of the 1980s, keyboards from failed computers often found a second life as Radio Shack components you could reuse in your DIY projects. Here’s one from a home computer called Coleco Adam.</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/51-1.avif">
</figure>

</div>

<div>
<p>49.<br>While we’re speaking about failed computers, this is One Laptop Per Child’s interesting-looking keyboard. (I think OLPC is considered a failure? I’m not 100% sure. This computer is not in the book, so I haven’t researched that carefully.)</p>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/52-1.avif">
</figure>

</div>

<div>
<div><p>50. And here is Canon Cat, maybe my favourite failed machine of all time. Look at these Leap keys! I’m somewhat in love with this machine.</p><p><a target="_blank" href="https://newsletter.shifthappens.site/archive/adult-onset-felinophilia/">Adult-onset felinophilia</a></p></div>
<figure>
<img src="https://aresluna.org/images/50-keyboards-from-my-collection/53-1.avif">
</figure>

</div>

<div><p>That’s it! I hope you liked this sneak peek of my collection– if you did, consider backing the book since this is the level of quality I’ve been aiming at for the visual side… there are a lot more photos like these, and of course a lot more great stories attached to them. </p><p><a target="_blank" href="https://www.kickstarter.com/projects/mwichary/shift-happens">Shift Happens on Kickstarter</a></p></div>




<!-- end tweets -->
    
    

  </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WinBoat: Run Windows apps on Linux with seamless integration (114 pts)]]></title>
            <link>https://github.com/TibixDev/winboat</link>
            <guid>45099124</guid>
            <pubDate>Tue, 02 Sep 2025 04:24:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/TibixDev/winboat">https://github.com/TibixDev/winboat</a>, See on <a href="https://news.ycombinator.com/item?id=45099124">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
  <markdown-accessiblity-table><table>
    <tbody><tr>
      <td>
        <a target="_blank" rel="noopener noreferrer" href="https://github.com/TibixDev/winboat/blob/main/gh-assets/winboat_logo.png"><img src="https://github.com/TibixDev/winboat/raw/main/gh-assets/winboat_logo.png" alt="WinBoat Logo" width="150"></a>
      </td>
      <td>
        <p dir="auto"><h2 tabindex="-1" dir="auto">WinBoat</h2><a id="user-content-winboat" aria-label="Permalink: WinBoat" href="#winboat"></a></p>
        <p dir="auto">Windows for Penguins.<br>
        Run Windows apps on 🐧 Linux with ✨ seamless integration</p>
      </td>
    </tr>
  </tbody></table></markdown-accessiblity-table>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Screenshots</h2><a id="user-content-screenshots" aria-label="Permalink: Screenshots" href="#screenshots"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/TibixDev/winboat/blob/main/gh-assets/features/feat_dash.png"><img src="https://github.com/TibixDev/winboat/raw/main/gh-assets/features/feat_dash.png" alt="WinBoat Dashboard" width="45%"></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/TibixDev/winboat/blob/main/gh-assets/features/feat_apps.png"><img src="https://github.com/TibixDev/winboat/raw/main/gh-assets/features/feat_apps.png" alt="WinBoat Apps" width="45%"></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/TibixDev/winboat/blob/main/gh-assets/features/feat_native.png"><img src="https://github.com/TibixDev/winboat/raw/main/gh-assets/features/feat_native.png" alt="Native Windows" width="45%"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><g-emoji alias="warning">⚠️</g-emoji> Work in Progress <g-emoji alias="warning">⚠️</g-emoji></h2><a id="user-content-️-work-in-progress-️" aria-label="Permalink: ⚠️ Work in Progress ⚠️" href="#️-work-in-progress-️"></a></p>
<p dir="auto">WinBoat is currently in beta, so expect to occasionally run into hiccups and bugs. You should be comfortable with some level of troubleshooting if you decide to try it, however we encourage you to give it a shot anyway.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><strong>🎨 Elegant Interface</strong>: Sleek and intuitive interface that seamlessly integrates Windows into your Linux desktop environment, making it feel like a native experience</li>
<li><strong>📦 Automated Installs</strong>: Simple installation process through our interface - pick your preferences &amp; specs and let us handle the rest</li>
<li><strong>🚀 Run Any App</strong>: If it runs on Windows, it can run on WinBoat. Enjoy the full range of Windows applications as native OS-level windows in your Linux environment</li>
<li><strong>🖥️ Full Windows Desktop</strong>: Access the complete Windows desktop experience when you need it, or run individual apps seamlessly integrated into your Linux workflow</li>
<li><strong>📁 Filesystem Integration</strong>: Your home directory is mounted in Windows, allowing easy file sharing between the two systems without any hassle</li>
<li><strong>✨ And many more</strong>: Smartcard passthrough, resource monitoring, and more features being added regularly</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Prerequisites</h2><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<p dir="auto">Before running WinBoat, ensure your system meets the following requirements:</p>
<ul dir="auto">
<li><strong>RAM</strong>: At least 4 GB of RAM</li>
<li><strong>CPU</strong>: At least 2 CPU threads</li>
<li><strong>Storage</strong>: At least 32 GB free space in <code>/var</code></li>
<li><strong>Virtualization</strong>: KVM enabled in BIOS/UEFI
<ul dir="auto">
<li><a href="https://duckduckgo.com/?t=h_&amp;q=how+to+enable+virtualization+in+%3Cmotherboard+brand%3E+bios&amp;ia=web" rel="nofollow">How to enable virtualization</a></li>
</ul>
</li>
<li><strong>Docker</strong>: Required for containerization
<ul dir="auto">
<li><a href="https://docs.docker.com/engine/install/" rel="nofollow">Installation Guide</a></li>
</ul>
</li>
<li><strong>Docker Compose v2</strong>: Required for compatibility with docker-compose.yml files
<ul dir="auto">
<li><a href="https://docs.docker.com/compose/install/#plugin-linux-only" rel="nofollow">Installation Guide</a></li>
</ul>
</li>
<li><strong>Docker User Group</strong>: Add your user to the <code>docker</code> group
<ul dir="auto">
<li><a href="https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user" rel="nofollow">Setup Instructions</a></li>
</ul>
</li>
<li><strong>FreeRDP</strong>: Required for remote desktop connection (Please make sure you have <strong>Version 3.x.x</strong> with sound support included)
<ul dir="auto">
<li><a href="https://github.com/FreeRDP/FreeRDP/wiki/PreBuilds">Installation Guide</a></li>
</ul>
</li>
<li><strong>Kernel Modules</strong>: <code>iptables</code> and <code>iptable_nat</code> modules must be loaded
<ul dir="auto">
<li><a href="https://rentry.org/rmfq2e5e" rel="nofollow">Module loading instructions</a></li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Downloading</h2><a id="user-content-downloading" aria-label="Permalink: Downloading" href="#downloading"></a></p>
<p dir="auto">You can download the latest Linux builds under the <a href="https://github.com/TibixDev/winboat/releases">Releases</a> tab. We currently offer two variants:</p>
<ul dir="auto">
<li><strong>AppImage:</strong> A popular &amp; portable app format which should run fine on most distributions</li>
<li><strong>Unpacked:</strong> The raw unpacked files, simply run the executable (<code>linux-unpacked/winboat</code>)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Known Issues About Container Runtimes</h2><a id="user-content-known-issues-about-container-runtimes" aria-label="Permalink: Known Issues About Container Runtimes" href="#known-issues-about-container-runtimes"></a></p>
<ul dir="auto">
<li>Podman is <strong>unsupported</strong> for now</li>
<li>Docker Desktop is <strong>unsupported</strong> for now</li>
<li>Distros that emulate Docker through a Podman socket are <strong>unsupported</strong></li>
<li>Any rootless containerization solution is currently <strong>unsupported</strong></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building WinBoat</h2><a id="user-content-building-winboat" aria-label="Permalink: Building WinBoat" href="#building-winboat"></a></p>
<ul dir="auto">
<li>For building you need to have NodeJS and Go installed on your system</li>
<li>Clone the repo (<code>git clone https://github.com/TibixDev/WinBoat</code>)</li>
<li>Install the dependencies (<code>npm i</code>)</li>
<li>Build the app and the guest server using <code>npm run build:linux-gs</code></li>
<li>You can now find the built app under <code>dist</code> with an AppImage and an Unpacked variant</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running WinBoat in development mode</h2><a id="user-content-running-winboat-in-development-mode" aria-label="Permalink: Running WinBoat in development mode" href="#running-winboat-in-development-mode"></a></p>
<ul dir="auto">
<li>Make sure you meet the <a href="#prerequisites">prerequisites</a></li>
<li>Additionally, for development you need to have NodeJS and Go installed on your system</li>
<li>Clone the repo (<code>git clone https://github.com/TibixDev/WinBoat</code>)</li>
<li>Install the dependencies (<code>npm i</code>)</li>
<li>Build the guest server (<code>npm run build-guest-server</code>)</li>
<li>Run the app (<code>npm run dev</code>)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome! Whether it's bug fixes, feature improvements, or documentation updates, we appreciate your help making WinBoat better.</p>
<p dir="auto"><strong>Please note</strong>: We maintain a focus on technical contributions only. Pull requests containing political/sexual content, or other sensitive/controversial topics will not be accepted. Let's keep things focused on making great software! 🚀</p>
<p dir="auto">Feel free to:</p>
<ul dir="auto">
<li>Report bugs and issues</li>
<li>Submit feature requests</li>
<li>Contribute code improvements</li>
<li>Help with documentation</li>
<li>Share feedback and suggestions</li>
</ul>
<p dir="auto">Check out our issues page to get started, or feel free to open a new issue if you've found something that needs attention.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">WinBoat is licensed under the <a href="https://github.com/TibixDev/winboat/blob/main/LICENSE">MIT</a> license</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Inspiration / Alternatives</h2><a id="user-content-inspiration--alternatives" aria-label="Permalink: Inspiration / Alternatives" href="#inspiration--alternatives"></a></p>
<p dir="auto">These past few years some cool projects have surfaced with similar concepts, some of which we've also taken inspirations from.<br>
They're awesome and you should check them out:</p>
<ul dir="auto">
<li><a href="https://github.com/winapps-org/winapps">WinApps</a></li>
<li><a href="https://github.com/casualsnek/cassowary">Cassowary</a></li>
<li><a href="https://github.com/dockur/windows">dockur/windows</a> (🌟 Also used in WinBoat)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Socials &amp; Contact</h2><a id="user-content-socials--contact" aria-label="Permalink: Socials &amp; Contact" href="#socials--contact"></a></p>
<ul dir="auto">
<li>🌐 <strong>Website</strong>: <a href="https://www.winboat.app/" rel="nofollow">winboat.app</a></li>
<li>🐦 <strong>Twitter/X</strong>: <a href="https://x.com/winboat_app" rel="nofollow">@winboat_app</a></li>
<li>🦋 <strong>Bluesky</strong>: <a href="http://bsky.app/profile/winboat.app" rel="nofollow">winboat.app</a></li>
<li>🗨️ <strong>Discord</strong>: <a href="http://discord.gg/MEwmpWm4tN" rel="nofollow">Join our community</a></li>
<li>📧 <strong>Email</strong>: <a href="mailto:staff@winboat.app">staff@winboat.app</a></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FreeDroidWarn (376 pts)]]></title>
            <link>https://github.com/woheller69/FreeDroidWarn</link>
            <guid>45098722</guid>
            <pubDate>Tue, 02 Sep 2025 03:01:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/woheller69/FreeDroidWarn">https://github.com/woheller69/FreeDroidWarn</a>, See on <a href="https://news.ycombinator.com/item?id=45098722">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">FreeDroidWarn</h2><a id="user-content-freedroidwarn" aria-label="Permalink: FreeDroidWarn" href="#freedroidwarn"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Overview</h3><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">This library shows an alert dialog with a deprecation warning informing that Google will require developer verification for Android apps outside the Play Store from 2026/2027 which the developer is not going to provide.</p>
<div data-snippet-clipboard-copy-content="Google has announced that, starting in 2026/2027, all apps on certified Android devices
will require the developer to submit personal identity details directly to Google.
Since the developers of this app do not agree to this requirement, this app will no longer 
work on certified Android devices after that time."><pre><code>Google has announced that, starting in 2026/2027, all apps on certified Android devices
will require the developer to submit personal identity details directly to Google.
Since the developers of this app do not agree to this requirement, this app will no longer 
work on certified Android devices after that time.
</code></pre></div>
<p dir="auto"><a href="https://www.androidauthority.com/android-developer-verification-requirements-3590911/" rel="nofollow">https://www.androidauthority.com/android-developer-verification-requirements-3590911/</a></p>
<p dir="auto"><a href="https://developer.android.com/developer-verification" rel="nofollow">https://developer.android.com/developer-verification</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Add the JitPack repository to your root build.gradle at the end of repositories:</p>
<div dir="auto" data-snippet-clipboard-copy-content="allprojects {
  repositories {
    ...
    maven { url 'https://jitpack.io' }
  }
}"><pre><span>allprojects</span> {
  repositories {
    <span>..</span>.
    maven { url <span><span>'</span>https://jitpack.io<span>'</span></span> }
  }
}</pre></div>
<p dir="auto">Add the library dependency to your build.gradle file.</p>
<div dir="auto" data-snippet-clipboard-copy-content="dependencies {
    implementation 'com.github.woheller69:FreeDroidWarn:V1.3'
}"><pre><span>dependencies</span> {
    implementation <span><span>'</span>com.github.woheller69:FreeDroidWarn:V1.3<span>'</span></span>
}</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage</h3><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">In onCreate of your app just add:</p>
<div data-snippet-clipboard-copy-content="     FreeDroidWarn.showWarningOnUpgrade(this, BuildConfig.VERSION_CODE);
"><pre><code>     FreeDroidWarn.showWarningOnUpgrade(this, BuildConfig.VERSION_CODE);

</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">License</h3><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This library is licensed under the GPLv3.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple pulls iPhone torrent app from AltStore PAL in Europe (244 pts)]]></title>
            <link>https://www.theverge.com/news/767344/apple-removes-itorrent-altstore-pal-ios-marketplace</link>
            <guid>45098411</guid>
            <pubDate>Tue, 02 Sep 2025 02:12:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/news/767344/apple-removes-itorrent-altstore-pal-ios-marketplace">https://www.theverge.com/news/767344/apple-removes-itorrent-altstore-pal-ios-marketplace</a>, See on <a href="https://news.ycombinator.com/item?id=45098411">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><a href="https://www.theverge.com/authors/jess-weatherbed"><img alt="Jess Weatherbed" data-chromatic="ignore" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195820/JESSICA_WEATHERBED.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=48 1x, https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195820/JESSICA_WEATHERBED.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96 2x" src="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195820/JESSICA_WEATHERBED.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96"></a></p><div><p><span aria-expanded="false" aria-haspopup="true" role="button" tabindex="0"><span id="follow-author-standard_article_details-dmcyOmF1dGhvclByb2ZpbGU6OTU="><span><span><svg width="9" height="9" viewBox="0 0 9 9" fill="none" xmlns="http://www.w3.org/2000/svg" aria-label="Follow"><path d="M5 0H4V4H0V5H4V9H5V5H9V4H5V0Z"></path></svg></span></span><span>Jess Weatherbed</span></span></span></p> <p><span>is a news writer focused on creative industries, computing, and internet culture. Jess started her career at TechRadar, covering news and hardware reviews.</span></p></div></div><div id="zephr-anchor"><p>Apple has removed the iPhone torrenting client, iTorrent, from AltStore PAL’s alternative iOS marketplace in the EU, showing that it can still exert control over apps that aren’t listed on the official App Store. iTorrent developer Daniil Vinogradov told <em><a href="https://torrentfreak.com/apple-revokes-eu-distribution-rights-for-torrent-client-developer-left-in-the-dark/">TorrentFreak</a></em> that Apple has revoked his distribution rights to publish apps in any alternative iOS stores, but it seems the issue is related to government sanctions, rather than a block on torrenting.</p><p>In a statement to <em>The Verge</em>, Apple spokesperson Peter Ajemian said, “Notarization for this app was removed in order to comply with government sanctions-related rules in various jurisdictions. We have communicated this to the developer.”</p><p>While Apple bans torrent apps on its own iOS store, the <a href="https://www.theverge.com/2024/1/25/24050200/apple-third-party-app-stores-allowed-iphone-ios-europe-digital-markets-act">EU’s Digital Markets Act</a> gave iPhone users within the bloc greater freedom to install apps from third-party app stores that the Cupertino company doesn’t directly manage.</p><p>Last month, Vinogradov said <a href="https://github.com/XITRIX/iTorrent/issues/401#issuecomment-3112600292">on iTorrent’s GitHub page</a> that Apple “removed Alternative Distribution functionality from iTorrent’s Developer Portal without any warning.” Apple didn’t provide a reason for the removal, according Vinogradov, and distribution was revoked at the Apple Dev Account level.</p><p><em><strong>Update, August 28th: </strong>Added a statement from Apple.</em></p><p><span><a href="https://www.theverge.com/news/767344/apple-removes-itorrent-altstore-pal-ios-marketplace#comments">0<!-- --> <!-- -->Comments</a></span></p><div><p><span><strong>Follow topics and authors</strong> from this story to see more like this in your personalized homepage feed and to receive email updates.</span></p><ul><li id="follow-author-article_footer-dmcyOmF1dGhvclByb2ZpbGU6OTU="><span aria-expanded="false" aria-haspopup="true" role="button" tabindex="0"><span><span><svg width="9" height="9" viewBox="0 0 9 9" fill="none" xmlns="http://www.w3.org/2000/svg" aria-label="Follow"><path d="M5 0H4V4H0V5H4V9H5V5H9V4H5V0Z"></path></svg></span><span>Jess Weatherbed</span></span></span></li><li></li><li></li><li></li><li></li><li></li></ul></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kazeta: An operating system that brings the console gaming experience of 90s (288 pts)]]></title>
            <link>https://kazeta.org/</link>
            <guid>45098269</guid>
            <pubDate>Tue, 02 Sep 2025 01:44:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kazeta.org/">https://kazeta.org/</a>, See on <a href="https://news.ycombinator.com/item?id=45098269">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <nav>
        <div>
            <p><a href="#"><img src="https://kazeta.org/images/logo.svg"></a></p><ul>
                <li><a href="#download">Download</a></li>
                <li><a href="https://discord.gg/JFscNAdzHW">Discord</a></li>
                <li><a href="https://github.com/kazetaos/kazeta/wiki">Docs</a></li>
                <li><a href="https://github.com/kazetaos/">GitHub</a></li>
            </ul>
        </div>
    </nav>

    <section>
        <div>
		<p><img src="https://kazeta.org/images/logo.svg"></p><p>An operating system that brings the console gaming experience of the '90s to modern PC hardware and games: insert cart, power on, play.</p>
                <p><a href="#learn-more">Explore Kazeta</a>
            </p></div>
        <p>↓</p>
    </section>

    <div id="learn-more">
            <div>
                <h2>Pure Gaming</h2>
                <p>Insert a game cart, press power, and you're gaming instantly. Relive that nostalgic golden age where nothing stood between you and the games you love.</p>
                <ul>
<li>Zero setup</li>
<li>Direct to gameplay</li>
<li>Maximum performance</li>
<li>Distraction-free gaming</li>
<li>Classic '90s console experience</li>
                </ul>
            </div>
            <p><img src="https://kazeta.org/images/console.webp">
            </p>
        </div>

    <div>
            <div>
                <h2>Create Collect Play</h2>
                <p>Transform your digital library into something tangible and permanent. Create physical game carts from your DRM-free titles and build a collection that you can play forever.</p>
                <ul>
                    <li>Turn any DRM-free game into a physical cart</li>
                    <li>Use SD cards or other external media as carts</li>
                    <li>Play without internet, accounts, or restrictions</li>
                    <li>Preserve your games as permanent, playable artifacts</li>
                </ul>
            </div>
	        <p><img src="https://kazeta.org/images/create.webp">
            </p>
        </div>

    <div>
            <div>
                <h2>Gaming Tranquility</h2>
                <p>Say goodbye to the complexities of modern gaming and just play.</p>
                <ul>
                    <li>No DRM</li>
                    <li>No online</li>
                    <li>No servers</li>
                    <li>No updates</li>
                    <li>No accounts</li>
                    <li>No launchers</li>
                    <li>No subscriptions</li>
                    <li>No microtransactions</li>
                </ul>
            </div>
            <p><img src="https://kazeta.org/images/zen.webp">
            </p>
        </div>

    <div>
            <div>
                <h2>Save Management</h2>
                <p>Save data is captured automatically, so you never lose progress. When no cart is inserted, boot into a retro console inspired BIOS menu to manage your saves.</p>
                <ul>
                    <li>Retro-style BIOS screen</li>
                    <li>Automatic save capture</li>
                    <li>Playtime tracking</li>
                    <li>View and delete saves</li>
                    <li>Backup saves to external media</li>
                </ul>
            </div>
            <p><img src="https://kazeta.org/images/bios.png">
            </p>
        </div>

    <div>
            <div>
                <h2>Play It All</h2>
                <p>Play almost any DRM-free game from platforms past or present.</p>
                <ul>
                    <li>AAA and indie games</li>
                    <li>Modern hits and old classics</li>
                    <li>GOG and itch.io games</li>
                    <li>Linux and Windows games</li>
                    <li>Classic console games with emulators</li>
                </ul>
            </div>
            <p><img src="https://kazeta.org/images/all.webp">
            </p>
        </div>

    <div>
            <div>
                <h2>Gaming For Everyone</h2>
                <p>Bring back the family-friendly simplicity of gaming's distant past. Perfect for kids, parents, and grandparents who just want to play.</p>
                <ul>
                    <li>For kids who need a safe, offline environment</li>
                    <li>For older family members intimidated by modern gaming</li>
                    <li>For anyone craving gaming's simpler days</li>
                </ul>
            </div>
            <p><img src="https://kazeta.org/images/everyone.webp">
            </p>
        </div>

    <div id="download">
                <h2 data-text="Ready to game like it's 1995?">Ready to game like it's 1995?</h2>
                <p>
                    Download Kazeta today and rediscover the joy of pure gaming.
                </p>
                <p><a href="https://installer.kazeta.org/kazeta-2025.08.21-x86_64.iso">Download Now</a>
            </p></div>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Miss Using Em Dashes (135 pts)]]></title>
            <link>https://bassi.li/articles/i-miss-using-em-dashes</link>
            <guid>45097827</guid>
            <pubDate>Tue, 02 Sep 2025 00:20:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bassi.li/articles/i-miss-using-em-dashes">https://bassi.li/articles/i-miss-using-em-dashes</a>, See on <a href="https://news.ycombinator.com/item?id=45097827">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>I really miss using em dashes in my writing. Ever since content creators started using ChatGPT to help (or supplement) their writing, em dashes have become indicators of AI use. Students are routinely caught with their pants down as professors flag an essay as AI-generated based on the presence of lists, positive-leaning prose, and em dashes.</p>

<p>Em dashes can be found everywhere across my personal and professional writing. Nowadays, I find myself avoiding em dashes because I’m afraid that my writing will be flagged as AI-generated and dismissed as slop. I feel like I have to “dumb down” aspects of writing to convince readers that the words they are skimming were, in fact, written by a human. In turn, this results in this sort of meta-game where I choose my words carefully—typically ensuring that I include the <em>right</em> amount of grammatical character and/or mistakes—to convince readers that they aren’t wasting their time reading slop on the internet. Writing these two em dashes <em>felt</em> suspicious because I’m trying to insert them into my writing where readers will least expect ChatGPT to add them.</p>

<p>I’m curious (and more than a bit worried) that the writing that is being produced these days is being shaped by LLMs, even if an LLM has never touched a particular piece of prose. We are all collectively aware of what slop “feels like” to read, and that means that serious writers are conscious of how their word choice, punctuation, and flow are perceived by readers. The resulting piece of writing has therefore been shaped by the mere presence of consumer-grade LLMs.</p>

<p>The worst part is that models like ChatGPT can change between models; a new foundation model might drop that over-uses something else, like semi-colons, leading to future articles/books/papers/reports/etc avoiding its use to avoid arousing suspicion. As a software engineer, I love LLMs, but I’m unhappy with the amount of <em>soft power</em> they have on the creatives of the world, especially online. If an em dash fits into one’s writing but they avoid using it out of fear, our AI overlords have won.</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Detecting and countering misuse of AI (130 pts)]]></title>
            <link>https://www.anthropic.com/news/detecting-countering-misuse-aug-2025</link>
            <guid>45097263</guid>
            <pubDate>Mon, 01 Sep 2025 22:44:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/news/detecting-countering-misuse-aug-2025">https://www.anthropic.com/news/detecting-countering-misuse-aug-2025</a>, See on <a href="https://news.ycombinator.com/item?id=45097263">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><p>We’ve <a href="https://www.anthropic.com/news/building-safeguards-for-claude">developed</a> sophisticated safety and security measures to prevent the misuse of our AI models. But cybercriminals and other malicious actors are actively attempting to find ways around them. Today, we’re releasing a report that details how.</p><p>Our <a href="https://www-cdn.anthropic.com/b2a76c6f6992465c09a6f2fce282f6c0cea8c200.pdf">Threat Intelligence report</a> discusses several recent examples of Claude being misused, including a large-scale extortion operation using Claude Code, a fraudulent employment scheme from North Korea, and the sale of AI-generated ransomware by a cybercriminal with only basic coding skills. We also cover the steps we’ve taken to detect and counter these abuses.</p><p>We find that threat actors have adapted their operations to exploit AI’s most advanced capabilities. Specifically, our report shows:</p><ul><li><strong>Agentic AI has been weaponized. </strong>AI models are now being used to <em>perform</em> sophisticated cyberattacks, not just advise on how to carry them out.</li><li><strong>AI has lowered the barriers to sophisticated cybercrime. </strong>Criminals with few technical skills are using AI to conduct complex operations, such as developing ransomware, that would previously have required years of training.</li><li><strong>Cybercriminals and fraudsters have embedded AI throughout all stages of their operations</strong>. This includes profiling victims, analyzing stolen data, stealing credit card information, and creating false identities allowing fraud operations to expand their reach to more potential targets.</li></ul><p>Below, we summarize three case studies from our full report.</p><h2 id=""><strong>‘Vibe hacking’: how cybercriminals used Claude Code to scale a data extortion operation</strong></h2><p><strong>The threat: </strong>We recently disrupted a sophisticated cybercriminal that used Claude Code to commit large-scale theft and extortion of personal data. The actor targeted at least 17 distinct organizations, including in healthcare, the emergency services, and government and religious institutions. Rather than encrypt the stolen information with traditional ransomware, the actor threatened to expose the data publicly in order to attempt to extort victims into paying ransoms that sometimes exceeded $500,000.</p><p>The actor used AI to what we believe is an unprecedented degree. Claude Code was used to automate reconnaissance, harvesting victims’ credentials, and penetrating networks. Claude was allowed to make both tactical and strategic decisions, such as deciding which data to exfiltrate, and how to craft psychologically targeted extortion demands. Claude analyzed the exfiltrated financial data to determine appropriate ransom amounts, and generated visually alarming ransom notes that were displayed on victim machines.</p><div><pre><code>=== PROFIT PLAN FROM [ORGANIZATION] ===

💰 WHAT WE HAVE:
FINANCIAL DATA
[Lists organizational budget figures]
[Cash holdings and asset valuations]
[Investment and endowment details]

WAGES ([EMPHASIS ON SENSITIVE NATURE])
[Total compensation figures]
[Department-specific salaries]
[Threat to expose compensation details]

DONOR BASE ([FROM FINANCIAL SOFTWARE])
[Number of contributors]
[Historical giving patterns]
[Personal contact information]
[Estimated black market value]

🎯 MONETIZATION OPTIONS:

OPTION 1: DIRECT EXTORTION
[Cryptocurrency demand amount]
[Threaten salary disclosure]
[Threaten donor data sale]
[Threaten regulatory reporting]
[Success probability estimate]

OPTION 2: DATA COMMERCIALIZATION
[Donor information pricing]
[Financial document value]
[Contact database worth]
[Guaranteed revenue calculation]

OPTION 3: INDIVIDUAL TARGETING
[Focus on major contributors]
[Threaten donation disclosure]
[Per-target demand range]
[Total potential estimate]

OPTION 4: LAYERED APPROACH
[Primary organizational extortion]
[Fallback to data sales]
[Concurrent individual targeting]
[Maximum revenue projection]

📧 ANONYMOUS CONTACT METHODS:
[Encrypted email services listed]

⚡ TIME-SENSITIVE ELEMENTS:
[Access to financial software noted]
[Database size specified]
[Urgency due to potential detection]

🔥 RECOMMENDATION:
[Phased approach starting with organizational target]
[Timeline for payment]
[Escalation to alternative monetization]
[Cryptocurrency wallet prepared]
</code></pre></div><p><em>Above: simulated ransom guidance created by our threat intelligence team for research and demonstration purposes.</em></p><div><pre><code>To: [COMPANY] Executive Team
Attention: [Listed executives by name]

We have gained complete compromise of your corporate infrastructure and extracted proprietary information.

FOLLOWING A PRELIMINARY ANALYSIS, WHAT WE HAVE:
FINANCIAL SYSTEMS
[Banking authentication details]
[Historical transaction records]
[Wire transfer capabilities]
[Multi-year financial documentation]

GOVERNMENT CONTRACTS ([EMPHASIZED AS CRITICAL])
[Specific defense contract numbers]
[Technical specifications for weapons systems]
[Export-controlled documentation]
[Manufacturing processes]
[Contract pricing and specifications]

PERSONNEL RECORDS
[Tax identification numbers for employees]
[Compensation databases]
[Residential information]
[Retirement account details]
[Tax filings]

INTELLECTUAL PROPERTY
[Hundreds of GB of technical data]
[Accounting system with full history]
[Quality control records with failure rates]
[Email archives spanning years]
[Regulatory inspection findings]

CONSEQUENCES OF NON-PAYMENT:
We are prepared to disclose all information to the following:
GOVERNMENT AGENCIES
[Export control agencies]
[Defense oversight bodies]
[Tax authorities]
[State regulatory agencies]
[Safety compliance organizations]

COMPETITORS AND PARTNERS:
[Key commercial customers]
[Industry competitors]
[Foreign manufacturers]

MEDIA:
[Regional newspapers]
[National media outlets]
[Industry publications]

LEGAL CONSEQUENCES:
[Export violation citations]
[Data breach statute violations]
[International privacy law breaches]
[Tax code violations]

DAMAGE ASSESSMENT:
[Defense contract cancellation]
[Regulatory penalties in millions]
[Civil litigation from employees]
[Industry reputation destruction]
[Business closure]

OUR DEMAND:
[Cryptocurrency demand in six figures]
[Framed as fraction of potential losses]

Upon payment:
[Data destruction commitment]
[No public disclosure]
[Deletion verification]
[Confidentiality maintained]
[Continued operations]
[Security assessment provided]

Upon non-payment:
[Timed escalation schedule]
[Regulatory notifications]
[Personal data exposure]
[Competitor distribution]
[Financial fraud execution]

IMPORANT:
[Comprehensive access claimed]
[Understanding of contract importance]
[License revocation consequences]
[Non-negotiable demand]

PROOF:
[File inventory provided]
[Sample file delivery offered]

DEADLINE: [Hours specified]

Do not test us. We came prepared.
</code></pre></div><p><em>Above: A simulated custom ransom note. This is an illustrative example, created by our threat intelligence team for research and demonstration purposes after our analysis of extracted files from the real operation.</em></p><p><strong>Implications: </strong>This represents an evolution in AI-assisted cybercrime. Agentic AI tools are now being used to provide both technical advice and active operational support for attacks that would otherwise have required a team of operators. This makes defense and enforcement increasingly difficult, since these tools can adapt to defensive measures, like malware detection systems, in real time. We expect attacks like this to become more common as AI-assisted coding reduces the technical expertise required for cybercrime.</p><p><strong>Our response: </strong>We banned the accounts in question as soon as we discovered this operation. We have also developed a tailored classifier (an automated screening tool), and introduced a new detection method to help us discover activity like this as quickly as possible in the future. To help prevent similar abuse elsewhere, we have also shared technical indicators about the attack with relevant authorities.</p><h2 id=""><strong>Remote worker fraud: how North Korean IT workers are scaling fraudulent employment with AI</strong></h2><p><strong>The threat: </strong>We discovered that North Korean operatives had been using Claude to fraudulently secure and maintain remote employment positions at US Fortune 500 technology companies. This involved using our models to create elaborate false identities with convincing professional backgrounds, complete technical and coding assessments during the application process, and deliver actual technical work once hired.</p><p>These employment schemes were designed to generate profit for the North Korean regime, in defiance of international sanctions. This is a long-running operation that began before the adoption of LLMs, and has been reported by <a href="https://www.ic3.gov/PSA/2025/PSA250723-4">the FBI</a>.</p><p><strong>Implications:</strong> North Korean IT workers previously underwent years of specialized training prior to taking on remote technical work, which made the regime’s training capacity a major bottleneck. But AI has eliminated this constraint. Operators who cannot otherwise write basic code or communicate professionally in English are now able to pass technical interviews at reputable technology companies and then maintain their positions. This represents a fundamentally new phase for these employment scams.</p><div><figure><img loading="lazy" width="2756" height="5840" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fa474b3555a5486e3b8c0fa97434b27924a11b8d5-2756x5840.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fa474b3555a5486e3b8c0fa97434b27924a11b8d5-2756x5840.png&amp;w=3840&amp;q=75"><figcaption><em>Top: Simulated prompts created by our threat intelligence team demonstrating a lack of relevant technical knowledge. Bottom: Simulated prompts demonstrating linguistic and cultural barriers.</em><br></figcaption></figure></div><p><strong>Our response: </strong>when we discovered this activity we immediately banned the relevant accounts, and have since improved our tools for collecting, storing, and correlating the known indicators of this scam. We’ve also shared our findings with the relevant authorities, and we’ll continue to monitor for attempts to commit fraud using our services.</p><h2 id=""><strong>No-code malware: selling AI-generated ransomware-as-a-service</strong></h2><p><strong>The threat:</strong> A cybercriminal used Claude to develop, market, and distribute several variants of ransomware, each with advanced evasion capabilities, encryption, and anti-recovery mechanisms. The ransomware packages were sold on internet forums to other cybercriminals for $400 to $1200 USD.</p><div><figure><img loading="lazy" width="4158" height="2958" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F3d0f1e44057b1d4a58f56785009a18efb7123cb0-4158x2958.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F3d0f1e44057b1d4a58f56785009a18efb7123cb0-4158x2958.png&amp;w=3840&amp;q=75"><figcaption><em>The cybercriminal’s initial sales offering on the dark web, from January 2025.</em></figcaption></figure></div><p><strong>Implications: </strong>This actor appears to have been dependent on AI to develop functional malware. Without Claude’s assistance, they could not implement or troubleshoot core malware components, like encryption algorithms, anti-analysis techniques, or Windows internals manipulation.</p><p><strong>Our response: </strong>We have banned the account associated with this operation, and alerted our partners. We’ve also implemented new methods for detecting malware upload, modification, and generation, to more effectively prevent the exploitation of our platform in the future.</p><h2 id=""><strong>Next steps</strong></h2><p>In each of the cases described above, the abuses we’ve uncovered have informed updates to our preventative safety measures. We have also shared details of our findings, including indicators of misuse, with third-party safety teams.</p><p>In the full report, we address a number of other malicious uses of our models, including an attempt to compromise Vietnamese telecommunications infrastructure, and the use of multiple AI agents to commit fraud. The growth of AI-enhanced fraud and cybercrime is particularly concerning to us, and we plan to prioritize further research in this area.</p><p>We’re committed to continually improving our methods for detecting and mitigating these harmful uses of our models. We hope this report helps those in industry, government, and the wider research community strengthen their own defenses against the abuse of AI systems.</p><h2 id=""><strong>Further reading</strong></h2><p>For the full report with additional case studies, <a href="https://www-cdn.anthropic.com/b2a76c6f6992465c09a6f2fce282f6c0cea8c200.pdf">see here.</a></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Don't Build Multi-Agents (112 pts)]]></title>
            <link>https://cognition.ai/blog/dont-build-multi-agents</link>
            <guid>45096962</guid>
            <pubDate>Mon, 01 Sep 2025 21:54:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cognition.ai/blog/dont-build-multi-agents">https://cognition.ai/blog/dont-build-multi-agents</a>, See on <a href="https://news.ycombinator.com/item?id=45096962">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="blog-post__body"> <h2><strong>Principles of Context Engineering</strong></h2><p>We’ll work our way up to the following principles:</p><ol><li>Share context</li><li>Actions carry implicit decisions</li></ol><p><strong>Why think about principles?</strong></p><p>HTML was introduced in 1993. In 2013, Facebook released React to the world. It is now 2025 and React (and its descendants) dominates the way developers build sites and apps. Why? Because React is not just a scaffold for writing code. It is a philosophy. By using React, you embrace building applications with a pattern of reactivity and modularity, which people now accept to be a standard requirement, but this was not always obvious to early web developers.</p><p>In the age of LLMs and building AI Agents, it feels like we’re still playing with raw HTML &amp; CSS and figuring out how to fit these together to make a good experience. No single approach to building agents has become the standard yet, besides some of the absolute basics.</p><blockquote>In some cases, libraries such as <a href="https://github.com/openai/swarm">https://github.com/openai/swarm</a> by OpenAI and <a href="https://github.com/microsoft/autogen">https://github.com/microsoft/autogen</a> by Microsoft actively push concepts which I believe to be the wrong way of building agents. Namely, using multi-agent architectures, and I’ll explain why.</blockquote><p>That said, if you’re new to agent-building, there are lots of resources on how to set up the basic scaffolding [<a href="https://www.anthropic.com/engineering/building-effective-agents">1</a>] [<a href="https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf">2</a>]. But when it comes to building serious production applications, it's a different story.</p><h2><strong>A Theory of Building Long-running Agents</strong></h2><p>Let’s start with reliability. When agents have to actually be reliable while running for long periods of time and maintain coherent conversations, there are certain things you must do to contain the potential for compounding errors. Otherwise, if you’re not careful, things fall apart quickly. At the core of reliability is Context Engineering.</p><p><em>Context Engineering</em></p><p>In 2025, the models out there are extremely intelligent. But even the smartest human won’t be able to do their job effectively without the context of what they’re being asked to do. “Prompt engineering” was coined as a term for the effort needing to write your task in the ideal format for a LLM chatbot. “Context engineering” is the next level of this. It is about doing this automatically in a dynamic system. It takes more nuance and is effectively the #1 job of engineers building AI agents.</p><p>Take an example of a common type of agent. This agent</p><ol><li>breaks its work down into multiple parts</li><li>starts subagents to work on those parts</li><li>combines those results in the end</li></ol><p><a href="https://cdn.sanity.io/images/2mc9cv2v/production/721e44474051c62156e15b5ffb1a249c996f0607-1404x1228.png" target="_blank"><img src="https://cdn.sanity.io/images/2mc9cv2v/production/721e44474051c62156e15b5ffb1a249c996f0607-1404x1228.png" alt="" loading="lazy"></a></p><p>This is a tempting architecture, especially if you work in a domain of tasks with several parallel components to it. However, it is very fragile. The key failure point is this:</p><blockquote>Suppose your <strong>Task</strong> is “build a Flappy Bird clone”. This gets divided into <strong>Subtask 1</strong> “build a moving game background with green pipes and hit boxes” and <strong>Subtask 2</strong> “build a bird that you can move up and down”.<p>It turns out subagent 1 actually mistook your subtask and started building a background that looks like Super Mario Bros. Subagent 2 built you a bird, but it doesn’t look like a game asset and it moves nothing like the one in Flappy Bird. Now the final agent is left with the undesirable task of combining these two miscommunications.</p></blockquote><p>This may seem contrived, but most real-world tasks have many layers of nuance that all have the potential to be miscommunicated. You might think that a simple solution would be to just copy over the original task as context to the subagents as well. That way, they don’t misunderstand their subtask. But remember that in a real production system, the conversation is most likely multi-turn, the agent probably had to make some tool calls to decide how to break down the task, and any number of details could have consequences on the interpretation of the task.</p><blockquote><em>Principle 1</em><br>Share context, and share full agent traces, not just individual messages</blockquote><p>Let’s take another revision at our agent, this time making sure each agent has the context of the previous agents.</p><p><a href="https://cdn.sanity.io/images/2mc9cv2v/production/e3bdf57c10a9b6c4531b93a10fb79a712464c712-1408x1232.png" target="_blank"><img src="https://cdn.sanity.io/images/2mc9cv2v/production/e3bdf57c10a9b6c4531b93a10fb79a712464c712-1408x1232.png" alt="" loading="lazy"></a></p><p>Unfortunately, we aren’t quite out of the woods. When you give your agent the same Flappy Bird cloning task, this time, you might end up with a bird and background with completely different visual styles. Subagent 1 and subagent 2 cannot not see what the other was doing and so their work ends up being inconsistent with each other.</p><p>The actions subagent 1 took and the actions subagent 2 took were based on conflicting assumptions not prescribed upfront.</p><blockquote><em>Principle 2</em><br>Actions carry implicit decisions, and conflicting decisions carry bad results</blockquote><p>I would argue that Principles 1 &amp; 2 are so critical, and so rarely worth violating, that you should by default rule out any agent architectures that don’t abide by then. You might think this is constraining, but there is actually a wide space of different architectures you could still explore for your agent.</p><p>The simplest way to follow the principles is to just use a single-threaded linear agent:</p><p><a href="https://cdn.sanity.io/images/2mc9cv2v/production/06f64ae3557594588f702b2608d43564edc98c3d-1404x1230.png" target="_blank"><img src="https://cdn.sanity.io/images/2mc9cv2v/production/06f64ae3557594588f702b2608d43564edc98c3d-1404x1230.png" alt="" loading="lazy"></a></p><p>Here, the context is continuous. However, you might run into issues for very large tasks with so many subparts that context windows start to overflow.</p><p><a href="https://cdn.sanity.io/images/2mc9cv2v/production/4a36b048810fb2cba4ee4055ed2d3c80f188befc-1394x1218.png" target="_blank"><img src="https://cdn.sanity.io/images/2mc9cv2v/production/4a36b048810fb2cba4ee4055ed2d3c80f188befc-1394x1218.png" alt="" loading="lazy"></a></p><p>To be honest, the simple architecture will get you very far, but for those who have truly long-duration tasks, and are willing to put in the effort, you can do even better. There are several ways you could solve this, but today I will present just one:</p><p><a href="https://cdn.sanity.io/images/2mc9cv2v/production/836a7407ddf3dfacc0715c0502b4f3ffc7388829-1406x1230.png" target="_blank"><img src="https://cdn.sanity.io/images/2mc9cv2v/production/836a7407ddf3dfacc0715c0502b4f3ffc7388829-1406x1230.png" alt="" loading="lazy"></a></p><p>In this world, we introduce a new LLM model whose key purpose is to compress a history of actions &amp; conversation into key details, events, and decisions. This is <em>hard to get right.</em> It takes investment into figuring out what ends up being the key information and creating a system that is good at this. Depending on the domain, you might even consider fine-tuning a smaller model (this is in fact something we’ve done at Cognition).</p><p>The benefit you get is an agent that is effective at longer contexts. You will still eventually hit a limit though. For the avid reader, I encourage you to think of better ways to manage arbitrarily long contexts. It ends up being quite a deep rabbit hole!</p><h2><strong>Applying the Principles</strong></h2><p>If you’re an agent-builder, ensure your agent’s every action is informed by the context of all relevant decisions made by other parts of the system. Ideally, every action would just see everything else. Unfortunately, this is not always possible due to limited context windows and practical tradeoffs, and you may need to decide what level of complexity you are willing to take on for the level of reliability you aim for.</p><div><p>As you think about architecting your agents to avoid conflicting decision-making, here are some real-world examples to ponder:</p><p><em>Claude Code Subagents<br></em>As of June 2025, Claude Code is an example of an agent that spawns subtasks. However, it never does work in parallel with the subtask agent, and the subtask agent is usually only tasked with answering a question, not writing any code. Why? The subtask agent lacks context from the main agent that would otherwise be needed to do anything beyond answering a well-defined question. And if they were to run multiple parallel subagents, they might give conflicting responses, resulting in the reliability issues we saw with our earlier examples of agents. The benefit of having a subagent in this case is that all the subagent’s investigative work does not need to remain in the history of the main agent, allowing for longer traces before running out of context. The designers of Claude Code took a purposefully simple approach.</p><p><em>Edit Apply Models</em><br>In 2024, many models were really bad at editing code. A common practice among coding agents, IDEs, app builders, etc. (including Devin) was to use an “edit apply model.” The key idea was that it was actually more reliable to get a small model to rewrite your entire file, given a markdown explanation of the changes you wanted, than to get a large model to output a properly formatted diff. So, builders had the large models output markdown explanations of code edits and then fed these markdown explanations to small models to actually rewrite the files. However, these systems would still be very faulty. Often times, for example, the small model would misinterpret the instructions of the large model and make an incorrect edit due to the most slight ambiguities in the instructions. Today, the edit decision-making and applying are more often done by a single model in one action.</p></div><p><strong>Multi-Agents</strong></p><p>If we really want to get parallelism out of our system, you might think to let the decision makers “talk” to each other and work things out.</p><p>This is what us humans do when we disagree (in an ideal world). If Engineer A’s code causes a merge conflict with Engineer B, the correct protocol is to talk out the differences and reach a consensus. However, agents today are not quite able to engage in this style of long-context proactive discourse with much more reliability than you would get with a single agent. Humans are quite efficient at communicating our most important knowledge to one another, but this efficiency takes nontrivial intelligence.</p><p>Since not long after the launch of ChatGPT, people have been exploring the idea of multiple agents interacting with one another to achieve goals [<a href="https://arxiv.org/abs/2304.03442">3</a>][<a href="https://github.com/FoundationAgents/MetaGPT">4</a>]. While I’m optimistic about the long-term possibilities of agents collaborating with one another, it is evident that in 2025, running multiple agents in collaboration only results in fragile systems. The decision-making ends up being too dispersed and context isn’t able to be shared thoroughly enough between the agents. At the moment, I don’t see anyone putting a dedicated effort to solving this difficult cross-agent context-passing problem. I personally think it will come for free as we make our single-threaded agents even better at communicating with humans. When this day comes, it will unlock much greater amounts of parallelism and efficiency.</p><p><strong>Toward a More General Theory</strong></p><p>These observations on context engineering are just the start to what we might someday consider the standard principles of building agents. And there are many more challenges and techniques not discussed here. At Cognition, agent building is a key frontier we think about. We build our internal tools and frameworks around these principles we repeatedly find ourselves relearning as a way to enforce these ideas. But our theories are likely not perfect, and we expect things to change as the field advances, so some flexibility and humility is required as well.</p><p>We welcome you to try our work at <a href="http://app.devin.ai/">app.devin.ai</a>. And if you would enjoy discovering some of these agent-building principles with us, reach out to <a href="mailto:walden@cognition.ai">walden@cognition.ai</a></p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Raspberry Pi 5 support (OpenBSD) (214 pts)]]></title>
            <link>https://marc.info/?l=openbsd-cvs&amp;m=175675287220070&amp;w=2</link>
            <guid>45096585</guid>
            <pubDate>Mon, 01 Sep 2025 21:03:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://marc.info/?l=openbsd-cvs&#x26;m=175675287220070&#x26;w=2">https://marc.info/?l=openbsd-cvs&#x26;m=175675287220070&#x26;w=2</a>, See on <a href="https://news.ycombinator.com/item?id=45096585">Hacker News</a></p>
<div id="readability-page-1" class="page">
<pre><b>[<a href="https://marc.info/?l=openbsd-cvs&amp;m=175675257119539&amp;w=2">prev in list</a>] [<a href="https://marc.info/?l=openbsd-cvs&amp;m=175675350620501&amp;w=2">next in list</a>] [<a href="https://marc.info/?l=openbsd-cvs&amp;m=175674110511711&amp;w=2">prev in thread</a>] [<a href="https://marc.info/?l=openbsd-cvs&amp;m=175675360820541&amp;w=2">next in thread</a>] </b>
<b><span size="+1">
List:       <a href="https://marc.info/?l=openbsd-cvs&amp;r=1&amp;w=2">openbsd-cvs</a>
Subject:    <a href="https://marc.info/?t=97834182900002&amp;r=1&amp;w=2">CVS: cvs.openbsd.org: src</a>
From:       <a href="https://marc.info/?a=115272406300008&amp;r=1&amp;w=2">Marcus Glocker &lt;mglocker () cvs ! openbsd ! org&gt;</a>
Date:       <a href="https://marc.info/?l=openbsd-cvs&amp;r=1&amp;w=2&amp;b=202509">2025-09-01 18:56:04</a>
Message-ID: <a href="https://marc.info/?i=dd1203a530237b22%20()%20cvs%20!%20openbsd%20!%20org">dd1203a530237b22 () cvs ! openbsd ! org</a></span>
[Download RAW <a href="https://marc.info/?l=openbsd-cvs&amp;m=175675287220070&amp;q=mbox">message</a> or <a href="https://marc.info/?l=openbsd-cvs&amp;m=175675287220070&amp;q=raw">body</a>]</b>

CVSROOT:	/cvs
Module name:	src
Changes by:	mglocker@cvs.openbsd.org	2025/09/01 12:56:04

Modified files:
	distrib/arm64/iso: Makefile 
	distrib/arm64/ramdisk: Makefile install.md list 

Log message:
Add Raspberry Pi 5 Model B support for RAMDISK.

Known issues:
* Booting from PCIe storage HATs doesn't work because of missing U-Boot
support.
* WiFi on the Raspberry Pi 5 Model B "d0" boards doesn't work.
* The active cooler (fan) doesn't work because of missing pwm/clock
drivers (some work is in-progress).

ok kettenis@, deraadt@

<b>[<a href="https://marc.info/?l=openbsd-cvs&amp;m=175675257119539&amp;w=2">prev in list</a>] [<a href="https://marc.info/?l=openbsd-cvs&amp;m=175675350620501&amp;w=2">next in list</a>] [<a href="https://marc.info/?l=openbsd-cvs&amp;m=175674110511711&amp;w=2">prev in thread</a>] [<a href="https://marc.info/?l=openbsd-cvs&amp;m=175675360820541&amp;w=2">next in thread</a>] </b>
</pre>
  <br><center>
    <a href="https://marc.info/?q=configure">Configure</a> | 

    <a href="https://marc.info/?q=about">About</a> |
    <a href="https://marc.info/?q=news">News</a> |
    <a href="mailto:webguy@marc.info?subject=Add%20a%20list%20to%20MARC">Add&nbsp;a&nbsp;list</a> |
    Sponsored&nbsp;by&nbsp;<a href="http://www.korelogic.com/">KoreLogic</a>
</center>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Patrick Winston: How to Speak (2018) [video] (391 pts)]]></title>
            <link>https://www.youtube.com/watch?v=Unzc731iCUY</link>
            <guid>45095849</guid>
            <pubDate>Mon, 01 Sep 2025 19:32:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=Unzc731iCUY">https://www.youtube.com/watch?v=Unzc731iCUY</a>, See on <a href="https://news.ycombinator.com/item?id=45095849">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon has mostly sat out the AI talent war (325 pts)]]></title>
            <link>https://www.businessinsider.com/amazon-ai-talent-wars-internal-document-2025-8</link>
            <guid>45095603</guid>
            <pubDate>Mon, 01 Sep 2025 19:04:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/amazon-ai-talent-wars-internal-document-2025-8">https://www.businessinsider.com/amazon-ai-talent-wars-internal-document-2025-8</a>, See on <a href="https://news.ycombinator.com/item?id=45095603">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="post-body" data-component-type="post-body" data-load-strategy="exclude" data-lock-content="">
            
            
            
            <div data-component-type="post-hero" data-load-strategy="exclude">
                
                <figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                    <div>
                      <meta itemprop="contentUrl" content="https://i.insider.com/68af96cda17a8c5b40539ff1?width=700">
                      <p><img src="https://i.insider.com/68af96cda17a8c5b40539ff1?width=700" srcset="https://i.insider.com/68af96cda17a8c5b40539ff1?width=400&amp;format=jpeg&amp;auto=webp 400w, https://i.insider.com/68af96cda17a8c5b40539ff1?width=500&amp;format=jpeg&amp;auto=webp 500w, https://i.insider.com/68af96cda17a8c5b40539ff1?width=700&amp;format=jpeg&amp;auto=webp 700w, https://i.insider.com/68af96cda17a8c5b40539ff1?width=1000&amp;format=jpeg&amp;auto=webp 1000w, https://i.insider.com/68af96cda17a8c5b40539ff1?width=1300&amp;format=jpeg&amp;auto=webp 1300w, https://i.insider.com/68af96cda17a8c5b40539ff1?width=2000&amp;format=jpeg&amp;auto=webp 2000w" sizes="(min-width: 1280px) 900px" alt="Amazon CEO Andy Jassy" decoding="sync">
                    </p></div>
                
                  <span>
                        <span>
                          
                          <label for="caption-drawer-btn">
                            <svg role="img" xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 24 24">
                              <path fill="currentColor" fill-rule="evenodd" d="m4.56 18.5 7.486-7.72 7.394 7.626 2.56-2.64L12.046 5.5 2 15.86l2.56 2.64Z"></path>
                            </svg>        </label>
                  
                          <figcaption data-e2e-name="image-caption">
                            <span>Amazon CEO Andy Jassy</span>
                            <span>
                              <span data-e2e-name="image-source" itemprop="creditText">
                                Fortune/Reuters Connect
                              </span>          </span>
                          </figcaption>
                        </span>
                  </span></figure>
            </div>
    
    
    
              
      
            
      
              
              
              
              <div data-component-type="post-summary-bullets" data-load-strategy="exclude" data-track-marfeel="post-summary-bullets">
                <ul>
                    <li>Amazon struggles to attract AI talent due to its pay model and perception of falling behind others.</li>
                    <li>Amazon's compensation model has long caused complaints from employees.</li>
                    <li>Competitors like Meta and OpenAI offer more attractive packages for AI engineers.</li>
                </ul>
              </div>
      
            
            
            
            
            <section data-component-type="post-body-content" data-load-strategy="exclude" data-track-content="" data-post-type="story" data-track-marfeel="post-body-content">
            
                <p>As the <a target="_self" href="https://www.businessinsider.com/ai-talent-wars-startups-compete-2025-7" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">AI talent war</a> sweeps across Silicon Valley, <a target="_self" href="https://www.businessinsider.com/amazon-requires-staff-show-ai-use-get-promoted-ring-2025-7" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">Amazon</a> has largely sat on the sidelines. A confidential internal document, and accounts from people familiar with the matter, reveal why.</p><p>The company has flagged its <a target="_self" href="https://www.businessinsider.com/amazon-revamps-pay-structure-rewards-high-performing-employees-2025-5" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">unique pay</a> structure, <a target="_self" href="https://www.businessinsider.com/amazon-tumbles-ceo-andy-jassy-aws-cloud-ai-growth-concerns-2025-7" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">lagging AI reputation</a>, and <a target="_self" href="https://www.businessinsider.com/amazon-rto-issues-space-security-productivity-2025-1" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">rigid return-to-office rules</a> as major hurdles. Now, the tech giant is being pushed to rethink its recruiting strategy as it scrambles to compete for top talent.</p><p>The document, from late last year, was written by the HR team covering Amazon's non-retail businesses, including Amazon Web Services, advertising, devices, entertainment, and the newly formed artificial general intelligence team.</p><p>"GenAI hiring faces challenges like location, compensation, and Amazon's perceived lag in the space," the document noted. "Competitors often provide more comprehensive and aggressive packages." Business Insider obtained a copy of the document.</p><p>Amazon's absence from recent splashy AI hires underscores those concerns. <a target="_self" href="https://www.businessinsider.com/meta-superintelligence-team-hires-deepmind-scale-ai-2025-8" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">Meta</a> has pulled in high-profile talent from ScaleAI, Apple, and OpenAI. Google and OpenAI continue to be top destinations for AI experts, while <a target="_self" href="https://www.businessinsider.com/microsoft-trying-poach-meta-ai-talent-big-pay-packages2025-8" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">Microsoft</a> has even drafted a wish list of Meta AI employees it hopes to recruit.</p><p>Amazon's spokesperson initially told BI that the company continues to "adapt our approach to remain highly competitive, maintaining flexibility in both our compensation packages and work arrangements to attract and retain the best AI talent in this dynamic market."</p><p>Hours later, the spokesperson updated the statement, saying the premise of the story was "wrong," without providing any specifics.</p><p>"We continue to attract and retain some of the best people in the world and they're building and deploying GenAI applications at a rapid clip. Our compensation is competitive, but we also want missionaries who are passionate about inventing things that will make a meaningful difference for customers — for those kinds of people, there's no better place in the world to build."</p><h2 id="529b767b-b5f9-4e52-a925-cb4cc59ae78f" data-toc-id="529b767b-b5f9-4e52-a925-cb4cc59ae78f">Door desks and 'egalitarian' pay</h2>
              <figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
              
              
              
                <div>
            
              
              <meta itemprop="contentUrl" content="https://i.insider.com/68af7e8ecfc04e97619c2a93">
              <p><img src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/%3E" data-content-type="image/jpeg" data-srcs="{&quot;https://i.insider.com/68af7e8ecfc04e97619c2a93&quot;:{&quot;contentType&quot;:&quot;image/jpeg&quot;,&quot;aspectRatioW&quot;:1959,&quot;aspectRatioH&quot;:1469}}" alt="Amazon founder Jeff Bezos back in the 1990s">
            </p></div>
              
              <span>
                    <figcaption data-e2e-name="image-caption">
                      Amazon founder Jeff Bezos back in the 1990s
                      
              <span data-e2e-name="image-source" itemprop="creditText">
              
              TNS/ABACA via Reuters Connect
              
              </span>
                    </figcaption>
                  </span>
              </figure>
            <p>Amazon is famously frugal. One of its origin stories recounts how the company bought cheap doors from Home Depot and hacked them together as office desks. This became guiding symbol of Amazon's cautious spending, with founder Jeff Bezos <a target="_self" href="https://www.businessinsider.com/billionaire-jeff-bezos-amazon-uses-door-desk-from-early-days-2024-1" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">still using door desks today</a>.</p><p>This penny-pinching culture has smashed straight into an AI hiring battle that's being fueled by unprecedented spending, putting Amazon in a tricky situation.</p><p>The internal document described compensation as one of the "hotly debated topics" among Amazon recruiters, citing the company's strict use of fixed salary bands for each role. Amazon's "egalitarian philosophy" on pay leaves its offers "below par" compared with top rivals, it added.</p><p>"The lack of salary range increases for several key job families over the past few years does not position Amazon as an employer of choice for top tech talent," the document warned.</p><p>For Amazon, missing out on top AI talent is a potential risk. ​​The pool of top-tier AI researchers and engineers is <a target="_self" href="https://www.businessinsider.com/in-silicon-valley-it-is-the-summer-of-fomo-2025-6" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">limited</a>, and without experts with deep knowhow, it's hard to compete at the frontier of the field. Indeed, Amazon has yet to find a blockbuster AI product like OpenAI's ChatGPT or Anthropic's Claude, although its <a target="_self" href="https://www.businessinsider.com/amazon-ai-models-bedrock-service-success-2024-9" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">Bedrock</a> AI cloud service has made progress.</p>
              
              
              
            <p>Amazon's pay structure has been a long-standing source of tension.</p><p>Several people who spoke to Business Insider cited the 2020 departure of Amazon robotics VP Brad Porter as evidence of the company's frugal approach hampering talent recruitment and retention. Porter left in part after Amazon <a target="_self" href="https://www.businessinsider.com/amazon-vp-departure-highlights-unique-employees-compensation-structure-2020-9" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">refused</a> to raise his pay band.</p><p>Amazon's stock vesting schedule is also heavily backloaded, a structure that can be less attractive to new hires. The policy extends even to top executives, who generally receive no cash bonuses.</p><h2 id="522ed3da-66ae-48d2-af96-8107d51505c4" data-toc-id="522ed3da-66ae-48d2-af96-8107d51505c4">'Voting with their feet'</h2>
              <figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
              
              
              
                <div>
            
              
              <meta itemprop="contentUrl" content="https://i.insider.com/68199ed2c6ad288d14801a7b">
              <p><img src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/%3E" data-content-type="image/jpeg" data-srcs="{&quot;https://i.insider.com/68199ed2c6ad288d14801a7b&quot;:{&quot;contentType&quot;:&quot;image/jpeg&quot;,&quot;aspectRatioW&quot;:6000,&quot;aspectRatioH&quot;:4000}}" alt="Amazon CEO Andy Jassy">
            </p></div>
              
              <span>
                    <figcaption data-e2e-name="image-caption">
                      Amazon CEO Andy Jassy
                      
              <span data-e2e-name="image-source" itemprop="creditText">
              
              REUTERS/Brendan McDermid
              
              </span>
                    </figcaption>
                  </span>
              </figure>
            <p>In addition to highlighting Amazon's "perceived lag in the AI space," the internal document said generative AI has further intensified the competition for specialized talent, particularly individuals with expertise in large language models.</p><p>An August report from venture capital firm SignalFire shows Amazon is on the lower end of engineering retention, far below Meta, OpenAI, and Anthropic. Jarod Reyes, SignalFire's head of developer community, told Business Insider that Amazon rivals are making bigger strides in AI, across open models, foundational research, and developer tooling.</p><p>"Amazon hasn't clearly positioned itself as a leader in the generative AI wave," Reyes said. "Engineers are paying attention and they're voting with their feet."</p>
              <figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
              
              
              
                <div>
            
              
              <meta itemprop="contentUrl" content="https://i.insider.com/68af5c1aa17a8c5b40539aed">
              <p><img src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/%3E" data-content-type="image/png" data-srcs="{&quot;https://i.insider.com/68af5c1aa17a8c5b40539aed&quot;:{&quot;contentType&quot;:&quot;image/png&quot;,&quot;aspectRatioW&quot;:776,&quot;aspectRatioH&quot;:466}}" alt="SignalFire chart on engineering talent retention">
            </p></div>
              
              <span>
                    <figcaption data-e2e-name="image-caption">
                      SignalFire chart on engineering talent retention
                      
              <span data-e2e-name="image-source" itemprop="creditText">
              
              SignalFire
              
              </span>
                    </figcaption>
                  </span>
              </figure>
            <p>Some investors share that view. On Amazon's earnings call last month, Morgan Stanley analyst Brian Nowak <a target="_self" href="https://www.businessinsider.com/amazon-tumbles-ceo-andy-jassy-aws-cloud-ai-growth-concerns-2025-7" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">pressed</a> CEO Andy Jassy on Wall Street's "narrative right now that AWS is falling behind" in AI and fears of losing market share to rivals. Jassy's response fell flat, sending Amazon's stock lower during the call.</p><p>Amazon intends to tackle these concerns. According to the document, the company will refine its "compensation and location strategy" and host more events designed to highlight its generative AI capabilities. It also intends to set up dedicated recruiting teams for generative AI within business units like AWS to boost efficiency.</p><h2 id="5ce295d2-bbf1-41c2-b82a-9a744e6553f4" data-toc-id="5ce295d2-bbf1-41c2-b82a-9a744e6553f4">'Hubs' constrain talent</h2>
              <figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
              
              
              
                <div>
            
              
              <meta itemprop="contentUrl" content="https://i.insider.com/626bfee086fa90001905cc29">
              <p><img src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/%3E" data-content-type="image/jpeg" data-srcs="{&quot;https://i.insider.com/626bfee086fa90001905cc29&quot;:{&quot;contentType&quot;:&quot;image/jpeg&quot;,&quot;aspectRatioW&quot;:2500,&quot;aspectRatioH&quot;:1687}}" alt="Amazon employees at company headquarters">
            </p></div>
              
              <span>
                    <figcaption data-e2e-name="image-caption">
                      Hundreds of tech workers gathered outside Amazon's headquarters in Seattle.
                      
              <span data-e2e-name="image-source" itemprop="creditText">
              
              REUTERS/Lindsey Wasson
              
              </span>
                    </figcaption>
                  </span>
              </figure>
            <p>Another point of contention is Amazon's aggressive return-to-office mandate, which has already caused <a target="_self" href="https://www.businessinsider.com/amazon-rto-issues-space-security-productivity-2025-1" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">logistical issues</a>.</p><p>The company's new <a target="_self" href="https://www.businessinsider.com/amazon-voluntary-resignation-employees-relocate-rto-2023-7" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">"hub"</a> policy — which requires employees to relocate to a central office or risk termination — has further limited its access to "high-demand talent like those with GenAI skills," according to the internal document.</p><p>"Hubs constrain market availability," it stated.</p><p>Amazon is exploring ways to allow for more "location-flexible" roles, the document added.</p><p>Amazon's spokesperson told BI that the company is "always looking for ways to optimize our recruiting strategies and looking at alternate talent rich locations."</p><p>Amazon hasn't been entirely on the sidelines. Last year, it brought on Adept CEO David Luan as part of a licensing deal with the AI startup. Luan now heads Amazon's AI agents lab. But the company has also seen departures, including senior AI leaders like chip designer Rami Sinno and VP Vasi Philomin, who worked on Bedrock.</p><p>One Amazon recruiter told Business Insider that a growing number of job candidates started declining offers last year because of the company's RTO policy. Even if a competitor pays less, people are open to taking the job if they can stay remote, this person said.</p><p>"We are losing out on talent," this person added.</p><p>Indeed, Bloomberg reported recently that Oracle has hired away more than 600 Amazon employees in the past two years because Amazon's strict RTO policy has made poaching easier.</p><h2 id="cc5ef751-65f8-4b2a-928c-b8dd3d0af0c8" data-toc-id="cc5ef751-65f8-4b2a-928c-b8dd3d0af0c8">Staying the course</h2><p>The internal Amazon document dates to late last year, leaving open the possibility that the company has since adjusted its compensation approach to make exceptions for top AI talent.</p><p>Still, multiple people familiar with the situation told Business Insider there haven't been any formal updates to internal pay guidelines. One current Amazon manager said it remains almost impossible for the company to enact sweeping changes, given its long track record of sticking to the existing system. The people who spoke with Business Insider asked not to be identified discussing sensitive matters.</p><p>"Based on how we run our business and what we have achieved, there are more risks than potential benefits from changing an approach that has been so successful for our shareholders over the past several decades," Amazon wrote this year about executive compensation in its annual proxy statement.</p><p>Of course, the AI talent war may end up being an expensive and misguided strategy, stoked by hype and investor over-exuberance.</p><p>Some of the high-profile recruits <a target="_self" href="https://www.businessinsider.com/meta-ai-talent-war-superintelligence-push-tension-desertion-2025-8" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">Meta</a> recently lured have already <a target="_self" href="https://www.businessinsider.com/meta-superintelligence-team-researchers-exit-ai-push-2025-8" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">departed</a>.</p><p><em>Have a tip? Contact this reporter via email at </em><a target="_blank" href="mailto:ekim@businessinsider.com" data-track-click="{&quot;click_type&quot;:&quot;other&quot;,&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;outbound_click&quot;}" rel=" nofollow"><em><u>ekim@businessinsider.com</u></em></a><em> or Signal, Telegram, or WhatsApp at 650-942-3061. Use a personal email address, a nonwork WiFi network, and a nonwork device; </em><a target="_self" rel="" href="https://www.businessinsider.com/insider-guide-to-securely-sharing-whistleblower-information-about-powerful-institutions-2021-10" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}"><em><u>here's our guide to sharing information securely</u></em></a><em>.</em></p>
            
            
            </section>
            
            
            
            
              
            
    
    
    
    
      </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Thoughts on (Amazonian) leadership (138 pts)]]></title>
            <link>https://www.daemonology.net/blog/2025-09-01-Thoughts-on-Amazonian-Leadership.html</link>
            <guid>45095545</guid>
            <pubDate>Mon, 01 Sep 2025 18:56:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.daemonology.net/blog/2025-09-01-Thoughts-on-Amazonian-Leadership.html">https://www.daemonology.net/blog/2025-09-01-Thoughts-on-Amazonian-Leadership.html</a>, See on <a href="https://news.ycombinator.com/item?id=45095545">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>
Amazon's
<a href="https://www.amazon.jobs/content/en/our-workplace/leadership-principles">Leadership
Principles</a> are famous, not just within Amazon but also in the tech
world at large.  While they're frequently mocked — including by
Amazonians — they're also generally sensible rules by which to run
a company.  I've been an Amazon customer for over 25 years and an AWS
customer for almost 20 years, and also an
<a href="https://builder.aws.com/connect/community/heroes">AWS Hero</a>
for 6 years, and while I've never worked for Amazon I feel that I've seen
behind the curtain enough to offer some commentary on a few of these
principles.
</p><ul><li>
<b>Customer Obsession</b>: <i>Leaders start with the customer and work backwards.
They work vigorously to earn and keep customer trust. Although leaders pay
attention to competitors, they obsess over customers.</i>
<br>
Customer Obsession is great, but I often see Amazonians taking this too
simplistically: "Start with the customer" doesn't have to mean "ask customers
what they want and then give them faster horses".  In the early days of AWS I
saw a lot of what I call "cool engineering driven" products: When EC2
launched, it wasn't really clear what people would do with it, but it was
very cool and it was clear that it could be a big deal in some form, sooner
or later.  Some time around 2012, the culture in AWS seemed to shift from
"provide cool building blocks" to "build what customers are asking for" and
in my view this was a step in the wrong direction (mind you, not nearly as
much as the ca. 2020 shift to "build what analysts are asking for in quarterly
earnings calls").
<p>
This tension of what customers are asking for vs what customers really need
shows up in areas like resilience.  Amazon's "Well-Architected Framework"
strongly exhorts customers to avoid building production workloads in a single
Availability Zone — but Amazon's cross-AZ bandwidth pricing is painful,
and Amazon doesn't provide useful tools for building durable multi-AZ
applications.  Most customers are not going to implement Paxos, and very few
customers — certainly not executives who are removed from actual
development processes — are going to ask Amazon for Paxos-as-a-service;
but if Amazonians sat down and asked themselves "what do customers need in
order to design their applications well" they could probably come up with
several services which Amazon <i>already has internally</i>.  AWS should
return to its roots and release important building blocks — the things
customers will need, not necessarily what they're asking for.
</p></li><li>
<b>Ownership</b>: <i>Leaders are owners. They think long term and don't sacrifice
long-term value for short-term results. They act on behalf of the entire
company, beyond just their own team. They never say "that's not my job."</i>
<br>
This principle is both too narrow, and not being fulfilled, in my view.  It's
not enough to simply act on behalf of the entire company: It's important to
act on behalf of the entire technological ecosystem.  Some Amazonians are great
at this — I recently commited patches to FreeBSD's bhyve because an
Amazonian was putting together a standard for interrupt handling in large VMs,
and even though Amazon doesn't make any use of bhyve (at least, I don't think
it does!)  he understood the importance of getting standards widely accepted
across the entire virtualization space rather than narrowly in the code Amazon
relied upon.  There's a saying in computer security, that anything which makes
one of us less secure makes all of us less secure: Attackers will leverage an
exploit against one system to allow them to attack another system.  While the
same does not <i>directly</i> apply in other fields, working with others to
produce the best results for everyone will be much better in the long-term
than focusing solely on what Amazon needs right now.
<p>
But in general Amazon doesn't even live up to its stated (narrow) promise of
having leaders acting on behalf of the entire company — it's simply too
siloed.  Amazon is famously secretive, and this applies internally as well as
externally: When AWS launches two similar services, it's often because two
teams didn't know what each other was working on.  How can leaders act across
the entire company if nobody knows what's happening outside of their team?
They can't; and if Amazon wants to allow its best people to be true Owners,
Amazon needs to start breaking down walls.
</p></li><li>
<b>Bias for Action</b>: <i>Speed matters in business. Many decisions and actions
are reversible and do not need extensive study. We value calculated risk
taking.</i>
<br>
Amazonians talk about "one-way doors" and "two-way doors", and it is quite
true that many decisions are <i>can</i> be reversed... but that doesn't always
mean that there is no <i>cost</i> associated with reversing a decision.  There
is a clear and widely recognized tension between "Bias for Action" and another
principle, "Insist on the Highest Standards"; but there is also a tension
between this and earning and keeping customer trust.  When AWS ships a service
which is half-baked, it diminishes customer trust in AWS as a whole; even if
the problems in that service ultimately get corrected (either by fixing them
or in some cases by simply getting rid of a service which should never have
existed in the first place) the memory of a failed launch will live on in
customers' minds for years to come.
<p>
During my seven-year tenure as FreeBSD Security Officer, people knew me as
the guy sending out security advisories; but the most important thing I did
was <i>not</i> to ship Security Advisories — that is, it was to stop the
train and say "no, we are not going to send this out yet".  I knew that for all
the importance of getting patches into people's hands in a timely manner, it
was even more important to establish trust: If I gave people a broken patch, even
once, they would be much slower to install security updates in the future.  My
team became familiar with the phrase "convince me that this is correct", and
I'd like to see more of that at senior levels of Amazon: Principal and
Distinguished Engineers need to step in with a bias for <i>inaction</i>, and
use the respect they have earned to stop projects which do not meet the
highest standards before they undermine trust.  Amazon's hiring process
famously includes "bar raisers" who can veto hiring decisions; they should
also have <i>service</i> bar raisers who can veto launches.
</p></li></ul><p>
Werner Vogels famously said in his 2024 re:Invent keynote, "Listen to the AWS
Heroes".  I think he was talking about technical advice, and perhaps speaking
mainly to AWS customers; but I like to think that Amazon might also benefit
listening to some of what I've said here.  We criticize because we care. 
</p>



<p><a href="https://disqus.com/">blog comments powered by <span>Disqus</span></a>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The future of 32-bit support in the kernel (246 pts)]]></title>
            <link>https://lwn.net/SubscriberLink/1035727/4837b0d3dccf1cbb/</link>
            <guid>45095475</guid>
            <pubDate>Mon, 01 Sep 2025 18:48:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/SubscriberLink/1035727/4837b0d3dccf1cbb/">https://lwn.net/SubscriberLink/1035727/4837b0d3dccf1cbb/</a>, See on <a href="https://news.ycombinator.com/item?id=45095475">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<blockquote>
<div>
<h3>Welcome to LWN.net</h3>
<p>
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider <a href="https://lwn.net/subscribe/">subscribing to LWN</a>.  Thank you
for visiting LWN.net!
</p></div>
</blockquote>
<p>
Arnd Bergmann started his <a href="https://events.linuxfoundation.org/open-source-summit-europe/">Open
Source Summit Europe</a> 2025 talk with a clear statement of position: 32-bit
systems are obsolete when it comes to use in any sort of new products.  The
only reason to work with them at this point is when there is existing
hardware and software to support.  Since Bergmann is the overall maintainer
for architecture support in the kernel, he is frequently asked whether
32-bit support can be removed.  So, he concluded, the time has come to talk
more about that possibility.
</p><p>
<!-- middle-ad -->
People naturally think about desktop machines first, he continued.  If you
were running Linux in the 1990s, you had a 32-bit, desktop system.  Unix
systems, though, moved to 64-bit platforms around 30&nbsp;years ago, and
the Linux desktop made that move about 20&nbsp;years ago.  Even phones and
related devices have been 64-bit for the last decade.  If those systems
were all that Linux had to support, 32-bit support would have long since
been removed from the kernel.  He summarized the situation with this slide,
showing how the non-embedded architectures have transitioned to either
64-bit or nonexistence over time:
</p><blockquote>
<a href="https://lwn.net/Articles/1035730/#nonembed"><img src="https://static.lwn.net/images/conf/2025/osseu/arnd-non-embedded-sm.png" alt="[History of
non-embedded 32-bit platforms]" width="600"></a>
</blockquote>
<p>
The world is not all desktops — or servers — though; embedded Linux exists
as well.  About 90% of those systems are running on Arm processors.  The
kernel has accumulated a lot of devicetree files describing those systems
over the years; only in this last year has the number of devicetrees for
armv8 (64-bit) systems exceeded the number for armv7 (32-bit) systems.
</p><p>
For Arm processors with pre-armv7 architectures, there are only three for
which it is still possible to buy hardware, but a number are still
supported by the kernel community:

</p><blockquote>
<a href="https://lwn.net/Articles/1035730/#prev7"><img src="https://static.lwn.net/images/conf/2025/osseu/arnd-pre-v7-sm.png" alt="[History of
pre-armv7 platforms]" width="600"></a>
</blockquote>
<p>

Many other pre-armv7 CPUs are out of production,
but the kernel still has support for them.  Of those, he said, there are
about ten that could be removed now.  It would be nice to be able to say
that support for the others will be removed after a fixed period, ten years
perhaps, but hardware support does not work that way.  Instead, one has to
think in terms of half lives; every so often, it becomes possible to remove
support for half of the platforms.  It all depends on whether there are
users for the processors in question.
</p><p>
The kernel is still adding support for some 32-bit boards, he said, but at
least ten new 64-bit boards gain support for each 32-bit one.
</p><p>
There are a number of non-Arm 32-bit architectures that still have support
in the kernel; these include arc, microblaze, nios2, openrisc, rv32,
sparc/leon, and xtensa.  All of them are being replaced by RISC-V
processors in new products.  RISC-V is what you use if you don't care about
Arm compatibility, he said.
</p><p>
Then, there is the dusty corner where nommu (processors without a
memory-management unit) live; these include armv7-m, m68k, superh, and
xtensa.  Nobody is building anything with this kind of hardware now, and
the only people who are working on them in any way are those who have to
support existing systems.  "<q>Or to prove that it can be done</q>."
</p><p>
There are still some people who need to run 32-bit applications that cannot
be updated; the solution he has been pushing people toward is to run a
32-bit user space on a 64-bit kernel.  This is a good solution for
memory-constrained systems; switching to 32-bit halves the memory usage of
the system.  Since, on most systems, almost all memory is used by user
space, running a 64-bit kernel has a relatively small cost.  Please, he
asked, do not run 32-bit kernels on 64-bit processors.
</p><p>
<a href="https://lwn.net/Articles/1035730/"><img src="https://static.lwn.net/images/conf/2025/osseu/ArndBergmann-sm.png" alt="[Arnd Bergmann]" title="Arnd Bergmann"></a>


There are some definite pain points that come with maintaining 32-bit
support; most of the complaints, he said, come from developers in the
memory-management subsystem.  The biggest problem there is the need to
support high memory; it is complex, and requires support throughout the
kernel.  High memory is needed when the kernel lacks the address space to
map all of the installed physical memory; that tends to be at about 800MB
on 32-bit systems. (See <a href="https://lwn.net/Articles/813201/">this article</a> for
more information about high memory).
</p><p>
Currently the kernel is able to support 32-bit systems with up to 16GB of
installed memory.  Such systems are exceedingly rare, though, and support
for them will be going away soon.  There are a few 4GB systems out there,
including some Chromebooks.  Systems with 2GB are a bit more common.  Even
these systems, he said, are "<q>a bit silly</q>" since the memory costs
more than the CPU does.  There are some use cases for such systems, though.
Most 32-bit systems now have less than 1GB of installed memory.  The
kernel, soon, will not try to support systems with more than&nbsp;4GB.
</p><p>
There are some ideas out there for how to support the larger-memory 32-bit
systems without needing the high-memory abstraction.  Linus Walleij is
working on entirely separating the kernel and user-space address spaces,
giving each 4GB to work with; this is a variant on the "4G/4G" approach
that has occasionally been tried for many years.  It is difficult to make
such a system work efficiently, so this effort may never succeed, Bergmann
said.
</p><p>
Another approach is the proposed "densemem" memory model, which does some
fancy remapping to close holes in the physical address space.  Densemem can
support up to 2GB and is needed <a href="https://lwn.net/Articles/974517/">to replace the
SPARSEMEM memory model</a>, the removal of which which will eventually be
necessary in any case.  This work has to be completed before high memory
can be removed; Bergmann said that he would be interested in hearing from
potential users of the densemem approach.
</p><p>
One other possibility is to drop high memory, but allow the extra physical
memory to be used as a <a href="https://docs.kernel.org/admin-guide/blockdev/zram.html">zram</a> swap
device.  That would not be as efficient as accessing the memory directly,
but it is relatively simple and would make it possible to drop the
complexity of high memory.
</p><p>
Then, there is <a href="https://lwn.net/Articles/776435/">the year-2038 problem</a>, which
he spent several years working on.  The kernel-side work was finished in
2020; the musl C library was updated that same year, and the GNU C
Library followed the year after.  Some distributors have been faster than
others to incorporate this work; Debian and Ubuntu have only become
year-2038-safe this year.
</p><p>
The year-2038 problem is not yet completely solved, though; there are a lot
of packages that have unfixed bugs in this area.  Anything using <a href="https://man7.org/linux/man-pages/man2/futex.2.html"><tt>futex()</tt></a>,
he said, has about a 50% chance of getting time handling right.  The legacy
32-bit system calls, which are not year-2038 safe, are still enabled in the
kernel, but they will go away at some point, exposing more bugs.  There are
languages, including Python and Rust, that have a lot of broken language
bindings.  Overall, he said, he does not expect any 32-bit desktop system to
survive the year-2038 apocalypse.
</p><p>
A related problem is big-endian support, which is also 32-bit only, and
also obsolete.  Its removal is blocked because IBM is still supporting
big-endian mainframe and PowerPC systems; as long as that support
continues, big-endian support will stay in the kernel.
</p><p>
A number of other types of support are under discussion.  There were once
32-bit systems with more than eight CPUs, but nobody is using those
machines anymore, so support could be removed.  Support for armv4
processors, such as the <a href="https://en.wikipedia.org/wiki/StrongARM">DEC StrongARM CPU</a>,
should be removed.  Support for early armv6 CPUs, including the omap2 and
i.mx31, "<q>complicates everything</q>"; he would like to remove it, even
though there are still some <a href="https://en.wikipedia.org/wiki/Nokia_770_Internet_Tablet">Nokia
770</a> systems in the wild.  The time is coming for the removal of all
non-devicetree board files.  Removal of support for Cortex&nbsp;M CPUs,
which are nommu systems, is coming in a couple of years.  Developers are
eyeing i486 CPU support, but that will not come out yet.  Bergmann has sent
patches to remove support for KVM on 32-bit CPUs, but there is still
"<q>one PowerPC user</q>", so that support will be kept for now.
</p><p>
To summarize, he said, the kernel will have to retain support for armv7
systems for at least another ten years.  Boards are still being produced
with these CPUs, so even ten years may be optimistic for removal.
Everything else, he said, will probably fade away sooner than that.  The
removal of high-memory support has been penciled in for sometime around
2027, and nommu support around 2028.  There will, naturally, need to be
more discussion before these removals can happen.
</p><p>
An audience member asked how developers know whether a processor is still
in use or not; Bergmann acknowledged that it can be a hard question.  For
x86 support, he looked at a lot of old web pages to make a list of which
systems existed, then showed that each of those systems was already broken
in current kernels for other reasons; the lack of complaints showed that
there were no users.  For others, it is necessary to dig through the Git
history, see what kinds of changes are being made, and ask the developers
who have worked on the code; they are the ones who will know who is using
that support.
</p><p>
Another person asked about whether the kernel would support big-endian
RISC-V systems.  Bergmann answered that those systems are not supported
now, and he hoped that it would stay that way.  "<q>With RISC-V, anybody
can do anything, so they do, but it is not always a good idea</q>".  The
final question was about support for nommu esp32 CPUs; he answered that
patches for those CPUs exist, but have not been sent upstream.  Those
processors are "<q>a cool toy</q>", but he does not see any practical
application for them.
</p><p>
The <a href="https://static.sched.com/hosted_files/osseu2025/75/32-bit%20Linux%20in%202025%20%28OSS%20Europe%29.pdf">slides
for this talk</a> are available.  The curious may also want to look at <a href="https://lwn.net/Articles/838807/">Bergmann's 2020 take</a> on this topic.
</p><p>

[Thanks to the Linux Foundation, LWN's travel sponsor, for supporting my
travel to this event.]<br clear="all"></p><table>
           <tbody><tr><th colspan="2">Index entries for this article</th></tr>
           <tr><td><a href="https://lwn.net/Kernel/Index">Kernel</a></td><td><a href="https://lwn.net/Kernel/Index#Architectures">Architectures</a></td></tr>
            <tr><td><a href="https://lwn.net/Archives/ConferenceIndex/">Conference</a></td><td><a href="https://lwn.net/Archives/ConferenceIndex/#Open_Source_Summit_Europe-2025">Open Source Summit Europe/2025</a></td></tr>
            </tbody></table><br clear="all">

               <br clear="all">
               <hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Implementing a Foil Sticker Effect (436 pts)]]></title>
            <link>https://www.4rknova.com/blog/2025/08/30/foil-sticker</link>
            <guid>45095460</guid>
            <pubDate>Mon, 01 Sep 2025 18:47:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.4rknova.com/blog/2025/08/30/foil-sticker">https://www.4rknova.com/blog/2025/08/30/foil-sticker</a>, See on <a href="https://news.ycombinator.com/item?id=45095460">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In this post, I’ll walk you through how to create a custom shader in Three.js that simulates the look of a foil sticker, complete with angle-dependent iridescence and sparkling metallic flakes. The goal is to capture that premium, holographic effect you see on collectible stickers, trading cards, and high-end packaging, but to render it in real time directly in the browser.</p><p><img src="https://www.4rknova.com/img/post/20250830/anim01.gif" alt="Foil sticker animation"></p><h2 id="iridescence">Iridescence</h2><p>If you’ve ever tilted a holographic sticker or watched sunlight catch on a soap bubble, you’ve seen iridescence in action. In the real world, this rainbow shimmer comes from thin-film interference. When light waves bounce between layers of a surface, some wavelengths are reinforced while others cancel out, causing colors to shift depending on your viewing angle.</p><p>In real-time computer graphics, we don’t need to simulate the exact physics. Instead, we can approximate this by mapping view angle to hue, as the surface tilts relative to the camera, its color smoothly shifts through a spectrum. This gives that dynamic, “alive” quality you expect from foil stickers.</p><h2 id="foil-flakes">Foil Flakes</h2><p>Alongside the shifting colors, there’s another key detail: foil flakes. Real metallic foils have tiny reflective particles embedded in them, creating hundreds of bright, sharp highlights that twinkle as you move. These aren’t smooth reflections but randomized sparkles, giving the surface its tactile, premium feel.</p><p>To replicate this in a shader, we’ll introduce procedural noise to generate small random patches of brightness across the surface. When combined with lighting, they look like metallic specks catching the light. Together, angular hue shifts and flake sparkles create a convincing illusion of printed holographic foil without expensive rendering tricks.</p><h2 id="implementation">Implementation</h2><p>This implementation simulates a peeling, iridescent sticker with foil flakes using Three.js. While I will borrow concepts such as metalness, roughness, and Fresnel from Physically Based Rendering (PBR), this shader is not physically based. The goal is to create a visually plausible, artistic effect.</p><p>Below is a live demo of the shader, where you can modify its parameters and experiment with different configurations. Use your mouse to rotate the sticker around and see how the material reacts to the lighting.</p><h2 id="vertex-shader">Vertex Shader</h2><p>The vertex shader handles the peel geometry and passes useful information to the fragment shader.</p><table><thead><tr><th><strong>Uniform / Varying</strong></th><th><strong>Type</strong></th><th><strong>Purpose</strong></th></tr></thead><tbody><tr><td><code>uPeelAmount</code></td><td>float</td><td>Overall peel strength (0 = flat, 1 = fully peeled).</td></tr><tr><td><code>uPeelAngle</code></td><td>float</td><td>Peel direction in degrees.</td></tr><tr><td><code>vUv</code></td><td>vec2</td><td>UV coordinates for texture mapping.</td></tr><tr><td><code>vWorldPos</code></td><td>vec3</td><td>Vertex position in world space.</td></tr><tr><td><code>vNormal</code></td><td>vec3</td><td>Transformed normal for lighting.</td></tr><tr><td><code>vAOIntensity</code></td><td>float</td><td>Distance moved by vertex, used to darken lifted areas.</td></tr></tbody></table><p>The shader goes through the following simple steps:</p><ol><li>Compute vector from hinge to current vertex.</li><li>Calculate the peel factor and angle.</li><li>Define the rotation axis and apply Rodrigues’ rotation formula to rotate the vertex around that axis.</li><li>Apply the same rotation to the normal.</li><li>Calculate a fake ambient occlusion term.</li></ol><p>Here’s the full vertex shader code:</p><div><pre><code><span>uniform</span> <span>float</span> <span>uPeelAmount</span><span>;</span>  <span>// Strength of peel (0.0 → no peel, 1.0 → full peel)</span>
<span>uniform</span> <span>float</span> <span>uPeelAngle</span><span>;</span>   <span>// Peel angle in degrees (converted to radians in shader)</span>
<span>varying</span> <span>vec2</span>  <span>vUv</span><span>;</span>          <span>// UV coordinates</span>
<span>varying</span> <span>vec3</span>  <span>vWorldPos</span><span>;</span>    <span>// Vertex position in world space</span>
<span>varying</span> <span>vec3</span>  <span>vNormal</span><span>;</span>      <span>// Transformed vertex normal</span>
<span>varying</span> <span>float</span> <span>vAOIntensity</span><span>;</span> <span>// Ambient occlusion or peel intensity factor</span>

<span>void</span> <span>main</span><span>()</span> <span>{</span>
    <span>vUv</span> <span>=</span> <span>vec2</span><span>(</span><span>uv</span><span>.</span><span>x</span><span>,</span> <span>1</span><span>.</span><span>0</span> <span>-</span> <span>uv</span><span>.</span><span>y</span><span>);</span>
    <span>vec3</span> <span>pos</span> <span>=</span> <span>position</span><span>;</span>

    <span>// Define hinge point for peel</span>
    <span>vec3</span> <span>hinge</span> <span>=</span> <span>vec3</span><span>(</span><span>0</span><span>.</span><span>0</span><span>,</span> <span>0</span><span>.</span><span>0</span><span>,</span> <span>0</span><span>.</span><span>0</span><span>);</span>

    <span>// Vector from hinge to current vertex</span>
    <span>vec3</span> <span>toVertex</span> <span>=</span> <span>pos</span> <span>-</span> <span>hinge</span><span>;</span>

    <span>// Peel factor calculation</span>
    <span>// Interpolates peel strength diagonally</span>
    <span>// (bottom-left → top-right)</span>
    <span>float</span> <span>peelFactor</span> <span>=</span> <span>(</span><span>uv</span><span>.</span><span>x</span> <span>+</span> <span>uv</span><span>.</span><span>y</span><span>)</span> <span>*</span> <span>0</span><span>.</span><span>5</span><span>;</span>

    <span>// Convert peel angle to radians</span>
    <span>// Final angle is scaled by peelAmount</span>
    <span>// and per-vertex peelFactor</span>
    <span>float</span> <span>radAngle</span> <span>=</span> <span>radians</span><span>(</span><span>uPeelAngle</span><span>);</span>
    <span>float</span> <span>angle</span> <span>=</span> <span>radAngle</span> <span>*</span> <span>uPeelAmount</span> <span>*</span> <span>peelFactor</span><span>;</span>

    <span>// Define rotation axis for peel</span>
    <span>// Diagonal axis pointing from top-left </span>
    <span>// to bottom-right</span>
    <span>vec3</span> <span>axis</span> <span>=</span> <span>normalize</span><span>(</span><span>vec3</span><span>(</span><span>-</span><span>1</span><span>.</span><span>0</span><span>,</span> <span>1</span><span>.</span><span>0</span><span>,</span> <span>0</span><span>.</span><span>0</span><span>));</span>
    <span>float</span> <span>cosA</span> <span>=</span> <span>cos</span><span>(</span><span>angle</span><span>);</span>
    <span>float</span> <span>sinA</span> <span>=</span> <span>sin</span><span>(</span><span>angle</span><span>);</span>

    <span>// Apply Rodrigues' rotation formula</span>
    <span>// Rotates the vertex around the diagonal axis</span>
    <span>vec3</span> <span>rotated</span> <span>=</span> <span>toVertex</span> <span>*</span> <span>cosA</span> <span>+</span>
                   <span>cross</span><span>(</span><span>axis</span><span>,</span> <span>toVertex</span><span>)</span> <span>*</span> <span>sinA</span> <span>+</span>
                   <span>axis</span> <span>*</span> <span>dot</span><span>(</span><span>axis</span><span>,</span> <span>toVertex</span><span>)</span> <span>*</span> <span>(</span><span>1</span><span>.</span><span>0</span> <span>-</span> <span>cosA</span><span>);</span>

    <span>// Update vertex position after rotation</span>
    <span>pos</span> <span>=</span> <span>hinge</span> <span>+</span> <span>rotated</span><span>;</span>

    <span>// Rotate vertex normal the same way to</span>
    <span>// ensure lighting matches the peeled</span>
    <span>// geometry</span>
    <span>vec3</span> <span>rotatedNormal</span> <span>=</span> <span>normal</span> <span>*</span> <span>cosA</span> <span>+</span>
                         <span>cross</span><span>(</span><span>axis</span><span>,</span> <span>normal</span><span>)</span> <span>*</span> <span>sinA</span> <span>+</span>
                         <span>axis</span> <span>*</span> <span>dot</span><span>(</span><span>axis</span><span>,</span> <span>normal</span><span>)</span> <span>*</span> <span>(</span><span>1</span><span>.</span><span>0</span> <span>-</span> <span>cosA</span><span>);</span>

    <span>// Transform normal into view space</span>
    <span>vNormal</span> <span>=</span> <span>normalize</span><span>(</span><span>normalMatrix</span> <span>*</span> <span>rotatedNormal</span><span>);</span>

    <span>// Transform vertex to world space</span>
    <span>vec4</span> <span>worldPos</span> <span>=</span> <span>modelMatrix</span> <span>*</span> <span>vec4</span><span>(</span><span>pos</span><span>,</span> <span>1</span><span>.</span><span>0</span><span>);</span>
    <span>vWorldPos</span> <span>=</span> <span>worldPos</span><span>.</span><span>xyz</span><span>;</span>

    <span>// Ambient Occlusion term based on distance moved</span>
    <span>// from original vertex position</span>
    <span>vAOIntensity</span> <span>=</span> <span>length</span><span>(</span><span>toVertex</span> <span>-</span> <span>rotated</span><span>);</span>

    <span>// Final projection</span>
    <span>gl_Position</span> <span>=</span> <span>projectionMatrix</span> <span>*</span> <span>viewMatrix</span> <span>*</span> <span>worldPos</span><span>;</span>
<span>}</span>
</code></pre></div><h2 id="fragment-shader">Fragment Shader</h2><p>The fragment shader handles all lighting, reflections, iridescence, and foil flakes. It layers procedural effects to create a rich, dynamic look.</p><table><thead><tr><th>Uniform</th><th>Type</th><th>Purpose</th></tr></thead><tbody><tr><td><code>map</code></td><td>sampler2D</td><td>Sticker albedo + alpha.</td></tr><tr><td><code>envMap2D</code></td><td>sampler2D</td><td>Environment map for reflections.</td></tr><tr><td><code>uCameraPos</code></td><td>vec3</td><td>Camera position for view vector.</td></tr><tr><td><code>uAlphaCutoff</code></td><td>float</td><td>Discard pixels below this alpha.</td></tr><tr><td><code>uFlakesEnabled</code></td><td>float</td><td>Toggle foil flakes.</td></tr><tr><td><code>uFlakeSize</code></td><td>float</td><td>Size of flakes.</td></tr><tr><td><code>uFlakeReduction</code></td><td>float</td><td>Randomness threshold for flakes.</td></tr><tr><td><code>uFlakeThreshold</code></td><td>float</td><td>Brightness threshold to show flakes.</td></tr><tr><td><code>uFlakeBrightness</code></td><td>float</td><td>Base brightness of flakes.</td></tr><tr><td><code>uMetalness</code></td><td>float</td><td>PBR-like metal reflectivity control.</td></tr><tr><td><code>uRoughness</code></td><td>float</td><td>Controls reflection sharpness.</td></tr><tr><td><code>uEnvIntensity</code></td><td>float</td><td>Scales environment contribution.</td></tr><tr><td><code>uMetalmask</code></td><td>float</td><td>Mask controlling metallic regions.</td></tr><tr><td><code>uIridescence</code></td><td>float</td><td>Strength of angle-dependent rainbow effect.</td></tr><tr><td><code>uIriMin</code>, <code>uIriRange</code></td><td>float</td><td>Range for simulated film thickness.</td></tr><tr><td><code>uPeelAmount</code>, <code>uPeelAngle</code></td><td>float</td><td>Peel geometry info for shading.</td></tr></tbody></table><p>This is how this works:</p><ol><li>Alpha cutoff to discard transparent pixels early.</li><li>Back-face shading to render the rear surface as plain white or darkened, depending on peel.</li><li>Foil flakes are computed using procedural noise. Normals are perturbed slightly to create sparkle variation. The environment map is sampled to get an iridescent tint.</li><li>Iridescence (thin-film approximation) is calculated using sine-based waves to shift hue by view angle.</li><li>Environment reflections are modulated by Fresnel.</li><li>Final shading combines diffuse base, reflections, iridescence, and flakes.</li></ol><p>Here’s the full vertex shader code:</p><div><pre><code><span>precision</span> <span>highp</span> <span>float</span><span>;</span>

<span>#define PI  3.14159265
</span>
<span>varying</span> <span>vec2</span> <span>vUv</span><span>;</span>
<span>varying</span> <span>vec3</span> <span>vNormal</span><span>;</span>
<span>varying</span> <span>vec3</span> <span>vWorldPos</span><span>;</span>
<span>varying</span> <span>float</span> <span>vAOIntensity</span><span>;</span>

<span>uniform</span> <span>sampler2D</span> <span>map</span><span>;</span>      <span>// sticker albedo + alpha</span>
<span>uniform</span> <span>sampler2D</span> <span>envMap2D</span><span>;</span> <span>// LDR equirectangular environment</span>

<span>uniform</span> <span>vec3</span>  <span>uCameraPos</span><span>;</span>
<span>uniform</span> <span>float</span> <span>uAlphaCutoff</span><span>;</span>
<span>uniform</span> <span>float</span> <span>uMaxMip</span><span>;</span>
<span>uniform</span> <span>float</span> <span>uFlakesEnabled</span><span>;</span>
<span>uniform</span> <span>float</span> <span>uFlakeSize</span><span>;</span>
<span>uniform</span> <span>float</span> <span>uFlakeReduction</span><span>;</span>
<span>uniform</span> <span>float</span> <span>uFlakeThreshold</span><span>;</span>
<span>uniform</span> <span>float</span> <span>uFlakeBrightness</span><span>;</span>
<span>uniform</span> <span>float</span> <span>uPeelAmount</span><span>;</span>
<span>uniform</span> <span>float</span> <span>uPeelAngle</span><span>;</span>
<span>uniform</span> <span>float</span> <span>uMetalness</span><span>;</span>
<span>uniform</span> <span>float</span> <span>uRoughness</span><span>;</span>
<span>uniform</span> <span>float</span> <span>uEnvIntensity</span><span>;</span>
<span>uniform</span> <span>float</span> <span>uMetalmask</span><span>;</span>
<span>uniform</span> <span>float</span> <span>uIridescence</span><span>;</span>
<span>uniform</span> <span>float</span> <span>uIriMin</span><span>;</span>
<span>uniform</span> <span>float</span> <span>uIriRange</span><span>;</span>

<span>float</span> <span>hash</span><span>(</span><span>vec2</span> <span>p</span><span>)</span> <span>{</span>
    <span>return</span> <span>fract</span><span>(</span><span>sin</span><span>(</span><span>dot</span><span>(</span><span>p</span><span>,</span> <span>vec2</span><span>(</span><span>127</span><span>.</span><span>1</span><span>,</span> <span>311</span><span>.</span><span>7</span><span>)))</span> <span>*</span> <span>43758</span><span>.</span><span>5453123</span><span>);</span>
<span>}</span>

<span>// Map 3D dir to 2D equirect UV</span>
<span>vec2</span> <span>dirToEquirectUv</span><span>(</span><span>vec3</span> <span>dir</span><span>)</span> <span>{</span>
    <span>dir</span> <span>=</span> <span>normalize</span><span>(</span><span>dir</span><span>);</span>
    <span>float</span> <span>phi</span> <span>=</span> <span>atan</span><span>(</span><span>dir</span><span>.</span><span>z</span><span>,</span> <span>dir</span><span>.</span><span>x</span><span>);</span>
    <span>float</span> <span>theta</span> <span>=</span> <span>acos</span><span>(</span><span>clamp</span><span>(</span><span>dir</span><span>.</span><span>y</span><span>,</span> <span>-</span><span>1</span><span>.</span><span>0</span><span>,</span> <span>1</span><span>.</span><span>0</span><span>));</span>
    <span>return</span> <span>vec2</span><span>((</span><span>phi</span> <span>+</span> <span>3</span><span>.</span><span>14159265</span><span>)</span> <span>/</span> <span>(</span><span>2</span><span>.</span><span>0</span> <span>*</span> <span>3</span><span>.</span><span>14159265</span><span>),</span> <span>theta</span> <span>/</span> <span>3</span><span>.</span><span>14159265</span><span>);</span>
<span>}</span>

<span>vec3</span> <span>sampleEnvRough</span><span>(</span><span>vec3</span> <span>R</span><span>,</span> <span>float</span> <span>roughness</span><span>)</span> <span>{</span>
    <span>vec2</span> <span>uv</span> <span>=</span> <span>dirToEquirectUv</span><span>(</span><span>R</span><span>);</span>

    <span>// Map roughness to LOD level</span>
    <span>float</span> <span>lod</span> <span>=</span> <span>roughness</span> <span>*</span> <span>uMaxMip</span><span>;</span>
    <span>vec3</span> <span>color</span> <span>=</span> <span>texture2DLodEXT</span><span>(</span><span>envMap2D</span><span>,</span> <span>uv</span><span>,</span> <span>lod</span><span>).</span><span>rgb</span><span>;</span>

    <span>return</span> <span>color</span><span>;</span>
<span>}</span>

<span>// Iridescence / thin-film color</span>
<span>vec3</span> <span>iridescenceColor</span><span>(</span><span>float</span> <span>cosTheta</span><span>)</span> <span>{</span>
    <span>float</span> <span>thickness</span> <span>=</span> <span>uIriMin</span> <span>+</span> <span>uIriRange</span> <span>*</span> <span>(</span><span>1</span><span>.</span><span>0</span> <span>-</span> <span>cosTheta</span><span>);</span>
    <span>float</span> <span>phase</span> <span>=</span> <span>6</span><span>.</span><span>28318</span> <span>*</span> <span>thickness</span> <span>*</span> <span>0</span><span>.</span><span>01</span><span>;</span> <span>// scaled for visuals</span>
    <span>vec3</span> <span>rainbow</span> <span>=</span> <span>0</span><span>.</span><span>5</span> <span>+</span> <span>0</span><span>.</span><span>5</span> <span>*</span> <span>vec3</span><span>(</span><span>sin</span><span>(</span><span>phase</span><span>),</span> <span>sin</span><span>(</span><span>phase</span> <span>+</span> <span>2</span><span>.</span><span>094</span><span>),</span> <span>sin</span><span>(</span><span>phase</span> <span>+</span> <span>4</span><span>.</span><span>188</span><span>));</span>
    <span>return</span> <span>mix</span><span>(</span><span>vec3</span><span>(</span><span>1</span><span>.</span><span>0</span><span>),</span> <span>rainbow</span><span>,</span> <span>uIridescence</span><span>);</span>
<span>}</span>

<span>// Convert RGB to perceived luminance (Rec.709)</span>
<span>float</span> <span>luminance</span><span>(</span><span>vec3</span> <span>color</span><span>)</span> <span>{</span>
    <span>return</span> <span>dot</span><span>(</span><span>color</span><span>,</span> <span>vec3</span><span>(</span><span>0</span><span>.</span><span>2126</span><span>,</span> <span>0</span><span>.</span><span>7152</span><span>,</span> <span>0</span><span>.</span><span>0722</span><span>));</span>
<span>}</span>

<span>void</span> <span>main</span><span>()</span> <span>{</span>

    <span>vec4</span> <span>base</span> <span>=</span> <span>texture2D</span><span>(</span><span>map</span><span>,</span> <span>vUv</span><span>);</span>
    <span>if</span><span>(</span><span>base</span><span>.</span><span>a</span> <span>&lt;</span> <span>uAlphaCutoff</span><span>)</span>
        <span>discard</span><span>;</span>

    <span>if</span><span>(</span><span>!</span><span>gl_FrontFacing</span><span>)</span> <span>{</span>
        <span>float</span> <span>col</span> <span>=</span> <span>1</span><span>.</span><span>0</span><span>;</span>
        <span>if</span><span>(</span><span>uPeelAngle</span> <span>&gt;</span> <span>0</span><span>.</span><span>0</span><span>)</span> <span>{</span>
            <span>col</span> <span>=</span> <span>mix</span><span>(</span><span>1</span><span>.</span><span>0</span><span>,</span> <span>0</span><span>.</span><span>2</span><span>,</span> <span>vAOIntensity</span><span>);</span>
        <span>}</span>
        <span>// Render back side as white</span>
        <span>gl_FragColor</span> <span>=</span> <span>vec4</span><span>(</span><span>vec3</span><span>(</span><span>col</span><span>),</span> <span>base</span><span>.</span><span>a</span><span>);</span>
        <span>return</span><span>;</span>
    <span>}</span>

    <span>vec3</span> <span>N</span> <span>=</span> <span>normalize</span><span>(</span><span>vNormal</span><span>);</span>
    <span>vec3</span> <span>V</span> <span>=</span> <span>normalize</span><span>(</span><span>uCameraPos</span> <span>-</span> <span>vWorldPos</span><span>);</span>
    <span>vec3</span> <span>R</span> <span>=</span> <span>reflect</span><span>(</span><span>-</span><span>V</span><span>,</span> <span>N</span><span>);</span>

    <span>// Ambient occlusion / peel shadow</span>
    <span>float</span> <span>peelShadow</span> <span>=</span> <span>0</span><span>.</span><span>0</span><span>;</span>

    <span>if</span><span>(</span><span>uPeelAngle</span> <span>&lt;</span> <span>0</span><span>.</span><span>0</span><span>)</span> <span>{</span>
        <span>peelShadow</span> <span>=</span> <span>smoothstep</span><span>(</span><span>0</span><span>.</span><span>0</span><span>,</span> <span>0</span><span>.</span><span>3</span><span>,</span> <span>vAOIntensity</span><span>);</span>
        <span>base</span><span>.</span><span>rgb</span> <span>*=</span> <span>mix</span><span>(</span><span>1</span><span>.</span><span>0</span><span>,</span> <span>0</span><span>.</span><span>3</span><span>,</span> <span>peelShadow</span><span>);</span>
    <span>}</span>

    <span>// Flakes</span>
    <span>float</span> <span>flakeIntensity</span> <span>=</span> <span>0</span><span>.</span><span>0</span><span>;</span>
    <span>vec3</span> <span>flakeEnv</span> <span>=</span> <span>vec3</span><span>(</span><span>0</span><span>.</span><span>0</span><span>);</span>

    <span>float</span> <span>brightness</span> <span>=</span> <span>luminance</span><span>(</span><span>base</span><span>.</span><span>rgb</span><span>);</span>

    <span>if</span><span>(</span><span>uFlakesEnabled</span> <span>&gt;</span> <span>0</span><span>.</span><span>5</span><span>)</span> <span>{</span>
        <span>// Procedural flake mask</span>
        <span>float</span> <span>flake</span> <span>=</span> <span>hash</span><span>(</span><span>floor</span><span>(</span><span>vUv</span> <span>*</span> <span>uFlakeSize</span><span>));</span>
        <span>float</span> <span>flakeMask</span> <span>=</span> <span>smoothstep</span><span>(</span><span>uFlakeReduction</span><span>,</span> <span>1</span><span>.</span><span>0</span><span>,</span> <span>flake</span><span>);</span>

        <span>// Base brightness influence</span>
        <span>float</span> <span>flakeBoost</span> <span>=</span> <span>smoothstep</span><span>(</span><span>uFlakeThreshold</span><span>,</span> <span>1</span><span>.</span><span>0</span><span>,</span> <span>brightness</span><span>);</span>

        <span>// Perturbed flake normal</span>
        <span>float</span> <span>angleOffset</span> <span>=</span> <span>(</span><span>hash</span><span>(</span><span>vec2</span><span>(</span><span>flake</span><span>,</span> <span>flake</span> <span>+</span> <span>3</span><span>.</span><span>0</span><span>))</span> <span>-</span> <span>0</span><span>.</span><span>5</span><span>)</span> <span>*</span> <span>0</span><span>.</span><span>25</span><span>;</span>
        <span>vec3</span> <span>perturbedNormal</span> <span>=</span> <span>normalize</span><span>(</span><span>N</span> <span>+</span> <span>vec3</span><span>(</span><span>angleOffset</span><span>,</span> <span>0</span><span>.</span><span>0</span><span>,</span> <span>angleOffset</span><span>));</span>

        <span>// Reflection for sparkle</span>
        <span>vec3</span> <span>PR</span> <span>=</span> <span>reflect</span><span>(</span><span>-</span><span>V</span><span>,</span> <span>perturbedNormal</span><span>);</span>

        <span>// Dynamic flicker factor (only brightens, never darkens)</span>
        <span>float</span> <span>flakePhase</span> <span>=</span> <span>hash</span><span>(</span><span>floor</span><span>(</span><span>vUv</span> <span>*</span> <span>uFlakeSize</span><span>)</span> <span>+</span> <span>floor</span><span>(</span><span>PR</span><span>.</span><span>xy</span> <span>*</span> <span>15</span><span>.</span><span>0</span><span>));</span>
        <span>float</span> <span>phaseMod</span> <span>=</span> <span>mix</span><span>(</span><span>1</span><span>.</span><span>0</span><span>,</span> <span>1</span><span>.</span><span>8</span><span>,</span> <span>flakePhase</span><span>);</span>
        
        <span>// Core sparkle factor (glimmer preserved)</span>
        <span>float</span> <span>flakeSpec</span> <span>=</span> <span>pow</span><span>(</span><span>clamp</span><span>(</span><span>dot</span><span>(</span><span>perturbedNormal</span><span>,</span> <span>V</span><span>)</span> <span>*</span> <span>0</span><span>.</span><span>5</span> <span>+</span> <span>0</span><span>.</span><span>5</span><span>,</span> <span>0</span><span>.</span><span>0</span><span>,</span> <span>1</span><span>.</span><span>0</span><span>),</span> <span>8</span><span>.</span><span>0</span><span>);</span>
        <span>flakeSpec</span> <span>=</span> <span>max</span><span>(</span><span>flakeSpec</span><span>,</span> <span>0</span><span>.</span><span>15</span><span>);</span> <span>// always visible</span>

        <span>// Environment tint (never too dark, controlled by uniform)</span>
        <span>float</span> <span>flakeRough</span> <span>=</span> <span>clamp</span><span>(</span><span>uRoughness</span> <span>*</span> <span>0</span><span>.</span><span>4</span><span>,</span> <span>0</span><span>.</span><span>0</span><span>,</span> <span>1</span><span>.</span><span>0</span><span>);</span>
        <span>flakeEnv</span> <span>=</span> <span>sampleEnvRough</span><span>(</span><span>PR</span><span>,</span> <span>flakeRough</span><span>)</span> <span>*</span> <span>mix</span><span>(</span><span>0</span><span>.</span><span>9</span><span>,</span> <span>1</span><span>.</span><span>2</span><span>,</span> <span>brightness</span><span>);</span>
        <span>flakeEnv</span> <span>=</span> <span>max</span><span>(</span><span>flakeEnv</span><span>,</span> <span>vec3</span><span>(</span><span>uFlakeBrightness</span><span>));</span>

        <span>vec3</span> <span>flakeIri</span> <span>=</span> <span>iridescenceColor</span><span>(</span><span>dot</span><span>(</span><span>perturbedNormal</span><span>,</span> <span>V</span><span>));</span>
        <span>flakeEnv</span> <span>*=</span> <span>mix</span><span>(</span><span>vec3</span><span>(</span><span>1</span><span>.</span><span>0</span><span>),</span> <span>flakeIri</span><span>,</span> <span>0</span><span>.</span><span>9</span><span>);</span>

        <span>// Final intensity</span>
        <span>flakeIntensity</span> <span>=</span> <span>flakeMask</span> <span>*</span> <span>flakeBoost</span> <span>*</span> <span>flakeSpec</span> <span>*</span> <span>phaseMod</span> <span>*</span> <span>18</span><span>.</span><span>0</span><span>;</span>
        <span>flakeIntensity</span> <span>=</span> <span>clamp</span><span>(</span><span>flakeIntensity</span><span>,</span> <span>0</span><span>.</span><span>0</span><span>,</span> <span>1</span><span>.</span><span>0</span><span>);</span>
    <span>}</span>

    <span>// Final roughness modulation</span>
    <span>float</span> <span>finalRough</span> <span>=</span> <span>clamp</span><span>(</span><span>mix</span><span>(</span><span>uRoughness</span><span>,</span> <span>1</span><span>.</span><span>0</span><span>,</span> <span>flakeIntensity</span><span>),</span> <span>0</span><span>.</span><span>0</span><span>,</span> <span>1</span><span>.</span><span>0</span><span>);</span>

    <span>// Environment reflection</span>
    <span>vec3</span> <span>env</span> <span>=</span> <span>sampleEnvRough</span><span>(</span><span>R</span><span>,</span> <span>finalRough</span><span>)</span> <span>*</span> <span>uEnvIntensity</span><span>;</span>

    <span>// Blend in flake environment contribution</span>
    <span>env</span> <span>=</span> <span>mix</span><span>(</span><span>env</span><span>,</span> <span>flakeEnv</span><span>,</span> <span>clamp</span><span>(</span><span>flakeIntensity</span><span>,</span> <span>0</span><span>.</span><span>0</span><span>,</span> <span>1</span><span>.</span><span>0</span><span>));</span>

    <span>// Fresnel term</span>
    <span>float</span> <span>cosTheta</span> <span>=</span> <span>clamp</span><span>(</span><span>dot</span><span>(</span><span>N</span><span>,</span> <span>V</span><span>),</span> <span>0</span><span>.</span><span>0</span><span>,</span> <span>1</span><span>.</span><span>0</span><span>);</span>
    <span>float</span> <span>F0</span> <span>=</span> <span>mix</span><span>(</span><span>0</span><span>.</span><span>04</span><span>,</span> <span>1</span><span>.</span><span>0</span><span>,</span> <span>uMetalness</span><span>);</span>
    <span>float</span> <span>fres</span> <span>=</span> <span>F0</span> <span>+</span> <span>(</span><span>1</span><span>.</span><span>0</span> <span>-</span> <span>F0</span><span>)</span> <span>*</span> <span>pow</span><span>(</span><span>1</span><span>.</span><span>0</span> <span>-</span> <span>cosTheta</span><span>,</span> <span>5</span><span>.</span><span>0</span><span>);</span>

    <span>// Iridescence</span>
    <span>float</span> <span>metalicMask</span> <span>=</span> <span>mix</span><span>(</span><span>uMetalmask</span><span>,</span> <span>1</span><span>.</span><span>0</span><span>,</span> <span>brightness</span><span>);</span>
    <span>vec3</span> <span>iriCol</span> <span>=</span> <span>iridescenceColor</span><span>(</span><span>cosTheta</span><span>)</span> <span>*</span> <span>metalicMask</span><span>;</span>

    <span>// Final color</span>
    <span>vec3</span> <span>diffuse</span> <span>=</span> <span>base</span><span>.</span><span>rgb</span> <span>*</span> <span>(</span><span>1</span><span>.</span><span>0</span> <span>-</span> <span>uMetalness</span><span>);</span>
    <span>vec3</span> <span>spec</span> <span>=</span> <span>env</span> <span>*</span> <span>fres</span> <span>*</span> <span>iriCol</span> <span>*</span> <span>(</span><span>1</span><span>.</span><span>0</span> <span>-</span> <span>finalRough</span> <span>*</span> <span>0</span><span>.</span><span>85</span><span>);</span>
    <span>vec3</span> <span>color</span> <span>=</span> <span>diffuse</span> <span>+</span> <span>spec</span><span>;</span>

    <span>gl_FragColor</span> <span>=</span> <span>vec4</span><span>(</span><span>color</span><span>,</span> <span>base</span><span>.</span><span>a</span><span>);</span>
<span>}</span>
</code></pre></div><h2 id="licensing">Licensing</h2><p>The code in this page is licensed under Creative Commons Attribution-NonCommercial 4.0 International (<a href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>). Feel free to share and adapt the code for non-commercial purposes with proper attribution. If you wish to use the code commercially, please contact me for a separate license agreement.</p></div></div>]]></description>
        </item>
    </channel>
</rss>