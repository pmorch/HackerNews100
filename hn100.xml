<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 21 May 2025 17:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Discord Unveiled: A Comprehensive Dataset of Public Communication (2015-2024) (119 pts)]]></title>
            <link>https://arxiv.org/abs/2502.00627</link>
            <guid>44052041</guid>
            <pubDate>Wed, 21 May 2025 14:45:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2502.00627">https://arxiv.org/abs/2502.00627</a>, See on <a href="https://news.ycombinator.com/item?id=44052041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Aquino,+Y" rel="nofollow">Yan Aquino</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bento,+P" rel="nofollow">Pedro Bento</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Buzelin,+A" rel="nofollow">Arthur Buzelin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dayrell,+L" rel="nofollow">Lucas Dayrell</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Malaquias,+S" rel="nofollow">Samira Malaquias</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Santana,+C" rel="nofollow">Caio Santana</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Estanislau,+V" rel="nofollow">Victoria Estanislau</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dutenhefner,+P" rel="nofollow">Pedro Dutenhefner</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Evangelista,+G+H+G" rel="nofollow">Guilherme H. G. Evangelista</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Porf%C3%ADrio,+L+G" rel="nofollow">Luisa G. Porfírio</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Grossi,+C+S" rel="nofollow">Caio Souza Grossi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rigueira,+P+B" rel="nofollow">Pedro B. Rigueira</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Almeida,+V" rel="nofollow">Virgilio Almeida</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pappa,+G+L" rel="nofollow">Gisele L. Pappa</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Meira,+W" rel="nofollow">Wagner Meira Jr</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2502.00627">View PDF</a>
    <a href="https://arxiv.org/html/2502.00627v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Discord has evolved from a gaming-focused communication tool into a versatile platform supporting diverse online communities. Despite its large user base and active public servers, academic research on Discord remains limited due to data accessibility challenges. This paper introduces Discord Unveiled: A Comprehensive Dataset of Public Communication (2015-2024), the most extensive Discord public server's data to date. The dataset comprises over 2.05 billion messages from 4.74 million users across 3,167 public servers, representing approximately 10% of servers listed in Discord's Discovery feature. Spanning from Discord's launch in 2015 to the end of 2024, it offers a robust temporal and thematic framework for analyzing decentralized moderation, community governance, information dissemination, and social dynamics. Data was collected through Discord's public API, adhering to ethical guidelines and privacy standards via anonymization techniques. Organized into structured JSON files, the dataset facilitates seamless integration with computational social science methodologies. Preliminary analyses reveal significant trends in user engagement, bot utilization, and linguistic diversity, with English predominating alongside substantial representations of Spanish, French, and Portuguese. Additionally, prevalent community themes such as social, art, music, and memes highlight Discord's expansion beyond its gaming origins.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Yan Aquino Amorim [<a href="https://arxiv.org/show-email/debf74d4/2502.00627" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Sun, 2 Feb 2025 02:17:14 UTC (1,433 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Animated Factorization (127 pts)]]></title>
            <link>http://www.datapointed.net/visualizations/math/factorization/animated-diagrams/</link>
            <guid>44051958</guid>
            <pubDate>Wed, 21 May 2025 14:39:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://www.datapointed.net/visualizations/math/factorization/animated-diagrams/">http://www.datapointed.net/visualizations/math/factorization/animated-diagrams/</a>, See on <a href="https://news.ycombinator.com/item?id=44051958">Hacker News</a></p>
<div id="readability-page-1" class="page">
<div id="enchilada">



<div id="credits">
<p><a href="http://www.datapointed.net/2012/10/animated-factorization-diagrams/" onclick="window.open(this.href);return false;">About</a>
</p></div>
<canvas width="300" height="300" id="canvas"></canvas>
</div>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Roto: A Compiled Scripting Language for Rust (130 pts)]]></title>
            <link>https://blog.nlnetlabs.nl/introducing-roto-a-compiled-scripting-language-for-rust/</link>
            <guid>44050222</guid>
            <pubDate>Wed, 21 May 2025 11:10:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.nlnetlabs.nl/introducing-roto-a-compiled-scripting-language-for-rust/">https://blog.nlnetlabs.nl/introducing-roto-a-compiled-scripting-language-for-rust/</a>, See on <a href="https://news.ycombinator.com/item?id=44050222">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="site-main">
<article>

    <header>

        

        


        <section>

            <ul>
                <li>
                    <a href="https://blog.nlnetlabs.nl/author/nlnetlabs/" aria-label="Read more of Team NLnet Labs">
                        <img src="https://blog.nlnetlabs.nl/content/images/size/w100/2022/07/nlnetlabs-logo-and-text-1-1.png" alt="Team NLnet Labs">
                    </a>
                </li>
            </ul>

            

        </section>

            <figure>
                <img srcset="https://images.unsplash.com/photo-1463567517034-628c51048aa2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDMzfHxzaGlwJTIwaGVsbXxlbnwwfHx8fDE3NDU4NDcyNjl8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=300 300w,
                            https://images.unsplash.com/photo-1463567517034-628c51048aa2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDMzfHxzaGlwJTIwaGVsbXxlbnwwfHx8fDE3NDU4NDcyNjl8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=600 600w,
                            https://images.unsplash.com/photo-1463567517034-628c51048aa2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDMzfHxzaGlwJTIwaGVsbXxlbnwwfHx8fDE3NDU4NDcyNjl8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1000 1000w,
                            https://images.unsplash.com/photo-1463567517034-628c51048aa2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDMzfHxzaGlwJTIwaGVsbXxlbnwwfHx8fDE3NDU4NDcyNjl8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000 2000w" sizes="(min-width: 1400px) 1400px, 92vw" src="https://images.unsplash.com/photo-1463567517034-628c51048aa2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDMzfHxzaGlwJTIwaGVsbXxlbnwwfHx8fDE3NDU4NDcyNjl8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Introducing Roto: A Compiled Scripting Language for Rust">
                    <figcaption><span>Photo by </span><a href="https://unsplash.com/@jbcreate_?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit"><span>Joseph Barrientos</span></a><span> / </span><a href="https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit"><span>Unsplash</span></a></figcaption>
            </figure>

    </header>

    <section>
        <p><em>By Terts Diepraam</em></p>
<p>We are working on an embedded scripting language for Rust. This language, called <a href="https://github.com/NLnetLabs/roto?ref=blog.nlnetlabs.nl">Roto</a>, aims to be a simple yet fast and reliable scripting language for Rust applications.</p>
<p>The need for Roto comes from <a href="https://nlnetlabs.nl/projects/routing/rotonda/?ref=blog.nlnetlabs.nl">Rotonda</a>, our BGP engine written in Rust. Mature BGP applications usually feature some way to filter incoming route announcements. The complexity of these filters often exceed the capabilities of configuration languages. With Rotonda, we want to allow our users to write more complex filters with ease. So we decided to give them the power of a full scripting language.</p>
<p>We have some hard requirements for this language. First, we need these filters to be fast. Second, Rotonda is critical infrastructure and so runtime crashes are unacceptable. This rules out dynamically typed languages, of which there are plenty in the Rust community.<sup><a href="#fn1" id="fnref1">[1]</a></sup> We want a statically typed language which can give us more type safety and speed. Finally, we want a language that is easy to pick up; it should feel like a statically typed version of scripting languages you're used to.</p>
<p>Roto fills this niche for us. In short, it's a statically typed, JIT compiled, hot-reloadable, embedded scripting language. To get good performance, Roto scripts are compiled to machine code at runtime with the <a href="https://cranelift.dev/?ref=blog.nlnetlabs.nl">cranelift</a> compiler backend.</p>
<p>Below is a small sample of a Roto script. In this script, we define a <code>filtermap</code>, which results in either <code>accept</code> or <code>reject</code>. In this case, we <code>accept</code> when the IP address is within the given range.</p>
<pre><code>filtermap within_range(range: AddrRange, ip: IpAddr) {
    if range.contains(ip) {
        accept ip
    } else {
        reject
    }
}
</code></pre>
<p>Instead of a <code>filtermap</code>, we could instead write a more conventional <code>function</code>, which can simply <code>return</code> a value. The <code>filtermap</code> is a construct that Roto supports to make writing filters easier.</p>
<p>The Roto code there might look quite simple, but there's a twist: <code>AddrRange</code> is not a built-in type. Instead, it is added to Roto by the host application (e.g. Rotonda), making it available for use in the script.<sup><a href="#fn2" id="fnref2">[2]</a></sup> Similarly, the <code>contains</code> method on <code>AddrRange</code> is provided by the host application as well. The full code necessary to run the script above is listed below. This example is also available <a href="https://github.com/NLnetLabs/roto/blob/a4edc7fcea79a2498798f69da4cdb9beb6ecd4d1/examples/addr_range.rs?ref=blog.nlnetlabs.nl">on our GitHub repository</a>.</p>
<pre><code>use std::net::IpAddr;
use std::path::Path;
use roto::{roto_method, FileTree, Runtime, Val, Verdict};

#[derive(Clone)]
struct AddrRange {
    min: IpAddr,
    max: IpAddr,
}

fn run_script(path: &amp;Path) {
    // Create a runtime
    let mut runtime = Runtime::new();
    
    // Register the AddrRange type into the runtime with a docstring
    runtime
        .register_clone_type::&lt;AddrRange&gt;("A range of IP addresses")
        .unwrap();
    
    // Register the contains method on AddrRange
    #[roto_method(runtime, AddrRange)]
    fn contains(range: &amp;AddrRange, addr: &amp;IpAddr) -&gt; bool {
        range.min &lt;= addr &amp;&amp; addr &lt;= range.max
    }
    
    // Compile the program
    let program =
        FileTree::read(path).compile(runtime).unwrap();

    // Extract the Roto filtermap, which is accessed as a function
    let function = program
        .get_function::&lt;(), (Val&lt;AddrRange&gt;, IpAddr), Verdict&lt;IpAddr, ()&gt;&gt;(
          "within_range"
        )
        .unwrap();
    
    // Run the filtermap
    let range = AddrRange {
        min: "10.10.10.10".parse().unwrap(),
        max: "10.10.10.12".parse().unwrap(),
    };

    let in_range = "10.10.10.11".parse().unwrap();
    println!("{:?}", function.call(&amp;mut (), range, in_range)));

    let out_of_range = "10.10.11.10".parse().unwrap();
    println!("{:?}", function.call(&amp;mut (), range, out_of_range));
}
</code></pre>
<p>Note that nothing in the script is run automatically when the script is loaded, as happens in many other scripting language. The host application decides which functions and filtermaps it extracts from the script and when to run them.</p>
<p>Roto is very tightly integrated with Rust. Many Rust types<sup><a href="#fn3" id="fnref3">[3]</a></sup>, methods and functions can be registered directly for use in Roto. These types can be passed to Roto at negligible cost; there is no serialization between Roto and Rust. For Rotonda, this means that Roto can operate on raw BGP messages without costly conversion procedures.</p>
<p>The registration mechanism also ensures that Roto is not limited to Rotonda and could easily be used outside that context. It is designed as a general scripting or plug-in language.</p>
<p>We have many planned features on the roadmap for Roto and will continue to improve this language. This also means that the language should not be considered stable, though we'd love to hear feedback if you experiment with it. If you're interested, check out the <a href="https://rotonda.docs.nlnetlabs.nl/en/stable/roto/00_introduction.html?ref=blog.nlnetlabs.nl">documentation</a>, <a href="https://github.com/NlnetLabs/roto?ref=blog.nlnetlabs.nl">repository</a> and <a href="https://github.com/NLnetLabs/roto/tree/main/examples?ref=blog.nlnetlabs.nl">examples</a>.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>E.g. <a href="https://rhai.rs/?ref=blog.nlnetlabs.nl">Rhai</a>, <a href="https://rune-rs.github.io/?ref=blog.nlnetlabs.nl">Rune</a>, <a href="https://github.com/mlua-rs/mlua?ref=blog.nlnetlabs.nl">Mlua</a>, <a href="https://deno.com/?ref=blog.nlnetlabs.nl">Deno</a>, <a href="https://pyo3.rs/?ref=blog.nlnetlabs.nl">PyO3</a>, <a href="https://github.com/PistonDevelopers/dyon?ref=blog.nlnetlabs.nl">Dyon</a> &amp; <a href="https://koto.dev/?ref=blog.nlnetlabs.nl">Koto</a>. <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>Note that the registering of types is not hindered by Rust's <a href="https://doc.rust-lang.org/reference/items/implementations.html?ref=blog.nlnetlabs.nl#orphan-rules">orphan rule</a>, because it doesn't require any specific traits apart from <code>Clone</code>. This makes it possible to expose types from external libraries to Roto. <a href="#fnref2">↩︎</a></p>
</li>
<li id="fn3"><p>Specifically, types implementing <code>Clone</code> or <code>Copy</code>. Types that don't implement these traits can be wrapped in an <code>Rc</code> or <code>Arc</code> to be passed to Roto. <a href="#fnref3">↩︎</a></p>
</li>
</ol>
</section>

    </section>


</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My new hobby: watching AI slowly drive Microsoft employees insane (841 pts)]]></title>
            <link>https://old.reddit.com/r/ExperiencedDevs/comments/1krttqo/my_new_hobby_watching_ai_slowly_drive_microsoft/</link>
            <guid>44050152</guid>
            <pubDate>Wed, 21 May 2025 10:57:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/ExperiencedDevs/comments/1krttqo/my_new_hobby_watching_ai_slowly_drive_microsoft/">https://old.reddit.com/r/ExperiencedDevs/comments/1krttqo/my_new_hobby_watching_ai_slowly_drive_microsoft/</a>, See on <a href="https://news.ycombinator.com/item?id=44050152">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Welcome to the <a href="https://old.reddit.com/r/ExperiencedDevs">/r/ExperiencedDevs</a> subreddit! We hope you will find this as a valuable resource in your journeys down the fruitful CS/IT career paths. This community leans towards being a specialized subreddit facilitating discussion amongst individuals who have gained some ground in the IT world. </p>

<p>For an idea of what is encouraged in this subreddit and what is not (please report anything that does not follow the rules):</p>

<h2><a href="https://www.reddit.com/r/ExperiencedDevs/about/rules/">Rules</a></h2>

<p><strong>1. Do not participate unless experienced (3+ years)</strong></p>

<p>If you have less than 3 years of experience as a developer, do not make a post, nor participate in comments threads except for the weekly “Ask Experienced Devs” auto-thread. No exceptions.</p>

<p><strong>2. No Disrespectful Language or Conduct</strong></p>

<p>Don’t be a jerk. Act maturely. No racism, unnecessarily foul language, ad hominem charges, sexism - none of these are tolerated here. This includes posts that could be interpreted as trolling, such as complaining about DEI (Diversity) initiatives or people of a specific sex or background at your company.</p>

<p>Do not submit posts or comments that break, or promote breaking the Reddit Terms and Conditions or Content Policy or any other Reddit policy.</p>

<p>Violations = Warning, 7-Day Ban, Permanent Ban.</p>

<p><strong>3. No General Career Advice</strong></p>

<p>This sub is for discussing issues specific to experienced developers. </p>

<p>Any career advice thread must contain questions and/or discussions that notably benefit from the participation of experienced developers. Career advice threads may be removed at the moderators discretion based on response to the thread."</p>

<p>General rule of thumb: If the advice you are giving (or seeking) could apply to a “Senior Chemical Engineer”, it’s not appropriate for this sub.</p>

<p><strong>4. No "Which Offer Should I Take" Posts</strong></p>

<p>Asking if you should ask for a raise, switch companies (“should I work for company A or company B”), “should I take offer A or offer B”, or related questions, is not appropriate for this sub. </p>

<p>This includes almost any discussion about a “hot market”, comparing compensation between companies, etc.</p>

<p><strong>5. No “What Should I Learn” Questions</strong></p>

<p>No questions like “Should I learn C#” or “Should I switch jobs into a language I don’t know?”</p>

<p>Discussion about industry direction or upcoming technologies is fine, just frame your question as part of a larger discussion (“What have you had more success with, RDBMS or NoSQL?”) and you’ll be fine.</p>

<p>tl;dr: Don’t make it about you/yourself.</p>

<p><strong>6. No “I hate X types of interviews" Posts</strong></p>

<p>This has been re-hashed over and over again. There is no interesting/new content coming out.</p>

<p>It might be OK to talk about the merits of an interview process, or compare what has been successful at your company, but if it ends up just turning into complaints your post might still be removed.</p>

<h2>Related Subs</h2>

<p><a href="https://www.reddit.com/r/cscareerquestions">CS Career Questions</a></p>

<p><a href="https://www.reddit.com/r/cscareerquestionsEU">CS Career Questions: Europe</a></p>

<p><a href="https://www.reddit.com/r/CS_Questions">CS Interview Questions</a></p>

<p><a href="http://www.reddit.com/r/learnprogramming">Learn Programming</a></p>

<p><a href="http://www.reddit.com/r/programming">General Programming Discussion</a></p>

<p><a href="http://www.reddit.com/r/coding">Coding</a></p>

<p><a href="http://www.reddit.com/r/compsci">CS Theory</a></p>

<p><a href="http://www.reddit.com/r/CSEducation">CS Education</a></p>

<p><a href="http://www.reddit.com/r/ITCareerQuestions">IT Career Questions</a></p>

<p><a href="http://www.reddit.com/r/telecommuting">Telecommuting</a></p>

<p><a href="http://www.reddit.com/r/jobs">General Job Discussion</a></p>

<p><a href="https://www.reddit.com/r/digitalnomad">Digital Nomads</a></p>

<p><a href="https://www.reddit.com/r/AskNetsec">Ask Network Security</a></p>

<p><a href="https://www.reddit.com/r/netsecstudents">NetSec Students</a></p>

<p><a href="https://www.reddit.com/r/careerguidance/">Career Guidance</a></p>

<hr>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The value isn't in the code (2022) (134 pts)]]></title>
            <link>https://jonayre.uk/blog/2022/10/30/the-real-value-isnt-in-the-code/</link>
            <guid>44046955</guid>
            <pubDate>Tue, 20 May 2025 23:33:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jonayre.uk/blog/2022/10/30/the-real-value-isnt-in-the-code/">https://jonayre.uk/blog/2022/10/30/the-real-value-isnt-in-the-code/</a>, See on <a href="https://news.ycombinator.com/item?id=44046955">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
                    
<h2><strong>Time is money, money is value</strong></h2>



<p>Okay, I’ll admit that calling your software worthless was a shameless clickbait tactic. However, I will assert that it’s not as valuable or indispensable as you might think it is. You need two things to solve a problem using software, skill and time.</p>



<p>Skill. Of course, you can create code using unskilled practitioners. The code might even work, but that’s not the same thing as solving the problem. Bad code is an issue in itself, so even if you solve your original problem, you now have a new, possibly bigger one to deal with. Skill is essential if you’re going to succeed and talent doesn’t come cheap.</p>



<p>Time. Coding can happen quickly, but solving problems is tricky. It takes time, and time is money. A software development team is not cheap to run, so any decent piece of software is going to incur significant cost. If you’re going to put investment into your solution, you should expect it to be worth something when you’re done.</p>



<p>So, why am I suggesting that the code is worthless? If it solves the problem, it’s delivering value. Worthless seems a bit harsh. It’s true that I’m exaggerating, but not by much. To understand, let’s take another look at that solution, and the time that went into it. It’s easy to focus on the software and ignore what surrounds it.</p>



<h2><strong>How to bake a cake</strong></h2>



<p>Firstly, there’s the team. You have to find the right, skilled, people, and bring them together. Then, they have to establish clear roles and responsibilities and learn to work together as a cohesive team. Finally that team has to establish relationships with the stakeholders and users, and familiarise themselves with the problem space.</p>



<p>This takes time.</p>



<p>Secondly, there’s the business logic. Even the simplest of solutions has to perform some sort of business processing. Someone has to work out what that logic needs to be and codify it.</p>



<p>This takes time.</p>



<p>Thirdly, there’s the design. If the code is user facing, it delivers an experience to the user. That experience is (hopefully) developed over time through careful design, feedback and iteration.</p>



<p>This takes time.</p>



<p>And finally, there’s the code. That also takes time, but that time is small in comparison to all the others. Sometimes it feels as if all the effort is going into the code and the other parts are incidental. However, the reality is that very little of the productive time ends up as code in the live solution.&nbsp;</p>



<p>Some of the code will be replaced by alternatives as part of the iterative feedback approach. Some of the code will remain, but will no longer be used as a result of changes to the design. Some of the code will have been written in anticipation of a situation that never arises.</p>



<p>The developer’s answer to all of this is “refactoring”. For those of you who don’t code, refactoring is the process of reviewing existing code and making a range of improvements whilst retaining the core functionality.</p>



<p>This takes time.</p>



<h2><strong>Knowledge is power</strong></h2>



<p>When you add all the time together, you get the cost of your software solution, and you could argue that this represents the value of the code. You might even argue that the value of the code exceeds the cost.</p>



<p>This is where I disagree. My opinion is that in doing this you are conflating the codebase and the solution. All the value is stored up in the team, the logic and the design, and very little of it is in the code itself. Originally this view was just a hypothesis without any form of proof, but over time I’ve had the opportunity to run experiments to test this hypothesis.</p>



<p>One particular experiment involved a web portal I was involved in developing as part of an all remote team back just before the turn of the millennium. Smartphones didn’t exist, and the internet was still a fairly new concept to many people. The portal was for a growing internet service provider.</p>



<p>The work took a team of just 7 people 6 months to develop and the result was a cross platform solution that could deliver content, email, calendering and messaging. It was ahead of its time, and also supported voice interaction via mobile phone and delivery of content via WAP. What’s WAP? Google it – the days before smartphones were far from sophisticated when it comes to mobile access to the internet.</p>



<p>The result was good enough to be bought up by a large organisation as their portal solution. That company had already spent longer than 6 months with a significantly larger team trying to create the same thing without success. We were rightly proud of what we’d achieved in such a short time and with such a small team.</p>



<p>So, my experiment related to something that was already considered very efficient in code development terms.</p>



<h2><strong>We can rebuild it!</strong></h2>



<p>What I did was spend a couple of weeks at home recreating the solution from scratch. The code now belonged to a third party so I wanted a completely new version I could call my own. I used nothing from the original apart from my own knowledge of the problem and my experience solving it. The code repository was no longer accessible, so I couldn’t copy anything from it, even if I’d wanted to. To do so would have also been unethical as it didn’t belong to me. So, every line of code was genuinely built from scratch.</p>



<p>Within two weeks I had a fully working solution that did everything the original did (and a bit more). I’d achieved this in no time at all, and I’d generated a fraction of the code compared to the original solution.</p>



<p>How did I achieve this? Was I some kind of coding genius? Obviously not. I was a very competent coder, very much at the top of my game, but I was just one coder. No matter how good you are, there’s a limit to how fast you can type.</p>



<p>No. I achieved this because the code contained very little of the real value. That was all stored in my head. The design was in my head, and with hindsight I could see all its flaws. I was therefore able to create a much more efficient and effective design based on that learning. All the mistakes had been made, so I was able to get this version of the code right the first time. Most of my tests ran successfully, and debugging time was almost non-existent.</p>



<p>I now knew what wasn’t needed. All those things we’d built in to cater for anticipated needs could now be whittled down to the few that really proved useful. I understood every piece of technology, every protocol and every library, so there was no learning curve.</p>



<h2><strong>Mostly worthless</strong></h2>



<p>From this, I can conclude that of the 6 months of time spent by 7 people creating this solution, hardly any of it related to the code. It could be completely discarded and rebuilt by one person in under two weeks. What’s more, it could be radically improved at the same time.</p>



<p>So, this is why I assert that your code is worthless when compared to the overall effort invested in the creation of your solution. And I’d go further than that. I’d suggest that, contrary to what intuition might tell you, refactoring might be better achieved by throwing the code away and starting again.</p>



<p>It’s a scary thought. You might even consider it ridiculously farfetched. I wouldn’t expect you to agree with me based on a blog post. However, what I would recommend is that you give it some serious thought, and maybe conduct a similar experiment of your own. If you do, let me know how you get on. I’d be genuinely interested to find out.</p>



<p>Maybe I’m special? Maybe I’m the mythical ten times developer? Personally, I doubt it very much. After all, I was one of the people who took six months to build it the first time round.</p>



<p>Oh, and remember this next time you have to fix someone else’s code. It’s easy to feel superior and knowledgeable. It’s tempting to laugh at the obvious mistakes made by those who came before, but you have the advantage of hindsight. Maybe consider instead that you might be standing on the shoulders of those who carved out the first path for you to follow.</p>
                    
                    
	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>                </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[“ZLinq”, a Zero-Allocation LINQ Library for .NET (240 pts)]]></title>
            <link>https://neuecc.medium.com/zlinq-a-zero-allocation-linq-library-for-net-1bb0a3e5c749</link>
            <guid>44046578</guid>
            <pubDate>Tue, 20 May 2025 22:29:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://neuecc.medium.com/zlinq-a-zero-allocation-linq-library-for-net-1bb0a3e5c749">https://neuecc.medium.com/zlinq-a-zero-allocation-linq-library-for-net-1bb0a3e5c749</a>, See on <a href="https://news.ycombinator.com/item?id=44046578">Hacker News</a></p>
Couldn't get https://neuecc.medium.com/zlinq-a-zero-allocation-linq-library-for-net-1bb0a3e5c749: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Instagram Addiction (115 pts)]]></title>
            <link>https://blog.greg.technology/2025/05/19/on-instagram-addiction.html</link>
            <guid>44046559</guid>
            <pubDate>Tue, 20 May 2025 22:25:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.greg.technology/2025/05/19/on-instagram-addiction.html">https://blog.greg.technology/2025/05/19/on-instagram-addiction.html</a>, See on <a href="https://news.ycombinator.com/item?id=44046559">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>i was addicted to instagram for about a month. it upended my life enough that i realized that i was living with something new-an undesirable habit. and once i realized this, it seemed like it would be wise to try to address it or at least think about it. easier said than done.</p>

<p>i left instagram for about three to four months cold turkey-i left a video saying i was leaving, then uninstalled the app.</p>

<p>there’s a few things to talk about here. first, as <a href="https://bsky.app/profile/saltzshaker.bsky.social">emily</a> noted to me, this is why kids today can be so bitter about their relationships with their own phones. kids (which is anyone younger than 43) watch movies from the early 2000s with a kind of wistfulness because, wow, people lived normal, natural lives without phones. maybe it’s obvious to you, but when you’re 43, you completely grew up without phones. phones were an absolutely new thing that happened in the middle of your 43-year old’s life.</p>

<p>i grew up quite familiar with the state of being absolutely bored to the gills. boredom was just a sensation that you had, like pain or hunger. you’d stare in some direction and you’d be bored. but that doesn’t seem to be a thing i feel anymore. now it’s mostly about dealing with self-control and somehow thinking about it all the time. it is not relaxing. the phone is in your pocket. it’s on your body.</p>

<hr>

<p>i hope i’m not using “addiction” in a way that trivializes it. some people (have to) deal with their own intense relationships with (substances, habits), and i would never want you to think that what i dealt with is the same as that.</p>

<p>obviously, so many forms of addiction have had an enormous impact on so many lives-upending them, transforming them, taking people from one direction (their wishes, dreams, circumstances) to a very opposite one.</p>

<p>i hope it’s clear i am more than sympathetic.</p>

<hr>

<hr>

<hr>

<hr>

<hr>

<hr>

<p>i developed a habit that would break my sleep in a specific way: i would get up at four a.m. and watch reels for about two to three hours.</p>

<p>what’s impressive about that is that two hours for a movie is nothing—it’s one movie. if i had been getting up in the middle of the night every night to watch superman, you’d think, “wow, this guy’s into superman.” i’d probably learn a lot of superman trivia that i could drop into regular conversation. maybe i’d wonder why i didn’t just do that after work as you could easily combine a nine-to-five job with an intense level of superman fandom.</p>

<p>but an instagram reel or any sort of short-form video…—really, “short-form” isn’t strong nor short enough of a word. six seconds isn’t short. it’s instant. it’s zero seconds. there is no difference between zero seconds and six seconds in any timeline unless you build atomic watches or run <a href="https://time.gov/">time.gov</a>. if any clock in your house was off by six seconds, you wouldn’t spend any amount of time thinking about that. and yet this new art form can tell a story with multiple edits within a single six-second timeframe.</p>

<p>it’s an incredible achievement. who among us even knows if akira kurosawa could make a good six-second reel? (please don’t use a generative ai to generate 6 second akira kurosawa-styled tiktok videos, even if nothing could ever prevent you from doing this.)</p>

<hr>

<p>ok but the six-second video format <em>is</em> extremely impressive from a narrative, storytelling, and comedy point of view. the art of editing has probably improved globally or developed some sort of new appendix, which i hope doesn’t metastasize into all moviemaking. but cutting your teeth on making a good six-second video must somehow give you incredible editing abilities. i totally get it as a training ground for filmmakers or anyone that’s into storytelling.</p>

<p>that’s the good part. the good part is the fun and addictive part—there are truly incredible videos out there, even though i am not going to link to them. if i told you the story of how heroin felt, i would probably not tell you where to get it. if you truly needed heroin (who needs heroin?), you’d be pursuing it anyway and would probably find your way to some heroin mart. but i hope you don’t do either heroin or short videos. the problem is, short videos are a lot harder to escape.</p>

<p>it was also so obvious to see that not every six-second video was good or seemed interesting to me. in that span of a few hours, i’d get a few—sometimes one, sometimes five, rarely 10—that were the funniest videos i had ever seen in my life. but that’s just a slot machine: you keep pressing the button like a rat and the button gives you drugs, but randomly, which is the worst/best kind of addiction-creating mechanism. it’s why inbox addiction is also a thing (that i have): never knowing when you’ll get an email, you keep reloading until you inevitably get one.</p>

<hr>

<hr>

<p>recently, i reinstalled instagram to post a video and a music video, then uninstalled it and reinstalled it and uninstalled it at least six times in the last week. unfortunately, the app felt just as good as before. i’m not sure if the algorithm got better—i understand that this is probably not what happened, and i was just missing it. but instagram knows me.</p>

<p>most of the videos it was offering were super bizarre, out-of-this-world noise performances where artists were setting instruments on fire, which appealed to me enormously. there were grandmas doing cooking and hip hop in mandarin, videos of religious sects with incredible sound and reverb—amazing soundscapes i had never seen before. all of this content came streaming back and it unfortunately felt really great.</p>

<p>during the period when i didn’t have the app, every alternative felt not as good. i tried to fill the place of the addictive cigarette with carrots, but carrots don’t (yet) contain nicotine, and you can taste the difference. i started reading the news again a lot—in a very “reload the news five times a day” kind of way.</p>

<p>the news didn’t feel as good, and going back to the news now still feels like a lesser version of the previous short video hole filling activity. news doesn’t know you as well as the algorithm. also it’s hard knowing that instagram is always there.</p>

<p>of course, the app also offers connection. there are multiple communities i joined or keep informed of or partake in or just watch, sometimes completely from a distance. but i can also strike up conversations, and i’ve been able to meet people from there. this isn’t saying anything new—online friendships have always been a thing, and those that cross from online to offline have been in my life for a while.</p>

<p>so instagram, like any social media app, combines addiction with everything else. addictive videos on one hand that fill holes, but those videos are also funny and original. they feel like a waste of time, but they’re also entertaining. you connect with people, but the messaging part is tied to the video part, and you can’t do one without the other. truly, any accidental ui swipe on instagram brings you back to reels—it’s the thing in the app with the most gravity.</p>

<hr>

<p>emily pointed out two things regarding this. first, using instagram on desktop is enough of a deterrent for her. it lacks the slickness of mobile, so it has fewer addictive attributes. she would want just that, but minus the reels, which isn’t possible in the desktop version unless you install a browser extension and muck around.</p>

<p>second, she’s convinced that in some future some laws might pass (in europe?) to force apps like instagram to give you an option to turn off reels. this would probably take the form of family controls that make sense for parents and would integrate with other tools used to limit screen time. but for adults, of course, you’re on your own (adulting is hard). i’m not sure what the options are or could be. apps that try to limit screen time feel “ridiculous” knowing i could uninstall them, and i wouldn’t want an app i couldn’t uninstall. you/i could set a family screen time restriction, give the password to a friend, but then they’d be responsible for managing your/my emotions anytime you/i’d have a flare-up, which is so completely worse.</p>

<hr>

<p>so here’s where i am today: i uninstalled the app and deleted it from my phone, as i’ve done dozens of times. then i went into my phone’s preferences and disabled the ability to install new apps and make in-app purchases.</p>

<p>so i’m just two very easy toggles away from reinstalling instagram and going back in. i’m also happy to share that i’ve actually created a calendar of things i do want to post, i.e. a social media content calendar! ((to be more intentional about when i use the app)) and i am looking forward to sharing that content because i think it might be funny, and someone might like it.</p>

<p>but! because of this, i already know that i’ll be turning those toggles off. and it will feel good again, and i’ll know that it’s not the right kind of good. and it will seem like that’s all there is to it, and that will also be not true.</p>

<p><img src="https://blog.greg.technology/assets/instagram/settings.jpg"></p>

  </div>
</article>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Semantic search engine for ArXiv, biorxiv and medrxiv (132 pts)]]></title>
            <link>https://arxivxplorer.com/</link>
            <guid>44046277</guid>
            <pubDate>Tue, 20 May 2025 21:49:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxivxplorer.com/">https://arxivxplorer.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44046277">Hacker News</a></p>
<div id="readability-page-1" class="page"><div tabindex="-1" id="___gatsby"><div><a aria-current="page" href="https://arxivxplorer.com/"><div data-gatsby-image-wrapper=""><picture><source type="image/webp" data-srcset="/static/e0cd7b5f0243163f253dd5c1aa2d8c9a/e73fe/icon.webp 40w,/static/e0cd7b5f0243163f253dd5c1aa2d8c9a/61ca6/icon.webp 80w" sizes="40px"><img data-gatsby-image-ssr="" layout="fixed" data-main-image="" sizes="40px" decoding="async" loading="lazy" data-src="/static/e0cd7b5f0243163f253dd5c1aa2d8c9a/f31ef/icon.png" data-srcset="/static/e0cd7b5f0243163f253dd5c1aa2d8c9a/f31ef/icon.png 40w,/static/e0cd7b5f0243163f253dd5c1aa2d8c9a/1f8a1/icon.png 80w" alt="arXiv Xplorer Logo" src="https://arxivxplorer.com/static/e0cd7b5f0243163f253dd5c1aa2d8c9a/f31ef/icon.png" srcset="https://arxivxplorer.com/static/e0cd7b5f0243163f253dd5c1aa2d8c9a/f31ef/icon.png 40w,https://arxivxplorer.com/static/e0cd7b5f0243163f253dd5c1aa2d8c9a/1f8a1/icon.png 80w"></picture></div></a><div><a href="https://arxivxplorer.com/guide/"><p>Guide</p></a><a href="https://scroll.arxivxplorer.com/"><p>Scroll</p></a></div></div><div><div><h4>The Semantic Search Engine for ArXiv.</h4><div></div></div></div><div><p>Thank you to arXiv, bioRxiv and medRxiv for their open access interoperability.</p><p>Built by<!-- --> <a href="https://x.com/tomtumiel" target="_blank" rel="noopener noreferrer">ttumiel <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M12.6.75h2.454l-5.36 6.142L16 15.25h-4.937l-3.867-5.07-4.425 5.07H.316l5.733-6.57L0 .75h5.063l3.495 4.633L12.601.75Zm-.86 13.028h1.36L4.323 2.145H2.865z"></path></svg></a> <!-- -->with<!-- --> <a href="https://platform.openai.com/docs/guides/embeddings" target="_blank" rel="noopener noreferrer">OpenAI's Embeddings</a>.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Litestream: Revamped (408 pts)]]></title>
            <link>https://fly.io/blog/litestream-revamped/</link>
            <guid>44045292</guid>
            <pubDate>Tue, 20 May 2025 19:58:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fly.io/blog/litestream-revamped/">https://fly.io/blog/litestream-revamped/</a>, See on <a href="https://news.ycombinator.com/item?id=44045292">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <figure>
                <img src="https://fly.io/blog/litestream-revamped/assets/litestream-revamped.png" alt="">
                <figcaption>
                  <span>Image by</span>
                  
<svg role="img" style="pointer-events: none; width: 17px; height: 17px;" viewBox="0 0 20 20" fill="currentColor" fill-rule="evenodd">
  <g buffered-rendering="static">
    <path fill-rule="evenodd" d="M1 8a2 2 0 012-2h.93a2 2 0 001.664-.89l.812-1.22A2 2 0 018.07 3h3.86a2 2 0 011.664.89l.812 1.22A2 2 0 0016.07 6H17a2 2 0 012 2v7a2 2 0 01-2 2H3a2 2 0 01-2-2V8zm13.5 3a4.5 4.5 0 11-9 0 4.5 4.5 0 019 0zM10 14a3 3 0 100-6 3 3 0 000 6z" clip-rule="evenodd"></path>
  </g>
</svg>

                    <a href="https://annieruygtillustration.com/" target="_blank">
                      Annie Ruygt
                    </a>
                </figcaption>
            </figure>
          <p><a href="https://litestream.io/" title="">Litestream</a> is an open-source tool that makes it possible to run many kinds of full-stack applications on top of SQLite by making them reliably recoverable from object storage. This is a post about the biggest change we’ve made to it since I launched it.</p>
<p>Nearly a decade ago, I got a bug up my ass. I wanted to build full-stack applications quickly. But the conventional n-tier database design required me to do sysadmin work for each app I shipped. Even the simplest applications depended on heavy-weight database servers like Postgres or MySQL.</p>

<p>I wanted to launch apps on SQLite, because SQLite is easy. But SQLite is embedded, not a server, which at the time implied that the data for my application lived (and died) with just one server.</p>

<p>So in 2020, I wrote <a href="https://litestream.io/" title="">Litestream</a> to fix that.</p>

<p>Litestream is a tool that runs alongside a SQLite application. Without changing that running application, it takes over the WAL checkpointing process to continuously stream database updates to an S3-compatible object store. If something happens to the server the app is running on, the whole database can efficiently be restored to a different server. You might lose servers, but you won’t lose your data.</p>

<p>Litestream worked well. So we got ambitious. A few years later, we built <a href="https://github.com/superfly/litefs" title="">LiteFS</a>. LiteFS takes the ideas in Litestream and refines them, so that we can do read replicas and primary failovers with SQLite. LiteFS gives SQLite the modern deployment story of an n-tier database like Postgres, while keeping the database embedded.</p>

<p>We like both LiteFS and Litestream. But Litestream is the more popular project. It’s easier to deploy and easier to reason about.</p>

<p>There are some good ideas in LiteFS. We’d like Litestream users to benefit from them. So we’ve taken our LiteFS learnings and applied them to some new features in Litestream.</p>
<h2 id="point-in-time-restores-but-fast"><a href="#point-in-time-restores-but-fast" aria-label="Anchor"></a><span>Point-in-time restores, but fast</span></h2>
<p><a href="https://fly.io/blog/all-in-on-sqlite-litestream/" title="">Here’s how Litestream was originally designed</a>: you run <code>litestream</code> against a SQLite database, and it opens up a long-lived read transaction. This transaction arrests SQLite WAL checkpointing, the process by which SQLite consolidates the WAL back into the main database file. Litestream builds a “shadow WAL” that records WAL pages, and copies them to S3.</p>

<p>This is simple, which is good. But it can also be slow. When you want to restore a database, you have have to pull down and replay every change since the last snapshot. If you changed a single database page a thousand times, you replay a thousand changes. For databases with frequent writes, this isn’t a good approach.</p>

<p>In LiteFS, we took a different approach. LiteFS is transaction-aware. It doesn’t simply record raw WAL pages, but rather ordered ranges of pages associated with transactions, using a file format we call <a href="https://github.com/superfly/ltx" title="">LTX</a>. Each LTX file represents a sorted changeset of pages for a given period of time.</p>

<p><img alt="a simple linear LTX file with 8 pages between 1 and 21" src="https://fly.io/blog/litestream-revamped/assets/linear-ltx.png"></p>

<p>Because they are sorted, we can easily merge multiple LTX files together and create a new LTX file with only the latest version of each page.</p>

<p><img alt="merging three LTX files into one" src="https://fly.io/blog/litestream-revamped/assets/merged-ltx.png"></p>
<p>This is similar to how an <a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree" title="">LSM tree</a> works.</p>
<p>This process of combining smaller time ranges into larger ones is called <em>compaction</em>. With it, we can replay a SQLite database to a specific point in time, with a minimal  duplicate pages.</p>
<h2 id="casaas-compare-and-swap-as-a-service"><a href="#casaas-compare-and-swap-as-a-service" aria-label="Anchor"></a><span>CASAAS: Compare-and-Swap as a Service</span></h2>
<p>One challenge Litestream has to deal with is desynchronization. Part of the point of Litestream is that SQLite applications don’t have to be aware of it. But <code>litestream</code> is just a process, running alongside the application, and it can die independently. If <code>litestream</code> is down while database changes occur, it will miss changes. The same kind of problem occurs if you start replication from a new server.</p>

<p>Litestream needs a way to reset the replication stream from a new snapshot. How it does that is with “generations”. <a href="https://litestream.io/how-it-works/#snapshots--generations" title="">A generation</a> represents a snapshot and a stream of WAL updates, uniquely identified. Litestream notices any break in its WAL sequence and starts a new generation, which is how it recovers from desynchronization.</p>

<p>Unfortunately, storing and managing multiple generations makes it difficult to implement features like failover and read-replicas.</p>

<p>The most straightforward way around this problem is to make sure only one instance of Litestream can replication to a given destination. If you can do that, you can store just a single, latest generation. That in turn makes it easy to know how to resync a read replica; there’s only one generation to choose from.</p>

<p>In LiteFS, we solved this problem by using Consul, which guaranteed a single leader. That requires users to know about Consul. Things like “requiring Consul” are probably part of the reason Litestream is so much more popular than LiteFS.</p>

<p>In Litestream, we’re solving the problem a different way. Modern object stores like S3 and Tigris solve this problem for us: they now offer <a href="https://aws.amazon.com/about-aws/whats-new/2024/11/amazon-s3-functionality-conditional-writes/" title="">conditional write support</a>. With conditional writes, we can implement a time-based lease. We get essentially the same constraint Consul gave us, but without having to think about it or set up a dependency.</p>

<p>In the immediacy, this will mean you can run Litestream with ephemeral nodes, with overlapping run times, and even if they’re storing to the same destination, they won’t confuse each other.</p>
<h2 id="lightweight-read-replicas"><a href="#lightweight-read-replicas" aria-label="Anchor"></a><span>Lightweight read replicas</span></h2>
<p>The original design constraint of both Litestream and LiteFS was to extend SQLite, to modern deployment scenarios, without disturbing people’s built code. Both tools are meant to function even if applications are oblivious to them.</p>

<p>LiteFS is more ambitious than Litestream, and requires transaction-awareness. To get that without disturbing built code, we use a cute trick (a.k.a. a gross hack): LiteFS provides a FUSE filesystem, which lets it act as a proxy between the application and the backing store. From that vantage point, we can easily discern transactions.</p>

<p>The FUSE approach gave us a lot of control, enough that users could use SQLite replicas just like any other database. But installing and running a whole filesystem (even a fake one) is a lot to ask of users. To work around that problem, we relaxed a constraint: LiteFS can function without the FUSE filesystem if you load an extension into your application code, <a href="https://github.com/superfly/litevfs" title="">LiteVFS</a>.  LiteVFS is a <a href="https://www.sqlite.org/vfs.html" title="">SQLite Virtual Filesystem</a> (VFS). It works in a variety of environments, including some where FUSE can’t, like in-browser WASM builds.</p>

<p>What we’re doing next is taking the same trick and using it on Litestream. We’re building a VFS-based read-replica layer. It will be able to fetch and cache pages directly from S3-compatible object storage.</p>

<p>Of course, there’s a catch: this approach isn’t as efficient as a local SQLite database. That kind of efficiency, where you don’t even need to think about N+1 queries because there’s no network round-trip to make the duplicative queries pile up costs, is part of the point of using SQLite.</p>

<p>But we’re optimistic that with cacheing and prefetching, the approach we’re using will yield, for the right use cases, strong performance — all while serving SQLite reads hot off of Tigris or S3.</p>
<figure>
  <figcaption>
    <h2>Litestream is fully open source</h2>
    <p>It’s not coupled with Fly.io at all; you can use it anywhere.</p>
      <a href="https://http//litestream.io/">
        Check it out <span>→</span>
      </a>
  </figcaption>
  <p><img src="https://fly.io/static/images/cta-cat.webp" srcset="https://fly.io/static/images/cta-cat@2x.webp 2x" alt="">
  </p>
</figure>

<h2 id="synchronize-lots-of-databases"><a href="#synchronize-lots-of-databases" aria-label="Anchor"></a><span>Synchronize Lots Of Databases</span></h2>
<p>While we’ve got you here: we’re knocking out one of our most requested features.</p>

<p>In the old Litestream design, WAL-change polling and slow restores made it infeasible to replicate large numbers of databases from a single process. That has been our answer when users ask us for a “wildcard” or “directory” replication argument for the tool.</p>

<p>Now that we’ve switched to LTX, this isn’t a problem any more. It should thus be possible to replicate <code>/data/*.db</code>, even if there’s hundreds or thousands of databases in that directory.</p>
<h2 id="we-still-sqlite"><a href="#we-still-sqlite" aria-label="Anchor"></a><span>We Still ❤️ SQLite</span></h2>
<p>SQLite has always been a solid database to build on and it’s continued to find new use cases as the industry evolves. We’re super excited to continue to build Litestream alongside it.</p>

<p>We have a sneaking suspicion that the robots that write LLM code are going to like SQLite too. We think what <a href="https://phoenix.new/" title="">coding agents like Phoenix.new</a> want is a way to try out code on live data, screw it up, and then rollback <em>both the code and the state.</em> These Litestream updates put us in a position to give agents PITR as a primitive. On top of that, you can build both rollbacks and forks.</p>

<p>Whether or not you’re drinking the AI kool-aid, we think this new design for Litestream is just better. We’re psyched to be rolling it out, and for the features it’s going to enable.</p>

          
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The NSA Selector (288 pts)]]></title>
            <link>https://github.com/wenzellabs/the_NSA_selector</link>
            <guid>44044459</guid>
            <pubDate>Tue, 20 May 2025 18:30:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/wenzellabs/the_NSA_selector">https://github.com/wenzellabs/the_NSA_selector</a>, See on <a href="https://news.ycombinator.com/item?id=44044459">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">the NSA selector</h2><a id="user-content-the-nsa-selector" aria-label="Permalink: the NSA selector" href="#the-nsa-selector"></a></p>
<p dir="auto">you can purchase the NSA selector at <a href="https://lectronz.com/products/thensaselector" rel="nofollow">my little shop over at lectronz</a>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/wenzellabs/the_NSA_selector/blob/main/pics/NSA_top.jpg"><img src="https://github.com/wenzellabs/the_NSA_selector/raw/main/pics/NSA_top.jpg" alt="the NSA selector top"></a>  <a target="_blank" rel="noopener noreferrer" href="https://github.com/wenzellabs/the_NSA_selector/blob/main/pics/NSA_front.jpg"><img src="https://github.com/wenzellabs/the_NSA_selector/raw/main/pics/NSA_front.jpg" alt="the NSA selector front"></a>   <a target="_blank" rel="noopener noreferrer" href="https://github.com/wenzellabs/the_NSA_selector/blob/main/pics/NSA_back.jpg"><img src="https://github.com/wenzellabs/the_NSA_selector/raw/main/pics/NSA_back.jpg" alt="the NSA selector back"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">what is it</h2><a id="user-content-what-is-it" aria-label="Permalink: what is it" href="#what-is-it"></a></p>
<p dir="auto">the NSA selector is a eurorack module with two ethernet jacks and one audio output.
any bit on the network will be sent to the audio output.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/wenzellabs/the_NSA_selector/blob/main/pics/NSA_selector_switch_schematic.png"><img src="https://github.com/wenzellabs/the_NSA_selector/raw/main/pics/NSA_selector_switch_schematic.png" alt="the NSA selector schematic"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">what it's not</h2><a id="user-content-what-its-not" aria-label="Permalink: what it's not" href="#what-its-not"></a></p>
<p dir="auto">this is not an "audio interface". we do not play back any "format" such as RTP
or MP3 or WAV ot the like. the eurorack module does not "speak" any protocol.
all traffic is forwarded from one network jack to the other unmodified.
it's just tapped, intercepted to convert it to audio.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">examples</h2><a id="user-content-examples" aria-label="Permalink: examples" href="#examples"></a></p>
<p dir="auto">watch <a href="https://www.youtube.com/watch?v=vfgySTaM1TI" rel="nofollow">the NSA selector video</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/wenzellabs/the_NSA_selector/blob/main/pics/NSA_selector_LAN_flow.png"><img src="https://github.com/wenzellabs/the_NSA_selector/raw/main/pics/NSA_selector_LAN_flow.png" alt="the NSA selector LAN flow"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">sequencer script</h3><a id="user-content-sequencer-script" aria-label="Permalink: sequencer script" href="#sequencer-script"></a></p>
<p dir="auto">in the folder <a href="https://github.com/wenzellabs/the_NSA_selector/blob/main/sequencer"><code>sequencer/</code></a> you find a very simple shell script, that mimics a
sequencer by network pings of different size.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">plain image transfer</h3><a id="user-content-plain-image-transfer" aria-label="Permalink: plain image transfer" href="#plain-image-transfer"></a></p>
<p dir="auto">if we transfer uncompressed, unencrypted images e.g. in the .bmp format,
we can hear the pixels. together with a small http server which is available
in the <a href="https://github.com/wenzellabs/the_NSA_selector/blob/main/fileserver"><code>fileserver/</code></a> folder. you can listen to your photos or drawing from
gimp (or photoshop in case you're a rich musician).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">encode audio to NSA's native format</h3><a id="user-content-encode-audio-to-nsas-native-format" aria-label="Permalink: encode audio to NSA's native format" href="#encode-audio-to-nsas-native-format"></a></p>
<p dir="auto">the NSA selector's native format is 4 bits and 25MS/s which originates from the
typical PHY MAC interface called <a href="https://en.wikipedia.org/wiki/Media-independent_interface" rel="nofollow">MII</a>.</p>
<p dir="auto">at first glance 4 bit audio appears to be really crappy, but we can use the
ridiculously high sample rate. what we need is called a delta-sigma modulator.
this lets us convert a simple mono 16 bit 48kHz .wav file to a 4 bit 25MHz
.nsa file.</p>
<p dir="auto">note that this saturates the link and through the added headers from ethernet,
IP, UDP or TCP and HTTP you'll get artifacts. and happy litte retransmissions.</p>
<p dir="auto">far from HiFi quality, but the method adds a lot of spice and excitement.</p>
<p dir="auto">there's a converter in the <a href="https://github.com/wenzellabs/the_NSA_selector/blob/main/upconverter"><code>upconverter/</code></a> folder.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">network overhead</h3><a id="user-content-network-overhead" aria-label="Permalink: network overhead" href="#network-overhead"></a></p>
<p dir="auto">here's what a network packet can look like on the wire:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/wenzellabs/the_NSA_selector/blob/main/pics/NSA_selector_network_packet.png"><img src="https://github.com/wenzellabs/the_NSA_selector/raw/main/pics/NSA_selector_network_packet.png" alt="the NSA selector network packet"></a></p>
<p dir="auto">and we're listening in on the "4B" side of the "4B5B encoding" layer.
so the first bits we hear are <a href="https://en.wikipedia.org/wiki/Ethernet_frame#Preamble_and_start_frame_delimiter" rel="nofollow">the preamble of the ethernet frame</a> and we follow up the stack. e.g. ethernet, IP, TCP, HTTP, BMP.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">delay, oversaturation</h3><a id="user-content-delay-oversaturation" aria-label="Permalink: delay, oversaturation" href="#delay-oversaturation"></a></p>
<p dir="auto">there's a neat little idea I had during development to add a delay to say a
sequencer pattern. actually it's more like an echo than a delay since it lacks
precise timing control.
ssh log into the remote machine that creates the e.g. ping traffic. then dump
the network traffic to the text console to double it. you can increase verbosity
levels of the dump to crank up the echo until total saturation of the link
and lost captured packets. see the video if this explanation is not clear to you.</p>
<p dir="auto">commands I usually use:</p>
<p dir="auto"><code>tcpdump -ni eth0</code></p>
<p dir="auto"><code>tcpdump -nvi eth0</code></p>
<p dir="auto"><code>tcpdump -nvi eth0 icmp</code></p>
<p dir="auto"><code>tcpdump -nvxi eth0</code></p>
<p dir="auto"><code>tcpdump -nxi eth0</code></p>
<p dir="auto"><code>tcpdump -nxi eth0 not port ssh</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">other network traffic</h3><a id="user-content-other-network-traffic" aria-label="Permalink: other network traffic" href="#other-network-traffic"></a></p>
<p dir="auto">be creative!</p>
<p dir="auto">there's so much I hadn't been listening into, like</p>
<ul dir="auto">
<li>online games - I guess a wide variety is waiting here, and some will be very distinctive</li>
<li>doomscrolling on the various platforms</li>
<li>network backup</li>
<li>IoT stuff</li>
<li>remote desktop protocols</li>
<li>write your own code</li>
<li>bundle together ping, netcat, socat, nmap and whatnot and make them MIDI controllable through a software registered MIDI client</li>
</ul>
<p dir="auto">if possible disable encryption, then you can profit from not only timing
pattern (of white noise), but also listen in on the plaintext payload.
the NSA loves plaintext.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">the tech bits</h2><a id="user-content-the-tech-bits" aria-label="Permalink: the tech bits" href="#the-tech-bits"></a></p>
<p dir="auto">the NSA selector is a fast ethernet (FE=100Mbps) network switch with three
ports. the two front ports are switched, and the third port is only available
internally as 4 bit MII bus. it is configured as mirror port of the two front
ports and wired to a 4 bit DAC and a low-pass-filter.</p>
<ul dir="auto">
<li>4 HP wide</li>
<li>current consumption:
<ul dir="auto">
<li>12V : 100mA</li>
<li>5V : nothing</li>
<li>-12V :   2mA</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">assembling the kit</h2><a id="user-content-assembling-the-kit" aria-label="Permalink: assembling the kit" href="#assembling-the-kit"></a></p>
<p dir="auto">both versions are available in my store, a fully assembled eurorack module
and a kit version where you have to solder on the front plate.</p>
<p dir="auto">watch my <a href="https://www.youtube.com/watch?v=SXlfyeYuZuQ" rel="nofollow">NSA selector kit assembly</a> video on youtube.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">store</h2><a id="user-content-store" aria-label="Permalink: store" href="#store"></a></p>
<p dir="auto">you can purchase the NSA selector at <a href="https://lectronz.com/products/thensaselector" rel="nofollow">my little shop over at lectronz</a>.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">SEO</h5><a id="user-content-seo" aria-label="Permalink: SEO" href="#seo"></a></p>
<p dir="auto">#eurorack #NSA #theNSAselector #wenzellabs #the_NSA_selector</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemma 3n preview: Mobile-first AI (128 pts)]]></title>
            <link>https://developers.googleblog.com/en/introducing-gemma-3n/</link>
            <guid>44044451</guid>
            <pubDate>Tue, 20 May 2025 18:29:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developers.googleblog.com/en/introducing-gemma-3n/">https://developers.googleblog.com/en/introducing-gemma-3n/</a>, See on <a href="https://news.ycombinator.com/item?id=44044451">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    
      
    

    

    

    

    
    <div>
          

<div>
    <p data-block-key="ghtsi">Following the exciting launches of <a href="https://blog.google/technology/developers/gemma-3/">Gemma 3</a> and <a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/">Gemma 3 QAT</a>, our family of state-of-the-art open models capable of running on a single cloud or desktop accelerator, we're pushing our vision for accessible AI even further. Gemma 3 delivered powerful capabilities for developers, and we're now extending that vision to highly capable, real-time AI operating directly on the devices you use every day – your phones, tablets, and laptops.</p><p data-block-key="bietk">To power the next generation of on-device AI and support a diverse range of applications, including advancing the capabilities of Gemini Nano, we engineered a new, cutting-edge architecture. This next-generation foundation was created in close collaboration with mobile hardware leaders like Qualcomm Technologies, MediaTek, and Samsung System LSI, and is optimized for lightning-fast, multimodal AI, enabling truly personal and private experiences directly on your device.</p><p data-block-key="74nf9"><a href="https://deepmind.google/models/gemma/gemma-3n/">Gemma 3n</a> is our first open model built on this groundbreaking, shared architecture, allowing developers to begin experimenting with this technology today in an early preview. The same advanced architecture also powers the next generation of <a href="https://deepmind.google/technologies/gemini/nano/">Gemini Nano</a>, which brings these capabilities to a broad range of features in Google apps and our on-device ecosystem, and will become available later this year. Gemma 3n enables you to start building on this foundation that will come to major platforms such as Android and Chrome.</p>
</div>   


    
    <div>
            
                <p><img src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_OjwrVp1.original.png" alt="Chatbot Arena Elo scores"></p><p>
                        This chart ranks AI models by Chatbot Arena Elo scores; higher scores (top numbers) indicate greater user preference. Gemma 3n ranks highly amongst both popular proprietary and open models.
                    </p>
                
            
        </div>
  <div>
    <p data-block-key="ghtsi">Gemma 3n leverages a Google DeepMind innovation called Per-Layer Embeddings (PLE) that delivers a significant reduction in RAM usage. While the raw parameter count is 5B and 8B, this innovation allows you to run larger models on mobile devices or live-stream from the cloud, with a memory overhead comparable to a 2B and 4B model, meaning the models can operate with a dynamic memory footprint of just 2GB and 3GB. Learn more in our <a href="https://ai.google.dev/gemma/docs/gemma-3n#parameters">documentation</a>.</p><p data-block-key="q3ib">By exploring Gemma 3n, developers can get an early preview of the open model’s core capabilities and mobile-first architectural innovations that will be available on Android and Chrome with Gemini Nano.</p><p data-block-key="64a2c">In this post, we'll explore Gemma 3n's new capabilities, our approach to responsible development, and how you can access the preview today.</p><h3 data-block-key="mcrw4" id="key-capabilities-of-gemma-3n"><b><br>Key Capabilities of Gemma 3n</b></h3><p data-block-key="54i3c">Engineered for fast, low-footprint AI experiences running locally, Gemma 3n delivers:</p><ul><li data-block-key="binsd"><b>Optimized On-Device Performance &amp; Efficiency:</b> Gemma 3n starts responding approximately 1.5x faster on mobile with significantly better quality (compared to Gemma 3 4B) and a reduced memory footprint achieved through innovations like Per Layer Embeddings, KVC sharing, and advanced activation quantization.</li></ul><ul><li data-block-key="cgst0"><b>Many-in-1 Flexibility:</b> A model with a 4B active memory footprint that natively includes a nested state-of-the-art 2B active memory footprint submodel (thanks to <a href="https://arxiv.org/abs/2310.07707">MatFormer</a> training). This provides flexibility to dynamically trade off performance and quality on the fly without hosting separate models. We further introduce mix’n’match capability in Gemma 3n to dynamically create submodels from the 4B model that can optimally fit your specific use case -- and associated quality/latency tradeoff. Stay tuned for more on this research in our upcoming technical report.</li></ul><ul><li data-block-key="1cvli"><b>Privacy-First &amp; Offline Ready:</b> Local execution enables features that respect user privacy and function reliably, even without an internet connection.</li></ul><ul><li data-block-key="dofaq"><b>Expanded Multimodal Understanding with Audio:</b> Gemma 3n can understand and process audio, text, and images, and offers significantly enhanced video understanding. Its audio capabilities enable the model to perform high-quality Automatic Speech Recognition (transcription) and Translation (speech to translated text). Additionally, the model accepts interleaved inputs across modalities, enabling understanding of complex multimodal interactions. (Public implementation coming soon)</li></ul><ul><li data-block-key="1mkna"><b>Improved Multilingual Capabilities:</b> Improved multilingual performance, particularly in Japanese, German, Korean, Spanish, and French. Strong performance reflected on multilingual benchmarks such as 50.1% on WMT24++ (ChrF).</li></ul>
</div>   


    
    <div>
            
                <p><img src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Artboard_1.original.png" alt="MMLU performance"></p><p>
                        This chart show’s MMLU performance vs model size of Gemma 3n’s mix-n-match (pretrained) capability.
                    </p>
                
            
        </div>
  <div>
    <h3 data-block-key="k4nng" id="unlocking-new-on-the-go-experiences"><b>Unlocking New On-the-go Experiences</b></h3><p data-block-key="1hssr">Gemma 3n will empower a new wave of intelligent, on-the-go applications by enabling developers to:</p><ol><li data-block-key="9jafp"><b>Build live, interactive experiences</b> that understand and respond to real-time visual and auditory cues from the user's environment.</li></ol><p data-block-key="c7kgq"><b><br></b>2. <b>Power deeper understanding</b> and contextual text generation using combined audio, image, video, and text inputs—all processed privately on-device.</p><p data-block-key="t3pn"><b><br></b>3. <b>Develop advanced audio-centric applications</b>, including real-time speech transcription, translation, and rich voice-driven interactions.</p><p data-block-key="7mf82"><br>Here’s an overview and the types of experiences you can build:</p>
</div>    <div>
    <h3 data-block-key="5sfqt" id="building-responsibly-together"><b>Building Responsibly, Together</b></h3><p data-block-key="4evp8">Our commitment to responsible AI development is paramount. Gemma 3n, like all Gemma models, underwent rigorous safety evaluations, data governance, and fine-tuning alignment with our safety policies. We approach open models with careful risk assessment, continually refining our practices as the AI landscape evolves.</p><h3 data-block-key="ei4ug" id="get-started:-preview-gemma-3n-today"><b><br>Get Started: Preview Gemma 3n Today</b></h3><p data-block-key="a8cmr">We're excited to get Gemma 3n into your hands through a preview starting today:</p><p data-block-key="a0cp7"><b><br>Initial Access (Available Now):</b></p><ul><li data-block-key="oaqh"><b>Cloud-based Exploration with Google AI Studio:</b> Try Gemma 3n directly in your browser on <a href="https://aistudio.google.com/app/prompts/new_chat?model=gemma-3n-e4b-it">Google AI Studio</a> – no setup needed. Explore its text input capabilities instantly.</li></ul><ul><li data-block-key="e7c6o"><b>On-Device Development with Google AI Edge:</b> For developers looking to integrate Gemma 3n locally, <a href="https://developers.googleblog.com/en/google-ai-edge-small-language-models-multimodality-rag-function-calling">Google AI Edge</a> provides tools and libraries. You can get started with text and image understanding/generation capabilities today.</li></ul><p data-block-key="cp8j"><br>Gemma 3n marks the next step in democratizing access to cutting-edge, efficient AI. We’re incredibly excited to see what you’ll build as we make this technology progressively available, starting with today's preview.</p><p data-block-key="46ck9">Explore this announcement and all Google I/O 2025 updates on <a href="https://io.google/2025/?utm_source=blogpost&amp;utm_medium=pr&amp;utm_campaign=event&amp;utm_content=">io.google</a> starting May 22.</p>
</div> 
      </div>
    

    

    
    
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google AI Ultra (299 pts)]]></title>
            <link>https://blog.google/products/google-one/google-ai-ultra/</link>
            <guid>44044367</guid>
            <pubDate>Tue, 20 May 2025 18:20:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/products/google-one/google-ai-ultra/">https://blog.google/products/google-one/google-ai-ultra/</a>, See on <a href="https://news.ycombinator.com/item?id=44044367">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="jump-content" tabindex="-1">
            

    
    

    <article>

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;Introducing Google AI Ultra: The best of Google AI in one subscription&quot;
  }">
      <div>
          
            <p>May 20, 2025</p>
          
          
            <p data-reading-time-render="">[[read-time]] min read</p>
          
        </div>
      
        <p>
          Get the highest access to our most capable AI models and premium features with this new plan by Google One.
        </p>
      
    </div>

    

    
      










<div>
    <figure>
      <div>
        <p><img alt="A dark background with rainbow ombre at the top, with text saying Google AI Ultra subscription, and tiles with text including Gemini app and Flow. Underneath that, neon jellyfish float over Earth." data-component="uni-progressive-image" fetchpriority="high" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Google_AI_Ultra_Blog_Header_Image.width-200.format-webp.webp" width="360px" data-sizes="(max-width: 1023px) 100vw,(min-width: 1024px and max-width: 1259) 80vw, 1046px" data-srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Google_AI_Ultra_Blog_Header_Image.width-800.format-webp.webp 800w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Google_AI_Ultra_Blog_Header_Imag.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Google_AI_Ultra_Blog_Header_Imag.width-1600.format-webp.webp 1600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Google_AI_Ultra_Blog_Header_Imag.width-2200.format-webp.webp 2200w">
        </p>
      </div>
      
    </figure>
  </div>






    

    
    <div data-reading-time="true" data-component="uni-article-body">

            
              





<uni-article-speakable page-title="Introducing Google AI Ultra: The best of Google AI in one subscription" listen-to-article="Listen to article" data-date-modified="2025-05-20T18:14:16.139622+00:00" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-script-src="https://www.gstatic.com/readaloud/player/web/api/js/api.js"></uni-article-speakable>

            

            
            
<!--article text-->

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Google AI Ultra: The best of Google AI in one subscription&quot;
         }"><p data-block-key="4bn2h">As we introduce new, emerging capabilities and state-of-the-art models, we want to provide more ways for people to get access to the best of what Google AI has to offer to help them be more knowledgeable, creative and productive.</p><p data-block-key="8pm6">Today, we’re introducing <a href="http://one.google.com/ai?utm_source=g1&amp;utm_medium=web&amp;utm_campaign=google_ai_plan_blog">Google AI Ultra</a>, a new AI subscription plan with the highest usage limits and access to our most capable models and premium features. If you're a filmmaker, developer, creative professional or simply demand the absolute best of Google AI with the highest level of access, the Google AI Ultra plan is built for you — think of it as your VIP pass to Google Al.</p><p data-block-key="1e6tn">Google AI Ultra is available today in the U.S. for $249.99/month (with a special offer for first-time users of 50% off for your first three months), and coming soon to more countries. Here’s what you get with Google AI Ultra:</p><ul><li data-block-key="aj66v"><b>Gemini:</b> Experience the absolute best version of our <a href="https://blog.google/products/gemini/gemini-app-updates-io-2025">Gemini app</a>. This plan offers the highest usage limits across Deep Research, cutting-edge video generation with Veo 2 and early access to our groundbreaking Veo 3 model. It's designed for coding, academic research and complex creative endeavors. In the coming weeks, Ultra subscribers will also get access to Deep Think in 2.5 Pro, our new enhanced reasoning mode.</li><li data-block-key="f9t9s"><b>Flow:</b> This <a href="https://blog.google/technology/ai/google-flow-veo-ai-filmmaking-tool/">new AI filmmaking tool</a> is custom-designed for Google DeepMind’s most advanced models (Veo, Imagen and Gemini). It enables the crafting of cinematic clips, scenes and cohesive narratives with intuitive prompting. Google AI Ultra unlocks the highest limits in Flow with 1080p video generation, advanced camera controls and early access to Veo 3.</li><li data-block-key="4h6gs"><b>Whisk:</b> Whisk helps you quickly explore and visualize new ideas using both text and image prompts. With Google AI Ultra, get the highest limits for Whisk Animate, which turns your images into vivid eight-second videos with Veo 2.</li><li data-block-key="53ou1"><b>NotebookLM:</b> Get access to the highest usage limits and enhanced model capabilities later this year, whether you're using NotebookLM for studying, teaching or working on your projects.</li><li data-block-key="a8ooh"><b>Gemini in Gmail, Docs, Vids and more:</b> Make everyday tasks easier with access to Gemini directly in your favorite Google apps like Gmail, Docs, Vids and more.</li><li data-block-key="7nm55"><b>Gemini in Chrome:</b> Starting tomorrow, get early access to Gemini directly within the Chrome browser. This feature allows you to effortlessly understand complex information and complete tasks on the web by using the context of the current page.</li><li data-block-key="58cj9"><b>Project Mariner:</b> This agentic research prototype can assist you in managing up to 10 tasks simultaneously — from research to bookings and purchases — all from a single dashboard.</li><li data-block-key="c05ih"><b>YouTube Premium:</b> An individual YouTube Premium plan lets you watch YouTube and listen to YouTube Music ad-free, offline and in the background.</li><li data-block-key="eaedq"><b>30 TB of storage:</b> Offers massive storage capacity across Google Photos, Drive and Gmail to keep your creations and important files secure.</li></ul></div>
  

  
    
  
    


  <uni-youtube-player-article index="2" thumbnail-alt="A dark background with rainbow ombre at the top, with text saying Google AI Ultra subscription, and tiles with text including Gemini app and Flow. Underneath that, neon jellyfish float over Earth." video-id="2CquRQiDzx8" video-type="video">
  </uni-youtube-player-article>


  


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Google AI Ultra: The best of Google AI in one subscription&quot;
         }"><p data-block-key="4bn2h">Our existing AI Premium plan, now called Google AI Pro, is also getting better. For no additional cost, Google AI Pro subscribers will now also get access to AI filmmaking capabilities in Flow, where you can try the tool with our Veo 2 model, as well as early access to Gemini in Chrome. These new benefits are coming to Google AI Pro subscribers in the U.S. first, with availability in more countries to follow.</p><p data-block-key="8rvm3">We're also expanding free access to Google AI Pro for a school year to university students in Japan, Brazil, Indonesia and the United Kingdom, <a href="https://blog.google/products/gemini/google-one-ai-premium-students-free/">in addition to the U.S.</a></p></div>
  

  
    






<uni-image-full-width alignment="large" alt-text="Table comparing Google AI Pro and Google AI Ultra plans." external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="Introducing Google AI Ultra: The best of Google AI in one subscription" custom-class="image-full-width--constrained-width uni-component-spacing">
  
  
    <p><img alt="Table comparing Google AI Pro and Google AI Ultra plans." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AI_plans_v5_-2x.width-100.format-webp.webp" loading="lazy" data-loading="{
            &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AI_plans_v5_-2x.width-500.format-webp.webp&quot;,
            &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AI_plans_v5_-2x.width-1000.format-webp.webp&quot;
          }">
    </p>
  
</uni-image-full-width>


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Google AI Ultra: The best of Google AI in one subscription&quot;
         }"><p data-block-key="4bn2h"><a href="http://one.google.com/ai?utm_source=g1&amp;utm_medium=web&amp;utm_campaign=google_ai_plan_blog">Sign up for Google AI Ultra today</a> to experience the best of Google AI.</p></div>
  

  
    







<uni-related-content-tout title="I/O 2025" cta="See more" summary="Here’s a look at everything we announced at Google I/O 2025." hideimage="False" eyebrow="Collection" image-alt-text="Text saying I/O25 in rainbow block letters against a grid" role="none" fullurl="https://blog.google/technology/developers/google-io-2025-collection/" pagetype="collectiondetailpage" isarticlepage="">
  
    <div slot="rct-image-slot">
      
    <figure>
        <picture>
            


    

    
        <source media="(max-resolution: 1.5dppx)" sizes="300px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_social_share.width-300.format-webp.webp 300w">
    
        <source media="(min-resolution: 1.5dppx)" sizes="512px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_social_share.width-512.format-webp.webp 512w">
    

    <img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_social_share.width-512.format-webp.webp" alt="Text saying I/O25 in rainbow block letters against a grid" sizes=" 300px,  512px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_social_share.width-300.format-webp.webp 300w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_social_share.width-512.format-webp.webp 512w" data-target="image" loading="lazy">
    


        </picture>
    </figure>


    </div>
  
</uni-related-content-tout>

  

  
    

  
    





<div role="form" data-component="uni-subscribe" data-analytics-module="{
    &quot;module_name&quot;: &quot;Newsletter&quot;,
    &quot;section_header&quot;: &quot;Get more stories from Google in your inbox.&quot;
  }">
    
    
    <div id="newsletter-form--form">
      <form data-method="POST" data-action="https://services.google.com/fb/submissions/thekeywordnewsletterprodv2/">
        <h2 id="subscribe_box_label">
          <span>Get more stories from Google in your inbox.</span>
          
        </h2>
        
      </form>
    </div>
    
    <div>
      <div>
        <p tabindex="-1" role="status" aria-live="off" aria-atomic="false">
          Done. Just one step more.
        </p>
        <p>
          Check your inbox to confirm your subscription.
        </p>
        <p>You are already subscribed to our newsletter.</p>
      </div>
      <p>
        You can also subscribe with a
        
        
      </p>
    </div>
  </div>

  

  


            
            

            
              




            
          </div>
  </article>
  





  

  


<div data-component="uni-related-articles" aria-roledescription="carousel" data-analytics-module="{
    &quot;module_name&quot;: &quot;Article Footer Related Stories&quot;,
    &quot;section_header&quot;: &quot;Related stories&quot;
  }">
        <h3>
          <p>
            Related stories
          </p>
        </h3>
      </div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Red Programming Language (191 pts)]]></title>
            <link>https://www.red-lang.org/p/about.html</link>
            <guid>44044306</guid>
            <pubDate>Tue, 20 May 2025 18:14:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.red-lang.org/p/about.html">https://www.red-lang.org/p/about.html</a>, See on <a href="https://news.ycombinator.com/item?id=44044306">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-1150177033025383393" itemprop="description articleBody">
<p><b>Red </b>is a next-gen programming language,&nbsp;strongly inspired by&nbsp;<a href="http://rebol.com/">REBOL</a>. Main features are:</p><ul>
<li><b>Human-friendly</b> <a href="http://www.rebol.com/rebolsteps.html" target="_blank">syntax</a></li>
<li><b><a href="http://en.wikipedia.org/wiki/Homoiconicity">Homoiconic</a></b>&nbsp;(Red is its own meta-language and own <b><a href="http://www.rebol.com/rebolsteps.html" target="_blank">data-format</a></b>)</li>
<li><b>Functional</b>, imperative, <b><a href="http://www.red-lang.org/2016/06/061-reactive-programming.html" target="_blank">reactive</a></b> and <b>symbolic </b>programming</li>
<li><b>Prototype</b>-based object support</li>
<li><b>Multi-</b>typing</li>
<li><b>Powerful pattern-matching&nbsp;</b><a href="http://www.red-lang.org/2016/12/entering-world-of-macros.html" target="_blank">Macros</a> system</li>
<li><b>Rich</b> set of built-in datatypes (50+)</li>
<li>Both <b>statically </b>and <b>JIT</b>-<b>compiled(*)&nbsp;</b>to native code</li>
<li><b>Cross-compilation</b> <a href="https://github.com/red/red/blob/master/encapper/usage.txt" target="_blank">done</a> <a href="https://github.com/red/red/blob/master/system/config.r" target="_blank">right</a></li>
<li>Produces executables of <b>less than 1MB</b>, with <b>no dependencies</b></li>
<li><b>Concurrency </b>and <b>parallelism </b>strong support (actors, parallel collections)(*)</li>
<li>Low-level <b>system programming</b> abilities through the built-in Red/System <a href="http://en.wikipedia.org/wiki/Domain-specific_language">DSL</a></li>
<li>Powerful <b>PEG <a href="http://www.red-lang.org/2013/11/041-introducing-parse.html" target="_blank">parser </a></b><a href="http://www.red-lang.org/2013/11/041-introducing-parse.html" target="_blank">DSL</a>&nbsp;built-in</li><li>Fast, compacting<b> Garbage Collector</b></li>
<li><b>Cross-platform native <a href="http://www.red-lang.org/2016/03/060-red-gui-system.html" target="_blank">GUI </a></b><a href="http://www.red-lang.org/2016/03/060-red-gui-system.html" target="_blank">system</a>, with a <a href="http://doc.red-lang.org/gui/VID.html" target="_blank">UI layout DSL</a> and <a href="http://doc.red-lang.org/gui/Draw.html" target="_blank">drawing DSL</a></li>
<li><b>Bridging</b> <a href="https://github.com/red/red/blob/master/bridges/java/hello.red" target="_blank">to the JVM</a></li>
<li>High-level <b>scripting </b>and <a href="http://en.wikipedia.org/wiki/Read-eval-print_loop"><b>REPL</b></a>&nbsp;GUI and CLI consoles included</li>
<li>Visual Studio Code<b>&nbsp;<a href="https://marketplace.visualstudio.com/items?itemName=red-auto.red" target="_blank">plugin</a></b>, with many helpful features</li>
<li>Highly <b><a href="http://www.red-lang.org/2017/03/062-libred-and-macros.html" target="_blank">embeddable</a></b></li>
<li><b>Low </b>memory footprint</li>
<li><b><a href="http://www.red-lang.org/p/download.html" target="_blank">Single-file</a></b>&nbsp;(~1MB)&nbsp;contains whole toolchain, full standard library and REPL&nbsp;(**)</li>
<li><b>No install, no setup</b></li>
<li><b>Fun</b>&nbsp;guaranteed!</li>
</ul>
<div>
<p>(*) Not implemented yet.</p><p>(**) Temporarily split in two binaries</p><p>
Red’s ambitious goal is to build the world’s first <b>full-stack language</b>, a language you can use from system programming tasks, up to high-level scripting through DSL. You've probably heard of the term "<a href="http://www.laurencegellert.com/2012/08/what-is-a-full-stack-developer/" target="_blank">Full-Stack Developer</a>". But what is a full-stack Language, exactly?</p><p>

Other languages talk about having "one tool to rule them all". Red has that mindset too, pushed to the limit - it's a single executable that takes in your source files on any platform, and produces a packaged binary for any platform, from any other. The tool doesn’t depend on anything besides what came with your OS...shipping as a single executable that about a megabyte.</p><p>

But that technical feat alone isn't enough to define Red's notion of a "Full-Stack Language". It's about the ability to bend and redefine the system to meet any need, while still working with literate code, and getting top-flight performance. &nbsp;So what's being put in your hands is more like a "language construction set" than simply "a language". Whether you’re writing a device driver, a platform-native GUI application, or a shared library... Red lets you use a common syntax to code at the right level of abstraction for the task.</p><p><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-ipzNCeWoKszMhyphenhyphen3My5TZK7xAnpvQBa8LBx6kZo2CVYCuGBD9-BaBg2A5qGn1IO_BiFdTaBpxXR4Nv2lWjtU-H85Yn3xok1IsvnZhpswRoulh6QZen1w23DWwyCeUA1TUZxhM_2pkxJ4/s1600/reichart-abstraction-diagram.png">
</p>
</div><p>
It was announced and presented for the first time&nbsp;at <a href="http://reborcon.esperconsultancy.nl/">ReBorCon 2011</a> conference (March, 2011).&nbsp;A more recent presentation video was given at the Recode conference in Montreal (July, 2013):</p><p>
<iframe allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/-KqNO_sDqm4" width="560"></iframe>
</p>
<p>
But if you are unable to visit YouTube, here are some slide decks explaining the reasons for building it, showing the main features and the roadmap.</p><p>
Recode 2013 presentation slides:&nbsp;<a href="http://static.red-lang.org/Recode2013-Red.pdf" target="_new">PDF version</a>.
</p><p>
And for historical purposes, here are some older presentations:</p><ul><li><a href="http://static.red-lang.org/Red-SFD2011-45mn.pdf" target="_blank">SFD 2011 conference slides</a></li><li><a href="http://static.red-lang.org/red-rebor2011.pdf" target="_blank">ReBor 2011 conference slides</a></li></ul>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemma 3n preview: Mobile-first AI (409 pts)]]></title>
            <link>https://developers.googleblog.com/en/introducing-gemma-3n/</link>
            <guid>44044199</guid>
            <pubDate>Tue, 20 May 2025 18:03:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developers.googleblog.com/en/introducing-gemma-3n/">https://developers.googleblog.com/en/introducing-gemma-3n/</a>, See on <a href="https://news.ycombinator.com/item?id=44044199">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    
      
    

    

    

    

    
    <div>
          

<div>
    <p data-block-key="ghtsi">Following the exciting launches of <a href="https://blog.google/technology/developers/gemma-3/">Gemma 3</a> and <a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/">Gemma 3 QAT</a>, our family of state-of-the-art open models capable of running on a single cloud or desktop accelerator, we're pushing our vision for accessible AI even further. Gemma 3 delivered powerful capabilities for developers, and we're now extending that vision to highly capable, real-time AI operating directly on the devices you use every day – your phones, tablets, and laptops.</p><p data-block-key="bietk">To power the next generation of on-device AI and support a diverse range of applications, including advancing the capabilities of Gemini Nano, we engineered a new, cutting-edge architecture. This next-generation foundation was created in close collaboration with mobile hardware leaders like Qualcomm Technologies, MediaTek, and Samsung System LSI, and is optimized for lightning-fast, multimodal AI, enabling truly personal and private experiences directly on your device.</p><p data-block-key="74nf9"><a href="https://deepmind.google/models/gemma/gemma-3n/">Gemma 3n</a> is our first open model built on this groundbreaking, shared architecture, allowing developers to begin experimenting with this technology today in an early preview. The same advanced architecture also powers the next generation of <a href="https://deepmind.google/technologies/gemini/nano/">Gemini Nano</a>, which brings these capabilities to a broad range of features in Google apps and our on-device ecosystem, and will become available later this year. Gemma 3n enables you to start building on this foundation that will come to major platforms such as Android and Chrome.</p>
</div>   


    
    <div>
            
                <p><img src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_OjwrVp1.original.png" alt="Chatbot Arena Elo scores"></p><p>
                        This chart ranks AI models by Chatbot Arena Elo scores; higher scores (top numbers) indicate greater user preference. Gemma 3n ranks highly amongst both popular proprietary and open models.
                    </p>
                
            
        </div>
  <div>
    <p data-block-key="ghtsi">Gemma 3n leverages a Google DeepMind innovation called Per-Layer Embeddings (PLE) that delivers a significant reduction in RAM usage. While the raw parameter count is 5B and 8B, this innovation allows you to run larger models on mobile devices or live-stream from the cloud, with a memory overhead comparable to a 2B and 4B model, meaning the models can operate with a dynamic memory footprint of just 2GB and 3GB. Learn more in our <a href="https://ai.google.dev/gemma/docs/gemma-3n#parameters">documentation</a>.</p><p data-block-key="q3ib">By exploring Gemma 3n, developers can get an early preview of the open model’s core capabilities and mobile-first architectural innovations that will be available on Android and Chrome with Gemini Nano.</p><p data-block-key="64a2c">In this post, we'll explore Gemma 3n's new capabilities, our approach to responsible development, and how you can access the preview today.</p><h3 data-block-key="chmin" id="key-capabilities-of-gemma-3n"><b><br>Key Capabilities of Gemma 3n</b></h3><p data-block-key="54i3c">Engineered for fast, low-footprint AI experiences running locally, Gemma 3n delivers:</p><ul><li data-block-key="binsd"><b>Optimized On-Device Performance &amp; Efficiency:</b> Gemma 3n starts responding approximately 1.5x faster on mobile with significantly better quality (compared to Gemma 3 4B) and a reduced memory footprint achieved through innovations like Per Layer Embeddings, KVC sharing, and advanced activation quantization.</li></ul><ul><li data-block-key="cgst0"><b>Many-in-1 Flexibility:</b> A model with a 4B active memory footprint that natively includes a nested state-of-the-art 2B active memory footprint submodel (thanks to <a href="https://arxiv.org/abs/2310.07707">MatFormer</a> training). This provides flexibility to dynamically trade off performance and quality on the fly without hosting separate models. We further introduce mix’n’match capability in Gemma 3n to dynamically create submodels from the 4B model that can optimally fit your specific use case -- and associated quality/latency tradeoff. Stay tuned for more on this research in our upcoming technical report.</li></ul><ul><li data-block-key="1cvli"><b>Privacy-First &amp; Offline Ready:</b> Local execution enables features that respect user privacy and function reliably, even without an internet connection.</li></ul><ul><li data-block-key="dofaq"><b>Expanded Multimodal Understanding with Audio:</b> Gemma 3n can understand and process audio, text, and images, and offers significantly enhanced video understanding. Its audio capabilities enable the model to perform high-quality Automatic Speech Recognition (transcription) and Translation (speech to translated text). Additionally, the model accepts interleaved inputs across modalities, enabling understanding of complex multimodal interactions. (Public implementation coming soon)</li></ul><ul><li data-block-key="1mkna"><b>Improved Multilingual Capabilities:</b> Improved multilingual performance, particularly in Japanese, German, Korean, Spanish, and French. Strong performance reflected on multilingual benchmarks such as 50.1% on WMT24++ (ChrF).</li></ul>
</div>   


    
    <div>
            
                <p><img src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Artboard_1.original.png" alt="MMLU performance"></p><p>
                        This chart show’s MMLU performance vs model size of Gemma 3n’s mix-n-match (pretrained) capability.
                    </p>
                
            
        </div>
  <div>
    <h3 data-block-key="2rib8" id="unlocking-new-on-the-go-experiences"><b>Unlocking New On-the-go Experiences</b></h3><p data-block-key="1hssr">Gemma 3n will empower a new wave of intelligent, on-the-go applications by enabling developers to:</p><ol><li data-block-key="9jafp"><b>Build live, interactive experiences</b> that understand and respond to real-time visual and auditory cues from the user's environment.</li></ol><p data-block-key="c7kgq"><b><br></b>2. <b>Power deeper understanding</b> and contextual text generation using combined audio, image, video, and text inputs—all processed privately on-device.</p><p data-block-key="t3pn"><b><br></b>3. <b>Develop advanced audio-centric applications</b>, including real-time speech transcription, translation, and rich voice-driven interactions.</p><p data-block-key="7mf82"><br>Here’s an overview and the types of experiences you can build:</p>
</div>    <div>
    <h3 data-block-key="q2vk1" id="building-responsibly-together"><b>Building Responsibly, Together</b></h3><p data-block-key="4evp8">Our commitment to responsible AI development is paramount. Gemma 3n, like all Gemma models, underwent rigorous safety evaluations, data governance, and fine-tuning alignment with our safety policies. We approach open models with careful risk assessment, continually refining our practices as the AI landscape evolves.</p><h3 data-block-key="eyvht" id="get-started:-preview-gemma-3n-today"><b><br>Get Started: Preview Gemma 3n Today</b></h3><p data-block-key="a8cmr">We're excited to get Gemma 3n into your hands through a preview starting today:</p><p data-block-key="a0cp7"><b><br>Initial Access (Available Now):</b></p><ul><li data-block-key="oaqh"><b>Cloud-based Exploration with Google AI Studio:</b> Try Gemma 3n directly in your browser on <a href="https://aistudio.google.com/app/prompts/new_chat?model=gemma-3n-e4b-it">Google AI Studio</a> – no setup needed. Explore its text input capabilities instantly.</li></ul><ul><li data-block-key="e7c6o"><b>On-Device Development with Google AI Edge:</b> For developers looking to integrate Gemma 3n locally, <a href="https://developers.googleblog.com/en/google-ai-edge-small-language-models-multimodality-rag-function-calling">Google AI Edge</a> provides tools and libraries. You can get started with text and image understanding/generation capabilities today.</li></ul><p data-block-key="cp8j"><br>Gemma 3n marks the next step in democratizing access to cutting-edge, efficient AI. We’re incredibly excited to see what you’ll build as we make this technology progressively available, starting with today's preview.</p><p data-block-key="46ck9">Explore this announcement and all Google I/O 2025 updates on <a href="https://io.google/2025/?utm_source=blogpost&amp;utm_medium=pr&amp;utm_campaign=event&amp;utm_content=">io.google</a> starting May 22.</p>
</div> 
      </div>
    

    

    
    
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Introducing Veo 3 and Imagen 4, and a new tool for filmmaking called Flow (754 pts)]]></title>
            <link>https://blog.google/technology/ai/generative-media-models-io-2025/</link>
            <guid>44044043</guid>
            <pubDate>Tue, 20 May 2025 17:46:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/technology/ai/generative-media-models-io-2025/">https://blog.google/technology/ai/generative-media-models-io-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=44044043">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
  }">
      <div>
          
            <p>May 20, 2025</p>
          
          
            <p data-reading-time-render="">[[read-time]] min read</p>
          
        </div>
      
        <p>
          Introducing Veo 3 and Imagen 4, and a new tool for filmmaking called Flow.
        </p>
      
    </div>

    

    
      










<div>
    <figure>
      <div>
        <p><img alt="Collage of various nature images generated by AI" data-component="uni-progressive-image" fetchpriority="high" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5.20v2_Hero_2097x1182.width-200.format-webp.webp" width="360px" data-sizes="(max-width: 1023px) 100vw,(min-width: 1024px and max-width: 1259) 80vw, 1046px" data-srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5.20v2_Hero_2097x1182.width-800.format-webp.webp 800w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5.20v2_Hero_2097x1182.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5.20v2_Hero_2097x1182.width-1600.format-webp.webp 1600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5.20v2_Hero_2097x1182.width-2200.format-webp.webp 2200w">
        </p>
      </div>
      
    </figure>
  </div>






    

    
    <div>
        
          
            <div data-component="uni-article-jumplinks" data-analytics-module="{
    &quot;module_name&quot;: &quot;Article Jumplinks&quot;,
    &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
  }">
  <nav aria-label="Article Jumplinks">
    <p><span>In this story</span>
    </p>
    
    
    <div>
      <ul id="article-jumplinks__list">
        
        <li>
          <a aria-label="link to Veo 3" href="#veo-3" id="veo-3-anchor">Veo 3</a>
        </li>
        
        <li>
          <a aria-label="link to Veo 2 updates" href="#veo-2-updates" id="veo-2-updates-anchor">Veo 2 updates</a>
        </li>
        
        <li>
          <a aria-label="link to Flow" href="#flow" id="flow-anchor">Flow</a>
        </li>
        
        <li>
          <a aria-label="link to Imagen 4" href="#imagen-4" id="imagen-4-anchor">Imagen 4</a>
        </li>
        
        <li>
          <a aria-label="link to Lyria 2" href="#lyria-2" id="lyria-2-anchor">Lyria 2</a>
        </li>
        
        <li>
          <a aria-label="link to Responsible creation" href="#responsible-creation" id="responsible-creation-anchor">Responsible creation</a>
        </li>
        
      </ul>
    </div>
    
  </nav>
</div>
          
          
          <div data-reading-time="true" data-component="uni-article-body">

            
              





<uni-article-speakable page-title="Fuel your creativity with new generative media  models and tools" listen-to-article="Listen to article" data-date-modified="2025-05-20T18:52:10.019676+00:00" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-script-src="https://www.gstatic.com/readaloud/player/web/api/js/api.js"></uni-article-speakable>

            

            
            
<!--article text-->

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
         }"><p data-block-key="zbevq">Today, we’re announcing our newest generative media models, which mark significant breakthroughs. These models create breathtaking images, videos and music, empowering artists to bring their creative vision to life. They also power amazing tools for everyone to express themselves.</p><p data-block-key="brd72">Veo 3 and Imagen 4, our newest video and image generation models, push the frontier of media generation, with their groundbreaking new capabilities. We're also expanding access to Lyria 2, giving <a href="https://deepmind.google/discover/blog/music-ai-sandbox-now-with-new-features-and-broader-access/">musicians</a> more tools to create music. Finally, we’re inviting visual storytellers to try <a href="http://flow.google/">Flow</a>, our new AI filmmaking tool. Using Google DeepMind’s most advanced models, Flow lets you weave cinematic films with more sophisticated control of characters, scenes and styles, to bring your story to life.</p><p data-block-key="3sps2">We’ve <a href="https://blog.google/technology/google-deepmind/deepmind-primordial-soup-collaboration">partnered closely with the creative industries</a> — filmmakers, musicians, artists, YouTube creators — to help <a href="https://blog.google/technology/ai/google-flow-veo-ai-filmmaking-tool/">shape these models</a> and products responsibly and to give creators new tools to realize the possibilities of AI in their art.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
         }"><h2 data-block-key="4kbui">Veo 3: Video, meet audio</h2><p data-block-key="6m4h7"><a href="http://deepmind.google/models/veo">Veo 3, our new state-of-the-art video generation model</a>, not only improves on the quality of Veo 2, but for the first time, can also generate videos with audio — traffic noises in the background of a city street scene, birds singing in a park, even dialogue between characters.</p></div>
  

  
    
  
    


  <uni-youtube-player-article index="4" thumbnail-alt="Veo created video" video-id="Yqg0oUaofH0" video-type="video">
  </uni-youtube-player-article>


  


  

  
    
  
    


  <uni-youtube-player-article index="5" thumbnail-alt="Veo created video" video-id="BTx-MnBCDvw" video-type="video">
  </uni-youtube-player-article>


  


  

  
    
  
    


  <uni-youtube-player-article index="6" thumbnail-alt="Veo created video" video-id="OW9q6SWTXt8" video-type="video">
  </uni-youtube-player-article>


  


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
         }">
        <p data-block-key="zbevq">Across the board, Veo 3 excels from text and image prompting to real-world physics and accurate lip syncing. It’s great at understanding; you can tell a short story in your prompt, and the model gives you back a clip that brings it to life. Veo 3 is available today for Ultra subscribers in the United States in the <a href="http://gemini.google.com/">Gemini app</a> and in <a href="http://flow.google/">Flow</a>. It’s also available for enterprise users on <a href="https://cloud.google.com/blog/products/ai-machine-learning/announcing-veo-3-imagen-4-and-lyria-2-on-vertex-ai">Vertex AI</a>.</p>
      </div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
         }"><h2 data-block-key="ao97o">Veo 2 updates: New capabilities built with and for filmmakers</h2><p data-block-key="bad3q">As we advance Veo 3, we’ve also added new capabilities to our popular Veo 2 model informed by our work with creators and <a href="https://youtu.be/Q4zqVrLI8a0">filmmakers</a>. Today, we’re launching several of these new capabilities, including:<br></p><ul><li data-block-key="4mc0b"><b>Our state-of-the-art reference powered video</b> capability allows you to give Veo images of characters, scenes, objects, and even styles for better creative control and consistency.</li><li data-block-key="emn6e"><b>Camera controls</b> help you define precise camera movements, including rotations, dollies and zooms, to achieve the perfect shot.</li><li data-block-key="eg4l4"><b>Outpainting</b> allows you to broaden your frame, turning your video from portrait to landscape, and making it easier to fit any screen size, intelligently adding to the scene.</li><li data-block-key="auro7"><b>Object add and remove</b> lets you add or erase objects from your videos. Veo understands scale, interactions, and shadows, and uses this understanding to create a natural, realistic-looking scene.</li></ul><p data-block-key="6heha">Reference powered video and camera controls are available now in <a href="http://flow.google/">Flow</a>. We're excited to bring all these <a href="http://deepmind.google/models/veo">new capabilities</a> to the <a href="https://cloud.google.com/blog/products/ai-machine-learning/announcing-veo-3-imagen-4-and-lyria-2-on-vertex-ai">Vertex AI API</a> in the coming weeks, and to more products over the next few months.</p></div>
  

  
    









<uni-image-carousel section-header="Fuel your creativity with new generative media  models and tools" images="[
    
      {
        
          &quot;src&quot;: [&quot; https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Veo2-CameraControls-Keyword-250519-r03.mp4 &quot;],
        
        &quot;alt&quot;: &quot;Veo 2 camera controls&quot;,
        &quot;isVideo&quot;: true,
        &quot;videoTitle&quot;: &quot;veo2controls&quot;
      },
    
      {
        
          &quot;src&quot;: [&quot; https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/comp_wm_us_genmedia_keyword_refernce2video_cap-000000-final.mp4 &quot;],
        
        &quot;alt&quot;: &quot;A woman walking in a hallway made with Veo2&quot;,
        &quot;isVideo&quot;: true,
        &quot;videoTitle&quot;: &quot;veo2&quot;
      },
    
      {
        
          &quot;src&quot;: [&quot; https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/us_wm_961d999b-860d-45d1-b9bd-6ca3ae960e9b_8fb2c1bc-fa94-433f-9426-96db830ca5c0.webm &quot;],
        
        &quot;alt&quot;: &quot;A knit scene made by Veo&quot;,
        &quot;isVideo&quot;: true,
        &quot;videoTitle&quot;: &quot;Knit veo&quot;
      },
    
      {
        
          &quot;src&quot;: [&quot; https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/us_wm_e36d3cd7-863b-4e52-aba1-966dbd062da0_36176986-0863-4322-a518-41d4251c66d1_.mp4 &quot;],
        
        &quot;alt&quot;: &quot;A knit scene made by Veo&quot;,
        &quot;isVideo&quot;: true,
        &quot;videoTitle&quot;: &quot;Outpainting&quot;
      },
    
      {
        
          &quot;src&quot;: [&quot; https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/us_wm_veo_2_792c9917-dcad-4272-bf7d-1ccf7ce1081d_bab3dce1-35d4-4c3d-b0f4-c6e819.webm &quot;],
        
        &quot;alt&quot;: &quot;Astronaut scene with Veo&quot;,
        &quot;isVideo&quot;: true,
        &quot;videoTitle&quot;: &quot;outer space&quot;
      },
    
      {
        
          &quot;src&quot;: [&quot; https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/us_wm_2b963087-a227-4984-b333-88eabb35553e_a46aa06d-c14f-4c85-a428-05332_QgLmCGP.mp4 &quot;],
        
        &quot;alt&quot;: &quot;Astronaut scene with Veo&quot;,
        &quot;isVideo&quot;: true,
        &quot;videoTitle&quot;: &quot;Image removal&quot;
      }
    
  ]">
  
    
  
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
</uni-image-carousel>

  

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
         }"><h2 data-block-key="zbevq">Flow: An AI filmmaking tool designed for Veo</h2><p data-block-key="p3q8">Built with and for creatives, <a href="https://blog.google/technology/ai/google-flow-veo-ai-filmmaking-tool/">Flow</a> is an AI filmmaking tool that lets you seamlessly create cinematic clips, scenes and stories by bringing together Google DeepMind’s most advanced models: Veo, Imagen and Gemini. Use natural language to describe your shots to Flow, manage the ingredients for your story — cast, locations, objects and styles — in a single convenient place, and use Flow to weave your narrative into beautiful scenes.</p><p data-block-key="aodop"><a href="http://flow.google/">Flow</a> is available today for Google AI Pro and Ultra plan subscribers in the U.S., with more countries coming soon.</p></div>
  

  
    
  
    


  <uni-youtube-player-article index="13" thumbnail-alt="Flow product sizzle video" video-id="A0VttaLy4sU" video-type="video" image="Flow_Sizzle Thumbnail 1280 x 720 (1)" video-image-url-lazy="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Flow_Sizzle_Thumbnail_1280_x_720_.width-100.format-webp.webp" video-image-url-mobile="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Flow_Sizzle_Thumbnail_1280_x_720_.width-700.format-webp.webp" video-image-url-desktop="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Flow_Sizzle_Thumbnail_1280_x_720.width-1000.format-webp.webp">
  </uni-youtube-player-article>


  


  

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
         }"><h2 data-block-key="zbevq">Imagen 4: Stunning quality and superior typography</h2><p data-block-key="1hinf">Our latest Imagen model combines speed with precision to create stunning images. Imagen 4 has remarkable clarity in fine details like intricate fabrics, water droplets, and animal fur, and excels in both photorealistic and abstract styles. Imagen 4 can create images in a range of aspect ratios and up to 2k resolution - even better for printing or presentations. It is also significantly better at spelling and typography, making it easier to create your own greeting cards, posters and even comics.</p></div>
  

  
    









<uni-image-carousel section-header="Fuel your creativity with new generative media  models and tools" images="[
    
      {
        
          
          
          &quot;src&quot;: [&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/wm_us_extra_batch_16_05_2015_.max-1536x1536.format-webp.webp&quot;,&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/wm_us_extra_batch_16_05_2015_.max-1536x1536.format-webp.webp&quot;],
        
        &quot;alt&quot;: &quot;Image of whale created by Imagen 4&quot;,
        &quot;isVideo&quot;: false,
        &quot;videoTitle&quot;: &quot;&quot;
      },
    
      {
        
          
          
          &quot;src&quot;: [&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_0_ComicCrocodile.max-1536x1536.format-webp.webp&quot;,&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_0_ComicCrocodile.max-1536x1536.format-webp.webp&quot;],
        
        &quot;alt&quot;: &quot;Comic strip created by Imagen 4&quot;,
        &quot;isVideo&quot;: false,
        &quot;videoTitle&quot;: &quot;&quot;
      },
    
      {
        
          
          
          &quot;src&quot;: [&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_47.max-1536x1536.format-webp.webp&quot;,&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_47.max-1536x1536.format-webp.webp&quot;],
        
        &quot;alt&quot;: &quot;Graphic created by Imagen 4&quot;,
        &quot;isVideo&quot;: false,
        &quot;videoTitle&quot;: &quot;&quot;
      },
    
      {
        
          
          
          &quot;src&quot;: [&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_14.max-1536x1536.format-webp.webp&quot;,&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_14.max-1536x1536.format-webp.webp&quot;],
        
        &quot;alt&quot;: &quot;Dog image created by Imagen 4&quot;,
        &quot;isVideo&quot;: false,
        &quot;videoTitle&quot;: &quot;&quot;
      },
    
      {
        
          
          
          &quot;src&quot;: [&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_79.max-1536x1536.format-webp.webp&quot;,&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_79.max-1536x1536.format-webp.webp&quot;],
        
        &quot;alt&quot;: &quot;Image of woman created by Imagen 4&quot;,
        &quot;isVideo&quot;: false,
        &quot;videoTitle&quot;: &quot;&quot;
      },
    
      {
        
          
          
          &quot;src&quot;: [&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_44.max-1536x1536.format-webp.webp&quot;,&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_44.max-1536x1536.format-webp.webp&quot;],
        
        &quot;alt&quot;: &quot;Lake painting created by Imagen 4&quot;,
        &quot;isVideo&quot;: false,
        &quot;videoTitle&quot;: &quot;&quot;
      },
    
      {
        
          
          
          &quot;src&quot;: [&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_57.max-1536x1536.format-webp.webp&quot;,&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_57.max-1536x1536.format-webp.webp&quot;],
        
        &quot;alt&quot;: &quot;Field photo created by Imagen 4&quot;,
        &quot;isVideo&quot;: false,
        &quot;videoTitle&quot;: &quot;&quot;
      },
    
      {
        
          
          
          &quot;src&quot;: [&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_7_Eggs.max-1536x1536.format-webp.webp&quot;,&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_7_Eggs.max-1536x1536.format-webp.webp&quot;],
        
        &quot;alt&quot;: &quot;Egg carton photo created by Imagen 4&quot;,
        &quot;isVideo&quot;: false,
        &quot;videoTitle&quot;: &quot;&quot;
      },
    
      {
        
          
          
          &quot;src&quot;: [&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_74.max-1536x1536.format-webp.webp&quot;,&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_74.max-1536x1536.format-webp.webp&quot;],
        
        &quot;alt&quot;: &quot;Knit scene created by Imagen 4&quot;,
        &quot;isVideo&quot;: false,
        &quot;videoTitle&quot;: &quot;&quot;
      },
    
      {
        
          
          
          &quot;src&quot;: [&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_1_ComicCat.max-1536x1536.format-webp.webp&quot;,&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_1_ComicCat.max-1536x1536.format-webp.webp&quot;],
        
        &quot;alt&quot;: &quot;Cat comic created by Imagen 4&quot;,
        &quot;isVideo&quot;: false,
        &quot;videoTitle&quot;: &quot;&quot;
      }
    
  ]">
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
</uni-image-carousel>

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
         }"><p data-block-key="zbevq">Imagen 4 is available today in the <a href="http://gemini.google.com/">Gemini app</a>, <a href="http://labs.google/whisk">Whisk</a>, <a href="https://cloud.google.com/blog/products/ai-machine-learning/announcing-veo-3-imagen-4-and-lyria-2-on-vertex-ai">Vertex AI</a> and across Slides, Vids, Docs and more in <a href="https://workspace.google.com/blog/product-announcements/new-ways-to-do-your-best-work">Workspace</a>.</p><p data-block-key="cu39m">Soon we’ll also be launching a fast variant of Imagen 4 that’s up to 10x faster than Imagen 3 — so you can explore ideas even faster.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
         }"><h2 data-block-key="cq60v">Lyria 2: Powerful composition and endless exploration</h2><p data-block-key="67lpr">In April, we <a href="https://deepmind.google/discover/blog/music-ai-sandbox-now-with-new-features-and-broader-access/">expanded access</a> to Music AI Sandbox, powered by <a href="https://deepmind.google/technologies/lyria">Lyria 2</a>. Music AI Sandbox offers musicians, producers and songwriters a set of experimental tools, which can spark new creative possibilities and help artists explore unique musical ideas. The expertise and valuable feedback from the music industry help us ensure our tools empower creators, while inviting creatives to realize the possibilities of AI in their art.</p><p data-block-key="8av0l">Lyria 2 brings powerful composition and endless exploration, and is now available for creators <a href="https://www.youtube.com/shorts/uQY6Z6O_dZQ">through YouTube Shorts</a> and enterprises in <a href="https://cloud.google.com/blog/products/ai-machine-learning/announcing-veo-3-imagen-4-and-lyria-2-on-vertex-ai">Vertex AI</a>. We've also made <a href="https://goo.gle/lyria-realtime">Lyria RealTime</a>, our interactive music generation model which powers <a href="https://labs.google/fx/tools/music-fx-dj">MusicFX DJ</a>, available via an <a href="https://ai.google.dev/gemini-api/docs/music-generation">API</a> and in <a href="https://aistudio.google.com/app/apps/bundled/midi-dj">AI Studio</a>. Lyria RealTime allows anyone to interactively create, control, and perform generative music in real time.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
         }"><h2 data-block-key="5n4ts">Responsible creation and collaboration with the creative community</h2><p data-block-key="4c2ru">Since launching in 2023, <a href="https://deepmind.google/technologies/synthid/">SynthID</a> has watermarked over 10 billion images, videos, audio files and texts, helping identify them as AI-generated and reduce the chances of misinformation and misattribution. Outputs generated by Veo 3, Imagen 4 and Lyria 2 will continue to have <a href="https://deepmind.google/technologies/synthid/">SynthID</a> watermarks.</p><p data-block-key="6k365">Today, we’re launching <a href="https://blog.google/technology/ai/google-synthid-ai-content-detector/">SynthID Detector</a>, a verification portal to help people identify AI-generated content. Upload a piece of content and the SynthID Detector will identify if either the entire file or just a part of it has SynthID in it.</p><p data-block-key="c84dh">With all our generative AI models, we aim to unleash human creativity and enable artists and creators to bring their ideas to life faster and more easily than ever before.</p></div>
  

  
    







<uni-related-content-tout title="I/O 2025" cta="See more" summary="We’re doing cutting-edge research to build the most helpful AI that’s more intelligent, agentic and personalized." hideimage="False" eyebrow="Collection" image-alt-text="Stylized 3D text &quot;IO25&quot; in vibrant, gradient colors on a white grid background." role="none" fullurl="https://blog.google/technology/developers/google-io-2025-collection/" pagetype="collectiondetailpage" isarticlepage="">
  
    <div slot="rct-image-slot">
      
    <figure>
        <picture>
            


    

    
        <source media="(max-resolution: 1.5dppx)" sizes="300px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_hero.width-300.format-webp.webp 300w">
    
        <source media="(min-resolution: 1.5dppx)" sizes="600px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_hero.width-600.format-webp.webp 600w">
    

    <img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_hero.width-600.format-webp.webp" alt="Stylized 3D text &quot;IO25&quot; in vibrant, gradient colors on a white grid background." sizes=" 300px,  600px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_hero.width-300.format-webp.webp 300w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_hero.width-600.format-webp.webp 600w" data-target="image" loading="lazy">
    


        </picture>
    </figure>


    </div>
  
</uni-related-content-tout>

  

  
    

  
    





<div role="form" data-component="uni-subscribe" data-analytics-module="{
    &quot;module_name&quot;: &quot;Newsletter&quot;,
    &quot;section_header&quot;: &quot;Get more stories from Google in your inbox.&quot;
  }">
    
    
    <div id="newsletter-form--form">
      <form data-method="POST" data-action="https://services.google.com/fb/submissions/thekeywordnewsletterprodv2/">
        <h2 id="subscribe_box_label">
          <span>Get more stories from Google in your inbox.</span>
          
        </h2>
        
      </form>
    </div>
    
    <div>
      <div>
        <p tabindex="-1" role="status" aria-live="off" aria-atomic="false">
          Done. Just one step more.
        </p>
        <p>
          Check your inbox to confirm your subscription.
        </p>
        <p>You are already subscribed to our newsletter.</p>
      </div>
      <p>
        You can also subscribe with a
        
        
      </p>
    </div>
  </div>

  

  


            
            

            
              




            
          </div>
        
      </div>
  </article></div>]]></description>
        </item>
    </channel>
</rss>