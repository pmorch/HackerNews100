<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 17 Dec 2024 21:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Inside the War Against Headlight Brightness (133 pts)]]></title>
            <link>https://www.theringer.com/2024/12/03/tech/headlight-brightness-cars-accidents</link>
            <guid>42443406</guid>
            <pubDate>Tue, 17 Dec 2024 17:43:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theringer.com/2024/12/03/tech/headlight-brightness-cars-accidents">https://www.theringer.com/2024/12/03/tech/headlight-brightness-cars-accidents</a>, See on <a href="https://news.ycombinator.com/item?id=42443406">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><figure data-sentry-component="ArticleHero" data-sentry-source-file="article-hero.tsx"><figcaption><span>Getty Images/Ringer illustration</span></figcaption></figure><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">The sun had already set in Newfoundland, Canada, and Paul Gatto was working late to give me a presentation on headlights. This, it should be said, is not his job. Not even close, really. Gatto, 28, is a front-end developer by day, working for a weather application that’s used by the majority of Canadian meteorologists, he told me on a video call, occasionally hitting his e-cig or sipping on a Miller Lite. As to how he ended up as one of the primary forces in the movement to make car headlights less bright—a movement that’s become surprisingly robust in recent years—even Gatto can’t really explain.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">“It is fucking weird,” he said. “I need something else to do with my spare time. This takes a lot of it.”</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Gatto is the founder of the subreddit <a href="http://reddit.com/r/fuckyourheadlights/">r/FuckYourHeadlights</a>, the internet’s central hub for those at their wits’ end with the current state of headlights. The posts consist of a mishmash of venting, meme-ing, and community organizing. A common entry is a photo taken from inside the car of someone being blasted with headlights as bright as an atomic bomb, and a caption along the lines of <a href="http://reddit.com/r/fuckyourheadlights/comments/10s3rmy/how_is_this_fucking_legal/">“How is this fucking legal?!”</a> Or users will joke about going back in time and <a href="http://reddit.com/r/fuckyourheadlights/comments/1d4nthv/whats_the_poblem_with_their_headlights/">Skynet-style</a> killing the Audi lighting engineer who first rolled out LED headlights. Or they’ll discuss ways to write to their congresspeople, like Mike Thompson, House Democrat of California, who <a href="http://reddit.com/r/fuckyourheadlights/comments/1axtnfy/congress_gets_involved/">recently expressed support</a> for the cause.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">At its worst, the forum resembles Grampa Simpson writing a letter to the president to say that there are <a href="https://www.youtube.com/watch?v=O5dmxBUbzBU">too many states</a>. (“I am <em>not</em> a crackpot,” Grampa adds.) But at its best, it feels like a balm for those of us who have been pushed to optical-induced madness on the roads. For those who feel like, as headlights get brighter, it’s actually becoming harder to see.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">After I <a href="https://x.com/Nate_Rgrs/status/1662241673043116036">posted</a> about the subreddit on X last year, Gatto emailed me, and it wasn’t long before we were on a video call and he was giving a PowerPoint presentation he’d made the night before. It was titled “The Internet-Focused History of FuckYourHeadlights, or: How I Learned to Stop Worrying About Headlights and Start Worrying About Society Itself.”</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">“First, I’d like to teach you some words,” Gatto said, priming me for a conversation about the minutiae of government policy and auto engineering. There was “obfuscation” (the intentional act of making something hard to understand), “mutual recursion” (when two things are defined in terms of each other, creating a loop), and “Ouroboros” (the snake eating its tail). Around this point, Victor Morgan, who was also on the call, cut in. “Paul,” he said, “where are you going with this, bud?”&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Morgan does this kind of thing from time to time. The two are partners—moderating the subreddit together and working in tandem to research headlight arcana—but it’s a good cop–bad cop dynamic. Neatly dressed and carefully spoken, Morgan, 41, is a mechanical engineer and lives far from Gatto, in Greenville, South Carolina. One of the first times the two spoke, it was because Morgan wanted to tell Gatto he didn’t like the subreddit’s name. They’ve since become good friends.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Gatto is the one focused on the bigger picture: digging up publicly available information, refining the messaging of their cause, optimizing the subreddit for search engines (he’s <a href="https://x.com/BarneyRetina/status/1836228891175784570">proud</a> of how high up the subreddit appears if you google “bright headlights”). Morgan is the one who sees their ideas through with measurements and engineering: He’s created a device for your rear windshield that pops up a reflective surface when someone’s lights are glaring at you from behind (<a href="https://www.owmyeyes.com/products/owmyeyes-light-the-road-not-my-face">$70, homemade</a>, questionably legal), and he’s gone to car dealerships to measure the brightness of different headlights. (According to Morgan’s numbers, Teslas and Hondas are among the brightest.)</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">If you want Rust Cohle–worthy lines about how it’s all connected, you go to Gatto. (“The automakers and the insurers can all be classified as the same financial entity back at this point,” he told me. “They will doctor what they need to, as far as I can see.”) If you want an engineer-worthy dose of hard-nosed practicality, you go to Morgan. (He has a cat whom he just calls “Cat.”) They’re an unlikely pair, but they are <em>not</em> crackpots.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Headlight brightness might almost seem like too random a subject for anyone to be as invested in as they are. But as it turns out, that’s kind of the point. “There’s a lot of issues that I care about,” Morgan said. “This is one that I think is niche enough that I can have an influence on.” Gatto’s motivation comes from the experience of watching his partner, Liz, struggle to recover from being hit by a cab while walking across the street. He sees headlights as “a realistic and tangible attack surface on the current trend toward antihuman design in our world, primarily guided by the auto industry.”</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">But deep down, what motivates them is the same twitch in the eye that brought me to the subreddit in the first place. “I’m not a very rageful person,” Gatto said, “but for some reason, these lights brought it out of me. And I kind of realized that’s why I had to do something about it. Because no one’s going to come help us.”</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">There appear to be two types of drivers in North America these days: those who think about headlights only when one of theirs goes out, and those who fixate on them every time they drive at night. If you’re in the first camp, consider yourself lucky. Those in the second camp—aggravated by the excess glare produced in this new era of light-emitting diode headlights—are riled up enough that the National Highway Traffic Safety Administration receives more consumer complaints about headlights than any other topic, several insiders told me.&nbsp;</p><figure data-sentry-component="Component" data-sentry-source-file="image.tsx"><div data-radix-aspect-ratio-wrapper=""><p><img alt="" data-sentry-element="Image" data-sentry-source-file="image.tsx" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphB_02-1-scaled.jpg&amp;w=640&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 640w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphB_02-1-scaled.jpg&amp;w=750&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 750w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphB_02-1-scaled.jpg&amp;w=828&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 828w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphB_02-1-scaled.jpg&amp;w=1080&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 1080w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphB_02-1-scaled.jpg&amp;w=1200&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 1200w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphB_02-1-scaled.jpg&amp;w=1920&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 1920w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphB_02-1-scaled.jpg&amp;w=2048&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 2048w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphB_02-1-scaled.jpg&amp;w=3840&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 3840w" src="https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphB_02-1-scaled.jpg&amp;w=3840&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30"></p></div><figure><figcaption><br>According to numbers we pulled from the Insurance Institute for Highway Safety’s publicly available data,&nbsp;headlight brightness has roughly doubled since 2015.</figcaption></figure></figure><figure data-sentry-component="Component" data-sentry-source-file="image.tsx"><div data-radix-aspect-ratio-wrapper=""><p><img alt="" data-sentry-element="Image" data-sentry-source-file="image.tsx" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphA_03-scaled.jpg&amp;w=640&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 640w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphA_03-scaled.jpg&amp;w=750&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 750w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphA_03-scaled.jpg&amp;w=828&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 828w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphA_03-scaled.jpg&amp;w=1080&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 1080w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphA_03-scaled.jpg&amp;w=1200&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 1200w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphA_03-scaled.jpg&amp;w=1920&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 1920w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphA_03-scaled.jpg&amp;w=2048&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 2048w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphA_03-scaled.jpg&amp;w=3840&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 3840w" src="https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphA_03-scaled.jpg&amp;w=3840&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30"></p></div><figure><figcaption><br>And at the same time, the number of demerits issued for lack of brightness have dropped precipitously. In other words: It’s not just your imagination. Headlights are brighter.</figcaption></figure></figure><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">It’s not just in the aggrieved drivers’ imaginations. Going by the publicly available data of the&nbsp;Insurance Institute for Highway Safety, headlight brightness has roughly doubled in the past 10 years—although you probably don’t need convincing if you’ve been paying attention over that span. Something <em>happened </em>out there, and a zap of light causing you to grimace behind the wheel suddenly went from a rarity to a routine occurrence.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">As opposed to the sepia-toned halogen lights we had mostly been using for generations of vehicles, LED lights—which are now used for the vast majority of new cars—come out blazing white or blue, like an omnipresent police flashlight shining at you during a traffic stop. And in what can be seen as a flawed attempt to match LED capabilities on other vehicles, it’s also become not uncommon, anecdotally speaking, for people to have their high beams on even on crowded highways and streets—something that’s technically illegal because it’s deemed to be a danger to other drivers. The strange thing is, though, I can’t say I notice much of a difference between one car’s high beams and another’s low beams anymore.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">John D. Bullough, a program director at the Mount Sinai Light and Health Research Center, has been studying light and its ramifications on health for the past 30 years, and he’ll tell you that the difference between older light sources and LEDs is night and day. “The first 20 years or so, [lighting] technology was very stable,” he told me over the phone. “In other words, you had incandescent light bulbs at home, and you had fluorescent lights in your office and sodium lights on the streets, and nothing really changed very much. And then, suddenly, the last 10 years of that 30 years have been a whirlwind because of LEDs. I mean, they’ve changed everything. Everything is LED. There essentially aren’t light bulbs anymore.”</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">LEDs offer benefits over older light sources, which is why they’ve been fast-tracked into near-universal use <a href="https://www.vox.com/science/2023/8/12/23827110/light-bulb-incandescent-led-energy-efficiency-ban-explained">via government-mandated performance standards</a>. They last longer and require less energy, making them more environmentally friendly, and are more customizable, making them suited for endless purposes. But LEDs are fundamentally different from what came before them—the light can be carefully aimed as opposed to emitted in all directions—and they’re vastly more powerful. And as they were rolled out en masse at a rapid pace, any <a href="https://nymag.com/strategist/article/led-light-bulbs-investigation.html">potential repercussions</a> would have to be discovered on the fly.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">“We’re all like human experiments,” said Mark Baker, when I called him to talk about his nonprofit, the Soft Lights Foundation, the mission of which is to advocate “for the protection of people and the environment from the harms of visible light radiation emitted by products that use light-emitting diodes.” Baker’s concern is with the broader integration of LEDs in society, but he shows up regularly in the headlight world, having recently organized a <a href="https://www.softlights.org/wp-content/uploads/2024/03/NHTSA-Petition-to-Limit-Headlight-Intensity.pdf">petition</a> that gathered nearly 60,000 signatures demanding that NHTSA limit headlight intensity. Unlike Morgan and Gatto, however, this isn’t a nights and weekends gig for him.&nbsp;&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Baker was working as a middle school math teacher in Northern California and thought of himself “as a regular person” before the mass implementation of LEDs. Then the world shifted while he was in traffic one day around 2016. He remembers looking at a Cadillac that had daytime-running LEDs—the non-primary headlights that run at all times on many modern vehicles—and “when my brain saw this light, I couldn’t look away,” he said. “Even though it was intense, I was drawn to it, and I started to feel a presence I’ve never felt, like evil.” As LEDs became more common, Baker became overwhelmed and had a mental breakdown, ending up in the hospital. He was diagnosed with mild autism spectrum disorder—which he says explains his hyper-fixation on bright lights—and couldn’t go back to work because of the LEDs in his classroom.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">When I spoke to Baker, 59, he was living in a sparsely lit rural area that he moved to with his partner, where he had been focusing on the Soft Lights Foundation since founding it in 2021. Baker told me he’s heard from a variety of people with various diagnoses—epilepsy, photophobia, migraines, lupus, autism—who struggle with LEDs. “I’ve got a guy that calls me from time to time wanting to commit suicide because of these LED lights in the Blue Ridge [Mountains],” he said. “We know that individuals are highly individualized. Each of us is going to react differently.”</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Baker is among a group of people who feel that NHTSA, which is responsible for regulating automotive safety, should have adjusted the rule book to accommodate LEDs before they were allowed in new cars. This was how NHTSA approached previous fundamental changes to headlight technology, Baker points out. As headlights went from circular to rectangular and from sealed beams to replaceable bulbs, the rules and accommodations changed with them. “Well, when LED headlights came out,” Baker said, “they skipped all that. They just started selling cars with the LED headlights.”&nbsp;</p><figure data-sentry-component="Component" data-sentry-source-file="image.tsx"><div data-radix-aspect-ratio-wrapper=""><p><img alt="" data-sentry-element="Image" data-sentry-source-file="image.tsx" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FOne-day-energy-conservation-in-Ukraine-following-damaged-infrastructure-by-Russian-attacks-scaled.jpg&amp;w=640&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 640w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FOne-day-energy-conservation-in-Ukraine-following-damaged-infrastructure-by-Russian-attacks-scaled.jpg&amp;w=750&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 750w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FOne-day-energy-conservation-in-Ukraine-following-damaged-infrastructure-by-Russian-attacks-scaled.jpg&amp;w=828&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 828w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FOne-day-energy-conservation-in-Ukraine-following-damaged-infrastructure-by-Russian-attacks-scaled.jpg&amp;w=1080&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 1080w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FOne-day-energy-conservation-in-Ukraine-following-damaged-infrastructure-by-Russian-attacks-scaled.jpg&amp;w=1200&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 1200w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FOne-day-energy-conservation-in-Ukraine-following-damaged-infrastructure-by-Russian-attacks-scaled.jpg&amp;w=1920&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 1920w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FOne-day-energy-conservation-in-Ukraine-following-damaged-infrastructure-by-Russian-attacks-scaled.jpg&amp;w=2048&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 2048w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FOne-day-energy-conservation-in-Ukraine-following-damaged-infrastructure-by-Russian-attacks-scaled.jpg&amp;w=3840&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 3840w" src="https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FOne-day-energy-conservation-in-Ukraine-following-damaged-infrastructure-by-Russian-attacks-scaled.jpg&amp;w=3840&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30"></p></div><figure><figcaption>The differences between blue-toned and sepia-toned headlights can be seen on a darkened street in Kyiv, Ukraine.</figcaption></figure><figcaption>Metin Aktas/Anadolu Agency via Getty Images</figcaption></figure><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">It is certainly true that Federal Motor Vehicle Safety Standard No. 108—NHTSA’s regulation covering all forms of vehicle lighting, conceived in 1968—has not been adjusted to create new restrictions for LEDs. That’s easy to tell because the requirements in the standard haven’t been adjusted <em>at all</em> since 1986. One person I spoke to, who previously worked at NHTSA and discussed their former employer on the condition of anonymity, described 108 as being among the biggest entries in the book, yet also “probably one of the least updated.” (In response to an interview request for this story, NHTSA asked that questions be emailed; in response to the questions, NHTSA provided a broad statement that did not answer specific questions, such as ones about the nature of updating 108. “Although headlighting technology has changed over the years,” the statement read, “NHTSA’s lighting standard has remained constant in limiting the amount of glaring light directed toward oncoming and preceding traffic.”)</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Several people I spoke to insisted that the main issue with 108’s guidelines when applied to LEDs is that there’s no maximum brightness for certain areas of a headlight. The guidelines set limits on areas of bulb emissions that tend to cause glare problems for other drivers, but those areas were determined based on the light output of older bulbs. LEDs’ output and maneuverability changed the game. Think of an LED headlight more like a pixelated television or computer screen rather than a light bulb. Because of that design, the luminosity of precise areas of the headlight can be limited while the overall brightness is pushed up, up, up.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Chris Trechter, a lighting-focused engineer who used to work for Magna International, the largest automobile parts manufacturer in North America, told me the company would adhere to 108 in making headlights for clients like General Motors but that the rule is “archaic.” “It does not account for LEDs,” he said, “and there are giant loopholes that allow you to throw basically unlimited light as long as you meet all the other aspects of 108.”</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">On a recent episode of the<em> Carmudgeon Show</em> podcast, auto journalist Jason Cammisa <a href="https://youtu.be/MkwjMV2of_8?t=697">described</a> a phenomenon occurring with some LED headlights in which there are observable minor spots of dimness among an otherwise bright field of light. “With complex arrays of LEDs and of optics,” he said, “car companies realized they can engineer in a dark spot where it’s being measured, but the rest of the field is vastly over-illuminated. And I’ve had now two car companies’ engineers, when I played stupid and said, ‘What’s the dark spot?’ … And the lighting engineers are all fucking proud of themselves: ‘That’s where they measure the fucking thing!’ And I’m like, ‘You assholes, you’re the reason that every fucking new car is blinding the shit out of everyone.’”&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Cammisa, who did not respond to an interview request, compared this situation to the massive dieselgate scandal, in which Volkswagen was caught rigging emissions tests. (Fines levied against VW ended up costing the company <a href="https://www.reuters.com/article/legal/vws-dieselgate-bill-hits-30-bln-after-another-charge-idUSKCN1C4270/">around $30 billion</a>.) To Cammisa, deliberately darkening specific areas of LEDs to bypass the testing system demonstrates a craven approach similar to cheating on emissions tests. He’s called it lightinggate. Or headlightgate. He’s workshopping the name.&nbsp;</p><figure data-sentry-component="Component" data-sentry-source-file="image.tsx"><div data-radix-aspect-ratio-wrapper=""><p><img alt="" data-sentry-element="Image" data-sentry-source-file="image.tsx" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FMercedes-Headlight-Ad-2016.png&amp;w=640&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 640w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FMercedes-Headlight-Ad-2016.png&amp;w=750&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 750w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FMercedes-Headlight-Ad-2016.png&amp;w=828&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 828w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FMercedes-Headlight-Ad-2016.png&amp;w=1080&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 1080w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FMercedes-Headlight-Ad-2016.png&amp;w=1200&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 1200w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FMercedes-Headlight-Ad-2016.png&amp;w=1920&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 1920w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FMercedes-Headlight-Ad-2016.png&amp;w=2048&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 2048w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FMercedes-Headlight-Ad-2016.png&amp;w=3840&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 3840w" src="https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FMercedes-Headlight-Ad-2016.png&amp;w=3840&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30"></p></div><figure><figcaption>This 2016 Mercedes-Benz ad campaign touting the power of the carmaker’s headlights has become notorious among anti-brightness activists.</figcaption></figure><figcaption>Mercedes-Benz</figcaption></figure><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">I reached out to over a dozen car companies for this story, and only one provided an interview: Audi. In subsequently talking to Audi spokesperson Mark Dahnke, I relayed Cammisa’s description of how LED headlights were being deceptively engineered, and Dahnke told me he was “not aware of anything like that.” But Trechter, the former auto lighting engineer, told me Cammisa’s account was “100 percent real.”&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">So why go through all this trouble just for more light? Cammisa believes it’s an <a href="https://youtu.be/MkwjMV2of_8?t=799">“arms race”</a>—part of a battle between automakers to make their cars seem as state of the art as possible. “‘I get into my <a href="https://lucidmotors.com/air">Lucid Air</a>, and I have great lighting in every direction,’” he said, taking on the theoretical perspective of a new customer. “‘Oh, my headlights are <em>great</em>!’” Consider it something like <a href="https://en.wikipedia.org/wiki/Loudness_war">the loudness war</a> in music, in which albums were being mixed at increasing volumes, particularly in the 2000s, to make the music more attention grabbing, at the cost of quality. The brightness war, if you will.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Another possible explanation comes via a nongovernmental entity that has become a major force in the automotive world: the IIHS, a nonprofit funded by insurance companies with the goal of limiting loss. In the absence of any real NHTSA presence in the modern headlight conversation, the IIHS’s headlight safety rating has become a North Star for auto manufacturers, ostensibly for advertising reasons.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">“It made our life hell to try and make light that bright,” said Trechter. “But GM wanted it for that safety rating, and that’s it.” According to Trechter, the way to get the best safety rating is to exploit the lack of intensity limits on certain areas of the headlight and make the light shine as far down the road as possible. “They would push us to get as much down-road punch as they could get,” he said, in reference to the automakers.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Matthew Brumbelow, a senior research engineer at the IIHS, explained the headlight safety rating in a more nuanced way. While “you can’t get a good rating and have super high glare,” he said, “you also can’t get a good rating and have a dim headlight that doesn’t glare anyone but also is too dim to help people avoid crashes on the road.” Brumbelow said both factors play into their rating but admitted it’s the brightness that they weigh more heavily. “That’s what we’ve seen,” he said. “A huge reduction in crash rates based on more light reaching the road.”&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">The “brighter is better” mentality is the ultimate thorn in the side of activists like Gatto and Morgan, who view it almost as an insult to their intelligence. Brumbelow pointed to a <a href="https://www.iihs.org/news/detail/good-iihs-headlight-ratings-linked-to-lower-crash-rates#:~:text=Controlling%20for%20differences%20in%20miles%20traveled%2C%20driver%2Drelated,single%2Dvehicle%20crash%20rate%2C%20compared%20with%20poor%2Drated%20ones.">2021 IIHS study</a> that demonstrated a 19 percent reduction in nighttime single-vehicle crashes for cars with good headlight safety ratings, but Morgan sees a major issue with that study. “Basically,” Morgan said, “it means that assholes with bright headlights are in less single-car accidents than the people that they blind.”</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">One logical solution to this debate would be to factor in crashes that have occurred due to excessive glare, but this has proved to be a challenge out of the reach of the IIHS and other headlight experts. If a car is in an accident due to another car’s excessive glare, for instance, the offending vehicle that caused the situation would likely never know it and just keep driving.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">The inability to clinically prove the dangers of headlight glare is at the heart of the issue. “You cannot mandate or prohibit something in the U.S. system without meeting a stringent cost-benefit requirement,” said Daniel Stern, chief editor of Driving Vision News and one of the foremost authorities in headlights and headlight policy.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Stern told me about a conversation he once had in the late 2000s with Richard Van Iderstine, then a recently retired automotive safety standards engineer at NHTSA with great rulemaking authority over headlights. Stern and Van Iderstine had met for lunch and continued talking into dinner, duking it out over the finer points of headlight philosophy. Two giants of the headlight world, going head to head. Stern remembers Van Iderstine telling him, “Look, I’m not stupid. I know what good lighting is, and I know what lousy lighting is. … Does glare cause crashes? Of course glare causes crashes. But I couldn’t prove it.” (Van Iderstine could not be reached for comment.)</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Finding a way to empirically measure the danger that glare presents to drivers is one problem. Another is that people can’t seem to agree on what the <em>cause</em> of excessive glare actually is. Stern, for his part, noted that the situation is “very, very complex” and “doesn’t have an answer that sounds like yes or no or 42.” That said, he told me he did recently finish a 38,000-word report on headlight glare for a non-U.S. government (he could not say which one) and is adamant that the biggest factor “by far” is headlight alignment—which is to say, quite literally, how accurately one’s headlights are pointed.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">This is the second-biggest thorn in Gatto and Morgan’s side. Headlight aim is important—no one is denying that. If any headlight, no matter how bright, is misaligned even slightly upward, it will cause glare for other drivers—and many headlights are poorly aligned out of the factory or become misaligned after an accident or installation error. But to someone like Morgan, alignment is only one part of the equation when headlights have become as intense as they have. “It’s a half-truth,” Morgan said, “without really saying that there’s other solutions here and that if you didn’t have laser-beam headlights, this issue would be less severe.” In his PowerPoint presentation, Gatto had drawn a penis on an image of Stern’s head. “That just kind of appeared there, sorry,” Gatto said.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Headlight alignment is certainly Stern’s primary foe—and he is not a fan of the likes of Baker and Morgan, either, whom he described in an email as having a fervor that “greatly outstrips their topical expertise and understanding, and likely qualifies them for IRS classification as religious entities.” But Stern does list other causes of glare as well. There’s headlight <em>condition</em> (a dirty lens can cause the light to refract in chaotic ways), headlight <em>size</em> (as LEDs become smaller, the “density” of the light increases, making it more intense), and headlight <em>color </em>(the eye is more sensitive to blue and white light, making glare seem more significant in modern LEDs, which are generally made in that color field).&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Vehicle size is another issue that comes up regularly, since NHTSA regulations for headlights don’t include a standardized mounting height, even as cars have <a href="https://www.vox.com/future-perfect/24139147/suvs-trucks-popularity-federal-policy-pollution">ballooned in size</a> in recent years. This means a perfectly aligned headlight in a larger car can still wreak havoc on a smaller car: “Where the [midsize] Civic might not give you glare,” Trechter, the former lighting engineer, said, “that F-350 [truck], if you’re sitting in a [sport-size] Miata, is gonna absolutely wreck your eyeballs.”</p><figure data-sentry-component="Component" data-sentry-source-file="image.tsx"><div data-radix-aspect-ratio-wrapper=""><p><img alt="" data-sentry-element="Image" data-sentry-source-file="image.tsx" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FOsram-scaled.jpg&amp;w=640&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 640w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FOsram-scaled.jpg&amp;w=750&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 750w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FOsram-scaled.jpg&amp;w=828&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 828w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FOsram-scaled.jpg&amp;w=1080&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 1080w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FOsram-scaled.jpg&amp;w=1200&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 1200w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FOsram-scaled.jpg&amp;w=1920&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 1920w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FOsram-scaled.jpg&amp;w=2048&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 2048w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FOsram-scaled.jpg&amp;w=3840&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 3840w" src="https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FOsram-scaled.jpg&amp;w=3840&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30"></p></div><figure><figcaption>LED headlight bulbs can be seen in a showroom in Osram's headquarters.</figcaption></figure><figcaption>Matthias Balk/picture alliance via Getty Images</figcaption></figure><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">In NHTSA’s brief statement to me, the only issue the organization brought up directly was aftermarket “LED conversion kits”—when headlight systems designed for halogen lights are swapped with LEDs—which it said are a “major contributor to excessive glare.” Due to the lack of data on what causes glare incidents on the road, it’s difficult to guess how much of a factor aftermarket LEDs are, but one thing’s for sure: They are illegal.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">By pointing to aftermarket LEDs, then, NHTSA essentially passes the buck to law enforcement. This is, perhaps not coincidentally, at the same time that traffic violations nationwide are in substantial decline; <a href="https://www.nytimes.com/interactive/2024/07/29/upshot/traffic-enforcement-dwindled.html">a <em>New York Times</em> study</a> reported a more than 50 percent reduction in traffic stops in many U.S. cities since the pandemic.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">The result of the hydra-headed headlight problem appears to be general paralysis; with numerous routes available to potentially improve the situation, none are really being pursued by automakers or regulators. “In engineering,” Gatto told me, “we talk about fail state. What happens when [a system] fails? Does it fail well, or does it fail horribly?” He added, “If you’re planning for multiple of these fail states all the time, maybe consider what the true impact of this is. … You have to kind of go against the whole ethos in order to actually evaluate: Are these too bright?”</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">There is one path that certain automakers would like to take to improve headlights—but first it has to make it through NHTSA regulations. Adaptive driving beam technology takes advantage of LED capabilities to adjust the spray of light away from other cars and objects while maintaining high-capacity light on the road. When I got on the phone with the Audi representative, Mark Dahnke, to discuss headlight brightness, it quickly became clear that he was mainly taking the call in the interest of hyping ADB.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">“It basically puts other drivers in the shadow of your full high-beam light,” Dahnke explained, “such that they are not blinded while providing not just you with more light but everyone with more light.” Dahnke pointed out that ADB has been rolled out in Europe for some time now and is not in the U.S. solely due to NHTSA restrictions. In February 2022, NHTSA belatedly published a rule to allow ADB, but because of the contradictory way the rule is written, no car company has yet been able to find an engineering formula to make it work. “We’re now three different generations of lighting behind the rest of the world in the U.S.,” Dahnke said, exasperated.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Gatto and Morgan, unsurprisingly, are not fans of ADB. They point out that with the way the technology currently works, the light being strategically dimmed is the <em>high beam</em>, meaning that the low beam will stay as bright as ever. “The adaptive high beam is only going to put more light on the road,” Morgan said, adding that people from Europe post in the subreddit about hating the headlight situation there, too. (In a <a href="https://unece.org/sites/default/files/2024-04/GRE-90-20e-reduced.pdf">2024 European study</a> conducted by the Federation Internationale de l’Automobile, 81 percent of respondents said headlight glare needed to be reduced via regulation.) ADB, Morgan insisted, is a corporate over-solution to a simple problem. “That’s their panacea,” he said, “and it’s baloney.”</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">In the course of automotive history, technology has saved countless lives, taking car accidents from a peak of 30.8 deaths per 100,000 people in 1937 to 13.8 per 100,000 in 2022—a 55 percent improvement. That makes it difficult to dismiss the idea that something <em>like</em> ADB could be the solution to the headlight problem. But Gatto feels that he’s dealing with a “philosophical enemy” in the techno-optimist idea that we can innovate our way out of this. “They want to insert this technology to have a maximum amount of trust in it,” he said, “and they believe that any bit of convenience outweighs any drawback they could get from it.”</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Paris Marx, a tech critic who focuses on transportation and hosts the <em>Tech Won’t Save Us </em>podcast, is all too familiar with the trickiness of this conversation. LED headlight technology, when viewed through the same lens as evolving tech like self-driving cars, presents a vision of a world in which corporations are asking for perpetual forgiveness rather than permission.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">“In general,” Marx told me, “we think about the positive outcomes that can be the result of adopting these things, but then thinking about the negative impacts only comes quite a bit later.” Assessing the notion that something as simple as a headlight could get this out of control, he said, “As usual, we’re trying to play catch-up and figure out how we’re going to address the problems that have been created by this thing that we thought was a solution.”</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">The most compelling argument I heard in defense of brighter lights is that, while glare is clearly a hazard, it may not be as <em>much</em> of a hazard as limited vision on the road. That is to say, brighter headlights—which could illuminate something like a deer on a dark, rural highway from farther away than ever before—may be preventing more accidents than they cause by shining in other drivers’ eyes. You’ll notice every overly bright light searing your brain, but you likely won’t really appreciate the accident you never had.</p><figure data-sentry-component="Component" data-sentry-source-file="image.tsx"><div data-radix-aspect-ratio-wrapper=""><p><img alt="" data-sentry-element="Image" data-sentry-source-file="image.tsx" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphC_02-scaled.webp&amp;w=640&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 640w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphC_02-scaled.webp&amp;w=750&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 750w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphC_02-scaled.webp&amp;w=828&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 828w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphC_02-scaled.webp&amp;w=1080&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 1080w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphC_02-scaled.webp&amp;w=1200&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 1200w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphC_02-scaled.webp&amp;w=1920&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 1920w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphC_02-scaled.webp&amp;w=2048&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 2048w, https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphC_02-scaled.webp&amp;w=3840&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30 3840w" src="https://www.theringer.com/_next/image?url=https%3A%2F%2Fwp.theringer.com%2Fwp-content%2Fuploads%2F2024%2F12%2FHeadlightsGraphC_02-scaled.webp&amp;w=3840&amp;q=75&amp;dpl=681dab30aca047401211554865665910a9df7e30"></p></div><figure><figcaption><p>Despite the increase in headlight brightness, data from the National Highway Traffic Safety Administration shows that fatal crashes in dark conditions remain relatively stable. In fact, crashes on lighted roads have risen slightly over the past two decades. It raises the question: Is all this extra brightness actually doing anything?</p>
</figcaption></figure><figcaption>NHTSA</figcaption></figure><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">“Is it a problem for people, including myself,” said Brumbelow, the IIHS engineer, “to be on the road and have bright headlights in my eyes and feel like I can’t see? Yes, that’s a problem. But the bigger follow-up question is: Is that my feeling, or is it a reality that I’m at an increased risk of crashing? And that’s really what we’re talking about.”</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Still, without concrete numbers indicating one way or another, this argument remains philosophical, and the stalemate continues. Morgan, anyway, is eager to figure this out from an engineering perspective, and he has the pie-in-the-sky idea of setting up some kind of nighttime traffic checkpoint, perhaps with the assistance of police, to assess high-glare events in everyday vehicles. “These are surmountable problems,” he said. “This is not like trying to determine divine intervention. These are problems made by men. We can figure this out. It just requires a little bit of effort.”</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">But he and Gatto know what they think the solution to all this is, and it’s admittedly the simplest proposal anyone has provided: Make the lights less bright. That’s it. “I’m just at: There’s too much glare in my eyes,” Morgan said, “and I want it to stop.”</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">At a certain point, I found myself more confused than when I started. There was name calling and corporate-interest lobbying and statistic volleying—often all at the same time. I needed a voice of reason. So I reached out to Ray Magliozzi.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Ray and his brother, Tom Magliozzi, hosted the radio show <em>Car Talk</em> for 35 years, from 1977 to 2012, serving as gurus of the car world and seasoning their advice with appropriately saucy New England–style banter and good humor. Tom died in 2014, but Ray still runs an auto mechanic shop in Cambridge and answers car questions via a blog. I decided to call the question line and ask Ray if he had any wisdom to share about headlights, not really expecting to get a reply.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">A few days later my phone started buzzing with a call from a New England area code, and there he was: the wonderful voice from the radio. I told Ray I was shocked to be talking to him, and he didn’t miss a beat: “You called <em>me</em>!”</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Magliozzi, 75, is actually fine with the brightness of modern headlights—when they work. Alignment, he noted, is extra important with lights like these, and he abhors the aftermarket LEDs and light bars that people attach to their cars in case “they’re going to encounter a moose or something.” But more than anything else, what bothers Magliozzi about the state of headlights is the increasing number of people who drive with high beams on all the time at night. “I think it’s selfishness to a large degree,” Magliozzi said.</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">I had talked about this with many people in the headlight sphere: the possibility that the power of LEDs, supposed to be a bastion of safety, was actually contributing to an “I’ve got mine” mentality on the roads, which has become largely identifiable across the driving realm. It’s impossible to prove a causal relationship, but the rise of LEDs has directly coincided with <a href="https://www.nytimes.com/2024/01/10/magazine/dangerous-driving.html">the rise of reckless driving and road rage, as well as a new surge of fatal accidents</a>. “It seems like there’s an anti-collectivist vibe,” was how Gatto put it. “It’s a behavior that’s emerging.”</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">Some may genuinely not know they’re driving with their high beams on—digital car dashboards have gotten more complex and less intuitive in recent years—but it’s difficult to believe that most permanent high-beam users aren’t doing it deliberately, in an anti-collectivist kind of way. Magliozzi was bummed out about it. “When I learned to drive, 1,001 years ago,” he said, “we were taught to be courteous and polite. And I think we need to get back there.” Maybe that return toward courteousness has to start with automakers and regulatory bodies, however they decide to move forward with the LED headlight dilemma.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">The first time I heard Baker, the founder of the Soft Lights Foundation, tell his story about having to essentially drop out of society, I suddenly felt guilty. I may be the type to squint angrily at headlight glare, but at the end of the day, I install the light bulbs available to me and move on with my life. For certain people, it’s clearly a much worse relationship.&nbsp;</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">I asked Baker what kind of lighting he uses in his day-to-day, and he said he does use LEDs—it’s almost impossible not to at this point—and that he’s found some he’s happy with, like gentle, amber-colored, 300-lumen bulbs for the bedroom. That’s the beauty of LED lights: They can be made for any design or purpose. Sometimes they just have to be adjusted until they’re right.</p><p data-sentry-component="Component" data-sentry-source-file="paragraph.tsx">“But my favorite light is one that I replaced over the dining room table,” Baker said. His partner bought a chandelier-style fixture, and they put an incandescent bulb in it from a stockpiled supply. “It’s great,” Baker said. “I will go over and turn the switch on just to enjoy that light.”</p><div data-sentry-component="SingleCreator" data-sentry-source-file="article-creator-block.tsx"><div><p><a data-sentry-element="Link" data-sentry-source-file="creator.tsx" href="https://www.theringer.com/creator/nate-rogers"><img alt="" data-sentry-element="Image" data-sentry-source-file="creator.tsx" loading="lazy" decoding="async" data-nimg="fill" src="https://www.theringer.com/avatar-dark.svg"></a></p></div><div><a data-sentry-element="Link" data-sentry-source-file="creator.tsx" href="https://www.theringer.com/creator/nate-rogers"><p>Nate Rogers</p></a><p><span>Nate Rogers is a writer in Los Angeles. His writing has appeared in The New York Times, Los Angeles Times, Stereogum, and elsewhere.</span></p></div></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Moon (1002 pts)]]></title>
            <link>https://ciechanow.ski/moon/</link>
            <guid>42443229</guid>
            <pubDate>Tue, 17 Dec 2024 17:26:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ciechanow.ski/moon/">https://ciechanow.ski/moon/</a>, See on <a href="https://news.ycombinator.com/item?id=42443229">Hacker News</a></p>
Couldn't get https://ciechanow.ski/moon/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Getting to Two Million Users as a One Woman Dev Team (538 pts)]]></title>
            <link>https://brightonruby.com/2024/getting-to-2-million-users-as-a-one-woman-dev-team/</link>
            <guid>42441333</guid>
            <pubDate>Tue, 17 Dec 2024 13:53:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brightonruby.com/2024/getting-to-2-million-users-as-a-one-woman-dev-team/">https://brightonruby.com/2024/getting-to-2-million-users-as-a-one-woman-dev-team/</a>, See on <a href="https://news.ycombinator.com/item?id=42441333">Hacker News</a></p>
<div id="readability-page-1" class="page">

  

  <!--
  <div class="font-bold block w-100 text-white bg-red-800 hover:bg-red-600 focus:outline-none focus:border-red-600">
    <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor"
      class="w-6 h-6">
      <path stroke-linecap="round" stroke-linejoin="round"
        d="M9 6.75V15m6-6v8.25m.503 3.498 4.875-2.437c.381-.19.622-.58.622-1.006V4.82c0-.836-.88-1.38-1.628-1.006l-3.869 1.934c-.317.159-.69.159-1.006 0L9.503 3.252a1.125 1.125 0 0 0-1.006 0L3.622 5.689C3.24 5.88 3 6.27 3 6.695V19.18c0 .836.88 1.38 1.628 1.006l3.869-1.934c.317-.159.69-.159 1.006 0l4.994 2.497c.317.158.69.158 1.006 0Z" />
    </svg>

    <a href="https://maps.apple.com/?address=Brighton,%20BN1%201UE,%20England&auid=15913380656425988054&ll=50.823661,-0.138391&lsp=9902&q=Brighton%20Dome"
      focus:shadow-outline-red active:bg-red-600 transition ease-in-out duration-150">
      Apple
    </a>

    <a href="https://goo.gl/maps/o5YdP27AZ8jNWdGT7" class=" font-bold block w-100 text-white bg-red-800 hover:bg-red-600 focus:outline-none focus:border-red-600
                focus:shadow-outline-red active:bg-red-600 transition ease-in-out duration-150">
      Google
    </a>
  </div>
  -->

  <main>
    <a href="https://ti.to/goodscary/brightonruby-2025">
  <p>
    Buy Tickets
    <span>for 2025</span>
    
  </p>
</a>


    <article>
  
  
    <p>
      <video preload="metadata" controls="" autopictureinpicture="" poster="">
        <source src="https://videos.brightonruby.com/videos/2024/nadia-odunayo-getting-to-two-million-users-as-a-one-woman-dev-team.mp4" type="video/mp4">
        <!-- <track label="English" kind="subtitles" srclang="en-us" src="JAWS-aria-errors.vtt" default=""> -->
        Sorry, your browser doesn’t support embedded videos, but don’t worry, you can
        <a href="https://videos.brightonruby.com/videos/2024/nadia-odunayo-getting-to-two-million-users-as-a-one-woman-dev-team.mp4">download it</a>.
        <!--
          The <a href="JAWS-aria-errors.vtt">caption file</a> is also available in case your video player can import it.
        -->
      </video>
    </p>
  

  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
      
    
  

  <div>
      <p>Nadia Odunayo has been so often the smiling face on the door of this event, but did you know she’s the founder and (more impressively!) one woman development team behind <a href="https://thestorygraph.com/">The StoryGraph</a>, a reading community of over a million book lovers. Her story is one of grit, insight and technical insights into what it takes to execute on the “one person framework”.</p>


      

      <hr>

      <p>Nadia Odunayo is the founder and CEO of The StoryGraph, the app that helps you to track your reading and choose which book to read next based on your mood and favorite topics and themes. She previously worked at Pivotal Labs as a software engineer and originally learnt to code at Makers Academy in London. In her spare time she loves to take dance class and, naturally, read!</p>

    </div>
</article>

  </main>

  

  <!-- Fathom - beautiful, simple website analytics -->
  
  <!-- / Fathom -->



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Launch HN: Langfuse (YC W23) – OSS Tracing and Workflows to Improve LLM Apps (133 pts)]]></title>
            <link>https://github.com/langfuse/langfuse</link>
            <guid>42441258</guid>
            <pubDate>Tue, 17 Dec 2024 13:43:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/langfuse/langfuse">https://github.com/langfuse/langfuse</a>, See on <a href="https://news.ycombinator.com/item?id=42441258">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/121163007/322221679-6035f0f3-d691-4963-b5d0-10cf506e9d42.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzQ0NjQxMDIsIm5iZiI6MTczNDQ2MzgwMiwicGF0aCI6Ii8xMjExNjMwMDcvMzIyMjIxNjc5LTYwMzVmMGYzLWQ2OTEtNDk2My1iNWQwLTEwY2Y1MDZlOWQ0Mi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMjE3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTIxN1QxOTMwMDJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04NzUzYmYxZDYyZDk5N2U4YjlmMTBlYmQxMDM1ZDI0ZmQzYzAzODhjMDFjNzA4YmJlZTkzZmE4NzVhODRhMmQ0JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.tA0rZUga9VSDhkjjyUP5lL7Ea7fgYWPFO4czrVqPmuQ"><img src="https://private-user-images.githubusercontent.com/121163007/322221679-6035f0f3-d691-4963-b5d0-10cf506e9d42.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzQ0NjQxMDIsIm5iZiI6MTczNDQ2MzgwMiwicGF0aCI6Ii8xMjExNjMwMDcvMzIyMjIxNjc5LTYwMzVmMGYzLWQ2OTEtNDk2My1iNWQwLTEwY2Y1MDZlOWQ0Mi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMjE3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTIxN1QxOTMwMDJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04NzUzYmYxZDYyZDk5N2U4YjlmMTBlYmQxMDM1ZDI0ZmQzYzAzODhjMDFjNzA4YmJlZTkzZmE4NzVhODRhMmQ0JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.tA0rZUga9VSDhkjjyUP5lL7Ea7fgYWPFO4czrVqPmuQ" alt="Langfuse GitHub Banner"></a></p>
<div dir="auto"><p dir="auto"><h2 tabindex="-1" dir="auto">Langfuse: Open Source LLM Engineering Platform</h2><a id="user-content-langfuse-open-source-llm-engineering-platform" aria-label="Permalink: Langfuse: Open Source LLM Engineering Platform" href="#langfuse-open-source-llm-engineering-platform"></a></p></div>
<div dir="auto"><p dir="auto"><h4 tabindex="-1" dir="auto">LLM Observability, Prompt Management, LLM Evaluations,<br>Datasets, LLM Metrics, and Prompt Playground</h4><a id="user-content-llm-observability-prompt-management-llm-evaluationsdatasets-llm-metrics-and-prompt-playground" aria-label="Permalink: LLM Observability, Prompt Management, LLM Evaluations,Datasets, LLM Metrics, and Prompt Playground" href="#llm-observability-prompt-management-llm-evaluationsdatasets-llm-metrics-and-prompt-playground"></a></p></div>
<div dir="auto">
   
   
   <p><span>Langfuse uses <a href="https://github.com/orgs/langfuse/discussions"><strong>Github Discussions</strong></a>  for Support and Feature Requests.</span>
   <br>
   <span>We're hiring. <a href="https://langfuse.com/careers" rel="nofollow"><strong>Join us</strong></a> in Product Engineering and Developer Relations.</span></p><p><a href="https://github.com/langfuse/langfuse/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/fcbbdcda9497833f606f4465f122fce4c5e9b1c2ce4a6da561d8fe5afb0f507e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d7265642e7376673f7374796c653d666c61742d737175617265" alt="MIT License" data-canonical-src="https://img.shields.io/badge/License-MIT-red.svg?style=flat-square"></a>
      <a href="https://www.ycombinator.com/companies/langfuse" rel="nofollow"><img src="https://camo.githubusercontent.com/e1e0029e353d103690da84a20e88b7051eebbcdede2a1b35d9d1b78b0b0295cf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f59253230436f6d62696e61746f722d5732332d6f72616e67653f7374796c653d666c61742d737175617265" alt="Y Combinator W23" data-canonical-src="https://img.shields.io/badge/Y%20Combinator-W23-orange?style=flat-square"></a>
      <a href="https://github.com/langfuse/langfuse/pkgs/container/langfuse"><img alt="Docker Image" src="https://camo.githubusercontent.com/0b0c3a9f26ebd1d035e726c23742147403771bafe84a8eeb4d77f44d3fa699bd/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f636b65722d6c616e67667573652d626c75653f6c6f676f3d446f636b6572266c6f676f436f6c6f723d7768697465267374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/badge/docker-langfuse-blue?logo=Docker&amp;logoColor=white&amp;style=flat-square"></a>
      <a href="https://pypi.python.org/pypi/langfuse" rel="nofollow"><img src="https://camo.githubusercontent.com/32bdb8ed067c141aa9b6935d039a8ad35e57cedaa7439d2aa554d5ce3cabaa8b/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f6c616e67667573653f7374796c653d666c61742d737175617265266c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465266c6162656c3d707970692532306c616e676675736526636f6c6f723d626c7565" alt="langfuse Python package on PyPi" data-canonical-src="https://img.shields.io/pypi/dm/langfuse?style=flat-square&amp;logo=python&amp;logoColor=white&amp;label=pypi%20langfuse&amp;color=blue"></a>
      <a href="https://www.npmjs.com/package/langfuse" rel="nofollow"><img src="https://camo.githubusercontent.com/d1c1236c9ea644243ec3e13287c02ee8b0ca42797dd42ba0630e71f3399b6614/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f646d2f6c616e67667573653f7374796c653d666c61742d737175617265266c6f676f3d6e706d266c6f676f436f6c6f723d7768697465266c6162656c3d6e706d2532306c616e676675736526636f6c6f723d626c7565" alt="langfuse npm package" data-canonical-src="https://img.shields.io/npm/dm/langfuse?style=flat-square&amp;logo=npm&amp;logoColor=white&amp;label=npm%20langfuse&amp;color=blue"></a>
   </p>
</div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Langfuse Overview</h2><a id="user-content-langfuse-overview" aria-label="Permalink: Langfuse Overview" href="#langfuse-overview"></a></p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=2E8iTvGo9Hs" rel="nofollow"><img src="https://private-user-images.githubusercontent.com/2834609/396482950-3926b288-ff61-4b95-8aa1-45d041c70866.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzQ0NjQxMDIsIm5iZiI6MTczNDQ2MzgwMiwicGF0aCI6Ii8yODM0NjA5LzM5NjQ4Mjk1MC0zOTI2YjI4OC1mZjYxLTRiOTUtOGFhMS00NWQwNDFjNzA4NjYucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTIxNyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDEyMTdUMTkzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZjBjMTQyMWE2OWY4MzI1NzJlMzZjNjQwZTNiZDU3YjVjMGFmOWQ1MTU4ZGQzOTkyYTRhYjFmMDU3NmM1OTBjZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.oHeaFLKEhOIlaxdLGPwSV6NwJ3-S0At1qrW7mQ7KmJ0" alt="Langfuse Overview Video" secured-asset-link=""></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Develop</h3><a id="user-content-develop" aria-label="Permalink: Develop" href="#develop"></a></p>
<ul dir="auto">
<li><strong>LLM Observability:</strong> Instrument your app and start ingesting traces to Langfuse (<a href="https://langfuse.com/docs/get-started" rel="nofollow">Quickstart</a>, <a href="https://langfuse.com/docs/integrations" rel="nofollow">Integrations</a> <a href="https://langfuse.com/docs/tracing" rel="nofollow">Tracing</a>)</li>
<li><strong>Langfuse UI:</strong> Inspect and debug complex logs (<a href="https://langfuse.com/docs/demo" rel="nofollow">Demo</a>, <a href="https://langfuse.com/docs/tracing" rel="nofollow">Tracing</a>)</li>
<li><strong>Prompt Management:</strong> Manage, version and deploy prompts from within Langfuse (<a href="https://langfuse.com/docs/prompts/get-started" rel="nofollow">Prompt Management</a>)</li>
<li><strong>Prompt Engineering:</strong> Test and iterate on your prompts with the <a href="https://langfuse.com/docs/playground" rel="nofollow">LLM Playground</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Monitor</h3><a id="user-content-monitor" aria-label="Permalink: Monitor" href="#monitor"></a></p>
<ul dir="auto">
<li><strong>LLM Analytics:</strong> Track metrics (cost, latency, quality) and gain insights from dashboards &amp; data exports (<a href="https://langfuse.com/docs/analytics" rel="nofollow">Analytics</a>)</li>
<li><strong>LLM Evaluations:</strong> Collect and calculate scores for your LLM completions (<a href="https://langfuse.com/docs/scores" rel="nofollow">Scores &amp; Evaluations</a>)
<ul dir="auto">
<li>Run (<a href="https://langfuse.com/docs/scores/model-based-evals" rel="nofollow">Model-based evaluations</a>) and LLM-as-a-Judge within Langfuse</li>
<li>Collect user feedback (<a href="https://langfuse.com/docs/scores/user-feedback" rel="nofollow">User Feedback</a>)</li>
<li>Manually score LLM outputs in Langfuse (<a href="https://langfuse.com/docs/scores/manually" rel="nofollow">Manual Scores</a>)</li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Test</h3><a id="user-content-test" aria-label="Permalink: Test" href="#test"></a></p>
<ul dir="auto">
<li><strong>Experiments:</strong> Track and test app behaviour before deploying a new version
<ul dir="auto">
<li>Datasets let you test expected in and output pairs and benchmark performance before deploying (<a href="https://langfuse.com/docs/datasets" rel="nofollow">Datasets</a>)</li>
<li>Track versions and releases in your application (<a href="https://langfuse.com/docs/experimentation" rel="nofollow">Experimentation</a>, <a href="https://langfuse.com/docs/prompts" rel="nofollow">Prompt Management</a>)</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Get started</h2><a id="user-content-get-started" aria-label="Permalink: Get started" href="#get-started"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Langfuse Cloud</h3><a id="user-content-langfuse-cloud" aria-label="Permalink: Langfuse Cloud" href="#langfuse-cloud"></a></p>
<p dir="auto">Managed deployment by the Langfuse team, generous free-tier (hobby plan), no credit card required.</p>
<p dir="auto"><strong><a href="https://cloud.langfuse.com/" rel="nofollow">» Langfuse Cloud</a></strong></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Self-Hosting Open Source LLM Observability with Langfuse</h2><a id="user-content-self-hosting-open-source-llm-observability-with-langfuse" aria-label="Permalink: Self-Hosting Open Source LLM Observability with Langfuse" href="#self-hosting-open-source-llm-observability-with-langfuse"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Localhost (docker)</h3><a id="user-content-localhost-docker" aria-label="Permalink: Localhost (docker)" href="#localhost-docker"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Clone repository
git clone https://github.com/langfuse/langfuse.git
cd langfuse

# Run server and database
docker compose up -d"><pre><span><span>#</span> Clone repository</span>
git clone https://github.com/langfuse/langfuse.git
<span>cd</span> langfuse

<span><span>#</span> Run server and database</span>
docker compose up -d</pre></div>
<p dir="auto"><a href="https://langfuse.com/docs/deployment/local" rel="nofollow">→ Learn more about deploying locally</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Self-host (docker)</h3><a id="user-content-self-host-docker" aria-label="Permalink: Self-host (docker)" href="#self-host-docker"></a></p>
<p dir="auto">Langfuse is simple to self-host and keep updated. It currently requires only a single docker container and a postgres database.
<a href="https://langfuse.com/docs/deployment/self-host" rel="nofollow">→ Self Hosting Instructions</a></p>
<p dir="auto">Templated deployments: <a href="https://langfuse.com/docs/deployment/self-host#platform-specific-information" rel="nofollow">Railway, GCP, AWS, Azure, Kubernetes and others</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Get Started</h2><a id="user-content-get-started-1" aria-label="Permalink: Get Started" href="#get-started-1"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">API Keys</h3><a id="user-content-api-keys" aria-label="Permalink: API Keys" href="#api-keys"></a></p>
<p dir="auto">You need a Langfuse public and secret key to get started. Sign up <a href="https://cloud.langfuse.com/" rel="nofollow">here</a> and find them in your project settings.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Ingesting Data · Instrumenting Your Application · LLM Observability with Langfuse</h3><a id="user-content-ingesting-data--instrumenting-your-application--llm-observability-with-langfuse" aria-label="Permalink: Ingesting Data · Instrumenting Your Application · LLM Observability with Langfuse" href="#ingesting-data--instrumenting-your-application--llm-observability-with-langfuse"></a></p>
<p dir="auto">Note: We recommend using our fully async, typed <a href="https://langfuse.com/docs/sdk" rel="nofollow">SDKs</a> that allow you to instrument any LLM application with any underlying model. They are available in <a href="https://langfuse.com/docs/sdk/python" rel="nofollow">Python (Decorators)</a> &amp; <a href="https://langfuse.com/docs/sdk/typescript" rel="nofollow">JS/TS</a>. The SDKs will always be the most fully featured and stable way to ingest data into Langfuse.</p>
<p dir="auto">See the <a href="https://langfuse.com/docs/get-started" rel="nofollow">→ Quickstart</a> to integrate Langfuse.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">LLM Observability Integrations</h3><a id="user-content-llm-observability-integrations" aria-label="Permalink: LLM Observability Integrations" href="#llm-observability-integrations"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Integration</th>
<th>Supports</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://langfuse.com/docs/sdk" rel="nofollow">SDK</a></td>
<td>Python, JS/TS</td>
<td>Manual instrumentation using the SDKs for full flexibility.</td>
</tr>
<tr>
<td><a href="https://langfuse.com/docs/integrations/openai" rel="nofollow">OpenAI</a></td>
<td>Python, JS/TS</td>
<td>Automated instrumentation using drop-in replacement of OpenAI SDK.</td>
</tr>
<tr>
<td><a href="https://langfuse.com/docs/integrations/langchain" rel="nofollow">Langchain</a></td>
<td>Python, JS/TS</td>
<td>Automated instrumentation by passing callback handler to Langchain application.</td>
</tr>
<tr>
<td><a href="https://langfuse.com/docs/integrations/llama-index/get-started" rel="nofollow">LlamaIndex</a></td>
<td>Python</td>
<td>Automated instrumentation via LlamaIndex callback system.</td>
</tr>
<tr>
<td><a href="https://langfuse.com/docs/integrations/haystack" rel="nofollow">Haystack</a></td>
<td>Python</td>
<td>Automated instrumentation via Haystack content tracing system.</td>
</tr>
<tr>
<td><a href="https://langfuse.com/docs/integrations/litellm" rel="nofollow">LiteLLM</a></td>
<td>Python, JS/TS (proxy only)</td>
<td>Use any LLM as a drop in replacement for GPT. Use Azure, OpenAI, Cohere, Anthropic, Ollama, VLLM, Sagemaker, HuggingFace, Replicate (100+ LLMs).</td>
</tr>
<tr>
<td><a href="https://langfuse.com/docs/integrations/vercel-ai-sdk" rel="nofollow">Vercel AI SDK</a></td>
<td>JS/TS</td>
<td>TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js.</td>
</tr>
<tr>
<td><a href="https://langfuse.com/docs/api" rel="nofollow">API</a></td>
<td></td>
<td>Directly call the public API. OpenAPI spec available.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">Packages integrated with Langfuse:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://langfuse.com/docs/integrations/instructor" rel="nofollow">Instructor</a></td>
<td>Library to get structured LLM outputs (JSON, Pydantic)</td>
</tr>
<tr>
<td><a href="https://langfuse.com/docs/integrations/dify" rel="nofollow">Dify</a></td>
<td>Open source LLM app development platform with no-code builder.</td>
</tr>
<tr>
<td><a href="https://langfuse.com/docs/integrations/ollama" rel="nofollow">Ollama</a></td>
<td>Easily run open source LLMs on your own machine.</td>
</tr>
<tr>
<td><a href="https://langfuse.com/docs/integrations/mirascope" rel="nofollow">Mirascope</a></td>
<td>Python toolkit for building LLM applications.</td>
</tr>
<tr>
<td><a href="https://langfuse.com/docs/integrations/flowise" rel="nofollow">Flowise</a></td>
<td>JS/TS no-code builder for customized LLM flows.</td>
</tr>
<tr>
<td><a href="https://langfuse.com/docs/integrations/langflow" rel="nofollow">Langflow</a></td>
<td>Python-based UI for LangChain, designed with react-flow to provide an effortless way to experiment and prototype flows.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Questions and feedback</h2><a id="user-content-questions-and-feedback" aria-label="Permalink: Questions and feedback" href="#questions-and-feedback"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Ideas and roadmap</h3><a id="user-content-ideas-and-roadmap" aria-label="Permalink: Ideas and roadmap" href="#ideas-and-roadmap"></a></p>
<ul dir="auto">
<li><a href="https://langfuse.com/roadmap" rel="nofollow">Roadmap</a></li>
<li><a href="https://github.com/orgs/langfuse/discussions">GitHub Discussions</a></li>
<li><a href="https://langfuse.com/ideas" rel="nofollow">Feature Requests</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Support and feedback</h3><a id="user-content-support-and-feedback" aria-label="Permalink: Support and feedback" href="#support-and-feedback"></a></p>
<p dir="auto">In order of preference the best way to communicate with us:</p>
<ul dir="auto">
<li><a href="https://github.com/orgs/langfuse/discussions">GitHub Discussions</a> (preferred): Contribute <a href="https://langfuse.com/ideas" rel="nofollow">ideas</a>, <a href="https://langfuse.com/gh-support" rel="nofollow">support requests</a> and <a href="https://langfuse.com/issues" rel="nofollow">report bugs</a></li>
<li><a href="https://langfuse.com/discord" rel="nofollow">Discord</a>: community support</li>
<li>Privately: contact at langfuse dot com</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing to Langfuse</h2><a id="user-content-contributing-to-langfuse" aria-label="Permalink: Contributing to Langfuse" href="#contributing-to-langfuse"></a></p>
<ul dir="auto">
<li>Vote on <a href="https://github.com/orgs/langfuse/discussions/categories/ideas">Ideas</a></li>
<li>Raise and comment on <a href="https://github.com/langfuse/langfuse/issues">Issues</a></li>
<li>Open a PR - see <a href="https://github.com/langfuse/langfuse/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> for details on how to setup a development environment.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This repository is MIT licensed, except for the <code>ee</code> folders. See <a href="https://github.com/langfuse/langfuse/blob/main/LICENSE">LICENSE</a> and <a href="https://langfuse.com/docs/open-source" rel="nofollow">docs</a> for more details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Misc</h2><a id="user-content-misc" aria-label="Permalink: Misc" href="#misc"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">GET API to export your data</h3><a id="user-content-get-api-to-export-your-data" aria-label="Permalink: GET API to export your data" href="#get-api-to-export-your-data"></a></p>
<p dir="auto"><a href="https://langfuse.com/docs/integrations/api" rel="nofollow"><strong>GET routes</strong></a> to use data in downstream applications (e.g. embedded analytics). You can also access them conveniently via the SDKs (<a href="https://langfuse.com/docs/query-traces" rel="nofollow">docs</a>).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Security &amp; Privacy</h3><a id="user-content-security--privacy" aria-label="Permalink: Security &amp; Privacy" href="#security--privacy"></a></p>
<p dir="auto">We take data security and privacy seriously. Please refer to our <a href="https://langfuse.com/security" rel="nofollow">Security and Privacy</a> page for more information.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Telemetry</h3><a id="user-content-telemetry" aria-label="Permalink: Telemetry" href="#telemetry"></a></p>
<p dir="auto">By default, Langfuse automatically reports basic usage statistics of self-hosted instances to a centralized server (PostHog).</p>
<p dir="auto">This helps us to:</p>
<ol dir="auto">
<li>Understand how Langfuse is used and improve the most relevant features.</li>
<li>Track overall usage for internal and external (e.g. fundraising) reporting.</li>
</ol>
<p dir="auto">None of the data is shared with third parties and does not include any sensitive information. We want to be super transparent about this and you can find the exact data we collect <a href="https://github.com/langfuse/langfuse/blob/main/web/src/features/telemetry/index.ts">here</a>.</p>
<p dir="auto">You can opt-out by setting <code>TELEMETRY_ENABLED=false</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Star History</h3><a id="user-content-star-history" aria-label="Permalink: Star History" href="#star-history"></a></p>
<a href="https://star-history.com/#langfuse/langfuse&amp;Date" rel="nofollow">
 <themed-picture data-catalyst-inline="true"><picture>
   <source media="(prefers-color-scheme: dark)" srcset="https://camo.githubusercontent.com/3e5cc364f471d385fc43aad9438960abfd5f1d444f397b7dd1011eec0ed0ed61/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6c616e67667573652f6c616e676675736526747970653d44617465267468656d653d6461726b" data-canonical-src="https://api.star-history.com/svg?repos=langfuse/langfuse&amp;type=Date&amp;theme=dark">
   <source media="(prefers-color-scheme: light)" srcset="https://camo.githubusercontent.com/69cd0e8abd538d9a5267856d84d067cc5313da4e9cb1bb2746f1e6f96dd93bc1/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6c616e67667573652f6c616e676675736526747970653d44617465" data-canonical-src="https://api.star-history.com/svg?repos=langfuse/langfuse&amp;type=Date">
   <img alt="Star History Chart" src="https://camo.githubusercontent.com/69cd0e8abd538d9a5267856d84d067cc5313da4e9cb1bb2746f1e6f96dd93bc1/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6c616e67667573652f6c616e676675736526747970653d44617465" data-canonical-src="https://api.star-history.com/svg?repos=langfuse/langfuse&amp;type=Date">
 </picture></themed-picture>
</a>
<p dir="auto"><h3 tabindex="-1" dir="auto">Open Source Projects Using Langfuse</h3><a id="user-content-open-source-projects-using-langfuse" aria-label="Permalink: Open Source Projects Using Langfuse" href="#open-source-projects-using-langfuse"></a></p>
<p dir="auto">Top open-source Python projects that use Langfuse, ranked by stars (<a href="https://github.com/langfuse/langfuse-docs/blob/main/components-mdx/dependents">Source</a>):</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Repository</th>
<th>Stars</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/127165244?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/127165244?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/langgenius">langgenius</a> / <a href="https://github.com/langgenius/dify">dify</a></td>
<td>54865</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/158137808?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/158137808?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/open-webui">open-webui</a> / <a href="https://github.com/open-webui/open-webui">open-webui</a></td>
<td>51531</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/131470832?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/131470832?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/lobehub">lobehub</a> / <a href="https://github.com/lobehub/lobe-chat">lobe-chat</a></td>
<td>49003</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/85702467?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/85702467?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/langflow-ai">langflow-ai</a> / <a href="https://github.com/langflow-ai/langflow">langflow</a></td>
<td>39093</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/130722866?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/130722866?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/run-llama">run-llama</a> / <a href="https://github.com/run-llama/llama_index">llama_index</a></td>
<td>37368</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/139558948?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/139558948?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/chatchat-space">chatchat-space</a> / <a href="https://github.com/chatchat-space/Langchain-Chatchat">Langchain-Chatchat</a></td>
<td>32486</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/128289781?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/128289781?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/FlowiseAI">FlowiseAI</a> / <a href="https://github.com/FlowiseAI/Flowise">Flowise</a></td>
<td>32448</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/31035808?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/31035808?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/mindsdb">mindsdb</a> / <a href="https://github.com/mindsdb/mindsdb">mindsdb</a></td>
<td>26931</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/119600397?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/119600397?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/twentyhq">twentyhq</a> / <a href="https://github.com/twentyhq/twenty">twenty</a></td>
<td>24195</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/60330232?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/60330232?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/PostHog">PostHog</a> / <a href="https://github.com/PostHog/posthog">posthog</a></td>
<td>22618</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/121462774?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/121462774?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/BerriAI">BerriAI</a> / <a href="https://github.com/BerriAI/litellm">litellm</a></td>
<td>15151</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/179202840?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/179202840?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/mediar-ai">mediar-ai</a> / <a href="https://github.com/mediar-ai/screenpipe">screenpipe</a></td>
<td>11037</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/105877416?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/105877416?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/formbricks">formbricks</a> / <a href="https://github.com/formbricks/formbricks">formbricks</a></td>
<td>9386</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/76263028?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/76263028?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/anthropics">anthropics</a> / <a href="https://github.com/anthropics/courses">courses</a></td>
<td>8385</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/78410652?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/78410652?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/GreyDGL">GreyDGL</a> / <a href="https://github.com/GreyDGL/PentestGPT">PentestGPT</a></td>
<td>7374</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/152537519?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/152537519?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/superagent-ai">superagent-ai</a> / <a href="https://github.com/superagent-ai/superagent">superagent</a></td>
<td>5391</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/137907881?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/137907881?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/promptfoo">promptfoo</a> / <a href="https://github.com/promptfoo/promptfoo">promptfoo</a></td>
<td>4976</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/157326433?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/157326433?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/onlook-dev">onlook-dev</a> / <a href="https://github.com/onlook-dev/onlook">onlook</a></td>
<td>4141</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/7250217?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/7250217?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/Canner">Canner</a> / <a href="https://github.com/Canner/WrenAI">WrenAI</a></td>
<td>2526</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/11855343?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/11855343?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/pingcap">pingcap</a> / <a href="https://github.com/pingcap/autoflow">autoflow</a></td>
<td>2061</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/85268109?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/85268109?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/MLSysOps">MLSysOps</a> / <a href="https://github.com/MLSysOps/MLE-agent">MLE-agent</a></td>
<td>1161</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/158137808?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/158137808?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/open-webui">open-webui</a> / <a href="https://github.com/open-webui/pipelines">pipelines</a></td>
<td>1100</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/18422723?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/18422723?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/alishobeiri">alishobeiri</a> / <a href="https://github.com/alishobeiri/thread">thread</a></td>
<td>1074</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/125468716?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/125468716?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/topoteretes">topoteretes</a> / <a href="https://github.com/topoteretes/cognee">cognee</a></td>
<td>971</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/188657705?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/188657705?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/bRAGAI">bRAGAI</a> / <a href="https://github.com/bRAGAI/bRAG-langchain">bRAG-langchain</a></td>
<td>823</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/169500408?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/169500408?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/opslane">opslane</a> / <a href="https://github.com/opslane/opslane">opslane</a></td>
<td>677</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/151867818?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/151867818?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/dynamiq-ai">dynamiq-ai</a> / <a href="https://github.com/dynamiq-ai/dynamiq">dynamiq</a></td>
<td>639</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/48585267?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/48585267?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/theopenconversationkit">theopenconversationkit</a> / <a href="https://github.com/theopenconversationkit/tock">tock</a></td>
<td>514</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/20493493?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/20493493?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/andysingal">andysingal</a> / <a href="https://github.com/andysingal/llm-course">llm-course</a></td>
<td>394</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/132396805?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/132396805?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/phospho-app">phospho-app</a> / <a href="https://github.com/phospho-app/phospho">phospho</a></td>
<td>384</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/178644984?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/178644984?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/sentient-engineering">sentient-engineering</a> / <a href="https://github.com/sentient-engineering/agent-q">agent-q</a></td>
<td>370</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/168552753?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/168552753?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/sql-agi">sql-agi</a> / <a href="https://github.com/sql-agi/DB-GPT">DB-GPT</a></td>
<td>324</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/60330232?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/60330232?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/PostHog">PostHog</a> / <a href="https://github.com/PostHog/posthog-foss">posthog-foss</a></td>
<td>305</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/154247157?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/154247157?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/vespperhq">vespperhq</a> / <a href="https://github.com/vespperhq/vespper">vespper</a></td>
<td>304</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/185116535?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/185116535?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/block">block</a> / <a href="https://github.com/block/goose">goose</a></td>
<td>295</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/609489?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/609489?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/aorwall">aorwall</a> / <a href="https://github.com/aorwall/moatless-tools">moatless-tools</a></td>
<td>291</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/2357342?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/2357342?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/dmayboroda">dmayboroda</a> / <a href="https://github.com/dmayboroda/minima">minima</a></td>
<td>221</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/66303003?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/66303003?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/RobotecAI">RobotecAI</a> / <a href="https://github.com/RobotecAI/rai">rai</a></td>
<td>172</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/148684274?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/148684274?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/i-am-alice">i-am-alice</a> / <a href="https://github.com/i-am-alice/3rd-devs">3rd-devs</a></td>
<td>148</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/171735272?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/171735272?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/8090-inc">8090-inc</a> / <a href="https://github.com/8090-inc/xrx-sample-apps">xrx-sample-apps</a></td>
<td>138</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/104478511?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/104478511?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/babelcloud">babelcloud</a> / <a href="https://github.com/babelcloud/LLM-RGB">LLM-RGB</a></td>
<td>135</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/15125613?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/15125613?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/souzatharsis">souzatharsis</a> / <a href="https://github.com/souzatharsis/tamingLLMs">tamingLLMs</a></td>
<td>129</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/169401942?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/169401942?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/LibreChat-AI">LibreChat-AI</a> / <a href="https://github.com/LibreChat-AI/librechat.ai">librechat.ai</a></td>
<td>128</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/51827949?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/51827949?s=40&amp;v=4" width="20" height="20" alt=""></a> &nbsp; <a href="https://github.com/deepset-ai">deepset-ai</a> / <a href="https://github.com/deepset-ai/haystack-core-integrations">haystack-core-integrations</a></td>
<td>126</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Crunch – a Scheme compiler with a minimal runtime (147 pts)]]></title>
            <link>https://www.more-magic.net/posts/crunch.html</link>
            <guid>42440767</guid>
            <pubDate>Tue, 17 Dec 2024 12:18:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.more-magic.net/posts/crunch.html">https://www.more-magic.net/posts/crunch.html</a>, See on <a href="https://news.ycombinator.com/item?id=42440767">Hacker News</a></p>
Couldn't get https://www.more-magic.net/posts/crunch.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Natural Number Game: build the basic theory of the natural numbers from scratch (119 pts)]]></title>
            <link>https://adam.math.hhu.de/#/g/leanprover-community/NNG4</link>
            <guid>42440016</guid>
            <pubDate>Tue, 17 Dec 2024 10:06:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://adam.math.hhu.de/#/g/leanprover-community/NNG4">https://adam.math.hhu.de/#/g/leanprover-community/NNG4</a>, See on <a href="https://news.ycombinator.com/item?id=42440016">Hacker News</a></p>
Couldn't get https://adam.math.hhu.de/#/g/leanprover-community/NNG4: Error: unable to verify the first certificate]]></description>
        </item>
        <item>
            <title><![CDATA[When should we require that firmware be free? (101 pts)]]></title>
            <link>https://mjg59.dreamwidth.org/70895.html</link>
            <guid>42439921</guid>
            <pubDate>Tue, 17 Dec 2024 09:49:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mjg59.dreamwidth.org/70895.html">https://mjg59.dreamwidth.org/70895.html</a>, See on <a href="https://news.ycombinator.com/item?id=42439921">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The distinction between hardware and software has historically been relatively easy to understand - hardware is the physical object that software runs on. This is made more complicated by the existence of programmable logic like FPGAs, but by and large things tend to fall into fairly neat categories if we're drawing that distinction.</p><p>Conversations usually become more complicated when we introduce firmware, but should they? According to Wikipedia, <q>Firmware is software that provides low-level control of computing device hardware</q>, and basically anything that's generally described as firmware certainly fits into the "software" side of the above hardware/software binary. From a software freedom perspective, this seems like something where the obvious answer to "Should this be free" is "yes", but it's worth thinking about <em>why</em> the answer is yes - the goal of free software isn't freedom for freedom's sake, but because the freedoms embodied in the <a href="https://www.gnu.org/philosophy/free-sw.en.html#fs-definition">Free Software Definition</a> (and by proxy the <a href="https://www.debian.org/social_contract#guidelines">DFSG</a>) are grounded in real world practicalities.</p><p>How do these line up for firmware? Firmware can fit into two main classes - it can be something that's responsible for initialisation of the hardware (such as, historically, BIOS, which is involved in initialisation and boot and then largely irrelevant for runtime[1]) or it can be something that makes the hardware work at runtime (wifi card firmware being an obvious example). The role of free software in the latter case feels fairly intuitive, since the interface and functionality the hardware offers to the operating system is frequently largely defined by the firmware running on it. Your wifi chipset is, these days, largely a software defined radio, and what you can do with it is determined by what the firmware it's running allows you to do. Sometimes those restrictions may be required by law, but other times they're simply because the people writing the firmware aren't interested in supporting a feature - they may see no reason to allow raw radio packets to be provided to the OS, for instance. We also shouldn't ignore the fact that sufficiently complicated firmware exposed to untrusted input (as is the case in most wifi scenarios) may contain exploitable vulnerabilities allowing attackers to gain arbitrary code execution on the wifi chipset - and potentially use that as a way to gain control of the host OS (see <a href="https://googleprojectzero.blogspot.com/2017/04/over-air-exploiting-broadcoms-wi-fi_4.html">this writeup</a> for an example). Vendors being in a unique position to update that firmware means users may never receive security updates, leaving them with a choice between discarding hardware that otherwise works perfectly or leaving themselves vulnerable to known security issues.</p><p>But even the cases where firmware does nothing other than initialise the hardware cause problems. A lot of hardware has functionality controlled by registers that can be locked during the boot process. Vendor firmware may choose to disable (or, rather, never to enable) functionality that may be beneficial to a user, and then lock out the ability to reconfigure the hardware later. Without any ability to modify that firmware, the user lacks the freedom to choose what functionality their hardware makes available to them. Again, the ability to inspect this firmware and modify it has a distinct benefit to the user.</p><p>So, from a practical perspective, I think there's a strong argument that users would benefit from most (if not all) firmware being free software, and I don't think that's an especially controversial argument. So I think this is less of a philosophical discussion, and more of a strategic one - is spending time focused on ensuring firmware is free worthwhile, and if so what's an appropriate way of achieving this?</p><p>I think there's two consistent ways to view this. One is to view free firmware as <em>desirable</em> but not <em>necessary</em>. This approach basically argues that code that's running on hardware that isn't the main CPU would benefit from being free, in the same way that code running on a remote network service would benefit from being free, but that this is much less important than ensuring that all the code running in the context of the OS on the primary CPU is free. The maximalist position is not to compromise at all - all software on a system, whether it's running at boot or during runtime, and whether it's running on the primary CPU or any other component on the board, should be free.</p><p>Personally, I lean towards the former and think there's a reasonably coherent argument here. I think users <em>would</em> benefit from the ability to modify the code running on hardware that their OS talks to, in the same way that I think users would benefit from the ability to modify the code running on hardware the other side of a network link that their browser talks to. I also think that there's enough that remains to be done in terms of what's running on the host CPU that it's not worth having that fight yet. But I think the latter is absolutely intellectually consistent, and while I don't agree with it from a pragmatic perspective I think things would undeniably be better if we lived in that world.</p><p>This feels like a thing you'd expect the Free Software Foundation to have opinions on, and it does! There are two primarily relevant things - the <a href="https://ryf.fsf.org/">Respects your Freedoms</a> campaign focused on ensuring that certified hardware meets certain requirements (including around firmware), and the <a href="https://www.gnu.org/distros/free-system-distribution-guidelines.html">Free System Distribution Guidelines</a>, which define a baseline for an OS to be considered free by the FSF (including requirements around firmware).</p><p>RYF <a href="https://ryf.fsf.org/about/criteria">requires</a> that all software on a piece of hardware be free other than under one specific set of circumstances. If software runs on (a) a secondary processor and (b) <q>within which software installation is not intended after the user obtains the product</q>, then the software does not need to be free. (b) effectively means that the firmware has to be in ROM, since any runtime interface that allows the firmware to be loaded or updated is intended to allow software installation after the user obtains the product.</p><p>The Free System Distribution Guidelines <a href="https://www.gnu.org/distros/free-system-distribution-guidelines.html#nonfree-firmware">require</a> that all non-free firmware be removed from the OS before it can be considered free. The recommended mechanism to achieve this is via <a href="https://www.fsfla.org/ikiwiki/selibre/linux-libre/">linux-libre</a>, a project that produces tooling to remove anything that looks plausibly like a non-free firmware blob from the Linux source code, along with any incitement to the user to load firmware - including even removing suggestions to update CPU microcode in order to mitigate CPU vulnerabilities.</p><p>For hardware that requires non-free firmware to be loaded at runtime in order to work, linux-libre doesn't do anything to work around this - the hardware will simply not work. In this respect, linux-libre reduces the amount of non-free firmware running on a system in the same way that removing the hardware would. This presumably encourages users to purchase RYF compliant hardware.</p><p>But does that actually improve things? RYF doesn't require that a piece of hardware have no non-free firmware, it simply requires that any non-free firmware be hidden from the user. CPU microcode is an instructive example here. At the time of writing, every laptop listed <a href="https://ryf.fsf.org/index.php/categories/laptops">here</a> has an Intel CPU. Every Intel CPU has microcode in ROM, typically an early revision that is known to have many bugs. The expectation is that this microcode is updated in the field by either the firmware or the OS at boot time - the updated version is loaded into RAM on the CPU, and vanishes if power is cut. The combination of RYF and linux-libre doesn't reduce the amount of non-free code running inside the CPU, it just means that the user (a) is more likely to hit since-fixed bugs (including security ones!), and (b) has less guidance on how to avoid them.</p><p>As long as RYF permits hardware that makes use of non-free firmware I think it hurts more than it helps. In many cases users aren't guided away from non-free firmware - instead it's hidden away from them, leaving them less aware that their freedom is constrained. Linux-libre goes further, refusing to even inform the user that the non-free firmware that their hardware depends on can be upgraded to improve their security.</p><p>Out of sight shouldn't mean out of mind. If non-free firmware is a threat to user freedom then allowing it to exist in ROM doesn't do anything to solve that problem. And if it isn't a threat to user freedom, then what's the point of requiring linux-libre for a Linux distribution to be considered free by the FSF? We seem to have ended up in the worst case scenario, where nothing is being done to actually replace any of the non-free firmware running on people's systems and where users may even end up with a reduced awareness that the non-free firmware even exists.</p><p>[1] Yes yes SMM</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How I used linear algebra to build an interactive diagramming editor (186 pts)]]></title>
            <link>https://medium.com/@ivan.ishubin/how-i-used-linear-algebra-to-build-an-interactive-diagramming-editor-and-why-matrix-math-is-d5bd552f2e8d</link>
            <guid>42438767</guid>
            <pubDate>Tue, 17 Dec 2024 06:10:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/@ivan.ishubin/how-i-used-linear-algebra-to-build-an-interactive-diagramming-editor-and-why-matrix-math-is-d5bd552f2e8d">https://medium.com/@ivan.ishubin/how-i-used-linear-algebra-to-build-an-interactive-diagramming-editor-and-why-matrix-math-is-d5bd552f2e8d</a>, See on <a href="https://news.ycombinator.com/item?id=42438767">Hacker News</a></p>
Couldn't get https://medium.com/@ivan.ishubin/how-i-used-linear-algebra-to-build-an-interactive-diagramming-editor-and-why-matrix-math-is-d5bd552f2e8d: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Make your QEMU faster (2022) (135 pts)]]></title>
            <link>https://linus.schreibt.jetzt/posts/qemu-9p-performance.html</link>
            <guid>42438449</guid>
            <pubDate>Tue, 17 Dec 2024 04:56:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://linus.schreibt.jetzt/posts/qemu-9p-performance.html">https://linus.schreibt.jetzt/posts/qemu-9p-performance.html</a>, See on <a href="https://news.ycombinator.com/item?id=42438449">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
            
            <article>
    
    <section>
      
      
        <p><em>The work on QEMU and this post was paid for by Determinate Systems, and the post was co-published on the <a href="https://determinate.systems/posts/qemu-fix">Determinate Systems blog</a>.</em></p>
<h2 id="background">Background</h2>
<p><a href="https://nixos.org/">NixOS</a> uses virtual machines based on <a href="https://www.qemu.org/">QEMU</a> extensively for running its test suite. In order to avoid generating a disk image for every test, the test driver usually boots using a <a href="http://man.cat-v.org/plan_9/5/intro">Plan 9 File Protocol (9p)</a> share (server implemented by QEMU) for the Nix store, which contains all the programs and config necessary for the test.</p>
<p>I was working on a VM test that copied a fairly large amount of data (~278k files totaling ~5.3GiB) out of the 9p-mounted Nix store, and was surprised at how long copying this data took. On NVMe devices, I would expect this to take a matter of seconds or minutes, but the test actually ended up taking over 2 <em>hours</em>, most of which was spent copying files from 9p. Since this is untenably long for incremental work, I decided to dig into it a bit, and was able to reduce the duration of the test to only 7 minutes. In this post, I’ll describe the whole journey.</p>
<h2 id="profiling-qemu">Profiling QEMU</h2>
<p>As a preface: I don’t have much experience in debugging performance issues! Most of what I used was novel to me. The first thing I wanted to do was find out where a lot of time was being spent. My guess was that it was in QEMU and not in the guest, though this guess being correct was a matter of pure luck. <a href="https://stackoverflow.com/questions/16999681/how-to-do-profiling-with-qemu">This stack overflow question</a> described a problem mostly but not entirely unlike mine. This led me down the wonky path of trying to use the <a href="http://poormansprofiler.org/">poor man’s profiler</a>, a little hack composed of gdb, some shell, and awk.</p>
<h2 id="stories-of-surprises-and-failure-poor-mans-profiler">Stories of surprises and failure: Poor man’s profiler</h2>
<p>I immediately ran into a minor roadblock with this approach. gdb said:</p>
<blockquote>
<p>warning: Target and debugger are in different PID namespaces; thread lists and other data are likely unreliable. Connect to gdbserver inside the container.</p>
</blockquote>
<p>Nix uses Linux namespaces to provide builds with some isolation from the system running the build in order to reduce the effects that the environment of a particular machine can have on the result of the build (”purity”). This includes PID namespaces, which prevent processes within the namespace from touching any processes outside the namespace. gdb was unhappy with being in a different PID namespace from the process it was targeting! I first attempted to get my gdb inside the sandbox using <code>nsenter</code>. The first surprise I encountered here was that entering a PID namespace does <em>not</em> cause utilities from procps, such as <code>ps</code>, <code>pgrep</code> and <code>top</code>, to report only on processes inside the new namespace:</p>
<pre><code>[root@oak:~]# pgrep -a qemu
1678991 /nix/store/6shk4z9ip57p6vffm5n9imnkwiks9fsa-qemu-host-cpu-only-for-vm-tests-7.0.0/bin/qemu-kvm [...]

[root@oak:~]# nsenter --target 1678991 --pid

🗣 This spawned a new shell within the build's PID namespace
[root@oak:~]# ps -f 1
UID          PID    PPID  C STIME TTY      STAT   TIME CMD
root           1       0  0 Sep02 ?        Ss     1:24 /run/current-system/systemd/lib/systemd/systemd</code></pre>
<p>What!? That’s not the PID1 I’m expecting! And I certainly haven’t been running this build since the 2nd of September.</p>
<p>I’ll omit the details of the hour of frustration that ensued, but it turns out that the <code>/proc</code> which <code>ps</code> and friends were reading from was still that of the root PID namespace — even though they were no longer running in it! This might cause some funny unexpected behaviour when using tools like <code>pkill</code>… But that’s a problem for another day.</p>
<p>With my newfound knowledge, I was able to work around this issue by also creating a new mount namespace and mounting the desired <code>proc</code> filesystem on top of the <code>/proc</code> we inherited from the root namespace.</p>
<pre><code>[root@oak:~]# nsenter -t 1684606 -p -- unshare -m

🗣 Now inside the build's PID namespace (through nsenter) and a new mount namespace (created by unshare)
[root@oak:~]# ps -f 1
UID          PID    PPID  C STIME TTY      STAT   TIME CMD
root           1       0  0 Sep02 ?        Ss     1:24 /run/current-system/systemd/lib/systemd/systemd

[root@oak:~]# mount -t proc proc /proc

[root@oak:~]# ps -f 1
UID          PID    PPID  C STIME TTY      STAT   TIME CMD
nixbld1        1       0  0 12:27 ?        Ss     0:00 bash -e /nix/store/9krlzvny65gdc8s7kpb6lkx8cd02c25b-default-builder.sh

[root@oak:~]# ps -ef
UID          PID    PPID  C STIME TTY          TIME CMD
nixbld1        1       0  0 12:27 ?        00:00:00 bash -e /nix/store/9krlzvny65gdc8s7kpb6lkx8cd02c25b-default-builder.sh
nixbld1        6       1  0 12:27 ?        00:00:00 /nix/store/pn7863n7s2p66b0gazcylm6cccdwpzaf-python3-3.9.13/bin/python3.9 /nix/store/kdi82vgfixayxaql77j3nj7
nixbld1        7       6 99 12:27 ?        00:04:00 /nix/store/6shk4z9ip57p6vffm5n9imnkwiks9fsa-qemu-host-cpu-only-for-vm-tests-7.0.0/bin/qemu-kvm -cpu max -na
root          46       0  0 12:29 pts/5    00:00:00 -bash
root          79      46  0 12:30 pts/5    00:00:00 ps -ef
🗣 That's better!

[root@oak:~]# pid=7 ./pprof
   1500 __futex_abstimed_wait_common,__new_sem_wait_slow64.constprop.1,qemu_sem_timedwait,worker_thread,qemu_thread_start,start_thread,clone3
    743 __lll_lock_wait,pthread_mutex_lock@@GLIBC_2.2.5,qemu_mutex_lock_impl,qemu_mutex_lock_iothread_impl,flatview_read_continue,flatview_read,address_space_rw,kvm_cpu_exec,kvm_vcpu_thread_fn,qemu_thread_start,start_thread,clone3
    100 syscall,qemu_event_wait,call_rcu_thread,qemu_thread_start,start_thread,clone3
     53 ioctl,kvm_vcpu_ioctl,kvm_cpu_exec,kvm_vcpu_thread_fn,qemu_thread_start,start_thread,clone3
     45 get_fid,v9fs_read,coroutine_trampoline,__correctly_grouped_prefixwc,??
     15 get_fid,v9fs_walk,coroutine_trampoline,__correctly_grouped_prefixwc,??
     11 alloc_fid,v9fs_walk,coroutine_trampoline,__correctly_grouped_prefixwc,??
      8 get_fid,v9fs_getattr,coroutine_trampoline,__correctly_grouped_prefixwc,??
      5 clunk_fid,v9fs_xattrwalk,coroutine_trampoline,__correctly_grouped_prefixwc,??
      5 alloc_fid,v9fs_xattrwalk,coroutine_trampoline,__correctly_grouped_prefixwc,??
      4 __lll_lock_wait,pthread_mutex_lock@@GLIBC_2.2.5,qemu_mutex_lock_impl,qemu_mutex_lock_iothread_impl,flatview_write_continue,flatview_write,address_space_rw,kvm_cpu_exec,kvm_vcpu_thread_fn,qemu_thread_start,start_thread,clone3
      3 get_fid,v9fs_xattrwalk,coroutine_trampoline,__correctly_grouped_prefixwc,??
      3 get_fid,v9fs_open,coroutine_trampoline,__correctly_grouped_prefixwc,??
      2 clunk_fid,v9fs_clunk,coroutine_trampoline,__correctly_grouped_prefixwc,??
      1 get_fid,v9fs_readlink,coroutine_trampoline,__correctly_grouped_prefixwc,??
      1 get_fid,v9fs_readdir,coroutine_trampoline,__correctly_grouped_prefixwc,??
      1 address_space_translate_internal,flatview_do_translate.isra,address_space_map,virtqueue_map_desc,virtqueue_split_pop,virtio_blk_handle_vq,virtio_queue_notify_vq.part,aio_dispatch_handler,aio_dispatch,aio_ctx_dispatch,g_main_context_dispatch,main_loop_wait,qemu_main_loop,main
      1</code></pre>
<p>The most common points in the code were:</p>
<ol type="1">
<li>Locking resources, at 1500 and 743 thread samples. Since the top result was on worker threads, I guessed that that wasn’t interesting and they were just waiting for work to become available.</li>
<li>Waiting for things to happen inside the VM, at 100 and 53 thread samples. That didn’t seem like a relevant issue though — I’d expect a VM monitor to be waiting for its guest to need something most of the time.</li>
</ol>
<p>The rest of the numbers were small enough that I (erroneously!) considered them uninteresting too. At this point I became frustrated enough with this branch of my investigation that I gave up on it.</p>
<p>I moved on to <a href="https://github.com/yoshinorim/quickstack/">quickstack</a>, <a href="https://github.com/NixOS/nixpkgs/pull/190335">packaged it</a>, and observed that it couldn’t get me any information on threads, before going all the way back to the Stack Overflow question to follow <a href="https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html">the other link</a> provided in the answer.</p>
<h2 id="flame-graphs-with-perf">Flame graphs with perf</h2>
<p>This was what really got me somewhere. After recording performance data using <code>perf record -F max -a -g -- sleep 20</code> while the build was running, I was able to generate a flame graph which made the source of the performance problems quite apparent. The following command:</p>
<div id="cb3"><pre><code><span id="cb3-1"><span># taken from the flamegraph page linked above</span></span>
<span id="cb3-2"><span>perf</span> script <span>|</span> <span>stackcollapse-perf.pl</span> <span>|</span></span>
<span id="cb3-3">  <span># We're only interested in the qemu process</span></span>
<span id="cb3-4">  <span>grep</span> ^qemu <span>|</span></span>
<span id="cb3-5">  <span># Make the graph a lot less tall by collapsing multiple consecutive unknown stack frames together</span></span>
<span id="cb3-6">  <span>awk</span> <span>'/unknown/ { gsub("(\\[unknown];){1,}", "[unknown...];", $0) } { print }'</span> <span>|</span></span>
<span id="cb3-7">  <span>flamegraph.pl</span> <span>&gt;</span> flamegraph.svg <span># and generate a flamegraph</span></span></code></pre></div>
<p>produced this nifty interactive SVG graph:</p>

<p>The prevalence of “fid”-related functions is quite apparent in this graph. So I jumped into the 9p docs and QEMU source code to find out that fids are numbers which refer to open files in a 9p connection, similar to file descriptors in the POSIX file API — so there is one fid for every file that the guest has open.</p>
<p>Let’s look at the previous implementation of <code>get_fid</code>, which QEMU’s 9p server uses in implementations of <code>stat</code> (getting file metadata), <code>open</code> (opening a file), and <code>read</code> (reading data from an open file), amongst others:</p>
<div id="cb4"><pre><code><span id="cb4-1"><span>static</span> V9fsFidState <span>*</span>coroutine_fn get_fid<span>(</span>V9fsPDU <span>*</span>pdu<span>,</span> <span>int32_t</span> fid<span>)</span></span>
<span id="cb4-2"><span>{</span></span>
<span id="cb4-3">    <span>int</span> err<span>;</span></span>
<span id="cb4-4">    V9fsFidState <span>*</span>f<span>;</span></span>
<span id="cb4-5">    V9fsState <span>*</span>s <span>=</span> pdu<span>-&gt;</span>s<span>;</span></span>
<span id="cb4-6"></span>
<span id="cb4-7">    <span>// Blog note: I've omitted some parts that are irrelevant to performance here.</span></span>
<span id="cb4-8">    QSIMPLEQ_FOREACH<span>(</span>f<span>,</span> <span>&amp;</span>s<span>-&gt;</span>fid_list<span>,</span> next<span>)</span> <span>{</span></span>
<span id="cb4-9">        <span>if</span> <span>(</span>f<span>-&gt;</span>fid <span>==</span> fid<span>)</span> <span>{</span></span>
<span id="cb4-10">            <span>return</span> f<span>;</span></span>
<span id="cb4-11">        <span>}</span></span>
<span id="cb4-12">    <span>}</span></span>
<span id="cb4-13">    <span>return</span> NULL<span>;</span></span>
<span id="cb4-14"><span>}</span></span></code></pre></div>
<p>QEMU iterates over <code>fid_list</code> to find the desired fid’s data. Finding an entry in a list by iterating over has a time complexity of <span><em>O</em>(<em>n</em>)</span> where <span><em>n</em></span> is the size of the list — in this case, the list of <em>all the files the guest has open</em> — so this is expensive! Moreover, some inspection of QSIMPLEQ (QEMU simple queue) reveals that it’s implemented as a linked list, a data structure which tends to exhibit poor cache locality.</p>
<p>Since my test copies many files from the 9p filesystem, as well as being booted from it, this lookup happens very frequently:</p>
<ul>
<li>One <code>stat</code> for getting a file’s metadata (permissions in particular)</li>
<li>One <code>open</code> to get a handle on a file</li>
<li>One <code>read</code> for small files, or many for larger files, to get the contents of the file</li>
</ul>
<p>That makes at least 3 operations which perform the lookup, for each of the 278000 files, which bring the inefficient lookup into a hot code path. <em>This</em> was the reason for the slowness.</p>
<h2 id="fixing-it">Fixing it</h2>
<p>What we really want is a structure where we can look up entries by fid more cheaply. We can’t just use an array-based vector, which would consistently give us <span><em>O</em>(1)</span> lookup, because fids are chosen by the client: we can’t rely on every newly allocated fid just being the smallest unoccupied one, and need to support arbitrary 32-bit integers. I opted for a hash table, as conveniently <a href="https://developer-old.gnome.org/glib/stable/glib-Hash-Tables.html">implemented by glib</a>, which QEMU already depends on. That provides us with <span><em>O</em>(1)</span> best-case complexity, while keeping the worst case at <span><em>O</em>(<em>n</em>)</span>. The exact real-world performance characteristics are significantly more complex than with the linked list, and there may be marginally more suitable data structures (or hash table implementations) out there, but we’re looking for a big easy win and not micro-optimisation here.</p>
<p>Rewriting the relevant code was surprisingly simple and uneventful, to the point that once I had it compiling, it just worked (an experience I’m not sure I’ve ever had before with C!). The results were spectacular: my previously &gt;2h test finished in 7 minutes. It also reduces the build time of NixOS’s ZFS AWS image from 19 minutes to 1. It was pretty clear to me that this needed to go upstream.</p>
<h2 id="contributing-the-fix">Contributing the fix</h2>
<p>QEMU uses an <a href="https://www.qemu.org/docs/master/devel/submitting-a-patch.html">email-based workflow</a>, where patches are sent as emails to a mailing list which maintainers are subscribed to. I hadn’t done this before, and it went somewhat chaotically, as you can see by looking at the threads
(<a href="https://lists.nongnu.org/archive/html/qemu-devel/2022-09/msg00472.html">v1</a>
<a href="https://lists.nongnu.org/archive/html/qemu-devel/2022-09/msg01266.html">v3</a>
<a href="https://lists.nongnu.org/archive/html/qemu-devel/2022-09/msg04051.html">v1a</a>
<a href="https://lists.nongnu.org/archive/html/qemu-devel/2022-09/msg04575.html">v5</a>
<a href="https://lists.nongnu.org/archive/html/qemu-devel/2022-10/msg00370.html">v6</a>
) in the list archives. There’s a lot of space for mistakes in email-based patch submission, and I made several:</p>
<ul>
<li>Forgetting to reply-all when answering review comments, so that the reply is visible to everyone interested and not just the reviewer</li>
<li>Missing the version tag on a resubmission (in my case, this was because of <a href="https://lists.nongnu.org/archive/html/qemu-devel/2022-09/msg01795.html">misleading docs</a>)</li>
<li>Sending a resubmission as a reply to the previous thread (that was just me failing to read the docs)</li>
<li>Losing a bunch of patch-documenting work while using git-publish because <code>git send-email</code> failed (nixpkgs does not enable Git’s email functionality in the default package, and git-publish unconditionally deletes the working files once the send command has run).</li>
</ul>
<p>But the reviewers, especially Christian Schoenebeck (thanks!) were helpful and patient, and in the end <a href="https://lists.nongnu.org/archive/html/qemu-devel/2022-10/msg00909.html">the patch made it</a> (though an apparently unrelated bug was discovered) and will soon be finding its way to a QEMU version near you!
If you can’t wait and you use Nix, you can pull the patch in with this overlay:</p>
<div id="cb5"><pre><code><span id="cb5-1"><span>final</span><span>:</span> <span>prev</span><span>:</span> <span>{</span></span>
<span id="cb5-2">  <span>qemu</span> <span>=</span> prev.qemu.overrideAttrs <span>(</span><span>o</span><span>:</span> <span>{</span></span>
<span id="cb5-3">    <span>patches</span> <span>=</span> o.patches <span>++</span> <span>[</span> <span>(</span>prev.fetchpatch <span>{</span></span>
<span id="cb5-4">      <span>name</span> <span>=</span> <span>"qemu-9p-performance-fix.patch"</span><span>;</span></span>
<span id="cb5-5">      <span>url</span> <span>=</span> <span>"https://gitlab.com/lheckemann/qemu/-/commit/8ab70b8958a8f9cb9bd316eecd3ccbcf05c06614.patch"</span><span>;</span></span>
<span id="cb5-6">      <span>sha256</span> <span>=</span> <span>"sha256-PSOv0dhiEq9g6B1uIbs6vbhGr7BQWCtAoLHnk4vnvVg="</span><span>;</span></span>
<span id="cb5-7">    <span>})</span> <span>];</span></span>
<span id="cb5-8">  <span>});</span></span>
<span id="cb5-9"><span>}</span></span></code></pre></div>
<h2 id="outcomes">Outcomes</h2>
<p>While the core of what I changed wasn’t hugely complex, this adventure had many collateral (positive!) outcomes for me:</p>
<ul>
<li>I wrote Nix packages for <a href="https://github.com/NixOS/nixpkgs/pull/190335">quickstack</a> (including an upstream PR that makes the build more generic) and <a href="https://github.com/NixOS/nixpkgs/pull/190309">git-publish</a>.</li>
<li>I used <code>perf</code> to work on performance issues for the first time, learning how useful it is and basics of how to use it.</li>
<li>I learnt that different methods of profiling can yield wildly different results — the results from perf painted a very different picture from the poor man’s profiler; I don’t understand why yet, but I’ll spend some more time learning about them soon.</li>
<li>I submitted my first patch to QEMU, getting to know the code a little.</li>
<li>I submitted my second patch to QEMU (trying to fix the patch submission docs).</li>
<li>I learnt how to submit patches by email, and which mistakes to avoid.</li>
<li>I learnt about the 9p protocol.</li>
<li>My tests now run much faster!</li>
<li>And I got the warm fuzzy feeling of having made a valuable contribution.</li>
</ul>
<p>My takeaway from it? Digging into a frustrating problem — even if I’m totally unfamiliar with the code involved and the technology I’m using — can be hugely rewarding!</p>
<p>Not only have I made my own work on the tests a lot more pleasant by reducing turnaround time, but this will go out to <em>all</em> QEMU users, and significantly reduce the load generated by installer tests on NixOS’s build farm.
This is what I love about working on open source software: if something’s wrong, I have the means to go in there and fix it, and can pass the benefits on to everyone else who uses it.</p>
<p>It doesn’t always go as well as this, but this kind of experience can make a great many less successful adventures worth it.</p>
    </section>
</article>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Running NetBSD on IBM ThinkPad 380Z (161 pts)]]></title>
            <link>https://luke8086.dev/netbsd-on-thinkpad-380z.html</link>
            <guid>42438431</guid>
            <pubDate>Tue, 17 Dec 2024 04:51:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://luke8086.dev/netbsd-on-thinkpad-380z.html">https://luke8086.dev/netbsd-on-thinkpad-380z.html</a>, See on <a href="https://news.ycombinator.com/item?id=42438431">Hacker News</a></p>
<div id="readability-page-1" class="page">

<p>
  Launched in 1998, the 380Z was one very fine ThinkPad.

  It was the last ThinkPad to come in the classic bulky and rectangular form factor.

  It was also one of the first to feature a huge 13.3" TFT display, powerful 233MHz Pentium II, and whopping 160 megs of RAM.
</p>

<p>
  I recently stumbled upon one in perfect condition on eBay, and immediately thought it'd be a cool vintage gadget to put on the desk.

  I only wondered if I could still use it for some slow-paced, distraction-free coding, using reasonably modern software.
</p>

<p>
  I evaluated a bunch of contemporary operating systems, including different variants of BSD and Linux.

  Usually, the experience was underwhelming in terms of performance, hardware support and stability.

  Well... except for NetBSD, which gave me such perfectly smooth ride, that I thought it was worth sharing.
</p>

<img src="https://luke8086.dev/images/netbsd-photo-380z.jpg" alt="ThinkPad 380Z booting NetBSD">

<h2>Upgrading hard drive</h2>

<p>
  First things first, to improve performance, I've replaced the original HDD with a 16GB mSATA.

  The 380Z comes with a standard 44-pin PATA interface, so I plugged it in through a generic mSATA-to-PATA adapter.

  It works <i>almost</i> seamlessly.
</p>

<p>
  One minor issue is that the BIOS-reported size is limited to 8GB.

  It doesn't matter to operating systems, but confuses some bootloaders.

  In general it's safer to use a smaller root partition and a separate one for <code>/home</code>. 

  However, NetBSD worked just fine off a single full-disk partition.
</p>

<h2>Connecting to network</h2>

<p>
  The 380Z doesn't have a built-in network card.

  Fortunately it has both CardBus and USB slots, so there are plenty of options.
</p>

<p>
  One that worked for me was Edimax EW-7108PCg — a WiFi CardBus card based on a RT2561S chipset.

  Another one was a generic no-name USB-to-LAN adapter based on RTL8153.
</p>

<h2>Booting the installer</h2>

<p>
  Despite having a USB port, the 380Z doesn't support USB boot.

  My CD drive was also erratic and practically unusable - a common issue in vintage laptops.

  Fortunately, the 380Z supports yet another, rather unusual boot option — using a CardBus disk.
</p>

<p>
  So I grabbed the <code>NetBSD-10.0-i386-install.img</code> image and wrote to a Compact Flash card.

  Then, plugged it in using a generic CF-to-CardBus adapter, and off it went.

  For NetBSD, booting from CardBus is business as usual, no questions asked.
</p>

<p>
  To be fair, with other systems it's also doable, but usually requires some tinkering to mount the root filesystem.
</p>


<h2>Installation</h2>

<p>
  NetBSD comes with a lightweight, friendly, text-mode installer.

  Step by step, it guides you through setting the keyboard layout, partitioning, selecting distribution sets, setting up the network, timezone, root shell and password, enabling the package manager, choosing optional daemons, and adding a user account.
</p>

<p>
  Unless you go for automatic full-disk mode, the trickiest part may be partitioning, since NetBSD uses its own custom scheme on top of MBR partitions.

  Fortunately, the whole process is <a href="https://www.netbsd.org/docs/guide/en/" target="_blank">well documented</a>.
</p>

<p>
  In my case, when prompted for distribution sets, I selected "custom installation", and selected all sets except for source and debug ones.

  The resulting installation took about 1.6GB of disk space, and included the entire X11 environment.
</p>

<img src="https://luke8086.dev/images/netbsd-shot-install.png" alt="NetBSD installer">

  
<h2>Enabling framebuffer</h2>

<p>
  NetBSD supports VESA console framebuffer, though it's not enabled by default, and not particularly advertised in the docs.

  Turning it on is as simple as adding <code>vesa on; vesa 1024x768;</code> commands to a menu item in <code>/boot.cfg</code>.
</p>

<p>
  By the way, it's too bad that NetBSD's console doesn't support unicode, so it's not very usable without X.

  I think this could be a killer feature for hardware with even less RAM.
</p>

<h2>Saving RAM</h2>

<p>
  Even NetBSD comes with some bloatware ;-)

  To save as much RAM as possible, you can turn it off by adding to <code>/etc/rc.conf</code>:
</p>

<pre><code>inetd=NO
postfix=NO
cron=NO
virecover=NO
makemandb=NO
powerd=NO
syslogd=NO
</code></pre>

<p>
  You can also reduce the amount of consoles by commenting them out in <code>/etc/ttys</code>.
</p>

<p>
  After reboot, you'll have a pretty clean slate:
</p>


<pre><code>t380z# ps auxc
USER PID %CPU %MEM   VSZ   RSS TTY   STAT STARTED    TIME COMMAND
root   0  0.0 18.6     0 30300 ?     DKl   3:45PM 0:00.08 system
root   1  0.0  0.9  5836  1504 ?     Ss    3:45PM 0:00.08 init
root 345  0.0  2.0 12724  3292 ?     Ss    3:45PM 0:00.09 wpa_supplicant
root 707  0.0  1.0  6104  1544 ttyE0 O+    3:45PM 0:00.03 ps
root 711  0.0  3.1 12848  4988 ttyE0 Ss    3:45PM 0:00.54 login
root 712  0.0  1.4  6576  2216 ttyE0 S     3:45PM 0:00.24 sh
root 709  0.0  1.0  5744  1556 ttyE1 Ss+   3:45PM 0:00.03 getty

t380z# cat /proc/meminfo
        total:    used:    free:  shared: buffers: cached:
Mem:  139943936 32768000 107175936        0  8208384 18546688
Swap: 167759872        0 167759872
MemTotal:    136664 kB
MemFree:     104664 kB
MemShared:        0 kB
Buffers:     104664 kB
Cached:       18112 kB
SwapTotal:   163828 kB
SwapFree:    163828 kB
</code></pre>

<h2>WireGuard</h2>

<p>
  In case you'd like to use WireGuard, NetBSD's got you covered.

  It supports it out of the box, with no extra dependencies.
  
  The entire documentation is a single man page and it's very straighforward.

  However, it only shows commands to manually execute, there's no dedicated config file.

  To make your setup persistent, you can simply add it to <code>/etc/rc.local</code>:
</p>

<pre><code>echo -n 'Setting up wireguard: '

ifconfig wg0 create
ifconfig wg0 inet [ip/prefix]
wgconfig wg0 set private-key /etc/wg/key
wgconfig wg0 add peer [peer-name] \
        [public-key]
        --allowed-ips=[ip/prefix] \
        --endpoint=[endpoint]
ifconfig wg0 up

echo 'done.'
</code></pre>

<h2>Setting locale</h2>

<p>
  The default locale in NetBSD is C.

  The installer doesn't prompt you to change it, and there isn't any dedicated config file.

  Instead, you need to set it manually, for example in <code>/etc/profile</code> or <code>~/.profile</code>:
</p>

<pre><code>export LANG=en_US.UTF-8
export LC_ALL=en_US.UTF-8
export LC_COLLATE=en_US.UTF-8
</code></pre>

<h2>The X server</h2>

<p>
  The GPU on 380Z is a NeoMagic MagicMedia 256AV and it has a dedicated driver in Xorg, called <code>neomagic</code>.

  However, only in NetBSD it worked out of the box without any tinkering, just by running <code>startx</code>.

  On other systems I tried, it required some adjustments or even resorting to plain VESA/FB drivers.
</p>


<h2>Window manager</h2>

<p>
  The default desktop environment of NetBSD, as included in the basesystem, consists of CTWM, XClock and XTerm.

  It looks <a href="https://luke8086.dev/images/netbsd-shot-ctwm.jpg" target="_blank">like this</a>.

  It's as minimal as it gets, it's not too ugly, and it's somewhat usable.
</p>

<p>
  I tried it for a bit, but couldn't get to like CTWM.

  Instead I've replaced it with <a href="https://fastestcode.org/emwm.html" target="_blank">EMWM</a> (Enhanced Motif Window Manager), which is similarly lightweight.
</p>

<img src="https://luke8086.dev/images/netbsd-shot-emwm.png" alt="NetBSD running EMWM">

<h2>Terminal emulator</h2>

<p>
  I haven't found a terminal with a smaller memory footprint than XTerm.

  Even urxvt or st were larger.

  However, for me XTerm is actually good enough, so I didn't spent much time looking for alternatives.

  I only adjusted some colors, changed the font to Terminus, and added shortcuts for copy-pasting:
</p>

<pre><code>XTerm*VT100.background: black
XTerm*VT100.foreground: grey90
XTerm*VT100.faceName: Terminus
XTerm*VT100.faceSize: 12
XTerm*VT100.allowBoldFonts: false
XTerm*VT100.translations: #override \
      Ctrl Shift <key>V:    insert-selection(CLIPBOARD) \n\
      Ctrl Shift <key>C:    copy-selection(CLIPBOARD)
</key></key></code></pre>

<h2>The shell</h2>

<p>
  In the basesystem, NetBSD comes with sh, csh and ksh.

  I've never bothered learning csh, but sh is fine for basic administrative tasks, and ksh is somewhere in between sh and bash in terms of functionality.

  I tried using ksh for a while but I kept missing features.

  Eventually I gave up on it and installed bash from packages.
</p>

<h2>Browsing web</h2>

<p>
  The most reasonable web browser for the 380Z is <a href="https://dillo-browser.github.io/" target="_blank">Dillo</a>.

  It lacks support for JavaScript and modern CSS layouts, so many pages render all over the place, but it's still useful for reading docs and static pages.

  To my surprise, Google seems to work, and doesn't block it with captcha.
</p>

<p>
  One could argue Dillo is <i>exactly</i> what a web browser is supposed to be — a productivity tool for browsing nicely-formatted hypertext documents.

  I only wish I had some time to contribute to its development.

  Well, maybe when I retire...
</p>

<img src="https://luke8086.dev/images/netbsd-shot-web.png" alt="NetBSD running Dillo browser">

<h2>Playing music</h2>

<p>
  NetBSD was the only system I tried that properly supported the built-in Crystal CS4237B soundcard.

  In fact, it detected it twice, with two different drivers, <code>wss</code>&nbsp;and&nbsp;<code>sb</code>.

  Unfortunately this caused a conflict and they both emited garbage noise.

  I was able to fix it by adding <code>usermod disable wss</code> to the bootloader line.
</p>

<p>
  The most viable player is mpg123.

  It manages to play an online low-fi radio stream with only slight stuttering when other CPU-heavy tasks are running.

  Itself it seems to take around 10-20% of the CPU time.

  The audio quality of the soundcard is also just adequate for low-fi music, so for sure it won't be too distracting.
</p>

<h2>Once set up, what's it good for?</h2>

<p>
  The 380Z has a very comfortable, high-profile, classic ThinkPad keyboard, as well as a crispy 4:3 display.

  It's not as snappy as your latest MacBook, but it's fine for many terminal-based tasks, for example:
</p>

<ul>
  <li>any kind of work over SSH, even through WireGuard</li>
  <li>tinkering with UNIX and learning its internals</li>
  <li>low-level coding in C, assembly, etc.</li>
  <li>developing TUI and even lightweight GUI apps</li>
  <li>taking notes, writing blog posts</li>
  <li>developing a modern web browser (building dillo only takes ~20min) ;-)</li>
  <li>learning how to solve problems in a resource constrained environment</li>
  <li><span><a href="https://luke8086.dev/retronews.html" target="_blank">wasting time</a> on social media</span> practicing patience and mindfulness</li>
</ul>

<h2>Final thoughts on NetBSD</h2>

<p>
  NetBSD is a lightweight, compact, finely engineered system.

  It doesn't get as much attention as its BSD cousins, not to mention Linux, but I just cannot overstate how <i>pleasant</i> it is to use.
</p>

<p>
  It doesn't overwhelm you with thousands of packages and dozens of boot services right in a fresh install.

  On the contrary, it does only what you tell it to do.

  It puts you in charge and makes you feel like you can understand it top to bottom.
</p>

<p>
  It happily boots on a 25-year old machine, like you moved back in time, but still provides you with a full repository of the latest software.
</p>

<p>
  It may not be mainstream enough for a daily driver, but I think it's the ultimate UNIX to put on a spare, underpowered machine.
</p>

<p><em>~luke, 2024-12-16</em></p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[MIT study explains why laws are written in an incomprehensible style (264 pts)]]></title>
            <link>https://news.mit.edu/2024/mit-study-explains-laws-incomprehensible-writing-style-0819</link>
            <guid>42438175</guid>
            <pubDate>Tue, 17 Dec 2024 03:52:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.mit.edu/2024/mit-study-explains-laws-incomprehensible-writing-style-0819">https://news.mit.edu/2024/mit-study-explains-laws-incomprehensible-writing-style-0819</a>, See on <a href="https://news.ycombinator.com/item?id=42438175">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
          

            <p>Legal documents are notoriously difficult to understand, even for lawyers. This raises the question: Why are these documents written in a style that makes them so impenetrable?</p><p>MIT cognitive scientists believe they have uncovered the answer to that question. Just as “magic spells” use special rhymes and archaic terms to signal their power, the convoluted language of legalese acts to convey a sense of authority, they conclude.</p><p>In a study appearing this week in the journal of the <em>Proceedings of the National Academy of Sciences</em>, the researchers found that even non-lawyers use this type of language when asked to write laws.</p><p>“People seem to understand that there’s an implicit rule that this is how laws should sound, and they write them that way,” says Edward Gibson, an MIT professor of brain and cognitive sciences and the senior author of the study.</p><p>Eric Martinez PhD ’24 is the lead author of the study. Francis Mollica, a lecturer at the University of Melbourne, is also an author of the <a href="https://www.pnas.org/doi/10.1073/pnas.2405564121" target="_blank">paper</a>.</p><p><strong>Casting a legal spell</strong></p><p>Gibson’s research group has been studying the unique characteristics of legalese since 2020, when Martinez came to MIT after earning a law degree from Harvard Law School. In a <a href="https://news.mit.edu/2022/legal-writing-understanding-0307" target="_blank">2022 study</a>, Gibson, Martinez, and Mollica analyzed legal contracts totaling about 3.5 million words, comparing them with other types of writing, including movie scripts, newspaper articles, and academic papers.</p><p>That analysis revealed that legal documents frequently have long definitions inserted in the middle of sentences — a feature known as “center-embedding.” Linguists have previously found that this kind of structure can make text much more difficult to understand.</p><p>“Legalese somehow has developed this tendency to put structures inside other structures, in a way which is not typical of human languages,” Gibson says.</p><p>In a <a href="https://news.mit.edu/2023/new-study-lawyers-legalese-0529" target="_blank">follow-up study</a> published in 2023, the researchers found that legalese also makes documents more difficult for lawyers to understand. Lawyers tended to prefer plain English versions of documents, and they rated those versions to be just as enforceable as traditional legal documents.</p><p>“Lawyers also find legalese to be unwieldy and complicated,” Gibson says. “Lawyers don’t like it, laypeople don’t like it, so the point of this current paper was to try and figure out why they write documents this way.”</p><p>The researchers had a couple of hypotheses for why legalese is so prevalent. One was the “copy and edit hypothesis,” which suggests that legal documents begin with a simple premise, and then additional information and definitions are inserted into already existing sentences, creating complex center-embedded clauses.</p><p>“We thought it was plausible that what happens is you start with an initial draft that’s simple, and then later you think of all these other conditions that you want to include. And the idea is that once you’ve started, it’s much easier to center-embed that into the existing provision,” says Martinez, who is now a fellow and instructor at the University of Chicago Law School.</p><p>However, the findings ended up pointing toward a different hypothesis, the so-called “magic spell hypothesis.” Just as magic spells are written with a distinctive style that sets them apart from everyday language, the convoluted style of legal language appears to signal a special kind of authority, the researchers say.</p><p>“In English culture, if you want to write something that’s a magic spell, people know that the way to do that is you put a lot of old-fashioned rhymes in there. We think maybe center-embedding is signaling legalese in the same way,” Gibson says.</p><p>In this study, the researchers asked about 200 non-lawyers (native speakers of English living in the United States, who were recruited through a crowdsourcing site called Prolific), to write two types of texts. In the first task, people were told to write laws prohibiting crimes such as drunk driving, burglary, arson, and drug trafficking. In the second task, they were asked to write stories about those crimes.</p><p>To test the copy and edit hypothesis, half of the participants were asked to add additional information after they wrote their initial law or story. The researchers found that all of the subjects wrote laws with center-embedded clauses, regardless of whether they wrote the law all at once or were told to write a draft and then add to it later. And, when they wrote stories related to those laws, they wrote in much plainer English, regardless of whether they had to add information later.</p><p>“When writing laws, they did a lot of center-embedding regardless of whether or not they had to edit it or write it from scratch. And in that narrative text, they did not use center-embedding in either case,” Martinez says.</p><p>In another set of experiments, about 80 participants were asked to write laws, as well as descriptions that would explain those laws to visitors from another country. In these experiments, participants again used center-embedding for their laws, but not for the descriptions of those laws.</p><p><strong>The origins of legalese</strong></p><p>Gibson’s lab is now investigating the origins of center-embedding in legal documents. Early American laws were based on British law, so the researchers plan to analyze British laws to see if they feature the same kind of grammatical construction. And going back much farther, they plan to analyze whether center-embedding is found in the Hammurabi Code, the earliest known set of laws, which dates to around 1750 BC.</p><p>“There may be just a stylistic way of writing from back then, and if it was seen as successful, people would use that style in other languages,” Gibson says. “I would guess that it’s an accidental property of how the laws were written the first time, but we don’t know that yet.”</p><p>The researchers hope that their work, which has identified specific aspects of legal language that make it more difficult to understand, will motivate lawmakers to try to make laws more comprehensible. Efforts to write legal documents in plainer language date to at least the 1970s, when President Richard Nixon declared that federal regulations should be written in “layman’s terms.” However, legal language has changed very little since that time.</p><p>“We have learned only very recently what it is that makes legal language so complicated, and therefore I am optimistic about being able to change it,” Gibson says.&nbsp;</p>        

      </div><div>
  
  
  

      <header>
      <h2>Press Mentions</h2>
    </header>
  
  
  

  <div>
    <div><h3>Los Angeles Times</h3><p>A study by researchers at MIT and elsewhere has found that both lawyers and non-lawyers use legalese when asked to write about laws, reports June Casagrande for <em>The Los Angeles Times.&nbsp;</em>The "researchers tested the hypothesis by asking 200 participants to write laws prohibiting crimes like drunk driving and burglary,” explains Casagrande. “Then they asked them to write stories about those crimes. The laws they wrote contained unnecessarily long, labyrinthine sentences with lots of parenthetical explanations crammed in. The stories, however, were written simply, without the parenthetical information stuffing.”&nbsp;</p></div>
    <div><h3>Fast Company</h3><div><p>Researchers at MIT have uncovered a possible reason why legal documents can be so difficult to read, finding that “convoluted legalese often acts as a way to convey authority,” reports Joe Berkowitz for <em>Fast Company</em>. The researchers “tested whether nonlawyers would end up using legalese if asked to write legal documents,” explains Berkowitz. “In the end, all subjects wrote their laws with complex, center-embedded clauses.”</p></div></div>
    <div><h3>Futurism</h3><p>Researchers at MIT have found that the use of legalese in writing “to assert authority over those less versed in such language,” reports Noor Al-Sibai for <em>Futurism</em>. “By studying this cryptic take on the English language, the researchers are hoping to make legal documents much easier to read in the future,” explains Al-Sibai.</p></div>
</div>


    

  
  

  
  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Waymo will bring autonomous vehicles to Tokyo (175 pts)]]></title>
            <link>https://waymo.com/blog/2024/12/partnering-with-nihon-kotsu-and-go-on-our-first-international-road-trip</link>
            <guid>42438009</guid>
            <pubDate>Tue, 17 Dec 2024 03:17:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://waymo.com/blog/2024/12/partnering-with-nihon-kotsu-and-go-on-our-first-international-road-trip">https://waymo.com/blog/2024/12/partnering-with-nihon-kotsu-and-go-on-our-first-international-road-trip</a>, See on <a href="https://news.ycombinator.com/item?id=42438009">Hacker News</a></p>
<div id="readability-page-1" class="page"><article aria-labelledby="P0-7-title"><a href="https://waymo.com/blog/"><img alt="" role="presentation" src="https://waymo.com/static/images/blog/icon-left-arrow.svg"><img alt="" role="presentation" src="https://waymo.com/static/images/blog/icon-left-arrow-rollover.svg"><span>Back to all posts</span></a><section><div><picture><source srcset="https://images.ctfassets.net/e6t5diu0txbw/lHhourBhrG3ffzwe7rNqU/b952ca5a0e2c73bf60f058081b7b96b8/Waymo_GO_NihonKotsu_Hero.jpg?fm=webp 2x, https://images.ctfassets.net/e6t5diu0txbw/lHhourBhrG3ffzwe7rNqU/b952ca5a0e2c73bf60f058081b7b96b8/Waymo_GO_NihonKotsu_Hero.jpg?w=1440&amp;fm=webp" media="(min-width: 600px)" type="image/webp" width="1920" height="1080"><source srcset="https://images.ctfassets.net/e6t5diu0txbw/lHhourBhrG3ffzwe7rNqU/b952ca5a0e2c73bf60f058081b7b96b8/Waymo_GO_NihonKotsu_Hero.jpg 2x, https://images.ctfassets.net/e6t5diu0txbw/lHhourBhrG3ffzwe7rNqU/b952ca5a0e2c73bf60f058081b7b96b8/Waymo_GO_NihonKotsu_Hero.jpg?w=1440" media="(min-width: 600px)" type="image/jpeg" width="1920" height="1080"><source srcset="https://images.ctfassets.net/e6t5diu0txbw/lHhourBhrG3ffzwe7rNqU/b952ca5a0e2c73bf60f058081b7b96b8/Waymo_GO_NihonKotsu_Hero.jpg?fm=webp 2x, https://images.ctfassets.net/e6t5diu0txbw/lHhourBhrG3ffzwe7rNqU/b952ca5a0e2c73bf60f058081b7b96b8/Waymo_GO_NihonKotsu_Hero.jpg?w=1024&amp;fm=webp" media="(min-width: 600px) and (max-width: 1023px)" type="image/webp" width="1920" height="1080"><source srcset="https://images.ctfassets.net/e6t5diu0txbw/lHhourBhrG3ffzwe7rNqU/b952ca5a0e2c73bf60f058081b7b96b8/Waymo_GO_NihonKotsu_Hero.jpg 2x, https://images.ctfassets.net/e6t5diu0txbw/lHhourBhrG3ffzwe7rNqU/b952ca5a0e2c73bf60f058081b7b96b8/Waymo_GO_NihonKotsu_Hero.jpg?w=1024" media="(min-width: 600px) and (max-width: 1023px)" type="image/jpeg" width="1920" height="1080"><source srcset="https://images.ctfassets.net/e6t5diu0txbw/lHhourBhrG3ffzwe7rNqU/b952ca5a0e2c73bf60f058081b7b96b8/Waymo_GO_NihonKotsu_Hero.jpg?fm=webp 2x, https://images.ctfassets.net/e6t5diu0txbw/lHhourBhrG3ffzwe7rNqU/b952ca5a0e2c73bf60f058081b7b96b8/Waymo_GO_NihonKotsu_Hero.jpg?w=1024&amp;fm=webp" media="(max-width: 599px)" type="image/webp" width="1920" height="1080"><img alt="A white Waymo vehicle in front of a a radiating red spiral, with Nihon Kotsu, Waymo, and GO logos in the footer" loading="lazy" srcset="https://images.ctfassets.net/e6t5diu0txbw/lHhourBhrG3ffzwe7rNqU/b952ca5a0e2c73bf60f058081b7b96b8/Waymo_GO_NihonKotsu_Hero.jpg 2x, https://images.ctfassets.net/e6t5diu0txbw/lHhourBhrG3ffzwe7rNqU/b952ca5a0e2c73bf60f058081b7b96b8/Waymo_GO_NihonKotsu_Hero.jpg?w=1024" src="https://images.ctfassets.net/e6t5diu0txbw/lHhourBhrG3ffzwe7rNqU/b952ca5a0e2c73bf60f058081b7b96b8/Waymo_GO_NihonKotsu_Hero.jpg?w=420" width="1920" height="1080"></picture></div><div><p>こんにちは(Konnichiwa) and hello, Japan! We're thrilled to announce that Waymo, in partnership with Nihon Kotsu and GO, will bring our autonomous vehicles to Tokyo for our first international <a href="https://waymo.com/blog/2023/11/road-trip-how-our-cross-country-testing-helps-advance-the-waymo-driver/"><u>road trip</u></a>. There, our Driver will learn and adapt to left-hand traffic and new driving nuances associated with operating in one of the world's most densely populated urban environments.</p><p>Our upcoming road trip to Tokyo gives us the chance to work alongside local partners, government officials, and community groups to understand the new landscape. We’ll learn how Waymo can serve Tokyo’s residents and become a beneficial part of the city’s transportation ecosystem. And every step of the way, we’ll take a rigorous approach to validating our technology's safety and performance.</p><p>This expansion into Japan aligns with the country's vision for the future of transportation. Over the years, the Japanese National and Tokyo Metropolitan governments have been proactively working to address the evolving transportation needs of society and foster the adoption of innovative technologies that can enhance safety and mobility. We are engaging with Japanese policymakers, regulators, and local safety officials to ensure a responsible and seamless implementation of Waymo's technology to Tokyo's streets.</p><p>The first Waymo, all-electric Jaguar I-PACEs will arrive in Tokyo in early 2025. Our partner, Nihon Kotsu, Tokyo’s largest taxi company, will oversee the management and servicing of the Waymo vehicles. The companies are working closely together to train Nihon Kotsu’s team on operating vehicles equipped with Waymo's autonomous driving system. Initially, Nihon Kotsu drivers will operate the vehicles manually to map key areas of the Japanese capital, including Minato, Shinjuku, Shibuya, Chiyoda, Chūō, Shinagawa, and Kōtō.&nbsp;</p><p>Through this initial phase in Tokyo, we’ll gain valuable experience that accelerates the development of the Waymo Driver, allowing us to evaluate how our AI-powered driver generalizes to new environments through simulation. This expansion follows Waymo's best-in-class <a href="https://waymo.com/blog/2020/10/sharing-our-safety-framework/"><u>safety framework</u></a>—the same approach that guided us from the world's first fully autonomous ride on public roads to tens of millions of miles on US roads, and soon, our first kilometers in Tokyo.&nbsp;&nbsp;</p><p>We look forward to sharing more as we introduce Waymo to Tokyo residents, providing updates on our progress along the way.</p></div></section></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Advent of Code on the Nintendo DS (173 pts)]]></title>
            <link>https://sailor.li/aocnds.html</link>
            <guid>42436440</guid>
            <pubDate>Mon, 16 Dec 2024 22:57:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sailor.li/aocnds.html">https://sailor.li/aocnds.html</a>, See on <a href="https://news.ycombinator.com/item?id=42436440">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-text">
            
<p><img src="https://sailor.li/static/aocnds/img/hero.png"></p><p>
    It is December. That means annoying Christmas things are everywhere, including but not limited
    to the annual programming semi-competition known as Advent of Code.
</p>

<p>
    The problem with Advent of Code is that it is a waste of time. Most of the puzzles are in the
    realm of either string processing (somewhat applicable to programming), logic puzzles (not
    really applicable to most programming), or stupid gotchas in the input format (annoyingly, very
    applicable to most programming). So to combat this a lot of people use Advent of Code as an
    excuse to learn a new programming language that they wouldn't otherwise have a reason to use.
</p>

<p>
    In this spirit, I've decided to do Advent of Code 2024 in Rust, a language don't I use that
    often.
</p>

<blockquote>
    I do actually know Rust, but I never learned how to use it. I just started writing it because I
    was born with an innate knowledge of the language, similar to how I know Java or Kotlin despite
    never having learned them.
</blockquote>

<p>
    However, writing standard userland Rust on a system with a runtime is too easy; it's like
    writing a more annoying version of Java. So instead I will write it for an embedded system which
    has no runtime, a limited amount of memory, and most importantly little to no existing ecosystem
    for me to fall back on.
</p>

<blockquote>
    All the code for this project is available
    <a href="https://github.com/Fuyukai/aocnds">in the repository.</a>
</blockquote>

<blockquote>
    <h4>Warning!</h4>

    <p>
        This post is long, verbose, and explains a lot about things that aren't relevant because
        this was written in tandem with developing the project and is meant to show everything I
        learned on the way. There's a lot of things here that might seem obvious to people who know
        things about executables or embedded systems.
    </p>

    <p>
        If I wanted to do this fast, I could've just copied everything BlocksDS does, but I
        deliberately avoided anything related to other Nintendo DS homebrew projects or SDKs as they
        are focused on teaching you how to put all of the blocks together, but not how to actually
        build the blocks yourself.
    </p>
</blockquote>

<h2>A quick overview of the Nintendo DS (Nitro)</h2>

<p>
    As the title of this post states, I'm going to be writing it on the Nintendo DS (the original
    edition). Whilst there is a fair bit of interest in the "embedded gaming" scene of its
    predecessor systems (the Game Boy and Game Boy Advance), as well as its successor system the
    3DS, the homebrew and emulation scene for the DS itself is relatively limited due to the more
    esoteric design of the system.
</p>

<p>
    The DS's codename was "Nitro", hence the model number NTR-001. Its successor system, the DSi,
    has the codename of "Twilight" with the model number TWL-001. I'll refer to the system
    exclusively using its codename, as Nitro sounds cooler and is easier to refer to than "Nintendo
    DS" is.
</p>

<blockquote>
    If you're interested in a more comprehensive overview of the system, I recommend this
    <a href="https://www.copetti.org/writings/consoles/nintendo-ds/">excellent article</a>
    by Rodrigo Copetti.
</blockquote>

<p>
    The Nitro uses two processors; an ARM946E-S running at 67MHz (in practice, less) which uses the
    ARM v5 instruction set, and an ARM7TDMI running at 33MHz which uses the ARM v4 instruction set.
</p>

<blockquote>
    <p>
        In fact, the ARM7 CPU is identical the one inside of the Game Boy Advance, meaning that when
        you play a GBA game on the Nitro it executes a small amount of code to lock out the main CPU
        and sets up the secondary processor into the same setup as the original console.
    </p>

    <p>
        The Twilight includes the same ARM7 CPU, just without the GBA slot. The 3DS/CTR also
        includes this CPU alongside its other two CPUs, which is used to run DS(i) games (and, when
        hacked, GBA games). Every Nintendo handheld up to the 3DS contains a GameBoy inside it!
    </p>
</blockquote>

<p>
    Unlike what you might expect, the two processors have different roles; the ARM9 processor is
    where the majority - if not all - of the game's code exists, and the ARM7 processor acts
    strictly as a coprocessor that controls interaction with the system's I/O. The two processors
    talk to each-other using a FIFO interface mapped in both processors' memory space. It's hard to
    get an actual source on this, but various comments spread across the internet talk about how
    most code on the ARM7 was heavily restricted by Nintendo, thus most of the power of the
    secondary processor goes entirely unused.
</p>

<p>
    This works out great for me, as it means I can focus only on writing code to run on the ARM9,
    and let the ARM7 spin idly.
</p>

<h2>Preparations</h2>

<p>
    I know a little bit about how the DS works from reverse-engineering a game over the last two
    years or so; but beyond that everything here is gleamed from a few sources:
</p>

<ul>
    <li>
        <a href="https://problemkaputt.de/gbatek.htm">GBATEK</a> which is a very large and very
        detailed technical reference on everything there is to know about the GameBoy Advance as
        well as the Nitro/Twilight.
    </li>
    <li>
        The aforementioned decompiled game,
        <a href="https://en.wikipedia.org/wiki/Infinite_Space">Infinite Space (2009)</a>. It's a
        pretty decent JRPG albeit unbelievably difficult, and is worth playing with cheats on a few
        times.
    </li>
    <li>
        The official ARM v5 ARM Architecture Reference Manual, (ARM v5 ARM) available from the A RM
        website.
    </li>
</ul>

<p>
    In addition, I set up an ARM cross-compiler using
    <a href="https://wiki.gentoo.org/wiki/Crossdev">Crossdev</a>, compiling only binutils and GDB.
</p>

<h2>Getting a working ROM</h2>

<p>There's three major tasks to do to finish this project:</p>

<ol>
    <li>Get something that even boots. This is 90% of the effort.</li>
    <li>Solve Advent of Code. This is also 90% of the effort.</li>
    <li>Actually display something to the screen. This is <i>also</i> 90% of the effort.</li>
</ol>

<h3>The sample program</h3>

<p>
    First, I need a sample program to actually <i>run</i> on the Nitro. Let's just write something
    that loops forever, helpfully annotated:
</p>

<pre data-snippet="00/00sample.rs"><code>#![no_std]   // Obviously, we don't have a runtime.
#![no_main]  // We use an extern "C" main instead of a Rust main.
#![allow(clippy::empty_loop)]  // Otherwise clippy gets mad at our infinite loop.

// This is an empty panic handler as we don't have any infrastructure to actually... well, handle
// panics in any form.
//
// On platforms with a runtime, ``std`` provides lots of helpful code to unwind all of the code
// and print a stacktrace, but we don't have that, so instead when ``panic!()`` (or, rather,
// ``core::panicking::panic``) is called, it calls this function instead.
//
// See https://fractalfir.github.io/generated_html/rustc_codegen_clr_v0_2_1.html for the
// nitty-gritty details.
#[panic_handler]
fn _handle_panic(_: &amp;core::panic::PanicInfo) -&gt; ! {
    loop {}
}

// Unsafe attributes are a new feature in Rust 2024, and means "you now need to prefix this
// attribute with unsafe".
//
// Without ``no_mangle``, this function would get optimised out (as nothing calls it), and even
// if it didn't, it would be given an unintelligible name so that it wouldn't conflict with
// functions from other packages called ``_start`` (as unwise as it would be to use that name).
//
// This also needs to be named ``_start``, or else the linker just won't output anything. More on
// that later!
#[unsafe(no_mangle)]
extern "C" fn _start() -&gt; ! {
    loop {}
}</code></pre>

<p>
    Next, Cargo/Rust needs to be configured to build for Nitro. The compiler triplet for ARM9 CPUs
    like the Nitro's is <code>armv5te-none-eabi</code>; i.e., ARM version 5, no OS (baremetal),
    using the embedded ABI. I could do <code>cargo build --target=armv5te-none-eabi</code> every
    time, but Cargo has the ability to do this automatically with the confusingly named
    <code>.cargo/config.toml</code> file:
</p>

<pre data-snippet="00/00config.toml"><code>[build]
target = "armv5te-none-eabi"</code></pre>

<p>Running <code>cargo build</code> now will grant me an error:</p>

<pre data-snippet="00/00cargo-build-err.txt"><code>$ cargo build
   Compiling aocnds v25.0.0 (/home/lura/dev/misc/aocnds)
error[E0463]: can't find crate for `core`
  |
  = note: the `armv5te-none-eabi` target may not be installed
  = help: consider downloading the target with `rustup target add armv5te-none-eabi`
  = help: consider building the standard library from source with `cargo build -Zbuild-std`</code></pre>

<p>
    The help text in this case is useless, as <code>armv5te-none-eabi</code> is a
    <a href="https://doc.rust-lang.org/nightly/rustc/platform-support.html">Tier 3</a>
    target and doesn't come with any pre-built standard library packages. Instead, I need to use the
    <a href="https://doc.rust-lang.org/cargo/reference/unstable.html#build-std"><code>build-std</code></a>
    feature of Cargo to compile the <code>core</code> (and, later, <code>alloc</code>) packages for
    my target.
</p>

<blockquote>
    This feature is (still) unstable and requires a nightly compiler + cargo version to use.
</blockquote>

<p>
    Now with this, I can build my program with a regular
    <code>cargo build --release</code>, which spits out a nice, 4440 byte
    <code>aocnds</code> executable (after stripping).
</p>

<h3>Wait, is that it?</h3>

<p>
    Okay, I don't have a ROM. But I do have a nice blob of ARM9 code that I can turn into a ROM,
    right?
</p>

<pre data-snippet="01/01-objcopy-broken.txt"><code># Just do a straight up memory copy with objcopy...
$ arm-none-eabi-objcopy -O binary aocnds arm9.bin
# Then disassemble it with objdump:
$ arm-none-eabi-objdump -D -b binary -marmv5te arm9.bin

arm9.bin:     file format binary


Disassembly of section .data:

00000000 &lt;.data&gt;:
       0:	00010010 	andeq	r0, r1, r0, lsl r0
       4:	00000001 	andeq	r0, r0, r1
       8:	00010010 	andeq	r0, r1, r0, lsl r0
       c:	00000001 	andeq	r0, r0, r1
	...
   10010:	eaffffff 	b	0x10014
   10014:	eafffffe 	b	0x10014</code></pre>

<blockquote>
    <p>
        If you don't know how to read a disassembly, there's three columns: the <i>address</i> of
        the instruction, the <i>raw bytes</i> of the instruction, and the <i>assembly</i>
        for the instruction.
    </p>
    <p>
        Objdump and other disassemblers don't discriminate between actual ARM machine code and
        random data included by the compiler and linker. Thanks to a quirk of the way the ARM
        instruction encoding was designed, most junk instructions end up being decoded as valid
        instructions, albeit nonsensical and usually nonfunctional.
    </p>
</blockquote>

<p>
    There's some junk at the beginning of the file, and then finally the actual code begins at
    offset 0x10010. By using <code>readelf</code> and <code>objdump</code> on the file, we can see
    that the 16 bytes of junk at the start is the same as the <code>.ARM.exidx</code> section:
</p>

<pre data-snippet="01/01-exidx.txt"><code>Disassembly of section .ARM.exidx:

000100d4 &lt;.ARM.exidx&gt;:
   100d4:       00010010        andeq   r0, r1, r0, lsl r0
   100d8:       00000001        andeq   r0, r0, r1
   100dc:       00010010        andeq   r0, r1, r0, lsl r0
   100e0:       00000001        andeq   r0, r0, r1</code></pre>

<p>
    The <code>0x10010</code> bytes of padding before the program text and after the ARM section is
    because the second header is loaded at a virtual address 0x10010 bytes later.
</p>

<pre data-snippet="01/01-progheaders.txt"><code>Program Headers:
  Type           Offset   VirtAddr   PhysAddr   FileSiz MemSiz  Flg Align
  PHDR           0x000034 0x00010034 0x00010034 0x000a0 0x000a0 R   0x4
  LOAD           0x000000 0x00010000 0x00010000 0x000e4 0x000e4 R   0x10000
  LOAD           0x0000e4 0x000200e4 0x000200e4 0x00008 0x00008 R E 0x10000
  GNU_STACK      0x000000 0x00000000 0x00000000 0x00000 0x00000 RW  0
  ARM_EXIDX      0x0000d4 0x000100d4 0x000100d4 0x00010 0x00010 R   0x4</code></pre>

<p>
    The code at <code>0x10010</code> is a <strong>B</strong>ranch instruction to
    <code>0x10014</code>, which is then a branch instruction to itself. Because the
    <code>b</code> instruction with a constant is always a relative jump,
    <code>eafffffe</code> means "jump zero bytes forwards"; or, <code>PC := PC</code> which matches
    the Rust code. Great!
</p>

<p>
    The outputted ARM9 blob can be packed into a ROM using
    <a href="https://github.com/devkitPro/ndstool"><code>ndstool</code></a> for now. Borrowing an
    ARM7 from a game, I can pack my blob like so:
</p>

<pre data-snippet="01/01-ndstool.txt"><code>$ ndstool -c rom.nds -9 arm9.bin -7 ~/aur/emulation/roms/nds/arm7-is.bin
$ ndstool -i rom.nds
...
0x20    ARM9 ROM offset                 0x200
0x24    ARM9 entry address              0x2000000
0x28    ARM9 RAM address                0x2000000
0x2C    ARM9 code size                  0x10018
...</code></pre>

<p><img src="https://sailor.li/static/aocnds/img/nogba_firstrun.png"></p><p>
    For the moment I'll use NO$GBA debug to load the rom; I'll use a better emulator with a GDB stub
    for debugging later. After hitting run, the emulated ARM9 skips past the junk at the start of
    the file (thankfully, that junk all decodes to <code>andeq</code> instructions, which don't do
    anything) until it reaches the infinite loop. Success! I have built my first rust binary for
    Nitro, and I didn't really have to do anything!
</p>

<h3>No, that's not it</h3>

<p>Unfortunately, I got lucky here. This program actually has some immediate problems:</p>
<ul>
    <li>
        <code>.bss</code> has not been zeroed, which is UB. At least I don't have any global
        variables.
    </li>
    <li>The stack pointer is set to a junk value, not real memory.</li>
</ul>
<p>Oh, but there's another really important one:</p>
<ul>
    <li>The code is not running where it expects to!</li>
</ul>

<p>Nitro has, essentially, three main memory areas:</p>
<ul>
    <li>
        <p>
            The Instruction Tightly Coupled Memory (ITCM), which is mapped at 0x0 and mirrored at
            <code>0x01000000</code> (but most commercial Nitro software uses the mirror at
            <code>0x01ff8000</code>). This is 32KiB.
        </p>
    </li>
    <li>
        <p>
            The Data Tightly Coupled Memory (DOCM), which is movable throughout the entire memory
            space but in practice is usually mapped at <code>0x027e0000</code>. This is only 16KiB.
            Most commercial Nitro software uses this for the stack.
        </p>
    </li>
    <li>
        <p>
            Main memory, which is mapped at <code>0x02000000</code>. This is 4MiB on retail units
            and 8MiB on debug units.
        </p>
    </li>
</ul>

<blockquote>
    There's also the "shared RAM", which is 32KiB in total and can be allocated to either the ARM7
    or the ARM9, either half and half or all to one. The ARM7 is crying out for memory as it only
    has 64KiB for itself, so this can all be allocated to the ARM7 and ignored. I believe all
    official Nitro games do this.
</blockquote>

<p>
    At boot, the software running on the cartridge is copied to the very bottom of main memory, and
    the BIOS jumps to the entrypoint specified by the cart header. With my binary, the entrypoint
    was set to be the very first address of main memory (the gigantic stack of empty
    <code>andeq</code>s) with the <code>_start</code> function trailing it. The more pressing manner
    is that the code in <code>_start</code> was built and linked with the assumption that it would
    be loaded at <code>0x200e4</code>, which it very much is not.
</p>

<blockquote>
    <p>A quick terminology lesson here, as some of those terms might not be familiar.</p>
    <br>
    <details>
        <summary>ELF terminology</summary>
        <p>
            <code>.data</code> and <code>.bss</code> are both <em>sections</em> in the final
            executable. Sections are what they sound like; literally a part of an executable file
            that is loaded by a program loader into memory when a program starts. Of course, my
            program doesn't <em>have</em> a program loader yet; it's just a raw blob of ARM9
            instructions interlaced with the other sections.
        </p>
        <p>
            The <code>.data</code> (and, <code>.rodata</code>) section contains
            <em>global variables</em> that have been explicitly initialised; in Rust, examples of
            this would be static globals, like so:
        </p>

        <pre data-snippet="02/02-rodata-mizuki.rs"><code>#[unsafe(no_mangle)]
pub static BLOB: &amp;str = "暁山瑞希";</code></pre>

        <p>
            This data is stored directly in the <code>.rodata</code> section of the file, and an
            entry in the Program Headers of the elf file is emitted to tell the dynamic loader to
            load these globals at the right spot in virtual memory.
        </p>
        <p>
            The <code>.bss</code> section is used for <em>uninitialised</em> global variables and is
            by convention set to all zeroes. To avoid bloating a file with emptiness the binary
            contains only the length of the section which will be allocated by the program loader as
            an empty block on startup. Again, this is something my program doesn't have and will
            need to set up manually.
        </p>
        <p>
            Also, for good measure, <code>.text</code> is the raw machine code for the program, also
            referred to as "program text" sometimes.
        </p>
    </details>
</blockquote>

<blockquote>
    <p>
        If all of the <code>b</code> instructions are relative, what's the problem with the code
        thinking it's loaded somewhere else?
    </p>

    <br>

    <details>
        <summary>Deeper explanation</summary>

        <p>Let's take this bit of code as an example:</p>

        <pre data-snippet="02/02-thumb-interwork.rs"><code>#[instruction_set(arm::t32)]
fn other() -&gt; u8 {
    1
}
#[unsafe(no_mangle)]
#[instruction_set(arm::a32)]
extern "C" fn _start() -&gt; ! {
    let _ = other();
    loop {}
}</code></pre>

        <p>
            Notice that the function <code>other</code> is using the Thumb instruction set and the
            function <code>_start</code> is using the Arm32 instruction set. This is linked to load
            at the start of memory like before, so let's look at the disassembly for it:
        </p>

        <pre data-snippet="02/02-thumb-interwork-disasm.txt"><code>Disassembly of section .text:

# Mangled name of "other"
00008000 &lt;_ZN6aocnds5other17h6d2efad51ed937f1E&gt;:
    8000:       2001            movs    r0, #1
    8002:       4770            bx      lr

00008004 &lt;_start&gt;:
    8004:       eb000001        bl      8010 &lt;___ZN6aocnds5other17h6d2efad51ed937f1E_from_arm&gt;
    8008:       eaffffff        b       800c &lt;_start+0x8&gt;
    800c:       eafffffe        b       800c &lt;_start+0x8&gt;

# Trampoline for ARM code to Thumb code
00008010 &lt;___ZN6aocnds5other17h6d2efad51ed937f1E_from_arm&gt;:
    8010:       e59fc000        ldr     ip, [pc]        @ 8018 &lt;___ZN6aocnds5other17h6d2efad51ed937f1E_from_arm+0x8&gt;
    8014:       e12fff1c        bx      ip
    8018:       00008001        andeq   r8, r0, r1
    801c:       00000000        andeq   r0, r0, r0</code></pre>

        <p>
            The linker has inserted a function trampoline to switch from ARM to Thumb mode. The
            Branch with Exchange instruction only takes an absolute register argument, unlike the
            normal branch with link instruction, so the assembled code uses the IP (R12) register.
            The IP register is loaded with a constant stored just after the function body -
            0x00008001, the absolute address of the actual <code>other</code> function in memory.
        </p>
        <p>
            Since the program is loaded at 0x02000000 this will jump straight into unmapped memory
            and cause the processor to fault.
        </p>
    </details>
</blockquote>

<h3>The first linker script</h3>

<p>I need to somehow tell the compiler and my code to do the following:</p>
<ul>
    <li>Compile with the assumption the code will be loaded and running at 0x02000000.</li>
    <li>
        Arrange the <code>.data</code>/<code>.rodata</code> section with the assumption it'll be
        loaded after 0x02000000 too.
    </li>
    <li>
        Set up a stack pointer that points into the Data Tightly Coupled Memory section so that I
        can actually use local variables.
    </li>
</ul>

<p>
    The first two can be done using a <em>linker script</em>. The misleadingly named linker script
    is a configuration file for the linker which tells it where to put the sections found in the
    intermediate files produced by the compiler into the final binary file.
</p>

<p>
    The average programmer will likely never interact with a non-default linker script in their
    life. Of the ones that do, only a tiny percentage will ever write their own linker script.
    Thankfully the programmers that <em>do</em> know how to write linker scripts have
    <a href="https://mcyoung.xyz/2021/06/01/linker-script">documented it</a>.
</p>

<pre data-snippet="03/03-ld-pt1.ld"><code>/* Standard cruft at the beginning of the file. I don't know what omitting this does,
  so let's not do that. */
OUTPUT_FORMAT("elf32-littlearm")
OUTPUT_ARCH(arm)
ENTRY(_start)

/*
 * The memory layout of the DS is pretty simple; for now, we'll just define the main memory.
 * It's a nice block of 4MB that's Readable, Writable, and eXecutable.
 */
MEMORY {
    main_ram (rwx) : ORIGIN = 0x02000000, LENGTH = 4M
}</code></pre>

<p>
    To actually use this file, I need to add another option to my <code>.cargo/config.toml</code> to
    tell the linker what script to use:
</p>

<pre data-snippet="03/03-config.toml"><code>[target.armv5te-none-eabi]
rustflags = [
    # The default ``rust-lld`` linker kinda works, but it doesn't support some linker script options
    # in weird and obscure ways. I'll just use my cross-compiler linker instead.
    "-Clinker=arm-none-eabi-ld",
    "-Clink-arg=-Tlinker.ld"
]</code></pre>

<p>I was curious to see if this would Just Work, so I built the ELF file and objdump'd it:</p>

<pre data-snippet="03/03-objdump-1.txt"><code>Disassembly of section .text:

02010010 &lt;_start&gt;:
 2010010:       eaffffff        b       2010014 &lt;_start+0x4&gt;
 2010014:       eafffffe        b       2010014 &lt;_start+0x4&gt;</code></pre>

<p>
    Aha! The 0x10000 bytes of junk is still there at the start, but now the code is correctly being
    loaded into Nitro's main memory. Not only that but the entrypoint is correct too:
</p>

<pre data-snippet="03/readelf.txt"><code>$ arm-none-eabi-readelf -h aocnds
ELF Header:
  Magic:   7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00
  Class:                             ELF32
  Data:                              2's complement, little endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              EXEC (Executable file)
  Machine:                           ARM
  Version:                           0x1
  Entry point address:               0x2010010</code></pre>

<p>
    If I compile a slightly modified program that calls a function with a
    <code>&amp;'static str</code> variable, I can also verify that it's using the right addresses
    for <code>.rodata</code>:
</p>

<pre data-snippet="03/rodata-mizuki.rs"><code>#[unsafe(no_mangle)]
pub static BLOB: &amp;str = "暁山瑞希";

// has to 1) exist 2) take the arg or the linker GCs us :(
fn other(_: &amp;str) {
    // bx lr
}

#[unsafe(no_mangle)]
extern "C" fn _start() -&gt; ! {
    other(BLOB);
    loop {}
}</code></pre> <pre data-snippet="03/rodata-mizuki-nm.txt"><code>Disassembly of section .rodata:

02000010 &lt;BLOB-0xc&gt;:
 2000010:       e5819ae6        str     r9, [r1, #2790] @ 0xae6
 2000014:       91e7b1b1        strhls  fp, [r7, #17]!
 2000018:       8cb8e59e        ldchi   5, cr14, [r8], #632     @ 0x278

0200001c &lt;BLOB&gt;:
 200001c:       02000010        andeq   r0, r0, #16
 2000020:       0000000c        andeq   r0, r0, ip

Disassembly of section .text:

02010024 &lt;_ZN6aocnds5other17hd0540aae0727baf3E&gt;:
 2010024:       e24dd008        sub     sp, sp, #8
 2010028:       e58d0000        str     r0, [sp]
 201002c:       e58d1004        str     r1, [sp, #4]
 2010030:       e28dd008        add     sp, sp, #8
 2010034:       e12fff1e        bx      lr

02010038 &lt;_start&gt;:
 2010038:       e59f1010        ldr     r1, [pc, #16]   @ 2010050 &lt;_start+0x18&gt;
 201003c:       e5910000        ldr     r0, [r1]
 2010040:       e5911004        ldr     r1, [r1, #4]
 2010044:       ebfffff6        bl      2010024 &lt;_ZN6aocnds5other17hd0540aae0727baf3E&gt;
 2010048:       eaffffff        b       201004c &lt;_start+0x14&gt;
 201004c:       eafffffe        b       201004c &lt;_start+0x14&gt;
 2010050:       0200001c        andeq   r0, r0, #28  &lt;-- BLOB &lt;-- BLOB-0xc</code></pre>

<p>
    The linker is placing the data in the right memory section and the reference after the body of
    <code>_start</code> is pointing to main memory. Success!
</p>

<h3>The C Runtime</h3>

<p>
    I'm still in no man's land here; without a stack pointer, I can't use local variables or call
    other functions as the compiler automatically emits instructions to manipulate the stack pointer
    on function entry and exit.
</p>

<blockquote>
    <p>
        The stack is an area of memory where local variables are stored. It's called a stack because
        it grows in a first-in last-out manner. When a function needs to use a local variable, it
        allocates some space in the stack at the address of <i>stack pointer</i> - one of the
        processor registers specifically designated for this purpose - then moves it downwards by
        the amount of space that variable takes. That way, every local variable in a function has
        its own memory address automatically assigned.
    </p>

    <p>
        The stack is also used to preserve the state of the processor registers from a previous
        function when calling a function. The number of registers preserved varies depending on
        something known as a <i>calling convention</i>; for 32-bit ARM in the embedded ABI, this is
        registers four through eleven, as well as the Link register which is used to return from a
        function to the previous one.
    </p>

    <p>
        When a function exits, it no longer needs the space for the local variables, so the stack
        pointer is reset back to where it was before the function was called, allowing the previous
        function to reuse that memory for other local variables or other function calls. The
        compiler does this automatically for me in assembly known as the function prologues and
        epilogues. Without a valid stack pointer, <i>none</i> of the operations described here are
        valid, and will cause the processor to fault.
    </p>
</blockquote>

<blockquote>
    <p>
        These instructions on entry/exit are known as the function prologue and epilogue, and are
        related to avoiding clobbering registers used by previous functions. You can find more
        information about the 32-bit ARM calling convention
        <a href="https://en.wikipedia.org/wiki/Calling_convention#ARM_(A32">here</a>).
    </p>
</blockquote>

<p>
    Before every single program is a small stub of hand-written assembly known as the
    <code>crt</code>, which stands for "C Runtime". This is also a misleading name, as it is neither
    a runtime nor exclusive to the C programming language. On Nitro, this has a few things it needs
    to do before jumping into the managed (i.e. my Rust) code:
</p>

<ul>
    <li>Set up the stack pointer</li>
    <li>Set up Coprocessor 15 for memory protection</li>
    <li>Zero out a memory area for the BSS segment</li>
    <li>Finally, jump into the high-level main function.</li>
</ul>

<blockquote>
    <p>
        If you don't know assembly or anything about it; that's okay. I didn't either until I
        started this. Sure, I could read <em>some</em> ARM assembly, but writing it is a whole
        different beast. Luckily it's a very nice instruction set that is easy to understand.
    </p>
</blockquote>

<p>
    Before I dive straight into writing all of this, let's get the infrastructure for the assembly
    working first. It's all going to go into a file named <code>start.s</code> in src, and can be
    included in the build process with the <code>global_asm!</code> macro, like so:
</p>

<pre><code>global_asm!(include_str!("start.s"));</code></pre>

<p>
    I'll just write a basic asm file that jumps straight to the Rust main function to make sure it
    builds properly:
</p>

<pre data-snippet="04/start-1.s"><code>// Put it in the ``.text`` section, i.e. where all the functions are.
.section ".text._start"
// Export it globally so the linker can see it. 
.global _start

_start:
    // Branch relative to the label (function) called main.
    b main</code></pre>

<p>
    I'll also go back and rename the Rust function from <code>_start</code> to <code>main</code> so
    that it gets picked up. Dumping the executable shows that it is indeed included in my build:
</p>

<pre data-snippet="04/start-1-nm.txt"><code>Disassembly of section .text:

02010010 &lt;_start&gt;:
 2010010:       eaffffff        b       2010014 &lt;main&gt;

02010014 &lt;main&gt;:
 2010014:       eaffffff        b       2010018 &lt;main+0x4&gt;
 2010018:       eafffffe        b       2010018 &lt;main+0x4&gt;</code></pre>

<h3>A real linker script</h3>

<p>
    Up until now, I've been coasting along on what is essentially a bunch of heuristics from the
    linker to put all of the sections in the right place. This <em>mostly</em> works, but let's see
    what happens if I add a region for the ITCM to the memory map in the linker script. The ITCM is
    officially mapped starting at <code>0x0</code>, but most official software pretends it's mapped
    at <code>0x01000000</code>. Because it's mirrored every <code>0x8000</code> bytes, most official
    software seems to pretend it actually starts at <code>0x01ff8000</code>, which is
    <code>0x8000</code> bytes before main memory.
</p>

<pre data-snippet="05/memory.diff"><code>MEMORY {
+   itcm (rwx)      : ORIGIN = 0x01ff8000, LENGTH = 32K
    main_ram (rwx)  : ORIGIN = 0x02000000, LENGTH = 4M
+   dtcm (rw)       : ORIGIN = 0x027e0000, LENGTH = 16K
}</code></pre>



<blockquote>
    I could just not add it to the linker, but eventually I want to move some of my code into the
    ITCM as it's significantly faster to fetch instructions from the ITCM than from main memory.
</blockquote>

<p>
    Attempting to compile with this linker script will result in an immediate error because the ARM
    unwind table doesn't fit into 32KiB:
</p>

<pre data-snippet="05/lld-error.txt"><code>  = note: rust-lld: error: section '.ARM.exidx' will not fit in region 'itcm': overflowed by 32784 bytes
          rust-lld: error: section '.text' will not fit in region 'itcm': overflowed by 98372 bytes</code></pre>

<p>
    This is because the heuristics the linker uses is to put all sections in the first memory
    segment it finds; in this case, it's the ITCM segment. I don't want unwind tables or the first
    half of my program text in the ITCM anyway!
</p>

<p>
    To solve this, I need to actually start defining some sections in the linker script. First, I'm
    going to make it so that Cargo will correctly rebuild the project if the script changes, rather
    than going "yup, looks good to me!", by creating a `build.rs` file in the root of the project:
</p>

<pre data-snippet="05/build.rs"><code>pub fn main() {
    println!("cargo::rerun-if-changed=linker.ld");
}</code></pre>

<p>Next, let's define some code sections in a... <code>SECTIONS</code> block:</p>

<pre data-snippet="05/sections.ld"><code>SECTIONS {
    /* The CRT always needs to be in main memory. Let's put it at the start for good measure. */
    .crt : ALIGN(4) { *(.crt); *(.crt.*) } &gt; main_ram =0x39

    /* The text section should be in main memory. Things that will go into the ITCM can be put
       there explicitly. */
    .text : ALIGN(4) { *(.text); *(.text.*) } &gt; main_ram =0x39
    /* The data section should also be in RAM. DTCM variables can be placed there explicitly. */
    .data : ALIGN(4) { *(.data); *(.data.*) } &gt; main_ram =0x39
    .rodata : ALIGN(4) { *(.rodata); *(.rodata.*); } &gt; main_ram =0x39
    /* The BSS section doesn't really... exist, but we need to allocate some space for it anyway. */
    .bss : ALIGN(4) { KEEP(*(.bss)); KEEP(*(.bss.*)); } &gt; main_ram

    /* Make sure the start of the heap is properly aligned. */
    . = ALIGN(4);
    __text_end = .;

    /* Let's get rid of anything we haven't explicitly specified. This includes some debugging
       info for now. */
    /DISCARD/ : { * }
}</code></pre>

<p>
    As the comments show, this forces the <code>crt</code> assembly to be in main memory and at the
    very start of the code, then the <code>.text</code> section after, and finally the
    <code>.(ro)data</code> sections. All of these sections have their unused bytes filled with a
    fixed <code>0x39</code> (e.g. the trailing, unaligned bytes for global data or THUMB
    instructions). The <code>.bss</code> section doesn't exist in the binary, but I need to mark it
    in the linker script so that the code knows where to zero it out at runtime.
</p>

<p>Finally, I can define some "variables" which will be exported to the final code:</p>

<pre data-snippet="05/variables.ld"><code>__bss_start = ADDR(.bss);
__bss_end = ADDR(.bss) + SIZEOF(.bss);</code></pre>

<p>With the BSS labels defined, I can write some assembly to zero the section out:</p>

<pre data-snippet="05/start-1.s"><code>// R0: value
// R1: ptr to start
// R2: size
_ASM_primitive_memset:
    // R12 (end) = start + size
    add r12, r1, r2
.L0:
    // Compare current pointer to end
    cmp r1, r12
    // Store multiple registers, increment address, only run if R1 &lt; R12
    // *R1 = R0, R1 += 4
    // Also, the double braces are because this is being compiled with ``global_asm!()``, and single
    // braces means it would think it's a parameter and flip out at me for not providing an ``R0``
    // parameter.
    // Doesn't clear the condition bits so...
    stmialt r1!, {{R0}}
    // ... jump back if the condition still matches
    blt .L0
    // Return
    bx lr

_start:
    // Clear BSS
    ldr r2, =__bss_size
    ldr r1, =__bss_start
    mov r0, #0
    bl _ASM_primitive_memset

    // For future-proofing, we do a branch-with-exchange in case ``main`` ends up as a thumb
    // function.
    ldr r0, =main
    bx r0</code></pre>

<h3>The rest of the CRT</h3>

<p>
    With a working linker script and my memory sections actively defined in the linker script, I can
    start filling in the rest of the CRT. First, I disable interrupts and wait for vertical sync:
</p>

<pre data-snippet="06/start-2.s"><code>_start:
    // The IME is at address 0x4000208, and is a 32-bit register.
    // The only bit in it that matters is bit zero, which acts as the enable bit.
    // 1 = interrupts controlled by IE
    // 0 = interrupts forcibly disabled.
    //
    // The ``mov`` instruction with an immediate can only operate on any 8-bit value that is shifted
    // by an even power of two, which 0x04000208 is not. 0x04000000 (the base address for I/O
    // registers) is, so that's loaded into register zero. Then, exploiting the fact that the
    // LSB of 0x04000000 is 0, we store that into IME and the upper bits are ignored.

    mov r0, #0x04000000
    str r0, [r0, #0x208]

    // Wait for vertical sync.
    // Note: The syntax ``.L&lt;name&gt;`` signifies a local label, which isn't exported as a symbol
    // in the final compiled object.
.Lvsync:
    // Load half-word at REG_DISP_VCOUNT. R0 was already the base of the I/O registers, so we
    // can just add 0x6 to it to load it into R1.
    ldrh r1, [r0, #0x6]
    // Compare to 0x0 and set the comparison flag.
    cmp r1, #0
    // Branch if not equal back to the VSYNC label.
    bne .Lvsync</code></pre>

<p>
    Next, I set up my stack pointers. I want it to be in the DTCM, as that is significantly faster
    than main memory. As eventually I'll want to copy things into the DTCM from main memory, I'll
    define some variables in the linker script:
</p>

<pre data-snippet="06/variables.ld"><code>__dtcm_region_start = ORIGIN(dtcm);
__dtcm_region_end = ORIGIN(dtcm) + LENGTH(dtcm) - 4;</code></pre>

<p>
    Now, I'll finally give myself a frame pointer - or, in fact, <i>three</i> frame pointers; one
    for each mode (Supervisor, IRQ, and System).
</p>

<blockquote>
    <p>
        The vast majority of CPUs ever made have a concept of special modes; these are sometimes
        called privilege modes, or rings, and are normally used to protect the kernel from user
        software and userland software from <em>other</em> userland software. Each mode has its own
        set of registers and state that is isolated from the other modes.
    </p>
    <p>
        Being that this is a game console - and an old, low-powered one at that - there's no need
        for this protection, so the modes are used primarily to separate the stacks and registers
        when doing things like processing exceptions or interrupts.
    </p>
</blockquote>

<blockquote>
    <h4>Trivia!</h4>

    <p>
        On x86 (but not AMD64), there are four rings, numbered zero through three. User code
        typically runs in Ring 3 and kernel code typically runs in Ring 0, with Ring 1 and 2 unused.
        Back in the day, OS/2 used Ring 2 for device drivers, which makes emulating/virtualising it
        a bit harder as one would have to support the middle rings.
    </p>
</blockquote>

<blockquote>
    <h4>More trivia!</h4>

    <p>
        ARM actually supports <em>seven</em> modes; in addition to those four, there's also User
        mode, FIQ mode, Abort mode, and... Undefined mode. I have no reason for User mode as there's
        no end-user programs (or even an MMU), and the other four modes are treated identically by
        the BIOS.
    </p>
</blockquote>

<p>
    To set the frame pointers, I need to switch into every mode and assign a value to the register.
    The mode is set by the lower four bits of the CPSR (Current Program Status Register), and all
    the other bits can safely be set to zero without issue. The offsets from the DTCM are calculated
    in the linker script rather than the assembly file as they are known at compile time:
</p>

<pre data-snippet="06/stacks.ld"><code>__stack_start_irq = __dtcm_region_end - 0x100;
__stack_start_svc = __stack_start_irq - 0x100;
__stack_start_sys = __stack_start_sys - 0x100;</code></pre>

<p>Then, it's just a few repeated instructions to change modes:</p>

<pre data-snippet="06/stacks.s"><code>// Stack setup for every mode
// 0b10010 == 0x12, IRQ mode
mov r0, #0x12
msr cpsr, r0
ldr sp, =__stack_start_irq

// 0b10011 == 0x13, Supervisor mode
mov r0, #0x13
msr cpsr, r0
ldr sp, =__stack_start_svc

// 0b11111 == 0x1f, System mode
mov r0, #0x1f
msr cpsr, r0
ldr sp, =__stack_start_sys</code></pre>

<p>
    With the stack pointers now valid and the <code>.bss</code> zeroed, I now have a
    <em>valid</em> Rust program that can be built and linked.
</p>

<h3>Coprocessor 15</h3>

<p>
    Coprocessor 15 (also known as the System Control coprocessor) is a special processor built-in to
    the ARM9 (and ARM7) chips that controls things mostly relating to memory layouts and memory
    protection. CP15 has sixteen registers, with the most interesting ones being:
</p>
<ul>
    <li>
        <p>
            Registers 2, 5, 6, 8, 10, and 13 control the memory protection subsystem, which is an
            alternative to the memory management unit subsystem (which the ARM9 doesn't have). This
            subsystem mostly pertains to caching and raising CPU exceptions on out-of-bounds reads.
        </p>
    </li>
    <li>
        <p>
            Registers 7 and 9 control the cache and write buffers, as well as the tightly coupled
            memory locations.
        </p>
    </li>
</ul>
<p>
    This is fiddly and annoying (my decompiled game has 45 (!) instructions to set it up)... so I'm
    simply going to blindly steal what said game does. I've helpfully commented everything too, so
    that future generations can understand it easier.
</p>

<br>

<details>
    <summary><code>setup_coprocessor</code> function (it's really long!)</summary>

    <pre data-snippet="07-cp15.s"><code>setup_coprocessor:
    // The creatively named MCR and MRC stand for "Move Coprocessor to Register" and
    // "Move Register to Coprocessor", respectively. The names are misleading; it really means
    // "do coprocessor command".

    // C1/C0/0 = System control, copy the definition into R0
    mrc p15, 0x0, r0, cr1, cr0, 0x0

    // load constant control value
    // this (temporarrily) disables the protection unit, the DTCM and ITCM, and disables 
    // caching for them
    ldr r1, =0x000F90053
    // BIC = Rd AND (NOT Rn)
    // clear any set bits in R0 that are set in R1, leave the rest alone
    bic r0, r0, r1
    // write it back
    mcr p15, 0x0, r0, cr1, cr0, 0x0

    // Disable caches for both TCMs (?)
    // C7,C5,0   0    Yes  Invalidate Entire Instruction Cache
    // C7,C6,0   0    Yes  Invalidate Entire Data Cache
    mov r0, #0
    mcr p15, 0, r0, c7, c5, 0
    mcr p15, 0, r0, c7, c6, 0
    // C7,C10,4  0    -    Drain Write Buffer
    mcr p15, 0, r0, c7, c10, 4

    // == Memory Protection == //
    // The protection regions are almost identical to the ones on GBATEK, which are in themselves
    // identical to the ones setup by the CRT of the game I've decompiled.
    // 
    // Control register C6 defines the region, C0-C7 all define a specific subregion.
    // Bit   0: 1 = enable protection, 0 = disable protection
    // Bit 1-5: 2 SHL value = region size
    // Bit 6-11: reserved
    // Bit 12-31: Region address * 4096
    //
    // The official ARM docs marks sizees less than 0b01011 as unpredictable, so the base unit is
    // in 4KB blocks?
    //
    // "The address of the first byte is required to be a multiple of the region size."

    // Protection region 0: 0x04000000, 64MiB (i.e. up to 0x8000000)
    // This is the I/O registers all the way up to the end of the OAM!
    ldr r0, =(0x04000000 | 0x33)
    mcr p15, 0, r0, c6, c0, 0

    // Protection region 1: 0x02000000, 4MiB
    // The compiled game I'm looking at has it incorrectly set to 8MiB. I guess the SDK always
    // sets it that high? This is main memory.
    ldr r0, =(0x02000000 | 0x2b)
    mcr p15, 0, r0, c6, c1, 0
    
    // Protection region 2: 0x027e0000, 128KiB (what?)
    // GBATEK: Region 2 and 7 are not understood?
    // Not going to set this. All zeroes to disable memory protection.
    mov r0, 0
    mcr p15, 0, r0, c6, c2, 0

    // Protection region 3: 0x08000000, 128MiB 
    // GBATEK: GBA Slot should be max 32MB+64KB, rounded up to 64MB, no idea why it is 128MB?
    ldr r0, =(0x08000000 | 0x35)
    mcr p15, 0, r0, c6, c3, 0

    // Protection region 4: 0x027e0000, 16KiB
    // This is the DTCM.
    ldr r0, =__dtcm_region_start
    orr r0, r0, #0x1b
    mcr p15, 0, r0, c6, c4, 0

    // Protection region 5: 0x01000000, 32KiB
    // ITCM. Thanks to mirroring, this repeats itself every 32KiB.
    ldr r0, =__itcm_region_start
    orr r0, r0, #0x1d
    mcr p15, 0, r0, c6, c5, 0

    // Protection region 6: 0xFFFF0000, 32KiB.
    // This is where the BIOS is mapped.
    ldr r0, =(0xFFFF0000 | 0x1d)
    mcr p15, 0, r0, c6, c6, 0

    // Protection region 7: 0x027FF000, 4KiB.
    // GBATEK says "shared work". I do wonder where it got that name from.
    //
    // The actual shared WRAM area is at the 0x03... addresses.
    // So... let's set it there.
    // Protection region 7: 0x037F8000, 32KiB. 
    ldr r0, =__shram_region_start
    orr r0, r0, #0x1d
    mcr p15, 0, r0, c6, c7, 0 

    // Protection region 2: 0x027FF00, 4KiB.
    // This is BIOS ram, see NDS BIOS RAM usage in GBATEK. Only realised this when looking through
    // addresses. 
    ldr r0, =(0x027FF00 | 0x17)
    mcr p15, 0, r0, c6, c2, 0

    // == Tightly Coupled Memory == //
    // C9, C1 controls the TCM Region. 
    //
    // The ARM manual states "Prior to ARMv6 it is IMPLEMENTATION DEFINED how TCMs are supported, 
    // "though generally this is through a System Control Coprocessor interface.""
    //
    // ITCM is fixed, so just set the size to 32MiB so it covers the entire first part of memory 
    // space. It'll get mirrored apparently.
    // Table B7-2: 32MiB is 0b10000 (&lt;&lt; 1), 16KiB is 0b00101 (&lt;&lt; 1).
    mov r0, 0x20
    mcr p15, 0, r0, c9, c1, 1

    // DTCM is movable, so load it at the right address and set its size to 16KiB.
    ldr r0, =__dtcm_region_start
    orr r0, r0, 0xa
    mcr p15, 0, r0, c9, c1, 0

    // == Protection Unit, Pt 2 == //
    // Register C2,C0 controls data caching and it's a bitfield for every region that needs caches.
    // 0x1 = instructions, 0x0 = data
    //
    // The only regions that needs caching is main memory, which is region 1, and the BIOS, which
    // is region 6. (The bitfield starts from the LSB.)
    mov r0, #0b01000010
    mcr p15, 0, r0, c2, c0, 0
    mcr p15, 0, r0, c2, c0, 1

    // C3,C0,0 is... write-bufferability? This is too far into the details of CPUs for me.
    // Just do what the official CRT does, which is region 1 (main memory).
    mov r0, #0b00000010
    mcr p15, 0, r0, c3, c0, 0

    // C5,C0 controls the permissions for the various memory protection regions. Immediate
    // value 2 and 3 control *extended* permissions, which give 4 bits per region with up to
    // six values. 2 = Data/Unified, 3 = Instruction. Immediate value 0 and 1 control basic 
    // permissions, with two bits per region.
    // 
    // We're just going to fill this with 0b11 for all eight regions as constructing the individual
    // permission bits is fiddly and not really needed.
    ldr r0, =0xffff
    mcr p15, 0, r0, c5, c0, 0
    mcr p15, 0, r0, c5, c0, 1

    // Re-enable ITCM, DTCM, caches, and protection unit.
    mrc p15, 0, r0, c1, c0, 0
    ldr r1, =0x0005707D
    orr r0, r0, r1
    mcr p15, 0, r0, c1, c0, 0

    bx lr</code></pre>
</details>

<h2>Emulator setup</h2>

<p>
    With a basic working binary and project skeleton ready, I can now run my rom. Whilst there's
    nothing stopping this from working on real hardware, it's a bit more difficult to attach a
    debugger to my O3DS so I'm going to use an emulator for nearly everything and only test the
    final products on my 3DS.
</p>

<blockquote>
    <h4>Editor's note!</h4>

    <p>
        This is a lie. I don't know where my 3DS charger is so I can't test it on real hardware yet.
    </p>
</blockquote>

<p>
    For debugging purposes, I need an emulator with a debugger. There's a handful of options
    available:
</p>
<ul>
    <li>
        <p>
            NO$GBA was the first Nitro emulator, and technically has a debugger. In practice it
            either doesn't work or I just can't figure out how to make it work beyond showing a
            memory dump.
        </p>
        <p>It's also proprietary and requires running via Wine.</p>
    </li>
    <li>
        <p>
            DeSmuME (when built with <code>USE="gdb"</code>) has a GDB stub, but it was a bit buggy
            the last time I used it. It also allegedly has a "View IO Registers" function, but it's
            permanently greyed out.
        </p>
    </li>
    <li>
        <p>
            melonDS, as of sometime in the last two years, has a GDB stub (which didn't work until
            sometime in the last few months) but no other debugging info.
        </p>
    </li>
    <li>
        <p>
            <a href="https://github.com/kelpsyberry/dust">dust</a> has full debugging features and a
            GDB stub, which seems perfect! But... it locks up when attaching GDB, and the registers
            are full of junk, so it's not really suitable for step-by-step debugging.
        </p>
    </li>
</ul>
<p>
    I'll use a mixture of melonDS and DeSmuME for this as the GDB stub in both does work but melonDS
    has some odd behaviours on halting that make it a bit more inconvenient. For ease of debugging,
    I'll un-discard the missing debug sections in the linker script:
</p>

<pre data-snippet="08/undebug.diff"><code>    __text_end = .;

-   /* Let's get rid of anything we haven't explicitly specified. This includes some debugging
-      info for now. */
-   /*/DISCARD/ : { * }*/</code></pre>

<p><img src="https://sailor.li/static/aocnds/img/gdb1.png"></p><p>
    I enable the GDB stub in melonDS (Config -&gt; Emu Settings -&gt; Devtools, enable GDB stub,
    check Break on startup), load my rom with <code>melonDS rom.nds</code>, then start my
    cross-compiled GDB with <code>env RUST_GDB=arm-none-eabi-gdb rust-gdb</code>. I can load the
    original ELF executable with <code>file ./target/armv5te-none-eabi/debug/aocnds</code>, then
    connect to the GDB stub with <code>target remote 127.0.0.1:3333</code>. From there, it's just
    like using GDB normally.
</p>

<blockquote>
    <p>
        I'm using the <a href="https://github.com/cyrus-and/gdb-dashboard">GDB dashboard</a> for a
        prettier GDB output.
    </p>
</blockquote>

<h2>Getting it to work, pt 1</h2>

<p>
    In a strict sense, a working bootstrap script and a payload is nearly all that's needed. I could
    solve the problem now and "output" it to a known fixed memory address, which can be read in the
    debugger, code golf style. Let's do that now, and get the boring work out of the way.
</p>

<p>Rust's standard library is actually several separate libraries glued together:</p>
<ul>
    <li>
        <p>
            The <code>core</code> library contains... well, the core code for the language. Pointer
            types, the <code>&amp;str</code> type, panic logic, the core formatting logic, and other
            core language traits are all defined in this library.
        </p>
    </li>
    <li>
        <p>
            The <code>alloc</code> library contains things that allocate memory onto the heap. These
            are types such as Box (heap pointers), Vec (heap arrays), and the reference counting
            utilities.
        </p>
    </li>
    <li>
        <p>
            The <code>std</code> library re-exports all of the former two libraries under the
            <code>std</code> namespace (see: <code>std::boxed::Boxed</code> is actually an
            <code>alloc::boxed::Box</code> in disguise) and contains things that allow interacting
            with the running system, such as filesystem or network I/O.
        </p>
    </li>
</ul>

<p>
    In a normal, hosted environment Rust uses the operating system's memory management functions
    (<code>malloc()</code> to allocate a block, <code>free()</code> to deallocate a block) to handle
    the heap, wrapped in internal tracking code. In this <code>no_std</code> environment, there are
    no operating system memory management functions, and I need to provide my own. Instead of
    writing the tracking code, I'll use a pre-built one known as <code>talc</code>. I'll also add
    the <code>spin</code> library as well:
</p>

<pre data-snippet="10/add-talc.toml"><code>[dependencies]
spin = { version = "=0.9.8" }
talc = { version = "=4.4.2" }</code></pre>

<p>Next, I can update my <code>main.rs</code> to create a new <em>global allocator</em>:</p>

<pre data-snippet="10/global_alloc.rs"><code>use talc::{ErrOnOom, Talc, Talck};

// Normally cargo links in all external libraries manually if using Rust 2018 or later, with one
// exception: ``alloc`` on no_std targets. It needs to be explicitly provided using ``extern crate``.
extern crate alloc;

#[global_allocator]
static ALLOCATOR: Talck&lt;spin::Mutex&lt;()&gt;, ErrOnOom&gt; = Talc::new(ErrOnOom).lock();</code></pre>

<blockquote>
    <p>
        The global allocator is a Rust-ism that means "the implementation of malloc()". Changing it
        is like hooking malloc and free in C/++ code.
    </p>
    <p>
        The alternative is local allocators via the allocator API, a bootleg mechanism to allow
        certain stdlib structures to use an explicitly provided allocator. Like most Rust features,
        it's currently unstable and that's not likely to change for the foreseeable future.
    </p>
</blockquote>

<p>Finally, I'll update the <code>config.toml</code> to build <code>alloc</code> as well:</p>

<pre data-snippet="10/add-build-std.toml"><code>[unstable]
build-std = ["core", "alloc"]</code></pre>

<p>
    This doesn't work yet because I haven't told the allocator where to start allocating things.
    Nevertheless, I like to compile things like this early because modern IDEs are terrible at
    actually showing errors:
</p>

<pre><code>error: could not compile `spin` (lib) due to 14 previous errors</code></pre>

<p><abbr title="Short for 'ah fuck.'">Ah</abbr>.</p>

<h3>A Multithreaded World</h3>

<p>In the programming world, there are two approaches to making multithreaded code safe:</p>
<ol>
    <li>Fuck you. This model is used by C/++, amongst others.</li>
    <li>
        Make it annoying (if correct) to use multithreaded code. This is the approach Rust uses, and
        by technicality pre-Python 3.13.
    </li>
</ol>
<p>
    In order to do anything with multiple threads in Rust, you need a set of operations known as
    <em>atomic</em> operations. There is a <em>lot</em> to be written about a safe model of
    multithreading, up to and including academic papers, but the gist of it is that atomic
    operations are used to perform safe, multi-threaded synchronisation and communication. For
    example, a compare-and-swap operation will <em>compare</em> the current value with a provided
    value, and if it matches, swap it with a different one. This means that to all threads, that
    value is <em>only</em> set if it matches, whereas a regular comparison and assignment might
    allow another thread to change the value between the two operations.
</p>

<p>
    On most architectures, such as x86_64 and AArch64, Rust provides a set of functions in the
    <code>core</code> library that directly map to hardware instructions or features that perform
    these operations. The ARM v5 architecture does not have these operations - it doesn't even have
    threads. When I try and compile the <code>spin</code> library, which relies heavily on atomics,
    it fails due to missing all of those atomic operations for this platform.
</p>

<p>
    Luckily, there is a library that fills in the missing atomic operations:
    <a href="https://docs.rs/portable-atomic"><code>portable-atomic</code></a>. Even luckier is that <code>spin</code> supports the usage of <code>portable-atomic</code>!
    Even luckier still is that <code>portable-atomic</code> has support for working safely using the
    <code>critical-section</code> library.
</p>

<blockquote>
    <h4>Warning!</h4>

    <p>
        Upon reading the above, some incredibly-credibly smart person has gone: but wait! There's no
        threads on ARMv5! You don't need to pull in a lock library because there will only ever be
        one thing asking for the lock!
    </p>

    <p>To which, I will respond:</p>

    <ol>
        <li>I don't care.</li>
        <li>Even without threads, I still need to implement atomics due to interrupts.</li>
        <li>The compiler complains really hard if I access static mutable variables.</li>
    </ol>
</blockquote>

<blockquote>
    <h4>Trivia!</h4>

    <p>
        A "critical section" is a global per-process lock.
        <code>portable-atomic</code> will enter the critical section before every "atomic" call and
        exit it afterwards, achieving atomic behaviour even in the presence of interrupts.
    </p>
</blockquote>

<h3>Rust-level Runtimes</h3>

<p>First, I'll add the new dependencies and features to <code>Cargo.toml</code>:</p>

<pre data-snippet="11/add-atomics.diff"><code>[dependencies]
+spin = { version = "=0.9.8", features = ["portable-atomic"] }
talc = { version = "=4.4.2" }
+portable-atomic = { version = "=1.10.0", default-features = false, features = ["critical-section"] }
+critical-section = { version = "=1.2.0", features = ["restore-state-u32"] }</code></pre>

<p>
    As this requires some extra code, I'm going to separate this out into its own file and include
    some of my previous runtime helpers such as the panic handler and the allocator instance:
</p>

<pre data-snippet="11/runtime-1.rs"><code>// File: runtime.rs
use talc::{ErrOnOom, Talc, Talck};

#[panic_handler]
fn _handle_panic(_: &amp;core::panic::PanicInfo) -&gt; ! {
    // TODO: Do something better than this
    loop {}
}

#[global_allocator]
static ALLOCATOR: Talck&lt;spin::Mutex&lt;()&gt;, ErrOnOom&gt; = Talc::new(ErrOnOom).lock();

// File: main.rs
mod runtime;</code></pre>

<p>Next, I need to provide an "implementation" of my critical section for the library:</p>

<pre data-snippet="11/crit-1.rs"><code>struct NitroCriticalSection;

unsafe impl critical_section::Impl for NitroCriticalSection {
    unsafe fn acquire() -&gt; critical_section::RawRestoreState {
        todo!()
    }

    unsafe fn release(restore_state: critical_section::RawRestoreState) {
        todo!()
    }
}

critical_section::set_impl!(NitroCriticalSection);</code></pre>

<p>This so-called implementation doesn't do anything yet, but it does let me build...</p>

<pre><code>= note: arm-none-eabi-ld: warning: /home/lura/dev/misc/aocnds/target/armv5te-none-eabi/debug/deps/aocnds-e996ce57e42739ae has a LOAD segment with RWX permissions
        arm-none-eabi-ld: /home/lura/dev/misc/aocnds/target/armv5te-none-eabi/debug/deps/aocnds-e996ce57e42739ae.9kjzfmijbuatij09ciw83phbt.rcgu.o:(.ARM.exidx.text.__rust_alloc_error_handler+0x0): undefined reference to `__aeabi_unwind_cpp_pr0'</code></pre>

<p>
    I'm not really sure why the allocation error handler is emitting calls to the C++ exception
    unwinding function instead of going through panic machinery, but I'll provide some stub
    implementations anyway:
</p>

<pre data-snippet="11/aeabi.rs"><code>#[unsafe(no_mangle)]
pub fn __aeabi_unwind_cpp_pr0() -&gt; ! {
    loop {}
}

#[unsafe(no_mangle)]
pub fn __aeabi_unwind_cpp_pr1() -&gt; ! {
    loop {}
}

#[unsafe(no_mangle)]
pub fn __aeabi_unwind_cpp_pr2() -&gt; ! {
    loop {}
}</code></pre>

<blockquote>
    <p>
        Even the Linux kernel
        <a href="https://elixir.bootlin.com/linux/v6.11.5/source/arch/arm/kernel/unwind.c#L40">has these stubbed out</a>. Thanks, binutils?
    </p>
</blockquote>

<p>
    Whilst this does build, this will immediately panic when attempting to allocate memory as the
    critical sections are stubs that don't do anything. I need to actually write a critical section
    handler.
</p>

<h3>Memory-mapped I/O</h3>

<p>
    The critical section needs to disable the Interrupt Master Enable register before entering, and
    then disable it again afterwards.
</p>

<blockquote>
    Not to be confused with the "Disable interrupts" flag on the Current Processor State register in
    the ARM architecture, obviously. I don't know why there's two, but one added explicitly by
    Nintendo seems more legit.
</blockquote>

<p>
    The IME is an example of <em>memory-mapped I/O</em>; hardware that is controlled by writing to
    specific memory addresses in the address space of the program. A lot of systems use
    memory-mapped I/O for hardware control; whilst modern x86 computers started out using
    <em>port-mapped I/O</em> (which uses special instructions and a separate memory space) AMD64 has
    moved to using memory-mapped I/O for modern peripherals. These special addresses are treated
    just like any other memory when reading or writing from them - although not all addresses are
    actually readable or writeable, and some will only allow reading or writing specific bits of the
    value.
</p>

<p>
    When I wrote the CRT for the program, the first thing I did was set the IME to zero. In
    assembly, this is easy enough; a
    <code>str &lt;Input reg&gt; [&lt;Address reg&gt;]</code> instruction just sets
    <code>Address := Input</code>. Reading or writing from arbitrary memory addresses in bare Rust
    may cause everything ever to violently explode, so I'm going to use a wrapper library called
    <code>voladdress</code>, which is not-so-coincidentally used by the GBA wrapper library.
</p>

<blockquote>
    <p>
        For more information why this may cause everything ever to violently explode, see
        <a href="https://lokathor.github.io/volatile/">this excellent post</a>. The most notable
        thing is that LLVM can just elide subsequent writes to an address, which is fine for normal
        memory but extremely wrong for memory-mapped I/O where writing has side effects.
    </p>
</blockquote>

<pre data-snippet="11/crit-fr.rs"><code>use voladdress::{Safe, VolAddress};

// Whilst only the lower bit is used, this isn't a boolean because representing non-0 or 1 as
// a boolean is UB.
static REG_IME: VolAddress&lt;u32, Safe, Safe&gt; =
    unsafe { VolAddress::new(0x4000208) };

unsafe impl critical_section::Impl for NitroCriticalSection {
    unsafe fn acquire() -&gt; critical_section::RawRestoreState {
        // Read it off first so that it can be restored properly. If the previous value was zero,
        // we need the restore to remain zero!
        let prev = REG_IME.read();
        REG_IME.write(0);
        return prev;
    }

    unsafe fn release(restore_state: critical_section::RawRestoreState) {
        REG_IME.write(restore_state);
    }
}</code></pre>

<p>
    Whilst the memory allocator will still <em>crash</em> (because I haven't given it anywhere to
    take memory from), at least it won't crash due to the lack of atomics.
</p>

<h3>The heap, for real</h3>

<p>
    In my linker script above, I defined a symbol called <code>__text_end</code>, which marks where
    the program code ends. This will be the start of my heap. I can define the end of the heap with
    <code>__memory_end = ORIGIN(main_ram) + LENGTH(main_ram) - LENGTH(dtcm) - 4;</code>. Combined,
    this will let me tell the allocator "okay, this is where you can use memory".
</p>

<blockquote>
    <p>
        Linker script variables (when exposed to higher level code) aren't really variables in the
        traditional sense: they're addresses. Global variables require space to be allocated in the
        <code>data</code> sections, whereas linker variables don't get that. To get the value of a
        linker variable, I have to get the address of it. It's a bit confusing.
    </p>
</blockquote>

<blockquote>
    <h4>Warning!</h4>

    <p>
        The <code>- LENGTH(dtcm)</code> is very important! The <code>talc</code> allocator stores
        metadata at the very <em>end</em> of memory, growing downwards. The stack exists at the very
        end of memory (in the DTCM), growing downwards. When the stack overwrites the metadata
        stored by the allocator, the next allocation call will check the previous metadata, see that
        it's all messed up, and panic.
    </p>
</blockquote>

<pre data-snippet="12/setup-allocator.rs"><code>// File: runtime.rs
use talc::Span;

unsafe extern "C" {
    static mut __text_end: u8;
    static mut __memory_end: u8;
}

/**
 * Sets up the heap allocator with the memory span defined by the linker.
 */
#[allow(static_mut_refs)]
pub fn setup_heap_allocator() {
    // Evil linker incantations!
    unsafe {
        let text_end = &amp;mut __text_end as *mut u8;
        let memory_end = &amp;mut __memory_end as *mut u8;
        let span = Span::new(text_end, memory_end);
        ALLOCATOR.lock().claim(span).unwrap();
    }
}

// File: main.rs
extern "C" fn main() -&gt; ! {
    setup_heap_allocator();

    // ...
}</code></pre>

<p>
    With this, I have a working heap that can allocate objects. The program binary size is ~81KiB
    (all that heap tracking machinery doesn't come cheap) or ~25KiB with <code>-Os</code>, which
    leaves ~3.6MiB of heap space. That's almost enough to run Windows 95.
</p>

<h3>Solving the problem</h3>

<p>
    The actual implementation of Day 1 Part 1 is trivial. This is not a very fast (or good)
    implementation, but it works. With no output, I'll just write to the fixed address 0x02200000,
    which is likely to be very past the end of the heap. I'll also add a hardware breakpoint, which
    will freeze GDB when it's done (if running in melonDS).
</p>

<pre data-snippet="13/day1pt1.rs"><code>// Outside main:
static PUZZLE_INPUT: &amp;str = include_str!("day1.txt");

// Format of the file is XXXXX   XXXXX
let mut first: Vec&lt;u32&gt; = Vec::new();
let mut second: Vec&lt;u32&gt; = Vec::new();

for line in PUZZLE_INPUT.split_terminator('\n') {
    let first_num: u32 = line[0..5].parse().unwrap();
    let second_num: u32 = line[8..13].parse().unwrap();
    first.push(first_num);
    second.push(second_num);
}

first.sort();
second.sort();

let mut sum: u32 = 0;
for (first, second) in first.iter().zip(second) {
    let diff = ((*first as i32) - (second as i32)).abs();
    sum += diff as u32;
}

let output: VolAddress&lt;u32, Safe, Safe&gt; = unsafe { VolAddress::new(0x02200000) };
output.write(sum);

unsafe {
    asm!("bkpt");
}</code></pre>

<p>Running this in GDB and checking memory at the breakpoint gives me my final answer:</p>

<pre><code>&gt;&gt;&gt; x/d 0x02200000
0x2200000:      2344935</code></pre>

<p>Plugging it into the Advent of Code website verifies the solution is correct.</p>

<h3>Doing it in a less lame way</h3>

<p>
    Only having output under the debugger is lame. Nitro has <em>two</em> screens that I'm not
    using!
</p>
<p>
    Nitro's graphics system is two GBAs, one per screen, and a 3D engine outputting to one screen. I
    don't need the 3D engine and I don't need the second screen, so I only need to program one of
    the GBAs (engine A).
</p>

<blockquote>
    <p>
        "2D Engine" is a Nintendo-ism for "graphics chip". It's a programmable way of doing 2D
        graphics per screen. Imagine it like a hardware game engine, same for the 3D engine.
    </p>
</blockquote>

<blockquote>
    <h4>Trivia!</h4>

    <p>
        Most Nitro games are fine with having 3D on one screen only; see Mario Kart DS as an
        example, where the top screen is the 3D game and the bottom screen is a 2D map. But some
        games managed to achieve 3D by a combination of two techniques:
    </p>
    <ol>
        <li>
            <p>
                Swapping which engine outputs to which screen every other frame. Frame 1 has engine
                A outputting to the main screen, and frame 2 has engine B outputting to the main
                screen.
            </p>
        </li>
        <li>
            <p>
                Using the <em>display capture</em> feature to capture the previous 3D frame and
                having engine B display it. This way Nitro games got 3D graphics on both screens, at
                the penalty of only being able to run at 30FPS.
            </p>
        </li>
    </ol>
</blockquote>

<h3>Interrupts &amp; Vertical Sync</h3>

<p>
    There's always some bureaucracy to do first. Whilst displays and screens can be thought of as
    just a 2D array of pixels, there's some intricacy in how these pixels are drawn. Historically,
    for Cathode Ray screens, the electron gun would physically move left to right until it hit the
    edge of the screen and it would have to reposition itself back to the left and go down one line.
    This is known as a <em>scanline</em>. When it reached the bottom of the screen it would then
    have to move the electron gun back to the top left and continue.
</p>
<p>
    The periods between moving the gun to the left and moving the gun to the top were known as the
    <em>horizontal blank</em> and <em>vertical blank</em> respectively. The horizontal blank isn't
    very useful but the vertical blank is <em>very</em> useful: it's the point where the game should
    update all of its graphics. This is because the display is a <em>live</em> view of the current
    graphical settings, and updating things whilst the screen is currently drawing results in
    graphical artefacts known as tearing. The behaviour of waiting for the vertical blank to draw is
    known as vertical sync. There are two ways to wait until vertical blank:
</p>
<ol>
    <li>
        <p>
            Wait until the VCOUNT register is past 160. Nitro's screen resolution is 256x160, so
            when the VCOUNT is above this the Nitro is in the vertical blank period.
        </p>
        <p>
            This has the problem that this spends all of its time spinning the CPU whilst waiting
            for the next frame.
        </p>
    </li>
    <li>
        <p>
            Ask the CPU to suspend until the next <em>vertical blank interrupt</em>. An interrupt is
            exactly what it sounds like; a mechanism for hardware to interrupt the currently running
            code and force it to deal with something else. (In this case, the hardware is the LCD
            controller.)
        </p>
    </li>
</ol>
<p>
    The latter is absolutely the correct approach, but that means dealing with a bunch of interrupt
    control code.
</p>

<h3>The IRQ handler</h3>

<p>
    The Nitro BIOS is a small (3KB) binary built-in to the hardware of the console. It's responsible
    for actually dealing with interrupts, as it is mapped to where the hardcoded ARM9 interrupt
    handlers are. When an interrupt happens, the ARM core switches mode into IRQ mode, and then
    jumps to the IRQ vector in the BIOS code.
</p>

<blockquote>
    <p>
        For embedded systems, vectors are a set of instructions at the very start of a memory region
        that are used for running the code upon boot or reset, for interrupts, or for exceptions.
        These are all jump opcodes to an actual function that does the work; for example, the reset
        vector for Nitro is located at <code>0xffff0000</code>, and is a single instruction that
        does a jump to <code>0xffff0110</code> which is the actual bring-up code for the system.
    </p>
    <p>
        The ARM9 has eight of these: Reset, Undefined, Supervisor Call, Prefetch Abort, Data Abort,
        Unused, Interrupt, and Fast Interrupt. On Nitro, only Reset, Supervisor Call, and Interrupt
        have assigned functions; the rest all share the vector for Fast Interrupt which is used for
        hardware debugging.
    </p>
    <p>If you see the word IRQ, that's the same as "interrupt".</p>
</blockquote>

<p>Here's the function in the BIOS responsible for handling interrupts, annotated by me:</p>

<pre data-snippet="14/irq_func.s"><code>// Save registers used by user code
// The other registers are saved by the IRQ function in its prologue and epilogue.
stmdb      sp!,{r0,r1,r2,r3,r12,lr}
// Load DTCM address (shifting off lower bits)
mrc        p15,0x0,r0,cr9,cr1,0x0
mov        r0,r0, lsr #0xc
mov        r0,r0, lsl #0xc
// Add fixed offset to the DTCM for the IRQ handler address
add        r0,r0,#0x4000
// Load BIOS return address
adr        lr,0xffff0290
// Jump to IRQ handler
ldr        pc,[r0,#-0x4]
// Restore registers used by user code
ldmia      sp!,{r0,r1,r2,r3,r12,lr}=&gt;local_18
// Jump back to previous code
subs       pc,lr,#0x4</code></pre>

<blockquote>
    <p>
        <code>subs pc, lr, #0x4</code> is a magic instruction that tells the ARM9 to switch from IRQ
        mode back to the previous mode. It returns to the code that was previously interrupted.
    </p>
</blockquote>

<p>
    Summarising this, the <em>address</em> of my interrupt handler needs to be at the address of
    DTCM + 0x3ffc. I'll put the code to load this into the <code>start.s</code> file, to avoid even
    more Rust-level linker script incantations:
</p>

<pre data-snippet="14/set_irq.diff"><code>+   // Less of a minefield to just set the IRQ handler in ``_start``
+   ldr r0, =irq_handler
+   ldr r1, =__dtcm_region_end
+   str r0, [r1]

    // After all of our setup is done, we can finally switch to main.</code></pre>

<p>
    The BIOS takes care of the busywork, so my interrupt handler can be a regular Rust function with
    the regular C calling convention:
</p>

<pre data-snippet="14/irq.rs"><code>// File: interrupts.rs
#[unsafe(no_mangle)]
pub extern "C" fn handle_irq() {
    todo!()
}</code></pre>

<p>
    Right now, interrupts are entirely disabled because the Interrupt Master Enable register is
    unset; even if it wasn't, there's another two registers I need to set to actually enable
    interrupts: the Interrupt Enable register (that's right, there's two) and the Display Status
    register. The Interrupt Enable register is a bitfield of the enabled interrupt types, with bit 0
    being vertical blank interrupts. I'll write a middleware function to handle setting IME and IE:
</p>

<pre data-snippet="14/wfi.rs"><code>// File: interrupts.rs
pub static REG_IME: VolAddress&lt;u32, Safe, Safe&gt; =
    unsafe { VolAddress::new(0x4000208) };

static REG_IE: VolAddress&lt;u32, Safe, Safe&gt; =
    unsafe { VolAddress::new(0x4000210) };

/**
 * Waits for the next interrupt, based on the provided mask.
 */
pub fn wait_for_interrupt(mask: u32) {
    let old_mask = REG_IE.read();

    REG_IE.write(mask);
    // enable global interrupts
    REG_IME.write(1);

    todo!("Actually halt the processor!")

    REG_IME.write(0);
    REG_IE.write(old_mask);
}</code></pre>

<p>
    I'll write a second wrapper function that sets DISPSTAT and then calls the
    <code>wait_for_interrupt</code> function as above:
</p>

<pre data-snippet="14/wfb.rs"><code>/**
 * Waits for the next vertical blank interrupt.
 */
pub fn wait_for_vblank() {
    let old_disp_stat = REG_DISPSTAT.read();
    // Bit 3 of DISPSTAT enables vertical blank interrupts
    let new_disp_stat = old_disp_stat | 0b100;

    REG_DISPSTAT.write(new_disp_stat);
    wait_for_interrupt(0b1);
    REG_DISPSTAT.write(old_disp_stat);
}</code></pre>

<p>
    Finally, I need to fill in the code that <em>actually</em> halts the system. The easiest way is
    to delegate it to the BIOS with an SWI (Software Interrupt) instruction; this will jump into the
    BIOS and call Software Interrupt #0x06 which is the interrupt for Halt.
</p>

<pre data-snippet="14/swi_halt.s"><code>.section .text

// See the definitions in ``supervisor.rs`` for more information.
// 
// GBATEK says:
// Caution: When invoking SWIs from inside of ARM state specify SWI NN*10000h, instead of
// SWI NN as in THUMB state.

.global SWI_Halt
SWI_Halt:
    swi #0x60000
    bx lr</code></pre>

<p>
    I'll replace the <code>todo!()</code> in the interrupt waiter with
    <code>unsafe { SWI_Halt() }</code>. The last thing to do is to <em>acknowledge</em> the
    interrupt by setting the same bit in the Interrupt Flags register (at <code>0x4000202</code>) as
    well as the bits at the fixed address DTCM + 3FFCh, which is done by another assembly function.
</p>

<pre data-snippet="14/irq-2.rs"><code>#[unsafe(no_mangle)]
pub extern "C" fn irq_handler() {
    let mask = REG_IF.read();
    if mask &amp; 0b1 == 0 {
        // don't care
        return
    }

    // clear V-blank bit
    REG_IF.write(0b1);
    unsafe {
        _set_irq_flags(0b1);
    }
}</code></pre>

<blockquote>
    <h4>Editor's note!</h4>

    <p>
        As it turns out, none of that was needed because the next section doesn't care about
        interrupts. Oh well. It's still good to have for the future, and to lower power usage by
        explicitly halting instead of spinning.
    </p>
</blockquote>

<h3>Drawing to the screen</h3>

<p>
    Nitro has a very configurable graphics system based around <em>backgrounds</em> and
    <em>sprites</em> (known as <em>objects</em> internally) each with various configurable modes. It
    also has an impressive ~650KiB of video memory (you could run Wing Commander: Privateer on
    that!) configurable into various blocks.
</p>
<p>
    The logical way to output the solution for the AoC problem is to create sprites for every digit,
    upload them to video memory, and arrange the sprites on screen. I'm not going to do that and
    instead I will use Display Mode 2, one of the four display modes for Engine A:
</p>
<ul>
    <li>Mode 0: Screen off</li>
    <li>Mode 1: Normal graphics mode, with backgrounds and objects</li>
    <li>Mode 2: Framebuffer mode, using video RAM</li>
    <li>Mode 3: Framebuffer mode, using DMA from main memory</li>
</ul>

<p>Mode 2 lets me treat the entire of VRAM Bank A as a framebuffer of 16-bit BGR555 pixels.</p>

<blockquote>
    <p>
        A framebuffer is a block of memory treated as a raw bitmap which is drawn upon to make a
        final frame. The framebuffer is copied to the output device (in my case, the LCD) and
        presented to the user. This is how graphics was done in the olden days, with code writing
        directly to the framebuffer.
    </p>

    <p>
        On modern graphics cards, the framebuffer is handled internally by the GPU and commands are
        instead sent to efficiently draw to it, rather than having the user code drawing directly to
        the framebuffer.
    </p>
</blockquote>

<blockquote>
    <p>
        The 650KiB of video memory is separated into multiple configurable banks which changes
        address in the VRAM area of memory depending on its configuration. Each VRAM bank has a
        register for controlling it's mode (three bits) and offset (two bits), as well as if it is
        enabled or not.
    </p>

    <p>
        Framebuffer mode and capture mode can use one of the first four banks, so for simplicity I
        will just use the first bank, which is 128KiB and is mapped at <code>0x6800000</code>
        in mode zero. This is more than enough to fit the entire framebuffer into the screen.
    </p>
</blockquote>

<p>
    I'm going to be using the
    <a href="https://docs.rs/embedded-graphics/latest/embedded_graphics/">embedded_graphics</a>
    library for drawing things on the screen, as it saves a lot of effort writing the individual
    primitives. First, I need a framebuffer implementation:
</p>

<pre data-snippet="15/fb.rs"><code>pub struct LcdFramebuffer {
    vram: VolBlock&lt;u16, Safe, Safe, 131072&gt;,
}

impl LcdFramebuffer {
    pub fn new() -&gt; LcdFramebuffer {
        let vram = unsafe { voladdress::VolBlock::new(0x6800000) };
        return LcdFramebuffer { vram };
    }
}

impl OriginDimensions for LcdFramebuffer {
    fn size(&amp;self) -&gt; embedded_graphics::prelude::Size {
        return Size::new(256, 192);
    }
}

impl DrawTarget for LcdFramebuffer {
    type Color = Bgr555;
    type Error = core::convert::Infallible;

    fn draw_iter&lt;I: IntoIterator&lt;Item = embedded_graphics::Pixel&lt;Self::Color&gt;&gt;&gt;(
        &amp;mut self,
        pixels: I,
    ) -&gt; Result&lt;(), Self::Error&gt; {
        let bound = self.size();

        for Pixel(coord, colour) in pixels {
            if coord.x &lt; 0 || coord.x &gt; bound.width as i32 {
                continue;
            }

            if coord.y &lt; 0 || coord.y &gt; bound.height as i32 {
                continue;
            }

            let pos = coord.x + (coord.y * 256);
            let offset = self.vram.index(pos as usize);
            offset.write(colour.into_storage());
        }

        return Ok(());
    }
}</code></pre>

<p>Next, I can enable VRAM Bank A by sitting bit 7 (enable) of the VRAMCTL_A in my main:</p>

<pre><code>VRAMCTL_A.write(0b10000000);</code></pre>

<p>
    Display mode is controlled by bits 16-17 of the DISPCNT_A register (Display Control for Engine
    A). I'll set that to Mode 2 like so:
</p>

<pre data-snippet="15/dispcnt.rs"><code>let mut dispcnt = DISPCNT_A.read();
let bits = 2u32 &lt;&lt; 16;
dispcnt |= bits;
DISPCNT_A.write(dispcnt);</code></pre>

<blockquote>
    <h4>Warning!</h4>

    <p>
        GBATEK notes that bit 7 of DISPCNT enabled forced blanking, turning the screen white and
        enabling faster access to VRAM. In all the emulators I tested, this does <i>not</i>
        work in framebuffer mode.
    </p>
</blockquote>

<p>Finally, I can draw the trademark OpenGL triangle with a Triangle primitive:</p>

<pre data-snippet="15/triangle.rs"><code>let mut lcd = LcdFramebuffer::new();
let tri = Triangle::new(Point::new(10, 10), Point::new(100, 10), Point::new(10, 100))
    .into_styled(PrimitiveStyle::with_fill(Bgr555::new(31, 0, 0)));
tri.draw(&amp;mut lcd).unwrap();</code></pre>

<p><img src="https://sailor.li/static/aocnds/img/opengl_triangle.png"></p><p>
    Unfortunately, upon seeing this I immediately became overwhelmed with evil power and used the
    <a href="https://docs.rs/tinybmp/0.6.0/tinybmp/">tinybmp</a> library to pull in an image and
    some text:
</p>

<p><img src="https://sailor.li/static/aocnds/img/yum_yum_squid.png"></p><blockquote>
    <p>
        The overhead of the inefficient debug code makes this comically slow. It takes about 20
        seconds for everything to be fully drawn at 60fps, because moving from main memory to video
        memory byte-by-byte is very sluggish.
    </p>

    <p>
        The right way to do this - without even optimising the inner loop - would be to draw to an
        in-memory framebuffer, then use a <abbr title="Direct Memory Access">DMA</abbr>
        transfer to copy into VRAM significantly faster; and, only after doing that, turn on the
        screen. At least it's fast enough in release mode.
    </p>
</blockquote>

<p>The final step is to actually hook up the Advent of Code solution to the font:</p>

<pre data-snippet="15/aoc.rs"><code>Text::new(
    format!("The solution is: {}", solve_aoc2021_pt1()).as_str(),
    Point::new(0, 175),
    font,
)
.draw(&amp;mut lcd)
.unwrap();</code></pre>

<h3>Conclusion</h3>

<p>
    This is a bit of a bootleg solution. The puzzle input is hardcoded into the program text which
    cuts in to precious memory; if I wanted to do more of the puzzles, I'd have to either hardcode
    them as well into memory (which takes up even more memory) or read them from the cart (which is
    an involved process). I'd also need to add the ability to select the solution, which would
    require input, which requires writing code onto the ARM7 because the ARM9 doesn't get to know
    input.
</p>

<p>
    But those are all topics for another day, because this post is already very long and
    implementing more complex features such as cart transfers or extended graphics is an entire
    entry's worth of detail in itself. So, I'll leave it at this.
</p>

<p>
    The final thing to say is that this project was disappointingly easy. It took me about two weeks
    to implement, working on and off, and most of it Just Worked; I only had to debug maybe two or
    three things. Hopefully extending this into a proper SDK will be harder.
</p>



        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Always go to the funeral (2005) (465 pts)]]></title>
            <link>https://www.npr.org/2005/08/08/4785079/always-go-to-the-funeral</link>
            <guid>42435972</guid>
            <pubDate>Mon, 16 Dec 2024 22:04:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.npr.org/2005/08/08/4785079/always-go-to-the-funeral">https://www.npr.org/2005/08/08/4785079/always-go-to-the-funeral</a>, See on <a href="https://news.ycombinator.com/item?id=42435972">Hacker News</a></p>
Couldn't get https://www.npr.org/2005/08/08/4785079/always-go-to-the-funeral: Error: timeout of 10000ms exceeded]]></description>
        </item>
    </channel>
</rss>