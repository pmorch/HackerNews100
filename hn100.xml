<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 10 Aug 2024 16:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Deep Live Cam: Real-time face swapping and one-click video deepfake tool (101 pts)]]></title>
            <link>https://deeplive.cam</link>
            <guid>41209181</guid>
            <pubDate>Sat, 10 Aug 2024 13:05:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deeplive.cam">https://deeplive.cam</a>, See on <a href="https://news.ycombinator.com/item?id=41209181">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="hero"><h2>Deep Live Cam<br> <!-- -->The Next Leap in Real-Time Face Swapping and Video Deepfake Technology</h2><p>Deep Live Cam harnesses cutting-edge AI to push the boundaries of real-time face swapping and video deepfakes.<br> <!-- -->Achieve high-quality face replacement with just a single image.</p></section><div><div id="features"><p><h4>editions</h4><h2>Deep Live Cam Supports Multiple Execution Platforms</h2></p></div><div><h2>Deep Live Cam: Bringing Your Ideas to Life</h2><p>Deep Live Cam is a state-of-the-art AI tool that delivers astonishingly accurate real-time face swapping and video deepfakes. Here's what sets it apart:</p><div><div><p>Swap faces in real-time using a single image, with instant preview capabilities.</p></div><div><div><p><span>One-Click Video Deepfakes</span></p></div><p>Generate high-quality deepfake videos quickly and easily with simple operations.</p></div><div><p>Run on various platforms including CPU, NVIDIA CUDA, and Apple Silicon, adapting to different hardware setups.</p></div><div><p>Built-in checks prevent processing of inappropriate content, ensuring legal and ethical use.</p></div><div><p>Leverages optimized algorithms for significantly faster processing, especially on CUDA-enabled NVIDIA GPUs.</p></div><div><p>Benefit from an active community providing ongoing support and improvements, keeping the tool at the cutting edge.</p></div></div></div><div><h2>How Deep Live Cam Works</h2><p>Deep Live Cam employs advanced AI algorithms to achieve real-time face swapping and video deepfakes.</p></div><section><h2>What Users Are Saying About Deep Live Cam on X</h2><p>Explore real experiences and creations shared by developers and users on X. See how Deep Live Cam is inspiring creativity and solving practical problems across various fields, from stunning face-swap effects to innovative applications.</p></section><div id="faq"><div><h2>Frequently Asked Questions About Deep Live Cam</h2><p>Get answers to common questions about Deep Live Cam</p></div><div><div><h3>What is Deep Live Cam?</h3><p>Deep Live Cam is an open-source tool for real-time face swapping and one-click video deepfakes. It can replace faces in videos or images using a single photo, ideal for video production, animation, and various creative projects.</p><hr></div><div><h3>What are the main features of Deep Live Cam?</h3><p>Deep Live Cam's key features include: 1) Real-time face swapping; 2) One-click video deepfakes; 3) Multi-platform support; 4) Ethical use safeguards.</p><hr></div><div><h3>How do I use Deep Live Cam?</h3><p>To use Deep Live Cam: 1) Set up the required environment; 2) Clone the GitHub repository; 3) Download necessary models; 4) Install dependencies; 5) Run the program; 6) Select source image and target; 7) Start the face-swapping process.</p><hr></div><div><h3>Which platforms does Deep Live Cam support?</h3><p>Deep Live Cam supports various execution platforms, including CPU, NVIDIA CUDA, Apple Silicon (CoreML), DirectML (Windows), and OpenVINO (Intel). Users can choose the optimal platform based on their hardware configuration.</p><hr></div><div><h3>How does Deep Live Cam prevent misuse?</h3><p>Deep Live Cam incorporates built-in checks to prevent processing of inappropriate content (e.g., nudity, violence, sensitive material). The developers are committed to evolving the project within legal and ethical frameworks, implementing measures like watermarking outputs when necessary to prevent abuse.</p><hr></div><div><h3>Is Deep Live Cam free to use?</h3><p>Yes, Deep Live Cam is an open-source project and completely free to use. You can access the source code on GitHub and use it freely.</p><hr></div><div><h3>Can I use Deep Live Cam for commercial purposes?</h3><p>While Deep Live Cam is open-source, for commercial use, you should carefully review the project's license terms. Additionally, using deepfake technology may involve legal and ethical considerations. We recommend consulting with legal professionals before any commercial application.</p><hr></div><div><h3>What are the hardware requirements for Deep Live Cam?</h3><p>Deep Live Cam's performance varies with hardware configuration. Basic functionality runs on standard CPUs, but for optimal performance and results, we recommend using CUDA-enabled NVIDIA GPUs or devices with Apple Silicon chips.</p><hr></div><div><h3>Does Deep Live Cam support real-time video stream processing?</h3><p>Yes, Deep Live Cam supports real-time video stream processing. You can use a webcam for real-time face swapping, with the program providing live preview functionality.</p><hr></div><div><h3>How can I improve the face-swapping results in Deep Live Cam?</h3><p>To enhance face-swapping results, try: 1) Using high-quality, clear source images; 2) Choosing source and target images with similar angles and lighting; 3) Adjusting program parameters; 4) Running the program on more powerful hardware.</p><hr></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A wonderful coincidence or an expected connection: why π² ≈ g (201 pts)]]></title>
            <link>https://roitman.io/blog/91</link>
            <guid>41208988</guid>
            <pubDate>Sat, 10 Aug 2024 12:24:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://roitman.io/blog/91">https://roitman.io/blog/91</a>, See on <a href="https://news.ycombinator.com/item?id=41208988">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>Let’s take a brief trip back to our school years and recall some lessons in mathematics and physics. Do you remember what the number π equals? And what is π squared? That’s a strange question too. Of course, it’s 9.87. And do you remember the value of the acceleration due to gravity, g? Of course, that number was drilled into our memory so thoroughly that it’s impossible to forget: 9.81 m/s². Naturally, it can vary, but for solving basic school problems, we typically used this value.</p><p>August 9, 2024</p></div><p><span><h2><strong>Mysterious equality</strong></h2><p>And now, here’s the next question: how on earth is it that π² is approximately equal to g? You might say that such questions aren’t asked in polite society. First of all, they aren’t exactly equal. There’s already a difference in the second decimal place. Secondly, π is a dimensionless number, while g is a physical quantity with its own units.</p><p>And yet, no matter how you look at it, this can’t just be a simple coincidence.</p><h2><strong>Not as simple as it seems</strong></h2><p>Let's start by taking a close look at the right side. The value 9.81 is in m/s². But these are far from the only units of measurement. If you express this value in any other units, the magic immediately disappears. So, this is no coincidence—let's dig deeper into the meters and seconds.</p><p>What exactly is a "meter," and how could it be related to π? At first glance, not at all. According to Wikipedia, a "meter is the distance light travels in a vacuum during a time interval of 1/299,792,458 seconds." Great, now we have seconds involved—good! But there's still nothing about π.</p><p>Wait a minute, why exactly 1/299,792,458? Why not, for example, 1/300? Where did this number come from in the first place? It seems we need to delve into the history of the unit of length itself to understand this better.</p><h2>A standard for every honest merchant</h2><p>In the past, people didn't bother much with standards: they only cared about what was convenient for measurement. For example, why not measure length in human cubits? It might not be precise, but it was cheap, reliable, and practical. And the fact that everyone's cubits were of different lengths? Sometimes that was even useful. If you needed to buy more cloth, you'd call the tallest person in the village and have them measure the fabric with their cubits.</p><p>Later on, of course, people began thinking about standardization. They started creating various standards. But this turned out to be inconvenient and cumbersome: you couldn't always run to a single standard for measurement. So, copies of the standards began to appear. And then copies of the copies...</p><p>Serious people decided that such chaos was hindering serious business, so they set a goal: to come up with a definition of a unit of length that wouldn't depend on any arbitrary standards. It should only depend on natural constants, so that anyone with some basic tools could reproduce and measure it.</p><h2>Bright dreams of standardization and insidious gravity</h2><p>A "standard-free" definition for the meter was actually proposed back in the 17th century. The Dutch mechanic, physicist, mathematician, astronomer, and inventor Christiaan Huygens suggested using a simple pendulum for this purpose. You take a small object and suspend it on a string. The length of the string should be such that the pendulum completes a full oscillation (returns to its original position) in exactly two seconds. This length of string was called the "universal measure" or the "Catholic meter." This length differed from the modern meter by about half a centimeter.</p><p>The proposal was well-received and adopted. However, problems soon arose. First, Huygens was dealing with what he called a "mathematical pendulum." This is a "material point suspended on a weightless, inextensible string." A material point and a weightless string are hardly the simple tools that every merchant would have on hand.</p><p>Second, it was quickly discovered that the length of the pendulum's string varied in different parts of the Earth. Gravity cunningly decreased as one approached the equator and did not cooperate with humanity's bright dream of standardization.</p><h2>An astonishing equation</h2><p>But let’s return to our mysterious equation. To find the period of small oscillations of a mathematical pendulum as a function of the length of the suspension, the following formula is used:</p><figure><img src="https://nkjhvudpdnbuifryqtzj.supabase.co/storage/v1/object/public/pictures/public/e6232f8b-513a-46c4-a056-a0537b26f421"><figcaption></figcaption></figure><p>And here it is—our π! Let's substitute the parameters of Huygens’ pendulum into this formula. The length of the string l in Huygens' pendulum equals 1. The T - oscillation equals 2. Plugging these values into the formula, we get π²=g.</p><p>So, have we found the answer to our question? Well, not quite. We already saw that the equality is only approximate. It doesn’t feel right to equate 9.87 and 9.81 exactly. Does this mean that the meter has changed since then?</p><h2>With revolutionary greetings from France</h2><p>Yes, indeed, it did change! This occurred during the reform of the units of measurement initiated by the French Academy of Sciences in 1791. Intelligent people suggested maintaining the definition of the meter through the pendulum, but with the clarification that it should specifically be a French pendulum—at the latitude of 45° N (approximately between Bordeaux and Grenoble).</p><p>However, this did not sit well with the commission in charge of the reform. The problem was that the head of the commission, Jean-Charles de Borda, was a fervent supporter of transitioning to a new (revolutionary) system of angle measurement—using grads (a grad being one-hundredth of a right angle). Each grad was divided into 100 minutes, and each minute into 100 seconds. The method of the seconds pendulum did not fit into this neat concept.</p><h2>The true and final meter</h2><p>In the end, they successfully got rid of the seconds and defined the meter as one forty-millionth of the Paris meridian. Or, alternatively, as one ten-millionth of the distance from the North Pole to the equator along the surface of the Earth’s ellipsoid at the longitude of Paris. This measurement slightly differed from the "pendulum" meter. The commission, without false modesty, dubbed the resulting value as the "true and final meter."</p><p>The idea of a universal standard accessible to everyone waved goodbye and faded into the sunset. Need an accurate standard for the meter? No problem! All you have to do is measure the length of a meridian and divide it by a few million. By the way, the French actually did this—they physically measured a portion of the Paris meridian, the arc from Dunkirk to Barcelona. They laid out a chain of 115 triangles across France and part of Spain. Based on these measurements, they created a brass standard. Incidentally, they made a mistake—they didn't account for the Earth's polar flattening.</p><h2><strong>Conclusion</strong></h2><p>Let's return to our equation once again. Now we know where the inaccuracy comes from: π² and g differ by about 0.06. If it weren't for yet another attempt to reform and improve everything, we would now have a slightly different value for the meter and the elegant equation π² = g. Later, scientists did return to defining the meter through unchanging and reproducible natural constants, but the meter standard was no longer the same.</p></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ladybird browser to start using Swift language this fall (159 pts)]]></title>
            <link>https://twitter.com/awesomekling/status/1822236888188498031</link>
            <guid>41208836</guid>
            <pubDate>Sat, 10 Aug 2024 11:52:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/awesomekling/status/1822236888188498031">https://twitter.com/awesomekling/status/1822236888188498031</a>, See on <a href="https://news.ycombinator.com/item?id=41208836">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Susan Wojcicki has died (150 pts)]]></title>
            <link>https://www.facebook.com/share/p/qe2ZMcs9Bz4K1SPt/</link>
            <guid>41207446</guid>
            <pubDate>Sat, 10 Aug 2024 05:07:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.facebook.com/share/p/qe2ZMcs9Bz4K1SPt/">https://www.facebook.com/share/p/qe2ZMcs9Bz4K1SPt/</a>, See on <a href="https://news.ycombinator.com/item?id=41207446">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Susan Wojcicki has died (510 pts)]]></title>
            <link>https://twitter.com/sundarpichai/status/1822132667959386588</link>
            <guid>41207415</guid>
            <pubDate>Sat, 10 Aug 2024 04:58:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/sundarpichai/status/1822132667959386588">https://twitter.com/sundarpichai/status/1822132667959386588</a>, See on <a href="https://news.ycombinator.com/item?id=41207415">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Defcon stiffs badge HW vendor, drags FW author offstage during talk (431 pts)]]></title>
            <link>https://twitter.com/mightymogomra/status/1822119942281650278</link>
            <guid>41207221</guid>
            <pubDate>Sat, 10 Aug 2024 03:59:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/mightymogomra/status/1822119942281650278">https://twitter.com/mightymogomra/status/1822119942281650278</a>, See on <a href="https://news.ycombinator.com/item?id=41207221">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Building a highly-available web service without a database (200 pts)]]></title>
            <link>https://blog.screenshotbot.io/2024/08/10/building-a-highly-available-web-service-without-a-database/</link>
            <guid>41206908</guid>
            <pubDate>Sat, 10 Aug 2024 02:37:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.screenshotbot.io/2024/08/10/building-a-highly-available-web-service-without-a-database/">https://blog.screenshotbot.io/2024/08/10/building-a-highly-available-web-service-without-a-database/</a>, See on <a href="https://news.ycombinator.com/item?id=41206908">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>If you’ve ever built a web service or a web app, you know the drill: pick a database, pick a web service framework (and in today’s day and age, pick a front-end framework, but let’s not get into that).</p>



<p>This has been the case for several decades now, and people don’t stop to question if this is still the best way to build a web app. Many things have changed in the last decade:</p>



<ul>
<li>Disk is a lot faster (NVMe)</li>



<li>Disk is a lot more robust (EBS/EFS etc.)</li>



<li>RAM is super cheap, for most startups you could probably fit all your data in RAM</li>



<li>You can rent a machine with hundreds of cores if your heart desires.</li>
</ul>



<p>This was not the case when I first worked at a Rails startup in 2010. But most importantly, there’s one new very important change that’s happened in the last decade:</p>



<ul>
<li>The <a href="https://en.wikipedia.org/wiki/Raft_(algorithm)">Raft Consensus algorithm</a> was published in 2014 with many robust implementations easily available.</li>
</ul>



<p>In this blog post, we’re going to break down a new architecture for web development. We use it successfully for <a href="https://screenshotbot.io/">Screenshotbot</a>, and we hope you’ll use it too.</p>



<p>I’ll break this blog post into three parts: Explore, Expand and Extract, obviously referencing <a href="https://tidyfirst.substack.com/">Kent Beck</a>‘s 3X. Your needs are going to vary in each of these stages of your startup, and I’m going to demonstrate how you use the architecture in all three phases.</p>



<h2>Explore</h2>



<p>So you’re a new startup. You’re iterating on a product, you have no idea how people are going to use it, or even <em>if</em> they’re going to use it.</p>



<p>For most startups today, this would mean you’ll pick Rails or Django or Node or some such, backed with a MySQL or PostgreSQL or MongoDB or some such.</p>



<p>“<em>Keep it simple silly</em>,” you say, and this seems simple enough.</p>



<p>But is this as simple as possibly can be? Could we make it simpler? What if the web service and the database instance were exactly one and the same? I’m not talking about using something like SQLite where your data is still serialized, I’m saying what if all the memory in your RAM <em>is</em> your database.</p>



<p>Imagine all the wonderful things you could build if you never had to serialize data into SQL queries. First, you don’t need multiple front-end servers talking to a single DB, just get a bigger server with more RAM and more CPU if you need it. What about indices? Well, you can use in-memory indices, effectively just hash-tables to lookup objects. You don’t need clever indices like B-tree that are optimized for disk latency. (In fact, you can use some indices that were probably not possible with traditional databases. <a href="https://blog.screenshotbot.io/2023/10/29/scaling-screenshotbot/">One such index</a> using functional collections was critical to the scalability of Screenshotbot.)</p>



<p>You also won’t need special architectures to reduce round-trips to your database.  In particular, you won’t need any of that Async-IO business, because your threads are no longer IO bound. Retrieving data is just a matter of reading RAM. Suddenly debugging code has become a lot easier too.</p>



<p>You don’t need any services to run background jobs, because background jobs are just threads running in this large process.</p>



<p>You don’t need crazy concurrency protocols, because most of your concurrency requirements can be satisfied with simple in-memory mutexes and condition variables.</p>



<p>But then comes the important part: how do you recover when your process crashes? It turns out that answer is easy, periodically just take a snapshot of everything in RAM.</p>



<p>Hold on, what if you’ve made changes since the last snapshot? And this is the clever bit: you ensure that every time you change parts of RAM, we write a transaction to disk. So if you have a line like <code>foo.setBar(2)</code>, this will first write a transaction that says we’ve changed the <code>bar</code> field of <code>foo</code> to 2, and then actually set the field to 2. An operation like <code>new Foo()</code> writes a transaction to disk to say that a Foo object was created, and then returns the new object.</p>



<p>And so, if your process crashes and restarts, it first reloads the snapshot, and replays the transaction logs to fully recover the state. (Notice that index changes don’t need to be part of the transaction log. For instance if there’s an index on field <code>bar</code> from <code>Foo</code>, then <code>setBar</code> should just update the index, which will get updated whether it’s read from a snapshot, or from a transaction.)</p>



<p>Finally, this architecture enables some new kind of code that couldn’t be written before. Since all requests are being served by the same process, which <em>usually</em> doesn’t get killed, it means you can store closures in memory that can be used to serve pages. For example on Screenshotbot, if you ever see a “<a href="https://screenshotbot.io/n/" rel="nofollow">https://screenshotbot.io/n/</a><em>nnnnnnn</em>” URL, it’s actually a closure on the server, where <em>nnnnnnn</em> maps to an internal closure. But amazingly, this simple change means we don’t need to serialize objects across page transitions. The closure has references to the objects, so we don’t need to pass around object-ids across every single request. In Javascript, this might hypothetically look like:</p>



<pre><code>function renderMyObject(obj) {
   return &lt;html&gt;...
            &lt;a href=(() =&gt; obj.delete()) &gt;Delete&lt;/a&gt;
            ...
          &lt;/html&gt;
} </code></pre>



<p>What this all means is that you can iterate quickly. If you have to debug, there’s exactly one service that you need to debug. If you need to profile code, there’s exactly one service you need to profile (no more MySQL slow query logs). There’s exactly one service to monitor: if that one service goes down the site certainly goes down, but since there’s only one service and one server, the probability of failure is also much lower. If the server dies, AWS will automatically bring up a new server to replace it within a few minutes.</p>



<p>It’s also a lot easier to write test code, since you no longer have to mock out databases.</p>



<h2>Expand</h2>



<p>So you’re moving fast, iterating, and building out ideas, and slowly getting customers along the way.</p>



<p>Then one day, you get a high-profile customer. Bingo, you’re now in the <em>Expand</em> phase of your startup.</p>



<p>But there’s a catch: this high-profile customer requires 99.999% availability. </p>



<p>Surely, the architecture we just described cannot handle this. If the server goes down, we would need to wait several minutes for AWS to bring it back up. Once it’s back up, we might wait several minutes for our process to even restore the snapshot from disk. Even re-deploys are tricky: restarting the service can bring down the server for multiple minutes.</p>



<p>And this is where the Raft Consensus Protocol comes in to place. </p>



<p>Raft is a wonderful algorithm and protocol. It takes your <em>finite state machine</em> (your web server/database), and essentially replicates the transaction log. So now, we can take our very simple architecture and replicate it across three machines. If the leader goes down, then a new leader is elected within seconds and will continue to serve requests.</p>



<p>We’ve just made our simple little service into a full-fledged highly-available database, without fundamentally changing how developers write code.</p>



<p>With this mechanism, you can also do a rolling deploy without ever bringing the server down. (Although we rarely restart our server processes, more on that in a moment.) Because there’s just one service, it’s also easy to calculate your availability guarantees.</p>



<h2>Extract</h2>



<p>So your startup is doing well, and you have thousands of large customers.</p>



<p>To be honest, Screenshotbot is not at this stage, I’ll talk about where we are in a moment. But we’re preparing for this possibility, with monitoring in place for predicted bottlenecks. </p>



<p>The solution here is something large companies already do with their databases: sharding. You can break up your web services into shards, each shard being its own cluster. In particular, at Screenshotbot we already do this: each of our enterprise customers get their own dedicated cluster. (Fun story: Meta <a href="https://engineering.fb.com/2023/05/16/data-infrastructure/mysql-raft-meta/">switched to Raft</a> to handle replication for each of its MySQL clusters, so we’re essentially doing the same thing but without using a separate database.)</p>



<p>I don’t know what else to expect, since I’m more of a solve-today’s-problem kind of person. The main bottleneck I expect to see is scaling the commit-thread. The read threads parallelize beautifully. There’s one commit-thread that’s applying each transaction one at a time. It turns out the disk latency is irrelevant for this, since the Raft algorithm will just commit multiple transactions together to disk. My main concern is that the CPU cost for applying the transactions will exceed the single core performance. I highly doubt that I would ever see this, but it’s a possibility. At this point we could profile the cost of commits and improve it (for instance, move some of the work out of the transaction thread),  or we could just figure out sharding. I’ll probably write another blog post when that happens.</p>



<h2>Our Stack</h2>



<p>Now that I’ve described the idea to you, let me tell you about our stack, and why it turned out to be so suitable for this architecture.</p>



<p>We use Common Lisp. My initial implementation of Screenshotbot did use MySQL, but I quickly swapped it out for <a href="https://github.com/bknr-datastore/bknr-datastore">bknr.datastore</a> exactly because handling concurrency with MySQL was hard and Screenshotbot is a highly concurrent app. BKNR Datastore is a library that handles the architecture described in the <em>Explore</em> section, but built for Common Lisp. (There are similar libraries for other languages, but not a whole lot of them.)</p>



<p>Common Lisp is also heavily multi-threaded, and this is going to be crucial for this architecture since your web requests are being handled by threads in a single process. Ruby or Python would be disqualified by this requirement.</p>



<p>We also use the idea of closures that I mentioned earlier. But this means we can’t keep restarting the server frequently (if you restart the server, you lose the closures). So reloading code is just hot-reloading code in the running process. It turns out Common Lisp is excellent at this: so much so that a large part of the standard is all about handling reloading code. (For instance, if the class definition changes, how do you update objects of that class? <a href="http://clhs.lisp.se/Body/f_reinit.htm#reinitialize-instance">There’s a standard for it</a>.)</p>



<p>Occasionally, we do restart the servers. Currently, it looks like we only restart the servers about once every month or two months. When we need to do this, we just do a rolling restart with our Raft cluster. We use a cluster of 3 servers per installation, which allows for one server to go down. We don’t use Kubernetes, we don’t need it (at least, not yet).</p>



<p>For the Raft implementation, we wrote our own custom library built on top of bknr.datastore. We built and open-sourced <a href="https://github.com/tdrhq/bknr.cluster">bknr.cluster</a>, that under the hood uses the fantastic <a href="https://github.com/baidu/braft">Braft</a> library from Baidu. Braft is super solid, and I can highly recommend it. Braft also handles background snapshots, which means while we’re taking snapshots, the server can still continue serving requests.</p>



<p>To store image files, or blobs that shouldn’t be part of the datastore, we use EFS (a highly available NFS) that is shared between the three servers. EFS is easier to work with than S3, because we don’t have to handle error conditions. EFS also makes our code more testable, since we aren’t interacting with an external server, and just writing to disk.</p>



<p><strong>How well does this scale?</strong> We have a couple of big enterprise customers, but one especially well-known customer. Screenshotbot runs on their CI, so we get API requests 100s of times for every single commit and Pull Request. Despite this, we only need a 4-core 16GB machine to serve their requests. (And similar machines for the replicas, mostly running idle.) Even with this, the CPU usage maxes out at 20%, but even then most of that comes from image processing, so we have a lot of room to scale before we need to bump up the number of cores.</p>



<h2>Summary</h2>



<p>I think this architecture is excellent for new startups, and I’m hoping more companies will adopt it. Obviously, you’ll need to build out some of the tooling we’ve built out for the language of your choice. (Although, if you choose to use Common Lisp, it’s all here for you to use, and all open-source.)</p>



<p>We’re super grateful to the folk behind bknr.datastore, Braft and Raft, because without their work we wouldn’t be able to do any of this.</p>



<p>If you think this was useful or interesting, I would appreciate it if you could share it on social media. Please reach out to me at <a href="mailto:arnold@screenshotbot.io">arnold@screenshotbot.io</a> if you have any questions.</p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rivian reduced electrical wiring by 1.6 miles and 44 pounds (139 pts)]]></title>
            <link>https://www.popsci.com/technology/rivian-zonal-electrical-architecture/</link>
            <guid>41206443</guid>
            <pubDate>Sat, 10 Aug 2024 00:12:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.popsci.com/technology/rivian-zonal-electrical-architecture/">https://www.popsci.com/technology/rivian-zonal-electrical-architecture/</a>, See on <a href="https://news.ycombinator.com/item?id=41206443">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-toc-container="">
			
<p>Unveiled just a couple of months ago, Rivian’s second-generation R1T pickup and R1S SUV will maintain their distinctive look, with playful headlamps and a sleek exterior shape. Underneath the surface is where the magic is taking place, specifically a wholly new electrical architecture the brand says is less costly and easier to service.</p>



<p><a rel="nofollow noreferrer" href="https://www.thedrive.com/news/the-rivian-r3s-retro-hatch-was-inspired-by-group-b-rally-legends" target="_blank">Rivian</a> senior vice president of electrical hardware Vidya Rajagopalan says the new electrical system offers more features, as well as an increase in sensing and computing capability. In the process of making the transition from Gen 1 to Gen 2 R1 vehicles, Rivian switched to a zonal architecture.</p>



<p>Ultimately, that results in a more sustainable option. The zonal approach reduces the total wiring length by a whopping 1.6 miles and enables each R1 vehicle to shed 44 pounds. Weight is a big deal for <a href="https://www.popsci.com/category/electric-vehicles/" target="_blank">EVs</a>, as it has a direct correlation to battery performance. Plus, the company claims a 20 percent savings in material costs and 15 percent reduction in its carbon footprint between Gen 1 and Gen 2.</p>



<p>All of it was developed in-house by Rivian’s hardware and software team, an impressive feat. Software complexity is a big deal, and Rivian is finding ways to simplify and streamline its golden goose as a software-defined automaker.</p>



<h2 id="h-zone-versus-domain-based-architecture"><strong>Zone versus domain-based architecture</strong></h2>



<p>In basketball, kids learn one-to-one defense early on. Each player is assigned to a competitor, and they stick to that person like glue for effective coverage. Zone defense requires each player to guard an area (zone) and any offensive player within those parameters. It’s a more elegant option, and one that requires more knowledge of the game.</p>



<p>Using a zonal approach, Rivian is showing off its technological prowess.</p>



<p>When Rivian engineers started building R1s, they designed a platform based on what’s called domain-based architecture, Rajagopalan explains. With this setup, each category of software is paired with a piece of hardware. Every time you open a door, raise the cabin temperature, slide your seats, turn on the lights, change the mode, and more, you could be connecting to different hardware. Those electrical control units, or ECUs, can multiply like Gremlins after dark.</p>



<figure><img decoding="async" width="2736" height="1324" loading="lazy" src="https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?strip=all&amp;quality=95" alt="x-ray-like image of SUV with ECU locations highlighted" srcset="https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png 2736w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1536&amp;h=743 1536w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=2048&amp;h=991 2048w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=930&amp;h=450 930w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=413&amp;h=200 413w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1364&amp;h=660 1364w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=827&amp;h=400 827w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1728&amp;h=836 1728w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1426&amp;h=690 1426w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=446&amp;h=216 446w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=835&amp;h=404 835w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1847&amp;h=894 1847w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1554&amp;h=752 1554w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1434&amp;h=694 1434w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=280&amp;h=135 280w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1440&amp;h=697 1440w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=289&amp;h=140 289w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=370&amp;h=179 370w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=308&amp;h=149 308w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=50&amp;h=24 50w" sizes="(max-width: 2736px) 100vw, 2736px"><figcaption>Each R1T and R1S is lighter in its second generation due to a vast reduction in electrical control units and wiring. <em>Image: Rivian</em> </figcaption></figure>



<p>“We had 17 ECUs [in Gen 1], each dedicated to a category,” Rajagopalan says. “Other manufacturers can have between 40-150 per vehicle, depending on how they work.”</p>



<p>Even though Rivian was using significantly fewer pieces of hardware than their competitors, they wanted to improve the system. More ECUs means more parts overall; consequently, that leads to increased opportunities for failure.</p>



<p>For the second generation of vehicles, four categories get their own ECUs: infotainment, autonomy, vehicle access, drive units, and its battery management system. Every other vehicle function is controlled by just three ECUs Rivian refers to as West, East, and South. The infotainment ECU alone is as powerful as a laptop and has the capabilities of a smartphone.</p>



<figure><img decoding="async" width="2573" height="1357" loading="lazy" src="https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?strip=all&amp;quality=95" alt="x-ray-like image of SUV with wiring locations highlighted" srcset="https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg 2573w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1536&amp;h=810 1536w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=2048&amp;h=1080 2048w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=853&amp;h=450 853w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=379&amp;h=200 379w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1251&amp;h=660 1251w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=758&amp;h=400 758w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1585&amp;h=836 1585w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1308&amp;h=690 1308w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=410&amp;h=216 410w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=766&amp;h=404 766w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1695&amp;h=894 1695w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1426&amp;h=752 1426w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1316&amp;h=694 1316w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=280&amp;h=148 280w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1440&amp;h=759 1440w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=289&amp;h=152 289w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=370&amp;h=195 370w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=308&amp;h=162 308w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=50&amp;h=26 50w" sizes="(max-width: 2573px) 100vw, 2573px"><figcaption>Switching from domain-based to zonal architecture allowed the company to reduce its complexity and improve scalability.&nbsp;<em>Image: Rivian</em> </figcaption></figure>



<p>However, the ECU that runs the vehicle’s autonomy platform is the most powerful computer in the R1S and R1T. The system includes an array of 11 internally developed cameras and five radars performing over 250 trillion operations per second, which<a rel="nofollow noreferrer" href="https://stories.rivian.com/meet-the-new-r1" target="_blank"> Rivian says is an industry-leading statistic</a>. As such, it connects to artificial intelligence, which helps identify and perceive the world in front of you to detect street signs, lanes, pedestrians, and more.</p>



<h2 id="h-reducing-parts-improves-scalability-and-reduces-cost"><strong>Reducing parts improves scalability and reduces cost</strong></h2>



<p>Fewer ECUs also results in less wiring, and another upshot of casting off ECUs is reduced weight.</p>



<p>“If your ECUs control function by function, you’ll have long wires running all over the car,” Rajagopalan says. “When you switch to a zone mindset, it’s more like a wheel-and-spoke function. Consolidation always wins; having fewer pieces reduces cost and makes manufacturing easier.”</p>



<p>It also makes room for scalability, something Rivian will need in order to grow.</p>



<p>Rajagopalan was working for Tesla as a senior director of engineering before joining Rivian at the end of 2020. Now she manages 400+ people at Rivian and has witnessed the evolution of the company. She says when Rivian first launched the first generation of its R1T and R1S, buyers loved the adventure-focused brand and the vehicles’ <a href="https://www.popsci.com/technology/rivian-r1t-r1s-handling/" target="_blank">off-road capabilities</a>. Since then, the automaker has proven it can ramp up its manufacturing capacity and make improvements.</p>



<p>With a new joint venture with Volkswagen in progress, Rivian will have the opportunity to build a common platform on a much larger scale. To start,<a rel="nofollow noreferrer" href="https://rivian.com/en-GB/newsroom/article/rivian-and-volkswagen-group-announce-plans-for-joint-venture" target="_blank"> VW will invest $1 billion in Rivian and invest another $4 billion over time</a>. In turn, VW will benefit from Rivian’s software-defined vehicle expertise and maverick approach.Meanwhile, we’ll be waiting for the <a href="https://www.popsci.com/technology/rivian-r3x-r2-2024/" target="_blank">smaller R2 and R3</a> to debut as Rivian ramps up.</p>
			
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Grace Hopper, Nvidia's Halfway APU (101 pts)]]></title>
            <link>https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/</link>
            <guid>41206025</guid>
            <pubDate>Fri, 09 Aug 2024 22:52:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/">https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/</a>, See on <a href="https://news.ycombinator.com/item?id=41206025">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Nvidia and AMD are the biggest players in the high performance GPU space. But while Nvidia has a huge GPU market share advantage over AMD, the latter’s CPU prowess makes it a strong competitor. AMD can sell both a CPU and GPU as one unit, and that capability has gotten the company wins in consoles and supercomputers. Oak Ridge National Laboratory’s Frontier supercomputer is one such example. There, AMD MI250X GPUs interface with a custom EPYC server CPU via Infinity Fabric.</p>
<p>Of course, Nvidia is not blind to this situation. They too have a high speed in-house interconnect, called NVLink. Nvidia has also dabbled with bundling CPUs alongside their GPUs. The Nintendo Switch’s Tegra X1 is a prominent example. But Tegra used relatively small CPUs and GPUs to target low power mobile applications. Grace Hopper is an attempt to get Nvidia’s CPU efforts into high performance territory. Beyond providing server-level CPU core counts and memory bandwidth, Grace Hopper comes with Nvidia’s top-of-the-line H100 datacenter GPU.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30408"><img decoding="async" width="688" height="387" data-attachment-id="30408" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_press_image/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_press_image.jpg?fit=1077%2C606&amp;ssl=1" data-orig-size="1077,606" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_press_image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_press_image.jpg?fit=1077%2C606&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_press_image.jpg?fit=688%2C387&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_press_image.jpg?resize=688%2C387&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_press_image.jpg?w=1077&amp;ssl=1 1077w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_press_image.jpg?resize=768%2C432&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>GH200 rendering from Nvidia, showing the CPU on the left and GPU on the right</figcaption></figure></div>
<p>I’ll be looking at GH200 as hosted on <a href="https://brokkr.hydrahost.com/">Hydra</a>. GH200 has several variants. The one I’m looking at has 480 GB of LPDDR5X memory on the CPU side, and 96 GB of HBM3 on the GPU side. I’ve already covered Neoverse V2 in Graviton 4, so I’ll focus on implementation differences rather than going over the core architecture again.</p>
<h2>System Architecture</h2>
<p>GH200 bundles a CPU and GPU together. The Grace CPU consists of 72 Neoverse V2 cores running at up to 3.44 GHz, supported by 114 MB of L3 cache. Cores and L3 cache sit on top of Nvidia’s Scalable Coherency Fabric (SCF). SCF is a mesh interconnect, with cores and L3 cache slices attached to mesh stops.</p>

<p>SCF’s responsibilities include ensuring cache coherency and proper memory ordering. From a core to core latency test, those requirements are satisfied with reasonably consistent latency across the mesh. Latency is generally comparable to Graviton 4’s, which uses Arm’s CMN-700 mesh interconnect.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30413"><img decoding="async" width="688" height="346" data-attachment-id="30413" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_c2c/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?fit=2945%2C1481&amp;ssl=1" data-orig-size="2945,1481" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_c2c" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?fit=2560%2C1287&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?fit=688%2C346&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=688%2C346&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?w=2945&amp;ssl=1 2945w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=768%2C386&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=1536%2C772&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=2048%2C1030&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=1200%2C603&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=1600%2C805&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=1320%2C664&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Core to core latency test on Grace’s CPU</figcaption></figure></div>
<p>DRAM access is handled by a 480-bit LPDDR5X-6400 setup, which provides 480 GB of capacity and 384 GB/s of theoretical bandwidth. Graviton 4 opts for a 768-bit DDR5-5200 setup for 500 GB/s of theoretical bandwidth and 768 GB of capacity. Nvidia may be betting on LPDDR5X providing lower power consumption, as DRAM power can count for a significant part of a server’s power budget.</p>
<p>GH200’s H100 GPU sits next to the Grace CPU. Even though both are sold as a single unit, it’s not an integrated GPU setup because the two chips have separate memory pools. Opting against an integrated GPU is a sensible decision because CPUs and GPUs have different memory subsystem requirements. CPUs are sensitive to memory latency and want a lot of DRAM capacity. GPUs require high memory bandwidth, but are less latency sensitive. A memory setup that excels in all of those areas will be very costly. GH200 avoids trying to square the circle, and its H100 GPU comes with 96 GB of dedicated HBM3 memory. That’s good for 4 TB/s of theoretical bandwidth, far more than what the LPDDR5X setup can provide.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30418"><img loading="lazy" decoding="async" width="688" height="386" data-attachment-id="30418" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/kbl_vega_m_hotchips/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/kbl_vega_m_hotchips.png?fit=954%2C535&amp;ssl=1" data-orig-size="954,535" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="kbl_vega_m_hotchips" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/kbl_vega_m_hotchips.png?fit=954%2C535&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/kbl_vega_m_hotchips.png?fit=688%2C386&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/kbl_vega_m_hotchips.png?resize=688%2C386&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/kbl_vega_m_hotchips.png?w=954&amp;ssl=1 954w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/kbl_vega_m_hotchips.png?resize=768%2C431&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>From Intel’s Hot Chips presentation</figcaption></figure></div>
<p>Conceptually, GH200’s design is similar to Intel’s Kaby Lake CPU with AMD’s Radeon RX Vega M graphics. That design also packages the CPU and GPU together as one unit. A 4 GB pool of HBM2 memory gives the GPU high memory bandwidth, while regular DDR4 memory gives the CPU high memory capacity and low latency. GH200 of course does this on a much larger scale on both the CPU and GPU side.</p>

<p>But an integrated GPU design has benefits too, mainly allowing for faster communication between the CPU and GPU. Games don’t require much bandwidth between the CPU and GPU, as long as there’s enough VRAM to handle the game in question. But compute applications are different, and can involve frequent data exchange between the CPU and GPU. Therefore, Nvidia connects the two dies with a high bandwidth proprietary interconnect called NVLink C2C. NVLink C2C offers 900 GB/s of cross-die bandwidth, or 450 GB/s in each direction. That’s an order of magnitude faster than a PCIe Gen 5 x16 link. </p>
<p>Besides higher bandwidth, NVLink C2C has hardware coherency support. The CPU can access HBM3 memory without explicitly copying it to LPDDR5X first, and the underlying hardware can ensure correct memory ordering without special barriers. Nvidia is confident enough in their NVLink C2C implementation that HBM3 memory is directly exposed to the CPU side as a NUMA node.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30500"><img loading="lazy" decoding="async" width="688" height="505" data-attachment-id="30500" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_numa_bw-2/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_bw-1.png?fit=789%2C579&amp;ssl=1" data-orig-size="789,579" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_numa_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_bw-1.png?fit=789%2C579&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_bw-1.png?fit=688%2C505&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_bw-1.png?resize=688%2C505&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_bw-1.png?w=789&amp;ssl=1 789w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_bw-1.png?resize=768%2C564&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Note that remote bandwidth for Grace Hopper is for CPU-owned memory allocated through standard Linux interfaces, which uses the CPU cores for all data transfer and does not make use of CUDA Copy Engines or other acceleration</figcaption></figure></div>
<p>Accessing the HBM3 memory pool across NVLink C2C provides comparable bandwidth to AMD’s current generation Zen 4 servers in a dual socket configuration. It’s a good performance, if a bit short compared to the theoretical figures. Bandwidth is still significantly higher than what AWS can achieve between two Graviton 4 chips, and shows the value of Nvidia’s proprietary interconnect. Bandwidth from Grace’s local LPDDR5X pool is also solid, and on par with AMD’s Bergamo with DDR5-4800.</p>
<p>Latency however is poor at nearly 800 ns, even when using 2 MB pages to minimize address translation penalties. That’s a difference of 592 ns compared to accessing directly attached LPDDR5X, which itself doesn’t offer particularly good latency.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30429"><img loading="lazy" decoding="async" width="688" height="520" data-attachment-id="30429" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_numa_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?fit=862%2C652&amp;ssl=1" data-orig-size="862,652" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_numa_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?fit=862%2C652&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?fit=688%2C520&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?resize=688%2C520&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?w=862&amp;ssl=1 862w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?resize=768%2C581&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?resize=200%2C150&amp;ssl=1 200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Part of this is undoubtedly because HBM isn’t a technology designed to offer good latency characteristics. But testing from the H100 GPU shows about 300 ns of DRAM latency, suggesting HBM3 latency is only a minor factor. NVLink C2C therefore appears to have much higher latency than AMD’s Infinity Fabric, or whatever Graviton 4 is using. Intel’s QPI also offers better latency.</p>
<p>To make things worse, the system became unresponsive during that latency test run. The first signs of trouble appeared when vi, a simple text editor, took more than several seconds to load. Even weak systems like a Cortex A73 SBC usually load vi instantly. Then, the system stopped responding to all keystrokes over SSH. When I tried to establish another SSH session, it probably got past the TCP handshake stage because it didn’t time out. But the shell never loaded, and the system remained unusable. I eventually managed to recover it by initiating a reboot through the cloud provider, but that sort of behavior is non-ideal.</p>
<p>Since GH200 is a discrete GPU, it’s insightful to compare link latency against other discrete GPU setups. Here, I’m using Nemes’s Vulkan uplink latency test, which uses <code>vkMapMemory</code> to map a portion of GPU VRAM into the test program’s address space. Latency is then measured using pointer chasing accesses, just like above.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30434"><img loading="lazy" decoding="async" width="688" height="328" data-attachment-id="30434" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_vs_pcie_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?fit=802%2C382&amp;ssl=1" data-orig-size="802,382" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_vs_pcie_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?fit=802%2C382&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?fit=688%2C328&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?resize=688%2C328&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?w=802&amp;ssl=1 802w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?resize=768%2C366&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?resize=800%2C382&amp;ssl=1 800w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>This comparison is more reasonable, and NVLink C2C offers better latency than some discrete GPU configurations. It lands right between setups with AMD’s RX 5700 XT and HD 7950. Latency with that in mind is quite reasonable. However, CPU code will need to be careful about treating HBM3 memory simply as another NUMA node because of its high latency.</p>
<h2>Grace’s Neoverse V2 Implementation</h2>
<p>A CPU core architecture’s performance can vary depending on implementation. Zen 4 for example behaves very differently depending on whether it’s in a server, desktop, or mobile CPU. Neoverse V2’s situation is no different, and can vary even more because Arm wants to give implementers as much flexibility as possible.</p>
<figure><table><tbody><tr><td></td><td>Nvidia Grace</td><td>Amazon Graviton 4</td><td>Arm Neoverse V2 Emulation Environment</td></tr><tr><td>Clock Speed</td><td>3.44 GHz<br>3.1 GHz base<br>3.0 GHz all-core SIMD</td><td>2.7-2.8 GHz</td><td>3 GHz</td></tr><tr><td>Core Count</td><td>72</td><td>96</td><td>32</td></tr><tr><td>L2 Cache Capacity</td><td>1 MB</td><td>2 MB</td><td>2 MB</td></tr><tr><td>Interconnect</td><td>Nvidia SCF</td><td>Arm CMN-700</td><td>ARM CMN-700</td></tr><tr><td>L3 Cache</td><td>114 MB</td><td>36 MB</td><td>32 MB</td></tr><tr><td>Main Memory</td><td>480 GB LPDDR5X-6400, 480-bit bus</td><td>768 GB DDR5-5200, 768-bit bus</td><td>DDR5-5600, 128-bit bus</td></tr></tbody></table></figure>
<p>Grace targets parallel compute applications. To that end, Nvidia opted for a large shared L3 cache and higher clock speeds. Less parallel parts of a workload can benefit from a flexible boost policy, giving individual threads more performance when power and thermal conditions allow. A large L3 might handle better when threads from the same process share data.</p>
<p>Graviton 4 on the other hand has to server a lot of customers while maintaining consistent performance. Neoverse V2 cores on Graviton 4 get a larger L2, helping reduce noisy neighbor effects. Low clock speeds minimize workload-dependent thermal or power throttling, Finally, a higher core count lets Amazon fit more of their smallest instances on a single server. A latency test shows the memory hierarchy differences well.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30439"><img loading="lazy" decoding="async" width="688" height="343" data-attachment-id="30439" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_latency_cycles/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_cycles.png?fit=1054%2C525&amp;ssl=1" data-orig-size="1054,525" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_latency_cycles" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_cycles.png?fit=1054%2C525&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_cycles.png?fit=688%2C343&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_cycles.png?resize=688%2C343&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_cycles.png?w=1054&amp;ssl=1 1054w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_cycles.png?resize=768%2C383&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Latency in cycles is identical up to L2, because that’s part of the Neoverse V2 core design. Differences start to appear at L3, and do so in dramatic fashion. Large mesh interconnects tend to suffer high latency, and high capacity caches tend to come with a latency cost too. L3 load-to-use latency is north of 125 cycles on Grace. With such a high L2 miss cost, I would have preferred to see 2 MB of L2 cache. Graviton 4 and Intel’s Sapphire Rapids both use 2 MB of L2 cache to counter L3 latency. AMD’s Zen 4 does have a 1 MB L2, but has much lower L2 miss costs.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30442"><img loading="lazy" decoding="async" width="688" height="343" data-attachment-id="30442" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_latency_ns/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_ns.png?fit=1054%2C525&amp;ssl=1" data-orig-size="1054,525" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_latency_ns" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_ns.png?fit=1054%2C525&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_ns.png?fit=688%2C343&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_ns.png?resize=688%2C343&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_ns.png?w=1054&amp;ssl=1 1054w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_ns.png?resize=768%2C383&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Higher clock speeds do hand Grace an advantage over Graviton 4 when accesses hit L1 or L2. But L3 latency is still sky-high at over 38 ns. Even <a href="https://chipsandcheese.com/2023/03/12/a-peek-at-sapphire-rapids/">Intel’s Sapphire Rapids</a>, which also accesses a giant L3 over a giant mesh, does slightly better with 33 ns of L3 latency.</p>
<p>L3 cache misses head to Grace’s LPDDR5X controllers. Latency at that point is over 200 ns. Graviton 4’s DDR5 is better at 114.08 ns, putting it in the same ballpark as other server CPUs.</p>
<h3>Bandwidth</h3>
<p>Higher clocks mean higher bandwidth, so a Neoverse V2 core in Grace is comfortably ahead of its counterpart in Graviton 4. Cache bandwidth isn’t quite as high as AMD’s, which can be a disadvantage because Nvidia positions Grace as a CPU for highly parallel workloads. Such workloads are likely to be vectorized, and Zen 4 is very well optimized for those cases. Even when both are pulling data from large L3 caches, a Zen 4 core has more bandwidth on tap.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30446"><img loading="lazy" decoding="async" width="688" height="296" data-attachment-id="30446" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_st_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?fit=1258%2C541&amp;ssl=1" data-orig-size="1258,541" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_st_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?fit=1258%2C541&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?fit=688%2C296&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?resize=688%2C296&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?w=1258&amp;ssl=1 1258w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?resize=768%2C330&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?resize=1200%2C516&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>To Nvidia’s credit, a single Grace core can pull more bandwidth from L3 than a Graviton 4 core can. This test uses a prefetcher-friendly linear access pattern. I suspect Grace has a very aggressive prefetcher willing to queue up a ton of outstanding requests from a single core. Single core bandwidth is usually <a href="https://en.wikipedia.org/wiki/Little%27s_law">latency limited</a>, and a L2 prefetcher can create more in-flight requests even when the core’s out-of-order execution engine reaches its reordering limits. But even the prefetcher can only go so far, and cannot cope with LPDDR5X latency. DRAM bandwidth from a single core is only 21 GB/s compared to Graviton 4’s 28 GB/s.</p>
<p>When all cores are loaded, Grace can achieve a cool 10.7 TB/s of L1 bandwidth. L2 bandwidth is around 5 TB/s. Both figures are lower than Graviton 4’s, which makes up for lower clock speeds by having more cores. AMD’s Genoa-X has the best of both worlds, with high per-cycle cache bandwidth, higher clock speeds, and 96 cores.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30449"><img loading="lazy" decoding="async" width="688" height="312" data-attachment-id="30449" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_mt_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?fit=1346%2C611&amp;ssl=1" data-orig-size="1346,611" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_mt_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?fit=1346%2C611&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?fit=688%2C312&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?resize=688%2C312&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?w=1346&amp;ssl=1 1346w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?resize=768%2C349&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?resize=1200%2C545&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?resize=1320%2C599&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>L3 bandwidth is hard to see from this test because Grace and Graviton 4 have a lot of L2 capacity compared to L3. I usually split the test array across threads because testing with a shared array tends to overestimate DRAM bandwidth. Requests from different cores to the same cacheline may get combined at some level. But testing with a shared array does help to estimate Graviton 4 and Grace’s L3 bandwidth.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30451"><img loading="lazy" decoding="async" width="688" height="312" data-attachment-id="30451" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_shared_arr_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?fit=1346%2C611&amp;ssl=1" data-orig-size="1346,611" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_shared_arr_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?fit=1346%2C611&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?fit=688%2C312&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?resize=688%2C312&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?w=1346&amp;ssl=1 1346w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?resize=768%2C349&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?resize=1200%2C545&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?resize=1320%2C599&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Grace has over 2 TB/s of L3 bandwidth, putting it ahead of Graviton 4’s 750 GB/s. Nvidia wants Grace to serve bandwidth hungry parallel compute applications, and having that much L3 bandwidth on tap is a good thing. But AMD is still ahead. Genoa-X dodges the problem of servicing all cores from a unified cache. Instead, each octa-core cluster gets its own L3 instance. That keeps data closer to the cores, giving better L3 bandwidth scaling and lower latency. The downside is Genoa-X has more than 1 GB of last level cache, and a single core only allocates into 96 MB of it.</p>
<h3>Some Light Benchmarking</h3>
<p>In-depth benchmarking is best left to mainstream tech news sites with deeper budgets and full time employees. But I did dig briefly into Grace’s performance.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30455"><img loading="lazy" decoding="async" width="688" height="370" data-attachment-id="30455" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_libx264/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264.png?fit=704%2C379&amp;ssl=1" data-orig-size="704,379" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_libx264" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264.png?fit=704%2C379&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264.png?fit=688%2C370&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264.png?resize=688%2C370&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>libx264 uses plenty of vector instructions, and can demand a lot of bandwidth. It’s the kind of thing I’d expect Grace to do well at, especially with the test locked to matching core counts. But despite clocking higher, Grace’s Neoverse V2 cores fail to beat Graviton 4’s.</p>
<p>7-Zip is a file compression program that only uses scalar integer instructions. The situation is no better there, and I ran the test several times despite the clock running on cloud instance time.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30458"><img loading="lazy" decoding="async" width="688" height="377" data-attachment-id="30458" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_7z/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z.png?fit=702%2C385&amp;ssl=1" data-orig-size="702,385" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_7z" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z.png?fit=702%2C385&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z.png?fit=688%2C377&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z.png?resize=688%2C377&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>Despite using the same command line parameters, 7-Zip wound up executing 2.58 trillion instructions to finish compressing the test file on GH200. On Graviton 4, the same work took a mere 1.86 trillion instructions. libx264’s instruction counts were similar on both Neoverse V2 implementations, at approximately 19.8 trillion instructions. That makes the 7-Zip situation a bit suspect, so I’ll focus on libx264.</p>
<blockquote>
<p>…counts cycles in which the core is unable to dispatch instructions from the front end to the back end due to a back end stall caused by a miss in the last level of cache within the core clock domain</p>
<p>Arm Neoverse V2 Technical Reference Manual</p>
</blockquote>
<p>Neoverse V2 has a STALL_BACKEND_MEM performance monitoring event. The description for this event is clear if a bit wordy. Let’s unpack it. L2 is the last level of cache that runs at core clock. Therefore, STALL_BACKEND_MEM only considers stalls caused by L3 and DRAM latency. Dispatching instructions from the frontend to the backend is what the renamer does, and we know the renamer is the narrowest part of Neoverse V2’s pipeline. Therefore, the event is counting L2 miss latency that the out-of-order engine couldn’t absorb. And, throughput lost from those events can’t be recovered by racing ahead later elsewhere in the pipeline.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30463"><img loading="lazy" decoding="async" width="686" height="370" data-attachment-id="30463" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_libx264_be_bound/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264_be_bound.png?fit=686%2C370&amp;ssl=1" data-orig-size="686,370" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_libx264_be_bound" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264_be_bound.png?fit=686%2C370&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264_be_bound.png?fit=686%2C370&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264_be_bound.png?resize=686%2C370&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>libx264 sees a massive increase in those stalls. Grace’s smaller L2 combined with worse L3 and DRAM latency isn’t a winning combination. Overall lost throughput measured at the rename stage only increased by a few percent. It’s a good demonstration of how Neoverse V2’s large backend can cope with extra latency. But it can’t cope hard enough, nullifying Grace’s clock speed advantage.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30466"><img loading="lazy" decoding="async" width="684" height="370" data-attachment-id="30466" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_7z_be_bound/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z_be_bound.png?fit=684%2C370&amp;ssl=1" data-orig-size="684,370" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_7z_be_bound" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z_be_bound.png?fit=684%2C370&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z_be_bound.png?fit=684%2C370&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z_be_bound.png?resize=684%2C370&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>The same counters in 7-Zip don’t show such a huge discrepancy, though Grace again suffers more from L2 miss latency. Grace’s poor showing in this workload is largely due to 7-Zip somehow executing more instructions to do the same work.</p>
<p>7-Zip and libx264 don’t benefit from Nvidia’s implementation choices, but that doesn’t mean Grace’s design is without merit. The large 114 MB L3 cache looks great for cache blocking techniques, and higher clocks can help speed up less parallel parts of a program. Some throughput bound programs may have prefetcher-friendly sections, which can be aided by Grace’s prefetcher. Specific workloads may do better on Grace than on Graviton 4, particularly if they receive optimizations to fit Grace’s memory subsystem. But that’s beyond the scope of this brief article.</p>
<h2>H100 On-Package GPU</h2>
<p>The GPU on GH200 is similar to the H100 SXM variant, since 132 Streaming Multiprocessors (SMs) are enabled out of 144 on the die. VRAM capacity is 96 GB compared to the 80 GB on separately sold H100 cards, indicating that all 12 HBM controllers are enabled. Each HBM controller has a 512-bit interface, so the GH200’s GPU has a 6144-bit memory bus. Even though GH200’s GPU is connected using a higher bandwidth NVLink C2C interface, it’s exposed to software as a regular PCIe device. </p>
<p><code>nvidia-smi</code> indicates GH200 has a 900W power limit. For comparison, H100’s SXM variant has a 700W power limit, while the H100 PCIe makes do with 350-400W. GH200 obviously has to share power between the CPU and GPU, but the GPU may have more room to breathe than its discrete counterparts when CPU load is low.</p>
<p>Compared to the PCIe version of the H100, GH200’s H100 runs at higher clocks, reducing cache latency. Otherwise, the H100 here looks a lot like other H100 variants. There’s large L1 cache backed by a medium capacity L2. H100 doesn’t have a gigantic last level cache like RDNA 2, CDNA 3, or Nvidia’s own Ada Lovelace client architecture. But it’s not a tiny cache either like on Ampere or other older GPUs.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30468"><img loading="lazy" decoding="async" width="688" height="321" data-attachment-id="30468" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_vk_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_latency.png?fit=1201%2C561&amp;ssl=1" data-orig-size="1201,561" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_vk_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_latency.png?fit=1201%2C561&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_latency.png?fit=688%2C321&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_latency.png?resize=688%2C321&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_latency.png?w=1201&amp;ssl=1 1201w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_latency.png?resize=768%2C359&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>VRAM latency sees a very substantial improvement, going down from 330 ns to under 300. It’s impossible to tell how much of this comes from higher clock speeds reducing time taken to traverse H100’s on-chip network, and how much comes from HBM3 offering better latency.</p>
<p>Bandwidth also goes up, thanks to more enabled SMs and higher clock speeds. Unfortunately, the test couldn’t get past 384 MB. That makes VRAM bandwidth difficult to determine. If things worked though, I assume GH200 would have higher GPU memory bandwidth than discrete H100 cards.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30472"><img loading="lazy" decoding="async" width="688" height="320" data-attachment-id="30472" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_vk_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_bw.png?fit=1196%2C556&amp;ssl=1" data-orig-size="1196,556" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_vk_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_bw.png?fit=1196%2C556&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_bw.png?fit=688%2C320&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_bw.png?resize=688%2C320&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_bw.png?w=1196&amp;ssl=1 1196w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_bw.png?resize=768%2C357&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Further tests would have been interesting. I wanted to test CPU to GPU bandwidth using the GPU’s copy engine. DMA engines can queue up memory accesses independently of CPU (or GPU) cores, and are generally more latency tolerant. Nemes does have a test that uses <code>vkCmdCopyBuffer</code> to test exactly that. Unfortunately, that test hung and never completed.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30489"><img loading="lazy" decoding="async" width="688" height="138" data-attachment-id="30489" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_pcie_errors/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?fit=1358%2C272&amp;ssl=1" data-orig-size="1358,272" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_pcie_errors" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?fit=1358%2C272&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?fit=688%2C138&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?resize=688%2C138&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?w=1358&amp;ssl=1 1358w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?resize=768%2C154&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?resize=1200%2C240&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?resize=1320%2C264&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Checking <code>dmesg</code> showed the kernel complaining about PCIe errors and graphics exceptions. I tried looking up some of those messages in Linux source code, but couldn’t find anything. They probably come from a closed source Nvidia kernel module. Overall, I had a frustrating experience exercising NVLink C2C. At least the Vulkan test didn’t hang the system, unlike running a plain memory latency test targeting the HBM3 memory pool. I also couldn’t use any OpenCL tests. <code>clinfo</code> could detect the GPU, but <code>clpeak</code> or any other application was unable to create an OpenCL context. I didn’t have the same frustrating experience with H100 PCIe cloud instances, where the GPU pretty much behaved as expected with Vulkan or OpenCL code. It’s a good reminder that designing and validating a custom platform like GH200 can be an incredibly difficult task.</p>
<h2>Final Words</h2>
<p>Nvidia’s GH200 and Grace CPU is an interesting Neoverse V2 implementation. With fewer cores and a higher power budget, Grace can clock higher than Graviton 4. But rather than providing better per-core performance as specifications might suggest, Grace is likely optimized for specific applications. General consumer workloads may not be the best fit, even well vectorized ones.</p>
<p>Previously I thought Arm’s Neoverse V2 had an advantage over Zen 4, because Arm can focus on a narrower range of power and performance targets. But after looking at Grace, I don’t think that captures the full picture. Rather, Arm faces a different set of challenges thanks to their business model. They don’t see chip designs through to completion like AMD and Intel. Those x86 vendors can design cores with a comparatively narrow set of platform characteristics in mind. Arm has to attract as many implementers as possible to get licensing revenue. Their engineers will have a harder time anticipating what the final platform looks like. </p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30486"><img loading="lazy" decoding="async" width="688" height="383" data-attachment-id="30486" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/arm_v2_hotchips_reference_platform/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?fit=1276%2C711&amp;ssl=1" data-orig-size="1276,711" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="arm_v2_hotchips_reference_platform" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?fit=1276%2C711&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?fit=688%2C383&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?resize=688%2C383&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?w=1276&amp;ssl=1 1276w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?resize=768%2C428&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?resize=1200%2C669&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Arm evaluated Neoverse V2 in an emulation environment that drastically differs from Grace and Graviton 4. Slide from Arm’s Hot Chips 2023 presentation</figcaption></figure></div>
<p>So, Neoverse V2 can find itself having to perform in an environment that doesn’t play nice with its core architecture. Nvidia’s selection of a 1 MB L2, high latency L3, and very high latency LPDDR5X present Neoverse V2 with a spicy challenge. As covered in the <a href="https://chipsandcheese.com/2024/07/22/arms-neoverse-v2-in-awss-graviton-4/">Graviton 4</a> article, Neoverse V2 has similar reordering capacity to Zen 4. I think Zen 4 would also trip over itself with 125 cycles of L3 latency and over 200 ns of memory latency. I don’t think it’s a coincidence that every Zen 4 implementation has a fast L3. Intel is another example, Golden Cove can see 11.8 ns of L3 latency in a Core i7-12700K, or 33.3 ns in a Xeon Platinum 8480+. Golden Cove has much higher reordering capacity, making it more latency tolerant. In a server environment, Golden Cove gets a 2 MB L2 cache as well.</p>
<p>GH200’s GPU implementation deserves comment too. It should be the most powerful H100 variant on the market, with a fully enabled memory bus and higher power limits. NVLink C2C should provide higher bandwidth CPU to GPU communication than conventional PCIe setups too.</p>
<p>But it’s not perfect. NVLink C2C’s theoretical 450 GB/s is difficult to utilize because of high latency. Link errors and system hangs are a concerning problem, and point to the difficulty of validating a custom interconnect. Exposing VRAM to software as a simple NUMA node is a good north star goal, because it makes VRAM access very easy and transparent from a software point of view. But with current technology, it might be a bridge too far.</p>

<p>Even though it’s not an iGPU, Grace Hopper might be Nvidia’s strongest shot at competing with AMD’s iGPU prowess. Nvidia has already scored a win with <a href="https://nvidianews.nvidia.com/news/aws-nvidia-strategic-collaboration-for-generative-ai">Amazon</a> and the <a href="https://www.bristol.ac.uk/news/2023/november/supercomputer-announcement.html">UK’s Isambard-AI supercomputer</a>. AMD’s MI300A is shaping up to be tough competition, with a win in Lawrence Livermore National Laboratory’s upcoming <a href="https://asc.llnl.gov/exascale/el-capitan">El Capitan</a> supercomputer. MI300A uses an integrated GPU setup, which speeds up CPU to GPU communication. However, it limits memory capacity to 128 GB, a compromise that Nvidia’s discrete GPU setup doesn’t need to make. It’s good to see Nvidia and AMD competing so fiercely in the CPU/GPU integration space, and it should be exciting to see how things play out.</p>
<p>If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;<a href="https://www.patreon.com/ChipsandCheese">Patreon</a>&nbsp;or our&nbsp;<a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ">PayPal</a>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;<a href="https://discord.gg/TwVnRhxgY2">Discord</a>.</p>

<div data-post_id="10949" data-instance_id="1" data-additional_class="pp-multiple-authors-layout-boxed.multiple-authors-target-the-content" data-original_class="pp-multiple-authors-boxes-wrapper pp-multiple-authors-wrapper box-post-id-10949 box-instance-id-1">
<p><span>
<ul>
<li>
<div>
<p><img alt="clamchowder" src="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=80&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=160&amp;d=identicon&amp;r=g 2x" height="80" width="80"> </p>
</div>

</li>
</ul>
</span>
</p></div>





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DARPA wants to bypass the thermal middleman in nuclear power systems (177 pts)]]></title>
            <link>https://www.ans.org/news/article-6276/darpa-wants-to-bypass-the-thermal-middleman-in-nuclear-power-systems/</link>
            <guid>41205439</guid>
            <pubDate>Fri, 09 Aug 2024 21:12:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ans.org/news/article-6276/darpa-wants-to-bypass-the-thermal-middleman-in-nuclear-power-systems/">https://www.ans.org/news/article-6276/darpa-wants-to-bypass-the-thermal-middleman-in-nuclear-power-systems/</a>, See on <a href="https://news.ycombinator.com/item?id=41205439">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="expand_6276"><p>DARPA is interested in ideas to take alpha, beta, gamma, and neutron radiation from “any type of reactor,” including fission and fusion, and from nuclear processes and radioisotope decay, and directly convert those radioactive emissions to electricity to meet the specific power and lifetime requirements of a range of nuclear power systems. Respondents have until Aug. 30 to share their ideas.</p><p><strong>There’s got to be a better way:</strong> “Methods to convert the energy of nuclear fission reactions and the decay of radioisotopes into electricity have not evolved since the invention of radioisotope power systems and fission reactors over 70 years ago and remain unoptimized,” the RFI says. They rely on thermal heat transfer, and “in each step of this indirect conversion method neutrons, heat, and energy are lost to the shielding material, working fluid, and other system materials.”</p><p>Advanced reactor designs that use alternative coolants, including helium, sodium, and salts, would still use what DARPA calls “heritage nuclear power conversion technology” with water and steam as the working fluids, as would the fusion power plants being planned today.</p><p><strong>Why now? </strong>Tabitha Dodson, the program manager for DARPA DSO, which is launching the RFI, told <em>Nuclear News</em> that “two big things” are driving the interest.</p><p>“One is the extreme surge of investment in small and advanced nuclear technologies, such as in fusion and space reactors, which do not have a concurrent pairing of advanced power generation methods that doesn’t involve liquid-based heat transfer,” she said. “Next, there has been an order of magnitude improvement in radiation tolerance and efficiency for voltaics in recent years with encouraging performance that indicates radiovoltaics could scale up as an array usable in nuclear reactors.”</p><p><strong>Exploring radiovoltaics:</strong> The RFI points to research in direct energy conversion based on radiation, or radiovoltaics, which includes semiconductor-based neutron, gamma, beta, and alpha voltaics. In radiovoltaics, radiation indirectly excites electron-hole pairs in a semiconductor lattice—a process that resembles but is distinct from how photons accomplish energy conversion in photovoltaics.</p><p>Radiovoltaics has been used to generate electricity from small amounts of decaying radioisotopes to produce commercially available microelectronics and small batteries at the nanowatt to milliwatt levels. “At least two challenges, however, [prevent] these from being viable solutions today for simultaneous high-power, long-duration applications,” according to the RFI. Those challenges are efficiency—"in most cases the efficiencies of radiovoltaics are 1–3 percent per radiation emission”—and the lifespan of candidate semiconductor materials, which is limited by their ability to withstand excess radiation energy over time and maintain performance.</p><p>That’s where the order of magnitude improvement in performance that Dodson noted comes in. Recent years have seen “the discovery of radiation-resilient materials that are still able to carry electrons into the conduction band to drive current despite the incursion of radiation-driven defects,” she said. Now, DARPA is interested in the possibility of applying direct energy conversion beyond the milliwatt power levels that have been demonstrated, and in solutions optimized for neutron and gamma radiation from a fission or fusion reactor.</p><p><strong>Scaled to fit:</strong> If durable, efficient radiovoltaics can be developed, Dodson sees the potential for them to fit a range of electricity needs. “If we could make an array of cells that could be scaled to any broad area, this power generation method could be paired with any sized nuclear reactor or even nuclear radioisotope power system. It could even be scaled up to a commercial-sized power plant or down to tiny-sized microelectronics. What’s appealing about voltaics versus thermoelectrics or working-fluid methods are how thin, light, pliable, and scalable they could be to any size or form factor,” Dodson said.</p><p>Dodson’s work at DARPA has included <a href="https://www.ans.org/news/article-4842/leading-draco-to-launch-an-interview-with-darpas-tabitha-dodson/">a key role</a> in establishing and then serving as program manager of the nuclear thermal propulsion rocket program DRACO—the Demonstration Rocket for Agile Cislunar Operations. Space missions and deployments stand to gain from a scalable high power direct energy conversion technology.</p><p>“The issues that come with outdated power generation equipment on the ground are magnified for space nuclear technologies since space reactors are size constrained and have to carry all of that equipment with them,” according to Dodson.</p><p>Here on Earth, progress in neutron- and gamma-based radiovoltaics could lead to greater availability of small neutron and gamma detectors for reactor instrumentation and control, she noted. The RFI suggests that, with systems scaled for large-scale electricity generation and with improved material lifetimes and energy conversion efficiency, “one could create energy-generating ‘smart shields’ for nuclear systems that simultaneously cut down on nuclear waste.” In such shields, “neutron radiation from a fusion or fission reactor could transmute and decay radiovoltaic lattice materials doped or layered with isotopes that are tuned to absorb the reactor’s neutrons, thereafter generating secondary emission alpha or beta particles to further energize radiovoltaics.”</p><p><strong>What is the ask?</strong> The RFI asks: “Is it possible to achieve [a] direct energy conversion nuclear power system, ranging in power from 10s of watts electric (We) to 100s of kWe?” DARPA wants information “on the potential to improve specific power greater than 1 We/kg conversion from watts-thermal per radiation emission product,” and information on the potential to improve damage tolerance of the voltaic to nuclear radiation to reach an operating lifetime comparable to the life of its nuclear source, on the scale of decades.</p><p>“We will learn what our boundary conditions are when respondents tell us what technologies in the field of voltaics are possible, and we’ll use that to see if there is sufficient scientific rationale make a case to present for further DARPA investment,” Dodson said. “I also hope people are going to start thinking about nuclear systems that use electromagnetic versus thermal-kinetic methods to harvest nuclear energetic reactions.”</p><p>Responses can be submitted until 4 p.m. (EDT) on August 30. Questions can be directed to Dodson at <a href="mailto:DARPA-SS-24-01@darpa.mil">DARPA-SS-24-01@darpa.mil</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Urchin Software Corp: The unlikely origin story of Google Analytics (2016) (220 pts)]]></title>
            <link>https://urchin.biz/urchin-software-corp-89a1f5292999</link>
            <guid>41205176</guid>
            <pubDate>Fri, 09 Aug 2024 20:32:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://urchin.biz/urchin-software-corp-89a1f5292999">https://urchin.biz/urchin-software-corp-89a1f5292999</a>, See on <a href="https://news.ycombinator.com/item?id=41205176">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><div><div><div><h2 id="ddd7">The unlikely origin story of Google Analytics, 1996–2005-ish</h2><div><a href="https://medium.com/@impunity?source=post_page-----89a1f5292999--------------------------------" rel="noopener follow"><div aria-hidden="false"><p><img alt="Scott Crosby" src="https://miro.medium.com/v2/resize:fill:88:88/1*3uQ_Wu_tc6tzDRqN6pYeAA.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a><a href="https://urchin.biz/?source=post_page-----89a1f5292999--------------------------------" rel="noopener  ugc nofollow"><div aria-hidden="false"><p><img alt="Urchin Software Corp. Vault" src="https://miro.medium.com/v2/da:true/resize:fill:48:48/1*oH65EY-qH6myOMQxSt3Teg.gif" width="24" height="24" loading="lazy" data-testid="publicationPhoto"></p></div></a></div></div><figure><figcaption>The first Urchin logo (Jason Collins)</figcaption></figure><p id="48ce"><em>TL, DR: Urchin Software Corporation was a web analytics company based in San Diego, CA. The founders of the company were Paul Muret, Jack Ancone, Brett Crosby, and Scott Crosby (yours truly). In April 2005 the company was acquired by Google, and the Urchin product became “Urchin from Google,” then later simply Google Analytics. As the 10th anniversary of the acquisition has recently passed, I thought it was a good time to get the history of the company down for posterity. I don’t expect this to be a riveting read for those not directly involved; it’s more of my attempt to close the book on that era.</em></p><p id="4f3b"><em>And maybe, if nothing else, I guess it suggests that despite the soup du jour — huge seed/A rounds, massive valuations, binary outcomes— you can sometimes do alright by just taking less money and more time.</em></p><figure><figcaption>Our first day at Google, April 21, 2005. Brett was on his honeymoon!</figcaption></figure><p id="70ef"><span>T</span>he predecessor to Urchin Software Corp. was originally started by Paul Muret and Scott Crosby (me) in late 1995. Prior to then, Paul had been working in the Space Physics dept. at UCSD, where he was exposed to HTML 1.0 after being tasked with putting the department’s syllabus online. Paul and I were post-college roommates at the time, living in the Bay Park neighborhood of San Diego. One night Paul came home from work and announced that he saw a business opportunity in building websites for businesses. To prove the point, he showed me his bright-blue-text-on-grey-background site for UCSD. Maybe some of the text even &lt;BLINK&gt;ed. I agreed and we started work on a business plan. This was presented to my proverbial rich uncle (Chuck Scott), who agreed to invest $10,000 in the “company” and provide a desk in a corner of his office at <a href="http://www.cbsscientific.com/" rel="noopener ugc nofollow" target="_blank">C.B.S. Scientific</a>. It would be quite awhile until he saw a return on that money.</p><figure><figcaption>Our first webserver, running at 50 mhz, was ~$3200 in 1995 money. That was about 1/3 of our total raised capital to-date.</figcaption></figure><p id="e296">Armed with Chuck’s cash, the new company bought a Sun SPARC 20 for webserving duty and procured a then-very-expensive ISDN line. Ever heard of “<a href="https://en.wikipedia.org/wiki/10BASE2" rel="noopener ugc nofollow" target="_blank">10base2</a>”? That’s how our office computers were networked — coaxial cable with fun twist-lock fittings, kinda like TV cable. Antiquated. Anyway, Paul and I then set about acquiring customers, which we slowly accomplished. Most were small businesses paying a modest monthly fee, like Cinemagic, a vintage movie-poster company run by a couple named Herb and Roberta. Or ReVest, a financial startup whose owner didn’t “do” email, so all edits to his site were communicated via thermal-transfer fax, which spooled out onto our floor for 6 or 8 pages every morning. Another was an obscure division of Pioneer Electronics that specialized in the even-then old-timey format known as <a href="https://en.wikipedia.org/wiki/LaserDisc" rel="noopener ugc nofollow" target="_blank">LaserDisc</a> [1] I know I know, the quality is better.</p><p id="2127">These “wins” had us feeling optimistic enough to lease some office space in a squat brownish-green building in San Diego’s faux-historic Old Town theme park-y part of town, not far from Rockin’ Baja Lobster. Our office had room for 4 desks, or 5 if you counted the vestibule (for a secretary?) In 1997 Brett Crosby (my younger brother) joined the company, and things started improving. We managed to sign two of the larger local employers, Sharp Healthcare, a hospital system, and Solar Turbines*, the power generation subsidiary of Caterpillar. But we still had a bunch of small customers, most of whom we hosted on our lone webserver, for a recurring fee. To accurately bill for bandwidth consumed (weird right? bandwidth used to be expensive), Paul wrote a simple log analyzer to tally bytes transferred, and gave it a nice web interface. He added referrers, “<a href="http://www.kaushik.net/" rel="noopener ugc nofollow" target="_blank">hits</a>”, pageviews, etc., and voilá, the first version of Urchin was born. After some further development to add date-range features, user authentication, etc., the product was demonstrated to customers, to generally favorable reviews.</p><p id="25bb">[*this deal paid $10k/month and kept us alive for at least a year; thanks Steve!]</p></div><div><figure></figure><figure><figcaption>Our first tradeshow ever, circa 1997. We borrowed these giant blue light boxes from an underwear startup, as I recall. They were 1" thick particle board and extremely heavy. And we had booth babes! Not really. They were friends who thought an internet tradeshow would be fun, so they hung out all day for free. Suffice to say they never volunteered again.</figcaption></figure></div><div><p id="17b0">Brett’s girlfriend at the time (Julie, now his wife) also worked in “the industry”, for <a href="http://www.rpa.com/" rel="noopener ugc nofollow" target="_blank">Rubin Postaer</a> Interactive (“RPI”, a subsidiary of RPA; A=Associates). RPA was and is a prominent advertising &amp; web development shop in LA, and they managed the Honda.com account. Sometime in late 1997, it was learned via our RPA spy (Julie) that Honda.com, which was using WebTrends at the time, was unable to process each day’s Apache access log before the end of the current day, dooming them to fall ever more behind. After some effort, we obtained a few days’ server logs to process as a demo, and the task was completed in ~30 minutes. We became the web analytics solution for American Honda from that point forward, and it became clear a business could be built on this log processing technology.</p><figure><figcaption>One of the earlier Urchin t-shirt designs, before we corporate-ized the logo. Cabel from <a href="http://panic.com/" rel="noopener ugc nofollow" target="_blank">Panic</a> was mad at us for that.</figcaption></figure><p id="d819">Around this time Jack Ancone joined the company as (initially) CFO and moved to San Diego. We moved into our office at 2165 India St. around the same time (Note: The <a href="http://ballastpoint.com/" rel="noopener ugc nofollow" target="_blank">Ballast Point</a> tasting room is now located directly across the street from our old office; suffice to say we would have never achieved anything had they been there in the late 1990s.) The company was then known as Quantified Systems, Inc., and work was divided into web dev, hosting, and software dev. Be divided and conquer yourself, to paraphrase Caesar.</p><figure><figcaption>Classy awnings right? The big Ballast Point tasting room/restaurant is now in the building barely visible to the left. They have since become a <a href="http://insidescoopsf.sfgate.com/blog/2015/11/16/san-diegos-ballast-point-sold/" rel="noopener ugc nofollow" target="_blank">legit unicorn</a>. So much for software being the way to get rich.</figcaption></figure><p id="72ee">In January 1998 we received our very first order for the “Pro” version of Urchin, for $199. [Side note: why does “Pro” always mean “lame” in the software world?] Anyway, shortly after, we made the decision to jettison the non-software parts of the business, and all hosting/webdev customers were rather abruptly “sold” ($0) to another local webdev shop. We were now a software company (high-five!).</p><p id="6d27">As such, we needed to raise some money. Tapping our family “networks” and one boutique VC (Green Thumb Capital, of NYC, who Jack brought on board*), we raised $1m, bringing our total outside capital to about $1.25m. We would never raise any further money (with the exception of ~$400k in debt, which was repaid with interest and warrants). Not for lack of trying… more on that later.</p><p id="7859">[*to their credit, Green Thumb never once hassled us nor apparently had they any thought of recovering their investment; I can only imagine their dumbfounded shock when they found out Google had agreed to acquire us.]</p><p id="2732">As we struggled to figure out how to sell “enterprise” software in the late 1990s, we decided to try an advertising-based approach to grab market share. For some reason, we were always more concerned with popularity than money… go figure. Internet companies of the era were often valued most highly if measured by “eyeballs” and we thought we could get lots of those by giving away the software for free and showing banner ads at the top of each page. So we released Urchin ASAP, the free counterpart to Urchin ISP. Both were designed to be used by hosting operations. We thought we could make some significant fraction of a cent per click on these ads, on top of some infinitesimal CPM… (reminds me of a classic SNL skit… Q: “how do you make money at the Citiwide Change Bank? A: <a href="https://video.yahoo.com/first-citywide-change-bank-2-000000534.html" rel="noopener ugc nofollow" target="_blank">Volume!</a>”) We never made anything on those banner ads, but we did get exposure. And the software was pretty good, for the era. Good enough to make our first real breakthrough.</p><figure><figcaption>One of the Urchin ASAP banner ads, which advertised itself when no one else wanted the space. Meta.</figcaption></figure><p id="94db">Before Tumblr, before Blogger, and contemporary with <a href="https://techcrunch.com/2009/04/23/yahoo-quietly-pulls-the-plug-on-geocities/" rel="noopener ugc nofollow" target="_blank">Geocities</a>, there was something called <a href="http://www.forbes.com/1999/02/01/mu3.html" rel="noopener ugc nofollow" target="_blank">Nettaxi</a>. What-Taxi? Right. But at the time they claimed something like 100,000 “sites”, and we viewed them as a fantastic source of eyeballs for our advertising-supported version of Urchin. They claimed to have no money for such a luxury as web statistics (glad that term got retired), so we “negotiated” a deal: Urchin would be 100% free for them in exchange for the anticipated ad revenue we’d get from all those eyeballs. How much money did we make? 4¢ or so maybe, I don’t recall ever actually getting a check. But that’s beside the point. From then on we claimed 100,000 “sites” were using Urchin, and that got us places.</p><p id="c6d9">Another “innovation” we came up with mirrored what Google does with its logo on special days — we created a dynamic Urchin guy in the upper-left of the interface called “the Urchin of the Day.” This was straight silly, but we thought it would endear us to customers. Maybe it did. It definitely occupied our design guy Jason Collins for the better part of year, as he got so into it he forgot about his more important work. The images he made still make me laugh. So good! We even had our then-friend, the pre-famous <a href="https://obeygiant.com/" rel="noopener ugc nofollow" target="_blank">Shepard Fairey</a>, do one, the “Power to the People” version.</p><figure><figcaption>7'4", 520 lb.</figcaption></figure><p id="81ef">He also did several promo posters and ads for us in exchange for free web hosting. Really nice, humble guy. Of course, now he gets presidents elected and such.</p><figure><figcaption>My favorite Urchin of the Day — Jason Collins (graphics guy) was a car nut and master of animated GIFs.</figcaption></figure><p id="589d">In 1999, Brett Crosby, VP of Sales and Marketing, was casting about trying to get Urchin 2.0 noticed. He had zeroed-in on Earthlink as our dream customer, mostly due to their vast reach and name recognition. Heck, they were almost as big as AOL! Of course, we had no idea how to reach anyone important at a place like that, so Brett did the natural thing and filled out a web form. Again, and again, and again. He must have submitted that thing 20 or 30 times. Finally, he got a response. Rob Maupin, VP of hosting (or something similar) agreed to a meeting. We were stunned. The all-powerful Earthlink would meet with a bunch of idiots like us? We could hardly believe it. So we piled into the fanciest car we had, Brett’s old Mercedes 420 SEL, which he had bought for $4,000, and drove up to Pasadena. I stayed at the office to “manage” and because I was scared of Earthlink.</p><p id="fc7c">Rob didn’t seem very impressed. He immediately dismissed the Urchin 2.0 interface as (and I quote) “too blue blue blue blue blue,” which I had to admit was true.</p><figure><figcaption><a href="https://www.linkedin.com/in/rmaupin" rel="noopener ugc nofollow" target="_blank">Rob Maupin</a>: We’ll try it if you make it less blue.</figcaption></figure><p id="ae67">But he agreed to try it, and the tests went well. Urchin was perhaps not the most full-featured web reporting tool in the world at the time, but it was fast, and that’s mainly what web hosts cared about. It also managed log files and helped sysadmins run things more easily, so the ops guys liked it. After we made some changes as Earthlink requested, we negotiated a pretty lame deal that ensured our eventual success: $4,000/month for unlimited Urchin software across all Earthlink-hosted websites. We were ecstatic.</p><figure><figcaption>Jack, Brett, and Jason Senn (head of Channel and carpentry), looking tough.</figcaption></figure><figure><figcaption>This photo (and the above) were for an article in Fortune in 2000 (“Traffic Aficionados”). Thanks <a href="https://www.linkedin.com/in/suzanne-koudsi-b28349ab" rel="noopener ugc nofollow" target="_blank">Suzi Koudsi</a>!</figcaption></figure><p id="fe28">In 2001, the company was rechristened Urchin Software Corporation. Product and sales were sufficiently far along that we thought it would be a good idea to raise more money. Anyone who’s pitched VCs knows it’s a long, distracting slog, and the business really suffers during the months the leadership is preoccupied. But things went pretty well, and after dozens of meetings and plenty of travel, we finally secured term sheets from two respectable VCs — Ampersand and <a href="https://en.wikipedia.org/wiki/John_Moores_%28baseball%29" rel="noopener ugc nofollow" target="_blank">JMI</a>. The closing was scheduled for late August, but then Labor Day rolled around, and a few more days passed, and finally the capital call to the LPs was supposed to happen on… September 12th (2001). Uhh, no. Suffice to say, the world had bigger things to worry about that day.</p><p id="72e3">By that point, having anticipated a capital infusion of approximately $7 million, we had ramped up hiring and infrastructure spending, as one does. We had leased two additional office spaces in the same building, and built out the interiors. So without the funding, we had little choice but to pare back down. On a Friday not long after, we laid off 12 people and shortly afterward relinquished one of our office spaces. We called it, somewhat unimaginatively, Black Friday. Our financial situation was bleak (like, we couldn’t make payroll in two weeks), and we saw no alternative but to borrow money from our rich uncles — Chuck Scott and <a href="http://jeromes.com/" rel="noopener ugc nofollow" target="_blank">Jerry Navarra</a>. They saved us, and got interest and warrants for their trouble. But Thanksgiving was uncomfortable for a couple years.</p><p id="a18b">2001 and 2002 were very difficult years for Urchin Software Corp. — I remember walking down our hallway, praying to the acoustic ceiling tiles on more than one occasion, “please, just let it die” — but it wouldn’t. Costs were cut way back, and some employees took voluntary pay cuts of up to 60% to help with cashflow (this “back pay” was eventually repatriated, thank goodness).</p><figure><figcaption>We always had stickers, because we were juvenile and liked sticking them on competitors’ booths and airplanes. Surfer Urchin is the rarest of the lot. Shepard Fairey designed a few bits of schwag in exchange for hosting <a href="http://obeygiant.com/" rel="noopener ugc nofollow" target="_blank">obeygiant.com</a>.</figcaption></figure><p id="3e9f">Tech spending was slow in the early 2000s post-bubble, and while things were gradually improving, revenue was uneven and not growing as hoped. Until 2002, our major source of inflow had been large annual licensing deals that were complex and long negotiations. Our biggest deal, at over $1 million, was negotiated by Jack Ancone with Cable &amp; Wireless, a major global telco/host/ISP.</p><figure><figcaption>Urchin 3.0’s interface, cobranded with the now-defunct Worldport, of Dublin. Urchin 3.x still runs on more than a few old servers in dusty corners of the internet.</figcaption></figure><p id="d893">Similar deals were negotiated with Winstar, KeyBridge, and Ireland-based Worldport, all VC-backed hosts with seemingly limitless resources. As it turned out, they were indeed limited. All of them expired before we got paid.</p><p id="227b">So to jumpstart sales, the decision was made to radically simplify our enterprise deals to hosting companies, even though it meant less money in the short term. Essentially, we made the financially-puny Earthlink deal the standard, but included some potential upside, at least. The Site License Model (“SLM”) was about as simple as it gets: $5,000/month per physical datacenter, all the Urchin software you want, 1-page contract, no legalese, and nothing really to negotiate.</p><figure><figcaption>The accurséd Winstar blimp flew lazy circles around Jack’s house in Bird Rock while he negotiated a $400,000 deal with them, which was finally executed; we never saw a dime. Beware of customers with blimps!</figcaption></figure><p id="762c">The SLM was immediately a hit, and we signed deals with many of the largest US and European hosting companies in the ensuing year. Rackspace (now part of IBM), Everyone’s Internet (aka EV1 Servers), The Planet, mediatemple, and many others signed on, with several agreeing to multi-datacenter deals. By around fall 2003, we were cashflow-positive on these alone, and we were also selling more and more individual licenses to self-hosted organizations including much of the Fortune 500 and many university systems.</p><p id="1fa8">We had released most of the sales team during our near-death experience in 2001, but the few who remained — Paul Botto, Nikki Morrissey, and Megan Cash— worked for nothing and slowly, fitfully started selling our way out of the doldrums. Once we finally figured out a workable commission model — low base, high commission with fully retroactive kickers — these three positively rocked. Paul and Megan went on to work for Google for years (Nikki declined to move north in 2005, in favor of having kids.) Paul is still the best sales/BD guy with whom I’ve ever worked.</p><figure><figcaption>Paul Botto accepts the Employee of the Month plaque, March 3rd 2002; note the drum kit in the background.</figcaption></figure><p id="829c">After a failed attempt at running an international office in Tokyo (the moneys did not come[2]), we launched a channel program, which ran in parallel to direct sales. I still think for certain markets where English isn’t the primary language, it makes sense to have a local partner. Japan in particular generated strong sales for many years, and for this we can thank Jason Senn, our channel guy and chief office builder (we were too cheap to hire contractors.) Japan is also the most fun place ever to party it up in the name of business. On-sens and fire festivals? Yes please.</p><p id="2dbc">If Urchin 2 got us in the door, and Urchin 3 didn’t suck, Urchin 4 was actually pretty respectable. It had the then-Apple-esque brushed-aluminum look, some fancy-for-the-time interface elements, and most importantly, it had the UTM. The UTM, or Urchin Traffic Monitor, was an early method for augmenting Apache (or IIS, etc.) log files with cookies, such that unique visitors could be established. This method entailed a line of javascript in the &lt;HEAD&gt; of each page on the site, and a small modification to the webserver’s logging behavior. Most of our competitors at the time used either logs only (old school) or javascript/cookies only (WebSideStory, etc.), and both necessarily missed out on a lot of available information. Urchin was the first to use both data sources in one unified collection method, neatly contained in augmented access-log files. Nowadays pretty much everything you’d want can be had via the cookie method (á la GA), but analyzing logs still has its advantages.</p><figure><figcaption>Urchin 4 had an easter egg that no one ever found, to my knowledge. If you clicked a random “rivet” in the sexy brushed aluminum interface, you’d be treated to a photo of the illustrious Urchin dev team: Doug Silver, Nathan Moon, Paul, Jonathon Vance, Rolf Schreiber, and Jim Napier. Most of these guys are still at Google (as of Aug. 2016).</figcaption></figure><p id="8a26">Urchin 4 continued our tradition of supporting way, way too many random platforms (Google still has Urchin 4 help: <a href="https://support.google.com/urchin/answer/28276?hl=en&amp;ref_topic=7389" rel="noopener ugc nofollow" target="_blank">check out the OS support</a>… ever heard of Yellow Dog Linux?). I had this idea that platform-carpet-bombing might get us into some big corporations or universities running AIX or HP-UX, but no, everyone bought the Linux or Windows IIS versions. I guess I just liked buying random servers off ebay and getting Apache and a compiler running. We even compiled a NeXT version at one point, if I’m not mistaken. But no DEC, at least (I couldn’t get the machine to boot up).</p><figure><figcaption>Paul, looking here like a cartel drug lord, withdrew something like $53,000 in cash for our xmas bonuses in 2004. Funny thing is, Google also gave out actual cash money bonuses for years after we joined — millions of dollars in currency. Great minds think alike I guess. Anyway, it was fun and no one got robbed. This was December 17, 2004.</figcaption></figure><p id="7e7a">Urchin 4 was the first release I really felt could compete against anyone, and not just with regard to back-end performance. But Urchin 5 was superior in every way, and I’m sure thousands of instances still run to this day. If anything, Urchin 5 was just too much of a good thing. Almost every menu item had submenus upon submenus. It was pretty overwhelming and dry, but analytics nerds dug it.</p><figure><figcaption>I’m embarrassed to see that “hits” was still part of Urchin at the time. <a href="http://www.kaushik.net/avinash/" rel="noopener ugc nofollow" target="_blank">Avinash</a> is rolling his eyes right now.</figcaption></figure><p id="5158">Urchin 5 had e-commerce/”ROI” tracking, the Campaign Tracking Module, and multiserver versions that could all conspire to get the price pretty high. But the real killer feature IMHO, which was released in Urchin 6, was individual visitor history drill-down. If this sounds potentially, um, sensitive, that’s because it is. Google wouldn’t touch this feature and it was summarily axed, never to return.</p><figure><figcaption>Individual visitor history drilldown — potentially controversial I guess. But at least there wasn’t a “composite sketch” of the visitor. That would have been SO COOL. This is Urchin 6.</figcaption></figure><p id="7c7b">Up through Urchin 5, we’d been a traditional licensed-software outfit — you pay us money, you own the software. But by 2004 it was obvious we needed a hosted version (“hosted” would later become “cloud”, but we didn’t know that yet.) So we bought a bunch of servers, upgraded our T1, and released Urchin 6, which was available in on-premises (you run it yourself) or hosted by us, for $500/month(!) We didn’t have much time left as an independent concern, but businesses were surprisingly willing to pay up for the privilege of not having to run Urchin themselves. That business was a winner from day 1.</p><figure><figcaption>Paul Botto, me, and Brett at <a href="https://en.wikipedia.org/wiki/Search_Engine_Strategies" rel="noopener ugc nofollow" target="_blank">Search Engine Strategies</a> 2004, San Jose, where we first met the Google people.</figcaption></figure><p id="8c04">By Summer 2004, Urchin had the largest installed base among web analytics vendors on a number-of-websites basis[3]. Tradeshows had become fun again, and we planned our biggest spread yet for Search Engine Strategies 2004, in San Jose. It was there that two Google people, <a href="https://www.felicis.com/team/wesley-chan/" rel="noopener ugc nofollow" target="_blank">Wesley Chan</a> (PM) and David Friedberg (Corp. Dev.), went “shopping” as they put it, for a web analytics company. I guess they weren’t overly put-off by what they saw.</p><figure><figcaption>The Google Dance was a big party thrown in conjunction with the Search Engine Strategies tradeshow for a few years. During the “Dance” in ’04, Paul, Jack, and Brett, iirc, were off doing some sneaky corp. dev. work.</figcaption></figure><p id="0fb0">A few weeks later Google had made an offer for the company. By then we had interest from other quarters too — remember WebSideStory? They were actually a public company at the time and offered us more. But I think we made the right choice.</p><figure><figcaption>Brett, some dude named Al, and David Friedberg, in 2006</figcaption></figure><p id="fe31">Sidebar: Friedberg left Google in 2006 to start what became The Climate Corporation, which was <a href="http://techcrunch.com/2013/10/02/monsanto-acquires-weather-big-data-company-climate-corporation-for-930m/" rel="noopener ugc nofollow" target="_blank">acquired</a> for over $1 billion by Monsanto. Now that’s an exit. He also started <a href="http://metromile.com/" rel="noopener ugc nofollow" target="_blank">Metromile</a> and <a href="http://eatsa.com/" rel="noopener ugc nofollow" target="_blank">Eatsa</a>. Legit!</p><figure><figcaption>By 2004, we were feeling pretty smug about ourselves, despite still being puny. Design by <a href="https://about.me/mayorrock" rel="noopener ugc nofollow" target="_blank">Merrick</a>.</figcaption></figure><p id="e952">Selling the company was a needlessly rough process. It should have wrapped up just after the Google IPO in late 2004, but frankly the Google lawyers were prickly CYA types, demanding all kinds of IP-related protection — the four founders were personally on the hook if we were later found to be violating anyone’s patents etc. As we were now part of the big G, it seemed distinctly possible that WebTrends or someone would see fit to sue us. In hindsight, probably all boilerplate, but it was scary to risk it all like that. By the time we finally got it done, it was April 2005, and Google’s stock had doubled (half our payout was in stock). Oh well.</p><figure><figcaption>Brett prepares to “fax” the signed acquisition agreement back to Google. By this time we were sufficiently profitable that it was a tough decision to sell. Brett signed the actual, final paperwork in a tuxedo about 30 seconds before walking down the aisle at his wedding.</figcaption></figure><p id="688b">The still-young Google of 2005 was, I think, a lot more fun than the Mature Google/Alphabet of today. It was small enough (~3,000 employees) that everyone could get together for one (awesome, incredible) holiday party. And MC Hammer was always around. That was cool.</p><figure><figcaption>Jack, MC Hammer, and <a href="https://lowercasecapital.com/proprietor/" rel="noopener ugc nofollow" target="_blank">Chris Sacca</a>, circa 2005.</figcaption></figure><p id="feef">The day we joined, Eric Schmidt took time out of his day to hang out with us and learn about our flavor of web analytics. He immediately saw the potential with regard to Adwords spend, and was ever after helpful and available. I really liked the guy. Years later, Brett (then Sr. Director of Marketing) had an office right next to Eric, and they were bros. Kinda. I mean, billionaires are not like the rest of us.</p><figure><figcaption>Eric Schmidt gets to know the Urchin sales team, plus some others. L-R: Nick Mihailovski, Mike Chipman, Jim Napier, Megan Cash, Eric Schmidt, Paul Botto, Rolf Schreiber, Jason Senn, and Jack Ancone.</figcaption></figure><p id="94fa">Our first office at Google (for some of us) was the “fishbowl” of Building 42, in the nucleus of the Mountain View campus. We were actually really close to Larry &amp; Sergey for awhile. Sergey had this laser engraver in his office with a long air duct snaking down the hall to vent the gases. He’s a nut, that Sergey. The fishbowl was also home to a new Google engineer named Mike Stoppelman, whose brother would soon start a company called Yelp. Mike is SVP of engineering there as of this writing.</p><figure><figcaption>Urchin stuff would plague the Google campus for years after we joined. I’m sure some of it still lurks in supply cabinets here and there.</figcaption></figure><p id="0b35">The pervasiveness of Google Analytics now seems kind of a given, but in the spring of 2005, we were fairly panicked that its Google-fied release would be greeted with a shrug. So Wesley Chan, the Google PM who spearheaded the integration effort, commenced a daily “war room” and everyone was given milestones (<a href="https://medium.com/@brettc/implementing-okrs-a-tale-from-the-trenches-f4c42059fa17" rel="noopener">OKRs</a>, in Google land) with pretty tight timelines. Paul got a bunch of engineering tasks, I got a bunch of sales-ish/adoption tasks around penetrating the Fortune-500, Brett got marketing/PR/branding stuff, and Jack got BD/partnerships.</p><figure><figcaption>Camo’ was hot in 2005… this was the last shirt we made pre-Google, and the first we gave away at TGIF once we joined.</figcaption></figure><p id="61d5">It was a fun and hectic time, with much of the effort centered around bringing the rest of Google up to speed — some of us toured the nation visiting the various remote Google offices, small outposts that appreciated visitors from the mothership. By the time we thought we were ready for a public launch in November of 2005, we were sweating it. Would anyone care? Turns out they did. After we announced “Urchin from Google” was now free for any website in the world, the demand was sufficiently high that even Google’s infrastructure (well, the part allocated to us) was groaning, and the SRE team made us shut down signups until we could arrange the server resources etc. we needed to reopen.</p><figure><figcaption>Google Analytics was once knows as “Urchin from Google,” catchy right?</figcaption></figure><p id="434e">This is one of those nice-to-have problems, but a lot of people were still pissed off. Several months later, signups were reopened via the old invitation model, and the GA saturation we know today started happening in earnest.</p><figure><figcaption>Alden DeSoto (chief GA tech writer), Jeff Veen, Brett, Greg Veen, Ryan Carver, and Jeff Gillis (GA marketing) sporting the new (2006) Google Analytics track jackets, celebrating the re-launch. A few years hence the Veens and Ryan would start TypeKit, which was later acquired by Adobe. 2nd exit for those guys, nice.</figcaption></figure><p id="c4cf">Now, Google buys a lot of companies. Some of these are huge successes, like YouTube and Keyhole (Google Earth), but many just kind of scatter to the four winds, despite being worthy. <a href="https://en.wikipedia.org/wiki/Dodgeball_%28service%29" rel="noopener ugc nofollow" target="_blank">Dodgeball</a>, for example. I think this happens partially because of the way Google buys companies and partially because of big-company inertia/fog. Under some dollar amount (I’ve heard $50 million) all it took was one VP to say “buy them!” and it was done. Once they get onboarded, that VP maybe had left, or may have just been distracted. Turns out no one else cares, and the corporate soup subsumes the employees and the product melts into a shapeless saltine. In reality, no one at Google much cared about a product if it didn’t generate at least something like $100 million per year. Urchin of course didn’t either, but we were lucky enough to have some powerful allies: Wesley Chan and Eric Schmidt. Wesley was a PM (product manager) who understood Google needed effective analytics to drive Adwords spend. He was also dead determined not to let “his” acquisition be a failure, and he never let up until it could motor under its own power. But of course nothing can compare to having the CEO on your team, and lucky for us, Eric immediately got how web traffic analysis could positively affect Adwords. A few years later, Google did a big internal study with a bunch of “quants” running various models, and they pretty well proved an XX% increase in ad spending across the broad swath of customers studied. That was big money, with a “B.”</p><figure><figcaption>Up and to the right! Now that I’ve moved out of San Francisco, I wear Google shirts a lot more often.</figcaption></figure><p id="4484">In 2006, the Urchin people started drifting around Google, and some left. Today, my guess is about 12–15 original Urchin types are still there, and some still work on GA. Most notably, Paul is a senior VP of engineering, with hundreds of engineers reporting to him. He owns not just GA, but display ads too. Smart guy, that <a href="http://adage.com/article/digital/google-s-neal-mohan-heads-youtube-product-boss/301409/" rel="noopener ugc nofollow" target="_blank">Paul</a>.</p><figure><figcaption>The last Urchin-specIfic shirt we ever made, in about 2009(?). Little-known fact: Urchin was sold as standalone software until 2012, since many educational, government, and corporate customers wanted on-premises analytics software. Some still do, and former Urchinite Mike Chipman started a company to serve them — the product is compatible with Urchin databases and is called <a href="http://actualmetrics.com/" rel="noopener ugc nofollow" target="_blank">Angelfish</a>.</figcaption></figure></div></div><div><p id="daae">Urchin/GA have touched a lot of people, and that’s probably the most satisfying thing about this whole adventure. It’s also great to see Urchin people like <a href="https://www.linkedin.com/in/nick-mihailovski-1199264" rel="noopener ugc nofollow" target="_blank">Nick Mihailovski</a> and Nathan Moon do so well at Google. And of course Paul Muret, who is now a top exec in the engineering org. I’m also happy and very relieved that all our investors made money and got Google shares at 2005 prices. Thank you again for making this bumbling comedy of errors work out in the end. We (obviously) couldn’t have done it without you.</p><figure><figcaption>We were based in San Diego, after all</figcaption></figure></div><div><p id="5f21">Footnotes</p><ol><li id="a948">Laserdiscs: The 12" ones, like a shiny record. I’m sure some people still prefer them.</li><li id="7334"><em>The moneys will come</em> was the mantra of one Chi Kwan, president of our Japan division. Quite possibly the stupidest thing we ever did was open an office in Tokyo. It absolutely incinerated cash, and didn’t sell anything. We must have blown close to a million dollars on that poorly considered idea. That said, we did finally figure out how to make money in Japan — partner with a distributor. They took a 70% cut, but they did everything, including put it on a CD in an actual physical box. Hats off to <a href="https://www.runexy.co.jp/en/" rel="noopener ugc nofollow" target="_blank">Runexy</a>! And we have a fun phrase to say all these years later. Anytime I’m short on cash, I just repeat “the moneys will come,” and they always do.</li><li id="4b4d">Most-sites marketshare — this did not translate to most revenue. Kind of a vanity metric, but we got a lot of mileage out of it.</li></ol><figure><figcaption>Viking Urchin? Seriously?</figcaption></figure></div><div><p id="1005">Epilogue: Some lessons learned</p><p id="3e5a">If I had to do it over again, I’d do a few things differently, of course. Here’s a distillation of my/our painfully learnt truths, along with some reinvention of the wheel wisdom we had to live to believe. Dispensing generalized advice is always a dicey business, but I think I can defend these. Read with a grain of salt.</p><ol><li id="bd7d">Be willing to leave money on the table. I can’t emphasize this enough. Deal velocity is far more important than getting every dollar you “should” from a customer. Doing two deals for $1 each is better than one deal for $2.</li><li id="c05e">Reduce legalese, reduce text, shrink your contracts. One page is best. Somebody, probably an underling concerned with CYA, is actually going to have to read that 40-pages of lawyerspeak and it’s going to take that poor sap a long time. Make it so your counterpart on the deal reads it instead and execute the deal that day. Shun terms that you need a lawyer to interpret. Warren Buffett lent Goldman Sachs $5 BILLION on a hand-written note. It’s a legally binding agreement. Simple=less room for wiggling, and no excuse to delay.</li><li id="f2dd">Time kills all deals (just to recap 1 and 2).</li><li id="245b">Specialize sooner. Narrow your market. No one believes you if you say you serve all customers. Legitimacy/credibility are contingent on tight focus. No one can do it all, but even if you could, no one will believe it.</li><li id="681f">Put more thought into the right sales incentives and don’t limit the earnings of your sales people. Make commissions bigger as targets are hit, retroactively to all dollars — eg., maybe the commission is 8% up to $100k/month, then it becomes 10%, which applies to the entire $100k, not just the marginal amount above that. If your sales people make more than the CEO from time to time, you’re on the right track.</li><li id="622d">Pay commissions at least monthly — quarterly is too long to wait, and will hurt motivation. Sales people think short term, as they should.</li><li id="e47c">Accounting transparency — be more transparent with employees, sooner, until you are really uncomfortable. All employees should have access to a sales dashboard showing daily/monthly sales in real time, along with a “here’s our nut” indicator so everyone knows what to shoot for — profitability. The nut itself can be just a simple number, but the more transparent the better, even with salaries. They should be defensible, after all.</li><li id="7d91"><a href="https://play.google.com/store/books/details?id=HHJVIpbpSgsC&amp;utm_source=HA_Desktop_US&amp;utm_medium=SEM&amp;utm_campaign=PLA&amp;pcampaignid=MKTAD0930BO1&amp;gl=US&amp;gclid=CNmb7uj4-MwCFVRtfgod5PwLoQ&amp;gclsrc=ds" rel="noopener ugc nofollow" target="_blank">Overcommunicate organizational clarity</a>.</li><li id="544d">Hiring is a bitch, firing is harder. Everyone talks about hiring “A” players and sh*t like that. Of course, that implies you can divine which ones they are without seeing their work <em>at your company</em>. You’re better at hiring than I, if so. The reality is that you need to have the backbone to fire people that don’t work out. Do it as soon as it’s clear it’s the right move. Be generous with people you offload — if they speak positively of your company after the experience, that means you did it right. Do your best to hire the best (smarter than you is a good thing to strive for), but let’s admit the truth: it’s a crapshoot. Try hard, then cut loose your mistakes. Then there’s no fault, no guilt.</li><li id="33bc">More on hiring: Degrees are for working at Google. Also take a good look at highschool/college dropouts that are awesome. They usually can’t get a job at Google, so they might actually meet with you.</li><li id="a5c6">Money will burn a hole in your pocket. Consider only raising as much as you need to get to break-even, plus some % to account for how slow things are in the real world. Every $1 you raise is another $10 you have to sell for to make VCs happy. Moving the goalposts farther away makes it harder to score (sorry, I hoped to avoid sports metaphors).</li><li id="184f">And finally, the most important “rule” of all: if you want someone to read your email, make it short.</li></ol><figure><figcaption>At lease two of the UoDs were sporting cocktails</figcaption></figure></div><div><p id="6c2e">Thanks for reading! You rule.</p></div></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Researchers discover potentially catastrophic exploit present in AMD chips (103 pts)]]></title>
            <link>https://www.engadget.com/cybersecurity/researchers-discover-potentially-catastrophic-exploit-present-in-amd-chips-for-decades-161541359.html</link>
            <guid>41204160</guid>
            <pubDate>Fri, 09 Aug 2024 18:20:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.engadget.com/cybersecurity/researchers-discover-potentially-catastrophic-exploit-present-in-amd-chips-for-decades-161541359.html">https://www.engadget.com/cybersecurity/researchers-discover-potentially-catastrophic-exploit-present-in-amd-chips-for-decades-161541359.html</a>, See on <a href="https://news.ycombinator.com/item?id=41204160">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Security researchers have found a vulnerability in AMD processors that has persisted for decades, <a data-i13n="cpos:1;pos:1" href="https://www.wired.com/story/amd-chip-sinkclose-flaw/" rel="nofollow noopener" target="_blank" data-ylk="slk:according to reporting by Wired;cpos:1;pos:1;elm:context_link;itc:0;sec:content-canvas"><ins>according to reporting by </ins><em><ins>Wired</ins></em></a>. This is a fascinating security flaw because it was found in the firmware of the actual chips and potentially allows malware to deeply infect a computer’s memory.</p><p>The flaw was discovered by <a data-i13n="cpos:2;pos:1" href="https://ioactive.com/event/def-con-talk-amd-sinkclose-universal-ring-2-privilege-escalation/" rel="nofollow noopener" target="_blank" data-ylk="slk:researchers from the security firm IOActive;cpos:2;pos:1;elm:context_link;itc:0;sec:content-canvas"><ins>researchers from the security firm IOActive</ins></a>, who are calling the AMD-based vulnerability a “Sinkclose" flaw. This potentially allows hackers to run their own code in the most privileged mode of an AMD processor, System Management Mode. This is typically a protected portion of the firmware. The researchers have also noted that the flaw dates back to at least 2006 and that it impacts nearly every AMD chip.</p><div data-embed-anchor="e32ad5a2-115e-5267-88fe-f25baeffb710"><blockquote placeholder="" data-theme="light"><p>"Researchers warn that a bug in AMD’s chips would allow attackers to root into some of the most privileged portions of a computer..." New piece from <a href="https://twitter.com/WIRED?ref_src=twsrc%5Etfw" rel="nofollow noopener" target="_blank" data-ylk="slk:@WIRED;elm:context_link;itc:0;sec:content-canvas">@WIRED</a> featuring research from IOActive Principal Security Consultants, Enrique Nissim &amp; Krzysztof Okupski. <a href="https://t.co/UuvzC2qyGI" rel="nofollow noopener" target="_blank" data-ylk="slk:https://t.co/UuvzC2qyGI;elm:context_link;itc:0;sec:content-canvas">https://t.co/UuvzC2qyGI</a></p><p>— IOActive, Inc (@IOActive) <a href="https://twitter.com/IOActive/status/1821906484135129102?ref_src=twsrc%5Etfw" rel="nofollow noopener" target="_blank" data-ylk="slk:August 9, 2024;elm:context_link;itc:0;sec:content-canvas">August 9, 2024</a></p></blockquote></div><p>That’s the bad news. Now onto some better news. Despite being potentially catastrophic, this issue is unlikely to impact regular people. That’s because in order to make full use of the flaw, hackers would already need deep access to an AMD-based PC or server. That’s a lot of work for a random home PC, phew, but could spell trouble for corporations or other large entities.</p><p>This is particularly worrisome for <a data-i13n="cpos:3;pos:1" href="https://www.engadget.com/the-uks-ministry-of-defence-was-hacked-and-the-country-is-reportedly-blaming-china-121954779.html" data-ylk="slk:governments and the like;cpos:3;pos:1;elm:context_link;itc:0;sec:content-canvas"><ins>governments and the like</ins></a>. In theory, malicious code could burrow itself so deep within the firmware that it would be almost impossible to find. As a matter of fact, the researchers say that the code would likely survive a complete reinstallation of the operating system. The best option for infected computers would be a one-way ticket to the trash heap.</p><p>“Imagine nation-state hackers or whoever wants to persist on your system. Even if you wipe your drive clean, it's still going to be there,” says Krzysztof Okupski from IOActive. “It's going to be nearly undetectable and nearly unpatchable.”</p><p>Once successfully implemented, hackers would have full access to both surveil activity and tamper with the infected machine. AMD has acknowledged the issue and says that it has “released mitigation options” for data center products and Ryzen PC products “with mitigations for AMD embedded products coming soon.” The company has also published a <a data-i13n="cpos:4;pos:1" href="https://www.amd.com/en/resources/product-security/bulletin/amd-sb-7014.html" rel="nofollow noopener" target="_blank" data-ylk="slk:full list of impacted chips;cpos:4;pos:1;elm:context_link;itc:0;sec:content-canvas"><ins>full list of impacted chips</ins></a>.</p><p>AMD has also emphasized just how difficult it would be to take advantage of this exploit. It compares using the Sinkclose flaw to accessing a bank’s safe-deposit boxes after already bypassing alarms, guards, vault doors and other security measures. IOActive, however, says that kernel exploits — the equivalent of plans to get to those metaphorical safe-deposit boxes — exist readily in the wild. “People have kernel exploits right now for all these systems,” the organization told Wired. “They exist and they're available for attackers.”</p><p>IOActive has agreed to not publish any proof-of-concept code as AMD gets to work on patches. The researchers have warned that speed is of the essence, saying “if the foundation is broken, then the security for the whole system is broken.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Attaching to a virtual GPU over TCP (285 pts)]]></title>
            <link>https://www.thundercompute.com/</link>
            <guid>41203475</guid>
            <pubDate>Fri, 09 Aug 2024 16:50:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thundercompute.com/">https://www.thundercompute.com/</a>, See on <a href="https://news.ycombinator.com/item?id=41203475">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div id="Intro-section"><div><p><h2>One cloud instance for any task</h2></p></div><div id="w-node-_9ca3ddf5-0c6b-ee93-9de7-e2c6a345a5f5-0f15f585"><div id="w-node-_9ca3ddf5-0c6b-ee93-9de7-e2c6a345a5f6-0f15f585"><p><h3>Scalable</h3></p><p>Scale your usage up or down instantly, without limits, while only being billed for what you use</p></div><div id="w-node-_9ca3ddf5-0c6b-ee93-9de7-e2c6a345a606-0f15f585"><p><h3>Flexible</h3></p><p>Switch GPUs instantly with a single command without leaving your instance</p></div><div id="w-node-_9ca3ddf5-0c6b-ee93-9de7-e2c6a345a616-0f15f585"><p><h3>Simple</h3></p><p>Run your existing code on Thunder Compute without changes or config</p></div></div></div><div><div><p><h3>Switch from CPU-only to GPU with one command</h3></p><p>Save money by developing on your CPU. When you want to scale, access a cluster of GPUs on-demand.</p><div><div><p><h6>Flexible</h6></p><p>Switch to the GPUs you need, when you need them</p></div><div><p><h6>Serverless</h6></p><p>Never worry about config, quotas, or reservations again</p></div></div></div><p><img src="https://cdn.prod.website-files.com/65862b088e807ceb0f15f514/66b538824d3f378b4bd80c3f_Multi-gpu-ezgif.com-crop.webp" loading="lazy" sizes="(max-width: 767px) 90vw, (max-width: 991px) 43vw, 41vw" srcset="https://cdn.prod.website-files.com/65862b088e807ceb0f15f514/66b538824d3f378b4bd80c3f_Multi-gpu-ezgif.com-crop-p-500.webp 500w, https://cdn.prod.website-files.com/65862b088e807ceb0f15f514/66b538824d3f378b4bd80c3f_Multi-gpu-ezgif.com-crop.webp 800w" alt=""></p></div><div id="w-node-_9ca3ddf5-0c6b-ee93-9de7-e2c6a345a64c-0f15f585"><p><h2>Same budget, more impact</h2></p><p>With Thunder Compute, never pay for idle GPUs. Give developers direct access to GPUs, with the freedom to scale quickly.</p></div><div><div><p><h3>Industry partners</h3></p></div><p><a href="https://www.ycombinator.com/"><img src="https://cdn.prod.website-files.com/65862b088e807ceb0f15f514/6688a801e138560c10aa94ff_Svg_YCombinator-svg.svg" loading="lazy" alt="" height="75"></a><a href="https://aws.amazon.com/"><img src="https://cdn.prod.website-files.com/65862b088e807ceb0f15f514/6688aaaf72f4780a482b3547_AmazonWebservices_Logo-svg.svg" loading="lazy" alt="" height="75"></a><a href="https://cloud.google.com/"><img src="https://cdn.prod.website-files.com/65862b088e807ceb0f15f514/6688a7beb3b8db193b1367ec_google-cloud-3-svg.svg" loading="lazy" alt="" height="75"></a><a href="https://azure.microsoft.com/"><img src="https://cdn.prod.website-files.com/65862b088e807ceb0f15f514/6688a9b17edba368b93c0559_Microsoft_Azure-Logo.wine-svg%20(1).svg" loading="lazy" width="Auto" height="75" alt=""></a></p></div><div><div><p><h3>Get more out of every card</h3></p><p>GPUs at other cloud providers are utilized on-average 15% of the time, while 85% of what you pay goes to waste. With long-term reservations you have to guess how many GPUs you will need, resulting in shortages and overpayment.</p><div><div><p><h6>For Developers</h6></p><p>A cluster of high-performance GPUs at your fingertips without ever having to talk to IT</p></div><div><p><h6>For Enterprises</h6></p><p>Shrink your cloud budget by eliminating idle GPU time</p></div></div></div><p><img src="https://cdn.prod.website-files.com/65862b088e807ceb0f15f514/66b5436c663990b66dbfac1a_image%20(2).png" loading="lazy" sizes="(max-width: 767px) 90vw, (max-width: 991px) 43vw, 41vw" srcset="https://cdn.prod.website-files.com/65862b088e807ceb0f15f514/66b5436c663990b66dbfac1a_image%20(2)-p-500.png 500w, https://cdn.prod.website-files.com/65862b088e807ceb0f15f514/66b5436c663990b66dbfac1a_image%20(2)-p-800.png 800w, https://cdn.prod.website-files.com/65862b088e807ceb0f15f514/66b5436c663990b66dbfac1a_image%20(2)-p-1080.png 1080w, https://cdn.prod.website-files.com/65862b088e807ceb0f15f514/66b5436c663990b66dbfac1a_image%20(2)-p-1600.png 1600w, https://cdn.prod.website-files.com/65862b088e807ceb0f15f514/66b5436c663990b66dbfac1a_image%20(2).png 1818w" alt=""></p></div><div><div><h2>How it works</h2></div><div data-w-id="9ca3ddf5-0c6b-ee93-9de7-e2c6a345a69c"><div data-w-id="9ca3ddf5-0c6b-ee93-9de7-e2c6a345a69f"><p><h6>Run your code</h6></p><p>Use the Thunder Compute CLI to run your existing GPU code without any setup</p></div><div data-w-id="9ca3ddf5-0c6b-ee93-9de7-e2c6a345a6aa"><p><h6>Match with GPUs</h6></p><p>Behind the scenes, we automatically match your request to a GPU. Never worry about setting up an instance again</p></div><div data-w-id="9ca3ddf5-0c6b-ee93-9de7-e2c6a345a6b5"><p><h6>Securely process request</h6></p><p>Thunder never stores your data and uses end-to-end encryption for all data transfers</p></div><div data-w-id="9ca3ddf5-0c6b-ee93-9de7-e2c6a345a6c0"><p><h6>Pay less</h6></p><p>Thunder shares GPUs to reach utilization over 5x greater than other cloud platforms, saving you money</p></div></div></div><div><p><h2>Contact us</h2></p><p>Have any questions or want to see a demo? Contact our team to learn more</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[.INTERNAL is now reserved for private-use applications (453 pts)]]></title>
            <link>https://www.icann.org/en/board-activities-and-meetings/materials/approved-resolutions-special-meeting-of-the-icann-board-29-07-2024-en#section2.a</link>
            <guid>41203368</guid>
            <pubDate>Fri, 09 Aug 2024 16:36:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.icann.org/en/board-activities-and-meetings/materials/approved-resolutions-special-meeting-of-the-icann-board-29-07-2024-en#section2.a">https://www.icann.org/en/board-activities-and-meetings/materials/approved-resolutions-special-meeting-of-the-icann-board-29-07-2024-en#section2.a</a>, See on <a href="https://news.ycombinator.com/item?id=41203368">Hacker News</a></p>
<div id="readability-page-1" class="page"><section _ngcontent-ng-c3323332204="" main-content="" _ngcontent-ng-c507697319=""><!----><iti-sectioned-page _ngcontent-ng-c3323332204="" _nghost-ng-c2837300113=""><!----><!----><iti-fragment-container _ngcontent-ng-c2837300113="" _nghost-ng-c2540813831=""><div _ngcontent-ng-c2837300113="" id="section1"><h2 _ngcontent-ng-c2837300113=""><span _ngcontent-ng-c2837300113="">1. Consent Agenda</span><a _ngcontent-ng-c2837300113=""></a><!----><!----></h2><!----><!----><!----><!----><!----><div _ngcontent-ng-c2837300113="" id="section1.a"><!----><h3 _ngcontent-ng-c2837300113=""><span _ngcontent-ng-c2837300113="">a. Competition, Consumer Trust and Consumer Choice (CCT) Review Pending Recommendations</span><a _ngcontent-ng-c2837300113=""></a><!----><!----></h3><!----><!----><!----><!----><p>Whereas, on 1 March 2019, the Board <a href="https://www.icann.org/en/board-activities-and-meetings/materials/approved-resolutions-special-meeting-of-the-icann-board-01-03-2019-en#1.a">took action</a> on each of the 35 recommendations issued within the <a href="https://www.icann.org/en/system/files/files/cct-final-08sep18-en.pdf">Competition, Consumer Trust, and Consumer Choice (CCT) Review Team Final Report</a> dated 8 September 2018, as specified within the scorecard titled "<a href="https://www.icann.org/en/system/files/files/resolutions-final-cct-recs-scorecard-01mar19-en.pdf">Final CCT Recommendations: Board Action (1 March 2019)</a>". The Board resolved to place 17 CCT recommendations into pending status (in whole or in part), and committed to take further action on these recommendations subsequent to the completion of intermediate steps identified in the scorecard.</p>
<p>Whereas, on 22 October 2020, the Board <a href="https://www.icann.org/en/board-activities-and-meetings/materials/approved-resolutions-regular-meeting-of-the-icann-board-22-10-2020-en#2.a.rationale">resolved</a> to take action on 11 of the 17 CCT pending recommendations, as specified within the scorecard titled <a href="https://www.icann.org/en/system/files/files/cct-pending-recs-board-action-22oct20-en.pdf">"Competition, Consumer Trust, Consumer Choice Review Team (CCT-RT) Pending Recommendations: Board Action on 11 Recommendations"</a>.</p>
<p>Whereas, on 10 September 2023, the Board took <a href="https://www.icann.org/en/board-activities-and-meetings/materials/approved-resolutions-regular-meeting-of-the-icann-board-10-09-2023-en#section1.d">action</a> on two CCT pending recommendations as specified within the scorecard titled "<a href="https://www.icann.org/en/system/files/files/scorecard-board-action-assessment-cct-pending-ssr2-recs-10sep23-en.pdf">Board Action/Rationale on &amp; ICANN org Assessment of Competition, Consumer Trust, Consumer Choice Review (CCT) Pending Recommendations 14 and 15, and Second Security, Stability and Resiliency of DNS Review (SSR2) Recommendations 9.2, 9.3, 12.1, 12.2, 12.3, 12.4, 13.1, 13.2 and 14.2</a>".</p>
<p>Whereas, on 23 May 2024, the Board Organizational Effectiveness Committee (OEC) considered the study ICANN org commissioned to address the Board's 1 March 2019 request on CCT Recommendations 2, 3, 4, and 5 to "identify what types of data would be relevant in examining the potential impacts on competition and, whether that data is available, and how it could be collected [...]", and the resulting ICANN org assessment. The study identified that domain name price, including the types of pricing as requested by the CCT recommendations, is just one element that may signal competition in the domain name market. The study noted the potential limitations of studying pricing and suggested non-pricing related data elements that could be used to assess competition and inform the work of the next CCT Review Team.</p>
<p>Whereas, on 18 July 2024, the OEC made a recommendation to the ICANN Board to reject CCT Recommendations 2, 3, 4, and 5.</p>
<p>Resolved (2024.07.29.01), the Board rejects CCT Recommendations 2, 3, 4, and 5, as documented in the <a href="https://www.icann.org/en/system/files/files/scorecard-cct-pending-recs-29jul24-en.pdf">Board Action/Rationale on &amp; ICANN org Assessment of Competition, Consumer Trust, Consumer Choice Review (CCT) Pending Recommendations 2, 3, 4, 5</a>. The Board directs the ICANN Interim President and CEO, or her designee(s), to continue to evaluate and supplement existing metrics with additional data elements such as those identified within the study that would be indicative of competition and consumer welfare, as available and feasible.</p><div _ngcontent-ng-c2837300113="" id="section1.a.rationale"><!----><!----><h4 _ngcontent-ng-c2837300113=""><span _ngcontent-ng-c2837300113="">Rationale for Resolution 2024.07.29.01</span><a _ngcontent-ng-c2837300113=""></a><!----><!----></h4><!----><!----><!----><p><strong>Why is the Board addressing the issue?</strong></p>
<p>The Competition, Consumer Trust and Consumer Choice (CCT) Review is one of the four <a href="https://www.icann.org/resources/reviews/specific-reviews">Specific Reviews</a> anchored in Article 4, Section 4.6 of the ICANN Bylaws. Specific Reviews are conducted by community-led review teams, which assess ICANN's performance in fulfilling its commitments. Reviews contribute to ensuring that ICANN serves the public interest, are critical to maintaining an effective multistakeholder model, and help ICANN achieve its mission, as detailed in Article 1 of the Bylaws.</p>
<p>The CCT Review is the first iteration of this effort. It was initiated under the Affirmation of Commitments (AoC), and calls for an assessment of the extent to which the expansion of generic top-level domains (gTLDs) has promoted competition, consumer trust and consumer choice. It also serves to assess the effectiveness of the application and evaluation process during the 2012 round of the New gTLD Program.</p>
<p><strong>What is the proposal being considered?</strong></p>
<p>This proposed action is in furtherance of <a href="https://www.icann.org/en/board-activities-and-meetings/materials/approved-resolutions-special-meeting-of-the-icann-board-01-03-2019-en#1.a">resolution</a> 2019.03.01.04 to place 17 CCT recommendations in "pending" status.</p>
<p>CCT Recommendations 2, 3, 4, and 5 respectively call for ICANN org to collect data on wholesale pricing for legacy gTLDs, transactional pricing for the gTLD marketplace, retail pricing for the domain marketplace, and secondary market data.</p>
<p>The Board recalls the Registries Stakeholder Group (<a href="https://mm.icann.org/pipermail/comments-cct-final-recs-08oct18/2018q4/000007.html">RySG</a><a href="https://mm.icann.org/pipermail/comments-cct-final-recs-08oct18/2018q4/000007.html">)</a>'s comment on the CCT Final Report that expressed some concerns, notably that "price information is generally business sensitive", and there is a lack of clarity on who will have access to such data once collected, who will "arbitrate access to the data, and to what extent" ICANN commissioned a report to identify what types of information are probative to assess the competition within the DNS market. The report demonstrates that domain name price, including the different types of pricing data as requested by the CCT, is just one of the elements that may signal competition in the domain name market. While lower prices could be one measure of benefit, offering innovation or a better product (not necessarily at a lower price) is another key element of measuring possible consumer benefit and competitive impact. The report discusses that some innovations within new gTLDs introduced after the 2012 New gTLD round, provide enhanced safeguards to consumers, such as those associated with special use TLDs such as .bank, providing heightened registration requirements within those TLDs. The Board notes that contracts with gTLD Registry Operators and accredited Registrars do not provide ICANN with the ability to collect the requested pricing data. Further, ICANN does not have access to pricing data related to country code top-level domains (ccTLDs), either through ccTLD managers or the registrars through which most of them are offering registrations. ICANN also does not have access to secondary market data on sales of domain names, which are not governed by ICANN agreements, either. Moreover, the Board notes that the Registries Stakeholder Group, <a href="https://mm.icann.org/pipermail/comments-cct-final-recs-08oct18/2018q4/000007.html">RySG</a>, stated that "not only should ICANN not involve itself with pricing studies, using parties' contracts with ICANN as a mechanism to force its production is terribly inappropriate". The Board is mindful that even if ICANN were to have access to such pricing data, the risks of collection and maintenance of such data within ICANN raise questions about the propriety of ICANN becoming a clearinghouse for pricing-related data, and the potential for ICANN to be used as a source for competitors to access sensitive pricing data on their competitors. The Board acknowledges that the CCT Review Team suggests that ICANN could outsource the compilation and storage of pricing related data to a third party, however that still creates a clearinghouse of pricing data under ICANN's control and does not mitigate the Board's concerns.</p>
<p>As a result, the Board rejects CCT Recommendations 2, 3, 4, and 5.</p>
<p>The Board acknowledges that the report provides substantial information on the types of non-pricing data elements that might support an evaluation of competition in the DNS market. The report states that, in forming the list of non-pricing data elements, the author "[has] not determined whether ICANN is able to access the items on the list. To the extent that a given item is not available to ICANN, obviously no analysis based on that item could be conducted." The Board notes that the ICANN org has already started an evaluation of the data points identified to assess if the data is available to ICANN, could be made available through contracts, or might be available through third-party data sources. Through this, ICANN has the opportunity to collect and maintain data that could meet the CCT Review Team's goal "to have access to data for use in evaluating competition within future reviews," beyond pricing.</p>
<p><strong>Which stakeholders or others were consulted?</strong></p>
<p>The Board received community feedback as part of the <a href="https://www.icann.org/en/public-comment/proceeding/competition-consumer-trust-and-consumer-choice-review-team-cct-final-report--recommendations-08-10-2018">public comment proceeding</a> on the CCT Final Report.</p>
<p>ICANN commissioned a third party, an economist, to address the ICANN Board's 1 March 2019 request to "identify what types of data would be relevant in examining the potential impacts on competition and, whether that data is available, and how it could be collected [...]" to inform the Board's decision.</p>
<p><strong>What significant materials did the Board review?</strong></p>
<p>The Board considered various significant materials and documents. In addition to the study, the Board consulted the review team's final report, and the <a href="https://itp.cdn.icann.org/en/files/specific-reviews/report-comments-cct-final-recs-01feb19-en.pdf" data-linktype="fileRedirectUrl">Staff Report of Public Comment Proceeding on Competition, Consumer Trust, and Consumer Choice Review Team (CCT) Final Report &amp; Recommendations</a>.</p>
<p><strong>Are there positive or negative community impacts?</strong></p>
<p>Taking action on the four CCT pending recommendations contributes to further addressing the outcome of the Specific Review, and enhancing ICANN's accountability.</p>
<p><strong>Are there fiscal impacts or ramifications on ICANN (strategic plan, operating plan, budget); the community; and/or the public?</strong></p>
<p>None.</p>
<p><strong>Are there any security, stability or resiliency issues relating to the DNS?</strong></p>
<p>None.</p>
<p><strong>Is this decision in the public interest and within ICANN's mission?</strong></p>
<p>This action is in the public interest as it is a fulfillment of ICANN Bylaws, as articulated in Section 4.6. It is also within ICANN's mission and mandate. ICANN reviews are an important and essential part of how ICANN upholds its commitments.</p>
<p><strong>Is this either a defined policy process within ICANN's Supporting Organizations or ICANN's Organizational Administrative Function decision requiring public comment or not requiring public comment?</strong></p>
<p>None required.</p><!----><!----></div><!----><!----></div><div _ngcontent-ng-c2837300113="" id="section1.b"><!----><h3 _ngcontent-ng-c2837300113=""><span _ngcontent-ng-c2837300113="">b. Contract Extension with Provider for Implementation and gTLD Application Process Management Resources for New gTLD Program: Next Round</span><a _ngcontent-ng-c2837300113=""></a><!----><!----></h3><!----><!----><!----><!----><p>Whereas, ICANN org has identified a critical need for qualified vendors with specialized expertise and capabilities to manage the size, scope, and complexities of the New gTLD Program: Next Round (Next Round) as they relate to development and management of the governance structure at a program level and to help deliver the scope of the project and achieve project objectives.</p>
<p>Whereas, the resources provided by the selected vendor have already demonstrated their value and expertise as part of the initial engagement that org entered into.</p>
<p>Whereas, ICANN org and the Board Finance Committee have recommended that the ICANN Board authorize the ICANN Interim President and CEO, or her designee(s), to take all steps necessary to extend the contract with the selected vendor through [Redacted – Confidential Negotiation Information], and make disbursements in furtherance of that extension, in an amount not to exceed [Redacted – Confidential Negotiation Information].</p>
<p>Resolved (2024.07.29.02), the Board authorizes the Interim President and CEO, or her designee(s), to take all steps necessary to extend the contract with the selected vendor through [Redacted – Confidential Negotiation Information], and make disbursements in furtherance of that extension, in an amount not to exceed [Redacted – Confidential Negotiation Information].</p>
<p>Resolved (2024.07.29.03), specific items within this resolution shall remain confidential for negotiation purposes pursuant to Article 3, section 3.5(b) of the ICANN Bylaws until the President and CEO determines that the confidential information may be released.</p><div _ngcontent-ng-c2837300113="" id="section1.b.rationale"><!----><!----><h4 _ngcontent-ng-c2837300113=""><span _ngcontent-ng-c2837300113="">Rationale for Resolutions 2024.07.29.02 – 2024.07.29.03</span><a _ngcontent-ng-c2837300113=""></a><!----><!----></h4><!----><!----><!----><p>In order to facilitate the New gTLD Program: Next Round (Next Round) governance structure outlined above, ICANN org has a need for external resources to be dedicated full-time to how the work gets done, including maintenance of project schedules, budget and human resource tracking, reporting, decision logs, and other areas of implementation management.</p>
<p>Following review and conversations with various firms through a targeted Request for Proposal (RFP) process, the designated provider was selected. The provider was engaged under an initial contract, which was then extended. Based on a positive outcome of the work performed thus far, and to ensure a natural continuation of work carried out, an extension is recommended through [Redacted – Confidential Negotiation Information].</p>
<p>The program implementation manager was engaged by ICANN org in October 2023 and has been supporting the Program with the overall management and oversight of the New gTLD Program. To date, the externally sourced implementation manager has established effective planning and controls for the program and provided the structure needed to drive a successful program.</p>
<p>The gTLD Application Process (GAP) senior project manager was contracted in June 2024 and has been working with the ICANN GAP project lead, the project team and numerous external vendors, providing the structure, tools, and processes to enable the project team to successfully deliver the scope of the project and achieve project objectives. The GAP project is the largest project within the Next Round, responsible for the design, development and implementation of the gTLD application process and system. ICANN is anticipating an extension of the GAP Project Manager contract through [Redacted – Confidential Negotiation Information].</p>
<p>The total value of a contract extension with the selected vendor will be in an amount not exceed [Redacted – Confidential Negotiation Information].</p>
<p>This action is within ICANN's Mission and is in the public interest as it is important to ensure that, in carrying out its Mission, ICANN utilizes available funding in the most effective and efficient manner so as to be in the best interests of ICANN and the global Internet community.</p>
<p>This decision will have a fiscal impact, but the impact has already been accounted for in the FY25 New gTLD Program budget and will be for the future budgets as well. Further, this decision should not have a negative impact on the security, stability or resiliency of the domain name system, and likely will have a positive impact.</p>
<p>This is an Organizational Administrative Function that does not require public comment.</p><!----><!----></div><!----><!----></div><div _ngcontent-ng-c2837300113="" id="section1.c"><!----><h3 _ngcontent-ng-c2837300113=""><span _ngcontent-ng-c2837300113="">c. Contract Extension with Provider for Vendor Management Resources for New gTLD Program: Next Round</span><a _ngcontent-ng-c2837300113=""></a><!----><!----></h3><!----><!----><!----><!----><p>Whereas, to help ensure cost efficiencies and risk mitigation, ICANN organization has identified a critical need for qualified vendors with specialized expertise and capabilities to manage the development of the vendor strategy and the Request for Proposal (RFP) requirements design for the evaluation of the New gTLD Program: Next Round (Next Round) applications.</p>
<p>Whereas, ICANN org and the Board Finance Committee (BFC) recommends that the ICANN Board authorize the Interim President and CEO, or her designee(s), to enter into an agreement with the selected vendor for an additional [Redacted – Confidential Negotiation Information], in an amount not to exceed [Redacted – Confidential Negotiation Information].</p>
<p>Resolved (2024.07.29.04), the Board authorizes the Interim President and CEO, or her designee(s), to contract for, and make disbursement in furtherance of, a [Redacted – Confidential Negotiation Information] of an existing contract with the selected vendor to provide a vendor strategist and a vendor sourcing consultant, in an amount not to exceed [Redacted – Confidential Negotiation Information].</p>
<p>Resolved (2024.07.29.05), specific items within this resolution shall remain confidential for negotiation purposes pursuant to Article 3, section 3.5(b) of the ICANN Bylaws until the President and CEO determines that the confidential information may be released.</p><div _ngcontent-ng-c2837300113="" id="section1.c.rationale"><!----><!----><h4 _ngcontent-ng-c2837300113=""><span _ngcontent-ng-c2837300113="">Rationale for Resolutions 2024.07.29.04 – 2024.07.29.05</span><a _ngcontent-ng-c2837300113=""></a><!----><!----></h4><!----><!----><!----><p>In order to support the New gTLD Program: Next Round (Next Round), ICANN org has a need for a vendor strategist and a vendor sourcing consultant to help define the Next Round vendor risk management and sourcing strategy in line with the New gTLD Program's objectives and needs. This includes the mitigation model for conflict of interest and subjectivities of certain deliverables, segregation of duty requirements, determination of overall number of vendors required to execute the Program with cost effectiveness in mind, and risk mitigation for the unknown volume and duration of work assigned to each vendor.</p>
<p>Following review and conversations with various firms, through a targeted RFP process, the designated provider was selected. The provider has been engaged under a 3-month initial contract. Upon a positive outcome of the initial contract, an extension is recommended in order to continue working with the existing vendor strategist and the vendor sourcing consultant for the following reasons:</p>
<ul>
<li>Experience and quality of resources leading organizations through vendor strategy and RFP development.</li>
<li>Lower cost of resources compared with other vendors.</li>
<li>Most experienced candidates, with the most applicable skillset, at the lowest cost.</li>
<li>Internal ICANN staff have limited experience in sourcing contracts of this size with large and experienced firms.</li>
<li>Expertise in how to set the strategy on utilization of vendors and techniques to structure RFPs in a way to minimize cost and risk.</li>
</ul>
<p>The vendor strategist will develop the vendor sourcing strategy and pricing model for multiple vendors delivering the gTLD application process capabilities, manage the development of vendor requirements and performance criteria to be included in the Request for Proposals (RFPs), and support ICANN staff in the negotiation of vendor contracts, and establishment of vendor governance and oversight, for the New gTLD Program Next Round.</p>
<p>The vendor sourcing consultant will work closely with the vendor strategist, vendor management team, procurement team, and various subject matter experts to ensure timely and effective development of RFP materials, training and on-boarding of vendors, and engagement with a number of vendors to carry on the application processing of the Next Round.</p>
<p>The total value of the [Redacted – Confidential Negotiation Information] contract extension with the selected vendor will be in an amount not exceed [Redacted – Confidential Negotiation Information].</p>
<p>This action is within ICANN's Mission and is in the public interest as it is important to ensure that, in carrying out its Mission, ICANN utilizes available funding in the most effective and efficient manner so as to be in the best interests of ICANN and the global Internet community.</p>
<p>This decision will have a fiscal impact, but the impact has already been accounted for in the FY25 New gTLD Program budget and will be for the future budgets as well. Further, this decision should not have a negative impact on the security, stability or resiliency of the domain name system, and likely will have a positive impact.</p>
<p>This is an Organizational Administrative Function that does not require public comment.</p><!----><!----></div><!----><!----></div><!----><!----></div><div _ngcontent-ng-c2837300113="" id="section2"><h2 _ngcontent-ng-c2837300113=""><span _ngcontent-ng-c2837300113="">2. Main Agenda</span><a _ngcontent-ng-c2837300113=""></a><!----><!----></h2><!----><!----><!----><!----><!----><div _ngcontent-ng-c2837300113="" id="section2.a"><!----><h3 _ngcontent-ng-c2837300113=""><span _ngcontent-ng-c2837300113="">a. Reserving .INTERNAL for Private-Use Applications</span><a _ngcontent-ng-c2837300113=""></a><!----><!----></h3><!----><!----><!----><!----><p>Whereas, on 18 September 2020, the Security and Stability Advisory Committee (SSAC) published <a href="https://www.icann.org/en/system/files/files/sac-113-en.pdf">SAC113: SSAC Advisory on Private-Use TLDs</a> (SAC113), recommending that the ICANN Board ensure a string is identified and reserved at the top level of the Domain Name System (DNS) for private use, and that this particular string must never be delegated.</p>
<p>Whereas, the Board Technical Committee and ICANN organization have evaluated the feasibility of the SSAC's advice in <a href="https://www.icann.org/en/system/files/files/sac-101-v2-en.pdf">SAC113</a> and developed a proposed approach for implementing the advice.</p>
<p>Whereas, on 20 October 2020, Göran Marby, President and Chief Executive Officer of ICANN org <a href="https://www.icann.org/en/system/files/correspondence/marby-to-cooper-kuhlewind-22oct20-en.pdf">wrote</a> Alissa Cooper, Chair, Internet Engineering Task Force (IETF) and Mirja Kühlewind, Chair, Internet Architecture Board (IAB) requesting further discussion on the recommendation of SAC113.</p>
<p>Whereas, on 12 November, 2020 Alissa Cooper on behalf of the Internet Engineering Steering Group and Mirja Kühlewind on behalf of the IAB <a href="https://www.icann.org/en/system/files/correspondence/cooper-kuhlewind-to-marby-12nov20-en.pdf">responded</a>.</p>
<p>Whereas, on 22 September 2022, the Board passed <a href="https://www.icann.org/en/board-activities-and-meetings/materials/approved-resolutions-regular-meeting-of-the-icann-board-22-09-2022-en#2.b">resolution 2022.09.22.08</a> directing ICANN org to conduct a Public Comment proceeding on a proposed procedure to identify and reserve a string for private use in accordance with the recommendation contained in SAC113.</p>
<p>Whereas, the Board has considered the letter received from the <a href="https://www.icann.org/en/system/files/correspondence/cooper-kuhlewind-to-marby-12nov20-en.pdf">Internet Architecture Board</a>, the comments received during the <a href="https://itp.cdn.icann.org/en/files/root-system/public-comment-summary-report-proposed-procedure-selecting-top-level-domain-string-private-use-27-03-2023-en.pdf" data-linktype="fileRedirectUrl">public comment proceeding</a>, the additional input the SSAC provided in <a href="https://www.icann.org/en/system/files/correspondence/rasmussen-to-davies-26apr23-en.pdf">SAC2023-05</a>, ICANN org's <a href="https://www.icann.org/en/system/files/correspondence/davies-to-rasmussen-16may23-en.pdf">response to SAC2023-05</a>, and the implementation recommendations from the Board Technical Committee and ICANN org relating to this advice.</p>
<p>Whereas, the Board resolved (<a href="https://www.icann.org/en/board-activities-and-meetings/materials/approved-resolutions-regular-meeting-of-the-icann-board-10-09-2023-en#section1.e">2023.09.10.09</a>) to direct the "Interim President and CEO, or her designee(s), to assess SAC113 candidate strings using the assessment criteria IANA has developed. This work is expected to involve the IANA functions that ICANN operates. After IANA has selected a string, the Board directs the Interim President and CEO, or her designee(s), to conduct a Public Comment proceeding to gather feedback on whether the string proposed by IANA meets the criteria defined in SAC113 Section 4.1. The Interim President and CEO, or her designee(s) shall then prepare and submit a report on the public comments received during this proceeding to assist the Board in determining whether to permanently reserve the string or not."</p>
<p>Whereas, the Board has considered the comments received during the <a href="https://itp.cdn.icann.org/en/files/root-system/public-comment-summary-report-proposed-top-level-domain-string-private-use-16-04-2024-en.pdf" data-linktype="fileRedirectUrl">second public comment proceeding</a> on the proposed string for reservation .INTERNAL.</p>
<p>Resolved (2024.07.29.06), the Board reserves .INTERNAL from delegation in the DNS root zone permanently to provide for its use in private-use applications. The Board recommends that efforts be undertaken to raise awareness of its reservation for this purpose through the organization's technical outreach.</p><div _ngcontent-ng-c2837300113="" id="section2.a.rationale"><!----><!----><h4 _ngcontent-ng-c2837300113=""><span _ngcontent-ng-c2837300113="">Rationale for Resolution 2024.07.29.06</span><a _ngcontent-ng-c2837300113=""></a><!----><!----></h4><!----><!----><!----><p><strong><em>Why is the Board addressing the issue now?</em></strong></p>
<p>In resolution 2022.09.22.08, the Board approved a four-step process to implement the recommendation contained in SAC113.</p>
<p>The four proposed steps were:</p>
<ol>
<li>Conduct a Public Comment proceeding on the proposed approach in steps 2, 3 and 4;</li>
<li>Instruct IANA to choose the string using the criteria described in SAC113;</li>
<li>Conduct a Public Comment proceeding on the proposed string chosen by IANA in step 2; and</li>
<li>Pass a Board resolution to reserve the proposed string.</li>
</ol>
<p>ICANN org completed the Public Comment of the first step and published a report on its outcome. The Board then instructed ICANN org to choose a string using the criteria described in SAC113. IANA chose the string .INTERNAL. ICANN org then completed a second Public Comment on the chosen string and published a report on its outcome.</p>
<p><strong><em>What is the proposal being considered?</em></strong></p>
<p>The Board is considering whether to reserve .INTERNAL from insertion in the DNS root zone permanently. Applicants of the next and subsequent gTLD application rounds will not be able to apply for the .INTERNAL top-level domain.</p>
<p><strong><em>Which stakeholders or others were consulted?</em></strong></p>
<p>SAC113 discusses many of the efforts, both ongoing and abandoned, in the Internet Engineering Task Force (IETF) to try and resolve this issue. Since the publication of SAC113 the ICANN Board and the Internet Architecture Board (IAB) have exchanged correspondence about SAC113, briefly summarized below.</p>
<p>In the <a href="https://www.icann.org/en/system/files/correspondence/marby-to-cooper-kuhlewind-22oct20-en.pdf">first correspondence</a> from the ICANN Board to the IETF/IAB Chairs, the Board asked for clarification on what the definition of a 'technical use' was for domain names. Since the <a href="https://datatracker.ietf.org/doc/html/rfc2860">Memorandum of Understanding (MoU)</a> between ICANN and the IETF considers 'assignments of domain names for technical uses' something the ICANN Board cannot delegate, assign, or instruct IANA to reserve unilaterally.</p>
<p>In its <a href="https://www.icann.org/en/system/files/correspondence/cooper-kuhlewind-to-marby-12nov20-en.pdf">response</a>, the IAB/IETF states:</p>
<blockquote>
<p>We understand SAC113 to be a proposal for the ICANN [B]oard to allocate an ICANN Reserved Name, and we believe that it being reserved by ICANN would necessarily require that the chosen string also be removed from consideration for any technical use specified by the IETF. In keeping with our commitment to a single, global namespace (RFC 2826), such a reservation would ensure that the IETF would not consider any special-use name with the same string. Procedurally, if the ICANN board chooses to reserve a string following the advice of SAC113, we would expect the string to be reserved within the IANA-managed reserved domain registry rather than the special-use domain names registry.</p>
</blockquote>
<p>The IAB/IETF did not voice any objection to the ICANN Board permanently reserving a top-level string.</p>
<p>During the first Public Comment Proceeding on the Proposed Procedure for Selecting a Top-Level Domain String for Private Use, ICANN received comments from the following groups.</p>
<ul>
<li>Business Constituency (BC)</li>
<li>Governmental Advisory Committee (GAC)</li>
<li>Intellectual Property Constituency (IPC)</li>
<li>Network Information Centre for United Kingdom of Great Britain and Northern Ireland (UKGBNI)</li>
<li>Registries Stakeholder Group (RySG)</li>
<li>Security and Stability Advisory Committee (SSAC)</li>
</ul>
<p>Two individuals also provided feedback in their individual capacities.</p>
<p>During the second Public Comment Proceeding ICANN received comments from the following groups.</p>
<ul>
<li>At-Large Advisory Committee (ALAC)</li>
<li>Amazon.com, Inc.</li>
<li>Business Constituency (BC)</li>
<li>Google</li>
<li>I Love Domains - United States o' America (ILDUSA)</li>
<li>The IO Foundation (IO)</li>
<li>Registries Stakeholder Group (RySG)</li>
<li>Security and Stability Advisory Committee (SSAC)</li>
</ul>
<p>24 individuals also provided feedback in their individual capacities.</p>
<p><strong><em>What concerns or issues were raised by the community?</em></strong></p>
<p>Community members have noted that, even if a top-level string is reserved for technical use, there is no way to compel equipment vendors, protocol designers, and others to use it. It is also not possible to determine the extent to which the chosen string will be used. It is therefore conceivable that implementing SAC113 could ultimately have no material effect on the DNS.</p>
<p>It is also likely not possible to choose a single string that will enjoy universal agreement as being the most appropriate string for this purpose. Different stakeholders and individuals may have different ideas of what the best string is for this purpose, and it will not be possible to identify a single string that will be acceptable to all stakeholders. This consequence is, however, distinct from the ability to choose a string that adheres to the criteria set forth in SAC113.</p>
<p>ICANN org initiated the first public comment proceeding on the proposed process and published a <a href="https://itp.cdn.icann.org/en/files/root-system/public-comment-summary-report-proposed-procedure-selecting-top-level-domain-string-private-use-27-03-2023-en.pdf" data-linktype="fileRedirectUrl">report on the public comment proceeding</a>.</p>
<p>In response to the report of the first public comment proceeding the SSAC provided additional input via a correspondence, <a href="https://www.icann.org/en/system/files/correspondence/rasmussen-to-davies-26apr23-en.pdf">SSAC2023-05: SSAC Response to Public Comment Summary Report on Proposed Procedure for Selecting a Private Use TLD</a> in which the SSAC commented:</p>
<blockquote>
<p>The SSAC certainly acknowledges that much expertise exists within ICANN org to implement policy decisions. However, implementation plans, e.g., the work products of Implementation Review Teams, are routinely published for public comment before actual implementation. Therefore, it is disappointing that [the summary] response effectively dismisses the request to provide a more detailed selection process (implementation plan) and make that available for Public Comment before that process is undertaken.</p>
</blockquote>
<p>ICANN org sent a <a href="https://www.icann.org/en/system/files/correspondence/davies-to-rasmussen-16may23-en.pdf">response to SSAC2023-05</a> describing the procedure and noting that the Board still had to make a decision on whether or not to proceed with instructing IANA to select a string for reservation.</p>
<p>The Board then proceeded with instructing ICANN org to proceed with choosing a string for reservation with Board resolution 2023.09.10.09. IANA then proposed .INTERNAL and initiated a <a href="https://itp.cdn.icann.org/en/files/root-system/public-comment-summary-report-proposed-top-level-domain-string-private-use-16-04-2024-en.pdf" data-linktype="fileRedirectUrl">second public comment proceeding</a>. Two themes were identified in the comments received that did not agree with the proposal.</p>
<p>The first was that .INTERNAL was too long. Six respondents to the Public Comment believed the selected string to be too long.</p>
<p>Additionally, one respondent believed that the string was not meaningful enough. This respondent viewed the analysis as insufficient to demonstrate the meaningfulness of the string, and concluded the assessment may need to be performed again.</p>
<p><strong><em>What significant materials did the Board review?</em></strong></p>
<p>The Board has reviewed SAC113, an Options Paper developed by ICANN org staff, correspondence between ICANN and the IAB, the MoU between ICANN and the IETF, the Public Comment Summary Report of the Proposed Procedure for Selecting a Top-Level Domain String for Private Use Public Comment, SSAC2023-05, ICANN org's response to SSAC2023-05, and the summary report on the second Public Comment Proceeding.</p>
<p><strong><em>What factors did the Board find to be significant?</em></strong></p>
<p>The Board recognizes that the problem highlighted in SAC113 is a legitimate and significant one that could, if not addressed, materially affect the DNS. Reserving .INTERNAL will not only close out SAC113, but also resolve a longstanding issue. Network administrators unable to use a name in the global DNS for their private, or internal, uses can now safely use .INTERNAL.</p>
<p><strong><em>Are there positive or negative community impacts?</em></strong></p>
<p>A positive impact from this Board resolution is to complete the process to provide a designated namespace for the private use of vendors and other users of the DNS. A negative impact is that there will be one fewer meaningful names available for delegation in the root zone.</p>
<p><strong><em>Are there fiscal impacts or ramifications on ICANN (strategic plan, operating plan, budget); the community; and/or the public?</em></strong></p>
<p>No additional fiscal impact is anticipated as a result of reserving .INTERNAL for private use.</p>
<p><strong><em>Are there any security, stability or resiliency issues relating to the DNS?</em></strong></p>
<p>The SSAC has identified many security, stability, and resiliency issues associated with the uncoordinated use of private-use names in SAC113. It is impossible to determine the extent to which reserving a string for private use will alleviate these issues. However, it will not introduce any new security, stability or resiliency issues. It will also not increase the severity of any known and existing security, stability, or resiliency issues.</p>
<p><strong><em>Is this decision in the public interest and within ICANN's mission?</em></strong></p>
<p>Reserving a string from delegation permanently is in the public interest for the reasons outlined in this resolution and rationale. It is also within the scope of ICANN's mission as described in the Bylaws. Specifically, Section 1.1 (a) (i) which states: "[ICANN] Coordinates the allocation and assignment of names in the root zone of the Domain Name System [..]".</p>
<p>In its <a href="https://www.icann.org/en/system/files/correspondence/cooper-kuhlewind-to-marby-12nov20-en.pdf">letter to the Board</a>, the IAB/IETF agreed that this reservation was within the scope of ICANN based on <a href="https://datatracker.ietf.org/doc/html/rfc2860">ICANN's MoU with the IETF</a>.</p>
<p>During the <a href="https://itp.cdn.icann.org/en/files/root-system/public-comment-summary-report-proposed-procedure-selecting-top-level-domain-string-private-use-27-03-2023-en.pdf" data-linktype="fileRedirectUrl">first public comment proceeding</a> there were no comments received stating that this reservation was not in the public interest or that it was not within ICANN's mission.</p>
<p><strong><em>Is this either a defined policy process within ICANN's Supporting Organizations or ICANN's Organizational Administrative Function decision requiring public comment or not requiring public comment?</em></strong></p>
<p>Reserving a string from delegation permanently is neither a defined policy process with ICANN's supporting organizations nor an ICANN administrative function. The Public Comment proceedings outlined in the four-step implementation plan are not required by the ICANN Bylaws, but are part of the proposed process for implementing SAC113. The purpose of this specific Board action is to finalize this process by reserving .INTERNAL permanently, thereby preventing applicants of the next and subsequent gTLD application rounds from applying for it.</p><!----><!----></div><!----><!----></div><div _ngcontent-ng-c2837300113="" id="section2.b"><!----><h3 _ngcontent-ng-c2837300113=""><span _ngcontent-ng-c2837300113="">b. GAC Advice in ICANN80 Communiqué Regarding the Applicant Support Program</span><a _ngcontent-ng-c2837300113=""></a><!----><!----></h3><!----><!----><!----><!----><p>Whereas, the Governmental Advisory Committee (GAC) met during the ICANN80 meeting in Kigali, Rwanda and issued advice to the ICANN Board in a <a href="https://gac.icann.org/advice/communiques/ICANN80%20GAC%20Communique-zh.pdf">communiqué</a> on 17 June 2024 (ICANN80 Kigali Communiqué).</p>
<p>Whereas, the ICANN80 Kigali Communiqué was the subject of an exchange between the Board and the GAC on 15 July 2024.</p>
<p>Whereas, in a 12 July 2024 <a href="https://gnso.icann.org/sites/default/files/policy/2024/correspondence/dibiase-to-sinha-12jul24-en.pdf">letter</a>, the GNSO Council provided its feedback to the Board concerning advice in the ICANN80 Kigali Communiqué relevant to Applicant Support Program (ASP) and Auctions: Mechanisms of Last Resort/Private Resolution of Contention Sets in New gTLDs.</p>
<p>Whereas, the Board developed a scorecard to respond to the GAC's advice in the ICANN80 Kigali Communiqué, taking into account the dialogue between the Board and the GAC and the information provided by the GNSO Council.</p>
<p>Whereas, the Board has noted that GAC Consensus Advice item 1.a.i does not align with the Board-adopted GNSO Guidance Process ("GGP") for Applicant Support Guidance Recommendation 9 as well as the New gTLD Subsequent Procedures ("SubPro") Policy Development Process Recommendation 17.1.</p>
<p>Whereas, the Board has identified concerns with GAC Consensus Advice item 1.a.ii including concerns regarding the feasibility of implementation and potentially significantly increasing the risks of conflicts of interest and legal challenges over ASP evaluation and related decision-making.</p>
<p>Whereas, the Bylaws <a href="https://www.icann.org/resources/pages/governance/bylaws-en/#article12">require</a> that "[i]n the event that the Board determines to take an action that is not consistent with Governmental Advisory Committee advice, it shall so inform the Governmental Advisory Committee and state the reasons why it decided not to follow that advice" and the Board and GAC are required to enter into a Bylaws Consultation process.</p>
<p>Resolved (2024.07.29.07), the Board adopts the scorecard titled "<a href="https://www.icann.org/en/system/files/files/scorecard-gac-advice-kigali-communique-board-action-29jul24-en.pdf">GAC Advice – ICANN80 Kigali Communiqué: Actions and Updates (29 July 2024)</a>" in response to items of GAC advice in the ICANN80 Kigali Communiqué.</p>
<p>Resolved (2024.07.29.08), the Board has determined that it intends to take an action that is not consistent or may not be consistent with GAC Consensus Advice item 1.a.i. and 1.a.ii in the ICANN80 Kigali Communiqué concerning the ASP, and hereby initiates the required Board-GAC Bylaws Consultation Process. The Board will provide written notice to the GAC to initiate the process as required by the Bylaws Consultation Process.</p><div _ngcontent-ng-c2837300113="" id="section2.b.rationale"><!----><!----><h4 _ngcontent-ng-c2837300113=""><span _ngcontent-ng-c2837300113="">Rationale for Resolutions 2024.07.29.07 – 2024.07.29.08</span><a _ngcontent-ng-c2837300113=""></a><!----><!----></h4><!----><!----><!----><p><a href="https://www.icann.org/resources/pages/governance/bylaws-en/#article12">Article 12, Section 12.2(a)(ix)</a> of the ICANN Bylaws permits the GAC to "put issues to the Board directly, either by way of comment or prior advice, or by way of specifically recommending action or new policy development or revision to existing policies." In its ICANN80 Kigali Communiqué (17 June 2024), the GAC issued advice to the Board regarding the Applicant Support Program (ASP) and Auctions: Mechanisms of Last Resort/Private Resolution of Contention Sets in New gTLDs. The GAC also provided a follow-up to previous advice regarding the Applicant Support Program and Urgent Requests for Disclosure of Registration Data. Article 12, Section 12.2(a)(x) of the ICANN Bylaws states that the</p>
<blockquote>
<p>advice of the Governmental Advisory Committee on public policy matters shall be duly taken into account, both in the formulation and adoption of policies. In the event that the Board determines to take an action that is not consistent with Governmental Advisory Committee advice, it shall so inform the Governmental Advisory Committee and state the reasons why it decided not to follow that advice. Any Governmental Advisory Committee advice approved by a full Governmental Advisory Committee consensus, understood to mean the practice of adopting decisions by general agreement in the absence of any formal objection ("GAC Consensus Advice"), may only be rejected by a vote of no less than 60% of the Board, and the Governmental Advisory Committee and the Board will then try, in good faith and in a timely and efficient manner, to find a mutually acceptable solution. The Governmental Advisory Committee will state whether any advice it gives to the Board is GAC Consensus Advice.</p>
</blockquote>
<p>The ICANN Bylaws require the Board to take into account the GAC's advice on public policy matters in the formulation and adoption of policies. At this time, the Board's current thinking and approach to implementing SubPro recommendations related Topic 17 and GNSO Guidance Process Recommendations concerning the Applicant Support Program is inconsistent or could be viewed as inconsistent with item 1.a.i and 1.a.ii of the GAC's advice in the ICANN80 Kigali Communiqué. Regarding item 1.a.i, the Board notes concerns that changes to the processing and evaluation of ASP applications at this stage may reduce the time supported applicants will have to access resources, would not solve the challenge of how to decide which applicants receive support over others, and may delay the opening of the New gTLD Program: Next Round. Regarding item 1.a.ii, the Board notes concerns that implementing this advice may substantially increase the risk of conflict of interest and legal challenges over the evaluation and related decision-making of ASP applications. Furthermore, the Board notes that ICANN org is conducting a Request-for-Proposal (RFP) process to contract an independent Standing Application Review Panel (SARP) and that changes to this established plan and approach may delay the launch of the ASP. The Board is taking action today on the GAC Consensus Advice to the ICANN Board in the ICANN80 Kigali Communiqué, including the items related to the ASP and Auctions: Mechanisms of Last Resort/Private Resolution of Contention Sets in New gTLDs as described in the <a href="https://www.icann.org/en/system/files/files/scorecard-gac-advice-kigali-communique-board-action-29jul24-en.pdf">scorecard</a> dated 29 July 2024. This decision is in the public interest and within ICANN's mission, as it is fully consistent with ICANN's Bylaws for considering and acting on advice issued by the GAC.</p>
<p>In adopting its response to the GAC advice in the ICANN80 Kigali Communiqué, the Board reviewed various materials, including, but not limited to, the following materials and documents:</p>
<ul>
<li>ICANN80 Kigali Communiqué (17 June 2024): <a href="https://gac.icann.org/advice/communiques/ICANN80%20GAC%20Communique-zh.pdf">https://gac.icann.org/advice/communiques/ICANN80%20GAC%20Communique-zh.pdf</a></li>
<li>GAC Response to ICANN80 Kigali GAC Communiqué and Initial Feedback for Consideration (08 July 2024): <a href="https://gac.icann.org/contentMigrated/gac-response-to-icann80-kigali-gac-communique-and-initial-feedback-for-consideration">https://gac.icann.org/contentMigrated/gac-response-to-icann80-kigali-gac-communique-and-initial-feedback-for-consideration</a></li>
<li>The GNSO Council's review of the advice in the ICANN80 Kigali Communiqué as presented in the 12 July 2024 letter to the Board: <a href="https://gnso.icann.org/sites/default/files/policy/2024/correspondence/dibiase-to-sinha-12jul24-en.pdf">https://gnso.icann.org/sites/default/files/policy/2024/correspondence/dibiase-to-sinha-12jul24-en.pdf</a></li>
</ul>
<p>The adoption of the <a href="https://www.icann.org/en/system/files/files/scorecard-gac-advice-kigali-communique-board-action-29jul24-en.pdf">GAC scorecard</a> will have a positive impact on the community because it will assist with resolving the advice from the GAC concerning gTLDs and other matters. There are no foreseen fiscal impacts associated with the adoption of this resolution. Approval of the resolution will not impact security, stability or resiliency issues relating to the DNS. This is an Organizational Administrative function that does not require public comment.</p><!----><!----></div><!----><!----></div><div _ngcontent-ng-c2837300113="" id="section2.c"><!----><h3 _ngcontent-ng-c2837300113=""><span _ngcontent-ng-c2837300113="">c. Re-initiation of Fundamental Bylaws Amendment on Accountability Mechanisms</span><a _ngcontent-ng-c2837300113=""></a><!----><!----></h3><!----><!----><!----><!----><p>Whereas, the ICANN community, through the Cross Community Working Group on New gTLD Auction Proceeds (CCWG-AP), made a recommendation that utilization of ICANN's Reconsideration and Independent Review processes (collectively, ICANN Accountability Mechanisms) should be limited in certain circumstances (Recommendation 7). During ICANN organization's (ICANN org) work to implement the ICANN Grant Program aligned with the CCWG-AP's full set of recommendations, Recommendation 7 posed a unique challenge, as the CCWG-AP's language was too narrow based on the implementation design.</p>
<p>Whereas, the ICANN Board has taken a series of actions and communications to move forward with the ICANN Grant Program while trying to address the issues presented by the original language of Recommendation 7, including:</p>
<ul>
<li>26 October 2023 resolutions revisiting the Board's prior approval of Recommendation 7 and directing the development of a more general process for the ICANN community to indicate when ICANN's Accountability Mechanisms should not be available;</li>
<li><a href="https://www.icann.org/en/system/files/correspondence/sinha-to-clemente-et-al-02mar24-en.pdf">2 March 2024 letter</a> to the CCWG-AP's Chartering Organizations specifying a proposed change to Recommendation 7 to remove a reference to the "Independent Application Assessment Panel" so that Recommendation 7 could better achieve its goal of preserving auction proceeds for grants instead of funding challenges to decisions on grant applications; and</li>
<li><a href="https://www.icann.org/en/board-activities-and-meetings/materials/approved-resolutions-regular-meeting-of-the-icann-board-21-01-2024-en#section2.d">21 January 2024 initiation</a> of a Fundamental Bylaws amendment process setting forth the general process for the ICANN community to limit access to ICANN's Accountability Mechanisms.</li>
</ul>
<p>Whereas, ICANN received <a href="https://www.icann.org/en/public-comment/proceeding/proposed-bylaws-updates-to-limit-access-to-accountability-mechanisms-27-02-2024">Public Comment</a> on the proposal for a general process on limitation of access to ICANN Accountability Mechanisms, and the community was <a href="https://itp.cdn.icann.org/en/files/governance/public-comment-summary-report-proposed-bylaws-updates-limit-access-accountability-mechanisms-02-07-2024-en.pdf" data-linktype="fileRedirectUrl">not supportive</a> of that proposal. Multiple commenters urged ICANN to present a Fundamental Bylaws amendment drafted to specifically implement the anticipated update to Recommendation 7 as set forth in the Board's <a href="https://www.icann.org/en/system/files/correspondence/sinha-to-clemente-et-al-02mar24-en.pdf">March 2024 letter</a>.</p>
<p>Whereas, in response to the outreach to the Chartering Organizations of the CCWG-AP on a proposal to update Recommendation 7, many of the Chartering Organizations to the CCWG-AP have already indicated their support or non-objection. One Chartering Organization still seeks further information about the relationship between the updating of Recommendation 7 and the amendment of the ICANN Bylaws.</p>
<p>Whereas, in response to the ICANN community's comments and input, the updated Fundamental Bylaws amendment now under consideration will, if approved, amend the ICANN Reconsideration process (at ICANN Bylaws, Article 4, Section 4.2) and the Independent Review process (at ICANN Bylaws, Article 4, Section 4.3) to specifically exclude claims or disputes "relating to decisions to approve or not approve an application to the ICANN Grant Program" from each of the relevant mechanisms.</p>
<p>Whereas, the Board will not be in a position to approve the Fundamental Bylaws amendments unless and until it has adopted an updated Recommendation 7 from the CCWG-AP.</p>
<p>Resolved (2024.07.29.09), the ICANN Board directs the ICANN Interim President and CEO, or her designee(s), to re-initiate a Fundamental Bylaws Amendment Process under Article 25, Section 25.2 of the ICANN Bylaws, through the posting of the proposed amendments to Article 4, Section 4.2(d) and Section 4.3(c) of the Bylaws for public comment. The ICANN Board confirms that it is no longer pursuing the previously posted amendment to Article 4, Section 4.1 of the Bylaws.</p><div _ngcontent-ng-c2837300113="" id="section2.c.rationale"><!----><!----><h4 _ngcontent-ng-c2837300113=""><span _ngcontent-ng-c2837300113="">Rationale for Resolution 2024.07.29.09</span><a _ngcontent-ng-c2837300113=""></a><!----><!----></h4><!----><!----><!----><p>The Fundamental Bylaws amendment process initiated today is not a decision on the Bylaws language. It is instead the clearance for posting for public comment the proposed amendments to Article 4, Section 4.2(d) (Reconsideration) and 4.3(c) (Independent Review) that will specifically exclude the ICANN Accountability Mechanisms from being available for challenges to decisions to approve or not approve an application to the ICANN Grant Program.</p>
<p>The Cross-Community Working Group on New gTLD Auction Proceeds '(CCWG-AP) Final Report included Recommendation 7, which states in relevant part "Existing ICANN accountability mechanisms such as IRP or other appeal mechanisms cannot be used to challenge a decision from the Independent Project Applications Evaluation Panel to approve or not approve an application. Applicants not selected should receive further details about where information can be found about the next round of applications as well as any educational materials that may be available to assist applicants. The CCWG recognizes that there will need to be an amendment to the Fundamental Bylaws to eliminate the opportunity to use the Request for Reconsideration and Independent Review Panel to challenge grant decisions." Though the Board previously accepted Recommendation 7 on 12 June 2022, on 23 October 2023, the Board revisited that action because of implementation challenges that arose during the design of the ICANN Grant Program. The Board explained:</p>
<blockquote>
<p>In order to account for the design work that has progressed since the Board's June 2022 action which defines different stages of assessment of individual applications, from admissibility to eligibility to substantive evaluation by an Independent Application Assessment Panel, the Board expects the limitation to restrict access to ICANN's accountability mechanisms for all decisions on those individual applications, not limited only to those made by the Independent Application Assessment Panel (as stated within the CCWG-AP recommendation). Anything short of this comprehensive view makes it possible that some applicants could have access to ICANN's accountability mechanisms for decisions on their individual applications as long as that action wasn't taken by the Independent Application Assessment Panel. If allowed, this uneven access to the accountability mechanisms still risks the use of auction proceeds to defend against accountability challenges on individual application decisions in a manner the CCWG-AP wished to protect against.</p>
</blockquote>
<p>The Board, considering alternative means to achieve the CCWG-AP's stated goal, asked ICANN org to produce a more general Bylaws amendment that would provide a process through which the ICANN community could signal its intent to limit the use of ICANN's Accountability Mechanisms. A draft Bylaws amendment to Article 4, Section 4.1, drafted to support this more general approach was posted for Public Comment in April 2024. The comments received in that forum confirmed the ICANN community was not satisfied with and did not support the general process approach.</p>
<p>The ICANN Board also, in March 2024, sent a letter to the Chartering Organizations to the CCWG-AP explaining that an update to the CCWG-AP's Recommendation 7 to remove the words "from the Independent Project Applications Evaluation Panel" would cure much of the Board's concerns in supporting full implementation of the CCWG-AP's Recommendation 7. That letter confirmed that there are two dependencies to the Board approving the distribution of any grants within the ICANN Grant Program: (1) updating Recommendation 7 and (2) amending the Bylaws to confirm the restriction access to ICANN's Accountability Mechanisms. To date, ICANN has heard back from all Chartering Organizations, with all but one stating either support or non-objection to proceeding with the updated text. The seventh Chartering Organization noted a need for further clarification of how the Board would resolve the issue of the disfavored Bylaws proposal as posted for comment in April.</p>
<p>Today's action makes clear that the Board has heard the community's dissatisfaction with the proposal posted for public comment in April 2024. Multiple commenters to the April Public Comment forum suggested that the preferred path would be for ICANN to pursue a Fundamental Bylaws amendment that specifically excluded decisions on individual applications from the reach of each of ICANN's Accountability Mechanisms. That is exactly the option that the Board is moving forward today. As both the Reconsideration and Independent Review processes already enumerate excluded topics for claims or disputes, the proposed amendment set forth today adds to each of those exclusions, relying on language as likely to be approved within an updated Recommendation 7 ("cannot be used to challenge a decision to approve or not approve an application").</p>
<p>This is a further procedural step necessary to meet the community's conversation on the Fundamental Bylaws amendment process. The Board is not taking any substantive action today on the ICANN Grant Program, as such a decision on updating Recommendation 7 still requires further input from the ICANN community.</p>
<p>The ICANN Board looks forward to the community discussion over these proposed changes.</p>
<p>Today's action is directly related to how the ICANN community may hold ICANN accountable to its mission and work. It is in the public interest, and is aligned with ICANN's Bylaws, to seek public comment on changes to ICANN's Bylaws and the ICANN Accountability Mechanisms defined therein.</p>
<p>Initiating the Fundamental Bylaws Amendment process is not anticipated to result in any impact to the security, stability or resiliency of the Internet's DNS. Nor is this action anticipated to result in any budgetary or financial implications.</p>
<p>This is an Organizational Administrative Function decision requiring public comment.</p><!----><!----></div><!----><!----></div><!----><!----></div><div _ngcontent-ng-c2837300113="" id="section3"><h2 _ngcontent-ng-c2837300113=""><span _ngcontent-ng-c2837300113="">3. Executive Session</span><a _ngcontent-ng-c2837300113=""></a><!----><!----></h2><!----><!----><!----><!----><!----><div _ngcontent-ng-c2837300113="" id="section3.a"><!----><h3 _ngcontent-ng-c2837300113=""><span _ngcontent-ng-c2837300113="">a. Interim President and CEO At-Risk Compensation for Second Half of FY24</span><a _ngcontent-ng-c2837300113=""></a><!----><!----></h3><!----><!----><!----><!----><p>Whereas, all Board members have confirmed that they do not have a conflict of interest with respect to establishing the amount of payment to the Interim President and CEO for the second half of FY24 at-risk compensation component.</p>
<p>Whereas, the Compensation Committee recommended that the Board approve payment to the Interim President and CEO her annual at-risk compensation component for the second half of FY24.</p>
<p>Resolved (2024.07.29.10), the Board hereby approves a payment to the Interim President and CEO for her annual at-risk compensation component for the second half of FY24.</p>
<p>Resolved (2024.07.29.11), specific items within this resolution shall remain confidential as an action "relating to personnel or employment matters", pursuant to Article 3, section 3.5.b of the ICANN Bylaws.</p><div _ngcontent-ng-c2837300113="" id="section3.a.rationale"><!----><!----><h4 _ngcontent-ng-c2837300113=""><span _ngcontent-ng-c2837300113="">Rationale for Resolutions 2024.07.29.10 – 2024.07.29.11</span><a _ngcontent-ng-c2837300113=""></a><!----><!----></h4><!----><!----><!----><p>When the Interim President and CEO was engaged, both in her role as Special Advisor to the President and SVP Global Stakeholder Engagement, and again as Interim President and CEO, she was offered a base salary, plus an at-risk component of her compensation package. Consistent with all personnel with the ICANN organization, the Interim President and CEO is to be evaluated against specific goals, which the Interim President and CEO established in coordination with the Compensation Committee, which were previously approved by the Board.</p>
<p>The Interim President and CEO provided to the Compensation Committee her assessment of the progress toward her FY24 goals. The Compensation Committee discussed and agreed that the Interim President and CEO should be awarded her at-risk compensation for the second half of FY24 and recommended that the Board approve such payment to the Interim President and CEO. The Board agrees with the Compensation Committee's recommendation.</p>
<p>Taking this decision is in furtherance of ICANN's Mission and is in the public interest in that it helps ensure that Interim President and CEO is sufficiently compensated in line with her performance in furtherance of the Mission, and which reflects that her goals are consistent with ICANN's Strategic and Operating plans.</p>
<p>While the decision to pay the Interim President and CEO her at-risk compensation for the second half of FY24 will have a fiscal impact on ICANN, it is an impact that was contemplated in the FY24 budget. This decision will not have an impact on the security, stability or resiliency of the domain name system.</p>
<p>This is an Organizational Administrative Function that does not require public comment.</p><!----><!----></div><!----><!----></div><!----><!----></div><!----><!----></iti-fragment-container></iti-sectioned-page><p> Published on 31 July 2024 </p><!----><!----><iti-link-callout-container _ngcontent-ng-c3323332204="" _nghost-ng-c3766845712=""><iti-callout-container _ngcontent-ng-c3766845712="" _nghost-ng-c2288160836=""></iti-callout-container></iti-link-callout-container><!----></section></div>]]></description>
        </item>
    </channel>
</rss>